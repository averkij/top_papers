{
    "paper_title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?",
    "authors": [
        "Xinyi He",
        "Qian Liu",
        "Mingzhe Du",
        "Lin Yan",
        "Zhijie Fan",
        "Yiming Huang",
        "Zejian Yuan",
        "Zejun Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 1 4 2 1 . 7 0 5 2 : r SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Xinyi He1, Qian Liu2, Mingzhe Du3, Lin Yan2, Zhijie Fan2, Yiming Huang4 Zejian Yuan1, Zejun Ma2 1Xian Jiaotong University, 2TikTok 3National University of Singapore, 4University of California San Diego Code performance optimization is paramount in real-world software engineering and critical for productionlevel systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field. Date: July 16, 2025 Home: https://swe-perf.github.io Correspondence: Qian Liu (qian.liu@tiktok.com) 1. Introduction Recent advances in Large Language Models (LLMs) have significantly enhanced automated code generation and software development assistance, exemplified by tools like GitHub Copilot (Microsoft, 2025) and Cursor (Cursor, 2025). This progress has spurred growing interest in repository-level software engineering challenges in real-world settings (Jimenez et al., 2024). Recent work has introduced multiple benchmarks for evaluating LLM code correctness, including SWE-Bench (Jimenez et al., 2024) and SWE-Dev (Du et al., 2025), as well as frameworks such as AgentLess (Xia et al., 2024) and OpenHands (Wang et al., 2024) that aim to improve the accuracy of LLM-generated code. However, while correctness is foundational in production environments, performance optimization often yields more profound system-wide benefits (Nascimento et al., 2023, Mancebo et al., 2021, Pereira et al., 2021). Performance optimization is task traditionally requiring specialized human expertise and poses significant challenges for LLMs. This raises critical research question: Can language models effectively optimize code performance in real-world repositories? While software engineering and code performance optimization have progressed significantly as distinct fields, current benchmarks struggle to evaluate tasks demanding their integration, especially for repository-level code performance optimization. Repository-level software engineering benchmarks (Jimenez et al., 2024, Mündler et al., 2024, Du et al., 2025, Miserendino et al., 2025b) face particular challenges in supporting This work was conducted during Xinyi and Yimings internship at TikTok. SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 1: The workflow of our SWE-Perf benchmark which evaluates code performance optimization capabilities of language models. The benchmark evaluates language models by providing source code and performance-related tests, challenging them to generate optimized patches. Model performance is evaluated by the runtime gains on the tests, with expert performance as reference. open-source code performance optimization. Evaluating whether LLMs can achieve meaningful optimizations is hindered by the lack of human reference implementations for optimal efficiency, making it unclear whether code can be improved further. Meanwhile, existing benchmarks focused on code performance (Du et al., 2024, Huang et al., 2024, Liu et al., 2024) primarily target function-level optimizations for algorithmic problems, consequently neglecting the complexities of repository-scale optimization. This represents critical departure from real-world practice, where collaborative improvements between files and modules typically unlock far greater optimization potential than isolated function-level changes. To address these gaps, we propose SWE-Perf, new benchmark to evaluate the ability of LLMs to optimize code performance in real-world repository-level software engineering scenarios. As shown in Figure 1, the objective is to optimize the performance of given repository codebase in the context of target functions. Each sample produces patch that is applied to the original codebase, and the modified codebase is subsequently evaluated for performance improvements. To construct SWE-Perf, we first extracted pull requests (PRs) from popular GitHub repositories, selecting those with strong indications of performance optimization potential. We then rigorously filtered 100K PRs by evaluating unit test runtimes on both pre-patch (original) and post-patch (modified) codebases. This process resulted in 140 data examples exhibiting observable and stable performance gains, drawn from 9 widely used GitHub repositories. Each example comprises the repository codebase, target functions, performance-related tests, the expert-authored patch, an executable environment (e.g., Docker image), and all runtime metrics. Crucially, the expert patch serves not only to confirm the feasibility of improvement but also as human-derived gold standard against which to evaluate LLM-generated code performance optimization edits. We conducted evaluations on several leading LLMs under two experimental settings: oracle (file-level) and realistic (repo-level). The oracle setting assesses whether model can optimize code performance when given relevant file contexts, whereas the realistic setting evaluates whether model can serve as an autonomous agent capable of navigating and operating within the entire repository context without constraints. The experimental results indicate that, relative to expert performance, all LLMs exhibit significant code performance gaps, highlighting opportunities for further advancement. Furthermore, by comparing expert and model results, we analyzed the characteristics and limitations of models in handling performance enhancement tasks, thereby providing insights for further research on improving model performance. In 2 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? summary, our main contributions are as follows: We collect the SWE-Perf dataset, the first benchmark designed to evaluate the ability of language models to optimize code performance on real-world repositories. We propose repository-level performance optimization data collection pipeline, which includes systematic method for data collection and set of identification metrics. This framework can be easily extended to other repository-level software engineering datasets. We design evaluation metrics specifically targeting performance optimization and conduct evaluations across several LLMs under two settings. Our experimental results highlight the remaining challenges that must be addressed to meet the practical requirements of performance-aware code optimization. 2. Related Work Code Efficiency Recent datasets targeting code efficiency, including Mercury (Du et al., 2024), EFFIBENCH (Huang et al., 2024), EvalPerf (Liu et al., 2024), and KernelBench (Ouyang et al., 2025), primarily focus on function-level performance optimization. While valuable for isolated evaluation, these approaches overlook the complexity of real-world efficiency challenges that span multiple files and modules. This oversimplification limits their ability to benchmark models capabilities in addressing cross-cutting concerns such as dataflow refactoring or parallelism, where optimization potential is typically more substantial. Repository-Level SWE Tasks SWE-Bench (Jimenez et al., 2024) first introduced benchmark for repositorylevel software engineering tasks. Subsequently, related datasets including SWE-Gym (Pan et al., 2024), SWE-Lancer (Miserendino et al., 2025a), SWE-Flow (Zhang et al., 2025), and SWE-Dev (Du et al., 2025) were developed to support various purposes, including model training and unit test evaluation. However, these datasets are primarily tailored for tasks with well-defined objectives, such as bug fixing. In contrast, efficiency optimization represents an open-ended problem lacking standardized solutions. This introduces additional complexities, including identifying optimization targets, designing performance-oriented changes, and requiring long-context understanding, planning capabilities, and efficiency-specific domain knowledge, capacities not fully addressed by existing datasets. concurrent work, GSO (Shetty et al., 2025), identifies performance-improving commits by combining an LLM-based judge with code-change heuristics, whereas our approach leverages pull requests and runtime environments for identification. Approaches to SWE Tasks To tackle repository-level tasks, prior work has explored two major paradigms: pipeline-based, and agent-based approaches. Pipeline-based techniques, such as Agentless (Xia et al., 2024), SWE-Fixer (Xie et al., 2025), use staged workflows to solve SWE problem. Agent-based systems, like OpenHands (Wang et al., 2024), SWE-Agent (Yang et al., 2024), SWE-Smith (Yang et al., 2025) and SWESearch (Antoniades et al., 2025) enable iterative reasoning across multiple steps via autonomous agents. However, these methods were not originally designed for open-ended code performance optimization, leaving room for adaptation and further exploration. 3. SWE-Perf Dataset The SWE-Perf dataset is constructed by collecting data with performance improvement potential from popular GitHub repositories. It is designed to evaluate the capability of LLMs to optimize the performance 3 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 2: The data collection pipeline of our SWE-Perf benchmark. The pipeline consists of: (1) collecting pull requests from popular repositories, (2) measuring performance of original and modified codebases using unit tests, (3) identifying performance-optimizing pull requests, (4) verifying stable performance improvements through statistical testing, and (5) extracting optimization targets for both oracle and realistic settings. of real-world software repositories. In the following sections, we provide detailed description of task formulation (3.1), data collection (3.2), and data statistics and distribution (3.3). 3.1. Task Formulation As illustrated in Figure 1, the input to the SWE-Perf task consists of codebase and set of target functions. The output is performance optimization patch or code, which can be applied to the original codebase to generate new codebase. The incorporation of target functions into task inputs is employed to restrict the evaluation scope to performance-related tests. This approach is motivated by the following considerations: 1. Running the full set of unit tests for an entire repository can be prohibitively time-consuming, which hinders the practical applicability of the benchmark. For example, in the case of the xarry repository, there are on average over 220,000 test cases, and testing just single sample can take over one hour (on single-core CPU). 2. Given the large codebase of most repositories, there are potentially many locations where performance can be improved. Without targeted guidance, identifying and optimizing relevant regions poses significant challenges for both the model and the evaluation process. However, as language models continue to advance and become more capable of handling full-repository 4 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? optimization, we encourage future work to explore approaches that omit the performance-related unit tests and instead directly optimize the entire codebase. 3.2. Data Collection Figure 2 illustrates the data collection process, and the specific steps of each phase are as follows: Phase 1: Collect Pull Requests (PRs). This phase is conducted by following the methodologies of SWEbench (Jimenez et al., 2024) and SWE-GYM (Pan et al., 2024). First, we collect high-star, popular GitHub repositories. Specifically, we adopt the same 12 repositories used in SWE-bench. Second, we crawl PRs from these popular repositories. As our subsequent filtering criteria differ from those used in SWE-bench, in order to obtain more data, we re-crawl the aforementioned repositories. Third, we apply attribute filtering. In SWE-GYM and SWE-bench, two main filtering criteria are used: (1) Resolves an issue, and (2) Contributes tests. In this work, we retain only the first criterion. The second criterion is not adopted because our focus lies in whether the PR affects performance, specifically execution time, rather than whether it changes the correctness of unit tests. Therefore, we allow PRs that do not contribute tests, that is, PRs that do not modify unit tests. Phase 2: Measure CodeBases Performance. Each PR collected in Phase 1 contains original and modified codebases. In this phase, we evaluate the performance of all unit tests contained in these codebases. The goal is to identify PRs that lead to performance improvements. 1. Build environment. To ensure correct execution of each codebase, we follow the approach in SWE-GYM to build corresponding Docker image and executable Docker container for each codebase. Codebases that fail to build successfully are excluded. To ensure the comparability of performance measurements, we constrain each container to single CPU core and 16 GB memory (5 CPU cores in Phase 4 and evaluation). 2. Execute unit tests. For each codebase, we run all unit tests inside its corresponding Docker container using pytest. This is the most time-consuming step in the entire data collection pipeline. First, the total number of codebases is large. By this stage, we have collected total of 25,264 codebases. Second, each codebase often contains large number of unit tests. For example, in the xarry repository, each codebase contains, on average, over 220,000 test cases, and testing single codebase may take over one hour on single-core CPU. 3. Record Runtimes. We use pytest to collect the execution time (runtime) of each unit test within the codebase. These runtimes serve as the basis for identifying performance changes between original and modified codebases in the subsequent phase. Codebases for which runtimes cannot be successfully collected are excluded from further analysis. To minimize environmental effects and ensure comparability of performance measurements across codebases, we took two specific steps in this phase: 1. We standardized the execution environment by limiting each Docker container to use one CPU core and 16 GB of memory. 2. Each codebase was evaluated in three repeated experimental runs, producing three runtime measurements per unit test, which are subsequently used in Phase 3. Phase 3: Identify PRs with Performance Optimizing Pull Requests. Based on the performance data collected in Phase 2, we identified pull requests that demonstrate significant performance improvements attributable to the associated code modifications. 5 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? 1. Filter PRs with performance optimization. After Phase 2, for each PR, we have the performance (i.e., runtimes) of all unit tests in both the original and modified codebases, with each unit test measured in three repeated runs. Our goal is to identify stable and significantly improved unit tests. PRs that do not contain such unit tests are discarded. There are two filtering criteria: (1) Correctness: The unit test must pass in both the original and modified codebases, i.e., the pytest result must be pass in both cases. (2) Performance Ratio: To ensure that the improvement is substantial in practical terms, we compute the optimized ratio of the modified to original runtimes. The optimized ratio is computed per unit test per pull request as the mean of three experimental replicates. The ratio must be below specified threshold (0.3). The ratio is calculated as: where, is the runtime for each unit test. Ratio = Roriginal Rmodi ied Roriginal 2. Select unit tests executing human patches. To ensure performance improvements are attributable to the associated code modifications, we dynamically execute unit tests for each screened PR. We identify unit tests that, during dynamic execution: (1) exercise the patched code segments modified in the PR, and (2) do not execute any unit tests modified within the PR. Phase 4: Verify Stable Performance Improvements. Following the initial screening, we obtain preliminary dataset. To ensure stable performance improvements, we verify results through the following procedure, retaining only unit tests whose performance gain exceeds predefined threshold: 1. Add warm-up. To mitigate initialization-induced timing inaccuracies, before each performance measurement, we execute three performance-related unit tests to warm up the environment. 2. Execute 20 repetitions. To ensure runtime stability, we run each unit test 20 times. 3. Filter outliers. Runtime outliers within the 20 measurements are identified and removed using the Interquartile Range (IQR) method with threshold multiplier of 1. The filtering criteria are defined as: Let Q1 and Q3 represent the first and third quartiles of the runtime sample, respectively. The interquartile range is IQR = Q3 Q1. Data points ri satisfying either condition below are classified as outliers and removed: ri < Q1 IQR, ri > Q3 IQR (1) where, = 1 (the threshold multiplier). 4. Calculate statistical performance. To confirm stable performance improvement, We compute statistically significant minimum performance gain (δ) for each test case using Algorithm 1. This δ represents the maximum value where the modified versions runtime distribution remains statistically significantly faster than the original version under conservative adjustments (Mann-Whitney test, < p_threshold (0.1). Unit tests with δ exceeding the threshold (0.05) constitute the final SWE-Perf dataset. Phase 5: Extract Optimization Targets. Following the aforementioned four phases, we have filtered PR data and associated unit tests that demonstrate stable performance improvements. In this phase, we extract the target functions for model optimization. We categorize the task into two settings and extract target functions accordingly: Oracle and Realistic. 6 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Algorithm 1: Compute Statistically Significant Minimum Performance Gain (δ) Input : = [a1, a2, . . . , an] : Filtered runtimes (original version) = [b1, b2, . . . , bm] : Filtered runtimes (modified version) α : Significance level (default = 0.1) step : Gain increment step (default = 0.01) max_x : Maximum gain to test (default = 1.0) Output : δ : Conservative minimum significant performance gain 0.0 δ 0.0 while max_x do Badj (1 x) MannWhitneyUTest(Badj, A, alternative = greater) if < α then δ + step // Pessimistically weaken improvement // Update conservative gain // Test next larger gain else break end end return δ // Significance lost, stop searching 1. Oracle (File-Level): This setting aims to provide the model with the oracle functions as the optimization target and the related entire files as contextual information, evaluating the models capability to generate purely performance-enhancing code. The target functions are directly modified. We extract this target functions from the human patch in the PR by combining AST (Abstract Syntax Tree) analysis with unified diff matching. 2. Realistic (Repo-Level): This setting simulates an end-to-end real-world scenario, providing the system (e.g., OpenHands Agent) with the functions measured during testing (unit test execution) as the optimization target. The system has greater freedom to modify code across the entire repository, measuring the systems ability to enhance performance repository-wide. The target functions are the directly measured functions, not necessarily the ones directly modified; the functions requiring modification might be those it calls. Compared to the Oracle setting, the Realistic setting is more challenging, requiring not only the models ability to generate performance-improving code but also capabilities like retrieval. We identify the target functions by using yappi to record functions dynamically executed during performancerelated unit tests. Combined with AST parsing of the unit test code, we determine the specific functions directly invoked by the unit test. We avoid using the unit test itself as the target to prevent test information leakage, which could lead the model to perform functional pruning modifying the function to retain only the functionality exercised by the test solely to meet the optimization metric. After the aforementioned five phases, we transform them into the SWE-Perf dataset, which consists of the following components: 1. CodeBase: The source code corresponding to the original codebase. 7 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Table 1: Average and maximum numbers characterizing different attributes of SWE-Perf. Category Metric Mean Max Size # Instances # Repos 140 9 Codebase # Files (non-test) # Lines (non-test) Expert Patch Tests Functions # Lines edited # Files edited # Func. Edited # Related tests Original runtime / # Oracle # Realistic 447.3 170k 131.1 4.3 7.6 8.1 0.28 7.6 30.1 1,972 502k 1,967 71 68 25.2 94 256 Performance Ratio 10.9% 87.8% Figure 3: Distribution of SWE-Perf across 9 open source popular GitHub repositories. 2. Executable Environment: The Docker image and container used to execute the original codebase. 3. Target Functions: The optimization-target functions categorized as oracle and realistic. 4. Performance-related Unit Tests: The unit tests identified as having performance improvements. 5. Runtime Metrics: The original and modified codebase runtime metrics for performance-related unit tests. 6. Expert Patch: The expert patch from the modified codebase. These serve as references for human-level performance optimization. 3.3. Data Statistics and Distribution In Phase 1, total of 102,241 pull requests were collected, from which 19,797 pull requests remained after filtering. In Phase 2, 34,397 distinct codebases were gathered, and test executions were successfully performed on 19,499 of them, yielding corresponding runtime data. In Phase 3, 4413 PRs were identified whose main and dev codebases both had available runtime data, from which 1,696 valid instances were derived. If Phase 4, 140 instances were identified. Detailed statistics and data distributions of SWE-Perf are presented in Figure 3 and Table 1. 4. Evaluation Methodology We designed three-level performance evaluation framework with three progressively stringent metrics: Apply, Correctness, and Performance. During evaluation, we first apply the model-generated patch/code to the original codebase (codebase_prei) to obtain the post-patch codebase (codebase_posti). Subsequently, within the corresponding Docker environment, we execute all performance-related tests (testi,j, {1, . . . , Ni}) on both the original and post-patch codebases, collecting the results (result_prei,j, result_posti,j) and runtime (runtime_prei,j, runtime_posti,j) measurements for comparison. It is worth noting that, to eliminate the impact of environmental variability on execution speed, we reevaluated the original codebase runtime during the testing phase, even when the original codebase runtime SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Table 2: Experimental results of several leading LLMs under both Oracle and Realistic setting on the SWE-Perf benchmark. Setting Methods Apply Correctness Performance Oracle (File-Level) Expert Claude-3.7-sonnet Claude-4-sonnet Claude-4-opus GPT-4o OpenAI-o1 OpenAI-o3 DeepSeek-V3 DeepSeek-R1 Gemini-2.5-Pro Qwen3-235B-A22B Realistic (Repo-Level) Claude-3.7-sonnet (OpenHands) Claude-3.7-sonnet (Agentless) 100.00% 100.00% 10.85% 66.43% 73.57% 85.71% 63.57% 66.42% 78.57% 47.85% 55.71% 95.00% 54.29% 87.86% 88.57% 61.43% 70.00% 78.57% 56.43% 63.57% 76.43% 42.86% 51.43% 83.57% 48.57% 77.86% 70.71% 1.24% 1.76% 1.28% 0.60% 0.41% 1.37% 0.54% 0.90% 1.48% 0.68% 2.26% 0.41% from data collection was available. This ensures full comparability between the original and post-patch codebase runtime measurements. The definitions and corresponding formulas for the three evaluation metrics are as follows: Apply: This metric evaluates whether the model-generated patches or code can be successfully applied to the original codebase without conflicts or errors. Apply = Napply , where, Napply is the number of successfully Ntotal applied samples, Ntotal is the number of total samples. Correctness: This metric assesses whether the patch preserves the functional correctness of the code, specifically whether all unit tests pass successfully after the patch is applied. Correctness = Ntotal i=1 [Ni j=1 result_posti,j = pass] Ntotal where result_posti,j is the post-patch result of the j-th unit test on the i-th sample. Ni is the number of performance-related unit tests for the i-th sample. is the logical AND for all tests j, indicating the sample must pass every test. Performance: This metric measures the statistically significant minimum performance gain introduced by the patch, based on runtime comparisons. The computation process resembles that of Phase 4 in data collection (3.2). After warm-up period, 20 repetitions and outliers filtering, the Minimum Performance Gain (pi,j) for each instance and each unit test is calculated using Algorithm 1. Performance is then calculated as follows: Per ormance = 1 Ntotal Ntotal i=1 Pi, Pi = 1 ni Ni j= pi,j (2) 5. Experiments This section presents the baseline setting (5.1), the main experimental results (5.2), and further analysis of the models performance (5.3). 9 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 4: Performance for different methods across the 9 repositories represented in SWE-Perf. The base model used in the Oracle setting is Claude-3.7-sonnet. 5.1. Baselines Recent work on repository-level software engineering tasks (e.g., SWE-Bench) can be broadly categorized into three paradigms: direct model approaches, pipeline-based methods and agent-based systems. We select representative state-of-the-art (SOTA) methods from each category for evaluation on the SWE-Perf benchmark. 1. Oracle: For the oracle setting, we adopt chain-of-thought prompting strategy to directly use model to enhance codebase performance. The model is provided with the oracle files extracted from expert patches and oracle target functions. single-pass inference is used to generate the patch. We evaluate with 10 popular models. 2. Agentless (Pipeline-Based): Agentless (Xia et al., 2024) follows pipeline-based approach to address the task. It employs fixed multi-stage workflow consisting of hierarchical fault localization, code repair, and candidate patch selection through regression and reproduction testing. 3. OpenHands (Agent-Based): OpenHands (Wang et al., 2024) adopts an agent-based methodology. It provides flexible and extensible platform for building autonomous software development agents, enabling iterative reasoning and interaction across multiple steps in the software engineering process. For both Agentless and OpenHands, we use Claude-3.7-sonnet as the base model. Because Claude-3.7sonnet is the officially recommended backend for OpenHands 1, as it has been reported to work best within OpenHands. The implementation details are provided in Appendix C. 5.2. Main Results The performance results of various methods on SWE-Perf are presented in Table 2. The comparative performance across different repositories is illustrated in Figure 4. From these empirical findings, we derive the following conclusions: 1https://github.com/All-Hands-AI/OpenHands 10 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 5: Performance for correct examples. The expert performance calculated using only correct examples from the corresponding method. The base model in the Oracle setting is Claude-3.7. Compared to the expert, all models exhibit substantial room for improvement on SWE-Perf. OpenHands demonstrates superior performance due to its agent-based methodology, providing flexible and extensible platform for autonomous software development agents that enable iterative reasoning and multi-step interaction throughout the software engineering process. However, significant performance gap of 8.59% persists between OpenHands and Expert, indicating considerable potential for further. The model exhibits the potential to surpass expert performance and achieve superior results. As can be observed in Figure 4, the model already rivals the performance of the Expert on certain repositories; for instance, on sklearn, OpenHands outperforms the Expert by 0.4%. This demonstrates that the model can achieve competitive edge over established expert methods in specific tasks or datasets, signifying an early-stage breakthrough. 5.3. Analysis of Model Capabilities This subsection analyzes performance from four aspects: (1) isolating performance from correctness metrics, (2) quantifying target functions impact, (3) evaluating runtime-performance relationships, and (4) identifying keyword patterns. We aim to uncover optimization bottlenecks and actionable improvement insights. Figure 6: Performance for correct examples. The expert performance calculated using only correct examples from the corresponding method. 11 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 7: Performance variation relative to the number of Oracle target functions. The base model in the Oracle setting is Claude-3.7-sonnet. Figure 8: Performance variation relative to the number of Realistic target functions. 5.3.1. Performance Analysis Decoupled from Correctness To decouple the models code performance enhancement capability from its code/patch generation correctness, we calculated performance metrics exclusively for correct examples by modifying the denominator in Equation (2) from Ntotal to Ncorrectness. The results are presented in Figure 5 and Figure 6. The Expert metric represents expert performance calculated using only correct samples from the corresponding method. OpenHands demonstrates superior performance particularly excelling in scenarios with higher potential performance ceilings. As illustrated in Figure 5, using identical models, OpenHands achieves an approximately 3% higher benchmark performance compared to alternative methods, highlighting its superior capability in translating model capacity into realized performance, especially near the achievable ceiling. 5.3.2. Impact of Target Functions on Performance To investigate the impact of the number of target functions on model performance, we present the statistics summarized in Figure 7 and Figure 8. As the number of target functions increases, performance improvements become increasingly difficult to achieve. As illustrated in Figure 8, the performance of experts declines with the addition of functions, indicating heightened difficulty in enhancing performance and lower performance ceiling. The model should prioritize learning from the expert to enhance its capability when handling larger number of target functions. Figure 8 reveals that OpenHands achieves performance comparable to the expert at lower function counts. However, the performance gap widens significantly as the number of functions increases. Future research aimed at improving model performance should focus on optimizing for multi-function scenarios. 5.3.3. Impact of Runtime on Performance To examine the models ability to improve performance across different runtimes, we plotted the barchart shown in Figure 9. As runtime increases, the performance ceiling rises correspondingly. As evidenced in Figure 9, the 12 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 9: Performance variation relative to the runtime of the original codebase. The base model in the Oracle setting is Claude-3.7-sonnet. expert models performance demonstrates progressive upward trend with extended runtime, indicating that longer computation times enable more sophisticated optimization and convergence towards higher performance potentials. The models capability to improve performance on cases with longer runtimes requires further enhancement. Figure 9 reveals that while expert performance continues to climb with increased runtime, the models performance plateaus (or remains stagnant). This divergence underscores the critical need to analyze and emulate the experts optimization strategies specifically under extended-runtime conditions to enhance the models performance scalability. 5.3.4. Keyword Analysis for Performance To further investigate the differences in modification strategies preferred by the model and the expert when enhancing performance, we respectively generated word clouds representing the lines added in patches, as shown in Figure 10 and Figure 11. OpenHands patches focus on low-level data structures and basic functionality. The word cloud reveals frequent terms such as \"children,\" \"identifier,\" \"time,\" and \"attributes,\" indicating that the changes are centered on refining structural components and enhancing fundamental operations. These optimizations likely address data management and attribute handling, focusing on the internal mechanics of the code. Expert patches emphasize high-level abstractions and data integrity. The presence of terms like \"literal,\" \"value,\" \"type,\" \"label,\" and \"dtype\" suggests that the optimizations are geared towards improving type safety, type annotations, and handling of data values. These changes likely aim to enhance error handling, code clarity, and overall system efficiency by addressing more abstract elements of the code. The model-generated patches exhibit strong focus on foundational infrastructure and low-level operations, as indicated by frequent terms such as miniconda3, envs, frozen, importlib, and bootstrapu0000. This suggests an emphasis on environment configuration, dependency management, and basic module handling. The presence of encoded fragments (u0000) further implies automated generation targeting syntactic adjustments or toolchain compatibility, rather than semantic, application-level optimization. The expert patches demonstrate clear orientation towards domain-specific functionality and performance13 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 10: Word cloud of lines added in OpenHands patches. Figure 11: Word cloud of lines added in Expert patches. critical enhancements. Key terms like sympy, time, workspace, and active packages reveal deliberate strategy to optimize core computational workflows (e.g., symbolic mathematics with sympy), resource management (workspace), and runtime efficiency (time). This reflects human expertise in restructuring high-level components to improve performance, maintainability, and domain-relevant operations. 6. Conclusion In conclusion, SWE-Perf addresses critical gap in current benchmarking by providing the first repositorylevel dataset focused on realistic code performance optimization, task traditionally reliant on human expertise and largely unexplored in prior LLM evaluations. Our benchmark and comprehensive baseline assessments reveal substantial room for improvement in current models, underscoring the complexity of cross-module and repository-scale optimizations in real-world software. The significant performance gaps observed between existing LLMs and expert-level optimization highlight the need for novel approaches that can handle the intricacies of repository-level performance enhancement. By establishing this new standard and evaluation framework, we pave the way for future research to advance the capabilities of language models in delivering meaningful, performance-aware code enhancements at scale, ultimately bridging the gap between automated code generation and expert-level optimization in production environments. 7. Acknowledgment This paper template is adapted from https://github.com/Arxiv-Template/Arxiv-Template. We extend our gratitude to the author for providing such an elegant template. 14 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?"
        },
        {
            "title": "References",
            "content": "Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. Swesearch: Enhancing software agents with monte carlo tree search and iterative refinement, 2025. URL https://arxiv.org/abs/2410.20285. Cursor. The ai code editor. https://www.cursor.com/en, 2025. Accessed: 2025-05-12. Mingzhe Du, Luu Anh Tuan, Bin Ji, Qian Liu, and See-Kiong Ng. Mercury: code efficiency benchmark for code large language models. Advances in Neural Information Processing Systems, 37, 2024. Yaxin Du, Yuzhu Cai, Yifan Zhou, Cheng Wang, Yu Qian, Xianghe Pang, Qian Liu, Yue Hu, and Siheng Chen. Swe-dev: Evaluating and training autonomous feature-driven software development. 2025. URL https://arxiv.org/abs/2505.16975. Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, and Jie M. Zhang. Effibench: Benchmarking the efficiency of automatically generated code. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1150611544. Curran Associates, Inc., 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. CoRR, abs/2408.06450, 2024. doi: 10.48550/ARXIV.2408.06450. URL https://doi.org/10.48550/arXiv.2408.06450. Javier Mancebo, Félix García, and Coral Calero. process for analysing the energy efficiency of software. Information and Software Technology, 134:106560, 2021. ISSN 0950-5849. doi: https://doi.org/ 10.1016/j.infsof.2021.106560. URL https://www.sciencedirect.com/science/article/pii/ S0950584921000446. Microsoft. Ai that builds with you. https://github.com/features/copilot, 2025. Accessed: 202505-12. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $ 1 million from real-world freelance software engineering?, 2025a. URL https://arxiv. org/abs/2502.12115. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? CoRR, abs/2502.12115, 2025b. doi: 10.48550/ARXIV.2502.12115. URL https://doi.org/10.48550/arXiv.2502.12115. Niels Mündler, Mark Niklas Mueller, Jingxuan He, and Martin Vechev. SWT-bench: Testing and validating real-world bug-fixes with code agents. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=9Y8zUO11EQ. Nathalia Nascimento, Paulo Alencar, and Donald Cowan. Artificial intelligence vs. software engineers: An empirical study on performance and efficiency using chatgpt. In Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering, CASCON 23, page 2433, USA, 2023. IBM Corp. 15 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Anne Ouyang, Simon Guo, Simran Arora, Alex L. Zhang, William Hu, Christopher Ré, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels?, 2025. URL https://arxiv.org/abs/2502.10517. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2024. URL https://arxiv.org/abs/2412. 21139. Rui Pereira, Marco Couto, Francisco Ribeiro, Rui Rua, Jácome Cunha, João Paulo Fernandes, and João Saraiva. Ranking programming languages by energy efficiency. Science of Computer Programming, 205: ISSN 0167-6423. doi: https://doi.org/10.1016/j.scico.2021.102609. URL https: 102609, 2021. //www.sciencedirect.com/science/article/pii/S0167642321000022. Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, and Ion Stoica. GSO: challenging software optimization tasks for evaluating swe-agents. CoRR, abs/2505.23671, 2025. doi: 10.48550/ ARXIV.2505.23671. URL https://doi.org/10.48550/arXiv.2505.23671. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. URL https://arxiv.org/abs/2407.16741. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/ 2405.15793. John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. CoRR, abs/2504.21798, 2025. doi: 10.48550/ARXIV.2504.21798. URL https://doi.org/10.48550/ arXiv.2504.21798. Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, and Junyang Lin. Swe-flow: Synthesizing software engineering data in test-driven manner. CoRR, abs/2506.09003, 2025. doi: 10.48550/ARXIV.2506.09003. URL https://doi.org/10.48550/arXiv. 2506.09003."
        },
        {
            "title": "References",
            "content": "Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. Swesearch: Enhancing software agents with monte carlo tree search and iterative refinement, 2025. URL https://arxiv.org/abs/2410.20285. 16 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Cursor. The ai code editor. https://www.cursor.com/en, 2025. Accessed: 2025-05-12. Mingzhe Du, Luu Anh Tuan, Bin Ji, Qian Liu, and See-Kiong Ng. Mercury: code efficiency benchmark for code large language models. Advances in Neural Information Processing Systems, 37, 2024. Yaxin Du, Yuzhu Cai, Yifan Zhou, Cheng Wang, Yu Qian, Xianghe Pang, Qian Liu, Yue Hu, and Siheng Chen. Swe-dev: Evaluating and training autonomous feature-driven software development. 2025. URL https://arxiv.org/abs/2505.16975. Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, and Jie M. Zhang. Effibench: Benchmarking the efficiency of automatically generated code. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1150611544. Curran Associates, Inc., 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. CoRR, abs/2408.06450, 2024. doi: 10.48550/ARXIV.2408.06450. URL https://doi.org/10.48550/arXiv.2408.06450. Javier Mancebo, Félix García, and Coral Calero. process for analysing the energy efficiency of software. Information and Software Technology, 134:106560, 2021. ISSN 0950-5849. doi: https://doi.org/ 10.1016/j.infsof.2021.106560. URL https://www.sciencedirect.com/science/article/pii/ S0950584921000446. Microsoft. Ai that builds with you. https://github.com/features/copilot, 2025. Accessed: 202505-12. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $ 1 million from real-world freelance software engineering?, 2025a. URL https://arxiv. org/abs/2502.12115. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? CoRR, abs/2502.12115, 2025b. doi: 10.48550/ARXIV.2502.12115. URL https://doi.org/10.48550/arXiv.2502.12115. Niels Mündler, Mark Niklas Mueller, Jingxuan He, and Martin Vechev. SWT-bench: Testing and validating real-world bug-fixes with code agents. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=9Y8zUO11EQ. Nathalia Nascimento, Paulo Alencar, and Donald Cowan. Artificial intelligence vs. software engineers: An empirical study on performance and efficiency using chatgpt. In Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering, CASCON 23, page 2433, USA, 2023. IBM Corp. Anne Ouyang, Simon Guo, Simran Arora, Alex L. Zhang, William Hu, Christopher Ré, and Azalia Mirhoseini. Kernelbench: Can llms write efficient gpu kernels?, 2025. URL https://arxiv.org/abs/2502.10517. 17 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2024. URL https://arxiv.org/abs/2412. 21139. Rui Pereira, Marco Couto, Francisco Ribeiro, Rui Rua, Jácome Cunha, João Paulo Fernandes, and João Saraiva. Ranking programming languages by energy efficiency. Science of Computer Programming, 205: ISSN 0167-6423. doi: https://doi.org/10.1016/j.scico.2021.102609. URL https: 102609, 2021. //www.sciencedirect.com/science/article/pii/S0167642321000022. Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, and Ion Stoica. GSO: challenging software optimization tasks for evaluating swe-agents. CoRR, abs/2505.23671, 2025. doi: 10.48550/ ARXIV.2505.23671. URL https://doi.org/10.48550/arXiv.2505.23671. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. URL https://arxiv.org/abs/2407.16741. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/ 2405.15793. John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. CoRR, abs/2504.21798, 2025. doi: 10.48550/ARXIV.2504.21798. URL https://doi.org/10.48550/ arXiv.2504.21798. Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, and Junyang Lin. Swe-flow: Synthesizing software engineering data in test-driven manner. CoRR, abs/2506.09003, 2025. doi: 10.48550/ARXIV.2506.09003. URL https://doi.org/10.48550/arXiv. 2506.09003. 18 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?"
        },
        {
            "title": "Appendix",
            "content": "A. Limitations and Safeguards: A.1. Limitations Our work has two primary limitations. First, the current version of SWE-Perf is constructed from limited set of open-source repositories, future work could expand the dataset to improve coverage and generalizability. Second, while we use human-written patches as ground truth to assess model performance, these may not represent the optimal achievable performance, potentially underestimating the true upper bound of improvement. A.2. Safeguards To ensure the responsible use of SWE-Perf, we implement the following safeguards. All codebases included in the dataset are drawn from permissively licensed open-source projects, and we exclude any repositories containing sensitive or personal information. We provide executable environments in sandboxed Docker containers to prevent unsafe code execution. Moreover, model-generated patches are evaluated automatically in isolated environments, reducing the risk of harmful or unintended system behavior. The dataset and code will be released under an academic research license, and users will be required to agree to the terms of use restricting deployment in sensitive or safety-critical systems. B. Data Collections All experiments were conducted on two Linux machines. Each machine is equipped with 256 logical CPU cores (128 physical cores across 2 sockets, with 2 threads per core) and 2.0 TiB of RAM. The statistical summary of data volume at each stage is presented in Table 3. Table 3: Table of data volume at each phase. Repo astropy django matplotlib seaborn flask requests xarray pylint pytest sklearn sphinx sympy sum Phase Phase2 Phase3 PRs Tasks Tasks with versions Unique codebase Codebase with pytest report Tasks with success runtimes Tasks after step1 Tasks after step2 11555 13200 18791 1115 2637 2507 4470 4553 6228 18079 6002 13104 102241 1947 4179 2576 304 285 223 1359 1174 1203 2576 1507 2464 1947 4179 2576 304 262 217 1354 911 989 2574 1385 2464 19162 3589 6887 4707 573 521 410 2485 1680 1860 4721 2629 4335 34397 2323 6399 1648 539 410 364 1000 1522 1717 431 2299 847 19499 529 - 495 199 62 151 517 806 583 181 630 260 4413 409 - 369 165 0 105 495 772 397 181 573 254 3720 195 - 163 76 - 43 327 405 0 94 236 157 Phase4 Tasks 12 - 3 6 - 54 3 2 32 8 20 140 Table 4 presents the runtime statistics for each codebase in Phase 2 - Step 2. The mapping table of repository abbreviations to their full names is provided in Table 5. 19 SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Table 4: Runtime statistics for each codebase in phase 2 - step 2. Only runtimes from successful pytest executions are included. All time measurements are in minutes. Test runtime refers to the time taken to execute pytest for an individual codebase, while Total duration denotes the overall execution time for single codebase, including operations such as Docker image building and pytest execution."
        },
        {
            "title": "Repo",
            "content": "astropy django matplotlib seaborn flask requests xarray pylint pytest sklearn sphinx sympy"
        },
        {
            "title": "Total duration",
            "content": "avg max avg max 4.36 1.52 10.69 4.15 1.04 1.88 21.57 0.91 16.96 14.56 0.10 9. 24.12 3.09 5.85 0.22 28.75 7.60 15.97 3.37 1.75 0.04 1.09 11.08 58.11 119.95 58.52 121.03 6.13 2.75 2.34 20.10 83.89 119.96 85.83 125.98 40.00 8.72 24.60 112.17 24.98 112.57 3.99 18.94 3.47 3.13 38.99 9.57 Table 5: Mapping table of repository abbreviations to full names."
        },
        {
            "title": "Repo full name",
            "content": "astropy astropy/astropy matplotlib matplotlib/matplotlib seaborn requests xarray pylint sklearn sphinx sympy mwaskom/seaborn psf/requests pydata/xarray pylint-dev/pylint scikit-learn/scikit-learn sphinx-doc/sphinx sympy/sympy SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 12: Word cloud of lines added in Agentless patches. Figure 13: Word cloud of lines added in Oracle(Claude-3.7) patches. C. Baselines C.1. Details of Baselines 1. Oracle: The specific prompts used for the Oracle is shown in Figure ??. (a) OpenAI/GPT: The model version used is o1-preview-2024-09-12, o3-2025-04-16, gpt-4o-2024-11-20, with temperature of 0.2, top-p of 0.1, and maximum token limit of 8192. (b) Claude: The model version is gcp-claude37-sonnet, gcp-claude4-opus, gcp-claude4-sonnet. The thinking feature is enabled, with thinking budget of 2000 tokens and maximum token output of 8192. (c) DeepSeek: The versions used are deepseek-r1-0528 and DeepSeek-V3. (d) Gemini: The versions used are gemini-2.5-pro-preview-05-06. (e) Qwen: The versions used are Qwen3-235B-A22B. 2. Agentless 2: The sample number is set to 1. 3. OpenHands 3: The maximum number of iterations is set to 50. C.2. Details of Word Cloud More comprehensive word clouds are presented in Figure 12, Figure 13. 2https://github.com/OpenAutoCoder/Agentless 3https://github.com/All-Hands-AI/OpenHands SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? Figure 14: Oracle prompt."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "TikTok",
        "University of California San Diego",
        "Xian Jiaotong University"
    ]
}