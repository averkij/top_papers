{
    "paper_title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF",
    "authors": [
        "Dipan Maity"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 1 5 6 4 0 . 2 0 6 2 : r SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF Novel Algorithm Combining Double Soft-Min Critics with Adaptive KL Thresholding Dipan Maity dipanai.xyz@gmail.com Abstract Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines Double Soft-Min Critic for pessimistic value estimation with new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPOs symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on 3B parameter model show SAFE achieves +5.15% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available athttps:// github.com/ryyzn9/SAFE Keywords: RLHF, PPO, KL Divergence, Double Critic, Entropy Control, PID Controller"
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning from Human Feedback (RLHF) Ouyang et al. [2022], Bai et al. [2022], Christiano et al. [2023] has become central paradigm for aligning large language models with human preferences. Despite its empirical success, RLHF training remains highly unstable, particularly in long-horizon on-policy optimization. Practitioners routinely observe failure modes including reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. Most existing stabilization approaches focus on single-axis control, typically through fixed or adaptive KullbackLeibler (KL) Schulman et al. [2017] regularization that constrains policy deviation from reference model. While effective at limiting distributional drift, KL-based control alone does not fully capture internal policy dynamics. In practice, policies may remain within acceptable KL bounds while undergoing progressive determinization, increased gradient sensitivity, or unstable value estimation, leading to degraded learning dynamics and brittle convergence behavior. critical but often overlooked source of instability is value overestimation in the critic network. Standard actor-critic methods rely on single value estimator that, due to bootstrapping 1 and function approximation error, systematically overestimates returns in high-variance regions. When the policy encounters outlier rewardswhether from genuine improvement or spurious reward model artifactsan overestimating critic amplifies these signals through inflated advantage estimates, driving overly aggressive policy updates that can trigger cascading instability. This positive feedback loop is particularly destructive in RLHF, where reward models themselves contain systematic biases and the policy operates over enormous discrete action spaces with delayed sparse feedback. These challenges become particularly pronounced in long training runs, where delayed instabilities often emerge after hundreds or thousands of optimization steps. Such late-stage failures are difficult to detect using instantaneous metrics and are not adequately addressed by static regularization schedules or symmetric divergence penalties. Addressing these coupled failure modes requires coordinated intervention across multiple levels of the optimization process. In this work, we introduce SAFE (Stable Alignment Finetuning with Entropy-aware control), stabilized RLHF optimization framework that integrates complementary control mechanisms across three interacting layers: value estimation, divergence regulation, and training dynamics adaptation. Rather than relying on single stabilizer, SAFE combines pessimistic value aggregation, entropy-conditioned divergence control, and predictive threshold adaptation into unified training architecture. Contributions. Our contributions are: We identify and characterize interacting instability modes in long-horizon RLHF trainingincluding value overestimation, distributional drift, and entropy collapsethat are not adequately addressed by standard KL regularization alone. We propose SAFE, multi-layer stabilization framework that integrates pessimistic double soft-min value estimation to suppress overconfident updates, entropy-aware predictive control that couples asymmetric divergence regulation with adaptive entropy-based thresholding, and reward-driven PID adaptation to regulate exploration dynamics across training phases. We demonstrate empirically that coordinated control across value, policy, and temporal dynamics improves training stability and robustness without sacrificing reward performance, and provide theoretical analysis suggesting SAFE may offer partial resistance to reward hacking through its pessimistic critic mechanism. Together, these results suggest that robust RLHF optimization requires explicit multi-level control of value estimation, divergence dynamics, and exploration behavior rather than static regularization alone."
        },
        {
            "title": "2.1 RLHF Pipeline",
            "content": "Reinforcement Learning from Human Feedback (RLHF) typically consists of three sequential stages Ouyang et al. [2022]: 1. Supervised Fine-Tuning (SFT): pretrained language model is adapted to follow instructions using curated human demonstrations. 2 2. Reward Model Training: separate model is trained to approximate human preference judgments over generated responses. 3. Reinforcement Learning Optimization: The policy is optimized to maximize reward model outputs while remaining close to the supervised reference policy. The resulting optimization objective is commonly formulated as: L(θ) = ExD, yπθ (cid:2)r(x, y)(cid:3) + β DKL (cid:0)πθ πref (cid:1), (1) where r(x, y) denotes the learned reward model score, πref is the frozen supervised policy, and β controls the strength of divergence regularization. This KL constraint plays dual role: it stabilizes optimization by limiting large policy updates and preserves linguistic quality by anchoring the learned policy to the pretrained distribution. Despite its success, PPO-based RLHF remains sensitive to entropy collapse, reward hacking, and late-stage instability, motivating more structured control mechanisms."
        },
        {
            "title": "2.2 Log-Probability Ratio Estimation in RLHF",
            "content": "In practical RLHF implementations, KL divergence is not computed exactly over the full vocabulary distribution. Instead, divergence is estimated using Monte Carlo sampling over tokens generated by the current policy.Ouyang et al. [2022], Bai et al. [2022]. Specifically, the commonly used estimator is the batch mean of per-token log-probability ratios:Ouyang et al. [2022]. (cid:98)DKL = 1 (cid:88) (cid:104) i=1 (cid:105) log πθ(ai si) log πref(ai si) , ai πθ. (2) While the true KL divergence DKL(πθπref) is always non-negative by definition, this finitesample estimator can assume negative values due to sampling variance and estimator bias. Negative estimates occur when sampled tokens have lower probability under the current policy than under the reference model on average. This phenomenon is especially common during highentropy exploration phases, where the policy samples broadly from low-probability tail regions of the distribution. Importantly, such negative values do not indicate negative KL divergence, but instead reflect stochastic sampling effects. Nevertheless, we observe empirically that the sign and magnitude of this estimator correlate strongly with policy behavior: Negative log-ratio estimates typically coincide with high-entropy exploratory regimes. Positive log-ratio estimates reflect increasing policy confidence and concentration on highreward modes. Sustained positive log-ratio estimates combined with decreasing entropy are frequently observed prior to exploitative behavior and reward hacking. This motivates using the log-ratio signal jointly with entropy to regulate policy stabilization."
        },
        {
            "title": "2.3 Double Critic Learning and Pessimistic Aggregation",
            "content": "Value overestimation is fundamental source of instability in actorcritic algorithms and arises from bootstrapped temporal-difference updates combined with function approximation error. When single critic systematically overestimates action values, policy updates become overly aggressive, amplifying optimization noise and often leading to training divergence. Double Q-learning mitigates this issue by maintaining two independently parameterized critics and applying pessimistic aggregation when computing Q-value targets Van Hasselt [2010], Fujimoto et al. [2018]. Instead of relying on single value estimate, the minimum of the two critics is used to form training targets: Vtarget(s) = min(Q1(s), Q2(s)). (3) This pessimistic selection reduces positive bias by lowering the probability that both critics simultaneously overestimate the same state or action. Although this introduces small underestimation bias, empirical studies have shown that this trade-off substantially improves stability and robustness across wide range of continuous control benchmarks Fujimoto et al. [2018]. Twin-critic architectures have since become standard component of modern off-policy and on-policy actorcritic algorithms, particularly in settings with high variance rewards and nonstationary policy updates. The core principle of pessimistic aggregation provides simple yet effective mechanism for stabilizing value estimation in large-scale reinforcement learning systems."
        },
        {
            "title": "2.4 Entropy Regularization and Exploration Stability",
            "content": "Entropy regularization is commonly used in reinforcement learning to encourage exploration by penalizing overly deterministic policies. In maximum-entropy frameworks such as Soft Actor-Critic Haarnoja et al. [2018b], entropy is treated as first-class optimization objective rather than secondary regularizer.Haarnoja et al. [2018a], Geist et al. [2019], Neu et al. [2017], Nachum et al. [2017]. In RLHF, however, entropy regularization is typically implemented using static coefficients. Such static entropy bonuses are insufficient to prevent premature entropy collapse during early training and can destabilize late-stage convergence when over-amplified. Language model alignment exacerbates this problem due to extremely large action spaces, delayed sequence-level rewards, and strong pretrained inductive biases. Once entropy collapses prematurely, policies rapidly overfit reward model artifacts and lose the ability to recover through exploration. These observations motivate adaptive entropy-aware stabilization mechanisms that dynamically regulate exploration based on training state rather than relying on fixed entropy coefficients."
        },
        {
            "title": "3 Method:",
            "content": "Value-Level Stabilization via Pessimistic Aggregation. At the foundation of SAFE is double soft-min critic architecture that directly addresses value overestimation bias. Following the principle of pessimistic value estimation Van Hasselt [2010], we maintain two independently parameterized critics and aggregate their estimates using differentiable soft-minimum operator: Vsoft(s) = α log (cid:16) eV1(s)/α + eV2(s)/α(cid:17)(cid:21) . (cid:20) 1 This soft aggregation provides smooth, pessimistic lower bound that reduces the probability of both critics simultaneously overestimating the same state. Unlike hard minimum operators that 4 can introduce gradient discontinuities, the soft-min formulation preserves differentiability while systematically suppressing optimistic outliers. Crucially, this pessimistic bias makes the policy less responsive to reward spikesincluding those arising from reward model exploitationthereby providing first line of defense against both training instability and reward hacking. We further stabilize critic learning through layer normalization applied to shared representations and Polyakaveraged target networks for bootstrap stability. Policy-Level Stabilization via Entropy-Aware Predictive Control. At the policy level, standard symmetric KL penalties fail because they penalize healthy exploratory deviations equally with dangerous overconfident drift. To address this fundamental limitation, SAFE introduces an entropy-aware predictive controller that integrates three complementary mechanisms into unified control architecture: (1) asymmetric penalization that selectively suppresses positive KL divergence (indicating deterministic drift toward reward model artifacts) while preserving negative deviations associated with high-entropy exploration, (2) entropy-gated scaling that amplifies corrective pressure precisely when policy entropy drops into low-stochasticity regimes characteristic of mode collapse and exploitation, and (3) PID-driven adaptive thresholding that dynamically adjusts divergence tolerance based on reward velocity rather than fixed schedules. By coupling directional divergence control with entropy dynamics and temporal reward trends, this controller detects exploitative behavior earlier and maintains stable regulation across varying training phases. Importantly, our approach does not impose hard entropy targets or rigid divergence constraints. Instead, SAFE adapts its control signals continuously in response to observed training behavior, allowing smooth transitions from exploratory optimization to stable convergence without introducing additional off-policy corrections or auxiliary objectives."
        },
        {
            "title": "3.1 Motivation: Late-Training Entropy Collapse in RLHF",
            "content": "In RLHF, KL divergence measures how much the policy model (the model being trained) has deviated from the reference model (the original, pre-trained model). The formula used per-token is: DKL(π πref ) = log π(xt) log πref (xt) (4) Averaged across tokens and batches, this gives single scalar KL value per training step. critical instability in long-horizon RLHF is late-stage policy entropy collapse: policies initially optimize successfully but undergo abrupt divergence characterized by rapid KL growth, entropy collapse, and catastrophic reward degradation. We observed the following failure pattern in double soft critic with standered kl experiments: Step KL Estimate Reward 1400 1450 1500 0.09 +0.12 +1.36 +2.89 0.71 0.70 0.59 0.41 Table 1: Example collapse trajectory showing transition from exploration (KL < 0) to exploitative divergence (KL > 0) with 42% reward loss. This transition corresponds to shift from exploratory behavior toward narrow exploitation of reward model artifacts, followed by irreversible policy degeneration. The problem is not negative 5 KL itself. The problem is that the adaptive controller INTERPRETS negative KL as everything is fine, remove constraints . TO prevent this we suggest making KL always positive, but always positive KL regularization fails to prevent this because symmetric penalties of the form LKL = β ˆDKL Dtarget suffer from two limitations: (1) negative KL estimates associated with healthy exploration are penalized equally to dangerous positive divergence, and (2) penalties react only to instantaneous magnitude, failing to detect accelerating trends that precede collapse. (5)"
        },
        {
            "title": "3.2 Asymmetric Controller Design",
            "content": "To address these limitations, we introduce an asymmetric controller that selectively penalizes overconfident divergence while preserving exploration and detecting rising trends early via momentum signals. Asymmetric Divergence Penalty. The controller activates only when divergence exceeds safety threshold τ : (cid:40) Lasym = λasym( ˆDKL τ )2, 0, ˆDKL > τ otherwise (6) where ˆDKL is the Monte Carlo log-ratio estimator and λasym controls penalty strength. This formulation imposes zero penalty on negative KL estimates, tolerates small positive deviations, and quadratically suppresses large over-confidence. Momentum-Based Early Warning. Value-based thresholding alone is insufficient because divergence often accelerates rapidly once exploitative behavior emerges. We augment the controller with momentum penalty based on KL velocity over sliding window w: KL ˆD(tw) , mt > 0 , Lmom = λmomm2 0, otherwise mt = ˆD(t) (7) KL (cid:40) This term activates when divergence is increasing, enabling early intervention even when absolute KL magnitude remains moderate. Combined Control Signal. The final penalty combines both components: LAKL = Lasym + Lmom. (8) Algorithm 1 summarizes the controller logic. The asymmetry is motivated by the role of the reference policy in RLHF: over-confidence typically indicates exploitation of reward model artifacts, while reduced confidence reflects exploration. Momentum control follows classical control theory, where reacting to derivative signals enables early intervention before large errors accumulate Astrom and Murray [2008]."
        },
        {
            "title": "3.3 Empirical Behavior",
            "content": "Figure 1 compares training dynamics under asymmetric control versus standard PPO. The asymmetric controller maintains KL within bounded ranges (KL < 0.5 throughout 2000 steps), prevents entropy collapse (entropy > 2.0), and eliminates late-stage reward degradation observed in PPO (2 crashes with > 20% reward loss). However, value function stability remains suboptimal (435 spikes > 0.1), motivating the entropy-aware predictive controller introduced in Section 3.4. 6 Figure 1: Training dynamics comparing asymmetric KL control ( blue) versus PPO (red) over 2000 steps. Asymmetric control achieves stronger KL regulation but exhibits higher value loss volatility, indicating the need for additional stabilization (Section 3.4)."
        },
        {
            "title": "3.4 Entropy-Aware Predictive Controller: Motivation and Design",
            "content": "While the Asymmetric KL Controller stabilizes divergence magnitude and growth dynamics, KL alone does not capture internal changes in policy confidence. During RLHF training, policies naturally become more deterministic as optimization progresses. However, excessive determinization can amplify the effect of KL deviations by concentrating probability mass on narrow token subsets. From training curves  (Fig. 1)  , we observe that reward stability and batch variance differ across controllers even when KL magnitudes remain comparable. This motivates incorporating policy 7 Algorithm 1 Asymmetric KL Controller Require: Current KL estimate ˆDKL, history buffer, threshold τ , window 1: Append ˆDKL to history buffer 2: Lasym λasym max(0, ˆDKL τ )2 3: if history length then 4: mt ( ˆD(t) KL )/w Lmom λmom max(0, mt)2 5: 6: else 7: 8: end if 9: return LAKL = Lasym + Lmom KL ˆD(tw) Lmom 0 entropy as modulating signal for divergence control rather than treating KL in isolation. Importantly, the controller does not assume direct causality between entropy and reward outcomes. Instead, entropy is used as stabilizing proxy for policy sharpness that influences how aggressively KL deviations should be penalized. The Entropy-Aware Predictive Controller constitutes the second stabilization layer in SAFE. Unlike the Asymmetric KL Controller, which focuses on directional divergence control, this component introduces adaptive thresholding, entropy-conditioned penalty modulation, and reward-driven predictive regulation. 3.4.1 Motivation: Empirical training traces indicate that while reward optimization may remain stable, KL trajectories and value losses can fluctuate significantly across phases of training. This motivates controller that: Adapts divergence constraints dynamically using reward feedback, Accounts for the interaction between KL magnitude and policy entropy, Adjusts control sensitivity across different training regimes. The Entropy-Aware Predictive Controller addresses these requirements by combining adaptive threshold control, entropy-based penalty scaling, and phase-aware modulation. 3.4.2 Dual-Timescale KL Tracking To stabilize control signals under noisy minibatch estimates, the controller maintains two exponential moving averages of KL divergence: KL(t) short = 0.9 KL(t1) short + 0.1 ˆD(t) KL, KL(t) long = 0.99 KL(t1) long + 0.01 ˆD(t) KL. (9) (10) The short-timescale tracker responds rapidly to transient spikes, while the long-timescale tracker captures slower distributional drift. This separation improves numerical stability and avoids reacting excessively to single-batch noise. 8 3.4.3 Reward-Driven Predictive Threshold Adaptation Rather than using fixed KL target, the controller adapts its divergence tolerance using PID controller driven by reward feedback. Let PID(rt) denote the proportionalintegralderivative update computed from the reward signal. The adaptive KL threshold is defined as: τt = (τbase + PID(rt)) ϕt, (11) where: τbase is the baseline divergence tolerance, ϕt is multiplicative factor provided by the phase detector, the PID controller adjusts sensitivity based on recent reward dynamics. This design allows the controller to relax divergence constraints during productive optimization phases and tighten them when reward improvement stagnates or destabilizes. 3.4.4 Entropy-Gated KL Penalty While KL divergence measures deviation from the reference policy, it does not capture the internal confidence of the current policy. To account for this interaction, the controller applies entropy-aware modulation to the KL penalty. First, base penalty is computed when the short-term KL estimate exceeds the adaptive threshold: lLbase = (cid:40) λ(KLshort τt)2, KLshort > τt 0, otherwise. This penalty is then scaled using an entropy-dependent factor: (cid:18) gt = max 0.5, Hfloor H(πθ) + ϵ (cid:19) , where: H(πθ) is the current policy entropy, Hfloor defines minimum entropy reference, ϵ ensures numerical stability. The final penalty becomes: (12) (13) LEPC = gt Lbase. (14) This mechanism does not directly predict entropy collapse. Instead, it modulates penalty strength such that low-entropy regimes receive stronger corrective pressure, while higher-entropy policies experience gentler regulation. 9 3.4.5 Phase-Aware Threshold Modulation Training dynamics differ substantially between early exploration, reward climbing, plateau phases, and late convergence. lightweight phase detector monitors reward statistics and applies multiplicative adjustment factor ϕt to the KL threshold. This enables: relaxed constraints during early exploration, tighter regulation during stagnation, stabilized behavior near convergence. Phase awareness improves controller robustness without introducing task-specific heuristics. 3.4.6 Gradient Preview Safety Gate To further prevent instability, the controller implements gradient preview mechanism. After computing tentative policy update, the resulting preview KL divergence is compared against maximum allowable limit: scale = (cid:40)1, Dmax ˆDpreview KL , ˆDpreview KL otherwise. Dmax (15) When necessary, the update magnitude is scaled down before application. This provides an additional safety layer against sudden divergence spikes. 3.4.7 Controller Algorithm Algorithm 2 Entropy-Aware Predictive Controller (Plan D) 1: Input: current KL estimate ˆDKL, entropy H(πθ), reward rt 2: Update short-term KL EMA 3: Update long-term KL EMA 4: Update reward-driven PID controller 5: Update training phase detector 6: Compute adaptive threshold τt = (τbase + PID) ϕt 7: if KLshort > τt then 8: 9: Compute base KL penalty Compute entropy scaling factor gt LEPC gt Lbase 10: 11: else 12: 13: end if 14: Return penalty and diagnostic statistics LEPC 0 3.4.8 Practical Role in SAFE Within the overall SAFE framework, the Entropy-Aware Predictive Controller provides: Adaptive divergence regulation across training phases, Improved robustness to noisy KL estimates, Stabilized interaction between policy confidence and divergence control, Additional protection against large post-update KL spikes. Combined with the Asymmetric KL Controller, this forms multi-layer stabilization architecture that operates at both directional divergence control and dynamic threshold regulation levels."
        },
        {
            "title": "4 Components",
            "content": "SAFE combines pessimistic value estimation, entropy-gated KL regulation, and adaptive threshold control into unified on-policy RLHF optimization framework. The method extends PPO-style policy optimization by introducing stabilization components that operate at the value estimation level, policy divergence regulation level, and training-dynamics adaptation level. At each training iteration, on-policy rollouts are collected, normalized advantages are computed using pessimistic critic ensemble, and multiple clipped PPO updates are performed with entropymodulated KL penalties and dynamically adjusted divergence thresholds."
        },
        {
            "title": "4.1 Double Soft-Min Critic",
            "content": "To mitigate value overestimation and stabilize advantage estimation, we employ twin-critic architecture with differentiable pessimistic aggregation. Given two independent value estimators V1(s) and V2(s), the soft-min aggregation is defined as: Vsoft(s) = α log (cid:104) eV1(s)/α + eV2(s)/α(cid:105)(cid:19) , (cid:18) 1 2 where α > 0 controls the smoothness of the approximation. This operator satisfies the following properties: Hard-min limit lim α0 Vsoft(s) = min(V1(s), V2(s)). (16) (17) Differentiability The log-sum-exp formulation preserves smooth gradients compared to nondifferentiable minimum operators. Pessimistic bias The aggregation suppresses optimistic value estimation errors that can destabilize policy updates. To stabilize critic training, Layer Normalization is applied to the shared hidden representation prior to the value heads. Polyak-averaged target critic is maintained for bootstrap stability: θtarget (1 τpolyak)θtarget + τpolyakθ, (18) where τpolyak denotes the soft update coefficient."
        },
        {
            "title": "4.2 Reward Normalization and Advantage Estimation",
            "content": "Reward signals produced by the reward model are normalized using running mean and variance statistics: Advantages are computed using the pessimistic critic estimate: = µr σr + ϵ . and standardized within each batch: = Vsoft(s), ˆA = µA σA + ϵ , (19) (20) (21) which reduces gradient variance and improves PPO optimization stability."
        },
        {
            "title": "4.3 Entropy-Gated KL Control Signal",
            "content": "Instead of using fixed KL penalty coefficient, SAFE applies entropy-modulated divergence regulation that adapts penalty strength based on the current policy entropy. 4.3.1 Monte Carlo KL Estimate per-batch log-ratio estimator is computed over policy-generated tokens: ˆDt = 1 (cid:88) (cid:104) i=1 log πθ(aisi) log πref(aisi) (cid:105) , ai πθ. This estimator is treated as control signal rather than strict divergence metric. To reduce high-frequency noise, short-horizon exponential moving average is applied: Only the smoothed estimate Dt is used for penalty computation. Dt = 0.9 Dt1 + 0.1 ˆDt. 4.3.2 Entropy-Gated Penalty The entropy-gated KL penalty is defined as: LKL = 0, λ( Dt τt)2 max (cid:16) 0.5, Hfloor H(πθ)+ϵe (cid:17) , Dt τt otherwise (22) (23) (24) where: τt is the adaptive divergence threshold, H(πθ) is the current policy entropy, Hfloor is minimum entropy reference, λ is scaling coefficient, 12 ϵe = 0.1 is numerical stabilizer. The entropy gate amplifies penalties when entropy becomes low and attenuates penalties when entropy remains high, allowing divergence regulation strength to adapt to the current exploration regime."
        },
        {
            "title": "4.4 PID-Controlled Adaptive Threshold",
            "content": "To adapt divergence tolerance during training, PID controller dynamically adjusts the KL threshold. The adaptive threshold is computed as: τt = τbase + Kpet + Ki (cid:88) k=0 ek + Kd(et et1), where the control error is defined using reward velocity: et = EMA(rt) vtarget. Here: EMA(rt) denotes exponentially smoothed reward, vtarget is fixed small positive target improvement rate, Kp, Ki, Kd are PID gains. The resulting threshold is clipped to bounded operating range: (25) (26) which prevents overly aggressive or overly permissive divergence constraints. τt clip(τt, 0.1, 0.6), (27)"
        },
        {
            "title": "4.5 Phase Detection Algorithm",
            "content": "Training dynamics differ substantially between early exploration, reward climbing, plateau phases, and late convergence. To adapt divergence tolerance across these regimes, we implement lightweight phase detector that monitors reward statistics over sliding windows and applies multiplicative adjustments ϕt to the KL threshold. Phase Detection Algorithm. The detector maintains 100-step reward history and classifies the current training phase based on mean reward trends and variance: The phase multipliers directly modulate the adaptive threshold τt = (τbase + PID(rt)) ϕt as described in Equation (25). This design enables: Warmup (ϕt = 1.5): Relaxed constraints permit broad exploration during initialization Climbing (ϕt = 1.2): Moderate constraints allow continued reward improvement Plateau (ϕt = 0.8): Tight constraints suppress drift when reward stagnates Converged (ϕt = 1.0): Normal constraints maintain stability near convergence Phase awareness improves controller robustness without introducing task-specific heuristics or manual scheduling. return Warmup, ϕt = 1.5 Algorithm 3 Training Phase Detection Require: Reward history H, current reward rt 1: Append rt to 2: if < 50 then 3: 4: end if 5: Rrecent H[50 :] {Last 50 steps} 6: Rold H[100 : 50] {Previous 50} 7: rrecent mean(Rrecent) 8: rold mean(Rold) 9: σrecent std(Rrecent) 10: if rrecent > rold + 0.01 then 11: 12: else if σrecent < 0.02 and rrecent > 0.7 then 13: 14: else 15: 16: end if return Converged, ϕt = 1.0 return Climbing, ϕt = 1.2 return Plateau, ϕt = 0."
        },
        {
            "title": "4.6 Preview KL Scaling (Optional)",
            "content": "The controller exposes optional preview-based KL scaling hooks that can be used to bound update magnitude. In the current implementation, these functions are defined but not activated during training."
        },
        {
            "title": "4.7 Policy and Value Optimization",
            "content": "The final optimization objective is: Ltotal = LPPO + 0.5Lvalue + LKL βH(πθ). PPO Objective (cid:104) LPPO = (cid:105) min(ρt ˆAt, clip(ρt, 1 ϵ, 1 + ϵ) ˆAt) , with importance ratio: ρt = πθ(atst) πold(atst) . (28) (29) (30) Value Loss. To stabilize critic learning under heavy-tailed reward distributions, we use clipped Huber regression objective, Lvalue = 1 2 (cid:2)Huberδ(Vsoft, r) + Huberδ(Vclip, r)(cid:3), (31) where denotes normalized rewards and Huberδ(, ) is the Huber loss with threshold δ Huber [1964]. The Huber loss behaves quadratically for small errors and linearly for large errors, combining the fast convergence of mean-squared error near the target with the robustness of absolute error for outliers. This caps the influence of rare but very large TD errors that arise from reward-model spikes or bootstrapping noise, preventing gradient explosions in the critic. Averaging the loss over both 14 the pessimistic soft-min value Vsoft and its clipped counterpart Vclip further reduces overestimation while keeping the value function responsive to genuine improvements, yielding smoother value traces and fewer destabilizing loss spikes during RLHF training."
        },
        {
            "title": "Additional Stabilization Mechanisms",
            "content": "Beyond the main control architecture, we employ several standard stabilization techniques that further reduce pathological updates and improve numerical robustness: Gradient norm clipping: We cap the magnitude of policy and value gradients to prevent single noisy minibatches or reward spikes from producing excessively large parameter updates that could destabilize training, following common practice for controlling exploding gradients in deep RL and sequence models Pascanu et al. [2013], Schulman et al. [2017]. Synchronized learning rates: Using matched step sizes for policy and critic helps avoid regimes where the critic badly lags behind the policy (underfitting) or overreacts to transient changes (overfitting), which can otherwise amplify valuepolicy feedback loops and degrade stability. Entropy regularization: small entropy bonus discourages premature collapse to highly deterministic policies, maintaining sufficient exploration so that the KL and entropy controllers operate on informative, non-degenerate trajectories, consistent with maximum-entropy RL objectives Haarnoja et al. [2018a]. Polyak target critic updates: We maintain slowly updated target critics via PolyakRuppert averaging Polyak and Juditsky [1992], which smooths bootstrap targets, reduces the variance and oscillation of TD errors, and makes the pessimistic critic ensemble more stable. Dual-timescale KL tracking: Shortand long-horizon moving averages of KL divergence provide reliable diagnostic signals for the controllers, separating transient batch noise from genuine distributional drift and enabling more calibrated adjustments of KL penalties."
        },
        {
            "title": "5 Training Algorithm",
            "content": "Algorithm 4 SAFE Training Step Require: Policy πθ, reference policy πref, critics V1, V2, target critics 1: Sample batch of prompts {xi} 2: Generate responses yi πθ(xi) 3: Compute rewards ri = R(xi, yi) 4: Normalize rewards ri 5: Compute policy entropy H(πθ) 6: Evaluate critics V1(si), V2(si) 7: Compute pessimistic value: 1, 2, reward model Vsoft(si) = SoftMin(V1(si), V2(si)) 8: Compute advantages: Ai = ri Vsoft(si) 9: Standardize advantages ˆAi 10: for each PPO epoch do 11: Compute importance ratio ρi Compute clipped PPO loss LPPO Compute log-ratio estimate: 12: 13: ˆD = E[log πθ(as) log πref(as)] 14: Update PID controller using reward velocity 15: Compute adaptive KL threshold τt Compute entropy-gated KL penalty LKL Compute clipped Huber value loss Lvalue 17: 18: Update policy and critic parameters using: 16: Ltotal = LPPO + 0.5Lvalue + LKL βH(πθ) 19: Apply gradient norm clipping 20: end for 21: Update target critics via Polyak averaging: θ (1 τ )θ + τ θ"
        },
        {
            "title": "6 Objective Function",
            "content": "Ltotal = LP + 0.5LHuber + LKL βH(πθ) Where: LP = E[min(ρtAt, clip(ρt)At)] (32) (33) LHuber = 1 2 [Huber(Vsof t, r) + Huber(Vclip, r)] (34)"
        },
        {
            "title": "7.1 Experimental Setup",
            "content": "We evaluate SAFE using custom RLHF training pipeline implementing the proposed entropyaware predictive control and pessimistic value estimation components. All methods are trained under identical computational budgets, datasets, and optimization settings to ensure fair comparison. Policy Model. We fine-tune the Qwen/Qwen2.5-3B Qwen Team [2024] causal language model using parameter-efficient LoRA adaptation. LoRA is applied to attention and feed-forward projection layers with rank = 128 and scaling factor α = 128. Only adapter parameters are updated during training. Reference Policy. frozen copy of the base model is maintained as the reference policy for KL regularization throughout training. Reward Model. Preference supervision is provided by the pretrained RLHFlow/ArmoRM-Llama3-8B-v0.1 reward model, which outputs scalar quality scores for generated responses. Reward model inference is performed in evaluation mode without gradient updates. Dataset. Experiments are conducted on the Anthropic/hh-rlhfRLHFlow Contributors [2024] dataset. We use 5,000 prompt samples for training and 500 held-out samples for evaluation. Prompts are extracted by truncating assistant responses and retaining the conversational context prefix. Optimization Configuration. Training is performed for 2,000 on-policy update steps using batch size 16 and gradient accumulation factor 2. Policy and critic learning rates are synchronized at 1 105. PPO clipping thresholds are set to ϵ = 0.2 for policy updates and value function clipping. Gradient norms are clipped to 1.0 for the policy network and 0.5 for the critic to improve numerical stability. Stabilization Components. SAFE incorporates the following stabilization mechanisms: Double soft-min critic with Layer Normalization, Entropy-aware KL penalty with entropy floor Hfloor = 2.0, PID-controlled adaptive KL threshold, Phase-aware threshold modulation, Preview-based KL step scaling with ceiling κmax = 0.5. All experiments use identical hardware environments and random seed initialization protocols where applicable. 17 Figure 2: Training dynamics of SAFE. Top row: reward trajectory, KL divergence with adaptive threshold, and value loss. Bottom row: policy entropy with entropy floor, completion length, and smoothed reward. The controller maintains entropy above the configured floor while dynamically regulating KL magnitude."
        },
        {
            "title": "7.2 Overall Performance",
            "content": "Table 2 reports aggregate performance statistics over 2,000 training steps. SAFE achieves higher average reward compared to PPO while maintaining comparable KL divergence magnitude. In addition, reward variance is substantially reduced, indicating more stable optimization behavior across training iterations."
        },
        {
            "title": "7.3 Training Stability Analysis",
            "content": "We evaluate training stability using rolling statistics and event-based indicators. Reward Stability. The coefficient of variation of reward decreases from 0.114 under PPO to 0.040 under SAFE. Rolling reward standard deviation is reduced from 0.0208 to 0.0123. Furthermore, PPO exhibits two reward collapse events, defined as drops exceeding 20% relative to recent averages, while no such events are observed for SAFE. KL Stability. Although average KL divergence remains similar across methods, SAFE exhibits lower KL volatility (rolling standard deviation 0.306 versus 0.526 for PPO). This indicates that Table 2: Overall training statistics averaged across the full training run. Metric Mean Reward Reward Std Final Reward (last 50) Mean KL Mean Value Loss Mean Completion Length SAFE 0.7249 0.0291 0.7287 0.1729 0.0582 127.3 PPO 0.6894 0.0788 0.7259 0.1313 0.0059 123.8 the adaptive threshold mechanism constrains large divergence excursions while allowing moderate exploratory deviations. Value Learning Dynamics. The critic loss in SAFE remains bounded with final value loss of 0.0075. While PPO achieves lower absolute value loss, SAFE exhibits smoother temporal behavior with fewer abrupt spikes, consistent with the use of pessimistic aggregation and Polyak target updates."
        },
        {
            "title": "7.4 Convergence Behavior",
            "content": "To analyze convergence dynamics, training is partitioned into early (033%), mid (3366%), and late (66100%) phases. SAFE increases average reward from 0.711 in the early phase to 0.731 in the late phase. PPO exhibits larger early-stage reward gains but maintains higher variability throughout training. KL divergence under SAFE trends downward across phases, whereas PPO exhibits increasing divergence during later stages. Both methods satisfy convergence criterion defined as rolling reward standard deviation below 0.05 during the final 100 training steps."
        },
        {
            "title": "7.5 Statistical Significance",
            "content": "We evaluate statistical significance using step-level reward distributions. Reward Comparison. Welchs t-test yields = 18.90 with < 1075, while the MannWhitney test yields < 1054. The corresponding effect size (Cohens = 0.60) indicates medium practical effect. KL Comparison. Differences in KL divergence are not statistically significant (p = 0.10), suggesting that reward improvements are not primarily driven by increased policy drift. 19 Figure 3: Training dynamics and stability comparison between SAFE and PPO. Top row: Reward trajectory with confidence intervals, KL divergence with adaptive thresholds, and value function loss. Middle row: Reward distribution, KL distribution, and completion length evolution. Bottom row: RewardKL trade-off scatter, cumulative reward accumulation, rolling reward stability, reward box plots, KL box plots, and batch-level reward variance. SAFE exhibits tighter reward confidence bands, improved reward stability, controlled KL excursions, and smoother cumulative reward growth compared to PPO, indicating improved robustness under long-horizon RLHF optimization. Table 3: Comparison of SAFE variants and PPO over 2,000 training steps. Higher reward is better. Lower variance, value loss spikes, and reward crashes indicate improved stability. Metric Mean Reward Final Reward (last 50) Peak Reward Reward Std (global) Reward Coefficient of Variation Reward Volatility (rolling std) Reward Crashes (>20%) Mean KL Divergence Final KL Divergence KL Std KL Volatility (rolling std) KL Trend (early late) Mean Value Loss Final Value Loss (last 50) Value Loss Spikes (>0.1) Mean Completion Length Completion Length Std Converged (last 100 steps) Late-stage Regression PPO Asymmetric-KL SAFE 0.725 0.672 0.689 0.729 0.714 0.726 0.748 0.747 0.748 0.029 0.054 0.079 0.040 0.081 0.114 0.0123 0.0343 0.0208 2 2 0 -0.053 -0.241 0.131 -0.054 -0.349 0.692 0.744 0.661 0.869 0.306 0.294 0.526 -1.30 -1.07 +0.70 0.058 0.226 0.0059 0.0075 0.114 0.0009 1256 28 435 127.3 119.4 123.8 3.1 9.7 12.6 Yes Yes Yes No Yes No"
        },
        {
            "title": "7.6 Qualitative Training Dynamics",
            "content": "Figure 3 illustrates representative training trajectories. The entropy-aware controller maintains entropy above the configured floor throughout training. Completion lengths stabilize earlier, and reward curves exhibit reduced oscillatory behavior relative to PPO. The adaptive KL threshold dynamically tracks training phases, tightening during plateau periods and relaxing during early exploration, consistent with the controller design."
        },
        {
            "title": "7.7 Discussion",
            "content": "These results indicate that combining pessimistic value estimation with entropy-aware predictive KL regulation improves training robustness and reduces instability under long-horizon RLHF optimization. Importantly, these improvements are achieved without substantially increasing average KL divergence. We emphasize that the reported gains reflect behavior on single benchmark and training configuration. Further evaluation is required to assess generalization across tasks and model scales."
        },
        {
            "title": "7.8 GPU Memory and Performance Overhead",
            "content": "To assess the computational cost of the additional control logic in SAFE, we tracked GPU memory usage and wall-clock time for both SAFE and PPO over the full 2,000-step run (Figure 4). Table 4 summarizes the key metrics. Memory Usage. Initial memory footprints after model load and trainer initialization are effectively identical (SAFE: 12.69 GB vs. PPO: 12.80 GB). Peak memory during training reaches 21 Figure 4: GPU memory and runtime comparison between SAFE and PPO. Left: GPU peak memory usage per training step over 2,000 iterations. Both methods exhibit similar memory profiles with comparable peak allocations (54 GB). Center: Wall-clock time per training step. SAFE maintains slightly faster step times with reduced variance, indicating that the additional control logic does not introduce computational bottlenecks. Right: Summary comparison of initialization memory, peak memory, and average step time. SAFE achieves near-identical resource usage with 0.9% memory overhead and 1.4% time overhead, demonstrating that the multi-layer stabilization framework is computationally efficient. 54.05 GB for SAFE and 54.04 GB for PPO, negligible difference of +6 MB. Average per-step allocated memory is similarly matched (15,554 MB vs. 15,569 MB). Overall, SAFE incurs net memory overhead of approximately 0.9%, indicating that the additional critics and control statistics do not materially increase GPU usage. Runtime. SAFE completes training in 1,951.7 compared to 1,980.1 for PPO, reduction of 28.4 over 2,000 steps. The average step time is 3.90 for SAFE versus 3.96 for PPO, corresponding to throughputs of 15.4 and 15.2 steps per minute, respectively. This translates to an effective time overhead of 1.4%, i.e., the additional controller logic does not slow training and may even slightly improve runtime due to smoother dynamics and fewer large, destabilizing updates. Conclusion. Across both memory and runtime metrics, SAFE behaves as drop-in replacement for PPO from hardware perspective: the multi-layer stabilization framework adds negligible GPU cost while providing the stability benefits described in the preceding sections."
        },
        {
            "title": "8 Experimental Analysis",
            "content": "This section analyzes the empirical behavior of SAFE under the Entropy-Aware Predictive Controller using the metrics and training traces described in the previous section. We focus on reward progression, stability characteristics, divergence control, and interaction between entropy and KL regulation."
        },
        {
            "title": "8.1 Reward Learning Dynamics",
            "content": "Figure 3 shows the evolution of batch reward and smoothed reward over training. SAFE exhibits rapid early improvement during the warmup phase followed by gradual stabilization. 22 Table 4: GPU memory and timing overhead comparison between SAFE and PPO. Metric SAFE PPO Memory Usage (MB) After Model Load After Trainer Init Peak During Training Avg Per-Step Allocated Timing (seconds) 12,687 12,696 54,050 15,554 12,803 116 12,805 109 +6 54,044 15 15,569 Total Training Time Avg Step Time Throughput (steps/min) 1,951.7 3.90 15.4 1,980.1 28.4 3.96 0.06 +0.2 15.2 Overhead Summary Memory Overhead Time Overhead 0.9% 1.4% Across the full training run, the average reward reaches 0.7249, compared to 0.6894 for PPO. More importantly, reward variance is substantially lower under SAFE (standard deviation 0.0291 versus 0.0788). This indicates that the controller primarily contributes to reducing training noise rather than simply increasing peak reward. Smoothed reward trajectories further show reduced oscillatory behavior, suggesting improved stability during late-stage optimization."
        },
        {
            "title": "8.2 KL Divergence Behavior",
            "content": "The adaptive KL controller maintains divergence within bounded operating region while allowing moderate exploratory fluctuations. Although the mean KL magnitude remains comparable to PPO, KL volatility is reduced by approximately 42% (rolling standard deviation 0.306 versus 0.526). The dynamic threshold adapts over time in response to reward trends and phase transitions. During early training, relaxed thresholds permit exploration, while tighter thresholds are applied during plateau regions to constrain excessive drift. This behavior is consistent with the controller design objective of regulating divergence without enforcing fixed KL target."
        },
        {
            "title": "8.3 Entropy Regulation and Exploration Stability",
            "content": "Policy entropy remains consistently above the configured entropy floor throughout training. Rather than enforcing fixed entropy bonus, the entropy-aware gating mechanism selectively amplifies KL penalties when entropy decreases. This interaction allows entropy to decay gradually instead of collapsing abruptly. As result, completion length stabilizes earlier and remains more consistent across training steps, indicating controlled policy determinization. Importantly, entropy stabilization is achieved without introducing additional explicit entropy maximization terms beyond the standard PPO entropy coefficient."
        },
        {
            "title": "8.4 Value Function Stability",
            "content": "The double critic with soft-min aggregation produces bounded value loss throughout training. Although absolute value loss remains higher than PPO, SAFE exhibits smoother temporal behavior with fewer abrupt divergence events. The use of Layer Normalization and Polyak target updates contributes to stabilizing bootstrap targets, while pessimistic aggregation reduces optimistic bias in advantage estimation. These effects are reflected in reduced reward volatility and improved convergence consistency."
        },
        {
            "title": "8.5 Phase-Aware Control Behavior",
            "content": "The phase detector identifies transitions between warmup, climbing, plateau, and convergence regimes. During climbing phases, thresholds are relaxed to accommodate reward improvement. During plateau phases, stricter constraints are applied to limit unnecessary policy drift. This adaptive modulation allows the controller to adjust its behavior according to training dynamics rather than relying on static scheduling rules."
        },
        {
            "title": "8.6 Statistical Reliability",
            "content": "Statistical testing confirms that reward improvements are unlikely to arise from random fluctuations. Welchs t-test and MannWhitney test both reject the null hypothesis of equal reward distributions with < 1050. The effect size (Cohens = 0.60) indicates moderate practical improvement. In contrast, KL divergence differences are not statistically significant (p = 0.10), suggesting that performance gains are primarily associated with improved stability rather than increased divergence."
        },
        {
            "title": "8.7 Failure Modes and Limitations",
            "content": "Despite improved stability, several limitations are observed. Value loss remains higher relative to PPO, reflecting the conservative bias introduced by pessimistic aggregation. Additionally, while reward volatility is reduced, KL spike frequency remains non-negligible, indicating that further smoothing of short-term divergence dynamics may be beneficial. These observations motivate future work on controller smoothing strategies and alternative critic regularization methods."
        },
        {
            "title": "8.8 Summary of Findings",
            "content": "Overall, the experimental results indicate that SAFE improves training stability by jointly regulating divergence, entropy dynamics, and value estimation behavior. The primary benefit arises from reduced reward volatility and more consistent convergence patterns rather than aggressive reward maximization. These findings support the use of adaptive control mechanisms as complementary stabilization tools within RLHF training pipelines."
        },
        {
            "title": "9 Conclusion",
            "content": "We presented SAFE, multi-layer RLHF stabilization framework combining pessimistic value estimation, entropy-gated KL control, and PID-based adaptive thresholds. Experiments on 3B 24 model show SAFE eliminates catastrophic collapses and reduces reward variance by 2.8 compared to PPO, with statistically significant 5.2% reward improvement. These results suggest that coordinated control across value, divergence, and temporal dynamics can improve training stability. However, important limitations remain. Value loss spikes persist (435 events vs PPOs 28), the method requires manual tuning of 8+ hyperparameters without systematic ablation, and evaluation is restricted to one model size, dataset, and reward model over 2,000 steps. Generalization beyond this setting is unproven. We can only hypothesize that the SAFE is more robust to reward hacking from the theoretical viewpoint, but we need proper empirical evidence to prove that. SAFE demonstrates that layered control mechanisms can reduce instability in specific experimental configuration, which is useful contribution to RLHF research. But broader claims about production readiness or general applicability require validation across multiple scales, tasks, and longer training horizons. Future work should prioritize systematic ablations, automated hyperparameter tuning, and multi-scale evaluation before deployment."
        },
        {
            "title": "10 Limitations",
            "content": "While SAFE demonstrates improved stability over PPO, several limitations constrain generalizability: Experimental Scope. Single scale: Evaluated only at 3B parameters; scaling behavior at 7B+ untested. Short horizon: Limited to 2,000 steps; late-stage dynamics beyond 5,000+ steps unknown. Single dataset/reward model: Anthropic HH-RLHF and ArmoRM-Llama3-8B only; generalization to other domains unvalidated. Methodological Gaps. Value instability persists: 435 value loss spikes vs PPOs 28, indicating soft-min introduces bias without eliminating instability. Manual tuning: 8+ hyperparameters (entropy floor, PID gains, phase thresholds) lack systematic ablation or sensitivity analysis. No component isolation: Relative contribution of pessimistic critics, entropy gating, and PID control unclear. Future work requires multi-scale evaluation (1B70B), long-horizon experiments (10,000+ steps), systematic ablations, automated hyperparameter tuning, diverse task validation, and direct reward hacking tests."
        },
        {
            "title": "References",
            "content": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Karl Johan Astrom and Richard Murray. Feedback systems: an introduction for scientists and engineers. Princeton University Press, 2008. 25 Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. URL https://arxiv.org/abs/1706. 03741. Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorIn International Conference on Machine Learning, pages 15871596. PMLR, critic methods. 2018. Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. theory of regularized markov decision processes. Proceedings of the 36th International Conference on Machine Learning (ICML), 97: 21602169, 2019. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML), volume 80, pages 18611870. PMLR, 2018a. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor, 2018b. URL https: //arxiv.org/abs/1801.01290. Peter J. Huber. Robust estimation of location parameter. The Annals of Mathematical Statistics, 35(1):73101, 1964. Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, volume 30, 2017. Gergely Neu, Anders Jonsson, and A. Gomez. unified view of entropy-regularized markov decision processes. In Proceedings of the European Workshop on Reinforcement Learning (EWRL), 2017. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 13101318, 2013. Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838855, 1992. 26 Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Available at https://arxiv.org/abs/2412.15115. RLHFlow Contributors. Armorm-llama3-8b-v0.1. https://huggingface.co/RLHFlow/ ArmoRM-Llama3-8B-v0.1, 2024. Reward model used for SAFE experiments. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward hacking. arXiv preprint arXiv:2209.13085, 2022. Hado Van Hasselt. Double q-learning. Advances in Neural Information Processing Systems, 23, 2010. Theoretical Analysis: Hypothesized Reward Hacking Resistance Reward hacking occurs when policy exploits spurious features of the learned reward model (e.g., length bonuses, keyword artifacts) instead of improving alignment with true human preferences Amodei et al. [2016], Skalse et al. [2022]. In this work, we do not directly evaluate SAFE on deliberately hackable reward models. The discussion in this section is therefore purely theoretical and should be interpreted as hypothesis, not as empirical evidence of robustness. Mechanism 1: Pessimistic Value Estimation Suppresses Exploitation Spikes. The double soft-min critic (Section 3.1) computes value estimates as Vsoft(s) = α log (cid:104) 1 2 (cid:16) eV1(s)/α + eV2(s)/α(cid:17)(cid:105) , which introduces systematic negative (pessimistic) bias. Reward hacking typically manifests as high-variance, high-reward outliers when the policy discovers an exploit. The pessimistic aggregation reduces the estimated value of such outliers, making them appear less attractive during advantage computation. Consequently, policy updates are less likely to chase these exploitation spikes than under single, potentially overestimating critic. Mechanism 2: Directional KL Control Detects Exploitative Drift. Section 2.2 shows that sustained positive log-ratio estimates combined with decreasing entropy often precede exploitative behavior. SAFEs asymmetric KL component (Section 3.1) penalizes positive deviations quadratically when ˆDKL > τ while imposing zero penalty on negative estimates. Since reward hacking involves shifting probability mass toward exploit patterns that depart from the reference distribution, such directional control may intervene earlier than symmetric penalties. The momentum term mt = ( ˆD(t) )/w further responds to accelerating divergence, potentially catching the onset of exploitation before catastrophic drift occurs. KL ˆD(tw) KL 27 Mechanism 3: Entropy-Gated Penalties Suppress Low-Entropy Exploitation. The entropy-gated penalty (Equation (14)) scales the KL loss by (cid:18) gt = max 0.5, Hfloor H(πθ) + ε (cid:19) , amplifying penalties when the policy entropy H(πθ) decreases. Many reward-hacking strategies (e.g., keyword stuffing, repetitive templates, extreme length manipulation) correspond to lowentropy, highly deterministic behavior. By increasing penalty strength precisely when entropy collapses, SAFE applies stronger corrective pressure in regimes most associated with exploitation, while allowing relatively unpenalized exploration when entropy remains high. Mechanism 4: Tighter Distributional Anchoring. Compared to PPO, SAFE maintains lower KL volatility and keeps the policy closer to the supervised reference throughout training  (Table 2)  . Because the reference policy has never been optimized against the reward model, it is less likely to have learned exploitative patterns. By constraining distributional drift around this reference, SAFE may reduce the policys capacity to discover and exploit reward model artifacts that lie far from the supervised training distribution. Theoretical Prediction and Caveats. Taken together, these mechanisms suggest that SAFE may provide partial resistance to reward hacking by: (1) reducing the attractiveness of high-variance exploits via pessimistic values, (2) detecting exploitative drift earlier through directional KL and momentum signals, (3) suppressing deterministic exploitation via entropy-gated penalties, and (4) constraining search to regions closer to the reference policy. However, this analysis is entirely mechanistic and does not constitute empirical validation. SAFE does not address the fundamental mismatch between the learned reward model r(x, y) and the true preference function. At best, it may slow or redirect exploitation, but it cannot eliminate reward hacking without improving reward model fidelity. rigorous assessment of SAFEs reward-hacking behavior requires dedicated experiments with explicitly hackable reward functions, adversarial and out-of-distribution prompts, and long training horizons. Designing and running such evaluations is an important direction for future work and is beyond the scope of the present study."
        }
    ],
    "affiliations": []
}