{
    "paper_title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "authors": [
        "Yunkai Zhang",
        "Yawen Zhang",
        "Ming Zheng",
        "Kezhen Chen",
        "Chongyang Gao",
        "Ruian Ge",
        "Siyuan Teng",
        "Amine Jelloul",
        "Jinmeng Rao",
        "Xiaoyuan Guo",
        "Chiang-Wei Fang",
        "Zeyu Zheng",
        "Jie Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \\textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \\textbf{TS-Insights}\\footnote{Available at \\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \\textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \\citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 1 5 2 1 1 . 2 1 5 2 : r Insight Miner: Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language Yunkai Zhang UC Berkeley yunkai_zhang@berkeley.edu Yawen Zhang Mineral yawenz@mineral.ai Ming Zheng Mineral zhengming@mineral.ai Kezhen Chen Mineral kezhenchen@mineral.ai Chongyang Gao Northwestern University Chongyanggao2026@u.northwestern.edu Ruian Ge UC Berkeley ruian_ge@berkeley.edu Siyuan Teng UC Berkeley siyuan_teng@berkeley.edu Amine Jelloul UC Berkeley amine_jelloul@berkeley.edu Jinmeng Rao Mineral jinmengrao@mineral.ai Xiaoyuan Guo Mineral xiaoyuanguo@mineral.ai Chiang-Wei Fang UC Berkeley chiangwei_fang@berkeley.edu Zeyu Zheng UC Berkeley zyzheng@berkeley.edu Jie Yang Mineral yangjie@mineral.ai"
        },
        {
            "title": "Abstract",
            "content": "Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, process that is both time-consuming and labor-intensive. In this paper, we propose Insight Miner, large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce TS-Insights2, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using novel agentic workflow, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA [1] and GPT-4, in generating time-series descriptions and insights. Our findings suggest promising direction Work done during internship at Google - Mineral.ai. 2Available at https://huggingface.co/datasets/zhykoties/time-series-language-alignment. NeurIPS 2023 AI for Science Workshop. for leveraging LMMs in time series analysis, and serve as foundational step toward enabling LLMs to interpret time series as native input modality."
        },
        {
            "title": "Introduction",
            "content": "Time series data is fundamental to wide range of domains. Traditionally, researchers have relied on statistical tools to analyze and interpret this data. Methods such as Autoregressive Integrated Moving Average (ARIMA) [2], Seasonal-Trend Decomposition using LoESS (STL) [3], and state space models [4] have long been the standard for forecasting, detecting seasonality, and understanding underlying trends. These techniques are particularly prevalent in fields like economics, meteorology, and transportation, providing essential interpretability for complex temporal data. Recently, many studies have explored the usage of LLMs for time-series tasks. Several studies have leveraged pretrained LMs (e.g., GPT-2) for tasks such as forecasting, classification, and anomaly detection [5, 6], achieving state-of-the-art performance and demonstrating the universality of pretrained representations. Other works have designed structured prompts to enable zero-shot or few-shot inference [7, 8]. However, these approaches primarily focus on tasks where the output is numerical (scalars or future time steps). Directly training LLMs to perform traditional numerical tasks does not inherently enable them to generate natural language insights or explain the data. On the other hand, the emergence of Large Multimodal Models (LMMs), such as LLaVA [9], has inspired new approaches to align domain-specific data with language models.A notable example is FinVis-GPT [10], which builds upon LLaVA to generate financial task-oriented dataset for alignment and instruction tuning. While FinVis-GPT demonstrates the feasibility of LMMs in analyzing financial charts, our work aims to generalize this success beyond specific domain. We focus on constructing time series analysis dataset for LMMs. To the best of our knowledge, no such dataset currently exists for aligning general time-series data with comprehensive textual descriptions. In summary, our main contributions are two-fold: 1) We present time series analysis dataset that enables LLMs to generate faithful and descriptive time series insights; and 2) We provide the first large-scale repository aligning time-series data into the language embedding space, paving the way for future research on LMMs for time-series data analysis."
        },
        {
            "title": "2 TS-Insights Dataset",
            "content": "To our knowledge, there are no existing large-scale datasets of time series and language description pairs, let alone for time series analysis. To bridge this gap, we design and generate the TS-Insights Dataset, the first dataset specifically curated for general time series analysis alignment. Formally, given time series datasets {Di}N i=1, where each dataset Di has Ti total time steps and Mi features, i.e., Di = {Xj}Ti j=1 and Xj RMi, the goal is to generate question-answer pair for each time series window Wk Rmkτk randomly sampled from the datasets, where τk represents the number of time steps and mk represents the number of features, which are both randomly subsampled from the chosen dataset.3 Each training sample consists of time series window Wk, question LQ , and an answer LA ), we create single-round instruction-following example [1]: . Using (Wk, LQ , LA Human: Wkn LQ < STOP > Assistant: LA < STOP > n. (1) To generate such datasets for modalities such as images [1] or biomedical images [11], the common practice is to prompt language-only model (e.g., GPT-4). For example, LLaVA [1] asks GPT-4 to generate multi-turn conversations based on image captions and object bounding boxes. However, the time series modality presents unique challenges: 1) there are no original captions available for time series window, 2) existing tools cannot readily convert time series segment into an input format that is suitable for language-only GPTs, and 3) the semantic meanings of time series windows are more difficult to be described in natural languages. 3To generate the current dataset, τk is randomly sampled from [30, 500]. 2 To address the third challenge, we focus on time series windows that contain single feature, i.e., Wk R1τk , and following traditional time series analysis [12], we generate descriptions based on the trend, the seasonality, and the residuals that are present in the window. naive solution is to feed in the raw time series as vector when prompting GPT-4, e.g., \"Given the time series [0.52, 0.98, 0.95, 0.91, 1.24, ..., 1.32], generate description about its trend, seasonality, and volatility.\" However, we found that GPT-4 fails to accurately extract each component from the raw vector.4 Instead, we implement tool-use pipeline where we leverage the Seasonal-Trend Decomposition (STL) algorithm to mathematically decompose the original time series into its constituent trend, seasonality, and residual components. We then generate description based only on one component at time. As proof of concept, we focus on the trend description in the current version of this paper. 2.1 Trend Generation Workflow To generate the trend description for given time series window Wk R1τk , we first apply an STL decomposition to extract the trend Wk = Tk + Sk + Rk, (2) Figure 1: Trend dataset generation workflow using statistical tools. where Tk, Sk, and Rk denote the extracted trend, seasonality, and residual components, respectively. We denote the value at each time step of the extracted trend as Tk = (ˆy1, ˆy2, , ˆyτk ). In some cases, Wk might not exhibit any seasonalities. In such cases, we fit Gaussian Process (GP) to the τk time steps in the window. Let Wk = (y1, y2, , yτk ), where yi represents the value at time step i. Wk is modeled by standard zero-mean GP, whose covariance structure is defined by kernel K(., .). Here, the kernel used is combination of Radial Basis Function (RBF) kernel to model the dependency among different time steps and white-noise kernel to model the observational noise. That is, Wk GP (µ(x), K(x, x)), where µ(x) = 0, K(x, x) = RBF (x, x) + σ2 δx,x, RBF (x, x) = σ2 , and σ2 are estimated from the data by maximizing the likelihood. We then compute the posterior mean of the Gaussian Process regression at the respective time steps to get Tk = (ˆy1, ˆy2, , ˆyτk ) as the extracted trend. ) and δx,x is the Kronecker delta. The parameters σ2 exp( (xx)2 2γ We then apply Gaussian kernel Fk = [F1, F2, , Fwk ], where wk is hyperparameter for the kernel size, to further smooth out the trend, and followed by downsampling with stride size sk 5: wk//2 (cid:88) yi = j=wk//2 ˆyskij Fwk//2+j (3) 4See Appendix for failure cases. 5We choose stride size sk so that τk//sk = 25. for = 1, 2, , τk//sk. Finally, we round each entry of (y1, y2, , yτk//sk ) to one decimal place and feed it to GPT-4. As such, one data sample pair consists of the original time series window Wk and the trend description generated by GPT-4. An overview of the workflow and the exact prompt we use is shown in Figure 1. 2.2 Trend Description Dataset Using the methodology described above, we generate 10,000 initial samples derived from 29 datasets in the Monash Time Series Forecasting Archive [13]. We reserve 11 additional datasets as holdout set, which are only used for evaluation but not for training. The 29 datasets span wide range of domains, including energy [14], weather, traffic [15], and healthcare [16]. Notably, we only sample windows from the train split of each dataset, defined to be the first 70% of the time steps in temporal order. Some datasets contain multiple levels of seasonalities, e.g., daily and weekly. Under the original granularity, each window might not contain enough time steps to discern the higher level of seasonalities, since at least two full cycles are required to conclude there to be seasonality. As trends should be described after seasonalities are removed, for each dataset, we also aggregate multiple time steps into one time step in order to introduce samples with more diversified patterns. To further increase the number of training samples in cost-efficient manner, for each GPT-4 labeled sample pair, we additionally apply nine different random augmentations to the original time series window Wk such that the trend description is still applicable to the augmented samples. We then rephrase the original description generated by GPT-4 using GPT-3.5-turbo in order to increase the language diversity. Therefore, for each original sample, we now have nine augmented samples, resulting in 100k total training samples. detailed list of test and holdout datasets, the number of samples we generate for each aggregated granularity level, as well as list of augmentation methods can be found in Appendix A."
        },
        {
            "title": "Insight Miner",
            "content": "We initialize our model using the pre-trained weights of LLaVA [1], state-of-the-art general-domain vision-language model, and continue finetuning the LLaVA weights to the time series domain. We use the same neural network architecture as LLaVA: we first convert the time series window into an image using line plot, feed the image into the vision encoder, and then use linear projection layer to map the vision output into the language embedding space. Finally, the language model takes in the projected image embeddings concatenated with the language instructions as the input and returns the language response. To align the time-series images with the LLM, we only finetune the linear projection layer, while keeping both the vision encoder and the language model frozen. For each training sample, we show the original time series to the model in the form of line plot and the language instruction is to ask it to describe the trend, and the goal is to predict the description generated by the GPTs. The final model is named Insight Miner. Note that the training cost of Insight Miner is relatively affordable as it was trained using 8 A100 40GiB GPUs. Each epoch takes around an hour to train. Once the model finishes training, it can be easily deployed at low inference cost."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct experiments to evaluate how well the trend dataset can enable large multimodal models to generate trend descriptions that are faithful to the original time series. More specifically, we sample 119 total windows for evaluation. Among these, 69 examples are from the test split (last 30%) of the same datasets we used for training, and the other 50 examples are from the holdout datasets which are not used for training entirely. The models we include for comparison are: LLaVA [1]: using the checkpoint publicly available on HuggingFace. Vision (3 epochs): finetuned from the above LLaVA checkpoint for three epochs using the generated trend dataset. It takes in the original time series window plotted using the lineplot function in the Seaborn package. 4 Vision (1 epoch): finetuned from the above LLaVA checkpoint for one epochs using the generated trend dataset. Engineering GPT: GPT-4 that takes in the extracted features as described in Section 2.1. Here, Vision (3 epochs) and Vision (1 epoch) are two versions of our Insight Miner trained using different number of epochs. As we observed feeding the raw time series vector into GPT-4 leads to inferior descriptions compared to Engineering GPT, we do not include it for evaluation in this section, but it is included in the eight case studies shown in Appendix B, along with the other four models. For each of the 119 samples, we generate one description using each of the above models, and ask three domain experts to manually score the descriptions generated. When presented to the domain expert, the descriptions from different models are shuffled in random order for each sample. score of 2 is given if the description matches the original time series, score of 1 is given if the description is partially correct, and score of zero is given if the description is not correct. We sum the scores from all human evaluators for all test (holdout) samples and normalize it to 0 1 to produce the final score for each model. The results are summarized in Figure 2. As we see, both of our models, Vision (3 epochs) and Vision (1 epoch), significantly outperforms the original LLaVA model. Additionally, training for more epochs seems to lead to better performance. In fact, using the vision encoder trained for three epochs can lead to performance that is competitive to GPT-4, although the latter requires first preprocessing the time series using heuristics and statistical tools. Notably, Vision (3 epochs) outperforms GPT-4 on the holdout datasets. We hypothesize that this is because the holdout datasets contain more datasets with complicated seasonalities than the test datasets. Even though Engineering GPT-4 has access to the extracted features, it essentially still performs zero-shot inference. In comparison, our model is finetuned using the proposed TS-Insights dataset and can better leverage the abundance of labeled samples. Figure 2: Description evaluation of different models by domain experts."
        },
        {
            "title": "5 Discussions",
            "content": "This work presents the first large dataset with 100k training samples for general time series analysis in the form of time series and natural language pairs. We show that the proposed dataset can enable existing large multimodal models to align time series data with textual descriptions and perform detailed analysis. In addition to the models evaluated in Section 4, we also tried to use OneFitsAll [5] as the time-series encoder to replace the vision encoder in LLaVA. Our initial attempt shows that using time-series encoder causes the model to fail to generate coherent descriptions for most samples, which is likely due to that unlike the original vision encoder, the time-series encoder is not pretrained. Therefore, we leave the pretraining of the time-series encoder as future work. It will be interesting to see whether the proposed dataset can enable large multimodal models to improve forecasting or classification accuracies, since the generated dataset allows them to associate the raw time series vector with common statistical concepts in the form of natural languages. In terms of the dataset itself, our workflow for generating trend descriptions sheds the light on how descriptions regarding other time series properties can be generated, e.g., the change in volatility, or outlier identification using the extracted residuals. more challenging task will be to generate descriptions for time series with multiple features, such as by studying their cross-correlations [17]."
        },
        {
            "title": "References",
            "content": "[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [2] George EP Box, Gwilym Jenkins, Gregory Reinsel, and Greta Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [3] Robert Cleveland, William Cleveland, Jean McRae, and Irma Terpenning. Stl: seasonal-trend decomposition. J. Off. Stat, 6(1):373, 1990. [4] James Hamilton. State-space models. Handbook of econometrics, 4:30393080, 1994. [5] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. arXiv preprint arXiv:2302.11939, 2023. [6] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023. [7] Hao Xue and Flora Salim. Promptcast: new prompt-based learning paradigm for time series forecasting. 2022. [8] Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, and Yanbin Lu. Temporal data meets llmexplainable financial time series forecasting. arXiv preprint arXiv:2306.11025, 2023. [9] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023. [10] Ziao Wang, Yuhang Li, Junda Wu, Jaehyeon Soon, and Xiaofeng Zhang. Finvis-gpt: multimodal large language model for financial chart analysis. arXiv preprint arXiv:2308.01430, 2023. [11] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day, 2023. [12] Peter Brockwell and Richard Davis. Time Series: Theory and Methods. Springer Series in Statistics, 1991. [13] Rakshitha Wathsadini Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob Hyndman, and Pablo Montero-Manso. Monash time series forecasting archive. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [14] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling longand short-term temporal patterns with deep neural networks. pages 95104, 2018. [15] Jean-Michel. Smart meter data from london area, 2019. [16] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track covid-19 in real time. The Lancet Infectious Diseases, 20:533534, 02 2020. [17] Bracewell R. Pentagram notation for cross correlation. The Fourier Transform and Its Applications. N, 1965."
        },
        {
            "title": "A Trend Dataset Details",
            "content": "The 20 datasets involved in generating the TS-Insights dataset are listed below. Dataset Name saugeenday_dataset rideshare_dataset_without_missing_values pedestrian_counts_dataset oikolab_weather_dataset nn5_daily_dataset_without_missing_values m1_yearly_dataset m1_quarterly_dataset m1_monthly_dataset london_smart_meters_dataset_without_missing_values kdd_cup_2018_dataset_without_missing_values kaggle_web_traffic_weekly_dataset kaggle_web_traffic_dataset_without_missing_values hospital_dataset fred_md_dataset elecdemand_dataset covid_mobility_dataset_without_missing_values covid_deaths_dataset cif_2016_dataset bitcoin_dataset_without_missing_values australian_electricity_demand_dataset Granularity Number of Samples 201 daily 1001 hourly 752 hourly 1141 hourly 301 daily 51 tridaily 51 weekly 100 yearly 121 quarterly 351 monthly 1000 half-hourly 800 hourly 800 weekly 800 daily 500 monthly 201 monthly 102 half-hourly 102 hourly 80 two-hourly 76 three-hourly 72 four-hourly 64 six-hourly 17 eight-hourly 17 twice-daily 9 daily 318 daily 280 daily 76 monthly 376 daily 600 half-hourly Total 10360 The following augmentations maybe applied to given time-series window each with probability of 50%: Jittering: Adding Gaussian noise to the original time series, where the standard deviation of the Gaussian noise is set to be the standard deviation from local rolling window of size 4. Scaling: Multiplying the original time series with constant. Shifting: Adding constant to the original time series. Smoothing: Convolving the original time series window with an average kernel of randomly sampled size. Downsampling: Only keeping every other steps, where is another randomly sampled integer. Note that multiple augmentations can be applied to get the final augmented window. The holdout datasets are Electricity Demand (hourly, three-hourly, six-hourly, weekly), M3 (monthly, quarterly, other), M4 (hourly, daily, weekly, monthly, quarterly), Traffic (hourly, bi-hourly, fourhourly), and Weather (daily). On HuggingFace, we release ten window samples for each holdout and test dataset. However, due to the lack of human resources, the evaluation reported in Section 4 was done only using the first three 7 samples from each dataset. Unfortunately, we only saved the original time series windows but not the augmented windows, but they should be able to be easily generated again using the scripted provided."
        },
        {
            "title": "B Case Studies",
            "content": "8"
        }
    ],
    "affiliations": [
        "Mineral",
        "Northwestern University",
        "UC Berkeley"
    ]
}