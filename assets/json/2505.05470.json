{
    "paper_title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Yangguang Li",
        "Jiaheng Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Wanli Ouyang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy improves from $59\\%$ to $92\\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 0 7 4 5 0 . 5 0 5 2 : r Flow-GRPO: Training Flow Matching Models via Online RL Jie Liu1,3,5 Gongye Liu2,3* Jiajun Liang3 Yangguang Li1 Jiaheng Liu4 Xintao Wang3 Pengfei Wan3 Di Zhang3 Wanli Ouyang1,5 1CUHK MMLab 3Kuaishou Technology 2Tsinghua University 4Nanjing University 5Shanghai AI Laboratory jieliu@link.cuhk.edu.hk Code: https://github.com/yifan123/flow_grpo"
        },
        {
            "title": "Abstract",
            "content": "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODEto-SDE conversion that transforms deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original models marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments."
        },
        {
            "title": "Introduction",
            "content": "Flow matching [2, 3] models have become dominant in image generation [4, 5] due to their solid theoretical foundations and strong performance in producing high-quality images. However, they often struggle with composing complex scenes involving multiple objects, attributes, and relationships [6, 7], as well as text rendering [8]. At the same time, online reinforcement learning (RL) [9] has proven highly effective in enhancing the reasoning capabilities of large language models (LLMs) [10, 11]. While previous research has mainly focused on applying RL to early diffusionbased generative models [12] and offline RL techniques like direct preference optimization [13] for flow-based generative models [14, 15], the potential of online RL in advancing flow matching generative models remains largely unexplored. In this study, we explore how online RL can be leveraged to effectively improve flow matching models. Training flow models with RL presents several critical challenges: (1) Flow models rely on deterministic generative process based on ordinary differential equations (ODEs) [3], meaning they cannot sample stochastically during inference. In contrast, RL relies on stochastic sampling to explore environments, where the agent learns by trying different actions and receiving feedback. This need for stochasticity in RL conflicts with the deterministic nature of flow matching models. (2) Online RL requires efficient sampling to gather training data, but flow models typically involve many iterative Equal contribution Figure 1: (a) GenEval performance rises steadily throughout Flow-GRPOs training and outperforms GPT-4o. (b) Image quality metrics on DrawBench [1] remain essentially unchanged. (c) Human Preference Scores on DrawBench improves after training. Results show that Flow-GRPO boosts the desired capability without degrading image quality or exhibiting reward-hacking behaviour. steps to generate each sample, significantly reducing sampling efficiency. This issue becomes even more critical with large, advanced models [5, 4], which are computationally intensive. Therefore, to make RL effective in tasks such as image or video generation, improving sampling efficiency within RL training is crucial. To address these challenges, we propose Flow-GRPO, which integrates GRPO [16] into flow matching models for text-to-image (T2I) generation, using two key strategies. First, we adopt the ODE-to-SDE strategy to overcome the deterministic nature of the original flow model. By converting the ODE-based flow into an equivalent Stochastic Differential Equation (SDE) framework, we introduce randomness while preserving the original marginal distributions. Second, to improve sampling efficiency in online RL, we apply the Denoising Reduction strategyreducing denoising steps during training while keeping the full schedule during inference. Our experiments show that using fewer steps maintains performance while significantly reducing data generation costs. We evaluate Flow-GRPO on T2I tasks with various reward types. (1) Verifiable rewards, using the GenEval [17] benchmark and visual text rendering task. GenEval includes compositional image generation tasks (e.g., generating specific object counts, colors, and spatial relationships), which can be automatically assessed with traditional detection methods. Flow-GRPO improves the accuracy of Stable Diffusion 3.5 Medium (SD3.5-M) [4] from 63% to 95% on GenEval, outperforming the state-of-the-art GPT-4o [18] model. For visual text rendering, SD3-Ms accuracy increases from 59% to 92%, greatly enhancing its text generation ability. (2) Model-based rewards, such as the human preference Pickscore [19] reward, show that our framework is task independent, demonstrating its generalizability and robustness. Importantly, all improvements are achieved with very little reward hacking, as demonstrated in Figure 1. To summarize, the contributions of Flow-GRPO are as follows: We are the first to introduce GRPO to flow matching models by converting deterministic ODE sampling into SDE sampling, showing that online RL is highly effective for T2I tasks. Specifically, our Flow-GRPO improves SD3.5-Medium accuracy from 63% to 95% without compromising image quality. We find that online RL for flow matching models can use fewer denoising steps for training sample generation, significantly accelerating training. Full denoising steps are still used during testing to maintain performance. We show that the Kullback-Leibler (KL) constraint is effective in preventing reward hacking, where reward increases but image quality or diversity decreases. We also emphasize that KL regularization is not empirically equivalent to early stopping. By adding an appropriate KL, we can achieve the same high reward as the KL-free version while maintaining image quality, though with longer training time."
        },
        {
            "title": "2 Related Work",
            "content": "RL for LLM. Online RL has proven effective in boosting the reasoning abilities of large language models (LLMs), notably DeepSeek-R1 [10] and OpenAI-o1 [11]. By iteratively alternating between exploration and exploitation, these systems generate increasingly detailed responses and steadily enhance overall performance. Most successful pipelines employ policy gradient algorithms such as Proximal Policy Optimization (PPO) [20] and the value-free Group Relative Policy Optimization (GRPO) [16]. Because GRPO dispenses with separate value network, it is more memory efficient; hence we adopt GRPO in this work. Nonetheless, PPO could be incorporated into flow matching model through an analogous procedure. Diffusion and Flow Matching. Diffusion-based [21, 22, 23] generative models create forward Markov chain that incrementally adds Gaussian noise to data and train neural network to approximate the reverse-time denoising vector field; integrating this learned field by discrete DDPM steps or probability flow SDE solvers can yields high fidelity samples. Flow matching [2, 3] instead treats generation as learning continuous-time normalizing flow whose vector field is trained by directly matching the velocity, enables deterministic sampling in few ODE steps. Thanks to its efficiency, flow matching attains competitive FID with orders-of-magnitude fewer denoising steps than diffusion samplers and has already become the dominant approach in current image generation [4, 5] and video generation [24, 25, 26, 27] models. [28, 29] unifies flow-based and diffusion-based methods using the SDE and ODE perspective. Our work builds on their theoretical foundations and introduce GRPO to flow-based models. Alignment for T2I. Recent work on aligning pretrained T2I models with human preferences has pursued several complementary directions. The main approaches are: (1) direct fine-tuning with scalar reward signals [30, 31, 32, 33]; (2) Reward-Weighted Regression (RWR) [34, 35, 36]; (3) Direct Preference Optimization (DPO) and related variants,[37, 38, 39, 14, 40, 41, 42, 43, 44, 36]; (4) Proximal Policy Optimization (PPO)-style policy gradients [45, 46, 47, 48, 49]; and (5) training-free alignment techniques [50, 51, 52]. These methods have successfully aligned T2I models with human preferences, improving aesthetics and semantic consistency. Building on this progress, we introduce GRPO for flow-matching modelsthe backbone of todays state-of-the-art T2I systems. Concurrent work [53] apply GRPO to text-to-speech flow models, but instead of converting the ODE to an SDE to inject stochasticity, they reformulate velocity prediction as estimating Gaussian distribution (predicting both the mean and variance of velocity), which necessitates retraining the pretrained model. [54] also recognize that stochasticity can be introduced from an SDE perspective, yet their work concentrates on inference-time scaling."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this section, we introduce the mathematical formulation of flow matching (Section 3.1) and describe how the denoising process can be mapped as multi-step MDP (Section 3.2). 3.1 Flow Matching Let x0 X0 denote data sample drawn from the true data distribution, and x1 X1 denote noise sample. Recent advanced image-generation models (e.g., [4, 5]) and video-generation models (e.g., [24, 26, 25, 27]) adopt the Rectified Flow [3] framework, which defines the noised data xt as xt = (1 t) x0 + x1, for [0, 1]. Then transformer model are trained to directly regress the velocity field vθ(xt, t) by minimizing the Flow Matching objective [2, 3]: (1) L(θ) = Et, x0X0, x1X1 (cid:2) vθ(xt, t)2(cid:3), (2) where the target velocity field is = x1 x0. 3 Figure 2: Overview of Flow-GRPO. Given prompt set, we introduce an ODE-to-SDE strategy to enable stochastic sampling for online RL. With Denoising Reduction (only = 10 steps), we efficiently gather low-quality but still informative trajectories. Rewards from these trajectories feed the GRPO loss, which updates the model online and yields an aligned policy. 3.2 Denoising as an MDP According to [12], the iterative denoising process in diffusion models can be framed as an Markov decision process (MDP) (S, A, ρ0, P, R). The state at time step is st (c, t, xt), the action is the denoised sample at xt1 produced by the network, and the policy is the conditional model π(at st) pθ(xt1 xt, c). The transition kernel is deterministic (st+1 st, at) (δc, δt1, δxt1) and the initial-state distribution is ρ0(s0) (p(c), δT , (0, I)), in which δy is the Dirac delta distribution with nonzero density only at y. The reward is non-zero only at the final step, R(st, at) r(x0, c) if = 0 and 0 otherwise."
        },
        {
            "title": "4 Flow-GRPO",
            "content": "In this section, we present the Flow-GRPO method, GRPO-inspired algorithm that improves flow models through online RL. We begin by revisiting the core concept of GRPO [16] and recasting it in the context of flow matching. Next, we discuss how to convert the deterministic probability-flow ODE sampler into reverse-time SDE with equivalent marginal distribution, thereby introducing stochasticity into the generation processan essential ingredient for applying GRPO or other RL techniques. Finally, we introduce Denoise Reduction (Section 4.3), practical data sampling method that dramatically accelerates training without degrading performance. 4.1 GRPO on Flow Matching Reinforcement learning aims to learn policy that maximizes expected cumulative reward under given environment. Typically the goal is to optimize policy πθ with regularized objective of the form: max θ E(s0,a0,...,sT ,aT )πθ (cid:34) (cid:88) t=0 (cid:32) R(st, at) βDKL(πθ( st)πref( st)) (cid:33)(cid:35) . (3) 4 Among various RL algorithms, GRPO [16] provides lightweight alternative to policy based methods such as PPO [20]. Unlike PPO [20], which requires training separate value functionnormally separate model of similar size to the policy network, GRPO introduces group relative formulation to estimate the advantage. In our setting, we recast the GRPO framework into the context of flow matching. Recall that the denoising process can be formulated as an MDP, as shown in Section 3.2. Given prompt c, the flow model pθ samples group of individual images {xi i=1 and the corresponding reverse-time trajectories {(xi i=1. Then, the advantage of the i-th image is calculated by normalizing the group-level rewards as follows: 1, , xi , xi 0)}G 0}G ˆAi = R(xi 0, c) mean({R(xi 0, c)}G std({R(xi 0, c)}G i=1) i=1) . GRPO employs clipped objective of Eq. 3, together with KL penalty term: JFlow-GRPO(θ) = cC,{xi}G i=1πθold (c) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i="
        },
        {
            "title": "1\nT",
            "content": "(cid:32) 1 (cid:88) (cid:16) min t=0 t(θ) ˆAi ri t, clip (cid:16) ri t(θ), 1 ε, 1 + ε (cid:17) (cid:17) ˆAi where 4.2 ODE-to-SDE ri t(θ) = pθ(xi pθold(xi t1 xi t1 xi t, c) t, c) . (4) (cid:33)(cid:35) βDKL(πθπref) , (5) (6) In Eq. 4 and Eq. 6, GRPO relies on stochastic sampling procedure in order to generate diverse batch of trajectories for advantage estimation and policy exploration. In the case of diffusion models, the forward process consists of sequence of random Gaussian noising steps, and its reverse-time sampler is implemented via Markov chain of Gaussians with progressively decreasing varianceequivalently, an approximate solver for the score-based SDE. Thus, diffusion sampling is naturally stochastic. While in the case of probability flow model, as previously discussed in Eq. 1, its forward process can be formulated as deterministic ODE: dxt = vtdt, (7) where vt is modeled via the flow matching objective in Eq. 2. common sampling method is to discretize this ODE (e.g., via the explicit Euler method or higher-order integrator) yielding one-to-one mapping between successive time steps. For example, with fixed step size t, the Euler scheme gives: xt1 = xt + tvθ(xt, t, c) (8) which produces single, deterministic trajectory {xT , xT 1, , x0} given any initial xT and prompt c. This deterministic approach fails to meet GRPOs policy update requirements in two key ways: (1) Eq. 6 requires computing p(xt1 xt, c), but deterministic sampling cannot provide probabilities. This limitation affects not only GRPO but all policy gradient methods, including the basic online REINFORCE [55] policy gradient, which also require probability computation. (2) Exploration is crucial for RL performance. As shown in Section 5.3, reduced randomness significantly lowers training efficiency. Deterministic sampling, which lacks randomness beyond the initial seed, is especially problematic. To address this limitation, we convert the deterministic Flow-ODE from Eq. 7 into an equivalent SDE that matches the original models marginal probability density function at all timesteps. We outline the key process here. detailed proof is provided in Appendix A. Following [23, 28, 29], we construct reverse-SDE formulation that preserves the marginal distribution: (cid:18) dxt = vt(xt) σ2 2 (cid:19) log pt(xt) dt + σtdw, (9) 5 where dw denotes Wiener process increments and σt control the level of stachasticity during generation. For rectified flow, the marginal score has relation with velocity: log pt(x) = 1 vt(x). Substituting Eq. 10 into Eq. 9 gives the final SDE: (cid:20) vt(xt) + dxt = σ2 2t (xt + (1 t)vt(xt)) dt + σtdw. (cid:21) Applying Euler-Maruyama discretization yields the final update rule: xt+t = xt + (cid:20) vθ(xt, t) + σ2 2t (cid:0)xt + (1 t)vθ(xt, t)(cid:1) (cid:21) + σt ϵ (10) (11) (12) where ϵ (0, I) injects stochasticity. We use σt = hyper-parameter that controls the noise level (See Section 5.3 for its impact on performance). 1t in this paper, where is scalar (cid:113) Eq. 12 reveals that the policy πθ(xt1 xt, c) is an isotropic Gaussian distribution. We can easily compute the KL divergence between πθ and the reference policy πref as closed form: DKL(πθπref) = xt+t,θ xt+t,ref2 2σ2 = 2 (cid:18) σt(1 t) 2t + 1 σt (cid:19)2 vθ(xt, t) vref(xt, t)2 (13) 4.3 Denoising Reduction To produce high-quality images, flow models typically require many denoising steps, making data collection costly for online RL. However, we find that large timesteps are unnecessary during online RL training. We can use significantly fewer denoising steps during sample generation, while retaining the original denoising steps during inference to get high-quality samples. Note that we set the timestep as 10 in training, while the inference timestep is set as the original default setting (T = 40) for SD3.5-M. Our experiments reveals that this approach enables fast training without sacrificing image quality at test time."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we empirically evaluate Flow-GRPOs ability to improving flow matching models on three representative tasks. (1) Composition Image Generation: This task requires precise object arrangement and attribute control. We report the results on GenEval. (2) Visual Text Rendering: rule-based task that stresses the accurate rendering of the text specified in the prompt. (3) Human Preference Alignment: This task aims to align T2I models with human preferences. We use PickScore [19] as our reward model. 5.1 Experimental Setup We introduce three tasks, detailing their respective prompts and reward definitions. For hyperparameter details and compute resource specifications, please refer to Appendix B.2 and Appendix B.2.1, respectively. Compositional Image Generation GenEval [17] assesses text-to-image models on complex compositional promptslike object counting, spatial relations, and attribute bindingacross six increasingly difficult compositional image generation tasks. We use its official evaluation pipeline, which detects object bounding boxes and colors, then infers their spatial relations. Training prompts are generated using scripts from GenEval2. The test set is strictly deduplicated: prompts differing only in object order (e.g., \"a photo of and B\" vs. \"a photo of and A\") are treated as identical, 2https://github.com/djghosh13/geneval/blob/main/prompts/create_prompts.py and these variants are removed from the training set. Based on the base models initial accuracy across the six tasks, we set the prompt ratio as Position : Counting : Attribute Binding : Colors : Two Objects : Single Object = 7 : 5 : 3 : 1 : 1 : 0. Rewards are rule-based and bounded in the range [0, 1]: Counting: = Ngen Nref/Nref. Position / Color: If the object count is correct, partial reward is assigned; the remainder is granted when the predicted position or color is also correct. Table 1: GenEval Result. Best scores are in blue , second-best in green . Results for models other than SD3.5-M are from [7] or their original papers. Obj.: Object; Attr.: Attribution. Model Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding LDM [56] SD1.5 [56] SD2.1 [56] SD-XL [57] DALLE-2 [58] DALLE-3 [58] Show-o [59] Emu3-Gen [60] JanusFlow [61] Janus-Pro-7B [62] GPT-4o [18] FLUX.1 Dev [5] SD3.5-L [4] SANA-1.5 4.8B [63] SD3.5-M [4] SD3.5-M+Flow-GRPO 0.37 0.43 0.50 0.55 0.52 0. 0.53 0.54 0.63 0.80 0.84 0.66 0.71 0.81 0.63 0.95 Diffusion Models 0.92 0.97 0.98 0.98 0.94 0.96 0.29 0.38 0.51 0.74 0.66 0. Autoregressive Models 0.95 0.98 0.97 0.99 0.99 0.52 0.71 0.59 0.89 0.92 0.23 0.35 0.44 0.39 0.49 0.47 0.49 0.34 0.45 0.59 0.85 Flow Matching Models 0.98 0.98 0.99 0.98 1.00 0.81 0.89 0.93 0.78 0.99 0.74 0.73 0.86 0.50 0. 0.70 0.76 0.85 0.85 0.77 0.83 0.82 0.81 0.83 0.90 0.92 0.79 0.83 0.84 0.81 0.92 0.02 0.04 0.07 0.15 0.10 0.43 0.11 0.17 0.53 0.79 0. 0.22 0.34 0.59 0.24 0.99 0.05 0.06 0.17 0.23 0.19 0.45 0.28 0.21 0.42 0.66 0.61 0.45 0.47 0.65 0.52 0. Figure 3: Qualitative Comparison on the GenEval Benchmark. Our approach demonstrates superior performance in Counting, Colors, Attribute Binding, and Position. 7 Table 2: Performance on Compositional Image Generation, Visual Text Rendering, and Human Preference benchmarks, evaluated by task performance, image quality, and preference scores. Acc: Accuracy; ImgRwd: ImageReward; UniRwd: UnifiedReward. Model Task Metric Image Quality Preference Score GenEval OCR Acc. PickScore Aesthetic DeQA ImgRwd PickScore UniRwd SD3.5-M 0.63 0.59 21. 5.39 4.07 0.87 22.34 3.33 Flow-GRPO (w/o KL) Flow-GRPO (w/ KL) 0.95 0.95 Flow-GRPO (w/o KL) Flow-GRPO (w/ KL) Flow-GRPO (w/o KL) Flow-GRPO (w/ KL) Compositional Image Generation 0.93 0.92 4.93 5.25 Visual Text Rendering 5.13 5.32 Human Preference Alignment 23.41 23.31 6.15 5.92 2.77 4. 3.66 4.06 4.16 4.22 0.44 1.03 0.58 0.95 1.24 1.28 21.16 22. 21.79 22.44 2.94 3.51 3.15 3.42 3.57 3.66 Visual Text Rendering [8] Text is common in images such as posters, book covers, and memes, so the ability to place accurate and coherent text inside the generated images is crucial for T2V models. In our settings, we define an OCR task, where each prompt follows the template sign that says text. Specifically, the placeholder text is the exact string that should appear in the image. We use GPT to produce 20K training prompts and 1K test prompts. Following [64], we measure text fidelity with the reward = Ngen Nref Nref , where Ngen is the number of characters detected in the generated image and Nref is the number of characters inside the quotation marks in the prompt. This reward also serves as our measure of text accuracy. Human Preference Alignment [19] This task aims to align T2I models with human preferences. We use PickScore 3 as our reward model, which is based on large-scale human annotated pairwise comparisons of images generated from the same prompt. For each image and prompt pair, PickScore provides an overall score that evaluates multiple criteria, such as the alignment of the image with the prompt and its visual quality. Image Quality Evaluation Metric Because the T2I model is trained to maximise predefined reward signal in RL, it is susceptible to reward hackingsituations in which the numerical reward rises but the quality or diversity of the generated images declines. This study aims to make online RL truly effective for T2I generation without compromising quality or diversity. To detect potential reward hacking beyond task-specific accuracy, we evaluate four automatic image quality metrics: Aesthetic score [65]: CLIP-based linear regressor that predicts an images aesthetic score on 110 scale. DeQA score [66]: multimodal large language model based image-quality assessment (IQA) model that quantifies how distortions, texture damage, and other low-level artefacts affect perceived quality. ImageReward [32]: general purpose T2I human preference reward model that captures textimage alignment, visual fidelity, and harmlessness. UnifiedReward [67]: recently proposed unified reward model for multimodal understanding and generation that currently achieves state-of-the-art performance on the human preference assessment leaderboard. All metrics are evaluated on DrawBench [1], comprehensive and challenging benchmark with diverse prompts for T2I models. 3https://huggingface.co/yuvalkirstain/PickScore_v1 8 5.2 Main Results Figure 1 and Table 1 show Flow-GRPOs GenEval performance steadily improving during training, ultimately outperforming GPT-4o. This occurs while maintaining both image quality metrics and preference scores on DrawBench, benchmark with diverse and comprehensive prompts for evaluating general model capabilities. Figure 3 offers qualitative comparisons. Beyond Compositional Image Generation, Table 2 details evaluations on Visual Text Rendering and Human Preference tasks. Flow-GRPO improved text rendering ability, again without decreasing image quality metrics and preference scores on DrawBench. For the Human Preference task, image quality did not decrease with or without KL regularization. However, we found that omitting KL caused collapse in visual diversitya form of reward hacking discussed further in Section 5.3. These results demonstrate that Flow-GRPO boosts desired capabilities without degrading image quality or visual diversity. 5.3 Analysis Reward Hacking We explored two approaches to mitigate reward hacking: (1) Multi-reward ensemble: We combined preference-based rewards (e.g., UnifiedReward, PickScore, ImageReward), image quality rewards (e.g., Aesthetic Score, DeQA) with task reward. However, this also led to local blurriness and reduced diversity. Due to the challenge of covering all aspects of generation with diverse reward models, we abandoned this approach. (2) KL constraint: We tuned the KL coefficient to maintain small, constant KL divergence during training, keeping the model close to its pretrained weight. This allows for task-specific reward optimization without harming general performance. As shown in Table 2, removing the KL constraint boosts performance on Compositional Image Generation and Visual Text Rendering but causes sharp decline in image quality and preference scores on DrawBench. In contrast, properly tuned KL constraint preserves quality while maintaining similar gains on task-specific metrics. In the Human Preference Alignment task, dropping the KL constraint doesnt hurt image qualitylikely due to PickScore overlap with evaluation metricsbut causes collapse in visual diversity. Outputs converge to single style, with different seeds producing nearly identical results. KL regularization prevents this collapse and preserves diversity. See Figure 7 in the Appendix C.2 for learning curves with and without KL, and Figure 4 for examples. Figure 4: Effect of KL Regularization. The KL penalty effectively suppresses reward hacking, preventing potential Quality Degradation (for GenEval and OCR) and Diversity Decline (for PickScore). Effect of Denoising Reduction Figure 5 (a) highlights Denoising Reductions significant impact on accelerating training. To explore how different timesteps affect optimization, these experiments were conducted without the KL constraint. Reducing data collection timesteps from 40 to 10 achieved over 4 speedup across all three tasks, without impacting final reward. While further tests showed this reduction didnt consistently improve speed (and occasionally slowed some tasks), we standardized on 10 timesteps for subsequent experiments, prioritizing the maintained reward. For the other two tasks, learning curves of reward versus training time are presented in Figure 6 in the Appendix C.1. 9 Effect of Noise Level Higher σt in the SDE boosts image diversity and exploration, vital for RL training. We control this exploration with noise level (Eq. 12). Ablation studies (Figure 5 (b)) show as impact on performance. Small (e.g., 0.1) limits exploration and slows reward improvement. Increasing (up to 0.7) enhances exploration and accelerates reward gains. Beyond this, further increases (e.g., = 0.7 1.0) yield no extra benefit, as exploration is sufficient. However, too much noise degrades image quality, leading to zero reward and failed training. We advise using the highest noise level that maintains good image quality. (a). Effect of Denoising Reduction on GenEval (b). Effect of Noise Level Ablation on the OCR Figure 5: Ablation studies on our critical design choices. (a) Denoising Reduction: Fewer denoising steps accelerate convergence and yield similar performance. (b) Noise Level: Moderate noise level (a = 0.7) maximises OCR accuracy, while too little noise hampers exploration. Generalization Analysis Flow-GRPO demonstrates strong generalization on unseen scenarios from GenEval  (Table 4)  . Specifically, it captures object number, color, and spatial relations, generalizing well to unseen object classes. It also effectively controls object count, generalizing from training on 2 4 objects to generate 5 6 objects. Furthermore, Table 3 shows Flow-GRPO achieves significant gains on T2I-CompBench++. This comprehensive benchmark for open-world compositional T2I generation features object classes and relationships substantially different from our models GenEvalstyle training data. Table 3: T2I-CompBench++ Result. This evaluation uses the same model presented in Table 1, which was trained on the GenEval-generated dataset. The best score is in blue . Model Color Shape Texture 2D-Spatial 3D-Spatial Numeracy Non-Spatial Janus-Pro-7B [62] EMU3 [60] FLUX.1 Dev [5] SD3.5-M SD3.5-M+Flow-GRPO 0.5145 0.7913 0.7407 0.7994 0. 0.3323 0.5846 0.5718 0.5669 0.6130 0.4069 0.7422 0.6922 0.7338 0.7236 0.1566 0.2863 0.2850 0.5447 0.2753 0.3866 0.3739 0.4471 0.4406 0.6185 0.5927 0.6752 0.3137 0.3127 0.3146 0. Table 4: Flow-GRPO demonstrates strong generalization. Unseen Objects: Trained on 60 object classes, evaluated on 20 unseen classes. Unseen Counting: Trained to render 2, 3, or 4 objects, was evaluated on rendering 5 or 6. Method Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Counting Unseen Objects Unseen Counting SD3.5-M SD3.5-M+Flow-GRPO 0.64 0.90 0.96 1.00 0.73 0.94 0.53 0. 0.87 0.97 0.26 0.84 0.47 0.77 0.13 0.48 Limitations & Future Work. While this work focuses on T2I tasks, Flow-GRPOs potential applicability to video generation [25, 27] raises several questions for future research: (1) Reward Design for Video: Defining effective reward models for video generation is crucial. Simple heuristics like object detectors or trackers can promote physical realism and temporal consistency, but more sophisticated models are needed. (2) Balancing Multiple Rewards: Effective video generation requires optimizing multiple objectives (e.g., realism, smoothness, coherence). Balancing these often conflicting signals is challenging and demands careful tuning. (3) Scalability: Video generation is significantly more resource-intensive than T2I. Scaling Flow-GRPO for video thus demands more efficient data collection and training pipelines."
        },
        {
            "title": "References",
            "content": "[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [2] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [3] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [5] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [6] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [7] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [8] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36:9353 9387, 2023. [9] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [12] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [13] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [14] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. [15] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [16] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [17] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 12 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. [20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [22] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [23] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [24] Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024. [25] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [26] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [28] Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. [29] Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. [30] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning textto-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. [31] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. [32] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. [33] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. [34] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. [35] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. 13 [36] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. [37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [38] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [39] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. [40] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89418951, 2024. [41] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2024. [42] Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. arXiv preprint arXiv:2402.10210, 2024. [43] Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. [44] Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024. [45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [46] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [47] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [48] Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie Oosterhuis, Maarten de Rijke, and Satya Narayan Shukla. simple and effective reinforcement learning method for text-to-image diffusion fine-tuning. arXiv preprint arXiv:2503.00897, 2025. [49] Zichen Miao, Jiang Wang, Ze Wang, Zhengyuan Yang, Lijuan Wang, Qiang Qiu, and Zicheng Liu. Training diffusion models towards diverse image generation with reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1084410853, 2024. [50] Po-Hung Yeh, Kuang-Huei Lee, and Jun-Cheng Chen. Training-free diffusion model alignment with sampling demons. arXiv preprint arXiv:2410.05760, 2024. [51] Zhiwei Tang, Jiangweizhi Peng, Jiasheng Tang, Mingyi Hong, Fan Wang, and Tsung-Hui Chang. Tuning-free alignment of diffusion models with direct noise optimization. arXiv preprint arXiv:2405.18881, 2024. 14 [52] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 3248332498. PMLR, 2023. [53] Xiaohui Sun, Ruitong Xiao, Jianye Mo, Bowen Wu, Qun Yu, and Baoxun Wang. F5r-tts: Improving flow matching based text-to-speech with group relative policy optimization. arXiv preprint arXiv:2504.02407, 2025. [54] Jaihoon Kim, Taehoon Yoon, Jisung Hwang, and Minhyuk Sung. Inference-time scaling for flow models via stochastic generation and rollover budget forcing. arXiv preprint arXiv:2503.19385, 2025. [55] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [57] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [58] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [59] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [60] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [61] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [62] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [63] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [64] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. [65] Chrisoph Schuhmann. Laion aesthetics, Aug 2022. [66] Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language models to regress accurate image quality scores using score distribution. arXiv preprint arXiv:2501.11561, 2025. [67] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. [68] Bernt Øksendal and Bernt Øksendal. Stochastic differential equations. Springer, 2003."
        },
        {
            "title": "A Mathematical Derivations for Stochastic Sampling using Flow Models",
            "content": "We present detailed proof here. To compute pθ(xt1 xt, c) in Equation 6 during forward sampling, we adapt flow models to stochastic differential equation (SDE). While flow models normally follow deterministic ODE: dxt = vtdt (14) We consider its stochastic counterpart. Inspired by the derivation from SDE to its probability flow ODE in SGMs [23], we aim to construct SDE with specific drift and diffusion coefficients so that its marginal distribution matches that of Eq. 14. We begin with the generic form of SDE: Its marginal probability density pt(x) evolves according to the FokkerPlanck equation [68], i.e., dxt = fSDE(xt, t)dt + σtdw, (15) tpt(x) = [fSDE(xt, t)pt(x)] + 1 2 2[σ2 pt(x)] Similarly, the marginal probability density associated with Eq. 14 evolves: tpt(x) = [vt(xt, t)pt(x)] (16) (17) To ensure that the stochastic process shares the same marginal distribution as the ODE, we impose: Observing that [fSDEpt(x)] + 1 2 2[σ2 pt(x)] = [vtpt(x)] 2[σ2 pt(x)] = σ2 = σ2 = σ2 2pt(x) (pt(x)) (pt(x) log pt(x)) Substituting Eq. 19 to Eq. 18, we arrive at the drift coefficients of the target SDE: Hence we can rewrite the reverse-time SDE in Eq. 15 as: fSDE = vt 1 2 σ2 log pt(x) (cid:18) dxt = vt(xt) σ2 2 (cid:19) log pt(xt) dt + σtdw, (18) (19) (20) (21) where dw denotes Wiener process increments and σt is the diffusion coefficient that controls the level of stachasticity during sampling. Once the score log pt(xt) is available, the stochastic process can be sampled directly. While the Flow-Matching framework, this score is tied to the velocity field vt. Specifically, let the data distribution. αt αt/t. All expectations are over x0 X0 and x1 (0, I), where X0 is For the linear interpolation xt = αtx0 + βtx1, we have: pt0(xtx0) = (cid:0)xt αtx0, β2 I(cid:1) , yielding the conditional score: log pt0(xtx0) = xt αtx0 β2 = x1 βt . The marginal score becomes: log pt(xt) = (cid:2) log pt0(xtx0) xt 1 βt E[x1 xt]. = (cid:3) 16 (22) (23) (24) (25) (26) (27) (28) (29) For the velocity field vt(xt), we derive: vt(x) = (cid:104) αtx0 + βtx1 xt = (cid:105) = αtE[x0 xt = x] + βtE[x1 xt = x] (cid:20) xt βtx1 αt αtβt αt (cid:18) βtβt = αtE = = αt αt αt αt (cid:21) xt = + βtE[x1 xt = x] E[x1 xt = x] + βtE[x1 xt = x] (cid:19) αtβ2 αt log pt(x), Substituting αt = 1 and βt = simplifies Equation 25 to: Solving for the score yields: vt(x) = 1 1 log pt(x). log pt(x) = 1 vt(x). Substituting Equation 27 into 21 gives the final SDE: (cid:20) vt(xt) + dxt = σ2 2t (xt + (1 t)vt(xt)) dt + σtdw. (cid:21) Applying Euler-Maruyama discretization yields the update rule: xt+t = xt + (cid:20) vθ(xt, t) + σ2 2t (cid:0)xt + (1 t)vθ(xt, t)(cid:1) (cid:21) + σt ϵ, where ϵ (0, I) injects stochasticity."
        },
        {
            "title": "B Further Details on the Experimental Setup",
            "content": "B.1 Model Specification The following table lists the base model and the reward models and their corresponding links. Models Links SD3.5-M [4] Aesthetic Score [65] DeQA score [66] ImageReward [32] UnifiedReward [67] https://huggingface.co/stabilityai/stable-diffusion-3.5-medium https://github.com/LAION-AI/aesthetic-predictor https://huggingface.co/zhiyuanyou/DeQA-Score-Mix3 https://huggingface.co/THUDM/ImageReward https://huggingface.co/CodeGoat24/UnifiedReward-7b-v1. B.2 Hyperparameters Specification Except for β, GRPO hyperparameters are fixed across tasks. We use sampling timestep = 10 and an evaluation timestep = 40. Other settings include group size = 24, an noise level = 0.7 and an image resolution of 512. The KL ratio β is set to 0.004 for GenEval and Text Rendering, and 0.001 for Pickscore. We use Lora with α = 64 and = 32. B.2.1 Compute Resources Specification We train our model using 24 NVIDIA A800 GPUs. The learning curves in Appendix C.2 provide details on the specific GPU hours."
        },
        {
            "title": "C Extended Experimental Results",
            "content": "C.1 Effect of Denoising Reduction We show the extended Denoising Reduction ablations of Visual Text Rendering and Human Preference Alignment tasks in Figure 6. (a). Visual Text Rendering (b). Human Preference Alignment Figure 6: Effect of Denoising Reduction C.2 Learning Curves with or without KL Figure 7 shows learning curves for three tasks, with and without KL. These results emphasize that KL regularization is not empirically equivalent to early stopping. Adding appropriate KL can achieve the same high reward as the KL-free version and maintain image quality, though it requires longer training. 18 (a). Compositional Image Generation (b). Visual Text Rendering Figure 7: Learning Curves with and without KL. KL penalty slows early training yet effectively suppresses reward hacking. (c). Human Preference Alignment"
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "Kuaishou Technology",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Tsinghua University"
    ]
}