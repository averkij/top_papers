{
    "paper_title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models",
    "authors": [
        "Xiaoxuan He",
        "Siming Fu",
        "Yuke Zhao",
        "Wanli Li",
        "Jian Yang",
        "Dacheng Yin",
        "Fengyun Rao",
        "Bo Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce \\textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 4 2 3 4 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Work in progress",
            "content": "TEMPFLOW-GRPO: WHEN TIMING MATTERS FOR GRPO IN FLOW MODELS Xiaoxuan He1,2, Siming Fu1, Yuke Zhao1, Wanli Li1, Jian Yang2, Dacheng Yin2, Fengyun Rao2, Bo Zhang1 1 ZheJiang University, 2 WeChat Vision, Tencent Inc"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlowGRPO (Temporal Flow GRPO), principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlowGRPO introduces two key innovations: (i) trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to stateof-the-art performance in human preference alignment and standard text-to-image benchmarks."
        },
        {
            "title": "INTRODUCTION",
            "content": "While text-to-image diffusion models have achieved unprecedented visual quality and semantic control Esser et al. (2024); Xie et al. (2024a); Labs et al. (2025), aligning their outputs with human preference remains formidable challenge. Reinforcement learning has emerged as promising solution, giving rise to the field of Diffusion RL Wallace et al. (2024); Black et al. (2023); Fan et al. (2023). However, the performance of these methods remains suboptimal, hindered by two ignoring the temporal dynamics of fundamental limitations that have been largely overlooked: generation and lacking intermediate feedback signals. These approaches apply uniform optimization across all timesteps and provide rewards only at completion, missing the varying importance of decisions throughout the generation process. The majority of existing approaches Gu et al. (2024); Hong et al. (2024), including recent works like Flow-GRPO Liu et al. (2025) and DanceGRPO Xue et al. (2025), treat the multi-step generation process as black box with temporally agnostic optimization. They apply uniform updates across all timesteps despite the fact that each timestep operates under different noise conditions and contributes differently to final image quality. Specifically, we plot the Fig. 1 (left) with applying SDE at only one timestep in the entire ODE trajectory, which ensures that any deviation in the final reward can be attributed to the stochastic exploration introduced at that specific step. As shown in Equal Contribution. Project Leader. Corresponding authors."
        },
        {
            "title": "Work in progress",
            "content": "Figure 1: (Left) Reward Variance Analysis: We plot the standard deviation of PickScore at each denoising step for 200 prompts, per prompt group size is 24. The results, obtained via applying SDE at only one step, reveal that reward variance is highest in the initial steps, indicating that earlystage interventions are most impactful for exploration. (Right) Method Illustration: By branching stochastic (SDE) exploration from specific, known state on deterministic (ODE) trajectory, we create controlled experiment. The resulting difference in the final reward can be unambiguously attributed to the exploration action taken at that precise branching point. Fig. 1 (left), the std of the reward varies dramatically between timesteps, reaching peak during early structural decisions (steps 0-2) and approaching zero during final refinements (steps 6-8). Yet Flow-GRPO maintains uniform treatment throughout, squandering high-impact exploration opportunities while focusing on later steps. Alternative approaches like SPO Liang et al. (2025) attempt to address temporal dynamics through process reward models, but training such models on semantically ambiguous intermediate states is notoriously difficult. This raises fundamental question: how can we effectively achieve precise credit assignment for intermediate actions while adapting optimization intensity to each timesteps exploration capacity? We address these limitations with TempFlow-GRPO, temporally-aware RL framework built on two key insights. First, we define the visualization method in the left of Fig. 1 as trajectory branching, which enables precise credit assignment by strategically introducing stochasticity at individual timesteps while maintaining deterministic evolution elsewhere (Fig. 1, Right). This provides provable guarantees: (1) reward variance localizes to the branching point, (2) improvements are directly attributable to specific exploration outcomes, and (3) existing reward models require no modification. Second, noise-aware policy weighting modulates optimization intensity based on each timesteps intrinsic noise level. Early high-noise stages receive larger weight updates to encourage structural exploration, while late low-noise stages receive gentler updates to preserve learned features. Together, these mechanisms create framework that is conceptually simple, computationally efficient, and seamlessly integrates into existing flow matching architecturesall while respecting the temporal dynamics that uniform approaches ignore. Our main contributions are threefold: We pinpoint temporal uniformitythe equal treatment of all timestepsas the primary limitation of flow-based GRPO. Our proposed TempFlow-GRPO overcomes this by introducing two key innovations: precise credit assignment to intermediate actions and noiseaware adaptation of optimization intensity. We introduce trajectory branching and noise-aware weighting to learn temporallystructured policies that respect generative dynamics. We demonstrate state-of-the-art performance on standard text-to-image benchmarks, achieving superior sample quality, human preference alignment, and compositional image generation compared to existing flow-based RL methods."
        },
        {
            "title": "Work in progress",
            "content": "Figure 2: (Left) Performance comparison on the PickScore benchmark. To ensure fair and robust evaluation, we first present Flow-GRPO (Fixed), an improved baseline where we replace the original global std stabilization with more appropriate group-wise std. On top of this stronger baseline, our core innovation, trajectory branching (+ Traj. Branch), delivers significant performance leap. Our full model, TempFlow-GRPO, integrates all components to achieve the highest performance. (Right) TempFlow-GRPOs dominance is further confirmed on the Geneval benchmark, where it substantially surpasses not only the Flow-GRPO baseline but also leading state-of-the-art models, including GPT-4o, SD3.5-M, and FLUX."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Alignment for Diffusion Models. Alignment for diffusion models has become rapidly emerging topic in generative modeling research. D3PO Yang et al. (2024) introduce the Direct Preference for Denoising Diffusion Policy Optimization method to directly fine-tune diffusion models. DiffusionDPO Wallace et al. (2024) is adapted from the Direct Preference Optimization (DPO Rafailov et al. (2023)), simpler alternative to RLHF which directly optimizes policy that best satisfies human preferences under classification objective. DyMO Xie & Gong (2025) propose plug-and-play training-free alignment method for aligning the generated images and human preferences during inference. Recently, Flow-GRPO Liu et al. (2025), the first method to integrate online reinforcement learning (RL) into flow matching models. However, Flow-GRPO applies uniform optimization pressure across all timesteps and suffers from sparse terminal reward problems, failing to account for the time-varying exploration potential inherent in the stochastic diffusion process. Our TempFlowGRPO addresses these limitations through trajectory branching for precise credit assignment and noise-aware policy weighting that aligns optimization pressure with the natural exploration capacity at each timestep. Process Reward. Recent studies have demonstrated that shaping the reward process, rather than solely relying on sparse terminal rewards, can significantly accelerate learning and improve policy performance. Zhang et al. (2025) introduces more comprehensive evaluation framework, which combines response-level and step-level metrics. ThinkPRM Khalifa et al. (2025) builds dataefficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating verification chain-of-thought (CoT). In diffusion models, SPO Liang et al. (2024) trains separate step-aware preference model that can be applied to both noisy and clean images. Despite this, PRMs require step-level supervision, making them expensive to train. There are several methods using outcome reward to replace process reward. PRIME Cui et al. (2025) enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. MathShepherd Wang et al. (2023) assigns reward score to each step of math problem solutions. Due to the significant challenges in scoring noisy images, there is an urgent need for an algorithm that enables process reward on flow models. Our TempFlow-GRPO elegantly circumvents the need for specialized process reward models by directly attributing outcome-based reward signals to interme-"
        },
        {
            "title": "Work in progress",
            "content": "diate exploratory actions, enabling precise credit assignment without the computational overhead of training step-level evaluators for semantically ambiguous noisy states."
        },
        {
            "title": "3 PRELIMINARY: FLOW-GRPO",
            "content": "Flow-GRPO enhances flow models using online RL. First, we revisit the core idea of GRPO. Then, we show how Flow-GRPO converts the deterministic ODE sampler into SDE sampler with same marginal distribution, which introduces the stochasticity needed for applying GRPO. GRPO. RL aims to learn policy that maximizes the expected cumulative reward. GRPO optimizes the policy model by maximizing the following objective: JFlow-GRPO(θ) = cC,{xi}G i=1πθold (c)f (r, ˆA, θ, ϵ, β) (1) where (r, ˆA, θ, ϵ, β) = ri t(θ) = 1 (cid:88) 1 i=1 pθ(xi pθold(xi t=0 t1xi t1xi t, c) t, c) 1 (cid:88) (min(ri t(θ) ˆAi t, clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi t) βDKL(πθπref)) , is the timestep. (2) Given prompt c, the flow model pθ samples group of individual images {xi corresponding reverse-time trajectories {(xi image is calculated by normalizing the group-level rewards as follows: i=1 and the i=1. Then, the advantage of the i-th 1, ..., xi , xi 0}G 0}G ˆAi = R(xi 0, c) mean({R(xi 0, c)}G std({R(xi 0, c)}G i=1) i=1) (3) Convert ODE to SDE. GRPO relies on stochastic sampling to generate diverse trajectories for advantage estimation and exploration. However, flow matching models use deterministic ODE for the forward process: Flow-GRPO converts the deterministic ODE into an equivalent SDE that matches the original models marginal probability density function at all timesteps. The final update rule is as follows: dxt = vtdt (4) xt+t = xt + [vθ(xt, t) + (xt + (1 t)vθ(xt, t))]t + σt tϵ (5) σ2 2t where ϵ (0, I) injects stochasticity and σt = the reference policy πref is closed form: (cid:113) 1t . And the KL divergence between πθ and DKL(πθπref) = xt+t,θ xt+t,ref 2σ2 = 2 ( σt(1 t) 2t + 1 σt )2vθ(xt, t) vref(xt, t)2 (6)"
        },
        {
            "title": "4 METHODS",
            "content": "4.1 TEMPORAL FLOW-GRPO While Flow-GRPO represents significant advancement in applying online RL to flow matching models, its direct adoption of the standard GRPO framework overlooks the unique, time-dependent"
        },
        {
            "title": "Work in progress",
            "content": "Figure 3: Overview of TempFlow-GRPO Framework. Our method performs trajectory branching by switching from ODE to SDE sampling at selected timesteps (t=k, j, i), injecting noise σt tϵ to create exploratory branches. Each branch generates distinct outcome with reward Ri, enabling precise credit assignment. The framework applies noise-aware weighting where ωi > ωj > ωk, prioritizing optimization at high-noise early stages (larger circles) over low-noise refinement phases (smaller circles), aligning learning intensity with each timesteps intrinsic exploration capacity. We visualize the models learning process as an astronaut exploring unknown planets: in early stages , the model explores vast possibility spaces with high uncertainty, while later stages involve focused navigation toward the final destination. dynamics inherent in the generative process. We identify two fundamental limitations that motivate our approach: Sparse Terminal Reward Problem: The current approach relies on sparse terminal rewards, assigning the same credit uniformly across all timesteps. This fails to differentiate the critical, highimpact decisions of early generation from the fine-tuning adjustments of later stages. Uniform Optimization Weighting: The GRPO objective applies uniform optimization among all steps, ignoring that the underlying SDE sampler possesses non-uniform noise. The potential for meaningful exploration is greater in high-noise early steps than in the low-noise refinement phase Xie & Gong (2025). To address these limitations, as illustrated in Fig. 3, we propose TempFlow-GRPO, unified framework that introduces two synergistic innovations: process rewards via trajectory branching and noise-aware policy weighting. 4.1.1 TRAJECTORY BRANCHING FOR PROCESS REWARDS Traditional process reward methods require specialized reward models to evaluate noisy intermediate states xt, which is exceptionally difficult due to the semantic ambiguity of partially-denoised representations. We propose an elegant alternative that leverages the deterministic-stochastic sampling methods of flow matching models. Key Insight: Instead of training complex process reward models, we use existing high-quality outcome-based reward models and directly attribute their scores to intermediate exploratory actions through novel trajectory branching mechanism. Consider generative process parameterized by θ. In flow matching models, the policy gradient can be written as: θJ (θ) = 1 (cid:88) k=0 ExT (0,I),ϵN (0,I)[θ log pθ(xk1xk) ˆAk] (7) Definition 1 (Trajectory Branching): We define trajectory branching operation where trajectory evolves deterministically at designated branching timestep k, where xk is sampled from initial noise xT using Eq. 4. At this branching point, stochasticity is introduced via noise variable ϵ in Eq. 5, yielding xk1 = SDE(xk, ϵ). The remainder of the trajectory xk2, xk3, . . . , x0 is generated deterministically as x0 = ODE(xk1). Theorem 1 (Credit Localization): Since all stochasticity and model controllability are concentrated at the branching point, the total reward variance and all parameter-dependent improvements are entirely attributable to the outcome of noise injection at k. This enables rigorous and efficient credit assignment localized to the branching point."
        },
        {
            "title": "Work in progress",
            "content": "Figure 4: (Left) comparative analysis between the reward standard deviation (Reward Std) and the noise level over the generative steps. The two curves show strong correlation. (Right) Scale terms reveal mismatch in standard GRPO: Scale terms are inversely proportional to noise level, causing low-noise refinement steps to dominate optimization despite having minimal impact on image content. Early steps that establish global structure receive weak gradients, while late steps that only adjust local details produce strong gradients. Our noise-aware reweighting compensates for this inverse relationship, ensuring that optimization intensity aligns with each timesteps actual capacity to influence the final image. k, ϵi)), c), In practice, we replace the reward for the k-th step from R(xi where xi k, ϵi) is sampled with ODE-SDE-ODE. Trajectory branching allows for the precise attribution of the terminal reward to step k, effectively creating temporally-aware reward signal. 0 is sampled with SDE and ODE(SDE(xi 0, c) to R(ODE(SDE(xi 4.1.2 NOISE-AWARE POLICY WEIGHTING While trajectory branching provides precise reward signals for individual timesteps, the generative process consists of sequence of potential branching points with fundamentally different characteristics. The SDE sampler exhibits time-varying stochasticity: the noise injection magnitude σt is large during initial generation stages and diminishes to near zero during final refinement stages. This non-uniform noise distribution implies that exploration capacity varies dramatically across timesteps. An exploratory action at an early stage has vastly different impact and risk compared to perturbations on near-perfect images. However, standard GRPO applies uniform optimization pressure, implicitly assuming equal learning importance at all stages. Reweighting by Reward Std. An intuitive approach to reweighting the policy loss is to utilize the standard deviation of the rewards. As illustrated in the left of Fig. 4, the reward standard deviation exhibits similar decaying trend as the denoising process unfolds, suggesting it could serve as meaningful indicator of learning importance. However, this approach introduces significant complexity. It would necessitate maintaining dynamic weighting hyperparameter for each timestep, likely tracked via an Exponential Moving Average (EMA Morales-Brotons et al. (2024)). This dynamic estimation process can be unstable and adds an undesirable layer of complexity to the training regime. Consequently, we seek more direct and elegant weighting scheme. Reweighting by Noise Level. We wonder if directly leverage the level of the exploration space itself as proxy for reweighting factor. We visualized the noise level alongside the reward standard deviation (Fig. 4, left) and observed striking correspondence between the two. This strong correlation suggests that the noise level, serves as an excellent and intrinsic proxy for the exploration capacity and associated risk at each timestep. We therefore propose to reweight the policy loss directly using the noise level. For each timestep t, we introduce weighting factor proportional to the noise level. Specifically, we modify the original policy gradient loss function to the following weighted form: Jpolicy(θ) = 1 G (cid:88) i=1 1 1 (cid:88) t=0 Norm(σt t)(min(ri t(θ) ˆAi t, clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi t) (8) The intuition behind this weighting strategy is to align the optimization pressure with the inherent properties of the generative process. In the early stages of generation, noise is large, amplifying the"
        },
        {
            "title": "Work in progress",
            "content": "learning signal during these high-noise, high-impact phases and encouraging the model to perform effective macroscopic exploration. As generation proceeds, noise diminishes, shifts the optimization focus towards fine-grained adjustments and stability, preventing aggressive exploration from corrupting high-fidelity state with noise or artifacts."
        },
        {
            "title": "4.2 POLICY GRADIENT-BASED THEORETICAL JUSTIFICATION",
            "content": "To provide deeper understanding of our approach, we now examine it from the policy gradient perspective. Notice that in the (cid:80)T 1 k=0 , is the timestep, and in the equation, is the value of the timestep. Simplifying Eq. 5, we obtain xk1 (µθ(xk, k), σ2 (cid:20) kkI), where: (cid:21) (xk + (1 k)vθ(xk, k)) σ2 2k (9) µθ(xk, k) = xk + vθ(xk, k) + Starting from the policy gradient formulation in Eq. 7, we have: θJ (θ) = 1 (cid:88) k= ExT (0,I),ϵN (0,I)[θ log pθ(xk1xk) ˆAk] Substituting xk1 in the log-probability: θJ (θ) = = 1 (cid:88) k= 1 (cid:88) k=0 ExT (0,I),ϵN (0,I) (cid:20) θ log exp (cid:18) xk1 µθ(xk, k)2 kk 2σ2 (cid:19) (cid:21) ˆAk ExT (0,I),ϵN (0,I) (cid:20) θ (cid:18) xk1 µθ(xk, k)2 kk 2σ2 (cid:19) (cid:21) ˆAk (10) (11) (12) Taking the gradient with respect to θ: θJ (θ) = 1 (cid:88) k=0 ExT (0,I),ϵN (0,I) (cid:20) xk1 µθ(xk, k) σ2 kk θµθ(xk, k) ˆAk (cid:21) (13) Since xk1 = µθ(xk, k) + σk ϵ where ϵ (0, I): θJ (θ) = 1 (cid:88) k=0 ExT (0,I),ϵN (0,I) (cid:20) ϵ σk θµθ(xk, k) ˆAk (cid:21) Expanding θµθ(xk, k): θµθ(xk, k) = θ (cid:20) xk + (cid:20)(cid:18) (cid:18) vθ(xk, k) + (xk + (1 k)vθ(xk, k)) (cid:19) (cid:21) σ2 2k σ2 k(1 k) 2k (cid:19) (cid:21) vθ(xk, k) = θ vθ(xk, k) + (cid:18) = 1 + (cid:19) σ2 k(1 k) 2k θvθ(xk, k) (14) (15) (16) (17) Substituting back: θJ (θ) = = 1 (cid:88) k=0 1 (cid:88) k=0 ExT (0,I),ϵN (0,I) ExT (0,I),ϵN (0,I) (cid:20) ϵ σk (cid:34)(cid:32) σk (cid:18) 1 + σk + σ2 k(1 k) 2k k(1 k) 2k (cid:19) (cid:33) θvθ(xk, k) ˆAk (cid:21) (18) ϵ θvθ(xk, k) ˆAk (19) (cid:35)"
        },
        {
            "title": "Work in progress",
            "content": "With σk = 1k , we get: (cid:113) σk σk k(1 k) 2k (cid:113) 1k (cid:113) 1k = = (cid:114) = 1 k(1 k) k(1 k) 2k (cid:114) = 2 k(1 k) Therefore: θJ (θ) = 1 (cid:88) k=0 ExT (0,I),ϵN (0,I) (cid:18) 1 + 2 (cid:19) (cid:114) (cid:124) k(1 k) (cid:123)(cid:122) Scale Term (cid:125) ϵ θvθ(xk, k) ˆAk (20) (21) (22) This reveals that the natural gradient coefficient is proportional to intrinsic exploration potential at timestep k. After reweighting, we have the following derivation: k, which captures the θJ (θ) = ExT (0,I),ϵN (0,I) 1 (cid:88) k=0 (cid:18) 1 + (cid:19) 2 ϵ θvθ(xk, k) ˆAk (23) (cid:124)(cid:123)(cid:122)(cid:125) Scale Term (cid:113) 1k Consider EϵN (0,I)[ϵ ˆAk], suppose the final reward is function of the small noise vector σk applied at certain step. When σk approximate the reward using first-order Taylor expansion: kϵT σk kϵ kϵ is small (and drawn from zero-mean Gaussian), we can kϵ) Rk(0) + σk kϵRkσk Rk(σk kϵ=0 (24) Since ˆAk is normalized version of Rk, the mean and std are as follows (let σk gk): mean = (cid:113) σk std = kϵ[Rk(0) + σk kϵ[(Rk(σk kϵT gk] = Rk(0) kϵ) mean)2] σk σk (cid:113) = = σk kgk kϵ[(σk kϵT gk)2] Therefore: ˆAk = Rk mean std = σk σk Eϵ[ϵ ˆAk] = Eϵ[ ϵϵT gk gk ] = kϵT gk kgk gk gk kϵRkσk kϵ=0 = (25) (26) Eq. 26 indicates that the norm of Eϵ[ϵ ˆAk] is invariant among the timesteps. So in Eq. 22 and Eq. 23, the scale terms that modulate the contribution of each timesteps model gradient θvθ(xk, k) to and k. the overall reward gradient simplify to distinct functions, which we denote as We visualize these scale terms under different flow shift, as depicted in the right of Fig. 4. As our analysis indicates, the setting of w/o reweighting exhibits highly imbalanced contribution from each timesteps model gradient to the final reward gradient. This non-uniformity systematically causes the contribution from the early denoising step where the model performs broad, structural exploration to be significantly smaller than the contribution from the late step, which focuses on fine-grained refinement. By employing our proposed noise-aware policy reweighting, this issue is substantially mitigated, as the scale term simplifies to being directly proportional to the step size, k. Furthermore, when the flow shift is set as 1, our method achieves perfect equilibrium: it ensures that the gradient contribution from every single timestep is precisely equal, thereby completely balancing this effect across the entire generation trajectory. (cid:113) k(1k)"
        },
        {
            "title": "Work in progress",
            "content": "Table 1: GenEval Result. Best scores are in blue , second-best in green . Results for models are from Flow-GRPO. Obj.: Object; Attr.: Attribution. Model Step Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Diffusion Models 0.92 0.97 0.98 0.98 0.94 0. 0.29 0.38 0.51 0.74 0.66 0.87 Autoregressive Models 0.95 0.98 0.97 0.99 0.99 0.52 0.71 0.59 0.89 0.92 Flow Matching Models 0.98 0.98 0.99 0. 0.81 0.89 0.93 0.78 GRPO based Methods 1.00 0.99 1.00 0.99 0.97 0.99 0.37 0.43 0.50 0.55 0.52 0.67 0.53 0.54 0.63 0.80 0. 0.66 0.71 0.81 0.63 0.95 0.90 0.97 0.23 0.35 0.44 0.39 0.49 0.47 0.49 0.34 0.45 0.59 0.85 0.74 0.73 0.86 0.50 0.95 0.90 0. 0.70 0.76 0.85 0.85 0.77 0.83 0.82 0.81 0.83 0.90 0.92 0.79 0.83 0.84 0.81 0.92 0.88 0.97 0.02 0.04 0.07 0.15 0.10 0.43 0.11 0.17 0.53 0.79 0. 0.22 0.34 0.59 0.24 0.99 0.85 0.98 0.05 0.06 0.17 0.23 0.19 0.45 0.28 0.21 0.42 0.66 0.61 0.45 0.47 0.65 0.52 0.86 0.80 0. LDM Rombach et al. (2022) SD1.5 Rombach et al. (2022) SD2.1 Rombach et al. (2022) SD-XL Podell et al. (2023) DALLE-2 Ramesh et al. (2022) DALLE-3 Betker et al. (2023) Show-o Xie et al. (2024b) Emu3-Gen Wang et al. (2024) JanusFlow Ma et al. (2025) Janus-Pro-7B Chen et al. (2025) GPT-4o Hurst et al. (2024) FLUX.1 Dev Black et al. (2025) SD3.5-L Esser et al. (2024) SANA-1.5 4.8B Xie et al. (2025) SD3.5-M Esser et al. (2024) - - - - - - - - - - - - - - - SD3.5-M+Flow-GRPO Liu et al. (2025) SD3.5-M+Flow-GRPO Liu et al. (2025) SD3.5-M+TempFlow-GRPO 5600"
        },
        {
            "title": "5 EXPERIMENT",
            "content": "Following Flow-GRPO, we validate our approach on Compositional Image Generation in Geneval Ghosh et al. (2023) and Human Preference Alignment in PickScore Kirstain et al. (2023). To ensure fair comparison, we normalized the weights applied to the policy loss to have mean of 1 at all timesteps. We use num groups as 48 and group size as 24. The beta of kl loss, is set to 0.001 for PickScore and 0.004 in Geneval. Unless otherwise specified, the image size is set to 512, consistent with the configuration used for Flow-GRPO. It is worth noting that we further improve upon the original Flow-GRPO method by proposing variant, Flow-GRPO (FIXED), which demonstrates superior performance. In our PickScore experiments, we adopt this improved version to ensure fair and strong baseline for comparison. 5.1 MAIN RESULTS Compositional Image Generation. We evaluate the compositional image generation capability of TempFlow-GRPO on the Geneval benchmark with its corresponding reward model. The experimental results are summarized in Tab. 1. As shown, our approach significantly improves the performance of the base model, increasing the overall score from 0.63 to 0.97. Furthermore, among GRPObased methods, our method substantially outperforms Flow-GRPO: it achieves performance of 0.97 within only 4,400 steps, whereas Flow-GRPO reaches only 0.90 under the same conditions. Additionally, as illustrated in Fig. 2, our method requires only about 2,000 steps to achieve score of 0.95, while Flow-GRPO needs approximately 5,600 steps to reach the same level. Overall, these results demonstrate that TempFlow-GRPO not only accelerates convergence but also achieves superior final performance compared to existing approaches. Human Preference Alignment. To further validate the generalizability of our approach, we conducted experiments on the PickScore benchmark, using PickScore as the reward model. As shown in the left of Fig. 2, our method, TempFlow-GRPO, achieves the highest performance, surpassing the original Flow-GRPO by approximately 1.3% and outperforming the improved baseline FlowGRPO (Fixed) by about 0.6 %. Notably, our method requires only 100200 training steps to match the performance of Flow-GRPO, and just 300400 steps to reach the level of Flow-GRPO (Fixed). These results on PickScore further demonstrate the general applicability of our method as unified flow-based RL algorithm across different reward models."
        },
        {
            "title": "Work in progress",
            "content": "Figure 5: Ablation Studies on trajectory branch and noise-aware policy reweighting. Left: PickScore Benchmark. Right: Geneval Benchmark. 5.2 ANALYSIS Ablation of TempFlow-GRPO. We conducted ablation studies to explore the effectiveness of our proposed components: the trajectory branch and noise-aware policy weighting. These ablations were performed on both the Geneval and PickScore benchmarks. As shown in the Fig. 5, on the PickScore benchmark, introducing the trajectory branch further improves the performance of FlowGRPO (Fixed), and applying noise-aware reweighting on top of this yields the highest pickscore results. On the Geneval benchmark, the benefit of the noise-aware strategy is even more significant: compared to Flow-GRPO, noise-aware policy reweighting boosts performance from 0.85 to 0.94, 9% improvement while the trajectory branch also brings about substantial gain of approximately 5%. These ablation results clearly demonstrate the effectiveness of our proposed methods. Result of 1024 resolution. We further explored the effectiveness of our approach across different image resolutions, using PickScore as the reward model. As illustrated in the Fig. 6, after 450 training steps, TempFlow-GRPO achieves 0.6% improvement in PickScore. Notably, our method requires only about 180 steps to reach the performance that Flow-GRPO (Fixed) attains after 450 steps. This further demonstrates the efficiency and effectiveness of our proposed method across varying resolutions. Qualitative Result. We also conducted qualitative analyses on the SD3.5-M, Flow-GRPO (Fixed), and TempFlow-GRPO. As shown in the visualization results in Fig 7, compared to Flow-GRPO (Fixed), TempFlowGRPO produces images with noticeably finer details and fewer visual artifacts or mistakes. In particular, our approach demonstrates superior capability in preserving complex structures and realistic textures. These qualitative improvements further highlighting the advantages of our method in generating high-quality, visually appealing images. Figure 6: Comparsion on PickScore benchmark in 1024 resolution."
        },
        {
            "title": "6 LIMITATION AND FUTURE WORK",
            "content": "Although our method achieves significant improvements in both performance and image quality, the current experiments are based on single reward model. In future work, we plan to focus"
        },
        {
            "title": "Work in progress",
            "content": "Figure 7: Qualitative comparison between SD3.5-M, Flow-GRPO and TempFlow-GRPO with PickScore as reward. We use the red box to indicate the error region. on incorporating multi rewards from more powerful models, aiming to enhance performance across multiple dimensions. Additionally, we intend to design comprehensive pipeline that can systematically improve various aspects of generative quality."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We presented TempFlow-GRPO, temporally-aware reinforcement learning framework that addresses fundamental limitations in existing flow-based GRPO methods. Our key insight is that the uniform treatment of all timesteps creates misalignment between optimization effort and actual impact on generation quality. Through trajectory branching, we enable precise credit assignment to intermediate actions without requiring specialized process reward models. Through noise-aware weighting, we ensure that optimization intensity matches each timesteps exploration potential, preventing both under-exploration of critical early decisions and over-optimization of minor refinements. Our extensive experiments demonstrate that TempFlow-GRPO achieves state-of-the-art per-"
        },
        {
            "title": "Work in progress",
            "content": "formance on human preference alignment and compositional generation benchmarks, surpassing both traditional GRPO methods and recent flow-based approaches."
        },
        {
            "title": "REFERENCES",
            "content": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Forest Labs Black et al. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-rpo: Aligning diffusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024. Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. Marginaware preference optimization for aligning diffusion models without reference. In First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, MoonarXiv preprint tae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv:2504.16828, 2025. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: arXiv preprint Flow matching for in-context image generation and editing in latent space. arXiv:2506.15742, 2025. Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2(5):7, 2024."
        },
        {
            "title": "Work in progress",
            "content": "Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-bystep preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1319913208, 2025. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 77397751, 2025. Daniel Morales-Brotons, Thijs Vogels, and Hadrien Hendrikx. Exponential moving average of weights in deep learning: Dynamics and benefits. arXiv preprint arXiv:2411.18704, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024a. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inferencetime compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024b. Xin Xie and Dong Gong. Dymo: Training-free diffusion model alignment with dynamic multiobjective scheduling. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1322013230, 2025. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025."
        },
        {
            "title": "Work in progress",
            "content": "Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025."
        }
    ],
    "affiliations": [
        "WeChat Vision, Tencent Inc",
        "ZheJiang University"
    ]
}