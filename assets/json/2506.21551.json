{
    "paper_title": "Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test",
    "authors": [
        "Ziyue Li",
        "Chenrui Fan",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks. Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's \"emergence of generalization\" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 5 5 1 2 . 6 0 5 2 : r Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test Ziyue Li, Chenrui Fan, Tianyi Zhou Department of Computer Science University of Maryland, College Park {litzy619,cfan42,tianyi}@umd.edu"
        },
        {
            "title": "Abstract",
            "content": "Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While existing studies of grokking usually train small model on one or two highlyspecific or toy tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of 7B large language model (LLM), i.e., OLMoE [15]. We compute the training loss on OLMoEs pretraining data and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks. Our study, for the first time, verifies that grokking still happens in the pretraining of practical, large-scale foundation models, though different data may enter their grokking stages asynchronously. We further demystify grokkings emergence of generalization by investigating the dynamics of LLMs internal states. Specifically, we find that training samples pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of single samples pathway reduces despite the converged loss. These indicate memorization-to-generalization conversion, providing mechanistic explanation of the delayed generalization. In the study, we develop two novel metrics to quantify the distance between pathways and the complexity of single pathway. We demonstrate their capabilities to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have crucial practical value to model pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we ground our findings in spectrum analysis of the tangent kernel for one-layer MoE, showing that more structured pathways reduce model complexity and improve the generalization bound."
        },
        {
            "title": "Introduction",
            "content": "Foundation models, e.g., large language models (LLMs) with Transformer [23] architectures, have demonstrated that long-term training for general-purpose task such as autoregressive prediction can gain powerful generalization across diverse downstream tasks and domains [3, 22, 2, 26]. Moreover, some essential capabilities, such as reasoning [7, 1] and prompting [24], also emerge during training. However, counter-intuitive delayed generalization termed grokking [18, 16, 21, 13] has been widely observed in training foundation models: the generalization (e.g., measured by test accuracy) still improves or even sharply rises long after training loss converged or plateaueda widely accepted indicator of overfitting in conventional machine learning that is detrimental to generalization. While transition from memorization to generalization is empirically observed during grokking [8], it Preprint. Under review. challenges our fundamental understanding of how and why this transition happens with converged training loss and the underlying mechanism of generalization. Despite recent attempts to explain the generalization induced by grokking, they have been largely confined to small models trained for hundreds to thousands of epochs on algorithm-synthesized small-scale data for one or two specific tasks [18, 11, 16, 14, 9, 4, 19]. Instead, LLMs pre-training often takes only one epoch on large-scale corpus of real data that might cover different domains and contain noise, imbalances, or redundancy [3, 26]. Hence, whether grokking still emerges in the pretraining of large foundation models remains an open question. Since every sample has been only learned once and the pretraining data are diverse, it is unclear whether and when different samples are memorized, and, whether they are memorized at the same time. Moreover, without repeatedly revisiting the same data, the conversion from memorization to generalization is mysterious: Where does the generalization capability of LLMs come from? When does it happen? Does it rely on memorization in advance (i.e., delayed generalization)? Unfortunately, these questions cannot be answered by the dynamics of training loss since the generalization performance can still vary with plateaued training loss, as observed in previous studies of grokking [18, 16]. It is also practically expensive to monitor the generalization performance during LLM pretraining, since pretrained checkpoint has to be finetuned before being evaluated on downstream tasks due to its lack of instruction-following capability and domain expertise, not mentioning potential confounders introduced by the finetuning recipes. Hence, finding or developing metric that is efficient to compute and accurate to reflect the dynamics of generalization during pretraining can offer both novel theoretical and practical insights on better understanding and improving the pretraining of foundation models. Main Contributions In this paper, we investigate the training dynamics and possible grokking in LLM pretraining to seek for mechanistic interpretation of its emerging generalization on downstream tasks. Our study is mainly conducted on the open-sourced checkpoints and data for pretraining of practical-scale LLM, OLMoE [15], which is mixture-of-experts (MoE) composed of 7-billion parameters. For the first time, we verify that grokking still happens during LLM pretraining. However, unlike the previously observed global and synchronous grokking for most data [18, 14, 16], LLM memorizes different domains/groups of training data at different training steps and takes varying amounts of time to start to generalize to their related test data/tasks. In other words, local grokking still occurs on each group of data but the starting time and lasting steps vary across groups. Due to the local grokking, generalization performance is usually unstable during the earlier pretraining stages but starts to steadily improve once sufficient data have been memorized. In addition, such local difference is related to data difficulty: the later memorized (loss converged) data group often takes longer to generalize. Since the training loss already converged before grokking starts, it is challenging to explain how the continual pretraining during grokking leads to the observed sharp rise of the generalization performance. In order to demystify the mechanism of memorization-to-generalization, we explore hidden states of LLMs and attempt to track their major changes after loss converges. Specifically, we look into the training dynamics of pathways in OLMoE, i.e., the expert choices across layers for each sample. We introduce two metrics to measure the complexity of pathways and how it changes over time: (1) the edit distance between different samples pathways; and (2) the consistency of selected experts embedding between consecutive layers for single samples. Empirical analysis on four domains reveal prominent trend during grokking that can explain the memorization-to-generalization mechanism: despite more data being introduced to the training, the edit distance declines and the consistency increases. This reflects decrease on the pathway complexity and smarter memorization that discovers and leverages more shareable knowledge across samples, leading to more efficient encoding of each individual sample. It explains why generalization improves without reducing the training loss: the model keeps finding more generalizable and shareable representations to memorize training data. Additionally, we provide theoretical explanation relating the pathway complexity with generalization bound of one-layer MoE. Practically, the two metrics show strong correlations with the test performance on standard benchmarks after instruction finetuning. Therefore, they provide more efficient, test-set/finetuning-free tool to monitor generalization during pretraining, which is essential to model developers and practitioners. This work takes the first step toward bridging the gaps of understanding grokking and memorizationto-generalization mechanism in LLM pretraining, by systematically analyzing the dynamics of pathway complexity in an MoE LLM. Our key findings and contributions can be summarized as: 2 Grokking still occurs during the one-pass pretraining of practical-scale LLMs but it is local and asynchronous for different data groups/domains, unlike global grokking for all data in previous works. Grokkings memorization-to-generalization mechanism can be explained by the dynamics of LLMs internal states such as MoE pathways, whose similarity between training samples and consistency across layers increase during grokking. They indicate smarter memorization using more shared knowledge across samples and lower complexity per sample leads to better generalization. This is also supported by theoretical connection between pathway complexity and generalization bound. The two novel metrics we developed to measure pathway complexity are computed on training data only without requiring any test/validation set or model finetuning, providing practically efficient tool to monitor the generalization during LLM pretraining."
        },
        {
            "title": "2 Related Work",
            "content": "Previous studies on grokking have primarily focused on relatively small models and simplistic tasks. Grokking was first identified by Power et al. [18], who observed the phenomenon using two-layer transformer trained on small-scale algorithmic dataset. Subsequent investigations have extended these findings to shallow transformers and densely connected networks [16, 17, 14, 12, 5], and CNNs [9]. Notably, all aforementioned studies employ multi-epoch training paradigms, which differ substantially from the one-pass training approach typical in LLM pretraining. Although Lv et al. [13] examined grokking behavior using 162M parameter transformer pretrained on 40 billion tokens for copying task, the scale of their model, dataset, and the simplicity of the task remain distant from practical, real-world scenarios. In contrast, our study is the first to investigate grokking in 7-billion parameter LLM across diverse and practical tasks, including mathematical reasoning, code generation, commonsense understanding, and domain-specific knowledge retrieval. Several works have focused on revealing the mechanisms behind grokking behavior. Liu et al. [11] demonstrated in simplified setting that grokking results from structured representations emerging during training. Nanda et al. [16] further revealed that grokking can be attributed to the gradual amplification of structured mechanisms encoded within model weights. Using single-layer ReLU network, Merrill et al. [14] associated grokking with subnetwork sparsity. Meanwhile, Humayun et al. [9] observed that decision boundaries become smoother during the grokking phase in CNN classification tasks. Beyond interpretability-focused research, Notsawo Jr et al. [17] identified oscillatory patterns in early training loss as predictors of subsequent grokking phenomena. Distinct from these works, our research provides an explanation of grokking within significantly larger LLMs through an interpretative analysis of MoE architectures. Additionally, we introduce practical indicators for monitoring generative behavior throughout LLM pretraining."
        },
        {
            "title": "3 Grokking (Delayed Generalization) in LLM Pretraining",
            "content": "A central challenge in large-scale pretraining is identifying when and why models begin to generalize. common belief holds that once training loss plateaus, generalization has been achieved. Yet, studies on small synthetic tasks reveal different picture: models often first memorize training data with little generalization, followed by an abrupt shift to high test accuracyhighlighting significant delay between memorization and generalization [18]. Whether this delayed transition also occurs in LLMs trained on real-world data, however, remains an open question. In this work, we present the first empirical evidence that delayed generalization also occurs during LLM pretrainingand that training loss is an unreliable proxy for generalization, often diverging from it in both timing and scale. We support this claim at two levels: (1) domain-level view showing that broad training progress does not necessarily translate into test accuracy gains, and (2) group-level view revealing that generalization consistently lags behind training, particularly for more difficult examples. These results challenge common assumptions and highlight the need for more faithful metrics to track generalization in LLMs. We conduct our evaluation using OLMoEs pretraining data as the training distribution, with corresponding out-of-distribution test sets. Full experimental and dataset details are provided in Appendix and C. 3 3.1 Domain-Level Analysis: Local Grokking Emerges Asynchronously across Domains To analyze how generalization unfolds during pretraining, we first isolate subset of training samples that the model has not only fit correctly, but has also learned stably over time. We refer to these reliably learned examples as grokking data, borrowing the term grokking to emphasize the internal transition from memorization to structured, generalizable representations. Identifying such data allows us to move beyond raw loss convergence and instead focus on the accumulation of enduring internal knowledge that may underpin downstream generalization. Formally, given training set = {(xi, yi)}N t=1 over training steps, we track each samples loss trajectory ℓi(t) = ℓ(xi, yi; θt) using cross-entropy loss. We say sample has converged at time its grokking timeif it satisfies: i=1 and model checkpoints {θt}T max t[t,T ] ℓi(t) ε and ℓi(t) ℓi(T ) δ, where ε is low-loss threshold and δ enforces stability. This filtering ensures that the selected samples reflect robust learning rather than transient memorization. We use grokking data to track training progress over time and relate it to emerging generalization behavior. Specifically, we analyze the dynamics across four distinct domains: arithmetic reasoning, code generation, commonsense question answering (QA), and domain-specific QA. Figure 1 plots, for each domain, the cumulative number of grokking data, alongside test accuracy on the corresponding downstream tasks. Training and test dynamics differ across domains. Training convergence and test accuracy evolve in distinct, domain-specific ways. In some cases, training samples steadily converge while test accuracy remains flat or random; in others, accuracy rises early and later declineswell before significant convergence occurs. These divergent patterns reveal that neither memorization nor generalization unfolds in uniform way across tasks. Figure 1: Converged groups of pretraining samples (accumulated) over training steps and test accuracy on downstream tasks across four domains. It shows the asynchronous memorization of local data groups and how it affects the global generalization: the test accuracy starts lately to rise sharply with the increasing converged training samples. But the starting time of grokking the induced gain vary across domains, explaining the difficulty to allocate global grokking. Math and coding tasks require more knowledge accumulation to generalize than commonsense and domain specific reasoning. Within each domain, training progress does not translate directly to generalization. Even as the model continues to learn and memorize training examples, test accuracy gains arrive unevenlyoften delayed or decoupled from convergence trends. This highlights central insight: accumulating converged training samples does not reliably predict when generalization occurs, even within the same domain. Relying on convergence metrics alone can therefore misrepresent the onset and scale of generalization, motivating the need for deeper, more informative indicators. 4 3.2 Group-Level Analysis: Later Grokking & Longer Delay for More Difficult Data To better understand this decoupling, we analyze generalization at finer granularity by tracking when individual training samples are learned. Using the grokking times defined above, we extract the set of samples that are eventually learned stably: Dgrok = {(xi, yi) < }. We then group these training samples based on their grokking onset: D(t) grok = {(xi, yi) = t}. For the test set, we similarly classify samples, focusing on those that eventually generalized successfully. To ensure strict separation from the training set, we apply membership inference attacks [25] to eliminate any test samples that may have been exposed during training. For each remaining sample, we track its prediction trajectory throughout training and assign it to generalization-time group based on the checkpoint at which its prediction becomes consistently accurate. For precise analysis of how improvements in training translate to generalization gains in testing, we pair each training group with the most similar test group. Specifically, we compute embeddings for samples using SentenceTransformer [20] and apply the Hungarian algorithm [10] to perform optimal one-to-one matching between training and test groups based on input similarity. Figure 2: Training loss and test accuracy of four data groups over pretraining steps with local grokking. The pretraining data and test samples in each group are matched based on their semantic similarities. We observe local grokking, i.e., delayed rise of test accuracy after training loss plateaued, on every group, though their delayed steps and grokking period lengths vary. From left to right, later converged and memorized (harder) samples exhibit systematically longer delays. Figure 2 illustrates the key finding: across all convergence-defined training groups and their matched test groups, training loss stabilizes well before test accuracy begins to improve. For training groups that converge early, the delay between convergence and test improvement is short, with accuracy rising soon after the loss flattens. For later-converging groups, the delay is substantially longer, often requiring many additional training steps before generalization appears in the corresponding test groups. This systematic generalization delay shows that the model requires time not only to memorize the training data but also to reorganize its internal representations into forms that support transfer to unseen examples. This delay manifests not only as temporal gap, but as qualitative transition: across groups, test accuracy stays near chance level for an extended period before abruptly rising to perfect generalization during the grokking phase. Importantly, the delay is not random: it scales predictably with sample difficulty, suggesting that the complexity of pattern determines not just how long it takes to learn, but also how long it takes to generalize. Thus, even at the group level, training loss alone remains an unreliable predictor of generalization readiness. Open Question. Both domain-level and group-level analyses reveal core insight: generalization does not arise automatically with training loss convergence. In some cases, models may memorize vast quantities of training data without any meaningful transfer to unseen examples. In other cases, they may require long post-convergence periods before generalization appears. This decoupling of memorization and generalization raises fundamental question for understanding the emergence of capability in large models: What internal changes occur within the model that enable this sudden emergence of generalization? Addressing this question is key to advancing diagnostic tools, improving training efficiency, and uncovering the mechanisms that drive learning in large-scale neural networks. In the next section, we analyze the models internal dynamics to tackle this challenge directly."
        },
        {
            "title": "4 Memorization-to-Generalization: Analysis of Routing Pathway Dynamics",
            "content": "Grokking is marked by sudden shift from memorization to generalization long after the training loss has plateaued. Thus, traditional metrics like the training loss provide limited insight into the models internal changes leading up to the shift. In contrast, MoE architectures offer unique window into the models the model dynamics. In an MoE, each input activates only small subset of experts, racing discrete sequence of selections (a routing pathway) across layers. This routing pathway effectively externalizes part of the models computation, making internal decision-making directly observable. In this section, we analyze the evolution of pathway patterns during grokking and introduce two metrics to quantify these changes (Figure 3): (i) the similarity of pathways across different inputs, and (ii) the consistency of single inputs pathway. Importantly, all metrics are computed on the grokking subsets {D(t) grok} identified in Section 3.2, which consist of training samples that have achieved stable, low loss at different checkpoints. Focusing on these grokking samples allows us to isolate and study the internal transition from memorization to generalization, beyond mere loss convergence. We show that as training progresses, pathways evolve from initially random and instance-specific to increasingly structured and shared across samples, providing concrete, mechanistic signal of the memorization-to-generalization transition. Figure 3: Pathway Complexity Metrics to Monitor Grokking. Left: Pathway similarity between samples is measured by edit distance on their sequences of expert choices. Right: Pathway consistency quantifies the smoothness of expert transitions between subsequent layers via cosine similarity of their weighted expert embeddings. 4.1 Pathway Distance and Knowledge Sharing between Training Samples One hypothesis is that as the model begins to generalize, it moves from highly individualized, inputspecific computation toward shared, structured computation across similar inputs. This shift would be reflected in increasing pathway similaritythe convergence of routing patternssignaling the emergence of shared mechanisms for solving related tasks. We test this by examining how expert pathway similarity evolves during pretraining. Each inputs pathway is the ordered sequence of selected experts through the MoE layers. Specifically, for input xi, we define its pathway si as: si = concat(e(i) 1 , e(i) 2 , . . . , e(i) ) where e(i) is the ordered list of expert indices at layer ℓ for input xi, as comma-separated string (e.g., ℓ 3,1,5). These are joined across layers with hyphens (e.g., 3,1,5-...-9,1). Expert selection dynamically includes top experts whose cumulative routing weights exceed predefined threshold. We quantify pathway similarity using sequence edit distance: Dpath(si, sj) = EditDistance(cid:0)si, sj), where EditDistance(, ) is the standard Levenshtein edit distance, counting the minimal number of insertions, deletions, or substitutions needed to transform one sequence into the other. Thus, edit 6 distance captures both local and global differences across the entire routing sequenceaccounting for mismatches in expert selection, variations in sequence length, and shifts in expert ordering. We track the average pairwise edit distance across samples over pretraining. Overall pattern. Our analysis, shown in Figure 4, reveals clear trajectory in pathway dynamics. Early in pretraining, most inputs follow nearly identical routes through the experts, resulting in low edit distance Dpath across samples. As training progresses, however, pathways begin to diverge: each sample carves out its own route, increasing Dpath between samples. Strikingly, this divergence reverses once convergence sets in. Inputs with similar semantic content are increasingly routed through similar expert sequences, reflecting an emergence of shared pathways. This convergence signals that the model is discovering common internal computations for related inputsa concrete marker of generalization. Layerwise pattern. Interestingly, this transition is highly nonuniform across depth, as shown in Figure 5. Shallow layers exhibit the steepest declines in Dpath at the grokking point, while deeper layers show smaller reductions. Notably, the final layer reverses this trend, with Dpath increasing post-grokking. These results suggest that grokking reorganizes the network in depth-specific manner: early layers converge toward universal pathways, whereas later layers maintainor even increaserouting flexibility, balancing shared representations with task-specific specialization. Figure 4: Pathway edit distance and training loss of two data groups (converged at 125k and 245k steps) over pretrianing steps across four domains. While training loss plateaued early, the edit distance between pathways keeps declining beyond convergence, indicating declining complexity of internal memorization and continuous transition from memorization to generalization. Figure 5: Layer-wise evolution of pathway edit distance during pretraining. The overall distance drops from early to later layers, indicating higher complexity in early layers. For each layer, the distance initially rises, then sharply declines at grokking onset. The shift is depth-dependentearly layers expert choices rapidly converges, later layers change more gradually, and the final layer diversifies after grokking. 4.2 Pathway Consistency and Complexity for Single Samples We next investigate how the routing complexity of individual inputs evolves during pretraining. Intuitively, more streamlined and consistent expert selection across layers may signal that the model has 7 discovered generalizable processing strategy. To capture this, we propose metric for single-sample pathway consistency, measuring the smoothness of expert transitions across consecutive layers. In typical MoE architecture, the router at each layer ℓ is implemented as single-layer MLPa linear transformation followed by softmaxthat computes routing scores over experts. The weight matrix of this router is denoted Wℓ RKd, where is the number of experts and is the embedding dimension. We define the expert embedding for expert at layer ℓ as: vk ℓ := Wℓ[k], for = 1, . . . , K. Each vk ℓ captures the routers parameterized representation of expert k, and can be interpreted as the anchor vector used to compute routing scores by comparing it against input features (e.g., via dot product or similarity). Although these embeddings are fixed across inputs, their relative geometry governs expert selection during inference. For given input xi, we define its routing embedding as the weighted sum: e(i) ℓ = (cid:88) k=1 g(i,k) ℓ vk ℓ , ℓ where g(i,k) independently parameterized across layers, we assume their embeddings {vk layers, enabling structural analysis of routing behavior. is the routing weight assigned to expert for input xi at layer ℓ. Although experts are ℓ } are comparable across To quantify routing smoothness, we define pathway consistency Ci as the normalized average cosine similarity between consecutive layer embeddings for input xi: Ci = 1 1 1 L1 (cid:88) ℓ=1 cos(ei,ℓ, ei,ℓ+1) maxℓ cos(ei,ℓ, ei,ℓ+1) + ϵ , where ϵ = 108 ensures numerical stability. Higher Ci indicates more erratic transitions, while lower values reflect smoother, more consistent routing across layers. Figure 7 tracks the evolution of pathway consistency and training loss across four tasks. key finding is that even after the loss stabilizes, pathway consistency continues to improverevealing ongoing refinement in the models internal routing. This suggests that smoother, more coherent cross-layer transitions emerge well beyond the point of loss convergence, positioning pathway consistency as valuable lens for probing deeper model organization beyond prediction error. Figure 6: Pathway consistency and training loss of two data groups (converged at 490k and 615k steps) over pretrianing steps across four domains. While training loss converges early, pathway consistency continues to improve, revealing continuous refinement and reducing complexity of expert choices beyond memorization, which indicates transition to generalization. 4.3 Pathway Complexity Metrics as Monitors of Generalization Given the strong link between routing dynamics and generalization, we ask: To what extent do our pathway metrics predict test performance across domains? To answer this, we compute both Pearson and Spearman correlations between test accuracy and two metrics across training checkpoints: (1) pairwise pathway similarity, measured by normalized edit distance, and (2) single-sample pathway consistency, measured by cross-layer cosine similarity. 8 Table 1: Correlation between pathway metrics and generalization (test accuracy) across four domains, compared with training/validation loss for monitoring generalization. p-value: < 0.1 , < 0.05 , < 0.01 , < 0.001 , > 0.1 , > 0.5 , > 0.8 . Preferred correlation: positive , negative . Domain-Specific Commonsense Math Code Pearson Spearman Pearson Spearman Pearson Spearman Pearson Spearman Metric Pathway Distance (pairwise) Pathway Consistency (sample-wise) Training Loss Training Loss (moving average) Validation Loss -0.9471 0.9246 -0.0435 0.4714 0.4801 -0.9487 0.9487 -0.2108 0.3162 -0.1054 -0.9290 0.9786 -0.0680 0.2481 0. -0.9276 0.9856 -0.2052 -0.2052 0.8721 -0.9821 0.9804 0.6427 0.7931 -0.5752 -0.9747 0.9747 0.4104 0.6669 -0. -0.9254 0.9917 0.7944 0.8705 -0.3339 -0.9487 0.9487 0.9487 0.9487 -0.3162 Figure 7: Correlation between generalization-monitor metrics and test accuracy, and linear regression on five checkpoints. Pathway edit distance is negatively correlated with test accuracy, while pathway consistency is positively correlatedboth aligning with expectations. In contrast, training loss and its moving average show weak or even misleading correlations with test accuracy, underscoring that lower training loss does not reliably indicate better generalization. We define the global grokking point as the first checkpoint after which test accuracy exhibits sustained upward trend (Figure 1), marking the onset of meaningful generalization. Our correlation analysis focuses on checkpoints after this grokking pointexcluding the initial random-accuracy phaseto isolate the mechanisms driving generalization. Table 1 summarizes the correlation results across four tasks. Pathway consistency (sample-wise) shows exceptionally strong positive correlations with test accuracyoften exceeding 0.97 and highly significantindicating that coherent, stable routing across layers is key marker of generalization. In contrast, pairwise pathway similarity consistently yields strong negative correlations (around -0.93), suggesting that as generalization improves, inputs tend to follow increasingly similar expert routes. By comparison, training and validation loss metrics exhibit weaker and less consistent correlations. Notably, while moving-average training loss shows relatively high correlation in certain domains (e.g., Code, Domain-Specific), its direction is inconsistent with expectation: since loss is minimized during training, we would expect negative correlation with test accuracynot the positive one observed here. This mismatch further underscores that conventional loss metrics fail to reflect the internal transition from memorization to generalization. These findings position pathway metrics as robust, domain-general indicators of generalization. Importantly, they rely only on training dynamicswithout requiring held-out validation datamaking them especially valuable for real-time monitoring in large-scale or resource-constrained settings. 4.4 Theoretical Connections & Explanations We establish fundamental connection between expert routing patterns and model generalization through spectral analysis of the routing kernel geometry. Our theory identifies the effective complexity of the routing mechanism as the key determinant of MoE generalization. Let denote the routing kernel Gram matrix with entries kernel Θroute captures parameter sensitivity of expert selection. Theorem 4.1 (Generalization Bound). Under NTK regime with experts, with probability 1 δ over samples: ij = Θroute(xi, xj), where the routing C1λ2 (cid:124) y(H + λI)2y (cid:125) (cid:123)(cid:122) Bias + C2 (cid:124) (cid:18) σ2Tr(H (H + λI)1) + σ2 log(1/δ) (cid:123)(cid:122) Variance + Noise (cid:19) (cid:125) 9 This explains the grokking phenomenon: When expert activations transition from random to structured, the effective dimension Tr(H (H + λI)1) collapses, producing sudden drops in generalization error. For detailed theoretical setup, assumptions, and proof of Theorem A.1, please refer to Appendix A."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents the first empirical evidence of grokking in large-scale LLM pretraining, showing that generalization emerges asynchronously across domainsoften well after training loss has converged. Through an analysis of routing dynamics in 7B-parameter MoE model, we propose two novel pathway-based metrics that accurately track generalization without requiring finetuning or external validation sets. Our findings reveal transition from memorization to generalization, marked by increased pathway similarity and consistency, and supported by theoretical connections between routing structure and generalization bounds. These insights offer foundation for more transparent and reliable monitoring of generalization in large-scale models. Limitations. Our study is limited to single model architecture due to checkpoint availability. Moreover, the theoretical analysis is restricted to one-layer MoE under NTK assumptions, which may not extend directly to deeper or more heterogeneous architectures."
        },
        {
            "title": "References",
            "content": "[1] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges, 2024. URL https://arxiv. org/abs/2402.00157. [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. [4] Branton DeMoss, Silvia Sapora, Jakob Foerster, Nick Hawes, and Ingmar Posner. The complexity dynamics of grokking, 2024. URL https://arxiv.org/abs/2412.09810. [5] Simin Fan, Razvan Pascanu, and Martin Jaggi. Deep grokking: Would deep neural networks generalize better?, 2024. URL https://arxiv.org/abs/2405.19454. [6] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1 (2):3, 2022. [7] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey, 2023. URL https://arxiv.org/abs/2212.10403. [8] Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Unified view of grokking, double descent and emergent abilities: perspective from circuits competition, 2024. URL https://arxiv.org/abs/2402.15175. [9] Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. Deep networks always grok and here is why, 2024. URL https://arxiv.org/abs/2402.15555. [10] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397, 1955. [11] Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:3465134663, 2022. [12] Ziming Liu, Eric Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data. arXiv preprint arXiv:2210.01117, 2022. [13] Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Rui Yan. Language models \"grok\" to copy, 2025. URL https://arxiv.org/abs/2409.09281. [14] William Merrill, Nikolaos Tsilivis, and Aman Shukla. tale of two circuits: Grokking as competition of sparse and dense subnetworks, 2023. URL https://arxiv.org/abs/2303. 11873. [15] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv. org/abs/2409.02060. [16] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. [17] Pascal Notsawo Jr, Hattie Zhou, Mohammad Pezeshki, Irina Rish, Guillaume Dumas, et al. Predicting grokking long before it happens: look into the loss landscape of models which grok. arXiv preprint arXiv:2306.13253, 2023. [18] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022. [19] Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of numerical stability, 2025. URL https://arxiv.org/abs/2501.04697. [20] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084. [21] Zhiquan Tan and Weiran Huang. Understanding grokking through robustness viewpoint, 2024. URL https://arxiv.org/abs/2311.06597. [22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. URL https://arxiv. org/pdf/1706.03762.pdf. [24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [25] Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Frank Yang, and Hai Li. Min-k%++: Improved baseline for detecting pre-training data from large language models. arXiv preprint arXiv:2404.02936, 2024. [26] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. survey of large language models, 2025. URL https://arxiv.org/abs/ 2303.18223."
        },
        {
            "title": "A Generalization Bound for MoE with Routing Kernel",
            "content": "A.1 Model Setup and Assumptions Consider MoE model where the router is fixed after pretraining, and the expert networks are trainable. Let be the total number of experts in the model. fk(ϕk, x) be the k-th experts output, with trainable parameters ϕk. gk(x) [0, 1] be the k-th routing weight, which is fixed and pre-determined for any input x. We assume (cid:80)K k=1 gk(x) = 1 (e.g., from fixed softmax layer or pre-computed weights). Full model: (θ, x) = (cid:80)K all trainable expert parameters. k=1 gk(x)fk(ϕk, x), where θ = (ϕ1, . . . , ϕK) is the collection of Key Assumptions: 1. NTK Regime for Experts: Expert parameters θ = (ϕ1, . . . , ϕK) are initialized as ϕk(0) width)), (0, I) for each k, and remain ϵ-close to initialization (θ(t) θ(0) = O(1/ enabling linearization via Neural Tangent Kernel (NTK). We assume ϕk(0) are independent across different experts k. 2. Fixed Routing: The routing weights gk(x) are fixed for all inputs and do not contain any trainable parameters. They satisfy maxk gk 1. 3. Label Noise: Training labels yi = (xi) + ϵi with ϵi (0, σ2), independent of xi. A.2 Routing Kernel Definition Define the NTK for this MoE configuration. Consistent with the general routing kernel definition in the main text, Θroute(x, x) here is specifically instantiated as: Θroute(x, x) = Eθ(0) (cid:20)(cid:28) (θ(0), x) θ , (θ(0), x) θ (cid:29)(cid:21) = (cid:88) j=1 gj(x)gj(x)Eϕj (0) (cid:20)(cid:28) fj(ϕj(0), x) ϕj , fj(ϕj(0), x) ϕj (cid:29)(cid:21) Let Kfj (x, x) = Eϕj (0) Then, the Routing Kernel in this fixed-routing scenario is: , fj (ϕj (0),x) ϕj (cid:104)(cid:68) fj (ϕj (0),x) ϕj (cid:69)(cid:105) be the NTK for the j-th expert network. Θroute(x, x) = (cid:88) j=1 gj(x)gj(x)Kfj (x, x) The structure of Θroute(x, x) in this fixed-routing setup provides crucial insights into how routing influences generalization. It is composed of two primary factors: the gating weight product gj(x)gj(x), and the individual expert NTKs Kfj (x, x). The term gj(x)gj(x) signifies the importance of the j-th expert for both inputs and x, highlighting that two inputs are considered \"similar\" by the kernel if they are significantly routed to the same experts. If inputs and are routed to distinct sets of experts, their contribution to the sum for given will be diminished. Simultaneously, Kfj (x, x) captures the functional similarity learned by the j-th expert for inputs and x. Thus, the overall routing kernel measures compounded similarity: inputs are similar not only if they are guided through similar pathways (weighted by gj(x)gj(x)), but also if the specific experts they pass through exhibit similar functional behavior for those inputs (Kfj (x, x)). This decomposition underscores how fixed routing patterns (reflected in gj(x)) set the stage for expert specialization and ultimately shape the global generalization properties of the MoE model. A.3 Main Theorem Theorem A.1 (Generalization Bound for MoE Experts). Under Assumptions 1-3, with probability 1 δ over training samples (xi, yi)n i=1, the excess risk satisfies: Ex,y (cid:2)(F (θ, x) (x))2(cid:3) C1λ2 (cid:124) y(H + λI)2y (cid:125) (cid:123)(cid:122) Bias + C2 (cid:124) (cid:18) σ2Tr(H (H + λI)1) + σ2 log(1/δ) (cid:123)(cid:122) Variance + Noise (cid:19) (cid:125) where properties of gk(x) and fk(x). The bound decays as (cid:0) 1 i,j = Θroute(xi, xj), λ > 0 is regularization, and C1, C2 are constants depending on the (cid:1) when λ = Θ(1). Proof. Under NTK regime, the model can be linearized around initialization: (θ, x) (θ(0), x) + θF (θ(0), x), θ θ(0) = (θ(0), x) (cid:125) (cid:123)(cid:122) (cid:124) Initial Output +Φ(x)θ where Φ(x) = θF (θ(0), x) and θ = θ θ(0). The quadratic error term θ2 is O(1/width) and thus negligible. Define centered labels yi = yi (θ(0), xi). The problem reduces to ridge regression: min θ 1 2 (cid:88) i= (cid:0)Φ(xi)θ yi (cid:1)2 + λ 2 θ2 with closed-form solution: θ = (ΦΦ + λI)1Φy = Φ(H + λI)1 where = ΦΦ is the kernel matrix with i,j = Θroute(xi, xj). The excess risk decomposes as: E[(F (θ, x) (x))2] = (E[ ˆf (x)] (x))2 (cid:125) (cid:124) (cid:123)(cid:122) Bias2 + E[( ˆf (x) E[ ˆf (x)])2] (cid:123)(cid:122) (cid:125) Variance (cid:124) +σ2 Bias Term: Bias2 λ2 n2 y(H + λI)2y C1λ2 y(H + λI)2y where C1 absorbs constants related to the magnitudes of gk(x) and expert gradients. Variance Term: The parameter covariance is Cov(θ) = σ2(ΦΦ + λI)1ΦΦ(ΦΦ + λI)1. For prediction variance: Variance = Ex[Φ(x)Cov(θ)Φ(x)] = σ2 Tr(H (H + λI)1) Confidence Term: By standard concentration inequalities (e.g., for sub-Gaussian noise), (cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:88) i=1 σ2 ϵ2 (cid:33) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ent2/(2σ4) = σ2 log(1/δ) Collecting all components and adjusting constants C1, C2 completes the proof. 14 A.4 Connecting Edit Distance to Effective Dimension Thus the bound shows that simple (low-dimensional) routing kernels generalize better. We now empirically test how routing pattern complexity (via edit distance) correlates with this effective dimension deff = Tr(H (H + λI)1). Our setup involves lightweight MoE architecture with 16 experts and 100-dimensional input space. Input data are synthetic Gaussian clusters, providing controlled environment to analyze pre-defined routing strategies. For each experimental trial, 200 samples are split into training and validation sets. We train one-layer MoE model with ReLU activations where only expert networks are trainable; the routers weights are initialized once and then frozen. To explore diverse fixed routing patterns, we perform multiple independent runs, each starting with different random initialization of the fixed router. Following training, we extract the average edit distance of the pre-determined expert routing paths and the effective dimension. As illustrated in Figure A.1, we observe strong positive correlation between these two quantities. This empirically demonstrates that the structural properties of fixed routing, measured by edit distance, meaningfully impact the complexity of the function space learned by the experts, as predicted by deff . This alignment supports edit distance as valuable diagnostic for MoE generalization. Figure A.1: Correlation of Edit Distance with Effective Dimension. Each point represents an independent experimental run where the router is initialized with different random seed and then fixed for expert training. This varying initialization leads to diverse fixed routing patterns. We observe strong positive correlation. This suggests that the structural diversity of pre-defined routing significantly impacts the complexity of the function space learned by experts."
        },
        {
            "title": "B Dataset Details",
            "content": "We conduct our evaluation on four domains used during OLMoE pretraining: math, code, commonsense, and domain-specific (scientific text). For each domain, we construct two sets: Training set: 10,000 examples randomly sampled from the corresponding OLMoE pretraining data. Test set: 10,000 examples from distinct dataset in the same domain, not used during pretraining. In the domain-specific setting, we further filter the pretraining set to include only samples containing the phrase Organic Chemistry to create more focused training distribution. Table A.1 summarizes the source links for all datasets. In addition to the training and test sets used for evaluation, we compute validation loss on separate domain-specific datasets to monitor in-domain learning dynamics and assess their relationship to generalization. Specifically, we aim to quantify the correlation between validation loss and test accuracy, to test whether validation loss can serve as predictor of generalization in each domain. 15 Table A.1: Sources of training and test data used in our domain-level evaluation. All datasets accessed via HuggingFace. Domain Pretraining Source Test Dataset Math Code Commonsense Domain-Specific open-web-math/open-web-math bigcode/starcoderdata emozilla/dolma-v1_7-books blester125/mediawiki-dolma (filtered by Organic Chemistry) hkust-nlp/dart-math-pool-math evalplus/humanevalplus tau/commonsense_qa cais/mmlu (\"college_chemistry\" subset) To this end, we select distinct validation datasets for each domain, ensuring they are disjoint from both the training and test sets while remaining representative: Table A.2: Datasets used for computing validation loss, used to assess correlation with test accuracy. Domain Validation Dataset Math Code Commonsense Domain-Specific qwedsacf/competition_math newfacade/LeetCodeDataset kowndinya23/flan2021-commonsense chemNLP/chemistry-bookshelves-merged These datasets are used solely for loss computation and correlation analysis."
        },
        {
            "title": "C Experiment Details",
            "content": "Compute Resources. All experiments are conducted on NVIDIA A100 GPUs with 80GB memory, using public OLMoE checkpoints. Instruction Tuning. The raw OLMoE checkpoints exhibit weak instruction-following ability, making direct evaluation of generalization behavior unreliableparticularly in text-based domains such as commonsense reasoning or code completion. To address this, we apply light instruction tuning using LoRA [6], not to improve task performance per se, but to make the pretrained model follow prompts in human-readable way. This ensures we can evaluate generalization while preserving the original pretraining behavior. To avoid overriding pretraining representations, we adopt minimal finetuning using small number of epochs and low-rank parameter updates. Specifically: Math: We finetune each checkpoint using the same math SFT dataset meta-math/MetaMathQA employed by OLMoE for 3 epochs with LoRA. Code: We use the HuggingFaceH4/CodeAlpaca_20K dataset and finetune for 3 epochs. Commonsense & Domain-Specific: We reuse the instruction-tuned checkpoint trained on yahma/alpaca-cleaned, shared across both domains. Key configurations of LoRA include: Target modules: q_proj, k_proj, v_proj LoRA rank: 32 LoRA dropout: 0.1 Learning rate: 5e-5 Batch size: 4 Max sequence length: 2048 This lightweight finetuning ensures reliable promptability while preserving the pretrained models generalization behavior. Applying Min-K%++ for Training-Test Separation. To maintain rigorous separation between the training and test datasets, we employ the Min-K%++ membership inference attack [25], state-ofthe-art, reference-free method designed to detect whether specific data samples are part of models training corpus. 16 In our experiments, we apply the Min-K%++ method to the test dataset to identify samples that the model is most likely to have encountered during training. Determining an optimal threshold for classification is challenging due to variability in model behaviors and data distributions. To address this, we adopt conservative approach by removing the top 10% of test samples with the highest Min-K%++ scores, thereby minimizing the risk of training data contamination in the test set. This strategy ensures that our evaluation metrics accurately reflect the models generalization capabilities on truly unseen data, enhancing the validity of our experimental results."
        }
    ],
    "affiliations": [
        "Department of Computer Science University of Maryland, College Park"
    ]
}