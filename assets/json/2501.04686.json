{
    "paper_title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics",
    "authors": [
        "Ruilin Luo",
        "Zhuofan Zheng",
        "Yifan Wang",
        "Yiyao Yu",
        "Xinzhe Ni",
        "Zicheng Lin",
        "Jin Zeng",
        "Yujiu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical reasoning of Large Language Models (LLMs). Recently, the introduction of derivative process supervision on CoT trajectories has sparked discussions on enhancing scaling capabilities during test time, thereby boosting the potential of these models. However, in multimodal mathematical reasoning, the scarcity of high-quality CoT training data has hindered existing models from achieving high-precision CoT reasoning and has limited the realization of reasoning potential during test time. In this work, we propose a three-module synthesis strategy that integrates CoT distillation, trajectory-format rewriting, and format unification. It results in a high-quality CoT reasoning instruction fine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively validate the state-of-the-art (SOTA) performance of the trained URSA-7B model on multiple multimodal mathematical benchmarks. For test-time scaling, we introduce a data synthesis strategy that automatically generates process annotation datasets, known as DualMath-1.1M, focusing on both interpretation and logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT reasoning capabilities to robust supervision abilities. The trained URSA-RM-7B acts as a verifier, effectively enhancing the performance of URSA-7B at test time. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD) verifying capabilities, showcasing its generalization. Model weights, training data and code will be open-sourced."
        },
        {
            "title": "Start",
            "content": "URSA: Understanding and Verifying Chain-of-thought Reasoning in"
        },
        {
            "title": "Multimodal Mathematics",
            "content": "Ruilin Luo 1 * Zhuofan Zheng 2 * Yifan Wang 1 Yiyao Yu 1 Xinzhe Ni 1 Zicheng Lin 1 Jin Zeng 2 Yujiu Yang"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 8 ] . [ 1 6 8 6 4 0 . 1 0 5 2 : r Chain-of-thought (CoT) reasoning has been widely applied in the mathematical reasoning of Large Language Models (LLMs). Recently, the introduction of derivative process supervision on CoT trajectories has sparked discussions on enhancing scaling capabilities during test time, thereby boosting the potential of these models. However, in multimodal mathematical reasoning, the scarcity of high-quality CoT training data has hindered existing models from achieving highprecision CoT reasoning and has limited the realization of reasoning potential during test time. In this work, we propose three-module synthesis strategy that integrates CoT distillation, trajectory-format rewriting, and format unification. It results in high-quality CoT reasoning instruction fine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively validate the state-of-the-art (SOTA) performance of the trained URSA-7B model on multiple multimodal mathematical benchmarks. For test-time scaling, we introduce data synthesis strategy that automatically generates process annotation datasets, known as DualMath-1.1M, focusing on both interpretation and logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT reasoning capabilities to robust supervision abilities. The trained URSA-RM-7B acts as verifier, effectively enhancing the performance of URSA-7B at test time. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD) verifying capabilities, showcasing its generalization. Model weights, training data and code will be open-sourced. *Equal contribution 1Tsinghua University 2ByteDance. Correspondence to: Yujiu Yang <yang.yujiu@sz.tsinghua.edu.cn>, Co-First Author: <lrl23@mails.tsinghua.edu.cn>. 1 Chain-of-thought (CoT) reasoning has proven to be highly effective in mathematical reasoning with Large Language Models (LLMs), serving various roles from multiple perspectives (Zhang et al., 2024e; Wei et al., 2022; Chu et al., 2023; Sprague et al., 2024; Zhang et al., 2022; An et al., 2023; Luo et al., 2024b; Mu et al., 2024). Recently, numerous studies have focused on supervising the quality of CoT-style reasoning during inference to investigate test-time scaling laws and push the performance boundaries of these models (Zhang et al., 2024b; Lightman et al., 2023; Setlur et al., 2024; Fu et al., 2024b). However, this pathway has not been fully explored in the context of mathematical reasoning with Multimodal Large Language Models (MLLMs). Before considering the introduction of CoT supervision paradigm for multimodal mathematical reasoning, we first identify the limitations of existing work: i) General MLLMs, which perform well across various leaderboards, are designed with balanced development approach. The models not specifically focus on enhancing mathematical capabilities, particularly in high-difficulty scenarios such as complex geometry problems (Shi et al., 2023). ii) Meanwhile, current open-source models in the multimodal mathematics do not yet exhibit satisfactory CoT reasoning capabilities. This may be due to the lack of high-quality and sophisticated CoTformatted reasoning data in the training datasets, restricting models to merely fitting short output patterns (Zhang et al., 2024e; Yang et al., 2024a; Zhang et al., 2023). To address the need for enhanced CoT reasoning capabilities in the multimodal mathematical domain, we begin by selecting LLM specifically designed for mathematical reasoning as our LLM backbone. During the instruction fine-tuning phase, we extensively collect existing open-source training data from the mathematical domain, including datasets such as Multimath (Peng et al., 2024), MAVIS (Zhang et al., 2024d), PUMA-VarsityTutors (Zhuang et al., 2024), and MathV-360K (Shi et al., 2024), which cover tasks like geometry problem solving, math word problems, and table QA. However, we observed that high-quality CoT-formatted reasoning data is either scarce or not sufficiently sophisticated. Recognizing the importance of CoT reasoning, we syntheURSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 1: We compare the reasoning ability of URSA-7B and other open-source MLLMs across different topics on MathVista and MathVerse, as well as their reasoning stability when faced with changes in modal information content. size targeted CoT-style data to create 1M high-quality instruction fine-tuning dataset. This significantly enhances MLLMs potential to reach their upper limits in mathematical reasoning. After developing robust CoT reasoning trajectory constructor, we consider to further achieve superior CoT reasoning with an advanced process reward model (PRM). However, in the context of MLLMs, additional challenges arise due to the integration of multimodal information. In multimodal scenarios, errors extend beyond typical logical inconsistencies in text-based reasoning, including modality-specific mismatches (Wang et al., 2024b; Gao et al., 2024; Liang et al., 2024). For instance, during the reasoning process of MLLMs, hallucinations such as coordinate recognition errors and ambiguous edge-angle relationships can occur, which may not be adequately addressed by the model. Therefore, when refining the multimodal PRM, it is crucial to reward not only doing things right but also seeing things precisely. To tackle these challenges, we propose novel dual-view process supervision data synthesis method aiming at error localization. This involves synthesizing data by employing binary error step localization with the Monte Carlo Tree Search (Browne et al., 2012) and step-wise targeted hallucination injection prompting, relying on the models own reasoning data distribution and the capabilities of external MLLMs, respectively. We evaluate URSA-7B on several widely-used datasets, such as MathVista (Lu et al., 2023a), MathVerse (Zhang et al., 2025), WE-MATH (Qiao et al., 2024), and DYNAMATH (Zou et al., 2024). URSA-7B not only achieves state-of-the-art (SOTA) performance among models of the same size but also surpasses powerful closed-source models in certain types of questions and dimensions. Additionally, our trained URSA-RM-7B effectively guides trajectory selection during reasoning, leading to better performance. The main contributions of this paper are summarized as follows: We contribute the URSA-7B model and the corresponding MMathCoT-1M instruction fine-tuning data. This significantly enhances MLLMs potential to reach their upper limits in mathematical reasoning. We innovatively propose dual-view process supervision data synthesis, contributing DualMath-1.1M data and URSA-RM-7B model. This approach effectively guides URSA-7B to achieve better performance in mathematical reasoning tasks. URSA-7B achieves SOTA results on several multimodal mathematics benchmarks, establishing stronger foundation model in this domain. Additionally, URSA-RM-7B demonstrates remarkable out-ofdistribution (OOD) capabilities, showcasing the feasibility of test-time scaling in multimodal mathematical reasoning. 2. Related Work Multimodal Math Reasoning The mathematical reasoning abilities of MLLMs have recently garnered widespread attention (Zhuang et al., 2024; Gao et al., 2023; Li et al., 2024a; Dong et al., 2024b; Hu et al., 2024; Yang et al., 2024b; Han et al., 2024; Guo et al., 2024). Compared to mathematical reasoning tasks in LLMs (Luo et al., 2023; Yu et al., 2023), multimodal mathematical reasoning requires MLLMs to capture conditions in the visual domain and 2 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 2: Data sources used by the URSA-7B model during the VL-alignment and SFT phases. perform cross-modal reasoning between images and text. Tasks such as geometric problems and chart reasoning are among the more challenging ones (Chen et al., 2021). Some works enhanced the input of visual mathematical signals through visual encoders in certain scenarios (Zhang et al., 2024d; Liu et al., 2024a; Chen et al., 2024a). More efforts focus on the synthesis of mathematical reasoning data, emphasizing the diversity and complexity of problems. MathLLaVA (Shi et al., 2024) proposes the MathV360K dataset by classifying images based on complexity and enhancing questions accordingly. GeoGPT4V (Cai et al., 2024) uses GPT-4V to simplify and expand question-answer pairs for alignment detection. Multimath (Peng et al., 2024) collects high-quality mathematical reasoning data from K-12 textbooks and uses GPT-4o for CoT data generation and validation. R-CoT (Deng et al., 2024) enhances problem diversity through two-stage reverse question-answer generation process. Methods based on data synthesis are favored by academia and industry due to their demonstrated efficiency (Sprague et al., 2024; Lu et al., 2023b; Huang et al., 2024; Fu et al., 2024a). Inference-time scaling in mathematical reasoning Recently, works exploring test-time scaling laws in LLMs reasoning have garnered attention (Zhang et al., 2024b; Gou et al., 2023; Gao et al., 2024; Lin et al., 2024; Zhang et al., 2024a; Kumar et al., 2024). These strategies attempt to identify correct reasoning trajectories from the diverse outputs of LLMs (Snell et al., 2024). However, such strategies remain under-explored in multimodal mathematical reasoning. Starting with self-consistency (Wang et al., 2022), the concept of scaling at test time is beginning to take shape. OpenAI has introduced the concept of verifier to supervise and select reasoning paths during test-time inference (Lightman et al., 2023). Math-shepherd (Wang et al., 2024b) evaluates intermediate steps based on their potential to lead to the correct answer. OmegaPRM (Luo et al., 2024a) constructs PRM training data and trains it with MCTS. However, in the context of multimodal mathematical reasoning, the lack of models with strong CoT reasoning capabilities has become significant bottleneck, coupled with limited exploration into the diversity of reward model training data construction for such scenarios. Figure 3: CoT augmentation and verifying for multimodal mathematical data from three type of sources using Gemini1.5-Flash-002. 3. Model Training Process In this section, we introduce the training process of URSA7B, as illustrated in Figure 5. In Section 3.1, we introduce the model architecture and the composition of the visionlanguage alignment data. In Section 3.2, we describe the data synthesis strategy for math SFT, named MMathCoT1M. In Section 3.3, we explain the data synthesis process for training PRM with DualMath-1.1M. 3.1. Vision-Language Alignment Traditional MLLMs, which are largely trained on tasks like OCR and document understanding for general purposes, are not well-suited for direct transfer to mathematical training (Wang et al., 2024a). In contrast, mathematicsspecific LLMs already possess substantial mathematical knowledge and foundational capabilities in mathematical CoT reasoning. Therefore, we choose composite architecture. Specifically, we use the same hybrid vision encoder as Deepseek-VL, which combines the SAM-B and SigLIP-L encoders (Lu et al., 2024). On the language model side, we utilize the advanced Qwen2.5-Math-7B-Instruct (Yang et al., 2024a). Following previous work, we use an MLP projector as an aligner between the vision encoder and the LLM (Liu et al., 2024d; Bai et al., 2023). We collect 960K vision-language alignment open-source data for training by combining and filtering open-source datasets, naming it URSA-alignment-960K. The details are listed in Figure 2. URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics to maximize the development of CoT reasoning-related analytical skills. SAn = G(PR; {xi, yi, ai}N2 i=1) (2) the data D3 = CoT-formatted This portion of {(xi, yi, ci)}N3 i=1 is sourced from Multimath300k-en (Peng et al., 2024), each consists of question xi, answer yi and CoT solution ci. We use prompt PF and to maintain consistency in the CoT output by simply modifying the templates to remove the original design that specified the knowledge points used before the reasoning steps. SC = G(PF ; {xi, yi, ci}N3 i=1) (3) MMathCoT-1M We continue to use for data quality validation, filtering out examples that show doubt about the standard answer or provide inconsistent results in the solution. Finally, we synthesize high-quality multimodal mathematical CoT reasoning dataset, MMathCoT-1M, as shown in Equation 4. DSF = {(xi, yi) SAo SAn SC}SAoSAnSC i= (4) We use instruction fine-tuning to train URSA-7B based on aligned model proposed in Section 3.1. Training loss is displayed in Equation 5. LSF = E(x,y)DSF (cid:88) t=1 logM(ytx, y:t) (5) We provide prompts for three types of data processing in Appendix C.1. 3.3. Dual-view Process Supervised Data Synthesis However, even though URSA-7B possesses strong CoT reasoning capabilities, it still has significant likelihood of producing incorrect reasoning trajectories. Inspired by the use of process supervision for test-time scaling in LLM reasoning (Zhang et al., 2024b; Setlur et al., 2024; Zelikman et al., 2022), we consider training multimodal process supervision model to extract high-quality CoT trajectories. In multimodal scenarios, we propose dual-view process reward data approach, focusing on both logically correct and visually accurate aspects. The former is derived from error localization in the incorrect reasoning trajectories of URSA-7B, while the latter comes from manually inserted image misinterpretations. Error Step Locating Engine Inspired by (Lightman et al., 2023; Luo et al., 2024a), the Monte Carlo Tree Search (MCTS) approach can help us identify erroneous steps in the trajectory of problem-solving. We use the binary labeling approach to mark erroneous steps, enabling automated generation of process reward signals. Figure 4: Demonstration of Misinterpretation Insertion Engine. 3.2. CoT Augmentation in Multimodal Mathematics To enhance the multimodal mathematical CoT reasoning capabilities of MLLMs, we collect and categorize existing open-source multimodal mathematical training data. Subsequently, we employ robust generator, Gemini-1.5Flash-002, denoted as to execute the CoT data synthesis strategy as illustrated in Figure 3. Gemini-1.5-Flash002 demonstrates performance in multimodal mathematical reasoning comparable to GPT-4o while being more costeffective (Team et al., 2023). Answer-only Answer-only data D1 = {(xi, yi)}N1 i=1 includes MathV360k (Shi et al., 2024), each contains question xi and ground-truth answer yi. However, answer-only training restricts the model from fully capturing the problemsolving process. This approach may lead the model to rely on memory-based reasoning, hindering it provide direct answers to more complex geometric reasoning problems through fast thinking (Trinh et al., 2024; Li et al., 2024b). For this type of data, we use for CoT path distillation. Specifically, given CoT distillation prompt PC, we provide the problem and corresponding standard answer, then prompt the model to output the reasoning trajectory leading to the answer. We filter out responses that refuse to answer or indicate that more conditions are needed. SAo = G(PC; {xi, yi}N1 i=1) (1) of category Analysis-formatted This data D2 = {(xi, yi, ai)}N2 i=1 includes MAVIS-Geo, MAVISMetaGen (Zhang et al., 2024d), VarsityTutors (Zhuang et al., 2024), and Geo170k-QA, each sample contains question xi, answer yi and texual analysis ai. Given rewriting prompt PR, we utilize to transcribe solutions, enhancing step-by-step reasoning trajectories and linguistic diversity URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 5: An illustration of the training process for URSA-7B and URSA-RM-7B, with data from the three stages coming from URSA-alignment-960K, MMathCoT-1M, and DualMath-1.1M, respectively. The modules that are frozen and those that need to be trained are distinguished in each stage. Algorithm 1 BinaryErrorLocating set Yn, we execute the BinaryErrorLocating operation. Input: Negative example set Yn = {yn mid step sampling num Nmid. ErrorLocatingSet [] for = 1 to do = 1, 2, ..., }, 0, StepLen(yn ) while <r do mid (l + r) // 2 MID RES Generate([yn if True not in Verify(MID RES) then i,1, yn i,2, ..., yn i,mid], Nmid) mid else mid + end if end while WrongStepLabeling(yn en ErrorLocatingSet.append(en ) , l) end for Output: ErrorLocatingSet Given positive and negative solution pairs {yp }N i=1, We obtain the positive example set Yp = {yp = 1, 2, ..., } and negative example set Yn = {yn = 1, 2, ..., }. For the positive example set Yp, we directly perform forward labeling on each step of all samples. For the negative example , yn SBEL = BinaryErrorLocating(Yn, Nmid) (6) In the experiment, we set Nmid = 16 in Equation 6. Misinterpretation Insertion Engine The reward signals generated by MCTS come from the logical correctness of the text modality, without giving special attention to the uniqueness of image and text fusion. Some previous works have also suggested that MLLMs exhibit considerable degree of misunderstanding of visual signals (Yan et al., 2024; Zheng et al., 2024) during reasoning. Therefore, we consider misinterpreting insertion technique to construct training data for visual information attention. As shown in Figure 4, the misinterpretation insertion engine involves three stages. In the first stage, the model extracts mathematical paradigm information from image. In the second stage, the model is prompted to identify potential misinterpretations within the correct solution. In the third stage, the model selects information to misinterpret, injects it, and continues the reasoning process to arrive at an incorrect trajectory. Finally, we obtain SM IE. By using two engines, we obtain 1.1M process-supervised training data named DualMath-1.1M. We continue training on URSA-7B with binary classification loss shown in URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Table 1: Comparison with closed-source MLLMs and open-source MLLMs on MATHVISTA testmini and MATHVERSE testmini. The best is bold, and the runner-up is underline. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively. Model Random Human GPT-4o (OpenAI, 2024) GPT-4V (OpenAI, 2023) Gemini-1.5-002-Flash (Team et al., 2023) Gemini-1.5-Pro (Team et al., 2023) Claude-3.5-Sonnet (Anthropic, 2024) Qwen-VL-Plus (Bai et al., 2023) mPLUG-Owl2-7B (Ye et al., 2023) MiniGPT4-7B (Zhu et al., 2023) LLaVA-1.5-13B (Liu et al., 2024b) SPHINX-V2-13B (Lin et al., 2023) LLaVA-NeXT-34B (Liu et al., 2024c) InternLM-XComposer2-VL (Dong et al., 2024a) Deepseek-VL (Lu et al., 2024) InternVL2-8B (Chen et al., 2024b) Qwen2-VL (Wang et al., 2024a) G-LLaVA-7B (Gao et al., 2023) Math-LLaVA-13B (Shi et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) Math-PUMA-DeepSeek-Math (Zhuang et al., 2024) MAVIS-7B (Zhang et al., 2024d) InfiMM-Math (Han et al., 2024) MultiMath-7B (Peng et al., 2024) URSA-7B over SOTA Open-Source Math MLLMs #Params MathVista MathVerse ALL GPS MWP FQA TQA VQA ALL TD TL TO VI VD VO Baselines 17.9 60.3 21.6 48.4 3.8 73. 18.2 59.7 19.6 63.2 26.3 55.9 12.4 64.9 12.4 71.2 12.4 70. 12.4 41.7 12.4 61.4 12.4 68.3 12.4 66.7 Closed-Source MLLMs 63.8 49.9 58.4 63.9 67.7 43. - 50.5 - - - 35.5 - 57.5 - - - 31.2 - 43.1 - - - 54.6 Open-Source General MLLMs 22.2 23.1 27.7 36.7 46.5 57.6 34.9 58.3 58.9 23.6 26.0 22.7 16.4 - 63.0 28.4 62.0 40. 10.2 13.4 18.9 23.1 - 73.7 55.9 59.1 64.0 22.7 18.6 23.8 54.6 - 55.0 26.8 58.7 69.1 Open-Source Math MLLMs 25.1 46.6 47.9 44.7 - - 50.0 59.8 +9.8 48.7 57.7 48.1 39.9 64.1 - 66. 79.3 +12.5 3.6 56.5 68.3 67.7 - - 61.8 75.3 +7.0 19.1 37.2 46.5 42.8 - - 40.1 44.6 -1.9 - 65.2 - - - 48. 27.2 30.4 43.0 41.8 - 56.3 32.9 61.4 60.1 25.0 51.3 46.2 42.4 - - 50.0 63.9 +12.6 - 38.0 - - - 51.4 27.9 30.2 30.2 43.0 - 39.7 34.6 49.7 58.1 28.7 33.5 30.2 31.3 - - 33. 40.2 +6.7 - 54.4 - 35.3 - 21.3 10.3 12.2 12.7 16.1 34.6 25.9 19.3 35.9 33.6 16.6 22.9 33.6 31.8 35.2 34.5 27.7 45.7 +10.5 - 63.1 - 39.8 - 26. 11.6 12.3 17.1 20.8 49.0 36.9 23.0 39.0 37.4 20.9 27.3 42.1 43.4 43.2 46.7 34.8 55.3 +8.6 - 56.6 - 34.7 - 21.2 11.4 12.9 12.0 14.1 37.6 28.3 23.2 33.8 33.5 20.7 24.9 35.0 35.4 37.2 32.4 30. 48.3 +11.1 - 60.3 - 44.5 - 25.2 13.8 13.4 22.6 14.0 30.1 42.5 23.1 36.0 35.0 21.1 27.0 39.8 47.5 - - 35.3 51.8 +4.3 - 51.4 - 32.0 - 18. 11.1 12.5 12.6 16.4 35.2 20.1 20.2 32.2 31.3 17.2 24.5 33.4 33.6 34.1 38.1 28.1 46.4 +8.3 - 32.8 - 36.8 - 19.1 9.4 14.8 12.7 15.6 28.9 24.4 18.4 30.9 30.3 14.6 21.7 31.6 31.6 29.7 32.4 25. 43.9 +11.5 - 50.3 - 33.3 21.8 8.0 8.7 9.0 16.2 22.4 19.8 11.8 27.7 28.1 9.4 16.1 26.0 14.7 31.8 15.8 15.0 28.6 -3. - - - - - - - - 7B 7B 13B 13B 34B 7B 7B 8B 7B 7B 13B 7B 7B 7B 7B 7B 7B - Equation 8 to obtain URSA-RM-7B. Mp is the trained URSA-RM-7B. DP RM = {(ei, yei) SBEL SM IE} (7) LP RM = E(e,y)DP RM (cid:88) (cid:104) j= yj log Mp(ej) (8) (cid:105) + (1 yj) log(1 Mp(ej)) 4. Experiment 4.1. Experimental Setup Benchmarks We use several benchmarks to evaluate the performance of URSA-7B. MathVista (Lu et al., 2023a) is popular dataset for multimodal reasoning. Mathverse (Zhang et al., 2025) focuses on mathematical skills, covering areas such as plane geometry, solid geometry, and functions. It tests the models reasoning abilities across different scenarios using both visual and language information. DYNAMATH (Zou et al., 2024) modifies problem elements to check the models reasoning robustness. WEMATH (Qiao et al., 2024) evaluates the end-to-end reason6 ing capability of MLLMs and is the first to do so. We test URSA-7Bs mathematical reasoning on these four datasets and assess URSA-RM-7Bs ability as verifier for inference scaling on MathVista and Mathverse. Details of the metrics for each dataset are in Appendix D. Baselines We select closed-source MLLMs, open-source general MLLMs, and open-source MLLMs specifically optimized for mathematical reasoning as baselines for comprehensive comparison. Specifically, for general MLLMs, we choose very powerful models like Qwen2-VL-7B and InterVL2-8B as baselines. For math MLLMs, we select most recent works such as Math-LLaVA, Math-PUMA, Multimath and InfiMM-Math. Implementation Details Our experiments are based on Python 3.10 and PyTorch 2.4.0+cu124. We use AdamW (Diederik, 2014) as the optimizer. For the three stages, the learning rates are set to 1e-4, 1e-5, and 5e-6, respectively, with warm-up ratio maintained at 0.02. The training epochs are 1, 2, and 2. We use Fully Shared Data Parallel (FSDP) as the distributed training framework. By default, our experiments are conducted using 32 NVIDIAURSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Table 2: The performance comparison with Closed-source MLLMs and Open-source MLLMs on four-dimensional metrics for WE-MATH testmini reasoning evaluation. The best results of Closed-source MLLMs are highlighted in green. The best and second-best results of Open-source MLLMs are highlighted in red and blue respectively. Model #Params Strict Loose AVG IK IG CM RM AVG IK IG CM RM Qwen-VL-Max (Bai et al., 2023) Gemini-1.5-Pro (Team et al., 2023) GPT-4V (OpenAI, 2023) GPT-4o (OpenAI, 2024) LLaVA-1.6 (Liu et al., 2024c) LLaVA-1.6 (Liu et al., 2024c) InternVL-Chat-V1.5 (Chen et al., 2024a) LLaVA-NeXT (Liu et al., 2024c) DeepSeek-VL (Lu et al., 2024) Phi3-Vision (Abdin et al., 2024) GLM-4V-9B (GLM et al., 2024) InternLM-XComposer2-VL (Dong et al., 2024a) InternVL2-8B (Chen et al., 2024b) Qwen2-VL (Wang et al., 2024a) G-LLaVA (Gao et al., 2023) Math-LLaVA (Shi et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) Math-PUMA-DeepSeek-Math-7B (Zhuang et al., 2024) InfiMM-Math (Han et al., 2024) URSA-7B over SOTA Open-Source Math MLLMs Closed-source MLLMs - - - - 10.5 26.4 31.1 42.9 65.1 42.9 39.8 31. 7.6 11.2 14.5 15.2 Open-source General MLLMs 7B 13B 26B 72B 7B 4.2B 9B 7B 8B 7B 3.3 5.2 12.7 13.4 6.3 10.6 14.9 12.7 26.6 25.6 78.3 69.1 56.4 58.9 69.1 58.9 53.0 56.4 45.5 47.1 2.5 3.2 10.5 7.1 4.6 9.0 9.5 10.5 13.5 14. Open-source Math MLLMs 13B 13B 7B 7B 7B 7B - 6.5 11.1 19.2 15.6 20.6 64.2 62.1 47.8 56.0 48.8 32.2 +11. 37.5 +10.3 4.6 3.6 13.7 7.2 12.2 10.7 -7.1 6.7 20.8 23.8 35.2 2.1 3.6 7.4 9.9 4.0 6.1 10.1 7.4 19.8 18.3 4.2 9.3 12.4 12.0 15. 75.5 54.8 47.9 34.2 89.1 86.9 77.6 71.0 84.8 81.1 73.1 77.6 51.6 52.2 86.6 72.8 67.8 67.4 61.7 25.5 46.0 51.4 60.6 13.8 22.0 31.0 31.5 21.0 29.8 35.1 31.0 44.9 43.0 22.3 31.3 41.0 35.8 - 65.1 42.9 39.8 31.2 78.3 69.1 56.4 58.9 69.1 58.9 53.0 56.4 45.5 47.1 64.2 62.1 47.8 56.0 - 26.9 +11.7 48.2 +13.5 53.5 +12. 37.5 +10.3 7.6 11.2 14.5 15.2 2.5 3.2 10.5 7.1 4.6 9.0 9.5 10.5 13.5 14.7 4.6 3.6 13.7 7.2 - 10.7 -7.1 21.7 40.4 44.2 53. 12.6 20.4 25.7 28.0 18.7 25.3 30.3 25.7 38.1 35.6 20.0 29.5 34.1 32.2 - 48.2 +14.1 20.3 12.0 3.3 1.1 34.7 26.2 22.4 17.9 29.0 21.3 19.3 22.4 7.0 7.0 36.0 13.9 11.4 12.4 - 7.0 +4.4 Table 3: Comparison with open-source MLLMs on DYNAMATH testmini dataset. Model ALL PG SG AG AL PT GT AR Closed-source MLLMs GPT-4o Claude-3.5-Sonnet Geimini-1.5-Pro 63.7 64.8 60.5 56.8 49.9 52.7 52.0 49.3 42.7 61.0 55.3 61.6 76.9 81.0 70. 51.8 44.1 20.6 58.1 69.4 65.2 61.5 61.2 54.2 Open-source MLLMs Llava-v1.5-7B Llava-v1.6-34B Deepseek-VL-7B-chat InternVL2-8B Qwen2-VL URSA-7B 16.6 27.1 21.5 39.7 42.1 44.7 10.5 21.4 16.0 33.9 40.3 48.1 7.3 25.3 13.3 37.3 38.7 38. 19.5 27.6 26.5 32.5 39.9 33.7 6.5 14.9 12.9 46.9 37.1 66.9 8.2 7.6 4.7 15.9 8.2 24. 32.3 32.7 32.3 42.1 44.8 39.2 10.8 23.1 12.7 37.3 39.2 38.5 H100-HBM3 devices. more detailed experiment setup and time costs are shown in Appendix A. 4.2. Main Results Results of URSA-7B We test the URSA-7B model on MathVerse, MathVista, and DYNAMATH, achieving SOTA performance across multiple metrics among open-source MLLM. As demonstrated in Table 1, we find that URSA-7B performs exceptionally well in both overall performance and math-related tasks on MathVista. Our overall perfor7 mance surpasses that of powerful open-source models such as Qwen2-VL and InternVL2-8B. Specifically, we exceed the second place by 12.5 and 7.0 percentage points in the math-related tasks GPS and MWP, respectively. Similarly, we also perform well on TQA. However, due to lack of special focus on FQA and VQA, URSA-7B, like other math MLLMs, performs poorly on FQA and VQA. MathVerse is benchmark more focused on verifying mathematical capabilities compared to MathVista, and previous math MLLMs did not show significant gap in overall performance compared to open-source general MLLMs. However, URSA-7B achieves superior results, surpassing the runner-up by 10.5 percentage points in overall performance. URSA-7B only slightly underperforms in Vision-Only tasks compared to MAVIS-7B, which has been enhanced on the visual encoder but surpasses current open-source MLLMs in other types of questions. In the Text-Dominant and Vision-Dominant subsets, compared to the existing SOTA model Math-PUMA among open-source math MLLMs, URSA-7B achieves percentage improvements of 27.4% and 38.9%, respectively. This indicates that the benefits brought by CoT reasoning enhancement are universal and do not require trade-off due to imbalances in different modality information. The results of URSA-7B on the WE-MATH dataset are shown in Table 2. Among the performances of closed-source URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Table 4: OOD performance when URSA-RM-7B works on Multimath-7B. Method Sampling Number N=4 N=8 N=16 N=32 N=64 MathVista-GPS Self-Consistency URSA-RM-7B 67.4 68. 67.9 69.7 68.2 70.4 68.7 70.7 68.9 70.8 MathVerse Self-Consistency URSA-RM-7B 29.1 31.0 29.7 32.7 30.1 33.0 30.2 33.2 30.2 33.0 ever, the issue of the verifiers role becoming saturated as increases still exists. We also compare the pass@N of Multimath-7B and find that URSA-7B has significantly higher upper limit in reasoning performance than Multimath-7B. On MathVerse, the pass@64 of URSA-7B shows relative improvement of 74.2% compared to single inference, while Multimath-7B only shows an improvement of 58.5%. This further demonstrates the role of training with high-quality CoT reasoning data in enhancing reasoning capabilities and raising the upper limit of test-time scaling benefits. Additionally, we test whether URSA-RM-7B can fulfill the role of an OOD verifier. We choose Multimath as the base model, as it is relatively stable in performing CoT solutions after mathematics instruction tuning. As shown in the results of Table 4, we find that URSA-RM-7B effectively serves as verifier for Multimaths CoT solutions, surpassing the performance of self-consistency. 5. Ablations 5.1. Ablation on CoT Data Synthesis In this section, we conduct ablation experiments on the CoT augmentation strategy. The results in Figure 7 and Figure 8 demonstrate that data enhanced by CoT augmentation better assist the model in reasoning. Specifically, the tasks most affected are GPS and MWP, which are the most mathematically related. This indicates that CoT reasoning capabilities play crucial role in the mathematical understanding and reasoning processes of MLLMs. Additionally, unrefined mixed data can cause the model to inconsistently choose between direct and CoT format reasoning, which somewhat contradicts the logical nature of System-2 reasoning. 5.2. Ablation on PRM Training Data Synthesis We conduct an ablation study on synthetic data for URSARM-7B to verify the effectiveness of the misinterpretation insertion engine. We validate this on MathVerse and Figure 6: Pass@N and Best-of-N results comparison on MathVerse and MathVista-GPS. models, URSA-7B has surpassed GPT-4V in average performance, IK, and CM, and has almost entirely outperformed Gemini-1.5-Pro, with only some gap remaining compared to GPT-4o. When compared to open-source general MLLMs and open-source math MLLMs, URSA-7B has almost completely surpassed the current SOTA models Qwen2-VL and Math-PUMA in both strict and loose settings, showing significant advantage. In addition, we use DYNAMATH to help verify the robustness of URSA-7Bs reasoning capabilities. As shown in Table 3, URSA-7B demonstrates superior performance on DYNAMATH compared to Qwen2-VL and InternVL2-8B, showing significant advantages in the Plane Geometry and Algebra subsets. This further confirms the benefits of enhancing the models CoT reasoning capabilities in terms of generalization. Test-time scaling In Figure 6, we show the pass@N performance of URSA-7B. With only 4 time sampling, URSA7B can achieve pass rate of 90.9 on the GPS task of Mathvista. The pass@64 performance on MathVista is 97.1, indicating that URSA-7B already has substantial coverage of geometric problem-solving strategies. On the MathVerse dataset, URSA-7B also demonstrates high upper limit of reasoning capability. Furthermore, when paired with URSARM-7B as verifier, the accuracy significantly surpasses self-consistency, making it better test-time scaling reward model tool for multimodal mathematical reasoning. HowURSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics fine-tuning to test-time scaling in the multimodal mathematics field, providing strong foundation and reward model for future work. We can further explore applications based on critic or reinforcement learning techniques on this basis."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. An, S., Zhou, B., Lin, Z., Fu, Q., Chen, B., Zheng, N., Chen, W., and Lou, J.-G. Skill-based few-shot selection for incontext learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1347213492, 2023. The claude 3 model Anthropic. sonnet, claude-3-model-card, 2024. Claude-3 Model Card. family: Opus, https://www.anthropic.com/ haiku. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2): 3, 2023. Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143, 2012. Cai, S., Bao, K., Guo, H., Zhang, J., Song, J., and Zheng, B. Geogpt4v: Towards geometric multi-modal large language models with geometric image generation. arXiv preprint arXiv:2406.11503, 2024. Chen, J., Tang, J., Qin, J., Liang, X., Liu, L., Xing, E., and Lin, L. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 513523, 2021. Chen, J., Li, D. Z. X. S. X., Zhang, Z. L. P., Xiong, R. K. V. C. Y., and Elhoseiny, M. Minigpt-v2: Large language model as unified interface for vision-language multitask learning. arXiv preprint arXiv:2310.09478, 2023. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024a. 9 Figure 7: Ablation Study w.r.t CoT Augmentation during Math SFT Stage on the MathVista testmini Set. Table 5: Ablation study on URSA-RM-7B. Method Sampling Number N=4 N=8 N=16 N=32 N= MathVista-GPS URSA-RM-7B URSA-RM-7B w/o SBEL URSA-RM-7B w/o SM IE 82.6 80.1 81.8 84.0 81.7 83.3 MathVerse URSA-RM-7B URSA-RM-7B w/o SBEL URSA-RM-7B w/o SM IE 53.2 49.9 52.8 54.2 50.7 53.7 85.0 82.2 84.1 54.7 51.8 53.8 86.4 83.1 85.6 55.0 52.0 53. 86.2 83.0 85.6 54.8 52.1 53.8 MathVista-GPS, and the results are shown in Table 1. The s1 configuration generally ensures stable improvement in the test-time performance of URSA-7B, confirming that enhancing the accuracy of the best-of-N verification process from the perspective of image-text matching is an effective strategy. 6. Conclusion In this work, we focus on enhancing the mathematical reasoning abilities of MLLMs and propose training streamline from high-quality CoT fine-tuning to multimodal math test-time scaling. During the instruction fine-tuning phase, we address the gap in existing multimodal mathematical open-source datasets concerning high-quality reasoning processes. Through CoT distillation, rewriting, and style unifying, we contribute high-quality mathematics CoT training dataset, URSA-1M. With only 1M-size dataset, we train the open-source URSA-7B model, which achieves several SOTA results on major benchmarks. Furthermore, we evolve from improving CoT reasoning capabilities to enhancing CoT supervision capabilities by proposing dualview supervised synthetic data and training URSA-RM-7B. As verifier, it effectively boosts URSA-7Bs reasoning abilities and demonstrates its capabilities in OOD scenarios. This work fills the gap from high-quality CoT instruction URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. Chu, Z., Chen, J., Chen, Q., Yu, W., He, T., Wang, H., Peng, W., Liu, M., Qin, B., and Liu, T. survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402, 2023. Deng, L., Liu, Y., Li, B., Luo, D., Wu, L., Zhang, C., Lyu, P., Zhang, Z., Zhang, G., Ding, E., et al. R-cot: Reverse chain-of-thought problem generation for geometric reasoning in large multimodal models. arXiv preprint arXiv:2410.17885, 2024. Diederik, P. K. Adam: method for stochastic optimization. (No Title), 2014. Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., et al. Internlmxcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024a. Dong, Y., Liu, Z., Sun, H.-L., Yang, J., Hu, W., Rao, Y., and Liu, Z. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024b. Fu, C., Lin, H., Long, Z., Shen, Y., Zhao, M., Zhang, Y., Dong, S., Wang, X., Yin, D., Ma, L., et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024a. Fu, D., Xiao, T., Wang, R., Zhu, W., Zhang, P., Pang, G., Jia, R., and Chen, L. Tldr: Token-level detective reward model for large vision language models. arXiv preprint arXiv:2410.04734, 2024b. Gao, B., Cai, Z., Xu, R., Wang, P., Zheng, C., Lin, R., Lu, K., Lin, J., Zhou, C., Xiao, W., et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. CoRR, 2024. Gao, J., Pi, R., Zhang, J., Ye, J., Zhong, W., Wang, Y., Hong, L., Han, J., Xu, H., Li, Z., et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. GLM, T., Zeng, A., Xu, B., Wang, B., Zhang, C., Yin, D., Rojas, D., Feng, G., Zhao, H., Lai, H., Yu, H., Wang, H., Sun, J., Zhang, J., Cheng, J., Gui, J., Tang, J., Zhang, J., Li, J., Zhao, L., Wu, L., Zhong, L., Liu, M., Huang, M., Zhang, P., Zheng, Q., Lu, R., Duan, S., Zhang, S., Cao, S., Yang, S., Tam, W. L., Zhao, W., Liu, X., Xia, X., Zhang, X., Gu, X., Lv, X., Liu, X., Liu, X., Yang, X., Song, X., Zhang, X., An, Y., Xu, Y., Niu, Y., Yang, Y., Li, Y., Bai, Y., Dong, Y., Qi, Z., Wang, Z., Yang, Z., Du, Z., Hou, Z., and Wang, Z. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Gou, Z., Shao, Z., Gong, Y., Shen, Y., Yang, Y., Duan, N., and Chen, W. Critic: Large language models can selfcorrect with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. Guo, J., Zheng, T., Bai, Y., Li, B., Wang, Y., Zhu, K., Li, Y., Neubig, G., Chen, W., and Yue, X. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. Han, X., Jian, Y., Hu, X., Liu, H., Wang, Y., Fan, Q., Ai, Y., Huang, H., He, R., Yang, Z., et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as visual chain of thought arXiv preprint for multimodal language models. arXiv:2406.09403, 2024. Huang, Y., Liu, X., Gong, Y., Gou, Z., Shen, Y., Duan, N., and Chen, W. Key-point-driven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Li, Z., Du, Y., Liu, Y., Zhang, Y., Liu, Y., Zhang, M., and Cai, X. Eagle: Elevating geometric reasoning through llm-empowered visual instruction tuning. arXiv preprint arXiv:2408.11397, 2024b. 10 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Liang, Z., Yang, T., Zhang, J., and Zhang, X. Unimath: foundational and multimodal mathematical reasoner. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 71267133, 2023. Liang, Z., Liu, Y., Niu, T., Zhang, X., Zhou, Y., and Yavuz, S. Improving llm reasoning through scaling inference computation with collaborative verification. arXiv preprint arXiv:2410.05318, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. Lin, Z., Gou, Z., Liang, T., Luo, R., Liu, H., and Yang, Y. CriticBench: Benchmarking LLMs for critique-correct reasoning. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 15521587, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.91. URL https: //aclanthology.org/2024.findings-acl.91. Liu, D., Zhang, R., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., Zhang, K., et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024a. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024b. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge, 2024c. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024d. Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Yang, H., et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023a. Lu, Y., Shen, M., Wang, H., Wang, X., van Rechem, C., Fu, T., and Wei, W. Machine learning for synthetic data generation: review. arXiv preprint arXiv:2302.04062, 2023b. Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Luo, L., Liu, Y., Liu, R., Phatale, S., Lara, H., Li, Y., Shu, L., Zhu, Y., Meng, L., Sun, J., et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024a. Luo, R., Wang, L., Lin, B., Lin, Z., and Yang, Y. Ptdsql: Partitioning and targeted drilling with llms in textto-sql. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 37673799, 2024b. Mu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao, Y., and Luo, P. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024. OpenAI. GPT-4V(ision) system card, 2023. URL https: //openai.com/research/gpt-4v-system-card. OpenAI. GPT-4o system card, 2024. URL https:// openai.com/research/gpt-4o-system-card. Peng, S., Fu, D., Gao, L., Zhong, X., Fu, H., and Tang, Z. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. Qiao, R., Tan, Q., Dong, G., Wu, M., Sun, C., Song, X., GongQue, Z., Lei, S., Wei, Z., Zhang, M., et al. We-math: Does your large multimodal model achieve arXiv preprint human-like mathematical reasoning? arXiv:2407.01284, 2024. Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J., Agarwal, R., Agarwal, A., Berant, J., and Kumar, A. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. Shi, C., Su, Y., Yang, C., Yang, Y., and Cai, D. Specialist or generalist? instruction tuning for specific nlp tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 15336 15348, 2023. 11 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Shi, W., Hu, Z., Bin, Y., Liu, J., Yang, Y., Ng, S.-K., Bing, L., and Lee, R. K.-W. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Sprague, Z., Yin, F., Rodriguez, J. D., Jiang, D., Wadhwa, M., Singhal, P., Zhao, X., Ye, X., Mahowald, K., and Durrett, G. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024b. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yan, Y., Wang, S., Huo, J., Li, H., Li, B., Su, J., Gao, X., Zhang, Y.-F., Xu, T., Chu, Z., et al. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. arXiv preprint arXiv:2410.04509, 2024. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024a. Yang, Z., Chen, J., Du, Z., Yu, W., Wang, W., Hong, W., Jiang, Z., Xu, B., Dong, Y., and Tang, J. Mathglmvision: Solving mathematical problems with multi-modal large language model. arXiv preprint arXiv:2409.13729, 2024b. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zhang, D., Lei, J., Li, J., Wang, X., Liu, Y., Yang, Z., Li, J., Wang, W., Yang, S., Wu, J., et al. Critic-v: Vlm critics help catch vlm errors in multimodal reasoning. arXiv preprint arXiv:2411.18203, 2024a. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024b. Zhang, P., Zhang, K., Li, B., Zeng, G., Yang, J., Zhang, Y., Wang, Z., Tan, H., Li, C., and Liu, Z. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024c. URL https://arxiv.org/ abs/2406.16852. Zhang, R., Wei, X., Jiang, D., Guo, Z., Li, S., Zhang, Y., Tong, C., Liu, J., Zhou, A., Wei, B., et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024d. Zhang, R., Zhang, B., Li, Y., Zhang, H., Sun, Z., Gan, Z., Yang, Y., Pang, R., and Yang, Y. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024e. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2025. 12 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Zheng, H., Xu, T., Sun, H., Pu, S., Chen, R., and Sun, L. Thinking before looking: Improving multimodal llm reasoning via mitigating visual hallucination. arXiv preprint arXiv:2411.12591, 2024. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Zhuang, W., Huang, X., Zhang, X., and Zeng, J. Math-puma: Progressive upward multimodal alignment arXiv preprint to enhance mathematical reasoning. arXiv:2408.08640, 2024. Zou, C., Guo, X., Yang, R., Zhang, J., Hu, B., and Zhang, H. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 13 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics A. Hyperparameter and Time cost In this section, we provide the specific parameter settings and time costs for the three stages. Unless otherwise specified, experiments are conducted on 32 NVIDIA-H100-HBM3 GPUs by default. Additionally, we provide important parameters used in data construction. During the generation of positive and negative example pairs, we set the temperature to 1.0, return sequences to 16, and top to 0.95. In the BinaryErrorLocating phase, we set the temperature to 0.3, return sequences to 16, and top to 0.95. We adapt the VLLM (Kwon et al., 2023) framework for the URSA-7Bs architecture (hybrid vision tower + MLP + Qwen2.5-math-Instruct is not original supported by VLLM) and use it as an acceleration tool during the inference phase. During the data pair generation phase, we use 16 NVIDIA-H100-HBM3 GPUs for inference, which takes approximately 28 hours. In the BinaryErrorLocating phase, we also use 16 NVIDIA-H100-HBM3 GPUs for inference, taking about 20 hours. Table 6: Hyperparameter setting and training time cost. Hyperparameters & Cost Stage 1 Stage Stage 3 1e-4 1 0.02 0.02 64 1e-5 2 0.02 0.01 128 5e-6 2 0.02 0.02 128 Aligner Vision Encoder, Aligner, Base LLM Base LLM 960K 3.5h 1.0M 11h 1.1M 12h Learning Rate Epoch Warm-up Ratio Weight Decay Batch Size Trainable Parts Data Size Time Cost B. Supplementary Results B.1. Supplementary Performance We provide detailed test results based on various mathematical abilities on MathVista. The results in Figure 7 show that URSA-7Bs greatest strengths are in algebra and geometry, while open-source general MLLMs perform better on scientific and statistical problems. When compared with open-source math MLLMs, URSA-7B maintains consistently leading level except in logical problems. We also include the accuracy results on the WE-MATH dataset in Table 8. In terms of accuracy across the three steps, URSA-7B outperforms all open-source general and math MLLMs. Looking at the four columns of results for PF and SF, URSA continues to excel in the geometry-related areas of plane and solid figures. In CCF and CCP, URSA-7B achieves the second-best results, only behind InternVL-8B and Qwen2-VL, respectively. We find that open-source math MLLMs perform weaker than general MLLMs on position-related topics, which may be due to insufficient training on data such as spatial entity images. As for DYNAMATH, it conducts sub-test criterion with knowledge level grading, including elementary school, high school, and undergraduate. The results in Table 9 present an interesting observation. Firstly, URSA-7B shows general advantage among open-source models, especially at the highest difficulty level, undergraduate. Secondly, although there is still significant gap compared to GPT-4o and Gemini-1.5-Pro, URSA-7B demonstrates certain degree of leading performance at the undergraduate difficulty level. In other words, as the level of knowledge increases, the decline in accuracy for URSA-7B is not as steep, which is positive sign indicating that CoT training enhances the models reasoning capabilities. However, the accuracy on lower-level questions has not reached an ideal state, which may be due to the dominance of errors in hard skills such as visual recognition and calculation. To specifically evaluate the models performance on geometry-related problems, we additionally report the performance of URSA-7B on GeoQA (Chen et al., 2021), which is benchmark specifically designed for geometry problems. As shown in Figure 10, URSA-7B surpasses MAVIS-7B by 5.2 percentages on the GeoQA test set, demonstrating URSA-7Bs expert-level advantage in the field of geometry. 14 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 8: Ablation Study w.r.t CoT Augmentation during Math SFT Stage on the MathVerse benchmark. B.2. Supplementary Ablation Study We provide ablation study of URSA-7B on MathVerse in Figure 8. Table 7: Comparison with close-source MLLMs open-source MLLMs on MATHVISTA testmini mathematics capabilities. Model #Params ALL ALG ARI GEO LOG NUM SCI STA Baselines Random Choice Human Performance - - 17.9 60.3 25.8 50.9 13.8 59.2 22.7 51. 13.4 40.7 8.8 53.8 15.8 64.9 14.3 63.9 Closed-source MLLMs Qwen-VL-Plus (Bai et al., 2023) GPT-4V (OpenAI, 2023) - - 43.3 49.9 39.1 53.0 32.0 49.0 39.3 51.0 18.9 21. 26.4 20.1 59.0 63.1 56.1 55.8 Open-source Genreral MLLMs mPLUG-Owl2-7B (Ye et al., 2023) LLaVA-1.5-13B (Liu et al., 2024c) MiniGPT-v2 (Chen et al., 2023) InternLM-XComposer2-VL-7B (Dong et al., 2024a) SPHINX-MoE (Lin et al., 2023) DeepSeek-VL (Lu et al., 2024) InternVL2-8B (Chen et al., 2024b) Qwen2-VL (Wang et al., 2024a) 7B 13B 7B 7B 8 7B 7B 8B 7B 22.2 25.7 23.1 47.8 42.3 34.9 58.3 58.9 23.6 19.6 28.1 32.0 31.7 29.2 59.8 44.1 G-LLaVA (Gao et al., 2024) Math-LLaVA (Shi et al., 2024) Multimath-7B (Peng et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) URSA-7B over SOTA Open-Source Math MLLMs Open-source Math MLLMs 7B 7B 7B 7B 7B - 25.1 46.6 50.0 47.9 59.8 +9.8 36.0 51.5 61.9 47.7 74.0 +12.1 19.2 28.6 21.0 51.6 41.6 38.8 56.4 57. 19.4 40.7 42.2 46.2 53.5 +7.3 23.9 17.6 24.7 30.5 30.5 27.2 60.3 43.1 37.6 56.2 64.9 47.3 77.4 +12.5 13.5 10.8 16.2 13.5 16.2 18.9 10.8 24. 15.2 23.3 23.3 21.6 21.6 -1.7 12.7 27.8 16.7 43.8 27.1 43.1 30.6 41.7 17.7 34.7 32.6 32.6 35.4 +0.7 26.3 33.6 25.4 37.7 50.8 35.3 59.0 66. 21.0 47.7 42.6 42.6 58.2 +10.5 21.4 22.9 17.9 62.8 50.8 33.2 68.8 75.1 15.1 42.3 49.2 55.8 57.1 +1.3 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Table 8: Accuracy comparison with close-source MLLMs and open-source MLLMs on WE-MATH testmini subset. First 3 columns show the overall performance on one-step, two-step and three-step problems. The other columns are used to demonstrate the performance in different problem strategies. Model #Params S1 S3 Mem PF SF TMF PD UCU AL CPF UPF CSF USF BTF CCF Dir Pos RoM CCP Closed-source MLLMs 58.1 49.2 51.4 30. 43.6 38.2 33.9 20.6 86.6 82.5 51.0 19.4 39.1 38.4 31.2 25.3 Open-source General MLLMs 30.6 20.8 25.3 37.2 31.1 30.6 33.1 34.2 26.7 43.6 43.6 28.5 15.8 32.7 38.2 29.7 28.5 33.3 27.9 25.5 35.2 26. 44.0 18.5 21.7 53.4 28.6 24.5 31.3 28.7 16.6 71.4 62.7 29.8 20.5 23.2 37.0 37.0 39.8 46.5 16.0 35.1 20.5 37.2 Open-source Math MLLMs 30.6 34.2 39.4 37.1 37.9 56.4 +27.0 32.7 34.6 36.4 33.2 34. 41.8 +5.4 33.3 30.3 63.5 - - 59.1 -4.4 29.1 17.9 42.5 - - 77.4 70.7 61.8 39.8 52.2 16.9 23.4 51.3 40.8 45.1 47.7 47.2 27.3 62.0 62. 32.0 39.2 60.2 - - 71.6 60.2 45.0 41.4 52.1 29.6 34.7 46.5 39.8 40.8 42.6 38.8 38.0 55.5 60.8 37.9 40.4 45.9 - - 32.5 -10.0 72.3 +12. 60.3 +14.4 84.5 76.6 70.0 43.6 44.2 15.6 25.3 50.6 41.0 51.9 51.4 50.0 24.2 67.1 65.7 19.6 37.1 66.2 - - 70.9 +4.7 62.3 56.3 57.5 48. 48.2 18.6 26.4 38.2 38.6 42.5 43.9 44.4 38.7 57.3 49.2 33.5 37.7 48.6 - - 66.0 +17.4 58.7 57.8 39.2 43.8 47.1 42.7 37.5 44.1 32.0 45.6 41.1 28.8 50.0 54.0 52.5 37.1 53.0 42.3 - - 51.4 -1.6 69.4 67.7 62.7 43.4 46.8 24.1 41.7 45.2 42.7 44.6 50.6 31.2 23.3 60.5 49.2 32.8 51.3 53.5 - - 59.8 +6.3 93.1 79.3 68.8 41. 65.7 17.6 26.9 41.0 41.0 44.5 65.5 48.6 24.5 58.6 48.1 31.2 30.8 31.2 - - 58.3 +27.1 72.7 57.5 54.1 35.1 50.5 43.3 28.9 49.3 42.7 40.7 53.9 49.2 41.0 63.6 68.2 33.2 30.8 37.7 - - 39.5 +1.8 47.5 47.8 40.7 40.7 36.5 28.9 37.1 36.8 44.0 47.5 55.2 26.4 51.7 44.5 55.0 25.6 40.9 40.4 - - 58.8 +17.9 73.3 63.3 60.0 26. 36.7 26.7 30.0 53.3 43.3 20.0 40.0 50.0 23.3 50.0 56.7 40.0 46.7 46.7 - - 53.3 +6.6 GPT-4o (OpenAI, 2024) GPT-4V (OpenAI, 2023) Gemini-1.5-Pro (Team et al., 2023) Qwen-VL-Max (Bai et al., 2023) InternVL-Chat-V1.5 (Chen et al., 2024a) LLaVA-1.6 (Liu et al., 2024c) LLaVA-1.6 (Liu et al., 2024c) GLM-4V-9B (GLM et al., 2024) MiniCPM-LLaMA3-V2.5 (Yao et al., 2024) LongVA (Zhang et al., 2024c) InternLM-XComposer2-VL (Dong et al., 2024a) Phi3-Vision (Abdin et al., 2024) DeepSeek-VL (Lu et al., 2024) InternVL2-8B (Chen et al., 2024b) Qwen2-VL (Wang et al., 2024a) G-LLaVA (Gao et al., 2023) Math-LLaVA (Shi et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) MAVIS w/o DPO (Zhang et al., 2024d) MAVIS (Zhang et al., 2024d) URSA-7B over SOTA Open-Source Math MLLMs - - - - 26B 7B 13B 9B 8B 7B 7B 4.2B 7B 8B 7B 7B 13B 7B 7B 7B 7B - 72.8 65.5 56.1 40. 49.4 23.0 29.4 47.3 39.8 43.5 47.0 42.1 32.6 59.4 59.1 32.4 38.7 53.3 56.9 57.2 63.1 +5.9 C. Prompt C.1. Prompt Used in SFT data construction In this section, we provide demonstration of the prompt used for MMathCoT-1M construction. The prompts used on analysis-formatted, answer-only and CoT-formatted source data are illustrated in Figure 9, 10, and 11. The response checking prompt is illustrated in Figure 12. C.2. Prompt Used in Misinterpretation Insertion Engine In this section, we provide the prompts used in the interpretation insertion engine for geometric problems and function or statistics-related problems in Figure 13 and Figure 14, respectively. Table 9: Comparison with close-source MLLMs open-source MLLMs on DYNAMATH testmini based on knowledge level. Model #Params ALL Elementary School High School Undergraduate Closed-source MLLMs GPT-4o (OpenAI, 2024) Claude-3.5-Sonnet (Anthropic, 2024) Gemini-1.5-Pro (Team et al., 2023) - - - 63.7 64.8 60.5 Open-sourced MLLMs Llava-v1.5-7B (Liu et al., 2024c) Llava-v1.6-34B (Liu et al., 2024c) Deepseek-VL-7B-Chat (Lu et al., 2024) InternVL2-8B (Chen et al., 2024b) Qwen2-VL (Wang et al., 2024a) URSA-7B 7B 34B 7B 8B 7B 7B 16.6 27.1 21.5 39.7 42.1 44.7 68.6 66.7 62.9 18.9 35.9 28.3 51.1 47.6 53.5 61.8 62.6 59.2 13.3 23.8 19.0 37.4 42.2 44. 36.8 33.3 37.1 11.7 16.6 16.0 19.6 24.4 41.8 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Table 10: Performance comparison of different MLLMs on GeoQA."
        },
        {
            "title": "Baselines",
            "content": "Random Choice Human UniMath (Liang et al., 2023) GPT-4V (OpenAI, 2023) Closed-source MLLMs Open-source MLLMs LLaVA-1.5 (Liu et al., 2024b) G-LLaVA (Gao et al., 2023) G-LLaVA (Gao et al., 2023) Math-PUMA-DeepSeek-Math-7B (Zhuang et al., 2024) Math-PUMA-Qwen2-7B (Zhuang et al., 2024) Multimath (Peng et al., 2024) MAVIS-7B w/o DPO (Zhang et al., 2024d) MAVIS-7B (Zhang et al., 2024d) URSA-7B over SOTA Open-Source MLLMs - - - - 13B 7B 13B 7B 7B 7B 7B 7B 7B - 17.1 92.3 50.0 45. 20.3 64.2 67.0 61.8 63.6 67.7 66.7 68.3 73.5 +5.2 D. Detailed description of Benchmarks In this section, we introduce the detailed subtasks and metrics of four used benchmarks to more precisely demonstrate the evaluation. MathVista MathVista (Lu et al., 2023a) comprises total of 5 subtasks: Geometry Problem Solving (GPS), Math Word Problem (MWP), Figure Question Answering (FQA), Textbook Question Answering (TQA) and Visual Question Answering (VQA). Additionally, MathVista evaluates the models performance in different mathematical thinking areas by testing the skills or capabilities required by various problems. Specifically, it includes Arithmetic (ARI), Geometry (GEO), Logical (LOG), Numeric (NUM), Scientific (SCI) and Statistical (STA). The performance comparison on capability is demonstrated in Table 7. MathVerse MathVerse (Zhang et al., 2025) is benchmark for testing the reasoning abilities of MLLMs when the information content in text and image modalities varies. Specifically, the models focus on performance in six scenarios: Text-Dominant (TD), Text-Lite (TL), Text-Only (TO), Vision-Intensive (VI), Vision-Dominant (VD) and Vision-Only (VO). WE-MATH WE-MATH (Qiao et al., 2024) is the first benchmark that decompose composite problems into sub-problems according to the required knowledge concepts. For reasoning evaluation, it designs four metrics to respectively showcase different issues in MLLMs mathematical reasoning, including Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM) and Rote Memorization (RM). Additionally, WE-MATH also conducts evaluations based on different types and steps of reasoning. In figure 7, the actual content corresponding to the abbreviations is as follows. Mem: Measurement, PF: Plane Figures, SF: Solid Figures, TMF: Transformations and Motion of Figures, PD: Position and Direction, AL: Angles and Length, UCU: Understanding and Conversion of Units, CPF: Calculation of Plane Figures, UPF: Understanding of Plane Figures, CSF: Calculation of Solid Figures, USF: Understanding of Solid Figures, BTF: Basic Transformations of Figures, CCF: Cutting and Combining of Figures, Dir: Direction, Pos: Position, RoM: Route Map, CCP: Correspondence of Coordinates and Positions. correspond. 17 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 9: Prompt PC used for CoT distillation on answer-only source data. URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 10: Prompt PR used for CoT solution rewriting on analysis-formatted source data. 19 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 11: Prompt PF used for unifying solution format across style-varied source data. URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 12: Prompt used for checking CoT augmentation response based on consistency and correction. 21 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 13: Prompt used for inserting interpretation into geometry-related samples. URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 14: Prompt used for inserting interpretation into function and statistics-related samples. 23 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics DYNAMATH DYNAMATH (Zou et al., 2024) is benchmark designed to evaluate the robustness of MLLMs in mathematical reasoning. Specifically, it includes tests across multiple dimensions, including Solid Geometry (SG), Plane Geometry (PG), Analytic Geometry (AG), Algebra (AL), Puzzle Test (PT), Graph Theory (GT), Arithmetic (AR), Scientific Figure (SF) and Statistics (ST). E. Case Study Some actual cases are presented in Figure 15 and Figure 16. An example demonstrating how URSA-RM-7B verifies step-level correctness is shown in Figure 17. 24 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 15: Case on MathVista-GPS. 25 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 16: Case on MathVerse. 26 URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics Figure 17: URSA-RM-7B works on URSA-7Bs sampling responses."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua University"
    ]
}