{
    "paper_title": "Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)",
    "authors": [
        "Nasrin Imanpour",
        "Shashwat Bajpai",
        "Subhankar Ghosh",
        "Sainath Reddy Sankepally",
        "Abhilekh Borah",
        "Hasnat Md Abdullah",
        "Nishoak Kosaraju",
        "Shreyas Dixit",
        "Ashhar Aziz",
        "Shwetangshu Biswas",
        "Vinija Jain",
        "Aman Chadha",
        "Amit Sheth",
        "Amitava Das"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection, Fake Image Detection, DIRE, LASTED, GAN Image Detection, AIDE, SSP, DRCT, RINE, OCC-CLIP, De-Fake, and Deep Fake Detection. However, we argue that the current state-of-the-art AGID techniques are inadequate for effectively detecting contemporary AI-generated images and advocate for a comprehensive reevaluation of these methods. We introduce the Visual Counter Turing Test (VCT^2), a benchmark comprising ~130K images generated by contemporary text-to-image models (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6). VCT^2 includes two sets of prompts sourced from tweets by the New York Times Twitter account and captions from the MS COCO dataset. We also evaluate the performance of the aforementioned AGID techniques on the VCT$^2$ benchmark, highlighting their ineffectiveness in detecting AI-generated images. As image-generative AI models continue to evolve, the need for a quantifiable framework to evaluate these models becomes increasingly critical. To meet this need, we propose the Visual AI Index (V_AI), which assesses generated images from various visual perspectives, including texture complexity and object coherence, setting a new standard for evaluating image-generative AI models. To foster research in this domain, we make our https://huggingface.co/datasets/anonymous1233/COCO_AI and https://huggingface.co/datasets/anonymous1233/twitter_AI datasets publicly available."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 4 5 7 6 1 . 1 1 4 2 : r Visual Counter Turing Test (VCT2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (VAI) Nasrin Imanpour1,, Shashwat Bajpai2,, Subhankar Ghosh3,, Sainath Reddy Sankepally4,, Abhilekh Borah5, Hasnat Md Abdullah6, Nishoak Kosaraju7, Shreyas Dixit8, Ashhar Aziz9, Shwetangshu Biswas10, Vinija Jain11, Aman Chadha12,13, Amit Sheth1, Amitava Das1 1University of South Carolina, USA 2BITS Pilani Hyderabad Campus, India 3Washington State University, USA 4International Institute of Information Technology, India 5Manipal University Jaipur, India 6Texas A&M University, USA 7Carnegie Mellon University, USA 8Vishwakarma Institute of Information Technology, India 9IIIT Delhi, India 10National Institute of Technology Silchar, India 11Amazon AI, USA 12Stanford University, USA 13Amazon GenAI, USA"
        },
        {
            "title": "Abstract",
            "content": "The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection, Fake Image Detection, DIRE, LASTED, GAN Image Detection, AIDE, SSP, DRCT, RINE, OCC-CLIP, DeFake, and Deep Fake Detection. However, we argue that the current state-of-the-art AGID techniques are inadequate for effectively detecting contemporary AI-generated images and advocate for comprehensive reevaluation of these methods. We introduce the Visual Counter Turing Test (VCT2), benchmark comprising 130K images generated by contemporary text-to-image models (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6). VCT2 includes two sets of prompts sourced from tweets by the New York Times Twitter account and captions from the MS COCO dataset. We also evaluate the performance of the aforementioned AGID techniques on the VCT2 benchmark, highlighting their ineffectiveness in detecting These authors contributed equally to this work. AI-generated images. As image-generative AI models continue to evolve, the need for quantifiable framework to evaluate these models becomes increasingly critical. To meet this need, we propose the Visual AI Index (VAI ), which assesses generated images from various visual perspectives, including texture complexity and object coherence, setting new standard for evaluating image-generative AI models. To foster research in this domain, we make our COCO and Twitter datasets publicly available. 1. Defending Against AI Apocalypse: The Urgency of Rediscovering Techniques for AIGenerated Image Detection The exponential growth of text-to-image generative AI models like Stable Diffusion(s) [13], DALL-E(s) [46], Midjourney [7], and Imagen [8] has revolutionized visual content creation, unlocking unprecedented creative potential. However, this rapid evolution and widespread accessibility presents significant challenges, particularly concerning the misuse of AI-generated images. In March 2023, an open letter [9] signed by numerous AI experts and industry leaders called for six-month halt on the development of AI systems tive AI-enabled disinformation. However, there are ongoing apprehensions about the susceptibility of these measures to deliberate tampering and the potential for malicious actors to bypass them entirely. In this paper, we offer comprehensive review of AGID techniques, emphasizing their limitations and exposing significant deficiencies in state-of-the-art (SoTA) methods. This paper serves as call to action for the scientific community to prioritize the development of more robust and effective AGID solutions. To this end, we introduce the Visual Counter Turing Test (VCT2), benchmark designed to evaluate the performance of AGID methods. VCT2 includes 130K images generated by SoTA text-to-image generative models (Stable Diffusion 2.1 [1], Stable Diffusion XL [2], Stable Diffusion 3 [3], DALL-E 3 [6], and Midjourney 6 [7]). The benchmark is constructed using two distinct sets of prompts: one derived from tweets extracted from the New York Times Twitter/X account and the other from captions from MS COCO [15] dataset. Amid extensive discussions on policymaking to regulate AI development, it is crucial to assess the quality of content generated by AI models. Therefore, to establish quantifiable framework for evaluating and ranking image generation models based on their visual quality, we introduce the Visual AI Index (VAI ) by assessing seven key metrics, including texture complexity, color distribution, object coherence, and contextual relevance. The VAI score is calculated through combination of these metrics and scaled to assess the likelihood of an image being real than AI-generated, where higher score indicates superior visual quality. We offer VAI as valuable tool for the wider AI community that can serve as rubric in AI-related policy-making. Our contributions are: Contributions We introduce the Visual Counter Turing Test (VCT2), benchmark for AGID assessment. We propose the Visual AI Index (VAI ), novel metric designed to determine visual quality of AI-generated images. VAI is evaluated based on factors such as texture complexity and object coherence. Empirically showing that popular AGID methods lack generalization and are susceptible to being easily circumvented. Both benchmarks CT 2 and VAI will be published as open-source (dataset and scripts) leaderboards. 2. AI-Generated Image Detection: An Overview of Current Methods In recent years, AI-generated image detection has emerged as critical area of research. This section provides literature review on synthetic/AI-generated image detection, as depicted in Figure 2. The detection techniques are cateFigure 1. fake image purporting to show an explosion near the Pentagon was shared by multiple verified Twitter accounts, causing confusion and leading to brief dip in the stock market. CNN cover story. more advanced than GPT-4. The central concern noted in the letter [9] is Should we let machines flood our information channels with propaganda and untruth?\". While individual viewpoints on the notion of moratorium may vary, the raised concern cannot be ignored. The findings of the latest (7th) evaluation of the European Commissions Code of Conduct [10] that seeks the eradication of mis/dis-information online reveals decline in companies responsiveness. The percentage of notifications reviewed by companies within 24 hours decreased, falling from 90.4% in 2020 to 64.4% in 2022. This decline likely reflects the increased accessibility of Gen AI models, leading to notable influx of AI-generated content on the web. With approximately 3.2 billion images and 720,000 hours of video uploaded to social media platforms daily [11] (as of 2020), the need for robust AI-Generated Image Detection (AGID) techniques is more pressing than ever. For illustration, consider this example. The false depiction of an explosion near the Pentagon (Figure 1) was shared by multiple verified (formerly, Twitter) accounts on May 22nd, 2023. Additionally, inaccurate reports of the explosion were aired on major Indian and Russian television networks. Following the images viral spread, the US stock market saw significant impact, with the Dow Jones Industrial Average dropping by approximately 80 points. Governments worldwide have begun discussions and have implemented measures to develop policies concerning AI systems. The European Union [12] has taken definitive stance by enacting legislation, while the United States [13] and others have introduced preliminary proposals regarding the regulatory framework for AI. One of the primary concerns among policymakers is that Generative AI could act as force multiplier for political disinformation. The combined effect of the generative text, images, videos, and audio may surpass the influence of any single modality\" [14]. Additionally, AI policymakers have raised significant concerns about the use of automatic labeling or invisible watermarks as technical solution to the challenges posed by generahttps://cetas.turing.ac.uk/publications/rapid-rise-generative-ai gorized into two main groups: Generation Artifact-Based Detection and Feature Representation-Based Detection. t e a d r G Generation Artifact-Based Detection Feature Representation-Based Detection NPR [16] DM Image Detection [17] Fake Image Detection [18] DRCT [19] CNNDetection [20] GAN Image Detection [21] DIRE [22] LASTED [23] De-Fake [24] Deep Fake Detection [25] SSP [26] AIDE [27] RINE [28] OCC-CLIP [29] Figure 2. Taxonomy of AI-Generated Image Detection techniques, detailing the detection techniques in two categories: Generation Artifact-Based and Feature Representation-Based. 2.1. Generation Artifact-Based Detection These techniques focus on detecting generation artifacts in both the spatial and frequency domains. Tan et al. [16] found that the up-sampling operator can create artifacts not just in the frequency patterns but also in how the pixels are arranged in the image. These artifacts are especially visible in images created by GANs or diffusion models. Building upon this observation, the authors introduce the concept of Neighboring Pixel Relationships as means to capture and characterize the generalized structural artifacts stemming from up-sampling operations. Corvi et al. [17] discovered that synthetic images, particularly those created by GANs and some diffusion models like GLIDE and Stable Diffusion, tend to have noticeable differences in their mid-to-high frequency signals compared to real images. However, these differences arent as noticeable in images generated by other models like DALL-E and ADM. While their method is very accurate at distinguishing between synthetic and real images when both types are clearly labeled in separate folders, it struggles to identify generated images effectively in realworld situations. Doloriel et al. [18] explore masked image modeling for universal fake image detection. They study both spatial and frequency domain masking and based on empirical analysis, propose deepfake detector via frequency masking. Chen et al. [19] focus on enhancing the generalizability of detectors by generating hard samples through high-quality diffusion reconstruction. These reconstructed images, which closely resemble real ones but contain subtle artifacts, help train detectors to better distinguish between real and generated images, even from unseen models. 2.2. Feature Representation-Based Detection These methods distinguish real images from synthesized images by leveraging representations obtained from neural networks, which are computational models that excel in various computer vision tasks such as image super-resolution, classificatin, segmentation, and point cloud completionon [3033]. Wang et al. [20] aim to build universal detector. The authors found that standard ResNet-50 classifier [34] with random blur and JPEG compression data augmentation, when trained on only one specific CNN generator (ProGAN), can generalize well to almost all other unseen architectures as well as models introduced later (StyleGAN2 [35], and StyleGAN3 [36]). Mandelli et al. [21] propose detector based on an ensemble of CNNs. For generalization purposes, the CNNs should provide orthogonal results, and the original images should be trusted more during testing. Wang et al. [22] measure the error between an input image and its reconstruction counterpart by pre-trained diffusion model. The authors observed that diffusion-generated images can be approximately reconstructed by diffusion model while real images cannot. Wu et al. [23] leverage language-guided contrastive learning to learn representations that capture the inherent differences in underlying real and synthesized image distributions. This method involves augmenting training images with carefully designed textual labels, which allows for joint image-text contrastive learning for forensic feature extraction. Sha et al. [24] addresses the challenge of detecting and attributing fake images generated by text-toimage models. The authors propose systematic approach that involves (i) building machine-learning classifier to detect fake images generated by various text-to-image models, (ii) attributing fake images to their source models to hold model owners accountable for misuse, and (iii) investigating how prompts affect detection and attribution, focusing on topics like person and prompt lengths between 25 and 75 words. Aghasanli et al. [25] presents novel approach to deepfake detection. The authors propose methodology that leverages features from fine-tuned Vision Transformers (ViTs) combined with Support Vector Machines (SVMs) to distinguish between real and fake images generated by various diffusion models. The method analyzes the support vectors of the SVMs to provide interpretability. Chen et al. [26] propose simple yet effective method that extracts single simplest patch from an image and sends its noise pattern to binary classifier. Yan et al. [27] propose AIDE, hybrid-feature model leveraging both high-level semantic information (using CLIP) and low-level artifacts. Koutlis et al. [28] utilizes intermediate layer outputs from CLIPs image encoder to detect AI-generated images more effectively. They also incorporate Trainable Importance Estimator to weigh the contributions of each Transformer block, resulting in generalizability across various generative models. Liu et al. [29] present method for identifying which generative model created given image in practical setting. The authors introduce OCC-CLIP, CLIP-based framework designed for few-shot one-class classification. This framework is particularly useful when only few images generated by model are available, and access to the models parameters is restricted. The OCC-CLIP model effectively attributes the source of generated images by distinguishing between real images and generated ones using combination of high-level and adversarial data augmentation techniques. 3. Visual Counter Turing Test (V CT 2) CT 2 is benchmark comprising 26K records (130K images generated by text-to-image models), each record with corresponding real image for the given caption. This benchmark also includes codebases for 15 SoTA AGID techniques, providing resource for evaluation. For image generation, we selected advanced text-to-image generative models like Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6 known for achieving outstanding synthetic image quality. We generated 16K records on tweets and 10K records on MS COCO captions as our Visual Counter Turing Test (VCT2) benchmark dataset. we make our COCO and Twitter datasets publicly available. In the following, we describe how the real twitter images and corresponding captions are collected. 3.1. Real Twitter Image Dataset The procedure for data collection from Twitter was meticulously designed to ensure the acquisition of relevant, highquality data for subsequent analysis. This section delineates the key aspects of the data collection procedure, including the data filtration process and data processing steps. Data collection from Twitter was automated using Python and Selenium. We programmatically accessed tweets and associated metadata from The New York Times (@nytimes) Twitter handle. Utilizing the NYT Twitter handle for data collection offers two primary advantages: First, as renowned and reputable news organization, data from its official Twitter handle guarantees the reliability and credibility of the information, given its content undergoes rigorous fact-checking and editorial review. Second, the diversity of content from the NYT handle is significant, encompassing broad spectrum of subjects such as national and international news, politics, culture, science, and more. This breadth is advantageous for generating wide range of images across various domains. The data collection process spanned 12 years (2011-2023) to capture representative sample of tweets. Selection criteria included tweets with associated images, as these images were intended for comparison with AI-generated counterparts. The collected Twitter data underwent series of preprocessing steps to prepare it for analysis. These steps encompass text normalization, removing hashtags and URLs, and retaining only alphanumeric characters. These preprocessing procedures aimed to enhance the quality of the data and facilitate subsequent analysis. 4. Detection Results Detection results, as shown in Table 1 and Table 2, are based on the performance of 15 SoTA AGID methods evaluated on synthetic datasets generated from MS COCO and Twitter prompts. AGID methods are assessed using three metrics: accuracy (Acc), recall (R), and precision (P). The datasets span five synthetic datasets corresponding to text-to-image models: SD 2.1, SDXL, SD 3, DALL-E 3, and Midjourney 6. Results for each generative model are computed on combined dataset of fake and real images. Overall, the results demonstrate substantial progress in AI-generated image detection but also reveal critical gaps in generalizability, robustness, and adaptability. Highperforming methods like De-Fake and DRCT show promise but fail to address the challenges posed by proprietary models. Meanwhile, the trade-offs between precision and recall in many methods highlight the need for balanced and adaptive detection approaches. These findings call for future work to focus on improving the robustness of detection techniques and developing methods that can handle the increasing diversity of generative models in real-world applications. 5. Visual AI Index (VAI) With the rapid advancement of AI technology, imagegenerative models are continuously evolving. To objectively assess and rank these models based on their visual quality, we introduce the Visual AI Index (VAI ), standardized metric designed for evaluation. The VAI evaluates 7 key metrics, including texture complexity, color distribution, object coherence, and contextual relevance. VAI is calculated by 100 (cid:80)7 , where xj is the value of the j-th metric, Lj is the lower bound of the j-th metric, µj is the mean value of the j-th metric. Finally, the VAI score is scaled to interval [0,100] by 100 VAI min(VAI ) max(VAI )min(VAI ) . higher score signifies superior visual quality, making the image less likely to be detected as AI-generated. In the following each metric is described. (cid:16) xj Lj 1µj j= (cid:17) Texture Complexity quantifies the variety and unpredictability of an images texture. It is determined by computing the entropy of the normalized Local Binary Pattern (LBP) histogram of the grayscale image using the formula (cid:80)P 1 HLBP (k) log2( HLBP (k) + ϵ). Here, HLBP (k) k=0 represents the normalized histogram value for LBP bin k, and is the total number of bins in the LBP histogram. The small constant ϵ (e.g., 1 106) is used to avoid taking the logarithm of zero. Color Distribution evaluates the variability in an images color distribution by analyzing the standard deviation of the Method CNNDetection (0.5) [20] NPR [16] DM Image Detection [17] Fake Image Detection [18] DIRE [22] LASTED [23] GAN Image Detection [21] AIDE [27] SSP [26] DRCT (ConvB) [19] DRCT (UnivB) [19] RINE [28] OCC-CLIP [29] De-Fake [24] Deep Fake Detection [25] SD 2.1 0.03 1.89 67.92 0.49 93.40 8.67 82.93 20.98 99.63 99.61 96.98 49.63 92.28 97.90 49. Acc 49.94 26.76 83.92 49.84 47.08 54.0 51.87 60.30 50.15 98.76 88.57 74.43 51.49 92.37 49.49 65.11 34.26 99.40 63.58 37.66 56.62 51.16 93.77 50.07 97.94 83.02 98.49 50.82 88.15 49.03 SDXL 0.07 1.73 40.00 0.48 98.57 9.86 91.75 28.91 99.63 95.75 98.73 13.71 14.95 95.62 51.43 Acc 49.96 26.68 69.96 49.83 49.67 61.13 56.35 64.34 49.95 96.83 89.45 56.47 47.11 91.23 51.43 77.52 33.15 98.91 66.68 47.07 61.20 53.72 96.75 49.97 97.86 83.27 94.76 41.91 87.90 49.65 Acc 49.93 27.96 63.58 50.02 48.59 51.87 58.26 57.11 50.34 80.72 84.90 61.99 50.60 91.30 49. SD 3 0.01 4.29 27.23 0.86 96.40 9.61 95.35 14.45 99.63 63.54 89.64 24.75 66.03 95.76 49.85 81.16 34.41 98.04 66.91 38.88 57.67 54.74 94.28 50.17 96.81 81.88 97.03 50.46 87.92 49.97 DALL-E 3 0.00 0.00 0.00 0.00 97.01 44.85 74.93 0.02 99.63 2.08 79.80 0.87 78.82 94.31 52.73 Acc 49.93 25.81 49.96 49.59 48.89 66.18 48.10 50.00 49.91 49.99 79.98 50.05 50.44 90.58 52.73 35.13 41.13 40.0 34.90 43.25 76.21 48.77 61.23 49.95 49.76 80.09 53.37 50.28 87.76 53.02 Midjourney 6 63.15 0.05 48.13 0.00 87.04 3.52 62.89 0.40 52.74 99.31 63.14 14.37 54.14 93.42 96.92 52.25 49.97 99.63 94.65 37.06 83.32 99.12 97.27 27.02 53.60 75.04 86.68 85.59 54.09 52. Acc 49.95 25.81 51.73 49.79 50.04 68.21 57.15 76.01 49.95 67.48 89.64 63.13 55.04 86.22 52.87 Table 1. Overall accuracy (Acc), recall (R), and precision (P) across various synthetic datasets generated from MS COCO prompts. All units are in %. Method CNNDetection (0.5) [20] NPR [16] DM Image Detection [17] Fake Image Detection [18] DIRE [22] LASTED [23] GAN Image Detection [21] AIDE [27] SSP [26] DRCT (ConvB) [19] DRCT (UnivB) [19] RINE [28] OCC-CLIP [29] De-Fake [24] Deep Fake Detection [25] SD 2.1 0.06 2.22 77.57 0.53 86.95 1.93 77.37 11.81 99.66 99.77 96.73 55.40 74.11 91.51 50.80 Acc 50.00 50.23 88.31 49.86 43.90 77.60 53.26 55.69 49.91 96.81 67.47 77.07 46.88 81.13 50. 52.21 50.89 97.82 56.33 36.20 59.60 52.25 81.98 49.95 94.20 61.02 97.79 47.98 75.78 51.84 SDXL 0.03 2.68 48.58 0.58 96.29 2.75 82.36 21.29 99.66 94.05 98.43 16.97 51.17 85.57 53.64 Acc 49.98 50.46 73.82 49.88 48.57 83.60 55.84 60.43 50.20 93.96 68.32 57.86 45.67 78.16 53.64 59.98 60.58 93.74 60.83 46.29 66.04 53.86 89.61 50.10 93.87 61.44 93.13 46.10 74.53 56.59 Acc 50.19 51.45 65.15 50.35 48.49 83.24 60.01 56.49 50.20 71.79 64.81 62.13 48.84 79.39 51.44 SD 3 0.44 4.66 31.24 1.51 96.13 2.75 91.04 13.41 99.66 49.73 91.40 25.50 67.54 88.03 51. 74.35 68.26 90.34 66.15 38.48 61.52 56.21 87.40 50.10 89.01 59.67 95.32 49.16 75.06 51.51 DALL-E 3 0.01 0.00 0.00 0.01 91.81 25.57 79.44 0.25 99.66 0.76 69.30 0.48 45.63 89.14 55.30 Acc 49.97 49.12 49.53 49.59 46.33 78.77 53.99 49.93 50.18 47.31 53.76 49.61 47.75 79.95 55.30 34.59 42.20 33.34 33.11 36.19 76.81 52.68 43.61 50.10 11.02 52.87 27.64 49.72 75.29 60.34 Table 2. Overall accuracy (Acc), recall (R), and precision (P) across various synthetic datasets generated from Twitter prompts. All units are in %. normalized color histogram in the HSV color space. It is calculated as std( HHSV (h, s, v)), where std() denotes the standard deviation of the normalized histogram HHSV (h, s, v) for hue h, saturation s, and value v. Object Coherence evaluates the extent and clarity of edge detection in an image, providing insight into the consisi,j E(i,j) , tency of object boundaries. It is determined using (cid:80) i,j 1 where E(i, j) represents the value of the Canny edge image at pixel (i, j), and the (cid:80) i,j 1 represents the total number of pixels in the image. (cid:80) Contextual Relevance evaluates the distribution of edge 2), strengths across the image. It is given by var( where var() denotes the variance, and Gx and Gy are the gradients computed using the Sobel filter in the horizontal and vertical directions, respectively. 2 + Gy Gx (cid:113) Image Smoothness evaluates how consistent the images 1+var(I) , where denotes texture is. It is quantified as the Laplacian of the grayscale image I. 1 Image Sharpness is quantified as max(I Iblurred). and Iblurred denote the grayscale and blurred image with Gaussian kernel, respectively. Image Contrast measures the degree of variation in intensity across an image. It is quantified by calculating the standard deviation of the pixel values in the grayscale image, expressed as std(I). Subsection 5.1 discusses VAI results and subsection 5.2 provides detailed comparative discussion of different AIgenerated images. 5.1. VAI Results VAI scores on CT 2 benchmark are shown in Table 3. The score for real images serves as baseline for comparison. In analyzing VAI scores based on images generated using COCO dataset prompts, we observe that DALL-E 3 images have relatively low VAI score of 55.52. In contrast, Midjourney proves the most challenging to detect as artificially generated, achieving high score of 93.65. Stable Diffusion models (SD 3 and SD 2.1) lie in between, with similar scores of 69.33 and 70.47, respectively. When examining images from the Twitter dataset, the Stable Diffusion models remain easily detectable, with SDXL scoring 52.82, followed by SD 3 and SD 2.1, with scores of 55.04 and 69.33, respectively. Midjourney again proves the hardest to detect, scoring Real Images COCO Twitter COCO Twitter COCO Twitter COCO Twitter COCO Twitter COCO Twitter Midjourney 6 DALL-E 3 SD 2.1 SDXL SD 3 85.61 81.34 70.47 69.33 65. 52.82 69.33 55.04 55.52 78.53 93. 89.62 Table 3. VAI scores for real images from the MS COCO and Twitter datasets and various synthetic datasets generated from their respective prompts. 89.62 (Midjourney was blocking image generation on Twitter prompts and this score is calculated on 500 images we were able to generate), while DALL-E 3 scores 78.53, placing it between Midjourney and the Stable Diffusion family. noteworthy pattern across the datasets is that Midjourney consistently achieves higher scores than real images. This discrepancy arises because real images often contain imperfections, noise, and environmental irregularities, whereas Midjourney-generated images are exceptionally consistent and smooth. Midjourney 6 excels in realistic textures, lighting, and patterns without noticeable artifacts, leading to higher scores. This difference becomes clearer upon closely examining examples, as illustrated in Figure 3, where real images and corresponding Midjourney-generated images reveal these subtle distinctions. Midjourney images demonstrate smooth textures, consistent lighting, and lack of noticeable artifacts, leading to higher scores. In contrast, real images display natural imperfections, noise, and environmental variability, with cartoonish, airbrushed look. The slightly higher scores for Midjourney can be attributed to its precise textures and visual consistency, in contrast to the natural inconsistencies and noise found in real-world images. (a) Midjourney 6 (b) Real (c) Midjourney (d) Real Figure 3. Comparison of Midjourney-generated and real images, highlighting the subtle distinctions that contribute to scoring differences. The correlation observed between the VAI scores depicted on the right side in Figure 4 and Figure 5, and the detection accuracy illustrated by the heat maps on the left (refer to Table 1 and Table 2 for precise data) suggests that the proposed index and detectability share correlated traits. Notably, all detection models exhibit difficulties in identifying images generated by Midjourney 6, as reflected in their respective accuracy, precision, and recall metrics. Another noteworthy point is the superior performance of the De-Fake method relative to other detection techniques. This trend subtly underscores that excelling in image generation does not necessarily equate to enhanced evasion of detection. This comparison underscores critical insight for the field: despite improvements in generation quality, the characteristics of the generated outputs still permit reliable detection, as evidenced by the robust performance of the De-Fake method. 5.2. Image Explanations Upon analyzing Local Binary pattern (LBP) texture and the pairwise scatter plots derived from multiple features of various AI-generated images, insights emerged that underscore both the utility and distinctive characteristics of these models in image generation. Subsequently, we discuss this analysis in the following subsections, where each section delves into specific aspects. 5.2.1. LBP Texture Analysis Local Binary Pattern (LBP) is commonly used for texture analysis, image recognition, and quality assessment. LBP plots can indirectly assess image quality, as sharper images generally produce more distinct patterns in their LBP representations. If the LBP pattern appears blurred or lacks clear edges, it may indicate loss of detail or lower resolution in the image. AI-generated images sometimes lose fine-grained texture, which would be visible as less distinctive LBP features. In Figure 7 we can see that image generated by Midjourney has specific facial textures and subtle expression lines whereas image generated by SD 3 has inconsistencies and lack of texture in certain areas. facial features, facial structures, hair lines, edges in clothing, and wrinkles are preserved in each segment for the Midjourney image but SD 3 image completely lost it. 5.2.2. Pairwise Scatter Plot Analysis The pairwise scatter plots shown in Figure 6 reveal distinct differences in the models distributions. DALL-E 3 and SDXL show higher object coherence across varying texture complexities, suggesting better object integrity maintenance. In contrast, SD 2.1 and SD 3 have more dispersed distributions, indicating less consistency. These observations also highlight evolutionary improvements, where newer models Text-to-Image Model Midjourney VAI (0-100) 93.65 SD 2.1 SD 3 SDXL DALL-E 3 70. 69.33 65.19 55.52 Figure 4. Right: VAI scores of 5 advanced text-to-image models on MS COCO prompts. Left: Accuracy heat maps showing the detectability of different AI-generated images using different AGID method. Text-to-Image Model Midjourney VAI (0-100) 89.62 DALL-E 3 SD 2.1 SD3 SDXL 78. 69.33 55.04 52.82 Figure 5. Right: VAI scores of 5 advanced text-to-image models on Twitter prompts. Left: Accuracy heat maps showing the detectability of different AI-generated images using different AGID method. like SD 3 and SDXL exhibit enhanced color distribution and contextual relevance. 5.2.3. Comparison of Image Generation Models Based on Specific Prompts The prompt used for the image generation was, \"Have you ever wondered why we name hurricanes? The New York Times meteorologist Judson Jones explains.\", DALL-E 3 attempted more comprehensive interpretation by including not just the hurricane but also capturing the essence of \"wondered\". It depicted person, emphasizing the latter part of the sentence. In contrast, other models like Midjourney 6 and SD 2.1 focused only on the hurricane. SDXL added elements like coconut trees, dark clouds, and rain, while SD 3 included both hurricane and person in its visual representation. However, the issue with the DALL-E 3 image is its aesthetic quality. It appears highly saturated with cartoonish, airbrushed look that makes it easy to identify as AI-generated. The prompt \"At least six candidates appear to have made the cut so far for the second Republican presidential debate on Sept 27 See which candidates have and have not qualified so far\" was used across various image generation models. Stable Diffusion versions consistently faced challenges with accurate pose estimation, resulting in distortions of human figures such as faces, hands, and legs. DALL-E 3s output, while avoiding distortions, produced images that appeared cartoonish and 2D, making them clearly identifiable as AI-generated and not depicting real people. Midjourney 6, however, managed to generate the most effective visual representation, achieving better overall quality in the depiction of human figures. SD 3 has shown significant improvement in handling text within images compared to SD 2.1 and SDXL. However, in terms of brightness and abrupt changes in pixel intensity, there has been no significant improvement observed between these versions. DALL-E 3 consistently attempts to include text to better convey the concepts behind prompts, enhancing understanding. In contrast, Midjourney 6 typically omits text from images, with text appearing in fewer than 0.5% of cases based on our testing. 5.2.4. Practical Implications and Model Comparison In scenarios demanding high object coherence, DALL-E 3 and SDXL emerge as preferable choices. DALL-E 3 excels in generating images where object integrity is paramount, making it ideal for tasks where the recognizability of objects is crucial. In contrast, the SD models, including SD 3 atypical for non-AI-generated images, which generally suffer degradation under similar conditions due to limitations in camera technology and environmental factors. Additionally, as shown in Figure 4 and Figure 5, DALL-E 3 exhibits the broadest distribution, peaking at lower scores in detection systems compared to the Stable Diffusion variants, indicating that its images are generally more easily identified as AI-generated. Conversely, SD 3 and SDXL show narrower, more sharply peaked distributions. Notably, SD 3 peaks at higher scores than SDXL, suggesting that it produces images that pose greater challenge for detection systems to classify correctly, reflecting its advancements in creating images with more naturalistic elements and fewer detectable artifacts. Meanwhile, Stable Diffusion 2.1s distribution peaks between those of DALL-E 3 and SDXL, presenting moderate challenge for detection systems in recognizing its images as AI-generated. Table 4 summarizes the notable limitations observed in various AI image generation models. Each model exhibits distinct challenges, such as text rendering issues, aesthetic inconsistencies, and quality of human depiction. Model Name Notable Limitations Midjourney 6 DALL-E 3 Stable Diffusion Family Facial details and posture quality Omits text within images Cartoonish, airbrushed look Table 4. Key observations on various image generation models. (a) Midjourney 6 (b) LBP of Midjourney 6 (c) Stable Diffusion (d) LBP of Stable Diffusion 3 Figure 7. Comparative analysis of texture patterns in images generated by different AI models using Local Binary Pattern (LBP) representation. 6. Conclusion In this paper, we critically assess the current state-of-theart in AI-generated image detection (AGID) techniques and (a) DALL-E 3 (b) Midjourney (c) Stable Diffusion 3 (d) Stable Diffusion 2.1 (e) Stable Diffusion XL Figure 6. Pairwise scatter matrix plot of various models, showcasing pairwise relationships between Texture Complexity (TC), Color Distribution Consistency (CDC), Object Coherence (OC), Contextual Relevance (CR), Image Smoothness (IS), ImageSharpness (ISH), and Image Contrast (IC). The color gradient indicates Object Coherence levels across the matrix. and SDXL, focus on contextual relevance and image sharpness, catering well to applications requiring high fidelity and detailed visualizations in complex scenes. Notably, these models exhibit consistent clustering within specific texture ranges, indicating predictable textural consistency which serves as potential marker for distinguishing AI-generated images from natural ones. Further insights reveal that models like SD 3 and SDXL maintain high object coherence across varying levels of image complexitya trait not typically observed in natural images, where object coherence often diminishes with increased complexity due to natural disruptions like noise. This uniformity in maintaining object coherence, regardless of other features, highlights significant distinction of AI-generated imagery. Moreover, the newer AI models, especially SDXL, demonstrate an exceptional ability to maintain sharpness and contrast under extreme imaging conditions, such as high complexity or low brightness. This capability is highlight their limitations in effectively detecting images produced by contemporary text-to-image models. Our evaluation, conducted using the newly introduced Visual Counter Turing Test (VCT2) benchmarkwhich comprises 130K images generated by advanced models such as Stable Diffusion 3, DALL-E 3, and Midjourney 6demonstrates that existing AGID methods are unable to keep pace with the rapid evolution of generative AI. Given the significant potential for misuse of AI-generated images, the inadequacies of current detection methods underscore the urgent need for paradigm shift in how we approach AGID. To this end, we propose the Visual AI Index (VAI ), novel framework designed to rigorously evaluate image-generative AI models. Focusing on key visual aspects such as texture complexity, color distribution, and object coherence, VAI aims to establish new standard for the evaluation of these models. We advocate for further research and development in this field, leveraging the provided datasets to enhance detection capabilities and ensure the integrity of visual media in digital era."
        },
        {
            "title": "References",
            "content": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [2] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2 [3] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 1, 2 [4] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 88218831. PMLR, 2021. 1 [5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1, 2 [7] Midjourney. 2024. https://www.midjourney.com/home. 1, 2 [8] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [9] Gary Marcus. Pause giant ai experiments: An open letter, Mar 2023. 1, 2 [10] European Commission. Eu code of conduct against online hate speech: latest evaluation shows slowdown in progress. 2022. 2 [11] Paula Dootson T.J. Thomson, Daniel Angus. 3.2 billion images and 720,000 hours of video are shared online daily. can you sort real from fake?, Nov 2020. 2 [12] European-Parliament. Proposal for regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts, May 2023. 2 [13] White-House. Blueprint for an ai bill of rights: Making automated systems work for the american people, May 2023. [14] Sarah Mercer Alexander Kasprzyk Ardi Janjeva, Alexander Harris and Anna Gausen. The rapid rise of generative ai: Assessing risks to safety and security. https://cetas.turing. ac.uk/sites/default/files/202312/cetas_research_report__the_rapid_rise_of_generative_ai_-_2023.pdf, 2023. 2 [15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2 [16] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection. arXiv preprint arXiv:2312.10461, 2023. 3, 5 [17] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 3, 5 [18] Chandler Timm Doloriel and Ngai-Man Cheung. Frequency masking for universal deepfake detection. arXiv preprint arXiv:2401.06506, 2024. 3, 5 [19] Baoying Chen, Jishen Zeng, Jianquan Yang, and Rui Yang. Drct: Diffusion reconstruction contrastive training towards universal detection of diffusion generated images. In Fortyfirst International Conference on Machine Learning. 3, 5 [20] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86958704, 2020. 3, 5 [21] Sara Mandelli, Nicolò Bonettini, Paolo Bestagini, and Stefano Tubaro. Detecting gan-generated images by orthogonal training of multiple cnns. In 2022 IEEE International Conference on Image Processing (ICIP), pages 30913095. IEEE, 2022. 3, [22] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusiongenerated image detection. arXiv preprint arXiv:2303.09295, 2023. 3, 5 [23] H. Wu, J. Zhou, and S. Zhang. Generalizable synthetic image detection via language-guided contrastive learning. arXiv preprint:2305.13800, 2023. 3, 5 [24] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. De-fake: Detection and attribution of fake images generated by textIn Proceedings of the 2023 to-image generation models. ACM SIGSAC Conference on Computer and Communications Security, pages 34183432, 2023. 3, 5 [25] Agil Aghasanli, Dmitry Kangin, and Plamen Angelov. Interpretable-through-prototypes deepfake detection for diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 467474, 2023. 3, 5 [26] Jiaxuan Chen, Jieteng Yao, and Li Niu. single simple patch is all you need for ai-generated image detection. arXiv preprint arXiv:2402.01123, 2024. 3, 5 [27] Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. sanity check for ai-generated image detection. arXiv preprint arXiv:2406.19435, 2024. 3, [28] Christos Koutlis and Symeon Papadopoulos. Leveraging representations from intermediate encoder-blocks for synthetic image detection. arXiv preprint arXiv:2402.19091, 2024. 3, 5 [29] Fengyuan Liu, Haochen Luo, Yiming Li, Philip Torr, and Jindong Gu. Which model generated this image? modelagnostic approach for origin attribution. 3, 5 [30] Nasrin Imanpour, Ahmad Naghsh-Nilchi, Amirhassan Monadjemi, Hossein Karshenas, Kamal Nasrollahi, and Thomas Moeslund. Memory-and time-efficient dense network for single-image super-resolution. IET Signal Processing, 15(2):141152, 2021. 3 [31] Alireza Bagheri Rajeoni, Breanna Pederson, Daniel Clair, Susan Lessner, and Homayoun Valafar. Automated measurement of vascular calcification in femoral endarterectomy patients using deep learning. Diagnostics, 13(21):3363, 2023. [32] Alireza Bagheri Rajeoni, Breanna Pederson, Ali Firooz, Hamed Abdollahi, Andrew Smith, Daniel Clair, Susan Lessner, and Homayoun Valafar. Vascular system segmentation using deep learning. Artificial Intelligence: Machine Learning, Convolutional Neural Networks and Large Language Models, 1:85, 2024. [33] Pingping Cai, Canyu Zhang, Lingjia Shi, Lili Wang, Nasrin Imanpour, and Song Wang. EINet: Point Cloud Completion via Extrapolation and Interpolation. European Conference on Computer Vision, 15098:377393, 2024. 3 [34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [35] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of the ing the image quality of stylegan. IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. 3 [36] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in neural information processing systems, 34:852863, 2021. 3 7. Appendix 7.1. Dataset Details The Visual Counter Turing Test (V CT 2) benchmark utilizes images generated from prompts sourced from two distinct datasets: MS COCO Captions: We utilized 10K captions from the MS COCO dataset to generate synthetic images using text-to-image models. The use of COCO ensures that our dataset captures wide range of everyday scenes and objects, enabling us to test the generalizability of AGID techniques across diverse contexts. Twitter Prompts: To ensure diversity and relevance, we utilized 16K tweets from the New York Times (@nytimes) Twitter handle spanning 2011-2023. We generated 26K records (130K images) synthetic images using five state-of-the-art text-to-image generative models: Stable Diffusion 2.1, XL, and 3: Known for achieving high-quality image synthesis. With each of these models, we generated 10K synthetic images on MS COCO prompts and 16K synthetic images on Twitter prompts, contributing significantly to the overall image count. DALL-E 3: Utilized for its capability to interpret complex prompts and generate high-quality imagery, despite its susceptibility to produce cartoonish aesthetics. With this model, we generated 10K synthetic images on MS COCO prompts and 16K synthetic images on Twitter prompts, providing high-quality imagery. Midjourney 6: Selected for its capability to generate exceptionally consistent and smooth textures, making it one of the more challenging models to detect. With this model, we generated 10K synthetic images on MS COCO prompts and only 500 synthetic images on Twitter prompts, as this model blocks image generation for most Twitter prompts. Table 5 presents 10 records of real images alongside synthetic images generated by different models, based on 10 Twitter prompts for which we had the corresponding Midjourney 6 image. Full dataset are publically available at COCO and Twitter. 7.2. Detection Techniques The detection techniques presented in Figure 2 were chosen to provide comprehensive evaluation of state-of-the-art AI-generated image detection techniques. By categorizing these methods into Generation Artifact-Based and Feature Representation-Based techniques, we aimed to capture both From the VAI scores on Twitter prompts, Midjourney 6 remained the top performer, followed closely by DALL-E 3. This suggests that proprietary models are particularly robust in generating high-quality images, even with more diverse and potentially less structured prompts. The accuracy heat maps in Figure 5 also highlight differences in how AGID methods perform across models. Methods like De-Fake and DRCT were particularly effective at detecting Midjourneygenerated images, whereas detection on DALL-E 3 and SDXL proved more challenging. This indicates that the texture and artifact characteristics differ significantly across these models, affecting detection reliability. These results underscore the challenges faced by AGID methods when applied to high-quality proprietary models. While some detection methods, like De-Fake and DRCT, performed consistently well, the VAI scores reveal that generated image quality plays significant role in detection difficulty. Future work should focus on improving the robustness of detection techniques against models that prioritize high visual fidelity, such as Midjourney 6 and DALL-E 3. The scatter plots in Figure 6 further illustrate the complexity of visual relationships across different metrics, such as Texture Complexity, Object Coherence, and Image Sharpness. Midjourney 6 and DALL-E 3 exhibit well-distributed clusters, indicating superior performance in maintaining coherence and complexity. Stable Diffusion variants, however, show mixed patterns, highlighting inconsistencies in texture handling and object boundaries, which aligns with the lower VAI scores observed for these models. The scatter plot analysis emphasizes that achieving balance across metrics like texture complexity and object coherence is critical for enhancing the performance of both generative and detection models. 7.5. Supplementary Figures The supplementary figures presented in this appendix (Figure 8 and Figure 9) were discussed in the main paper in subsection 5.2.3 but omitted due to space limitations. Here, we include these figures to provide further insights and visual examples for the concepts and results covered in the paper. explicit artifacts left by generation processes and more subtle feature-level discrepancies. This dual approach helps in covering wide spectrum of detection challenges, ensuring robustness against different generation methods. 7.3. Detection Performance Overview Tables 1 and 2 provide an overview of the performance of different detection techniques across synthetic datasets generated from MS COCO and Twitter prompts, respectively. The metrics measured are Accuracy (Acc), Recall (R), and Precision (P), providing insights into each models ability to differentiate real from AI-generated images. 7.3.1. Performance by Detection Technique CNNDetection, NPR and Fake Image Detection: These methods showed variable results, characterized by low recall but higher precision across several models. This indicates tendency to correctly identify generated images when detected, but with many instances being missed (false negatives). DM Image Detection and De-Fake: DM Image Detection demonstrated high precision across all models, particularly excelling with Stable Diffusion versions and Midjourney 6, effectively capturing generated images. De-Fake consistently showed high accuracy, recall, and precision, indicating its reliability. GAN Image Detection, SSP and DIRE: These methods had mixed performance, particularly excelling in precision. DRCT (ConvB and UnivB): Both versions of DRCT showed strong accuracy, recall, and precision across most models but experienced slight performance drop with Midjourney 6, indicating challenges with proprietary models. OCC-CLIP: OCC-CLIP had lower recall with SDXL but balanced performance for DALL-E 3 and Midjourney 6. The results indicate that there is no one-size-fits-all solution for detecting AI-generated images. Different generative models pose unique challenges, and the performance of each detection method varies based on its ability to identify specific artifacts. De-Fake and DRCT (ConvB and UnivB) were the most consistent performers, highlighting their robustness across models. Future research should aim to improve detection for proprietary models like Midjourney 6 and DALL-E 3, where many techniques struggled. 7.4. Visual AI Index Overview Midjourney 6 achieved the highest Visual AI Index (VAI ) score on MS COCO prompts, indicating superior visual coherence and quality compared to other models. Stable Diffusion 2.1 also showed relatively high performance, suggesting that diffusion-based methods can achieve strong visual results but may still be outperformed by proprietary methods like Midjourney 6. (a) DALL-E 3 (b) Midjourney 6 (a) DALL-E 3 (b) Midjourney 6 (c) Stable Diffusion 3 (d) Stable Diffusion 2. (c) Stable Diffusion 3 (d) Stable Diffusion 2.1 (e) Stable Diffusion XL Figure 8. Generated images by different AI models for the prompt \"At least six candidates appear to have made the cut so far for the second Republican presidential debate on Sept 27 See which candidates have and have not qualified so far.\" (e) Stable Diffusion XL Figure 9. Generated images by different AI models for the prompt \"Have you ever wondered why we name hurricanes? The New York Times meteorologist Judson Jones explains.\" Table 5. Real images and synthetic images generated by different models."
        },
        {
            "title": "Real Image",
            "content": "SD 2."
        },
        {
            "title": "SD XL",
            "content": "SD 3.0 DALL-E 3 Midjourney"
        }
    ],
    "affiliations": [
        "Amazon AI, USA",
        "Amazon GenAI, USA",
        "BITS Pilani Hyderabad Campus, India",
        "Carnegie Mellon University, USA",
        "IIIT Delhi, India",
        "International Institute of Information Technology, India",
        "Manipal University Jaipur, India",
        "National Institute of Technology Silchar, India",
        "Stanford University, USA",
        "Texas A&M University, USA",
        "University of South Carolina, USA",
        "Vishwakarma Institute of Information Technology, India",
        "Washington State University, USA"
    ]
}