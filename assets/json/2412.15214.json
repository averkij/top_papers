{
    "paper_title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis",
    "authors": [
        "Hanlin Wang",
        "Hao Ouyang",
        "Qiuyu Wang",
        "Wen Wang",
        "Ka Leong Cheng",
        "Qifeng Chen",
        "Yujun Shen",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: https://ppetrichor.github.io/levitor.github.io/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 4 1 2 5 1 . 2 1 4 2 : r LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis Hanlin Wang1,2 Hao Ouyang2 Qiuyu Wang2 Wen Wang3,2, Ka Leong Cheng4,2 Qifeng Chen4 Yujun Shen2 Limin Wang1, 1State Key Laboratory for Novel Software Technology, Nanjing University 2Ant Group 3Zhejiang University 4The Hong Kong University of Science and Technology Figure 1. LeviTor is capable of generating videos with controlled occlusion, better depth changes, and complex 3D orbiting movement based on user inputs. Given an initial frame, users can easily draw 3D trajectory using our inference pipeline to represent their desired movements for designated area. We highly recommend viewing the supplementary materials for detailed video demonstrations."
        },
        {
            "title": "Abstract",
            "content": "The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with new dimension, i.e., the depth dimension, such that users are allowed to assign relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose pioneering method for 3D trajectory control Corresponding author. in image-to-video synthesis by abstracting object masks into few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: LeviTor. 1. Introduction Controlling object trajectories in video generation [45, 47, 53, 60] is fundamental task with wide-ranging applications in computer graphics, virtual reality, and interactive media. Precise trajectory control allows for generation of dynamic scenes where objects move in desired paths, enabling creators to create realistic and compelling visual content. Such control is crucial for tasks like animating characters in virtual environment, simulating physical phenomena, and developing advanced visual effects that require objects to interact seamlessly within scene. Despite its importance, controlling object trajectories in video synthesis presents significant challenges. Traditional methods [47, 53, 60] often rely on 2D trajectory inputs drawn directly on images. While these approaches allow for motion representation to some extent, they inherently suffer from ambiguity and complexities associated with interpreting 2D motions in 3D space. Consider the example of animating hot air balloon flowing over building as illustrated in Fig. 1. 2D trajectory drawn on the image cannot distinguish whether the balloon should pass in front of or behind the building. This ambiguity arises because single 2D path can correspond to multiple 3D trajectories due to the lack of 3D information, making it insufficient for precise control over object movements in 3D space. However, extracting accurate 3D trajectories poses additional difficulties, especially in scenes with occlusions or complex interactions between objects. For users, inputting valid 3D trajectories is also non-trivial. It often demands specialized knowledge and tools to define object paths accurately within 3D space, which can be barrier for artists and non-expert users aiming to create video content. To address these challenges, we propose LeviTor, novel model that fine-tunes pre-trained video generation models to incorporate an efficient and effective 3D trajectory control mechanism. Our approach introduces an innovative representation of control signal by combining depth information with K-means clustered points of object masks in video. Such control signal can clearly indicate the occlusion and depth changes between objects through the aggregation or separation of clustered points and their depth. This fusion also captures essential 3D attributes of objects trajectory without the need for explicit 3D trajectory estimation, thus simplifying the modeling of complex object motions and interactions. For training, we utilize the recently released high-quality Video Object Segmentation (VOS) dataset from SAM2 [33], which provides rich annotations conducive to our method. By integrating depth cues with clustered points, our representation effectively encodes the objects spatial movements and depth variations over time. This method not only enhances the models ability to interpret and generate accurate 3D motions but also mitigates issues related to occlusions and depth ambiguities. We also design user-friendly inference pipeline that lowers the barrier for users to input 3D trajectories. Users can simply draw trajectories on 2D images and adjust point depths interactively, which the system then interprets as 3D paths for object movements. This approach streamlines the process, making it accessible to users without extensive technical expertise in 3D modeling or animation. Our method demonstrates superior performance both quantitatively and qualitatively compared to existing approaches. We achieve accurate 3D trajectory control in image-to-video synthesis task where previous baselines fail, producing realistic and consistent object movements. In summary, our contributions are as follows: We introduce LeviTor, novel method for controlling 3D object trajectories in video synthesis by combining depth information with K-means clustered points without the need for explicit 3D trajectory tracking. We leverage the high-quality SAV dataset for training, effectively capturing complex object motions and interactions in diverse scenes. We develop user-friendly inference pipeline that simplifies the input of 3D trajectories, making it accessible to broader range of users. To the best of our knowledge, this work is the first to introduce 3D object trajectory control in image-to-video synthesis, paving the way for more advanced and accessible video generation techniques. 2. Related Work 2.1. Video Diffusion Models Diffusion models [14, 37, 38] have demonstrated unprecedented power in video generation. Video Diffusion Models (VDMs) [15] are broadly categorized into Text-toVideo (T2V) and Image-to-Video (I2V) frameworks, aiming to generate video samples from text prompts or image prompts. T2V generation [4, 5, 7, 9, 10, 16, 21, 30, 36, 41, 46, 52] has been extensively studied in recent years, introducing text descriptions to semantically control the content of video generation. Previous works [5, 10, 12, 42, 44, 59] incorporate temporal layers into large pretrained text-toimage (T2I) diffusion models [34]. Subsequent studies [4, 5, 24, 52, 58] have expanded T2V capabilities by utilizing large text-video pairs, achieving improved results. Building upon T2V, I2V synthesis [4, 8, 27, 49, 52, 55, 58] has also been widely explored. Given still image, I2V aims to animate it into video clip that retains all visual content from the image and exhibits naturally suggested dynamics. Many recent works, such as SVD [4], VideoCrafter2 [8], CogVideoX [52] and Open-Sora [58] support both T2V and I2V simultaneously. They extend T2V models to I2V using pretrained T2V models and achieve promising results. Despite producing high-quality videos, these models rely limiting fine-grained control on text or image prompts, and potentially leading to actions misaligned with user intentions. For precise control, some works [1719, 29, 31, 43, 48, 50, 56, 61, 62] employ multimodal video sequences as conditions, such as pose [17, 29, 31, 50, 56, 61], depth [13, 48, 62], or sound [19, 25, 26, 39], treating video generation as video translation task. Although these mod2 synthetic data. The Video Object Segmentation (VOS) datasets [6, 33], particularly with the recent release of SAM2 [33], offer high-quality videos with precise object mask annotations, making it an appropriate choice for our purposes. Nevertheless, two primary challenges remain: 1. The dataset lacks explicit 3D trajectory information, which is essential for training model to understand and synthesize 3D motions. Therefore, we need to implicitly express the 3D motion information contained in the data. 2. The provided mask annotations are too detailed for practical user input, as users cannot be expected to supply such fine-grained masks or dense 3D trajectories for control. Thus it is necessary to design representation of 3D trajectories that is easy for users to input. To address these issues, we propose using K-means points extracted from the object masks along with their depth information as control signals. Specifically, we apply K-means clustering to the pixels of the mask to obtain set of representative control points: (cid:8)(xi t, yi t)(cid:9)N i=1 = K-means(Mt, ), (1) t, yi where Mt denotes all object masks at frame t, is the number of clusters (control points), and (xi t) is the 2D coordinate of control point at frame t. These control points not only simplify user input but also encapsulate implicit 3D information. As illustrated in Figure 2, the spatial distribution and density of the K-means points reflect changes in the objects depth and motion. For example, as motorbike moves closer to the camera, the points spread out due to perspective scaling, indicating depth changes. Similarly, during occlusions, the distribution of points on the car shifts, capturing the occlusion dynamics. Then we employ depth estimation network, DepthAnythingV2 [51], to predict relative depth maps {Dt}L t=1 for frames in the dataset, where is the video length. In this way, we avoid the need of absolutely accurate depth information, making it easier for users to interact. We sample the depth at each control point: (2) t), = Dt(xi di t, yi where di is the depth value at control point in frame t. This process enriches the control points with depth information, effectively providing approximate 3D coordinates without requiring explicit 3D annotations. By combining the 2D coordinates and the estimated depth values, we construct the control trajectories: = (cid:110)(cid:8)(cid:0)xi t, yi t, di (cid:1)(cid:9)L (cid:111)N t=1 i=1 , (3) Figure 2. An example of object movement and occlusion represented by K-means clustered points. els achieve precise control, they require per-frame dense control signals, which makes them cumbersome and not user-friendly in real-world applications. Therefore, simpler yet precise control mechanisms are needed. Trajectorybased control offers an effective method for manipulating video generation, combining simplicity with precision. 2.2. Trajectory Control in Video Generation Video synthesis via trajectory control has recently gained popularity due to its ability to achieve precise motion control. Early works [13, 11] employed recurrent neural networks or optical flow to guide motion. Methods like TrailBlazer [28] utilize bounding boxes to direct subject motion in video generation. MotionCtrl [45] encodes trajectory coordinates into dense vector maps, and DragNUWA [53] transforms sparse strokes into dense flow spaces; both use these representations as guidance signals. Tora [57] employs motion variational autoencoder [22] to embed trajectory vectors into the latent space, preserving motion information across frames. Although these methods facilitate trajectory control, they often lack semantic understanding of entities, making control over video generation less refined. To address this issue, DragAnything [47] combines entity representation extraction with 2D Gaussian representation to achieve entity-level controllable video generation. TrackGo [60] uses user-provided free-form masks and arrows to define target regions and movement trajectories, serving as precise blueprints for video generation. However, all these methods consider 2D trajectories in image space, leading to ambiguities in real 3D environments. In this paper, we introduce an innovative control signal representation that combines depth information with K-means clustered points from object masks in video, achieving accurate entity-level and 3D trajectory control. 3. Method 3.1. Problem Formulation To learn realistic object motion, the training dataset should contain high-quality videos with accurate object motions. However, existing datasets that provide 3D motion trajectories are either limited in size or consist solely of This representation allows users to efficiently specify 3D trajectories by simply selecting points on 2D image and adjusting depth values as needed. We thus design our training and inference pipeline as in Sec. 3.2 and Sec. 3.3. 3 which is injected into the Stable Video Diffusion (SVD) [4] using ControlNet [54] to generate video that aligns with the 3D trajectory. Our control signal generation process is shown in Fig. 3. Our training process can be represented as: (cid:104)(cid:13) (cid:13)ϵ ϵc θ = Ezt,z0,t,ϵN (0,I) (cid:0)zt; t, z0, ctraj 2(cid:105) (cid:1)(cid:13) (cid:13) , (6) Figure 3. Control signal generation process of LeviTor. 3.3. Inference Pipeline where z0 denotes VAE-encoded latent feature of the first frame, ctraj means the control signal and ϵc θ is the combination of the denoising U-Net and the ControlNet branch. }N i=1}L 3.2. Training Pipeline Given VOS format video RLHW 3, it provides the ground truth masks of multiple objects in the video, represented as {{M j=1, where denotes the number of object masks in each frame. For each mask , we conduct K-means algorithm to obtain center points as control signal. Specifically, we first calculate the area ratio of to the entire image and multiply hyper-parameter α to determine the approximate number of cluster points: SM k = ( ) α (4) Then we assess whether there is significant change of SM , which indicates 3D related situations such as the object being occluded, moving out of the frame, or changing distance from the lens. To achieve this, we go through all video frames and calculate the ratio of the maximum to minimum area of the ith object. If the ratio exceeds 10, we ensure that the value of is not less than 3 in order to better represent the changes of this object along the temporal dimension: max(k, 3), if = k, max({S min({S otherwise, }L j=1) j=1) > 10, }L (5) We later ensure 8 to avoid the issue of having too many control points. We perform K-means clustering with the calculated value on and use the resulting cluster centers as control points. After extracting key points for all objects in each frame, we obtain the 2D coordinate information of all control points and instance information that show which object the point belongs to. We then use DepthAnythingV2 [51] to estimate the relative depth of each frame. Thus we can assign depth value to the corresponding 2D coordinate trajectories to get 3D trajectories. Finally, we represent the 2D trajectories with Gaussian heatmap and concatenate the trajectories, instance points, and depth points to serve as control signal, We have designed user-friendly interactive system for inference and the overview is provided in Fig. 4. Take an image as input, the system first automatically extracts depth information and object masks from the image using DepthAnythingV2 and SAM. Then users can utilize the retrieval panel to select the masks of objects to be moved by simply clicking on the image. They can also get relative depth values of clicked points automatically. After that, the user can use the interactive panel to click on more points to form the object trajectory. At the same time, the user can refer to the relative depth values of previously obtained click positions to input depth information of points within the trajectory according to their needs, thereby providing the corresponding 3D trajectories. With the sparse 3D trajectories and selected masks provided by user as input, we need to convert it into corresponding multi-points control information. This is because requiring users to input multiple point trajectories that comply with physical laws to represent correct occlusions and depth changes is hard. Generally, they only input single trajectory to indicate the movement of an object. Thus we need this conversion to represent the 3D movements of objects through the clustering or dispersion of control points. We achieve this by generating 3D rendered object masks then selecting control points with Kmeans, as illustrated in Fig. 5. Specifically, we first combine the 2D coordinates of pixels in the starting image with their depth values to obtain 3D spatial points, represented as {Pi}n i=1, where means the number of pixels in selected masks. Then we transform these points into the camera coordinate system. We assume that all camera intrinsic parameters are all the same and the camera to be still, so the rotation matrix is an identity matrix. The first step of transformation is converting 2D pixel points with their depth value into the camera coordinate system and moving the points belonging to user selected masks in this transformed 3D space: i=1 = {xi, yi, di}n [Xi, Yi, Zi]T = K1 [xi, yi, 1]T di, [X i]T = [Xi, Yi, Zi]T + T, , i, (7) Figure 4. Inference pipeline of LeviTor, which consists of user retrieval panel, interactive panel, 3D rendered object masks generation and video synthesis. Users can easily draw 3D trajectories through our retrieval panel and interactive panel, and our system later use these inputs to generate user desired videos. 3D space also fully comply with the laws of physics. By mapping points to 3D space and then rendering them back to 2D mask images, we convert sparse user controls into dense mask representations. These masks can accurately reflect the movement and occlusion of objects. Next, we compute cluster centers using K-means based on the masks obtained from rendering. By combining these with user-specified depth changes, we derive an appropriate number of control trajectories to generate the final video using our LeviTor. Further selecting control points with K-means is necessary because the movement process in 3D space cannot represent non-rigid transformations. If we directly use dense mask for control, it will only result in straightforward translation of the object, as demonstrated in Fig. 8. By converting the mask into moderate number of trajectory control signals, the generative model can capture the motion variation of the object while also adding some details of non-rigid movements. 4. Experiments 4.1. Experiment Settings Implementation details. We use SVD [4] as our base model. During training, we sample 16 consecutive frames from videos at spatial resolution of 288512. Specifically, we center-crop the video to an aspect ratio of 288/512, then resize the video frames to the resolution of 288512. Our LeviTor is trained for 200K iterations using the AdamW optimizer with learning rate of 1e-5. All training is conducted on 16 NVIDIA A100 GPUs with total batch Figure 5. 3D rendered object masks generation pipeline. here denotes the perspective projection matrix of camera and is the moving vectors assigned by users. After that, we render these points back to 2D images: [xi, yi]T = (cid:0)[X i]T , IDi)(cid:1) , , i, (8) is rendering function which we implement with renderer function in PyTorch3D [32] and IDi is the instance that the ith point belongs to. All the points are assigned the corresponding instance information, so rendering them back results in images with masks of different objects. In this way, we represent the movements, occlusion, and size changes due to forward and backward movements of objects only with the sparse trajectories input by the user. At the same time, the changes in 2D masks rendered from 5 Figure 6. Qualitative comparison with DragAnything [47] and DragNUWA [53]. LeviTor and DragAnything both support moving userselected mask areas, whereas DragNUWA directly encodes trajectories as control signals and does not support user selection of operation areas. The top two rows show evaluation on control of mutual occlusion between objects. The left bottom images show comparison of forward and backward object movements control. The right bottom images show case of complex motion implementation. size equal to 16. Datasets. training, we utilize the high-quality For Video Object Segmentation (VOS) dataset Segment Anything Video (SA-V) [33], which consists of 51K diverse videos and 643K high-quality spatio-temporal segmentation masks. We conduct an evaluation on the DAVIS [6] dataset and split videos into clips with 16 frames for testing. Inspired by DragAnything [47], we apply K-means to the mask of each object in the start frame to select points in each mask area as control points. Then, we employ Co-Tracker [20] to track these control points to generate corresponding point trajectories as the ground truth. Metrics. Following [45, 47], we adopt Frechet Video Distance [40] (FVD) to measure video quality and assess image quality using Frechet Inception Distance [35] (FID). For motion controllability evaluation, we leverage ObjMC [45], which computes the Euclidean distance between the generated and pre-defined trajectories. Trajectories of generated videos are extracted using Co-Tracker. 4.2. Comparison with Other Approaches We compare our methods with DragNUWA [53] and DragAnything [47], which enable motion control on given images and have publicly available code. We conduct both 6 qualitative and quantitative comparisons. Qualitative comparison. For qualitative analysis, we focus on verifying the crucial role of introducing 3D trajectories into video generation, which includes the following three aspects: 1) The control of mutual occlusion between objects; 2) Better control for forward and backward object movements in relation to the lens; 3) The implementation of complex motions (such as orbiting). Qualitative comparison results are shown in Fig. 6, where we input trajectory to all the same 2D control models. The top two rows of images show the verification results of occlusion control. In this case, we provide our LeviTor with different depth variations: the depth in the first row changes from far to near, while the depth in the second row only moves closer without being closer to the camera than the buildings on street side. The generated results perfectly meet our requirements, with the tornadoes progressing from far to near and gradually getting larger. Meanwhile, tornado in the first row sweeps across the front of the building, while in the second row it just passes behind the building. In contrast, the other two methods can only It can be control the generation through 2D trajectories. observed that DragAnything misinterprets the movement of the tornado as forward movement of the camera, resulting in blurry output. On the other hand, DragNUWA correctly understands that the tornado needs to be moved. However, since it lacks consideration of changes in depth, the size of the tornado hardly changes after the movement, which does not comply with physical laws. Evaluation results on control for forward and backward object movements in relation to the lens are shown as the left-bottom images in Fig. 6. It is clear that 2D trajectory cannot provide depth information, so DragAnything and DragNUWA can only simulate planets motion that conforms to that trajectory, resulting in blurry and uncertain videos. In contrast, LeviTor can generate accurate and clear movements of two planets based on user-specified inputs meanwhile conforming to physical laws. Based on the information input by users, we can derive 3D trajectories to control the movement of objects, which represent users desired object occlusions and size changes. Furthermore, we can simulate more complex motions, such as object orbiting. The right-bottom images in Fig. 6 shows an example and our model is able to accurately simulate the situation of black bowl rotating around vase and correctly handle the occlusion relationships. Instead, DragAnything cannot directly interpret the 2D trajectory to achieve our desired swirling effect. It only generates video where the bowl moves from right to left and then back. During this movement, the bowl also undergoes distortion and blurring. DragNUWA treats this 2D input as camera trajectory, resulting in video that shows stationary table and bowl filmed from different angles. Table 1. Quantitative comparison on DAVIS [6]. Settings Methods FID FVD ObjMC Single-Point Multi-Points DragAnything [47] DragNUWA1.5 [53] 36.69 44. 327.41 330.17 LeviTor (Ours) 28.79 226.45 DragAnything [47] DragNUWA 1.5 [53] 36.04 42. 324.95 299.96 LeviTor (Ours) 25.41 190.44 42.19 33.03 37. 38.86 23.12 25.97 The qualitative comparison results demonstrate that by introducing 3D trajectory control which allows for easy input by users, our LeviTor can better manage the proximity changes of objects. It can also produce video results that cannot be generated with only 2D trajectories, such as controlling object occlusion and executing complex movements like orbiting. Additionally, since our pipeline includes all object masks automatically extracted by SAM [23], LeviTor ensures that only objects selected by users can be moved. This prevents interpreting object movement as camera movement. And camera movement can be implemented by moving the mask of the selected background (as shown in Fig. 7). Quantitative comparison. We evaluate the quantitative results with two input settings: Single-Point and MultiPoints. The setting of Single-Point is consistent with the evaluation of previous work [47], which means that only one point trajectory is selected for each mask as the condition to generate the video. However, as we mentioned in Sec. 3, one single point trajectory cannot represent forward and backward movements of object in relation to the lens or occlusion. Therefore, we also conduct evaluation with the Multi-Points setting, in which we select at most 8 points in each mask and use their trajectories as condition. Tab. 1 shows the quantitative comparison results of our LeviTor with baselines on DAVIS. Using the same SVD as base model, our method achieves significant advantage in both FID and FVD metrics, thanks to the consideration of 3D trajectory and training on high-quality VOS dataset SA-V. Besides, increasing the number of control trajectories can effectively benefit DragNUWA and our method. This indicates that considering object size changes over time and occlusion is effective. DragAnything is trained using single trajectory with object mask semantic information in the first frame, thus increasing the number of trajectories doesnt match the training and the improvement is limited. Our LeviTor performs worse than DragNUWA on the ObjMC metric, which we attribute to the fact that we do not use tracking methods to obtain complete point trajectories and require the generated video to perfectly match these trajectories. Instead, we directly extracted K-means cluster centers of all masks in each frame as control signal to place greater emphasis on the overall motion and temporal 7 Figure 8. Ablation on number of inference control points. Table 2. Ablations on Object Instance and Depth information. Depth Instance FID FVD ObjMC 27.83 28.04 25.45 227.58 221.29 199.44 25.41 190.44 29.82 29.13 25.40 25. sword passes through the persons head. Number of control points for inference. During inference, our model can choose different number of control points to strike balance between motion amplitude and generation quality. Fig. 8 illustrates an example, where we multiply the initial number of control points by scale to evaluate the impact of different numbers of control points on generation results. It can be seen that when there are few control points, the generated result exhibits significant movement amplitude, but the object may experience some deformation or blurring during the motion. However, too many control points can get close to the objects mask. Although taking these points as control ensures the reasonableness of the objects shape, it prevents the model from generating the result of its movement. As shown in the last row of Fig. 8, the puppy will translate directly from back to front. Users can therefore adjust the number of control points according to their needs to achieve the desired generation results. 5. Conclusion for implementing 3D object In this paper, we have presented LeviTor, novel model trajectory control in image-to-video synthesis. Taking depth information combined with K-means clustered points as control signal, our approach captures essential 3D attributes without the need of explicit 3D trajectory estimation. Our user-friendly inference pipeline allows users to input 3D trajectories by simply drawing on 2D images and adjusting point depths, making the synthesis process more accessible. For future work, we aim to extend our model by incorporating Figure 7. Ablation on Instance and Depth information. Enlarged details are shown in red boxes. Zoom in for better viewing. changes of objects. 4.3. Ablation Studies In this section, we conduct ablations to study how depth points, instance information and the number of control points for inference affect our synthesis results with the Multi-Points setting. Depth and instance information. Tab. 2 shows the results of training LeviTor without depth or object instance input, which suggest that both depth and instance information are helpful to our model learning. Compared to depth information, object instance is more important because it represents the objects corresponding to different control points. Without this information, model can easily confuse the control points of different objects, leading to blurred and unrealistic results. Depth information of objects is to some extent in the degree of clustering of points, so its impact is relatively small. We also present qualitative ablation result in Fig. 7, which suggests that without instance or depth information, the model can easily confuse occlusion relationship between objects, resulting in blurry and unrealistic generation results. Specifically, without object instance information, the model estimates control points that do not belong to itself as its own, leading to phenomena such as the fusion of the sword and person, or deformation of the sword. Without depth information, the model confuses the front and back relationships when inpainting an object based on control points, resulting in cases where the body of sword disappears but the tip of implicit 8 more advanced video base models capable of capturing deformable objects and intricate dynamics, in order to better handle non-rigid motions. This enhancement will broaden the applicability of our approach to wider range."
        },
        {
            "title": "References",
            "content": "[1] Pierfrancesco Ardino, Marco De Nadai, Bruno Lepri, Elisa Ricci, and Stephane Lathuili`ere. Click to move: Controlling video generation with sparse motion. In Int. Conf. Comput. Vis., 2021. 3 [2] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. Understanding object dynamics for interactive image-to-video synthesis. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [3] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. ipoke: Poking still image for controlled stochastic video synthesis. In Int. Conf. Comput. Vis., 2021. 3 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, Stable video diffusion: Scaling and Robin Rombach. latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. 2, 4, 5 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2 [6] Sergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto Montes, Kevis-Kokitsi Maninis, and Luc Van Gool. The 2019 DAVIS challenge on VOS: unsupervised multi-object segmentation. CoRR, abs/1905.00737, 2019. 3, 6, 7, 1 [7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. CoRR, abs/2310.19512, 2023. [8] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2 [9] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Int. Conf. Comput. Vis., 2023. 2 [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toimage diffusion models without specific tuning. In Int. Conf. Learn. Represent., 2024. 2 [11] Zekun Hao, Xun Huang, and Serge J. Belongie. Controllable In IEEE Conf. video generation with sparse trajectories. Comput. Vis. Pattern Recog., 2018. 3 [12] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for highfidelity video generation with arbitrary lengths. CoRR, abs/2211.13221, 2022. 2 [13] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, and Qifeng Chen. Animate-a-story: Storytelling with retrieval-augmented video generation. CoRR, abs/2307.06940, 2023. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising In Adv. Neural Inform. diffusion probabilistic models. Process. Syst., 2020. 2 [15] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video In Adv. Neural Inform. Process. Syst., diffusion models. 2022. 2 [16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In Int. Conf. Learn. Represent., 2023. 2 [17] Li Hu. Animate anyone: Consistent and controllable imageIn IEEE Conf. to-video synthesis for character animation. Comput. Vis. Pattern Recog., 2024. [18] Zhitong Huang, Mohan Zhang, and Jing Liao. LVCD: reference-based lineart video colorization with diffusion models. CoRR, abs/2409.12960, 2024. [19] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power of sound (tpos): Audio reactive video generation with stable diffusion. In Int. Conf. Comput. Vis., 2023. 2 [20] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. CoRR, abs/2307.07635, 2023. 6 [21] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Int. Conf. Comput. Vis., 2023. 2 [22] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Int. Conf. Learn. Represent., 2014. [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross B. Girshick. Segment anything. In ICCV, pages 3992 4003. IEEE, 2023. 7 [24] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 2 [25] Seungwoo Lee, Chaerin Kong, Donghyeon Jeon, and Nojun Kwak. Aadiff: Audio-aligned video synthesis with text-toimage diffusion. CoRR, abs/2305.04001, 2023. 2 [26] Vivian Liu, Tao Long, Nathan Raw, and Lydia B. Chilton. Generative disco: Text-to-video generation for music visualization. CoRR, abs/2304.08551, 2023. 2 [27] Zichen Liu, Yihao Meng, Hao Ouyang, Yue Yu, Bolin Zhao, Daniel Cohen-Or, and Huamin Qu. Dynamic typography: Bringing text to life via video diffusion prior. CoRR, abs/2404.11614, 2024. 2 9 [28] Wan-Duo Kurt Ma, John P. Lewis, and W. Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. CoRR, abs/2401.00896, 2024. [29] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: PoseIn guided text-to-video generation using pose-free videos. Assoc. Adv. Artif. Intell., 2024. 2 [30] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex RavAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. CoRR, abs/2302.01329, 2023. 2 [31] Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, and Yueting Zhuang. Dancing avatar: Pose and text-guided human motion videos synthesis with image diffusion model. CoRR, abs/2308.07749, 2023. 2 [32] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. CoRR, Accelerating 3d deep learning with pytorch3d. abs/2007.08501, 2020. 5 [33] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross B. Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. CoRR, abs/2408.00714, 2024. 2, 3, 6 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In IEEE Conf. synthesis with latent diffusion models. Comput. Vis. Pattern Recog., 2022. [35] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, 2020. Version 0.3.0. 6 [36] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In Int. Conf. Learn. Represent., 2023. 2 [37] Jascha Sohl-Dickstein, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised In Int. learning using nonequilibrium thermodynamics. Conf. Mach. Learn., 2015. 2 A. Weiss, Eric [38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. CoRR, abs/2010.02502, 2020. 2 [39] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. EMO: emote portrait alive - generating expressive portrait videos with audio2video diffusion model under weak conditions. CoRR, abs/2402.17485, 2024. 2 [40] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. CoRR, abs/1812.01717, 2018. 6 [41] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In Int. Conf. Learn. Represent., 2023. 2 [42] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. CoRR, abs/2308.06571, 2023. 2 [43] Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Interactive frame interpolation, Chunhua Shen. Framer: 2024. 2 [44] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. LAVIE: high-quality video generation with cascaded latent diffusion models. CoRR, abs/2309.15103, 2023. 2 [45] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. 1, 3, 6 [46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Int. Conf. Comput. Vis. IEEE, 2023. 2 [47] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In Eur. Conf. Comput. Vis., 2024. 1, 2, 3, 6, [48] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Makeyour-video: Customized video generation using textual and structural guidance. CoRR, abs/2306.00943, 2023. 2 [49] Jinbo Xing, Menghan Xia, Yong Zhang, Hao Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In Eur. Conf. Comput. Vis., 2024. 2 [50] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image In IEEE Conf. Comput. animation using diffusion model. Vis. Pattern Recog., 2024. 2 [51] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 3, 4 [52] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. CoRR, abs/2408.06072, 2024. 2 [53] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. CoRR, abs/2308.08089, 2023. 1, 2, 3, 6, 7 [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. ICCV, pages 38133824. IEEE, 2023. 4 In [55] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. CoRR, abs/2311.04145, 2023. 2 [56] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. CoRR, abs/2406.19680, 2024. 2 [57] Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. CoRR, abs/2407.21705, 2024. 3 [58] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 2 [59] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient CoRR, video generation with latent diffusion models. abs/2211.11018, 2022. 2 [60] Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. CoRR, abs/2408.11475, 2024. 1, 2, [61] Bingwen Zhu, Fanyi Wang, Tianyi Lu, Peng Liu, Jingwen Su, Jinxiu Liu, Yanhao Zhang, Zuxuan Wu, Guo-Jun Qi, Zero-shot high-fidelity and poseand Yu-Gang Jiang. In Int. Joint Conf. Artif. controllable character animation. Intell., 2024. 2 [62] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. CoRR, abs/2403.14781, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Visualization results We have placed the visualization results generated by LeviTor in the folder of supplementary material. You can open the HTML file within the folder locally to view the video synthesis results. Please refresh the webpage if the GIFs appear to be out of sync. B. More Ablations on the Number of Control"
        },
        {
            "title": "Points for Inference",
            "content": "In this section, we show more examples of choosing different numbers of control points to generate videos with LeviTor. We conduct inference with our default number of control points and with more densely packed points, respectively. The results are shown in Fig. S1. It can be seen that with the default number of control points, our LeviTor can reasonably represent the state of fluid movement and human running. However, since the generation strictly follows the control points, the more control points used, the less space is left for our model to produce some non-rigid movements, resulting in the unreasonable results of waves floating in the air and people gliding on the road. This demonstrates that overly dense control points cannot generate non-rigid motion well. Thus, we implement LeviTor with multiple clustered points control rather than directly using object masks as the condition. In this way, users can flexibly adjust the number of control points as needed to generate both rigid and nonrigid motions. Table S1. Quantitative comparison with Single-point Control on DAVIS [6]."
        },
        {
            "title": "Methods",
            "content": "FID FVD ObjMC Single-Point Control Ours 30.91 25.41 253.73 190.44 38.21 25. C. Comparison with Single-point Control One of our key motivations is to represent 3D motions by utilizing the clustering and dispersion of multiple points within object masks. Another more intuitive idea is whether we can represent 3D motion using 2D trajectories combined with depth information. That is, representing 3D trajectory through single 2D trajectory along with changes of depth values input by users. To validate this idea, we use the center point of each objects mask as control point and train the model with the value change of that point as the generation condition. We conduct both qualitative and quantitative analysis. Qualitative results in Fig. S2 show that such single-point control can not Figure S1. Ablation results on the Number of Control Points for Inference. We highly recommend viewing the visualization results for detailed video demonstrations. represent 3D motions well. The first two examples test the representation of occlusion. It can be observed that single point with depth changes controlling struggles to accurately express occlusion, resulting in the disappearance of the purple light cluster and the deformation and merging of the cars. The third example tests the control of forward and backward movements. Compared to our LeviTor, single-point control is not very sensitive to size changes caused by forward and backward movement. Quantitative results in Tab. S1 also show the advantage of 3D motion representation with clustering and dispersion of multiple points. The ablation study in Tab. 2 of the main text indicates that the value of depth does not significantly affect the quality of the generated results. And results in this section show that 2D trajectories with depth value changes can not represent 3D motions. These conclusions both suggest that in our method, the clustering and dispersion of multiple control points are the key aspects of 3D motion representation, while depth information is generally used for moving objects in 3D space to obtain rendered object 1 Figure S3. Bad Cases of LeviTor. In the second row, of objects. For example, in the first row of Fig. S3, the horse faces become blurry while walking, and the movement of their legs is also quite unnatural. Similarly, in Fig. S1, the movement of the persons feet while running also appears unnatural. the elephants front leg suddenly turns into back one, and then regenerated front leg appears. We attribute this phenomenon to the fact that the underlying video base model Stable Video Diffusion (SVD) [4] we apply is unable to reconstruct small faces and tends to produce artifacts when generating large-scale movements. We are going to enhance our model by integrating more advanced video-based models in the future, hoping to better capture deformable objects and complex dynamics to handle large-scale and non-rigid motions. Figure S2. Comparison with Single-point Control model. We highly recommend viewing the visualization results for detailed video demonstrations. masks. D. Bad Case Analysis We, in this section, list some bad generation cases for analysis. Results shown in Fig. S3 indicate that our LeviTor has difficulties in reconstructing small faces and generating scenes with large motions. It may also confuse similar parts"
        }
    ],
    "affiliations": [
        "Ant Group",
        "State Key Laboratory for Novel Software Technology, Nanjing University",
        "The Hong Kong University of Science and Technology",
        "Zhejiang University"
    ]
}