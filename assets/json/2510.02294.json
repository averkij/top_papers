{
    "paper_title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data",
    "authors": [
        "Ziyin Zhang",
        "Zihan Liao",
        "Hang Yu",
        "Peng Di",
        "Rui Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 9 2 2 0 . 0 1 5 2 : r F2LLM Technical Report F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data Ziyin Zhang1,2 Zihan Liao1 Hang Yu ,1 Peng Di,1 Rui Wang,2 1Ant Group 2Shanghai Jiao Tong University https://github.com/codefuse-ai/CodeFuse-Embeddings https://huggingface.co/collections/codefuse-ai/codefuse-embeddings Figure 1: (Left): MTEB performance comparison between LLM-based embedding models. (Right): F2LLM, trained solely on open-source non-synthetic data, achieves strong balance between embedding performance, training data, and model size. Higher scores indicate better performance (left axis), fewer training data (right axis), and smaller model size (bottom axis)."
        },
        {
            "title": "Abstract",
            "content": "We introduce F2LLM - Foundation to Feature Large Language Models, suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous topranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from opensource, non-synthetic datasets, striking strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as strong, reproducible, and budget-friendly baseline for future works. Correspondence to: Hang Yu <hyu.hugo@antgroup.com>, Peng Di <dipeng.dp@antgroup.com>, Rui Wang <wangrui12@sjtu.edu.cn>. 1 F2LLM Technical Report"
        },
        {
            "title": "Introduction",
            "content": "In recent years, LLM-based embedding models have rapidly advanced information retrieval, clustering, classification, and other embedding-based applications, as witnessed by their increasing performance on the MTEB leaderboard (Muennighoff et al., 2023; Enevoldsen et al., 2025). These models adapt foundation LLMs such as Mistral (Jiang et al., 2023) and Qwen3 (Yang et al., 2025) to generate high-quality text embeddings via contrastive training on query-document pairs, efficiently utilizing their textual understanding acquired during large-scale pretraining. However, most SOTA embedding models either adopt sophisticated multi-stage training pipelines involving large-scale weakly supervised pretraining (Li et al., 2023; Chen et al., 2024; Lee et al., 2025a; Zhang et al., 2025), or rely on costly synthetic data generated by LLMs (Wang et al., 2024; Lee et al., 2025a;b), posing significant challenges in reproducing these models. Moreover, most of these models only open-source model checkpoints but not training script sand data, leading to inconsistencies in the literature. To address these challenges, we introduce F2LLM, the new state-of-the-art embedding model family from Codefuse after D2LLM (Liao et al., 2024). Unlike the previously mentioned models, F2LLM is finetuned directly from foundation models on 6 million high-quality query, document, and hard negative tuples, collected solely from open-source non-synthetic datasets and covering diverse range of task types. Moreover, F2LLM is fully open-source, with model checkpoints, training data, and training code all publicly released, providing strong, reproducible baseline for future research in training and improving embedding models. As of September 2025, F2LLM 4B ranks 2nd on the MTEB English leaderboard among models around 4B size, and 7th overall, only after models that are either closed-source or trained on hundreds of millions of data. Meanwhile, F2LLM 1.7B ranks 1st among models of 1B-2B size, making it an ideal choice for applications with limited computational resources."
        },
        {
            "title": "2 Related Work",
            "content": "Adapting LLMs into embedding models has dominated the embedding literature in recent years. Most of these models adopt two-stage training pipeline, with billion-scale weakly supervised contrastive pretraining followed by supervised finetuning (Li et al., 2023; Chen et al., 2024; Lee et al., 2024; Zhang et al., 2024; Lee et al., 2025b; Zhang et al., 2025). few of them, however, choose different designs, either benefiting from high-quality data or being constrained by heterogeneous data formats. For example, E5-Mistral (Wang et al., 2024) and LGAI-Embedding (Choi et al., 2025) are both finetuned in single stage, while NV-Embed (Lee et al., 2025a) is first trained on retrieval data with both in-batch loss and hard negative loss, and then on mixture of retrieval and non-retrieval data with only hard negative loss. Another common characteristic of LLM-based embedding models is the usage of synthetic data. E5-Mistral (Wang et al., 2024), Gecko (Lee et al., 2024), NV-Embed (Lee et al., 2025a), GeminiEmbedding (Lee et al., 2025b) and Qwen3-Embedding (Zhang et al., 2025) all utilize existing LLMs to generate high-quality data for supervised finetuning or even pretraining. However, recently LGAIEmbedding (Choi et al., 2025) achieved SOTA performance on the MTEB leaderboard, demonstrating that carefully curated composite of open-source datasets can lead to the same performance without the expansive costs of synthetic data generation. Finally, the increase in embedding performance often comes at the cost of increased computation or modified model architectures. Models such as BGE-ICL (Li et al., 2025), PromptEOL (Jiang et al., 2024), and LGAI-Embedding (Choi et al., 2025) utilize in-context learning to improve embedding quality, while ECHO (Springer et al., 2025) repeats the input to overcome the limitation of causal attention in LLMs. Other models, in contrast, choose to remove the causal mask (BehnamGhader et al., 2024; Muennighoff et al., 2025; Lee et al., 2025a;b; Zhao et al., 2025). While this modification to attention does not increase computational cost, research has demonstrated that it does not lead to consistent performance gain either (Li et al., 2025). 2 F2LLM Technical Report In contrast to prior work, F2LLM is trained directly from foundation models in single stage, without any modification to the base LLMs architecture or input format. More importantly, F2LLM is trained solely on open-source data, marking it as reproducible and budget-friendly baseline for future works."
        },
        {
            "title": "3 F2LLM",
            "content": "To address the problem of scattered and inconsistent training data in embedding model research, we compile large-scale composite covering 4.9M retrieval samples, 0.2M classification samples, and 0.8M clustering samples in unified format. For all datasets, we use the training set provided by MTEB whenever available (Muennighoff et al., 2023; Enevoldsen et al., 2025), and otherwise perform decontamination if corresponding test set exists in MTEB. The complete list of data sources and related statistics are given in Appendix A."
        },
        {
            "title": "3.1 Data Collection and Formatting",
            "content": "To simplify the training process of embedding models, we compile data from different types of tasks in unified format. Each data sample consists of (query, positive passage, hard negative n) tuple, where is set to 24 for retrieval and clustering tasks, and 1 for classification tasks. Following the practices of SOTA models, we append task-specific instruction to each query in the following format: qinst = Instruct: {task_instruction} Query:{q}, (1) where the complete list of task instructions are given in Appendix B."
        },
        {
            "title": "3.1.1 Retrieval Data and Margin-Based Adaptive Hard Negative Mining",
            "content": "In F2LLM training data, retrieval, summarization, NLI, STS, and paraphrase (i.e. duplicate detection) datasets are collated into unified retrieval format. Existing retrieval and question-answering datasets typically come in the form of queries, passages, and query-passage relation matrix. For each query, we sample related passage as the positive, and mine hard negatives using the Sentence Transformers library (Filippova & Altun, 2013). For each query in dataset, we retrieve the top 100 most relevant passages using Qwen3-Embedding0.6B (Zhang et al., 2025) but exclude the top 5 passages to avoid false negatives. We filter these passages to retain only those with score below 0.8 and also less than 95% below the score for the positive. We then select the top 24 passages in the remaining pool as hard negatives for the query, and discard the query if there are fewer than 24 passages left. Summarization datasets are also considered in this category, where the original text serves as the passage and its summary as the query. For natural language inference (NLI) datasets, we retain only premises with at least one entailed hypothesis as queries, and sample one of the entailed hypotheses as the positive. If the query is also paired with neutral or contradictory hypotheses, we add them as hard negatives, and mine the remaining hard negatives as described above. For textual similarity (STS) datasets, we follow the practice of Lee et al. (2025a) and construct query-positive pair from any sentence pair whose similarity score is at least 4 and another pair with the query and positive switched. Hard negatives are also mined as described above. Finally, for duplicate detection datasets (e.g. QQP Sharma et al., 2019), we use duplicate questions as query-positive pairs, and similarly mine hard negatives."
        },
        {
            "title": "3.1.2 Classification and Clustering Data",
            "content": "For binary classification datasets, we utilize Amazon Counterfactual (ONeill et al., 2021), Amazon Polarity (McAuley & Leskovec, 2013), IMDb sentiment classification (Maas et al., 2011), Toxic 3 F2LLM Technical Report Conversations (cjadams et al., 2019), and CoLA (Warstadt et al., 2019). Following previous works (Lee et al., 2025a), we treat each input as query, its label text (e.g. toxic) as the positive passage, and the other classs label text (e.g. not toxic) as one hard negative. For multi-way classification and clustering datasets, we also follow Lee et al. (2025a). For each query, random sample from the same class or cluster is used as its positive passage, and 24 samples from other classes are selected as hard negatives."
        },
        {
            "title": "3.2 Training",
            "content": "Given query qi, its corresponding document d+ negative loss is computed using the contrastive learning loss , and hard negatives i,1, i,2, , i,n, hard ℓhard = log es(qi,d+ )/τ + s(qi,d i,j)/τ es(qi,d+ )/τ j=1 , (2) where τ = 0.05 is the temperature, and s() is similarity score implemented as the cosine similarity between query and document embeddings. Similarly, in-batch loss is computed by ℓin-batch = log )/τ es(qi,d+ s(qi,d+ j=1 )/τ , where is the batch size. The final loss is an unweighted sum of hard negative and in-batch loss: ℓ = ℓhard + ℓin-batch. (3) (4) Hard negative loss is computed for all tasks. In contrast, in-batch loss is only computed for retrieval tasks. This is achieved by implementing custom multitask dataloader, which ensures that samples in each micro batch belong to the same data source. In each training step, the dataloader on each GPU independently samples data source with probabilities proportional to the size of the data source to ensure that all datasets finish one epoch before any dataset starts the second iteration. If retrieval or clustering dataset is selected, 7 negatives are randomly sampled from the pool of 24 negatives for each query in the batch, while for classification datasets only one negative is used as described in Section 3.1.2. Hard negative loss is calculated independently on each GPU for all datasets, while in-batch loss is calculated only for retrieval tasks, using all passages in the entire mini batch1. We note that this differs from recent works that disable in-batch loss entirely after blending in non-retrieval data (Lee et al., 2025a), allowing for greater sample efficiency. Table 1: Hyperparameters for training F2LLM models. Model Learning Rate Global Batch Size Num. GPUs Micro Batch Size 0.6B 1.7B 4B 1e-5 9e-6 8e-6 512 512 512 16 16 32 32 32 16 We use Qwen3 models (Yang et al., 2025) as the backbone, and directly conduct contrastive finetuning without any weakly supervised pretraining due to the large scale and high quality of our 1We use micro batch to refer to data on single GPU in one optimization step, and mini batch to refer to the collection of data on all GPUs in one optimization step. 4 F2LLM Technical Report Table 2: Top models on the MTEB leaderboard. Average column is the micro average of all 41 tasks."
        },
        {
            "title": "Model",
            "content": "Classification"
        },
        {
            "title": "Clustering",
            "content": "PairClassification"
        },
        {
            "title": "STS",
            "content": "Summarization"
        },
        {
            "title": "Average Rank",
            "content": "Seed1.5-Embedding Seed1.6-Embedding Gemini Embedding QZhou-Embedding 7B Qwen3-Embedding 8B LGAI-Embedding 7B GTE-Qwen2 7B GeoEmbedding 7B Qwen3-Embedding 4B F2LLM 4B F2LLM 1.7B Jasper 2B Qwen3-Embedding 0.6B F2LLM 0.6B 89.88 92.42 90. 88.97 90.43 89.97 88.52 89.67 89.84 91.68 90.86 90.27 85.76 90.56 60.83 59.22 59.39 61.65 58.57 59.25 58.97 56. 57.51 68.54 64.13 60.52 54.05 60.36 Closed-source 87.39 85.07 87.70 92.43 87.52 88.67 85.90 82. 87.01 83.75 83.27 88.14 84.37 81.49 7-8B 4B 1-2B <1B 50.67 50.28 48.60 51.77 51.56 49.13 50.47 48.32 50.76 50.05 49.84 50.00 48.18 47. 67.45 64.90 64.35 67.12 69.44 66.18 58.09 60.91 68.46 59.63 57.83 56.05 61.83 55.70 87.23 86.87 85. 91.65 88.58 86.69 82.69 80.60 88.72 84.20 83.90 84.37 86.57 82.48 36.44 37.10 38.28 33.05 34.83 38.93 35.74 30. 34.39 33.19 29.88 37.19 33.43 24.54 74.76 74.07 73.30 75.97 75.22 74.12 70.72 70.21 74.60 73. 72.01 71.41 70.70 70.03 3 6 8 1 2 5 11 13 4 7 9 12 14 dataset. We train models of three sizes: 0.6B, 1.7B, 4B. The models are trained with AdamW optimizer (Loshchilov & Hutter, 2019) and cosine learning rate decay for 2 epochs with 500 warmup steps. ZeRO stage 2 (Rajbhandari et al., 2020), Flash Attention 2 (Dao, 2024), and gradient checkpointing are enabled to reduce GPU memory usage. Max input length is set to 1024 tokens, and the rest of the hyperparameters are given in Table 1."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate the models on 41 English tasks in MTEB (Muennighoff et al., 2023; Enevoldsen et al., 2025, task details given in Appendix B), and compare the results with state-of-the-art embedding models on the leaderboard in Table 2. Overall, F2LLM achieves performance comparable to SOTA embedding models trained on hundreds of millions of data samples. F2LLM-4B ranks 2nd among models of similar size and 7th overall on the leaderboard, while F2LLM-1.7B ranks 1st among models in 1B-2B size, and F2LLM-0.6B ranks 2nd among models with less than 1B parameters. Notably, F2LLM excels in clustering tasks, with the 4B model scoring 68.54, setting new record among all models."
        },
        {
            "title": "5 Conclusion",
            "content": "We present F2LLM, family of fully open embedding LLMs that achieve strong balance between model size, training data, and embedding performance. The model checkpoints, training dataset, and training code are released, positioning F2LLM as strong, reproducible, and budget-friendly baseline for future research in text embedding models."
        },
        {
            "title": "References",
            "content": "Eneko Agirre, Daniel M. Cer, Mona T. Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: pilot on semantic textual similarity. In Eneko Agirre, Johan Bos, and Mona T. Diab (eds.), Proceedings of the 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012, 5 F2LLM Technical Report Montréal, Canada, June 7-8, 2012, pp. 385393. The Association for Computer Linguistics, 2012. URL https://aclanthology.org/S12-1051/. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. CoRR, abs/2404.05961, 2024. doi: 10.48550/ARXIV.2404.05961. URL https: //doi.org/10.48550/arXiv.2404.05961. Vera Boteva, Demian Gholipour Ghalandari, Artem Sokolov, and Stefan Riezler. full-text learning to rank dataset for medical information retrieval. In Nicola Ferro, Fabio Crestani, Marie-Francine Moens, Josiane Mothe, Fabrizio Silvestri, Giorgio Maria Di Nunzio, Claudia Hauff, and Gianmaria Silvello (eds.), Advances in Information Retrieval - 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20-23, 2016. Proceedings, volume 9626 of Lecture Notes in Computer Science, pp. 716722. Springer, 2016. doi: 10.1007/978-3-319-30671-1_58. URL https://doi.org/10.1007/ 978-3-319-30671-1_58. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. large annotated corpus for learning natural language inference. In Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 632642. The Association for Computational Linguistics, 2015. doi: 10.18653/V1/D15-1075. URL https://doi.org/10.18653/v1/d15-1075. Iñigo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. Efficient intent detection with dual sentence encoders. CoRR, abs/2003.04807, 2020. URL https://arxiv. org/abs/2003.04807. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024. doi: 10.48550/ARXIV.2402.03216. URL https: //doi.org/10.48550/arXiv.2402.03216. Xi Chen, Ali Zeynali, Chico Q. Camargo, Fabian Flöck, Devin Gaffney, Przemyslaw A. Grabowicz, Scott Hale, David Jurgens, and Mattia Samory. Semeval-2022 task 8: Multilingual news article similarity. In Guy Emerson, Natalie Schluter, Gabriel Stanovsky, Ritesh Kumar, Alexis Palmer, Nathan Schneider, Siddharth Singh, and Shyam Ratan (eds.), Proceedings of the 16th International Workshop on Semantic Evaluation, SemEval@NAACL 2022, Seattle, Washington, United States, July 14-15, 2022, pp. 10941106. Association for Computational Linguistics, 2022. doi: 10.18653/V1/20 22.SEMEVAL-1.155. URL https://doi.org/10.18653/v1/2022.semeval-1.155. Jooyoung Choi, Hyun Kim, Hansol Jang, Changwook Jun, Kyunghoon Bae, Hyewon Choi, Stanley Jungkyu Choi, Honglak Lee, and Chulmin Yun. Lgai-embedding-preview technical report. CoRR, abs/2506.07438, 2025. doi: 10.48550/ARXIV.2506.07438. URL https: //doi.org/10.48550/arXiv.2506.07438. cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and nithum. Jigsaw unintended bias in toxicity classification, 2019. URL https://kaggle.com/competition s/jigsaw-unintended-bias-in-toxicity-classification. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. SPECTER: documentlevel representation learning using citation-informed transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 22702282. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACLMAIN.207. URL https: //doi.org/10.18653/v1/2020.acl-main.207. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=mZn2Xyh9Ec. 6 F2LLM Technical Report Kenneth C. Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Veysel Çagatan, Akash Kundu, and et al. MMTEB: massive multilingual text embedding benchmark. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum ?id=zl3pfz4VCV. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pp. 35583567. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1346. URL https://doi.org/10.18653/v1/p19-1346. Katja Filippova and Yasemin Altun. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, meeting of SIGDAT, Special Interest Group of the ACL, pp. 14811491. ACL, 2013. doi: 10.18653/V1/D13-1155. URL https://doi.org/10.18653/v1/d13-1155. Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gökhan Tür, and Prem Natarajan. MASSIVE: 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 42774302. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.235. URL https://doi.org/10.18653/v1/2023.acl-long.235. Gregor Geigle, Nils Reimers, Andreas Rücklé, and Iryna Gurevych. TWEAC: transformer with extendable QA agent classifiers. CoRR, abs/2104.07081, 2021. URL https://arxiv.org/abs/21 04.07081. Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C. Lipton. Amazonqa: review-based question answering task. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 49965002. ijcai.org, 2019. doi: 10.24963/IJCAI.2019/694. URL https://doi.org/ 10.24963/ijcai.2019/694. Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 16931701, 2015. URL https://proceedings. neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/10.48550/arXiv.2310.06825. Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling sentence embeddings with large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 31823196. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-EMNLP.181. URL https://doi.org/10.18653/v1/2024.findi ngs-emnlp.181. 7 F2LLM Technical Report Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLPIJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 25672577. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1259. URL https://doi.org/10.18653/v1/D19-1259. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 16011611. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL https://doi.org/10.18653 /v1/P17-1147. Mi-Young Kim, Juliano Rabelo, Randy Goebel, Masaharu Yoshioka, Yoshinobu Kano, and Ken Satoh. COLIEE 2022 summary: Methods for legal document retrieval and entailment. In Yasufumi Takama, Katsutoshi Yada, Ken Satoh, and Sachiyo Arai (eds.), New Frontiers in Artificial Intelligence - JSAI-isAI 2022 Workshop, JURISIN 2022, and JSAI 2022 International Session, Kyoto, Japan, June 12-17, 2022, Revised Selected Papers, volume 13859 of Lecture Notes in Computer Science, pp. 5167. Springer, 2022. doi: 10.1007/978-3-031-29168-5_4. URL https://doi.org/10.1007/978-3-031 -29168-5_4. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466, 2019. doi: 10.1162/TACL_A_00276. URL https://doi.org/10.1162/ta cl_a_00276. Ken Lang. Newsweeder: Learning to filter netnews. In Armand Prieditis and Stuart Russell (eds.), Machine Learning, Proceedings of the Twelfth International Conference on Machine Learning, Tahoe City, California, USA, July 9-12, 1995, pp. 331339. Morgan Kaufmann, 1995. doi: 10.1016/B978-1-55860 -377-6.50048-7. URL https://doi.org/10.1016/b978-1-55860-377-6.50048-7. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025a. URL https://openreview.net/forum?id=lgsyLSsDRe. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernández Ábrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. Gecko: Versatile text embeddings distilled from large language models. CoRR, abs/2403.20327, 2024. doi: 10.48550/ARXIV.2403.20327. URL https://doi.org/10.48550/arXiv.2403.20327. Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo Hernández Ábrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, Xiaoqi Ren, Shanfeng Zhang, Daniel Salz, Michael Boratko, Jay Han, Blair Chen, Shuo Huang, Vikram Rao, Paul Suganthan, Feng Han, Andreas Doumanoglou, Nithi Gupta, Fedor Moiseev, Cathy Yip, Aashi Jain, Simon Baumgartner, Shahrokh Shahi, Frank Palma Gomez, Sandeep Mariserla, Min Choi, Parashar Shah, Sonam Goenka, Ke Chen, Ye Xia, Koert Chen, Sai Meher Karthik Duddu, Yichang Chen, Trevor Walker, Wenlei Zhou, Rakesh Ghiya, Zach Gleicher, Karan Gill, Zhe Dong, Mojtaba Seyedhosseini, Yun-Hsuan Sung, Raphael Hoffmann, and Tom Duerig. Gemini embedding: Generalizable embeddings from gemini. CoRR, abs/2503.07891, 2025b. doi: 10.48550/ARXIV.2503.07891. URL https://doi.org/10.48550/arXiv.2503.07891. F2LLM Technical Report Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. PAQ: 65 million probably-asked questions and what you can do with them. Trans. Assoc. Comput. Linguistics, 9:10981115, 2021. doi: 10.1162/TACL_A_0 0415. URL https://doi.org/10.1162/tacl_a_00415. Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Defu Lian, Yingxia Shao, and Zheng Liu. Making text embedders few-shot learners. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=wfLuiDjQ0u. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. MTOP: comprehensive multilingual task-oriented semantic parsing benchmark. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 29502962. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EACL-MAI N.257. URL https://doi.org/10.18653/v1/2021.eacl-main.257. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. CoRR, abs/2308.03281, 2023. doi: 10.48550/ARXIV.2308.03281. URL https://doi.org/10.48550/arXiv.2308.03281. Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, and Wei Zhang. D2LLM: decomposed and distilled large language models for semantic search. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1479814814. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.791. URL https: //doi.org/10.18653/v1/2024.acl-long.791. Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang Zhai. Linkso: dataset for learning to retrieve similar question answer pairs on software development forums. In Yijun Yu, Erik M. Fredericks, and Premkumar T. Devanbu (eds.), Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering, NL4SE@ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 4, 2018, pp. 25. ACM, 2018. doi: 10.1145/3283812.3283815. URL https://doi.org/10 .1145/3283812.3283815. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. S2ORC: the semantic scholar open research corpus. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 49694983. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.447. URL https://doi.org/10.18653/v1/2020.acl-main.447. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pp. 142150. The Association for Computer Linguistics, 2011. URL https://aclanthology.org/P11 -1015/. Wei Chen Maggie, Phil Culliton. Tweet sentiment extraction, 2020. URL https://kaggle.com/com petitions/tweet-sentiment-extraction. Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www18 open challenge: Financial opinion mining and question answering. In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis 9 F2LLM Technical Report (eds.), Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, pp. 19411942. ACM, 2018. doi: 10.1145/3184558.3192301. URL https://doi.org/10.1145/3184558.3192301. Philip May. Machine translated multilingual sts benchmark dataset., 2021. URL https://github.c om/PhilipMay/stsb-multi-mt. Julian J. McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Qiang Yang, Irwin King, Qing Li, Pearl Pu, and George Karypis (eds.), Seventh ACM Conference on Recommender Systems, RecSys 13, Hong Kong, China, October 12-16, 2013, pp. 165172. ACM, 2013. doi: 10.1145/2507157.2507163. URL https://doi.org/10.1 145/2507157.2507163. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pp. 20062029. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.EACL-MAIN.148. URL https://doi.org/10.18653/v1/2023.eacl-main.148. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=BC4lIvfSzv. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 1797 1807. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1206. URL https://doi.org/10.18653/v1/d18-1206. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. dAvila Garcez, and Greg Wayne (eds.), Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL https://ceur-ws. org/Vol-1773/CoCoNIPS_2016_paper9.pdf. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 48854901. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACLMAIN.441. URL https: //doi.org/10.18653/v1/2020.acl-main.441. James ONeill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. wish would have loved this one, but didnt - multilingual dataset for counterfactual detection in product review. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 70927108. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.568. URL https://doi.org/10.18653/v1/2021.emnlp-main.568. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. 10 F2LLM Technical Report IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https://doi.org/10.1109/SC41405. 2020.00024. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 23832392. The Association for Computational Linguistics, 2016. doi: 10.18653/V1/D16-1264. URL https://doi.org/10.18653/v1/d16-1264. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: contextualized affect representations for emotion recognition. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 3687 3697. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1404. URL https://doi.org/10.18653/v1/d18-1404. Lakshay Sharma, Laura Graesser, Nikita Nangia, and Utku Evci. Natural language understanding with the quora question pairs dataset. CoRR, abs/1907.01041, 2019. URL http://arxiv.org/ab s/1907.01041. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=Ahlrf2HGJR. Flax Sentence Embeddings Team. Stackexchange title-body pairs, 2021a. URL https://huggingfac e.co/datasets/flax-sentence-embeddings/stackexchange_title_body_jsonl. Flax Sentence Embeddings Team. Stack exchange question pairs, 2021b. URL https://huggingfac e.co/datasets/flax-sentence-embeddings/. MTEB Team. Arxiv raw data, 2022a. URL https://huggingface.co/datasets/mteb/raw_arxiv. MTEB Team. Biorxiv raw data, 2022b. URL https://huggingface.co/datasets/mteb/raw_biorx iv. MTEB Team. Medrxiv raw data, 2022c. URL https://huggingface.co/datasets/mteb/raw_med rxiv. Sentence Transformers Team. Embedding training data, 2021c. URL https://huggingface.co/dat asets/sentence-transformers/. Sentence Transformers Team. Reddit title-body pairs, 2021d. URL https://huggingface.co/datas ets/sentence-transformers/reddit-title-body. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: largescale dataset for fact extraction and verification. In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 809819. Association for Computational Linguistics, 2018. doi: 10.18653/V1/N18-1074. URL https://doi.org/10.18653/v1/n18-1074. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R. Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artières, Axel-Cyrille Ngonga Ngomo, Norman Heino, Éric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. BMC Bioinform., 16:138:1138:28, 2015. doi: 10.1186/S12859-015-0564-6. URL https://doi.org/10.1186/s12859 -015-0564-6. 11 F2LLM Technical Report Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 241251. Association for Computational Linguistics, 2018. doi: 10.18653/V1/P18-1023. URL https://aclanthology.org/P18-1023/. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 75347550. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.609. URL https: //doi.org/10.18653/v1/2020.emnlp-main.609. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1189711916. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.642. URL https: //doi.org/10.18653/v1/2024.acl-long.642. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Trans. Assoc. Comput. Linguistics, 7:625641, 2019. doi: 10.1162/TACL_A_00290. URL https: //doi.org/10.1162/tacl_a_00290. Adina Williams, Nikita Nangia, and Samuel R. Bowman. broad-coverage challenge corpus for sentence understanding through inference. In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 11121122. Association for Computational Linguistics, 2018. doi: 10.18653/V1/N18-1101. URL https://doi.org/10.18653/v1/n18-1101. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 23692380. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL https://doi.org/10.18653/v1/d18-1259. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval. In Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, pp. 13931412. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.EMNLP-INDUSTRY.103. URL https://doi.org/10.18653/v1/20 24.emnlp-industry.103. 12 F2LLM Technical Report Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: multi-lingual benchmark for dense retrieval. CoRR, abs/2108.08787, 2021. URL https://arxiv.org/abs/2108.08787. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Trans. Assoc. Comput. Linguistics, 11:11141131, 2023. doi: 10.1162/TACL_A_00595. URL https://doi.org/10.1162/tacl_a_00595. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. CoRR, abs/2506.05176, 2025. doi: 10.48550/ARXIV.2506.05176. URL https://doi.org/10.48550/arXiv.2506.05176. Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Qian Chen, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, and Min Zhang. Kalm-embedding-v2: Superior training techniques and data inspire versatile embedding model. CoRR, abs/2506.20923, 2025. doi: 10.48550/ARX IV.2506.20923. URL https://doi.org/10.48550/arXiv.2506.20923. F2LLM Technical Report"
        },
        {
            "title": "A Training Data",
            "content": "Table 3: Classification and clustering datasets used for training F2LLM. Average query and corpus lengths are measured by the number of Qwen3 tokens, and the length of queries includes instructions. Dataset Type # Query Corpus Size Query Length Corpus Length Source URL Amazon Counterfactual Classification 8,663 Amazon Polarity Classification 100,000 IMDb Toxic tions CoLA Classification 24,904 ConversaClassification 49,900 Classification 9, 2 2 2 2 2 Amazon Reviews Clustering 100,000 99,966 Banking77 Clustering 9, Emotion Clustering 17,944 MTOP Intent Clustering 17, MTOP Domain Clustering 17,538 Massive Scenario Clustering 13, Massive Intent Clustering 13,547 9,993 17,944 17, 17,531 13,488 13,488 26,732 Clustering 26, Clustering 83,476 299,733 Clustering 83,486 299, Clustering 57,296 Clustering 57,296 Clustering 18, Clustering 18,659 57,215 57,204 18,653 18, Clustering 80,000 489,218 Sentiment Tweet Extraction Arxiv-ClusteringP2P Arxiv-ClusteringS2S Biorxiv-ClusteringP2P Biorxiv-ClusteringS2S MedrxivClustering-P2P MedrxivClustering-S2S Reddit-ClusteringP2P Reddit-ClusteringS2S StackExchangeClustering-P2P StackExchangeClustering-S2S 47 115 314 28 63 31 54 28 26 26 40 245 38 42 453 45 194 4 ONeill et al. (2021) McAuley Leskovec (2013) & 2 Maas et al. (2011) 3 2 15 cjadams et al. (2019) et al. Warstadt (2019) McAuley Leskovec (2013) Casanueva et al. (2020) & 21 Saravia et al. (2018) 9 Li et al. (2021) 10 Li et al. (2021) 8 FitzGerald et al. (2023) FitzGerald et al. (2023) 19 Maggie (2020) 222 Team (2022a) 17 Team (2022a) 338 Team (2022b) 22 Team (2022b) 431 Team (2022c) 25 Team (2022c) 175 Team (2021d) https://huggingface.co/datasets/mteb /amazon_counterfactual https://huggingface.co/datasets/mteb /amazon_polarity https://huggingface.co/datasets/mteb /imdb https://huggingface.co/datasets/mteb /toxic_conversations_50k https://gluebenchmark.com/tasks https://huggingface.co/datasets/mteb /amazon_reviews_multi https://huggingface.co/datasets/mteb /banking77 https://huggingface.co/datasets/mteb /emotion https://huggingface.co/datasets/mteb /mtop_intent https://huggingface.co/datasets/mteb /mtop_domain https://huggingface.co/datasets/mteb /amazon_massive_scenario https://huggingface.co/datasets/mteb /amazon_massive_intent https://huggingface.co/datasets/mteb /tweet_sentiment_extraction https://huggingface.co/datasets/mteb /raw_arxiv https://huggingface.co/datasets/mteb /raw_arxiv https://huggingface.co/datasets/mteb /raw_biorxiv https://huggingface.co/datasets/mteb /raw_biorxiv https://huggingface.co/datasets/mteb /raw_medrxiv https://huggingface.co/datasets/mteb /raw_medrxiv https://huggingface.co/datasets/sent ence-transformers/reddit-title-body https://github.com/UKPLab/TWEAC-qa-a gent-selection/tree/master/data/redd it/train https://huggingface.co/datasets/flax -sentence-embeddings/stackexchange_ti tle_body_jsonl https://github.com/UKPLab/TWEAC-qa-a gent-selection/tree/master/data/stac kexchange/train https://huggingface.co/datasets/SetF it/20_newsgroups Clustering 58,141 104,725 34 15 Geigle et al. (2021) Clustering 80,000 430,913 304 281 Team (2021a) Clustering 56, 674,714 32 13 Geigle et al. (2021) TwentyNewsgroups Clustering 11, 10,994 230 216 Lang (1995) Total Total Classification Clustering 193,038 822, 10 2,678,655 14 F2LLM Technical Report Table 4: Retrieval datasets used for training F2LLM. Average query and corpus lengths are measured by the number of Qwen3 tokens, and the length of queries includes instructions."
        },
        {
            "title": "Type",
            "content": "# Query Corpus Size"
        },
        {
            "title": "Retrieval",
            "content": "22,848 8,"
        },
        {
            "title": "Retrieval",
            "content": "54,585 320,"
        },
        {
            "title": "Retrieval",
            "content": "112,075 358,"
        },
        {
            "title": "Retrieval",
            "content": "18,801 104,"
        },
        {
            "title": "Retrieval",
            "content": "938,771 2,492,"
        },
        {
            "title": "Retrieval",
            "content": "89,509 20,951 25 32 42 30 30 210 10 14 Wachsmuth et al. (2018) Bowman (2015) Williams (2018) al. al. et et"
        },
        {
            "title": "145 Lewis et al. (2021)",
            "content": "162 Rajpurkar (2016) et al."
        },
        {
            "title": "Retrieval",
            "content": "754,705 2,401,"
        },
        {
            "title": "Retrieval",
            "content": "365,503 4,472,"
        },
        {
            "title": "Retrieval",
            "content": "97,209 75,178 HotpotQA Retrieval 120,528 1,217, FEVER ELI5 Retrieval 106,605 441,174 Retrieval 161,345 215,884 FiQA2018 Retrieval 7,452 32, BioASQ Retrieval 125,248 149,900 NFCorpus Retrieval MIRACL Mr.TyDi SciFact Retrieval Retrieval Retrieval 1,283 3,379 3,547 859 3,270 31, 79,695 4,506 TriviaQA Retrieval 60,025 1,014, COLIEE Retrieval 454 532 PubMedQA Retrieval 60,227 61,233 S2ORC-TitleAbstract S2ORC-TitleCitation S2ORC-AbstractCitation Amazon QA SPECTER Retrieval 250, 2,476,989 Retrieval 132,879 1,619,105 Retrieval Retrieval 59,340 24,717 894,812 199,028 XSum Retrieval 184,383 214,562 CNN_DM Retrieval 100,000 290, Retrieval 175,477 175,477 Retrieval 183,559 158, Sentence Compression StackExchangeDupQuestions-S2S StackExchangeDupQuestions-P2P QQP StackOverflowDupQuestions Retrieval 243, Retrieval 19,847 445,064 315,577 STS12 STS Retrieval 1,858 Retrieval 389 STSBenchmark Retrieval 3,297 2,612 1,380 9,247 Total Retrieval 4,918,949 22,050,569 26 30 45 42 33 27 23 27 40 36 61 39"
        },
        {
            "title": "82 Nguyen et al. (2016)",
            "content": "145 Kwiatkowski et al. (2019) 103 Yang et al. (2018) 324 Thorne et al. (2018) 229 Fan et al. (2019) 234 Maia et al. (2018) 295 Tsatsaronis et al. (2015) 333 Boteva et al. (2016) 158 Zhang et al. (2023) 148 Zhang et al. (2021) 353 Wadden (2020) et al. 149 Joshi et al. (2017) 123 Kim et al. (2022) 314 Jin et al. (2019) 136 Lo et al. (2020) 21 Lo et al. (2020) 38 44 82 26 31 69 Gupta et al. (2019) 14 Cohan et al. (2020) 459 766 33 Narayan (2018) Hermann (2015) et al. et al. Filippova & Altun (2013) 14 Team (2021c) 27 40 505 25 13 Sharma et al. (2019) 11 Liu et al. (2018) 27 Agirre et al. (2012) 494 Chen et al. (2022) 14 May (2021) 15 Retrieval 231,587 1,615,793 259 256 Lo et al. (2020) Retrieval 203, 124,283 189 180 Team (2021c) https://huggingface.co/datasets/BeIR /arguana-generated-queries https://huggingface.co/datasets/stan fordnlp/snli https://huggingface.co/datasets/nyu-m ll/multi_nli https://huggingface.co/datasets/face book/anli https://huggingface.co/datasets/sent ence-transformers/paq https://huggingface.co/datasets/rajp urkar/squad https://huggingface.co/datasets/flax -sentence-embeddings/stackexchange_ti tlebody_best_voted_answer_jsonl https://huggingface.co/datasets/mteb /msmarco https://huggingface.co/datasets/sent ence-transformers/natural-questions https://huggingface.co/datasets/mteb /hotpotqa https://huggingface.co/datasets/mteb /fever https://huggingface.co/datasets/Pavi three/eli5 https://huggingface.co/datasets/mteb /fiqa https://huggingface.co/datasets/BeIR /bioasq-generated-queries https://huggingface.co/datasets/mteb /nfcorpus https://huggingface.co/datasets/mira cl/miracl https://huggingface.co/datasets/mteb /mrtidy https://huggingface.co/datasets/mteb /scifact https://huggingface.co/datasets/sent ence-transformers/trivia-qa-triplet https://www.modelscope.cn/datasets/s entence-transformers/coliee https://huggingface.co/datasets/qiao jin/PubMedQA https://huggingface.co/datasets/sent ence-transformers/s2orc https://huggingface.co/datasets/sent ence-transformers/s2orc https://huggingface.co/datasets/sent ence-transformers/s2orc https://github.com/amazonqa/amazonqa https://huggingface.co/datasets/sent ence-transformers/specter https://huggingface.co/datasets/Edin burghNLP/xsum https://huggingface.co/datasets/abis ee/cnn_dailymail https://huggingface.co/datasets/sent ence-transformers/sentence-compressi on https://huggingface.co/datasets/sent ence-transformers/stackexchange-dupli cates https://huggingface.co/datasets/sent ence-transformers/stackexchange-dupli cates https://gluebenchmark.com/tasks https://huggingface.co/datasets/mteb /stackoverflowdupquestions-reranking https://huggingface.co/datasets/mteb /sts12-sts https://huggingface.co/datasets/mteb /sts22-crosslingual-sts https://huggingface.co/datasets/mteb /stsbenchmark-sts F2LLM Technical Report"
        },
        {
            "title": "B Embedding Instructions",
            "content": "Table 5: Instructions for evaluation tasks in MTEB."
        },
        {
            "title": "Instruction",
            "content": "Given claim, find documents that refute the claim. Identify the main and secondary category of arXiv papers based on the titles. Classify given Amazon customer review text as either counterfactual or not counterfactual. Identify the main and secondary category of arXiv papers based on the titles and abstracts. AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna AskUbuntuDupQuestions Retrieve duplicate questions from AskUbuntu forum. BIOSSES Banking77Classification BiorxivClusteringP2P.v2 CQADupstackGamingRetrieval CQADupstackUnixRetrieval ClimateFEVERHardNegatives FEVERHardNegatives FiQA2018 HotpotQAHardNegatives Given multi-hop question, retrieve passages that answer the question. Given claim about climate change, retrieve documents that support or refute the claim. Given claim, retrieve documents that support or refute the claim. Given financial question, retrieve passages that answer the question. Retrieve semantically similar text. Given an online banking query, find the corresponding intents. Identify the main category of bioRxiv papers based on the titles and abstracts. Given question, retrieve questions that are semantically equivalent. Given question, retrieve questions that are semantically equivalent."
        },
        {
            "title": "ImdbClassification",
            "content": "Classify the sentiment expressed in the given movie review text from the IMDB dataset. Given user utterance as query, find the user scenarios. MTOPDomainClassification Classify the intent domain of the given utterance in task-oriented conversation. MassiveIntentClassification Given user utterance as query, find the user intents. MassiveScenarioClassification MedrxivClusteringP2P.v2 MedrxivClusteringS2S.v2 MindSmallReranking Identify the main category of medRxiv papers based on the titles and abstracts. Identify the main category of medRxiv papers based on the titles. Retrieve relevant news articles based on user browsing history. Given scientific paper title, retrieve paper abstracts that are cited by the given paper. Retrieve semantically similar text."
        },
        {
            "title": "SCIDOCS",
            "content": "Retrieve semantically similar text. SICK-R STS12, STS14, STS13, STS15, STS17, STS22.v2, STSBenchmark SprintDuplicateQuestions Retrieve duplicate questions from Sprint forum. StackExchangeClustering.v2 StackExchangeClusteringP2P.v2 SummEvalSummarization.v2 TRECCOVID Touche2020Retrieval.v3 ToxicConversationsClassification TweetSentimentExtractionClassification TwentyNewsgroupsClustering.v2 TwitterSemEval2015 TwitterURLCorpus Identify the topic or theme of StackExchange posts based on the titles. Identify the topic or theme of StackExchange posts based on the given paragraphs. Given news summary, retrieve other semantically similar summaries. Given query on COVID-19, retrieve documents that answer the query. Given question, retrieve passages that answer the question. Classify the given comments as either toxic or not toxic. Identify the topic or theme of the given news articles. Retrieve tweets that are semantically similar to the given tweet. Retrieve tweets that are semantically similar to the given tweet. Classify the sentiment of given tweet as either positive, negative, or neutral 16 F2LLM Technical Report Table 6: Instructions for training data of F2LLM."
        },
        {
            "title": "Instruction",
            "content": "Arguana, SQuAD, BioASQ, NFCorpus, MIRACL, Mr.TyDi PAQ, StackExchange, MSMARCO, Natural Questions SNLI, MNLI, ANLI HotpotQA FEVER ELI5 FiQA2018 SciFact TriviaQA COLIEE PubMedQA S2ORC-Title-Abstract S2ORC-Title-Citation S2ORC-AbstractCitation"
        },
        {
            "title": "SPECTER",
            "content": "XSum, CNN_DM"
        },
        {
            "title": "Retrieval",
            "content": "Given question, retrieve passages that answer the question."
        },
        {
            "title": "Retrieval",
            "content": "Given web search query, retrieve relevant passages that answer the query. Given premise, retrieve hypotheses that are entailed by the premise. Given multi-hop question, retrieve passages that answer the question. Given claim, retrieve documents that support or refute the claim. Given question from Reddit ELI5 forum, retrieve passages that answer it. Given financial question, retrieve passages that answer the question. Given scientific claim, retrieve passages that support or refute the claim. Given trivia question, retrieve passages that can answer it. Given legal statement, retrieve articles that support it. Given question, retrieve paper abstracts from PubMed that can answer it. Given papers title, retrieve the corresponding abstract. Given papers title, retrieve papers that cite it. Given papers abstract, retrieve abstract of papers that cite it. Given question about product, retrieve Amazon reviews that can help answer it. Given scientific paper title, retrieve paper titles that are cited by the given paper. Given news summary, retrieve the original news article. Given compressed sentence, retrieve the original sentence before compression. QQP, StackExchangeDupQuestions-S2S, StackExchangeDupQuestions-P2P StackOverflowDupQuestions STS12, Benchmark Amazon Counterfactual Amazon Polarity STS22, STS-"
        },
        {
            "title": "Retrieval",
            "content": "Given question, retrieve questions that are semantically equivalent."
        },
        {
            "title": "Retrieval",
            "content": "Retrieve duplicate questions from StackOverflow forum."
        },
        {
            "title": "Retrieval",
            "content": "Retrieve semantically similar text."
        },
        {
            "title": "Classification",
            "content": "Classify given Amazon customer review text as either counterfactual or not counterfactual. Classification Classify the given Amazon review into positive or negative sentiment."
        },
        {
            "title": "Classification",
            "content": "Classify the sentiment expressed in the given movie review text from the IMDB dataset. Toxic Conversations CoLA Amazon Reviews Banking"
        },
        {
            "title": "Emotion",
            "content": "MTOP Intent MTOP Domain Massive Scenario Massive Intent Tweet Sentiment Extraction Classification Classify the given comments as either toxic or not toxic. Classification Classify the given sentence as linguistically acceptable or not acceptable. Clustering Clustering Classify the given Amazon review into its appropriate rating category. Given an online banking query, find the corresponding intents. Classify the emotion expressed in the given Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise. Classify the intent of the given utterance in task-oriented conversation. Classify the intent domain of the given utterance in task-oriented conversation. Given user utterance as query, find the user scenarios. Given user utterance as query, find the user intents."
        },
        {
            "title": "Clustering",
            "content": "Classify the sentiment of given tweet as either positive, negative, or neutral. Arxiv-Clustering-P2P"
        },
        {
            "title": "Clustering",
            "content": "Arxiv-Clustering-S2S Biorxiv-ClusteringP2P Biorxiv-ClusteringS2S Medrxiv-ClusteringP2P Medrxiv-ClusteringS2S Reddit-ClusteringP2P Reddit-Clustering-S2S Clustering StackExchangeClustering-P2P StackExchangeClustering-S2S TwentyNewsgroups"
        },
        {
            "title": "Clustering",
            "content": "Identify the main and secondary category of arXiv papers based on the titles and abstracts. Identify the main and secondary category of arXiv papers based on the titles. Identify the main category of bioRxiv papers based on the titles and abstracts. Identify the main category of bioRxiv papers based on the titles. Identify the main category of medRxiv papers based on the titles and abstracts. Identify the main category of medRxiv papers based on the titles. Identify the topic or theme of Reddit posts based on the titles and posts. Identify the topic or theme of Reddit posts based on the titles. Identify the topic or theme of StackExchange posts based on the given paragraphs. Identify the topic or theme of StackExchange posts based on the titles. Identify the topic or theme of the given news articles."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Shanghai Jiao Tong University"
    ]
}