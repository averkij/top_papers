{
    "paper_title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections",
    "authors": [
        "Zeyu Cai",
        "Ziyang Li",
        "Xiaoben Li",
        "Boqian Li",
        "Zeyu Wang",
        "Zhenyu Zhang",
        "Yuliang Xiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present UP2You, the first tuning-free solution for reconstructing high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D photos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body images with minimal occlusions, or well-calibrated cross-view captures), UP2You directly processes raw, unstructured photographs, which may vary significantly in pose, viewpoint, cropping, and occlusion. Instead of compressing data into tokens for slow online text-to-3D optimization, we introduce a data rectifier paradigm that efficiently converts unconstrained inputs into clean, orthogonal multi-view images in a single forward pass within seconds, simplifying the 3D reconstruction. Central to UP2You is a pose-correlated feature aggregation module (PCFA), that selectively fuses information from multiple reference images w.r.t. target poses, enabling better identity preservation and nearly constant memory footprint, with more observations. We also introduce a perceiver-based multi-reference shape predictor, removing the need for pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and in-the-wild captures demonstrate that UP2You consistently surpasses previous methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5 minutes per person), and versatile (supports arbitrary pose control, and training-free multi-garment 3D virtual try-on), making it practical for real-world scenarios where humans are casually captured. Both models and code will be released to facilitate future research on this underexplored task. Project Page: https://zcai0612.github.io/UP2You"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 1 8 4 2 . 9 0 5 2 : r UP2YOU: FAST RECONSTRUCTION OF YOURSELF FROM UNCONSTRAINED PHOTO COLLECTIONS Zeyu Cai1,2 Ziyang Li2 Xiaoben Li1 Boqian Li1 Zeyu Wang3 Zhenyu Zhang2 Yuliang Xiu1 1Westlake University 2Nanjing University 3The Hong Kong University of Science and Technology (Guangzhou) Shared Corresponding Author Project Page: https://zcai0612.github.io/UP2You Figure 1: Overview of UP2You. Our method reconstructs high-quality, textured 3D clothed portraits from unconstrained photo collections. It robustly handles highly diverse and unstructured inputs by rectifying them into orthogonal multi-view images and corresponding normal maps, making them compatible with traditional reconstruction algorithms."
        },
        {
            "title": "ABSTRACT",
            "content": "We present UP2You, the first tuning-free solution for reconstructing high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D photos. Unlike previous approaches that require clean inputs (e.g., full-body images with minimal occlusions, or well-calibrated cross-view captures), UP2You directly processes raw, unstructured photographs, which may vary significantly in pose, viewpoint, cropping, and occlusion. Instead of compressing data into tokens for slow online text-to-3D optimization, we introduce data rectifier paradigm that efficiently converts unconstrained inputs into clean, orthogonal multi-view images in single forward pass within seconds, simplifying the 3D reconstruction. Central to UP2You is pose-correlated feature aggregation module (PCFA), that selectively fuses information from multiple reference images w.r.t. target poses, enabling better identity preservation and nearly constant memory footprint, with more observations. We also introduce perceiver-based multi-reference shape predictor, removing the need for pre-captured body templates. Extensive experiments on 4D-Dress, PuzzleIOI, and in-the-wild captures demonstrate that UP2You consistently surpasses previous methods in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress). UP2You is efficient (1.5 minutes per person), and versatile (supports arbitrary pose control, and training-free multi-garment 3D virtual try-on), making it practical for real-world scenarios where humans are casually captured. Both models and code will be released to facilitate future research on this underexplored task."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reconstructing 3D clothed humans from unconstrained photo collections, like the personal albums (Fig. 2-Left), is challenging and largely unexplored research frontier. Unlike prior tasks such 1 as single-image 3D reconstruction [20, 45, 62, 88, 89], monocular video-based reconstruction [15, 25, 32], or multi-view 3D reconstruction [52, 61, 96], this problem is distinguished by the highly unstructured nature of the input: appearance information is present but scattered across photos where subjects are often partially captured or occluded, and camera as well as body poses are rarely synchronized. As result, establishing accurate 2D-to-3D correspondences is extremely difficult, even with the help of most advanced off-the-shelf human-centric estimators (i.e., camera, body pose, landmarks, geometric cues, etc). In contrast, traditional 3D reconstruction algorithms typically assume clean captures (i.e., full-body capture with simple poses, synchronized cameras, etc), where well-aligned 2D-to-3D correspondences can be readily established using the estimators above. Two potential strategies to address above challenges: 1) Data Compressor: Crop and group photos into local and global patches (e.g., head, full-body) [97], or segment input photos into multiple assets (e.g., garments, hair, face, accessories) [90], then compress these patches or assets into learnable tokens, and finally assemble them as text prompt to generate 3D humans via text-to-3D techniques [59]; 2) Data Rectifier: Convert the incoming dirty or incomplete captures into clean and complete ones, e.g., orthogonal orbit views with canonical poses, which are easier to reconstruct with traditional 3D reconstruction algorithms. Essentially, the data compressor operates mainly at the representation level, without substantially improving the generative models ability to ensure 3D consistency and identity preservation limitation noted in PuzzleAvatar [90] as unpredictable hallucination. The data rectifier, however, refines not only the input data but also the generative models prior, via continued training on synthetic multi-view renderings of high-fidelity 3D clothed humans, enabling more consistent 3D reconstruction in terms of both identity and viewpoint, from unconstrained photographs. UP2You falls in the second category, as shown in Fig. 2. PuzzleAvatar [90] is the representative of the first strategy, it first decompose the unconstrained photos into multiple asset soups, all of which are linked with unique learned tokens via DreamBooth [66], then it compose these assets into 3D full-body representation via score-distillation sampling (SDS) [59], where the 3D reconstruction task is reformulated as text-to-3D task, bypassing explicit canonicalization. However, this process takes hours since both DreamBooth fine-tuning and SDS-based optimization are time-consuming and unstable, see Fig. 2. Additionally, ground-truth SMPL-X meshes are needed for initialization, as predicting shape parameters from unconstrained photo collections is non-trivial. Regarding the second strategy converting inputs into orthogonal orbit views some attempts [20, 45, 57] have been made. However, these methods are restricted to single-image inputs and cannot fully leverage the multiple unconstrained photos. Essentially, these methods act more as data inpainters [80] synthesizing unseen views from seen capture rather than as data rectifiers that unify the messy observations into structured output. Designed mainly for constrained inputs (i.e., single image with full-body coverage), these methods cannot handle unconstrained photos or scale up the reconstruction accuracy with the number of inputs. To the best of our knowledge, UP2You is the first work to unlock the data rectifier strategy on unconstrained photo collections, directly transforming raw unconstrained photo collections into orthogonal views while faithfully preserving subject identity. This is not trivial extension of prior arts, as it 1) requires effectively aggregating information from multiple unconstrained inputs, which may vary significantly in terms of body poses, camera viewpoints, croppings, and occlusions; 2) must be efficient enough to process varying numbers of input photos (ranging from one to dozens) without incurring significant computational overhead; and 3) needs to overcome the dependency on ground-truth body shapes, which are often unavailable in real-world scenarios. Specifically, UP2You aggregates ReferenceNet features [24], extracted from unconstrained photos according to body poses, via the proposed Pose-Correlated Feature Aggregation (PCFA) module. This module implicitly learns correlation weights between unconstrained reference images and target pose conditions (i.e., SMPL-X normal maps). Guided by these correlation maps, PCFA uses an optimized topk strategy to selectively aggregate the most informative image features for generating each orthogonal view. As result, the memory footprint remains nearly constant regardless of the number of input photos, enabling effective and efficient information fusion. To get rid of the dependence on ground-truth body shapes, we design shape predictor based on perceiver structure [31, 43] to regress SMPL-X shape parameters directly from unconstrained photo collections. Lastly, with another MV-Adapter [29] to generate multi-view normal maps, followed by mesh carving and texture baking [45], UP2You reconstructs high-quality textured meshes from unconstrained photos in 1.5 minutes. We evaluate our generation results on PuzzleIOI, 4D-Dress, and 2 Figure 2: Paradigm differences between previous works and UP2You. Top: Previous works like PuzzleAvatar [90] and AvatarBooth [97] compress unconstrained photos into implicit personal tokens and DreamBooth weights [66] through fine-tuning, then generate 3D humans via SDS optimization [66]. Bottom: UP2You directly rectifies unconstrained photo collections into orthogonal view images and normals, then reconstructs textured human meshes, achieving superior quality while reducing processing time from 4 hours to 1.5 minutes. self-collected in-the-wild datasets. Our method surpasses other state-of-the-art approaches in both geometric accuracy (Chamfer-15%, P2S-18% on PuzzleIOI) and texture fidelity (PSNR-21%, LPIPS-46% on 4D-Dress), while also demonstrating flexibility and superior generalization for single-image reconstruction, and enabling 3D virtual try-on application, all without extra training. Our main contributions w.r.t. the prior arts are as follows: Efficient. As Fig. 2 shows, unlike previous DreamBooth + SDS paradigm (>4 hours), UP2You acts as data rectifier instead, to directly generate clean multi-views from dirty unconstrained inputs in one forward pass (<15 secs). It can process one, several, or dozens of photos with nearly constant memory footprint. The full pipeline, including multi-view normal generation plus mesh carving and texture baking, completes in 1.5 minute. Effective. Thanks to the PCFA module, which selectively aggregates the most informative regions from the reference images for synthesizing target views, UP2You significantly outperforms prior SOTAs (PuzzleAvatar, AvatarBooth, PSHuman) in both geometry accuracy and texture fidelity, and delivers consistent shape and identity regardless of input forms or pose conditions. Notably, the reconstruction quality even scales up with more unconstrained inputs, echoing the principle of The More You See in 2D, the More You Perceive in 3D [18]. Versatile. PuzzleAvatar requires an A-posed body template with ground-truth shape for 3D initialization, while UP2You is flexible to random pose control, directly regresses body shapes from unconstrained photos, and inherently supports multi-garment 3D virtual try-on, for free."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 3D CLOTHED HUMAN RECONSTRUCTION The field of 3D clothed human reconstruction has been extensively studied over the past few decades. Early methods primarily focused on reconstructing human geometry and texture from dense multiview image captures [32, 49, 58]. Subsequent research has broadened the scope to include full-shot monocular video inputs [15, 16, 25, 86], enabling more flexible and accessible data acquisition. Recent advances in generative models, particularly diffusion models [22, 39, 65, 77], and the emergence of SDS-based 3D human generators [27, 41, 47, 50, 82, 82], have further propelled the field. An increasing number of video-based human reconstruction approaches now leverage learned generative priors to address common challenges in real-world video captures, such as occlusions [16, 56], view inconsistencies [33], and poor texture details [79]. Such generative priors, learned from large-scale datasets, play more crucial role for the inherently ill-posed problem of 3D human reconstruction, especially when the input data is sparse or incomplete. The most sparse input format is single image [20, 28, 30, 45, 62, 64, 67, 68, 88, 89, 102]. In essence, it can be regarded as conditional generation problem [28], since large portions of the geometry such as the unseen backside and occluded regions must be plausibly inferred or 3 synthesized from the visible pixels. Building on this reconstruction as conditional generation paradigm, numerous works have further advanced the field [2, 12, 45, 100]. Apart from multi-view posed captures, full-shot monocular video, and single image, numerous works have sought to expand the range of input modalities, for example, by incorporating dual front-back captures [35, 52] or multi-view unposed full-body images [26, 63, 92, 96, 103] to improve reconstruction fidelity and completeness. Despite these advances, existing methods still fall short of handling truly unconstrained photos those with partial views, occlusions, extreme camera viewpoints, dynamic body poses, and inconsistent aspect ratios. Accurately estimating body shape [9, 40, 42, 85, 93, 99] from such unconstrained photo collections is nearly impossible. Moreover, given multiple dirty reference images, image-based HMR methods often fail to deliver consistent results. This inconsistency manifests as significant variations in the predicted body shapes for the same subject some reconstructions may appear unnaturally thin, others excessively fat, and some may completely fail, especially in cases of partial or occluded inputs. As shown in Tab. 2 and Fig. 8, it becomes challenging to determine which, if any, of the predicted shapes truly represent the subject. In contrast, UP2You addresses these data format constraints by functioning as comprehensive data rectifier, directly transforming unstructured or dirty inputs into orthogonal clean views, with consistent 3D and identity, that can be seamlessly utilized for robust 3D reconstruction. 2.2 UNCONSTRAINED PHOTOS TO 3D Most real-world data is inherently unstructured, presenting significant challenges for 3D reconstruction tasks that require reliable spatial correspondences. The earliest work in Unconstrained Photos to 3D can be traced back to Photo Tourism [76], which reconstructs 3D scenes from large collections of Internet photos. Recent advances in neural rendering and generative models have further advanced this field, enabling more robust and realistic 3D reconstructions from unstructured image collections [7, 46, 83]. However, these methods primarily focus on rigid objects or scenes and cannot be directly applied to 3D clothed human reconstruction, which involves highly articulated and non-rigid structures. critical open question is how to effectively extract and aggregate identity features from unconstrained photos not only for general objects [38, 98, 104], but especially for dynamic humans and reproduce them in 3D-consistent manner. Several works on subject-driven image generation [1, 3, 10, 11, 13, 37, 66, 69, 81], as well as ID-consistent 2D human portrait generation [6, 60, 71, 80, 91], are discussed in the Sup.Mat. (Appendix B). However, these methods are primarily designed for 2D image generation and lack the mechanisms to ensure cross-view consistency or the precise latent feature aggregation necessary for high-fidelity 3D reconstruction. The most relevant works addressing this challenge are PuzzleAvatar [90] and AvatarBooth [97]. Both first employ few-shot personalization [3, 66], as Total Selfie [6] and RealFill [80], to distill identity information from unconstrained photos into customized diffusion model, as unique tokens. Subsequently, guided by these unique tokens, they utilize Score Distillation Sampling (SDS) [5, 34, 59, 95] to optimize neural-based 3D representation [53, 72]. In short, the entire pipeline of these methods can be summarized as unconstrained photos personalized diffusion models with learned specialized tokens SDS-based Text-to-3D. However, fine-tuning diffusion models and optimization-based SDS methods are extremely time-consuming. Moreover, these fine-tuning approaches act as form of lossy compression: the strong priors of diffusion models often override subject-specific features, leading to loss of identity and fine-grained details, or even introducing unpredictable hallucinations. In contrast, UP2You is tuning-free method that faithfully reconstructs 3D humans from unconstrained photos in just 1.5 minutes, while well preserving human identities."
        },
        {
            "title": "3 METHOD",
            "content": "Our objective is to reconstruct high-quality textured mesh from unconstrained photos with unknown camera parameters and human poses. To this end, we first generate orthogonal full-body images from the unconstrained inputs, conditioned on SMPL-X normal maps that contain both camera and pose information (Sec. 3.1). Next, we utilize these orthogonal multi-view RGB images to generate corresponding multi-view normal maps, which serve as geometric cues for detailed mesh reconstruction (Sec. 3.2). To handle in-the-wild images without SMPL-X annotations, we further 4 Figure 3: Pipeline of UP2You. Given unconstrained input photos I, we first predict the SMPL-X shape parameters (Sec. 3.3) and initialize the SMPL-X mesh with predefined pose and expression parameters. We then generate orthogonal view images based on and SMPL-X normal rendering with the proposed PCFA methodpredict correlation maps and select most informative features (Sec. 3.1). Finally, we produce multi-view normal maps from and V, and reconstruct the final textured mesh (Sec. 3.2). introduce body shape estimator capable of inferring human body shape by integrating information from handful of unconstrained photos (Sec. 3.3). 3.1 ORTHOGONAL MULTI-VIEW IMAGES GENERATION To tackle orthogonal multi-view image generation from unconstrained photo collections, we adopt MV-Adapter [29] as our backbone (introduced in Appendix C). MV-Adapter integrates ReferenceNet [24] as the reference image encoder and incorporates raymaps into the diffusion UNet as view conditions, enabling the synthesis of six orthogonal views. For our task, we use orthogonal SMPL-X normal maps as view conditions. Unlike the original MV-Adapter, which handles only single-image inputs, our approach extends it to process multiple unconstrained photos. As shown in Fig. 3, given unconstrained reference images = {I1, ..., IN } of person in the same outfit, our goal is to synthesize orthogonal target views = {V1, ..., VM }, each conditioned on corresponding SMPL-X normal map = {P1, ..., PM }. To extract the most informative features for each target view, we introduce the Pose-Correlated Feature Aggregation (PCFA) module, which predicts correlation maps = {Ci i=1 between reference and target views (see Fig. 4). Based on C, PCFA select features for each target viewpoint for the generation of orthogonal views V. 1, ..., Ci }M Correlation Map Prediction. Using all reference features for ortho-view generation is computationally intensive, as memory usage grows with the number of unconstrained references. However, many reference pixels are irrelevant for given target view (e.g., back-view references for front-view synthesis). Therefore, we adaptively determine each references contribution based on the target pose to reduce computational cost. Figure 4: Pose-Dependent Correlation Map. Correlation is colored as Higher Lower. To achieve this, we disentangle human-specific identity features from viewpoint correlation information in the unconstrained reference inputs. Drawing inspiration from [23, 36], we predict correlation maps for reference images conditioned on target poses, as illustrated in Fig. 4. For each target pose Pi, {1, 2, ..., }, we estimate correlation map that indicates the pixel-wise relevance of each reference image for generating the corresponding view. Specifically, we employ pose image encoder pose and DINOv2 [54] model ref to extract features from the target pose image and all reference images: Xpose = pose(Pi) and Xref = ref(I), where Xref represents the concatenation of all DINOv2 outputs {Xref j=1. Subsequently, we feed both }N 5 Xpose and Xref into transformer block that comprises layers of self-attention and cross-attention, where Xpose functions as the query, key, and value in self-attention operations, and as the query in cross-attention operations, while Xref serves as both key and value in cross-attention operations. Through , an output feature Oi = (Xpose , Xref) that integrates reference information relevant to the target pose is produced. We derive the image correlation map Ci by computing the attention map between Oi and Xref: Ai = WqOi WkXref , Ci = [Ci 1, Ci 2, ..., Ci ] = ReLU(AvgPool(mean(Ai))), (1) (2) Here, Wq and Wk are learnable projection matrices applied to Oi and Xref, respectively. The resulting attention map Ai RlN hw captures the relevance between the target pose and reference features. To obtain the final reference correlation scores, we compute the mean along the first dimension of Ai using mean() : RlN hw RN hw. In this context, is the token number of Oi, and denote the height and width of Xref, and is the feature dimension of WqOi and WkXref. We further apply AvgPool to smooth the predicted correlation map and ReLU to suppress negative values. The correlation maps of PCFA are based on fine-grained semantic correlation between target bodies and DINO features of references. Unlike previous methods [23, 36] that depend on landmark similarity, our correlation map encodes richer outfit details, enabling more accurate reconstruction. Feature Selection. The predicted correlation maps enable PCFA to selectively aggregate the most informative reference features for each target view. Specifically, we utilize ReferenceNet as the reference image encoder to extract multi-scale reference features = {F1, F2, ..., FL}, where is the number of layers. For each target pose Pi and the reference feature Fk RN Skc at layer k, we first interpolate the corresponding correlation map Ci RN hw to get ˆC = Interpk(Ci) that aligns with the spatial dimensions of Fk. Here Sk denotes the spatial size of Fk, and Interpk() : RN hw RN Sk denotes the interpolation operator. We then select the most relevant reference features ˆF employ the topk selection strategy to obtain the selected indices of Fk: for view Pi based on ˆC . Specifically, we [ki 1, ki 2, ..., ki γSk ] = sort(topk( ˆC )[: γSk]), (3) 1, ki 2, ..., ki where [ki ] are the indices of the selected features, topk() returns the top γSk indices, and γ controls the proportion of features retained. To preserve spatial order, we apply sort(). Using these indices, we extract the selected reference features ˆF RγSkc: γSk ˆF = Fk[ki 1, ki 2, ..., ki γSk ] ˆCi[ki 1, ki 2, ..., ki γSk ]. (4) Given the aggregated reference features ˆF = { ˆF k=1, we synthesize the orthogonal multi-view images as = Drgb( ˆF, rgb(P)), where Drgb is our multi-view image generation model and rgb() is the pose guider that encodes the pose condition into Drgb. k, ..., ˆF k, ˆF }L 1 2 3.2 NORMAL MAP GENERATION AND MESH RECONSTRUCTION For multi-view reconstruction (MVS) [44, 45, 48, 51, 87], we generate multi-view clothed normal maps from the generated images V, conditioned on target poses P, and reconstruct the final textured mesh using both and N. 6 Normal Map Generation. To ensure multiview consistency and provide strong geometric cues for normal map generation, we follow [89] and incorporate SMPL-X normal renderings as additional conditions. As Fig. 5 shows, we also adopt MV-Adapter as the backbone of clothed normal generator Dnormal. We utilize the generated orthogonal RGB views as reference inputs, and employ the pose guider normal() to incorporate multi-view pose conditions. The multi-view clothed normal maps are then generated via = Dnormal(V, normal(P)). Figure 5: Normal Map Generation Pipeline. The main input difference with Fig. 3 is the generated multi-view orthogonal images V, instead of unconstrained inputs I. Mesh Carving and Texture Baking. Starting from the initial SMPLX mesh, we refine mesh details using the generated via differentiable mesh deformation [55], and project per-vertex colors from V, following PSHuman [45]. To better preserve hand geometry, we replace the hand region with that from the initial mesh as in ECON [89], and then perform texture baking using the generated multi-view RGB images. 3.3 MULTI-REFERENCE SHAPE PREDICTOR The initial SMPL-X mesh is critical to the entire UP2You pipeline, as it provides the pose condition for multi-view generation and serves as the basis for mesh reconstruction. SMPL-X mesh R107543 are defined as T(β, θ, ψ), where β, θ, ψ are shape, pose, and expression parameters respectively. While the target pose and expression of the SMPL-X template can be predefined (e.g., T-pose or A-pose with neutral expression), the body shape parameters must be estimated from unconstrained input images. Existing shape predictors [20, 45, 62] are typically designed for single-image scenarios and struggle to effectively leverage multiple unconstrained references. To address this limitation, we introduce multi-reference shape predictor, S, as illustrated in Fig. 6. The prediction process is formulated as βpred = S(τ , Xref), where βpred denotes the predicted shape parameters, τ are learnable query tokens, and Xref are DINOv2 features extracted from the reference images. Our shape predictor employs perceiver-style architecture [31, 43] that can use query tokens to effectively aggregate multi-view information. The prediction head is lightweight transformer, similar to the camera head design in [83]. Figure 6: Multi-reference Shape Predictor. Figure 7: Inference Process of UP2You. Given only unconstrained photos as inputs, UP2You can generate high-quality textured mesh. In summary, UP2You generates textured 3D humans from unconstrained photo inputs through three key components: the shape predictor, multi-view image & normal generator, and mesh carving & texture baking steps. The complete inference pipeline is illustrated in Fig. 7, with detailed explanations and inference time analysis provided in Appendix D. 7 PuzzleIOI 4D-Dress in-the-wild AvatarBooth PuzzleAvatar Ours (Image) Ours (Mesh) PSNR 16.879 21.664 23.896 24.539 SSIM LPIPS Chamfer 0.860 0.916 0.926 0.940 0.1544 0.0639 0.0545 0. 6.635 3.204 - 2.724 P2S Normal 0.0274 6.697 0.0150 3.165 - - 0.0115 2.605 PSNR 18.186 21.376 25.848 25.540 SSIM LPIPS Chamfer 0.850 0.887 0.920 0.918 0.1718 0.1081 0.0576 0.0654 6.846 1.956 - 1. P2S Normal CLIP-I DINO 0.619 6.978 0.742 2.045 0.932 - 1.119 0.916 0.0311 0.0170 - 0.0122 0.878 0.907 0.972 0.971 Table 1: Quantitative Comparison with Baselines. UP2You achieves the best texture fidelity, geometry accuracy, and perception similarity."
        },
        {
            "title": "4.1 SETTINGS",
            "content": "Dataset. We train our multi-view image generation, normal map generation, and shape prediction models on the THuman2.1 [94], Human4DiT [70], 2K2K [17], and CustomHumans [21] datasets. For evaluation, we use the PuzzleIOI [90] and 4D-Dress [84] datasets as test sets. To further validate our approach, we collect an in-the-wild (in-the-wild) dataset comprising 12 distinct identities. Details on dataset selection and processing procedures are provided in Appendix D.2. Baselines. We comprehensively compare UP2You with 1) album-to-human reconstruction methods, including PuzzleAvatar [90] and AvatarBooth [97]. Since single-view reconstruction is special case of the unconstrained setting, we also include the leading 2) single-view method, PSHuman [45], in our comparisons. To ensure fair evaluation and isolate the impact of pose estimation errors, we provide ground truth SMPL-X parameters for all baseline methods. 3) For shape prediction, we present the first approach to estimate SMPL-X shape parameters from multiple unconstrained inputs. We compare our shape predictor with two single-input methods: Semantify [14], which is specifically designed for shape prediction, and PromptHMR [85], state-of-the-art human mesh recovery method. Unless stated otherwise, results on PuzzleIOI and 4D-Dress use 12 reference images. 4) For in-the-wild, we use all available references (812) for each identity. Additional model and training details are in Sup.Mat.s Appendices D.1 and D.3. Metrics. For PuzzleIOI and 4D-Dress (with textured 3D GT), we report geometric metrics (Chamfer, P2S, Normal map L2) and image quality metrics (PSNR, SSIM, LPIPS). For in-the-wild, we use perceptual similarity (CLIP-I, DINO) between generated and frontal reference. Shape prediction is assessed by vertex-to-vertex (V2V) distance on all datasets. More details in Sup.Mat.s Appendix D.4. 4.2 COMPARISONS Quantitative Results. The quantitative results in Tab. 1 show that UP2You consistently surpasses all baselines across both 2D and 3D evaluation metrics on the PuzzleIOI and 4D-Dress datasets. Importantly, UP2You also achieves strong perceptual quality scores on the in-the-wild dataset, demonstrating its robustness and effectiveness in real-world unconstrained scenarios. For single-view reconstruction, Tab. 3 shows that UP2You outperforms PSHuman on all 2D and 3D metrics. This is expected, as single front-view input is special case of the unconstrained multi-view scenario for which UP2You is designed. Training on the more challenging unconstrained task enables our model to generalize well and excel in the simpler constrained single-view input setting. As shown in Tab. 2 and Fig. 8, our shape predictor outperforms single-view methods [14, 85], achieving more accurate and consistent results. Single-input baselines show high variance and instability, especially with partial input or failed detections. Leveraging multiple inputs, our method delivers more robust shape prediction, with performance further improving as more unconstrained references are used. 8 Figure 8: Shape Prediction Error Map. Figure 9: Qualitative Comparisons on PuzzleIOI and 4D-Dress. Figure 10: Qualitative Comparisons on in-the-wild Data. Figure 11: UP2You vs. PSHuman. Num of Semantify References Mean Var Mean 9.212 9.661 9.403 9.287 11.087 11.066 10.978 11.097 4.234 5.706 6.424 6.597 3 6 9 PromptHMR Var 10.370 17.465 18.218 19.418 Ours 7.967 7.427 7.403 7.399 PSHuman Ours Mesh PSNR 24.134 26.651 SSIM LPIPS Chamfer 0.905 0. 0.0895 0.0527 2.759 0.927 P2S Normal 0.0189 2.926 0.0096 0.949 Table 2: V2V() Comparions of Shape Prediction Reuslts. Table 3: Comparison of Single-Image based Reconstruction. Qualitative Results. The qualitative comparisons in Fig. 9 and Fig. 10 show that UP2You achieves high-fidelity, reference-faithful 3D reconstructions with strong realism and detail preservation. In contrast, baselines like AvatarBooth and PuzzleAvatar often fail to capture fine facial details and produce blurrier, less realistic results with poor subject-specific consistency. Figure 11 shows singleview 3D human reconstruction comparisons. Our method generalizes well to single-view inputs, producing visually comparable results to PSHuman, but with more accurate limb reconstruction due to consistent multi-view guidance. More visual comparisons and results are in Appendices E.1 and G. Mean Concat Corr. Feature Aggregation sum Image Encoder PuzzleIOI topk CLIP DINOv2 Ref Net PSNR 23.896 17.412 20.545 20.167 20.152 19.744 SSIM LPIPS 0.0545 0.926 0.1227 0.864 0.0949 0.893 0.1002 0.889 0.0976 0.891 0.1415 0.886 PSNR 25.848 19.614 23.366 23.412 23.405 23.393 4D-Dress SSIM LPIPS 0.0576 0.920 0.1098 0.876 0.0791 0.901 0.0794 0.904 0.0801 0.903 0.0813 0.904 Ours A. B. C. D. E. Table 4: Ablation Studies of our orthogonal view image generation model. 4.3 ABLATION STUDIES Multi-View Image Generation. In Tab. 4, we analyze our multi-view image generation model on the PuzzleIOI and 4D-Dress datasets. For feature aggregation, we compare simple averaging (A), concatenation (B), and our proposed PCFA, which achieves the best results. We also test weighted sum strategy (C) after correlation map prediction. For reference feature extraction, we evaluate CLIP (D), DINOv2 (E), and ReferenceNet. Quantitative results show our design outperforms all alternatives, more visual analysis shows in Sup.Mat.s Appendix F. Correlation Maps. Our correlation map prediction module identifies and prioritizes key regions in reference images based on the target pose. As shown in Fig. 12, visualizations for frontand back-view targets confirm that our maps effectively select the most relevant areas for view generation. This targeted focus improves generation quality and reduces GPU memory usage by retaining only the most informative features. More visual results are shown in Appendix E.2. Figure 12: Predicted Correlation Maps. Ours Number of References. In UP2You, providing more unconstrained photos as input enables the extraction and refinement of additional details across orthogonal views, thereby enhancing the reliability of the generated results  (Fig. 13)  . Furthermore, as demonstrated in Tab. 5, the PCFA module efficiently selects fixed number of informative features while maintaining low GPU memory usage, in contrast to direct concatenation, which scales memory consumption linearly. PSNR SSIM LPIPS GPU PSNR SSIM LPIPS GPU 18.02 24.159 24.33 25.041 30.89 25.646 25.848 37.96 Table 5: Multi-View Generation with Different Number of References. 0.0894 0.0807 0.0796 0.0791 22.759 23.267 23.362 23.366 3 refs 6 refs 9 refs 12 refs 0.0680 0.0623 0.0592 0. 0.897 0.901 0.901 0.901 18.65 19.40 20.16 20.88 0.912 0.917 0.918 0.920 Concat Robustness of Target Pose Condition. While previous experiments highlight the strong generation ability of UP2You, most target poses are in the selected A-pose configuration. Since 4D-Dress provides ground-truth multi-view images of persons with different poses, we further test robustness by randomly selecting three diverse target poses per identity from the 4D-Dress dataset and evaluating our multi-view image generation performance with the same reference inputs. As shown in Tab. 6, UP2You maintains high-quality results across varied target poses using the same unconstrained photo inputs. Figure 14 further demonstrates the visual results, where identity is consistently preserved across different poses. Table 6: ID Consistency. UP2You achieves highquality multi-view image generation results in 4D-Dress dataset in three different pose condition. SSIM LIPIPS 0.0664 0.911 0.0744 0.902 0.0715 0.904 PSNR Pose 24.983 24.400 Pose 24.519 Pose 10 Figure 13: Generated Multi-View Image Results with Different Number of References. With more references input, more results are noticed and generated by our model, like facial details and clothing patterns. Figure 14: Robustness of target pose conditions. Our method can generate high-quality multi-view images under different pose conditions with the same reference inputs, demonstrating that identity information is effectively disentangled from pose conditions in our approach. 7. V2V (mm) Ref Group Ref Group Ref Group 7.503 Analysis of Shape Predictor. To evaluate whether our shape predictor can regress consistent shape parameters, we assess our shape prediction model using different groups of unconstrained reference inputs from the same identity. As shown in Tab. 7, our method achieves stable shape predictions across all input groups. Since the aggregated pixel-level features from reference inputs may contain information about personal shape characteristics, the multi-view image generation model in UP2You exhibits some degree of robustness to shape variations. However, in extreme cases, more accurate shape predictions can significantly enhance the quality of the final 3D human generation. We evaluate the impact of our shape predictor on the overall inference pipeline of UP2You and find that incorporating the proposed shape predictor leads to measurable improvements in generation quality on the in-the-wild dataset. As demonstrated in Fig. 15, our shape predictor enables more identity-consistent Table 7: Shape prediction consistency on the 4D-Dress dataset. We input three different groups of 12 reference images of the same person into our shape predictor. The vertex-to-vertex (V2V) error of the predicted results shows stable values with low variance, demonstrating that our shape predictor is robust to unconstrained inputs. 7.443 results for individuals with extreme body shapes, while Tab. 8 provides quantitative evidence that the proposed shape predictor improves performance on the in-the-wild dataset. w/ Shape Predictor w/o Shape Predictor Ours Image Ours Mesh Ours Image Ours Mesh CLIP-I DINO 0.972 0. 0.971 0.916 0.969 0.927 0.969 0.911 Table 8: Effects of Shape Predictor on the in-the-wild Dataset. Generation results with the aid of shape predictor have better performance. Figure 15: Shape Predictor Helps to Generate More Identity-Consistent Results for People in Extreme Shape."
        },
        {
            "title": "5 APPLICATION",
            "content": "3D Virtual Try On: The capability of UP2You to process multiple unconstrained inputs naturally enables its application to 3D Virtual Try-on. As shown in Fig. 16, given two collections of unconstrained photos featuring different identities, UP2You can seamlessly swap the upper and lower garments through the combination of segmented image collections. Both the identity and the clothing details are well-preserved throughout the process. Figure 16: Examples of 3D Virtual Try-On."
        },
        {
            "title": "6 LIMITATIONS AND FUTURE WORKS",
            "content": "While our method shows promising results in generating high-quality 3D human avatars from unconstrained photos, there are still some limitations that we plan to address in future work: 12 Dependence on 3D Data for Training: Our method relies on dataset of 3D human models for training the diffusion model. Acquiring high-quality 3D data can be challenging and may limit the diversity of the generated avatars. In future work, we aim to explore semi-supervised or unsupervised approaches that can leverage large-scale 2D image or video datasets to reduce this dependence on 3D data. Texture Misalignment: Our method generates 6 orthogonal views for mesh reconstruction and texturing, which is insufficient for high-quality texture baking. Texture misalignment issues may arise in some cases  (Fig. 17)  . In future work, we plan to adopt video generation models as the base framework for dense view synthesis to address this limitation. Multiple Inference Stages: When processing in-the-wild photos, our mesh reconstruction pipeline involves four sequential stages: shape prediction, multi-view image generation, multi-view normal map generation, and mesh reconstruction. This multi-stage inference approach slows down the generation process and may introduce cumulative errors. We plan to develop feed-forward model that directly predicts the final results. Figure 17: Failure Cases of UP2You. Since only 6 orthogonal views {0, 45, 90, 135, 180, 270} are generated, the backside texture of generated humans is lacking in guidance, making the problem of texture misalignment."
        },
        {
            "title": "7 CONCLUSION",
            "content": "UP2You acts as data rectifier, converting unconstrained photos into orthogonal views suitable for MVS. The method is computationally efficient (1.5 minutes per person on single GPU), achieves SOTA quality, demonstrates robustness to in-the-wild data, and effectively preserves identity and clothing style across diverse input forms and pose conditions. Additionally, UP2You can be extended to broader applications, such as 3D Virtual Try-On."
        },
        {
            "title": "8 ACKNOWLEDGEMENTS",
            "content": "We thank Siyuan Yu for the help in Houdini Simulation, Shunsuke Saito, Dianbing Xi, Yifei Zeng for the fruitful discussions, and the members of Endless AI Lab for their help on data capture and discussions. This work is funded by the Research Center for Industries of the Future (RCIF) at Westlake University, the Westlake Education Foundation."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. Neural Space-Time Representation for Text-to-Image Personalization. Transactions on Graphics (TOG), 2023. 4, 22 [2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-Image 3D Human Digitization with Shape-Guided Diffusion. In International Conference on Computer Graphics and Interactive Techniques in Asia (SIGGRAPH Asia), 2023. 4 [3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-AScene: Extracting Multiple Concepts from Single Image. In International Conference on Computer Graphics and Interactive Techniques in Asia (SIGGRAPH Asia), 2023. 4, 22 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: Frontier Large Vision-Language Model with Versatile Abilities. arXiv preprint arXiv:2308.12966, 2023. 22 [5] Zeyu Cai, Duotun Wang, Yixun Liang, Zhijing Shao, Ying-Cong Chen, Xiaohang Zhan, and Zeyu Wang. DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping. In Pacific Conference on Computer Graphics and Applications (PG), 2024. 4 [6] Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, and Steven Seitz. Total Selfie: Generating Full-Body Selfies. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4, [7] Bardienus Pieter Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud. Mast3R-SFM: Fully-Integrated Solution for Unconstrained Structurefrom-Motion. In International Conference on 3D Vision (3DV), 2025. 4 [8] Facebook. DINOv2-Large. dinov2-large, 2023. 23 https://huggingface.co/facebook/ [9] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Collaborative Regression of Expressive Bodies Using Moderation. In International Conference on 3D Vision (3DV), 2021. [10] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit Style-content Separation Using B-LoRA. In European Conference on Computer Vision (ECCV), 2024. 4, 22 [11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 4, 22 [12] Xiangjun Gao, Xiaoyu Li, Chaopeng Zhang, Qi Zhang, Yanpei Cao, Ying Shan, and Long Quan. Contex-human: Free-view rendering of human from single image with textureconsistent synthesis. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4 [13] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. TokenVerse: Versatile Multi-Concept Personalization in Token Modulation Space. Transactions on Graphics (TOG), 2025. 4, 22 [14] Omer Gralnik, Guy Gafni, and Ariel Shamir. Semantify: Simplifying the Control of 3D Morphable Models Using CLIP. In International Conference on Computer Vision (ICCV), 2023. 8 [15] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-Supervised Scene Decomposition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, [16] Chen Guo, Junxuan Li, Yash Kant, Yaser Sheikh, Shunsuke Saito, and Chen Cao. Vid2AvatarPro: Authentic Avatar from Videos in the Wild via Universal Prior. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 14 [17] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and Hae-Gon Jeon. High-Fidelity 3D Human Digitization from Single 2K Resolution Images. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 8, 23 [18] Xinyang Han, Zelin Gao, Angjoo Kanazawa, Shubham Goel, and Yossi Gandelsman. The More You See in 2D the More You Perceive in 3D. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 23 [20] Xu He, Zhiyong Wu, Xiaoyu Li, Di Kang, Chaopeng Zhang, Jiangnan Ye, Liyang Chen, Xiangjun Gao, Han Zhang, and Haolin Zhuang. MagicMan: Generative Novel View Synthesis In AAAI Conference on of Humans with 3D-Aware Diffusion and Iterative Refinement. Artificial Intelligence, 2025. 2, 3, [21] Hsuan-I Ho, Lixin Xue, Jie Song, and Otmar Hilliges. Learning Locally Editable Virtual Humans. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 8, 23 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 3, 22 [23] Fa-Ting Hong, Zhan Xu, Haiyang Liu, Qinjie Lin, Luchuan Song, Zhixin Shu, Yang Zhou, Duygu Ceylan, and Dan Xu. Free-Viewpoint Human Animation with Pose-Correlated Reference Selection. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 5, 6 [24] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5, [25] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. GaussianAvatar: Towards Realistic Human Avatar Modeling from Single Video via Animatable 3D Gaussians. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3 [26] Han Huang, Liliang Chen, and Xihao Wang. UnconFuse: Avatar Reconstruction from Unconstrained Images. In European Conference on Computer Vision (ECCV), 2022. 4 [27] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, and Qing Wang. HumanNorm: Learning Normal Diffusion Model for High-Quality and Realistic 3D Human Generation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [28] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-Guided Reconstruction of Lifelike Clothed Humans. In International Conference on 3D Vision (3DV), 2024. 3 [29] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. MV-Adapter: Multi-View Consistent Image Generation Made Easy. In International Conference on Computer Vision (ICCV), 2025. 2, 5, 22, 23 [30] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. Arch: Animatable Reconstruction of Clothed Humans. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [31] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning (ICML), 2021. 2, 7 [32] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. InstantAvatar: Learning Avatars from Monocular video in 60 Seconds. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3 15 [33] Yudong Jin, Sida Peng, Xuan Wang, Tao Xie, Zhen Xu, Yifan Yang, Yujun Shen, Hujun Bao, and Xiaowei Zhou. Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models. arXiv preprint arXiv:2507.13344, 2025. 3 [34] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-Free Score Distillation. In International Conference on Learning Representations (ICLR), 2024. [35] Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, and Hanbyul Joo. Chupa: Carving 3D Clothed Hhumans from Skinned Shape Priors using 2D Diffusion Probabilistic Models. In International Conference on Computer Vision (ICCV), 2023. 4 [36] Xianghao Kong, Qiaosong Qi, Yuanbin Wang, Anyi Rao, Biaolong Chen, Aixi Zhang, Si Liu, and Hao Jiang. ProFashion: Prototype-Guided Fashion Video Generation with Multiple Reference Images. arXiv preprint arXiv:2505.06537, 2025. 5, 6 [37] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. MultiConcept customization of Text-to-Image Diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 4, 22 [38] Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, and Samaneh Azadi. Generating MultiImage Synthetic Data for Text-to-Image Customization. arXiv preprint arXiv:2502.01720, 2025. 4, 22 [39] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 3, [40] Boqian Li, Haiwen Feng, Zeyu Cai, Michael Black, and Yuliang Xiu. ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness. In International Conference on Computer Vision (ICCV), 2025. 4 [41] Boqian Li, Xuan Li, Ying Jiang, Tianyi Xie, Feng Gao, Huamin Wang, Yin Yang, and Chenfanfu Jiang. Garmentdreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details. In International Conference on 3D Vision (3DV), 2025. 3 [42] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. HybrIK: Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 4 [43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping LanguageImage Pre-Training with Frozen Image Encoders and Large Language Models. In International Conference on Machine Learning (ICML), 2023. 2, 7 [44] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, et al. Era3D: High-Resolution Multiview Diffusion Using Efficient Row-Wise Attention. Conference on Neural Information Processing Systems (NeurIPS), 2024. 6, 22 [45] Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Xiaowei Chi, Siyu Xia, Yan-Pei Cao, Wei Xue, et al. PSHuman: Photorealistic Single-Image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3, 4, 6, 7, 8, [46] Yihui Li, Chengxin Lv, Hongyu Yang, and Di Huang. Micro-Macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images. In AAAI Conference on Artificial Intelligence, 2025. 4 [47] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, and Michael Black. TADA! Text to Animatable Digital Avatars. In International Conference on 3D Vision (3DV), 2024. 3 [48] Tingting Liao, Yujian Zheng, Yuliang Xiu, Adilbek Karmanov, Liwen Hu, Leyang Jin, and Hao Li. SOAP: Style-Omniscient Animatable Portraits. In International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), 2025. 6 16 [49] Lixiang Lin, Songyou Peng, Qijun Gan, and Jianke Zhu. FastHuman: Reconstructing HighQuality Clothed Human in Minutes. In International Conference on 3D Vision (3DV), 2024. 3 [50] Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, and Ziwei Liu. HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [51] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3D: Single Image to 3D Using Cross-Domain Diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 6 [52] Jia Lu, Taoran Yi, Jiemin Fang, Chen Yang, Chuiyun Wu, Wei Shen, Wenyu Liu, Qi Tian, and Xinggang Wang. Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds. arXiv preprint arXiv:2508.14892, 2025. 2, 4 [53] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In European Conference on Computer Vision (ECCV), 2020. 4 [54] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning Robust Visual Features without Supervision. Transactions on Machine Learning Research Journal (TMLR), 2024. 5 [55] Werner Palfinger. Continuous remeshing for inverse rendering. Computer Animation and Virtual Worlds, 2022. [56] Zhuoyang Pan, Angjoo Kanazawa, and Hang Gao. SOAR: Self-Occluded Avatar Recovery from Single Video In the Wild. arXiv preprint arXiv:2410.23800, 2024. 3 [57] Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization. Transactions on Graphics (TOG), 2024. 2 [58] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit Neural Representations with Structured Latent Codes for Human Body Modeling. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023. 3 [59] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. DreamFusion: Text-to-3D Using 2D Diffusion. In International Conference on Learning Representations (ICLR), 2023. 2, 4 [60] Guocheng Qian, Kuan-Chieh Wang, Or Patashnik, Negin Heravi, Daniil Ostashev, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman. Omni-ID: Holistic Identity Representation Designed for Generative Tasks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 4, [61] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and Matthias Nießner. GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [62] Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, et al. LHM: Large Animatable Human Reconstruction Model from Single Image in Seconds. In International Conference on Computer Vision (ICCV), 2025. 2, 3, 7 [63] Lingteng Qiu, Peihao Li, Qi Zuo, Xiaodong Gu, Yuan Dong, Weihao Yuan, Siyu Zhu, Xiaoguang Han, Guanying Chen, and Zilong Dong. PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images. arXiv preprint arXiv:2506.13766, 2025. 4 17 [64] Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, et al. AniGS: Animatable Gaussian Avatar from Single Image with Inconsistent Gaussian Reconstruction. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 [65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3, [66] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 4, 22 [67] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In International Conference on Computer Vision (ICCV), 2019. 3 [68] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. PIFuHD: Multi-Level PixelAligned Implicit Function for High-Resolution 3D Human Digitization. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 [69] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs. In European Conference on Computer Vision (ECCV), 2024. 4, 22 [70] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. 360-degree Human Video Generation with 4D Diffusion Transformer. Transactions on Graphics (TOG), 2024. 8, 23 [71] Fei Shen and Jinhui Tang. Imagpose: unified conditional framework for pose-guided person generation. Conference on Neural Information Processing Systems (NeurIPS), 2024. 4, 22 [72] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep Marching Tetrahedra: Hybrid Representation for High-Resolution 3D Shape Synthesis. Conference on Neural Information Processing Systems (NeurIPS), 2021. 4 [73] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: Single Image to Consistent Multi-View Diffusion Base Model. arXiv preprint arXiv:2310.15110, 2023. 22 [74] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multiview Diffusion for 3D Generation. In International Conference on Learning Representations (ICLR), 2024. 22 [75] Yukai Shi, Jianan Wang, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, Heung-Yeung Shum, et al. TOSS: High-quality Text-guided Novel View Synthesis from Single Image. In International Conference on Learning Representations (ICLR), 2024. 22 [76] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo Tourism: Exploring Photo In International Conference on Computer Graphics and Interactive Collections in 3D. Techniques (SIGGRAPH), 2006. 4 [77] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In International Conference on Learning Representations (ICLR), 2021. 3, 22 [78] Stabilityai. Stable-Diffusion-2-1-Base. https://huggingface.co/stabilityai/ stable-diffusion-2-1-base, 2023. 23 [79] Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, and Matthias Niessner. GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-View Diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 18 [80] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, et al. Realfill: Referencedriven Generation for Authentic Image Completion. Transactions on Graphics (TOG), 2024. 2, 4, 22 [81] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: Extended Textual Conditioning in Text-to-Image Generation. arXiv preprint arXiv:2303.09522, 2023. 4, 22 [82] Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Xiaohang Zhan, and Zeyu Wang. Headevolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation. In International Conference on 3D Vision (3DV), 2025. 3 [83] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual Geometry Grounded Transformer. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 4, 7 [84] Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, and Otmar Hilliges. 4D-DRESS: 4D Dataset of Real-World Human Clothing with Semantic Annotations. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 8, [85] Yufu Wang, Yu Sun, Priyanka Patel, Kostas Daniilidis, Michael Black, and Muhammed Kocabas. PromptHMR: Promptable Human Mesh Recovery. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 4, 8 [86] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira KemelmacherShlizerman. HumanNerf: Free-Viewpoint Rendering of Moving People from Monocular Video. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [87] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3D: High-Quality and Efficient 3D Mesh Generation from Single Image. Conference on Neural Information Processing Systems (NeurIPS), 2024. 6 [88] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael Black. ICON: Implicit Clothed Humans Obtained from Normals. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3 [89] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael Black. ECON: Explicit Clothed Humans Optimized via Normal Integration. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 7 [90] Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, and Michael Black. PuzzleAvatar: Assembling 3D Avatars from Personal Albums. Transactions on Graphics (TOG), 2024. 2, 3, 4, 8, [91] Yifang Xu, Benxiang Zhai, Yunzhuo Sun, Ming Li, Yang Li, and Sidan Du. HiFi-Portrait: Zero-shot Identity-Preserved Portrait Generation with High-fidelity Multi-face Fusion. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 4, 22 [92] Xihe Yang, Xingyu Chen, Daiheng Gao, Shaohui Wang, Xiaoguang Han, and Baoyuan Wang. Have-Fun: Human Avatar Reconstruction from Few-Shot Unconstrained Images. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4 [93] Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zeng, Chen Wei, Qingping Sun, Haiyi Mei, Yanjun Wang, Hui En Pang, Mingyuan Zhang, et al. SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation. arXiv preprint arXiv:2501.09782, 2025. 4 [94] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 8, 23 19 [95] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi. Text-to-3D with Classifier Score Distillation. In International Conference on Learning Representations (ICLR), 2024. [96] Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, and Xiaowei Zhou. HumanRAM: Feed-Forward Human Reconstruction and Animation Model Using Transformers. In International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), 2025. 2, 4 [97] Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, and Xun Cao. Avatarbooth: HighQuality and Customizable 3D Human Avatar Generation. arXiv preprint arXiv:2306.09864, 2023. 2, 3, 4, 8 [98] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, Ting-Chun Wang, Ming-Yu Liu, and Yogesh Balaji. JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-toImage Generation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4, 22 [99] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop. In International Conference on Computer Vision (ICCV), 2021. 4 [100] Jingbo Zhang, Xiaoyu Li, Qi Zhang, Yanpei Cao, Ying Shan, and Jing Liao. HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4 [101] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral Reference for High-Resolution Dichotomous Image Segmentation. CAAI Artificial Intelligence Research, 2024. [102] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. PaMIR: Parametric Model-Conditioned Implicit Representation for Image-Based Human Reconstruction. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021. 3 [103] Xiangyu Zhu, Tingting Liao, Xiaomei Zhang, Jiangjing Lyu, Zhiwen Chen, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Li, and Zhen Lei. MVP-Human Dataset for 3-D Clothed Human Avatar Reconstruction From Multiple Frames. IEEE Transactions on Biometrics, Behavior, and Identity Science, 2023. 4 [104] Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, and Hongsheng Li. EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM. In International Conference on Machine Learning (ICML), 2024. 4, 22 20 22 22 22 23 23 23 23 23 24 25 25 28"
        },
        {
            "title": "Table of Contents",
            "content": "A Use of Large Language Models Related Work B.1 Subject-driven and ID-consistent Image Generation . . . . . . . . . . . . . . . Priliminary Implementation Details . D.1 Model Structure . D.2 Dataset . . D.3 Training Details . . D.4 Evaluation Metrics . . D.5 Inference Process . . . . . . . . . . . . . . . . . . . . . . Additional Visual Results E.1 Qualitative Comparisons . . E.2 Correlation Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Visual Results of Different Orthogonal Images Generation Designs Generation Results of UP2You"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "We used large language model to assist with copy editinggrammar checking, wording suggestions, and minor style and clarity improvementsafter the scientific content, methodology, analyses, and conclusions had been written by the authors."
        },
        {
            "title": "B RELATED WORK",
            "content": "B.1 SUBJECT-DRIVEN AND ID-CONSISTENT IMAGE GENERATION With the advent of powerful generative models [22, 39, 65, 77], subject-driven image generation has made remarkable progress in recent years. Various approaches have been proposed to generate images of specific subjects, such as optimizing specialized tokens to encode subject concepts [1, 11, 81], learning personalized modulation vectors for each concept [13], or fine-tuning pre-trained diffusion models [3, 10, 37, 66, 69] using handful of reference images. Additionally, methods like JeDi [98] and SynCD [38] utilize global self-attention mechanisms to effectively fuse information from multiple images of target subject, while EasyRef [104] leverages Vision-Language Models (VLMs) [4]. For human-centric generation, several methods have been developed to handle identity preservation. For instance, Omni-ID [60], IMAGPose [71], and HiFi-Portrait [91] utilize specialized image encoders to process multiple reference images for ID-preserving image synthesis. However, extending these techniques to the full body is non-trivial, as the human bodys highly articulated structure and nonrigid deformations introduce significant challenges for feature fusion. To tackle this, approaches like Total Selfie [6], and RealFill [80] employ few-shot personalization via fine-tuning [66] to capture consistent identities, including both facial features and overall appearance. Nevertheless, these methods are tailored for 2D image generation and lack the mechanisms needed to ensure cross-view consistency or the precise latent feature aggregation required for high-fidelity 3D reconstruction."
        },
        {
            "title": "C PRILIMINARY",
            "content": "We review the fundamentals of multi-view diffusion models [44, 7375], with particular focus on MV-Adapter [29], which serves as the foundation for the multi-view generation of UP2You. Multi-View Diffusion Models. Multi-view diffusion models extend single-view generation by introducing multi-view attention mechanisms, enabling the synthesis of images that are consistent across different viewpoints. Several works [74, 75] generalize the self-attention mechanism of standard diffusion models to operate over all pixels from multiple views. Specifically, given in as the input to the attention block, multi-view self-attention concatenates features from views, allowing the model to capture global dependencies. However, this approach incurs significant computational overhead due to the need to process all pixels across all views. To mitigate this, row-wise selfattention [44, 45] leverages geometric correspondences between orthogonal views. For example, Era3D [44] restricts attention to the current view and corresponding rows from other views, which is well-suited for orthogonal multi-view generation and substantially reduces computational cost. Building on row-wise self-attention, MV-Adapter [29] introduces an image-to-multiview (I2MV) generator with parallel attention architecture. The original self-attention block is modified as: self = SelfAttn(f in) + MVAttn(f in) + RefAttn(f in, F) + in, (5) Here, MVAttn represents the row-wise self-attention mechanism, while RefAttn is crossattention module that integrates the reference image feature into in. The feature is extracted from the input image using the reference network [24]: = R(I). The I2MV generation process in MV-Adapter is formulated as = D(F, P(P)), where = {V1, V2, . . . , VM } denotes the set of generated multi-view images, represents the multi-view diffusion model, = {P1, P2, . . . , PM } specifies the target viewpoint conditions, and is the condition encoder that fuses viewpoint conditions into D. In MV-Adapter, only MVAttn, RefAttn, and are trained for I2MV generation. Each is encoded as camera ray representation, referred to as raymap. Typically, = 6 orthogonal views are generated, corresponding to the target view angles {0, 45, 90, 135, 180, 270}. 22 Given the efficient plug-and-play adapter training mechanism of MV-Adapter, combined with the robust feature extraction capabilities of ReferenceNet for processing unconstrained photographs, we adopt MV-Adapter as our multi-view diffusion model architecture. Furthermore, considering our focus on human-centric tasks, we utilize SMPL-X normal rendering as the viewpoint condition P."
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "D.1 MODEL STRUCTURE the adopt framework architecture We the stable-diffusion-2-1-base version [78] as the foundation for both multi-view image and normal generation. The number of selected reference features γ is set to 2.0 during both training and inference phases. We employ the DINOv2-Large [8] variant of the DINOv2 encoder ref. For the pose image encoder ref, we implement lightweight ResNet [19] architecture. The learnable shape tokens τ R101024 are configured to align with the dimensions of ref, and the perceiver blocks in comprise 6 layers of cross-attention. of MV-Adapter with [29] D.2 DATASET We train our multi-view image generation, normal map generation, and shape prediction models using the THuman2.1 [94], Human4DiT [70], 2K2K [17], and CustomHumans [21] datasets. Since our task requires handling scenarios where individuals with the same identity appear in different poses, we manually filter the data and group samples by identity. The final training dataset comprises 6,921 scans spanning 2,091 distinct identities. For each scan, we render 6 orthogonal views ({0, 45, 90, 135, 180, 270}) of both images and normal maps, along with the corresponding SMPL-X normal rendering. Additionally, we render 8 views of each scan using randomly selected perspective cameras to provide unconstrained photos. During orthogonal image generation training, for each case, we randomly select 3 to 8 reference images from other cases sharing the same identity. For evaluation, we select 40 identities from PuzzleIOI [90] and additionally choose A-pose configurations from all 68 identities in 4D-Dress [84], while utilizing the remaining poses as reference views. To ensure that SMPL-X camera normal rendering accurately represents viewpoint information, we rotate all scans so that the front view corresponds to zero azimuth. Beyond synthetic data, we also collect an in-the-wild dataset comprising 12 identities for further evaluation, ensuring robust evaluation in diverse scenarios. D.3 TRAINING DETAILS We train the image and normal generation models end-to-end using denoising losses Lrgb and Lnormal , respectively. During training, Lrgb jointly optimizes the components pose, , Wq, Wk, AvgPool, rgb, and Drgb. In normal maps generation training, Lnormal optimizes normal and Dnormal. For shape prediction, we employ the loss function Lv = T(βpred) T(βgt) to compute the vertex-wise distance between SMPL-X meshes generated from the predicted shape parameters βpred and the ground-truth shape parameters βgt. The complete training process for the image and normal generation models requires approximately 3 and 2 days, respectively, on 8 NVIDIA 5880 GPUs. We employ batch size of 1 per GPU under bfloat16 mixed precision and train for 50,000 iterations. All pose, input, and output image resolutions are consistently set to 768 768. The reference images for both image and normal generation are also configured at 768 768 resolution, while the target orthogonal view angles follow the same configuration as MV-Adapter. The shape prediction model undergoes training for 100,000 iterations on 8 NVIDIA 5880 GPUs with batch size of 8 per GPU, requiring approximately 10 hours. We apply constant learning rate of 5 105 with warm-up for training all models. D.4 EVALUATION METRICS We employ three complementary metrics to assess geometric accuracy: (1) Chamfer distance (bidirectional point-to-surface distance in cm), which measures overall geometric similarity; (2) P2S 23 distance (unidirectional point-to-surface distance in cm), which captures reconstruction completeness; and (3) L2 error for Normal maps rendered from four canonical views ({0, 90, 180, 270}), which evaluates fine-grained surface detail preservation. We render multi-view color images from the same four canonical viewpoints and evaluate appearance fidelity using three established image quality metrics: PSNR (Peak Signal-to-Noise Ratio) for pixel-level accuracy, SSIM (Structural Similarity) for structural consistency, and LPIPS (Learned Perceptual Image Patch Similarity) for perceptual similarity. For the in-the-wild dataset, which lacks 3D ground truth, we assess reconstruction quality using perceptual similarity metrics CLIP-I and DINO computed between the generated front view and the captured reference front view image with A-pose. We further evaluate shape prediction accuracy by computing vertex-to-vertex (V2V) distances between predicted and ground truth SMPL-X meshes under canonical T-pose (zero pose and expression). D."
        },
        {
            "title": "INFERENCE PROCESS",
            "content": "The inference process of UP2You for unconstrained photo inputs mainly consists of four steps as follows: (1) Use to estimate SMPL-X shape parameters βpred from I, and initialize the SMPL-X mesh with βpred and predefined pose (e.g., A-pose with zero expression) to obtain the pose condition P. (2) Generate multi-view images using Drgb, conditioned on and P. (3) Generate multi-view normal maps using Dnormal, conditioned on and P. (4) Reconstruct the textured mesh using the initialized SMPL-X mesh, V, and N. For data preand post-processing, we employ [101] to remove backgrounds from input unconstrained photos. Additionally, the reference masks are resized and adapted to the correlation maps to enhance the models focus on foreground regions. Inference Time. The complete pipeline requires approximately 1.5 minutes to generate textured mesh from single unconstrained input. Specifically, the shape prediction step takes about 1 second, multi-view image generation requires approximately 15 seconds, normal map generation takes about 15 seconds, and mesh reconstruction, along with other processing steps (e.g., foreground segmentation, data postprocessing, and file saving), takes nearly 1 minute."
        },
        {
            "title": "E ADDITIONAL VISUAL RESULTS",
            "content": "E.1 QUALITATIVE COMPARISONS We present additional qualitative comparison results in Figs. 18 to 21, including mesh reconstruction, front-view 3D human reconstruction, and shape prediction comparisons. Please zoom in for details. Figure 18: More Qualitative Comparisons on 4D-Dress and PuzzleIOI datasets. 25 Figure 19: More Qualitative Comparisons on in-the-wild dataset. Figure 20: More Qualitative Comparisons of Single Image 3D Human Reconstruction with PSHuman. Figure 21: Error Maps of Shape Prediction. 27 E.2 CORRELATION MAPS Pose-dependent correlation maps generation is an important module of UP2You, as the first part of the proposed PCFA, it predicts the most relevant regions of input unconstrained photos for the conditioned pose. With the latter feature selection strategy, PCFA can focus on informative features for viewpoint generation. In Fig. 22, we provide more results of the generated correlation maps. Figure 22: Visualize Results of Correlation Maps. Given the input reference images and target pose for multi-view image generation, the predicted correlation maps can effectively identify and discriminate correlated regions within the reference inputs. For example, when generating images in the front-view, reference regions that correspond to front-facing views exhibit higher correlation values, demonstrating the models ability to selectively attend to relevant spatial information."
        },
        {
            "title": "DESIGNS",
            "content": "Here, we present the generated visual results in Fig. 23 for different design choices in the multi-view image generation model. As indicated earlier, approach directly concatenates all reference features for viewpoint generation, which may provide irrelevant features during generation and lead to poor results. Approach averages all reference features as global guidance. This method is time-efficient but loses important color features and generates suboptimal results. Approach uses weighted sum strategy to aggregate reference features after computing the correlation map, which loses details in some regions since regions with high correlation values may overlap. Approaches and utilize CLIP and DINOv2 features, respectively, rather than ReferenceNet as in our method. CLIP features have low resolution and are difficult to preserve details such as facial and clothing textures, while DINOv2 is texture-insensitive and thus difficult to restore reference textures accurately. Figure 23: Visual Comparisons of Different Multi-View Image Generation Designs. 29 GENERATION RESULTS OF UP2YOU Figures 24 and 25 show the detailed generation results of UP2You for two representative cases, including the reference images used, generated multi-view images and normal maps, and rendered images and normal maps after mesh reconstruction. Figure 24: Generated Results of UP2You. 30 Figure 25: Generated Results of UP2You."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Westlake University"
    ]
}