{
    "paper_title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
    "authors": [
        "Ellis Brown",
        "Arijit Ray",
        "Ranjay Krishna",
        "Ross Girshick",
        "Rob Fergus",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks."
        },
        {
            "title": "Start",
            "content": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding Ellis Brown1 Arijit Ray2 Ranjay Krishna3 Ross Girshick4 Rob Fergus1 Saining Xie1 1New York University 2Boston University 3AllenAI 4Vercept https://ellisbrown.github.io/sims-v"
        },
        {
            "title": "Abstract",
            "content": "Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on realworld video data, obtaining diverse footage with precise spatial annotations remains bottleneck. To alleviate this bottleneck, we present SIMS-Va systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify minimal set of three question categories (metric measurement, perspectivedependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks. 5 2 0 2 3 1 ] . [ 2 8 6 6 4 0 . 1 1 5 2 : r Figure 1 SIMS-V enables learning real-world spatial concepts in simulation. We generate spatially-rich videos with dense spatial annotations via privileged simulator data, creating diverse question-answer pairs. Models trained on this simulated data transfer effectively to real-world spatial reasoning benchmarks. 1. Introduction Despite strong performance on most benchmarks [64, 34, 1, 51, 66], multimodal large language models (MLLMs) struggle with understanding spatial reasoning [36, 8, 46, 20, 54, 25]. This challenge is particularly acute for spatiotemporal reasoning in videos, where models must track and reason about spatial configurations across time [59]. Many attribute this weakness to lack of spatial annotations in web-scale image-text pretraining data [5, 47, 59, 46, 12]. While real-world video data with precise 3D spatial annotations would address this gap, such data is prohibitively expensive to collect at scale. Simulators offer compelling alternative with perfect ground truth and scalable generation, but sim-to-real transfer remains fundamental challenge in computer vision [53, 39, 55, 35]. Critical questions remain: What properties of simulated spatial data are necessary for effective transfer to real-world video understanding? Which question types and data configurations matter most? To investigate these questions, we present SIMS-V, simulated instruction-tuning framework for multimodal spatial video understanding. SIMS-V leverages procedurally generated 3D environments and the privileged information available in simulators to programmatically create rich question-answer pairs about spatial configurations across videos. As concrete instantiation, we generate SIMS-VSI, dataset comprising over 200k spatial question-answer pairs across 2.5k video trajectories spanning 1.2k diverse indoor scenes. Unlike prior work using simulation for single-image spatial tasks [46], we formulate complex questions requiring spatiotemporal reasoningperspective-taking, metric distance estimation, and temporal tracking of object appearances across minutes-long video trajectories. This controlled framework enables us to systematically investigate which properties of simulated data drive effective sim-to-real transfer. Through controlled experiments, we uncover key insights about designing effective simulated spatial training data. We identify minimal set of three question categories (metric measurement, perspective-taking, and spatiotemporal tracking) that prove most effective for developing transferable spatial intelligence. Notably, this focused mix outperforms comprehensive training set mirroring the full VSI-Bench distribution, demonstrating that carefully selected supervision on core spatial reasoning dimensions can be sufficientand even superiorto comprehensive coverage. Additionally, we find that spatial reasoning capabilities can be learned remarkably efficiently: training on just 5K targeted simulated examples enables performance competitive with models trained on orders of magnitude more data. Applying these insights, we demonstrate that 7B-parameter LLaVA model fine-tuned on just 25K simulated examples achieves competitive performance with large proprietary models like GPT-4o [27] and Gemini-1.5-Pro [52]. To ensure rigorous evaluation of genuine visual reasoning capabilities, we report results on both VSI-Bench [59] and its robustified counterpart VSI-BenchDebiased [4] which is refined to minimize non-visual shortcuts. Our approach also demonstrates strong generalization, maintaining performance on general video understanding benchmarks while showing substantial improvements on diverse spatial tasks including embodied reasoning [37] and real-world scenarios [19, 38]. The remainder of this paper is organized as follows: We first situate our work within related research on spatial reasoning, video-language models, and synthetic data (Sec. 5). We then detail the SIMS-V framework for generating controlled simulated spatial training data (Sec. 2). Next, we present systematic experiments investigating which question types, data mixes, and quantities drive effective real-world transfer (Sec. 3). We evaluate transfer of understanding to real-world benchmarks (Sec. 4). Finally, we discuss limitations and future directions (Sec. 6). 2 Figure 2 The SIMS-V pipeline generates diverse spatial training data with perfect ground truth. We procedurally generate 3D scenes using AI2-THOR [30], ProcTHOR [17], and Objaverse [16], capture agent navigation trajectories, extract dense annotations (global spatial layout and perframe observations), and programmatically generate quality-controlled question-answer pairs spanning diverse spatial reasoning categories. This systematic pipeline enables controlled ablations of question types and data configurations, maintaining perfect spatial ground truth. 2. Simulated Spatial Instruction-Tuning Simulators offer uniquely powerful tool for addressing this data bottleneck. Unlike real-world footage, simulators provide perfect ground truth spatial and per-observation information while enabling systematic control over scene complexity and camera trajectories. This combination allows us to investigate the minimal requirements for learning transferable spatial reasoning capabilities. Using this framework, we generate SIMS-VSI, dataset comprising over 200k question-answer pairs across 2.5k video trajectories through 1.2k diverse indoor scenes. In this section, we describe our Simulated Instruction-tuning for Multimodal Spatial-understanding (SIMS-V) framework for generating high-quality spatial training data. 2.1. Simulated Video Generation To create simulated instruction-tuning data, we first generate realistic indoor environments and capture trajectories of agents navigating through these spaces, carefully extracting comprehensive metadata at each step. Scene Generation and Trajectory Capture We leverage the AI2-THOR simulator [30] with environments procedurally generated by ProcTHOR [17] using objects from Objaverse [16] for added diversity. Each generated environment contains approximately 3050 objects across 38 rooms, providing complex spatial arrangements necessary for challenging reasoning tasks. We capture multiple trajectories through each environment, touring all rooms from various starting points to ensure comprehensive spatial coverage. To accomplish this, we plan shortest path traversal to the center of each room, followed by 360-degree rotation at the center to capture 3 full view of the room. Videos are captured at 10 frames per second with resolution of 680 384, resulting in sequences of 601,800 frames (12s3min) per trajectory. We render scenes in the highest quality available via the simulator by default. Dense Annotation Extraction Our pipeline extracts two complementary metadata types: Observation-level data. Per-frame information including visible objects, instance segmentation masks, and agent position & viewpoint. This captures all information about what artifacts are visible at each time step. Global spatial data. Comprehensive scene-level information including room layouts, 3D object positions, and object metadata. This provides perfect ground truth for generating spatial questions even about objects currently outside the agents field of view. 2.2. Spatial Question-Answer Generation Using our dense metadata, we programmatically generate precise spatial reasoning questions with guaranteed ground truth answers. This automated process leverages both observation-level and global spatial metadata to create questions that would be prohibitively expensive to annotate in real-world video data. Specifically, our question generation pipeline follows these steps: 1. Metadata preprocessing: We first identify salient objects (those visible above minimum pixel threshold) and extract their spatial relationships, temporal appearances, and 3D positions. 2. Question formulation: Based on the preprocessed metadata, we generate questions across diverse spatial reasoning categories, such as layout understanding (counting, measurements, distances), relative spatial relationships (positional and directional), and temporal-spatial memory (appearance order). 3. Format variation: For each question, we automatically generate both open-ended and multiplechoice versions, with carefully balanced multiple-choice distractors. 4. Quality control: The generation process incorporates rigorous quality control, including minimum visibility thresholds for objects, distance requirements between objects, and angle thresholds for directional questions. This ensures that all generated questions are clearly answerable from the video content without ambiguity. This systematic data generation framework enables us to conduct controlled experiments investigating which properties of simulated data drive effective transfer to real-world spatial reasoning, which we present in the following sections. 3. Investigating Simulated Data Properties To investigate what properties of simulated data are most important for developing real-world spatial reasoning capabilities, we conduct systematic study examining question types, data mix design, and quantity requirements. Using LLaVA-Video-7B [65] with Qwen2 [58] as our base model, we fine-tune on various configurations of our SIMS-VSI data and evaluate on VSI-Bench [59], comprehensive benchmark that tests spatial intelligence using real-world videos. Figure 3 Examples of different question types used in our experiments. Each question is shown alongside its corresponding visual context from simulated environment. The questions span diverse spatial reasoning capabilities including numerical measurement, relative positioning, and temporal tracking. Full details of all question formats are provided in Appendix B.2. 3.1. Question Type Transfer Analysis We first investigate which individual types of spatial reasoning questions are most effective for developing transferable spatial intelligence in video language models. Given the vast range of possible spatial questions that could be generated from simulator metadata, identifying which question formats provide the strongest learning signal is crucial for efficient training. To establish rigorous baseline for evaluating sim-to-real transfer, we generate questions that mirror the format and structure of those in the VSI-Bench evaluation set (see Fig. 3). This ensures any performance gains can be directly attributed to the models ability to leverage simulated data rather than differences in question formulation or evaluation criteria. 1 To systematically evaluate question effectiveness, we tune LLaVA-Video-7B on 5K examples of each question type separately, and then evaluate performance across all categories on the VSI-Bench test set. This approach allows us to measure both the direct impact of each question type on its corresponding benchmark category and any transfer effects to other spatial reasoning capabilities. As shown in Fig. 4, training on single type primarily boosts its own category, with selective cross-task (off-diagonal) effects. Among broader families, spatiotemporal (appearance order) and measurement (absolute distance) drive the largest improvements: appearance order yields MC: +28.9 and OE: +10.2 on its category, while absolute distance yields OE: +16.9 and MC: +11.6. Cross-task gains are modest but interpretable, e.g., absolute distance room size (OE: +12.2) and relative distance appearance order / route planning (MC: +7.2 / +4.6). In contrast, object counting can hurt its own category (OE: 22.4, MC: 5.2), possibly due to greater object diversity in our simulated QAs compared to those in VSI-Bench. These patterns suggest emphasizing temporal and metric cues over broad type coverage when constructing simulated supervision. 1We did not implement route planning questions due to its complexity. 5 Figure 4 Training on individual question types yields large on-task gains with localized crosstask effects. We fine-tune LLaVA-Video-7B on 5k simulated questions of each question type and format (rows), evaluating each model on all VSI-Bench question types (columns). Values are performance Δ vs. the pretrained baseline (positive is green, negative is red). 3.2. Data Mix Design and Scaling Next, we explore how combining several question types can create more comprehensive training mixes that develop robust spatial understanding across multiple dimensions. VSI-Baseline Mix As our first approach, we create training mix that closely mirrors the VSIBench test set in both question types and distribution. This approach minimizes the gap between training and evaluation tasks, allowing us to evaluate the effect of sim-to-real transfer without other confounding factors such as differences in question phrasing. However, now we wish to evaluate the mixes of question types that generalize to other question types as well in the VSI-Benchmark and other spatial benchmarks. 3Q Minimal Mix Building on insights from our individual question type analysis, we hypothesize that minimal set of complementary question types could be sufficient to develop strong spatial reasoning capabilities, without needing to mirror the full VSI-Bench distribution. We select three representative questions that cover the core dimensions of spatial reasoning: Measurement: Absolute distance estimation (open-ended) tests the ability to perceive and estimate metric properties of space. Perspective: Relative direction determination (multiple-choice) evaluates understanding of perspective-dependent spatial configurations. Spatiotemporal: Appearance order tracking (multiple-choice) assesses the ability to track objects across time and remember their temporal relationships. These question types complement each other, requiring the model to develop comprehensive spatial representations covering metric properties, directional relationships, and temporal dynamics. How much data is needed? To understand the data efficiency of our approach and compare the two mixes, we scale the training set from 1K to 25K examples for both the VSI-Baseline and 3Q Minimal mixes. As shown in Fig. 5, performance on VSI-Bench climbs rapidly for both mixes, 6 Pct 16.9 19.3 5.8 11.4 Q-Type Open-Ended Abs Dist Obj Size Rm Size Obj Ct Multi-Choice 14.4 Rel Dist 19.6 Rel Dir Rt Plan 0.0 App Ord 12. Figure 5 Minimal 3Q mix is more data-efficient than comprehensive coverage. Left: Both training mixes show rapid improvement on VSI-Bench, with 3Q consistently outperforming the full baseline mix despite using only three question types. At 5K examples, we surpass Gemini-1.5 Flash; at 25K, we approach Gemini-1.5 Pro. Right: Distribution of question types in VSI-Baseline mix, which mirrors the VSI-Bench test set composition. Full Q-Type Dist. with the 3Q Minimal mix reaching 42.5% at just 5K examplesalready surpassing Gemini-1.5 Flash (42.1%). Notably, the 3Q mix consistently outperforms the VSI-Baseline mix across all data scales, achieving 44.4% at 25K examples and nearly matching Gemini-1.5 Pro (45.4%), despite being trained on more focused set of question types. This demonstrates two key insights: (1) high-quality spatial annotations enable remarkably data-efficient learning, with strong performance achievable from just thousands of examples; and (2) carefully selected minimal question set can be more effective than comprehensive coverage, suggesting that focused supervision on core spatial reasoning dimensions is sufficient to develop transferable capabilities. Based on these findings, we use the 3Q Minimal mix for our main results and generalization experiments. 4. Sim-to-Real Evaluation on Real-World Benchmarks Having established that our 3Q Minimal mix provides the most effective and efficient training approach through controlled ablations, we now rigorously evaluate its ability to transfer spatial reasoning capabilities from simulation to real-world videos. We train both LLaVA-Video-7B and LLaVA-OneVision-7B on 25K examples to assess sim-to-real transfer strength and architectural robustness. While both models share the same backbone (SigLIP vision encoder and Qwen2-7B language model), they differ in their specialization: LLaVA-Video is optimized for video understanding with video-centric pretraining and temporal token allocation, while LLaVA-OneVision is generalist model trained for single-image, multi-image, and video tasks. This controlled comparison at identical parameter scale allows us to isolate whether improvements stem from our spatial training data itself rather than architecture-specific effects. 4.1. Transfer to VSI-Bench Spatial Reasoning Our training data was explicitly designed as simulated train set for the VSI-Bench test set to investigate the potential of sim-to-real transfer for spatial reasoning in video. 7 Table 1 VSI-Bench Performance: Competitive with large proprietary models. LLaVA-Video-7B fine-tuned on 25K SIMS-V examples achieves 44.4%, surpassing GPT-4o (34.0%), the larger LLaVAVideo-72B (41.2%), and approaching Gemini-1.5 Pro (45.4%). Strong gains in appearance order (+26.4%) and absolute distance (+20.0%) demonstrate effective spatial transfer to real-world videos. LLaVA-OneVision-7B fine-tuned on the same set achieves similar gains. a N Average 34. 34.0 42.1 45.4 51.5 39.3 33.4 47.1 48.7 46.5 28.6 31.7 34.6 37.0 42.1 56. D z bj m bj Numerical Answer l t e Multiple-Choice Answer t O 32.0 29.9 33. 62.1 5.3 30.8 30.9 34.9 43.8 53.5 64.1 64.3 38.2 54.4 43.6 42.8 46.2 49.8 56.2 43.8 25.0 25. 37.0 37.7 51.3 61.1 36.1 47.9 41.3 41.0 46.3 47.8 28.3 28.4 31.5 31.5 36.0 45.9 25.0 25. 28.5 37.8 34.6 71.3 42.4 34.0 40.3 40.0 38.0 48.6 24.5 41.2 15.2 36.0 44.4 35.2 +8.4 +6.3 +10.7 +20.0 42.5 40.9 25.2 39.3 34.7 14.8 35.0 35.3 41.8 31.2 40.4 39.1 +6.6 +16.4 +5.4 +4.4 56.5 37.0 24.1 46.9 38.1 41.2 -5.7 +14.0 41.8 57.0 23.1 47.6 29.1 44.9 +6.0 -2.7 33.0 33.5 35. 41.7 44.1 53.8 51.4 49.2 36.1 31.9 42.4 49.7 58.3 47.3 46.5 -3.2 +9.7 +4.9 +1.6 +26.4 32.0 42.8 47.6 34.9 46.1 53.2 43.4 27.8 38.4 31.4 48.9 45.1 44.4 28.9 51.0 -2.5 +21.1 -2.2 +1.0 +6.7 Methods Statistics Chance Level (Rand.) Chance Level (Freq.) Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Gemini-2.5 Pro Open-source Models LLaVA-Video 72B LLaVA-Video 7B + 25k SIMS-V 3Q Δ Improvement LLaVA-OneVision 72B LLaVA-OneVision 7B + 25k SIMS-V 3Q Δ Improvement Table 2 VSI-Bench-Debiased Performance: Gains persist with reduced non-visual shortcuts. Results on the debiased version [4] show consistent improvements (LLaVA-Video +7.7%, LLaVAOneVision +5.4%), confirming that SIMS-V develops genuine visual reasoning rather than exploiting statistical artifacts. Category-level patterns mirror the original benchmark. Methods Proprietary Models (API) Gemini-1.5 Flash Gemini-1.5 Pro Gemini-2.5 Pro Open-source Models LLaVA-Video 72B LLaVA-Video 7B + 25k SIMS-V 3Q Δ Improvement LLaVA-OneVision 72B LLaVA-OneVision 7B + 25k SIMS-V 3Q Δ Improvement a N Average t b bj m bj Numerical Answer l t e Multiple-Choice Answer t O 39.9 40.1 49.1 44.8 43.2 43.2 35.0 37.1 55. 28.5 28.9 34.4 45.7 51.2 56.7 56.2 44.5 40.9 48.9 48.0 40.8 35.5 51.4 61.0 34.7 38.7 46. 28.2 24.8 43.0 41.4 33.6 69.5 38.8 33.7 42.6 34.8 27.7 34.3 45.8 33.7 22.3 36.8 22.1 34.7 15.4 30.7 37.6 38.4 31.5 32.3 -2.4 +15.5 +7.7 +6.6 +9.0 +16.1 40.8 44.3 34.7 35.6 36.5 21.8 18.0 31.4 31.3 25.7 16.3 28.5 22.1 39.1 26.6 33.9 32.4 28.7 +8.6 +5.8 +1.0 +5.4 +3.0 +7.8 37.8 43.9 37.3 38.6 37.9 42.6 39.2 51.9 35.7 -2.9 +9.3 +1.3 37.3 39.0 32.0 37.1 34.2 39.7 37.4 43.9 33.6 -3.5 +4.2 +3. 25.4 48.1 34.0 20.2 60.1 19.3 -0.9 +26.1 47.8 21.9 23.7 27.7 52.2 22.8 -0.9 +24.5 8 Evaluation on VSI-Bench-Debiased. To ensure our results reflect genuine visual reasoning rather than exploitation of statistical shortcuts, we evaluate on both the original VSI-Bench and VSIBench-Debiased [4]. VSI-Bench-Debiased is refined version that systematically reduces non-visual shortcuts through diagnostic analysis, providing more rigorous test of visual understanding. We present detailed results on both benchmarks: the original for comparison with proprietary models and prior work, and the debiased version to demonstrate robust visual reasoning capabilities. Our 3Q Minimal mix model achieves 44.4% on the original VSI-Bench (Tab. 1), surpassing GPT4o (34.0%) and approaching Gemini-1.5 Pro (45.4%), despite being trained on just 25K examples. The model shows particularly strong improvements in spatiotemporal (appearance order: +26.4%) and measurement (absolute distance: +20.0%) capabilities. Critically, these improvements transfer consistently to VSI-Bench-Debiased (Tab. 2), where LLaVA-Video-7B achieves 38.4% (+7.7% over baseline) and LLaVA-OneVision-7B achieves 33.9% (+5.4% over baseline). The consistent gains across both benchmark versions confirm that our approach develops genuine visual reasoning capabilities rather than exploiting statistical shortcuts, as the debiased version is specifically designed to reduce non-visual solvability. Additionally, the strong performance of LLaVA-OneVision demonstrates that our findings are not architecture-specific but reflect fundamental principles of learning spatial reasoning from simulation. 4.2. Generalization to Diverse Benchmarks To verify that spatial-focused training does not degrade general capabilities and that learned skills transfer beyond our target VSI-Bench evaluation, we assess performance across diverse tasks spanning video understanding, embodied reasoning, and real-world scenarios. Maintained General Video Understanding. Our approach maintains stable performance on general video understanding benchmarks. On VideoMME [19], comprehensive benchmark covering diverse real-world videos, performance remains essentially unchanged (LLaVA-Video: 63.3%63.1%, LLaVA-OneVision: 58.3%59.0%). Similarly, EgoSchema [38], which tests longform egocentric video understanding, shows modest improvements (+2.2% for LLaVA-Video, stable for OneVision). This demonstrates that spatial-focused tuning does not cause catastrophic forgetting of general video reasoning capabilities. Transfer to Embodied and Real-World Spatial Tasks. Beyond maintaining general capabilities, our approach shows strong positive transfer to spatial tasks in new domains. On OpenEQA [37], an embodied question-answering benchmark, LLaVA-Video improves dramatically (+8.6%), demonstrating that spatial concepts learned in simulated indoor environments transfer to embodied reasoning scenarios. Similarly, MMRealWorld [63], which features high-resolution real-world outdoor images, shows consistent improvements (+4.5% for LLaVA-Video, +1.1% for OneVision), indicating that learned spatial understanding generalizes beyond the indoor training domain. Architectural Robustness and Limitations. The consistent improvements across both LLaVAVideo (+8.4% on VSI-Bench) and LLaVA-OneVision (+5.4%) confirm that our findings reflect general principles of spatial learning from simulation rather than architecture-specific effects. The larger gains for LLaVA-Video are expected given its video-centric pretraining and specialized temporal token allocation, while the solid improvements for the generalist LLaVA-OneVision model demonstrate that SIMS-V training transfers effectively even to models not specifically optimized for video. However, we observe modest negative transfer on some benchmarks, which we attribute to catastrophic forgetting from training exclusively on SIMS-VSI data without mixing with general 9 Table 3 SIMS-V training transfers to embodied and real-world scenarios while maintaining general capabilities. Results across diverse benchmarks show strong spatial transfer (OpenEQA +8.6%, MMRealWorld +4.5% for LLaVA-Video) and stable egocentric (EgoSchema) and general video understanding (VideoMME) performance. Model Proprietary Models GPT-4o [27] Gemini-1.5-Pro [52] Gemini-2.5 Pro [13] Open-Source Models LLaVA-Video 72B LLaVA-OneVision 72B InternVL2.5 8B [11] Qwen-VL-2.5 7B [3] LLaVA-Video 7B + 25k SIMS-V 3Q Δ Improvement LLaVA-OneVision 7B + 25k SIMS-V 3Q Δ Improvement VSI-B VSI-BDeb. OpenEQA MME.RWlite EgoSchema VideoMME 34.0 45.4 51.5 41.2 40.9 34.6 33.5 36.0 44.4 +8.4 35.0 40.4 +5.4 40.1 49.1 36.8 35.6 24.9 29.6 30.7 38.4 +7.7 28.5 33.9 +5. 43.8 43.0 34.6 43.2 +8.6 42.1 42.2 +0.1 36.0 48.2 35.2 39.7 +4.5 48.6 49.7 +1.1 72.2 66.7 62.8 50.6 65.0 56.9 59.1 +2.2 60.8 60.5 -0. 71.9 75.0 68.8 66.7 64.2 65.1 63.3 63.1 -0.2 58.3 59.0 +0.7 instruction data. Future work should explore optimal strategies for combining SIMS-VSI with broader multimodal training data to maximize spatial gains while preserving general capabilities. 5. Related Work Our work sits at the intersection of spatial reasoning in video-language models and synthetic data generation for multimodal training. Spatial and 3D Reasoning in Vision-Language Models. Spatial understanding has been long-standing challenge in computer vision, with early work focusing on grounding objects in 2D [42, 29, 41] or 3D [9, 24, 2] using language descriptions. Recent work addresses more complex spatial relationships between multiple objects [50, 33, 26, 45]. Despite advances in MLLMs [20, 8, 12, 54, 44, 60], spatial reasoning remains fundamental weakness. Recent approaches address this through pseudo-annotations on real images [12] or synthetic data for single-image tasks [46], but video-based spatial reasoning with temporal dependencies [59] remains under-explored, as most video benchmarks emphasize action recognition [38, 40] or embodied object recognition [37] rather than spatiotemporal reasoning. Video-Language Models and Datasets. Our work builds on multimodal foundation models [43, 28, 56, 57, 49] and recent video-language models [34, 66, 1, 51, 64]. While video-language datasets have evolved from static images [21, 26] to temporal reasoning [19], they predominantly focus on action recognition [38, 40, 14, 23] rather than geometric spatial relationships. For example, questions like what color is the cup? require only identifying the relevant frame, whereas we focus on spatiotemporal reasoning requiring metric distance estimation, perspective-taking, and tracking object appearances across time. 10 Learning from Synthetic Data for Spatial Understanding. Synthetic data has shown promise for improving spatial capabilities in vision-language models. SAT [46] uses simulation with actionbased learning to improve spatial reasoning in static images, while SpatialRGPT [12] employs pseudo-annotations on real images. REVISION [7] introduces 3D rendering pipeline to improve spatial fidelity in text-to-image generation models, focusing on generating spatially accurate images from text prompts and evaluating MLLMs on spatial consistency. Similarly, Going Beyond Nouns [6] uses the large-scale SyViC synthetic dataset to improve compositional understanding of attributes and relations in static images. Broader work in robotics demonstrates synthetic datas utility for manipulation [32, 39] and navigation [18, 61, 48]. Our work differs in three key aspects: (1) We focus on video-based spatiotemporal reasoning for question-answering, requiring temporal memory and perspective-taking across minutes-long trajectories, rather than static image understanding or generation tasks; (2) We systematically investigate which properties of simulated data (question types, data mixes, training scale) drive real-world transfer through controlled ablations; and (3) We demonstrate strong empirical results, with our 7B model surpassing larger baselines and achieving competitive performance with proprietary models while showing robust generalization to embodied and real-world scenarios. To enable these systematic investigations, we next describe our simulation-based data generation framework. 6. Discussion Future Work and Limitations. We focus our investigation on spatial capabilities in video-language models using the LLaVA family. Future work should explore whether our findings generalize to alternative recent architectures [22, 15, 10]. Additionally, as noted in our generalization results, our experiments focus on fine-tuning exclusively on SIMS data without mixing with general instruction data, which can lead to some catastrophic forgetting. Investigating optimal strategies for combining SIMS-V with broader multimodal training data, and training at larger data scale represent important avenues for maximizing spatial gains while preserving general capabilities. An exciting direction is co-designing simulated training data with model-specific processing characteristics. For instance, video-language models subsample frames during inference (e.g., 64 frames from 1800 in 3-min video at 10 fps); ensuring that critical spatial information in the training samples remains visible across all possible subsampled frame sets could further improve learning efficiency. The perfect ground truth of simulators enables such optimizatione.g., validating that each training example remains answerable under the target models specific frame sampling strategy. Conclusion. We propose SIMS-V, systematic framework for generating spatially-rich video training data from 3D simulators, and use it to investigate which properties of simulated data enable effective sim-to-real transfer. Through controlled ablations, we identify minimal effective question types and demonstrate remarkable data efficiency, with just 5K25K simulated examples enabling competitive performance with large proprietary models. Our approach shows robust generalization to embodied and real-world scenarios while maintaining general video understanding capabilities. We hope that our findings and released dataset pave the way for leveraging simulations as scalable data source to improve spatial reasoning in video language models. Acknowledgments We are grateful to the Ai2 Prior team for the excellent framework, codebase, and support that enabled this work. We thank Jihan Yang for helpful discussions and Shusheng Yang and Anjali Gupta for reviewing our manuscript. E.B. is supported by the DoD NDSEG Fellowship Program. S.X. acknowledges support from the MSIT IITP grant (RS-2024-00457882) and the NSF award IIS-2443404."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, 2020. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, and Saining Xie. Benchmark designers should train on the test set to expose exploitable non-visual shortcuts. arXiv preprint arXiv:2511.04655, 2025. [5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dat aset, 2022. [6] Paola Cascante-Bonilla, Khaled Shehada, James Seale Smith, Sivan Doveh, Donghyun Kim, Rameswar Panda, Gul Varol, Aude Oliva, Vicente Ordonez, Rogerio Feris, and Leonid Karlinsky. Going beyond nouns with vision & language models using synthetic data. In ICCV, 2023. [7] Agneet Chatterjee, Yiran Luo, Tejas Gokhale, Yezhou Yang, and Chitta Baral. Revision: Rendering tools enable spatial fidelity in vision-language models. In ECCV, 2024. [8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. [9] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, 2020. [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [12] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. NeurIPS, 2025. 12 [13] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [14] Pradipto Das, Chenliang Xu, Richard Doell, and Jason Corso. thousand frames in just few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR, 2013. [15] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: Universe of Annotated 3D Objects. In CVPR, 2023. [17] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. In NeurIPS, 2022. [18] Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, Ranjay Krishna, Dustin Schwenk, Eli VanderBilt, and Aniruddha Kembhavi. Spoc: Imitating shortest paths in simulation enables effective navigation and manipulation in the real world. In CVPR, 2024. [19] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, 2025. [20] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. [21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. [22] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [23] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, 13 James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the World in 3,000 Hours of Egocentric Video. In CVPR, 2022. [24] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. NeurIPS, 2023. [25] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. In NeurIPS, 2023. [26] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. [27] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [29] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In ICCV, 2021. [30] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Kumar Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv preprint arXiv:1712.05474, 2017. [31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [32] Toru Lin, Kartik Sachdev, Linxi Fan, Jitendra Malik, and Yuke Zhu. Sim-to-real reinforcement learning for vision-based dexterous manipulation on humanoids. arXiv preprint arXiv:2502.20396, 2025. [33] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. In TACL, 2023. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [35] Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, and Hongsheng Li. Stereogan: Bridging synthetic-to-real domain gap by joint optimization of domain translation and stereo matching. In CVPR, 2020. 14 [36] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? In CVPR, 2023. [37] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024. [38] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. NeurIPS, 2023. [39] Jan Matas, Stephen James, and Andrew Davison. Sim-to-real reinforcement learning for deformable object manipulation. In CoRL, 2018. [40] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In ICCV, 2019. [41] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In ECCV, 2022. [42] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [44] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? In ICLR, 2025. [45] Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael Ryoo, and TsungYu Lin. Learning to localize objects improves spatial reasoning in visual-llms. In CVPR, 2024. [46] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. In COLM, 2025. [47] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. [48] Sneha Silwal, Karmesh Yadav, Tingfan Wu, Jay Vakil, Arjun Majumdar, Sergio Arnaud, Claire Chen, Vincent-Pierre Berges, Dhruv Batra, Aravind Rajeswaran, et al. What do we learn from large-scale study of pre-trained visual representations in sim and real environments? In ICRA, 2024. 15 [49] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: foundational language and vision alignment model. In CVPR, 2022. [50] Yu-Chuan Su, Soravit Changpinyo, Xiangning Chen, Sathish Thoppay, Cho-Jui Hsieh, Lior Shapira, Radu Soricut, Hartwig Adam, Matthew Brown, Ming-Hsuan Yang, and Boqing Gong. 2.5d visual relationship detection. Computer Vision and Image Understanding, 2022. [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [52] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [53] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. IROS, 2017. [54] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 2025. [55] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine, Kate Saenko, and Trevor Darrell. Adapting deep visuomotor representations with weak pairwise constraints. Algorithmic Foundations of Robotics XII, 2020. [56] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. In NeurIPS, 2022. [57] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [58] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2 Technical Report. arXiv preprint arXiv:2412.15115, 2024. [59] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. In CVPR, 2024. [60] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Danhao Lu, Rob Fergus, Yann LeCun, Li FeiFei, and Saining Xie. Cambrian-S: Towards Spatial Supersensing in Video. arXiv preprint arXiv:2511.04670, 2025. [61] Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, and Luca Weihs. Poliformer: Scaling on-policy rl with transformers results in masterful navigators. In CoRL, 2025. 16 [62] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In CVPR, 2023. [63] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? In ICLR, 2025. [64] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. [65] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video Instruction Tuning with Synthetic Data. arXiv preprint arXiv:2410.02713, 2024. [66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In ICLR, 2024."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides comprehensive details about our experimental setup, dataset characteristics, and additional results, organized as follows: details model architectures and training configurations describes the SIMS-VSI dataset statistics, question templates, and prompt formats presents supplementary experimental analyses A. Implementation Details We provide comprehensive details about our model architecture, training setup, and evaluation methodology to ensure reproducibility. A.1. Model Architecture We conduct experiments using two video-language models with identical backbones but different video specializations: LLaVA-Video-7B [64] Vision encoder: SigLIP-SO400M-patch14-384 [62] Language model: Qwen2-7B [58] Video processing: Up to 64 frames per video with SlowFast-style temporal pooling Each frame encoded into 1212 grid of visual tokens LLaVA-OneVision-7B [31] Vision encoder: SigLIP-SO400M-patch14-384 [62] Language model: Qwen2-7B [58] Video processing: Up to 32 frames per video with per-frame token pooling Generalist model trained for single-image, multi-image, and video tasks For videos longer than the maximum frame count, we uniformly sample frames across the video duration. A.2. Training Configuration We fine-tune our models using the following hyperparameters: Learning rate: 1e-6 (main and vision tower) Optimizer: AdamW (via DeepSpeed) Weight decay: 0.0 Scheduler: Cosine with 3% warmup Global batch size: 32 Precision: Mixed precision (BF16) Gradient checkpointing: Enabled Maximum sequence length: 32,768 tokens 18 Hardware: 8 A100-80GB GPUs or 8 H100 GPUs For data scaling experiments, we maintain the same number of total training steps by adjusting the number of epochs for different dataset sizes (more epochs for smaller datasets). A.3. Data Processing For each simulated video, we: Sample frames at 10 FPS Resize frames to 384 384 pixels Apply standard normalization based on the vision encoders requirements B. SIMS-VSI Dataset Details As concrete instantiation of our framework, we release SIMS-VSI, comprising comprehensive spatial reasoning data across diverse simulated indoor environments. B.1. Dataset Statistics SIMS-VSI contains: 203,048 total question-answer pairs 2,507 unique video trajectories 1,261 unique indoor scenes 9 question types in both open-ended and multiple-choice formats Question Type Distribution. The dataset is balanced across question types and formats, with approximately 12.5k examples per question type per format (open-ended vs. multiple-choice), except for room size estimation which has approximately 2.5k examples per format due to its scene-level (rather than object-level) nature. See Tab. 4 for the complete breakdown. Table 4 Per-question-type breakdown of SIMS-VSI dataset. Question Type Open-Ended Multiple-Choice Absolute Distance Appearance Order Object Count Relative Direction (Easy) Relative Direction (Medium) Relative Direction (Hard) Relative Distance Object Size Estimation Room Size Estimation 12,510 12,477 12,503 12,250 12,247 12,230 12,295 12,503 2,509 12,510 12,477 12,503 12,250 12,247 12,230 12,295 12,503 2,509 Total 101,524 101, 19 Figure 6 Question type transfer patterns remain consistent on VSI-Bench-Debiased. Similar to Fig. 4, appearance order and absolute distance show strong cross-task transfer on the debiased benchmark, confirming that observed patterns reflect genuine spatial learning rather than exploitation of statistical shortcuts. B.2. Question Templates Each question type follows specific template that is populated with environment-specific details extracted from the simulators metadata. These templates create diverse, challenging tasks testing different aspects of spatial understanding, from metric estimation to perspective-taking and temporal tracking. B.3. Prompt Templates We use standardized prompts to format questions during training: C. Additional Results and Analysis This section provides supplementary experimental results that complement the main paper. C.1. Question Type Transfer on VSI-Bench-Debiased To verify that the question type transfer patterns observed in Fig. 4 (main paper) are not artifacts of statistical shortcuts, we repeat the analysis on VSI-Bench-Debiased. 20 Table 5 Question templates used in SIMS-V to generate diverse spatial reasoning challenges. Each template creates questions that test specific aspect of spatial understanding, from metric estimation to perspective-taking and temporal tracking. Question Type Template Object Counting How many {category}(s) are in this room? Object Size Estimation What is the length of the longest dimension (length, width, or height) of the {category}, measured in centimeters? Room Size Estimation What is the size of this room (in square meters)? If multiple rooms are shown, estimate the size of the combined space. Absolute Distance Relative Distance Relative Direction (Hard) Measuring from the closest point of each object, what is the direct distance between the {object1} and the {object2} (in meters)? Measuring from the closest point of each object, which of these objects ({choice_a}, {choice_b}, {choice_c}, {choice_d}) is the closest to the {category}? If there are multiple instances of an object category, measure to the closest. am standing by the {positioning_object} and facing the If {orienting_object}, frontright, back-left, or back-right? The directions refer to the quadrants of Cartesian plane (if am standing at the origin and facing along the positive y-axis). is the {querying_object} to my front-left, Relative Direction (Med) Relative Direction (Easy) am standing by the {positioning_object} and facing the If {orienting_object}, is the {querying_object} to my left, right, or back? An object is to my back if would have to turn at least 135 degrees in order to face it. am standing by the {positioning_object} and facing the If {orienting_object}, is the {querying_object} to the left or the right of the {orienting_object}? Spatiotemporal Distance Which of these objects ({choice_a}, {choice_b}, {choice_c}, {choice_d}) is the closest to the ego-position at the last frame in the video? Appearance Order Route Planning What will be the first-time appearance order of the following categories in the video: {choice_a}, {choice_b}, {choice_c}, {choice_d}? You are robot beginning at the {start_obj} and facing the {orienting_obj}. You want to navigate to the {end_obj}. You will perform the following actions (Note: for each [please fill in], choose either turn back, turn left, or turn right.): {actions} You have reached the final destination. Table 6 Prompting templates combined with question templates. Prompt Type Template Video Introduction (pre-prompt) Multiple-Choice (post-prompt) Numerical Answer (post-prompt) Please answer the question using single word or phrase. These are frames of video. Answer with the options letter from the given choices directly."
        }
    ],
    "affiliations": [
        "AllenAI",
        "Boston University",
        "New York University",
        "Vercept"
    ]
}