{
    "paper_title": "Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models",
    "authors": [
        "Shuhong Zheng",
        "Zhipeng Bao",
        "Ruoyu Zhao",
        "Martial Hebert",
        "Yu-Xiong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness."
        },
        {
            "title": "Start",
            "content": "DIFF-2-IN-1: BRIDGING GENERATION AND DENSE PERCEPTION WITH DIFFUSION MODELS Shuhong Zheng1 Zhipeng Bao2 Ruoyu Zhao3 Martial Hebert2 Yu-Xiong Wang1 1University of Illinois Urbana-Champaign {szheng36, yxw}@illinois.edu zhao-ry20@mails.tsinghua.edu.cn 2Carnegie Mellon University {zbao, hebert}@cs.cmu.edu 3Tsinghua University 4 2 0 2 ] . [ 1 5 0 0 5 0 . 1 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have emerged as powerful generative modeling tools for various high-fidelity image synthesis tasks (Song et al., 2021; Ho et al., 2020; Rombach et al., 2022; Zhang et al., 2023b). Beyond their primary synthesis capabilities, diffusion models are increasingly recognized for their expressive representation abilities. This has spurred interest in leveraging them for dense pixel-level visual perception tasks, such as semantic segmentation (Baranchuk et al., 2022; Wu et al., 2023; Xu et al., 2023a) and depth estimation (Saxena et al., 2023b; Zhao et al., 2023). Nonetheless, most existing approaches treat diffusion models as standalone component for perception tasks, either employing them for off-the-shelf data augmentation (Burg et al., 2023), or utilizing the diffusion network as feature extraction backbone (Xu et al., 2023a; Zhao et al., 2023; Ji et al., 2023; Saxena et al., 2023a). These efforts overlook the unique diffusion-denoising process inherent in diffusion models, thus limiting their potential for discriminative dense visual perception tasks. Inspired by foundational studies that explore the interplay between generative and discriminative learning (Rubinstein & Hastie, 1997; Ng & Jordan, 2001; Raina et al., 2003; Ulusoy & Bishop, 2005), we argue that the diffusion-denoising process plays critical role in unleashing the capability of diffusion models for the discriminative visual perception tasks. The diffusion process corrupts the visual input with noise, enabling the generation of abundant new data with diversity. Subsequently, the denoising process removes the noise from noisy images to create high-fidelity data, thus obtaining informative features for discriminative tasks at the same time. As result, the diffusiondenoising process naturally connects the generative process with discriminative learning. Interestingly, this synergy further motivates us to propose novel unified diffusion modeling framework that integrates both discriminative and generative learning within single, coherent paradigm. From the generative perspective, we focus on synthesizing photo-realistic multi-modal paired data (i.e., RGB images and their associated pixel-level visual attributes) that accurately capture various types of visual information. Simultaneously, the unified diffusion model can achieve promising 1 Figure 1: single, unified diffusion-based model for both generative and discriminative learning. If the model receives an RGB image as input, its function is to predict an accurate visual attribute map. Simultaneously, the model is equipped to produce photo-realistic and coherent multimodal data sampled from Gaussian noise. We use depth as an example here for illustration, and the framework is also applicable to other visual attributes such as segmentation, surface normal, etc. results in different visual prediction tasks from the discriminative standpoint. As an example illustrated in Figure 1, when considering RGB and depth interactions, if the model receives an RGB image as input, its function is to predict an accurate depth map. Meanwhile, the model is equipped to produce photo-realistic and coherent RGB-depth pairs sampled from noise. Despite its conceptual simplicity, fully operationalizing the unified framework acquiring enhanced performance for both multi-modal generation and dense perception such as by effectively leveraging generated samples for discriminative tasks presents non-trivial challenges. In particular, the generation process inevitably produces data of relatively inferior quality compared to real data. Additionally, generated samples may exhibit considerable data distribution gaps from the target domain. To address these challenges, we introduce Diff-2-in-1, diffusion framework bridging multi-modal generation and discriminative dense visual perception within one unified diffusion model. The core design within our Diff-2-in-1 is self-improving learning mechanism, featuring two sets of parameters for our unified diffusion model during the training process. Specifically, the creation parameters are tailored to generate additional multi-modal data for discriminative learning, while the exploitation parameters are employed for utilizing both the original and synthetic data to learn the discriminative dense visual perception task. Meanwhile, the creation parameters continuously undergo self-improvement based on the weights of the exploitation parameters via exponential moving average (EMA). With our novel design of two sets of parameters interplaying with each other, the discriminative learning process can benefit from the synthetic samples generated by the model itself, while the quality of the generated data is iteratively refined at the same time. We validate the effectiveness of Diff-2-in-1 through extensive and multi-faceted experimental evaluations. We start with the evaluation of the discriminative perspective, demonstrating its superiority over state-of-the-art discriminative baselines across various tasks in both single-task and multi-task settings. We additionally show that Diff-2-in-1 is generally applicable to different backbones and consistently boosts performance. Next, we ablate the experimental settings such as different training data sizes, to gain comprehensive understanding of our method. Finally, we demonstrate the realism and usefulness of the multi-modal data generated by our Diff-2-in-1. Our contributions include: (1) We propose Diff-2-in-1, unified framework that seamlessly integrates multi-modal generation and discriminative dense visual perception based on diffusion models. (2) We introduce novel self-improving mechanism that progressively enhances multi-modal generation in self-directed manner, thereby effectively boosting the discriminative visual perception performance via generative learning. (3) Our method demonstrates consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation under both realism and usefulness."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Pixel-level dense visual perception covers broad range of discriminative computer vision tasks including depth estimation (Godard et al., 2019; Eigen et al., 2014; Guo et al., 2018; Liu et al., 2015), segmentation (Long et al., 2015; Xie et al., 2021), surface normal prediction (Wang et al., 2015; Bae et al., 2021), keypoint detection (Cao et al., 2021; 2017; Zhou et al., 2018; Toshev & Szegedy, 2014), etc. After the convolutional neural network (CNN) (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Zeiler & Fergus, 2014; Szegedy et al., 2015) shows great success in ImageNet classification (Deng et al., 2009) even outperforming humans (He et al., 2016), adopting CNN for dense prediction tasks (Wang et al., 2015; Eigen & Fergus, 2015; Eigen et al., 2014) becomes prototype for model design. With Vision Transformer (ViT) (Dosovitskiy et al., 2021) later becoming revolutionary advance in architecture for vision models, an increasing number of visual perception models (Ranftl et al., 2022; 2021; Xie et al., 2021) start to adopt ViT as their backbones, benefiting from the scalability and global perception capability brought by ViT. Generative modeling for discriminative tasks. The primary objective of generative models has traditionally been synthesizing photo-realistic images. However, recent advancements have expanded their utility to the generation of useful images for downstream visual tasks (Zhan et al., 2018; Zhu et al., 2018; Aleotti et al., 2018; Pilzer et al., 2018; Zhang et al., 2023c; Zhu et al., 2024; Zheng et al., 2023b; Bao et al., 2022). This is typically accomplished by generating images and corresponding annotations off-the-shelf, subsequently using them for data augmentation in specific visual tasks. Nowadays, with the emergence of powerful diffusion models in high-fidelity synthesis tasks (Song et al., 2021; Ho et al., 2020; Rombach et al., 2022; Zhang et al., 2023b; Wang et al., 2022; Chen et al., 2023), there has been growing interest in applying them to discriminative tasks. Among them, ODISE (Xu et al., 2023a) and VPD (Zhao et al., 2023) extract features using the stable diffusion model (Rombach et al., 2022) to perform discriminative tasks such as segmentation and depth estimation. DIFT (Tang et al., 2023) and its concurrent work (Luo et al., 2023; Zhang et al., 2023a; Hedlin et al., 2023) utilize diffusion features for identifying semantic correspondence. DDVM (Saxena et al., 2023a) solves depth and optical flow estimation tasks by denoising from Gaussian noise with RGB images as condition. Diffusion Classifier (Li et al., 2023a) utilizes diffusion models to enhance the confidence of zero-shot image classification. Other studies (Trabucco et al., 2024; Feng et al., 2023; Burg et al., 2023) have explored using diffusion models to augment training data for image classification. Different from them, we propose unified diffusion-based model that can directly work for discriminative dense visual perception tasks, and simultaneously utilize its generative process to facilitate discriminative learning through the proposed novel self-improving algorithm."
        },
        {
            "title": "3 UNIFIED DIFFUSION MODEL: DIFF-2-IN-1",
            "content": "3.1 PRELIMINARY: LATENT DIFFUSION MODELS Diffusion models (Ho et al., 2020) are latent variable models that learn the data distribution with the inverse of Markov noising process. Instead of leveraging the diffusion models in the RGB color space (Song et al., 2021; Ho et al., 2020), we build our method upon the state-of-the-art latent diffusion model (LDM) (Rombach et al., 2022). First, an encoder is trained to map an input image into spatial latent code = E(x). decoder is then tasked with reconstructing the input image such that D(E(x)) x. Considering the clean latent z0 q(z0), where q(z0) is the posterior distribution of z0, LDM gradually adds Gaussian noise to z0 in the diffusion process: q(ztzt1) = (zt; (cid:112)1 βtzt1, βtI), where βt is variance schedule that controls the strength of the noise added in each timestep. We can derive closed-form process from Equation 1 to convert clean latent z0 to noisy latent zT of arbitrary timestep : (1) zT q(zT z0) = (zT ; αT z0, (1 αT )I), (2) where the notation αT = 1βT and αT = (cid:81)T zT is nearly equivalent to sampling from an isotropic Gaussian distribution. s=1 αs makes the formulation concise. When , Figure 2: Our self-improving learning paradigm with two sets of interplayed parameters during training. The data creation parameter θC generates samples serving as additional training data for the data exploitation parameter θE, while θE performs discriminative learning and provides guidance to update θC through exponential moving average. Finally, θC performs both discriminative and generative tasks during inference. The denoising process takes inverse operations from the diffusion process. We estimate the denoised latent at timestep 1 from by: pθ(zt1zt) = (zt1; µθ(zt, t), Σθ(zt, t)), (3) where the parameters µθ(zt, t), Σθ(zt, t) of the Gaussian distribution are estimated from the model. As revealed by Ho et al. (2020), Σθ(zt, t) has few effects on the results experimentally, therefore estimating µθ(zt, t) becomes the main objective. reparameterization is introduced to estimate it: µθ(zt, t) = (cid:18) zt 1 αt βt 1 αt (cid:19) ϵθ(zt, t) , (4) where ϵθ(zt, t) is denoising network to predict the additive noise ϵ for zt at timestep t. The final objective is: LLDM := EE(x),ϵN (0,1),t (cid:104) ϵ ϵθ (zt, t)2 2 (cid:105) . (5) 3.2 UNIFIED MODEL BEYOND RGB GENERATION In this section, we use diffusion-based models for both discriminative and generative tasks to form our Diff-2-in-1 framework. Concretely, for diffusion-based unified model Φ, we want it to predict task label ˆy = Φdis(x) given input image x; meanwhile, after training, it can generate multi-modal paired data from Gaussian: ( x, y) = Φgen(ϵ). We describe how we achieve this below. Discriminative perspective. Previous work (Xu et al., 2023a; Zhao et al., 2023) has demonstrated the possibility of using diffusion models for perceptual tasks. Following VPD (Zhao et al., 2023), with the latent code = E(x) from given image x, we perform one-step denoising on through the denoising U-Net (Ronneberger et al., 2015) to produce multi-scale features. Afterward, we rescale and concatenate those features and further pass them to task head for downstream prediction. Generative perspective. To generate multi-modal data consisting of paired RGB and visual attributes, we first produce latent vector z0 by denoising from Gaussian with conditional text. Next, 4 Figure 3: Real data samples from NYUv2 and synthesized samples generated from Gaussian noise. The distribution of the generated data varies from the real data distribution. Figure 4: In-distribution data generation using partial noise. We generate in-distribution data by denoising from noisy image at timestep with 0 < < Tmax. larger leads to greater diversity, whereas smaller enhances the resemblance to the original distribution. we directly generate the color image by passing it to the LDM decoder; meanwhile, we perform another one-step denoising with z0 and send the resulting multi-scale features to the task head to obtain the corresponding label y. The two perspectives reflect different usages of the unified diffusion model while they are not fully separated: performing generation can be treated as process of denoise-and-predict for noisy image at timestep = ; while predicting labels can be treated as process of data generation conditioned on given latent vector z0. This special connection motivates the design of our Diff-2in-1."
        },
        {
            "title": "4 LEARNING MECHANISM OF DIFF-2-IN-1",
            "content": "To effectively leverage the generated multi-modal data for dense visual perception, we propose self-improving mechanism for our Diff-2-in-1 framework to make the discriminative and generative processes interact with each other, as shown in Figure 2. The details are described as below. 4.1 WARM-UP STAGE Since pretrained diffusion models are only designed for RGB generation, we need warm-up stage to activate the task head in Figure 2 for additional tasks. To achieve this, we train our unified diffusion model using its discriminative learning pipeline with all the original training data with loss = (cid:88) i= Lsup(fθW(xi), yi), (6) where Lsup is the supervised loss for our chosen discriminative task on the original paired training data Dtrain = {xi, yi}N i=1. We obtain set of parameter weights θW after this warm-up stage. 4.2 DATA GENERATION Many approaches (Feng et al., 2023; Burg et al., 2023) that use diffusion models for data augmentation generate data from Gaussian noise as discussed in Section 3.2. However, as shown in Figure 3, the synthetic samples generated from Gaussian noise have non-negligible distribution shift from the original training data, posing huge obstacles to utilizing the generated data for boosting the discriminative task performance. To narrow down the domain gap between the generated data and 5 Model SkipNet (Bansal et al., 2016) GeoNet (Qi et al., 2018) PAP (Zhang et al., 2019) GeoNet++ (Qi et al., 2022) Bae et al. (2021) Bae et al. (2021) GNA on Bae et al. DA-Fusion (Trabucco et al., 2024) on Bae et al. Diff-2-in-1 on Bae et al. (Ours) iDisc (Piccinelli et al., 2023) iDisc (Piccinelli et al., 2023) GNA on iDisc DA-Fusion (Trabucco et al., 2024) on iDisc Diff-2-in-1 on iDisc (Ours) Training Samples 795 30,816 12,795 30,816 30,816 795 795 795 795 30,816 795 795 795 795 11.25 () 47.9 48.4 48.8 50.2 62.2 56.6 56.4 58.1 67.4 63.8 57.3 56.9 58.7 68.7 22.5 () 70.0 71.5 72.2 73.2 79.3 76.8 76.7 77.5 83.4 79.8 76.4 76.2 78.3 83.7 30 () Mean () Median () RMSE () 77.8 79.5 79.8 80.7 85.2 83.0 83.0 83.6 88.2 85.6 82.9 82.4 83.4 88. 28.2 26.9 25.5 26.7 23.5 26.6 26.7 26.1 22.0 22.8 26.4 26.7 26.2 21.6 19.8 19.0 18.6 18.5 14.9 17.2 17.3 16.8 13.2 14.6 17.8 18.1 17.3 12.7 12.0 11.8 11.7 11.2 7.5 9.3 9.3 8.9 6.5 7.3 8.8 8.9 8.6 6.0 Table 1: Surface normal evaluation on NYUv2 (Silberman et al., 2012; Ladicky et al., 2014). When applying our Diff-2-in-1 on top of state-of-the-art baselines, we achieve consistently and significantly better performance with notably fewer training data, demonstrating the advantages of data efficiency from our unified diffusion model. Additionally, Diff-2-in-1 outperforms augmentation methods GNA and DA-Fusion, proving the usefulness of the multi-modal data generated by our pipeline, and the effectiveness of our self-improving mechanism in utilizing synthetic data. Model Swin-L (Liu et al., 2021b) ConvNeXt-L (Liu et al., 2022) ConvNeXt-XL (Liu et al., 2022) MAE-ViT-L/16 (He et al., 2022) CLIP-ViT-B (Rao et al., 2022) VPD (Zhao et al., 2023) DA-Fusion (Trabucco et al., 2024) on VPD Diff-2-in-1 on VPD (Ours) mIoU () 52.1 53.2 53.6 53.6 50.6 53.7 54.0 54. Table 2: Comparison with diffusion-based segmentation method VPD (Zhao et al., 2023). The other baselines follow the setting of VPD, which utilize features from supervised pretraining (Liu et al., 2021b; 2022), self-supervised pretraining (He et al., 2022), and visual-language pretraining (Rao et al., 2022) combined with learnable segmentation head (Xiao et al., 2018). Our proposed Diff-2in-1 further improves the performance of the diffusion-based VPD model. original data, inspired by SDEdit (Meng et al., 2022) and DA-Fusion (Trabucco et al., 2024), we use the inherent diffusion-denoising mechanism to control the data generation process. Concretely, we add noise to the latent zi of an image xi from the training set using Equation 2 at timestep satisfying 0 < < Tmax, where Tmax is the maximum timestep in the training process of diffusion models (Tmax = 1000 for all our experiments). This process partially corrupts the image with noise, yet maintains degree of the original content, as depicted in the first row of Figure 4. After denoising the noisy image with Equation 3 and decoding with the variational autoencoder, we obtain the synthetic image xi with different content but relatively small domain gap, as shown in the second row of Figure 4. At the same time, we can obtain the prediction yi which is decoded from the task head of the unified diffusion model. As shown in the third row of Figure 4, the generated annotations (surface normal as an example) well match the generated RGB images. The timestep , representing the noise level, acts as modulator, balancing the diversity of the generated samples and the fidelity to the in-distribution data: higher noise levels lead to greater diversity, whereas lower levels enhance the resemblance to the original distribution. 4.3 SELF-IMPROVING STAGE To more effectively utilize the generated multi-modal data, we propose self-improving mechanism inspired by the mean teacher learning system (Tarvainen & Valpola, 2017). As shown in Figure 2, our self-improving mechanism introduces the following two sets of parameters, both are initialized with θW, to iteratively perform the self-improvement for both generative and discriminative learning. Data creation network (θC) is used to create samples through the generative process within our unified diffusion model. During every iteration, for batch of real paired data {(xi, yi)}m i=1, we additionally generate paired samples {( xi, yi)}n i=1 with θC following the data creation scheme described in Section 4.2. Both real and synthetic data are used for data exploitation. 6 Model Cross-stitch (Misra et al., 2016) PAP (Zhang et al., 2019) PSD (Zhou et al., 2020) PAD-Net (Xu et al., 2018) NDDR-CNN (Gao et al., 2019) MTI-Net (Vandenhende et al., 2020) ATRC (Bruggemann et al., 2021) DeMT (Xu et al., 2023c) MQTransformer (Xu et al., 2023b) DeMT (Xu et al., 2023c) InvPT (Ye & Xu, 2022) DA-Fusion (Trabucco et al., 2024) on InvPT Diff-2-in-1 on InvPT (Ours) TaskPrompter (Ye & Xu, 2023) DA-Fusion (Trabucco et al., 2024) on TaskPrompter Diff-2-in-1 on TaskPrompter (Ours) Normal Depth Semseg mIoU () RMSE () mErr () 0.6290 0.6178 0.6246 0.6270 0.6288 0.5365 0.5363 0.5474 0.5785 0.5474 0.5183 0.5167 0.5015 0.5152 0.5065 0.5041 20.88 20.82 20.87 20.85 20.89 20.27 20.18 20.02 20.81 20.02 19.04 18.81 18.60 18.47 18.15 17.91 36.34 36.72 36.69 36.61 36.72 45.97 46.33 51.50 49.18 51.50 53.56 53.70 54.71 55.30 55.13 55. Table 3: Comparison with state-of-the-art methods on the multi-task NYUD-MT (Silberman et al., 2012) benchmark. Our Diff-2-in-1 brings additional performance gain to the state-of-the-arts. Model Semseg Saliency Normal mIoU () mIoU () maxF () mErr () Parsing ASTMT (Maninis et al., 2019) PAD-Net (Xu et al., 2018) MTI-Net (Vandenhende et al., 2020) ATRC-ASPP (Bruggemann et al., 2021) ATRC-BMTAS (Bruggemann et al., 2021) MQTransformer (Xu et al., 2023b) DeMT (Xu et al., 2023c) InvPT (Ye & Xu, 2022) DA-Fusion (Trabucco et al., 2024) on InvPT Diff-2-in-1 on InvPT (Ours) TaskPrompter (Ye & Xu, 2023) DA-Fusion (Trabucco et al., 2024) on TaskPrompter Diff-2-in-1 on TaskPrompter (Ours) 68.00 53.60 61.70 63.60 67.67 71.25 75.33 79.03 79.33 80.36 80.89 80.81 80. 61.10 59.60 60.18 60.23 62.93 60.11 63.11 67.61 68.45 69.55 68.89 69.23 69.73 65.70 65.80 84.78 83.91 82.29 84.05 83.42 84.81 84.45 84.64 84.83 84.47 84.35 14.70 15.30 14.73 14.30 14.24 14.74 14.54 14.15 14.04 13.89 13.72 13.70 13.64 Table 4: Comparison on the multi-task PASCAL-Context (Mottaghi et al., 2014) benchmark. Equipped with our Diff-2-in-1, the state-of-the-art methods reach an overall better performance. Data exploitation network (θE) is used for exploring the parameter space by exploiting both the original and the synthetic data samples to learn the discriminative task. With those + samples, θE is updated via the discriminative loss: = (cid:88) i=1 Lsup(fθE (xi), yi) + (cid:88) i=1 Lsyn(fθE ( xi), yi), (7) where Lsyn is the loss term for synthetic data for which we regard the generated annotation yi as the ground truth. It has the same format as the supervised loss Lsup. Feedback from data exploitation: EMA optimization. The additional generated data from θC naturally facilitate the training of θE. In response, we apply feedback from θE to θC to update its weights every few iterations via the exponential moving average (EMA) strategy: θC αθC + (1 α)θE, (8) where α [0, 1) is momentum hyperparameter that is usually set to close to 1. As discussed in Section 3.2, the data generation process essentially follows denoise-and-predict manner, so Equation 8 ensures θC to have better task prediction head thereby producing higher-quality multimodal data. large α maintains the overall quality of the generated data, preventing θC from getting distracted by the inevitable inferior data. After the self-improvement, only one set of parameter, θC, is used to perform both generative and discriminative tasks during inference. The capability of this final model, regarding both discriminative learning and multi-modal data generation, gets promoted simultaneously. 7 Model Diff-2-in-1 on Bae et al. (2021) Diff-2-in-1 on iDisc (Piccinelli et al., 2023) 300 600 800 300 600 11.25 () 67.2 67.4 67.3 68.6 68.7 68.5 22.5 () 83.3 83.4 83.3 83.6 83.7 83.6 30 () Mean () Median () RMSE () 88.1 88.2 88.1 88.4 88.4 88.3 13.3 13.2 13.3 12.8 12.7 12.8 22.1 22.0 22.1 21.6 21.6 21.6 6.6 6.5 6.6 6.0 6.0 6. Table 5: Ablation study on different timesteps during the data generation process within Diff-2in-1. medium timestep = 600 achieves the best performance, but overall Diff-2-in-1 is robust to different choices of . Model Bae et al. (2021) Diff-2-in-1 on Bae et al. (2021)(Ours) Source Target ScanNet NYUv2 NYUv2 NYUv2 ScanNet NYUv2 11.25 () 59.0 62.2 63. 22.5 () 77.5 79.3 80.4 30 () Mean () Median () RMSE () 83.7 85.2 86.0 16.0 14.9 14.6 24.7 23.5 23.3 8.4 7.5 7.3 Table 6: Cross-domain evaluation on the surface normal estimation task of NYUv2 (Silberman et al., 2012; Ladicky et al., 2014). The performance of our method trained on ScanNet even outperforms the baseline Bae et al. trained on NYUv2, suggesting our generalizability to unseen datasets."
        },
        {
            "title": "5 EXPERIMENTAL EVALUATION",
            "content": "5.1 EVALUATION SETUP We first evaluate our proposed Diff-2-in-1 in the single-task settings with surface normal estimation and semantic segmentation as targets. Next, we apply Diff-2-in-1 in multi-task settings of NYUDMT (Silberman et al., 2012) and PASCAL-Context (Mottaghi et al., 2014) to show that it can provide universal benefit for more tasks simultaneously. Datasets and metrics. We evaluate surface normal estimation on the NYUv2 (Silberman et al., 2012; Ladicky et al., 2014) dataset. Different from previous methods that leverage additional raw data for training, we only use the 795 training samples. We include the number of training samples for each method in Table 1 for reference. Following Bae et al. (2021) and iDisc (Piccinelli et al., 2023), we adopt 11.25, 22.5, 30 to measure the percentage of pixels with lower angle error than the corresponding thresholds. We also report the mean/median angle error and the root mean square error (RMSE) of all pixels. We evaluate semantic segmentation on the ADE20K (Zhou et al., 2017) dataset and use mean Intersection-over-Union (mIoU) as the metric. For multi-task evaluations, NYUD-MT spans across three tasks including semantic segmentation, monocular depth estimation, and surface normal estimation; PASCAL-Context takes semantic segmentation, human parsing, saliency detection, and surface normal estimation for evaluation. We adopt mIoU for semantic segmentation and human parsing, RMSE for monocular depth estimation, maximal F-measure (maxF) for saliency detection, and mean error (mErr) for surface normal estimation, following the same standard evaluation schemes (Misra et al., 2016; Zhang et al., 2019; Zhou et al., 2020; Xu et al., 2018; Gao et al., 2019; Vandenhende et al., 2020; Bruggemann et al., 2021; Xu et al., 2023b; Maninis et al., 2019; Xu et al., 2023c; Ye & Xu, 2022; 2023). Key implementation details. To speed up training, instead of creating the paired data on the fly which takes significantly longer time due to denoising, we pre-synthesize certain number of RGB images and later use θC to produce corresponding labels during the self-improving stage. More details about datasets, baselines, and implementations are included in Section in the appendix. 5.2 DOWNSTREAM TASK EVALUATION Surface normal estimation. We build our Diff-2-in-1 on two state-of-the-art surface normal prediction frameworks: Bae et al. (2021) and iDisc (Piccinelli et al., 2023). Our Diff-2-in-1 creates 500 synthetic pairs with timestep = 600 (refer to Section 4.2). Besides conventional methods, we include two additional baselines with diffusion-based data augmentation. DA-Fusion (Trabucco et al., 2024) generates in-distribution RGB images with labels sharing similar spirit as us, but only focuses on improving image classification task. To adapt it for dense pixel prediction, we adopt an off-the-shelf captioning strategy (Li et al., 2023b) to replace its textual inversion and apply the pretrained instantiated model to get the pixelwise annotations for the generated images. Afterward, the generated RGB-annotation pairs are utilized in the same way as DA-Fusion originally uses RGB-class pairs to boost the performance. Gaussian Noise Augmentation (GNA) is self8 Figure 5: Ablation study on different data settings with our Diff-2-in-1. Green line: Performance of the baseline VPD. Yellow line: Performance with our Diff-2-in-1. Gray bars: Improvement in each data setting. Our Diff-2-in-1 could consistently bring performance gain for all different data settings with more benefits in mid-range data settings. Figure 6: Multi-modal samples generated by Diff-2-in-1 on NYUDMT (Silberman et al., 2012). Our method can generate high-quality RGB images and precise multimodal annotations, further facilitating discriminative learning via our self-improvement. constructed baseline that generates additional data by denoising from Gaussian noise, then applies the self-improving strategy to utilize the generated data. With the results shown in Table 1, we observe: (1) When applying our Diff-2-in-1 on top of the state-of-the-art baselines, we achieve significantly better performance with notably fewer training (2) data, demonstrating the great advantages of data efficiency from unified diffusion model. Our Diff-2-in-1 has better performance than other augmentation methods like GNA and DA-Fusion, showcasing the usefulness of the multi-modal data generated by our pipeline, and the effectiveness of synthetic data utilization with our self-improving mechanism. (3) Our Diff-2-in-1 is general design that can universally bring benefits to different discriminative backbones. Semantic segmentation. We instantiate our Diff-2-in-1 on VPD (Zhao et al., 2023), diffusionbased segmentation model. For self-improving, we synthesize one sample for each image in the training set. With the results shown in Table 2, we observe that the diffusion-based VPD can benefit from our paradigm by effectively performing self-improvement to leverage the generated samples. Multi-task evaluations. We apply our Diff-2-in-1 on two state-of-the-art multi-task methods, InvPT (Ye & Xu, 2022) and TaskPrompter (Ye & Xu, 2023). total of 500 synthetic samples are generated for NYUD-MT following the surface normal evaluation. For PASCAL-Context, one sample is synthesized for each image in the training set with our Diff-2-in-1. The comparisons on NYUD-MT and PASCAL-Context are shown in Table 3 and Table 4, respectively. The results validate that our Diff-2-in-1 is versatile design that can elevate the performance of wide variety of vision tasks. 5.3 ABLATION STUDY In this section, we offer better understanding of the superiority of our Diff-2-in-1 by answering the three primary questions. More ablations are included in Section in the appendix. How does timestep in data creation affect final performance? As illustrated in Figure 4, the timestep balances the trade-off between the content variation and domain shift of the generated data. We ablate different timesteps {300, 600, 800} in the experiments on surface normal instantiated on Bae et al. (2021) and iDisc (Piccinelli et al., 2023). The results in Table 5 indicate that we achieve the best performance when = 600, with balance of data diversity and quality. Nevertheless, it is noteworthy that our performance is generally robust to different choices of . How robust is Diff-2-in-1 for domain shift? We perform the cross-domain evaluation to show that our Diff-2-in-1 has strong generalizability. We train both the baseline Bae et al. (2021) and our Diff-2-in-1 on the ScanNet (Dai et al., 2017) dataset for the surface normal estimation task, and evaluate the performance on the test set of NYUv2 (Silberman et al., 2012; Ladicky et al., Setting GT Only GT + Syn (Before Self-improving) GT + Syn (After Self-improving) 11.25 () 56.6 57.5 57.8 22.5 () 76.8 77.1 77.1 30 () Mean () Median () RMSE () 83.0 83.3 83.3 26.6 26.5 26.5 17.2 17.1 17. 9.3 9.1 9.0 Table 7: Comparison between two data settings. GT Only: Use real samples to train Bae et al. (2021) until converges. GT + Syn: Further finetune the converged model with real and synthetic samples. Synthetic data further boost the performance of converged model, demonstrating their realism. Backbone Bae et al. (2021) iDisc (Piccinelli et al., 2023) Setting Synthetic Real Synthetic Real 11.25 () 67.4 67.5 68.7 68.7 22.5 () 83.4 83.5 83.7 83.7 30 () Mean () Median () RMSE () 88.2 88.2 88.4 88.4 22.0 22.0 21.6 21.5 13.2 13.2 12.7 12.8 6.5 6.5 6.0 6. Table 8: Comparison between using generated samples and unlabeled real images in NYUv2 surface normal estimation. Comparable performance proves the premium quality of our generated data. 2014). Interestingly, with the results shown in Table 6, we find that the performance of our method trained on ScanNet even outperforms the baseline Bae et al. trained on NYUv2, suggesting the generalizability of our method to unseen datasets and its great potential in real practice. How Diff-2-in-1 is helpful in different data settings? We ablate different settings when the number of available training samples for Diff-2-in-1 varies to investigate whether it is more helpful in data abundance or data shortage scenarios. We run this ablation for semantic segmentation on the ADE20K dataset: we randomly select 10% (2K) to 90% (18K) samples with 10% (2K) intervals in between, assuming that Diff-2-in-1 only gets access to partial data. In each setting, one additional sample for each image is generated using our data generation scheme. With the results shown in Figure 5, we offer the following observations: (1) Diff-2-in-1 consistently boosts the performance under all settings, with improvement ranging from 0.8 to 1.4 in mIoU, indicating the effectiveness and robustness of our method. (2) Diff-2-in-1 provides more benefits in the data settings from 40% (8K) to 70% (14K). We analyze the reasons including that when the data are scarce, it is relatively hard to train good model via Equation 6 to provide high-quality multimodal synthetic data for self-improvement. On the other hand, when the data are already adequate, there is less demand for more diverse data. Under both scenarios, the benefit of our method is still noticeable yet less significant. 5.4 SYNTHETIC DATA EVALUATION In addition to Figure 4, we visualize samples generated by our method on NYUD-MT (Silberman et al., 2012) in Figure 6. Diff-2-in-1 is able to generate high-quality RGB images and precise multi-modal annotations, further facilitating discriminative learning via our self-improvement. More qualitative visualizations can be found in Section in the appendix. Below, we additionally examine the realism and usefulness of the generated data. Generated samples serving as data augmentation. We select surface normal estimation as the target task and train an external discriminative model, Bae et al. (2021), under the following two settings: (1) only use the original 795 samples to train the model until convergence (GT Only); and (2) finetune the converged model in GT Only using the mixture of original samples and generated samples from our Diff-2-in-1 before the self-improving stage (GT + Syn). For (2), we generate 500 synthetic samples with = 600 and naively merge them together with the original samples. We report two variants of setting (2) with generated samples before or after the self-improving stage in Table 7. We have the following observations: firstly, the synthetic samples are capable of boosting the performance of converged model, indicating that the generated RGB and annotation maps are consistent. Moreover, the generated multi-modal data get refined during the self-improving stage, verifying the effectiveness of our method towards generation. Synthetic data V.S. real data. In the surface normal task, we replace the 500 generated samples with 500 additional real captured images from NYUv2 raw video clips. The annotations of them are produced by our Diff-2-in-1 on the fly. Then, we use the same training strategy to train Diff-2-in-1. As shown in Table 8, using our generated data achieves comparable performance to using the real captured data, proving the premium quality of the synthetic data."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we bridge generative and discriminative learning by proposing unified diffusionbased framework Diff-2-in-1. It enhances discriminative learning through the generative process by creating diverse while faithful data, and gets the discriminative and generative processes to interplay with each other using self-improving learning mechanism. Extensive experiments demonstrate its superiority in various settings of discriminative tasks, and its ability to generate high-quality multimodal data characterized by both realism and usefulness. More discussions about limitations and future work can be found in Section in the appendix."
        },
        {
            "title": "REFERENCES",
            "content": "Filippo Aleotti, Fabio Tosi, Matteo Poggi, and Stefano Mattoccia. Generative adversarial networks for unsupervised monocular depth prediction. In ECCV Workshops, 2018. Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In ICCV, 2021. Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr Revisited: 2D-3D model alignment via surface normal prediction. In CVPR, 2016. Zhipeng Bao, Martial Hebert, and Yu-Xiong Wang. Generative modeling for multi-task visual learning. In ICML, 2022. Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Labelefficient semantic segmentation with diffusion models. In ICLR, 2022. David Bruggemann, Menelaos Kanakis, Anton Obukhov, Stamatios Georgoulis, and Luc Van Gool. Exploring relational context for multi-task dense prediction. In ICCV, 2021. Max F. Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. Image retrieval outperforms diffusion models on data augmentation. TMLR, 2023. Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2D pose estimation using part affinity fields. In CVPR, 2017. Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. OpenPose: Realtime multi-person 2D pose estimation using part affinity fields. TPAMI, 2021. Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2Tex: Text-driven texture synthesis via diffusion models. In ICCV, 2023. Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with common multi-scale convolutional architecture. In ICCV, 2015. David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. In NeurIPS, 2014. Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In ICCV, 2023. 11 Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L. Yuille. NDDR-CNN: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction. In CVPR, 2019. Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into selfsupervised monocular depth estimation. In ICCV, 2019. Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning monocular depth by distilling cross-domain stereo networks. In ECCV, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. In NeurIPS, 2023. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. DDP: Diffusion model for dense visual prediction. In ICCV, 2023. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In NeurIPS, 2012. Lubor Ladicky, Bernhard Zeisl, and Marc Pollefeys. Discriminatively trained dense surface normal estimation. In ECCV, 2014. Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In ICCV, 2023a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023b. Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Open-vocabulary object segmentation with diffusion models. In ICCV, 2023c. Fayao Liu, Chunhua Shen, and Guosheng Lin. Deep convolutional neural fields for depth estimation from single image. In CVPR, 2015. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. InstaFlow: One step is enough for high-quality diffusion-based text-to-image generation. In ICLR, 2024. Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. In ICLR, 2021a. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021b. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. ConvNet for the 2020s. In CVPR, 2022. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 12 Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. In NeurIPS, 2023. Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multiple tasks. In CVPR, 2019. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In CVPR, 2016. Ron Mokady, Amir Hertz, and Amit H. Bermano. ClipCap: CLIP prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan L. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. Andrew Ng and Michael Jordan. On discriminative vs. generative classifiers: comparison of logistic regression and naive Bayes. In NeurIPS, 2001. Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015. Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. iDisc: Internal discretization for monocular depth estimation. In CVPR, 2023. Andrea Pilzer, Dan Xu, Mihai Puscas, Elisa Ricci, and Nicu Sebe. Unsupervised adversarial depth estimation using cycled generative networks. In 3DV, 2018. Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. GeoNet: Geometric neural network for joint depth and surface normal estimation. In CVPR, 2018. Xiaojuan Qi, Zhengzhe Liu, Renjie Liao, Philip H.S. Torr, Raquel Urtasun, and Jiaya Jia. GeoNet++: Iterative geometric neural network with edge-aware refinement for joint depth and surface normal estimation. TPAMI, 2022. Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew McCallum. Classification with hybrid generative/discriminative models. In NeurIPS, 2003. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2022. Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. DenseCLIP: Language-guided dense prediction with context-aware prompting. In CVPR, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. Y. Dan Rubinstein and Trevor Hastie. Discriminative vs. informative learning. In KDD, 1997. Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David J. Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. In NeurIPS, 2023a. Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J. Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023b. 13 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from RGBD images. In ECCV, 2012. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. In NeurIPS, 2023. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. Alexander Toshev and Christian Szegedy. DeepPose: Human pose estimation via deep neural networks. In CVPR, 2014. Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. In ICLR, 2024. Ilkay Ulusoy and Christopher M. Bishop. Generative versus discriminative methods for object recognition. In CVPR, 2005. Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. MTI-Net: Multi-scale task interaction networks for multi-task learning. In ECCV, 2020. Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Semantic image synthesis via diffusion models. arXiv preprint arXiv:2207.00050, 2022. Xiaolong Wang, David Fouhey, and Abhinav Gupta. Designing deep networks for surface normal estimation. In CVPR, 2015. Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. DiffuMask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In ICCV, 2023. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021. Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. PAD-Net: Multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing. In CVPR, 2018. Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. ODISE: Open-vocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, 2023a. Yangyang Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, and Lefei Zhang. Multi-task learning with multi-query transformer for dense prediction. TCSVT, 2023b. Yangyang Xu, Yibo Yang, and Lefei Zhang. DeMT: Deformable mixer transformer for multi-task learning of dense prediction. In AAAI, 2023c. Hanrong Ye and Dan Xu. InvPT: Inverted pyramid multi-task transformer for dense scene understanding. In ECCV, 2022. Hanrong Ye and Dan Xu. TaskPrompter: Spatial-channel multi-task prompting for dense scene understanding. In ICLR, 2023. 14 Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014. Fangneng Zhan, Shijian Lu, and Chuhui Xue. Verisimilar image synthesis for accurate detection and recognition of texts in scenes. In ECCV, 2018. Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements DINO for zero-shot semantic correspondence. In NeurIPS, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023b. Mingtong Zhang, Shuhong Zheng, Zhipeng Bao, Martial Hebert, and Yu-Xiong Wang. Beyond RGB: Scene-property synthesis with neural radiance fields. In WACV, 2023c. Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang. Pattern-affinitive propagation across depth, surface normal and semantic segmentation. In CVPR, 2019. Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In ICCV, 2023. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In ICML, 2023a. Shuhong Zheng, Zhipeng Bao, Martial Hebert, and Yu-Xiong Wang. Multi-task view synthesis with neural radiance fields. In ICCV, 2023b. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In CVPR, 2017. Ling Zhou, Zhen Cui, Chunyan Xu, Zhenyu Zhang, Chaoqun Wang, Tong Zhang, and Jian Yang. Pattern-structure diffusion for multi-task learning. In CVPR, 2020. Xingyi Zhou, Arjun Karpur, Linjie Luo, and Qixing Huang. StarMap for category-agnostic keypoint and viewpoint estimation. In ECCV, 2018. Xinyue Zhu, Yifan Liu, Jiahong Li, Tao Wan, and Zengchang Qin. Emotion classification with data augmentation using generative adversarial networks. In PAKDD, 2018. Zhen Zhu, Yijun Li, Weijie Lyu, Krishna Kumar Singh, Zhixin Shu, Soeren Pirk, and Derek Hoiem. Consistent multimodal generation via unified GAN framework. In WACV, 2024. 15 In the appendix, we first include additional implementation details in Section A. Then, in Section B, we perform additional ablations on different implementation choices of text prompters, timesteps to perform discriminative learning with Diff-2-in-1, etc., to provide more informative guidelines about how to apply our Diff-2-in-1 on discriminative tasks. Afterwards, we provide additional qualitative results in Section C, including comparisons of the performance on discriminative tasks and the multimodal generation quality of our proposed Diff-2-in-1. Finally, we present dicussions of limitations and future work in Section D."
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 ARCHITECTURE DETAILS Feature extraction from diffusion models. We first describe how we extract features for downstream dense prediction tasks from the pretrained stable diffusion model (Rombach et al., 2022) in our framework, which is generally applicable to all the model instantiations discussed below. We take the latent vector obtained from the VAE encoder in stable diffusion as input for the denoising network, followed by one-step denoising to obtain the features. Since the denoising operation in stable diffusion is realized by U-Net (Ronneberger et al., 2015) module, multi-scale features can be obtained through the one-step denoising process for given image. As we use the publicly released stable diffusion pretrained weight Stable Diffusion v1-5 which is finetuned on 512 512 resolution, the input images are also resized to 512 512 before being processed by our model. Therefore, the raw multi-scale features {f raw i=0 extracted from our model are in the spatial resolutions of 8 8, 16 16, 32 32, and 64 64. Following Li et al. (2023c), for each pair of features raw i1, raw (1 3) with adjacent resolutions, we upsample the lower-resolution feature to the higher-resolution one, concatenating them, and processing with convolutional layer: }3 proc = Conv(Up(f raw i1), raw ). (9) Then, we get the processed multi-scale features {f proc specific network architectures when we build our Diff-2-in-1 on existing works. }3 i=1 which are further used for fitting into the Surface normal estimation. For both Bae et al. (2021) and iDisc (Piccinelli et al., 2023), the surface normal maps are decoded from multi-scale features extracted by their original encoder. When instantiating our Diff-2-in-1 upon them, we replace their original encoders with the unified model described above. If the decoder requires feature map with spatial resolution unavailable in {f proc }3 i=1, we use similar strategy as Equation 9 to obtain the feature of new spatial resolui tion. If the features required are of higher resolution than the existing features, then we increase the resolution range of the features by proc i+1 = Conv(Up(f proc ), Deconv(f proc (10) )), where the upsampling and deconvolutional (Noh et al., 2015) layers increase the feature size by the same ratio. For obtaining lower resolution features, we simply replace the upsampling and deconvolutional layers in Equation 10 with downsampling and convolutional layers. The upsampling or downsampling factor in Equation 10 is set to 2. Moreover, we can iteratively perform Equation 10 multiple times if the required features are more than twice larger or smaller than the features {f proc }3 i=1 from the pretrained stable diffusion model. Semantic segmentation. As VPD (Zhao et al., 2023) also builds upon stable diffusion (Rombach et al., 2022), we directly apply the self-improving algorithm in our Diff-2-in-1 on VPD to boost its performance. Multi-task learning. The decoder of InvPT (Ye & Xu, 2022) requires multi-scale features. Therefore, we use the same strategy as the surface normal estimation methods (Bae et al., 2021; Piccinelli et al., 2023) to provide the decoder with the required features. The decoder of TaskPrompter (Ye & Xu, 2023) only requires single-scale features. Therefore, we use Equation 10 to resize all the features in {f proc }3 i=1 to this specific scale. As result, the multi-scale knowledge extracted from stable diffui sion can be injected into the TaskPrompter framework. Additionally, both InvPT and TaskPrompter adopt pretrained ViT (Dosovitskiy et al., 2021) or Swin Transformer (Liu et al., 2021b) as their encoders. To better utilize the prior knowledge within the original encoders, we merge the knowledge from the two sources by adding the features from stable diffusion to their original encoders. 16 Figure A: Captions generated by ClipCap (Mokady et al., 2021) and BLIP-2 (Li et al., 2023b) on the NYUv2 (Silberman et al., 2012) dataset. The generated captions using these two off-the-shelf image captioning models not only have similar semantic meanings, but also share similar text formats. Model Diff-2-in-1 on Bae et al. (2021) Diff-2-in-1 on iDisc (Piccinelli et al., 2023) Caption None ClipCap (Mokady et al., 2021) BLIP-2 (Li et al., 2023b) None ClipCap (Mokady et al., 2021) BLIP-2 (Li et al., 2023b) 11.25 () 66.0 67.3 67.4 67.2 68.7 68.7 22.5 () 83.0 83.4 83.4 83.4 83.7 83. 30 () Mean () Median () RMSE () 88.0 88.2 88.2 88.1 88.4 88.4 22.0 22.0 22.0 21.7 21.6 21.6 13.6 13.2 13.2 13.0 12.7 12.7 7.0 6.5 6.5 6.6 6.0 6.0 Table A: Ablation study on using text prompts from different off-the-shelf image captioning models ClipCap (Mokady et al., 2021) and BLIP-2 (Li et al., 2023b) to generate samples with Diff-2-in1. The evaluation is conducted on the surface normal estimation task on the NYUv2 (Silberman et al., 2012; Ladicky et al., 2014) dataset. Our Diff-2-in-1 is robust to different choices of image captioning models. Nevertheless, it is necessary to have an image captioning model to provide text prompts in the denoising process during data generation. Summary. From the instantiations above, we have the following guidelines for converting existing methods to the unified diffusion-based models in our Diff-2-in-1: (1) By default, we replace the encoders in the original models with the stable diffusion feature extractor; (2) If the features required by the original decoder is unavailable in the multi-scale features, we can use Equation 10 to expand the range of the multi-scale features; (3) If the original model design contains pretrained encoder, we consider merging the knowledge of the stable diffusion model and the pretrained encoder. A.2 TEXT PROMPTS Our Diff-2-in-1 uses the generative nature of diffusion models to create samples, which requires text prompts as conditions during the denoising process to generate high-quality samples. However, the text prompts are not always available in our target datasets. To solve this challenge, we use the off-the-shelf image captioning model BLIP-2 (Li et al., 2023b) to generate text descriptions for each image. The generated text descriptions serve as conditions when performing denoising to generate new data samples with our Diff-2-in-1. We further show in the ablation study in Section that the choice of the image captioning model has little influence on the performance. A.3 ADDITIONAL TRAINING DETAILS In the warm-up stage, we follow the same hyperparameters of the learning rate, optimizer, and training epochs of the original works that our Diff-2-in-1 builds on. In the self-improving stage, the exploitation parameter θE continues the same training scheme in the warm-up stage, while the creation parameter θC updates once when θE consumes 40 samples. Thus, the interval of the EMA update for θC depends on the batch size used in the self-improving stage. For the surface normal estimation and semantic segmentation tasks, we adopt batch size of 4, so the EMA update happens every 10 iterations. For the multi-task frameworks, the batch size is 1, so we perform the EMA update every 40 iterations. The momentum hyperparameter α for the EMA update is set as 0.999 for multi-task learning on PASCAL-Context (Mottaghi et al., 2014), and 0.998 for the rest of the task settings. 17 Setting Direct Finetuning LoRA Finetuning Diff-2-in-1 (Ours) 11.25 () 58.0 64.8 67.4 22.5 () 76.5 82.0 83.4 30 () Mean () Median () RMSE () 82.4 87.4 88.2 16.9 14.1 13.2 26.5 22.8 22.0 8.7 7.3 6. Table B: Ablation study on strategies to finetune the diffusion backbone. Direct Finetuning: Directly finetune the denoising U-Net. LoRA Finetuning: Adopt LoRA (Hu et al., 2022) to finetune the UNet. Their unsatisfactory results indicate that the features extracted from the finetuned network are less informative and have worse generalizability. The information loss introduced by finetuning is inevitable even if using the parameter-efficient finetuning technique LoRA to mitigate forgetting. In contrast, our diffusion-denoising strategy injects external knowledge from the pretrained stable diffusion to the samples, without risks of forgetting the discriminative ability of diffusion models."
        },
        {
            "title": "B ADDITIONAL ABLATION STUDY",
            "content": "What text prompts to use for the unified diffusion model? As mentioned in Section A.2, we adopt BLIP-2 to generate text prompts for creating new samples based on the reference images. What if the text prompters are less powerful? We show that different choices of image captioning models have marginal influence on the performance of our Diff-2-in-1. We first show the captions generated by BLIP-2 and another relatively weaker model ClipCap (Mokady et al., 2021) in Figure A. The captions generated by these two off-the-shelf models have similar semantic meanings, as well as sharing similar formats of [Place] with [Object 1], [Object 2], ..., [Object N-1], and [Object N]. We further evaluate the performance of using the text prompts from ClipCap and BLIP-2 to generate synthetic samples for the self-improving learning system in Diff-2-in-1. The results are shown in Table A. We can observe that once again there is no large difference between the two variants and both of them greatly outperform the baseline, demonstrating that our Diff-2-in-1 is robust to different text prompters used during the denoising process for data generation. Nonetheless, it does not indicate that the image captioning model is dispensable. If we completely get rid of the image captioning model and do not use text as the condition during denoising (None for Caption in Table A), we could observe an evident drop in the performance on discriminative tasks. Should we finetune the diffusion backbone? As shown in Figure 3, if the generation process of our unified diffusion model starts from Gaussian noise, the generated samples will have an evident domain shift from the original distribution. Therefore, we adopt the halfway diffusion-denoising mechanism to synthesize in-distribution data. Another potential solution to overcome the domain shift issue is to finetune the stable diffusion backbone. We test this setting with two finetuning strategies for comprehensive ablation: (1) directly finetune all the parameters of the denoising U-Net (Direct Finetuning); (2) adopt parameter-efficient finetuning strategy Low-Rank Adaptation (LoRA) (Hu et al., 2022) on the denoising modules of stable diffusion (LoRA Finetuning). We conduct the experiments on the surface normal task on the NYUv2 dataset with Bae et al. (2021) as the task head. The results are shown in Table B. The inferior performance of using the finetuned stable diffusion indicates that the diffusion-denoising data generation scheme and the self-improving learning system in our Diff-2-in-1 are essential. One factor for the unsatisfactory performance of using finetuning is that the finetuning process incurs loss in the generalization capability, especially during finetuning with limited data (e.g., 795 samples on NYUv2), making the features extracted from the stable diffusion model less informative for decoding visual task predictions. In comparison, our proposed diffusion-denoising data generation scheme injects external knowledge from the pretrained stable diffusion model to the samples in the training data, without risks of knowledge forgetting with respect to its discriminative ability. What timestep to choose for discriminative feature extraction? In our current experiments, we follow existing works ODISE (Xu et al., 2023a) and VPD (Zhao et al., 2023) to adopt = 0 as the timestep for feature extraction from the pretrained stable diffusion model. We ablate different timesteps for extracting features from stable diffusion in Table C. The performance is generally satisfactory with relatively small timesteps , which add little noise to the clean latents before extracting features from denoising U-Net. We do not attentively optimize for the best and it is likely that better may exist in other settings which can further improve the performance of our Diff-2-in-1. We leave the exploration of optimal for different tasks as future work. 18 0 50 100 150 11.25 () 67.4 67.5 66.9 65. 22.5 () 83.4 83.3 82.6 81.6 30 () Mean () Median () RMSE () 88.2 88.1 87.5 86.7 22.0 22.0 22.4 23.0 13.2 13.2 13.5 14.0 6.5 6.5 6.5 6.8 Table C: Ablation study on extracting features from the pretrained stable diffusion model with different timesteps on NYUv2 surface normal evaluation. Our Diff-2-in-1 achieves better performance with smaller in this task setting. α N/A (Baseline) 0.99 0.993 0.996 0.998 0.999 11.25 () 62.2 67.1 67.3 67.3 67.4 67.1 22.5 () 79.3 83.2 83.4 83.4 83.4 83.3 30 () Mean () Median () RMSE () 85.2 88.1 88.2 88.2 88.2 88.1 23.5 22.1 22.0 22.0 22.0 22.1 14.9 13.4 13.3 13.3 13.2 13. 7.5 6.6 6.6 6.6 6.5 6.7 Table D: Ablation study on different α for the EMA update within Diff-2-in-1. α = 0.998 reaches the best performance in this setting of surface normal prediction with Bae et al. (2021) on NYUv2. Nonetheless, our Diff-2-in-1 is robust to different α within broad range. How to choose hyperparameters for the EMA update? We ablate the choice of α [0.99, 0.999] for the EMA update according to guidelines in Liu et al. (2021a). The results with Bae et al. (2021) on the NYUv2 (Silberman et al., 2012; Ladicky et al., 2014) surface normal task are shown in Table where α = 0.998 achieves the best performance. Nevertheless, the performance of our Diff-2-in-1 is robust to different choices of α within broad range."
        },
        {
            "title": "C MORE VISUALIZATIONS",
            "content": "We provide more qualitative results from the following two aspects: (1) performance comparison with state-of-the-art methods on discriminative tasks and (2) multi-modal data generation quality of the synthetic samples from our Diff-2-in-1. C.1 COMPARISONS ON DISCRIMINATIVE TASKS The qualitative comparisons of our Diff-2-in-1 and the baselines are shown in Figures B, (surface normal prediction) and (multi-task). Our Diff-2-in-1 outperforms the baselines, demonstrating the competence of our unified diffusion-based model in the discriminative perspective. C.2 DATA GENERATION QUALITY We display the synthetic multi-modal data from our Diff-2-in-1 data creation framework in Figures E, (RGB-normal pairs) and G, (RGB and multiple annotations) to show that Diff-2-in-1 has powerful generation ability that is capable of generating high-quality and consistent samples."
        },
        {
            "title": "D DISCUSSIONS AND FUTURE WORK",
            "content": "Limitation. One major limitation of this work is that adopting diffusion models for data generation is relatively time-consuming as diffusion models typically need multi-step denoising to produce samples. To alleviate this shortcoming, current advancement on accelerating the inference process of diffusion models (Zheng et al., 2023a; Lu et al., 2022; Yin et al., 2024; Liu et al., 2024) can be adopted to speed up the data generation process. Future work. Looking ahead, the potential applications of this unified approach are vast. Future research directions include extending this methodology to other types of tasks, such as 3D detection, and refining and optimizing the Diff-2-in-1 framework such as more efficient data creation scheme and knowledge transfer to new domain. 19 Figure B: Qualitative results on the surface normal prediction task of NYUv2 (Silberman et al., 2012; Ladicky et al., 2014). Our proposed Diff-2-in-1 outperforms the baseline with more accurate surface normal estimations, indicating that our unified diffusion-based models excel at handling discriminative tasks. The black regions in the ground truth visualizations are invalid regions. 20 Figure C: Qualitative results on the surface normal task of NYUv2 (Silberman et al., 2012; Ladicky et al., 2014). Our proposed Diff-2-in-1 outperforms the baseline with more accurate surface normal estimations, indicating that our unified diffusion-based models excel at handling discriminative tasks. The black regions in the ground truth visualizations are invalid regions. 21 Figure D: Qualitative results on the multi-task datasets NYUD-MT (Silberman et al., 2012) and PASCAL-Context (Mottaghi et al., 2014). Diff-2-in-1 has superior performance compared to the baselines, demonstrating the effectiveness of our unified diffusion-based model design. Zoom in for the regions with bounding boxes to better see the comparison. 22 Figure E: Synthetic samples from our method after the Diff-2-in-1 framework is trained on the surface normal task of NYUv2 (Silberman et al., 2012; Ladicky et al., 2014). The odd rows are the generated RGB images while the even rows are the generated surface normal maps. The model is capable of generating diverse and high-fidelity images with the corresponding surface normal maps matching the generated RGB images. 23 Figure F: Synthetic samples from our method after the Diff-2-in-1 framework is trained on the surface normal task of NYUv2 (Silberman et al., 2012; Ladicky et al., 2014). The odd rows are the generated RGB images while the even rows are the generated surface normal maps. The model is capable of generating diverse and high-fidelity images with the corresponding surface normal maps matching the generated RGB images. 24 Figure G: Synthetic samples from our method after the Diff-2-in-1 framework is trained on the multitask setting of NYUD-MT (Silberman et al., 2012). Each batch of samples contains four rows: RGB, depth map, surface normal map, and semantic labels (from top to bottom). The generated samples are of high quality with their multi-task annotations. 25 Figure H: Synthetic samples from our method after the Diff-2-in-1 framework is trained on the multi-task setting of PASCAL-Context (Mottaghi et al., 2014). Each batch of samples contains five rows: RGB, semantic labels, human parsing labels, saliency map, and surface normal map (from top to bottom). If the human parsing labels are all black, it means that there is no human in the generated image. The generated samples are of high quality with their multi-task annotations."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign",
        "Carnegie Mellon University",
        "Tsinghua University"
    ]
}