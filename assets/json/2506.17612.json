{
    "paper_title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent",
    "authors": [
        "Yunlong Lin",
        "Zixu Lin",
        "Kunjie Lin",
        "Jinbin Bai",
        "Panwang Pan",
        "Chenxin Li",
        "Haoyu Chen",
        "Zhongdao Wang",
        "Xinghao Ding",
        "Wenbo Li",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 1 6 7 1 . 6 0 5 2 : r JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent Yunlong Lin1* Zixu Lin1* Kunjie Lin1* Jinbin Bai5 Panwang Pan4 Chenxin Li3 Haoyu Chen2 Zhongdao Wang6 Xinghao Ding1 Wenbo Li3 Shuicheng Yan5 1 Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, Fujian, China 2 The Hong Kong University of Science and Technology (Guangzhou) 3 The Chinese University of Hong Kong 4 Bytedance 5 National University of Singapore 6 Tsinghua University Project Page: https://jarvisart.vercel.app/"
        },
        {
            "title": "Abstract",
            "content": "Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities."
        },
        {
            "title": "Introduction",
            "content": "Photo retouching is fundamental to modern photography, enabling users to manipulate exposure, color, contrast, and tone for expressive, high-quality images. Commercial tools such as Adobe Lightroom and PicsArt offer extensive manual controls but demand specialized expertise and significant time investment, creating barriers for non-experts. Existing automated methodsincluding zeroand first-order optimization [11, 36, 46, 56], reinforcement learning [47, 23, 22], and diffusion-based editing [59, 2, 49]improve automation yet remain limited in stylistic diversity, fine-grained adjustment, and scene generalization. More recently, instruction-guided multimodal models such as GPT-4o [18] and Gemini-2-Flash [44] have enabled natural-languagedriven editing but frequently compromise content fidelity, intricate attribute control, and high-resolution support. Equal Contributions. Project Leader Corresponding Authors. Preprint. Under review. Figure 1: JarvisArt supports multi-granularity retouching goals, ranging from scene-level adjustments to region-specific refinements. Users can perform intuitive, free-form edits through natural inputs such as text prompts, bounding boxes, or brushstrokes. Furthermore, users can edit any-resolution images with JarvisArt. Purple: multi-modal context understanding. Green: retouching strategy reasoning. Orange: decision-making in tool orchestration. LLM [9, 4, 51, 27]-powered agents have driven breakthroughs in autonomous task execution and problem solving, inspiring us to explore novel photo-retouching paradigm: an intelligent, userfriendly artist agent that interprets the users intent and delivers professional-level edits. To this end, we introduce JarvisArt, which (1) accurately parses visual inputs and natural-language instructions, (2) embeds professional retouching expertise to emulate an artists reasoning, (3) efficiently manages over 200 Lightroom operations, and (4) supports both global and local adjustments through an intuitive interface. All planning and tool invocations are fully transparent, allowing users to interactively refine the retouching workflow to suit their individual preferences. To translate this vision into practice, we must tackle three core challenges:the scarcity of high-quality data (source/target images, textual instructions, and editing configurations), the need for expert-level reasoning strategies, and the absence of standardized Agent-to-Lightroom integration protocol. To overcome these, we first design data-generation pipeline that yields the MMArt-55K dataset, comprising 5K standard and 50K Chain-of-Thoughtenhanced multi-granularity samples. Next, we employ two-stage post-training regime: (1) supervised fine-tuning (SFT) to instill workflow of understanding reasoning decision-making (recording Lightroom operations into ROC file), as illustrated in Figure 1, and (2) Group Relative Policy Optimization for Retouching (GRPOR) augmented with multi-dimensional tool-use rewardsnamely, retouching-operation accuracy (evaluating both global and region-specific parameter prediction) and perceptual quality (assessing the visual fidelity of retouched outputs)to refine decision-making and generalization. Finally, we introduce the Agent-to-Lightroom (A2L) protocol to enable seamless, automated Lightroom editing with bidirectional feedback. Consequently, JarvisArt deeply understands the intent of the user, generates diverse stylistic renditions, and seamlessly executes global and region-specific adjustments to produce visually compelling results (see Figure 1). Our contributions can be summarized as follows: 2 We introduce JarvisArt, an intelligent artist agent powered by an MLLM and linked to over 200 Lightroom operations, capable of producing diverse, user-driven stylistic edits that surpass current automated methods and rival professional human retouchers. We design scalable data-synthesis pipeline to construct the MMArt dataset, comprising 5K standard instruction-based and 50K Chain-of-Thoughtenhanced multi-granularity samples for detailed retouching tasks. We develop two-stage post-training regime: SFT followed by GRPO-R with tailored tool-use rewards to enhance the agents reasoning, tool proficiency, and generalization. We establish an Agent-to-Lightroom communication protocol that enables seamless collaboration between JarvisArt and Lightroom, facilitating fully automated editing workflows."
        },
        {
            "title": "2 Related Work",
            "content": "Photo Retouching. Existing automated pipelines pipelines have been proposed to streamline manual retouching. Zerothand First-order optimizations [11, 36, 35, 6, 46, 56, 45] were early attempts, but they are constrained by limited parameter prediction and reliance on pre-trained proxies. RL-based methods [47, 23, 22, 13, 37] attempt to mimic human workflows and offer some transparency but fail to capture artistic vision and lack deeper user interaction. Diffusion models [59, 2, 49] dominate high-fidelity image synthesis but rely on static prompts and lack multi-turn reasoning or flexible language alignment, limiting open-ended editing. Additionally, recent unified image editing models have achieved dual breakthroughs in comprehension and generation. Notable examples include closed-source models like GPT-4o [18] and Gemini-2-Flash [44], as well as open-source models such as Janus-Pro [7], UniTok [33], QLIP [64], and VARGPT-v1.1 [66]. Despite these breakthroughs, three key limitations remain: (1) destructive editing by regenerating all pixels, compromising content preservation; (2) lack of interactive and interpretable local attribute control (e.g., softening or brightening skin); and (3) the absence of arbitrary-resolution editing due to generative model architectural constraints. Conversely, our study presents an interactive and interpretable retouching paradigm that integrates multimodal understanding with expert-level editing tools for non-destructive photo retouching. JarvisArt empowers users through human-agent collaboration loop, enabling scene-level edits alongside precise region-specific tweaks-blending creative flexibility with the rigor of professional workflow. Reinforcement Fine-Tuning. Rule-based reinforcement fine-tuning, as demonstrated by OpenAIs o1 [19] and Deepseek-R1 [9], has shown impressive performance in tasks such as mathematical reasoning [4, 19, 52, 55], and code generation [16, 20, 58, 61]. Subsequent research has extended this approach to multimodal models, designing task-specific reward functions for visual perception tasks. These include correct class prediction in image classification [38, 5, 34], Intersection-over-Union (IoU) metrics in image localization and detection [31, 15, 53, 41], accurate click position prediction in GUI grounding tasks [32, 48], and effective interaction with search engines to leverage up-to-date external information [21]. However, unlike these tasks with single correct answer, our task involves tool-integrated retouching, which requires predicting multiple tools and their parameters. Designing effective reward signals to support learning in this setting remains an open and underexplored challenge. In this paper, we propose customized tool-use rewards, enabling JarvisArt to equip advanced artistic reasoning and tool invocation capabilities. LLM-Empowered Agent. LLM-powered agents have revolutionized AI systems due to three key developments: 1) unprecedented reasoning capabilities of LLMs [9, 4, 51]; 2) advancements in tool manipulation and environmental interaction [25, 43, 12, 14, 30] and 3) sophisticated memory architectures that support longitudinal experience accumulation [10, 62, 50, 54]. Despite these advancements, three fundamental limitations persist when applying LLM agents to professional photo retouching: 1) the absence of domain-specific retouching knowledge base, which hinders accurate interpretation of user intent, 2) limited decision-making abilities in selecting suitable tools and determining precise parameter values, and 3) absence of standardized protocols to ensure compatibility with professional retouching software integrations. To address these limitations, we propose JarvisArt, powerful artistic agent that integrates three core capabilities: (1) professional retouching expertise for precise understanding of user instructions, (2) proficiency with commercial retouching tools in Lightroom, and (3) standardized communication protocols for seamless Lightroom integration. 3 Figure 2: The data generation pipeline comprises three main stages: 1) Curation of diverse sourcetarget examples covering varied scenes and styles with corresponding Lightroom configurations; 2) Generation of diverse user instrcutions that reflects different creative intents; 3) Production of Chain-of-Thought traces that simulate human artist reasoning process."
        },
        {
            "title": "3 Method",
            "content": "We begin by outlining the overall workflow of JarvisArt (Sec. 3.1). Next, we introduce comprehensive data generation pipeline that constructs MMArt, high-quality dataset comprising instruction and reasoning samples for agentic photo retouching tasks (Sec. 3.2). Finally, we investigate the core components of JarvisArt (Sec. 3.3), including two-stage post-training pipeline and the Agent-toLightroom (A2L) protocol, which allows seamless collaboration between JarvisArt and Lightroom. 3.1 Overview JarvisArt is an interactive, MLLM-based photo-retouching system that supports both scene-level and region-level edits. In addition to textual instructions, users can specify local areas via free-form brushstrokes or draggable bounding boxes. In Figure 1, JarvisArts pipeline comprises three stages: 1) Multi-modal context understanding to parse user directives, image content, and regions of interest; 2) Strategic reasoning grounded in photographic principles to formulate retouching plan; and 3) Tool orchestration to select appropriate Lightroom operations and parameters. These operations are executed automatically through the A2L protocol. Formally, JarvisArt implements function: (Q, Isrc) = {t1, t2, . . . , tn}, where is the user query, Isrc the source image, and each ti denotes specific Lightroom edit (e.g., exposure+0.03). The final output is obtained by Iedit = g(Isrc, ), with g() representing Lightrooms execution environment. 3.2 Data Generation Pipeline We design three-stage data-generation pipeline (Figure 2) to construct MMArt with explicit Chainof-Thought (CoT) annotations. Each sample is five-tuple Isrc, Itgt, Q, C, O, where Isrc and Itgt are the before-/after-retouch images, the users instruction, the CoT reasoning wrapped in <think> tags, and the retouching operation configuration (ROC) file of tool invocations and parameters within <answer> tags. The pipeline proceeds as follows: 1) Curation of diverse sourcetarget 4 Figure 3: Overview of the two-stag post-training framework. Initially, JarvisArt undergoes supervised fine-tuning (SFT) on CoT-annotated data to develop foundational artistic reasoning and tool-use skills. Following this, we apply the Group Relative Policy Optimization for Retouching (GRPO-R) algorithm to further enhance the JarvisArts reasoning, tool proficiency, and generalization. examples covering varied scenes and styles, and the corresponding Lightroom configurations; 2) Generation of natural-language instructions that reflect user intents; 3) Production of step-by-step reasoning traces. Further statistics and examples of MMArt can be found in Appendix A. Stage I: Generation of image pairs and Lightroom configuration. We source raw images from PPR10K [26], the Adobe Lightroom community, and licensed open-source collections, then curate diverse library of global and local artistic presets. Leveraging Qwen2.5-VL-72B [51] for multimodal role-playing and Grounding DINO [29] for precise region localization, we simulate expert-level edits in four steps: 1) Region detection, in which Grounding DINO [29] identifies regions of interest (confidence > 0.8); 2) Preset recommendation, where Qwen2.5-VL-72B [51] proposes global and local presets based on image aesthetics; 3) Preset application, applying each recommendation in Lightroom to generate five candidate retouched images; and 4) Human-in-the-loop validation, selecting the most artistically pleasing outputs. Each finalized sample comprises Isrc, Itgt, O, denoting the source image, the retouched image, and the detailed record of Lightroom operations. The role-playing prompts are detailed in Appendix A.4. Stage II: Generation of user instructions. To simulate diverse editing intents, we employ Qwen2.5VL-72B [51] with role-playing prompt (Appendix A.4) to translate each Isrc, Itgt, triplet into both scene-level and region-level instructions Q. We generate descriptions for two user typescasual users and professional editors with advanced aesthetic sensibilities, ensuring coverage of simple global edits as well as precise, localized adjustments. Stage III: Generation of reasoning processes. For each sample quadruple Isrc, Itgt, Q, O, we first apply QVQ-maxs [51] advanced visual reasoning to generate initial CoT annotations. To remove redundancy and enforce human-like coherence, we subsequently refine these traces using Qwen2.5VL-72B [51] through iterative multimodal prompts, producing concise, context-rich reasoning processes C. Full prompt templates are provided in Appendix A.4. 3.3 JarvisArt Framework 3.3.1 CoT Supervised Fine-tuning Drawing on Deepseek-R1 [9], we initialize JarvisArt via supervised fine-tuning on CoT annotations to bootstrap its subsequent reinforcement learning. This phase 1) enforces consistent, structured output format, 2) instills foundational reasoning skills spanning user-intent interpretation and aesthetic judgment, and 3) establishes preliminary proficiency in selecting Lightroom tools and configuring their parameters. 3.3.2 Reasoning-oriented Reinforcement Learning Building on the SFT-initialized model, as shown in Figure 3, we apply group relative policy optimization for retouching(GRPO-R) [40] (Appendix B.1) to further refine JarvisArts artistic reasoning 5 and tool-use proficiency. GRPO-R trains the agent with three interpretable, task-specific rewards: format reward Rf that enforces structured output, retouching operation accuracy reward Rroa that measures the correctness of selected tools and their parameter settings, and perceptual quality reward Rpq that assesses the visual fidelity of the retouched image. The overall objective is thus = Rf + Rroa + Rpq [0, 3]. Format reward. Following prior work [9, 53, 41, 48], we include format reward Rf [0, 1] to enforce structured outputs: reasoning must appear within <think> tags and tool invocations within <answer> tags, ensuring consistent and reliable parsing. Retouching operation accuracy reward. Inspired by existing explorations of reward designs [39, 21, 32, 48] in the fields of GUI and web searching. We consider over 200 retouching tools in Lightroom, containing both global adjustmentssuch as exposure, highlights, and tone curveand local refinements using six types of masks: 1) linear masks for directional gradients, 2) radial masks for circular or elliptical regions, 3) object masks for isolating subjects (e.g., people or objects), 4) color masks for hue-specific adjustments, 5) luminance masks for brightness-based selections, and 6) portrait masks for fine-tuning facial features such as skin and eyes. Further details are provided in Appendix E. To assess the accuracy of predicted tools and their parameters, pre = {T pre }, against the ground truth tgt = {T tgt }, we define the ROA reward based on three evaluation criteria: ➊ Tool name matching: , ..., pre , ..., tgt 1 1 NT pre NT tgt NT pre NT tgt where NT pre and NT tgt are the sets of tool names in the predicted and target sequences, respectively. ➋ Parameter name matching: rname = [0, 1] , (1) rparam = (cid:88) (cid:88) tgt tgt pre pre keys(T pre keys(T pre ) keys(T tgt ) keys(T tgt ) ) [0, tgt] , (2) where keys() denotes the set of parameter names associated with predicted or ground-truth tool. It is noted that an overlap in parameter names occurs only when the predicted and ground-truth tool names match. ➌ Parameter value matching: (cid:88) (cid:88) (cid:88) (cid:88) Sk (cid:0)T pre [k], tgt [k](cid:1) [0, keys(T tgt )] , (3) rvalue = tgt tgt pre T pre kkeys(Ttgt ) tgt tgt where Sk () [0, 1] quantifies the correspondence between predicted and ground-truth parameter values, with value of 1 indicating an exact match. Specifically, if the key is absent in pre , then pre [k] is undefined and Sk = 0. The computation of Sk depends on the parameter type: scalar differences for standard numerical values, intersection-over-union (IoU) for object masks, endpoint distance for linear masks, geometric similarity for radial masks, color distance between sampled points for color masks, luminance range differences for luminance masks, and category-specific criteria for portrait masks. Refer to Appendix B.2 for further details. Finally, the retouching operation accuracy reward is computed by measuring the matching degree between pre and tgt: Rroa = 1 rname + rparam tgt + rvalue tgt keys(T tgt tgt (cid:80) [0, 1] . ) (4) Perception quality reward. While parameter-based rewards offer critical guidance, they may not fully capture the perceptual quality of the final image, as different parameter settings can produce visually similar results. To address this limitation, we introduce the PQ reward, which evaluates two key aspects: 1) global tone consistency via color distribution matching, and 2) pixel-wise fidelity. The reward is defined as: Rpq = γ CD(Iedit, Itgt) + (1 γ) L(Iedit, Itgt) [0, 1] , (5) where Iedit is the retouched image and Itgt is the target image. CD() measures color distribution similarity in CIELAB space [60] and L() denotes the pixel-wise distance. Both metrics are normalized to the range [0, 1], with higher values indicating better similarity. The weighting factor is empirically set to γ = 0.4 to balance both terms. Table 1: Quantitative evaluation on MMArt-Bench. We highlight the best and second-best instruction-based results. SC, PQ, and refer to the metrics evaluated by Gemini-2-Flash. The RC means the metric calculated on specific mask region. Scene-level Region-level Method Instruction RSFNet [37] 3DLUT [57] InstructPix2Pix [2] MagicBrush [59] OmniGen [49] VARGPT-v1.1 [66] Step1X-Edit [28] Gemini-2-Flash [44] GPT-4o [18] JarvisArt 103 SCRC PQRC ORC 102 L2RC L1102 L2103 SC PQ L1RC 13.69 8.80 - 12.39 8.33 - 26.38 25.99 11.61 11.50 - - - - - - - - - - 15.67 18.39 28.49 27.05 24.28 23.07 22.84 12. 47.51 65.25 133.45 126.47 105.91 6.54 7.79 7.10 3.93 4.09 3.85 4.25 4.42 4.13 1.83 1.38 1.48 7.52 8.67 8.01 90.99 92.23 7.62 8.78 8.08 8.73 9.66 9.18 12.62 12.37 25.16 23.71 15.43 16.52 15. 30.56 7.53 9.82 8.52 7.63 33.39 32.81 109.10 107.32 45.85 52.88 47.87 12. 4.70 3.04 6.17 1.38 8.32 8.04 8.59 8.08 5.36 3.41 7.56 1.15 9.04 9.25 9.48 9. 4.91 3.13 6.72 1.08 8.66 8.61 9.03 8.69 3.3.3 Agent-to-Lightroom Protocol Figure 4 presents the Agent-to-Lightroom (A2L) protocol, standardized client-server interface that integrates JarvisArt with Lightroom. The workflow comprises five stages: 1) handshake, 2) file verification, 3) sandboxed execution, 4) async processing, and 5) result return. A2L features dual-transport communication, structured message format, and resource management. Messages use bardelimited commands for processing, status, and error handling, enhancing clarity and efficiency. It manages source images and retouching operation configuration (ROC) files, supporting ROC-to-Lua translation, and integrity checks. The source image can by directly retouched by Lua file in Lightroom. The Lua file can be directly applied in Lightroom to retouch the source image. Additional details are provided in the supplementary materials."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Figure 4: Agent-to-Lightroom protocol. Implementation details. We adopt Qwen2.5-VL-7B-Instruct [1] as the base model for JarvisArt. The CoT supervised fine-tuning phase is performed on 50K CoT-annotated instances from MMArt, with batch size of 2, learning rate of 1e-5, and training for 2 epochs using the Llama-Factory framework [65] on 8 A100 (80G) GPUs. The reinforcement learning phase, employing the GRPO-R algorithm, is conducted on 5K standard instruction samples from MMArt, using the veRL framework [42]. For each training step, we sample batch of 2, learning rate of 1e-6, and generate 4 responses per query, training for 2 epochs on 16 A100 (80G) GPUs. MMArt-Bench. To provide comprehensive evaluation of JarvisArts performance, we introduce the MMArt-Bench, which is sampled from the MMArt dataset. It includes four main scenarios: portrait, landscape, street scenes, and still life, with 50 instances per category, totaling 200 instances. Each primary category contains multiple subcategories (Appendix A.1). For region-level evaluation, we utilize portrait subset comprising 50 human-centered images with mask annotations. Evaluation metrics. Following previous works [59, 24], six assessment metrics are use for evaluation: L1, L2, SC, PQ, and O. L1 and L2 to measure the average pixel-level absolute difference between the retouched image and reference image. SC evaluates the alignment between the instruction text and the image (010 scale). PQ evaluates contextual coherence and artifact presence (010 scale). The overall score is calculated as = SC PQ. For region-specific evaluation, we apply these six metrics to specified mask region. Further details are provided in Appendix C.1. 7 Figure 5: Visual comparison of different methods on MMArt-Bench. Baselines. For fair comparison, we evaluate JarvisArt against leading open-source photo retouching methods, including 3DLUT [57] and RSFNet [37], as well as instruction-driven editing models such as InstructPix2Pix [2], MagicBrush [59], OmniGen [49], VARGPT-v1.1 [66] and Step1XEdit [28]. Proprietary solutions such as GPT-4o2 [18] and Gemini-2Flash [44]3 are also included for comparison. Notably, all test images are cropped to 512 512 resolution, as some baselines are incapable of processing high-resolution or arbitrarily sized inputs. 4.2 Experimental Results 4.2.1 Evaluation on MMArt-Bench As shown in Table 1, JarvisArt outperforms most open-source instruction-based baselines, achieving state-of-the-art performance across all 10 evaluation metrics. Compared to closed-source models such as GPT-4o [19] and Gemini-2-Flash [44], JarvisArt achieves superior content preservationfor instance, an L1102 score of 12.44, which is 45.6% lower (and thus better) than GPT-4os score of 22.84. JarvisArt also demonstrates competitive instruction-following capability (O = 8.52), closely matching GPT-4o (O = 9.18) and outperforming Gemini-2 Flash (O = 8.08). Notably, in the local editing settingwhere content fidelity is especially criticalthe advantage of our method over GPT-4o and Gemini-2-Flash is significantly amplified. As illustrated in Figure 5, especially in portrait scenarios, competing methods often exhibit noticeable uncanny valley effects, producing significant visual artifacts that diverge from users creative intent. In contrast, JarvisArt mitigates these issues through its Lightroom-integrated workflow, enabling high-quality, non-destructive editing. More results in Appendix D. 4.2.2 User Preference Study Evaluating instruction-driven photo retouching remains inherently subjective, as even expert evaluators often disagree on the \"optimal\" outcome. To quantify preferences, we conducted user preference study on the MMArt-Bench, recruiting 80 participants to evaluate four advanced algo2The results are obtained based on ChatGPT APP in May 2025. 3The results are obtained based on Gemini API in May 2025. 8 Figure 6: User preference study. Figure 7: Questionnaire results and user ratings comparing JarvisArt with the commercial Adobe Lightroom system. Ratings are on 5-point Likert scale (1=strongly disagree, 5=strongly agree). rithms: Step1X-Edit [28], Gemini-2-Flash [44], GPT-4o [18], and JarvisArt. Evaluations focus on two criteria: (1) image consistency (preservation of source image content) and (2) aesthetic quality (visual appeal of retouched results). five-point ordinal scale (worst = 2, poor = 4, fair = 6, good = 8, and excellent = 10) for quantitative metrics. Results in Figure 6 show JarvisArt achieves best subjective quality, producing edits favored by users. To evaluate the effectiveness and usability of JarvisArt, we recruited 30 participants from diverse backgrounds, including postgraduate students, artists, and computer vision researchers. All participants had prior experience with image editing, covering broad spectrum of skill levels to ensure realistic representation of user proficiency. To mitigate learning effects, participants were randomly assigned to two groups: Group used JarvisArt before Lightroom, while Group used Lightroom first and then JarvisArt. Each participant completed comprehensive evaluation consisting of 10 questions per system, covering four key categories: Complexity and Efficiency, Consistency and Integration, Ease of Use, and Overall Satisfaction. Detailed evaluation results are shown in Figure 7. The main findings are summarized as follows: Ease of use: All participants rated JarvisArt as easy to use (Q1, score 3), with 66.7% awarding the highest score of 5. For independent operation and learning speed (Q2Q4), over 90% of users rated JarvisArt 4 or 5, indicating that most users could quickly and independently learn to use the system with minimal need for technical support. Complexity and efficiency: Regarding system complexity (Q5), more than 96.67% of participants (score 3) considered JarvisArt appropriately complex, in contrast to Lightroom, which was frequently perceived as overly complicated. Additionally, 86.67% of users rated JarvisArt 4 or 5 for smoothness of use (Q6), suggesting that our design effectively reduced cognitive load and facilitated efficient task completion. 9 Figure 8: Visualization of the reward trends across training steps of for JarvisArt. Consistency and integration: For feature integration (Q7), 90% of users rated JarvisArt 4 or 5, and for system consistency (Q8), 93.3% did so, both significantly higher than Lightroom (16.67% and 40%, respectively). These results indicate more cohesive and intuitive user experience with JarvisArt. Overall satisfaction: In terms of willingness to use in the future (Q9), 93.33% of users rated JarvisArt 4 or 5, and for confidence in use (Q10), 90% also gave high scores, both outperforming Lightroom (43.3% and 86.7%, respectively). This demonstrates strong user satisfaction and acceptance of JarvisArt. 4.2.3 Visualization of Reward Trends for GRPO-R Figure 8 shows additional visualizations of GRPO-R training. The format reward converges quickly early on. While the PQ reward initially fluctuates and grows gradually, the ROA reward rises more rapidlylikely because the model inherits \"parameter preferences\" from the SFT phase. As result, it first focuses on the more easily optimized ROA, then gradually shifts attention to the PQ reward, which requires longer exploration due to the broader search space, where different edit operations may yield similar visual outcomes. Moreover, unlike Deepseek-R1 [9], JarvisArt does not display clear aha moment. This absence may stem from the lack of intermediate visual feedback during the artistic reasoning process. For example, when the model makes hypothetical retouching adjustment like highlight+5, it cannot obtain the corresponding visual result, preventing the model from validating this steps correctness within the decision-making chain. Unlike mathematical problem-solving, where each step can be validated immediately, our artistic reasoning involves numerous retouching parameters. If we perform step-wise validation for each parameter, it would require high concurrency in calling Lightroom. This is impractical due to the high computational cost and the slow training speed. Investigating step-wise visual rewards within proxy validation environments may offer promising approach to eliciting the aha moment. We intend to explore in future work."
        },
        {
            "title": "5 Ablation Study",
            "content": "Training strategy. We assess the impact of different post-training strategies by comparing model performance under three settings: 1) SFT on 50K CoT-enhanced samples, 2) GRPO-R training on 5K standard samples from scratch; and 3) GRPO-R fine-tuning basd on SFT-initial model. Rows 24 in Table 2 show that SFT yields better results than GRPO-R trained from scratch. This is likely because, without SFT to instill the basic reasoning and tool-use abilities, the GRPO-R training process must explore significantly larger search space, thereby hindering optimization. Our combined SFT+GRPO-R strategy achieves the best results, suggesting that GRPO-R can effectively enhance the SFT-initialized models reasoning, tool proficiency, and generalization by expanding its exploration capacity. Reward design. As shown in Rows 68 of Table 2, individual reward combinations (Format+ROA or Format+PQ) result in suboptimal performance, with Format+PQ performing slightly betterpossibly because PQ aligns more closely with the ultimate objective of enhancing visual quality and offers broader optimization space to escape local optima. The full combination (Format+PQ+ROA) achieves the highest performance. This result aligns with our intuition that parameter-oriented (ROA) and perception-driven (PQ) rewards are complementary: ROA ensures parameter accuracy, while PQ maintains visual fidelity. The multi-dimensional reward system provides balanced optimization signal, guiding the model to predict accurate edit operations while preserving high visual quality. 10 Table 2: Ablation studies on different training strategies and reward design. Configurations Training strategy only SFT only RL SFT + RL (Ours) Reward design Format + ROA Format + PQ Format + ROA + PQ (Ours) L1102 L2103 SC PQ 14.42 17.55 12. 14.09 13.78 12.44 44.38 58.19 30.56 40.36 35.41 30.56 7.32 6.88 7.53 7.45 7.48 7.53 8.67 8.13 9. 8.77 8.92 9.82 7.94 7.38 8.52 8.04 8.15 8."
        },
        {
            "title": "6 Conclusion",
            "content": "This report introduces JarvisArt, an interactive and interpretable MLLM-guided agent that integrates with 200+ Lightroom editing tools, enabling non-destructive editing on images of any-resolution. To develop this artist agent, we propose new data generation pipeline that curates the MMArt55K dataset, comprising 5K standard and 50K CoT-enhanced samples. Based on this dataset, we train JarvisArt using two-stage post-training regimen: 1) CoT SFT to instill basic reasoning and tool-use abilities, and 2) GRPO-R to improve the agents reasoning, tool proficiency, and generalization through customized tool-use rewards: retouching operation accuracy reward for assessing the predicted editing operations, and the perceptual quality reward to evaluate the visual fidelity of the edited outputs. Furthermore, to enable seamless, automated Lightroom editing, we introduce the Agent-to-Lightroom protocol. Evaluation results from our MMArt-Bench demonstrate that our proposed algorithm significantly outperforms existing advanced image editing algorithms."
        },
        {
            "title": "Appendices",
            "content": "Our Appendices includes the following sections: Sec.A Details of the MMArt Dataset. Statistics of the MMArt Dataset. Comparison of Existing Datasets. Data Samples of MMArt. Prompt Templates. Sec.B Additional Method Details. Group Relative Policy Optimization. Details of Reward Calculation. Sec.C Additional Experimental Details. Calculation of Local Metrics. Prompts for MLLM-based Metrics. Sec.D Additional Experimental Results. Additional Quantitative Evaluation by Qwen-2.5-VL-72B. Examples of Intricate Retouching Tasks with JarvisArt. More Visual Comparisons. Comparison on MIT-FiveK. Sec.E Details of Retouching Tools in Lightroom. Details of the MMArt dataset. A.1 Statistics of the MMArt dataset Figure 9(a) illustrates the composition and distribution of scenarios in our MMArt dataset. The dataset is structured into four major scene categories that reflect common real-world photo retouching contexts: portrait (40.8%, including shooting purposes, shooting time/lights, subjects, and indoor/outdoor scenes), landscape (33.3%, comprising nature, city, aerial photography,travel, underwater, night scene and architecture), street scenes (5.71%, including sports, life, event and documentary), and still life (20.2%, encompassing food, close-up scenes, black/white photography, art and animals). Each major category contains diverse set of subcategories, ensuring comprehensive coverage and representativeness. Furthermore, Figure 9(b) displays word cloud of user instructions, highlighting the linguistic diversity of the instructions. A.2 Comparison of Existing Datasets Table 3 presents comparison between our MMArt dataset and existing image editing datasets. MMArt is designed with the following key properties to facilitate advanced research in image retouching: Real Images: All samples in MMArt are real photographs, ensuring the datasets authenticity and practical value for real-world applications. Diverse User Instructions: Each image is paired with detailed user instructions, capturing wide variety of editing intentions and reflecting the diversity of natural language expressions. Flexible Resolution: MMArt supports images of any resolution, including high-resolution samples, making it suitable for both research and practical deployment scenarios. Chain-of-Thought (CoT) Annotations: The dataset provides CoT reasoning annotations, which help to reveal the underlying logic and step-by-step process of user intent understanding and image editing. Lightroom Retouching Configuration: For every sample, MMArt includes comprehensive Lightroom parameter configurations, allowing for non-destructive, reproducible, and transparent image editing. Table 3: Comparison of MMArt and existing retouching datasets in terms of data properties. InstructP2P [2] MagicBrush [59] UltraEdit [63] MGIE [8] HQEdit [17] FiveK [3] MMArt Property Real Image? User Instructions? Any Resolution? High Resolution? CoT Annotations? Lightroom Configuration? Figure 9: Statistics of the MMArt dataset. (a) The dataset is divided into four primary scenarios: portrait, landscape, street scenes, and still life, each containing variety of subcategories. (b) word cloud illustrates the rich linguistic diversity found in user instructions. These properties make MMArt high-quality, flexible, and richly annotated resource for the development and evaluation of advanced image retouching techniques. A.3 Data Samples of MMArt The diversity of collected photos is shown in Figure 10. Moreover, Figure 11 demonstrates MMArt samples with Chain-of-Thought (CoT) reasoning, while Figure 12 shows standard examples without CoT annotations. A.4 Prompt Templates templates utilized throughout The prompt the various stages of MMArt are summarized hereAesthetic Preset Recommendation (Figure 22), User Instruction Simulation (Figures 23 and 24), and Chain-of-Thought Data Construction (Figure 25 and 26)."
        },
        {
            "title": "B Additional Method Details",
            "content": "B.1 Group Relative Policy Optimization the model generates set of potential responses In GRPO, given task question, {O1, O2, . . . , ON }. Each response is evaluated by taking the corresponding actions and computing its reward {R1, R2, . . . , RN }. Unlike PPO, which relies on single reward signal and critic to estimate the value function, GRPO normalizes these rewards to calculate the relative advantage of each response. The relative quality Ai of the i-th response is computed as Ai = ri Mean({r1, r2, . . . , rN }) Std({r1, r2, . . . , rN }) , Figure 10: Visual examples to demonstrate the diversity of the proposed dataset. where Mean and Std represent the mean and standard deviation of the rewards, respectively. This normalization step ensures that responses are compared within the context of the group, allowing GRPO to better capture nuanced differences between candidates. Policy updates are further constrained by minimizing the KL divergence between the updated and reference models, ensuring stable RL learning. Refer to [9, 40] for more details. B.2 Details of Reward Calculation The parameter value matching function Sk() for each parameter is determined based on its specific type. Let pre denote the predicted and ground truth values for the k-th parameter, respectively. For notational simplicity, we omit the subscript in the following formulas. The calculation proceeds as follows: and tgt k Scalar Parameters. For scalar parameters such as exposure or contrast, the matching function is defined as: (cid:18) = max 0, 1 (cid:19) pre tgt Vmax Vmin [0, 1], where represents the absolute error between the predicted and ground truth values. Linear Gradient Masks. We assess the similarity between predicted and target linear gradient masks by measuring the distances between their start points ps = (xs, ys) and end points pe = (xe, ye), with coordinates normalized to [0,1] for resolution invariance. The similarity score is computed as: = max (cid:0)0, 1 ppre ptgt ppre ptgt (cid:1) [0, 1], where denotes Euclidean distance. Radial Gradient Masks. We measure similarity between predicted and target radial gradient masks using three geometric parameters: center position = (x, y), scale factors (W, H), and rotation angle θ. Center point similarity is given by: Scenter = max(0, 1 2 cpre ctgt) [0, 1], where cpre and ctgt are normalized to [0,1]. Further, scaling similarity compares width/height ratios: Sscale = max(0, 1 pre/W tgt 1 pre/H tgt 1) [0, 1], The angle numerical value similarity is defined by: (cid:18) Sangle = max 0, 1 (cid:19) θpre θtgt θmax θmin [0, 1], The final similarity score combines these components as follows: = 0.4 Scenter + 0.4 Sscale + 0.2 Sangle [0, 1]. Object Masks. For object masks, the similarity score is defined as the Intersection-overUnion (IoU) between the predicted Bpre and ground truth Btgt bounding boxes. Each box is parameterized as [x1, y1, x2, y2]. The similarity score is computed as: = IoU(Bpre, Btgt) [0, 1], where higher values indicate better alignment, with = 1 denoting perfect overlap and = 0 indicating no intersection. Portrait Masks. In portrait masks, the model predicts different special category IDs to denote distinct regions, such as ID=0 for face, ID=1 for hair, ID=2 for eyes, ID=3 for skin, etc. The matching score is defined as follows: = (cid:26) 1, if the predicted and target category IDs coincide, 0, otherwise. Color Range Masks. To evaluate color range mask similarity, we sample representative points from both predicted and target color distributions and compute the mean CIEDE2000 color difference E100 in LAB color space. The similarity score is given by: (cid:32) = max 0, 1 1 (cid:88) n=1 (cid:33) E100(cpre , ctgt ) [0, 1], where cpre distributions, respectively. and ctgt denote the n-th sampled colors from the predicted and ground-truth Luminance Range Masks. To evaluate luminance range mask similarity, we compare the predicted and target luminance extremes by computing their absolute differences. The similarity score is defined as: (cid:32) min ltgt lpre min + lpre max ltgt 2(ltgt where the denominator normalizes by the target luminance range to ensure scale invariance. max ltgt min) = max [0, 1], 0, 1 max (cid:33)"
        },
        {
            "title": "C Additional Experimental Details",
            "content": "C.1 Calculation of Local Metrics To evaluate the models effectiveness in localized regions, we compute six metricsL1RC, L2RC, SCRC, PQRC, and ORCusing human-centric masks from the portrait subset of MMArt-Bench. For L1RC and L2RC, inspired by PPR10K [26], given an image with resolution , we define weighting matrix WI = [wi,j] RHW , where wi,j = 1 for human regions and wi,j = α(α 1) for background regions, with α empirically set to 0.5. For instance, the human-centric L1 difference metric is expressed as: L1RC = WI pred WI tgt, where pred and tgt are the predicted and target images, respectively, and denotes element-wise multiplication. The L2RC metric is defined in similar manner. For SCRC, PQRC, and ORC, with α empirically set to 0, we focus solely on the mask region of the edited image and prompt the MLLM to emphasize local adjustments. 15 Table 4: Quantitative evaluation on MMArt-Bench. We highlight the best and second-best instruction-based results. SC, PQ, and refer to the metrics evaluated by Qwen2.5-VL-72B [1]. The RC means the metric calculated on specific mask region. Scene-level Region-level Method Instruction RSFNet [37] 3DLUT [57] InstructPix2Pix [2] MagicBrush [59] OmniGen [49] VARGPT-v1.1 [66] Step1X-Edit [28] Gemini-2-Flash [44] GPT-4o [18] JarvisArt 103 SCRC PQRC ORC L1102 L2103 SC PQ L1RC 102 L2RC 13.70 8.80 - 12.26 8.34 - 11.62 11.51 26.38 26.00 - - - - - - - - - - 15.62 18.31 28.40 27.04 24.17 23.06 22.77 12.66 47.26 64.76 132.82 126.26 105.14 6.17 5.81 5.47 3.93 2.25 2.44 4.14 2.16 2.70 1.27 0.17 0.29 7.03 4.94 5. 90.96 91.79 7.65 6.77 7.00 8.52 7.37 7.85 12.48 12.44 24.85 23.59 15.27 16.74 15.67 31.88 6.19 8.51 6. 7.75 32.69 32.93 106.81 105.86 44.93 53.76 47.60 12.38 4.67 2.80 3.80 0.09 7.50 7.33 8. 7.54 3.85 2.15 3.85 0.02 6.89 7.17 7.87 8.46 3.64 2.01 3.67 0.03 7.11 7.19 7. 7.91 Table 5: Quantitative evaluation on MIT-FiveK [3]. We highlight the best and second-best instruction-based results. SC, PQ, and refer to the metrics evaluated by Gemini-2-Flash. Method Instruction L1102 L2103 SC PQ InstructPix2Pix [2] MagicBrush [59] OmniGen [49] VARGPT-v1.1 [66] Step1X-Edit [28] Gemini-2-Flash [44] GPT-4o [18] JarvisArt 16.23 17.29 28.53 26.96 22.08 18.69 21.49 12.98 49.54 53.45 128.59 117.16 91. 61.27 78.11 30.05 6.36 4.92 3.12 2.94 7.20 7.86 8.72 7.36 8.34 5.50 2.48 2.00 8. 9.22 9.76 9.82 7.15 4.95 2.57 2.29 7.69 8.47 9.22 8. C.2 Prompt for MLLM-based Metrics As shown in Figure 21, we present the evaluation prompts utilized for both scene-level and regionlevel assessments of the Semantic Consistency (SC) and Perceptual Quality (PQ) metrics. Notably, the overall score is calculated as = SC PQ."
        },
        {
            "title": "D Additional Experimental Results",
            "content": "D.1 Additional Quantitative Evaluation by Qwen2.5-VL-72B As shon in Table 4, to further evaluate MLLM-based metrics, we conducted an additional quantitative analysis using Qwen2.5-VL-72B [1]. Our findings suggest that such metrics may be unreliable, struggle to effectively reflect models instruction-following capability. Despite this, our model demonstrates instruction-following performance comparable to that of contemporary SOTA closedsource model GPT-4o, while achieving significant improvement in content fidelity. D.2 Examples of Intricate Retouching Tasks with JarvisArt Figures 13-16 present the challenging retouching examples, which involve both global and local editing demands, as well as vague user instructions. JarvisArt excels in understanding these ambiguous intentions, applying modifications at both the scene and region levels, and delivering visually effective results in the final images. D.3 More Visual Comparisons Figures 17-20 present additional photo retouching results from the MMArt-Bench, highlighting the superiority of JarvisArt in terms of instruction adherence, content fidelity, and visual appeal, while 16 also achieving the style most similar to the target image. Notably, we also include visual comparison with two commercial editing tools: Adobe Lightroom and Google Photo auto-retouching modes. D.4 Comparison on MIT-FiveK To assess the generalization ability of our system, we conduct comprehensive qualitative and visual comparisons on the MIT-FiveK [3] benchmark dataset. Specifically, we randomly select 50 data samples from MIT-FiveK [3] and generate corresponding user instructions based on the source images, expert Cs reference images, and the associated retouching parameter configurations (see Section 3.2 for implementation details). As shown in Table 5 and Figures 19 and 20, our system achieves state-of-the-art performance in both instruction-following and content preservation metrics. These results confirm the robustness and effectiveness of our method in faithfully executing user instructions while maintaining original image content. Furthermore, our approach consistently outperforms existing baselines on multiple real-world benchmarks, highlighting its strong generalization ability and practical applicability in interactive photo retouching. 17 Figure 11: Examples of MMArt data annotated with Chain-of-Thought (CoT) reasoning. 18 Figure 12: Data samples from MMArt with standard instructions. 19 Figure 13: An example of JarvisArt empowering users to achieve interactive and interpretable editing, transforming their ambiguous intentions into artistic visual outcomes. 20 Figure 14: An example of JarvisArt empowering users to achieve interactive and interpretable editing, transforming their ambiguous intentions into artistic visual outcomes. 21 Figure 15: Editing results with JarvisArt are generated under complex prompts, with all retouching operations performed in Lightroom environment, allowing for iterative adjustments. 22 Figure 16: Editing results with JarvisArt are generated under complex prompts, with all retouching operations performed in Lightroom environment, allowing for iterative adjustments. 23 Figure 17: Visual comparisons of all state-of-the-art editing methods alongside two automatic retouching modes from commercial software. 24 Figure 18: Visual comparisons of all state-of-the-art editing methods alongside two automatic retouching modes from commercial software. 25 Figure 19: Visual comparisons of all instruction-based editing methods on MIT-FiveK [3]. 26 Figure 20: Visual comparisons of all instruction-based editing methods on MIT-FiveK [3]. Figure 21: Prompt for MLLM-based metrics (SC, PQ) from scene-level and region-level. 28 Figure 22: Role-playing prompt for preset recommendation. 29 Figure 23: Prompt for simulating the professional user instructions Figure 24: Prompt for simulating the casual user instructions. 31 Figure 25: Prompt for generating the initial Chain-of-Thought (COT) annotations. Figure 26: Prompt for generating the refined Chain-of-Thought (COT) annotations."
        },
        {
            "title": "E Details of Retouching Tools in Lightroom",
            "content": "We provide an overview of key Lightroom tools generated by JarvisArt, focusing on the functionality of retouching tools and their associated parameters: Table 6: Lightroom Tools with Functional Description and Parameter Type. Tool Name Basic Adjustments WhiteBalance Temperature Tint Exposure2012 Contrast2012 Highlights2012 Shadows2012 Whites2012 Blacks2012 Texture Clarity2012 Dehaze Vibrance Saturation IncrementalTemperature IncrementalTint Tone Curve ToneCurveName2012 ToneCurvePV2012 ToneCurvePV2012Red ToneCurvePV2012Green ToneCurvePV2012Blue ParametricShadows ParametricDarks ParametricLights ParametricHighlights ParametricShadowSplit ParametricMidtoneSplit ParametricHighlightSplit Functional Description Type Str. Overall color temperature (As Shot, Auto, Custom) Num. Blue-yellow balance (2000-10000 Kelvin) Num. Green-magenta balance (-150 to +150) Num. Overall brightness (-5.0 to +5.0 stops) Num. Difference between light/dark areas (-100 to +100) Num. Adjusts bright areas (-100 to +100) Num. Adjusts dark areas (-100 to +100) Num. Fine-tunes brightest parts (-100 to +100) Fine-tunes darkest parts (-100 to +100) Num. Enhances/smooths medium textures (-100 to +100) Num. Num. Enhances/reduces local mid-tone contrast (-100 to +100) Reduces/adds atmospheric haze (-100 to +100) Saturation of less-saturated colors (-100 to +100) Overall color intensity (-100 to +100) Relative temperature adjustment (-100 to +100) Relative tint adjustment (-100 to +100) Num. Num. Num. Num. Num. Predefined curve shape (Linear, Custom) Custom RGB tone curve points (x,y: 0-255) Custom Red channel tone curve points Custom Green channel tone curve points Custom Blue channel tone curve points Adjusts shadow tonal regions (-100 to +100) Adjusts dark tonal regions (-100 to +100) Adjusts light tonal regions (-100 to +100) Adjusts highlight tonal regions (-100 to +100) Boundary: shadows/darks (10-50) Boundary: darks/lights (25-75) Boundary: lights/highlights (50-90) Detail Sharpness SharpenRadius SharpenDetail SharpenEdgeMasking LuminanceSmoothing ColorNoiseReduction ColorNoiseReductionDetail ColorNoiseReductionSmoothnessSmoothness of color noise reduction (0-100) Enhances edge definition (0-150) Width of sharpening effect (0.5-3.0) Amount of sharpening for details (0-100) Masks sharpening to edges (0-100) Reduces luminance noise (0-100) Reduces color noise (0-100) Fine-tunes color noise reduction (0-100) HSL/Color (per color: Red, Orange, Yellow, Green, Aqua, Blue, Purple, Magenta) HueAdjustment<Color> SaturationAdjustment<Color> Adjusts saturation of specific color (-100 to +100) LuminanceAdjustment<Color> Adjusts brightness of specific color (-100 to +100) Shifts hue of specific color (-100 to +100) Num. Num. Num. Color Grading SplitToningShadowHue SplitToningHighlightHue Hue for shadows in split toning (0-359) Hue for highlights in split toning (0-359) Num. Num. Continued on next page 33 Str. Dict. Dict. Dict. Dict. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Table 6: Lightroom tools with functional description and parameter type. (Continued) Tool Name Functional Description SplitToningShadowSaturation SplitToningHighlightSaturation Saturation for highlights (0-100) SplitToningBalance Saturation for shadows (0-100)"
        },
        {
            "title": "ColorGradeMidtoneHue\nColorGradeMidtoneSat\nColorGradeMidtoneLum\nColorGradeShadowLum\nColorGradeHighlightLum\nColorGradeBlending\nColorGradeGlobalHue\nColorGradeGlobalSat\nColorGradeGlobalLum",
            "content": "Effects PostCropVignetteAmount GrainAmount ShadowTint Balance between shadow/highlight toning (-100 to +100) Midtone hue for color grading (0-359) Midtone saturation for color grading (0-100) Midtone luminance for color grading (0-100) Luminance for shadows (0-100) Luminance for highlights (0-100) Blending of color grading effect (0-100) Global hue adjustment (0-359) Global saturation adjustment (0-100) Global luminance adjustment (0-100) Darkens/lightens image corners (-100 to +100) Adds film grain effect (0-100) Adjusts color tint in shadows (-100 to +100) Camera Calibration (for Red, Green, Blue primary channels) <PrimaryColor>Hue <PrimaryColor>Saturation Shifts primary colors hue (-100 to +100) Adjusts primary colors saturation (-100 to +100) Lens Blur (Overall: Dict.) LensBlur.Active LensBlur.BlurAmount LensBlur.FocalRange LensBlur.BokehShape LensBlur.BokehShapeDetail LensBlur.HighlightsThreshold Brightness threshold for bokeh (0-100) LensBlur.HighlightsBoost LensBlur.CatEyeAmount LensBlur.CatEyeScale Enables/disables lens blur effect Strength of blur effect (0-100) Defines focal plane (\"x1 y1 x2 y2\") Bokeh shape identifier (default 0) Definition of bokeh shape edges (0-100) Enhances out-of-focus highlights (0-100) Simulates cats eye bokeh effect (0-100) Size of cats eye effect (0-100) Advanced Color Grading (PointColors - each point is Dict.) SrcHue SrcSat SrcLum HueShift SatScale LumScale RangeAmount HueRange Source hue for adjustment (0-6.28 rad) Source saturation for adjustment (0-1.0) Source luminance for adjustment (0-1.0) Hue shift amount (-1 to +1) Saturation scale (-1 to +1) Luminance scale (-1 to +1) Effect application amount (0-1.0) Falloff for hue adjustment (LowerNone, LowerFull, UpperFull, UpperNone: 0-1.0) Falloff for saturation adjustment (sub-props same as HueRange) Falloff for luminance adjustment (sub-props same as HueRange) SatRange LumRange Look (Overall: Dict.) Look.Name Look.Amount Look.Parameters Name of the look preset Intensity of the look effect (0.0-1.0) Dictionary of specific adjustments applied by the look Type Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Bool. Num. Str. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Dict. Dict. Dict. Str. Num. Dict. Continued on next page 34 Table 6: Lightroom tools with functional description and parameter type. (Continued) Tool Name Functional Description Type (e.g., ProcessVersion, ToneCurvePV2012, Parametric adjustments, SplitToning, ColorGrade, ConvertToGrayscale, LookTable, RGBTable, RGBTableAmount) Amount for the correction group (0-1, default 1) Activates the correction group Name for the correction group Local exposure adjustment (-1 to +1) Local contrast adjustment (-1 to +1) Local highlights adjustment (-1 to +1) Local shadows adjustment (-1 to +1) Local whites adjustment (-1 to +1) Local blacks adjustment (-1 to +1) Local clarity adjustment (-1 to +1) Localized Mask Adjustments (MaskGroupBasedCorrections - Array of Dicts.) Per Correction Group: CorrectionAmount CorrectionActive CorrectionName LocalExposure2012 LocalContrast2012 LocalHighlights2012 LocalShadows2012 LocalWhites2012 LocalBlacks2012 LocalClarity / LocalClarity2012 LocalDehaze LocalTexture LocalHue LocalSaturation LocalCurveRefineSaturation LocalToningHue LocalToningSaturation LocalTemperature LocalTint LocalLuminanceNoise LocalMoire LocalDefringe LocalGrain LocalSharpness <Channel>Curve Local dehaze adjustment (-1 to +1) Local texture adjustment (-1 to +1) Local hue adjustment (-1 to +1) Local saturation adjustment (-1 to +1) Local saturation curve refinement (0-100) Local toning hue (0-359) Local toning saturation (-1 to +1) Local temperature adjustment (-1 to +1) Local tint adjustment (-1 to +1) Local luminance noise reduction (-1 to +1) Local moire reduction (-1 to +1) Local defringe adjustment (-1 to +1) Local grain adjustment (-1 to +1) Local sharpness adjustment (-1 to +1) Local tone curve for Red, Green, Blue, or Main channels (points \"x,y\") Local specific color adjustments (dictionary of string-encoded points) Array of mask definitions for the group LocalPointColors CorrectionMasks Per Mask in CorrectionMasks: What MaskActive MaskName MaskBlendMode MaskInverted MaskValue MaskSubType ReferencePoint Gesture Top/Left/Bottom/Right Angle Midpoint Mask type (e.g., \"Mask/Image\", \"Mask/CircularGradient\") Activates this specific mask Name of the mask (e.g., \"Subject\", \"Sky\") Mask blending (0=Add, 1=Intersect) Inverts the mask area Mask opacity (0.0-1.0) AI Mask subtype (Subject, Sky, Person etc.) / Object type Center point for AI masks (\"x y\") Polygon points for object/region mask Coordinates for radial gradient (0-1) Rotation angle for radial gradient (0-360) Center point of radial gradient (0-100) Continued on next page Num. Bool. Str. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Num. Dict. Dict. Array Str. Bool. Str. Num. Bool. Num. Num. Str. Array Num. Num. Num. Table 6: Lightroom tools with functional description and parameter type. (Continued) Tool Name Functional Description"
        },
        {
            "title": "Feather\nFlipped\nMaskSubCategoryID",
            "content": "Edge feathering for radial gradient (0-100) Flips radial gradient direction Category ID for person parts mask (Face, Eyes, etc.) Type Num. Bool. Num."
        },
        {
            "title": "References",
            "content": "[1] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] T. Brooks, A. Holynski, and A. A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [3] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning photographic global tonal adjustment with database of input / output image pairs. In The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition, 2011. [4] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [5] L. Chen, L. Li, H. Zhao, Y. Song, and Vinci. R1-v: Reinforcing super generalization ability in visionlanguage models with less than $3. GitHub repository: https://github.com/Deep-Agent/R1-V, 2025. [6] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pages 1526, 2017. [7] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [8] T.-J. Fu, W. Hu, X. Du, W. Y. Wang, Y. Yang, and Z. Gan. Guiding instruction-based image editing via multimodal large language models. In International Conference on Learning Representations (ICLR), 2024. [9] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Z. Guo, L. Xia, Y. Yu, T. Ao, and C. Huang. Lightrag: Simple and fast retrieval-augmented generation. 2024. [11] N. Hansen. The cma evolution strategy: comparing review. Towards new evolutionary computation: Advances in the estimation of distribution algorithms, pages 75102, 2006. [12] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023. [13] Y. Hu, H. He, C. Xu, B. Wang, and S. Lin. Exposure: white-box photo post-processing framework. ACM Transactions on Graphics (TOG), 37(2):117, 2018. [14] R. Huang, M. Li, D. Yang, J. Shi, X. Chang, Z. Ye, Y. Wu, Z. Hong, J. Huang, J. Liu, et al. Audiogpt: In Proceedings of the AAAI Understanding and generating speech, music, sound, and talking head. Conference on Artificial Intelligence, pages 2380223804, 2024. [15] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Y. Hu, and S. Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [16] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [17] M. Hui, S. Yang, B. Zhao, Y. Shi, H. Wang, P. Wang, Y. Zhou, and C. Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. [18] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [20] F. Jiao, G. Guo, X. Zhang, N. F. Chen, S. Joty, and F. Wei. Preference optimization for reasoning with pseudo feedback. arXiv preprint arXiv:2411.16345, 2024. 37 [21] B. Jin, H. Zeng, Z. Yue, D. Wang, H. Zamani, and J. Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [22] Z. Ke, C. Sun, L. Zhu, K. Xu, and R. W. Lau. Harmonizer: Learning to perform white-box image and video harmonization. In European conference on computer vision, pages 690706. Springer, 2022. [23] S. Kosugi and T. Yamasaki. Unpaired image enhancement featuring reinforcement-learning-controlled image editing software. In Proceedings of the AAAI conference on artificial intelligence, pages 11296 11303, 2020. [24] M. Ku, D. Jiang, C. Wei, X. Yue, and W. Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023. [25] LangChain. Langchain: Build context-aware reasoning applications. https://github.com/ langchain-ai/langchain, 2023. [26] J. Liang, H. Zeng, M. Cui, X. Xie, and L. Zhang. Ppr10k: large-scale portrait photo retouching dataset with human-region mask and group-level consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 653661, 2021. [27] Y. Lin, Z. Lin, H. Chen, P. Pan, C. Li, S. Chen, W. Kairun, Y. Jin, W. Li, and X. Ding. Jarvisir: Elevating In Proceedings of the IEEE/CVF autonomous driving perception with intelligent image restoration. Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [28] S. Liu, Y. Han, P. Xing, F. Yin, R. Wang, W. Cheng, J. Liao, Y. Wang, H. Fu, C. Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [29] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [30] Z. Liu, T. Hoang, J. Zhang, M. Zhu, T. Lan, J. Tan, W. Yao, Z. Liu, Y. Feng, R. RN, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482, 2024. [31] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [32] Z. Lu, Y. Chai, Y. Guo, X. Yin, L. Liu, H. Wang, G. Xiong, and H. Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [33] C. Ma, Y. Jiang, J. Wu, J. Yang, X. Yu, Z. Yuan, B. Peng, and X. Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [34] F. Meng, L. Du, Z. Liu, Z. Zhou, Q. Lu, D. Fu, B. Shi, W. Wang, J. He, K. Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [35] A. Mosleh, A. Sharma, E. Onzon, F. Mannan, N. Robidoux, and F. Heide. Hardware-in-the-loop end-to-end optimization of camera image processing pipelines. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75297538, 2020. [36] J. Nishimura, T. Gerasimow, R. Sushma, A. Sutic, C.-T. Wu, and G. Michael. Automatic isp image quality tuning using nonlinear optimization. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 24712475. IEEE, 2018. [37] W. Ouyang, Y. Dong, X. Kang, P. Ren, X. Xu, and X. Xie. Rsfnet: white-box image retouching approach using region-specific color filters. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1216012169, 2023. [38] Z. Pan and H. Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse. arXiv preprint arXiv:2503.18470, 2025. [39] C. Qian, E. C. Acikgoz, Q. He, H. Wang, X. Chen, D. Hakkani-Tür, G. Tur, and H. Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [40] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [41] H. Shen, P. Liu, J. Li, C. Fang, Y. Ma, J. Liao, Q. Shen, Z. Zhang, K. Zhao, Q. Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [42] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [43] Significant-Gravitas. Autogpt. https://github.com/Significant-Gravitas/AutoGPT, 2023. [44] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [45] E. Tseng, F. Yu, Y. Yang, F. Mannan, K. S. Arnaud, D. Nowrouzezahrai, J.-F. Lalonde, and F. Heide. Hyperparameter optimization in black-box image processing using differentiable proxies. ACM Trans. Graph., 38(4):271, 2019. [46] E. Tseng, Y. Zhang, L. Jebe, X. Zhang, Z. Xia, Y. Fan, F. Heide, and J. Chen. Neural photo-finishing. ACM Trans. Graph., 41(6):2381, 2022. [47] J. Wu, Y. Wang, L. Li, F. Zhang, and T. Xue. Goal conditioned reinforcement learning for photo finishing tuning. Advances in Neural Information Processing Systems, 37:4629446318, 2024. [48] X. Xia and R. Luo. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [49] S. Xiao, Y. Wang, J. Zhou, H. Yuan, X. Xing, R. Yan, S. Wang, T. Huang, and Z. Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [50] T. Xie, F. Zhou, Z. Cheng, P. Shi, L. Weng, Y. Liu, T. J. Hua, J. Zhao, Q. Liu, C. Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023. [51] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. technical report. arXiv preprint arXiv:2412.15115, 2024. [52] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [53] Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, et al. R1onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [54] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [55] H. Ying, S. Zhang, L. Li, Z. Zhou, Y. Shao, Z. Fei, Y. Ma, J. Hong, K. Liu, Z. Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. [56] K. Yu, Z. Li, Y. Peng, C. C. Loy, and J. Gu. Reconfigisp: Reconfigurable camera image processing pipeline. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42484257, 2021. [57] H. Zeng, J. Cai, L. Li, Z. Cao, and L. Zhang. Learning image-adaptive 3d lookup tables for high IEEE Transactions on Pattern Analysis and Machine performance photo enhancement in real-time. Intelligence, 44(4):20582073, 2020. [58] K. Zhang, G. Li, Y. Dong, J. Xu, J. Zhang, J. Su, Y. Liu, and Z. Jin. Codedpo: Aligning code models with self generated and verified source code. arXiv preprint arXiv:2410.05605, 2024. [59] K. Zhang, L. Mo, W. Chen, H. Sun, and Y. Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [60] X. Zhang, B. A. Wandell, et al. spatial extension of cielab for digital color image reproduction. In SID international symposium digest of technical papers, volume 27, pages 731734. Citeseer, 1996. [61] Y. Zhang, S. Wu, Y. Yang, J. Shu, J. Xiao, C. Kong, and J. Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. [62] Z. Zhang, X. Bo, C. Ma, R. Li, X. Chen, Q. Dai, J. Zhu, Z. Dong, and J.-R. Wen. survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024. 39 [63] H. Zhao, X. S. Ma, L. Chen, S. Si, R. Wu, K. An, P. Yu, M. Zhang, Q. Li, and B. Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [64] Y. Zhao, F. Xue, S. Reed, L. Fan, Y. Zhu, J. Kautz, Z. Yu, P. Krähenbühl, and D.-A. Huang. Qlip: Text-aligned visual tokenization unifies auto-regressive multimodal understanding and generation. arXiv preprint arXiv:2502.05178, 2025. [65] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. [66] X. Zhuang, Y. Xie, Y. Deng, D. Yang, L. Liang, J. Ru, Y. Yin, and Y. Zou. Vargpt-v1. 1: Improve visual autoregressive large unified model via iterative instruction tuning and reinforcement learning. arXiv preprint arXiv:2504.02949, 2025."
        }
    ],
    "affiliations": [
        "Bytedance",
        "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen, Fujian, China",
        "National University of Singapore",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Tsinghua University"
    ]
}