{
    "paper_title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
    "authors": [
        "Yuchen Yan",
        "Jin Jiang",
        "Zhenbang Ren",
        "Yijun Li",
        "Xudong Cai",
        "Yang Liu",
        "Xin Xu",
        "Mengdi Zhang",
        "Jian Shao",
        "Yongliang Shen",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks."
        },
        {
            "title": "Start",
            "content": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models Yuchen Yan1,2,*, Jin Jiang2,3, Zhenbang Ren1,4, Yijun Li1, Xudong Cai1,5, Yang Liu2, Xin Xu6, Mengdi Zhang2, Jian Shao1,, Yongliang Shen1,, Jun Xiao1, Yueting Zhuang1 1Zhejiang University 2Meituan Group 4University of Electronic Science and Technology of China 5Beijing University of Posts and Telecommunications 6The Hong Kong University of Science and Technology {yanyuchen, syl, jshao}@zju.edu.cn 3Peking University 5 2 0 2 1 ] . [ 1 1 0 8 5 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have demonstrated remarkable performance in complex reasoning tasks. critical component of their training is the incorporation of reference-based reward systems within reinforcement learning (RL), where model outputs are evaluated against ground truth references. However, existing reward benchmarks focus on preference comparisons between responses rather than evaluating verification against ground truth references, leaving critical gap in our ability to evaluate verification systems used in reasoning model training. In this paper, we introduce VerifyBench and its challenging variant VerifyBench-Hard, two benchmarks specifically designed to assess reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Our comprehensive evaluation reveals that while larger model-based verifiers show promise on standard cases, all current systems demonstrate substantial room for improvement on challenging instances. Through systematic analysis of performance patterns across reasoning tasks and error categories, we provide insights for advancing reference-based reward systems. These benchmarks establish standardized framework for improving verification accuracy, ultimately enhancing reasoning capabilities in models trained via RL. Benchmark ZJU-REAL/VerifyBench"
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) have exhibited remarkable capabilities, significantly assisting humans across diverse practical domains (DeepSeek-AI et al., 2025b; Grattafiori et al., 2024; Yang et al., 2025). Reinforcement learning *Contribution during internship at Meituan Group. Corresponding author. 1 Figure 1: The core distinction between VerifyBench and existing reward benchmarks (Lambert et al., 2024; Liu et al., 2024) is illustrated as follows. Upper panel: Existing reward benchmarks assess the accuracy of reward system by comparing the ranking of two completions for the same question. Lower panel: In contrast, our proposed VerifyBench evaluates the accuracy of reward system by determining the correctness of single completion using reference answer. from human feedback (RLHF) has been crucial to this progress, with reward models playing central role by evaluating and scoring model-generated responses to guide training. This approach has led to the development of numerous benchmarks (Lambert et al., 2024; Liu et al., 2024; Zhou et al., 2025) for systematic reward model evaluation, focusing primarily on pairwise preference judgments between competing responses. The emergence of specialized large reasoning models (LRMs) (DeepSeek-AI et al., 2025a; Qwen Team, 2024; Kimi Team et al., 2025) such as OpenAIs o1(OpenAI, 2024) and DeepSeekR1(DeepSeek-AI et al., 2025a) has fundamentally changed this landscape. These models achieve unprecedented performance on reasoning tasks through specialized reinforcement learning techniques that differ from standard RLHF approaches. key distinction in training methodologies for LRMs is their reliance on reference-based reward systems, where rewards are assigned based on alignment between model-generated responses and authoritative reference answers. This approach has been implemented variously across leading models, with Deepseek-R1 (DeepSeek-AI et al., 2025a) employs rule-based reward to prevent reward hacking, whereas models like Seed1.5-Thinking (Seed et al., 2025) adopt model-based reward systems to generate more precise and robust signals. Despite the widespread adoption of referencebased reward systems in training state-of-the-art reasoning models, significant gap exists in our ability to evaluate these systems systematically. Current benchmarks focus almost exclusively on preference-based evaluation, assessing rewards on their ability to rank competing responses correctly. This approach fails to capture the requirements of reference-based verification, where responses must be judged against objective ground truths rather than relative preferences. The absence of dedicated benchmarks for reference-based reward systems has limited researchers ability to assess, compare, and improve their verification methodologies effectively, potentially impeding progress in reasoning model development. To address this critical gap, we introduce VerifyBench, benchmark specifically designed to evaluate the accuracy of reference-based reward systems. VerifyBench differs fundamentally from existing reward benchmarks by focusing on absolute correctness judgments rather than relative preference assessments. While traditional benchmarks ask reward models to determine which of two responses is better, VerifyBench challenges systems to verify whether single response correctly aligns with reference answer, more accurately reflecting the actual use case in reasoning model training. In this paper, we present VerifyBench, benchmark specifically designed to evaluate the accuracy of reference-based reward systems. To create VerifyBench, we curated diverse collection of instructions paired with reference answers sourced from existing open datasets. Responses to these instructions were generated by multiple open-source and proprietary LLMs. The correctness of each response was assessed using both automated model judgments and human evaluations. Each instance in VerifyBench was verified by at least two human annotators to ensure label consistency and reliability, thereby producing high-quality benchmark for the evaluation of reward systems. Recognizing the need to differentiate between various verification techniques and to push the boundaries of current capabilities, we further developed VerifyBench-Hard, more challenging variant of our benchmark. This dataset focuses on contentious cases where leading models produce highly conflicting judgments, providing more stringent test for reward system accuracy. VerifyBench-Hard samples were carefully selected based on disagreement patterns among high-performing models, then subjected to thorough human annotation to ensure label quality. Our contributions are summarized as follows: 1. To better reflect realistic reinforcement learning (RL) scenarios for reasoning models, we construct VerifyBench, benchmark derived from existing models and datasets, to provide an objective evaluation of the accuracy of reference-based reward systems. 2. We further develop VerifyBench-Hard, more challenging benchmark curated from cases exhibiting high disagreement among multiple models. This dataset contains larger proportion of difficult-to-verify samples, highlighting substantial potential for improvement in current models. 3. We conduct comprehensive empirical analysis of model performance on both VerifyBench and VerifyBench-Hard, offering actionable insights to advance the accuracy of reference-based reward systems and enhance RL training in reasoning tasks."
        },
        {
            "title": "2 Preliminaries",
            "content": "Reference-free Reward Models In reinforcement learning (RL) for large language models (LLMs), the reward model plays crucial role by approximating real-world reward signals associated with model-generated outputs. typical reward model takes as input users query along with the corresponding LLM-generated response r, and produces reward signal, formally defined as: = Rφ(q, r) (1) where represents the users query, denotes the response generated by the LLM, and φ encapsulates either the learned parameters of the reward model or the heuristic criteria used to evaluate the quality of the response given and r. 2 Evaluation of Reference-free Reward Models Generally, reward models produce scalar outputs whose scales can vary significantly across different implementations, complicating direct numerical comparisons. Consequently, current benchmarks evaluate reward models using pairwise comparative approach. Formally, given dataset comprising tuples (q, rw, rl), where represents users query, and rw and rl denote two candidate responses with rw considered superior to rl, the accuracy of reward model is quantified as the proportion of instances in which the model correctly assigns higher score to rw than to rl. Mathematically, this accuracy metric is defined as: Acc = 1 (cid:88) (q,rw,rl)D [Rφ(q, rw) > Rφ(q, rl)] (2) where I() is the indicator function, and Rφ denotes the reward model parameterized by φ. Reference-based Reward Models With the emergence of advanced reasoning models such as DeepSeek-R1, reference-based reward systems have been integrated into reinforcement learning (RL) frameworks for large reasoning models (LRMs). These models require training on extensive datasets, which typically include authoritative reference answers. Consequently, the reward assignment task shifts towards evaluating the alignment between the model-generated outputs and their corresponding reference answers. Formally, this reward calculation can be expressed as: = Rφ(q, gt, r) (3) where denotes the user-issued query, gt denotes the ground-truth reference answer, represents the model-generated response, and φ encapsulates either the learned parameters of the reward model or the established evaluation criteria used to assess the alignment among q, gt, and r. Evaluation of Reference-based Reward Models In this paper, we propose reference-based reward benchmark designed to systematically evaluate reward models within reinforcement learning (RL) frameworks for large reasoning models (LRMs). Unlike traditional reward evaluation benchmarks, which rely on pairwise comparisons, our approach leverages explicit reference answers to directly assess the correctness of individual model-generated responses. Concretely, given dataset consisting of instances (q, gt, r, y), where denotes the userissued query, gt represents the ground-truth reference answer, is the model-generated response, and is the binary correctness label assigned to the response, we evaluate the reward model by measuring its accuracy in correctly predicting these labels. Formally, the accuracy metric is defined as: Acc = 1 (cid:88) [E(Rφ(q, gt, r)) = y] (q,gt,r,y)D (4) where Rφ(q, gt, r) denotes the reward model parameterized by φ or defined by heuristic verification rules, producing predictions indicative of the correctness of response relative to the provided reference answer gt. The function E() represents an operation (e.g., thresholding or discretization) mapping continuous reward scores into discrete correctness predictions suitable for direct comparison with the ground-truth labels y."
        },
        {
            "title": "3 Benchmark Construction",
            "content": "In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, to evaluate reference-based reward systems. The VerifyBench benchmark is designed to reflect naturally distributed data, whereas VerifyBench-Hard comprises samples exhibiting high levels of disagreement among models, thereby assessing models ability to provide reliable judgments in ambiguous or challenging scenarios. 3.1 Construction of VerifyBench Query Curation To emulate realistic reinforcement learning (RL) scenarios involving referencebased reward systems, we curate comprehensive collection of open-source reasoning problems paired with corresponding reference answers. These problems encompass three primary categories, general reasoning, logical reasoning and mathematical reasoning, and are aggregated from 41 distinct sources. complete list of these data sources is provided in Appendix B. Answer Type Labeling To comprehensively evaluate model performance across diverse answer formats, we define four canonical answer types: numerical values, algebraic expressions, multiple-choice selections, and free-form strings. Utilizing general-purpose LLM Llama3.3-70B-Instruct (Grattafiori et al., 2024), we performed automatic answer-type classification with 3 Figure 2: Overview of the benchmark construction process. The upper section outlines the pipeline used to construct VerifyBench, whereas the lower section details the pipeline for VerifyBench-Hard. The components highlighted by black boxes denote the final entries included in the benchmark. prompt(Appendix C.1). Questions that fall outside these categories, such as proof-based or openended prompts, were excluded from further analysis. Following classification, we randomly sampled 2,000 instances per answer type, resulting in final candidate pool of 8,000 questions. Completion Generation and Pre-annotation We subsequently employed 22 widely used opensource and proprietary models (see Appendix E) to generate single-shot completions for the curated set of 8,000 questions, resulting in total of 176,000 completions. To assign initial correctness labels, we utilized Llama-3.3-70B-Instruct (Grattafiori et al., 2024) within prompt-based judgment framework. For each question, we randomly selected four completions, two labeled as correct and two labeled as incorrect by the model, and retained them for subsequent human annotation. Human Annotation We conducted human annotation for the aforementioned questions and their associated completions. The annotation procedure comprised two primary tasks: (1) identifying the most appropriate answer type for each question based on its formulation and corresponding groundtruth answer, and (2) evaluating the correctness of each of the four completions. Each question was independently annotated by at least two annotators. If their annotations were consistent, the labeling was finalized; otherwise, third annotator resolved disagreements to ensure consistency and finalize the labels. Benchmark Construction Following human annotation, we identified notable biases in the models predictions regarding both answer types and completion correctness, leading to imbalanced data distributions. To mitigate this issue, we performed controlled downsampling to ensure uniform category-level representation and balanced correctness labels. Specifically, we retained 250 questions per answer type, resulting in total of 1,000 questions. Each question is paired with exactly two completions, one correct and one incorrect. The resulting dataset, VerifyBench, thus comprises 2,000 well-balanced question-answer-completioncorrectness tuples. Detailed statistics for VerifyBench are provided in Table 1. 3.2 Construction of VerifyBench-Hard To construct VerifyBench-Hard, we employed specialized data generation pipeline consisting of the following key steps: Completion Generation To construct the dataset, we first generated single-turn completions for the queries described in Section 3.1 using 4 Statistics for VerifyBench and VerifyBench-Hard Statistics VerifyBench VerifyBench-Hard # of unique questions # of unique completions # of correct completions # of wrong completions 1000 2000 1000 1000 Statistics of Answer Type # of Numeric Values # of Expressions # of Multi-choice # of String 500 500 500 500 Statistics of Domain # of General Reasoing # of Logic Reasoning # of Math Reasoning 404 498 1098 945 1000 291 709 252 88 430 230 303 315 382 Table 1: Benchmark statistics of VerifyBench and VerifyBench-Hard. collection of 18 open-source models. Due to the substantial volume of generations and the associated computational costs, closed-source models were excluded from this stage. In total, we produced approximately 1.45 million completions. Difficulty Filtering Next, we employed five top-performing large models on VerifyBench (Llama-3.3-70B-Instruct (Grattafiori et al., 2024), Llama-4-Scout-17B-16E-Instruct (Meta AI, 2025), Qwen2.5-72B-Instruct (Qwen et al., 2025), Qwen330B-A3B, and Qwen3-32B (Yang et al., 2025)), which span diverse range of model sizes and architectures, to evaluate the correctness of the generated completions. Based on their judgments, we identified question-answer-completion tuples exhibiting model disagreement, specifically those for which two models assessments diverged from the other three. To ensure balanced and comprehensive representation, we applied stratified sampling across data domains and sources, ultimately selecting 2,000 examples for human annotation. Human Annotation We subsequently subjected the selected samples to human annotation, focusing on two key aspects: identifying the answer type and determining the correctness of each completion. Each instance was annotated independently by at least two annotators. In cases where both annotators agreed, the annotation was finalized; when disagreement occurred, third annotator was consulted to resolve the conflict. Benchmark Construction Following human annotation, we excluded samples identified as unsuitable for inclusion in our benchmark. This filtering resulted in final set of 1,000 question-answercompletion-correctness tuples. In contrast to VerifyBench, which enforces balanced structure with one correct and one incorrect completion per question, VerifyBench-Hard is derived through natural sampling. We observed that larger models are more likely to erroneously accept incorrect answers as correct, resulting in natural skew towards incorrect completions within the dataset. Detailed statistics for VerifyBench-Hard are provided in Table 1."
        },
        {
            "title": "4 Evaluation Results",
            "content": "This section presents the evaluation results and analyses of our proposed benchmark. Section 4.1 reports the primary evaluation outcomes. In Section 4.2, we investigate the impact of reference answers on the verification process. Section 4.3 provides comparative analysis between our benchmark and existing reward benchmarks, as well as the performance of several general-purpose reward models on VerifyBench and VerifyBench-Hard. 4.1 Overall Performance We evaluate the performance of various verification approaches on both VerifyBench and VerifyBenchHard. For rule-based baselines, we adopt the widely used math-verify (Kydlíˇcek, 2025) method. In the LLM-as-a-judge setting, we prompt LLMs to perform verification; detailed prompt templates are provided in Appendix C.2. Our evaluation yields several key findings and insights: Existing models perform well on VerifyBench: The primary objective in constructing VerifyBench is to establish benchmark for the objective evaluation of reference-based reward systems. To this end, we designed the dataset with balanced distribution across diverse domains and answer types, pairing each question with both correct and an incorrect completion. This structure facilitates rigorous and fair assessment of reward model performance. Notably, state-of-the-art LLMs already demonstrate strong performance on this benchmark: GPT-4o-mini achieves an average accuracy of 92.85%, while Qwen3-32B reaches 95.8%, highlighting the high reliability of LLMs as verifiers in this context. VerifyBench-Hard is challenging: To more effectively differentiate the performance of various models, we constructed VerifyBench-Hard by selecting cases in which multiple LLMs exhibited Model/Method VerifyBench VerifyBench-Hard Num Exp MC Str AVG Num Exp MC Str AVG rule-based functions math-verify 83.60 72. 19.40 8.60 45.90 76.19 82.95 8. 10.43 32.50 OpenAI/gpt-4o-2024-11-20 OpenAI/gpt-4o-mini meta-llama/Llama-4-Scout-17B-16E-Instruct meta-llama/Llama-3.3-70B-Instruct meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.2-1B-Instruct Qwen/Qwen3-235B-A22B Qwen/Qwen3-30B-A3B Qwen/Qwen2.5-72B-Instruct Qwen/Qwen3-32B Qwen/Qwen3-8B Qwen/Qwen3-4B Qwen/Qwen3-1.7B microsoft/phi-4 01-ai/Yi-1.5-9B-Chat-16K google/gemma-3-1b-it 94.80 95.80 94.20 88.80 72.20 65.80 44.40 96.40 96.60 95.40 97.60 96.40 95.20 83.20 92.60 90.40 55.40 LLM-as-a-judge 90.20 89.80 86.80 77.80 70.60 63.60 41.00 92.40 91.80 89.80 94.00 93.00 91.60 81.00 86.40 87.40 56. 96.80 95.80 89.80 88.40 77.00 56.80 37.60 97.00 97.40 95.60 99.00 96.20 93.60 80.60 93.00 88.00 43.00 90.80 90.00 89.25 78.00 72.40 57.60 53.60 89.40 90.20 88.60 92.60 90.40 87.60 79.60 85.40 85.00 56.00 93.15 92.85 90.01 83.25 73.05 60.95 44.15 93.80 94.00 92.35 95.80 94.00 92.00 81.10 89.35 87.70 52.65 71.43 69.05 48.02 54.37 51.19 33.33 22.22 70.24 64.68 70.63 69.05 68.65 71.03 48.81 59.52 65.48 32.14 65.91 72.73 39.77 45.45 35.23 28.41 13.64 72.73 70.45 60.23 81.82 78.41 62.50 38.64 57.95 63.64 19.32 75.35 74.19 46.98 60.70 45.12 38.84 29.07 70.93 69.53 61.40 68.14 73.02 75.58 60.93 54.19 62.09 33. 71.30 72.17 55.22 47.39 33.91 27.39 27.39 69.57 56.52 56.09 77.83 66.52 71.74 41.74 57.39 54.78 40.87 72.60 72.30 48.50 54.70 43.20 33.90 25.60 70.60 65.40 62.40 71.80 70.90 72.40 51.50 56.60 61.40 33.70 Table 2: Overall performance(%) of VerifyBench and VerifyBench-Hard. Num stands for Numeric Values, Exp stands for Expressions, MC stands for Multi-choice and Str stands for String. substantial disagreement in their verification outputs. Evaluation results demonstrate that model performance on VerifyBench-Hard is significantly lower than on VerifyBench. The highest accuracy achieved is 72.4%, representing 20% decrease compared to performance on VerifyBench. This performance gap underscores substantial opportunities for improvement in the precise verification capabilities of current LLMs. Small-scale models still have potential for development: In practical reinforcement learning scenarios, the inference efficiency of the reward system significantly impacts the overall training speed. Since such verification tasks typically involve generative inference, their computational cost is comparable to that of the rollout process itself. Thus, efficiently leveraging smaller models to perform verification is practical concern worth exploring. According to our results, models with smaller parameters (<3B parameters) exhibit notably poorer performance on VerifyBench, achieving 81.10% accuracy with Qwen3-1.7B and only 60.95% accuracy with Llama-3.2-3B-Instruct, while larger-scale models can achieve over 90% accuracy. Therefore, enhancing the capability of smaller models on these verification tasks represents valuable direction for future research. Model Llama-4-Scout-17B-16E-Instruct Llama-3.3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Llama-3.2-1B-Instruct Qwen3-235B-A22B Qwen3-30B-A3B Qwen2.5-72B-Instruct Qwen3-32B Qwen3-8B Qwen3-4B Qwen3-1.7B VerifyBench w/ Ref w/o Ref 90.01 83.25 73.05 60.95 44.15 93.80 94.00 92.35 95.80 94.00 92.00 81.10 73.95-16.06 75.00-8.25 64.10-8.95 55.35-5.60 44.50+0.35 80.15-13.65 78.25-15.75 77.30-15.05 78.90-16.90 75.75-18.25 74.40-17.60 62.10-19.00 Table 3: Evaluation results(%) about how including the reference answer in the prompt influences the performance of LLM-as-a-judge. 4.2 Reference-answers Play an Important Role in Verification The benchmark proposed in this work fundamentally differs from existing reward benchmarks by explicitly incorporating reference answers, thereby aligning more closely with the training setups of contemporary reasoning LLMs. To isolate the impact of reference answers on verification performance, we conduct an ablation study in which models are evaluated without reference answers provided in the prompt; the prompt format used is detailed in Appendix C.3. 6 Model RM-Bench Reward Bench VerifyBench Num Exp MC Str AVG General Reward Models Skywork/Skywork-Reward-Llama-3.1-8B internlm/internlm2-20b-reward Ray2333/GRM-llama3-8B-sftreg internlm/internlm2-7b-reward 72.29 72.06 71.33 72.42 93.33 92.16 88.50 90.02 60.80 65.60 64.80 73.20 64.80 64.80 58.40 68.00 59.60 61.20 58.80 66.80 68.80 70.00 67.60 70. 63.48 65.40 62.40 69.60 Domain-specific Reward Models Qwen/Qwen2.5-Math-RM-72B Qwen/Qwen2-Math-RM-72B 76.28 62.61 82.11 75.54 83.60 79. 79.20 78.40 73.60 73.20 75.60 72.80 78.00 75.90 Table 4: The performance(%) of existing reward models on VerifyBench without access to reference answers, as well as comparison with existing reward benchmarks. Experimental results, summarized in Table 3, reveal performance degradation of approximately 518% when reference answers are excluded. These findings underscore the crucial role of reference answers in reasoning-oriented RL, suggesting they provide more reliable and informative supervision signal during reward modeling. 4.3 Performance of Reference-free Reward Models To enable more comprehensive evaluation of existing reward models, we additionally assessed several reference-free reward models and benchmarked their performance on conventional pairwise reward evaluation datasets for comparison. Notably, each question in our proposed VerifyBench consists of one correct and one incorrect completion, enabling straightforward reformulation into standard pairwise evaluation instances. The experimental results are summarized in Table 4. Our experimental results show that VerifyBench introduces level of challenge comparable to existing reward benchmarks, with the absence of reference answers. Reference-free reward models achieve sub-80% accuracy on VerifyBench, highlighting its difficulty. Furthermore, domain-specific reward models exhibit inferior performance on general reward benchmarks compared to VerifyBench, validating the benchmarks design objectives."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Error Analysis To gain deeper insights from VerifyBench, we introduce more fine-grained taxonomy for each answer type and analyze model performance across these subcategories. This detailed analysis helps identify specific reasoning tasks or answer formats where models are particularly error-prone. We subdivide the Numeric Values category into 8 subtypes, Expressions into 5 subtypes, Multi-choice into 3 subtypes, and String into 2 subtypes. Table 5 presents the comparative performance of different models across these detailed categories. We further analyze subcategories within each major category that exhibit below-average accuracy. The following error-prone subtypes are identified as the most frequent sources of incorrect judgments: Numeric Values: Complex numbers and answers containing multiple numerical values; Expressions: Algebraic formulas and equations; Multi-choice: Multi-answer choice problems; String: Strings requiring semantic consistency verification. We analyzed the samples most prone to errors and identified common underlying issue: models frequently fail to fully comprehend the question or clearly recognize the intended objective. For instance, in cases involving multi-value answers, the ordering of values is typically irrelevant. However, if the sequence of values in the models output differs from the golden answer, models often incorrectly classify the response as erroneous. Similarly, errors within the Expressions category, particularly involving algebraic formulas and equations, predominantly result from inadequate mathematical comprehension. Specifically, when model outputs an unsimplified expression, superficial textual discrepancies compared to the ground-truth answer can be significant. Rather than evaluating whether the expression is mathematically equivalent upon Figure 3: The performance(%) of RFT across different LLM judges which have various performance on VerifyBench. Answer Type Q32B g4o L70B L3B Numeric Values Integer Constant Float Number Radical Complex Number Angle Non-decimal number 100 Multiple Values 94.80 97.60 96.88-0.72 96.88 96.88-0.72 95.31 96.77 98.39 98.39 95.16 96.77-0.83 96.77 96.77-0.83 96.77 65.80 65.62-0.18 70.31 61.29-4.51 88.80 93.75 92.19 90.32 87.10-1.70 75.81 85.48-3.32 59.68-6.12 93.55 66.13 93.55-1.25 88.71-0.09 64.52-1.28 96.77-0.83 87.10-7.70 79.03-9.77 62.90-2. 63.60 90.20 77.80 94.00 91.54-2.46 84.62-5.58 67.69-10.11 56.92-6.68 87.50-6.5 78.12-12.08 70.31-7.49 60.94-2.66 60.94-2.66 96.09 60.00-3.60 98.00 75.78 96.09 82.81 78.00 86.72 94.53 98.00 94. Expressions Algebraic formula Equation Interval Set Matrix Multi-choice Single-choice Multiple-choice Finite state selection 99.40 99.00 99.39 98.21-0.79 94.05-2.75 77.98-10.42 49.40-7.40 88.40 92. 96.80 98.17 56.80 59.15 98.21 95.24 61.90 String Specific Semantic 92.60 93.60 90.38-2.22 85.26-5.54 69.87-8.13 54.49-3.11 78.00 81.69 90.80 93.31 57.6 59.01 Table 5: Model performance(%) across the fine-grained taxonomy on VerifyBench. Q32B stands for Qwen332B, g4o stands for gpt-4o-2024-11-20, L70B stands for Llama-3.3-70B-Instruct and L3B stands for Llama3.2-3B-Instruct. simplification, models prematurely deem the output incorrect, thereby leading to verification failures. 5.2 Correlation Analysis We constructed VerifyBench and VerifyBenchHard with the goal of improving the effectiveness of RL for reasoning models by enhancing the accuracy of reference-based reward systems. To evaluate the practical utility of our benchmark, we performed correlation analysis between VerifyBench and real-world RL performance. In our experiments, we applied rejection sampling to implement reference-based reward systems. For each question in the GSM8K and MATH 8 training sets, we generated 64 candidate completions using Qwen2.5-Math-7B-Instruct (Yang et al., 2024) with sampling temperature of 0.7. These responses were subsequently filtered by three verifier models with varying performance levels on VerifyBench: Llama-3.1-8B-Instruct, Qwen3-4B, and Qwen3-1.7B. Only completions consistently verified as correct were retained to form the SFT training data. We conducted independent SFT training runs accordingly, with full hyperparameter configurations provided in the Appendix D. reasoning et The resulting models were evaluated on mulbenchmarks, tiple mathematical including GSM8K (Cobbe 2021), al., MATH500 (Hendrycks et al., 2021; Lightman et al., 2023), and SVAMP (Patel et al., 2021). As shown in Figure 3, on GSM8K, MATH500, and SVAMP, using Qwen3-4B, verifier that achieves higher accuracy on VerifyBench, consistently loweroutperforms Llama-3.1-8B-Instruct, scoring verifier, under the same training steps. This highlights the strong alignment between VerifyBench and practical usage. Our benchmarks serve as reliable tools to guide the development of reward systems, leading to more effective training and improved model performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present two dedicated benchmarks, VerifyBench and VerifyBench-Hard, to evaluate reference-based reward systems in the context of reasoning-focused reinforcement learning. These benchmarks were built with high-quality, carefully curated data and extensive human annotation. Our results reveal that current verifiers, especially those with smaller model sizes, still face considerable challenges in accurately assessing reasoning completions. Through detailed analysis, we provide insights into the strengths and weaknesses of existing systems and highlight opportunities for improvement. The proposed benchmarks fill critical gap in the evaluation landscape, offering principled foundation for understanding verifier accuracy and guiding the development of more effective reasoning models trained via reinforcement learning."
        },
        {
            "title": "Limitations",
            "content": "Limited Data Domain In this paper, we utilize datasets exclusively from general reasoning, logical reasoning, and mathematical reasoning, which do not cover the full spectrum of reasoning types, such as commonsense reasoning. Consequently, our test sets may not adequately evaluate the quality of reward systems in out-of-domain scenarios. Bias from Human Annotation The construction of VerifyBench and VerifyBench-Hard involved extensive human annotation. Although all annotators were trained and double-checking strategy was employed, it remains challenging to entirely eliminate annotation bias inherent in manual labeling processes. Reward Hacking Could Not Be Identified While our experiments demonstrate that rule-based reward systems perform worse than model-based approaches on both VerifyBench and VerifyBenchHard, critical issue remains unaddressed: reward hacking. Future research should focus on detecting and evaluating reward hacking phenomena. Proof Problems Excluded During annotation, our guidelines explicitly excluded proof-based questions. We believe such problems require more specialized verification methods, such as formal languages like Lean4. Consequently, proof questions are not included in this study, and their verification remains an open research challenge. Binary Scoring System The benchmark constructed in this paper employs binary scoring system, where each completion is labeled as either correct or incorrect. However, real-world scenarios often involve more nuanced cases, such as partially correct reasoning processes or correct solutions to subproblems. Introducing more fine-grained evaluation scheme could better capture these complexities."
        },
        {
            "title": "Ethical Considerations",
            "content": "All human annotators involved in constructing the benchmarks were assigned reasonable workloads and fairly compensated for their contributions. Our annotation process involves minimal subjective preference. Human annotators performed the verification tasks following our detailed instructions. The content of the annotations does not involve ethical issues and poses no ethical risks."
        },
        {
            "title": "References",
            "content": "01 AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, et al. 2025. Yi: Open foundation models by 01.ai. arXiv preprint. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, et al. 2020. Abductive commonsense reasoning. arXiv preprint. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. Deep reinforcement learning from human preferences. arXiv preprint. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, et al. 2021. Training verifiers to solve math word problems. arXiv preprint. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, et al. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, et al. 2025b. Deepseek-v3 technical report. arXiv preprint. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. 2024. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. arXiv preprint. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopoulos, et al. 2024. How to evaluate reward models for rlhf. arXiv preprint. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, et al. 2025. Gemma 3 technical report. arXiv preprint. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, et al. 2024. The llama 3 herd of models. arXiv preprint. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, et al. 2024. Folio: Natural language reasoning with first-order logic. arXiv preprint. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, et al. 2025. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, et al. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, et al. 2025. Big-bench extra hard. arXiv preprint. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. 2023. Boardgameqa: dataset for natural language reasoning with contradictory information. arXiv preprint. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, et al. 2025. Process reward models that think. arXiv preprint. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, et al. 2025. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, et al. 2023. Efficient memory management for large language model serving with pagedattention. arXiv preprint. Hynek Kydlíˇcek. 2025. Math-verify: Math verification library. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, L. J. Miranda, Bill Yuchen Lin, et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, et al. 2024. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, et al. 2023. Lets verify step by step. arXiv preprint. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, et al. 2025. Inference-time scaling for generalist reward modeling. arXiv preprint. Meta AI. 2025. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodalintelligence/. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, et al. 2022. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial nli: new benchmark for natural language understanding. arXiv preprint. OpenAI. 2024. openai https://openai.com/index/introducing-openaio1-preview/. Introducing o1. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, et al. 2024. Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. arXiv preprint. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint. Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, and Chitta Baral. 2024. Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. arXiv preprint. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. 2025. Qwen2.5 technical report. arXiv preprint. Qwen Team. 2024. the on https://qwenlm.github.io/blog/qwq-32b-preview/. boundaries Qwq: of Reflect deeply unknown. the Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, et al. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint. 10 Abulhair Saparov and He He. 2023. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. arXiv preprint. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, et al. 2025. minimalist approach to llm reasoning: From rejection sampling to reinforce. arXiv preprint. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. arXiv preprint. ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, et al. 2025. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, et al. 2024. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. Clutrr: diagnostic benchmark for inductive reasoning from text. arXiv preprint. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, et al. 2022. Learning to summarize from human feedback. arXiv preprint. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint. Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. 2021. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, et al. 2023. Self-consistency improves chain of thought reasoning in language models. arXiv preprint. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, et al. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, et al. 2023. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: set of prerequisite toy tasks. arXiv preprint. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, et al. 2025. Tablebench: comprehensive and complex benchmark for table question answering. arXiv preprint. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2025a. Are large language models really good logical reasoners? comprehensive evaluation and beyond. IEEE Transactions on Knowledge and Data Engineering, 37(4):16201634. Yuhui Xu, Hanze Dong, Lei Wang, Caiming Xiong, and Junnan Li. 2025b. Reward models identify consistency, not causality. arXiv preprint. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. 2025. Qwen3 technical report. arXiv preprint. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, et al. 2024. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint. Nathan Young, Qiming Bao, Joshua Bensemann, and Michael Witbrock. 2022. Abductionrules: Training transformers to explain unexpected inputs. arXiv preprint. Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: reading comprehension dataset requiring logical reasoning. arXiv preprint. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, et al. 2025. Naturalreasoning: Reasoning in the wild with 2.8m challenging questions. arXiv preprint. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large language models perform in arithmetic tasks? arXiv preprint. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. 2024. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint. Zhang, Yifan. 2024. Stackmathqa: curated collection of 2 million mathematical questions sourced from stack exchange. https://huggingface.co/datasets/mathai/StackMathQA. and answers Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, et al. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint. Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, et al. 2021. Ar-lsat: Investigating analytical reasoning of text. arXiv preprint. 11 Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, et al. 2025. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint."
        },
        {
            "title": "A Related Works",
            "content": "A.1 Reward Systems for Reinforcement Learning Early reward models (RMs) (Christiano et al., 2023; Stiennon et al., 2022; Ouyang et al., 2022), trained to predict human preference rankings, typically treat the entire response as the evaluation unit. However, such outcome-level RMs lack insight into intermediate reasoning steps, making step-level error correction infeasible (Xu et al., 2025b). To address this limitation, process-level RMs (Lightman et al., 2023; Setlur et al., 2024) have been introduced to assign scores at each reasoning step, thereby providing stepwise feedback. Despite their effectiveness, process-level RMs require extensive manual step-level annotations, resulting in exponential increases in data collection costs and training complexity (Khalifa et al., 2025). Building on these et advances, DeepSeekR1 (DeepSeek-AI employs al., 2025a) rule-based reward functions that leverage predefined, maintainable rules for pattern matching and logical validation, offering simplicity and efficiency. However, as task diversity expands, the manual creation of such rules faces significant challenges related to scalability and coverage, ultimately limiting its applicability in open-ended generation scenarios. More recently, DeepSeek-GRM (Liu et al., 2025) and ThinkPRM (Khalifa et al., 2025) have explored integrating reasoning capabilities into RMs by developing generative reward models (GRMs). GRMs reformulate the scoring task as tokengeneration problem: before outputting numerical score, the model first generates chain-of-thought (CoT) (Wei et al., 2023) that explicates its evaluation criteria and rationale. This approach not only bridges the interpretability gap between blackbox discriminative models and brittle rule-based systems but also substantially enhances test-time scaling capabilities. A.2 Evaluation of Reward Systems There are two primary approaches to evaluating reward systems. The first approach employs standardized benchmarks that objectively assess reward system effectiveness by designing diverse tasks and datasets (Frick et al., 2024). The second approach examines the performance of reward systems when integrated directly into downstream optimization loops, such as Best-of-N selection (Nakano et al., 2022) or rejection sampling fine-tuning (Zelikman et al., 2024; Xiong et al., 2025), to measure their impact on generation quality and alignment. Reward system benchmarks can be further categorized into outcome-level (Liu et al., 2024; Lambert et al., 2024) and process-level (Lightman et al., 2023; Zheng et al., 2024) suites. In constructing these benchmarks, researchers generate multiple responses to the same prompt by varying model architectures or hyperparameters. During the manual annotation phase, outcome-level benchmarks require annotators to compare or assign multi-point scores to complete responses, emphasizing overall preference. In contrast, process-level benchmarks provide fine-grained gold verdicts by requiring stepby-step correctness labels for each reasoning step. Beyond benchmark-based evaluation, practical applications of reward systems serve as another common assessment method. In the Bestof-N (BoN) paradigm, WebGPT (Nakano et al., 2022) introduced using reward model to score candidate answers and select the top-ranked response. Subsequent work has framed reward models as downstream rankersfor example, SelfConsistency in chain-of-thought models (Wang et al., 2023), where the reward model identifies the most coherent solution among candidates. Unlike Best-of-N, rejection sampling fine-tuning (RFT) (Zelikman et al., 2024; Xiong et al., 2025) samples multiple trajectories from the current policy, scores them using reward model, and retains only the highest-scoring examples as silver supervision for further fine-tuning. This approach has proven particularly effective at bootstrapping reasoning capabilities without requiring full preference-learning pipelines."
        },
        {
            "title": "B Data Source",
            "content": "Table 7 provides comprehensive overview of all datasets used in constructing VerifyBench, detailing their respective licenses and the number of samples drawn from each. All data usage strictly complies with the terms and conditions stipulated by the original sources."
        },
        {
            "title": "E LLM Usage",
            "content": "We list all the LLMs we used to generate completions for curated question in Table 6."
        },
        {
            "title": "F Examples of VerifyBench",
            "content": "We provide some examples of VerifyBench with four different answer types in Figure 7, 8, 9 and 10. Series OpenAI Model gpt-4o-2024-11-20 gpt-4o-mini anthropic claude-3.7-sonnet deepseek-math deepseek-math-7b-instruct (Shao et al., 2024) deepseek-math-7b-rl (Shao et al., 2024) DeepSeek gemma-3 LlamaQwen2.5 DeepSeek-V3 (DeepSeek-AI et al., 2025b) DeepSeek-R1 (DeepSeek-AI et al., 2025a) DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025a) DeepSeek-R1-Distill-Qwen-32B (DeepSeek-AI et al., 2025a) gemma-3-1b-it (Gemma Team et al., 2025) gemma-3-4b-it (Gemma Team et al., 2025) gemma-3-12b-it (Gemma Team et al., 2025) Llama-3.3-70B-Instruct (Grattafiori et al., 2024) Llama-3-8B-Instruct (Grattafiori et al., 2024) Qwen2.5-7B-Instruct (Qwen et al., 2025) Qwen2.5-72B-Instruct (Qwen et al., 2025) Qwen2.5-Math Qwen2.5-Math-1.5B-Instruct (Yang et al., 2024) Qwen2.5-Math-7B-Instruct (Yang et al., 2024) Qwen2.5-Math-72B-Instruct (Yang et al., 2024) QwQ Yi-1.5 QwQ-32B (Qwen Team, 2024) Yi-1.5-9B-Chat-16K (AI et al., 2025) Yi-1.5-34B-Chat (AI et al., 2025) Table 6: LLMs used in this paper."
        },
        {
            "title": "C Prompts",
            "content": "C.1 Prompt for Answer Type Classification We present the prompt we used to generate answer types in Figure 4. C.2 Prompt for LLM-as-a-judge We present the prompt we used in LLM-as-a-judge evaluation with reference answer in Figure 5. C.3 Prompt for LLM-as-a-judge without Reference We present the prompt we used in LLM-as-a-judge evaluation with reference answer in Figure 6."
        },
        {
            "title": "D Experimental Details",
            "content": "Training. For the rejection sampling fine-tuning experiments, we used Llama-3.1-8B (Grattafiori et al., 2024) as the base model for SFT. The learning rate was set to constant value of 1e-5. Training was conducted using the Megatron-LM framework, with global batch size of 256 and context length of 4096. To accelerate training, we packed the training samples and trained for one epoch in total. All training experiments were conducted on 32 Ascend H910B-64G GPUs. used evaluation, we Evalution. For the vLLM(Kwon et al., 2023) framework for inference. To reduce evaluation variance, we set the temperature to 0.7 and sampled each test example 16 times, then computed the average accuracy. All inference were conducted on 8 NVIDIA A100-80G."
        },
        {
            "title": "License",
            "content": "# of Questions general_reasoning logic_reasoning BBH (Suzgun et al., 2022) BBEH (Kazemi et al., 2025) MMLU_pro (Wang et al., 2024) natural_reasoning (Yuan et al., 2025) AbductionRules (Young et al., 2022) anlg (Bhagavatula et al., 2020) anli (Nie et al., 2020) ARLSAT (Zhong et al., 2021) bAbI15 (Weston et al., 2015) bAbI16 (Weston et al., 2015) BoardgameQA (Kazemi et al., 2023) clutrr (Sinha et al., 2019) FOLIO (Han et al., 2024) hellaswag (Zellers et al., 2019) logicbenchBQA (Parmar et al., 2024) logicbenchMCQA (Parmar et al., 2024) LogiQA (Liu et al., 2020) MultiLogiEval (Patel et al., 2024) NeuLRabductive (Xu et al., 2025a) NeuLRdeductive (Xu et al., 2025a) NeuLRinductive (Xu et al., 2025a) ProntoQA (Saparov and He, 2023) ProofWriter (Tafjord et al., 2021) ReClor (Yu et al., 2020) tablebench (Wu et al., 2025) MIT Apache 2.0 Apache 2.0 CC-BY-NC 4. MIT / CC-BY-NC 4.0 MIT / / CC-BY-4.0 CC-BY-NC 4.0 CC-BY-SA-4.0 MIT MIT MIT / MIT / / / Apache 2.0 / / Apache 2.0 math_reasoning AIME24 AIME25 asdiv-a (Miao et al., 2021) Math Odyssey (Fang et al., 2024) GPQA_diamond (Rein et al., 2023) gsm8k (Cobbe et al., 2021) math401 (Yuan et al., 2023) mathematics (Saxton et al., 2019) MATH(Hendrycks et al., 2021) OlympiadBench-EN (He et al., 2024) SVAMP (Patel et al., 2021) NuminaMath-CoT (Li et al., 2024) orca-math-word-problems (Mitra et al., 2024) MIT ArtOfProblemSolving stackmathqa (Zhang, Yifan, 2024) DeepMath-103K-RL (He et al., 2025) MIT MIT CC-BY-NC 4.0 MIT MIT MIT / Apache 2.0 MIT MIT MIT Apache 2.0 self-curated CC-BY-4.0 MIT 4520 6511 2000 1000 1000 1000 230 1000 1000 1000 1000 134 1000 1000 1000 1000 1000 1000 1000 1000 500 1000 500 886 30 30 1218 389 198 1319 392 3360 5000 675 1000 20000 10000 7997 10000 20000 Table 7: The datasets we used and the number of samples drawn from each, including the license information of these datasets. 14 Figure 4: Prompt for answer type classification. Figure 5: Prompt for LLM-as-a-judge evaluation. Figure 6: Prompt for LLM-as-a-judge evaluation without reference answers. 16 Figure 7: data example from VerifyBench with answer type: Numeric Values. Figure 8: data example from VerifyBench with answer type: Expression. Figure 9: data example from VerifyBench with answer type: Multi-choice. Figure 10: data example from VerifyBench with answer type: String."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "Meituan Group",
        "Peking University",
        "The Hong Kong University of Science and Technology",
        "University of Electronic Science and Technology of China",
        "Zhejiang University"
    ]
}