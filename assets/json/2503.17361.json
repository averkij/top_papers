{
    "paper_title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation",
    "authors": [
        "Sophia Tang",
        "Yinuo Zhang",
        "Alexander Tong",
        "Pranam Chatterjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment."
        },
        {
            "title": "Start",
            "content": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation Sophia Tang1,2, Yinuo Zhang1,3, Alexander Tong4,5, Pranam Chatterjee1,6,7, 1Department of Biomedical Engineering, Duke University 2Management and Technology Program, University of Pennsylvania 3Center of Computational Biology, Duke-NUS Medical School 4Mila, Quebec AI Institute, 5Université de Montréal 6Department of Computer Science, Duke University 7Department of Biostatistics and Bioinformatics, Duke University Corresponding author: pranam.chatterjee@duke.edu"
        },
        {
            "title": "Abstract",
            "content": "Flow matching in the continuous simplex has emerged as promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, generative framework on the simplex based on novel Gumbel-Softmax interpolant with time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment."
        },
        {
            "title": "Introduction",
            "content": "Generative modeling has transformed the design of biological sequences, enabling de novo protein design [13], DNA regulatory elements [4, 3], and peptides [57]. However, generating structured sequences in discrete spaces remains an open challenge due to the inherent non-differentiability of categorical variables. Traditional autoregressive models, such as ProtGPT2 [2] and ProGen2 [1], learn sequence distributions by iteratively predicting tokens, but suffer from compounding errors, bias accumulation, and limited global coherence. To address these issues, generative models based on diffusion [811] and flow matching [12, 4, 3, 13] have been developed to enable non-autoregressive sampling of sequences. 5 2 0 2 1 2 ] . [ 1 1 6 3 7 1 . 3 0 5 2 : r Discrete diffusion [10, 11, 8] and flow-matching [12, 3] models, iteratively reconstruct sequences by modeling forward and reverse noise processes in Markovian framework. These approaches have demonstrated success in DNA sequence design [4, 3], protein generation [9, 14], and recently, multi-objective generation of therapeutic peptides [7]. However, these methods operate in the fully discrete state space, which means that the noisy sequence at each time step is fully discrete sequence of one-hot vectors sampled from continuous categorical distributions. This can result in discretization errors during sampling when abruptly restricting continuous distributions to single token. This presents the question: Can we generate discrete sequences by iteratively fine-tuning continuous probability distributions? This is the motivation behind discrete flow matching models on the simplex [4, 13], which defines smooth interpolation from uniform prior over the simplex to unitary distribution concentrated at single vertex. Despite these advances, previous discrete simplex-based flow-matching methods have yet to be applied to de novo design tasks like protein and target-specific peptide design that require learning diverse flow trajectories that scale to higher simplex dimensions. Furthermore, there remains lack of controllability at inference time due to strictly deterministic paths and the absence of modular trainingfree guidance methods. To address these gaps, we introduce Gumbel-Softmax Flow Matching (Gumbel-Softmax FM), generative framework that transforms noisy to clean data on the interior of the simplex by defining novel Gumbel-Softmax interpolant with time-dependent temperature parameter. By applying Gumbel noise during training, Gumbel-Softmax FM avoids overfitting to the training data, increasing the exploration of diverse flow trajectories. We also introduce STGFlow, training-free classifier-based guidance strategy that enables training-free classifier-based guidance for target-binding peptide generation. Our key contributions are as follows: 1. Gumbel-Softmax Flow Matching. We introduce Gumbel-Softmax FM, generative framework that leverages temperature-controlled Gumbel-softmax interpolants for smooth transport from noisy to clean distributions on the simplex. We define new velocity field that follows mixture of learned interpolations between categorical distributions that converge to high-quality sequences (Section 3). 2. Gumbel-Softmax Score Matching. As an alternative generative framework using the same Gumbel-softmax interpolant, we propose Gumbel-Softmax SM that estimates the gradient of probability density at varying temperatures to enable sampling from high-density regions on the simplex (Section 4). 3. Straight-Through Guided Flow Matching (STGFlow). Given the lack of post-training guidance methods for discrete flow matching, we introduce Straight-Through Guided Flow Matching, novel training-free classifier-based guidance algorithm that leverages straightthrough gradients to guide the flow trajectory towards high-scoring sequences (Section 5). We apply this method to generate high-affinity peptide binders to target proteins (Section 6.4). 4. Biological Sequence Generation. We apply our framework to conditional DNA promoter design, de novo protein sequence generation, and target-binding peptide design, demonstrating competitive performance compared to autoregressive and discrete diffusion-based baselines (Section 6). Our framework offers several theoretical and empirical advantages over autoregressive and discrete diffusion models, and we believe it will serve as foundation for controllable flow matching for discrete sequence generation."
        },
        {
            "title": "2 Preliminaries",
            "content": "We consider noisy uniform distribution over the (V 1)-dimensional simplex p0(x0) and clean distribution p1(x1) over discrete samples x1 from dataset D. The challenge of generative modeling over the simplex consists of defining time-dependent flow ψt that smoothly interpolates between p0 and p1. Then, we can generate samples from p1 by first sampling from p0 the applying learned velocity field that transports distributions from p0 to p1. 2 2.1 The Gumbel-Softmax Distribution The Gumbel-Softmax distribution or Concrete distribution [15, 16] is relaxation of discrete random variables onto the interior of the simplex 1 = {x RV xi [0, 1], (cid:80)V j=1 xj = 1}. This continuous relaxation is achieved by adding i.i.d. sampled Gumbel noise gi = log( log Ui)), where Ui Uniform(0, 1), scaling down by the temperature parameter τ > 0, and applying the differentiable softmax function across the distribution such that the elements sum to 1. Given parameters πi (ϵ, ) representing the original logits of each category where ϵ is small constant to avoid undefined logarithms, the Gumbel-Softmax random variable is given by xi = SM (cid:18) log πi + gi τ (cid:19) = exp (cid:16) log πi+gi τ (cid:17) (cid:80)V j=1 exp (cid:16) log πj +gj τ (cid:17) (1) where SM() denotes the softmax function. We observe that as τ 0, the distribution converges to one-hot vector where xk 1 and xj 0 for = given that = arg maxk (log πk + gk). Conversely, as τ , the distribution approaches uniform distribution where xj 1 for all [1, ]. 2.2 Discrete Flow Matching Flow matching [1719] is simulation-free generative framework that aims to transform noisy samples x0 p0 from source distribution p0 to clean samples x1 p1 from the data distribution p1 by learning to predict the marginal velocity field ut(xt) that transports p0 to p1 as mixture of conditional velocity fields uθ (xtx1) parameterized by neural network. The interpolant ψt(x1) : [0, 1] 1 1 1 is function that defines the flow from clean distribution x1 on vertex of the simplex to the intermediate distribution xt at time t, which satisfies the constraints ψ0(x0x1) = x0 and ψ1(x0x1) = x1 pt. Therefore, the conditional velocity field is given by the time-derivative of ψt(x1). ut(xtx1) = dt ψt(x1) (2) where ut TxtV 1 and TxtV is the set of tangent vectors to the manifold at point xt. For velocity field ut to generate pt, it must satisfy the continuity equation given by pt(xt) = (pt(xt)ut(xt)) (3) where is the divergence operator that describes the total outgoing flux at point xt along the flow trajectory. The flow matching (FM) objective is to train parameterized model uθ (xt) to approximate ut given noisy sample xt at time [0, 1] by minimizing the squared norm LFM = Et,xt (cid:13) (cid:13)uθ (xt) ut(xt)(cid:13) 2 (cid:13) (4) But since computing ut(xt) requires marginalizing over all possible trajectories and is intractable, we condition the velocity field on each data point x1 and compute the conditional flow-matching (CFM) objective given by LCFM = Et,xt (cid:13) (cid:13)uθ (xt) ut(xtx1)(cid:13) 2 (cid:13) (5) which is tractable and has the same gradient as the unconditional flow-matching loss θLFM = θLCFM [20, 21]. Among existing discrete flow matching methods, there are two methods of defining discrete flow: defining the interpolant ψt(x1) that connects noisy sample x0 to clean one-hot sample x1 and defining the probability path which pushes density from the prior distribution p0 to the target data distribution p1. In this work, we define new temperature-dependent interpolant and derive the corresponding velocity field. 2.3 Score Matching Generative Models Score matching [22] is another generative matching framework that learns the gradient of the conditional probability density path xt log pt(xt) (defined as the score) of the interpolation between 3 Figure 1: Overview of Gumbel-Softmax Flow Matching. Gumbel-softmax transformations are applied to clean one-hot sequences for varying temperatures dependent on time. The embedded noisy distributions are passed into parameterized flow or score model and error prediction model to predict the conditional flow velocity and score function. noisy and clean data. By parameterizing the score function with sθ(xt, t), we can minimize the score matching loss given by Lscore = Ept(xt) xt log pt(xt) sθ(xt, t)2 (6) Similarly to flow-matching, directly learning xt log pt(xt) is intractable, so we learn the conditional probability path xt log pt(xtx1) conditioned on x1 p1(x1) by minimizing Lscore = Ept(xtx1),p1(x1) xt log pt(xtx1) sθ(xt, t)2 which we show in Appendix D.1 equals the unconditional score function by expectation over x1. (7)"
        },
        {
            "title": "3 Gumbel-Softmax Flow Matching",
            "content": "In this work, we present Gumbel-Softmax Flow Matching (FM), novel simplex-based flow matching method that defines the noisy logits at each time step with the Gumbel-Softmax transformation, enabling smooth interpolation between noisy and clean data by modulating the temperature τ (t), which changes as function of time. 3.1 Defining the Gumbel-Softmax Interpolant We propose new definition of the discrete probability path by gradually decreasing the temperature of Gumbel-Softmax categorical distribution as function of time where the maximum probability corresponds to the target token. First, we define monotonically decreasing function τ (t) (0, ) to prevent the Gumbel-Softmax distribution from being undefined at τ = 0. τ (t) = τmax exp(λt) (8) where τmax is the initial temperature set to large number so that the categorical distribution resembles uniform distribution, λ controls the decay rate, and is the time that goes from = 0 to = 1. Now, we define the conditional interpolant xt = ψt(x1 = ek) with [0, 1] and Gumbel-noise scaled by factor β as ψt(x1 = ek) = exp (cid:16) δik+(gi/β) τ (t) (cid:17) (cid:80)V j=1 exp (cid:16) δjk+(gj /β) τ (t) (cid:17) (9) where τ (t) = τmax exp(λt) and πi = exp(δik). δik is the Kronecker delta function that returns 1 when = and 0 otherwise. This decaying time-dependent temperature function τ (t) ensures that the distribution becomes more concentrated at the target vertex as 1. Gumbel noise is applied during training to ensure that the model learns to reconstruct clean sequence given contextual information. 4 Proposition 1 (Continuity). The proposed conditional vector field and conditional probability path together satisfy the continuity equation (Equation 3) and thus define valid flow matching trajectory on the interior of the simplex. We provide the proof of continuity in Appendix C.2. This definition of the flow satisfies the boundary conditions. For = 0, τ (t) = τmax which produces near-uniform distribution ψ0(x0x1) 1 . For = 1, exp(λt) 0 (faster decay for larger λ) and τ (t) 0, meaning the flow trajectory converges to the vertex of the simplex corresponding to the one-hot vector ψ1(x0x1) x1. 3.2 Reparameterizing the Velocity Field From our definition of the Gumbel-Softmax interpolant, we derive the conditional velocity field ut(x0x1) by taking the derivative of the flow (Appendix C.1). ut,i(xx1 = ek) = λ τ (t) xt,i (cid:88) j=1 (cid:18) (cid:19) xt,j (δik + gi) (δjk + gj) (10) Proposition 2 (Probability Mass Conservation). The conditional velocity field preserves the probability mass and lies in the tangent bundle at point xt on the simplex TxtV 1 = {ut RV 1, ut = 0}. Proof in Appendix C.3. Instead of directly regressing ut(xtx1) by minimizing LCFM defined in Equation 5, we train denoising model that predicts the probability vector xθ(xt, t) 1 given the noisy interpolant xt by minimizing the negative log loss. Lgumbel = Ept(xtx1=ek),p1(x1) [ logxθ(xt, t), x1] (11) During inference, we compute the predicted marginal velocity field as the weighted sum of the conditional velocity fields scaled by the predicted token probabilities. uθ (xt) = (cid:88) k=1 ut(xx1 = ek)xθ(xt, t), ek (12) Proposition 3 (Valid Flow Matching Loss). If pt(xt) > 0 for all xt Rd and [0, 1], then the gradients of the flow matching loss and the Gumbel-Softmax FM loss are equal up to constant not dependent on θ such that θLFM = θLgumbel Proof in Appendix C.3. By our definition of the Gumbel-Softmax interpolant, the intermediate distributions during inference represent mixture of learned conditional interpolants ψt(x1) from the training data. Since the denoising model is trained to predict the true clean distribution, we can set the Gumbel-noise random variable in the conditional velocity fields to 0 during inference as we want the velocity field to point toward the predicted denoised distribution. Therefore, the conditional velocity field becomes ut(xtx1 = ek) = λ τ (t) xt,k (ek xt) (13) which points toward the target vertex ek at magnitude proportional to xt,k(1 xt,k) and away from all other vertices at magnitude proportional to xt,ixt,k. We observe that the velocity field vanishes both at the vertex and the (V 2)-dimensional face directly opposite to the vertex and increases as 1 and τ (t) 0, accelerating towards the target vertex at later time steps."
        },
        {
            "title": "4 Gumbel-Softmax Score Matching",
            "content": "As an alternative to our flow matching framework, we propose Gumbel-Softmax Score Matching (Gumbel-Softmax SM), score-matching method that learns the gradient of the probability density path xt log pt(xt) associated with the Gumbel-Softmax interpolant. 5 4.1 The Exponential Concrete Distribution When computing Gumbel-Softmax random variables, the exponentiation of small values associated with low-probability tokens can result in numerical underflow. Since the logarithm of 0 is undefined, this could result in numerical instabilities when computing the log probability density. To avoid instabilities, we take the logarithm of the Gumbel-Softmax probability distribution (known as the EXPCONCRETE distribution) [16] given by xi = log . Expanding the logarithm, we get that the ith element EXPCONCRETE random variable is defined as (cid:16) log πi+gi τ SM (cid:17)(cid:17) (cid:16) xi = log πi + (gi/β) τ log (cid:88) j=1 exp (cid:18) log πj + (gj/β) τ (cid:19) Translating this into our time-varying interpolant where πi = exp(δik), we define (cid:19) ψt(x1 = ek) = log exp δik + (gi/β) τ (t) (cid:88) j=1 (cid:18) δjk + (gj/β) τ (t) (14) (15) By our derivation in Appendix D.1, the score defined as the gradient of the log-probability density of the EXPCONCRETE interpolant with respect to the ith element xt,i is given by xt,i log pt(xtx1) = τ (t) + τ (t)V SM δik τ (t)xt,i (16) (cid:18) (cid:19) 4.2 Learning the Gumbel-Softmax Probability Density Given that the Gumbel-Softmax interpolant naturally converges towards the one-hot target token distribution, it follows that learning the evolution of probability density across training samples would enable generation in regions with high probability density. Our goal is to train parameterized model to learn to estimate the gradient of the log-probability density of the Gumbel-Softmax interpolant such that the gradient converges at regions with high probability density. To achieve this, we define the score parameterization similar to [23], given by sθ(xt, t) = τ (t) + τ (t)V SM(cid:0)fθ(xt, t)(cid:1) where sθ(xt, t) xt,j log pt(xt) (17) where θ minimizes the reparameterized score-matching loss given by Lscore = Ept(xtx1),p1(x1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:2) τ (t) + τ (t)V SM(δik τ (t)xt,i)(cid:3) (cid:2) τ (t) + τ (t)V SM(fθ(xt, t)(cid:3) (cid:13) (cid:13) 2 = τ (t)2V 2Ept(xtx1),p1(x1)SM(cid:0)δik τ (t)xt,i (cid:1) SM(fθ(xt, t)(cid:1)2 (18) The softmax function applied after parameterization ensures dependencies are preserved across the predicted output vector which defines the rate of probability flow towards each vertex. Since τ (t) 0 when 1, we remove the scaling term to ensure the losses are evenly scaled over time. (19) Proposition 4. The gradient of the EXPCONCRETE log-probability density is proportional to the gradient of the Gumbel-softmax log-probability density such that GS log pθ(xtx1) xj ExpConcrete Lscore = Ept(xtx1),p1(x1)SM(cid:0)δik τ (t)xt,i (cid:1) SM(fθ(xt, t)(cid:1)2 log pθ(xtx1). xj Proof in Appendix D.2. Therefore, by minimizing Lscore, we obtain model that effectively transports intermediate Gumbel-Softmax distributions towards clean distributions in high-probability regions of the discrete state space."
        },
        {
            "title": "5 Straight-Through Guided Flows (STGFlow)",
            "content": "In this section, we present Straight-Through Guided Flows (STGFlow) novel classifier-based guidance method that guides the pre-trained conditional flow velocities towards sequences with higher classifier probabilities pϕ(yxt) which does not require training time-dependent classifier or classifier-guided velocity field. STGFlow leverages straight-through gradient estimators to compute gradients of classifier scores from discrete sequence samples with respect to the continuous logits from which they were sampled. The unconditionally predicted logits are refined using the gradients in temperature-dependent manner, sharpening the guidance as 1. 6 5.1 Straight-Through Gradient Estimators Straight-through gradient estimators aim to solve the problem of taking gradients with respect to discrete random variables. Consider reward function R(z) that takes discrete sequence of length sampled from learned distribution pθ(z), and our goal is to maximize the reward max θ = min θ Ezpθ (cid:2)R(z)(cid:3) (20) Given the non-differentiability of R(z) with respect to the parameters θ, the Straight-Through Gumbel-Softmax estimator (ST-GS) [15] evaluates the gradient of the reward function through surrogate of the discrete random variable defined as the tempered softmax distribution over the continuous logits from which was sampled. θR = R(z) dθ SMτ (pθ(z)) (21) 2: Straight-Through Guided Flows Figure (STGFlow). We compute the gradients of the classifier function with respect to discrete sequences sampled from the intermediate token distribution xt, which act as guided flow velocity that steers the unconditional trajectory towards sequences with optimal scores. ST-GS preserves the forward evaluation of the reward function while enabling low-variance gradient estimation for back-propagation of the gradient that does not need to be defined over continuous relaxations of discrete variables over the simplex. Instead, they only need to be defined for discrete sequences, which is the case for most pre-trained classifier models. 5.2 Straight-Through Guided Flow Matching We extend the idea of ST-GS to define novel post-training guidance method. At each time step t, we compute the Gumbel-Softmax velocity field uθ (xt) and take step. Then, from the updated logits, we sample discrete sequences {x1,1, . . . , x1,M } from the top logits in xt re-normalized with the softmax function. For each sequence x1,m, we compute classifier score using our pre-trained classifier pϕ(yx1,m). Since the gradient through the argmax function is either 0 or undefined, we compute the gradient of the classifier model with respect to the surrogate softmax distribution. xtpϕ(yx1,m) = pϕ(yx1,m) x1,m dxt SM(cid:0)xt (cid:1) Evaluating the straight-through gradient with respect to the probability of each token, we have xt,ipϕ(yx1,m) = (cid:40) pϕ(yx1,m) x1 pϕ(yx1,m) x1 (cid:2)SM(xt,i) (1 SM(xt,k)) (cid:3) (cid:2) SM(xt,i)SM(xt,k)(cid:3) = = (22) (23) where denotes the index of the sampled token such that x1,m = ek. During inference, the partial derivative term pϕ(yx1,m) is computed with automatic differentiation with respect to each sequence position, enabling position-specific guidance. Finally, we guide the flow trajectory by adding the aggregate gradient across all sequences scaled by constant γ to get x1,m xt = xt + γ (cid:88) m=1 xtpϕ(yx1,m) (24) Proposition 5 (Conservation of Probability Mass of Straight-Through Gradient). The straight through gradient xtpϕ(yx1,m) preserves probability mass and lies on the tangent bundle at point xt on the simplex. Proof in Appendix E. Conceptually, the straight-through gradient acts as guiding velocity that steers the unconditional velocity toward valid, optimal sequences. Pseudocode for STGFlow is provided in Algorithm 5."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Simplex-Dimension Toy Exepriment Setup. Following Stark et al. [4], we conduct toy experiment that evaluates the KL divergence between the empirically-generated distribution and random distribution of sequence length 4 over the (V 1)-dimensional simplex (V 1)4 for = {20, 40, 60, 80, 100, 120, 140, 160, 512}. The sequence length is set to 4 and the number of integration steps was set to 100 across all experiments. Training. We trained Linear FM [4], Dirichlet FM [4], Fisher FM [13], and Gumbel-Softmax FM each for 50K steps on 100K sequences from randomly generated distribution. We evaluated the KL divergence KL(qpdata) where is the normalized distribution from 51.2K sequences generated by the model and pdata is the distribution from which the training data was sampled. Results. As shown in Table 7, Gumbel-Softmax FM achieves superior performance to Dirichlet FM when scaled to dimensions 60, with stable KL divergence in the range 0.02 0.05 for all simplex dimensions up to = 512. Although Gumbel-Softmax FM achieves higher KL divergence than Fisher FM, we note that the use of optimal transport in Fisher FM results in learning straight, deterministic flows that can result in overfitting to the training data. This can be observed when comparing the curves of the validation mean-squared error loss between the predicted and true conditional velocity fields summed over the simplex and sequence length dimensions (Figure 7). 6.2 Promoter DNA Sequence Design Following the procedures of previous works [24, 4], we evaluate Gumbel-Softmax FM for conditional DNA promoter design and show superior performance to discrete diffusion and flow-matching baselines. Setup. Promoter DNA is the strand of DNA adjacent to gene that binds to RNA polymerase and transcription factors to promote gene transcription and expression. The objective is to train conditional flow model with the regulatory signal concatenated to the noisy input sequence to minimize the mean squared error (MSE) between the predicted regulatory activity of the generated sequence with the true sequence, predicted with pre-trained Sei model [25]. Training. Following Stark et al. [4], we trained on train/test/validation split of 88, 470/3, 933/7, 497 promoter sequences that are 1,024 base pairs in length. For each batch of size 256, we applied the Gumbel-Softmax transformation according to Equation 9 with τmax = 10.0 and λ = 3.0 for uniformly distributed time steps [0, 1] over each training batch. The training objective was to minimize the negative log loss between the true one-hot tokens x1 and predicted logits xθ(xt, t) from varying temperatures dependent on uniformly sampled U(0, 1). We trained Dirichlet FM [4], Fisher FM [13], and Gumbel-Softmax FM parameterized with 20-layer 1D CNN architecture for 150K steps and evaluated the MSE across all validation batches. Model Bit Diffusion (Bit Encoding)* Bit Diffusion (One-Hot Encoding)* D3PM-Uniform* DDSM* Language Model* Dirichlet Flow Matching Fisher Flow Matching Gumbel-Softmax Flow Matching (Ours) MSE () 0.041 0.040 0.038 0.033 0. 0.029 0.030 0.029 Table 1: Evaluation of promoter DNA generation conditioned on transcription profile. MSE was evaluated across all validation batches between the predicted signal of conditionally generated sequence and the true sequence. Regulatory signals were predicted with pretrained Sei model [25]. Numbers with * are from Stark et al. [4] Results. The MSE values for the diffusion and autoregressive language model baselines [24, 26, 8] were taken from [4, 13], but the simplex-based flow baselines were retrained. Gumbel-Softmax FM produces lower signal MSE compared to diffusion and language model baselines and similar MSE to Dirichlet and Fisher FM. 8 Table 2: Evaluation metrics for generative quality of protein sequences. Metrics were calculated on 100 unconditionally generated sequences from each model, including EvoDiff and ProtGPT2. The arrow indicates whether () or () values are better. Model Test Dataset (random 1000) EvoDiff ProtGPT2 ProGen2-small Gumbel-Softmax Flow Matching (Ours) Gumbel-Softmax Score Matching (Ours) Params () pLDDT () pTM () pAE () Entropy () Diversity (%) () - 640M 738M 151M 198M 198M 74.00 31.84 54.92 49.38 52.54 49.40 0.63 0.21 0.41 0.28 0.27 0. 12.99 24.76 19.39 23.38 16.67 15.71 4.0 4.05 3.85 2.55 3.41 3.37 71.8 93.2 70.9 89.3 86.1 82. 6.3 De Novo Protein Sequence Design Next, we evaluate the quality of unconditionally-generated de novo protein sequences with GumbelSoftmax SM and Gumbel-Softmax FM. Despite operating in the continuous simplex space with considerably smaller backbone model, we demonstrate competitive generative quality compared to discrete diffusion and autoregressive baselines. Setup. Given the larger vocabulary size of protein sequences, we compared both the performance of GumbelSoftmax FM and GumbelSoftmax SM for this task. For both models, we applied the Gumbel-Softmax transformation with varying temperatures τ (t) for time steps U(0, 1) and τmax = 10.0. The decay rates were set to λ = 3.0 for both models and the noise scale was set to β = 2.0. The models were trained following Algorithm 1 for Gumbel-Softmax FM and 3 for Gumbel-Softmax SM. Sampling was performed following Algorithm 2 and Algorithm 4. Figure 3: Predicted structures of de novo generated proteins from GumbelSoftmax FM. The structures, pLDDT, pAE, and pTM scores are predicted with ESMFold [27] Training. We collected 68M Uniref50 and 207M OMG_PROT50 data [28, 29]. total of 275M protein sequences were first clustered to remove singletons using MMseqs2 linclust [30] (parameters set to min-seq-id 0.5 -c 0.9 cov-mode 1). We keep the sequences between lengths of 20 to 2500 and entries with only wild-type residues to avoid effects from outliers. The singleton sequences are removed. The resulting representative sequences undergo random 0.8/0.1/0.1 data splitting. We trained for 5 epochs on 7 NVIDIA A100 GPUs. Results. We compare the quality of our protein generation method against state-of-the-art de novo protein sequence generation models including the discrete diffusion model EvoDiff [31], large language model ProtGPT2 [2], and the autoregressive model ProGen2-small [32]. For 100 unconditionally generated sequences per model, we compute the pLDDT, pTM, pAE scores using ESMFold [33] as well as the token entropy and sequence diversity. Additional details on evaluation metrics are given in Appendix G.3. BLASTp runs for the proteins we generated indicate no homolog hits, highlighting again the novelty of the proteins we generated and indicating that our model is not sub-sampling from known homologous protein sequences. As summarized in Table 2, both Gumbel-Softmax SM and Gumbel-Softmax FM produce proteins with comparable pLDDT, pTM, and pAE scores to discrete baselines without significantly compromising sequence entropy and diversity. We believe further optimization of hyperparameters, leveraging informative priors, or functional/structural guidance would improve the generative quality of Gumbel-Softmax FM. 9 Figure 4: Gumbel-Softmax FM generated peptide binders for three targets with no known binders. (A) 10 a.a. designed binder to JPH3 (structure generated with AlphaFold3) involved in Huntingtons Disease-Like 2. (B) 10 a.a. designed binder to GFAP (PDB: 6A9P) involved in Alexander Disease. (C) 7 a.a. designed binder to eIF2B (PDB: 6CAJ) involved in Vanishing White Matter Disease. Docked with AutoDock VINA and polar contacts within 3.5 Å are annotated. Additional targets are shown in Table 4. 6.4 Peptide Binder Design Finally, we integrate guidance into Gumbel-Softmax FM to generate de novo peptides with high binding affinity to protein targets. We generate peptide binders with similar or higher binding affinity to proteins with known peptide binders and diverse, rare disease-associated proteins without known peptide binders. Setup. First, we generated de novo peptide binders for 10 structured targets with known peptide binders using our STGFlow algorithm (Algorithm 5). To guide the flow paths, we train targetbinding cross-attention-based regression model (Appendix F.2) that takes an amino acid representation of peptide binder and protein target and predicts the Kd/Ki/IC50 score, where scores < 6.0 indicate weak binding, scores within 6.0 7.5 indicate medium binding, and scores > 7.5 indicate strong binding. Using dataset of 1781 experimentally validated peptides, our model achieved strong Spearman correlation coefficient of 0.96 on the training set and 0.64 on the validation set. Training. We fine-tuned our Gumbel-Softmax FM protein generator for 600 epochs on 17, 479 peptides (0.8/0.2 train/validation split) between 6 50 amino acids in length curated from the PepNN [34], BioLip2 [35], and PPIRef [36] datasets. Results. First, we compare peptide binders generated by Gumbel-Softmax FM coupled with STGFlow guidance to existing peptide binders to 13 protein targets  (Table 3)  . After generating 20 de novo peptides of the same length as the existing binders, we computed the ipTM and pTM scores using AlphaFold3 to evaluate the predicted confidence of the peptide-protein complexes and the docking scores using AutoDock VINA to evaluate the free energy of the binding interaction (See Appendix G.4 for details on evaluation metrics). From the final de novo generated peptides with optimized classifier scores against each target, we show that Gumbel-Softmax FM can consistently generate peptides with superior ipTM () and VINA docking scores () compared to experimentally-validated binders  (Table 3)  , indicating the efficacy of guided flow matching strategy in generating peptides with high binding affinity. Figure 5: Comparison of existing and Gumbel-Softmax FM designed binder to protein 4EZN. AutoDock VINA docking score of the designed binder (6.5 kcal/mol; magenta) is lower than that of the existing binder (4.1 kcal/mol; green) indicating stronger binding affinity. Polar contacts within 3.5 Å are annotated. Additional comparisons of existing and designed binders are in Table 3. 10 Table 3: Comparison of ipTM and VINA docking scores for existing and designed peptide binders to protein targets. The ipTM scores are calculated by AlphaFold3 for peptide-protein complexes using both existing peptides and peptides designed by guided Gumbel-Softmax FM. *Contains unnatural amino acid which cannot be processed by AlphaFold3. PDB ID existing binder ipTM () pTM () VINA Docking Score (kcal/mol) () existing designed existing designed existing designed GLP-1R (3C5T) HXEGTFTSDVSSYLEGQAAKEFIAWLVRGRG 1AYC 2Q8Y 3EQS 3NIH 4EZN 4GNE 4IU7 5E1C 5EYZ 5KRI 7LUL 8CN1 ARLIDDQLLKS ALRRELADW GDHARQGLLALG RIAAA VDKGSYLPRPTPPRPIYNRN ARTKQTA HKILHRLLQD KHKILHRLLQDSSS SWESHKSGRETEV KHKILHRLLQDSSS RWYERWV ETEV * 0.68 0.44 0.80 0.85 0.54 0.89 0.93 0.83 0.73 0.83 0.94 0. 0.65 0.67 0.70 0.71 0.86 0.59 0.76 0.79 0.80 0.81 0.77 0.91 0.86 * 0.88 0.83 0.88 0.91 0.85 0.76 0.91 0.91 0.77 0.91 0.93 0.72 0.66 0.88 0.84 0.86 0.90 0.87 0.76 0.94 0.91 0.78 0.91 0.92 0.82 -5.7 -5.3 -6.7 -4.4 -6.2 -4.1 -5.0 -4.6 -4.3 -2.9 -3.5 -6.5 -6.0 -7.5 -4.6 -6.8 -4.7 -5.7 -6.5 -4.8 -5.9 -5.1 -6.9 -5.5 -7.6 -6.9 Table 4: Comparison of ipTM and VINA docking scores for designed peptide binders and scrambled negative control to protein targets with no known binders. The ipTM and pTM scores are calculated by AlphaFold3 and docking scores are calculated by AutoDock VINA for peptides designed by Gumbel-Softmax FM with STGFlow. Designed sequences are randomly permuted to generate scrambled negative control for comparison. *No PDB structure available. Used AlphaFold3 predicted structure for docking. PDB ID Protein Name Disease ipTM () pTM () VINA Docking Score (kcal/mol) () designed scramble designed scramble designed scramble 6A9P 6CAJ 3HVE 6W5V * 2CKL GFAP eIF2B Gigaxonin NPC2 JPH3 BMI Alexander Disease Vanishing White Matter Disease Giant Axonal Neuropathy Niemann-Pick Disease Type Huntingtons Disease-Like 2 (HDL2) Medulloblastoma 0.62 0.61 0.75 0.80 0.72 0.71 0.38 0.39 0.54 0.34 0.60 0.43 0.31 0.77 0.83 0.79 0.49 0.81 0.29 0.76 0.82 0.77 0.49 0.73 -5.9 -9.1 -6.8 -6.5 -7.9 -6. -3.7 -9.0 -6.2 -5.6 -7.8 -6.2 To further validate the versatility of our framework, we evaluated peptide binders guided for six proteins involved in various diseases with no pre-existing peptide binders (Figure 8; Table 4). We generated 20 peptide binders that are 5 15 amino acids in length with Gumbel-Softmax FM and STGFlow guidance and randomly permuted the sequence to generate scrambled negative control for comparison. Notably, our designed binders demonstrate strong ipTM higher than 0.62 and VINA docking scores below 5.9. Despite the short sequence length, we also show that scrambling the order of amino acids consistently decreases the binding affinity compared to the unscrambled binder, indicating that our guidance strategy effectively captures dependencies across tokens that lead to higher-affinity peptides  (Table 4)  . Furthermore, the docked peptides show complementary structures to the target protein with several polar contacts within 3.5 Å (Figure 8). Since pTM () scores are dominated by the confidence in the protein target structure, there are no significant differences in the scores between the designed binders and control peptides; however, we still observe slightly higher scores indicating that our designed binders enhance the stability of the protein structure. Plotting the predicted binding affinity scores over the iteration or time step, we consistently see sharp upward curves, which proves the efficacy of STGFlow in optimizing classifier scores (Figure 6)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce Gumbel-Softmax Flow and Score Matching, novel discrete framework that learns interpolations between noisy and clean data by modulating the temperature of the Gumbel-Softmax distribution. By parameterizing straight continuous-time interpolation with stochastic Gumbel noise, we overcome limitations of existing discrete generative models, such as computationally expensive iterative denoising in discrete diffusion [811], high variance training in Dirichlet Flow Matching [4], and restrictive probability constraints in Fisher Flow Matching [13]. 11 We apply our model to three key biological sequence generation tasks: conditional DNA promoter design, de novo protein sequence generation, and target-binding peptide design. For promoter design, Gumbel-Softmax FM generates functional DNA sequences with enhanced transcriptional activity, outperforming previous discrete generative approaches. For target-protein guided peptide binder design with STGFlow, our de novo peptides show superior binding affinity against known binders for 10 proteins and strong binding affinity to six rare neurological disease-associated proteins with no known peptide binders, opening up numerous therapeutic opportunities for these understudied diseases. For protein sequence generation, our method enables the design of structurally feasible proteins while maintaining sequence diversity and uniqueness against known proteins. By bridging discrete flow matching with Gumbel-Softmax relaxations, our work provides scalable and theoretically grounded framework for discrete sequence modeling on the simplex. Future directions include extending the approach to multi-objective sequence optimization, incorporating task-specific priors to enhance design constraints, and applying Gumbel-Softmax FM to other structured biological design problems, such as RNA sequence engineering and regulatory circuit design."
        },
        {
            "title": "8 Declarations",
            "content": "Acknowledgments. We thank the Duke Compute Cluster, Pratt School of Engineering IT department, and Mark III Systems, for providing database and hardware support that has contributed to the research reported within this manuscript. Author Contributions. S.T. devised and developed model architectures and theoretical formulations, and trained and benchmarked models. Y.Z. advised on model design and theoretical framework, trained and benchmarked models, and performed molecular docking. S.T. drafted the manuscript and S.T. and Y.Z. designed the figures. A.T. reviewed mathematical formulations and provided advising. P.C. designed, supervised, and directed the study, and reviewed and finalized the manuscript. Data and Materials Availability. The codebase will be freely accessible to the academic community at https://huggingface.co/ChatterjeeLab/GumbelFlow. Funding Statement. This research was supported by NIH grant R35GM155282 as well as grant from the EndAxD Foundation to the lab of P.C. Competing Interests. P.C. is co-founder of Gameto, Inc. and UbiquiTx, Inc. and advises companies involved in peptide therapeutics development. P.C.s interests are reviewed and managed by Duke University in accordance with their conflict-of-interest policies. S.T., Y.Z., and A.T. have no conflicts of interest to declare."
        },
        {
            "title": "References",
            "content": "[1] Ali Madani, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M. Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z. Sun, Richard Socher, James S. Fraser, and Nikhil Naik. Large language models generate functional protein sequences across diverse families. Nature Biotechnology, 41(8):10991106, January 2023. ISSN 1546-1696. doi: 10. 1038/s41587-022-01618-2. URL http://dx.doi.org/10.1038/s41587-022-01618-2. [2] Noelia Ferruz, Steffen Schmidt, and Birte Höcker. Protgpt2 is deep unsupervised language model for protein design. Nature Communications, 13(1), July 2022. ISSN 2041-1723. doi: 10. 1038/s41467-022-32007-7. URL http://dx.doi.org/10.1038/s41467-022-32007-7. [3] Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking International Conference on guidance for discrete state-space diffusion and flow models. Learning Representations, 2025. doi: 10.48550/ARXIV.2406.01572. URL https://arxiv. org/abs/2406.01572. [4] Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. ICML, 2024. 12 [5] Suhaas Bhat, Kalyan Palepu, Lauren Hong, Joey Mao, Tianzheng Ye, Rema Iyer, Lin Zhao, Tianlai Chen, Sophia Vincoff, Rio Watson, Tian Z. Wang, Divya Srijay, Venkata Srikar Kavirayuni, Kseniia Kholina, Shrey Goel, Pranay Vure, Aniruddha J. Deshpande, Scott H. Soderling, Matthew P. DeLisa, and Pranam Chatterjee. De novo design of peptide binders to conformationally diverse targets with contrastive language modeling. Science Advances, 11(4), January 2025. ISSN 2375-2548. doi: 10.1126/sciadv.adr8638. URL http://dx.doi.org/10.1126/ sciadv.adr8638. [6] Tianlai Chen, Madeleine Dumas, Rio Watson, Sophia Vincoff, Christina Peng, Lin Zhao, Lauren Hong, Sarah Pertsemlidis, Mayumi Shaepers-Cheu, Tian Zi Wang, Divya Srijay, Connor Monticello, Pranay Vure, Rishab Pulugurta, Kseniia Kholina, Shrey Goel, Matthew P. DeLisa, Ray Truant, Hector C. Aguilar, and Pranam Chatterjee. Pepmlm: Target sequence-conditioned generation of therapeutic peptide binders via span masked language modeling. arXiv, 2024. doi: 10.48550/ARXIV.2310.03842. URL https://arxiv.org/abs/2310.03842. [7] Sophia Tang, Yinuo Zhang, and Pranam Chatterjee. Peptune: De novo generation of therapeutic peptides with multi-objective-guided discrete diffusion. arXiv, 2024. doi: 10.48550/ARXIV. 2412.17780. URL https://arxiv.org/abs/2412.17780. [8] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021. doi: 10.48550/ARXIV.2107.03006. URL https://arxiv.org/ abs/2107.03006. [9] Mingyang Wang, Shuai Li, Jike Wang, Odin Zhang, Hongyan Du, Dejun Jiang, Zhenxing Wu, Yafeng Deng, Yu Kang, Peichen Pan, Dan Li, Xiaorui Wang, Xiaojun Yao, Tingjun Hou, and Chang-Yu Hsieh. Clickgen: Directed exploration of synthesizable chemical space via modular reactions and reinforcement learning. Nature Communications, 15(1), November 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-54456-y. URL http://dx.doi.org/10.1038/ s41467-024-54456-y. [10] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. Simplified and generalized masked diffusion for discrete data. Advances in Neural Information Processing Systems, 2024. doi: 10.48550/ARXIV.2406.04329. URL https://arxiv.org/abs/2406. 04329. [11] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 2024. doi: 10.48550/ARXIV.2406.07524. URL https://arxiv.org/abs/2406.07524. [12] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems, 2024. doi: 10.48550/ARXIV.2407.15595. URL https://arxiv.org/abs/2407.15595. [13] Oscar Davis, Samuel Kessler, Mircea Petrache, Ismail Ilkan Ceylan, Michael Bronstein, and Avishek Joey Bose. Fisher flow matching for generative modeling over discrete data. Advances in Neural Information Processing Systems, 2024. doi: 10.48550/ARXIV.2405.14664. URL https://arxiv.org/abs/2405.14664. [14] Shrey Goel, Vishrut Thoutam, Edgar Mariano Marroquin, Aaron Gokaslan, Arash Firouzbakht, Sophia Vincoff, Volodymyr Kuleshov, Huong T. Kratochvil, and Pranam Chatterjee. Memdlm: De novo membrane protein design with masked discrete diffusion protein language models. arXiv, 2024. doi: 10.48550/ARXIV.2410.16735. URL https://arxiv.org/abs/2410. 16735. [15] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. International Conference on Learned Representations, 2017. doi: 10.48550/ARXIV.1611.01144. URL https://arxiv.org/abs/1611.01144. [16] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables, 2016. URL https://arxiv.org/abs/1611.00712. [17] Stefano Peluchetti. Non-denoising forward-time diffusions, 2022. URL https://openreview. net/forum?id=oVfIKuhqfC. [18] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint 2209.14577, 2022. [19] Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint 2303.08797, 2023. [20] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. International Conference on Learning Representations, 2023. doi: 10.48550/ARXIV.2210.02747. URL https://arxiv.org/abs/2210.02747. [21] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid RectorBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. doi: 10.48550/ARXIV.2302.00482. URL https://arxiv.org/abs/2302.00482. [22] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 2019. doi: 10.48550/ARXIV. 1907.05600. URL https://arxiv.org/abs/1907.05600. [23] Ahsan Mahmood, Junier Oliva, and Martin Andreas Styner. Anomaly detection via gumbel noise score matching. Frontiers in Artificial Intelligence, 7, September 2024. ISSN 2624-8212. doi: 10.3389/frai.2024.1441205. URL http://dx.doi.org/10.3389/frai.2024.1441205. [24] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation, 2023. URL https://arxiv.org/abs/2305. 10699. [25] Kathleen M. Chen, Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. sequencebased global map of regulatory activity for deciphering human genetics. Nature Genetics, 54(7):940949, July 2022. ISSN 1546-1718. doi: 10.1038/s41588-022-01102-2. URL http://dx.doi.org/10.1038/s41588-022-01102-2. [26] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning, 2022. URL https://arxiv.org/abs/2208.04202. [27] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan Dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomiclevel protein structure with language model. Science, 379(6637):11231130, March 2023. [28] Baris Suzek, Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy Wu. Uniref: comprehensive and non-redundant uniprot reference clusters. Bioinformatics, 23(10):1282 1288, 2007. [29] Andre Cornman, Jacob West-Roberts, Antonio Pedro Camargo, Simon Roux, Martin Beracochea, Milot Mirdita, Sergey Ovchinnikov, and Yunha Hwang. The omg dataset: An open metagenomic corpus for mixed-modality genomic language modeling. 2024. doi: 10.1101/2024.08.14.607850. URL https://www.biorxiv.org/content/early/2024/ 08/17/2024.08.14.607850. [30] Martin Steinegger and Johannes Söding. Clustering huge protein sequence sets in linear time. Nature communications, 9(1):2542, 2018. [31] Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Neil Tenenholtz, Robert Strome, Alan M. Moses, Alex X. Lu, Nicolò Fusi, Ava P. Amini, and Kevin K. Yang. Protein generation with evolutionary diffusion: sequence is all you need. September 2023. doi: 10.1101/2023.09.11. 556673. URL http://dx.doi.org/10.1101/2023.09.11.556673. [32] Erik Nijkamp, Jeffrey A. Ruffolo, Eli N. Weinstein, Nikhil Naik, and Ali Madani. Progen2: Exploring the boundaries of protein language models. Cell Systems, 14(11):968978.e3, November 2023. ISSN 2405-4712. doi: 10.1016/j.cels.2023.10.002. URL http://dx.doi. org/10.1016/j.cels.2023.10.002. 14 [33] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, March 2023. ISSN 1095-9203. doi: 10.1126/science.ade2574. URL http://dx.doi.org/10.1126/science. ade2574. [34] Osama Abdin, Satra Nim, Han Wen, and Philip M. Kim. Pepnn: deep attention model for the identification of peptide binding sites. Communications Biology, 5(1), May 2022. ISSN 2399-3642. doi: 10.1038/s42003-022-03445-2. URL http://dx.doi.org/10.1038/ s42003-022-03445-2. [35] Chengxin Zhang, Xi Zhang, Lydia Freddolino, and Yang Zhang. Biolip2: an updated structure database for biologically relevant ligandprotein interactions. Nucleic Acids Research, 52 (D1):D404D412, July 2023. ISSN 1362-4962. doi: 10.1093/nar/gkad630. URL http: //dx.doi.org/10.1093/nar/gkad630. [36] Anton Bushuiev, Roman Bushuiev, Petr Kouba, Anatolii Filkin, Marketa Gabrielova, Michal Gabriel, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky, Stanislav Mazurenko, and Josef Sivic. Learning to design protein-protein interactions with enhanced generalization, 2023. URL https://arxiv.org/abs/2310.18515. [37] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv, 2024. doi: 10.48550/ARXIV.2402.04997. URL https://arxiv.org/abs/ 2402.04997. [38] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. International Conference on Machine Learning, 2024. doi: 10.48550/ARXIV.2310.16834. URL https://arxiv.org/abs/2310.16834. [39] Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. Continuous Time Framework for Discrete Denoising Models. October 2022. URL https://openreview.net/forum?id=DmT862YAieY. [40] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky T. Q. Chen. Multisample flow matching: Straightening flows with minibatch couplings. International Conference on Machine Learning, 2023. doi: 10.48550/ARXIV.2304. 14772. URL https://arxiv.org/abs/2304.14772. [41] Xi Zhang, Yuan Pu, Yuki Kawamura, Andrew Loza, Yoshua Bengio, Dennis L. Shung, and Alexander Tong. Trajectory flow matching with applications to clinical time series modeling, 2024. URL https://arxiv.org/abs/2410.21154. [42] Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, and Ricky T. Q. Chen. Guided flows for generative modeling and decision making, 2023. URL https://arxiv.org/ abs/2311.13443. [43] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2022. doi: 10.48550/ARXIV.2207. 12598. URL https://arxiv.org/abs/2207.12598. [44] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations, 2021. doi: 10.48550/ARXIV.2011. 13456. URL https://arxiv.org/abs/2011.13456. [45] William Peebles and Saining Xie. Scalable diffusion models with transformers. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. doi: 10.48550/ARXIV.2212.09748. URL https://arxiv.org/abs/2212.09748. [46] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. URL https://arxiv.org/ abs/2104.09864. [47] Ruochi Zhang, Haoran Wu, Yuting Xiu, Kewei Li, Ningning Chen, Yu Wang, Yan Wang, Xin Gao, and Fengfeng Zhou. Pepland: large-scale pre-trained peptide representation model for comprehensive landscape of both canonical and non-canonical amino acids. arXiv, 2023. doi: 10.48550/ARXIV.2311.04419. URL https://arxiv.org/abs/2311.04419. [48] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 26232631, 2019. [49] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael ONeill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvile Žemgulyte, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, and John M. Jumper. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630(8016):493500, May 2024. ISSN 1476-4687. doi: 10.1038/s41586-024-07487-w. URL http://dx.doi.org/10.1038/s41586-024-07487-w. [50] Jerome Eberhardt, Diogo Santos-Martins, Andreas Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):38913898, 2021. [51] Schrödinger, LLC. The PyMOL molecular graphics system, version 1.8. November 2015."
        },
        {
            "title": "A Extended Background",
            "content": "A.1 Flow Matching on the Simplex Here, we discuss the motivation behind discrete flow matching [37, 12], and specifically on the interior of the simplex [4, 13]. This discussion will help motivate the contribution of our work from past iterations. Discrete diffusion models [8, 38, 39] operate by applying categorical noise in the form of xt Cat(Q x0) that convert the clean sequence of one-hot categorical distributions x0 to noisy sequence zt. Then, parameterized model learns to iteratively reconstruct the clean sequence x0 from the noisy sequence zt by taking discrete backward transitions given by . However, this method operates in the fully discrete state space, zs Cat meaning that the noisy sequence at each time step is fully discrete sequence of one-hot vectors sampled from continuous categorical distributions. This can result in discretization errors during sampling when abruptly restricting continuous distributions to single token. This presents the question: Can we generate discrete sequences by iteratively fine-tuning continuous probability distributions? QstztQ z xθ(zt,t) xθ(zt,t) (cid:17) (cid:16) This is the motivation behind discrete flow matching models on the simplex [4, 13], which defines smooth interpolation ψt(x1) from prior uniform distribution over the simplex x0 to unitary distribution concentrated at single vertex x1 over the time interval [0, 1]. To ensure that noisy can be transformed into valid clean sequences at inference, the interpolant must satisfy the boundary conditions given by ψ0(x1) 1 where is the size of the token vocabulary. The advantage of this approach over fully discrete methods is the ability to refine probability distributions given the neighboring distributions rather than noisy discrete tokens that accumulate discretization errors at each time step. A.2 Deterministic vs. Stochastic Interpolants The linear interpolant [20, 40] defines deterministic flow ψt(xtx0, x1) = tx0 + (1 t)x1 between pair of fixed endpoints (x0, x1). Optimal transport [21] further defines an optimal mapping π(x0, x1) that minimizes cost function c(x0, x1) often squared distance cost c(x0, x1) = d2(x0, x1)between paired endpoints. Although the deterministic perspective is optimal for tasks like matching trajectories [41], it lacks expressivity and diversity for de novo design tasks like protein or peptide-binder design. This approach also prevents the flow model from effectively learning to redirect specific token trajectories that do not reflect the data distribution during inference given the sequence context. By defining stochastic interpolant with Gumbel-noise where each token has small probability of being transformed into distribution where the token with the highest probability does not match the true token during training, the model still needs to predict the clean distribution xθ(xt, t) or the target generating velocity field uθ (xt) but with more ambiguity given that not all distributions are on the deterministically biased towards the target token. This pushes the model to place greater weight on the global context of each token and learn dependencies across tokens to generate valid clean sequence despite the increased ambiguity. Furthermore, this approach injects path variability to improve generalization and exploration of diverse flows for de novo design tasks. A.3 Guided Flow Matching key limitation of current discrete flow matching techniques is the lack of training-free guidance strategies. Flow matching guidance [42, 43] is performed either with classifier-based or classifier-free guidance. Classifier-Free Guidance. In classifier-free guided flow matching [42], the guided velocity field is obtained by training guided flow model uϕ (x) and taking the linear combination of the guided and unconditional velocities scaled by parameter γ. (xy) and an unconditional flow model uθ (25) This strategy requires training an additional guided flow model on quality-labeled data, which is often scarce. Given that flow models require more training data than simple regression and classification models, classifier-based guidance is preferred for scalability. uθ (xy) = (1 γ)uθ (x) + γuθ (xy) 17 Classifier-Based Guidance. In classifier-based guided flow matching [44], time-dependent classifier pϕ (yxt) that predicts classifier score given noisy samples xt separately from the unconditional generator. Then, we sample with guided velocity field given by (xt) + γxt log pϕ uθ,ϕ (xt) = uθ (yxt) (26) which requires projection back to the simplex for guided discrete flows. For simplex-based flows, this approach typically involves additional training of noisy classifiers that predict the classifier score given intermediate distributions over the simplex at each time step. Not only are these noisy classifiers less accurate than large pre-trained classifiers on clean sequences, but they also require extensive training as all noise levels need to be included in the training task. STGFlow overcomes these limitations by defining guided flow velocity using the straight-through gradients of the scoring model on discrete sequences sampled with respect to the relaxed Gumbelsoftmax probabilities. To ensure that the scores of sampled sequences are representative of the relaxed distribution, we sample sequences and take the aggregate gradient as the guided velocity. This provides modular training-free strategy for discrete flow matching guidance that conserves the probability mass constraint (Proof in Appendix E). Relation to Prior Simplex-Based Flow Matching Models In this section, we discuss and compare Gumbel-Softmax FM with two related methods for discrete flow matching on the simplex: Dirichlet Flow Matching [4] and Fisher Flow Matching [13]. B.1 Dirichlet Flow Matching The Dirichlet distribution is an extension of the Beta distribution for multiple variables and models the probability of the next variable being in one of discrete categories given parameter vector α = (α1, . . . , αV ). Intuitively, it acts as distribution of smooth categorical vectors 1 that lie on the probability simplex given that each category [1 . . . ] was observed with frequency αi. Increasing αi for given category would increase the probability of sampling near the ith vertex of the simplex. Dirichlet FM [4] defines the conditional probability path as pt(xx1 = ek) = Dir(x; α = 1 + ek) = 1 B(α1, . . . , αV ) (cid:89) i=1 xαi1 t,i (27) At = 0, the distribution reduces to uniform prior over 1, with an equal probability of sampling near any vertex. As , αk increases while αj for all = remain constant, so the probability density converges to the kth vertex. As shown in [4], this distribution satisfies the boundary constraints. To compute the target vector field, we start with the following equation ut(xtx1 = ek) = Ixt,k(t+1,V 1) B(t + 1, 1) (1 xt,k)V 1 xt,k (ek xt) (28) Similar to our approach, Dirichlet FM trains denoising model by minimizing negative log loss and computes the velocity field as the linear combination of the conditional velocity fields as in Equation 12. Although the Dirichlet probability path provides support over the entire simplex at all time steps, it suffers from high variance during training due to the stochastic nature of sampling from the Dirichlet distribution. Since flow matching learns mixture of conditional velocity fields, there exists inherent variability during inference. Our definition of Gumbel-Softmax interpolant ensures straighter flow paths and lower variance during training as Gumbel noise largely preserves the relative probabilities between categories. B.2 Fisher Flow Matching Fisher FM [13] overcomes the instability of the Fisher-Rao metric at the vertices of the simplex via sphere map φ : 1 SV 1 that maps point in the interior of the (V 1)- dimensional simplex to point on the positive orthant of the (V 1)-dimensional hypersphere. The + where φ(x) = 18 conditional velocity field ut(xtx1) of the linear interpolant on the sphere is given by ψt(x1) = expx0 (cid:0)t logx0(x1)(cid:1) ut(xtx1) = logxt(x1) 1 (29) During inference, the parameterized velocity field uθ(xt) RV is projected onto the tangent bundle of the hypersphere Txt SV + via the following mapping which minimizes the mean-squared error with the true conditional velocity field given by uθ (xt) = uθ(xt) xt, uθ(xt)2xt Lfisher = EtU (0,1),pt(xtx1),p1(x1) (cid:13) (cid:13) (cid:13) (cid:13) uθ(t, xt) logxt(x1) 1 (cid:13) 2 (cid:13) (cid:13) (cid:13) SV + (30) (31) Fisher FM addresses the high training variance of Dirichlet FM without the pathological properties of linear flows on the simplex by projecting the linear interpolant to the positive orthant of the -dimensional hypersphere, which is isometric to the (V 1)-dimensional simplex. However, projecting velocity fields to and from the tangent space of the hypersphere can lead to inconsistencies when applying guidance methods. Empirically, we found that the Fisher FM exhibits significantly high validation MSE loss during training, especially for increasing simplex dimensions, suggesting that the parameterization easily overfits to training data and is not optimal for de novo design tasks such as protein generation or peptide design."
        },
        {
            "title": "C Flow Matching Derivations",
            "content": "C.1 Deriving the Conditional Velocity Field We derive the conditional velocity field at point xt denoted as ut(xx1 = ei) by taking the derivative of the interpolant ψt(x1 = ei) with respect to time t. ut,i(xtx1 = ek) = = dt dt Letting zi = exp (cid:16) log πi+gi (cid:17) τmax exp(λt) , we have ψt,i(x0x1 = ek) exp (cid:16) log πi+gi (cid:17) τmax exp(λt) (cid:16) log πj +gj (cid:80)V j=1 exp τmax exp(λt) (cid:17) (32) ut(xtx1 = ek) = = exp(zi) (cid:80)V dt j=1 exp(zj) dt exp(zi)(cid:1) (cid:16)(cid:80)V (cid:0) (cid:17) j=1 exp(zj) (cid:16)(cid:80)V exp(zi) (cid:17)2 j=1 exp(zj) (cid:16) dt (cid:80)V (cid:17) j=1 exp(zj) (33) First, we compute dt exp(zi) dt exp (cid:18) log πi + gi (cid:19) τmax exp(λt) = exp(zi) = exp(zi) = exp(zi) 19 (cid:18) log πi + gi (cid:19) τmax exp(λt) dt log πi + gi τmax log πi + gi τmax dt exp(λt) λ exp(λt) (34) Then, we compute dt (cid:80) exp(zj) dt (cid:88) j=1 exp (cid:18) log πj + gj (cid:19) τmax exp(λt) = = (cid:88) j= dt exp (cid:18) log πj + gj (cid:19) τmax exp(λt) (cid:88) (cid:18) j=1 exp(zj) log πj + gj τmax (cid:19) λ exp(λt) (35) Then, substituting these terms back into the expression for ut, we get ut,i(xtx1 = ek) (cid:16)(cid:80)V j=1 exp (zj) (cid:17) exp(zi) log πi+gi τmax λ exp(λt) exp (zi) (cid:80)V j=1 (cid:16) exp(zj) log πj +gj τmax λ exp(λt) (cid:17) (cid:16)(cid:80)V j=1 exp (zj) (cid:17) exp(zi) λ exp(λt) τmax (cid:16)(cid:80)V j=1 exp (zj) (cid:17)2 (cid:34)(cid:32) (cid:88) j=1 (cid:33) exp(zj) (log πi + gi) (cid:88) (cid:18) j=1 exp(zj) (log πj + gj) (cid:19)(cid:35) exp(zi) λ exp(λt) τmax (cid:16)(cid:80)V j=1 exp (zj) (cid:17)2 (cid:34) (cid:88) j= (cid:18) exp(zj) (log πi + gi) (log πj + gj) (cid:19)(cid:35) exp(zi) j=1 exp (zj) (cid:80)V λ exp(λt) τmax = ψt,i(x1) λ exp(λt) τmax (cid:34) (cid:88) j= (cid:34) (cid:88) j=1 (cid:18) (cid:18) exp(zj) (cid:80) exp (zj) (cid:18) (cid:18) (cid:19)(cid:19)(cid:35) (log πi + gi) (log πj + gj) (cid:19)(cid:19)(cid:35) ψt,j(x1) (log πi + gi) (log πj + gj) = = = = = λ τ (t) xt,i (cid:88) j= (cid:18) (cid:19) xt,j (log πi + gi) (log πj + gj) (36) By our definition of the Gumbel-Softmax interpolant, the intermediate distributions during inference represent mixture of learned conditional interpolants ψt(x1) from the training data. Since the denoising model is trained to predict the true clean distribution, we can set the Gumbel-noise random variable in the conditional velocity fields to 0 during inference as we want the velocity field to point toward the predicted denoised distribution. Substituting in πi = exp(δik), we have ut,i(xtx1 = ek) = λ τ (t) xt,i (cid:88) j= xt,j (δik δjk) Since δij = 1 only when is the index of the target token = and 0 otherwise, the velocity field can be rewritten as ut,i(x0x1 = ek) = (cid:40) λ τ (t) xt,i λ τ (t) xt,i (cid:80)V (cid:80)V j=1 j=1 (cid:0)xt,j (1 δjk)(cid:1) (cid:0)xt,j (δjk)(cid:1) = = λ exp(λt) τmax λ exp(λt) τmax (cid:40) λ exp(λt) τmax λ exp(λt) τmax = = xt,i xt,i (cid:16)(cid:80)V (cid:16) j=1 xt,j (cid:80)V j=1 xt,jδjk (cid:17) j=1 xt,jδjk (cid:80)V xt,i (1 xt,k) xt,i (xt,k) = = Rewriting in vector form, we get ut(xtx1 = ek) = λ τ (t) xt,k (ek xt) which points toward the target vertex ek. 20 (cid:17) = i = (37) (38) C.2 Proof of Continuity Proposition 1. The proposed conditional vector field and conditional probability path satisfy the continuity equation and thus define valid flow-matching trajectory in the interior of the simplex. pt(x) = (pt(x)ut(xt)) (39) Proof of Proposition 1. During training, each clean sequence x1 is transformed into some noisy interpolant ψt(xt) with sampled Gumbel-noise vector Gumbel(0, 1). Therefore, we can rewrite the interpolant as deterministic path conditioned on the one-hot distribution x1 and Gumbel-noise vector ψt(x1) = ψt(x1, g) = SM (cid:19) (cid:18) x1 + τ (t) (40) With this definition, we can define deterministic probability path as the Dirac delta function along the interpolant xt = ψt(x1) as So, we can rewrite the continuity equation as pt(xx1) = δ(x ψt(x1)) pt(xx1) = δ(x ψt(x1)) (cid:18) (cid:19) ψt(x1) = δ(x ψt(x1)) ψt(x1) (41) (42) First, we will simplify the right-hand side (RHS) of the continuity equation. Taking the derivative with respect to t, we get Taking the distributional derivative with an arbitrary test function (x) independent of t, we have pt(xx1) = δ(x ψt(xt)) = = = (cid:90) t δ(x ψt(xt, g))dx (x) (cid:90) (x)δ(x ψt(xt))dx (ψt(x1)) Since ψt(x1) RV , we apply the multivariable chain rule to get pt(xx1) = (ψt(x1)) ψt(x1) (43) (44) Now, we integrate the left-hand side (LHS) of the continuity equation with an arbitrary test function. (cid:90) (cid:20) δ(x ψt(x1)) (x) (cid:21) ψt(x1) (cid:20)(cid:90) dx = (x)δ(x ψt(x1))dx (cid:21) ψt(x1) (45) Using integration by parts, we can write the term inside the bracket as (cid:90) (cid:90) (x)δ(x ψt(x1))dx = (x)δ(x ψt(x1)) δ(x ψt(x1))f (x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:123)(cid:122) = (cid:125) (cid:124) = (cid:90) δ(x ψt(x1))f (x) (46) Substituting this back into the LHS, we get (cid:21) (cid:90) (cid:20) δ(x ψt(x1)) (x) ψt(x) dx = (cid:20) (cid:90) δ(x ψt(x1))f (x) (cid:21) ψt(x1) = (ψt(x1)) ψt(x1) (47) We have shown that both sides of the continuity equation produce the same expression when integrated against any arbitrary test function (x). So, we can conclude pt(xx1) = δ(x ψt(x1)) ψt(x1) = (pt(xx1)ut(xtx1)) (48) Now that we have shown the continuity equation holds for the conditional probability density and flow velocities, it follows that the continuity equation holds for the unconditional flow. Following the proof in [21], we have dt pt(xt) = = = dt (cid:90) x1 (cid:90) x1 (cid:90) pt(xtx1)p1(x1)dx x1 dt pt(xtx1)p1(x1)dx1 (cid:18) (cid:19) pt(xtx1)ut(xtx1)p1(x1) dx1 (substitute conditional continuity) = (cid:18)(cid:90) pt(xtx1)ut(xtx1)p1(x1)dx1 (cid:19) x1 = (pt(xt)ut(xt)) which concludes the proof. C.3 Proof of Flow Matching Propositions (49) Proposition 1 (Probability Mass Conservation) The conditional velocity field preserves probability mass and lies on the tangent bundle at point xt on the simplex TxtV 1 = {ut RV 1, ut = 0}. Proof of Proposition 1. We show that the conditional velocity field derived from the Gumbel-Softmax interpolant preserves probability mass such that (cid:88) i=1 ut,i(xtx1 = ek) = 0 (50) Summing up the velocities for all [1 . . . ], we have (cid:88) i= ut(x0x1 = ek) = (cid:34) (cid:88) i=1 λ τ (t) xt,i (cid:88) (cid:18) xt,j (log πi + gi) (log πj + gj) (cid:19)(cid:35) j=1 (cid:34) (cid:88) j=1 (cid:34) (cid:34) (cid:88) i= (cid:88) (cid:34) i=1 (cid:34) (cid:88) i=1 (cid:34) (cid:88) i=1 = λ τ (t) = λ τ (t) = λ τ (t) = λ τ (t) = 0 (cid:35)(cid:35) (cid:35)(cid:35) xt,i xt,j(log πi + gi) (cid:88) xt,j(log πj + gj) (cid:88) j=1 xt,j j=1 (cid:88) j=1 xt,i (log πi + gi) xt,j(log πj + gj) xt,i(log πi + gi) xt,i(log πi + gi) (cid:35) xt,j(log πj + gj) xt,i (cid:88) j=1 (cid:35) xt,j(log πj + gj) (cid:88) i=1 (cid:88) j=1 (51) which proves that our velocity field always preserves the probability mass t. Proposition 3. (Valid Flow Matching Loss) If pt(xt) > 0 for all xt Rd and [0, 1], then the gradients of the flow matching loss and the Gumbel-Softmax FM loss are equal up to constant not dependent on θ such that θLFM = θLgumbel Proof of Proposition 3. We can rewrite the conditional velocity field derived in Appendix C.1 as ut(xtx1 = ek) = = λ τ (t) λ τ (t) xt,k (ek xt) (cid:88) i=1 xt,i (ei xt) ei, x1 (52) Furthermore, the predicted velocity field is given by uθ (xt) = (cid:88) i=1 ut(xtx1 = ei) ei, xθ = λ τ (t) (cid:88) i=1 xt,i(ei xt) ei, xθ (53) Substituting the velocity field expressions into the flow-matching loss, we obtain (xt)2 Ept(xt)ut(xtx1) uθ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = Ept(xt) λ τ (t) (cid:88) xt,i (ei xt) ei, x1 λ τ (t) (cid:88) i=1 xt,i(ei xt) ei, xθ (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) i=1 (cid:88) (cid:88) i=1 (cid:88) i=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) i=1 (cid:88) i= (cid:88) i=1 Ept(xt) Ept(xt) Ept(xt) Ept(xt) Ept(xt) (cid:20) xt,i (ei xt) ei, x1 xt,i(ei xt) ei, xθ (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:20) xt,i (ei xt) ei, x1 ei, xθ (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) xt,i (ei xt) ei, x1 xθ xt,i (ei xt) (x1 xθ)i (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) xt,i(x1 xθ)iei (cid:88) i=1 xt,i(x1 xθ)ixt (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) = = = = = = λ2 τ (t)2 λ2 τ (t)2 λ2 τ (t)2 λ2 τ (t)2 λ2 τ (t)2 λ2 τ (t) Ept(xt) xt (x1 xθ) xtxt, x1 xθ2 (54) The remainder of the proof extends that of [20, 21], which proved that the conditional flow matching loss θLCFM = θLFM under similar constraints. First, we further expand the conditional flow-matching loss as follows Ept(xt)ut(xtx1) uθ (xt, t)2 = = = = λ2 τ (t)2 λ2 τ (t)2 λ2 τ (t)2 λ2 τ (t)2 Ept(xt) xt (x1 xθ) xtxt, x1 xθ2 Ept(xt) xt x1 xt xθ xtxt, x1 xθ Ept(xt) xt x1 xt (xθ xt, x1 xθ)2 (cid:28) (cid:20) Ept(xt) xt x12 2 xt x1, xt (xθ xt, x1 xθ) (cid:29) + xt (xθ xt, x1 xθ)2 (cid:21) Then, taking the gradient with respect to θ, we have (xt, t)2 = θEpt(xt)ut(xtx1) uθ λ2 τ (t)2 θEpt(xt) (cid:20) λ2 τ (t)2 2θEpt(xt) = (cid:28) (cid:20) xt x12 2 (cid:28) xt x1, xt (xθ xt, x1 xθ) (cid:29) + xt (xθ xt, x1 xθ)2 (cid:21) xt x1, xt (xθ xt, x1 xθ) (cid:29) + θEpt(xt) (cid:13) (cid:13) (cid:13) (cid:13) xt (xθ xtx1, xtxθ) 2(cid:21) (cid:13) (cid:13) (cid:13) (cid:13) Now, we rewrite x1 as the expectation over noisy samples xt learned by the model. By Bayes theorem, we have (55) p(x1x1) = pt(xtx1)p1(x1) pt(xt) Then, defining x1 as an expectation over pt(xt), we get x1 = Ep(x1xt) [x1] (cid:90) = = x1p(x1xt)dx1 x1 (cid:90) x1 pt(xtx1)p1(x1) pt(xt) dx1 Now, we substitute this into the first expectation in the gradient to get Ept(xt) (cid:28) (cid:29) xt x1, xt (xθ xtx1, xtxθ) (56) (57) = = = = = (cid:90) (cid:90) (cid:90) xt xt (cid:28) (cid:28) (cid:28) xt (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) x1 x1 xt xt xt (cid:28) x1 pt(xtx1)p1(x1) pt(xt) (cid:18) (cid:28) dx1, xt xθ xt (cid:90) x1pt(xtx1)p1(x1)dx1, xt xθ xt (cid:18) (cid:28) x1pt(xtx1)p1(x1)dx1, xt xθ (cid:18) (cid:29) (cid:90) x1 x1 pt(xtx1)p1(x1) pt(xt) (cid:29)(cid:19) (cid:29) dx1, xtxθ pt(xt)dxt x1pt(xtx1)p1(x1)dx1, xtxθ dxt (cid:29)(cid:19) (cid:29) (cid:19) (cid:29) x1 (cid:90) x1 xtx1, xtxθ pt(xtx1)p1(x1)dx1 dxt xt x1, xt (xθ xtx1, xtxθ) pt(xtx1)p1(x1)dx1dxt xt x1 (cid:90) (cid:90) (cid:28) x1 xt xt x1, xt (xθ xtx1, xtxθ) pt(xtx1)p1(x1)dxtdx1 (cid:29) = Ept(xtx1),p1(x1) (cid:28) (cid:29) xt x1, xt (xθ xt, x1 xθ) (58) where we use the linearity properties of integration. Following similar logic, we have Ept(xt)xt (xθ xtx1, xtxθ)2 = = = = (cid:90) (cid:90) xt xt (cid:90) xt (xθ xtx1, xtxθ)2pt(xt)dxt (cid:13) (cid:13) (cid:13) (cid:13) (cid:90) (cid:18) (cid:28) xt xθ xt (cid:90) x1 pt(xtx1)p1(x1) pt(xt) dx1, xtxθ 2 (cid:29)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) pt(xt)dxt xt (xθ xtx1, xtxθ)2 pt(xtx1)p1(x1)dx1dxt xt (cid:90) x1 (cid:90) x1 xt xt (xθ xtx1, xtxθ)2 pt(xtx1)p1(x1)dxtdx1 = Ept(xtx1),p1(x1)xt (xθ xtx1, xtxθ)2 (59) using the fact that the squared norm can be expressed as bilinear inner product. 24 Substituting these terms back into the gradient of the flow-matching loss, we get θEpt(xt)ut(xtx1) uθ (xt, t)2 (cid:28) (cid:29) (cid:20) 2θEpt(xt) xt x1, xt (xθ xt, x1 xθ) + θEpt(xt) xt (xθ xtx1, xtxθ) (cid:13) (cid:13) (cid:13) (cid:13) 2(cid:21) (cid:13) (cid:13) (cid:13) (cid:13) (cid:20) 2θEpt(xtx1),p1(x1) (cid:28) xt x1, xt (xθ xt, x1 xθ) (cid:29) + θEpt(xtx1),p1(x1) (cid:13) (cid:13) (cid:13) (cid:13) xt (xθ xtx1, xtxθ) (cid:13) (cid:13) (cid:13) (cid:13) 2(cid:21) = = λ2 τ (t)2 λ2 τ (t)2 = θEpt(xt) (cid:20) λ2 τ (t)2 xt (x1 xθ) xtxt, x1 xθ (cid:21) = λ2 τ (t)2 θEpt(xt) xt (x1 xθ) xtxt, x1 xθ2 which concludes the proof that θLgumbel = θLFM."
        },
        {
            "title": "D Score Matching Derivations",
            "content": "D.1 Derivation of the Score Function (60) We start by showing that the score function of the marginal probability density xt log pt(xt) is proportional to the conditional probability density xt log pt(xtx1) given that pt(xt) = Ex1p1(x1) Taking the gradient of the marginal log probability density and substituting in the definition of pt(xt), we have (cid:2)pt(xtx1)(cid:3). xt log pt(xt) = xtpt(xt) pt(xt) = = = = = xt Ex1pdata (cid:2)pt(xtx1)(cid:3) pt(xt) (cid:2)p(x1)pt(xtx1)(cid:3)dx1 xt (cid:82) x1 x1 (cid:82) (cid:82) (cid:90) pt(xt) p(x1)xtpt(xtx1)dx1 pt(xt) p(x1)pt(xtx1) xt pt(xtx1) pt(xt) pt(xtx1) dx1 pt(xtx1)p(x1) pt(xt) (cid:123)(cid:122) =pt(x1xt) (cid:124) (cid:125) xt log pt(xtx1)dx1 = Ex1pt(x1xt) [xt log pt(xtx1)] (61) which proves that with the perfect model such that pt(x1) = p(x1xt), the gradient of the marginal log-probability density is exactly the expectation of the conditional log-probability density over the training data xt log pt(xt) = Ex1p1(x1) [xt log pt(xtx1)]. Theorem 2. The gradient of the log-probability density of the EXPCONCRETE distribution is given by xt,i log pt(xtx1) = τ (t) τ (t)V SM δik τ (t)xt,i (62) (cid:18) (cid:19) Proof of Theorem 2. First, we start by defining the probability density of the EXPCONCRETE distribution. From [16], integrating out the Gumbel-noise random variable we have pt(x) = (V 1)!τ (cid:32) (cid:88) i=1 πj exp(τ xt,j) (cid:33) (cid:32) (cid:89) i= (cid:33) πi exp(τ xt,i) (63) 25 where xt,i is defined as logit from the EXPCONCRETE distribution xt,i = log πi + gi τ log (cid:88) j=1 exp (cid:18) log πj + gj τ (cid:19) (64) Taking the logarithm of the probability path, we have log pt(xtx1) = log[(V 1)!] + (V 1) log τ + log = log[(V 1)!] + (V 1) log τ + = log[(V 1)!] + (V 1) log τ + = log[(V 1)!] + (V 1) log τ + (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 (cid:33) πi exp(τ xt,i) log (cid:32) (cid:89) i= (cid:88) j=1 πj exp(τ xt,j) log (πi exp(τ xt,i)) log (cid:88) j= exp (log (πj exp(τ xt,j))) (log πi τ xt,i) log (cid:18) exp log πj τ xt,j (cid:88) j=1 log πi (cid:88) i=1 τ xt,i log (cid:88) j=1 (cid:18) exp log πj τ xt,j (65) (cid:19) (cid:19) Then differentiating with respect to the logit of single token xt,j, we get xt,j log pt(xtx1) = xt,i = τ (cid:88) i=1 (cid:32) τ xt,i xt,i log (cid:18) (cid:19) exp log πj τ xt,j (cid:88) j=1 exp(log πi τ xt,i)(τ ) (cid:33) (cid:33) 1 j=1 exp(log πj τ xt,j) (cid:80)V (cid:32) = τ + τ (cid:80)V exp(log πi τ xt,i) i=1 exp(log πj τ xj) (cid:18) (cid:19) = τ + τ SM log πi τ xt,i (66) Introducing time-dependence with τ (t) = τmax exp(λt) and target token dependence with πi = exp(δik), we have xt,i log pt(xtx1) = τ (t) τ (t)V SM δik τ (t)xt,i (67) (cid:18) (cid:19) D.2 Proof of Score Matching Propositions Proposition 4. The gradient of the EXPCONCRETE log-probability density is proportional to the gradient of the Gumbel-softmax log-probability density such that GS log pθ(xtx1) xt,j ExpConcrete log pθ(xtx1). xt,j Proof of Proposition 4. As derived in [16], the explicit probability density of the Gumbel-Softmax distribution is defined as p(x) = (V 1)!τ 1 (cid:32) (cid:88) i=1 πi xτ t,i (cid:33)V (cid:89) (cid:32) i=1 (cid:33) πi xτ +1 t,i (68) We now derive the log-probability density of the Gumbel-Softmax distribution as log p(x) = log[(V 1)!] + (V 1) log τ log = log[(V 1)!] + (V 1) log τ log (cid:88) i=1 (cid:88) i=1 πi xτ t,i πi xτ t,i + + (cid:88) i=1 (cid:88) i=1 (cid:32) log (cid:33) πi xτ +1 t,i log (πi) (τ + 1) (cid:88) i=1 log(xt,i) (69) 26 Taking the gradient with respect to single token xt,j, we have GS xt,j log pt(xtx1) = xt,j log (cid:32) (cid:88) i=1 (cid:33) πi xτ t,i (cid:32) xt,j (τ + 1) (cid:33) log(xt,i) (cid:88) i= (cid:33) (cid:32) πjτ xτ +1 t,j τ + 1 xt,j = 1 (cid:80)V i=1 πi xτ t,i = = = = = = τ xt,j τ xt,j τ xt,j τ xt,j 1 xt,j 1 xt,j (cid:33) τ + 1 xt,j πjxτ t,j i=1 πixτ t,i exp(log(πjxτ t,j )) i=1 exp(log(πixτ t,i )) (cid:80)V (cid:80)V (cid:33) τ + 1 xt,j (cid:32) (cid:32) (cid:32) (cid:80)V exp(log πj τ xt,j) i=1 exp(log πi τ xt,i) τ + 1 xt,j (cid:1) SM(cid:0) log πi τ xt,i (cid:33) τ + 1 xt,j (cid:18) (cid:18) τ + τ SM(cid:0) log πi τ xt,i (cid:19) (cid:1) 1 xt,j ExpConcrete xt,j (cid:19) log pt(xtx1) 1 xt,j (70) Therefore, we show that the gradients of the Gumbel-Softmax and EXPCONCRETE distributions are proportional to each other. Furthermore, we derive that the score of Gumbel-Softmax distribution further amplifies the scores for tokens with low probabilities by dividing by xt,j and subtracting x1 t,j . Straight-Through Guided Flow Derivations Proposition 5. (Probability Mass Conservation of Straight-Through Gradient) The straight through gradient xtpϕ(yx1,m) preserves probability mass and lies on the tangent bundle at point xt on the simplex TxtV 1 = {xtpϕ(yx1,m) RV 1, xtpϕ(yx1,m) = 0}. Proof of Proposition 5. First, we recall our definition of the straight-through gradient of the classifier score pϕ(yx1,m) as xt,ipϕ(yx1,m) = (cid:40) pϕ(yx1,m) x1 pϕ(yx1,m) (cid:2)SM(xt,i) (1 SM(xt,k)) (cid:3) (cid:2) SM(xt,i)SM(xt,k)(cid:3) = = Taking the sum over the simplex dimensions, we have (cid:88) i=1 xt,i pϕ(yx1,m) = = = pϕ(yx1,m) x1 pϕ(yx1,m) x1 (cid:20) pϕ(yx1,m) SM(xt,k) (1 SM(xt,k)) (cid:88) i=k SM(xt,i)SM(xt,k) SM(xt,k) (1 SM(xt,k)) SM(xt,k) (cid:88) i=k SM(xt,i) (cid:21) SM(xt,k) (1 SM(xt,k)) SM(xt,k) (1 SM(xt,k)) = 0 which concludes the proof. In addition, it follows that the sum of straight-through gradients also preserves probability mass and lies on the tangent space of the simplex at any point."
        },
        {
            "title": "F Model Architecture",
            "content": "F.1 Diffusion Transformer To parameterize our flow and score matching models for the protein and peptide sequence generation tasks, we leverage the Diffusion Transformer (DiT) architecture [45] which integrates time condition27 Figure 6: Predicted binding-affinity scores over iteration of Gumbel-Softmax FM guided with STGFlow for target-binding peptide generation. The predicted binding affinity is the mean regression scores of the discrete sequences sampled at each integration step. The gradients of the scores are used to compute the guided velocity. ing with adaptive layer norm (adaLN) and positional information with Rotary Positional Embeddings (RoPE) [46]. Our model consists of 32 DiT blocks, 16 attention heads, hidden dimension of 1024, and dropout of 0.1. Layers Input Dimension Output Dimension Table 5: Diffusion Transformer Architecture Sequence Distribution Embedding Module Feed-Forward + GeLU DiT Blocks 32 Adaptive Layer Norm (time conditioning) Multi-Head Self-Attention (h = 16) + Rotary Positional Embeddings Dropout + Residual Adaptive Layer Norm (time conditioning) FFN + GeLU DiT Final Block Adaptive Layer Norm (time conditioning) Linear vocab size vocab size 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 vocab size F.2 Peptide-Binding Affinity Classifier We trained multi-head cross-attention network with ESM-2 650M [27] protein and peptide sequence embeddings to predict the binding affinity of peptide to protein sequence. We trained on 1781 sequences from the PepLand [47] protein-peptide binding dataset containing the protein-target sequence, peptide sequence, and the experimentally-validated Kd/Ki/IC50 binding affinity score, where higher values indicate stronger binding. In addition to the normalized binding affinity scores through regression, we also classified affinities into three categories: low (< 6.0), Medium (6.0 7.5), and Tight ( 7.5), with thresholds based on mean and Q3 quantile from the data distribution. The combined classification and regression approach helped the model better capture relationships between protein embeddings and binding affinities. Data was split in 0.8/0.2 ratio with stratification preserving the score distribution. We used OPTUNA [48] for hyperparameter optimization, tracking validation correlation, and F1 scores across 10 trials, resulting in an optimal learning rate of 3.84e 05 and dropout rate of 0.15. We retrain the whole classifier  (Table 6)  with the optimized set of parameters. After training for 50 epochs with early stopping based on validation Spearman correlation, the model achieved Spearman 28 correlation of 0.96 on training data and 0.64 on validation data, with F1 scores of 0.97 and 0.61 respectively. Layers Protein Dimension Peptide Dimension Table 6: Peptide-Binding Affinity Classifier Embedding Module CNN Layers 3 (Kernel Sizes: 3,5,7)"
        },
        {
            "title": "ReLU Activation",
            "content": "Global Pooling (Max + Avg) Linear Layer Layer Norm Cross-Attention 4 Multi-Head Attention (h = 8) Linear Layer ReLU Dropout Linear Layer Shared Prediction Head"
        },
        {
            "title": "Linear Layer\nReLU\nDropout\nRegression Head",
            "content": "1280 (1280, L) (64, L) per kernel (64 3, L) 384 384 1280 (64 3, L) per kernel (64, L) per kernel 64 3 2 384 384 384 2048 2048 2048 384 384 2048 2048 2048 384 1024 1024"
        },
        {
            "title": "G Experimental Details",
            "content": "G.1 Simplex-Dimension Toy Experiment We reproduce the experimental setup of the toy experiment in Davis et al. [13]. We train 100, 000 sequences sampled from randomly generated distribution over the (K 1)-dimensional simplex for = {20, 40, 60, 80, 100, 120, 140, 160, 512}. We extend the experiment to dimension 512 to evaluate performance in higher simplex dimension. For the model architecture, we follow Stark et al. [4] and parameterize all benchmark models with 5-layer CNN with approximately 1M parameters that vary slightly with simplex dimension. After 50K steps, we evaluate the KL divergence KL(qpdata) where is the normalized distribution from 51.2K sequences generated by the model and pdata is the distribution from which the training data was sampled. Table 7: KL divergences of toy experiment for increasing simplex dimensions compared to benchmark models. The sequence length is set to constant of 4 across all experiments. The toy models are trained on 100K sequences from random distribution. KL divergence is evaluated for 51.2K sequences after 50K training steps. Simplex Dimension 40 60 80 100 120 160 512 Linear FM Dirichlet FM Fisher FM (Optimal Transport) Gumbel-Softmax FM (Ours) 0.013 0.007 0.0004 0.029 0.046 0.017 0.007 0.027 0.070 0.032 0.007 0. 0.100 0.035 0.007 0.027 0.114 0.028 0.008 0.030 0.112 0.024 0.043 0.029 0.156 0.039 0.013 0.035 0.146 0.053 0.013 0.038 0.479 0.554 0.036 0. G.2 Hyperparameter Selection Maximum Temperature τmax. The maximum temperature controls the uniformity of the probability distribution at = 0 when exp(λt) = 1. Theoretically, the probability distribution is fully uniform ψ0(xtx1) = 1 when τmax . Empirically, we find that setting τmax = 10.0 ensures that the distribution is near uniform at = 0 even after applying Gumbel noise, satisfying the boundary condition ψ0(xtx1) 1 . 29 Figure 7: Validation MSE loss over training step of simplex-dimension toy experiment. Fisher FM exhibits significantly higher validation MSE loss during training than Gumbel-Softmax FM despite the same loss calculation, suggesting that the parameterization easily overfits to training data. Decay Rate λ. The decay rate determines how quickly the temperature drops as 1. decay rate of λ = 1 means that the function becomes exp(t) which drops the temperature to 0.367 at = 1. Since we want the temperature to approach 0 to increase the concentration of probability mass at the vertex, we set λ = 3.0 such that τ (t) = τmax exp(3.0t). For larger decay rates λ = 10.0, the distribution converges too quickly to vertex which may cause overfitting. Stochasticity Factor β. We can tune the effect of the Gumbel noise applied during training by scaling down by factor β 1.0 such that gi = log( log(Ui+ϵ)+ϵ) . For larger β, the stochasticity decreases and for smaller β, the stochasticity increases. For the toy experiment, we found similar performance for noise factors ranging between β = 2.0 10.0. The remaining experiments were conducted with β = 2.0. β Nsteps Step Size η and Integration Steps Nsteps. For Gumbel-Softmax FM, the step size is equal to = 1 since we are integrating the velocity field from = 0 1. For Gumbel-Softmax SM, the step size determines the rate of convergence to high-probability density regions. Small step sizes η 0.1 increase computation cost and number of steps needed to converge. In contrast, larger step sizes 0.1 η 1.0 increase the speed of convergence but may result in mode-collapse to the high-density regions. Empirically, we found that step size of η = 0.5 is optimal with the number of integration steps Nsteps = 100. Guidance Scale γ. Given that the softmax gradients tend to be small, especially for low-probability tokens, the guidance scale γ amplifies the gradient value across all tokens to ensure effective guidance. For the target-guided peptide design experiments, we set γ = 10.0 to scale the guidance term to be in the order 101 which produced increasing classifier scores over iterations. Number of Guidance Samples . For STGFlow, the number of guidance samples determines the number of discrete sequences that are sampled from the distribution xt at each time step to compute the aggregate straight-through gradient. Larger enables more informed and precise guidance based on the culmination of the classifier on various token combinations to determine tokens that lead to enhanced classifier scores, while smaller results in more spurious guidance that may not lead to truly optimal sequences. We found that = 10 maintained good balance between effective guidance while minimizing computational costs. G.3 Protein Evaluation Metrics We evaluate protein generation quality based on the following metrics computed by ESMFold [27]. 1. pLDDT (predicted Local Distance Difference Test) measures residue-wise local structural confidence on scale of 0-100. Proteins with mean pLDDT > 70 generally correspond to correct backbone prediction and more stable proteins. 2. pTM (predicted Template Modeling) measures global structural plausibility. High pTM corresponds to high similarity between predicted structure and hypothetical true structure. 3. pAE (predicted Alignment Error) measures the confidence in pair-wise positioning of residues. Low pAE scores correspond to low predicted pair-wise error. 30 In addition, we compute: 1. Token entropy measures the diversity of tokens within each sequence. It is defined as the Shannon entropy, where pi is the probability of i-th unique token divided by the total number of tokens in the sequence. = (cid:88) i=1 pi log2(pi) 2. Diversity is calculated as 1 pairwise sequence identity within batch of generated sequences with equal length. G.4 Peptide Evaluation Metrics We evaluate our de novo peptide binders based on two metrics that measure their affinity to their target protein. ipTM Score. We use AlphaFold3 [49] to compute the interface predicted template modeling (ipTM) score which is on the scale from 0-1 and measures the accuracy of the predicted relative positions between residues involved in the interaction between the two sequences. pTM Score. We use AlphaFold3 [49] to compute the predicted template modeling (pTM) score which is on scale from 0-1 and measures the accuracy of the predicted structure of the whole peptide-protein complex. This score is less relevant when evaluating binding affinity since it can be dominated by the stability of the target protein. VINA Docking Score. We use Autodock Vina [50] (v 1.1.2) for in silico docking of the peptide binders to their target proteins  (Table 3)  to confirm binding affinity. The complex was first docked with Alphafold3 for the starting conformation [49]. The final results were visualized in PyMol [51] (v 3.1), where the residues in the protein targets with polar contacts to the peptide binder with distances closer than 3.5 Å are annotated. Figure 8: Gumbel-Softmax FM generated peptide binders for three targets with no known binders. (A) 7 a.a. designed binder to NPC2 (PDB: 6W5V) involved in Niemann-Pick Disease Type C. (B) 10 a.a. designed binder to BMI1 (PDB: 2CKL) involved in Medulloblastoma. (C) 10 a.a. designed binder to Gigaxonin (PDB: 3HVE) involved in Giant Axonal Neuropathy. Docked with AutoDock VINA and polar contacts within 3.5 Å are annotated. Additional targets are shown in Table 4."
        },
        {
            "title": "H Algorithms",
            "content": "In this section, we provide detailed procedures for the training and inference of the flow and scorematching models. Algorithm 1 and 2 describe training and sampling with Gumbel-Softmax FM, respectively. Algorithm 3 and 4 describe training and sampling with Gumbel-Softmax SM, respectively. We consider x1 as single token in sequence for simplicity, but in practice, the training and sampling is conducted on sequence of tokens of length L. 31 Algorithm 1 Training Gumbel-Softmax Flow Matching Inputs: Training sequences of one-hot vectors x1 D, parameterized neural network NNθ(xt, t), maximum temperature τmax, decay rate λ, and learning rate η. procedure TRAINING GUMBEL-SOFTMAX FM for x1 in batch do Sample Uniform(0, 1) Set τ (t) τmax exp(λt) Sample Uniform(0, 1)V Sample Gumbel noise vector = log( log(U + ϵ) + ϵ) Given the clean token x1 = ek, sample noisy interpolant for time xt,i exp (cid:16) δik+(gi/β) τ (t) (cid:17) (cid:80)V j=1 exp (cid:16) δjk+(gj /β) τ (t) (cid:17) if denoise then Predict xθ(xt, t) NNθ(xt, t) Minimize negative log loss Ldenoise Ex1D (cid:104) log(x(k) θ (xt, t)) (cid:105) else (xt) NNθ(xt, t) Predict uθ Calculate ut(xtx1) λ exp(λt) Optimize denoising loss Lmse Ex1Duθ τmax [(xt x1)(1 xt,k) (xt (1 x1))xt,k] (xt) ut(xtx1)2 end if θ θ + ηθLdenoise end for end procedure Algorithm 2 Unconditional Sampling with Gumbel-Softmax Flow Matching Inputs: Trained neural network NNθ(xt, t), number of integration steps Nstep Output: Clean sequence from learned data distribution procedure SAMPLING GUMBEL-SOFTMAX FM Compute step size 1 Nstep Sample uniform distribution x0 1 Set xt x0 for = 0 1 do Compute τ (t) τmax exp(λt) if denoise then Predict xθ(xt, t) NNθ(xt, t) for all simplex dimensions [1, ] do ut(xtx1 = ek) = λ τ (t) xt,k (ek xt) end for Calculate conditional velocity field uθ (xt) (cid:88) k=1 ut(xx1 = ek) xθ(xt, t), ek else Directly predict conditional velocity field uθ (xt) NNθ(xt, t) end if Take step xt xt + uθ xt SIMPLEXPROJ(xt) (xt) end for Sample sequence arg max(xt) return end procedure Algorithm 3 Training Gumbel-Softmax Score Matching Inputs: Training sequences of one-hot vectors x1 D, parameterized neural network NNθ(xt, t), maximum temperature τmax, decay rate λ, and learning rate η. procedure TRAINING GUMBEL-SOFTMAX SM for x1 in batch do Sample Uniform(0, 1) Set τ (t) τmax exp(λt) Sample Uniform(0, 1)V Sample Gumbel noise vector = log( log(U + ϵ) + ϵ) Given the clean token x1 = ek, sample noisy interpolant for time xt,i exp (cid:16) δik+(gi/β) τ (t) (cid:17) (cid:80)V j=1 exp (cid:16) δjk+(gj /β) τ (t) (cid:17) Predict fθ(xt, t) NNθ(xt, t) Optimize loss given x1 = ek Lscore Ex1Dfθ(xt, t) (δik + τ (t)xt,i)2 θ θ + ηθLscore end for end procedure Algorithm 4 Unconditional Sampling with Gumbel-Softmax Score Matching Inputs: Trained score model sθ(xt, t), step size , noise factor β Output: Clean sequence from learned data distribution procedure SAMPLING x0 1 Set xt x0 for = 0 1 do Compute τ (t) τmax exp(λt) Predict fθ(xt, t) NNθ(xt, t) Compute predicted score sθ(xt, t) τ (t) + τ (t)V SM(cid:0)fθ(xt, t)(cid:1) xt xt + sθ(xt, t) xt SIMPLEXPROJ(xt) end for Sample sequence arg max(xt) return end procedure 33 Algorithm 5 Straight-Through Guided Flow Matching (STGFlow) Inputs: Trained simplex-based flow matching model uθ (xt), trained classifier model pϕ(yx) : that takes sequence of length and returns classifier score, number of integration steps Niter Output: Clean sequence from learned data distribution with optimized classifier score procedure GUIDED SAMPLING WITH STGFLOW Compute step size 1 Nstep x0 1 Set xt x0 for = 0 1 do Predict unguided conditional velocity field uθ Take step xt xt + uθ Compute top-k distribution SM (topk(xt)) Sample sequences from topk distribution x1,m SM (topk(xt)) Initialize total guided velocity uϕ for each x1,m do (xt) as in Algorithm 2 (xtx1, y) 0 (xt) Compute score pϕ(yx1,m) Compute straight-through gradient with respect to distribution xt (cid:40) pϕ(yx1,m) x1 pϕ(yx1,m) xtpϕ(yx1,m) = [SM(xt,i) (1 SM(xt,k))] [SM(xt,i)SM(xt,j)] (xtx1, y) + xtpϕ(yx1,m) Add to total guidance uϕ (xtx1, y) uϕ = = end for Add total guided velocity xt xt + γ uϕ (xtx1, y) end for Sample sequence xt return end procedure 34 Figure 9: Predicted structures of de novo generated proteins with Gumbel-Softmax FM. Generated proteins demonstrate diverse structural generation."
        }
    ],
    "affiliations": [
        "Center of Computational Biology, Duke-NUS Medical School",
        "Department of Biomedical Engineering, Duke University",
        "Department of Biostatistics and Bioinformatics, Duke University",
        "Department of Computer Science, Duke University",
        "Management and Technology Program, University of Pennsylvania",
        "Mila, Quebec AI Institute",
        "Université de Montréal"
    ]
}