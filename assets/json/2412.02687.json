{
    "paper_title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
    "authors": [
        "Viet Nguyen",
        "Anh Nguyen",
        "Trung Dao",
        "Khoi Nguyen",
        "Cuong Pham",
        "Toan Tran",
        "Anh Tran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 2 7 8 6 2 0 . 2 1 4 2 : r SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance Viet Nguyen1* Anh Nguyen1 Trung Dao1 Khoi Nguyen1 Cuong Pham1,2 1VinAI Research Toan Tran1 Anh Tran1 2Posts & Telecom. Inst. of Tech. https://snoopi-onestep.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into onestep ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher models performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance - SwiftBrush (PG-SB), which employs randomscale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting new state-of-the-art benchmark for one-step diffusion models. 1. Introduction Diffusion models have recently become popular due to their capacity to produce high-quality, diverse outputs in image [2, 4, 34, 39, 52], 3D [23, 35, 48], audio [10, 17] and video *Equal contribution. [3, 11, 50, 53] synthesis. Unlike other generative models, such as Generative Adversarial Networks (GANs), diffusion models gradually refine their output through series of steps, allowing them to achieve diverse yet impressively detailed and realistic outputs. However, this iterative refinement is both time-consuming and computationally demanding, which limits their practicality in real-world applications. As result, theres increasing interest in techniques to speed up diffusion models while maintaining output quality. Several strategies are being explored to enhance their efficiency, including optimizing model architectures, improving sampling methods, and using latent space representations instead of image space. Recently, particularly promising approach has gained traction: timestep distillation. Researchers have found that by adding specific constraints while training another model to mimic pre-trained multi-step teacher, they can reduce the required steps, creating faster and more efficient student that retains much of the original quality with minimal adjustments. Current approaches for accelerating text-to-image diffusion models fall into two main categories: image-dependent training and image-free training methods. Image-dependent methods, which rely on real images or synthetic ones generated by the teacher models, include adversarial-based approaches [19, 41, 42, 51] and reconstruction-based methods [12, 19, 28, 56]. While effective, these methods demand huge storage, high memory, extensive computational power and might suffer from mode collapse due to too strong image supervision. In contrast, image-free methods [33], which mainly use score distillation losses like Score Distillation Sampling (SDS) [36] or Variational Score Distillation (VSD) [48], avoid the need for image datasets, instead requiring minimal data, often just text prompts. Models such as SwiftBrushv2 (SBv2) [7], even when omitting imageregularization term, making it fully image-free distillation approach, have achieved impressive quality, even outperforming their multi-step counterparts on certain benchmarks. Hence, in this paper, we focus on the image-free approach and start with SBv2 as the baseline. 1 Figure 1. Comparison of various methods, including SNOOPI and alternatives (SwiftBrush, DMD, YOSO, PG-SB), using the PixArt-α backbone. Each method generates an image with the prompt Vincent van Gogh (green). PG-SB helps stabilize the training process of image-free distillation, addressing the typical instability in such training. SNOOPI, combination of PG-SB and the NASA module, enables effective negative prompting (red) even for one and few-step models, allowing for the removal of specific unwanted features and providing enhanced control over image attributes. However, our research identifies two key challenges in current one-step distillation approaches using VSD. First, we observe training instability across different architectures, primarily due to the reliance on fixed guidance scale. Second, we identify broader issue affecting all onestep generators: the lack of negative prompt guidance, which limits the ability to exclude specific elements from generated imagesa feature commonly supported in multistep models [8, 15, 20]. These issues limit the flexibility and control of one-step models, which reduces their practical effectiveness despite their faster generation times. Prior research [54] has attempted to address the stability issues of the VSD framework, but these solutions come with their own limitations. Techniques like increased teacher update frequency [54] and adversarial loss terms [19, 42] have been explored to stabilize the training process. However, these approaches can add significant computational burden, partly due to the additional proposed technique itself and partly due to the introduction of image supervision. Since were focusing on building an image-free distillation, given these limitations, there is pressing need to conduct deeper analysis of the VSD framework to understand its challenges and develop more efficient, stable solutions. Alongside these stability limitations, as previously mentioned, one-step diffusion models currently lack support for negative prompting, feature widely used in multi-step diffusion models to enhance image quality and filter out unwanted features. In multi-step models, negative prompting techniques are particularly useful for refining alignment with text prompts in text-to-2D generation, and they also help address issues like the Janus problem in text-to-3D generation [1, 8, 20] where models produce mirrored or duplicated features like faces. One common way to integrate negative prompts into multi-step diffusion model sampling is using Classifier-free Guidance (CFG) [15]. Given two noise predictions from the positive prompt and negative prompt, CFG subtracts negative noise prediction from the positive noise prediction iteratively throughout the denoising process, inherently steering the final output away from the negative prompt. However, one-step models do not have the iterative refinement process required by CFG, making it difficult to apply negative prompting in these frameworks. This limitation reduces the flexibility and control of one-step models. To address this, novel approach is needed to incorporate the negative prompt signal directly within the one-step generation process, enabling these models to effectively avoid unwanted features without iterative sampling. We address these challenges by introducing robust framework called SNOOPI for the one-step model, illustrated in Fig. 1, that enhances stability and control dur- (1) We propose Proper ing both training and inference. Guidance - SwiftBrush (PG-SB), dynamic guidance approach that varies the guidance scale of teacher models during training to stabilize VSD. This flexible CFG improves the adaptability of image-free distillation across diverse diffusion backbones without adding computational or memory overhead. (2) We introduce method called NegativeAway Steer Attention (NASA) used in inference, which incorporates negative prompt guidance into one-step models by adjusting cross-attention layers, effectively reducing unwanted features in generated images. By steering attention within intermediate feature space, NASA offers image control capabilities that were previously achievable only in multi-step diffusion models. Our contributions are threefold: We propose Proper Guidance-SwiftBrush (PG-SB), applying varied guidance scale to teacher model outputs during training, to enhance stability across different diffusion backbones without additional computational costs. We introduce Negative-Away Steer Attention (NASA), the first method to integrate negative prompt guidance into one-step diffusion models, effectively suppressing unwanted visual attributes. Extensive experimental results on established benchmarks demonstrate that our method outperforms strong baselines, validating its stability and effectiveness. 2 2. Related Work 2.1. Text-to-image Diffusion Distillation Recent advances in text-to-image diffusion models have focused on accelerating generation through several key methodological streams: conventional distillation, trajectory optimization, and distribution matching. Conventional Distillation. Early work in model distillation explored two main directions. Progressive distillation methods [32, 40] iteratively train sequence of student models, each halving the required sampling steps while preserving generation quality. While effective, this approach necessitates multiple training phases, increasing overall computational cost. In parallel, direct distillation techniques [12, 28] generate synthetic noise-image pairs using teacher model to train students via reconstruction loss, offering more straightforward training process but often struggling with quality retention. Recent work has augmented these methods with adversarial training [19, 42], achieving better image quality at the cost of increased training complexity. Trajectory Optimization. Trajectory-straightening methods [25, 26] simplify the complex denoising trajectories into near-linear paths, enabling faster sampling without compromising quality. Meanwhile, consistency-based frameworks [21, 44, 46] use self-consistency properties to train models to align outputs across multiple points in the ODE-based denoising trajectory. Distribution Matching. More recent work [7, 33, 54, 55] has explored distribution matching techniques. These methods directly align the output distributions of teacher and student models, particularly through Variational Score Distillation (VSD). These techniques can be broadly categorized into two approaches: image-dependent training and imagefree training. Image-dependent methods, which rely on real images for adversarial loss [41, 54] or synthetic ones generated by the teacher models for reconstruction loss [55]. Despite their effectiveness in preserving image quality, these approaches face significant practical limitations: they require substantial storage capacity, consume high memory during training, demand extensive computational resources, and risk mode collapse due to intensive image supervision. In contrast, image-free approaches [7, 33] have demonstrated promising results by relying solely on prompt text for training, offering significant reduction in resource requirements while maintaining competitive performance. However, image-free techniques often encounter training instability, particularly within VSD-based frameworks. Existing solutions to stabilize training [55] introduce significant memory and computational overhead, limiting their practical application. Our work addresses this gap by introducing simple yet effective method that enhances training stability without additional computational burden, making image-free variational score distillation more accessible for real-world applications. 2.2. Sampling Guidance and Negative Prompting Classifier-free guidance (CFG) [15] and negative prompting are key techniques for content control in image generation, from text-to-2D to text-to-3D synthesis. In multistep diffusion models, these methods enhance desired image features and suppress undesired ones, improving visual fidelity and reducing artifacts like the Janus problem in 3D tasks [1, 8, 20]. Score-based approaches like NFSD [20] employ negative prompting to steer generated samples toward real-image distributions, while SDS-Bridge [31] uses negative prompts to model source distributions more accurately. Despite their success, negative prompting has yet to be integrated into one-step or few-step diffusion models. To bridge this gap, we introduce Negative-Away Steer Attention (NASA), the first method to enable negative prompting in one-step and few-step diffusion models, enhancing control in faster generation settings. 3. Background Diffusion Models [16, 38, 45] produce high-quality images by progressively denoising inputs. This technique involves forward process that gradually adds noise to data and reverse process that reconstructs the original data by removing noise. While the original diffusion models act on image space [16, 45], LDM [38] processes on latent space produced by pretrained VAE for more efficient computation. In particular, starting with data point x0 sampled from an unknown distribution q(x0), the forward process gradually diffuses x0 into standard Gaussian noise xT (0, I) through consecutive timesteps, where is the identity matrix. At each time-step t, noisier version of x0, denoted as xt, is drawn from qt(xtx0) = (αtx0, σ2 I) using standard Gaussian noise ϵ (0, I): xt = αtx0 + σtϵ, {0, . . . , }, (1) where {(αt, σt)}T t=1 defines the noise schedule, with boundary conditions (α0, σ0) = (1, 0) for the clean sample and (αT , σT ) = (0, 1) for pure noise. Conversely, the reverse process aims to gradually reconstruct the original data by denoising the input over steps, starting from an initial random Gaussian noise xT (0, I). The model is trained by minimizing the difference between the noise estimated by the model, ϵϕ, parameterized by ϕ, and the actual noise in Eq. (1): min ϕ EtU (0,T ),ϵN (0,I)ϵϕ(xt, t) ϵ2 2. (2) Text-to-Image Diffusion Models guide the sampling pro3 cess using an additional text prompt to produce outputs that are both photorealistic and aligned with the provided textual descriptions. The training objective, slightly modified from the unconditional loss in Eq. (2), is defined as: min ϕ EtU (0,T ),ϵN (0,I),yϵϕ(xt, t, y) ϵ2 2. (3) Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both conditional and an unconditional model. At each sampling step, CFG adjusts the denoisers output using control parameter κ > 1, allowing for controlled guidance that aligns more closely with the desired conditions: ˆϵϕ(xt, t, y, κ) = (1 κ) ϵϕ(xt, t) + κ ϵϕ(xt, t, y). (4) Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y, as follows: ˆϵϕ(xt, t, y, y, κ) = (1 κ) ϵϕ(xt, t, y) + κ ϵϕ(xt, t, y). (5) Variational Score Distillation (VSD) is powerful framework that utilizes pretrained diffusion-based text-to-image models to enhance text-based generation across both 3D and 2D domains. Originally proposed for text-to-3D tasks [48], VSD leverages diffusion-based score matching to align Neural Radiance Fields (NeRFs) with text prompt, enabling the generation of intricate 3D objects [48]. This approach extends effectively to 2D image synthesis as well, where VSD enables rapid, high-quality text-to-image generation in single step with models such as SwiftBrush and DMD [7, 33, 54, 55], sidestepping the computational burden of multi-step diffusion methods. The central aim of VSD is to ensures that the renderings of differentiable generator align with the probability density of plausible images as guided by the 2D diffusion model. To accomplish this, VSD employs two-teacher approach that uses fixed pretrained diffusion model ϵψ and an adaptive LoRA-based teacher ϵϕ. While training, the student model θ produces 2D images ˆx0 = θ(z, y) using an input noise (0, I) and text prompt y. Noisy images ˆxt = αt ˆx0 + σtϵ is then fed into both teacher models. The LoRA teacher model ϵϕ aligns with the student distribution by minimizing denoising L2 loss on singlestep samples. This arrangement supports robust and adaptive guidance suitable for variety of generative architectures. The gradient of the learning objective with respect to Figure 2. FID 2K progression throughout training. The default baseline is SwiftBrush for both SDv1.5 and PixArt-α. DMD2Trick indicates the Two Time-scale Update Rule technique [54], which increases the update frequency of the LoRA teacher to 5 times after each student update. These baselines demonstrate notable instability, leading to variability in image quality metrics such as FID. In contrast, our approach, which employs CFG with randomly selected values from uniform distribution, results in more stable training process and generates higher-quality samples. the student model parameters θ is defined as: θLVSD = Et,y,z,κ=C (cid:20) w(t) (ˆϵψ(ˆxt, t, y, κ) ˆϵϕ(ˆxt, t, y, κ) (6) θ(z, y) θ (cid:21) , where is constant and w(t) is the time-dependent weight that adjusts the contribution of each timestep, aligning the students outputs with the diffusion models predictions. Through alternating updates of the student and LoRA teacher, VSD enables efficient, high-fidelity generation across both text-to-image and text-to-3D tasks. 4. Proper Guidance - SwiftBrush 4.1. Motivation While the VSD framework offers solid foundation for model distillation, its training process often experiences instability. For example, while successfully working on Stable Diffusion 2.1 (SDv2.1) [38], our re-implementation of SwiftBrush and SwiftBrushv2 (SB) [7, 33] fail in distilling other model backbones such as Stable Diffusion 1.5 (SDv1.5) [38] and PixArt-α [6] when using fixed CFG scale in the VSD framework. Their unstable results are substantiated by quantitative evaluation shown in Fig. 2. To address this instability, previous studies have proposed techniques such as using image datasets as regularizers, increasing the LoRA teacher update frequency, and adding adversarial loss terms [54, 55]. However, these methods have two main drawbacks: (1) they rely on image supervision, and (2) they significantly increase memory Figure 3. Qualitative comparison of different methods for integrating negative prompts into the generation process using one-step generator. As shown, simply applying CFG to one-step generator is equivalent to blending (i.e., adding) two images together, resulting in an unusable output. Alternatively, negating the negative embedding from the positive embedding shows minimal impact on the final image. In contrast, NASA is the first method to successfully steer the generation away from negative attributes in one-step generator, while also producing high-quality results. κ 2 3 4 5 FID CLIP Precision Recall Teacher LoRA Teacher FID 10.77 9.60 10.35 11.53 0.31 0.32 0.32 0.33 0.32 0.54 0.59 0.60 0.61 0.60 0.53 0.50 0.47 0. 0.48 10.72 10.81 9.51 9.03 U(2, 5) 9.86 Table 1. Results on the zero-shot MS COCO-2014 30K benchmark [24] of SDv2.1 [38] when using PNDM Scheduler with different CFG scale κ. usage and training time. In pursuit of image-free distillation, we instead investigate the existing SB framework to identify the root cause of the instability. We begin by examining the impact of the CFG scale in multi-step diffusion models, specifically SDv2.1. Our analysis of the MS COCO 2014 benchmark [24], with results shown in Tab. 1, reveals that model performance shifts considerably as the guidance scale changes. Typically, users set the guidance scale based on their specific goal, whether to prioritize image fidelity or text alignment. However, this approach is less desirable in training, as we want the model to be versatile across different scenarios. Given this, we seek to make the teacher model adaptable across various scenarios without compromising overall performance. As shown in Tab. 1, varying the guidance scale randomly allows us to achieve competitive results across all metrics while maintaining flexibility across different guidance levels. 4.2. Method Based on our findings, we aim to broaden the guidance distribution provided by the teacher in the SB distillaTable 2. Ablation studies on Random CFG in VSD loss with SDv2.1 backbone on 2K prompts from MS COCO-2014 [24]. tion framework by introducing Proper Guidance - SwiftBrush (PG-SB) which applies randomized guidance scale during teacher inference. Specifically, rather than fixing κ = C, we sample its value from uniform distribution (κmin, κmax) where κmin and κmax are predefined constants. In the SB distillation pipeline, however, CFG is applied in two places: once to the frozen teacher and once to the LoRA teacher. As shown in Tab. 2, applying this technique only to the LoRA teacher improves model performance. However, applying it solely to the frozen teacher destabilizes the distillation process and eventually harms the student performance. The gradient of our PG-SBs VSD loss is defined by: θLPG-SB VSD =Et,y,z,κU (κmin,κmax) (cid:20) w(t) (ˆϵψ(ˆxt, t, y, κ) ˆϵϕ(ˆxt, t, y, κ) θ(z, y) θ (cid:21) . (7) We hypothesize that broadening the output distribution 5 Figure 4. Left: An overview of the Negative-Away Steer Attention (NASA) pipeline. Positive (green) and negative (red) prompts are fed into text encoder to generate positive and negative text features. The NASA module then processes these features, which adjusts the one-step diffusion model to steer the output image away from the negative features, refining it based on the positive features. Right: The details of the NASA module. It processes queries (Ql) in layer l, note we will omit the subscript in subsequent notations to improve readability, with positive (V+, K+) and negative (V, K) key-value pairs to create positive (Z+) and negative (Z) attention outputs. The final output (ZNASA) is calculated by subtracting the weighted negative features (Z) from the positive features (Z+). of the LoRA teacher enables it to better align with the students output, mainly since the student may not initially perform well at one function evaluation or one NFE generation. Interestingly, once the LoRA teachers output distribution is expanded, the student can also adapt to the broader range of outputs from the frozen teacher when using randomized guidance scale, achieving the best results when varied guidance scales are applied to both teachers. 5. Negative-Away Steer Attention 5.1. Motivation Negative prompting is an essential technique for improving control in text-to-image and text-to-3D synthesis. This method helps exclude unwanted elements in generated images and mitigate the Janus problem in 3D generation [1, 20, 31]. However, no existing approach supports negative prompts in single-step or few-step diffusion models. widely method for implementing negative prompting relies on the CFG mechanism. However, applying CFG directly to one-step diffusion models presents significant challenges. CFG was designed for multi-step models, where the output is iteratively refined, allowing for gradual adjustments using both negative and unconditional outputs. In contrast, one-step models generate images in single pass without the iterative denoising process, making them inherently less flexible. Directly applying CFG to these models leads to an undesirable blending of images, resulting in unnatural artifacts, as shown in Fig. 3. This highlights crucial limitation: the CFG mechanism, though effective in multi-step models, is not directly translatable to one-step 6 models without significant modifications. An alternative approach manipulates text embeddings by subtracting the negative embedding from the positive one to steer the model away from undesired attributes. However, as shown in Fig. 3, this method has minimal effect on the generated images, highlighting the need for more robust solution to negative prompting in one-step models. 5.2. Method Instead of applying CFG in the input space (text embedding) or output space (synthesized image) as in multi-step diffusion, we propose moving CFG to the intermediate feature space of the diffusion model. We hypothesize that cross-attention layer features are particularly well-suited for this approach. Specifically, cross-attention layers in these models capture semantic connections between image regions and text features [13], making them ideal for controlling feature emphasis. By modulating these cross-attention maps, we can filter unwanted features by subtracting attention responses to negative prompts from those to positive prompts. This filtering occurs in the feature space, meaning that undesired elements are excluded before the image is generated, reducing the chance of blending artifacts. = cpWk,l, and V+ Given the latents features at current layer l, Zl, and positive text features cp extracted from CLIP text encoder, we define the query, key, and value matrices as Ql = ZlWq,l, K+ = cpWv,l, where Wq,l, Wk,l, and Wv,l are projection matrices at current layer l. For negative text features cn, we use the same projection matrices, defining the key and value matrices as = cnWk,l and = cnWv,l. The NASA mechanism is formulated as follows: Z+ = Softmax = Softmax (cid:18) Ql(K+ ) (cid:18) Ql(K )"
        },
        {
            "title": "ZNASA\nl",
            "content": "= Z+ α , (cid:19) (cid:19) V+ , , (8) (9) (10) where α is the scale factor to control the degree of negative feature removal. The overall pipeline of the NASA mechanism is illustrated in Fig. 4. 6. Experiments 6.1. Experimental Setup Metrics and benchmark. In diffusion model evaluation, Frechet Inception Distance (FID) [14] on the zero-shot MSCOCO2014 benchmark has traditionally been the standard metric. However, recent studies [5, 37, 54] suggest that FID does not always correlate with actual image quality and adopt human-focused alignment metrics like Human Preference Score v2 (HPSv2) [49] for benchmarking. Our experiments also follow this conclusion, so we choose HPSv2 as the primary metric for evaluating the models. Additionally, we use CLIP scores to assess text alignment and precision/recall [22] to evaluate general image quality and mode coverage. For completeness, we still report FID results. Except for HPSv2, all metrics are computed following the protocols in [7, 18]: generating 30K images using MSCOCO2014 prompts, resizing them to 256 256, and comparing with center-cropped images from the full dataset. Implementation details. We base our method on SBv2 [7] with our proposed modifications on CFG scale κ. We use total of 3.3M prompts from JourneyDB [47] and subset of LAION [43]. All our training is conducted on four NVIDIA A100 40GB GPUs with the total batch size of 64. The learning rates are set to 1e6 for the student model and 1e3 for the LoRA teacher, using the AdamW optimizer [27]. We utilize three backbones SDv1.5 [38], SDv2.1 [38] and PixArt-α [6] as the frozen teachers and LoRA teachers are initialized with rank = 64 and scaling γ = 128. The teacher models CFG scales are set as follows: κmin = 0.5, κmax = 4 for SDv1.5 and SDv2.1 and κmin = 0.5, κmax = 3 for PixArt-α. Further details can be found in the Appendix. 6.2. Evaluation of PG-SB Tab. 4 and Tab. 5 present thorough quantitative comparison between our approach and prior distilled text-to-image diffusion models. The results consistently show the superiority of our model in HPSv2 scores, with the highest average score across all tested backbones. This reinforces 7 NASA SDXL-LCM SDXL-DMD2 SDXL-DMD2 PG-SB 1 step 4 steps 4 steps 1 step 43% 97% 27% 100% 25% 100% 38% 92% Table 3. Comparison of success rate of unwanted feature removal in generated images between models with and without NASA. the robustness of our method, as discussed in Sec. 4.2 and demonstrated in Fig. 2, while also revealing key insight. By gaining deep understanding of the underlying framework, we made simple yet impactful change with minimal overhead, leading to significant performance improvements. This suggests that integrating complex loss functions may not always be necessary for achieving state-ofthe-art results. Remarkably, our method achieves these outcomes without requiring image-level supervision, restoring SBv2 as an image-free distillation approach. 6.3. Evaluation of NASA To assess the effectiveness of NASA, we conduct experiments with our model and other one-step and few-step models, such as LCM [29] and DMD2 [54]. We design small set of positive and negative prompts and generate 100 images per model, both with and without negative prompts (further details are provided in the Appendix). We then calculate the percentage of images that successfully exclude the feature specified by the negative prompt. For example, given the positive prompt photo of person and the negative prompt male, we measure the percentage of generated images that do not depict male person. The average results across all prompt sets are summarized in Tab. 3. Our findings show that NASA is compatible with both one-step and few-step models, enabling precise control over unwanted features in generated images. Note that the difference in accuracy between PG-SB and other models is primarily attributed to the disparity in backbone size. The SDXL-based architecture is well-known for its superior text alignment, which stems from its larger generator backbone and the use of an ensemble of text encoders. Additionally, Fig. 5 and Fig. 6 present qualitative results of our method. Images generated with conditional negative prompt retain key semantic features such as pose and background, while filtering out unwanted attributes specified by the negative prompt. Furthermore, when using negative prompts like worst quality, low quality, ugly, duplicate, out of frame, deformed, blurry, bad anatomy, watermark, our method improves image quality, as shown in Tab. 4 and Tab. 5. Notably, integrating NASA with PG-SB on the PixArt-α-based backbone achieves record-breaking HPSv2 score of 31.08, surpassing all other one-step diffusion models baselines. Figure 5. Effect of different scale values (0.0 to 1.0) in NASA with SDXL-DMD2 model, illustrating the progressive influence on visual details and composition. 8 Figure 6. Qualitative results of NASA. Each row displays images generated with positive (green) and negative (red) prompt pair. Models using NASA effectively exclude unwanted features, aligning outputs with desired traits. Method Anime Photo Concept Art Paintings Average Stable Diffusion 1.5-based backbone SDv1.5 [38] InstaFlow-0.9B [26] DMD2 [54] PG-SB PG-SB + NASA 26.51 26.10 26.39 27.18 27.19 27.19 26.62 27.00 27.58 27.59 26.06 25.92 25.80 26.69 26.71 SDv2.1 [38] SB [33] SBv2 [7] PG-SB PG-SB + NASA PixArt-α [6] YOSO [30] DMD [55] PG-SB PG-SB + NASA Stable Diffusion 2.1-based backbone 27.48 26.91 27.25 27.56 27.71 26.89 27.21 27.62 27.84 27.99 26.86 26.32 26.86 26.97 27.14 PixArt-α-based backbone 28.79 28.57 28.46 30.39 31.24 29.17 28.09 28.67 29.09 29.55 29.62 28.79 29.31 32.19 32. 26.12 25.95 25.83 26.62 26.63 27.46 26.37 26.77 27.03 27.27 28.69 28.55 28.41 29.69 30.96 26.47 26.15 26.26 27.02 27.03 27.17 26.70 27.13 27.35 27.53 29.07 28.50 28.71 30.34 31. Table 4. HPSv2 comparisons between our method and previous works. denotes reported numbers, denotes our rerun based on the publicly available model checkpoints. 9 Method #Params NFEs FID CLIP Precision Recall Image-free? SDv1.5 (cfg = 3) [38] UFOGen [51] InstaFlow-0.9B [26] DMD [55] DMD2 [54] PG-SB PG-SB + NASA SDv2.1 (cfg = 3) [38] SB [33] SBv2 [7] PG-SB PG-SB + NASA PixArt-α (cfg = 4.5) [6] YOSO [30] DMD [55] PG-SB PG-SB + NASA 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.6B 0.6B 0.6B 0.6B 0.6B Stable Diffusion 1.5-based backbone 25 1 1 1 1 1 1 8.78 12.78 13.33 11.49 8.29 10.08 9.94 0.30 - 0.30 0.32 0.30 0.31 0.31 Stable Diffusion 2.1-based backbone 25 1 1 1 1 9.60 15.46 8.14 8.91 8. 0.32 0.30 0.32 0.34 0.34 PixArt-α-based backbone 20 1 1 1 1 26.85 26.04 30.22 22.03 21.98 0.32 0.30 0.32 0.33 0.33 0.59 - 0.53 - 0.59 0.57 0.57 0.59 0.47 0.57 0.57 0. 0.52 0.46 0.55 0.52 0.52 0.53 - 0.45 - 0.52 0.47 0.48 0.50 0.46 0.52 0.49 0.50 0.23 0.30 0.18 0.24 0.25 Table 5. Quantitative comparisons between our method and others on zero-shot MS COCO-2014 benchmark. For multi-step SD models, we report each with the CFG scale that returns the best FID. denotes reported numbers, denotes our rerun based on the publicly available model checkpoints. - denotes unreported results. 7. Discussion and Conclusion"
        },
        {
            "title": "References",
            "content": "Limitations: Despite its promising results, PG-SB currently lacks support for few-step models, limiting its applicability to downstream tasks. Additionally, while NASA proves effective, achieving high-quality images depends on selecting an appropriate scale for removing negative features, though the parameter does not require extensive tuning. Furthermore, NASA is designed specifically for architectures with cross-attention layers, leaving support for other architectures, such as MM-DiT [9], as future work. Conclusion: This paper introduces SNOOPI, framework that improves stability and control in one-step diffusion models for both training and inference. PG-SB dynamically adjusts the guidance scale to stabilize VSD across various model backbones without increasing computational cost. Additionally, NASA is the first approach to incorporate negative prompt guidance in one-step diffusion models, enabling the effective reduction of unwanted features in generated images through cross-attention adjustments. Experimental results demonstrate that SNOOPI outperforms strong baselines, showcasing its stability and effectiveness. 10 [1] Mohammadreza Armandpour, Ali Sadeghian, Huangjie Zheng, Amir Sadeghian, and Mingyuan Zhou. Re-imagine the negative prompt algorithm: Transform 2d diffusion ArXiv, into 3d, alleviate janus problem and beyond. abs/2304.04968, 2023. 2, 3, 6 [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 1 [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation. ArXiv, abs/2401.12945, 2024. [4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. ArXiv, abs/2310.00426, 2023. 1 [5] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv: 2403.04692, 2024. 7 [6] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 4, 7, 9, 10 [7] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 1, 3, 4, 7, 9, 10, 13, 14 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems, 34:87808794, 2021. 2, 3 [9] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for International Conference high-resolution image synthesis. on Machine Learning, 2024. [10] Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. ArXiv, abs/2402.04825, 2024. 1 [11] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, Renrui Zhang, Junlin Xi, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye, He Tong, Jingwen He, Yu Jiao Qiao, and Hongsheng Li. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. ArXiv, abs/2405.05945, 2024. 1 [12] Zhengyang Geng, Ashwini Pokle, and Zico Kolter. Onestep diffusion distillation via deep equilibrium models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 1, 3 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control.(2022). In International Conference on Learning Representations, 2023. 6 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Neural Information Processing Systems, 2017. 7 [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems, 33, 2020. 3 [17] Rongjie Huang, Jia-Bin Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiaoyue Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. ArXiv, abs/2301.12661, 2023. 1 [18] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 7 [19] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling Diffusion Models into Conditional GANs. In European Conference on Computer Vision (ECCV), 2024. 1, 2, [20] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani arXiv preprint Lischinski. Noise-free score distillation. arXiv: 2310.17590, 2023. 2, 3, 6 [21] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. 3 [22] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. 7 [23] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 1 [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. [25] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [26] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. arXiv preprint arXiv:2309.06380, 2023. 3, 9, 10, 14 [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 7 [28] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 1, 3 [29] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv: 2310.04378, 2023. 7 [30] Yihong Luo, Xiaolong Chen, Xinghua Qu, and Jing Tang. You only sample once: Taming one-step text-to-image synarXiv preprint thesis by self-cooperative diffusion gans. arXiv: 2403.12931, 2024. 9, 10, [31] David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Rethinking score distillation as bridge between image distributions. ArXiv, abs/2406.09417, 2024. 3, 6 [32] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. 11 [46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 3 [47] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. JourneyDB: benchmark for generative image understanding. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 7 [48] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. 1, [49] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. ArXiv, abs/2306.09341, 2023. 7 [50] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain arXiv preprint images with video diffusion priors. arXiv:2310.12190, 2023. 1 [51] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81968206, 2023. 1, 10 [52] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. ArXiv, abs/2305.18295, 2023. 1 [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. 1 [54] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 2, 3, 4, 7, 9, 10, 14 [55] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 3, 4, 9, 10, 14 [56] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In International Conference on Machine Learning, pages 4239042402. PMLR, 2023. On distillation of guided diffusion models. In CVPR, 2023. 3 [33] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3, 4, 9, 10 [34] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. International Conference on Learning Representations, 2023. 1 [35] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. ArXiv, abs/2209.14988, 2022. 1 [36] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In The Eleventh International Conference on Learning Representations, 2023. 1 [37] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv: 2404.13686, 2024. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 3, 4, 5, 7, 9, 10 [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1 [40] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 3 [41] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. 1, 3 [42] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. 1, 2, 3 [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 7 Improved techarXiv preprint [44] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. [45] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 3 12 SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance"
        },
        {
            "title": "Supplementary Material",
            "content": "8. Implementation Details 10. Additional Qualitative Results of PG-SB Fig. 7, Fig. 8, and Fig. 9 present additional uncurated samples synthesized by our model using three different backbones: PixArt-α, SDv1.5, and SDv2.1, respectively. PG-SB: Following SBv2 [7], we also use the clamped CLIP loss with margin of τ = 0.37, starting with weight of 0.1 and gradually reducing to zero. Tab. 6 provides the additional hyperparameters for training PG-SB. Hyperparameter SDv1. SDv2.1 PixArt-α Dataset Batch size Training iterations Mixed-Precision (BF16) (κmin, κmax) Clip weight τ lr of student lr of LoRA teacher LoRA rank LoRA scaling γ JDB + LAION JDB + LAION 64 60k Yes (0.5, 4) 0.1 0.37 1e6 1e3 64 128 64 40k Yes (0.5, 4) 0.1 0.37 1e6 1e3 64 JDB 64 60k Yes (0.5, 3) 0.1 0.37 1e6 1e3 64 128 Table 6. Hyperparameters used for training PG-SB. NASA: We provide the PyTorch pseudo-code for the NASA Attention algorithm, as outlined in Sec. 5. The implementation is straightforward and is provided in Algorithm 1. 9. More Analysis on NASA 9.1. Experimental Details in Tab. 3 For clarification, in Tab. 3, we evaluate the effectiveness of NASA across four models: SDXL-LCM, SDXL-DMD2 (with both 1-step and 4-step settings), and our PG-SB model using the SDv2.1 backbone. For the without NASA case, all models use scale of α = 0, while for the with NASA case, scale of α = 0.5 is applied. The evaluation involves 6 positive-negative prompt pairs. For each model and prompt pair, 100 images are generated. Human evaluations are then used to determine the percentage of images that successfully exclude the feature specified by the negative prompt. The detailed results from Tab. 3 are provided in Tab. 7. 9.2. More Quantitative Results Tab. 8 presents additional quantitative results when applying NASA to various models. Using negative prompt such as worst quality, low quality, ugly, duplicate, out of frame, deformed, blurry, bad anatomy, watermark, our NASA method demonstrates slight improvement in the HPSv2 scores across all models. 13 Positive prompt Negative prompt photo of person photo of person photo of pet male female young old cat dog SDXL-LCM SDXL-DMD 4 steps 4 steps SDXL-DMD2 1 step PG-SB 1 step 52% 99% 26% 100% 29% 100% 42% 81% 48% 100% 74% 100% 71% 100% 58% 88% 55% 100% 44% 100% 45% 92% 39% 96% 7% 100% 56% 100% 93% 100% 61% 79% 36% 100% 12% 100% 60% 100% 32% 99% 64% 100% 88% 100% 40% 100% 68% 99% Table 7. Comparison of success rates of unwanted feature removal in generated images before and after applying the NASA model. Method Anime Photo Concept Art Paintings Average InstaFlow-0.9B [26] InstaFlow-0.9B + NASA (α = 0.19) DMD2 [54] DMD2 + NASA (α = 0.04) PG-SB PG-SB + NASA (α = 0.04) Stable Diffusion 1.5-based backbone 25.92 26.04 25.80 25.80 26.69 26.71 26.10 26.24 26.39 26.41 27.18 27.19 26.62 26.74 27.00 27.02 27.58 27.59 SBv2 [7] SBv2 + NASA (α = 0.30) PG-SB PG-SB + NASA (α = 0.30) YOSO [30] YOSO + NASA (α = 0.20) DMD [55] DMD + NASA (α = 0.45) PG-SB PG-SB + NASA (α = 0.80) Stable Diffusion 2.1-based backbone 26.86 26.93 26.97 27.14 27.25 27.45 27.56 27.71 27.62 27.85 27.84 27.99 PixArt-α-based backbone 28.79 28.80 29.31 29.34 32.19 32. 28.09 28.10 28.67 28.71 29.09 29.55 28.57 28.62 28.46 28.50 30.39 31.24 25.95 26.00 25.83 25.83 26.62 26.63 26.77 27.09 27.03 27.27 28.55 28.57 28.41 28.52 29.69 30.96 26.15 26.26 (+0.11) 26.26 26.27 (+0.01) 27.02 27.03 (+0.01) 27.13 27.33 (+0.20) 27.35 27.53 (+0.18) 28.50 28.52 (+0.02) 28.71 28.77 (+0.06) 30.34 31.08 (+0.74) Table 8. HPSv2 comparisons between our method and previous works. denotes reported numbers, denotes our rerun based on the publicly available model checkpoints. 14 Algorithm 1 NASA Attention. class NASA_AttnProcessor(nn.Module): def __init__(self, scale=1.0): super().__init__() self.scale = scale def __call__( self, attn, z, emb, neg_emb, attn_mask, neg_attn_mask, temb, scale=1., *args, **kwargs, ): # Input preparation residual = if attn.spatial_norm is not None: = attn.spatial_norm(z, temb) input_ndim = z.ndim if input_ndim == 4: bsz, channel, height, width = z.shape = z.view(bsz, channel, height * width).transpose(1, 2) bsz, sequence_length, _ = z.shape if emb is None else emb.shape _, neg_sequence_length, _ = z.shape if neg_emb is None else neg_emb.shape if attn_mask is not None: attn_mask = attn.prepare_attention_mask(attn_mask, sequence_length, bsz) attn_mask = attn_mask.view(bsz, attn.heads, -1, attn_mask.shape[-1]) if neg_attn_mask is not None: neg_attn_mask = attn.prepare_attention_mask(neg_attn_mask, neg_sequence_length, bsz) neg_attn_mask = neg_attn_mask.view(bsz, attn.heads, -1, neg_attn_mask.shape[-1]) if attn.group_norm is not None: = attn.group_norm(z.transpose(1, 2)).transpose(1, 2) query = attn.to_q(z) if emb is None: emb = else: if attn.norm_cross: emb = attn.norm_emb(emb) neg_emb = attn.norm_emb(neg_emb) # Compute cross-attention for positive embedding key = attn.to_k(emb) value = attn.to_v(emb) inner_dim = key.shape[-1] head_dim = inner_dim // attn.heads query = query.view(bsz, -1, attn.heads, head_dim).transpose(1, 2) key = key.view(bsz, -1, attn.heads, head_dim).transpose(1, 2) value = value.view(bsz, -1, attn.heads, head_dim).transpose(1, 2) if attn.norm_q is not None: query = attn.norm_q(query) if attn.norm_k is not None: key = attn.norm_k(key) = F.scaled_dot_product_attention( query, key, value, attn_mask=attn_mask, dropout_p=0.0, is_causal=False ) = z.transpose(1, 2).reshape(bsz, -1, attn.heads * head_dim) = z.to(query.dtype) # Compute cross-attention for negative embedding neg_key = attn.to_k(neg_emb) neg_value = attn.to_v(neg_emb) neg_key = neg_key.view(bsz, -1, attn.heads, head_dim).transpose(1, 2) neg_value = neg_value.view(bsz, -1, attn.heads, head_dim).transpose(1, 2) neg_z = F.scaled_dot_product_attention( query, neg_key, neg_value, attn_mask=neg_attn_mask, dropout_p=0.0, is_causal=False ) neg_z = neg_z.transpose(1, 2).reshape(bsz, -1, attn.heads * head_dim) neg_z = neg_z.to(query.dtype) # Equation 10 in main paper z_nasa = - self.scale * neg_z z_nasa = attn.to_out[0](z_nasa) z_nasa = attn.to_out[1](z_nasa) if input_ndim == 4: z_nasa= z_nasa.transpose(-1, -2).reshape(bsz, channel, height, width) if attn.residual_connection: z_nasa = z_nasa + residual z_nasa = z_nasa / attn.rescale_output_factor return z_nasa 15 Figure 7. Additional qualitative images generated by our PG-SB model with the PixArt-α backbone. 16 Figure 8. Additional qualitative images generated by our PG-SB model with the SDv1.5 backbone. 17 Figure 9. Additional qualitative images generated by our PG-SB model with the SDv2.1 backbone."
        }
    ],
    "affiliations": [
        "Posts & Telecom. Inst. of Tech.",
        "VinAI Research"
    ]
}