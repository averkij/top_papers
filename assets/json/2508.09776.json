{
    "paper_title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study",
    "authors": [
        "Mahdi Dhaini",
        "Juraj Vladika",
        "Ege Erdogan",
        "Zineb Attaoui",
        "Gjergji Kasneci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 6 7 7 9 0 . 8 0 5 2 : r Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study Mahdi Dhaini (cid:0)[0000000278313141], Juraj Vladika[0000000249419166], Ege Erdogan[0000000161703541], Zineb Attaoui[0009000339909588], and Gjergji Kasneci[0000000231237268] Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany firstname.lastname@tum.de Abstract. In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-ofthe-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore promising avenue for scalable, automated LLMbased textual explanation generation for extending NLP datasets and enhancing model performance. Keywords: Explainable natural language processing Natural language explanations Natural language inference Large language models."
        },
        {
            "title": "Introduction",
            "content": "Recent NLP advancements are driven by PLMs and LLMs, achieving state-ofthe-art results across various tasks [2]. However, their black-box nature limits understanding of their predictions, prompting increased interest in Explainable NLP, where methods from Explainable AI explain model decision-making [31] to enhance trust and transparency, which is essential for advancing practical applications in sensitive domains. (cid:0) Corresponding author 2 key challenge in Explainable NLP is the lack of definitive ground-truth explanations [17]. Researchers address this by collecting human-generated textual explanations, creating explainable datasets [6]. These datasets serve both as benchmarks for evaluating model-generated explanations and as training data to improve models predictive performance [38]. However, human annotation is resource-intensive, impacting dataset scale and quality [29]. Recently, leveraging LLMs text-generation capabilities for explanations has gained attention [37], though evaluating these explanations quality and effectiveness in downstream tasks remains an open research question. In this paper, we address these critical gaps by focusing on two primary objectives. First, we leverage multiple LLMs to automatically generate textual explanations and rigorously evaluate their quality using comprehensive suite of metrics. Second, we investigate how the incorporation of these LLM-generated explanations impacts the performance of various PLMs and LLMs on downstream tasks, particularly within the NLI framework. Our work is guided by the following research question: How do LLM-generated textual explanations impact the performance of PLMs and LLMs on downstream predictive tasks? Our contributions are as follows: We employ four LLMs of varying sizes and complexity to automatically generate explanations for two explainable NLI datasets in both zero-shot and few-shot settings. We evaluate the quality of the generated explanations using multiple metrics, including both reference-based measures and an innovative LLM-based evaluation approach. We examine the impact of incorporating LLM-generated explanations during both fine-tuning and inference, comparing their effects against humanannotated explanations and no-explanation baseline across four distinct BERT-based models and three LLMs."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Natural Language Inference (NLI) is one of the most fundamental NLP tasks [9]. The goal is, given two pieces of text, premise and hypothesis, to determine logical relation between them as one of the three classes: entailment, contradiction, or neutral. The turning point in NLI was the construction of the Stanford NLI (SNLI) corpus in 2015 [1], dataset of half million examples, constructed with crowd-sourced effort where photos were captioned and then paired with entailed, contradicted, or neutral sentences written by annotators. Modern PLMs like BERT [5] and RoBERTa [20], as well as autoregressive LLMs like GPT, can often solve popular NLI datasets with an above-human performance, owing to the linguistic patterns and world knowledge acquired during their pre-training on huge corpora. 1 We release our code on GitHub 3 Explainable NLP and Datasets The growing interest in Explainable NLP is evident from multiple surveys like [21,38], some addressing specific tasks or methods [22]. Interest in explainable NLP has led to the creation of explainable datasets for tasks such as hate-speech classification [23] and claim verification [33]. comprehensive review of these datasets is provided in [38]. Textual explanations typically fall into highlights, structured, or free-text (natural language) categories and are annotated by authors, experts, and crowd-sourcing with most datasets relying on human annotators. However, human annotation presents several challenges. Collecting high-quality explanations is time-consuming and resource-intensive [10].Human annotators explanations may also suffer from subjectivity and inconsistency, potentially hindering model performance rather than aiding it [39]. Additionally, the diversity in explanation types introduces further complexities [32]. Generating LLM-explanations Due to the limitations of human-annotated explanations, recent research has explored using LLMs to generate Natural Language Explanations (NLE) and justifications for model decisions. Compared to traditional post-hoc feature attribution methods, NLEs provide human-readable justifications, which can enhance transparency and user understanding. [24] employed LLMs as rationalizers for knowledge-intensive tasks such as multiplechoice question answering. [35] investigated improving LLM-generated NLE quality through tandem learning setup. [40] examined how various prompting techniques, such as CoT, can improve NLEs on commonsense reasoning tasks. These studies demonstrate the growing interest in leveraging LLMs to generate and refine NLEs, particularly for tasks requiring explanation-driven reasoning. However, none of the previously mentioned works investigate how extending datasets with LLM-generated explanations can impact the performance of PLMs and LLMs on downstream tasks. Evaluating NLEs NLEs are text snippets and can be evaluated with standard NLG metrics [30]. When human-written (gold) references exist, reference-based metrics are applicable. Traditional metrics, such as BLEU [25] and ROUGE [18], assess word overlaps between generated and reference texts. However, these metrics have become less suitable with the rise of LLMs, as they penalize expressive variations in wording. Consequently, semantic metrics like the embedding-based BERTScore [41] and distribution-based MAUVE [27] have gained popularity. Recently, evaluation methods using LLM-as-judge metrics have emerged, employing crafted prompts for LLMs to return numerical scores assessing generated texts, exemplified by G-Eval [19]. Closely related work Among the previously mentioned works, the closest to ours is [39], which investigates how human explanations can impact the predictions of two PLMs. However, their study is limited to BART and T5 and focuses solely on human explanations. Additionally, [11] reviews studies employing different types of human explanations (highlights, structured, and free-text) to improve NLP models. However, they solely review studies incorporating humanannotated explanations. In contrast, while we also incorporate human explanations, our primary focus is on generating and investigating LLM-generated 4 NLEs. We evaluate the impact of these explanations on four PLMs, including the recent ModernBERT [36], as well as three LLMs of varying sizes."
        },
        {
            "title": "3 Experimental Setup",
            "content": "We designed comprehensive experimental framework that systematically integrates both humanand LLM-generated explanations into two benchmark datasets. For reproducibility, we provide all prompt templates for explanation generation, evaluation, and LLM performance evaluation in the repository."
        },
        {
            "title": "3.1 Datasets",
            "content": "We use two datasets in our experiments. The first dataset, e-SNLI [3], is an extension of the SNLI dataset with human-annotated natural language explanations. It contains premise-hypothesis pairs labeled as entailment, neutral, or contradiction, depending on how the premise relates to the hypothesis. The second dataset is the HealthFC dataset [34]. It consists of 750 health-related claims, labeled by medical experts and backed with evidence from systematic reviews and clinical trials. Each claim is paired with pieces of evidence and includes verdict (supported, refuted, not enough information), as well as brief explanations for the verdict. For our experiments, we extracted balanced subset of e-SNLI consisting of 840 examples, ensuring an equal representation of entailment, neutral, and contradiction instances. This subset size was deliberately chosen to closely match the 750-instance HealthFC dataset, enabling fair and controlled comparison across our evaluation framework. Even though HealthFC is officially dataset for automated fact-checking (claim verification), it is common to model this task as an NLI task. We provide further details on the datasets with examples in Appendix 5.1."
        },
        {
            "title": "3.2 Generating Natural Language Explanations with LLMs",
            "content": "In our pipeline, we focus on generating NLEs using multiple LLMs. We further extend both datasets we consider with explanations we generate using GPT-4o mini [13], Mixtral-7B [14], Gemma2-9B [7], and LLama3-70B [8]. For Mixtral7B, Gemma2-9B, and LLama3-70B we use the APIs provided by Groq2 while for GPT-4o mini we use OpenAI APIs3. We selected LLMs ranging in size from 7B to 70B parameters4, to analyze how these factors influence both the quality of the generated text and the impact of generated explanations on downstream task performance. The rationale for selecting diverse LLMs, rather than models within the same family differing only in size, is to ensure broader variety in the sources of explanations. We discuss later in the paper how this approach could be expanded in future work. 2 https://groq.com/ 3 https://platform.openai.com/ 4 along with GPT-4o mini, whose exact size is unknown. 5 We generate explanations from the four LLMs under two settings: few-shot and zero-shot. After initial prompt validation, we explicitly instructed LLMs not to reveal or hint at labels in their explanations to avoid biasing the evaluation during inference. The few-shot setting examines if LLM explanations improve after exposure to human-written examples and evaluates the impact of these explanations on downstream tasks. Both zero-shot and few-shot prompts are provided in our repository; the few-shot prompts include four (premise-hypothesisexplanation) examples from the dataset. Due to our hardware constraints, we do not perform any memory-heavy approaches like fine-tuning of LLMs or reinforcement learning. We leave these for future work. We provide more details on the explanations generation process, including prompts used in Appendix 5.3."
        },
        {
            "title": "3.3 Evaluating LLM-Natural Language Explanations",
            "content": "We compare LLM-generated explanations with human-provided explanations from our selected datasets . Specifically, we employ the widely used BLEU, ROUGE, and BERTScore metrics. Beyond these conventional metrics, we incorporate the recent MAUVE. and the LLM-as-judge G-Eval framework that has been increasingly used in recent NLG research. We use G-Eval to measure human likeness in LLM-generated explanations, in particular, the clarity, coherence, and structure of the LLM-generated explanation. We refer readers to Appendix 5.4 for further details on G-Eval computation, implementation specifics, and metric libraries."
        },
        {
            "title": "3.4 Models for NLI Predictions",
            "content": "Fine-tuning PLMs. For the downstream NLI task predictions, we use four PLMs (BERT , DeBERTa [12] , RoBERTa , and ModernBERT [36]). For each run with certain kind of or without explanations, we perform 80/20 train/test split and fine-tune the PLMs on the train set for 10 epochs using the AdamW optimizer with learning rate of 3e-6 for ModernBERT and 1e-5 for the other PLMs. We repeat this five times with stratified 5-fold cross-validation and report results averaged over the five splits. Experiments with LLMs. We also use three LLMs: GPT-4o mini Qwen 2.5 (7B) and Llama3.3-70B. For GPT, we use the OpenAI API, and for the two open-source LLMs, we use the API provided by Together AI . We give the LLM the premise-hypothesis (or claim-evidence) pairs as input, and optionally add the humanor LLM-generated explanations at the end of the hypothesis for e-SNLI and the claim for HealthFC. We adopt zero-shot inference approach without fine-tuning. Instead, the generated explanations are directly appended to the hypothesis in the prompt. Zero-shot inference is well established in current literature as resource-efficient method that leverages the inherent generalization capabilities of LLMs without additional overhead. Moreover, we do not adopt resource-intensive approaches such as fine-tuning for LLMs, even with the existence of lighter approaches like PEFT, as the primary focus of this study is to measure the impact of different explanations on the performance, rather than to 6 compare zero-shot with fine-tuned LLMs performance. We provide further details on prompting the LLMs in Appendix 5.5. Our experimental setup covers the complete cross-product of explanation methods and classification models, covering all possible combinations, including cases where identical LLMs function in both explainer and classifier roles."
        },
        {
            "title": "4 Analysis and Discussion",
            "content": "Our results stem from an extensive experimental design covering multiple dimensions. Specifically, we evaluated two NLI datasets (e-SNLI and HealthFC), employed four different LLMs to generate explanations, and tested each in both zero-shot and few-shot settings, yielding 16 distinct explanation generation scenarios. We present the evaluation results in Table 1. Furthermore, we assessed downstream classification performance across four PLMs and three LLM classifiers. By analyzing metrics such as accuracy and macro F1 across these diverse combinations spread across Figure 1 and Tables 2, 3, our study offers comprehensive insight into how various explanation generation strategies affect NLI performance. While possible that our insights are specific only to the two chosen datasets, we try to make our takeaways general and widely applicable."
        },
        {
            "title": "4.1 Generation and Evaluation of LLM-explanations",
            "content": "Table 1 presents average metric scores for LLM-generated explanations. GPT-4o mini generally scores highest on e-SNLI, while Llama3-70B leads on HealthFC. GPT-4o mini outperforms others on e-SNLI in BLEU, ROUGE-1, and BERTScoreF1, whereas Llama3-70B excels in these metrics for HealthFC. GPT-4o mini consistently achieves top G-Eval scores, suggesting its explanations align closely with human judgment. However, G-Eval score differences across models are small, indicating similar overall quality. Mistral-7B achieves the highest MAUVE scores in multiple settings, implying greater diversity and coherence. Scores slightly improve from zero-shot to few-shot settings, particularly BLEU and ROUGE-1 on e-SNLI, but these improvements are minor, indicating limited benefit from in-context examples. Additionally, model size alone doesnt ensure better performance; smaller models like Gemma2-9B and Mistral-7B sometimes perform competitively or better. Our analysis shows LLMs do not consistently prefer their own explanations. Human explanations generally provide more significant performance gains, especially on e-SNLI. GPT-4o mini excels on e-SNLI and Llama3-70B on HealthFC, with BLEU, ROUGE, and BERTScore strongly correlating with downstream improvements. Overall, while scores improve slightly between zero-shot and few-shot settings (notably in BLEU and ROUGE-1 on e-SNLI), these improvements are marginal. This indicates that providing in-context examples from the dataset does not significantly enhance the generated explanations according to these metrics. Furthermore, model size alone does not guarantee better performance, as seen when comparing Gemma2-9B, Mistral-7B, and Llama3-70B, where smaller models sometimes achieve competitive or even higher scores. 7 (a) PLMs on e-SNLI (b) PLMs on HealthFC (c) LLMs on e-SNLI (d) LLMs on HealthFC Fig. 1: (Zoom in for better reading) Plots of the models performance on eSNLI and HealthFC. Top row (ab) shows average Macro F1 for the four PLMs (BERT-base, DeBERTa-base, ModernBERT, RoBERTa-base); bottom row (cd) shows average Macro F1 for the three LLMs (GPT-4o mini, Llama3, Qwen2.5). In each panel, bars are grouped by explanation input condition: no explanations (gray), human explanations (green), and explanations generated by four LLMs in zero-shot (blue) vs. few-shot (orange) settings. 8 Table 1: Average scores of the evaluation metrics across different LLMs on eSNLI and HealthFC datasets in zero-shot and few-shot settings. The highest value for each metric is highlighted in bold. Dataset Metric Gemma2-9B Mistral-7B Llama3-70B GPT4o-mini e-SNLI (zero-shot) e-SNLI (few-shot) HealthFC (zero-shot) HealthFC (few-shot) BLEU ROUGE-1 BERTScore F1 MAUVE G-Eval BLEU ROUGE-1 BERTScore F1 MAUVE G-Eval BLEU ROUGE-1 BERTScore F1 MAUVE G-Eval BLEU ROUGE-1 BERTScore F1 MAUVE G-Eval 0.032 0.295 0.876 0.004 0.171 0.037 0.299 0.878 0.004 0.163 0.017 0.269 0.881 0.004 0.197 0.018 0.261 0.884 0.004 0. 0.033 0.314 0.876 0.047 0.166 0.043 0.352 0.882 0.107 0.170 0.022 0.285 0.878 0.115 0.194 0.023 0.294 0.881 0.180 0.187 0.029 0.277 0.872 0.013 0.165 0.037 0.316 0.878 0.040 0. 0.030 0.313 0.883 0.083 0.192 0.030 0.309 0.886 0.197 0.192 0.039 0.333 0.881 0.029 0.176 0.051 0.366 0.885 0.084 0.174 0.024 0.292 0.883 0.023 0.214 0.023 0.291 0.884 0.095 0. 4."
        },
        {
            "title": "Influence of Explanations on the Performance of PLMs",
            "content": "Both human and LLM explanations improve PLMs performance. Results in Figures 1a, 1b show that for both of our datasets, incorporating explanations generated both by humans and LLMs result in better predictive performance compared to the baseline of no explanations independent of the LLM used to generate the explanations. This could be related to the explanations providing additional information beneficial for the task, and the models learning to use that information since they are trained with the explanations as well. Relative benefit from human and LLM explanations varies between datasets. Table 2 displays the change in performance after incorporating LLMgenerated explanations, compared to human explanations and the no-explanation baseline. Most significantly, LLM-generated explanations lead to better performance than human explanations with the HealthFC dataset, but worse performance on e-SNLI. This indicates that even though LLM-generated explanations are consistently more beneficial than having no explanations, humans can write more beneficial explanations than LLMs on certain datasets. Table 2: Performance impact by LLM-generated explanations over the baseline of no explanations and human-written explanations, averaged over the four PLMs we have used as classifiers. Subscripts indicate standard deviations. 9 (a) e-SNLI Improvement over No Explanations Human Explanations Explainer Method Accuracy Macro F1 Accuracy Macro F1 0.1790.088 0.1190.087 0.1200.088 GPT-4o Few-shot 0.1710.087 0.2040.066 0.0960.066 0.0950.066 Zero-shot 0.1940.066 0.0910.068 0.2070.070 0.2080.068 Gemma2 Few-shot 0.0830.070 0.0950.066 0.2040.066 0.2040.066 Zero-shot 0.0870.066 0.1510.076 0.1480.076 0.1480.076 Few-shot 0.1420.076 0.1220.089 0.1770.088 0.1770.089 Zero-shot 0.1130.088 0.1170.072 0.1800.072 0.1820.072 Mixtral Few-shot 0.1100.072 0.1280.065 0.1710.065 0.1710.065 Zero-shot 0.1190. Llama3 (b) HFC Improvement over No Explanations Human Explanations Explainer Method Accuracy Macro F1 Accuracy Macro F1 0.0270.093 GPT-4o Few-shot 0.0370.045 0.0440.074 Zero-shot 0.0460.039 0.0280.072 Gemma2 Few-shot 0.0370.034 0.0350.072 Zero-shot 0.0390.040 0.0710.093 Few-shot 0.0700.054 0.0470.127 Zero-shot 0.0600.062 0.0190.088 Mixtral Few-shot 0.0370.042 0.0590.091 Zero-shot 0.0560.049 0.0740.093 0.0920.074 0.0760.072 0.0820.072 0.1190.093 0.0950.127 0.0660.088 0.1070.091 0.0100.045 0.0190.039 0.0100.034 0.0110.040 0.0420.054 0.0330.062 0.0090.042 0.0280.049 Llama 4."
        },
        {
            "title": "Influence of Explanations on the Performance of LLMs",
            "content": "LLM explanations struggle to outperform the no-explanation baseline. As both Table 3 and Figures 1c, 1d show, in most cases, providing the classifier LLMs with LLM-generated explanations does not lead to better performance than having no explanations. This is in stark contrast to the results for the PLMs, where having explanations always led to benefits over the baseline. This difference might be because the LLMs used as classifiers are not explicitly trained on the explanations, and thus do not learn to use that information. The logic-based explanations of e-SNLI are akin to the CoT mechanism that LLMs deploy when answering reasoning questions. These explanations only improved the performance of PLMs, which seemingly do not have such mechanism in their predictive process, but hurt the performance of LLMs, where the explanations clashed with their internal reasoning. Conversely, the summary-style explanations of HealthFC serve to provide additional context and background 10 Table 3: Performance impact by LLM-generated explanations over the baseline of no explanations and human-written explanations, averaged over the three LLMs we have used as classifiers. Subscripts indicate standard deviations. (a) e-SNLI Improvement over No Explanations Human Explanations Explainer Method Accuracy Macro F1 Accuracy Macro F1 GPT-4o Few-shot 0.0720.047 0.0780.054 0.2260.047 0.2330.054 Zero-shot 0.0710.061 0.0780.068 0.2260.061 0.2340.068 Gemma2 Few-shot 0.1370.056 0.1490.066 0.2920.056 0.3040.066 Zero-shot 0.1550.060 0.1690.072 0.3100.060 0.3250.072 Few-shot 0.1190.057 0.1300.067 0.2740.057 0.2850.067 Zero-shot 0.1000.062 0.1130.072 0.2550.062 0.2690.072 Mixtral Few-shot 0.1060.052 0.1160.058 0.2610.052 0.2710.058 Zero-shot 0.1250.060 0.1350.069 0.2800.060 0.2910.069 Llama (b) HFC Improvement over No Explanations Human Explanations Explainer Method Accuracy Macro F1 Accuracy Macro F1 0.0550.074 GPT-4o Few-shot 0.0670.057 0.0450.074 0.0230.057 0.0560.068 Zero-shot 0.0760.066 0.0440.068 0.0140.066 0.0630.096 Gemma2 Few-shot 0.0390.066 0.0360.096 0.0510.066 0.0740.061 Zero-shot 0.0430.048 0.0260.061 0.0470.048 0.1550.053 0.1040.044 Few-shot 0.0140.044 0.1980.042 0.1420.036 Zero-shot 0.0520.036 0.0610.059 Mixtral Few-shot 0.0750.061 0.0380.059 0.0150.061 0.1060.057 0.0530.058 Zero-shot 0.0370.058 0.0060.057 0.0550.053 0.0980.042 Llama3 knowledge, and helped the PLMs and only Llama among LLMs. Our findings highlight the importance of tailoring explanation strategies to both the model type and task characteristics. LLM explanations come close to human explanations. Averaged over the classifier LLMs, the results in Table 3 show that on e-SNLI human explanations are considerably more beneficial than LLM explanations, with improvements in accuracy around 20-30%. On the HealthFC dataset, LLM explanations are more helpful, but with smaller differences in accuracy ranging from as low as 1% to 20%. These results, combined with the comparisons per model in Figures 1c, 1d indicate that human explanations are more helpful for LLMs than LLM-generated explanations in more cases and more strongly. Averaged over the classifier LLMs, the results in Table 3 show that on e-SNLI human explanations are considerably more beneficial than LLM explanations, with improvements in accuracy around 20-30%. 11 Effect of human explanations on LLMs varies strongly between datasets and models. Finally, Figures 1c, 1d show that while human explanations consistently lead to improvements over the baseline on e-SNLI, they only improve the performance of Llama 3 on HFC, and to smaller extent. With both GPT-4o mini and Qwen 2.5, human explanations instead lead to performance decreases of around 10%. These results again support the claim that LLMs are less successful in using the provided explanations to their benefit compared to PLMs fine-tuned on the explanations, and that the extent to which the LLMs make use of the explanations varies between datasets and LLMs. LLMs do not necessarily favor their own explanations. We show in Figures 1c, 1d that particularly comparing GPT-4o mini and Llama3, providing explanations generated by the models from the same model family as the classifier model do not necessarily lead to better performance than providing explanations generated by models from different families. On e-SNLI both models perform best with explanations generated by GPT-4o, and on HFC, with explanations generated by Llama3. This implies that the impact of the explanations rely more on the model generating the explanations rather than whether the explanation and the classifier models belong to the same family."
        },
        {
            "title": "4.4 Different Types of Explanations",
            "content": "The explanations in the two datasets serve different purpose. For e-SNLI, the explanations aim to clarify the logical reasoning process using which an entailment label was determined (e.g., The person is standing, therefore they cannot be sitting). On the other hand, explanations in the HealthFC dataset serve as summary of the full-text evidence articles and aim to describe what was discovered (e.g., Analyzed studies have found positive effect of the drug on the illness). This could explain the differences between performances of different models for different explanations. The logic-based explanations of e-SNLI are akin to the chain-of-thought (CoT) mechanism that LLMs deploy when answering reasoning questions. These explanations only improved the performance of PLMs, which seemingly do not have such mechanism in their own predictive process, but hurt the performance of LLMs, where the explanations clashed with their internal reasoning process. Conversely, the summary-style explanations of HealthFC serve to provide additional context and background knowledge to the models, which could explain why they improved the performance of PLMs and, in some cases, even LLMs. Providing additional evidence in prompts to models in an explanatory way augments their knowledge state and leads to improved final reasoning predictions. In addition, we also experimented with providing randomly chosen explanations from the datasets but observed worse performance than providing actual explanations. This implies that, unsurprisingly, the content of the explanations influences the models predictions"
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced novel LLM-based framework for automatically generating textual explanations for NLI tasks. Our evaluation demonstrates that these automated rationales exhibit competitive quality to human annotations and can significantly enhance downstream model performance. This framework presents new opportunities for leveraging LLM explanations to augment nonexplainable datasets and improve downstream model classification performance for both PLMs and LLMs. This work in particular highlights the potential of leveraging NLEs to improve LLMs reasoning performance. Future work will explore extending the framework to broader set of datasets to encompass wider range of tasks and complexities and further refine prompt engineering and explanation generation via refinement techniques [35] , verification and refinement [28], and consistency fine-tuning [4]. Additionally, incorporating emerging evaluation metrics such as TIGERScore [15] and Prometheus [16] will enable more comprehensive quality assessments, while comparisons with advanced reasoning LLMs like OpenAI o3 and DeepSeek R1 could further validate our approach. In addition, we plan to extend our selection of LLMs used for generating explanations by experimenting with LLMs from the same family of different sizes (e.g., Gemma-9b vs Gemma-27b) to measure the impact of size on the quality of explanations per the metrics used in this study. Finally, another point of improvement is measuring and improving the faithfulness of self-explanations by LLMs [26], as we have observed that when asked to output the most important words for their predictions, LLMs frequently assign high importance to peripheral words in the prompt such as those describing the labels or denoting parts of the input such as the explanations provided. Limitations. Our study is constrained by the sizes of the datasets considered and by the inherent challenges of evaluation metrics (e.g., MAUVE requires large output samples, and API costs for G-Eval can be prohibitive). In addition, the selection of LLMs we employed for generating explanations is limited by using one size per model family, as discussed in the future work, the study could benefit from extending this selection to models from the same family and of different sizes to systematically measure the effect of size on LLMs of same family. Despite these limitations, our findings underscore the strong potential of natural language explanations by LLM from different families and sizes in extending datasets with rationales and improving PLMs and LLMs performance in classification tasks. Acknowledgments. We would like to thank the anonymous reviewers for their helpful suggestions. This research has been supported by the German Federal Ministry of Education and Research (BMBF) grant 01IS23069 Software Campus 3.0 (TU München)."
        },
        {
            "title": "References",
            "content": "1. Bowman, S.R., Angeli, G., Potts, C., Manning, C.D.: large annotated corpus for learning natural language inference. In: Proceedings of the 2015 Conference on 13 Empirical Methods in Natural Language Processing. pp. 632642. Association for Computational Linguistics (2015). https://doi.org/10.18653/v1/D15-1075, https: //aclanthology.org/D15-1075 2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Advances in Neural Information Processing Systems. vol. 33, pp. 18771901 (2020), https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf 3. Camburu, O.M., Rocktäschel, T., Lukasiewicz, T., Blunsom, P.: e-SNLI: Natural language inference with natural language explanations. In: Advances in Neural Information Processing Systems. pp. 95399549 (2018), https://proceedings.neurips. cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf 4. Chen, Y., et al.: Towards consistent natural-language explanations via explanationconsistency finetuning. In: Proceedings of the 31st International Conference on Computational Linguistics. pp. 75587568. Association for Computational Linguistics (2025), https://aclanthology.org/2025.coling-main. 5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 41714186. Association for Computational Linguistics (2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/N19-1423 6. DeYoung, J., Jain, S., Rajani, N.F., Lehman, E., Xiong, C., Socher, R., Wallace, B.C.: ERASER: benchmark to evaluate rationalized NLP models. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 44434458. Association for Computational Linguistics (2020). https://doi.org/ 10.18653/v1/2020.acl-main.408, https://aclanthology.org/2020.acl-main.408 7. Google DeepMind Team: Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118 (2024), https://arxiv.org/abs/2408.00118 8. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Fan, A., et al.: The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024), https://arxiv.org/ abs/2407.21783 9. Gubelmann, R., Katis, I., Niklaus, C., Schärli, N., Furrer, D., Uzdilli, F., Cieliebak, M.: Capturing the varieties of natural language inference: systematic survey of existing datasets and two novel benchmarks. Journal of Logic, Language and Information 33, 2148 (2024). https://doi.org/10.1007/s10849-023-09410-4 10. Hartmann, J., et al.: Survey on how human explanations improve model learning. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. pp. 10001010. Association for Computational Linguistics (2022), https://aclanthology.org/2022.emnlp-main.67 11. Hartmann, M., Sonntag, D.: survey on improving NLP models with human explanations. In: Proceedings of the Workshop on Learning with Natural Language Supervision. pp. 4047 (2022), https://aclanthology.org/2022.lnls-1.5 12. He, P., Liu, X., Gao, J., Chen, W.: DeBERTa: Decoding-enhanced BERT with disentangled attention. arXiv preprint arXiv:2006.03654 (2020), https://arxiv.org/ abs/2006. 14 13. Hurst, A., et al.: GPT-4o system card. arXiv preprint arXiv:2410.21276 (2024), https://arxiv.org/abs/2410.21276, includes description of the GPT-4o mini variant (released July 18, 2024) 14. Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Le Scao, T., Gervet, T., Lavril, T., Wang, T., Lacroix, T., El Sayed, W.: Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024), https: //arxiv.org/abs/2401.04088 15. Jiang, D., Li, Y., Zhang, G., Huang, W., Lin, B.Y., Ren, X.: TIGERScore: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research (2024), https://openreview.net/forum?id=EE1CBKC0SZ 16. Kim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun, S., Shin, S., Kim, S., Thorne, J., Seo, M.: Prometheus: Inducing fine-grained evaluation capability in language models. In: International Conference on Learning Representations (2024), https://openreview.net/forum?id=8euJaTveKw 17. Lei, T., Barzilay, R., Jaakkola, T.: Rationalizing neural predictions. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 107117. Association for Computational Linguistics (2016), https://aclanthology. org/D16-1011 18. Lin, C.Y.: ROUGE: package for automatic evaluation of summaries. In: Text Summarization Branches Out. pp. 7481. Association for Computational Linguistics (2004), https://aclanthology.org/W0419. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., Zhu, C.: G-eval: NLG evaluation using GPT-4 with better human alignment. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 25112522. Association for Computational Linguistics (2023). https://doi.org/10.18653/v1/2023.emnlp-main. 153, https://aclanthology.org/2023.emnlp-main.153 20. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: RoBERTa: robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692 (2019), https://arxiv.org/abs/1907.11692 21. Madsen, A., et al.: Post-hoc interpretability for neural NLP: survey. ACM Computing Surveys 55(8) (2022). https://doi.org/10.1145/3533378 22. Mardaoui, D., Garreau, D.: An analysis of LIME for text data. In: Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS). Proceedings of Machine Learning Research, vol. 130, pp. 34933501 (2021), https://proceedings.mlr.press/v130/mardaoui21a.html 23. Mathew, B., et al.: HateXplain: benchmark dataset for explainable hate speech detection. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 1486714875 (2021), https://ojs.aaai.org/index.php/AAAI/article/ view/17745 24. Mishra, A., Rahman, S., Kim, H., Mitra, K., Hruschka, E.: Characterizing large language models as rationalizers of knowledge-intensive tasks. In: Findings of the Association for Computational Linguistics: ACL 2024. pp. 81178139. Association for Computational Linguistics (2024), https://aclanthology.org/2024.findings-acl. 25. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: method for automatic evaluation of machine translation. In: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311318. Association for Computational Linguistics (2002). https://doi.org/10.3115/1073083.1073135, https://aclanthology.org/P02-1040 15 26. Parcalabescu, L., Frank, A.: On measuring faithfulness or self-consistency of natural language explanations. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics (2024), https://aclanthology.org/2024.acl-long 27. Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S., Choi, Y., Harchaoui, Z.: MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In: Advances in Neural Information Processing Systems (2021), https://openreview.net/forum?id=Tqx7nJp7PR 28. Quan, X., Valentino, M., Dennis, L.A., Freitas, A.: Verification and refinement of natural language explanations through LLM-symbolic theorem proving. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. pp. 29332958. Association for Computational Linguistics (2024), https://aclanthology.org/2024.emnlp-main.172 29. Rajani, N.F., McCann, B., Xiong, C., Socher, R.: Explain yourself! leveraging language models for commonsense reasoning. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 49324942. Association for Computational Linguistics (2019). https://doi.org/10.18653/v1/P19-1487, https://aclanthology.org/P1930. Schmidtová, P., Mahamood, S., Balloccu, S., Dušek, O., Gatt, A., Gkatzia, D., Howcroft, D.M., Plátek, O., Sivaprasad, A.: Automatic metrics in natural language generation: survey of current evaluation practices. In: Proceedings of the 17th International Natural Language Generation Conference. pp. 557583. Association for Computational Linguistics (2024), https://aclanthology.org/2024.inlg-main.44 31. Søgaard, A.: Explainable Natural Language Processing. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-01052-1 32. Tan, C., et al.: On the diversity and limits of human explanations. In: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics. pp. 22192236. Association for Computational Linguistics (2022), https://aclanthology.org/2022.naacl-main.158 33. Vladika, J., Hacajová, I., Matthes, F.: Step-by-step fact verification system for medical claims with explainable reasoning. In: Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics. pp. 805816. Association for Computational Linguistics (2025). https://doi.org/ 10.18653/v1/2025.naacl-short.68, https://aclanthology.org/2025.naacl-short.68 34. Vladika, J., Schneider, P., Matthes, F.: HealthFC: Verifying health claims with evidence-based medical fact-checking. In: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING). pp. 80958107. European Language Resources Association (2024), https://aclanthology.org/2024.lrec-main.709 35. Wang, Q., Anikina, T., Feldhus, N., Ostermann, S., Möller, S., Schmitt, V.: Cross-refine: Improving natural language explanation generation by learning in tandem. In: Proceedings of the 31st International Conference on Computational Linguistics. pp. 11501167. Association for Computational Linguistics (2025), https://aclanthology.org/2025.coling-main.77 36. Warner, B., et al.: Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. In: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 25262547. Association for Computational Linguistics, Vienna, Austria (2025), https://aclanthology.org/2025.acl-long. 16 37. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chen, E., et al.: Chain-of-thought prompting elicits reasoning in large language models. In: Advances in Neural Information Processing Systems (2022), https://openreview. net/forum?id=_VjQlMeSB_J 38. Wiegreffe, S., Marasović, A.: Teach me to explain: review of datasets for explainable NLP. In: Proceedings of the NeurIPS 2021 Datasets and Benchmarks Track (2021), https://openreview.net/forum?id=HKEOxkKQpZ 39. Yao, B., Sen, P., Popa, L., Hendler, J., Wang, D.: Are human explanations always helpful? towards objective evaluation of human natural language explanations. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. pp. 1469814713. Association for Computational Linguistics (2023), https://aclanthology.org/2023.acl-long.821 40. Yeo, W.J., Satapathy, R., Goh, R.S.M., Cambria, E.: How interpretable are reasoning explanations from prompting large language models? In: Findings of the Association for Computational Linguistics: NAACL 2024. pp. 21482164 (2024), https://aclanthology.org/2024.findings-naacl.138 41. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: BERTScore: Evaluating text generation with BERT. In: International Conference on Learning Representations (2020), https://openreview.net/forum?id=SkeHuCVFDr"
        },
        {
            "title": "5.1 Further Datasets Details",
            "content": "HealthFC is officially dataset for automated fact-checking (claim verification), it is common to model this task as an NLI task. In this case, the hypothesis is the input claim being fact-checked, and the premise is the evidence text. Since the original evidence articles in HealthFC were very long, we took only the top 5 most relevant evidence sentences selected by the original authors. The factchecking labels supported, refuted, and not enough information are then mapped to the NLI labels entailment, contradiction, and neutral, respectively. Table 4 presents an example of an instance from each dataset. Table 4: Instance example from e-SNLI and HealthFC datasets. Dataset Premise Hypothesis Label The man is standing and holding newspaper. Entailment Entailment Can masks reduce corona infections when worn by large proportion of the population? e-SNLI man leans against pay phone while reading paper. HealthFC Studies show that masks slow can the spread of the coronavirus after three years of the pandemic, as there are now relatively data meaningful this. supporting masks, Surgical in turn, seem to reduce the risk of self-infection with the coronavirus. Human Explanation If the man is reading paper, he is reading newspaper. International studies suggest that the number of corona infections decreases when many people wear masks, whether fabric, surgical, or FFP2 masks. However, it is not possible to precisely quantify how great the protective effect actually is. It probably depends on the type of masks and the proportion of people who wear them."
        },
        {
            "title": "5.2 Generating NLEs with LLMs",
            "content": "For the LLMs used for generating NLEs, we used the APIs provided by Groq 5 for Mixtral-7B, Gemma2-9B, and LLama3-70B while for GPT-4o mini we use OpenAI APIs 6. 5 https://groq.com/ 6 https://platform.openai.com/ 18 For the zero-shot setting on e-SNLI, we prompt the LLMs as follows: Given the following: Premise: \"{premise}\" Hypothesis: \"{hypothesis}\" Label: \"{label_num}\" (where entailment = 0, neutral = 1, contradiction = 2) Provide exactly one sentence that directly connects the premise to the hypothesis. Do not include any prefixes like \"Explanation:\" or \"Here is the explanation.\" Start directly with the explanation sentence. The explanation should not explicitly hint at the label or contain the label itself in any form. Focus solely on reasoning that connects the premise to the hypothesis without revealing the classification. on HeathFC, the prompt is: Given the following: Claim: \"{claim}\" Sentences: \"{evidence}\" Label: \"{label_number}\" (where Supported = 0, Not enough information = 1, Refuted=2) Now, given the provided claim, sentences, and label, provide only the concise explanation in one sentence, directly referencing the claim and the provided sentences. Do not include any prefixes like \"Explanation:\" or \"Here is the explanation.\" Start directly with the explanation sentence. The explanation should not explicitly hint at the label or contain the label itself in any form. Focus solely on reasoning that connects the premise to the hypothesis without revealing the classification. For the few-shot setting, we extend the prompt templates with four (premisehypothesis-explanation) instances from the dataset as examples."
        },
        {
            "title": "5.3 Results and Examples of NLE Generation with LLMs",
            "content": "We generated natural language explanations using four LLMs for e-SNLI and HealthFC, under zero-shot and few-shot settings. This results in total of 16 additional LLM-generated explanation sets (datasets LLMs settings), which extend the original datasets. We set the temperature to zero during explanation generation to ensure deterministic outputs. Tables 5 & 6 provides examples of 19 Table 5: Examples of LLM-generated explanations of the four LLMs for zeroshot and few-shot prompts for the same instance in e-SNLI presented in Table 4) Gemma2-9B Mistral-7B Llama3-70B GPT4o-mini Zero-shot The premise states the man is leaning against pay phone, which implies he is standing, and reading paper suggests he is holding it. Few-shot The premise states the man is reading paper, which could be newspaper. The mans posture of leaning against the pay phone implies he is standing, and his engagement with the paper suggests he is holding it, possibly reading newspaper. The man, who is leaning against pay phone, is engaging in the activity of reading paper, which is type of holding newspaper. the The mans posture, his weight with pay against phone, suggests that he is standing, and the object he is reading is likely newspaper, given its typical format and size. Since the man is reading paper, it is likely that the paper is newspaper. The man is indeed standing while he is reading newspaper, as indicated by his position against the pay phone. The man is leaning against pay phone, which suggests he is standing while he reads the paper. explanations from the four LLMs for one instance in e-SNLI and HealthFC, respectively."
        },
        {
            "title": "5.4 Evaluating NLEs",
            "content": "We evaluate the quality of the LLM-generated explanations using BLEU, ROUGE, BERTScore, MAUVE, and G-Eval, as described in Section 3.3. We compute GEval using the DeepEval library7, while the remaining metrics are implemented via rouge-score8, BLEU9, BERTScore10, and MAUVE11 libraries. For computing G-Eval scores in our pipeline, we use GPT-3.5-turbo as the judge model to limit potential bias as GPT-4o mini is among the LLMs used to generate explanations. We apply the following prompt template to compute the scores: 7 https://github.com/confident-ai/deepeval 8 https://pypi.org/project/rouge-score/ 9 https://www.nltk.org/_modules/nltk/translate/bleu_score.html 10 https://pypi.org/project/bert-score/ 11 https://github.com/krishnap25/mauve 20 Table 6: Examples of LLM-generated explanations for zero-shot and few-shot prompts for the same instance in HealthFC presented in Table 4) Gemma2-9B Mistral-7B Llama3-70B GPT4o-mini ZS The sentences suggest that masks can slow the spread of the corona virus and reduce the risk of selfinfection, implying potential for reducing infections. sentence The first claim the supports by indicating that studies show masks can slow down the spread of the Corona virus, the second sentence additional provides evidence that surgical masks specifically can reduce the risk of self-infection. while The claim that masks can reduce corona infections when worn by large proportion of the population is supported by the evidence that masks can slow down the spread of the Corona virus and reduce the risk of infecting oneself with the virus. the that that sentences inThe studies dicate have shown masks can slow the spread coronavirus of and surgical masks can reduce the risk of self-infection, that suggesting masks worn by large proportion of the population could indeed help reduce infections. FS The provided sentences that suggest masks can slow the spread of the corona virus, but they do not explicitly state whether effect is significant enough when large proportion of the population wears them. this sentence The first provides evidence that masks can slow down the spread of the Corona virus, supporting the claim that can masks reduce corona infections when worn by large proportion of the population. The claim that masks can reduce corona infections when worn by large proportion of the population is supported by studies showing that masks can slow down the spread of the Corona virus and reduce the risk of infection. Studies indicate that masks can slow the spread of the corosuggesting navirus, that widespread mask usage may help reduce infections in the population. Evaluation Criteria: Human-Likeness - Measures how natural and human-like an LLMs explanations appear, assigning score based on similarity to human writing. Evaluation Steps: 1. Analyze the human explanation for style, clarity, and focus. 2. Examine the LLM-generated explanation, noting patterns and structure. 3. Compare both explanations in terms of writing style, coherence, and emphasis. 4. explanation). 5. Assign score from 1 (least human-like) to 5 (most human-like) based on these comparisons. (e.g., excessive verbosity, overIdentify LLM-specific artifacts"
        },
        {
            "title": "5.5 Experimenting with LLMs for NLI",
            "content": "We adopt zero-shot inference approach without fine-tuning. Instead, the generated explanations are directly appended to the hypothesis in the prompt. This strategy is driven by practical considerations: fine-tuning large-scale LLMs would incur substantial computational costs, require specialized hardware, and is often 21 infeasible given the models enormous parameter sizes. We prompt the LLMs using the following template, with the optional explanation: Classify the relationship as one of entailment (0), neutral (1), or contradiction (2). Premise: <premise> Hypothesis: <hypothesis> Explanation: <explanation> Use the explanation provided and choose only one label number and the six most important keywords used to answer. Example: 0; word1, ..., word6."
        }
    ],
    "affiliations": [
        "Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany"
    ]
}