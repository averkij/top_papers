{
    "paper_title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
    "authors": [
        "Vardaan Pahuja",
        "Yadong Lu",
        "Corby Rosset",
        "Boyu Gou",
        "Arindam Mitra",
        "Spencer Whitehead",
        "Yu Su",
        "Ahmed Awadallah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible."
        },
        {
            "title": "Start",
            "content": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents Vardaan Pahuja1, Yadong Lu2*, Corby Rosset2, Boyu Gou1, Arindam Mitra2, Spencer Whitehead2, Yu Su1, Ahmed Awadallah2 1The Ohio State University 2Microsoft Research, Redmond pahuja.9@osu.edu, yadonglu@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of humanlevel capabilities in more realistic online settings. key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to wide range of users in the community. Leveraging this dataset, we train Explorer, multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as key driver for improving web agent capabilities. We hope this study makes state-ofthe-art LMM-based agent research at larger scale more accessible."
        },
        {
            "title": "Introduction",
            "content": "Graphical User Interfaces (GUIs) serve as the primary medium for user interaction across digital environments. Within the GUI environment, LLMbased agents (Su et al., 2024) have shown great potential in automating complex workflows for human users. These agents are designed to operate across diverse interfaces, including the web (Deng *Equal Contribution. Work done during internship at Microsoft Research. Project Lead. et al., 2023; Zhou et al., 2024; Zheng et al., 2024), desktop (Xie et al., 2024; Wu et al., 2024; Bonatti et al., 2024), and mobile platforms (Rawles et al., 2023; Yan et al., 2023). Navigating modern GUI interfaces, which integrate textual, graphical, and interactive components, typically requires agents to possess visual grounding, long-term planning, and memory management capabilities. Recent work (Cheng et al., 2024; Gou et al., 2024) has demonstrated the effectiveness of synthetic data for enhancing visual grounding (Gou et al., 2024; Chen et al., 2024a; Kapoor et al., 2024; Chen et al., 2024b) and planning (Xu et al., 2024c; Zhang et al., 2024). Developing end-to-end GUI agents with long-term planning and grounding capabilities requires training on multi-step trajectory data (Xu et al., 2024a,c; Qin et al., 2025). However, existing trajectory datasets are primarily human-annotated (Deng et al., 2023; Li et al., 2024; Lu et al., 2024) or leverage synthetic data just for task proposal curation (Lai et al., 2024; Chen et al., 2024a). And human annotation is expensive to scale for collecting large and diverse training datasets. Therefore, synthetic data has emerged as promising alternative to human-annotated data (Hartvigsen et al., 2022; Sahu et al., 2022; Ye et al., 2022; Tang et al., 2023; Mukherjee et al., 2023; Mitra et al., 2024). Collecting trajectorylevel datasets presents unique challenges: 1) curating diverse set of task intents at scale, 2) deploying an agent capable of interacting with realworld environment to complete these tasks through series of actions, and 3) verifying whether the task is accomplished by the executed action sequence. Data diversity is essential for equipping generalist web agents with broad range of skills. Existing work on synthetic web trajectory generation employs self-instruct for task proposal generation (He et al., 2024b). It formulates task proposals from homepages or parametric LLM knowledge, overlooking the richer content available in deeper 5 2 0 F 7 1 ] . [ 1 7 5 3 1 1 . 2 0 5 2 : r Figure 1: Data Generation Pipeline. The task proposer agent generates an initial task and the first action based on the website homepage. The task is then iteratively refined in subsequent steps by the refiner agent. Finally, the task summarizer agent constructs an overall task description from the action sequence, followed by task verification to assess correctness. web pages, which is essential for achieving broader task diversity. Another line of work leverages web tutorials as form of supervision for generating web trajectories (Ou et al., 2024; Xu et al., 2024a). While web tutorials effectively cover common daily user tasks, the resulting trajectory data exhibits limited domain diversity in terms of website and domain coverage  (Table 1)  . Additionally, informationseeking tasks remain underrepresented. Due to these limitations, web agents trained on existing synthetic trajectory datasets have not seen much success in more realistic online evaluation settings. To enhance web agents performance in real-world settings, it is essential to incorporate greater diversity in their training trajectories. In this work, we develop scalable and diverse web trajectory data synthesis recipe for training GUI agent models. Inspired by how humans learn to use the internet, we leverage exploration as key mechanism for achieving diversity in task intents. We introduce Explorer (EXPLorationdriven web trajectORy gEneratoR), framework for systematic web exploration to generate diverse, high-quality trajectory datasets. Unlike prior work that relies on static task proposals, Explorer dynamically explores web environments to curate diverse, real-world tasks. This exploration-based approach ensures broader task coverage and better generalization to real-world scenarios. We instantiate this framework using popular URLs from several sources, such as Tranco (Pochat et al., 2019) and similarweb.com as seeds. Our dataset comprises 94K diverse web trajectories spanning 49K unique URLs, making it the largest web trajectory dataset to date. Each trajectory is richly annotated with artifacts such as screenshots, raw and set-of-mark (Yang et al., 2023) annotated versions, HTML, and the accessibility tree, enabling comprehensive web agent training. To construct this dataset, we develop multi-agent pipeline that starts with an abstract task proposal and iteratively refines it into more specific task through web exploration (Figure 1). Unlike previous approaches, our pipeline generates tasks better grounded in real-world websites, improving task relevance and diversity. To demonstrate the effectiveness of our dataset, we train small language models using just the synthetic data and outperform existing web agent baselines by significant margin. The main contributions of this work are as follows: We develop scalable and easily customizable multi-agent pipeline for web agent trajectory synthesis. This pipeline leverages exploration as core mechanism to generate diverse trajectory data, ensuring broad domain coverage and skill diversity in the resulting dataset. We leverage this pipeline to generate diverse and high-quality GUI trajectory dataset consisting of 94K trajectories, spanning 49K # Trajectories # Websites Modality RUSS (Xu et al., 2021) Mind2Web (Deng et al., 2023) WebLINX (Lu et al., 2024) GUIAct (Chen et al., 2024a) OpenWebVoyager (He et al., 2024b) AgentTrek (Xu et al., 2024a) NNetnav (Murty et al., 2024) Explorer 80 2350 969 2482 1165 10.4K 6K 94K 22 137 155 121 48 127 HTML HTML + Screenshot HTML + Screenshot Screenshot A11y tree + Screenshot A11y tree + HTML + Screenshot A11y tree + Screenshot 49K A11y tree + Screenshot (raw + SoM) + HTML Table 1: Comparison to existing web agent benchmarks. unique URLs with 720K screenshots and 33M web elements, making it the largest web trajectory dataset of this scale. We demonstrate the effectiveness of our dataset by training small language models, which achieve strong performance on both online and offline benchmarks, significantly surpassing existing web agent baselines, including those with larger parameter counts."
        },
        {
            "title": "2.1 LLM-based Web Agents",
            "content": "Recent advances in multimodal language models have facilitated the development of web agents autonomous systems designed to interact with realworld websites to perform everyday tasks (Deng et al., 2023; Hong et al., 2024; Cheng et al., 2024; Zheng et al., 2024). Key challenges for web agents include long-term planning, visual grounding, and memory management. To improve long-context understanding, WebAgent (Gur et al., 2024) utilizes multiple LLMs - one for planning, summarization, and grounded program synthesis. SeeAct (Zheng et al., 2024) adopts two-step procedure of planning followed by grounding at each step using GPT-4 to accomplish web agent tasks. Another line of work employs vision-only approach to train GUI grounding model that directly predicts pixel coordinates for executing GUI agent tasks (Cheng et al., 2024; Kapoor et al., 2024; Gou et al., 2024). However, significant bottleneck remains the lack of large-scale, high-quality web trajectory data for training robust agents. Our work presents new framework for synthesizing large-scale web trajectory data to train end-to-end web agents."
        },
        {
            "title": "2.2 Web Agent Benchmarks and Datasets",
            "content": "Early benchmarks for web tasks such as MiniWob++ (Liu et al., 2018) focused on testing lowlevel actions on simulated websites. However, these simulated websites fail to capture the complexity of the real-world web. Mind2Web (Deng et al., 2023) introduces trajectory-level dataset with 2K tasks across 137 real-world websites and 31 domains. However, it employs static evaluation method that penalizes alternative valid execution paths. To overcome this limitation, followup work has explored alternative evaluation approaches, including functional correctness-based evaluation in WebArena (Zhou et al., 2024) and key-node-based evaluation in Mind2Web-Live (Pan et al., 2024b). Towards the goal of making web agents more capable of performing realistic tasks, GAIA (Mialon et al., 2024) and AssistantBench (Yoran et al., 2024) introduce benchmarks that include time-consuming information-seeking tasks. In this work, we develop Explorer, multimodal web agent trained on our synthetic dataset, and showcase its strong performance across online and offline benchmarks, including Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++."
        },
        {
            "title": "2.3 Data Synthesis for Web Agents",
            "content": "Early efforts to acquire trajectory data for training web agents primarily relied on crowd-sourcing (Deng et al., 2023; Lu et al., 2024). However, human annotation is cost-prohibitive, prompting the adoption of synthetic data generation approaches to facilitate large-scale data collection. AutoWebGLM (Lai et al., 2024) and GUIAct (Chen et al., 2024a) utilize LLMs to generate task proposals, which human experts subsequently annotate. OpenWebVoyager (He et al., 2024b) employs web agent to execute auto-generated task descriptions. However, since these task descriptions are generated using LLMs without exploring website, they fail to capture the full diversity of possible tasks on that website. Another line of work, including Synatra (Ou et al., 2024) and AgentTrek (Xu et al., 2024a), leverages web tutorials to guide web trajectory generation. Meanwhile, concurrent Information View the detailed 7-day weather forecast for Toronto, ON on The Weather Network website. Analyze Teslas stock performance over various time periods on Yahoo Finance. Convert 100 US Dollars to Euros using the XE currency converter. Find directions from Seattle, WA to Bellevue, WA using Bing Maps. Service Research the French Bulldog breed on the American Kennel Club website, including its popularity and family life traits. Find the nearest Penske truck rental location in Anaheim, California, and start the reservation process for truck. Explore and purchase subscription for the UpToDate Pro Suite on the Wolters Kluwer website. Entertainment Find the Basscon presents: Darren Styles EDM event on Eventbrite, save it, and share it on Twitter. View the details of the Photography Competition Winners - Season and share the article on Twitter. Shopping Browse through the fall home decor section on the Target website to explore variety of fall-themed home decor items. Purchase three-seat fabric sofa, specifically the UPPLAND Sofa, from IKEAs website. Travel Search for flights from Seattle to New York, select travel dates, and explore various flight options. Find the weight of baggage allowance for economy class on qatarairways. Table 2: Example task descriptions from Explorer. effort (Murty et al., 2024) employs an explorationbased trajectory generation in WebArenas sandbox, while our work focuses on more realistic web agent evaluation on live websites. To address diversity limitations in prior trajectory synthesis work, we design bottom-up web trajectory synthesis pipeline that explores websites dynamically while maintaining coherent high-level task intent."
        },
        {
            "title": "3 Data Recipe",
            "content": "We design an automatic web trajectory synthesis pipeline that explores websites to generate diverse web trajectories. It utilizes Playwright* to execute actions and collect metadata from real-world websites, starting from an initial URL. The metadata includes screenshots, HTML, A11y tree, and actions in grounded and natural language forms. The action space is given in Table 3."
        },
        {
            "title": "3.1 Website Selection",
            "content": "We use combination of URL sources to generate the synthetic web trajectories. We obtain the top 100 URLs from similarweb.com corresponding to the high-traffic portion of the web with transactional tasks like booking flights, restaurant reservations, government services, sports, entertainment, etc. The Tranco (Pochat et al., 2019) URLs include 49K URLs representing the head portion of the web, which is less trafficked but popular nonetheless. We filter out harmful websites containing violent or explicit content to ensure safety compliance. *https://playwright.dev/ For 4K subset of trajectories, we instruct GPT-4o to navigate to the target website by formulating Google search query based on the task description. Overall, we generate 94K trajectories across both sources. The complete data generation takes 50 hours, utilizing 60 parallel processes. The viewport resolution is up to 1980 1080. Action Type Description click [elem] type [elem] [text] select [elem] [text] goto [url] search_google [query] scroll [up/down] Click on elem. Type text Select text from dropdown list. Go to url. Search for query on Google. Scroll up or down. Count 415K 62K 5K 26K 4K 213K Table 3: Action space for web navigation in Explorer. Metric # Total trajectories # Success trajectories # Unique URLs Average steps per trajectory Average elements per image # Tokens # Elements # Images Cost per trajectory Cost per successful trajectory Value 175K 94K 49K 7.7 46.3 830M 33.3M 720K $0.15 $0. Table 4: Dataset statistics for Explorer. The number of unique URLs, average steps per trajectory, average elements per image, and number of tokens, elements, and images correspond to the successful trajectories."
        },
        {
            "title": "3.2 Data Generation Pipeline",
            "content": "We aim to develop generalized pipeline for web exploration to collect diverse web trajectory data. To enhance diversity, we adopt bottom-up approach, starting with low-level actions and progressively shaping them into high-level task descriprobot detection mechanisms, CAPTCHA verification, login prompts, or payment requests. Task Refiner. The task refiner agent receives the initial task proposal or the refined task description from the previous step, along with the corresponding action history as inputs. It then predicts the next action consistent with the input task description and the updated refined task description while incorporating the complete action history. By iteratively refining the task description after each action, the agent ensures that the updated task remains aligned with the action history. Task Summarizer. This module processes the entire action and screenshot history to predict an overall task description that aligns with the trajectory. The task summary is expected to be high level, i.e., it should describe what the task entails while omitting how it is accomplished. Task Verifier. Inspired by Pan et al. (2024a), the task verifier agent receives the task description and action history, serving as critic to evaluate whether the trajectory successfully completes the specified task. In addition to the screenshots of the trajectory, it also receives markdown representation of the last page. This ensures the verifier has the full context of the websites final state, even when the viewport cannot capture all the content. Such automatic evaluation of web trajectories has been widely adopted in prior work (Xu et al., 2024a; He et al., 2024a; Koh et al., 2024). Figure 1 illustrates the above pipeline. The prompts for the above agents are given in Appendix D."
        },
        {
            "title": "3.3 Dataset Analysis",
            "content": "Explorer comprises web trajectories spanning diverse domains, including services, entertainment, shopping, travel, and information, ensuring broad task diversity. Figure 3 visualizes the domain and subdomain distribution, highlighting the datasets rich diversity. Sample tasks from Explorer are presented in Table 2. To the best of our knowledge, Explorer with 94K trajectories is the largest web trajectory dataset of this scale. Table 1 shows comparison with existing web agent datasets from the literature. The detailed statistics are given in Table 4. Figure 2 shows the histogram of token distribution across the dataset. Beyond diversity, Explorer is also highly scalable and cost-efficient. Our approach achieves cost of $0.28 per successful trajectory, making it Figure 2: Histogram of token distribution across trajectories in Explorer. The red vertical line shows the 90th percentile of the distribution. Model Cost per trajectory Mind2Web (Deng et al., 2023) AgentTrek (Xu et al., 2024a) Explorer $0.85 $0.55 $0.28 Table 5: Cost comparison with other trajectory generation approaches. tions while maintaining coherent task intent. In the first step, the proposer agent generates an abstract task, which is refined to more specific task through refinement process. For instance, starting from the Amazon homepage, the initial task proposal might be Find todays deals on Amazon, which is progressively refined into Proceed to checkout for the Amazon eero Pro 6E mesh WiFi router with 36% off (Figure 1). Since the agents execute actions alongside the refinement process, the generated tasks respect real-world constraints, such as product availability, available color options, and other specifications, ensuring practical applicability. Our pipeline consists of the following LLM-powered agents: Task Proposer. Given website homepage, including its screenshot and accessibility tree, the task proposer agent generates diverse initial tasks that could be performed on that website. The task descriptions at this stage are instructed to be highlevel and abstract versions of the real-world tasks, which will be refined into more specific tasks in later stages. Along with generating the task proposal, the agent proposes and executes the first action toward completing that task. Furthermore, the agent is instructed to halt upon encountering We use GPT-4o as the agent backbone throughout the data generation process. Figure 3: Data composition for Explorer. Its extensive diversity showcases its potential to train end-to-end generalist web agents. approx. 2 more cost-effective than AgentTrek (Xu et al., 2024a) (which incurs $0.55 per trajectory) and significantly cheaper than human annotation  (Table 5)  . Unlike human annotation, which requires training crowd workers and continuous quality monitoring, Explorers automated generation pipeline eliminates these bottlenecks, ensuring scalability with minimal overhead. By integrating diversity, scalability, and cost efficiency, Explorer sets new benchmark for generating large-scale web trajectory datasets, making it valuable resource for training generalist web agents."
        },
        {
            "title": "4 Experiments",
            "content": "We use the synthetic trajectories generated by our pipeline to train small multimodal language models (SLMs) for web agent tasks. To ensure computational efficiency, we select 40K trajectories from the full set for training. We further refine this subset by filtering out trajectories that contain more than two scroll actions to mitigate potential model bias toward excessive scrolling behavior. Finally, we use 30K trajectories obtained after filtering to fine-tune multimodal language models like Phi-3.5V (Abdin et al., 2024) and Qwen2-VL-7B (Wang et al., 2024a). For brevity, we denote the models trained on Phi-3.5V and Qwen2-VL-7B as Explorer-4B and Explorer-7B, respectively. To test the effectiveness of our data for web-based agentic tasks, we evaluate Explorer-4B and Explorer-7B on Mind2Web-Live (Pan et al., 2024b), MultimodalMind2Web (Deng et al., 2023; Zheng et al., 2024), and MiniWob++ (Liu et al., 2018). Multimodal-Mind2Web. MultimodalMind2Web is an offline web agent benchmark comprising 2K open-ended tasks spanning 137 websites across 31 domains. Each task comprises sequence of actions with screenshots, action type, and HTML. We follow the setting in Zheng et al. (2024) and report element accuracy, operation F1, and step success rate (SR) as evaluation metrics. Mind2Web-Live. Mind2Web-Live is benchmark modified from the original Mind2Web dataset to test web agents on live websites rather than static trajectories. The benchmark evaluates performance using key-node-based evaluation approach rather than using golden action sequence, requiring valid trajectories to reach annotated key nodes across 104 test tasks in Mind2Web. Since Mind2Web-Live relies on real-world dynamic websites, it encounters robot detection such as reCAPTCHA, which hinders testing (Xu et al., 2024c). To address this, we select subset of 83 test set tasks that remain consistently accessible throughout our tests. Following Pan et al. (2024b), we report the average step success rate, completion rate, and full task success rate (SR) on the test set. The average step success rate and completion rate represent the proportion of completed key nodes, using macro and micro averages, respectively. MiniWob++. This benchmark consists of lowlevel tasks on single webpage. Typical examples include clicking sequence of buttons, selecting items from drop-down list, and filling out form. We use the subset of 46 tasks used for evaluation in prior work (Zeng et al., 2024; Ou et al., 2024). The final score is obtained by averaging the results of four runs per task. We use the zero-shot evaluation setting, which does not use any environmentspecific trajectories for training."
        },
        {
            "title": "5 Results",
            "content": "5.1 In-domain Evaluation As an intrinsic evaluation of the trajectory collection pipeline, we generate 100 test tasks using Explorer, disjoint from the train set. The SLM agents are tasked with executing the given tasks on live websites while an LLM-as-a-judge verifier ( 3.2) evaluates the correctness of their actions at the trajectory level  (Table 7)  . We observe that the fine-tuned agents significantly outperform their pre-trained counterparts. Thus, using in-domain web trajectory data training helps, which is valuable sanity check. It also surpasses GPT-4o, further underscoring our synthetic datas quality."
        },
        {
            "title": "5.2 Mind2Web-Live Results",
            "content": "We evaluate Explorer-4B and Explorer-7B trained on the synthetic trajectory dataset  (Table 6)  . We make the following observations from the results: Improvement over base pre-trained models. We observe that Explorer-7B yields improvements of 5.1% and 4.8% in average step success rate and key node completion rate, respectively, compared to the pre-trained Qwen2-VL-7B model. Similarly, Explorer-4B obtains gains of 15.5% and 15.9% in average step success rate and key node completion rate, respectively, over its pre-trained counterpart. In terms of full task success rate, Phi-3.5V improves significantly from 2.4% to 18.1%, while Qwen2-VL-7B improves from 14.5% to 19.3%. To the best of our knowledge, this represents the state-of-the-art performance on Mind2Web-Live for end-to-end web agents of this size that are trained exclusively on synthetic data. Improvement over higher capacity pre-trained models. Despite having much fewer parameters, we observe that Explorer-4B outperforms strong baselines such as Mistral-7B-Instruct-0.3 and Qwen2-72B-Instruct in full task SR by margins of 8.5% and 2.7%, respectively. The Phi3.5V model obtains an 18.1% full task success rate, which is better than GPT-3.5 (15.4%), despite using orders of magnitude fewer parameters. The corresponding results for the entire set of 104 tasks, including unreachable websites, are given in Appendix A."
        },
        {
            "title": "5.3 Multimodal-Mind2Web Results",
            "content": "Following Deng et al. (2023), we obtain the top-50 elements from pre-trained DeBERTa (He et al., 2021) candidate generation model which are then used to construct the accessibility tree and SoM image inputs. The results are shown in Table 9. Among baselines, we include API-based models for in-context learning GPT-3.5, GPT-4, and SeeAct (Zheng et al., 2024). SeeAct is web agent that performs web tasks using two-step procedure of action generation and grounding using GPT-4V. Additionally, we include baselines that fine-tune small language models using synthetic data, followed by further fine-tuning on the Mind2Web training set. SeeClick (Cheng et al., 2024) introduces visual grounding model (QwenVL) trained on synthetically-generated grounding data. EDGE (Chen et al., 2024b) synthesizes QA data on webpages to improve the grounded GUI understanding capabilities of MLLMs. ScribeAgentLarge (Shen et al., 2024) and MiniCPM-GUI (Chen et al., 2024a) use human-annotated trajectory data to train web agents. AgentTrek (Xu et al., 2024a) is GUI agent baseline that also utilizes synthetic trajectory data to fine-tune SLMs for Mind2Web, similar to our setting. We observe that Explorer7B fine-tuned on synthetic data from Explorer plus Mind2Web outperforms all baselines in average step success rate. Notably, it surpasses AgentTrek, which uses the same Qwen2-VL-7B MLLM backbone, highlighting the superior quality of our dataset. The broad domain coverage and task diversity in Explorer contribute to its superior generalization across environments."
        },
        {
            "title": "5.4 MiniWob++ Results",
            "content": "Table 8 shows the results on the MiniWob++ benchmark in the zero-shot evaluation setting. Among baselines, we have API-based models, in-context learning using open-source LMs, and agentic models like AgentLM (Zeng et al., 2024), CodeActAgent (Wang et al., 2024b), Lemur-Chat (Xu et al., 2024b) and AgentFlan (Chen et al., 2024c) which include web-based demonstrations in their instruction tuning dataset. Synatra-CodeLlama-7B (Ou et al., 2024) and AgentTrek (Xu et al., 2024a) also synthesize web-agent trajectories automatically. We observe that Explorer outperforms GPT-4 and Model API-based Models GPT-4o GPT-3.5 Open-source Instructed Models Mistral-7B-Instr.-0.3 (Jiang et al., 2023) Qwen2-72B-Instruct (Bai et al., 2023) Qwen2-VL-7B (Wang et al., 2024a) Phi-3.5V (Abdin et al., 2024) Supervised Fine-Tuning Explorer-4B Explorer-7B Avg. Step SR (%) Completion Rate (%) Task SR (1) (%) Full Task SR (%) 58.5 32.8 40.2 28. 44.0 45.3 52.8 36.5 29.5 40.9 35.4 23.5 39.4 40.2 44.6 24.1 34.9 20. 31.3 34.9 25.3 15.4 9.6 15.4 14.5 2.4 18.1 19.3 Table 6: Results on Mind2Web-Live benchmark. Missing values are denoted by . The results for GPT-4 and Mistral-7B have been reproduced on our Linux servers. The results for GPT-3.5 and Qwen2-72B-Instruct have been taken from Pan et al. (2024b). The full task success rate represents the successful completion of all key nodes for given task. The average step success rate represents the proportion of completed key nodes, macro-averaged across tasks. The completion rate represents the proportion of completed key nodes, micro-averaged across tasks. Task SR (1) represents task SR with tolerance of up to one error/key node. Model GPT-4o Phi-3.5V Explorer-4B Qwen2-VL-7B Explorer-7B Full Task SR (%) 16.0 1.0 17. 6.0 18.0 Table 7: In-domain evaluation results. The fine-tuned Explorer models achieve significant improvements over their pre-trained counterparts and surpass closed-source LLMs, including GPT-4o. Model API-based Models GPT-3.5 GPT-4 Open-source Instructed Models Phi-3.5V Qwen2-VL-7B Llama3-chat-8B Llama3-chat-70B Accuracy (%) 39.57 53.04 35.87 36.96 31.74 48.70 Open-source Interactive Data Finetuned Models AgentLM-7B (Zeng et al., 2024) CodeActAgent-7B (Wang et al., 2024b) AgentFlan-7B (Chen et al., 2024c) Lemur-chat-70B (Xu et al., 2024b) AgentLM-70B (Zeng et al., 2024) Synatra-CodeLlama-7B (Ou et al., 2024) AgentTrek-7B (Xu et al., 2024a) Explorer-4B Explorer-7B 15.65 9.78 20.87 21.30 36.52 38.20 45.28 46.74 53.26 Table 8: Results on MiniWob++ benchmark (Liu et al., 2018) in zero-shot evaluation setting. The baseline numbers correspond to Ou et al. (2024). Explorer outperforms much larger models by significant margin. general-purpose agents like AgentLM, CodeActAgent, Lemur-Chat, and AgentFlan. Explorer-4B surpasses Synatra-CodeLlama-7B and AgentTrek7B despite using much smaller model with 4.2B params, highlighting our synthetic datas superior quality and potential for out-of-distribution (OOD) generalization."
        },
        {
            "title": "5.5 Data Scaling Experiments",
            "content": "We conduct experiments with different data scales for Explorer-4B to analyze the impact of training data size. Specifically, we subsample the original trajectory dataset to utilize 50% and 25% of its original size. Figure 4 presents the resulting performance curves. Our results show that, even with just 25% of the training data, the model exhibits rapid performance gains over the base pre-trained model. Increasing the dataset size further leads to gradual improvements across all reported metrics. However, the increase in the overall task success rate is more gradual compared to the stepwise metrics, as it is more coarse-grained metric."
        },
        {
            "title": "6.1 Ablation Studies",
            "content": "We conduct ablation studies to assess the impact of various design choices on overall performance  (Table 10)  . To evaluate the importance of visual modality, we experiment with using just the textual modality for the Phi-3.5V model, replacing it with the text-only Phi-3-mini (Abdin et al., 2024). In addition to Qwen2-VL-7B and Phi-3.5V, we also evaluate LLaVA-Mistral-7B (Liu et al., 2023), strong Model Train Data Cross-Task Cross-Website Cross-Domain Avg. Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR In-Context Learning GPT-3.5 GPT-4 SeeAct (Zheng et al., 2024) Supervised Fine-Tuning 19.4 40.8 46. SeeClick-9.6B (Cheng et al., 2024) EDGE-9.6B (Chen et al., 2024b) MiniCPM-GUI-3.1B (Chen et al., 2024a) Syn. + M2W 23.8 38.0 Syn. traj. Scribe-Agent-L.-32B (Shen et al., 2024) Syn. + M2W 60.8 AgentTrek-7B (Xu et al., 2024a) Syn. + M2W 26.3 Syn. + M2W Explorer-4B Explorer-4B Explorer-4B Explorer-7B Explorer-7B Explorer-7B 36.5 Syn. traj. 48.1 M2W Syn. + M2W 53. 43.6 Syn. traj. 51.8 M2W Syn. + M2W 56.5 59.2 63.1 73.4 86.2 86.8 52.9 88.9 82.9 88.0 88.1 86.6 88.0 90.3 16.8 32.3 40. 23.7 30.0 20.8 35.6 55.7 33.2 44.8 50.7 39.6 48.3 53.2 14.9 30.2 38 21.9 20.3 34.1 57.6 44.1 49.1 55. 48.7 56.3 60.5 56.5 61.0 67.8 82.9 81.7 52.7 88.1 87.7 87.2 89.5 87.7 89.7 90.7 14.1 27.0 32. 18.8 21.1 17.3 32.5 51.4 39.3 45.0 51.4 44.5 52.0 56.7 25.2 35.4 42.4 22.1 17.9 39.4 56.0 42.5 46.9 49. 47.6 50.9 55.7 57.9 61.9 69.3 84.1 74.5 54.7 87.5 86.3 87.7 88.8 87.2 88.9 90.4 24.1 29.7 36. 18.3 29.7 36.5 20.2 22.4 14.6 37.3 52.6 39.8 44.6 47.2 44.7 48.1 53.0 20.9 24.5 17.6 35.1 53.2 37.4 44.8 49. 43.0 49.5 54.3 Table 9: Multimodal-Mind2Web evaluation results. The baseline numbers have been taken from Zheng et al. (2024); Cheng et al. (2024); Chen et al. (2024b,a); Shen et al. (2024). The last column denotes the average step success rates over the three test splits. Explorer significantly outperforms existing GUI agent baselines. Model Avg. Step SR (%) Completion Rate (%) Full Task SR (%) LLaVA-Mistral-7B Phi-3-mini (text-only) Phi-3.5V Qwen2-VL-7B 32.0 36.6 44.0 45.3 30.3 34.0 39.4 40.2 4.8 13.3 18.1 19.3 Table 10: Ablation studies on language models used for fine-tuning (Mind2Web-Live). MLLM baseline. Our results show that omitting the visual modality leads to sharp 4.8% drop in performance for Phi-3.5V, underscoring its importance for effective GUI grounding. Furthermore, LLaVAMistral-7B significantly underperforms compared to both Qwen2-VL-7B and Phi-3.5V, highlighting the necessity of stronger MLLM backbone for improved GUI agent performance."
        },
        {
            "title": "6.2 Failure Modes of Trajectory Generation",
            "content": "We analyze cases where generated trajectory is ultimately rejected by the task verifier agent. Our goal is to synthesize trajectory data that closely resembles human-annotated datasets for training web agents. However, since our pipeline collects trajectories through an exploration-driven approach, some trajectories result from random, incoherent action sequences that fail to align with welldefined task intent. For instance, in shopping tasks, the agent may explore various products without demonstrating an intent to purchase (e.g., by adding items to the cart). Another failure case arises when the agent encounters errors on the final page due to automated browser detection, CAPTCHA verification, or an unresponsive website. We note that the verifier agent is instructed to judge trajectory as successful if the task is completed, except for the final login and payment steps. The trajectories in failure modes are still valuable for web agents to learn low-level tasks such as form filling, basic interaction with web elements, and visual grounding."
        },
        {
            "title": "6.3 Case Studies for Mind2Web-Live",
            "content": "We randomly sample 20 error cases for Explorer on Mind2Web-Live to gain insights for future improvement. These errors fall into the following categories: Task deviation: The agent executes actions unrelated to the given task, thus failing to complete it. Missing key steps: The agent retrieves results that partially satisfy the required constraints, e.g., the agent finds womens clothes of the correct size but incorrect type or color. Grounding error: The agent fails to interact with valid element on the page. Website unresponsive: The agent executes the correct action, but the website does not respond. Figure 4: Experiments with data scaling using Explorer-4B on Mind2Web-Live. We experiment with using 100%, 50%, and 25% of the trajectory data. All results are averaged over three runs. All metrics exhibit improvement with increase in data scale. highlight the critical role of data scale in enhancing web agents performance. Future work will focus on extending this framework to encompass broader range of GUI environments, such as operating systems with diverse applications. GUI agents require specialized skills for different tasks, including information-seeking, operational, and navigation skills. Efficient exploration of the environment to acquire these skills presents another promising avenue for future research."
        },
        {
            "title": "Limitations",
            "content": "Explorer explores the web environment autonomously, which may occasionally result in incoherent tasks. Synthetic data collection using closedsource LLMs can be costly due to associated API expenses. While this work serves as proof of concept, future research will focus on developing tailor-made open-source LLMs for this task. Additionally, some website content remains inaccessible due to login requirements, leading to insufficient data for those websites. Figure 5: Statistics for different error cases in Mind2Web-Live evaluation. Task deviation is the most prevalent error type. Failure to reach the correct website: This happens when the agent fails to output the correct website URL or use the search engine to arrive at the correct website. Figure 5 presents the statistics for these error types."
        },
        {
            "title": "Ethical Considerations",
            "content": "In this work, we introduce Explorer, scalable framework for synthesizing web trajectories on large scale. By leveraging thorough web exploration, Explorer ensures diversity in both domains and the skills acquired by web agents. Unlike previous approaches, our framework generates contextually grounded trajectories that adapt to realworld constraints, improving both task relevance and generalization. We instantiate this framework using URLs collected from diverse sources. Explorer outperforms existing web agent baselines by significant margin on both online and offline web agent benchmarks. Furthermore, our results The synthetic data collection pipeline proposed in this paper is intended solely for academic research on GUI agents, with strict ethical safeguards to prevent unauthorized website interactions. To ensure ethical compliance and mitigate risks, we prompt our agents to automatically terminate upon encountering CAPTCHA verifications, login prompts, or payment requests, ensuring that no actual transactions or bookings occur. Additionally, we filter out websites containing violent or explicit content and strictly adhere to privacy regulations, ensuring that no personal information is used during action execution. To enforce responsible data collection, we monitor subset of automatically generated trajectories to ensure compliance with website access policies. Moreover, we distribute the workload across websites to prevent excessive requests and minimize the impact on any single domain."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank colleagues from the OSU NLP group and Microsoft for their feedback. This research was supported in part by ARL W911NF2220144 and computational resources from the Ohio Supercomputer Center (Center, 1987)."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. 2024. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264. Ohio Supercomputer Center. 1987. Ohio supercomputer center. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. 2024a. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317. Xuetian Chen, Hangcheng Li, Jiaqing Liang, Sihang Jiang, and Deqing Yang. 2024b. Edge: Enhanced grounded gui understanding with enriched arXiv preprint multi-granularity synthetic data. arXiv:2410.19461. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024c. Agent-flan: Designing data and methods of effective agent tuning for large language models. In Findings of ACL. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of ACL. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. In Proceedings of NeurIPS. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2024. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243. Izzeddin Gur, Hiroki Furuta, Austin V. Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. real-world webagent with planning, long context understanding, and program synthesis. In Proceedings of ICLR. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of ACL. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024a. Webvoyager: Building an end-toend web agent with large multimodal models. In Proceedings of ACL. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, and Dong Yu. 2024b. Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. arXiv preprint arXiv:2410.19609. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In Proceedings of ICLR. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. 2024. Cogagent: visual language model for GUI agents. In Proceedings of CVPR. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. 2024. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In Proceedings of ECCV. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. 2024. Tree search for language model agents. arXiv preprint arXiv:2407.01476. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, and Zhengyang Wu. 2024b. Webcanvas: Benchmarking web agents in online environments. In Agentic Markets Workshop at ICML 2024. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. 2024. Autowebglm: large language model-based web navigating agent. In KDD. Victor Le Pochat, Tom van Goethem, Samaneh Tajalizadehkhoob, Maciej Korczynski, and Wouter Joosen. 2019. Tranco: research-oriented top sites ranking hardened against manipulation. In Proceedings of Network and Distributed System Security Symposium. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the effects of data scale on UI control agents. In Proceedings of NeurIPS Datasets and Benchmarks Track. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. 2025. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided exploration. In Proceedings of ICLR. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Proceedings of NeurIPS. Xing Han Lu, Zdenek Kasner, and Siva Reddy. 2024. Weblinx: Real-world website navigation with multiturn dialogue. In Proceedings of ICML. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: benchmark for general AI assistants. In Proceedings of ICLR. Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, et al. 2024. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Shikhar Murty, Dzmitry Bahdanau, and Christopher Manning. 2024. Nnetscape navigator: Complex demonstrations for web agents without demonstrator. arXiv preprint arXiv:2410.02907. Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. 2024. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale. In Proceedings of NeurIPS. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 2024a. Autonomous evaluation and refinement of digital agents. In Proceedings of Conference on Language Modeling. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023. Androidinthewild: large-scale dataset for android device control. In Proceedings of NeurIPS Datasets and Benchmarks Track. Gaurav Sahu, Pau Rodríguez, Issam H. Laradji, Parmida Atighehchian, David Vázquez, and Dzmitry Bahdanau. 2022. Data augmentation for intent classification with off-the-shelf large language models. In Proceedings of the 4th Workshop on NLP for Conversational AI, ConvAI@ACL. Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet Talwalkar. 2024. Scribeagent: Towards specialized web agents using production-scale workflow data. arXiv preprint arXiv:2411.15004. Yu Su, Diyi Yang, Shunyu Yao, and Tao Yu. 2024. Language agents: Foundations, prospects, and risks. In Proceedings of EMNLP: Tutorial Abstracts. Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023. Does synthetic data generation of arXiv preprint llms help clinical text mining? arXiv:2303.04360. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024b. ExeIn cutable code actions elicit better LLM agents. Proceedings of ICML. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. 2024. OS-copilot: Towards generalist computer agents with self-improvement. In Proceedings of ICLR 2024 Workshop on Large Language Model (LLM) Agents. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. Gpt-4v(ision) is generalist web agent, if grounded. In Proceedings of ICML. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2024. Webarena: realistic web environment for building autonomous agents. In Proceedings of ICLR. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. 2024. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Proceedings of NeurIPS Datasets and Benchmarks Track. Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James A. Landay, and Monica Lam. 2021. Grounding open-domain instructions to automate web support tasks. In Proceedings of NAACL. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. 2024a. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605. Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. 2024b. Lemur: Harmonizing natural language In Proceedings of and code for language agents. ICLR. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. 2024c. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454. An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. 2023. Gpt4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. 2022. Zerogen: Efficient zero-shot learning via dataset generation. In Proceedings of EMNLP. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. 2024. Assistantbench: Can web agents solve realistic and time-consuming tasks? In Proceedings of EMNLP. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. Agenttuning: Enabling generalized agent abilities for llms. In Findings of ACL. Jiwen Zhang, Jihao Wu, Teng Yihua, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024. Android in the zoo: Chain-of-action-thought for GUI agents. In Findings of EMNLP."
        },
        {
            "title": "Appendices",
            "content": "This supplementary material provides additional details omitted in the main text. Appendix A: Mind2Web Training and Evaluation Details Appendix B: Trajectory Synthesis Cost Analysis Appendix C: Reasoning Generation Agent Appendix D: Prompt Details Appendix E: Trajectory Examples Mind2Web Training and Evaluation"
        },
        {
            "title": "Details",
            "content": "Table A.2 shows the hyperparameters and training time for experiments on Mind2Web-Live and Multimodal-Mind2Web. All experiments use Nvidia H100 GPUs. A.1 Mind2Web-Live We exclude the following websites - https: https://www.sixflags.com, //www.kbb.com, https://www.viator.com, https://www. menards.com, https://www.amctheatres.com, https://www.cargurus.com, https://www. gamestop.com, https://www.cabelas.com, https://www.rei.com due to denial of access faced during our tests. Table 6 shows the results on Mind2Web-Live for 83 out of 104 tasks across the remaining 37 websites. The results on the whole Mind2Web-Live evaluation set are given in Table A.1. The results in Table 6 are reported as the maximum over three runs, accounting for intermittent website access issues that may affect For Mind2Web-Live, evaluation consistency. the dataloader first samples training instances at the trajectory level and then randomly samples step from the trajectory to construct the final training instance. Thus, the number of epochs is calculated at the trajectory level. We use viewport resolution of 1280 720 during inference. The Mind2Web-Live dataset is released under the MIT license which permits its use for academic research. A.2 Multimodal-Mind2Web Following Deng et al. (2023), we obtain the top-50 elements from pre-trained DeBERTa (He et al., 2021) candidate generation model, which are then used to construct the accessibility tree and SOM image inputs. Following Ou et al. (2024), we always include the ground truth element in the input. We use viewport resolution of 1280 720 which includes the GT element during inference. We follow the setting in Zheng et al. (2024) and report element accuracy, operation F1, and step SR as evaluation metrics. All experiments on MultimodalMind2Web use single training and evaluation run. The dataloader uniformly samples training instances from the set of action steps across all trajectories. The Multimodal-Mind2Web dataset is released under the Responsible AI license which permits its use for academic research."
        },
        {
            "title": "B Trajectory Synthesis Cost Analysis",
            "content": "We use GPT-4o-turbo, which costs $2.5 per 1M tokens for our trajectory synthesis. Each proposal or refinement stage uses 3.6K textual tokens on average. Each input image costs $0.0028. The calculation assumes an average of 7.7 steps per trajectory, including the proposal stage. Table B.3 shows the breakdown for the different stages of trajectory generation. Total cost = $0.0128 7.7 + $0.02581 + $0.02381 = $0.148 The average cost per raw trajectory is $0.15. The success rate is estimated as 53.1%. Thus, the average cost per successful trajectory is estimated to be $0.28. Phase Cost per step Total cost Proposal Refinement Verification Summarization $0.0128 $0.0128 $0.02381 $0.02581 $0.0128 $0.0856 $0.02381 $0.02581 Table B.3: Cost breakdown for different modules in the pipeline."
        },
        {
            "title": "C Reasoning Generation Agent",
            "content": "Inspired by Xu et al. (2024c), the reasoning generation agent is pre-trained Qwen2-VL-7B model that takes as input the current action, high-level task description, screenshot, accessibility tree, and action history. It then outputs post-hoc reasoning trace for performing that action. These reasoning traces are helpful for training GUI agents in chainof-thought style. Model API-based Models GPT-4o GPT-3.5 Open-source Instructed Models Mistral-7B-Instruct-0.3 (Jiang et al., 2023) Qwen2-72B-Instruct (Bai et al., 2023) Qwen2-VL-7B (Wang et al., 2024a) Phi-3.5V (Abdin et al., 2024) Supervised Fine-Tuning Explorer-4B Explorer-7B Avg. Step SR (%) Completion Rate (%) Task SR (1) (%) Full Task SR (%) 56.4 33.0 37.9 27. 41.6 42.0 50.4 36.5 28.6 40.9 33.3 22.3 36.7 36.9 44.2 25.0 31.7 21. 30.8 32.7 22.1 15.4 11.5 15.4 12.5 1.9 16.4 16.4 Table A.1: Results on Mind2Web-Live benchmark. The results for GPT-4, GPT-3.5, and Mistral-7B have been reproduced on our Linux servers. The full task success rate (SR) represents the successful completion of all key nodes for given task. The average step success rate represents the proportion of completed key nodes, macroaveraged across tasks. The completion rate represents the proportion of completed key nodes, micro-averaged across tasks. Task SR (1) represents task SR with tolerance of up to one error/key node. Our Phi-3.5V model, finetuned on synthetic trajectory data from Explorer, outperforms much larger models, including Mistral-7B and Qwen2-72B-Instruct, by significant margin and is comparable to GPT-3.5. Dataset Model Train Data Hyperparamerters Train time (hours) M2W-Live M2W-Live batch_size:64, epoch:2, learning_rate:1 105 Qwen2-VL-7B batch_size:64, epoch:2, learning_rate:1 105 Qwen2-VL-7B Qwen2-VL-7B Syn. + M2W batch_size:64, epoch:2, learning_rate:1 105 Syn. M2W Phi-3.5V Phi-3.5V Phi-3.5V Syn. M2W batch_size:64, epoch:2, learning_rate:4 105 batch_size:64, epoch:2, learning_rate:1 105 Syn. + M2W batch_size:64, epoch:2, learning_rate:4 105 Multi.-M2W Qwen2-VL-7B Phi-3.5V Syn. Syn. batch_size:64, epoch:10, learning_rate:4 105 batch_size:64, epoch:10, learning_rate:4 105 15 1.5 15.5 12.5 1 12.5 17 Table A.2: Hyperparameters used in our experiments."
        },
        {
            "title": "D Prompt Details",
            "content": "The prompts for the task proposer agent, task refiner agent, task summarizer agent, and task verifier agent are given in Table D.5, Table D.7, Table D.8, and Table D.9, respectively. The training prompt for Explorer is given in Table D.10. System Role What does this webpage show? Imagine you are real user on this webpage. Given the webpage screenshot and parsed HTML/accessibility tree, please provide single task that user might perform on this page and the corresponding first action towards completing that task. Do the following step by step: 1. Generate single task that user might perform on this webpage. Be creative and come up with diverse tasks 2. Given the webpage screenshot and parsed HTML/accessibility tree, generate the first action towards completing that task (in natural language form). 3. Given the webpage screenshot, parsed HTML/accessibility tree, and the natural language action, generate the grounded version of that action. ACTION SPACE: Your action space is: [click [element ID], type [element ID] [content], select [element ID] [content of option to select], scroll [up], scroll [down], and stop]. Action output should follow the syntax as given below: click [element ID]: This action clicks on an element with specific ID on the webpage. type [element ID] [content]: Use this to type the content into the field with id. By default, the \"Enter\" key is pressed after typing. Both the content and the ID should be within square braces as per the syntax. select [element ID] [content of option to select]: Select an option from dropdown menu. The content of the option to select should be within square braces. When you get (select an option) tags from the accessibility tree, you need to select the serial number (element_id) corresponding to the select tag, not the option, and select the most likely content corresponding to the option as input. scroll [down]: Scroll the page down. scroll [up]: Scroll the page up. IMPORTANT: To be successful, it is important to STRICTLY follow the below rules: Action generation rules: 1. You should generate single atomic action at each step. 2. The action should be an atomic action from the given vocabulary - click, type, select, scroll (up or down), or stop. 3. The arguments to each action should be within square braces. For example, \"click [127]\", \"type [43] [content to type]\", \"scroll [up]\", \"scroll [down]\". 4. The natural language form of action (corresponding to the field \"action_in_natural_language\") should be consistent with the grounded version of the action (corresponding to the field \"grounded _action\"). Do NOT add any additional information in the grounded action. For example, if particular element ID is specified in the grounded action, description of that element must be present in the natural language action. selected, 5. tion_in_natural_language\") should always specify the actual text to be typed. 6. You should issue stop action if the current webpage asks to log in or for credit card information. 7. To input text, there is NO need to click the textbox first, directly type content. After typing, the system automatically hits the ENTER key. 8. STRICTLY Avoid repeating the same action (click/type) if the webpage remains unchanged. You may have selected the wrong web element. 9. Do NOT use quotation marks in the action generation. language form of action (\"acthe type action is the natural If Task proposal rules: 1. You should propose tasks that are relevant to the website and can be completed using the website. 2. You should only propose tasks that do not require login to execute the task. 3. You should propose tasks that are clear and specific. 4. For each task, provide concrete information or constraints, and use mock-up information (identifier, number, personal information, name, attributes, etc.) to make the task more specific and realistic. 5. The task description should provide all the necessary information to complete the task. 6. The task should be feasible to complete by real user and should not require any additional information that is not available on the website. The output should be in below format: Continued on next page Continued from previous page OUTPUT FORMAT: Please give HTML/accessibility tree, \"In summary, <TASK>:str, \"grounded_action\": <ACTION>:str}\"``` the then put your answer within ``` parsed screenshot, ```, for example, ```{\"task\": the proposed task and the corresponding action is: \"action_in_natural_language\":<ACTION_IN_NATURAL_LANGUAGE>:str, analysis of short User Role Website URL: {INIT_URL} Parsed HTML/Accessibility Tree: {A11Y_TREE} {SCREENSHOT} Table D.5: Prompt for Task Proposer Agent. System Role What does this webpage show? Imagine you are real user on this webpage, and your overall task is {OVERALL_TASK}. This is the list of actions you have performed that lead to the current page {PREV_ACTION_LIST}. You are also given the webpage screenshot and parsed HTML/accessibility tree. Do the following step by step: 1. Please predict what action the user might perform next that is consistent with the previous action list in natural language. 2. Then based on the parsed HTML/accessibility tree of the webpage and the natural language action, generate the grounded action. 3. Update the overall task aligned with this set of actions. ACTION SPACE: Your action space is: [click [element ID], type [element ID] [content], select [element ID] [content of option to select], scroll [up], scroll [down], and stop]. Action output should follow the syntax as given below: click [element ID]: This action clicks on an element with specific id on the webpage. type [element ID] [content]: Use this to type the content into the field with id. By default, the \"Enter\" key is pressed after typing. Both the content and the id should be within square braces as per the syntax. select [element ID] [content of option to select]: Select an option from dropdown menu. The content of the option to select should be within square braces. When you get (select an option) tags from the accessibility tree, you need to select the serial number (element_id) corresponding to the select tag, not the option, and select the most likely content corresponding to the option as input. scroll [down]: Scroll the page down. scroll [up]: Scroll the page up. IMPORTANT: To be successful, it is important to STRICTLY follow the below rules: Action generation rules: 1. You should generate single atomic action at each step. 2. The action should be an atomic action from the given vocabulary - click, type, select, scroll (up or down), or stop 3. The arguments to each action should be within square braces. For example, \"click [127]\", \"type [43] [content to type]\", \"scroll [up]\", \"scroll [down]\". 4. The natural language form of action (corresponding to the field \"action_in_natural_language\") should be consistent with the grounded version of the action (corresponding to the field \"grounded _action\"). Do NOT add any additional information in the grounded action. For example, if particular element ID is specified in the grounded action, description of that element must be present in the natural language action. selected, 5. tion_in_natural_language\") should always specify the actual text to be typed. 6. You should issue the stop action when the given list of input actions is sufficient for web task. 7. You should issue stop action if the current webpage asks to log in or for credit card information. 8. To input text, there is NO need to click the textbox first, directly type content. After typing, the system automatically hits the ENTER key. 9. STRICTLY Avoid repeating the same action (click/type) if the webpage remains unchanged. You may have selected the wrong web element. 10. Do NOT use quotation marks in the action generation. language form of action (\"acthe type action is the natural If Task proposal rules: 1. You should propose tasks that are relevant to the website and can be completed using the website itself. 2. The overall task should be well-aligned to the entire set of actions in history plus the current generated action. It should not be focused just on the current action. 3. You should only propose tasks that do not require login to execute the task. 4. You should propose tasks that are clear and specific. 5. For each task, provide concrete information or constraints, and use mock-up information (identifier, number, personal information, name, attributes, etc.) to make the task more specific and realistic. 6. The task description should provide all the necessary information to complete the task. 7. The task should be feasible to complete by real user and should not require any additional information that is not available on the website. The output should be in below format: Continued on next page Continued from previous page OUTPUT FORMAT: Please give short analysis of the screenshot, parsed HTMthen put your answer within ``` ```, L/accessibility tree, and history, for examthe proposed task and the corresponding action is: ```{\"task\": ple, \"In summary, <TASK>:str, \"action_in_natural_language\":<ACTION_IN_NATURAL_LANGUAGE>:str, \"grounded_action\": <ACTION>:str}\"``` User Role Website URL: {INIT_URL} Parsed HTML/Accessibility Tree: {A11Y_TREE} {SCREENSHOT} Table D.7: Prompt for Task Refiner Agent. System Role Given list of actions performed on the website {WEBSITE_URL} and the corresponding screenshots List of actions: {ACTION_LIST} Your task is to come up with single task description that will be accomplished by performing these actions in the given sequence on the website. IMPORTANT: 1. The task must contain some actions: Buy, Book, Find, Check, Choose, show me, search, browse, get, compare, view, give me, add to cart, ..., ideally involving transactions/finding information on specific product or service. 2. You should propose tasks that are clear and specific. 3. The task description should provide all the necessary information to complete the task. 4. The task description must indicate the domain of the website at the end of the task with the format: ... on task website, for instance, Purchase laptop on Amazon, Book hair appointment on Yelp, etc. 5. The task should be feasible to complete by real user and should not require any additional information that is not specified in this input. 6. The task description should specify constraints like given budget, product features, and other specifications that can narrow down the search to particular item/product. 7. Do NOT use any quotation marks (either single or double) in the task description. The output should be in the below format: OUTPUT FORMAT: Please first give some analysis of the actions and screenshots and then output the overall task description. put your answer within ``` ```, for example, In summary, the answer is: ```<TASK_DESCRIPTION>:str```. Table D.8: Prompt for Task Summarizer Agent. System Role You are an expert in evaluating the performance of web navigation agent. The agent is designed to help human user navigate website to complete task. Given the users intent, the agents action history, the final state of the webpage, and the agents response to the user, your goal is to decide whether the agents execution is successful or not. There are four types of tasks: 1. Transaction: The user wants to perform transaction on the webpage, such as booking ticket, ordering product, etc. The bot should at least initiate the add-to-cart or checkout process. It is still success if the bot has done actions of add to cart or checkout and encounters the login page. If the bot fails to do so, the task is considered failure. 2. Information seeking: The user wants to obtain certain information from the webpage, such as information of product, reviews, map info, comparison of map routes, etc. The bots response must contain the information the user wants, or explicitly state that the information is not available. Otherwise, e.g. the bot encounters an exception and responds with the error content, the task is considered failure. Besides, be careful about the sufficiency of the agents actions. For example, when asked to list the top-searched items in shop, the agent should order the items by the number of searches, and then return the top items. If the ordering action is missing, the task is likely to fail. 3. Site navigation: The user wants to navigate to specific page. Carefully examine the bots action history and the final state of the webpage to determine whether the bot successfully completes the task. No need to consider the bots response. 4. Content modification: The user wants to modify the content of webpage or configuration. Carefully examine the bots action history and the final state of the webpage to determine whether the bot successfully completes the task. No need to consider the bots response. IMPORTANT - If product has been added to the bag/cart in the action list but just the purchase is pending, it should be counted as success. - If you see the checkout page for the product you want to purchase, it should be counted as success. - Format your response into two lines as shown below: Thoughts: <your thoughts and reasoning process> Status: \"success\" or \"failure\" Table D.9: Prompt for Task Verifier Agent (adapted from Pan et al. (2024a)). System Role You are an expert at completing instructions on webpage screens. You will be presented with screenshot image with some numeric tags. If you decide to click somewhere, you should choose the numeric element index closest to the location you want to click. You should decide the action to continue this instruction. You will be given the accessibility tree of the current screen in the format: [element_idx] [role] [alt text or button name]. Here are the available actions: {\"action\": \"goto\", \"action_natural_language\": str, \"value\": <the URL to go to>} {\"action\": \"google_search\", \"action_natural_language\": str, \"value\": <search query for google>} {\"action\": \"click\", \"action_natural_language\": str, \"idx\": <element_idx>} {\"action\": \"value\": <the text to enter>} {\"action\": \"select\", \"action_natural_language\": str, \"idx\": <element_idx>, \"value\": <the option to select>} {\"action\": \"scroll [up]\", \"action_natural_language\": str} {\"action\": \"scroll [down]\", \"action_natural_language\": str} Your final answer must be in the above format. \"type\", \"action_natural_language\": <element_idx>, str, \"idx\": User Role The instruction is to {TASK DESCRIPTION}. History actions: {PREVIOUS ACTIONS} Here is the screen information: {ACCESSIBILITY TREE} Think about what you need to do with the current screen, and output the action in the required format in the end. Table D.10: Prompt for Web Agent Training."
        },
        {
            "title": "E Trajectory Examples",
            "content": "Figure E.1 shows sample trajectory executed on the IKEA website. Figure E.2 shows the set-ofmark annotations and accessibility tree inputs of the model during trajectory generation, model training, and inference. Figure E.1: Example synthetic trajectory from Explorer. Each step shows the set-of-mark annotated screenshot along with the grounded action taken by the GPT-4 agent. Figure E.2: Visualization of the model inputs during trajectory generation, model training, and inference. The example corresponds to step 2 of the trajectory in Figure 1."
        }
    ],
    "affiliations": [
        "Microsoft Research, Redmond",
        "The Ohio State University"
    ]
}