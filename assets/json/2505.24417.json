{
    "paper_title": "EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering",
    "authors": [
        "Runnan Lu",
        "Yuxuan Zhang",
        "Jailing Liu",
        "Haifa Wang",
        "Yiren Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 1 4 4 2 . 5 0 5 2 : r EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering Runnan Lu1 Yuxuan Zhang2 1National University of Singapore Jiaming Liu3 Haofan Wang4 Yiren Song1 2The Chinese University of Hong Kong 3Tiamat AI 4Liblib AI songyiren725@gmail.com Figure 1: Text-rendered results generated by EasyText, which supports text rendering in over ten languages and produces high-quality results. It can render text either with explicit positional control or in layout-free manner, and effectively handles curved and slanted regions. The displayed texts do not appear in the prompts; they are included solely to illustrate the intended rendering targets."
        },
        {
            "title": "Abstract",
            "content": "Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration. Project page: https://github.com/songyiren725/EasyText Corresponding author. Preprint. Under review. Table 1: Functionality evaluation of EasyText in comparison to other competitors Functionality Position Control Irregular Regions Multi-Lingual Long Text Rendering Text-Image Blending Unseen Characters Generalization Glyph-SDXL-v2 AnyText SD3.5 FLUX-dev Jimeng AI 2.1 EasyText"
        },
        {
            "title": "Introduction",
            "content": "Scene text rendering plays crucial role in various real-world applications. However, most existing methods such as TextDiffuser [7, 8], Diff-font [14] and modern commercial models like FlUXdev [20] and Ideogram [1], are primarily limited to English, making multilingual text rendering still challenging task. GlyphBy-T5-V2 [25] was the first to enable multilingual text rendering by introducing specially designed glyph encoder and employing multi-stage training strategy, achieving text rendering in 10 languages on large-scale dataset of millions of images. Inspired by how humans learn to write, we gain several key insights: (1) Imitative writing is significantly easier than recallinghumans typically begin by mimicking (i.e., writing with visual reference) before progressing to memory-based writing. (2) Once familiar with one language, humans naturally develop the ability to reproduce text in other unfamiliar languages even without understanding themtreating it more like drawing than writing. Motivated by this, we argue that training AI to imitate rather than recall is more efficient and effective strategy for text rendering. The task of scene text rendering faces several key challenges: (1) Multilingual character modeling is highly complexfor example, Chinese alone has over 30,000 characters. Extending to multilingual settings drastically expands the character space, making joint modeling harder, especially for rare or low-frequency characters with poor rendering accuracy. This is further complicated by language imbalance and font variability. (2) Text-background integration is often unnatural; existing methods struggle to blend rendered text with scene content, leading to visual artifacts such as disconnection or pasted-on effects that undermine image realism. (3) Preserving generative priors is difficult, as fine-tuning on large-scale text-image datasets improves rendering capabilities but often degrades the models general image generation ability. To this end, we present EasyText (shown in Fig. 1), novel multilingual text image generation framework based on Diffusion Transformers [30, 46]. We encode text into font tokens via VAE and concatenate them in the latent space with denoised latents. By leveraging the powerful in-context capabilities of DiT, EasyText achieves high-quality and accurate text rendering. Additionally, we propose simple yet effective position control strategy called Implicit Character Position Alignment, which allows for precise control of character positions through positional encoding interpolation and replacementenabling both position-aware rendering and layout-free generation. EasyText is also highly efficient in terms of data usage. Unlike GlyphBy-T5 V1/V2 [24, 25], which rely on contrastive synthetic data or massive real-world scene text datasets, our method simply overlays text randomly on natural images during the pretraining stage to learn glyph features. In this stage, to enable the model to learn glyph imitating rather than simple shape copying, we employ multi-font mapping approach. Multiple different fonts are overlaid in the synthetic training images, while the condition image uses only standard font. Afterwards, we fine-tune lightweight LoRA [16] on only 20K high-quality multilingual scene text images, enhancing the visual consistency between text and background. As shown in Table 1, we highlight some of the capabilities demonstrated by EasyText including: (a) Position control, precisely positioning text in specific locations within an image; (b) Irregular regions, enabling it to handle text rendering in irregular regions such as slanted or curved regions; (c) Multilingual text handling, supporting text generation in multiple languages such as Chinese, English, Japanese, etc; (d) Long text rendering, which can render extended text passages; (e) Text-image blending, which integrates text seamlessly into images, maintaining natural visual consistency; and (f) Unseen character generalization, allowing the model to exhibit generalization capability on unfamiliar and unseen characters. In summary, our contributions are: 2 1.We propose EasyText, framework that teaches AI to \"imitate\" rather than \"recall,\" achieving high-quality multilingual text rendering by harnessing the in-context learning power of Diffusion Transformers. 2.We introduce Implicit Character Position Alignment, which precisely controls text placement via lightweight position encoding operations and supports layout-free generation. 3.Extensive experiments demonstrate the effectiveness and simplicity of our method, showing superior performance on challenging scenarios such as long text, multi-text layouts, irregular regions, and unseen characters."
        },
        {
            "title": "2.3 Visual Text Generation\nText generation and rendering is a classic task, where early methods mainly relied on generative\nadversarial networks (GANs) [51, 6, 2, 42] and vector stroke techniques [43, 39, 40]. Recent work on\nvisual text generation with diffusion models primarily focuses on optimizing text encoding and spatial\ncontrol mechanisms to improve the fidelity and controllability of rendered text. Character-aware\nencoders are increasingly adopted: GlyphDraw [27], GlyphControl [52], and AnyText [45] embed\nglyph or OCR features into conditional inputs, while TextDiffuser-2 [8] leverages character-level\ntokenization to improve alignment. However, semantic bias and tokenizer limitations still persist.\nSpatial control has been addressed by introducing explicit layout-related conditions. TextDiffuser [7]\nand ControlText [18] use segmentation or layout masks, and UDiffText [60] and Brush Your Text [53]\nrefine attention to enforce region-level alignment. For multilingual rendering, works like AnyText [45],\nGlyphControl [52], and Glyph-ByT5-v2 [25] bypass tokenizers using glyph encoders or tokenizer-\nfree models. Yet, rendering quality remains limited by encoder–diffusion compatibility. Newer\nmodels (GlyphDraw2 [26], JoyType [21], AnyText2 [44], FonTS [34], RepText [48]) improve layout\nprecision, while closed models (Kolors 2.0 [19], Seedream 3.0 [12], GPT-4o [29]) show promising\nresults but often suffer from limited spatial controllability and suboptimal performance in multi-text\nlayouts. Despite progress, challenges remain in complex layouts and multilingual text, motivating\nfurther integration of glyph, position, and semantic signals.",
            "content": "3 Figure 2: Overview of EasyText. We adopt two-stage training strategy: large-scale pretraining for glyph generation and spatial mapping, followed by fine-tuning for visual-text integration and aesthetic refinement. Character positions from the condition input are aligned with target regions via implicit character position alignment, and training proceeds with image-conditioned LoRA."
        },
        {
            "title": "3.1 Preliminary: FLUX",
            "content": "FLUX is an advanced generative framework significantly enhancing generation quality. Departing from traditional U-Net-based diffusion models (such as Stable Diffusion), it adopts Transformer backbone (DiT) to improve long-sequence modeling and complex semantic understanding. The architecture employs pre-trained T5 [32] encoder to project textual inputs into dense semantic embeddings CT RM d. For visual processing, variational autoencoder (VAE) performs nonlinear dimensionality reduction on pixel space, generating compact latent representations that preserve essential visual semantics. Additionally, the framework processes encoded noise patches RN through rotary position embeddings (RoPE), maintaining spatial coherence during the iterative denoising process. These noise and text tokens are then fused by concatenation to form an imagetext token sequence and then fed in parallel into transformer-based denoising model. The model iteratively denoises and progressively restores high-quality image aligned with the textual prompt."
        },
        {
            "title": "3.2 Overall Architecture",
            "content": "The training pipeline adopts two-stage strategy. The first stage involves training on large-scale, synthetically generated dataset with balanced multilingual coverage, enabling the model to learn glyph generation across diverse scripts and accurate spatial mapping. In the second stage, we fine-tune the model on high-quality annotated dataset to improve both the aesthetic quality of rendered text and its integration with complex scene content. For conditioning, the input text image is encoded into the same latent space as the target image via VAE. Its positional encoding is aligned with the target region through implicit character position alignment, then concatenated with the target images position encoding before being passed to the FLUX DiT block. (Shown in Fig. 2) Image-conditioned LoRA adaptation is then applied, with both the VAE and text encoder kept frozen."
        },
        {
            "title": "3.3 Character Representation in EasyFont",
            "content": "In contrast to conventional font conditioning approaches that typically employ symbolic or parametric representations, our methodology innovatively adopts visually-grounded paradigm inspired by glyph morphology. The key differentiation of our character representation lies in its image-based conditioning architecture, where discrete image patches serve as fundamental conditioning units. This design philosophy stems from two critical observations: 4 First, we establish unified multilingual representation to addresses the intrinsic typographic differences between writing systems. For alphabetic scripts (e.g., English), we use 64-pixel-high images with widths adaptive to text length, naturally capturing the connected structure of alphabetic word formations. This horizontal layout preserves crucial inter-character relationships and spacing conventions unique to Western typography. Second, for logographic systems (Chinese, Japanese, etc.), we allocate individual 6464 square images per character. This configuration respects the isolated nature of ideographic characters while maintaining consistent resolution across all glyphs. The square aspect ratio optimally captures the balanced structure inherent to CJK characters, where stroke complexity is uniformly distributed within square design space. The image-based character representations are encoded via VAE encoder into latent features. Subsequently, the latent representations undergo patchification through linear projection layer, which reshapes and projects the spatial features into sequence of conditioning tokens."
        },
        {
            "title": "3.4\nTo achieve precise spatial control in font generation, we propose an Implicit Character Position\nAlignment (ICPA) mechanism through positional encoding extrapolation. Given a conditional image\npatch Pc ∈ R64×Wc×3 containing source typography features, Wc denotes the total width of the\nconditional image patch, and a target rendering region Ωt = [x1 : x2, y1 : y2] in the output image, our\nmethod establishes position-aware feature correspondence through positional encoding extrapolation.",
            "content": "Linear Alignment via Affine Transform. Linearly interpolate the position in the condition image (of size 64 Wc) to the position in the target bounding box Ωt = [x1 : x2, y1 : y2]. Assuming [0, Wc 1] and [0, 63] (i.e. the condition image spans indices 0 to Wc horizontally and 0 to 63 vertically), the affine mapping Taff : (u, v) (cid:55) (x, y) is defined as: 63 Taff(u, v) = (x2 x1), (y2 y1) Wc 1 x1 + y1 + (1) (cid:16) (cid:17) , which maps the normalized horizontal coordinate u/(Wc 1) to point between x1 and x2, and similarly v/63 to point between y1 and y2, achieving linear scaling and translation of the patch coordinates into the target domain. The positional encoding is then re-aligned by applying the affine transform to each coordinate in condition image (u, v). Specifically, the transformed positional encoding at the target location PEt(x, y) = Taff(u, v) is assigned from the location in condition image. Thus, the new positional encoding for points in the condition image is given by: PEc(u, v) = PEt(x, y), (2) where the target position (x, y) is computed via the affine mapping from the patch coordinates. Nonlinear Alignment via Thin-Plate Spline Interpolation. For nonlinear spatial alignment, we use Thin-Plate Spline (TPS) interpolation. TPS warping defines smooth map that exactly interpolates set of control point correspondences between the source patch and target region, while minimizing bending (second-derivative) distortion. Let {(ui, vi)}K i=1 be landmark points in the source patch Pc and {(xi, yi)}K i=1 their corresponding positions in the target region Ωt. The TPS mapping TTPS : (u, v) (cid:55) (x, y) can be expressed as an affine base plus radial basis deformation term: TTPS(u, v) = (cid:33) (cid:32)u 1 + (cid:88) i=1 (cid:16)(cid:112)(u ui)2 + (v vi)2 (cid:17) wi ϕ , ϕ(r) = r2 log r, (3) where R23 represents the affine part of the transformation and wi R2 are the TPS warp coefficients associated with each control point. The kernel ϕ(r) = r2 ln is the fundamental solution of the biharmonic equation in 2D. The parameters and wi are determined by solving the interpolation constraints TTPS(ui, vi) = (xi, yi), = 1, . . . , K, (4) together with additional conditions to ensure well-posed solution ((cid:80)K i=1 wiui = 0, (cid:80)K i=1 wivi = 0). This yields smooth, non-linear warp TTPS that maps every source coordinate (u, v) to target coordinate (x, y), honoring the given correspondences. Similar to linear alignment, target location is obtained via PEt(x, y) = TTPS(u, v). Accordingly, the corresponding positional encoding PEc(u, v) can be computed as in Equation 2. i=1 wi = 0, (cid:80)K 5 Layout-Free Position Alignment via Positional Offset Injection. To enable flexible layout-free rendering, we also introduce an effective positional offset strategy. Instead of aligning coordinates, we shift the positional encoding of the conditional patch by fixed scalar offset wt, which represents the width of the target image. Let (i, j) index the spatial position in Pc. Target coordinates (x, y) are obtained by applying positional offset to the original coordinates: (cid:26) = + wt = (5) This ensures positional encoding uniqueness without binding the conditional patch to any specific target location, enabling more flexible text rendering."
        },
        {
            "title": "3.5 EasyText Dataset Construction\nWe construct two datasets tailored to the specific needs of the pretraining and fine-tuning stages.",
            "content": "Large-Scale Synthetic Dataset. This dataset is designed to provide broad coverage across scripts and languages. Target images are generated by rendering multilingual text onto background images using script-based synthesis. The dataset includes scripts such as Latin, Chinese, Korean, and others, resulting in nearly 1 million samples. To reduce redundancy and improve efficiency, languages sharing the same writing system are grouped by scriptfor example, English, Italian, and German are treated as single Latin script class. The rendered text is composed of random combinations of characters from the corresponding script, covering its full character set. To encourage the model to learn generalizable glyph representations rather than simply copying shapes from the condition image, the target text is rendered using diverse fonts, while the condition image is rendered using standardized font. This font decoupling enables robust learning of character structure across styles. Dataset statistics are summarized in Table 2. High-Quality Human-Annotated Dataset. The second dataset comprises 20k high-quality imagetext box pairs, primarily in Chinese and English. Text regions are extracted using PP-OCR [10] and filtered to ensure annotation quality. In these examples, text is more naturally integrated into scene content, often with stylized typography and visually consistent stroke patterns. The rendered text is not included explicitly in the prompt, but instead triggered by placeholder tokens (e.g., <sks1>, <sks2>), supporting semantic alignment between text and background. This dataset facilitates better visual-text fusion and improves typographic aesthetics. This two-stage training strategy reduces the need for large-scale text-image datasets, requiring only small fine-tuning setparticularly well-suited in multilingual settings where annotated data is scarce."
        },
        {
            "title": "4.1 Experimental Settings\nImplementation Details. Our method is implemented based on the open-source FLUX-dev frame-\nwork with LoRA-based parameter-efficient tuning. In the first stage, we set the LoRA rank to 128 and\ntrain on 4 H20 GPUs. The model is first trained at 512×512 resolution for 48,000 steps with a batch\nsize of 24, and then at 1024×1024 for 12,000 steps. In the second stage, we reduce the LoRA rank\nto 32 and fine-tune at 1024 resolution for 10,000 steps with a batch size of 8. We use the AdamW\noptimizer with a learning rate of 1e-4.",
            "content": "Evaluation Metric. To evaluate the accuracy of multilingual text generation, we adopt two complementary metrics: (i) Character-level precision which measures the correctness of generated characters relative to ground-truth texts. This reflects the models ability to produce accurate predictions at the most basic unit level for each language. (ii) Sentence-level precision, which assesses whether the entire text line content within each controlled text box is rendered completely correctly. This captures the overall consistency and correctness at the region level. Multilingual Benchmark. We construct multilingual benchmark with 90 language-agnostic prompts across 10 languages. For each language, prompts are paired with language-specific text to generate condition images while preserving semantic intent. Four images are generated per promptlanguage pair. For logographic scripts (Chinese, Japanese, Korean), the average number of characters per text box is 7.1, and for alphabetic scripts, it is 14.3. Each image contains an average of 1.7 text boxes. To evaluate the generated results, three volunteers were tasked with assessing the accuracy of the text in the images, ensuring reliable multilingual generation quality. 6 Table 2: Statistics of multilingual data used in large-scale pre-training and fine-tuning. Script Type Large-Scale Pre-Training Unique Chars Font types Samples Table 3: Character/Sentence level precision for English and Chinese across different methods. English Chinese Chars Pre Sen Pre Chars Pre Sen Pre Fine-Tuning Samples Method Chinese Korean Japanese Thai Vietnamese Latin Greek 7000 4308 2922 2128 91 104 79 18 13 17 3 19 30 19 230K 100K 100K 100K 100K 180K 100K 5.5K 0.1K 0.1K 16K EasyText(pretrain) EasyText(fine-tune) Glyph-SDXL-v2 FLUX Stable Diffusion 3.5 Jimeng 2.1 ANYTEXT 0.9968 0.9945 0.9959 0.9180 0.9556 0.9812 0.8978 0.9813 0.9625 0.9688 0.7062 0.7938 0.8687 0.6364 0.9344 0.9312 0.9258 0.9214 0.8824 0.6562 0.6438 0.6250 0.6813 0.6071 Table 4: Multilingual text generation precision with our unified and generalized model, compared to Glyph-SDXL-v2. Language EasyText (pre) EasyText (fine) Glyph-ByT5-v2 Chars Chars Chars Sent Sent Sent 0.9867 0.8552 0.9673 0.7500 0.9812 0.8625 French 0.9818 0.8333 0.9732 0.7619 0.9828 0.7976 German 0.9341 0.7125 0.9203 0.6713 0.9441 0.7437 Korean 0.9265 0.7179 0.9194 0.6562 0.9059 0.6187 Japanese 0.9740 0.9125 0.9638 0.8571 0.9385 0.8333 Italian 0.9628 0.7443 0.9334 0.6500 Thai Vietnamese 0.9605 0.7312 0.9401 0.6474 0.9702 0.7685 0.9360 0.6875 Greek Table 5: Quantitative evaluation of text-to-image rendering across diverse metrics and GPT-4obased assessments.Ours (pre) and Ours (ft) denote the pretrained and finetuned versions of our model, respectively. Metric Ours(pre) Ours(ft) Glyph-v2 SD3.5 FLUX CLIPScore OCR Acc.(%) 0.3318 84.32 0.3486 88.72 0.3140 82.33 0.3519 0.3348 76.09 79.33 Evaluation by GPT-4o Image Aesthetics Text Aesthetics Text Quality Text-Image Fusing 81.58 65.14 84.58 74.48 83.86 73.79 90.66 80.28 75.86 65.28 86.25 74.80 84.58 72.06 85.79 81.12 83.61 71.90 86.95 78."
        },
        {
            "title": "4.2 Comparision results",
            "content": "Multi-lingual Precision Evaluation. We evaluate our model on multilingual benchmark against state-of-the-art commercial models (e.g., FLUX [20], Jimeng AI [5], SD3.5 [11]) and other methods (e.g., Glyph-ByT5-v2 [25], AnyText [45]), following the benchmark and metrics outlined in Sec. 4.1. For languages with limited support in prior rendering methods, we compare primarily with Glyph-SDXL-v2. All baseline models are evaluated using their official inference settings. For each sample, four images are generated with identical image descriptions and target texts across all methods. For models lacking conditional input support, the target text is included in the prompt; Evaluation focuses solely on text accuracy, regardless of spatial layout or surrounding content. Comprehensive Quality Assessment. Beyond precision evaluation, we assess generation quality using CLIPScore and OCR Accuracy for objective comparison against existing baselines. We further incorporate GPT-4o assessment and user study across four subjective criteria: Image Aesthetics, Text Aesthetics, Text Quality, and Text-Image Fusing, to evaluate overall fidelity and alignment. The results show that our model surpasses competing methods in text rendering precision across several languages, including English and Italian  (Table 4)  and achieves strong character-level accuracy in Chinese, though slightly behind Jimeng AI at the sentence level. It also performs well in unsupported languages like Thai and Greek. In Table 5, it also leads in OCR accuracy and improves over FLUX in CLIPScore, indicating higher visual-text alignment. These trends are further supported by GPT-4o evaluation results. After pretraining, the model demonstrates strong text rendering performance from condition images. However, it shows limited text-image coherence, as indicated by lower CLIPScore and GPT-4o evaluations. Fine-tuning alleviates this issue, significantly improving CLIPScore, Text Aesthetics, and overall visual-text alignment. Although rendering accuracy slightly declines due to flexible positioning and artistic font usage, the trade-off is acceptable given the qualitative gains. Notably, while most competing methods use datasets exceeding 1M samples, our fine-tuning relies on only 20k annotated examples. Moreover, the large-scale pretraining set can be replaced with synthetic data, reducing dependence on large, annotated datasets."
        },
        {
            "title": "4.3 Qualitative Results\nWe provide qualitative comparisons of multilingual text rendering, including Chinese, English, and\nother languages. Results are shown in Fig. 4. Compared to previous region-controlled text rendering\nmethods, our approach demonstrates significant improvements in visual quality and text fidelity, with\nbetter visual-text integration, higher OCR accuracy, and enhanced aesthetic coherence (Shown in Fig.",
            "content": "7 Figure 3: Qualitative comparison of EasyText with other methods, focusing on the generation quality of both text and images, reveals that EasyText demonstrates outstanding performance. Figure 4: Qualitative comparison of EasyText across multiple languages of the same prompt. 3). Additionally, unlike commercial text-to-image models, our method allows for more precise spatial control over rendered text, particularly in generating multiple paragraphs of relatively long text (e.g., 45 paragraphs with around 20 characters each) while maintaining consistent layouts. We further evaluate the models ability to handle challenging text generation scenarios that are typically difficult for existing methods. Specifically, our approach generalizes well to unseen characters, slanted or non-rectilinear text regions, and long text sequences, while maintaining structural consistency and legibility. Detailed qualitative results are provided in the appendix."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "To validate the effectiveness of using diverse fonts in synthetic pretraining, we conducted controlled experiment where both the target and condition images were rendered using the same standard font. While this setup yields high initial precision, it mainly captures direct shape mapping and fails to generalize after fine-tuning on real-world data(Shown in Fig. 5). This highlights the importance of font diversityrendering targets with multiple fonts while keeping the condition font fixedfor learning robust, transferable representations. we compare our full approachusing both the condition input and implicit character position alignment-with three ablated variants: (1) removing position alignment, (2) removing the condition input, and (3) using the original FLUX model. As shown in Fig. 6, incorporating position mapping substantially improves spatial precision and text accuracy, particularly in multi-text scenarios. In comparison, when removing the condition input, the target render text is instead provided in the prompt(in our full method, renter text content is given via the 8 Figure 5: Comparison of text rendering on the pretraining synthetic dataset using multiple font overlays versus single font overlays. The red boxes highlight the erroneous regions. Figure 6: Ablation study comparing the full conditioned method with: (1) layout-free (2) without condition inputs, and (3) original FLUX. The red boxes highlight the erroneous regions. condition input). This demonstrates that our fine-tuned model effectively retains the strong generation capability of the base FLUX model."
        },
        {
            "title": "5 Limitation",
            "content": "The Implicit Character Position Alignment mechanism is less effective when character positions are substantially overlapping, occasionally leading to reduced rendering accuracy. In addition, training across multiple scripts results in confusions between simple but visually similar characters from different writing systems, such as rendering the Vietnamese character \"ệ\" as \"e\". These cases are infrequent but consistently observed."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose EasyText, diffusion-based framework for multilingual and controllable text generation in text-to-image synthesis. The method learns to mimic font rendering by mapping glyph features into the same latent space as the image via VAE, and achieves Implicit Character Position Alignment by combining position encoding with spatial targets, enabling precise and flexible text placement. Benefiting from two-stage training strategypretraining on large-scale synthetic multi-font dataset followed by fine-tuningEasyText reduces the need for real multilingual data It also demonstrates strong visual-text integration, while maintaining high rendering accuracy. 9 effectively embedding text into complex scenes. Extensive experiments confirm the effectiveness and generalization ability of our approach. References [1] Ideogram: v3.0. https://ideogram.ai/, 2025. [2] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, and Trevor Darrell. Multi-content gan for few-shot font style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 75647573, 2018. [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. [5] ByteDance. Jimeng ai 2.1. https://jimeng.jianying.com/. [6] Junbum Cha, Sanghyuk Chun, Gayoung Lee, Bado Lee, Seonghyeon Kim, and Hwalsuk Lee. Few-shot compositional font generation with dual memory. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIX 16, pages 735751. Springer, 2020. [7] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36:93539387, 2023. [8] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, pages 386402. Springer, 2024. [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [10] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. Pp-ocr: practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020. [11] Patrick Esser, Kulal, Andreas Blattmann, and ... Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning (ICML), 2024. [12] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [13] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: arXiv preprint Leveraging adaptive position embeddings for versatile virtual clothing tasks. arXiv:2501.15891, 2025. [14] Haibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, and Qiao Yu. Diff-font: Diffusion model for robust one-shot font generation. International Journal of Computer Vision, 132(11):5372 5386, 2024. [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [17] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. [18] Bowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao, Lyle Ungar, and Camillo Taylor. Controltext: Unlocking controllable fonts in multilingual text rendering without font annotations. arXiv preprint arXiv:2502.10999, 2025. 10 [19] Kolors Team. Kolors 2.0. https://github.com/Kwai-Kolors/Kolors. [20] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [21] Chao Li, Chen Jiang, Xiaolong Liu, Jun Zhao, and Guoxin Wang. Joytype: robust design for multilingual visual text creation. arXiv preprint arXiv:2409.17524, 2024. [22] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback: Project page: liming-ai. github. io/controlnet_plus_plus. In European Conference on Computer Vision, pages 129147. Springer, 2024. [23] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. [24] Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, pages 361377. Springer, 2024. [25] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Lin Liang, Lijuan Wang, Ji Li, and Yuhui Yuan. Glyph-byt5-v2: strong aesthetic baseline for accurate multilingual visual text rendering. arXiv preprint arXiv:2406.10208, 2024. [26] Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, and Zhenyu Yang. Glyphdraw2: Automatic generation of complex glyph posters with diffusion models and large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 59555963, 2025. [27] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation. arXiv preprint arXiv:2303.17870, 2023. [28] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 42964304, 2024. [29] OpenAI. GPT-4o. https://chatgpt.com/. [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [34] Wenda Shi, Yiren Song, Dengming Zhang, Jiaming Liu, and Xingxing Zou. Fonts: Text rendering with typography and style controls. arXiv preprint arXiv:2412.00136, 2024. [35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [36] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. [37] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. [38] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanything: Harnessing diffusion transformers for multi-domain procedural sequence generation. arXiv preprint arXiv:2502.01572, 2025. [39] Yiren Song, Xuning Shao, Kang Chen, Weidong Zhang, Zhongliang Jing, and Minzhe Li. Clipvg: Textguided image manipulation using differentiable vector graphics. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 23122320, 2023. [40] Yiren Song and Yuxuan Zhang. Clipfont: Text guided vector wordart generation. In BMVC, page 543, 2022. [41] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [42] Licheng Tang, Yiyang Cai, Jiaming Liu, Zhibin Hong, Mingming Gong, Minhu Fan, Junyu Han, Jingtuo Liu, Errui Ding, and Jingdong Wang. Few-shot font generation by learning fine-grained local styles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 78957904, 2022. [43] Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michaël Gharbi, Oliver Wang, Alec Jacobson, and Evangelos Kalogerakis. Vecfusion: Vector font generation with diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79437952, 2024. [44] Yuxiang Tuo, Yifeng Geng, and Liefeng Bo. Anytext2: Visual text generation and editing with customizable attributes. arXiv preprint arXiv:2411.15245, 2024. [45] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [47] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong. Grid: Visual layout generation. arXiv preprint arXiv:2412.10718, 2024. [48] Haofan Wang, Yujia Xu, Yimeng Li, Junchen Li, Chaowei Zhang, Jing Wang, Kejia Yang, and Zhibo Chen. Reptext: Rendering visual text via replicating. arXiv preprint arXiv:2504.19724, 2025. [49] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [50] Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. Stablegarment: Garment-centric generation via stable diffusion. arXiv preprint arXiv:2403.10783, 2024. [51] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild. In Proceedings of the 27th ACM international conference on multimedia, pages 15001508, 2019. [52] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: glyph conditional control for visual text generation. Advances in Neural Information Processing Systems, 36:4405044066, 2023. [53] Lingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, and Yu Qiao. Brush your text: Synthesize any scene text on images via diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 72157223, 2024. [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [55] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8069 8078, 2024. [56] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. Fast personalized text to image synthesis with attention injection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61956199. IEEE, 2024. [57] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stable-makeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024. 12 [58] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [59] Yuxuan Zhang, Qing Zhang, Yiren Song, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. arXiv preprint arXiv:2407.14078, 2024. [60] Yiming Zhao and Zhouhui Lian. Udifftext: unified framework for high-quality text synthesis in arbitrary images via character-aware diffusion models. arXiv preprint arXiv:2312.04884, 2023."
        },
        {
            "title": "Appendix",
            "content": "This supplementary material is organized into three sections. Section describes the training datasets used for pretraining and fine-tuning, including illustrative examples. Section presents diverse and comprehensive generated examples. Section outlines the process and sample cases of the GPT-based evaluation and user study."
        },
        {
            "title": "A Additional Dataset Details",
            "content": "The main paper provides detailed description of the datasets design and composition. This section complements that discussion by offering additional details and presenting example instances from the dataset to further illustrate its structure. Figure 7: Sample images from the synthetic dataset used in the pretraining. A.1 Synthetic Dataset for Large-Scale Pretraining For the training dataset used in pretraining, we collected background images from several publicly available datasets without textual content, such as LAION-Aesthetics. For each script (i.e., languagespecific character set), we rendered text in various fonts onto these backgrounds. During the rendering process, we recorded both the text content and the corresponding bounding box coordinates, producing synthetic imagetext pairs designed for text rendering tasks. This approach enabled the creation of large-scale multilingual dataset of approximately 1 million samples, offering practical alternative 13 for low-resource languages, where high-quality annotated imagetext pairs are often unavailable. Some sample instances are shown in Fig. 7. A.2 High-Quality Dataset for Fine-Tuning For the fine-tuning stage, we collected around 20K images containing text from copyright-free websites. Text regions were detected and filtered using PP-OCR, and each image was further annotated with caption generated by GPT-4 Mini. Some sample instances are shown in Fig. 8. Figure 8: Sample images from the high-quality dataset used in the fine-tuning."
        },
        {
            "title": "B Additional Generation Results",
            "content": "We present additional diverse generation results produced by EasyText  (Fig. 9)  . Fig. 10 and Fig. 11 further illustrate the models ability to generate text in multiple languages, all achieved using single, general-purpose model. Additional figures also highlight other powerful capabilities of EasyText  (Fig. 12)  , including layout-free text generation, adaptation to curved or slanted regions, and strong generalization to unseen characters and languagesboth of which were never encountered during training."
        },
        {
            "title": "C Additional Evaluation Details",
            "content": "We selected several metrics to assess the quality of text rendering results, including Image Aesthetics, Text Aesthetics, Text Quality, and Text-Image Fusion. These metrics were evaluated through two distinct evaluation methods: GPT-based automatic evaluation and user study. 14 Figure 9: Additional text rendering results generated by EasyText Table 6: User study results (%): Comparison of different methods based on user preferences across four evaluation metrics. The percentages represent the average proportion of times each method was selected as the best-performing one across multiple groups of samples."
        },
        {
            "title": "Metrics",
            "content": "EasyText (pretrain) EasyText (fine-tune) Glyph-v2 SD3.5 FLUX AnyText Image Aesthetics Font Aesthetics Text Quality Text-Image Integration 15.05 15.91 11.65 15.15 25.01 31.06 22.97 34.09 16.67 9.85 14.39 14.62 7.58 21.31 15.15 13.64 21.10 13.96 11.36 12. 14.39 14.39 15.91 12.14 C.1 GPT-Based Evaluation The evaluation procedure for an individual image using GPT is illustrated in Fig. 13. The figure shows an example of the dialogue-based evaluation process for clarity; in practice, the evaluations were conducted by directly querying the GPT-4o API, which automatically returned the scores. Evaluations were performed at scale, and the final scores for each metric were computed as the average across all samples. C.2 User Study Design The user study followed questionnaire-based format, as detailed in Fig. 14, which illustrates an example group of comparison images presented in the questionnaire. Approximately 50 questionnaires were distributed. Each questionnaire compared outputs from different models across ten groups of generated results.The results of the user study are shown in Table 6. Figure 10: Results of EasyText across multiple languages, showing the first group of languages. 16 Figure 11: Results of EasyText across multiple languages, showing the second group of languages. 17 Figure 12: Additional results generated by EasyText, showcasing unique capabilities beyond existing methods, such as handling unseen characters and languages. Figure 13: Example dialogue illustrating the evaluation process using GPT-4o. 19 Figure 14: An example group of comparison images from the user study questionnaire."
        }
    ],
    "affiliations": [
        "Liblib AI",
        "National University of Singapore",
        "The Chinese University of Hong Kong",
        "Tiamat AI"
    ]
}