{
    "paper_title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "authors": [
        "Dacheng Li",
        "Shiyi Cao",
        "Tyler Griggs",
        "Shu Liu",
        "Xiangxi Mo",
        "Shishir G. Patil",
        "Matei Zaharia",
        "Joseph E. Gonzalez",
        "Ion Stoica"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought."
        },
        {
            "title": "Start",
            "content": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters! Dacheng Li * 1 Shiyi Cao * 1 Tyler Griggs * 1 Shu Liu * 1 Xiangxi Mo 1 Shishir G. Patil 1 Matei Zaharia 1 Joseph E. Gonzalez 1 Ion Stoica 1 5 2 0 2 1 1 ] . [ 1 4 7 3 7 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-ofthoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1preview models score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. *Equal contribution 1Department of Electrical Engineering and Computer Sciences, University of California, Berkeley. Correspondence to: Ion Stoica <istoica@berkeley.edu>. 1 Codes are available at https://github. com/NovaSky-AI/SkyThought. 1. Introduction Large reasoning models (LRMs) leverage long chain-ofthoughts (Long CoTs) with reflection, backtracking, and self-validation to tackle challenging reasoning tasks (Jaech et al., 2024; Guo et al., 2025; Team, 2024). However, the process of eliciting Long CoTs from available LLMs remains unclear, as existing methods are either closedsourced (Jaech et al., 2024; Team, 2024) or expensive to replicate (Guo et al., 2025). In this paper, we first show that, surprisingly, an LLM can be cheaply and easily taught to produce Long CoT responses, significantly improving its reasoning capabilities. In particular, we find that this learning process can be both dataefficient and parameter-efficient. By performing fully supervised fine-tuning (SFT) with only 17k samples generated by DeepSeek R1, the Qwen2.5-32B-Instruct model achieves performance competitive with OpenAI o1-preview across wide range of math and coding tasks (Team, 2024; Yang et al., 2024; Jaech et al., 2024). In particular, it achieves 90. 8% in Math-500 (+6.0%), 56.7% in AIME 2024 (+40.0%), 85.0% in AMC 2023 (+17.5%), 60.3% in OlympiadBench (+12.7%) and 57.0% in LiveCodeBench (+8.1%) (Jain et al., 2024). Even further, the model can achieve o1-preview performance by updating fewer than 5% parameters with LoRA fine-tuning (Hu et al., 2021). We show that the model successfully learns to reflect and revise its intermediate thoughts (e.g., frequently using reasoning keywords such as Alternatively and Wait, but) and adopts long, coherent CoTs to tackle challenging problems  (Fig. 1)  . Moreover, we identify the Long CoT structure as the key characteristic of distilled data for eliciting strong reasoning performance rather than the specific contents of individual reasoning steps within the Long CoT. To test this, we conduct two sets of controlled studies by altering either the content of individual reasoning steps or the overall logical structure. To alter content, we perturb samples by replacing numbers with random digits or deleting reasoning keywords. Submission and Formatting Instructions for ICML 2025 (a) Responses of the base model, with Long CoT SFT, and with Long CoT LoRA. (b) Performance of different models on five difference reasoning benchmarks. Figure 1: Learning to reason is dataand parameter-efficient. When fine-tuned on small amount (17k) of Long CoT samples distilled and reject-sampled from DeepSeek-R1 with either LoRA or full-parameter tuning, the model easily learns to perform reflection and backtracking by using keywords such as However and Alternatively (Top). Consequently, the fine-tuned models improve significantly across five popular math and coding benchmarks (Bottom). For fine-tuning, the base model is Qwen2.5-32B-Instruct. Surprisingly, we find that these perturbations have little impact on the model performance: even when 50% of numbers in training samples are randomly changed, the model only observes 3.3% lower accuracy on the most challenging math benchmark, AIME 2024, as compared to training with correct samples. To alter the global reasoning structure, we separate responses into reasoning steps and randomly shuffle, insert, or delete these steps. We observe that the trained model is much more sensitive to structural perturbations that break logical coherency in the long CoT. For example, when 67% of the training samples reasoning steps are shuffled, accuracy drops by 13.3% on AIME 2024 problems relative to training with correct samples. In summary, our key contributions are: 1. We demonstrate that an LLM can learn Long CoT reasoning in data-efficient and parameter-efficient manner (i.e., LoRA). With fewer than 17k samples, we fine-tune the Qwen2.5-32B-Instruct model to be competitive with o1-preview. 2. We identify the structure of Long CoT as critical to the learning process rather than the content of individual reasoning steps. To validate this finding, we perform two groups of controlled experiments that modify either the structure or contents of samples. 3. We conduct comprehensive ablations across model sizes and architectures, dataset sizes, data generation models (DeepSeek R1 and QwQ-32B-Preview), and on five popular math and coding reasoning benchmarks. 2 2. Related work 3. Simple distillation is effective Submission and Formatting Instructions for ICML 2025 Test Time Scaling for Large Language Models Scaling test-time compute has proven effective in enhancing the the reasoning capabilities of LLMs. This can be broadly categorized into two directions: single long CoT and repeatedly sampled CoT. The former trains models, such as OpenAI o1, DeepSeek R1, and Qwen QwQ, to generate individual, long CoT responses with in-context reflection and backtracking to handle complex reasoning tasks (Guo et al., 2025; Jaech et al., 2024; Team, 2024). Alternatively, repeated sampling methods, such as Best-of-N or search-guided generation (e.g., MCTS), improve reasoning performance by invoking multiple responses from the model, sometimes guided by search algorithms and reward models (Snell et al., 2024; Brown et al., 2024). In this paper, we focus on distilling the ability to generate individual, Long CoTs, and show it can be done in dataand parameter-efficient manner. Training to improve reasoning capabilities of LLMs LLM reasoning capabilities can be improved by approaches such as iterative self-improvement and reinforcement learning (RL) (Zelikman et al., 2022; Lightman et al., 2023; Lambert et al., 2024; Yuan et al., 2024; Guo et al., 2025). More recently, Tulu-3 introduces Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance in tasks such as math and coding (Hendrycks et al., 2021c; Jain et al., 2024; LI et al., 2024). PRIME proposes RL-based method without process labels (Yuan et al., 2024). The recent release of DeepSeek R1 (Guo et al., 2025) demonstrates that LLMs can learn to produce long CoT and improve reasoning using pure RL-based approach. Instead of bootstrapping reasoning ability, this paper focuses on the surprising dataand parameter-efficiency of distilling reasoning abilities from an existing reasoning model to an LLM. Distillation Distilling the outputs or logits generated by larger or more capable model has become standard technique to enhance model performance (Hinton, 2015). Typically, responses generated by higher-quality models are used to perform supervised fine-tuning on smaller models (Lambert et al., 2024). The Vicuna model, for instance, demonstrates that ChatGPT-generated responses can be used to effectively and cheaply distill high-quality chatting capabilities (Zheng et al., 2023). In this paper, we show that reasoning capabilities can also be cheaply distilled. We note that concurrent work has also observed similar trends in distilling reasoning capability (Min et al., 2024; Huang et al., 2024). Our paper differs from these recent works by demonstrating that reasoning distillation can be achieved efficiently with minimal parameter updates. We also provide an in-depth analysis of the key factors driving reasoning improvements, including the roles of the reasoning structure and content, as well as comprehensive evaluations and ablations across different data sizes and teacher models. In this section, we present our distillation process and show that small amount of well-curated data, along with simple parameter-efficient fine-tuning method (e.g., LoRA), can effectively improve reasoning capabilities in large language model. 3.1. Experiments Setup Distillation data curation. We use DeepSeek-R1 (Guo et al., 2025) and QwQ-32B-Preview (Team, 2024), two open-source models with reasoning capabilities, to generate our distillation data. We select difficult prompts from the AMC/AIME 1, Math, and Olympiad subset from the Numina-Math dataset (LI et al., 2024), as Min et al. (2024) implies that hard problems can improve performance. We also incorporate coding problems from APPS (Hendrycks et al., 2021a) and TACO (Li et al., 2023) datasets. Specifically, we use GPT-4o-mini to classify the difficulty of the prompts according to the AoPS standard (Achiam et al., 2023), and select math problems of difficulty higher than Leval 3, Olympiad higher than Level 8, and all AIME/AMC problems. We verify the correctness of the traces by checking against ground truth solutions using exact matching for math problems and code execution for coding problems. In total, we curated 12k math and 5k coding problems with correct responses from QwQ to serve as our training data. For R1 samples, we directly use the public R1-17k reasoning dataset2 that is curated following similar procedure. Training details. We perform training using LlamaFactory (Zheng et al., 2024). We train the Qwen2.5-32BInstruct using batch size of 96, learning rate 1e-5 with warm-up ratio of 0.1 and linear learning rate decay (Yang et al., 2024), following similar hyperparameters in (Min et al., 2024). We use the next token prediction loss as the training objective (Radford, 2018). We use the same hyperparameters except 1e-4 learning rate for LoRA fine-tuning. Evaluation setup. We evaluate our models on five popular reasoning benchmarks for math and coding, including Math500, OlympiadBench, AIME-20243, AMC234 (Hendrycks et al., 2021c; He et al., 2024) and LiveCodeBench (Jain et al., 2024). For LiveCodeBench, we report weighted average accuracy across its easy, medium, and hard difficulty levels. 3.2. Key Insights Small amount of data is enough. In Fig. 1b, we present the performance of models fine-tuned with the 17k R1 trained 1 These prompts are from previous years of competition, which do not include AIME 2024 and AMC 2023 in our evaluation suite. 2 3 4 huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k. huggingface.co/datasets/AI-MO/aimo-validation-aime. huggingface.co/datasets/AI-MO/aimo-validation-amc. 3 Submission and Formatting Instructions for ICML samples. Both the supervised fine-tuned (SFT) and LoRA fine-tuned models learn to generate Long CoT responses and improve significantly on all benchmarks with just 17k training samples. We investigate the effect of distillation Figure 2: Model accuracy with different data sizes, and comparison to DeepSeek R1. The teacher model is DeepSeek R1, and the student model is Qwen-32B-Instruct trained with full parameter fine-tuning. While the student model continues to benefits from more SFT data from DeepSeek R1, small amount of data, e.g., 16k is sufficient to significantly boost the average performance by 15.2%. data size, ranging from 4k to 64k samples from R1, The results, presented in Fig. 2, shows that small amount of data, e.g. 16k is enough to significantly improve the model performance (from average 54.2 to 69.4). LoRA fine-tuning without performance degradation. We next investigate the extent to which distilling Long CoT reasoning is knowledge-intensive. In addition to the results using 17k R1 samples as demonstrated in Fig. 1b, we also report the results for both SFT and LoRA fine-tuning with 7k and 17k QwQ samples in Tab. 1. Prior work (Ghosh et al., 2024; Biderman et al., 2024) suggests that LoRA fine-tuning substantially under-performs full fine-tuning for knowledge-intensive tasks, and is limited to learning response initiation and style tokens. However, our results in Fig. 1b and Tab. 1 show that LoRA fine-tuned models achieve similar or even superior reasoning performance compared to full-parameter SFT across math and coding benchmarks. Additionally, we find that model fine-tuned with LoRA using just 7k QwQ samples performs comparably to one trained on 17k QwQ-distilled samples. This demonstrates that reasoning distillation can be achieved efficiently with both minimal parameter updates and minimal data. As shown in Fig. 1a, the LoRA fine-tuned model easily learns to generate Long CoT responses with reflection and self-verification. These observations suggest that Long CoT reasoning ability may not rely on deep knowledge acquisition but rather on learning structured reasoning patterns, which can be effectively distilled in parameter-efficient 4 manner. This also aligns with prior findings that methods such as Chain-of-Thought prompting elicit Short CoT reasoning primarily by shaping response structure rather than instilling deep factual knowledge (Wei et al., 2022; Yao et al., 2023). Table 1: Model accuracy with SFT and LoRA (rank=64). Fine-tuning performed on Qwen2.5-32B-Instruct with QwQ samples. Olympiad. is short for OlympiadBench, LCB. is short for LiveCodeBench. We find that the learning process of Long CoT can be parameter efficient. MATH500 AIME24 AMC23 Olympiad. LCB. Qwen2.5-32B-Inst. QwQ o1-preview 7k QwQ Samples SFT LoRA (r=64) 17k QwQ Samples SFT LoRA (r=64) 84.8 90.4 85. 87.8 86.6 87.8 86.6 16.7 33.3 44.6 33.3 40.0 33.3 33.3 67.5 75.0 87. 77.5 77.5 70.0 90.0 47.6 58.1 59.2 57.3 57.2 56.7 56.0 48.9 59.1 59. 57.5 56.6 57.9 56.2 4. Long CoT: Structure Is The Key Motivated by the observation that fine-tuning with small number of samples can significantly enhance model reasoning performance, we investigate the key factors driving this improvement. Specifically, we explore the contributions of two dimensions to the learning process: 1. The local content within reasoning step, including the correctness of the final answer, numbers in math derivations, and the use of reasoning keywords. 2. The global reasoning structure, including reflection, self-validation, and backtracking across multiple reasoning steps to form logically coherent long CoT. To understand their impact, we conduct two studies: (1) we perturb the content within individual reasoning steps such as the final answer, numerical digits, and reasoning keywords(4.1), and (2) we modify the global reasoning structure by inserting, deleting, and shuffling reasoning steps(4.2). We compare the performance of models trained on perturbed samples against both the base Qwen2.5-32BInstruct model (i.e., Original) and model trained on correct, unperturbed samples (i.e., Correct), as shown in Tab. 2. Our findings show that the learning process is highly sensitive to modifications in the global reasoning structure, but remarkably tolerant to errors in the local contents. Experiment setup In this section, we use QwQ-32BPreview to produce the distillation data and select subset of 4618 correct responses as the training set (out of the Submission and Formatting Instructions for ICML 2025 12k math data in 3). All perturbations in this section are performed on this dataset unless otherwise stated. We train models on each separate variant of the dataset with the same hyperparameters as in 3 and report performance in Tab. 2. 4.1. Wrong or Corrupted Local Content To study the importance of local content within individual steps, we preserve the overall reasoning structure while systematically perturbing the local content in training samples with different approaches. Wrong Answer Samples. During our training data curation process in 3, we only include samples that yield correct final answers. To assess whether correctness of the final answer is necessary for learning reasoning patterns, we instead train the model using an equivalent number of samples (4.6k) that lead to the wrong answer. Surprisingly, we find that training the base model without any samples that reach correct final answer still achieves an average accuracy of 63.1% across benchmarks, only 3.2% lower than training with entirely correct samples. Digits Corrupted Samples. Building on the previous experiment, we next examine the role of correctness in the intermediate reasoning steps. To evaluate this, we corrupt correct samples by replacing each digit with random number between 0 and 9. Note that this is severe corruption that can lead to nonsensical statements such as 1+1=3. Surprisingly, even when 70% of the digits are corrupted, the model still maintains an average performance of 62%, only 4.3% below the correct sample baseline, demonstrating robustness to incorrect content. However, when all digits are corrupted, the average performance plunges to 2.7%. Reasoning Keyword Removal. Given the prevalence of reasoning keywords in responses from LRMs (e.g., wait, let me think again, but), one theory is that these specific phrases may invoke the reflection and back-tracking necessary to elicit strong reasoning performance. To evaluate it, we use GPT-4o-mini to identify sentences with occurrences of these reasoning keywords and randomly remove fraction of them (e.g., 20%, 50%, 100%). Our results show that even after removing all (100%) such keywords, the model still achieves an average accuracy of 63%, which is within 3.3% of accuracy from the model trained with correct samples. This suggests that these particular keywords do not fundamentally impact the model reasoning performance. Conclusion. We find that errors in local content such as incorrect mathematical derivations or missing reasoning keywords have minimal impact on overall performance. Table 2: Effect of trace perturbations on reasoning performance 4. All models are trained with base Qwen2.532B-Instruct. Olympiad. is short for OlympiadBench. In particular, we study (1) traces with modified reasoning step contents: wrong answers, corrupted digits, and removed reasoning keywords, and (2) traces with modified structure: deleted, inserted, or shuffled steps. We find that structural perturbations are far more detrimental to model accuracy than content perturbations. MATH500 AIME24 AMC23 Olympiad. Avg. Baselines Original Correct Content Modifications 84.8 89.2 16.7 40.0 67.5 77.5 47.6 58. 56.7 66.3 Wrong Answers 88.6 30.0 77.5 56. 63.1 Corrupted Digits 100% 70% 50% 20% Removed keywords 100% 50% 20% Structure Modifications Shuffled Steps 100% 67% 33% Deleted Steps 100% 67% 33% Inserted Steps 100% 67% 33% 5.4 85.6 87.6 88.4 86.6 87.6 87.2 81.8 82.0 85.6 79.2 84.2 88.2 77.0 81.8 86. 0.0 30.0 36.7 30.0 33.3 36.7 33.3 23.3 26.7 33.3 13.3 26.7 23.3 10.0 20.0 33.3 2.5 77.5 77.5 82. 77.5 82.5 72.5 70.0 72.5 75.0 60.0 55.0 80.0 50.0 60.0 77.5 2.8 54.8 55.0 57.2 54.4 56.7 56. 49.1 47.6 55.3 45.4 48.1 57.7 41.1 46.0 57.2 2.7 62.0 64.2 64.5 63.0 65.9 62.3 56.1 57.2 62. 49.5 53.5 62.3 44.5 52.0 63.7 4.2. Corrupted Global Reasoning Structure Next, we examine the importance of reasoning structure by performing three modifications to the reasoning traces: deletion, insertion, and shuffle. We first note that our system prompt (Appendix C) instructs the model to generate responses with thoughts enclosed in the tags begin of thought and end of thought and the final solution and step-by-step explanation in begin of solution and end of solution. All modifications are performed on the thoughts, while the solution block is left unmodified. We use Llama-3.3-70B-Instruct (Dubey et al., 2024) to separate each reasoning trace into distinct reasoning steps, with boundaries determined by occurrences of backtracking, selfvalidation, reflection, or other breaks from linear sequence of thoughts. We then generated nine modified variants of the dataset by applying each modification (insertion, deletion, and shuffle illustrated in Fig. 3) to 33%, 67%, or 100% of reasoning steps in the 4,618 correct traces. Each variant is used to train the base model, Qwen2.5-32B-Instruct, and Submission and Formatting Instructions for ICML 2025 Figure 3: Reasoning step modifications. To evaluate perturbations to global structure across reasoning steps, we perform three modifications: deletion, insertion, and shuffling. These modifications break logical consistency across steps and degrade model accuracy far more than changes to local content within reasoning steps. we report the resulting performance in Tab. 2 and response lengths and reasoning keyword counts in Appendix D. Deleted reasoning steps. As reasoning steps are increasingly deleted from the training data, model accuracy steadily declines and eventually regresses to the base model performance. Notably, retaining only the final solution and extensive step-by-step explanation (i.e., 100% deletion case) does not suffice to learn strong reasoning capabilities. This suggests that correct long CoT demonstrations alone are insufficient. Instead, examples of handling errors and dead ends with backtracking, reflection, and self-validation are important for eliciting robust reasoning. At 67% deletion, the model imitates reasoning keywords (relative to the base model, keyword usage increases 45, and output token increases 9), but its accuracy does not improve accordingly. Consistent with 4.1, this validates that merely adopting reasoning keywords and long responses is insufficient. We note, however, that as more steps are deleted, the response lengths also decrease significantly, which could contribute to reduced accuracy. We hypothesize that it is the breaking of logical consistency between steps that causes accuracy degradation and validate this further in the following analysis. Inserted reasoning steps. To further validate the importance of logical structure, we replace subset of each traces reasoning steps with random sample of reasoning steps from other samples in the training set that lead to correct results. Unlike deletion, this approach generally preserves the original length of the reasoning trace, ensuring that accuracy degradation is not due simply to producing fewer steps. Relative to model variants trained with deleted reasoning steps, variants trained on inserted steps generate longer responses with more reasoning keywords, yet accuracy nonetheless deteriorates to, and even below, the level of the base model. Interestingly, each inserted step is itself coherent and originates from correct reasoning trace in the training data. Yet these internally-coherent steps appear in sequences that lack logical consistency and often from separate domain (e.g., combinatorics step may be inserted into geometry solution), leading to contradictions and disjointed reflections. For instance, the model trained with inserted reasoning steps frequently references earlier steps that do not exist (e.g., Alternatively, consider different approach without specifying the prior approach) or enumerates edge cases in an inconsistent order (e.g., declaring Case 2 without Case 1). While the model readily produces coherent individual steps that reflect on problem, the CoT fails to exhibit continuity across reasoning steps. This aligns with the observations in the deletion setting: mere increase in reasoning steps or keywords is insufficient for robust reasoninglogical consistency across steps is critical factor. Shuffled reasoning steps. We next examine whether preserving the domain of each reasoning step, eliminating potential cross-domain confusion, but randomizing their order likewise impacts the models ability to reason. As the amount of shuffling increases, response length and reasoning keyword usage remain high, and in fact exceed the model trained on correctly ordered traces, yet accuracy declines sharply. Similar to the insertion experiments, the model imitates the syntax of per-step reasoning but loses logical consistency across steps. For instance, we find that over 92% of model responses begin with backtracking or self-validation keyword (e.g., Alternatively, or Wait), even though there is no preceding content to correct or reconsider. The model also references prior calculations or cases that were never actually introduced in any preceding step. Thus, while the shuffled traces still contain valid domain-specific reasoning steps, their rearrangement leads to incoherent overall solutions. In other words, domain alignment alone does not prevent logical breakdown. Conclusion. Taken together, these findings show that providing error-free CoT demonstrations, increasing response lengths, imitating reasoning keywords and correct short CoT within individual steps, and preserving domain relevance for each step are not sufficient to produce effective reasoning Further, our experiments on incorrect traces (4.1) demonstrate that learning reasoning capability is largely robust to local inaccuracies or miscalculations. Instead, global structural consistency is essential to elicit coherent long CoTs with the reflection, revision, and validation behaviors that produce strong reasoning performance. 6 Submission and Formatting Instructions for ICML 2025 5. Ablation Study In this section, we conduct series of ablation studies to answer the following questions: 1. (5.1) Does fine-tuning on Long CoT data lead to degraded performance on non-reasoning tasks? 2. (5.2) How much does the Long CoT fine-tuning enhance the performance of different student models? 3. (5.3) How does Long CoT model performance compare to the Best-of-N sampling performance of the base model? 4. (5.4) How does Long CoT fine-tuning compare to Short CoT fine-tuning with the same dataset? 5.1. Performance on Non-Reasoning Benchmarks Table 3: Distilled Model Performance on Non-Reasoning Tasks. The teacher model is QwQ-32B-Preview, and the student model is Qwen2.5-32B-Instruct. Compared to QwQ, distilled models retain most of the base models capabilities. MMLU ARC-C IEval MGSM Qwen2.5-32B-Inst. QwQ 17k R1 Samples SFT LoRA (r=256) 17k QwQ Samples SFT LoRA (r=64) 7k QwQ Samples SFT LoRA (r=64) 74.1 71. 73.0 75.5 78.4 78.5 79.8 79.1 49.4 49.7 49.0 47.3 49.5 46. 48.6 47.4 78.7 42.5 77.8 78.4 75.8 74.1 70.6 75.4 42.3 19. 33.7 38.7 33.0 30.6 30.1 31.1 While simple distillation enhances reasoning capabilities, it is essential to ensure that these improvements do not come at the cost of catastrophic forgetting or decline in general language understanding and instruction-following abilities, which are crucial for broader task generalization. To assess this, we evaluate the performance of our SFT and LoRA fine-tuned models mentioned in 3 on diverse set of benchmarks: MMLU (multi-task language understanding), ARC-C (science exam question), IEval (instructionfollowing), and MGSM (multilingual grade-school math problems) (Hendrycks et al., 2021b; Clark et al., 2018; Mitchell et al., 2023; Cobbe et al., 2021). As shown in Tab. 3, the base instruction model (Qwen2.532B-Instruct) performs well in all these tasks. The QwQ model, despite its strong reasoning capabilities, suffers significant degradation in instruction-following (i.e., 42.5% on IEval) and multilingual tasks (i.e., 19.1% on MGSM). In contrast, fine-tuning (through both SFT and LoRA) only on small amount of Long CoT reasoning data from R1 or QwQ allows the distilled models to retain most of the base instruction models capabilities, avoiding the drastic performance drop seen in QwQ. 5.2. Effect on Different Student Models Figure 4: Generalization to other models. Accuracy for models of different sizes and architectures without SFT (green) and with SFT (blue). Most models show significant improvements when fine-tuned with 17k samples from R1Preview, showing that the Long CoT fine-tuning is beneficial across models. In this section, we examine whether Long CoT reasoning capabilities can be elicited with different student models via fine-tuning (as described in 3). Specifically, we train with the 17k samples on Qwen2.5-7B-Math, Qwen2.5-7-Base, Qwen2.5-7B-Instruct, Llama-3.1-8B, Qwen2.5-14B-Base, Qwen2.5-14B-Instruct, Gemma2-27B-it and Qwen2.5-32BInstruct (Yang et al., 2024; Dubey et al., 2024; Team et al., 2024). We find that seven out of eight models improve noticeably across multiple benchmarks, showing the effect of Long CoT as general improvement across models. However, not all models have showed the same degree of improvements as for Qwen2.5-32B-Instuct. These findings suggest promising future directions for understanding the performance upper bound and data efficiency with various teacher and student models in the space of reasoning. 5.3. Comparison to Best-of-N As discussed in 5.2, not all student models achieve significant performance improvements through Long CoT finetuning. We hypothesize that this variation is influenced by several factors, such as the extent to which the training data distribution differs from that of the student models and the inherent capabilities of the student models in these tasks. In this section, we compare the test-time scaling (Ahn et al., 2024; Snell et al., 2024) performance of the base model with its performance after Long CoT fine-tuning to understand the relationship between models ability to benefit from Long CoT fine-tuning and its intrinsic capabilities. 7 Submission and Formatting Instructions for ICML 2025 Table 4: Comparison of number of output tokens reasoning keywords, and the performance between training with Short or Long CoT. The original model is Qwen2.532B-Instruct. Benchmarks are ordered from easy to hard, where the model trained with Long CoT learns to produce longer CoTs and uses more keywords for harder problems. Dataset Original Short CoT Long CoT Avg. output tokens MATH500 AMC23 OlympiadBench AIME24 684 728 1275 825 0.00 0.00 0.01 0.00 Avg. keywords per response MATH500 AMC23 OlympiadBench AIME24 Performance MATH500 AMC23 OlympiadBench AIME24 84.8 67.5 47.6 16. 515 605 948 687 0.00 0.00 0.01 0.07 3972 5037 8616 15902 41.75 39.20 97.20 260.90 70.4 (-14.4) 55.0 (-12.5) 36.4 (-11.2) 13.3 (-3.4) 89.2 (+4.4) 77.5 (+10.0) 58.5 (+10.9) 40.0 (+23.3) training on the 4.6k samples of Long CoT generated by QwQ-32B-Preview (4), and the short CoT denoted in the NuminaMath-CoT dataset (LI et al., 2024). Tab. 4 summaries the statistics. Training with Long CoT enables the model to use more reasoning keywords (full list in Appendix B), produces longer responses to harder problems, and is the key to improved performance. 6. Conclusion Large reasoning models unlock new capabilities by using longer chain of thoughts that involves reflection and backtracking to answer challenging problems. In this paper, we show that such capability can be easily fine-tuned using few thousand examples and with low-rank adapters. We further show that the key to the learning process is the logical structure of the samples rather than the content of individual reasoning steps. Finally, we discuss several ablations with various teacher-student models and compare them to the best-of-N approach. Together, our work deepens the understanding of what is needed to instill large language models with strong reasoning capabilities and identify potential future directions."
        },
        {
            "title": "Impact Statement",
            "content": "Figure 5: SFT with Long CoT vs Best-of-N. Accuracy of Qwen2.5-32B-Instruct before SFT (Zero-Shot), after SFT on 17k R1 samples (Fine-tuned), and Best-of-N samples on OlympiadBench. We find that fine-tuning on Long CoT achieves performance similar to Best of 2 to 16 samples. Specifically, we compare the performance of Long CoT finetuning against Best-of-N sampling approach, where we generate 128 samples per prompt using an oracle verifier to select the best response. To introduce diversity, we employ temperature of 0.5 and top-p sampling with threshold of 0.8. The results, presented in Fig. 5, show that the Long CoT fine-tuned model performs comparably to Best-of-N sampling with 2 to 16 instances across all student models. Notably, the test-time scaling trends closely align with the improvements observed from Long CoT fine-tuning. For example, with eight parallel samples, Llama-3.1-8B achieves less than 10% accuracy on OlympiadBench, and similarly, fine-tuning with correct Long CoT traces results in only marginal improvement. comparable trend is observed in Gemma2-27B-it and Qwen2.5-Math-7B, reinforcing the relationship between test-time sampling efficiency and the benefits of Long CoT fine-tuning. The performance of Best-of-N sampling continues to improve beyond 128 samples, suggesting that further gains are possible. This highlights the potential for enhancing Long CoT models through context scaling or by leveraging broader range of reasoning paths inherent to the original model, potentially unlocking even higher performance. 5.4. Comparison to Short CoT Fine-tuning In this section, we provide direct comparison to training with short CoT. In particular, we compare results This paper aims to contribute to the advancement of Machine Learning. While our work may have various societal implications, we do not find any that require specific emphasis currently. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "Acknowledgement",
            "content": "This work is funded by the Sky Computing Lab at UC Berkeley. We extend our sincere gratitude to Sumanth Hegde, Eric Tang, and Kourosh Hakhamaneshi from AnyScale for their invaluable feedback, assistance in building the evaluation infrastructure, and and contributions to the best-of-n experiments. We are grateful for the generous compute resources support from Databricks, Lambda Labs, and Anyscale. In particular, we thank Jonathan Frankle (Databricks) and Chuan Li (Lambda Labs) for facilitating access to these resources."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., and Yin, W. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. Biderman, D., Portes, J., Ortiz, J. J. G., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ghosh, S., Evuru, C. K. R., Kumar, S., S, R., Aneja, D., Jin, Z., Duraiswami, R., and Manocha, D. closer look at the limitations of instruction tuning. arXiv preprint arXiv:2402.05119, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with apps. NeurIPS, 2021a. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. International Conference on Learning Representations (ICLR), 2021b. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021c. Hinton, G. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E., Xia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. 9 Submission and Formatting Instructions for ICML Yuan, L., Li, W., Chen, H., Cui, G., Ding, N., Zhang, K., Zhou, B., Liu, Z., and Peng, H. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. LI, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numina- [https://huggingface.co/AI-MO/ math. NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf), 2024. Li, R., Fu, J., Zhang, B.-W., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Min, Y., Chen, Z., Jiang, J., Chen, J., Deng, J., Hu, Y., Tang, Y., Wang, J., Cheng, X., Song, H., et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. Mitchell, E., Raileanu, R., Raffel, C., Levine, J., Tsvetkov, Y., and Manning, C. D. Ieval: An instruction following benchmark. arXiv preprint arXiv:2310.07724, 2023. Radford, A. Improving language understanding by generative pre-training. 2018. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Team, Q. Qwq: Reflect deeply on the boundaries of the unknown, 2024. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yao, S. et al. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. 10 Submission and Formatting Instructions for ICML 2025 A. Short and Long CoT Response Examples"
        },
        {
            "title": "For how many integers n does the expression",
            "content": "(cid:115) log(n2) (log n)2 log 3 represent real number, where log denotes the base 10 logarithm? 11 Submission and Formatting Instructions for ICML 2025 Short CoT answer (1/2) To solve the problem, we start by analyzing the given expression and the conditions under which it represents real number. The expression is: (cid:115) log(n2) (log n)2 log 3 For the square root to represent real number, the argument of the square root, which is the fraction inside, must be nonnegative. Additionally, since we are dealing with logarithms, must be positive integer. Lets break down the solution into detailed steps: 1. Condition for the Denominator The denominator of the fraction is log 3. For the fraction to be nonnegative, we need log 3 > 0. This implies that log > 3, which translates to Therefore, must be greater than 1000. 2. Simplifying the Numerator The numerator of the fraction is log(n2) (log n)2. Using logarithmic properties, we can rewrite log(n2) as 2 log n. Thus, the numerator becomes > 103 = 1000. 3. Analyzing the Fraction Substituting the simplified numerator into the fraction, we get: 2 log (log n)2. For this fraction to be nonnegative, given that the denominator is positive (from step 1), the numerator 2 log (log n)2 must also be nonnegative. This leads to the inequality: 2 log (log n)2 log 3 4. Solving the Inequality The inequality can be rewritten as Factoring, Solving this inequality, we find that Converting back to n, this implies: 2 log (log n)2 0. 2 log (log n)2 0 (log n)2 2 log 0. log n(log 2) 0. 0 log 2. 1 100. 5. Combining Conditions From step 1, we have > 1000, and from step 4, we have 1 100. However, these two conditions are contradictory. The correct interpretation should be that must satisfy both the condition of being greater than 1000 (from the denominator analysis) and the condition derived from the inequality, which was misinterpreted in the combination step. The correct combination should focus on the range derived from the inequality, which is 1 100, and disregard the incorrect application of the denominator condition in this context. 12 Submission and Formatting Instructions for ICML Short CoT answer (2/2) 6. Conclusion The integers that satisfy the given conditions are those for which Counting these integers, we find there are such integers. 1 100. 13 Submission and Formatting Instructions for ICML 2025 Long CoT answer (1/4) <begin of thought> So Ive got this problem here: (cid:115) log(n2) (log n)2 log And need to find out for how many integer values of this expression represents real number. The logarithm here is base 10. First off, since its square root, the expression inside the square root has to be non-negative for the whole thing to be real number. So, that means: log(n2) (log n)2 log 3 0 Additionally, the argument of the logarithm has to be positive because logarithms are only defined for positive real numbers. So, n2 > 0, which is true for all = 0, and > 0 since n2 > 0 is always true for = 0, but we need to ensure > 0 because the logarithm of negative number is undefined in real numbers. So, has to be positive integer. Now, lets simplify the expression inside the square root. know that log(n2) = 2 log n, because of the logarithm power rule. So, substituting that in: Let me denote log = to make it easier. So, the expression becomes: 2 log (log n)2 log can factor the numerator: So, the expression is: 2x x2 3 2x x2 = x(2 x) x(2 x) 3 need to analyze when this fraction is non-negative. To do that, can look at the sign of each factor: - = log - 2 = 2 log - 3 = log 3 So, the expression is: log n(2 log n) log 3 need to determine when this fraction is greater than or equal to zero. Let me consider the critical points where each factor is zero: 1. log = 0 = 100 = 1 2. 2 log = 0 log = 2 = 102 = 100 3. log 3 = 0 log = 3 = 103 = 1000 These points divide the number line into intervals where the expression doesnt change sign. So, the intervals to consider are: - 1 < < 100 - 100 < < 1000 - > 1000 14 Submission and Formatting Instructions for ICML 2025 Long CoT answer (2/4) 2 < 0, which is not acceptable. Wait, but need to consider as positive integers, and 1. But actually, has to be at least 1, but = 1 makes log = 0, which is critical point. So, lets consider the intervals: - 1 < < 100 - = 100 - 100 < < 1000 - = 1000 - > 1000 need to test the sign of the expression in each interval. First, lets consider = 1: log 1 = 0, so the numerator is 0, and the denominator is 0 3 = 3, so the fraction is 0/ 3 = 0, which is acceptable since its zero. Next, for 1 < < 100: Lets pick = 10, for example. log 10 = 1 So, = 1 Then, 2 = 1 3 = 2 2 = 1 So, 11 Wait, but earlier thought the expression inside the square root needs to be non-negative. So, this interval doesnt work. Next interval: = 100 log 100 = 2 Numerator: 2 (2 2) = 0 Denominator: 2 3 = 1 So, 0/ 1 = 0, which is acceptable. Then, 100 < < 1000 Lets pick = 10000.5 = 102.5 316.2, but since is integer, lets take = 300 log 300 2.477 So, 2.477 2 0.477 3 0.523 So, 2.477(0.477) Next, = 1000 log 1000 = 3 Denominator: 3 3 = 0, so the expression is undefined. Not acceptable. Finally, > 1000 Lets pick = 10000 log 10000 = 4 So, = 4 2 = 2 3 = 1 So, 4(2) So, summarizing: - = 1: acceptable - 1 < < 100: not acceptable - = 100: acceptable - 100 < < 1000: acceptable - = 1000: not acceptable - > 1000: not acceptable Therefore, the acceptable ranges are: - = 1 0.523 2.26 > 0, which is acceptable. 1 = 8 < 0, which is not acceptable. 1.182 0.523 Submission and Formatting Instructions for ICML 2025 Long CoT answer (3/4) - = 100 - 100 < < 1000 Wait, but = 100 is included in 100 < < 1000? No, = 100 is separate point. Actually, = 100 is the upper limit of 100 < < 1000, but since = 100 is acceptable, need to include it. Wait, no, = 100 is separate point where the expression is zero, which is acceptable. So, the acceptable are: - = 1 - 100 < 1000 Wait, but earlier thought = 1000 is not acceptable, but = 1000 is not included in 100 < 1000, which is correct. So, now, need to count the integer values of in these ranges. First, = 1: 1 value Second, 100 < 1000: thats from 100 to 999, inclusive. So, 999 100 + 1 = 900 values. Therefore, total integers are 1 + 900 = 901. But wait, earlier thought = 100 is acceptable, and it is included in 100 < 1000, so no need to count it separately. Wait, no, = 100 is included in 100 < 1000, which already includes 100 to 999, so total is 900 values. Plus = 1, so total is 901. But, need to make sure that for all in 100 < 1000, the expression is positive. Wait, except for = 100, where its zero, which is acceptable. But in the interval 100 < < 1000, its positive, as tested earlier. So, yes, total is 901. Therefore, the number of integer values of that satisfy the condition is 901. **Final Answer** <end of thought> <begin of solution> To determine for how many integer values of the expression 901 (cid:115) log(n2) (log n)2 log 3 represents real number, we first note that the expression inside the square root must be non-negative: log(n2) (log n)2 log 3 0 Additionally, since logarithms are only defined for positive real numbers, we require > 0. 1. Simplifying the expression Using the logarithm power rule, we have: Thus, the expression becomes: log(n2) = 2 log 2 log (log n)2 log 3 Let = log n. Then, the expression simplifies to: x(2 x) 3 16 Submission and Formatting Instructions for ICML Long CoT answer (4/4) 2. Finding critical points The critical points are where the numerator or denominator equals zero: - = 0 = 1 - 2 = 0 = 2 = 100 - 3 = 0 = 3 = 1000 3. Analyzing intervals The intervals to consider are: - 1 < < 100 - = 100 - 100 < < 1000 - = 1000 - > 1000 Testing the sign of the expression in each interval: - For = 1: The expression equals 0, which is acceptable. - For 1 < < 100: The expression is negative, not acceptable. - For = 100: The expression equals 0, acceptable. - For 100 < < 1000: The expression is positive, acceptable. - For = 1000: The expression is undefined, not acceptable. - For > 1000: The expression is negative, not acceptable. 4. Counting acceptable integers The acceptable ranges are: - = 1 - 100 < 1000 The number of integers in 100 < 1000 is 900 (from 100 to 999 inclusive). Adding = 1 gives total of: Thus, the number of integer values of that satisfy the condition is: 1 + 900 = 901 901 <end of solution> B. Reasoning keywords We create bank of reasoning keywords by manual inspection of reasoning responses from QwQ and R1. We identify these following words and phrases as frequent indicators of reflection, backtracking, and self-validation: Alternatively, Wait, Just to be thorough, Just to make sure, Let me just double-check, Let me try another, Let me verify, Let me check, Hmm, But, Maybe should consider, Maybe can consider. C. Long CoT System Prompt We adopt the following system prompt from Still-2 (Min et al., 2024): Long CoT System Prompt Your role as an assistant involves thoroughly exploring questions through systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <begin of thought> thought with steps separated with nn} <end of thought> Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <begin of solution> final formatted, precise, and clear solution <end of solution> Now, try to solve the following question through the above guidelines: D. Average response lengths and keyword counts 17 Submission and Formatting Instructions for ICML 2025 Table 5: Average keyword counts and output tokens for deleted steps. Dataset 0% 33% 67% 100% Avg. output tokens Math AMC 2023 OlympiadBench AIME 2024 3551 4838 7234 13088 2979 6612 6802 11889 Avg. keywords per response Math AMC 2023 OlympiadBench AIME 2024 32 39 77 28 85.6 70 143 2078 4623 4978 6798 20 77.8 56 90 482 609 595 620 0.017 0 0.009 0 Table 6: Average keyword counts and output tokens for inserted steps. Dataset 0% 33% 67% 100% Avg. output tokens Math AMC 2023 OlympiadBench AIME 2024 3551 4838 7234 13088 4189 7089 7558 3900 5464 6990 12864 5383 5137 5407 5304 Avg. keywords per response Math AMC 2023 OlympiadBench AIME 2024 32 39 77 143 39 98 76 127 39 44 80 41 35 38 44 Table 7: Average keyword counts and output tokens for shuffled steps. Dataset 0% 33% 67% 100% Avg. output tokens Math AMC 2023 OlympiadBench AIME 3551 4838 7234 13088 4284 6802 8942 13451 5784 10198 12154 16221 5613 8661 12167 18054 Avg. keywords per response Math AMC 2023 OlympiadBench AIME 2024 32 39 77 45 65 111 161 61 74 166 201 70 67 137"
        }
    ],
    "affiliations": [
        "Department of Electrical Engineering and Computer Sciences, University of California, Berkeley"
    ]
}