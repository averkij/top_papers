{
    "paper_title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization",
    "authors": [
        "Aditya Tomar",
        "Coleman Hooper",
        "Minjae Lee",
        "Haocheng Xi",
        "Rishabh Tiwari",
        "Wonjun Kang",
        "Luca Manolache",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2$\\times$ memory savings compared to KV caching. By applying XQuant, we achieve up to $\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10$\\times$ memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5$\\times$ memory savings with only $0.1$ perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 9 3 0 1 . 8 0 5 2 : r XQUANT: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization Aditya Tomar* 1 Coleman Hooper* 1 Minjae Lee2 Haocheng Xi1 Rishabh Tiwari1 Wonjun Kang2 Luca Manolache1 Michael W. Mahoney1,3,4 Kurt Keutzer1 Amir Gholami1,"
        },
        {
            "title": "Abstract",
            "content": "Although LLM inference has emerged as critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQUANT, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2 memory savings compared to KV caching. By applying XQUANT, we achieve up to 7.7 memory savings with < 0.1 perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that values are similar across layers. Building on this observation, we introduce XQUANT-CL, which exploits the cross-layer similarity in the embeddings for extreme compression. Across different models, XQUANT-CL attains up to 10 memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5 memory savings with only 0.1 perplexity degradation. Notably, despite using standard uniform quantization, XQUANT-CL is able to surpass intricate KV cache quantization methods that employ non-uniform quantization with outlier-aware strategies. Given the aforementioned trends in compute versus memory scaling for future generations of hardware platforms, XQUANT adopts forward-looking perspective to accelerate LLM inference: XQUANT seeks to exploit the rapidly increasing compute capabilities to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across wide range of models."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have seen widespread adoption as standard paradigm across range of Natural Language Processing (NLP) applications [6, 3, 31, 34]. While LLMs achieve remarkable performance on these tasks, they have substantial inference costs due to their large parameter count as well as the number of memory operations required when running generation. Prior work has demonstrated how LLM inference tends to be Memory BandwidthBound, rather than Compute-Bound [20, 33, 27], and therefore reducing the memory footprint of LLMs is critical to enable downstream applications. For short context lengths and small batch sizes, the model weights are typically the memory bottleneck. However, for long context lengths and large batch sizes, the main memory bottleneck for LLM inference is the Key-Value (KV) cache, which is the embedded representation of the entire sequence used in the self-attention mechanism and which grows linearly with respect to the sequence length [33, 27]. During inference, generating each new token requires repeatedly loading and storing the entire KV cache, which becomes prohibitively expensive and leads to substantial slowdown. This motivates efforts to reduce KV cache memory operations to speed up the inference process. One promising solution to compress the KV cache is through KV cache quantization [15, 23]. *Equal Contribution 1 Figure 1: Perplexity degradation (lower is better) versus memory compression factor (higher is better) evaluated using Llama-2-7B on WikiText-2 for state-of-the-art KV cache quantization methods and for our XQUANT, across {4,3,2}-bit widths. The top right edge of the plot represents the optimal configuration that attains the most memory compression and the least perplexity degradation. Memory compression factor and perplexity degradation are with respect to the FP16 baseline. As shown in Table 4, XQUANT-CL achieves only 0.01 perplexity degradation while getting 10 memory savings with 3-bit quantization, and 0.1 perplexity degradation while getting 12.5 memory compression with 2-bit quantization. Quantizing the KV cache reduces the memory footprint and number of memory operations required during decoding by using fewer bits to represent the Keys and Values. However, while existing methods retain accuracy even when quantizing the KV cache to low precision (e.g., 4-bit quantization), further reducing the bit-width of KV activations often degrades model performance. In this work, we present XQUANT, method which quantizes the input activations of each layer, rather than the KV cache, to reduce the required memory consumption. Our method is visualized in Figure 2. Quantizing provides 2 memory savings compared with quantizing the KV cache, since we only need to store one tensor per layer instead of separate Keys and Values. Interestingly, we also find that is more amenable to extremely low-bit quantization than the KV cache. Moreover, although rematerializing KV cache from requires additional computation during decoding, we can afford this cost because LLM inference is progressively becoming more memory-bandwidth bound. This phenomenon will be increasingly prevalent with future hardware platforms, as the rate of improvement in compute capabilities continue to outpace increases in memory bandwidth and capacity [11]. In this work, we make the following contributions (which are summarized in Figure 1): To reduce the memory consumption for LLM inference, XQUANT quantizes the input activations, providing 2 memory savings relative to quantizing the KV cache directly, and then rematerializes the KV activations on-the-fly during inference (see Section 3.1). We show that for the same memory footprint ( 7.7 savings compared to FP16), XQUANT attains up to 0.9 less perplexity degradation compared to KV cache quantization and < 0.1 perplexity degradation compared to the FP16 baseline (see Section 4.1). For ultra-low precision quantization with accuracy comparable to the FP16 baseline, we present XQUANT-CL, method that exploits the cross-layer similarity in the embeddings. Our approach compresses the differences in between successive layers, which have much smaller range as result of the residual stream in the 2 Figure 2: visualization of how XQUANT reduces the memory footprint by caching the input embedding (X) instead of the KV cache. We use the cached input to rematerialize the Keys and Values in order to compute attention. This increases the amount of computation required when computing attention. However, since LLM inference is typically memory bandwidth-bound, we can accelerate inference by reducing memory operations, even at the expense of additional compute operations. Transformer[36] architecture (see Section 3.2). Using standard asymmetric uniform quantization, we observe only 0.01 perplexity degradation with 3-bit quantization while attaining 10 memory savings compared to the FP16 baseline, and only 0.1 perplexity degradation with 2-bit quantization and 12.5 memory savings relative to FP16 (see Section 4.3). Remarkably, XQUANT-CL outperforms state-of-the-art KV cache quantization methods like KVQuant [15] that use complex techniques such as non-uniform quantization and outlier-aware dense-and-sparse strategies. On the Wikitext-2 and C4 datasets, we reduce perplexity degradation by 0.4 compared to KVQuant while using 1.9 less memory (see Section 4.3). We extend XQUANT and XQUANT-CL to support new models that use Grouped Query Attention (GQA) [1]. For GQA models, we decompose the Key and Value projection weight matrices offline using the singular value decomposition (SVD), and we then down-project the input activations into smaller latent space, reducing memory consumption. Interestingly, we observe that the distribution in this latent space is suitable for extremely low precision quantization (see Section 3.3). For 2-bit quantization, XQUANT achieves less than 2.2 perplexity degradation relative to KV cache quantization (see Section 4.1). XQUANT-CL further improves performance by resulting in only 0.1 perplexity degradation compared to the FP16 baseline while saving 6.7 memory (see Section 4.3). Finally, we provide system-level analysis of computational overheads and memory savings from rematerialization. We show that XQUANT is able to significantly reduce memory consumption while preserving accuracy, which will ultimately result in speedup even at the cost of additional computation, as compute continues to dominate memory capacity and bandwidth (see Section 3.4)."
        },
        {
            "title": "2.1 Memory Wall for LLM Inference",
            "content": "When assessing the performance of kernels on target hardware platform, their runtime is determined by either 1) the amount of compute operations that need to be performed or 2) the amount of memory operations that need to be performed. Which of these two factors is the bottleneck depends on the characteristics of the target hardware as well as those of the kernel. Arithmetic Intensity, which is defined as the ratio of the number of compute operations that need to be performed per byte transferred to or from memory, is typical metric to evaluate kernel and is usually expressed in units of floating point operations (FLOPs) per byte: Arithmetic Intensity ="
        },
        {
            "title": "Compute Operations Performed\nBytes Transfered",
            "content": ", (1) We can characterize the target hardware platform in terms of the ratio of the peak compute performance of the device versus the amount of memory bandwidth that it provides. The ratio of peak compute to memory bandwidth, typically also expressed as FLOPs per byte similar to arithmetic intensity, is referred to here as the ridge point for that hardware platform (we use this term as it aligns with the point on Roofline plot [37] which delineates compute-bound versus memory-bandwidth bound kernels for target hardware platform): Ridge Point ="
        },
        {
            "title": "Peak Compute Throughput\nPeak Memory Bandwidth",
            "content": ". (2) When assessing whether provided kernel is memory bandwidth-bound or compute-bound, we need only compare the arithmetic intensity of the kernel with the ridge point of our target hardware platform. If the arithmetic intensity is larger, then the kernel is compute-bound when run on the target hardware; if the arithmetic intensity is smaller, then the kernel will be memory-bandwidth bound [33, 37, 45]. When considering LLM inference, key challenge is that it has low arithmetic intensity for most batch sizes and context length regimes [19, 20, 33]. This is because generating each token only requires performing low intensity matrix-vector multiplications, whereas loading these matrices requires far more memory operations. Thus, LLM inference performs very few floating point operations per byte loaded from memory. Additionally, when we assess scaling trends with LLMs and hardware compute and memory, there is growing discrepancy between the memory capabilities of existing devices and an LLMs demands. This phenomenon has been termed the Memory Wall problem [11]. There are two key components to this problem: 1. The scaling trends in terms of memory requirements for modern LLMs have dramatically outpaced both the increases in memory capacity and bandwidth on hardware. 2. The scaling in peak computational performance is orders of magnitude greater than the corresponding increases in memory capacity and bandwidth on hardware. Taking these factors together, it is critical to reduce the number of memory operations required for LLM inference. If we can additionally reduce the memory requirements by increasing the amount of computation, this will be beneficial, due to the growing gap between peak computational performance and memory capabilities of hardware platforms."
        },
        {
            "title": "Motivation",
            "content": "While increasing computation in exchange for reduced memory usage may introduce latency on todays hardware, this tradeoff is expected to become increasingly favorable. Our work leverages this trend, aiming to reduce memory bottlenecks by utilizing additional compute, ultimately enabling faster LLM inference on future hardware generations."
        },
        {
            "title": "2.2 Activation Rematerialization",
            "content": "Prior work has explored rematerializing activations, where activations are recomputed on-the-fly from smaller checkpointed state, as partial solution to the memory wall problem [16]. Checkmate [16] allowed training large DNNs in memory-constrained GPUs by retaining subset of intermediate activations as checkpoints, with other activations 4 being discarded and then rematerialized from these checkpointed states. Checkmate solves for the optimal recomputation setup for the target hardware platform with provided memory constraints. Recent work has also explored rematerializing the KV cache from the input embedding in the context of serving systems which offload some of the KV entries to the CPU. HCache [10] offloads to CPU memory, and performs restoration by moving from CPU to GPU and then recomputing to recover the Keys and Values. [21] also performs rematerialization to recover the Keys and Values from X, and determines the optimal amount of rematerialization to perform, given combination of compute and memory constraints. While these works focus on system-level optimizations to determine the amount of rematerialization that can be performed, our work focuses on compressing rather than KV cache activations to attain improved savings relative to existing KV cache compression methods, and then exploits cross-layer similarity in to attain greater memory savings. There has also been prior work on trying to avoid storing Keys and Values separately for the KV cache. ElAttention [42] merged the Key and Value projection matrices into other matrices in the model, thereby allowing for computing attention directly using the input embedding. However, this method is not compatible with the rotary positional embedding (RoPE) encodings or with models that use Grouped Query Attention. Slim-Attention [12] similarly aimed to only cache Keys and to multiply them by the inverse Key projection matrix to recover the values (and to merge this inverse matrix into other matrices in the model offline). This approach also requires applying RoPE on-the-fly during inference and is not compatible with models that use grouped-query attention. Moreover, such inverses are not guaranteed to be numerically stable."
        },
        {
            "title": "2.3 KV Cache Quantization",
            "content": "KV cache quantization has emerged as promising method for reducing the memory requirements for the KV cache by using fewer bits to represent each floating point element in each KV cache entry. Previous work has quantized the Key distributions per-channel and the Value distributions per-token in order to adapt to the outlier channels in Keys [15, 23]. crucial aspect of Key cache quantization is handling RoPE [30]. Prior methods have either applied pre-RoPE Key quantization [15] or quantized the Key cache using polar-form representations [38, 13], in order to retain accuracy. Mixed-precision KV cache quantization has been used in order to preserve model accuracy by retaining particularly sensitive tokens in higher precision [44, 14]. Prior work has also explored retaining initial pivot tokens intact [22]; this builds on prior work that identified initial tokens as attention sink, tokens which are disproportionately important for preserving model accuracy [39]."
        },
        {
            "title": "2.4 Low-Rank Decomposition for KV Cache and KV Rematerialization",
            "content": "There has also been prior work which has applied low-rank decomposition to the KV cache or the corresponding projection matrices, in order to cache KV entries with reduced latent dimension before rematerializing the original KV entries. xKV [4] exploits the fact that the singular vectors for the KV cache entries for successive layers are well aligned to group the KV cache entries across layers and apply SVD to the concatenated KV caches. Loki [29] finds that the keys are low-rank and performs attention in lower dimension to identify the most important keys, and then only loads those keys for full-dimensional sparse attention. While Loki only reduces memory operations, Eigen Attention [28] actually compresses the KV cache and only does attention in low-rank space. There have also been multiple works which have applied low-rank decomposition to the weights in order to project the KV cache to lower dimension [47, 5, 41]. LoRC [47] applies SVD to the weight matrices and keeps more singular values at earlier layers to minimize error amplification. Palu [5] performs low-rank decomposition on the weights offline ahead of inference, and then caches the intermediate KV cache entries. ReCalKV [41] reorders heads before applying SVD to groups of heads in order to retain accuracy and reduce rematerialization overhead. In contrast with these works that aim to compress the KV cache using low-rank decomposition, XQUANT aims to quantize the embeddings in order to get 2 memory savings without applying low-rank decomposition. XQUANT-CL also exploits the cross-layer similarity in the distributions by compressing the differences across layers, which provides substantial memory reduction for the same accuracy relative to compressing the KV cache directly."
        },
        {
            "title": "3 Algorithm: Sacrificing Compute to Alleviate Memory Bottlenecks",
            "content": "In this section, we introduce our algorithms XQUANT (Section 3.1) and XQUANT-CL (Section 3.2), and we discuss them in the context of Multi-Head Attention (MHA) models [36]. Then in Section 3.3, we discuss how we extend our 5 Figure 3: Comparison of the post-norm input embeddings X, pre-RoPE Keys, and Values for successive layers in the Llama-3.1-8B model. The distributions were collected using test sample with 2K sequence length from Wikitext-2. Although the Keys and Values exhibit distinct differences across successive layers, the embeddings bear remarkable similarity. We exploit this similarity using cross-layer compression in XQUANT-CL. algorithm to support Grouped Query Attention (GQA) models [1]. Note that we specifically address GQA because of its widespread popularity and adoption as an optimization technique for KV cache reduction among many model families such as Llama, Mistral, Gemma, Qwen, etc. [8, 18, 32, 43]. Lastly, in Section 3.4, we discuss the tradeoffs between compute and memory operations that our methods make."
        },
        {
            "title": "3.1 XQUANT: Quantizing X Instead of KV",
            "content": "The core idea of XQUANT is to reduce the memory requirements of KV caching by checkpointing the input activations and regenerating the Keys and Values from these smaller checkpoints when they need to be used to compute attention. This is shown in Figure 2, where we quantize and cache the input embedding X, which requires 2 less memory than using standard KV caching. The drawback with caching is that it requires rematerializing the KV cache on-the-fly; this requires multiplying the input embedding by projection matrices Wk and Wv. However, as outlined in Sections 2.1 and 3.4, since the arithmetic intensity is typically low for attention in LLM decoding, we can afford to perform additional computation for rematerialization in order to reduce the number of memory operations required. Note that whenever we refer to X, we mean the input activations after layer normalization has been applied to them [36, 46, 40]."
        },
        {
            "title": "3.2 XQUANT-CL: Leveraging Cross-Layer Similarity in X",
            "content": "As shown in Figure 3, the embeddings across successive layers are remarkably similar (when compared with the similarities of KV cache embeddings across layers). This observed property can be attributed to the residual stream which flows from each layers input to the outputs of the attention and multi-layer perceptron blocks within that layer, where these outputs are added to the residual stream [36]. In this way, each layers function can be understood as simply refining its input, and since this refinement occurs gradually, it is intuitive that the inputs of successive layers are not substantially dissimilar [17]. This represents promising opportunity for further compression if we can exploit cross-layer similarity. 6 Figure 4: Illustration of XQUANT-CL algorithm during decoding. Besides Layer 0, the input to all other layers is cross layer approximation, computed using the deltas of all previous layers and the input of Layer 0. The input of Layer 0 is summed with each layers delta so it can be treated as an accumulator, allowing us to avoid loading all 1 deltas to compute Layer Ns X. After layer is done processing, the input embedding to the layer for the last token is subtracted from the output activations of the layer (the same shape as single token), and this delta is quantized and appended to the ˆX cache. XQUANT-CL during prefill is visualized in Figure A.1 in Appendix A, which shows how the full ˆX is computed and cached for each layer. To exploit the cross-layer similarity in X, we propose XQUANT-CL, which compresses the differences between for successive layers. Our algorithm during prefill is illustrated in Figure A.1 in Appendix A, and our algorithm for decoding is visualized in Figure 4. Instead of directly quantizing for layer i, we instead quantize Xi = Xi Xi1, which has much smaller range and hence is much easier to quantize. For this method, we leave the first layer X0 in higher precision, and then compute the differences relative to X0. However, if we directly compute Xi X0 for layer which is far from layer 0, Xi would have accumulated enough changes such that it has diverged substantially from X0, meaning that this delta is no longer easy to quantize. To address this, we quantize and cache ˆXi = Q(Xi ˆXi1), where is the quantization function and ˆXi1 is the cross-layer approximation for the previous layers X. We then ˆXj. This sequential delta summation allows for the explicit accumulation of approximate Xi as ˆXi = X0 + the refinements that each layer applies to its input, allowing us to exploit the easy quantization properties of these individual deltas for extreme compression. However with the above formulation, computing ˆX for layer requires loading all 1 previous deltas, which is very expensive. Thus, we maintain an accumulator which sums the deltas of all previous layers (as highlighted in Figure 4). This way, computing ˆXi only requires loading the accumulator ( ˆXi1 = X0 + i1 j=1 ˆXj) and single delta ˆXi. j="
        },
        {
            "title": "3.3 Support for Grouped-Query Attention Models",
            "content": "One challenge with extending XQUANT to many newer LLMs is that these models typically use Grouped Query Attention (GQA) [1], as opposed to standard Multi-Headed Attention (MHA) [36] used by the models weve addressed thus far. As mentioned previously, we explicitly address GQA models since GQA has seen broad adoption for LLMs as way of reducing the size of the KV cache [8, 18, 32, 43]. GQA reduces the memory consumption of the KV cache by sharing Keys and Values amongst subset of the attention heads. The challenge with extending our methodology to GQA models is that they down-project the input activation when computing the Keys and Values. This means that while the embeddings have shape (where is the sequence length and is the hidden dimension), and are each, where is the number of query heads that share Keys and Values. For example, the Llama-3.1-8B model[25] has hidden dimension of 4,096 and uses = 4. Therefore, has dimension 4K and the Key and Value activations have dimension 1K each, so the KV cache is of shape 2K. Caching KV would therefore require storing 2 vectors of size 1K each for each token, whereas caching would require storing single vector of size 4K for each token. This means that if we naively apply XQUANT to GQA models, we will have 2 memory overhead for 7 Figure 5: diagram outlining how we apply XQUANT for GQA-based models. GQA down-projects the input embedding (X) to smaller d/g dimension when computing the Keys and Values. Hence, if we naively quantize the input rather than the KV cache, this will potentially have greater memory consumption. To address this, we first apply SVD to the Wk and Wv matrices offline. Online during prefill, we down-project by Uk to get Xk and by Uv to get Xv before applying XQUANT, thereby reducing memory consumption. For generation, each new token is also down-projected into this latent space, appended to the Xk and Xv cache, and quantized. The concatenated [Xkxk] is multiplied by (Σ ) to recompute the Keys, and the concatenated [Xkxv] is multiplied by (ΣvBT ) to recompute the values. Note that the group size is typically greater than or equal to 4, meaning that naively caching uses greater than or equal to 2 as much memory as simply doing KV caching. kBT the same precision, which negates the benefits of our approach."
        },
        {
            "title": "3.3.1 Extending XQUANT to support GQA",
            "content": "To apply XQUANT for models which use GQA, we apply Singular Value Decomposition (SVD) offline to the weight matrices in order to allow the input activations to be stored in lower-dimensional latent space. Our algorithm is , respectively. During visualized in Figure 5. We apply SVD to the Wk and Wv matrices to obtain Uk prefill, we down-project the input embeddings as XUk and XUv (where Uk and Uv are each ), which lowers the dimensionality of by 2 and results in the same memory footprint as the GQA KV cache. During generation, each new tokens input activation is similarly down-projected by Uk and Uv to get xk and xv, respectively. These are appended to the cached XUk and XUv, which are then multiplied by Σ , respectively, to get the Keys are each fused (Σ and BT are multiplied to merge them into single matrix) and Values. Moreover, Σ to serve as the new weight matrices that the latent Uk and Uv, respectively, are multiplied with to rematerialize the KV cache. This also reduces the recomputation cost during inference, as the fused Σ are smaller square matrices of shape . We elaborate on the associated cost in more detail in Section 3.4. Importantly, the SVD and weight fusing are done offline and therefore dont add any latency overhead. and UvΣvBT and ΣvBT and ΣvBT and ΣvBT kBT kBT kBT kBT Σ Importantly, note that while this approach has the same memory consumption as KV quantization for the same bit width, the down-projected distributions are easier to quantize, giving us higher accuracy for the same bit width (see Section 4.1). In fact, the latent distributions reveal very interesting structure: we find that the latent XUk groups all outliers on the first channel, and we observe this for all layers of the model across several different models on different 8 datasets (see Figures B.2, B.3). Note that we quantize and cache XUk without applying the Σ matrix of singular values. Uk is matrix with orthonormal columns, so observing this structure in XUk where all the outliers lie on the first channel is very interesting. We do not, however, observe any similar interesting structure in the XUv distribution. This is in keeping with observations made by other works which find that Keys have outlier channels whereas Values do not have clearly structured axis where outliers lie [23, 15]. We believe that the outliers present in the first channel of XUk are simply distributed to other channels once the latent distribution is transformed by BT , which gives rise to the outlier channels found in the Keys. We discuss this further in Appendix B. We utilize per-channel quantization for XUk and per-token quantization for XUv, as we find this to be the best configuration resulting in the least accuracy degradation. This is similar to [23, 15] which also quantize the Keys per-channel and Values per-token."
        },
        {
            "title": "3.3.2 Extending XQUANT-CL to support GQA",
            "content": "g , on which we perform an SVD to get Ukv We also extend our cross-layer method to support GQA models. For GQA models, XQUANT-CL faces the same issue addressed in Section 3.3.1, specifically that GQA models down-project of shape to the KV cache of shape . Since has the same shape as X, naively caching this delta for GQA models results in more memory 2 overhead than storing the KV cache. To address this, we column-wise concatenate the Wk and Wv projection matrices, resulting in Wkv = [WkWv] Rd2 kv. Here, Ukv Rd2 kv is discarded. We down-project by Ukv, is shared subspace for the individual projection matrices, and Σ resulting in the same memory footprint as the KV cache, and we quantize and cache this latent distribution. Although this approach has the same memory consumption as KV quantization, the deltas are easier to quantize despite the latent projection, giving us higher accuracy for the same bit width (see Section 4.3). Crucially, the SVD is performed offline, so there is no additional latency overhead during inference. When computing ˆXi, which is the approximation of for layer i, we need to merge our accumulator ˆXi1 of shape with the latent XiUkv. To do this, we up-project XiUkv using UT kv, add this result to the accumulator ˆXi1, and multiply the updated accumulator ˆXi by Wkv to complete the KV cache rematerialization. One concern is whether up-projecting the latent delta by UT kv is able to retrieve the original delta: for the non-square matrix Wkv, the SVD produces non-square Ukv with orthonormal columns such that UT but UkvUT kv = Id. However, when the quantization function is the identity function, up-projecting the latent XiUkv by UT kv is lossless reconstruction of the original Xi when computing the KV cache for layer i: kvUkv = I2 kvBT kvBT Σ [KV]i = ( ˆXi1 + Q(XiUkv) UT where UT Σ kv) Ukv , UkvUT kvBT kv kv = Id, Q(x) = kvUkv = I2 = ( ˆXi1Ukv + XiUkv) Σ = ( ˆXi1 + Xi) Ukv Σ kvBT kv = ( ˆXi1 + Xi) Wkv = ˆXi [WkWv]. kvBT kv"
        },
        {
            "title": "3.4 System-Level Analysis of Rematerialization",
            "content": "Here, we present system-level modeling that outlines the computational and memory overheads from rematerialization with XQUANT and XQUANT-CL. For this analysis, we count multiply-accumulate operations as two FLOPs. Assume we are using model with hidden dimension and assume sequence length l. To apply XQUANT on MHA models, the amount of computation required for rematerialization for single layer is 2 2 d2. Suppose that we apply e-bit quantization for X. Then the total number of memory operations in bytes is equal to 8 d; whereas for the KV cache quantization, it would have been 2 8 d. To apply XQUANT on GQA models, the amount of computation required for rematerialization for single layer is 2 2 ( )2, where is the number of query heads that share Keys and Values. This is factor of g2 less floating point operations compared to MHA models. Suppose that we apply e-bit quantization for X. Then the total number of memory operations in bytes is equal to 2 8 , which is the same as KV cache quantization. However, for the same e-bit quantization, XQUANT achieves much higher accuracy than KV cache quantization (see Section 4.1). Equivalently, XQUANT achieves similar accuracy with 9 smaller to KV cache quantization with larger effective e, meaning that for the same accuracy, XQUANT performs fewer memory operations. Here, we also provide an example to demonstrate the amount of the rematerialization that can be performed without having the additional compute operations become the latency bottleneck. Here, we assume the NVIDIA H100 GPU as our hardware target, which has ridge point of = Peak FLOPs 2 TB/s = 378, and we assume that we can overlap the KV cache recomputation with loading the model weights (which corresponds to 2 12 d2 additional memory operations for single layer for the Llama-2-7B model). We can solve for the maximum amount that we can reconstruct without compute becoming the bottleneck: Memory BW = 756TFLOPs 2 2 d2 8 + 2 12 d2 Solving this equation for = 378, = 4K, and = 2 gives maximum sequence length of 2.3K that can be = (3) rematerialized without having compute operations become the bottleneck. We can perform similar analysis for the Llama-3.1-8B model (which requires 2 13 d2 + 2 2 ( )2 memory operations for the weights for single layer, including the overhead of loading the Wk/Wv matrices in SVD-decomposed form, and which has = 4): = 2 2 ( )2 8 + 2 13 d2 + 2 2 ( ) (4) Solving this equation for = 378, = 4K, = 4, and = 2 gives maximum sequence length of 40.6K that can be rematerialized without having compute operations become the bottleneck. With XQUANT-CL, we perform an additional 2 compute operations for each layer, since we need to add the the cached delta ˆXi to the accumulator ˆXi1. However, this cost is negligible: if we combine it with the aforementioned rematerialization cost, we get 2 2 d2 + 2 = 2 2 (d + 1 2 ). XQUANT-CL also requires additional memory operations, as we need to load and store both the accumulator and the cached delta at each layer. Since we keep the accumulator in higher precision, we perform eb 8 memory operations in bytes at each layer, where eb is the accumulators precision (typically eb = 4 bits; see Section 4.3). Additionally, we perform 8 memory operations required to load the cached delta, which is the same as XQUANT. For GQA models, since we are caching the quantized down-projected XiUkv at each layer, the number of memory operations for the delta is 2 8 , which is the same for XQUANT applied on GQA models. After loading this delta, we up-project it by UT kv to merge with the accumulator ˆXi1, and then apply the Key and Value weight projections onto the updated accumulator. In total, this requires 2 4 compute operations."
        },
        {
            "title": "4 Empirical Results",
            "content": "To evaluate our method, we use the Llama-2-7B/13B, Llama-3.1-8B, and Mistral-7B-v0.3 models [35, 25, 18]. We measure perplexity on the WikiText-2 and C4 datasets [24, 26], and we perform downstream task evaluation on the LongBench and GSM8K datasets [2, 7]. Note that the Llama-2-7B/13B models use MHA, whereas Llama-3.1-8B and Mistral-7B use GQA. We compare our method against KIVI [23] and KVQuant [15], two representative state-of-theart KV cache quantization methods. For KIVI, we use stronger baseline (referred to here as KIVI*). The original KIVI method quantizes the Keys after rotary positional embeddings (RoPE) are applied to them [23]. However, [15] finds that applying RoPE to the Keys results in less structured distribution, whereas the Keys have more structured outlier channels pre-RoPE. We therefore follow [15] and quantize the Keys before applying RoPE. For KVQuant, we use the best configuration from [15], which performs non-uniform, per-vector dense-and-sparse quantization. KVQuants non-uniform quantization requires deriving per-layer sensitivity-weighted non-uniform datatypes from calibration dataset offline to better represent the distributions. We follow [15] and use 16 calibration samples of sequence length 2K from WikiText-2. The dense-and-sparse quantization isolates outliers separately for each vector and preserves them in higher precision. Note that XQUANT and XQUANT-CL use simple uniform quantization, with no special outlier handling or calibration required. For the KV cache quantization baselines, we quantize the Keys per-channel and the Values per-token 10 Table 1: XQUANT evaluation using perplexity on WikiText-2 and C4. Left: (MHA) Llama-2-7B/13B; Right: (GQA) Llama-3.1-8B, and Mistral-7B. We provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline) and group rows by similar memory consumption. For MHA models, each group shows XQUANT using slightly less memory compared to KIVI*, as XQUANT only needs to store scale factors and zero points for single tensor, whereas KIVI* needs to store the same for and V. Method KV Llama-2-7B Llama-2-13B (MHA) Wiki2 Wiki2 C4 Baseline KIVI*-4bit XQUANT-8bit KIVI*-3bit KIVI*-2bit XQUANT-4bit XQUANT-3bit 1.00 0.27 0.26 0.20 0.14 0.13 0. 5.47 5.49 5.47 5.58 6.42 5.54 6.65 7. 7.30 7.26 7.40 8.46 7.36 8.65 4.88 4.90 4. 4.97 5.61 4.94 5.22 6.73 6.75 6.73 6. 7.67 6.79 7.07 Method KV Llama-3.1-8B Mistral-7B (GQA) Wiki2 C4 Wiki2 C4 Baseline KIVI*-4bit XQUANT-4bit KIVI*-3bit XQUANT-3bit KIVI*-2bit XQUANT-2bit 1.00 0.27 0.27 0.20 0.20 0.14 0. 6.24 6.31 6.28 6.59 6.43 9.95 7.74 9.54 9.66 9. 10.13 9.89 15.98 12.27 5.32 5.34 5.33 5.43 5.39 6.36 5. 8.47 8.51 8.49 8.62 8.57 9.88 9.13 [23, 15]. We use group size of 128 for all quantization experiments. For generative tasks, in order to be able to leverage per-channel quantization during decoding, we leave the final residual tokens unquantized (up to group size tokens), similar to the residual method in [23]. We adopt this residual method across all generation experiments for fair comparison."
        },
        {
            "title": "4.1 Main Results",
            "content": "In Table 1, we report perplexity results for Llama-2-7B/13B, Llama-3.1-8B, and Mistral-7B models on WikiText-2 and C4. Perplexity has been measured using teacher forcing with the output logits of all input tokens. For XQUANT applied to MHA models, we apply per-token quantization to X, whereas for KIVI*, we follow [23, 15] and we apply per-channel quantization to the pre-RoPE Keys and per-token quantization to the Values. For XQUANT applied to GQA models, we cache XUk and XUv, and we find that applying per-channel quantization to the latent XUk and per-token quantization to the latent XUv is the best configuration. Overall, we find that XQUANT not only substantially outperforms KIVI* for the same memory footprint, but it also retains accuracy very close to the FP16 baseline for great memory savings. For the same memory footprint, XQUANT achieves 0.88 and 0.67 less perplexity degradation compared to KIVI* for the Llama-2-7B and Llama-2-13B models, respectively. Relative to the FP16 baseline, XQUANT achieves under 0.1 perplexity degradation, while using 7.7 less memory. Similarly, for GQA models, XQUANT pushes the boundaries of 2-bit quantization by achieving up to less than 2.2 perplexity degradation compared to KIVI* on Llama-3.1-8B. For Mistral-7B, relative to the FP16 baseline, XQUANT achieves < 0.1 perplexity degradation in 3-bit with 5 memory savings, and only 0.01 perplexity degradation in 4-bit with 3.7 memory savings. In 2-bit precision, XQUANT achieves 0.57 less perplexity degradation compared to KIVI*, while getting 7.1 more memory savings relative to the FP16 baseline."
        },
        {
            "title": "4.2 Downstream Task Evaluation",
            "content": "We also provide cross-task evaluation to demonstrate the applicability of our strategy for range of downstream tasks. Table 2 provides evaluation on LongBench, benchmark suite containing range of long-context length tasks including in-context learning, document Q/A, summarization, and coding tasks. We report XQUANT results for the Llama-2-7B-Chat (MHA) and Llama-3.1-8B (GQA) models, and we also provide baseline comparisons against KIVI* [15]. For Llama-2-7B-Chat, XQUANT-4bit attains the same accuracy as KIVI*-4bit and the same accuracy as the baseline, while providing 2 additional memory compression. For Llama-3.1-8B-Instruct, XQUANT provides improved accuracy on average for the same bit width across all precision settings. We also include results on long-generation reasoning tasks to demonstrate the applicability of our method for complex reasoning. Table 3 provides evaluation for the GSM8K dataset [7] (using lm-eval-harness [9]), which evaluates arithmetic reasoning capabilities. We use the chain-of-thought (CoT) configuration, and we report strict match accuracy. We report results for the Llama-2-7B-Chat model, and we also provide baseline comparisons against KIVI* [15]. 11 Table 2: XQUANT evaluation on LongBench. We report results for the (MHA) Llama-2-7B-chat and (GQA) Llama3.1-8B-Instruct models. We include baseline comparisons with KIVI* [23] (details for this configuration are provided in Section 4.1). We report accuracy on each task as well as average cross-task accuracy. We also provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline). Config KV Budget Single-Doc. QA Multi-Doc. QA Summarization Few-shot Learning Code"
        },
        {
            "title": "HPQA",
            "content": "2Wiki"
        },
        {
            "title": "RB LCC",
            "content": "Avg. All KV KIVI*-4bit KIVI*-3bit KIVI*-2bit XQUANT-4bit XQUANT-3bit All KV KIVI*-4bit XQUANT-4bit KIVI*-3bit XQUANT-3bit KIVI*-2bit XQUANT-2bit 1.00 0.27 0.20 0.14 0. 0.10 1.00 0.27 0.27 0.20 0.20 0.14 0.14 19. 22.1 18.7 18.1 13.4 19.0 21.1 21.2 21.0 21.3 11. 16.2 31.2 45.5 30.0 30.8 29.6 30.9 23.7 25. 46.0 46.7 45.8 46.3 36.8 38.3 36.7 37.3 37.3 31.1 36. 35.3 53.8 54.4 55.1 55.4 52.0 41.1 49.6 Llama-2-7B-Chat 31.5 31.9 30.4 23.1 32.6 19.0 8.3 8.4 8. 4.9 8.6 7.3 26.9 27.3 26.7 24.1 26.8 27. Llama-3.1-8B-Instruct 47.1 31.4 34.8 45.5 47.9 43.0 43. 30.0 35.5 31.0 30.5 31.4 30.5 21.9 28.4 34.6 34.9 34.3 34. 30.5 34.0 27.8 28.7 26.0 19.8 27.7 21.4 55. 55.7 54.5 54.0 54.4 44.4 48.6 20.7 21.4 21.2 20.7 21. 20.5 25.3 25.3 25.7 25.2 25.1 24.4 24.8 26. 25.9 26.2 25.2 26.2 25.2 27.5 27.2 27.5 27.4 27. 26.0 27.3 64.0 83.3 41.3 52.4 58.3 37. 64.0 61.0 55.5 66.5 83.0 84.1 70.6 82.4 40.9 40.7 39.6 41. 52.6 57.5 52.0 57.2 48.4 51.6 54.8 60.5 37.1 36.5 32.1 37.5 57.5 77. 38.1 49.7 52.4 32.8 72.5 91.7 43. 56.6 63.4 48.5 74.0 73.0 70.5 73.5 68.0 69.0 90.5 91. 90.7 91.6 87.8 88.7 44.1 43.6 43.5 43.4 44.7 44.7 55.8 64.0 55.9 62. 49.0 61.6 53.9 62.7 38.2 41.0 52.3 57.6 48.4 48.6 47.2 47.9 39.9 44.6 Table 3: XQUANT evaluation on GSM8K. We report results for the Llama-2-7B-chat model. We include baseline comparisons with KIVI* [15] (details for this configuration are provided in Section 4.1). We also provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline). Config All KV KIVI*-4bit KIVI*-3bit KIVI*-2bit XQUANT-4bit XQUANT-3bit Accuracy KV Cache Size (Normalized) 0.132 0.149 0.130 0.092 0.129 0.086 1.00 0.27 0. 0.14 0.13 0.10 We find that XQUANT-4bit attains similar accuracy to KIVI*-3bit, while providing 1.5 additional memory savings, and outperforms KIVI*-2bit while using 2 less memory."
        },
        {
            "title": "4.3 Results with Cross-Layer Compression Method",
            "content": "In Table 4, we provide an evaluation for our XQUANT-CL method, which retains near FP16 accuracy while achieving great memory savings with standard asymmetric uniform quantization. We report results for Llama-2-7B/13, Llama3.1-8B, and Mistral-7B-v0.3. We also provide baseline comparisons against KIVI* [23], which also uses asymmetric uniform quantization, and KVQuant [15], which uses non-uniform, per-vector dense-and-sparse quantization (see Section 4.1 for all configuration details). In our results, we list KVQuant as KVQuant-ebit-o%, where is the quantization bit-width and o% is the percentage of outliers per layer stored in sparse, high precision format. We use an outlier threshold of 1%, which is the best configuration shown in [15]. Note that for KIVI*, XQUANT, and XQUANT-CL, we keep the first 3 layers in higher precision (4-bit). We find that keeping these early layers in higher precision noticeably mitigates perplexity degradation and also results in comparable memory footprint with KVQuant, which uses additional memory to keep outliers in higher precision. This is in keeping with the findings of [17] which showed that the first few layers of network with residual connections do substantial representation 12 Table 4: XQUANT evaluation using perplexity (PPL) on WikiText-2 and C4 for (MHA) Llama-2-7B/13B and (GQA) Llama-3.1-8B and Mistral-7B models. We provide KV cache size estimates (normalized to the KV cache size of the FP16 baseline). Note that the first 3 layers are quantized in 4-bit for KIVI*, XQUANT, and XQUANT-CL. This is done so that these methods have comparable memory footprint to KVQuant, which stores additional memory for outliers. Method KV Wiki Llama-2-7B C4 (MHA) Llama-2-13B C4 Wiki2 KV Llama-3.1-8B C4 Wiki2 (GQA) Mistral-7B C4 Wiki baseline 1.00 5.47 KIVI*-4bit 0.27 KVQuant-4bit-1% 0.27 0.13 0.13 XQUANT-4bit XQUANT-CL-4bit KIVI*-3bit 0.21 KVQuant-3bit-1% 0.21 0.10 0.10 XQUANT-3bit XQUANT-CL-3bit KIVI*-2bit 0.15 KVQuant-2bit-1% 0.15 0.08 0.08 XQUANT-2bit XQUANT-CL-2bit 5.49 5.49 5.54 5.48 5.57 5.56 6.10 5.48 6.20 5.99 11.21 5.57 7.26 7.30 7.28 7.36 7.27 7.39 7.36 8.20 7. 8.22 7.83 15.27 7.39 4.88 4.90 4.90 4.94 4.89 4.96 4.96 5.10 4.92 5.46 5.34 7.00 5.11 6. 6.75 6.75 6.79 6.74 6.82 6.81 6.95 6.78 7.49 7.23 10.25 7.09 1.00 6.24 0.27 0.27 0.27 0. 0.21 0.21 0.21 0.21 0.15 0.15 0.15 0.15 6.30 6.30 6.31 6.29 6.55 6.48 6.43 6.32 8.72 7.45 7.38 6.60 9. 9.65 9.63 9.65 9.61 10.05 9.90 9.87 9.67 13.54 11.49 11.54 10.15 5.32 8.48 5.34 5.34 5.33 5. 5.41 5.41 5.37 5.34 6.14 5.87 5.69 5.46 8.50 8.50 8.50 8.49 8.61 8.58 8.57 8.51 9.59 9.10 9.01 8.67 learning (large transformations of the input) whereas later layers apply small iterative refinements to the input. Thus, the deltas for the first few layers are harder to quantize, whereas the deltas for the remaining layers of the network are much easier to quantize in low-bit precision. For XQUANT-CL, we use the third layer as the higher-precision base layer which becomes the accumulator. For Llama-2-7B, relative to the FP16 baseline, XQUANT-CL results in only 0.1 perplexity degradation in 2-bit with 12.5 memory savings, and only 0.01 perplexity degradation in 3-bit with 10 memory savings. Compared to KVQuant-2bit-1%, XQUANT-CL-2bit results in 0.42 less perplexity degradation while using 1.9 less memory. Similarly for Llama-2-13B, XQUANT-CL-2bit achieves 0.23 less perplexity degradation compared to KVQuant-2bit1% on WikiText-2 while using 1.9 less memory. On Llama-3.1-8B, in 2-bit precision, XQUANT-CL retains perplexity of 10.15 on C4, which is more than 3 points less than KIVI*-2bit (13.54) and more than 1 point less than KVQuant-2bit-1% (11.49). On Mistral-7B, XQUANT-CL-2bit saves 6.7 memory while only facing perplexity degradation of 0.14 compared to the FP16 baseline on WikiText-2. Impressively, for only perplexity degradation of 0.12 relative to KVQuant-4bit-1% on WikiText-2, XQUANT-CL-2bit uses 1.8 less memory than KVQuant-4bit-1%. Overall, XQUANT-CL outperforms state-of-the-art KV cache quantization methods for ultra low precision bit widths, achieving near FP16 accuracy with 1.5 2 memory savings over KV cache quantization and 6 12 memory savings over the FP16 baseline."
        },
        {
            "title": "5 Conclusion",
            "content": "As compute capabilities continue to outpace memory bandwidth and capacity on modern GPU hardware, LLM inference is increasingly becoming more memory-bandwidth bound. In light of this hardware scaling trend, natural strategy is to perform additional compute operations in order to reduce memory requirements. In this work, we adopt forward-looking vision to speed up inference of LLMs by exploiting the compute scaling trends on newer generation hardware. Specifically, LLM inference is typically memory-bandwidth bound due to loading the large KV cache when generate each token. To address this, we aim to reduce the memory requirements for LLM inference through reducing the size of the KV cache activations in exchange for higher computation cost, thus reducing the number of memory operations needed to generate each token and speeding up inference. We propose XQUANT, which quantizes the layer input activations in order to reduce memory consumption by 2 relative to KV caching, and rematerializes the Keys and Values on-the-fly during inference. We then extend our basic method and propose XQUANT-CL, which 13 exploits the cross-layer compressibility of the embeddings between successive layers. We find that using simple uniform quantization, XQUANT and XQUANT-CL surpass state-of-the-art KV cache quantization methods like KVQuant that use non-uniform quantization and dense-and-sparse quantization, while also retaining accuracy close to the FP16 baseline. Relative to the FP16 baseline, XQUANT achieves under 0.1 perplexity degradation while also using 7.7 less memory for the Llama-2-7B and Llama-2-13B models. XQUANT-CL achieves 12.5 memory savings with only 0.1 perplexity degradation in 2-bit precision, and 10 memory savings with only 0.01 perplexity degradation in 3-bit precision compared to the FP16 baseline. Both XQUANT and XQUANT-CL reduce perplexity degradation by several points compared to KIVI* and KVQuant low-bit precision quantization. With the growing discrepancy between compute and memory capabilities on hardware platforms, rematerialization methods like XQUANT can help exploit the available computation in order to accelerate memory bandwidth-bound LLM inference, while also retaining near FP16 accuracy even in low-bit precision."
        },
        {
            "title": "Limitations",
            "content": "Our work focuses on reducing the memory requirements for LLM inference by quantizing the input embeddings and rematerializing KV cache activations. While this approach allows for aggressive memory compression with minimal accuracy loss, it requires additional compute operations to perform rematerialization, which may increase latency on particular hardware platforms. Additionally, XQUANT-CL reduces the memory capacity requirements, but it requires additional compute and memory operations in order to rematerialize KV cache activations due to having to load the accumulator. However, in memory-constrained scenarios where the goal is to attain near FP16 accuracy, XQUANT-CL can be an optimal choice."
        },
        {
            "title": "Acknowledgements",
            "content": "We acknowledge gracious support from the FuriosaAI team including Jihoon Yoon, Kevin Galim, Heeju Kim, and Hyung Il Koo, as well as from Intel, Apple, NVIDIA, and Mozilla. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research. Furthermore, we appreciate support from Google Cloud, the Google TRC team and Prof. David Patterson. Prof. Keutzers lab is sponsored by the Intel corporation, UC Berkeley oneAPI Center of Excellence, Intel VLAB team, as well as funding through BDD and BAIR. MWM would also like to acknowledge DARPA, DOE, NSF, and ONR. This work was supported in part by the Director, Office of Science, Office of Advanced Scientific Computing Research, of the U.S. Department of Energy under Contract No. DE-AC0205CH11231. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred."
        },
        {
            "title": "References",
            "content": "[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. [2] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [4] Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, and Mohamed Abdelfattah. xkv: Cross-layer svd for kv-cache compression. arXiv preprint arXiv:2503.18893, 2025. [5] Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, and Kai-Chiang Wu. Palu: Compressing kv-cache with low-rank projection. In Proceedings of International Conference on Learning Representations (ICLR), April 2025. 14 [6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [9] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. [10] Shiwei Gao, Youmin Chen, and Jiwu Shu. Fast state restoration in llm serving with hcache. In Proceedings of the Twentieth European Conference on Computer Systems, pages 128143, 2025. [11] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. IEEE Micro, 2024. [12] Nils Graef and Andrew Wasielewski. Slim attention: cut your context memory in half without lossk-cache is all you need for mha. arXiv preprint arXiv:2503.05840, 2025. [13] Insu Han, Praneeth Kacham, Amin Karbasi, Vahab Mirrokni, and Amir Zandieh. Polarquant: Quantizing kv caches with polar transformation. arXiv preprint arXiv:2502.02617, 2025. [14] Yefei He, Luoming Zhang, Weijia Wu, Jing Liu, Hong Zhou, and Bohan Zhuang. Zipcache: Accurate and efficient kv cache quantization with salient token identification. Advances in Neural Information Processing Systems, 37:6828768307, 2024. [15] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:12701303, 2024. [16] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedings of Machine Learning and Systems, 2:497511, 2020. [17] Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. In International Conference on Learning Representations, 2018. [18] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [19] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference. In Architecture and System Support for Transformer Models (ASSYST @ISCA 2023), 2023. [20] Sehoon Kim, Coleman Richard Charles Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. In International Conference on Machine Learning, pages 2390123923. PMLR, 2024. [21] Sanghyeon Lee, Hongbeen Kim, Soojin Hwang, Guseul Heo, Minwoo Noh, and Jaehyuk Huh. Efficient llm inference with activation checkpointing and hybrid caching. arXiv preprint arXiv:2501.01792, 2025. 15 [22] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact. In Findings of the Association for Computational Linguistics ACL 2024, pages 77167741, 2024. [23] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In International Conference on Machine Learning, pages 3233232344. PMLR, 2024. [24] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [25] Meta. Llama 3.1: https://ai.meta.com/blog/meta-llama-3-1, 2024. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints, 2019. [27] Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding. In The Thirteenth International Conference on Learning Representations. [28] Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. Eigen attention: Attention in low-rank space for kv cache compression. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1533215344, 2024. [29] Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki: Low-rank keys for efficient sparse attention. Advances in Neural Information Processing Systems, 37:1669216723, 2024. [30] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [31] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [32] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, 16 Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025. [33] Rishabh Tiwari, Haocheng Xi, Aditya Tomar, Coleman Hooper, Sehoon Kim, Maxwell Horton, Mahyar Najibi, Michael Mahoney, Kurt Keutzer, and Amir Gholami. Quantspec: Self-speculative decoding with hierarchical quantized kv cache. In International Conference on Machine Learning, 2025. [34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 59986008, 2017. [37] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):6576, 2009. [38] Songhao Wu, Ang Lv, Xiao Feng, Yufei Zhang, Xun Zhang, Guojun Yin, Wei Lin, and Rui Yan. Polarquant: Leveraging polar transformation for efficient key cache quantization and decoding acceleration. arXiv preprint arXiv:2502.00527, 2025. [39] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations. [40] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International conference on machine learning, pages 1052410533. PMLR, 2020. [41] Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, and Xiaokang Yang. Recalkv: Low-rank kv cache compression via head reordering and offline calibration. arXiv preprint arXiv:2505.24357, 2025. [42] Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan Duan, and Ruofei Zhang. Elattention: Memory efficient lossless attention for generation. In International Conference on Machine Learning, pages 1164811658. PMLR, 2021. [43] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [44] June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, and Dongsoo Lee. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024. 17 [45] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. [46] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 1238112392, 2019. [47] Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and Yelong Shen. arXiv preprint Lorc: Low-rank compression for llms kv cache with progressive compression strategy. arXiv:2410.03111, 2024. 18 Prefill for XQUANT-CL Here we include visualization of the prefill phase of inference for XQUANT-CL. In particular, we show how the deltas between the embeddings for successive layers are calculated, quantized, and cached. Note that besides Layer 0s input X0, the input for all other layers is an approximation calculated using the sum of X0 and the previous quantized deltas. Figure A.1: Illustration of XQUANT-CL algorithm during prefill. Besides Layer 0, the input to all other layers is cross layer approximation, computed using the deltas of all previous layers and the input of Layer 0. The input of Layer 0 is summed with each layers delta so it can be treated as an accumulator, allowing us to avoid loading all 1 deltas to compute Layer Ns X. After layer is done processing, the input embeddings to the layer for all tokens in the sequence are subtracted from the output activations of the layer (the same shape as the input embeddings), and this delta is quantized and cached as ˆX."
        },
        {
            "title": "B Observed Outlier Property When Applying XQUANT for GQA Models",
            "content": "Σ kBT and UvΣvBT Recap As reminder, we decompose the Key and Value projection matrices Wk and Wv using the SVD to get respectively. We down-project by Uk and Uv and quantize and cache these latent distribuUk tions, while also fusing Σ kBT into another weight matrix, both of which respectively multiply Uk and Uv to rematerialize the Keys and Values. into one weight matrix and ΣvBT Observed Outlier Property When we project by Uk, we observe an interesting property of these distributions across all layers for all models, where Uk transforms such that all outliers are grouped on the first channel of the resulting matrix. Although we observe this phenomenon for both MHA and GQA models, we discuss it here in the context of GQA models. This is because down-projecting by Uk gives us memory savings and is relevant to the XQUANT algorithm for GQA models, whereas for MHA models, we simply cache itself. We visualize the X, XUk, and XUv distributions for the Llama-3.1-8B and Mistral-7B models in Figures B.2 and B.3 respectively. For each model, we visualize the distributions for 3 different layers on the WikiText-2 and C4 datasets to ensure that this observed property cannot simply be attributed to specific dataset, model, or layer. Note that these distributions do 19 , d Figure B.1: RLd, where is the sequence length, and is the hidden dimension. Uk Rd and Σ where is the group size used by GQA models. Note that σ0 σ1 σ > 0. We find that XUk has massive outliers in the first channel across all layers for different models on different datasets (see Figures B.2, B.3). Multiplying XUk by Σ preserves the ordering of outlier channels (first channel still has the largest magnitude outliers). Σ When multiplying XUk k, as highlighted in red above. The top-k largest magnitude values from this first row can be used to identify which channels in the Keys are outlier channels. This can be achieved offline without any calibration data, and simply by inspecting the weights of BT . interacts with the first outlier channel of XUk , the first row of BT by BT Σ not include the scaling term Σ from the SVD they only visualize each of the transformations that the left singular vectors matrices Uk and Uv with orthonormal columns apply on the input X. As mentioned in 3.3.1, we notice pattern in the XUk distributions across all layers of both models, where the first channel has massive outliers across all tokens, whereas all other channels are much smaller in magnitude. We do not, however, observe any similar interesting quality for the XUv distributions. This observation implies that the activation embeddings for all tokens have high cosine similarity with the first column vector of Uk, which is also the left singular vector associated with the largest singular value of Wk. In other words, the activation embeddings for all tokens roughly lie close to the direction where Wk applies maximum scaling. We attempt to exploit this finding by keeping the first outlier channel of XUk in FP16, while quantizing the remaining channels using per-channel quantization with group-size 128 as before. We use the same setup as discussed in Section 4. These results are shown in Table B.1. The data for KIVI* and XQUANT have been duplicated from Table 1 to serve as reference point. We find that keeping the outlier channel in FP16 gives non-trivial benefits for 2-bit quantization. For instance, Llama-3.1-8B sees an improvement of 0.2 perplexity on C4 in 2-bit precision. Connection to KV cache quantization methods Previous KV cache quantization methods such as KVQuant [15] preserve outlier values in the Keys in higher precision to reduce accuracy degradation from quantization. Since the Keys have been observed to have distinct outlier channels (see Figure 3), determining which outliers to preserve in the entire Keys matrix requires finding these outlier channels [23, 15]. To identify these outlier channels, methods like [15] run models on calibration datasets, which allows them to note which channels tend to be outliers across diverse data samples. Having observed the outlier behavior of the first channel in XUk, we explore if outlier channels in the Keys can be identified by inspecting the SVD decomposition of the Wk matrix offline without the need to run calibration. We follow this line of reasoning: XUk is an ordered distribution where the first channel (i.e., first column) is associated with the largest singular value of Wk, the second channel is associated with the second largest singular value, and so on. If we multiply XUk by Σ k, the first channel in XUk gets scaled by the largest singular value σ0, the second channel in XUk gets scaled by the second largest singular value σ1, and so on. This matrix multiplication is visualized in Figure B.1. Thus, XUk Σ preserves the ordering of the outlier channels (i.e., the first channel in XUk is still the Σ outlier channel). Finally to complete the rematerialization of Keys, (XUk , which necessarily distributes the outliers in the first channel to other channels. These are the channels that KV cache quantization k) is multiplied by BT 20 Table B.1: We attempt to exploit the structured distribution of XUk by keeping the first outlier channel in FP16. The table shows XQUANT evaluation using perplexity on WikiText-2 and C4 on Llama-3.1-8B and Mistral-7B. We duplicate the KIVI* and XQUANT results from Table 1 to serve as reference point. For 2-bit precision, keeping the first channel in FP16 results in some perplexity improvement. Method Baseline KIVI*-4bit XQUANT-4bit XQUANT-4bit-FP16-outlier-channel KIVI*-3bit XQUANT-3bit XQUANT-3bit-FP16-outlier-channel KIVI*-2bit XQUANT-2bit XQUANT-2bit-FP16-outlier-channel KV Llama-3.1-8B Mistral-7B (GQA) Wiki C4 Wiki2 C4 1.00 0.27 0.27 0.27 0.20 0.20 0. 0.14 0.14 0.14 6.24 6.31 6.28 6.28 6.59 6.43 6.42 9.95 7.74 7.63 9. 9.66 9.60 9.60 10.13 9.89 9.87 15.98 12.27 12.05 5.32 8.47 5.34 5.33 5. 5.43 5.39 5.38 6.36 5.79 5.75 8.51 8.49 8.49 8.62 8.57 8.56 9.88 9.13 9.08 methods try to identify in the Keys distribution via running calibration. However, we hypothesize that the column in BT whose first element has the largest magnitude compared to the first element of all other columns will also tend to be the column of the Keys containing the outlier channel. For example, in Figure B.1, the second column of BT is shown to contain bmax, which is the largest magnitude value in the first row of BT . This means that the second column of the Keys will tend to be the outlier channel. This is because the values in the first row vector of BT k) when performing are the scalars which interact with the first outlier channel in (XUk matrix multiplication. So, the column in the first row vector of BT with the largest scalar that \"hits\" the first column in k) will result in the largest values in the Keys. In Figure B.1, the largest outlier channel σ0Xu0 is highlighted (XUk in red, and the first row vector in BT containing the scalars which multiply this outlier channel are also highlighted in red. Σ Σ However, there are two cases in which the above hypothesis does not hold: 1) other row vectors in BT contain scalars which cause another channel that is not the first channel in XUk to blow up in magnitude and 2) the largest scalar of BT first row vector results in most values of the outlier channel having an opposite sign to the next largest outlier channel of (XUk k), causing their sum to have smaller magnitude. Nevertheless, these scenarios are less likely given that the other channels are themselves much smaller in magnitude than the first channel, so it is unlikely that 1) any other channel could blow up to surpass the first outlier channel in magnitude and 2) adding the first outlier channel to another channel whose elements are the opposite sign will impact the first outlier channel much. Σ We test this hypothesis by using the above method to determine the outlier channels in the Keys, and then check how many of these predictions are correct by comparing with the ground truth. For the ground truth, we pick the channel in the Keys matrix which has the largest average magnitude. As there can be multiple outlier channels in the Keys, we choose the column indices for the top-k largest magnitude values in the first row of BT as our predictions. We then simply check if the ground truth index for the outlier channel in the Keys appears in any of the predicted indices by the above method. We mark the predictions as correct if any one of them contain the ground truth index, and we report the final accuracy as the percentage of correct predictions across the Keys for all layers of the model. We test this method on two different datasets, WikiText-2 and C4, to ascertain whether this weights-only based analysis is robust to different data. We also test on the Llama-3.1-8B and Mistral-7B models to check the methods efficacy across different models. Results are listed in Table B.2. We find that for Llama-3.1-8B, only 8 of the largest magnitude values of the first of BT are needed to determine the outlier channel for the Keys. For Mistral-7B, using the top-8 largest magnitude values attains 96.88% accuracy. Moreover, for both models, the accuracy is consistent across datasets, demonstrating the robustness of this method to determine the outlier indices for the Keys. 21 Table B.2: Percentage of outlier channels predicted correctly by only analyzing the top-k values of the weight matrix BT offline, without any calibration data. For Llama-3.1-8B, only 8 of the largest magnitude values of the first row of BT are needed to determine the outlier channel for the Keys. top-k Llama-3.1-8B Mistral-7B WikiText2 C4 WikiText C4 k=1 k=2 k=4 k=8 66.14% 71.15% 75.35% 71.91% 72.08% 75.08% 87.55% 83.48% 87.71% 90.62% 93.75% 93.75% 100% 100% 96.88% 96.88% 22 Figure B.2: (left), XUk (middle), and XUv (right) distributions for Llama-3.1-8B on samples from WikiText-2 (top) and C4 (bottom). Figure B.3: (left), XUk (middle), and XUv (right) distributions for Mistral-7B on samples from WikiText-2 (top) and C4 (bottom)."
        }
    ],
    "affiliations": [
        "KAIST",
        "Lawrence Berkeley National Laboratory",
        "UC Berkeley",
        "University of California, Berkeley"
    ]
}