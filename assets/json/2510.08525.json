{
    "paper_title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
    "authors": [
        "Wenjie Du",
        "Li Jiang",
        "Keda Tao",
        "Xue Liu",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results."
        },
        {
            "title": "Start",
            "content": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression WHICH HEADS MATTER FOR REASONING? RL-GUIDED KV CACHE COMPRESSION Wenjie Du1 Li Jiang2,3 Keda Tao1,4 Xue Liu2,5 Huan Wang1, 1 Westlake University https://kurt232.github.io/RLKV_Web 2 McGill University 4 Zhejiang University 3 Mila 5 MBZUAI 5 2 0 2 9 ] . [ 1 5 2 5 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each heads cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advanced reasoning large language models (LLMs) (Jaech et al., 2024; Team et al., 2025; Guo et al., 2025; DeepMind, 2025) exhibit complex reasoning behaviors, such as self-reflection to revisit previous steps and exploration of alternative approaches, and achieve revolutionary performance on challenging mathematical and coding problems. However, this breakthrough creates an unprecedented memory bottleneck: the extension of chain-of-thought (CoT) reasoning generates significantly more tokens compared to conventional instruct models. For instance, Llama-3.1-8B-R1 (BF16) requires 16GB additional GPU memory for 32k CoT generation with single query, primarily due to quadratic attention computation and expanding KV cache. This makes batch processing nearly impossible and fundamentally limits the practical deployment of reasoning models. Key-Value (KV) cache compression methods have demonstrated effectiveness for instruct models in long-context scenarios. These methods adopt one of two strategies: token dropping or head reallocation. Token-dropping methods selectively evict less important tokens from each head (Zhang et al., 2023; Li et al., 2024; Cai et al., 2025; Yang et al., 2024b; Qin et al., 2024), while head-reallocation methods identify critical heads and allocate full KV cache to them while applying compressed KV cache to others. However, as shown in Figure 1 (left), both categories degrade significantly when applied to reasoning models while maintaining stable performance on their instruct counterparts. This performance degradation correlates strongly with generation length: in the MBPP (Austin et al., 2021) coding task, both model variants achieve nearly identical uncompressed performance, yet the reasoning variant generates 3341 tokens on average-8x longer than the 439 tokens of the instruct variant. This controlled comparison isolates extended CoT generation as the primary cause of compression challenges rather than differences in model capability, revealing the inherent difficulty in Corresponding authors: wanghuan@westlake.edu.cn 1 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Figure 1: Motivation. Left: Existing KV cache compression methods underperform on reasoning models. Token-dropping and head-reallocation methods maintain relatively stable performance on Llama-3.1-8B-Inst but drop substantially on Llama-3.1-8B-R1, due to the 8x longer generation sequences in the reasoning models (MBPP results shown). Right: Failure modes: Token-dropping methods degenerate to repetitive behavior due to dropping critical tokens, while head-reallocation methods generate unnecessary steps, suggesting reasoning process degradation. See Appendix A.1 for complete results. compressing long reasoning sequences. This incompatibility stems from KV caches fundamental role shift in reasoning models: rather than mere computational optimization, it becomes the carrier of reasoning behavior itself, storing critical states for CoT consistency and self-reflection, making compression inherently detrimental to reasoning performance. To understand how existing methods underperform in preserving reasoning behaviors, we analyze their specific error modes as compression rates increase, as illustrated in Figure 1 (right). Models with token-dropping methods lose reasoning behaviors, as they inevitably discard reasoning-critical information, disrupting the CoT consistency and leading to loops with repeated tokens. Even recent R-KV approach (Cai et al., 2025), designed specifically for reasoning models, cannot escape this inherent limitation. In contrast, models with head-reallocation methods relatively maintain reasoning behaviors but tend to generate useless steps up to maximum length, suggesting going astray in the reasoning process. This reveals that head-reallocation methods relatively preserve sequence information integrity in some heads by allocating full KV cache for them while compressing others (Xiao et al., 2023). However, they may mistakenly compress heads critical for reasoning behaviors, since their head identification targets retrieval heads (Wu et al., 2024). These methods rely on static patterns from prefill attention (Fu et al., 2024; Tang et al., 2024a) or single-forward-pass training (Xiao et al., 2024; Bhaskar et al., 2025), inherently failing to capture dynamic reasoning behaviors that emerge during extended CoT sequences. These findings motivate our key insight that KV heads exhibit functional heterogeneity in reasoning models, where subset of heads are critical to reasoning behaviors and naturally require full KV cache to maintain CoT consistency. We term such heads with this role as reasoning heads. To validate and exploit this insight, we propose RLKV, novel reasoning heads identification framework, which employs reinforcement learning (RL) to identify those heads by directly optimizing the relationship between the allocation of each heads KV cache usage and reasoning quality. As illustrated in Figure 2, our method observes reasoning behaviors in generated samples and assigns rewards during RL training. These reward signals guide RL with sparsity pressure to optimize learnable gating adapters that control the mixing of full attention and local attention (Xiao et al., 2023). The gating adapters quantify each heads reliance on full versus local KV cache access, with L1 penalty encouraging sparsity. Through this RL optimization, the adapter values inherently distinguish reasoning heads from compressible heads, directly identifying which heads are essential for reasoning behaviors. In this way, our method consequently identifies reasoning heads and allocates full KV cache to them while applying compressed constant KV cache to others, effectively preserving reasoning behaviors during KV cache compression. Our work makes three main contributions. First, to our knowledge, we are the first to systematically identify which heads matter for reasoning, introducing the concept of reasoning heads that are 2 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Figure 2: Overview of RLKV: Our method proposes to utilize RL to identify reasoning heads. The RL pipeline naturally captures reasoning behaviors, since it samples the current models generations to produce reward signals. The reward function evaluates the samples to assess reasoning quality. We employ learnable gating adapters to mix full attention and local attention for each head, quantifying each heads reliance on full versus local KV cache access. We apply an L1 penalty to encourage adapter sparsity, while RL optimizes the adapters to preserve reasoning behaviors. After training, we identify reasoning heads with high adapter values and allocate full KV cache to them while applying compressed KV cache to others for efficient inference. functionally distinct from retrieval heads. Second, we achieve state-of-the-art compression performance, enabling lossless reasoning capability with 20-50% KV cache usage reduction across diverse reasoning tasks and models. Third, through controlled masking experiments that selectively remove top-critical heads, we demonstrate that reasoning heads are significantly more critical than retrieval heads, with their removal causing substantially greater performance degradation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Efficient LLM Inference. Various techniques reduce KV cache overhead through architectural or system optimizations. Grouped-Query Attention (GQA) (Ainslie et al., 2023) and Multi-head Latent Attention (MLA) (Liu et al., 2024a) reduce the number of KV heads by sharing them across query heads, achieving significant memory reduction but requiring expensive pre-training from scratch. Linear attention methods (Gu & Dao, 2023; Yang et al., 2025) maintain constant memory usage during inference by avoiding the quadratic attention computation, but exhibit reduced modeling capacity compared to standard transformer architectures. KV cache quantization (Liu et al., 2024b; Tao et al., 2025; Hooper et al., 2024; Duanmu et al., 2024; Su et al., 2025; Yue et al., 2024) and system-level optimizations, such as paged KV cache (Kwon et al., 2023), KV cache reuse (Zheng et al., 2024), and sparsely loading KV cache (Tang et al., 2024b), provide orthogonal improvements by reducing the precision or optimizing the storage/retrieval of cached states. While valuable, these methods treat KV cache as opaque data without exploiting the inherent sparsity patterns. KV Cache Compression. Recent works mainly exploit sparsity in long-context scenarios for instruct models, including token-dropping and head-reallocation methods. (1) Token-dropping methods (Zhang et al., 2023; Li et al., 2024; Cai et al., 2025; Yang et al., 2024b; Qin et al., 2024) apply eviction strategies across all heads or intra-layer heads based on attention scores. H2O (Zhang et al., 2023) maintains important tokens KV cache based on accumulated attention scores plus sliding window for recent tokens. Specifically, recent R-KV (Cai et al., 2025), designed for reasoning models, primarily adds similarity-based clustering to priority evict redundancy tokens KV cache during both prefill and decoding phases. However, they inevitably discard reasoning-critical information and disrupt the CoT consistency as compression rates increase. (2) head-reallocation methods (Fu et al., 2024; Tang et al., 2024a; Xiao et al., 2024; Bhaskar et al., 2025) maintain full KV cache only for identified retrieval heads (Wu et al., 2024) in long-context scenarios while applying compressed KV cache (Xiao et al., 2023) to others. Ada-KV (Fu et al., 2024) and RazorAttention (Tang et al., 2024a) use proxy metrics of attention scores, while DuoAttention (Xiao et al., 2024) and PruLong (Bhaskar et al., 2025) are learning-based methods for head identification. DuoAttention minimizes single-forward output deviation on synthetic long-context recall task, while PruLong uses nexttoken loss on long-context pre-training corpora. However, these methods do not capture the rea3 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression soning behaviors that emerge during dynamically extending CoT generation, resulting in degraded reasoning performance as compression rates increase. Reinforcement Learning for Efficiency. RL has proven effective in Neural architecture search (Zoph & Le, 2017; Zoph et al., 2018), where it treats architecture choices as sequential decisions, and model pruning (He et al., 2018), where it learns layer-wise pruning ratios that maximize accuracy under resource constraints. However, the limitation is the high computational cost due to the large optimization space. Our work utilizes gating values assigned to each KV head to reduce the optimization space and make RL feasible and efficient. For reasoning language models, recent works apply RL tuning to mitigate overthinking (Hou et al., 2025; Liu et al., 2025) by learning to reduce CoT length while maintaining reasoning capability, thereby indirectly decreasing KV cache requirements. Our work is orthogonal to these methods, employing lightweight RL training to identify reasoning heads that guide KV cache compression while preserving reasoning capability."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we present RLKV, novel reasoning heads identification framework to guide efficient KV cache compression for reasoning LLMs, as illustrated in Figure 2. These identified reasoning heads are essential for reasoning behaviors, while others are compressible. To achieve this, we first use mixed attention with gating adapters to quantify each heads reliance on complete or compressed KV cache usage. Then we apply RL with sparsity pressure to optimize the gating adapters based on verifiable reward signal, naturally capturing reasoning behaviors. Finally, we introduce two complementary stabilization techniques to address the conflict between dense regularization and sparse rewards as the sparsity of adapters increases. 3.1 MIXED ATTENTION WITH GATING ADAPTERS Identifying reasoning heads requires estimating individual KV heads robustness of complete KV cache usage; therefore, we build upon mixed attention (Xiao et al., 2024), which uses lightweight gating adapters to quantify each heads reliance on full versus local KV cache access. Specifically, it combines two attention modes by attention mask, including full attention mapping to the full KV cache, and streaming attention (Xiao et al., 2023) mapping to the constant KV cache size containing initial sink tokens and recent tokens. The mixed attention on each head can be formulated as: out mix attni,j = αi,j out full attn + (1 αi,j) out streaming attn, (1) where α [0, 1]LH represents the learnable gating parameters for layers and heads, with αi,j represents the weight assigned to full attention on the j-th head in the i-th layer. This design dramatically reduces the optimization space to only gating parameters by freezing all LLM parameters, making it feasible to apply RL for identifying reasoning heads. 3.2 RL FOR REASONING HEAD IDENTIFICATION Reasoning LLMs are often post-trained using reinforcement learning with verifiable reward (RLVR) (Guo et al., 2025; Team et al., 2025), which enhances reasoning capabilities by evaluating generated samples based solely on final answer correctness. During this RL training process, reasoning behaviors are naturally exhibited in the sampled CoT sequences, while reward signals directly reflect reasoning quality. These two characteristics make RLVR ideal for reasoning heads identification. In concrete, we optimize the gating adapters α using Group Relative Policy Optimization (GRPO) (Shao et al., 2024) on mathematical reasoning problems with two key modifications. First, to maximize the discriminative power of reward signals for reasoning head identification, we remove the KL penalty that conventionally limits reward signal strength to prevent overoptimization. Second, we apply L1 regularization (Tibshirani, 4 Figure 3: Gating adapter distribution after RLKV training on two models, which both are GQA architecture. Which Heads Matter for Reasoning? RL-Guided KV Cache Compression 1996) to the adapters by incorporating the scaled L1 penalty term βα1/(L H)into the objective function to encourage adapter sparsity. The reward signal preserves high αi,j values for reasoning heads requiring full KV cache access, while the L1 penalty drives αi,j toward 0 for compressible heads. The overall objective is defined to maximize: (cid:88) i=1 1 (cid:124) min (cid:18) πα(oiq) παold (oiq) Ai, clip (cid:18) πα(oiq) παold(oiq) (cid:19) (cid:19) , 1 ϵ, 1 + ϵ Ai (cid:123)(cid:122) reward signal (cid:125) β (cid:123)(cid:122) (cid:124) L1 penalty α1 (cid:125) , (2) where is the input query, {oi}G using group of rewards {r1, r2, , rG} tailored to outputs: i=1 are sampled outputs, Ai is the normalized advantage, computed Ai = ri mean(r1, r2, , rG) std(r1, r2, , rG) . (3) The clipping mechanism with threshold ϵ prevents excessive policy updates, and β controls the regularization strength. The policy πα represents the models generation probability distribution conditioned on the current gating parameters α, and the advantage Ai is positive for outputs leading to correct reasoning and negative for incorrect reasoning. This optimization naturally converges to sparse solution where reasoning heads maintain high α values, as demonstrated in Figure 3 3.3 STABILIZATION FOR RL TRAINING As adapters become increasingly sparse, the mixed attention of reasoning heads degenerates to the streaming attention, severely degrading the models reasoning capacity, as shown in Figure 4. This degradation renders the reward signal increasingly sparse and unstable, while the L1 penalty remains dense across all parameters. This imbalance creates vicious cycle, where degraded performance leads to sparser rewards, making the dense L1 penalty relatively stronger, which further drives adapters toward zero with no recovery capability. To resolve this destructive training dynamic and stabilize the training process, we introduce two complementary techniques that address this challenge from both the reward and penalty perspectives. Figure 4: The conflict of sparse reward versus dense penalty leads to training collapse without our stabilization techniques. As adapters become sparse (decreasing average), model performance degrades (dropping reward), creating vicious cycle where dense L1 penalties dominate increasingly sparse rewards. Self-distillation Sampling. Overly challenging problems during RL training lead to frequent failures and unstable reward signals. In contrast to typical RLVR that utilizes sparse rewards for capability enhancement, our work leverages RL for capability preservation under sparsity constraints. Consequently, we focus on constructing high-quality training data that produces stable reward signals to improve learning efficiency. We construct training data by first filtering all problems the model initially solves correctly, then curating them to 3k using curriculum sampling strategy (Team et al., 2025). We use output token lengths as proxy for difficulty, enabling curriculum control that maintains stable reward signals throughout the training process. See Section 4.1 for training dataset details. Adaptive Penalty Weighting. To address the penalty imbalance, we modulate the scaling weight β of the L1 penalty based on the reward signal. Our design incorporates two protective mechanisms to prevent training collapse. First, we use adaptive scaling centered around target reward of 0.7 to smoothly decay penalty when performance degrades and increase it when performance improves. Second, we implement hard cutoff at threshold τ to completely eliminate regularization when reasoning capability severely degrades. We implement this through dynamic weight that replaces the constant hyperparameter β: β(r, τ ) = I(r > τ ) β (exp(r) 1), = mean(r1, r2, , rG), (4) 5 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Figure 5: Performance comparison of RLKV against KV cache compression baselines across reasoning benchmarks. We evaluate RLKV (Ours) and existing methods on two reasoning models (Llama-3.1-8B-R1 and Qwen-2.5-7B-R1) across four benchmarks (GSM8K, MATH, AIME24, MBPP) at sparsity levels of 0.2, 0.4, 0.6, and 0.8. RLKV consistently outperforms all baselines across different sparsity levels, demonstrating particularly strong advantages at high sparsity levels (0.4 or 0.6) where competing methods suffer significant performance degradation. Complete numerical results are provided in Appendix A.3. where the exponential function (exp(r)1) provides the adaptive scaling, and the indicator function I(r > τ ) provides the hard cutoff based on mean reward in the current group. The end result is set of identified reasoning heads that require full KV cache access, while nonreasoning heads can utilize compressed KV cache access, achieving significant memory compression without sacrificing reasoning capability. During inference, we use the learned gating parameters to rank all KV heads and select the top-k heads with the highest α values to maintain full KV cache access according to the target compression ratio. The remaining heads still use full attention but with compressed KV cache, which retains only initial sink tokens and recent tokens. Refer to Section 4.1 for further details of deployment and inference."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETUPS Models, Datasets, and Baselines. We evaluate RLKV on two mainstream small reasoning models, including Llama-3.1-8B-R1 and Qwen-2.5-7B-R1 (Guo et al., 2025), both are supervised finetuned from respective base models on DeepSeekR1 distilled CoT data (Guo et al., 2025). We conduct experiments on four benchmarks, using three datasets of increasing difficulty mathematical reasoning, GSM8K (Cobbe et al., 2021) for elementary problems, Math500 (Lightman et al., 2023) for intermediate problems and AIME24 (MMA, 2024) for advanced problems, to evaluate performance across difficulty levels, and MBPP (Austin et al., 2021) for Python programming to assess out-of-distribution generalization. We compare our method with KV cache compression approaches including H2O (Zhang et al., 2023) and R-KV (Cai et al., 2025), which are typical token-dropping methods, and DuoAttention (Xiao et al., 2024), which is head-reallocation method. Implementation Details. We implement RLKV by integrating MixedAttention into AReaL (Fu et al., 2025) and SGLang (Zheng et al., 2024). AReaL is an asynchronous distributed RL framework for updating adapters, and AReaL uses SGLang as the generation backend. We optimize gating adapters using GRPO with 4 samples per query and AdamW (Loshchilov & Hutter, 2017) with learning rate 0.01. We filter 3,000 mathematical problems from DeepScaleR (Luo et al., 2025) following our curriculum sampling strategy. During training, local attention uses 128 sink and 256 local tokens; for evaluation, non-reasoning heads use compressed KV cache only with 16 sink and 6 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression 64 local tokens. To ensure fair comparison, we augment all baselines with equivalent token overhead and convert fixed-budget methods to dynamic allocation. Details are provided in Appendix A.2. 4.2 MAIN RESULTS Figure 5 presents the performance of RLKV against baselines across two reasoning models and four benchmarks at sparsity levels of 0.2, 0.4, 0.6, and 0.8. RLKV consistently outperforms all baselines at different levels of sparsity, with particularly strong advantages at high sparsity, such as 0.4 and 0.6, where other methods suffer significant performance degradation. Remarkably, RLKV even surpasses the full KV cache baseline on AIME24, the most challenging mathematical reasoning benchmark, for Llama-3.1-8B-R1 at 0.4 and Qwen-2.5-7B-R1 at 0.2, respectively. This counter-intuitive result suggests that our identified reasoning heads capture the essential components for complex reasoning, while non-reasoning heads may introduce noise that degrades performance when given full KV cache access. Notably, the performance degradation pattern at 0.8 sparsity directly reflects the relationship between reasoning head quantity and capability: as sparsity increases (retaining fewer reasoning heads), performance systematically decreases. This trend demonstrates that complex reasoning fundamentally depends on sufficient number of reasoning heads with full KV cache access, making lossless compression at extreme ratios inherently challenging. 4.3 ANALYSES ON REASONING HEADS VERSUS RETRIEVAL HEADS Head Importance Analyses Figure 6 presents head importance analyses by applying compressed KV cache to different types of heads: reasoning heads identified by RLKV, retrieval heads from DuoAttention, and randomly selected heads. We progressively replace the top fraction of heads with compressed KV cache and evaluate performance degradation. Reasoning heads identified by RLKV demonFigure 6: The importance of heads identified is equivalently strate significantly steeper performance illustrated by replacing the top ratio of them with comdegradation, indicating they are substanpressed KV cache. Compared to retrieval heads and random heads, reasoning heads identified by RLKV are more crucial tially more important than retrieval heads to model performance, and are sensitive to compressed KV and random heads. Combined with the cache access. main results in Figure 5, this reveals an important asymmetry: compressing even small fraction of top reasoning heads causes significant degradation, while maintaining complete capability requires preserving multiple reasoning heads. Qwen-2.5-7B-R1 shows more gradual degradation than Llama-3.1-8B-R1 at low compression ratios (0.1 and 0.2), indicating that its reasoning capability may be more distributed across multiple heads rather than concentrated in few critical ones at these levels. Since Qwen-2.5-7B-R1 achieves stronger reasoning with fewer total heads (112 vs 256), this suggests more efficient utilization of its top reasoning heads, making it more robust to small-scale individual head compression. Error Mode Analyses We analyze the distinct error modes exhibited by models when reasoning heads and retrieval heads guide KV cache compression on the Math500 benchmark. Error modes repetare categorized into three types: itive errors (excessively repeating token sequences), incorrect errors (generating wrong answers), and overlength errors (generating sequences that exceed normal length baselines). Figure 7 reveals that models tend to produce repetitive generation errors when reasoning heads are compressed at higher levels, while models with compressed retrieval heads exhibit more Figure 7: The analysis reveals distinct error modes when reasoning heads versus retrieval heads work with compressed KV cache on Math500 benchmark. Reasoning heads tend toward repetitive generation errors as compression increases, while retrieval heads exhibit more varied error modes across different settings. 7 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Table 1: RLKV achieves near lossless performance (full KV cache) up to the sparsity thresholds shown for Llama-3.1-8B-R1 (a) and Qwen-2.5-7B-R1 (b) across four benchmarks. Red background denotes performance below the fullKV-cache baseline, whereas green background denotes performance above it. RLKV exhibits the smallest performance degradation among the other methods and, on some benchmarks, even improves over the fullKV-cache baseline. For all values, higher is better. The best result of the metric in each benchmark is in bold. All values are reported as percentages. Method Full H2O R-KV DuoAttention RLKV (Ours) Method Full H2O R-KV DuoAttention RLKV (Ours) Lossless Sparsity Threshold GSM8K (Math) 0.4 Math500 (Math) 0.5 AIME24 (Math) 0.4 MBPP (Code) 0. 89.2 67.6 (-21.5) 88.3 (-0.8) 87.1 (-2.0) 88.4 (-0.8) 83.0 8.8 (-74.2) 68.6 (-14.4) 74.6 (-8.4) 85 (+2) 36.7 3.3 (-33.3) 13.3 (-23.3) 20 (-16.7) 40 (+3.3) 62.6 0.6 (-62) 29.4 (-33.2) 60.6 (-2) 63.8 (+1.2) (a) Llama-3.1-8B-R1 Lossless Sparsity Threshold GSM8K (Math) 0.4 Math500 (Math) 0.4 AIME24 (Math) 0.2 MBPP (Code) 0.3 89.1 17.4 (-71.7) 84 (-5.1) 82.0 (-7.1) 90.1 (+1.0) 87.8 13.4 (-74.4) 73.4 (-14.4) 74.2 (-13.6) 86 (-1.8) 43.3 6.7 (-36.7) 30 (-13.3) 26.7 (-16.7) 50 (+6.7) 63.2 2.2 (-61) 34.6 (-28.6) 59.4 (-3.8) 62 (-1.2) (b) Qwen-2.5-7B-R1 varied error modes across different settings. This consistency in reasoning head-related errors suggests their collaborative role in maintaining complex logical states during reasoning, whereas retrieval heads appear to have more multifaceted roles. See Appendix A.4 for more details. 4.4 MEMORY EFFICIENCY To demonstrate RLKVs memory efficiency, we evaluate its compression performance while maintaining accuracy across two reasoning models and four benchmarks, as shown in Table 1 (a) and Table 1 (b). Values show performance with difference from full KV cache in parentheses, where light green indicates performance exceeding the full KV cache baseline and light red indicates performance below it. RLKV consistently outperforms baselines across all sparsity levels, achieving GPU memory reductions of 20-50% with minimal performance degradation across different models and benchmarks. Notably, different reasoning tasks exhibit varying sensitivity to compression, reflecting the heterogeneous and complex mechanisms underlying reasoning head functionality. When generation length exceeds 8k, 16k, or even 32k tokens, RLKV enables deployment on memoryconstrained hardware and allows for higher inference parallelism by reducing memory bottlenecks. 4.5 ABLATION STUDIES We conduct ablation studies using Qwen-2.5-7B-R1 on the Math500 benchmark to assess the impact of adaptive penalty weighting, self-distillation sampling, and base L1 penalty weight in RLKV. Adaptive Penalty Weighting. Figure 8 (left) demonstrates that adaptive penalty weighting significantly enhances performance by breaking the vicious cycle between sparse rewards and dense L1 penalty. Without this mechanism, increasing adapter sparsity leads to degraded reasoning performance, which generates sparser reward signals while the L1 penalty remains dense, creating an imbalance that drives training toward collapse with no recovery capability. 8 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Figure 8: Ablation study on key components of RLKV training framework. We evaluate three critical components using Qwen-2.5-7B-R1 on Math500. Left: Adaptive penalty weighting prevents training collapse by stabilizing conflicting dynamics between sparse rewards and L1 penalty, while its absence leads to ineffective exploration and training failure. Middle: Self-distillation sampling maintains stable reward signals by training on appropriately challenging problems, compared to unstable signals from overly difficult problems. Right: Base L1 penalty weight β = 0.001 achieves optimal sparsity-performance balance, while excessive penalty causes over-compression and insufficient penalty leads to premature convergence. Self-distillation Sampling. Self-distillation sampling provides stable reward signals throughout training, as shown in Figure 8 (middle). In contrast to typical RLVR that utilizes sparse rewards for capability enhancement, our work leverages RL for capability preservation under sparsity constraints. Training on problems suited to the models reasoning capability maintains relatively stable reward signals throughout optimization, while training on overly challenging problems leads to unstable and sparse reward signals that provide weak and insufficient guidance for head identification. Base L1 penalty Weight. The base regularization weight β controls the strength of L1 penalty applied to gating adapters during RL training. Figure 8 (right) shows that moderate β value of 0.001 achieves an optimal balance between sparsity and reward signal strength. Excessive penalty (β = 0.005) dominates the optimization process, weakening reward signals through overcompression, while insufficient penalty (β = 0.0002) fails to induce adequate sparsity, leading to premature convergence with limited exploration of the reward landscape."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose RLKV, novel RL framework for identifying reasoning heads to guide KV cache compression in reasoning models. RLKV directly optimizes the relationship between each heads KV cache usage and reasoning quality through reinforcement learning and we achieve competitive performance at diverse KV cache budget sparsity levels and reduce 20-50% KV cache usage while preserving full reasoning capability across Llama-3.1-8B-R1 and Qwen-2.5-7B-R1 on GSM8K, MATH, AIME24, and MBPP benchmarks. Then we analyze the reasoning heads importance and error modes, revealing the importance and complexity of reasoning heads in reasoning models. RLKV provides new perspective on understanding reasoning models and opens up new avenues for efficient inference of reasoning LLMs."
        },
        {
            "title": "6 FUTURE WORK",
            "content": "RLKV opens several promising avenues for future research. First, the significant variability in reasoning heads distribution across different models and tasks presents an exciting opportunity to develop deeper understanding of the heterogeneous nature of reasoning mechanisms in reasoning LLMs. Second, while RLKV effectively identifies reasoning heads for compression, exploring the complete functional roles of these heads beyond reasoning could unlock new insights into model interpretability and architectural design. Third, advancing compression techniques to maintain strong performance at extremely high compression ratios (80% and above) represents compelling challenge that could further bridge the gap between memory efficiency and reasoning capability preservation. These research directions hold significant potential for advancing both our understanding of reasoning in large language models and their practical deployment efficiency. 9 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In EMNLP, 2023. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, and Danqi Chen. Cache me if you can: How many kvs do you need for effective long-context lms? arXiv preprint arXiv:2506.17121, 2025. Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, LiWen Chang, Jiuxiang Gu, et al. R-kv: Redundancy-aware kv cache compression for training-free reasoning models acceleration. arXiv preprint arXiv:2505.24133, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Google DeepMind. Gemini. https://deepmind.google/models/gemini/, 2025. Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. Skvq: Sliding-window key and value cache quantization for large language models. arXiv preprint arXiv:2405.06219, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. The Llama 3 Herd of Models, July 2024. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Junxian Guo, Haotian Tang, Shang Yang, Zhekai Zhang, Zhijian Liu, and Song Han. Block Sparse https://github.com/mit-han-lab/Block-Sparse-Attention, Attention. 2024. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In ECCV, 2018. Coleman Richard Charles Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. In NeurIPS, 2024. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 10 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, 2023. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. NeurIPS, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR, 2023. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024a. Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. Learn to reason efficiently with adaptive length-based reward shaping. arXiv preprint arXiv:2505.15612, 2025. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In ICML, 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. URL https: //pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview -with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. MMA. ary URL american-invitational-mathematics-examination-aime. Februhttps://maa.org/math-competitions/ invitational mathematics examination American 2024. aime, - Ziran Qin, Yuchen Cao, Mingbao Lin, Wen Hu, Shixuan Fan, Ke Cheng, Weiyao Lin, and Jianguo Li. Cake: Cascading and adaptive kv cache eviction with layer preferences. In ICLR, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, and Kehong Yuan. Rotatekv: Accurate and robust 2-bit kv cache quantization for llms via outlier-aware adaptive rotations. arXiv preprint arXiv:2501.16383, 2025. Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Danning Ke, Shikuan Hong, Yiwu Yao, and Gongyi Wang. Razorattention: Efficient kv cache compression through retrieval heads. In ICLR, 2024a. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: query-aware sparsity for efficient long-context llm inference. In ICML, 2024b. Keda Tao, Haoxuan You, Yang Sui, Can Qin, and Huan Wang. Plug-and-play 1. x-bit kv cache quantization for video large language models. arXiv preprint arXiv:2503.16257, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267288, 1996. 11 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In ICLR, 2023. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. In ICLR, 2024. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024a. Dongjie Yang, Xiaodong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao. Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. In ACL, 2024b. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In NeurIPS, 2025. Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang Nie. Wkvquant: Quantizing weight and key/value cache for large language models gains more. arXiv preprint arXiv:2402.12065, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models. In NeurIPS, 2023. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody H. Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs. In NeurIPS, 2024. Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In ICLR, 2017. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018. 12 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
        },
        {
            "title": "DECLARATION OF THE USE OF LARGE LANGUAGE MODELS",
            "content": "In this paper, we only use LLMs to help with grammar checking and polishing the writing. All conceptual contributions, framework design, implementation, and experimental evaluations were performed by the authors without assistance from LLMs."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MOTIVATION STUDY We provide comprehensive motivation study on two mainstream small reasoning models (Llama3.1-8B-R1 and Qwen-2.5-7B-R1 (Guo et al., 2025)) and their instruct variants (Llama-3.1-8B-Inst (Dubey et al., 2024) and Qwen-2.5-7B-Inst 1 (Yang et al., 2024a)). We conduct the evaluation on two typical token-dropping methods (H2O (Zhang et al., 2023) and R-KV (Cai et al., 2025)) and one head-reallocation method (DuoAttention (Xiao et al., 2024)) across four benchmarks, including GSM8K (Cobbe et al., 2021), Math500 (Lightman et al., 2023), AIME25 (MMA, 2024), MBPP (Austin et al., 2021). Figure 9 presents that all compression methods maintain relatively stable performance on instruct models but drop substantially on reasoning models as compression increases. We further analyze the error modes on reasoning models in the above evaluation. We observed three error modes: repetitive errors (excessively repeating token sequences), incorrect errors (generating wrong answers), and overlength errors (generating sequences that exceed normal length baselines), as illustrated in Figure 10. The detailed error modes can be seen in Figure 11. Figure 9: Comprehensive evaluation of KV cache compression methods across all model pairs and benchmarks reveals consistent patterns of performance degradation. H2O, R-KV, and DuoAttention maintain relatively stable performance on instruction-following models but exhibit significant drops on their reasoning counterparts as the KV cache budget decreases. This performance degradation becomes particularly severe at higher sparsity levels, with notable declines observed on reasoningintensive benchmarks including GSM8k, Math500, AIME25, and MBPP. A.2 EXPERIMENT DETAILS Dataset Construction. We construct training data from the DeepScaleR dataset (Luo et al., 2025), which contains about 40,000 diverse and challenging mathematical reasoning problems. For each model, we generate solutions using the respective reasoning model with greedy decoding, filter 1We use Qwen-2.5-Math-7B-Instruct (Yang et al., 2024a) as the instruct baseline, abbreviated as Qwen2.5-7B-Inst for naming consistency, since Qwen-2.5-7B-R1 (deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) was based on Qwen-2.5-Math-7B 13 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Figure 10: The instances of three error modes. Figure 11: Comprehensive error mode analyses of KV cache compression methods across reasoning models reveal distinct failure patterns. Token-dropping methods (H2O, R-KV) consistently exhibit repetitive errors, as they inevitably discard reasoning-critical information during compression. In contrast, the head-reallocation method DuoAttention tends to show more over-length errors compared to token-dropping methods, suggesting that while it relatively preserves sequence information integrity, it still struggles to fully preserve reasoning capability. correct solutions, then randomly sample 3,000 problems for training. The selected problems are distributed across different output token lengths as follows: 600 problems each for 0-2k and 2k-4k tokens, 1,000 problems for 4k-6k tokens, and 800 problems for 6k-8k tokens. Hardware and Hyperparameter Settings. All experiments are conducted on 2 NVIDIA A100 GPUs with 80GB memory each, one for backward computation and one for sample generation. Training runs for 2 epochs, totaling 185 steps with batch size of 32. All evaluations are conducted on NVIDIA RTX5090 GPUs. We optimize the gating adapters using AdamW optimizer with β1 = 0.9, β2 = 0.999, weight decay of 0.017, and learning rate of 0.01 with constant schedule. For GRPO training configuration, we disable KL penalty and use recommendation setting of AReaL; for GRPO sampling configuration, we use 4 samples per query with sampling temperature of 1.0. The hyperparameters are shown in Table 2. Local Attention Implementation. During training, we employ an efficient block-sparse attention approximation implementation (Guo et al., 2024) in AReaL (Fu et al., 2025) to update adapter weights, while using mask matrices for prefilling and custom Triton kernels for decoding in SGLang (Zheng et al., 2024) to generate samples. For inference, we only store the partial KV cache of first 16 sink tokens and recent 64 local tokens for non-reasoning heads, while reasoning heads maintain the full KV cache. 14 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Table 2: Training Hyperparameters. Parameter Llama-3.1-8B-R1 Qwen-2.5-7B-R1 Regularization weight β Reward threshold τ Sink token size Local token size Max sequence length 1e-3 0.5 128 256 8192 1e-3 0.55 128 256 8192 Baseline Implementation. To ensure fair comparison with baseline methods, we make several adjustments. For H2O and R-KV, we augment them with the same sink and local token overhead (16+64 tokens) that our method uses. Since H2O and R-KV only support preset fixed KV cache budgets, we convert their fixed budgets to dynamic allocation that increases with sequence length. For example, if the fixed budget is 50% of the full KV cache, then at sequence length 1000, they use 500 tokens of KV cache, and at sequence length 2000, they use 1000 tokens of KV cache. For DuoAttention, we replicate their approach with default settings on our models and use the same inference settings as our method. Evaluation Settings. We evaluate all methods using greedy decoding on RTX 5090 36G GPUs or RTX 4090 24G GPUs with batch size of 1. For all datasets, we use regex to extract the final answer from the generated text, using Pass@1 as the evaluation metric. For GSM8K, Math500, and MBPP, we use 8192 max sequence length; for AIME24, we use 16384 max sequence length. We achieved near official reported performance without KV cache compression. We use eager attention implementation for H2O and R-KV since they need to use attention scores, while we use flash attention for DuoAttention and our method. Table 3: Llama-3.1-8B-R1 performance (%) under different KV cache compression methods and budgets. RLKV (Ours) shows competitive performance across settings. Red background denotes performance below the fullKV-cache baseline, whereas green background denotes performance above it. For all values, higher is better. The best result of the metric in each benchmark is in bold. Dataset Method KV Cache Budget Sparsity GSM8K (Math) Math500 (Math) AIME24 (Math) MBPP (Code) H2O R-KV DuoAttention RLKV (Ours) H2O R-KV DuoAttention RLKV (Ours) H2O R-KV DuoAttention RLKV (Ours) H2O R-KV DuoAttention RLKV (Ours) 0.2 85.7 (-3.5) 88.5 (-0.6) 88.8 (-0.4) 89.2 (+0.1) 60.6 (-22.4) 79.8 (-3.2) 84.4 (+1.4) 84.4 (+1.4) 3.3 (-33.3) 26.7 (-10.0) 33.3 (-3.3) 36.7 (+0.0) 7.2 (-55.4) 46.4 (-16.2) 62.0 (-0.6) 62.8 (+0.2) 0. 67.6 (-21.5) 88.3 (-0.8) 87.1 (-2.0) 88.4 (-0.8) 27.0 (-56.0) 77.8 (-5.2) 81.6 (-1.4) 84.6 (+1.6) 3.3 (-33.3) 13.3 (-23.3) 20.0 (-16.7) 40.0 (+3.3) 0.6 (-62.0) 29.4 (-33.2) 60.6 (-2.0) 63.8 (+1.2) 0.6 23.7 (-65.4) 79.1 (-10.1) 77.8 (-11.4) 79.5 (-9.7) 6.0 (-77.0) 56.8 (-26.2) 69.0 (-14.0) 78.0 (-5.0) 0.0 (-36.7) 10.0 (-26.7) 6.7 (-30.0) 20.0 (-16.7) 0.0 (-62.6) 14.6 (-48.0) 40.4 (-22.2) 51.8 (-10.8) 0.8 2.9 (-86.3) 48.6 (-40.6) 28.5 (-60.6) 47.4 (-41.8) 2.6 (-80.4) 18.8 (-64.2) 28.6 (-54.4) 49.6 (-33.4) 0.0 (-36.7) 0.0 (-36.7) 0.0 (-36.7) 0.0 (-36.7) 0.0 (-62.6) 1.0 (-61.6) 7.4 (-55.2) 6.0 (-56.6) A.3 FULL RESULTS Tables 3 and 4 present the complete numerical results of RLKV and baselines for Llama-3.1-8B-R1 and Qwen-2.5-7B-R1 respectively, across all benchmarks and KV cache compression budgets. Values in parentheses indicate the performance difference compared to the full KV cache setting, with positive values in green indicating improvement and negative values in red indicating degradation. 15 Which Heads Matter for Reasoning? RL-Guided KV Cache Compression Table 4: Qwen-2.5-7B-R1 performance (%) under different KV cache compression methods and budgets. RLKV (Ours) shows competitive performance across settings. Red background denotes performance below the fullKV-cache baseline, whereas green background denotes performance above it. For all values, higher is better. The best result of the metric in each benchmark is in bold. Dataset Method KV Cache Budget Sparsity 0.2 0. 0.6 0.8 GSM8K (Math) Math500 (Math) AIME24 (Math) MBPP (Code) H2O R-KV DuoAttention RLKV (Ours) H2O R-KV DuoAttention RLKV (Ours) H2O R-KV DuoAttention RLKV (Ours) H2O R-KV DuoAttention RLKV (Ours) 51.1 (-38.0) 87.7 (-1.4) 88.9 (-0.2) 90.7 (+1.6) 37.6 (-50.2) 82.0 (-5.8) 83.6 (-4.2) 89.0 (+1.2) 6.7 (-36.7) 30.0 (-13.3) 26.7 (-16.7) 50.0 (+6.7) 12.6 (-50.6) 41.4 (-21.8) 60.4 (-2.8) 63.2 (+0.0) 17.4 (-71.7) 84.0 (-5.1) 82.0 (-7.1) 90.1 (+1.0) 13.4 (-74.4) 73.4 (-14.4) 74.2 (-13.6) 86.0 (-1.8) 0.0 (-43.3) 20.0 (-23.3) 13.3 (-30.0) 26.7 (-16.7) 1.6 (-61.6) 31.0 (-32.2) 55.0 (-8.2) 56.8 (-6.4) 3.5 (-85.6) 67.9 (-21.1) 57.1 (-32.0) 83.1 (-6.0) 4.6 (-83.2) 54.8 (-33.0) 56.4 (-31.4) 71.4 (-16.4) 0.0 (-43.3) 6.7 (-36.7) 0.0 (-43.3) 13.3 (-30.0) 0.2 (-63.0) 12.2 (-51.0) 35.0 (-28.2) 40.2 (-23.0) 1.4 (-87.6) 16.1 (-72.9) 7.2 (-81.9) 7.7 (-81.3) 2.6 (-85.2) 9.2 (-78.6) 8.4 (-79.4) 15.8 (-72.0) 0.0 (-43.3) 0.0 (-43.3) 0.0 (-43.3) 0.0 (-43.3) 0.0 (-63.2) 0.4 (-62.8) 1.0 (-62.2) 0.6 (-62.6) Figure 12: The analysis reveals distinct error patterns when reasoning heads versus retrieval heads work with compressed KV cache across four benchmarks. A.4 DETAILS OF ERROR MODES ANALYSES Figure 12 presents the comprehensive error mode analysis across all models and benchmarks. We observe three error modes: repetitive errors (excessively repeating token sequences), incorrect errors (generating wrong answers), and overlength errors (generating sequences that exceed normal length baselines). Our method RLKV shows consistency in error modes across different models and benchmarks, while DuoAttention exhibits more varied error modes across different settings."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "McGill University",
        "Mila",
        "Westlake University",
        "Zhejiang University"
    ]
}