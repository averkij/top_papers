{
    "paper_title": "Rethinking the shape convention of an MLP",
    "authors": [
        "Meng-Hsi Chen",
        "Yu-Ang Lee",
        "Feng-Ting Liao",
        "Da-shan Shiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 6 9 7 1 0 . 0 1 5 2 : r a"
        },
        {
            "title": "RETHINKING THE SHAPE CONVENTION OF AN MLP",
            "content": "Meng-Hsi Chen1: Yu-Ang Lee1,2 Feng-Ting Liao1: Da-shan Shiu1 1MediaTek Research 2National Taiwan University"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-layer perceptrons (MLPs) conventionally follow narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecksa scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks."
        },
        {
            "title": "1\nMulti-layer perceptrons (MLPs) are classical neural network building blocks with a well-established\narchitectural convention. A typical MLP block expands from an input dimension to a wider hidden\ndimension, then contracts back to an output dimension, resulting in a \"narrow-wide-narrow\" shape.\nThis expansion allows the network to perform complex transformations in the higher-dimensional\nhidden space. The feedforward layer in a transformer typically has a hidden dimension 2 to 4 times\nlarger than the token dimension (Vaswani et al., 2017; Jiang et al., 2023).",
            "content": "Beyond improving gradient flow (He et al., 2016b), skip connections enable incremental learning where networks refine representations through additive corrections rather than complete transformations. When applied to MLPs, the conventional approach maintains narrow-wide-narrow blocks where skip connections operate at the narrower input/output dimensions. However, this convention constrains all residual updates to operate with the input dimensions. In this paper, we challenge this very convention, and hypothesize that performing incremental improvement is more effective at higher dimensionality. We thus propose to invert the shape of the MLP when an MLP is accompanied by skip connection, i.e. taking \"wide-narrow-wide\" (Hourglass) shape. This design maintains the skip connection at the expanded latent dimension while residual computations flow through narrow bottleneck instead. Our hypothesis is motivated by theoretical insights suggesting that higher-dimensional spaces provide richer feature representations for residual learning, potentially enabling more effective incremental refinements than updates constrained to narrow dimensions. Implementing wide-narrow-wide MLPs requires lifting input signals to expanded dimensions via linear projection. While conventional practice trains this projection end-to-end, we hypothesize that fixed random projectionsinspired by reservoir computingachieve comparable performance when These authors contributed equally. :Correspondence: meng-hsi.chen@mtkresearch.com, ft.liao@mtkresearch.com"
        },
        {
            "title": "Preprint",
            "content": "expansion factors are large. The advantages of such fixed random input projection can offset the additional burden of having to carry one more matrix-vector computing layer. To test our hypothesis empirically, we conduct architectural comparisons between conventional (\"narrowwidenarrow\") and Hourglass (\"widenarrowwide\") MLP stacks. We evaluate both architectures on generative tasks, including generative classification, denoising, and super-resolution on MNIST, as well as denoising and super-resolution on ImageNet-32 images. Through systematic architectural search, we characterize the performanceparameter count Pareto frontiers for both designs. Our results demonstrate that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs, even when accounting for the additional parameters in the input projection layer. Furthermore, our ablation studies confirm that the linear input projection can indeed remain fixed at its random initialization with negligible impact on performance, validating both our architectural hypothesis and our parameter-efficient design choice. Breaking from the conventional expand-then-contract MLP paradigm opens previously unexplored architectural trade-offs. Our experiments reveal that as parameter budgets increase, Pareto-optimal Hourglass architectures consistently favor deeper networks with wider skip connections and narrower bottleneck dimensionsa scaling pattern distinct from conventional MLPs. Our contributions are: We propose inverting the conventional narrow-wide-narrow paradigm to wide-narrow-wide (Hourglass) MLP design, with an input projection to lift natural signal to the wide dimension. We propose that that the required input projection can be fixed at random initialization with negligible performance impact, enabling efficient implementations of wide-narrow-wide MLPs. Through empirical validation on generative tasks, we show that the wide-narrow-wide design consistently leads to superior Pareto frontiers compared to the conventional design. Our experiments reveal that Pareto-optimal Hourglass architectures consistently favor deeper networks with wider skip connections and narrower bottleneck dimensions as the parameter count increases. Supported by the results, we believe that our intuition extends beyond MLPs to other skip-connected architectures including Transformers and Vision Transformerswe discuss these broader implications in our Future Work section."
        },
        {
            "title": "2 BACKGROUNDS AND RELATED WORKS\n2.1 SKIP CONNECTIONS AND INCREMENTAL IMPROVEMENT IN DEEP NETWORKS\nSkip connections, introduced in ResNets (He et al., 2016a), originally addressed gradient flow\nproblems in deep networks but also enable a distinct computational paradigm. Rather than learning\ncomplete transformations, residual blocks learn correction terms: a block computes y “ x ` ∆F pxq,\nwhere ∆F pxq represents a learned correction to the input x. This formulation allows each layer to\ncontribute incremental improvements to the evolving representation, enabling effective training of\nvery deep architectures.",
            "content": "This incremental refinement principle has become fundamental across diverse modern architectures. Transformers (Vaswani et al., 2017) apply residual connections twice per blockonce for selfattention and once for feed-forward processingwith each sublayer contributing additive refinements. Generative models exemplify this principle explicitly: diffusion models learn denoising steps xt1 xt ` ϵθpxt, tq (Ho et al., 2020), while flow matching models integrate along learned vector fields dx dt vθpx, tq (Lipman et al., 2023). The common thread across these architectures is the preference for small, targeted corrections over complete transformations. 2.2 MLP BLOCKS AND SKIP CONNECTION PLACEMENT Multi-layer perceptrons (MLPs) serve as canonical case study for skip connection placement. standard MLP block with skip connections follows the pattern: where xi, xi`1 Rdx, W1 Rdhˆdx, W2 Rdxˆdh, and by common convention, dh ą dx. xi`1 xi ` W2σpW1normpxiqq (1)"
        },
        {
            "title": "Preprint",
            "content": "This creates in \"narrow-wide-narrow\" computational graph: the input dimension dx expands to the hidden dimension dh, then contracts back to dx to match the skip connection. MLP has been embedded in various modern neural network architectures. When instantiated, the skip connection connects to narrower dimension dx ă dh. For instance, the original transformer used dh 4dx (Vaswani et al., 2017). Modern language models typically employ expansion dh{dx between 2-4 (Jiang et al., 2023; Grattafiori et al., 2024) in their feedforward section."
        },
        {
            "title": "2.3 THEORETICAL FOUNDATIONS FOR HIGH-DIMENSIONAL REPRESENTATIONS",
            "content": "Several theoretical frameworks suggest that operations in higher-dimensional spaces offer computational advantages. Information Preservation via Random Projections Multiple fields demonstrate that random up-projections preserve essential information regardless of the specific projection used. Reservoir computing employs fixed random input projections in Echo State Networks (Jaeger, 2001), while random features show that any appropriately distributed projection can approximate shift-invariant kernels (Rahimi & Recht, 2007). The Johnson-Lindenstrauss lemma formalizes this principle: random matrices satisfying basic distributional properties preserve geometric structure with high probability (Johnson & Lindenstrauss, 1984). Compressive sensing provides additional theoretical support. Under sparsity assumptions, signals can be recovered from remarkably few random measurements, provided sufficient ambient dimensionality (Candès & Tao, 2005; Donoho, 2006). The shared insight is that, as long as they satisfy appropriate distributional properties, random projections to higher dimensions preserve information structure while being largely invariant to the specific projection matrix chosen whether Gaussian, Rademacher, or sparse (Achlioptas, 2003). Linear Separability in High Dimensions Covers theorem (Cover, 1965) demonstrates that projecting data into sufficiently high-dimensional spaces increases the probability of linear separability. Among kernel methods, Support Vector Machines implicitly operate in high-dimensional feature spaces through the kernel trick, while random feature approximations (Rahimi & Recht, 2007) show that wider representations can approximate complex functions with simpler operations. 2.4 RELATED WORK While we focus on MLPs, it is worth noting that several non-MLP architectures already place skip connections at the widest parts of their computational graphs, though without the intentional dimensional expansion that we hypothesize benefits our proposed wide-narrow-wide MLP design. U-Net architectures (Ronneberger et al., 2015) place skip connections between corresponding layers in encoder-decoder networks, effectively connecting at the widest feature map dimensions before spatial downsampling. The skip connections preserve detailed spatial information at full resolution while processing occurs at coarser scales. Mixture-of-Experts (MoE) architectures (Shazeer et al., 2017; Zhang et al., 2022), when routing inference through small number of active experts, can be viewed as temporarily creating widenarrow-wide computational pattern. Similarly, LoRA (Hu et al., 2022) parameter-efficient fine-tuning (PEFT) method appends additional wide-narrow-wide paths to any weight matrix. However, because these architectures operate at naturally occurring wide dimensions rather than artificially expanded feature spaces, they are not directly comparable to the wide-narrow-wide MLP proposed in this work."
        },
        {
            "title": "3 WIDE–NARROW–WIDE INCREMENTAL–IMPROVING MLP",
            "content": "We propose inverting the conventional narrow-wide-narrow MLP design to create wide-narrow-wide (hourglass) blocks. Based on the theoretical foundations discussed in Section 2.3, we hypothesize that architectures with skip connections operating at higher dimensions may enable more advantageous incremental refinement. Under the constraint of maintaining comparable parameter count, this architectural change results in individual MLP blocks with the wide-narrow-wide shape, as illustrated in Fig. 1(a). Skip connections preserve information at the wider dimension while the residual path computes incremental improvement through narrow bottleneck. This design offers additional architectural flexibility: by using narrower bottleneck dimensions, one can construct deeper networks while maintaining the same parameter budget."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: (a) Illustration of wide-narrow-wide MLP block. The two endpoints zi and zi`1 have higher dimensionality compared to the hidden hi. Skip connection thus connects two highdimensional endpoints, rather than two low-dimensional ones in existing convention. Components that do not depend on dimensionality (e.g., normalization, element-wise nonlinearity) are omitted for clarity. (b) Illutration of full network whose core is stack of wide-narrow-wide MLP blocks. An input projection network Win is required to adapt the input dimensionality of to the dimensionality of the latent z. An output projection network Wout is used to adapt to the desired task."
        },
        {
            "title": "3.1 WIDE–NARROW–WIDE MLP\nWe describe a network whose core consists of purely wide-narrow-wide MLP. Such a network is built\non three distinct stages.\nInput-to-latent projection. The input signal x P Rdx , which can be a natural signal, is first\nprojected to the latent space of dz dimensions via an input projection:",
            "content": "z0 Winx, Win Rdzˆdx . (2) For adapting input signal to wide-narrow-wide MLP, we consider expansive (up) projection, dz ą dx. When we compare to network of conventional narrow-wide-narrow MLPs, we follow the common practice of injecting the input signal directly into an MLP, skipping this input projection. stack of MLP blocks. For block 0, 1, . . . , 1, the incremental improvement is computed and applied in the highdimensional space: zi`1 zi ` i2 i2 Rdzˆdh . If dz ą dh, the MLP is of the wide-narrow-wide type. Conventional MLP has dz ă dh. i1 Rdhˆdz , normpziq , H i1 σi (3) ` Output conversion. At the output of the residual blocks, an additional output network Wout shall be used to convert the last latent zL into the format demanded by the desired task. For instance, for training objective aiming to evolve one noised image to prototypical one, linear projection Wout Rdxˆdz can be used: ˆy WoutzL. (4) If one is interested in only the class tag among classes of the input x, linear projection Wout RCˆdz followed by softmax operation for distribution over classes can be applied, ˆy softmaxpWoutzLq. (5) We note that during pretraining, network with only the input-to-latent projection and the residual blocks can directly learn to predict the optimal output latent. Post-training, an output conversion network can be augmented and then finetuned end-to-end on task-specific data. INPUT-TO-LATENT PROJECTION STRATEGY 3.2 Conventional practice trains the input projection Win : Rdx Ñ Rdz end-to-end with the rest of the network. However, based on the theoretical foundations discussed in Section 2.3, we propose an alternative approach: using fixed random projection matrix that remains unchanged throughout training. We hypothesize that when the expanded dimension dz is sufficiently larger than the input dimension dx, the performance gap between randomly initialized projection and learned one becomes unnoticeable. This hypothesis is motivated by results from reservoir computing, random features, and compressive sensing, which demonstrate that appropriately distributed random matrices can preserve"
        },
        {
            "title": "Preprint",
            "content": "essential information structure regardless of their specific realization. If this hypothesis holds, fixed random projections offer several practical advantages over learned projections below: Reduced parameter count: The projection matrix Win no longer contributes to the trainable parameter budget, allowing more training resources to be allocated to the processing layers. Reduced bandwidth requirement: Random matrices with known structure (e.g., sparse or circulant patterns) can be generated just-in-time efficiently by custom kernels or custom circuits rather than stored in memory and transferred over the processor-memory interface. This is particularly valuable for architectures like transformers that are often memory-bandwidth limited. Reduced memory capacity: If random matrices are computed on demand, this naturally reduces the memory capacity requirement for both training and inference. We evaluate this hypothesis empirically in Section 4, comparing the performance of learned versus fixed random input projections across multiple tasks."
        },
        {
            "title": "3.3 MLP SHAPE AND DEPTH STRATEGY",
            "content": "With the wide-narrow-wide MLP paradigm, the total number of MLP parameters for mandatory stages is dxdz `2Ldzdh. Achieving optimal performance under total parameter constraint requires one to properly balance the design parameters dz, dh, and L. In general, the higher the latent dimension dz, the more expressive the signal space in which the network solves task becomes. That expressivity can directly translate into both ease and robustness of learning and performance at convergence. However, this must be counterbalanced by the depth of narrow-wide-narrow MLPs L. For many tasks, the deeper the network, the better the performance at convergence. Having small dh can indeed enable larger seemingly without consequence, but in practice employing an overly deep network can entail certain difficulties."
        },
        {
            "title": "4 EXPERIMENTS AND RESULTS\nWe evaluate the proposed wide-narrow-wide (Hourglass) MLP architecture against conventional\nnarrow-wide-narrow baselines across multiple generative tasks and datasets. Our experimental\ndesign focuses on three key questions: (1) Do Hourglass architectures achieve superior performance-\nparameter trade-offs compared to conventional designs? (2) How do optimal architectural choices\n(latent dimension, bottleneck width, and depth) differ between Hourglass and conventional designs?\nand (3) How does the choice of fixed versus learned input projections affect performance? We conduct\nsystematic architectural searches to characterize the Pareto frontiers for both designs, enabling direct\ncomparison of their efficiency at equivalent parameter budgets.",
            "content": "4.1 EXPERIMENTAL SETUP We evaluate our approach on two image datasets: MNIST (LeCun et al., 2010) and ImageNet-32 (Chrabaszcz et al., 2017), across multiple generative tasks that test different aspects of representation learning and refinement capabilities. For MNIST, we consider three tasks: (1) generative classification, where the model learns to transform an input image of digit into corresponding prototypical image before classification; (2) denoising, where the model removes artificially added Gaussian noise from corrupted images; and (3) superresolution, where the model upsamples low-resolution inputs to recover high-resolution images. For ImageNet-32, we focus on the more challenging tasks of (1) denoising natural images with complex textures and structures, and (2) super-resolution that requires preserving fine-grained visual details across diverse object categories. These tasks are particularly well-suited for evaluating our hypothesis because they require incremental refinement of visual representations exactly the type of processing we expect to benefit from wider skip connections. All experiments use the network architecture illustrated in Figure 1(b): an input projection Win, followed by residual MLP blocks, and an output projection Wout. The key difference between the Hourglass and Conventional models lies in the internal shape of each MLP block. This controlled comparison ensures that both architectures share the same training objectives, input/output configurations, and overall structure, isolating the effect of skip connection placement."
        },
        {
            "title": "Preprint",
            "content": "Our architectural search systematically explores the design space defined by: latent dimension dz, hidden dimension dh, and the number of residual blocks L. Additionally, we investigate whether the input projection Win should be learned end-to-end or fixed at random initialization. Detailed experimental settings are provided in Appendix A.1."
        },
        {
            "title": "4.2 MAIN RESULTS AND OBSERVATIONS\nWe evaluate both architectures by characterizing their performance-parameter Pareto frontiers for each\ndataset and task combination. The Pareto frontier captures the trade-off between model complexity\n(number of parameters) and performance (measured by PSNR and SSIM). A model is Pareto-optimal\nif no other model achieves better performance with fewer parameters—these models represent the\nmost efficient designs at their respective parameter budgets.",
            "content": "Our analysis reveals that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs across all tested tasks. As parameter budgets increase, the optimal Hourglass configurations favor deeper networks with wider latent dimensions but narrower bottleneck dimensions. Additionally, while Hourglass architectures inherently require dimensional expansion for optimal performance, we observe that conventional MLPs can also benefit from random input projections that preserve dimensionality. 4.2.1 GENERATIVE CLASSIFICATION TASK An MNIST generative classification task requires model to take in an input digit image, generates prototypical digit image, and then makes classification based on the latter. Figure 2(b) shows qualitative examples from the Hourglass model. For model training, one image per digit class is chosen to serve as the ground truth digit image. Figure 2(a) compares the Pareto frontiers of Hourglass and conventional MLPs on the MNIST generative classification task. As shown in Figure 2(a), the Hourglass architecture consistently achieves better performancecomplexity trade-off, reaching higher PSNR values across wide range of parameter counts. In particular, when the required accuracy is low in the 26 dB range, the Hourglass architecture achieves superior performance with significantly fewer parameters. (a) (b) Figure 2: Generative Classification Task on MINST. (a) Performancecomplexity Pareto front. Fronts are searched with each configuration repeated 5 times. \"Widenarrowwide\" MLPs outperform conventional \"narrowwidenarrow\" ones. (b) Samples predicted by our proposed Hourglass model. 4.2.2 GENERATIVE RESTORATION TASKS We evaluate both architectures on two common generative restoration tasks: denoising and superresolution. Figures 3 and 4 present the PSNRparameter Pareto fronts for MNIST and ImageNet-32. Across datasets and tasks, the proposed widenarrowwide (Hourglass) MLP consistently outperforms the conventional narrowwidenarrow baseline. In denoising (Figure 3(b)), the Hourglass model attains 22.31 dB PSNR with only 66M parameters, whereas the best conventional model requires 75M to reach the same score. On MNIST (Figure 3(a)), this advantage persists across the entire complexity range. For super-resolution (Figure 4), the Hourglass design again dominates. On ImageNet-32, it achieves 24.00 dB with 69M parameters, outperforming the 87M-parameter conventional model. The gap is particularly pronounced in the mid-range budget regime. On MNIST, Hourglass MLPs similarly produce better reconstructions at every tested parameter count."
        },
        {
            "title": "Preprint",
            "content": "These results suggest that performing residual updates in high-dimensional latent space enhances restoration fidelity and parameter efficiency, especially under tight or mid-range model budgets. (a) MNIST (b) ImageNet-32 Figure 3: Generative Restoration Task - Denoising. Performance-complexity Pareto fronts on MINST and ImageNet-32 are searched with each configuration repeated 5 times. Optimal configurations are shown in Table 1. (a) MNIST (b) ImageNetFigure 4: Generative Restoration Task - Super-resolution. Performance-complexity Pareto fronts on MINST and ImageNet-32 are searched with each configuration repeated 5 times. Optimal configurations are shown in Table 2. 4.2.3 PARETO-OPTIMAL ARCHITECTURE CONFIGURATIONS In both denoising and super-resolution tasks, Tables 1 and 2 summarize the best-performing models on ImageNet-32 under various parameter budgets. Three consistent trends emerge: Hourglass models achieve higher PSNR with fewer parameters. Across denoising and super-resolution tasks, Hourglass architectures consistently surpass the PSNR of conventional models while using substantially fewer parameters, demonstrating superior efficiency. Hourglass architectures favor depth and moderate bottlenecks. Optimal configurations typically use 4 or 5 with dh between 270 and 765, in contrast to conventional designs that rely on shallow depth (L ď 3) and very wide hidden layers (dh ě 3075). High-dimensional skip connections improve parameter efficiency. Models with large dz (commonly 3075 or larger) and relatively small dh maintain or improve PSNR, confirming the benefits of residual learning in wide latent spaces. Together, these results confirm that placing skip connections in high-dimensional layers yields more expressive and efficient models with better performancecomplexity trade-offs. 4.3 EFFECT OF FIXED VS. TRAINABLE INPUT PROJECTION To verify our hypothesis in Section 3.2 that randomly initialized projection is sufficient to preserve essential information from input signal, we investigate whether the input projection Win in the Hourglass architecture can be randomly initialized and fixed. On the ImageNet-32 denoising task, we compare two variants under the configuration pdz, dh, Lq p3546, 270, 5q: (1) Fixed: Win is"
        },
        {
            "title": "Preprint",
            "content": "Architecture Params (M) Conventional Hourglass 37.77 43.52 56.66 68.17 75.55 84.23 22.07 22.35 24.06 26.33 27.53 28.30 29.45 31.36 33.01 35.19 37.11 37.71 42.42 47.08 54.13 57.27 62.42 66.04 66.86 77.10 80. dz 3072 3072 3072 3072 3072 3072 3546 3546 3546 3546 3546 3075 3546 3546 3075 3546 3546 3075 3075 3075 3075 3075 3546 3546 3075 3546 3075 dh 3075 4012 3075 4012 3075 3546 8 16 64 128 270 765 270 270 765 270 270 765 765 1146 1146 1560 1146 1560 1560 1560 PSNR (µ 5σ dB) Architecture Params (M) 1 1 2 2 3 3 5 5 5 5 3 2 4 5 3 7 8 4 5 4 5 4 5 4 5 5 5 21.408 0.005 21.411 0.004 22.186 0.012 22.213 0.015 22.313 0.004 22.325 0. 21.506 0.007 21.575 0.012 21.767 0.010 21.921 0.010 21.936 0.009 21.960 0.017 22.029 0.012 22.082 0.012 22.136 0.007 22.147 0.005 22.164 0.017 22.210 0.006 22.242 0.005 22.256 0.011 22.288 0.003 22.291 0.005 22.303 0.003 22.307 0.003 22.323 0.002 22.335 0.010 22.346 0.004 Conventional Hourglass 30.69 33.58 49.58 55.37 68.48 87.37 14.18 15.32 16.67 17.70 20.02 22.83 23.19 25.92 30.63 32.95 35.32 35.33 40.00 46.81 47.05 50.18 54.25 57.87 68.93 70.75 85.03 dz 3072 3072 3072 3072 3072 3072 3546 3546 3546 3546 4012 4576 3546 3075 3075 3075 3546 3075 3075 3546 3075 3075 3546 3546 3546 3546 3546 dh 3075 3546 3075 3546 3075 3075 16 48 86 115 115 115 270 765 765 1146 765 765 1146 1560 1146 1560 1146 1560 1560 2014 2014 PSNR (µ 5σ dB) 1 1 2 2 3 4 5 5 5 5 5 5 5 3 4 3 4 5 4 3 5 4 5 4 5 4 5 23.442 0.005 23.442 0.005 23.885 0.007 23.886 0.010 23.976 0.008 23.994 0.004 23.631 0.008 23.752 0.010 23.799 0.012 23.813 0.011 23.823 0.011 23.829 0.009 23.839 0.023 23.878 0.012 23.916 0.014 23.923 0.004 23.925 0.007 23.941 0.010 23.960 0.008 23.962 0.012 23.975 0.002 23.983 0.004 23.984 0.006 23.994 0.003 24.000 0.002 24.001 0.006 24.009 0.004 Table 1: Pareto optimal model configurations for denoising task on ImageNet-32. An image is linearized to vector of dimension dx 3072. Table 2: Pareto optimal model configurations for super-resolution task on ImageNet-32. An image is linearized to vector of dimension dx 768. randomly initialized and frozen (20.47M parameters); (2) Trainable: Win is updated during training (31.36M parameters). As shown in Figure 5, the trainable model is only marginally better than the fixed model. These results suggest that the gains from learning Win are minor, and fixed projections offer strong parameter-efficient alternativeparticularly useful in low-resource or hardware-constrained settings. Figure 5: Input projection fixed with random projection matrix. Comparison between fixed and trainable input projection Win for Hourglass MLP on ImageNet-32 denoising. We use architecture pdz, dh, Lq p3546, 270, 5q. The fixed-projection model performs comparably to the trainable one. 4.4 ABLATION STUDIES ON HOURGLASS MLP DESIGN To further explore the design trade-offs within the proposed widenarrowwide (Hourglass) MLP architecture, we conduct ablation studies focusing on two key hyperparameters: the bottleneck dimension dh and the number of residual blocks L. Effect of bottleneck width dh: We fix the high-dimensional residual space to dz 3546 and the number of residual blocks to 5, and vary the bottleneck width dh. As shown in Figure 6(a), increasing dh improves PSNR, but the gains diminish beyond dh 270. This suggests that moderate bottlenecks are sufficient for high performance, enabling significant parameter savings. Effect of residual depth L: We fix dz 3546, dh 270, and vary the number of residual blocks L. As shown in Figure 6(b), performance improves with deeper stacks, but quickly plateaus around 5, indicating that relatively shallow Hourglass MLPs are sufficient for strong results."
        },
        {
            "title": "Preprint",
            "content": "(a) Varying the bottleneck dimension dh (b) Varying the number of residual blocks Figure 6: Ablation study of optimal dh and dimension for the Hourglass MLP architecture."
        },
        {
            "title": "5 DISCUSSIONS AND FUTURE WORK\nOur experimental results demonstrate that wide-narrow-wide (Hourglass) MLP architectures consis-\ntently outperform conventional designs across multiple generative tasks, supporting our hypothesis\nthat skip connections at higher dimensions enable more effective incremental refinement. The combi-\nnation of expanded latent dimensions and random input projections achieves superior performance-\nparameter trade-offs compared to traditional narrow-wide-narrow architectures.",
            "content": "In this section, we discuss the limitation of our work and the broader implications of \"wide-narrowwide\" MLP. Scaling to High-Resolution Applications Due to limited computational capacity, our experiments focus on relatively low-dimensional image datasets to isolate the impact due to architectural differences between conventional and Hourglass MLP designs. However, many real-world applications involve much higher-dimensional inputshigh-resolution images, long sequences, or rich feature representations. Naive MLP approaches become computationally prohibitive for them. We identify two promising directions for scaling our insights to such domains. First, wide-narrow-wide blocks could be integrated into existing architectures like MLP-Mixer (Tolstikhin et al., 2021) or other similar frameworks. The design of an MLP-Mixer aims at maintaining rich representations while keeping computational costs comparable to MLP designs with dimensionality equal to the image width modules. Second, the Hourglass design could enhance U-Net architectures commonly used in image-to-image translation and generative modeling. The input would first be projected into higher-dimensional latent space before entering the U-Net encoder-decoder pipeline. Then, the concept of wide-narrowwide shapes can be employed for resolution conversion and for attention. Extension to Transformer Architectures. Looking ahead, the \"wide-narrow-wide\" MLP architecture presents compelling opportunities for enhancing computational efficiency in modern transformer-based models (Figure 7 (a)). By enabling iterative refinement of representations at expanded dimensionalities, this approach could yield compute-optimal architectures with significantly reduced parameter counts compared to current scaling paradigms. Figure 7: Extend the wide-narrow-wide intuition to the transformer. (a) The classic transformer block with Multi-Head Self-Attention and conventional narrowwidenarrow FFN. (b) modified transformer block with block with one or more widenarrowwide FFNs and dimensionality compliant multi-head latent attention sublayer. Components that do not change dimensionality (e.g., normalization, elementwise nonlinearity) are omitted for clarity."
        },
        {
            "title": "Preprint",
            "content": "As illustrated in Figure 7 (b), adapting our findings to transformer architectures requires coordinated modifications across self-attention and FF layer. Notably, FF layer cannot operate at expanded dimensions in isolationthe self-attention mechanism must process representations at matching wider dimensionalities to maintain architectural coherence. To preserve computational efficiency, we thus propose incorporating efficient attention mechanisms such as Multi Head Latent Attention (DeepSeek-AI et al., 2025), which maintains reduced attention head sizes while operating over wider representations. Furthermore, our empirical findings on the efficacy of deeper stacks of \"widenarrow-wide\" blocks suggest that FF adaptations should incorporate multiple iterative refinement blocks with \"wide-narrow-wide\" architectural pattern within each FF layer. As result, such designs could enable more sophisticated representational transformations while maintaining favorable parameter-to-performance ratios, potentially advancing the state-of-the-art in efficient large-scale model architectures."
        },
        {
            "title": "REFERENCES",
            "content": "Dimitris Achlioptas. Database-friendly random projections: Johnsonlindenstrauss with binary coins. doi: 10. 1016/S0022-0000(03)00025-4. URL https://www.sciencedirect.com/science/ article/pii/S0022000003000254. Journal of Computer and System Sciences, 66(4):671687, 2003. Emmanuel J. Candès and Terence Tao. Decoding by linear programming. IEEE transactions on information theory, 51(12):42034215, 2005. doi: 10.1109/TIT.2005.858979. Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs/1707.08819, 2017. URL http://arxiv.org/ abs/1707.08819. Thomas M. Cover. Geometrical and statistical properties of systems of linear inequalities with IEEE Transactions on Electronic Computers, EC-14(3): applications in pattern recognition. 326334, 1965. doi: 10.1109/PGEC.1965.264137. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437."
        },
        {
            "title": "Preprint",
            "content": "David L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):12891306, 2006. doi: 10.1109/TIT.2006.871582. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,"
        },
        {
            "title": "Preprint",
            "content": "Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2016a. doi: 10.1109/CVPR.2016.90. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), pp. 630645. Springer, 2016b. URL https://arxiv.org/abs/1603.05027. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 4c5bcfec8584af0d967f1800aa1c4b03-Paper.pdf. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In International and Weizhu Chen. LoRA: Low-rank adaptation of large language models. Conference on Learning Representations (ICLR), 2022. URL https://openreview.net/ forum?id=nZe72R8yS0. Herbert Jaeger. The \"echo state\" approach to analysing and training recurrent neural networks. Technical report, German National Research Center for Information Technology, 2001. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,"
        },
        {
            "title": "Preprint",
            "content": "Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv. org/abs/2310.06825. William Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into Hilbert space. In Conference in modern analysis and probability, volume 26 of Contemporary Mathematics, pp. 189206. American Mathematical Society, 1984. Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS07, pp. 11771184, Red Hook, NY, USA, 2007. Curran Associates Inc. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention MICCAI 2015, pp. 234241. Springer International Publishing, 2015. doi: 10.1007/978-3-319-24574-4_28. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations (ICLR), 2017. URL https: //arxiv.org/abs/1701.06538. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: an all-mlp architecture for vision. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. URL https://arxiv.org/ abs/1706.03762. Zhengyuan Zhang, Yann Baccou, and Yann N. Dauphin. MoEfication: Transformer feed-forward layers are mixtures of experts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 61676177, Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.acl-long.425."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DETAILS OF EXPERIMENT SETTINGS A.1.1 SUMMARY OF DATASETS AND TASKS Table 3 summarizes the datasets, tasks, and input/output signal dimensions. Table 3: Summary of datasets, tasks, and input/output sizes Dataset MNIST MNIST MNIST ImageNet-32 ImageNet-32 Task Generative Classification Denoising Super-resolution Denoising Super-resolution Input Size 28 ˆ 28 ˆ 1 28 ˆ 28 ˆ 1 (noisy) 14 ˆ 14 ˆ 1 32 ˆ 32 ˆ 3 (noisy) 16 ˆ 16 ˆ Output Size 28 ˆ 28 ˆ 1 28 ˆ 28 ˆ 1 28 ˆ 28 ˆ 1 32 ˆ 32 ˆ 3 32 ˆ 32 ˆ 3 Recover high-resolution natural scene image Description Generate GT image for predicted class Remove artificially added noise Recover high-resolution handwritten image Remove artificially added noise"
        },
        {
            "title": "Preprint",
            "content": "A.1.2 TRAINING SETTING DETAILS All experiments were conducted using NVIDIA RTX A6000 and RTX 3090 GPUs. The images were mapped to [0,1] before training, and we employed the AdamW (Loshchilov & Hutter, 2017) optimizer with linear learning rate scheduler and no warm-up period. MNIST. The original training set of 60,000 images was randomly partitioned into 50,000 samples for training and 10,000 for validation, while the original test set of 10,000 images was reserved for final evaluation. The MLP architectural parameters were searched over the ranges dh r4, 2500s, dz r785, 4500s, and r1, 40s, while the learning rate t1ˆ104, 5ˆ104, 1ˆ103, 5ˆ103u. . All experiments were repeated 5 times, and we report the mean and standard deviation (µ σ) across runs. Note that during grid search, we constrained dz ą dx and dh ă dz for the Hourglass architecture, while dh ą dz for the conventional MLP, following their respective architectural definitions. Generative Classification: Ground truth images were randomly selected for each digit. Training was conducted with batch size of 128 for 50 epochs. Denoising: Noisy images were prepared by adding Gaussian noise (mean = 0, std = 0.25). Training used batch size 128 for 30 epochs. Super-resolution: Downscaled images were prepared using bicubic interpolation, reducing the original 28 ˆ 28 ˆ 1 images to 14 ˆ 14 ˆ 1. Training applied 4ˆ data augmentation (original, horizontal flip, vertical flip, and combined horizontal-vertical flip) with batch size 128 for 50 epochs. ImageNet-32. The complete original training set of 1,281,167 images was utilized for training, and the original validation set of 50,000 images was randomly split into 25,000 samples for validation and 25,000 for testing. We report the performance on the test set using the model that achieved the lowest validation loss. The MLP architectural parameters were searched over the ranges dh r4, 2500s, dz r8, 2200s, and r1, 30s, while the learning rate t1 ˆ 104, 3 ˆ 104, 5 ˆ 104, 7 ˆ 104u. All experiments were repeated 5 times, and we report the mean and 5ˆ standard deviation (µ 5σ) across runs. Note that during grid search, we constrained dz ą dx and dh ă dz for the Hourglass architecture, while dh ą dz for the conventional MLP, following their respective architectural definitions. Denoising: Noisy images were prepared by adding Gaussian noise (mean = 0, std = 0.25). Training used 4ˆ data augmentation (original, horizontal flip, vertical flip, and combined horizontal-vertical flip) with batch size 512 for 2 epochs. Super-resolution: Downscaled images were prepared using bicubic interpolation, reducing the original 32 ˆ 32 ˆ 3 images to 16 ˆ 16 ˆ 3. Training applied 4ˆ data augmentation (original, horizontal flip, vertical flip, and combined horizontal-vertical flip) with batch size 512 for 2 epochs."
        }
    ],
    "affiliations": [
        "MediaTek Research",
        "National Taiwan University"
    ]
}