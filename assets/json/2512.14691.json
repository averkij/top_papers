{
    "paper_title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": [
        "Zefan Cai",
        "Haoyi Qiu",
        "Tianyi Ma",
        "Haozhe Zhao",
        "Gengze Zhou",
        "Kung-Hsiang Huang",
        "Parisa Kordjamshidi",
        "Minjia Zhang",
        "Xiao Wen",
        "Jiuxiang Gu",
        "Nanyun Peng",
        "Junjie Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 1 9 6 4 1 . 2 1 5 2 : r MMGR: Multi-Modal Generative Reasoning Zefan Cai,1, Haoyi Qiu,2, Tianyi Ma,3,, Haozhe Zhao,4, Gengze Zhou5, Kung-Hsiang Huang6, Parisa Kordjamshidi3, Minjia Zhang4, Xiao Wen7, Jiuxiang Gu8, Nanyun Peng2, Junjie Hu1 1University of WisconsinMadison, 2University of California, Los Angeles, 3Michigan State University, 4University of Illinois UrbanaChampaign, 5University of Adelaide, 6Salesforce AI Research, 7Microsoft, 8Adobe Research Equal Contribution, Done during visit to the University of California, Los Angeles. Video foundation models have made striking progress in synthesizing visually compelling and temporally coherent content, yet their viability as world simulators hinges on whether they internalize the physical, logical, and spatial constraints that govern reality. Existing evaluation metricssuch as Fréchet Video Distance (FVD)largely emphasize perceptual fidelity, leaving critical reasoning failures undetected, including hallucinations that violate causal structure, physical laws, and global consistency. To address this gap, we propose principled evaluation framework grounded in five core reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal reasoning. Building on this framework, we introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), comprehensive benchmark suite designed to assess generative reasoning across three complementary domains: Abstract Reasoning (e.g., ARC-AGI, Sudoku), Embodied Navigation (e.g., real-world 3D navigation and localization), and Physical Commonsense (e.g., sports and compositional physical interactions). MMGR evaluates both video and image generative models using fine-grained, domainspecific metrics that require holistic correctness rather than partial success. We benchmark state-ofthe-art video generation modelsincluding Veo-3, Sora-2, and Wan-2.2alongside leading image generation models such as Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image, revealing pronounced performance asymmetry across modalities. While current models achieve moderate success on Physical Commonsense tasks, they fail catastrophically on Abstract Reasoning (achieving < 10% accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Through detailed quantitative analysis and human evaluation, we identify key limitations in existing training paradigms: severe imbalance favoring perceptual data over symbolic reasoning, architectural weaknesses in maintaining global state consistency, and optimization objectives that reward visual plausibility over causal correctness. By unifying abstract logic, embodied interaction, and intuitive physics under single evaluation framework, MMGR provides diagnostic lens into the reasoning deficits of modern generative models and outlines concrete roadmap toward physically grounded, logically consistent, and reasoning-aware world models. Repo: https://github.com/Zefan-Cai/MMGR Contact: zefncai@gmail.com, haoyiqiu@g.ucla.edu, matiany3@msu.edu, haozhez6@illinois.edu"
        },
        {
            "title": "1 Introduction",
            "content": "The field of generative artificial intelligence has achieved paradigm shift with the advent of large-scale text-tovideo models (OpenAI, 2024b; Ho et al., 2022a; Singer et al., 2022; Blattmann et al., 2023). These systems can now synthesize photorealistic, diverse, and temporally rich scenes from simple natural-language prompts. This capacity to generate dynamic visual narratives promises to revolutionize filmmaking, scientific visualization, embodied simulation, and robotics. However, as generative models scale, evaluation remains critical bottleneck. Conventional metricssuch as Fréchet Video Distance (FVD) (Unterthiner et al., 2018a), Inception Score (IS) (Salimans et al., 2016), and CLIP-based similarity (Radford et al., 2021)prioritize perceptual fidelity: assessing whether video looks realistic or aligns semantically with caption. Yet, these metrics remain blind to world consistency and physical plausibility. Consequently, model might render visually stunning billiards shot where balls pass through one another, or navigation sequence where an agent teleports through wallshallucinations that satisfy texture-based metrics while violating fundamental laws of reality. 1 Figure 1 Overview of our proposed Multi-Modal Generative Reasoning (MMGR) benchmark. MMGR assesses whether generative modelsboth video and imagecan perform coherent reasoning across three domains: Abstract Reasoning, Embodied Navigation, and Physical Commonsense. Given an input image and generation prompt, video models (Veo-3, Sora-2, Wan-2.2) produce multi-frame trajectories, while image models (Nano-Banana/Pro, GPT-4o-image, Qwen-image) generate single-frame solutions. VLM-based evaluator (Gemini-2.5-Pro) then scores each output using structured criteria, including an overall primary metric. For curated subset of samples, we additionally conduct human evaluations. The full pipeline enables fine-grained, domain-sensitive analysis of generative reasoning capabilities. We argue that for video generation to evolve from mere image animation to genuine world modeling (Ha & Schmidhuber, 2018; LeCun, 2022), models must acquire foundational reasoning capabilities akin to human intuitive physics and cognition. Moving beyond superficial fidelity (Huang et al., 2024; Liu et al., 2024b), we propose formal evaluation framework asking: Can video model reason about the physical and logical constraints of the content it generates? Drawing on theories of core knowledge and cognitive development (Spelke & Kinzler, 2007; Lake et al., 2017), we posit that robust world simulation rests on five complementary pillars of reasoning: 1. Physical Reasoning: Understanding intuitive physics, such as object permanence, gravity, collisions, and material properties. This capability aligns with theories of core knowledge in human cognition (Spelke & Kinzler, 2007; Baillargeon, 1987; Ullman et al., 2017; Piloto et al., 2022) and is prerequisite for robust simulation and interaction (Battaglia et al., 2013; Yi et al., 2020; Wu et al., 2015; Bear et al., 2021; Riochet et al., 2021; Bakhtin et al., 2019; Allen et al., 2020). 2. Logical Reasoning: Manipulating abstract concepts, following rules, and performing logical operations (e.g., if happens, then follows). This mirrors the symbolic processing required for System 2 reasoning (Kahneman, 2011; Marcus, 2001; Lake et al., 2017), enabling generalization beyond simple pattern matching (Chollet, 2019; Johnson et al., 2017; Xu et al., 2024; Barrett et al., 2018; Zhang et al., 2021; Webb et al., 2023). 3. 3D Spatial Reasoning: Understanding 3D spatial relationships, navigating environments, and grasping topology. This involves building an internal cognitive map of the world (Tolman, 1948; Gibson, 1979; Epstein et al., 1999) to ensure geometric consistency across camera viewpoints (Hudson & Manning, 2019; Zhong et al., 2020; Wu et al., 2022). 4. 2D Spatial Reasoning: The accurate interpretation of visual layouts, shapes, and relative positions in the projected image plane. This relies on compositional image understanding (Biederman, 1987; Kosslyn, 1980) to correctly ground spatial prepositions in complex prompts (Johnson et al., 2017; Chollet, 2019; Hudson & Manning, 2019). 5. Temporal Reasoning: Modeling causality, the order of events, and long-range dependencies. This captures the human perceptual ability to segment continuous streams into discrete causal events (Michotte, 1963; Zacks & Tversky, 2001), which is essential for maintaining narrative coherence (Xiao et al., 2020; Piergiovanni et al., 2020; Zhou et al., 2022; Yi et al., 2020). 2 Figure 2 Overview of the three domains in the MMGR benchmark. MMGR evaluates multi-modal generative reasoning across Domain 1: Abstract Reasoning, Domain 2: Embodied Navigation, and Domain 3: Physical Commonsense. (1) Abstract Reasoning includes Maze Solving, Sudoku Solving, ARC-AGI, and Math Challenge tasks, which test logical, 2D spatial, and temporal reasoning. (2) Embodied Navigation spans four environment-conditioned tasks: Panoramic View Last-Mile Navigation, Top-down View Real-World Navigation, 3D Real-World Navigation, and Simultaneous Localization and Generation (SLAG). The four tasks probe 2D/3D spatial reasoning, physical scene understanding, and coherent temporal planning. (3) Physical Commonsense covers Physical Concept scenarios and Sports activities, evaluating whether models produce videos that follow intuitive physics such as force, momentum, rotation, material behavior, and continuous motion. Together, these domains provide comprehensive testbed for assessing models ability to generate physically plausible, spatially grounded, and logically coherent solutions. Table 1 Overview of MMGRs three task domains and their alignment with the five core reasoning abilities. Each task is annotated with the specific reasoning skills it evaluates along with the total number of samples per task. Together, these domains provide comprehensive and systematic assessment of foundational reasoning competencies across abstract reasoning, embodied navigation, and physical commonsense. Physical Logical 3D Spatial 2D Spatial Temporal # Samples Benchmark Task Domain Domain 1: Abstract Reasoning Maze (Section 5) Sudoku (Section 6) ARC-AGI (Section 7) Math (Section 8) Domain 2: Embodied Navigation 3D Real-World Navigation (Section 12) Last-Mile Navigation (Ego-centric) (Section 10) Top-down View Navigation (Section 11) Simultaneous Localization and Generation (Section 13) Domain 3: Physical Commonsense Physical Concept (Section 14) Sports (Section 14) 240 300 456 327 120 120 120 25 25 Total Evaluation Samples 1,853 We explicitly distinguish 2D from 3D spatial reasoning because they rely on fundamentally different perceptual and computational mechanisms. While 2D reasoning operates on planar relationshipssuch as adjacency and relative positioning3D reasoning necessitates depth estimation, viewpoint transformation, and occlusion handling. This separation mirrors human cognition, which processes flat representations (e.g., maps) differently than volumetric environments1. Building upon this five-ability framework, we introduce MMGR (Multi-Modal Generative Reasoning), benchmark suite designed to systematically assess generative reasoning across diverse settings. MMGR encompasses three complementary domainsranging from abstract logic to embodied interactionthat each necessitate the coordination of multiple reasoning abilities (see Table 1, Figure 1, Figure 2): 1. Abstract Reasoning: Evaluates Logical, 2D Spatial, and Temporal reasoning in non-photorealistic environments. Key tasks include synthetic Maze environments (Ivanitskiy et al., 2023), Sudoku (Seely et al., 2025) (applying rule-based 2D spatial logic), Math (visualizing symbolic solution paths), and ARC-AGI (Chollet, 2019; Xu et al., 2024) (performing spatiallogical transformations). 2. Embodied Navigation: Assesses the synthesis of Physical, 2D/3D Spatial, and Temporal reasoning from an agent-centric perspective. Models are tasked with generating successful trajectories within diverse settings, specifically complex real-world navigation scenes utilizing both egocentric and top-down views (Chang et al., 2017; Ramakrishnan et al., 2021b; Savva et al., 2019; Anderson et al., 2018b; Zhu et al., 2017; Ramakrishnan et al., 2021a; Chaplot et al., 2020a; Deitke et al., 2020). 3. Physical Commonsense: Probes the understanding of intuitive physics (Battaglia et al., 2013; Yi et al., 2020; Bear et al., 2021; Bakhtin et al., 2019) and object dynamics. The scope extends from fundamental concepts (leveraging the VideoPhy (Bansal et al., 2024) ontology) to compositional sports scenarios that require modeling physically plausible interactions consistent with real-world constraints. By evaluating state-of-the-art image generative models (i.e., Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image) and video generative models (i.e., Veo-3, Sora-2, Wan-2.2) (DeepMind, 2025b; OpenAI, 2024a; Qwen, 2024; DeepMind, 2025a; OpenAI, 2025; Wan, 2025) on MMGR, we deliver the first comprehensive characterization of their generative reasoning capabilities. Our results reveal consistent trend: while models demonstrate encouraging performance on Physical Commonsense tasks (e.g., Sports: 60%), they struggle markedly with Abstract Reasoning challenges such as ARC-AGI (<10%) (Chollet, 2019; Barrett et al., 2018) and with long-horizon, multi-step planning in the Embodied Navigation domain (e.g., S.L.A.G.: 3.64% holistic success (Ivanitskiy et al., 2023; Savva et al., 2019)). 1Our benchmark targets this dichotomy to enable fine-grained diagnosis of model capabilities: abstract tasks like Sudoku and ARC-AGI probe 2D grid-based logic, whereas embodied navigation tasks demand coherent 3D spatial understanding. 4 These performance patterns illuminate several critical deficiencies in current training recipes, offering guide for future model development: Training Data Imbalance: While current video corpora abound in naturalistic physical interactions (e.g., sports, everyday dynamics)explaining strong Physical Commonsense performancethey severely lack structured, symbolic reasoning data. This deficit leads to near-random performance on logic-heavy tasks like Sudoku (< 7%) and ARC-AGI. Furthermore, the stark disparity between Final Correctness (74%) and Intermediate Correctness (12%) on Math tasks (e.g., GSM8K) suggests that models are merely memorizing answer patterns rather than learning genuine multi-step reasoning. Architectural Limitations: The pronounced divergence between primary success metrics and holistic Overall scores (e.g., 80.56% vs. 20.83% on 3D Real-World Navigation) indicates that current architectures sacrifice global consistency for local plausibility. With Scene Consistency dropping to 40.28% and Destination Integrity to 25.45%, models struggle to enforce long-range spatial and temporal coherence. This highlights an urgent need for mechanismssuch as external memory, world-state representations, or structured latent spacesto sustain context across extended generation horizons. Optimization Objective Gaps: Current objectives prioritize perceptual fidelity (via reconstruction loss or adversarial objectives) over reasoning correctness. Consequently, models optimize for appearance rather than logical validityrendering visually convincing mazes or equations without actually solving them. Future work must integrate auxiliary objectives that reward rule adherence and causal consistency, potentially leveraging reinforcement learning from structured feedback or neuro-symbolic supervision. Ultimately, MMGR provides unified framework for diagnosing these limitations, charting path toward video generation systems that are physically grounded, logically consistent, and truly reasoning-aware."
        },
        {
            "title": "2 Related Work",
            "content": "Video Generation Models. The field of video generation has witnessed paradigm shift, evolving from early GAN-based approaches (Vondrick et al., 2016; Tulyakov et al., 2018) to diffusion-based systems (Ho et al., 2022b; Singer et al., 2022) and large-scale transformer architectures (Yan et al., 2021; Hong et al., 2022). Contemporary state-of-the-art models, including Sora (OpenAI, 2024b), Veo (DeepMind, 2024), and Kling (Kuaishou, 2024), demonstrate exceptional capacity for synthesizing high-fidelity, photorealistic video with complex temporal dynamics. However, while these models excel at surface-level perceptual quality, the extent to which they internalize the underlying physical laws and logical constraints of the world remains an active area of inquiry. Evaluation of Generative Models. Traditional evaluation metrics have largely prioritized appearance quality over semantic consistency. Metrics such as FVD (Unterthiner et al., 2018b) and Inception Score (IS) (Salimans et al., 2016) capture perceptual fidelity, while more recent benchmarks (Huang et al., 2024; Liu et al., 2024a) focus on textvideo alignment and basic temporal consistency. These tools, however, are insufficient for probing world modeling capabilities. They fall short of evaluating whether model possesses the reasoning skills necessary to generate content that is not only visually plausible but also logically coherent and physically robust over long horizons. From Visual Understanding to Generative Reasoning. Prior benchmarks in video understanding (Girdhar & Ramanan, 2020; Goyal et al., 2017; Chollet, 2019) primarily assess discriminative modelstesting their ability to recognize interactions or perform symbolic reasoning on existing inputs. Similarly, embodied AI benchmarks (Savva et al., 2019) rely on rigid simulators to test perception. Our work shifts this paradigm from understanding to generation: requiring models to not merely interpret video, but to manifest reasoning processes through synthesis. Recent studies have begun to explore this frontier. Wiedemer et al. (2025) identify emergent Chain-of-Frames (CoF) reasoning in models like Veo-3, while Guo et al. (2025) utilize MME-CoF to expose failures in geometric consistency. Tong et al. (2025) further demonstrate competitive performance by Sora-2 across vision tasks. We build upon these insights by formalizing five-ability reasoning framework. Unlike prior works that focus on specific failure modes or emergent properties, we provide 5 Table 2 Summary of MMGR benchmark statistics across three domains. Domain / Task # Samples Primary Metric Domain 1: Abstract Reasoning Maze Sudoku ARC-AGI Math Domain 2: Embodied Navigation 3D Real-World Navigation Last-Mile Navigation Top-down View Navigation SLAG Domain 3: Physical Commonsense Physical Concept Sports 240 300 456 327 120 120 120 120 25 25 Total 1, Valid Solution Valid Solution Valid Solution Valid Solution Overall Success Overall Success Overall Success Overall Success Physical Plausibility Physical Plausibility holistic assessment spanning Abstract Reasoning, Embodied Navigation, and Physical Commonsense, creating generative adaptations of rigorous tasks such as ARC-AGI to test the limits of current world models."
        },
        {
            "title": "3 Benchmark Overview",
            "content": "Our three evaluation domains are grounded in the principle that world modeling necessitates both internal and external simulation capabilities (Ha & Schmidhuber, 2018; LeCun, 2022). Abstract Reasoning targets internal simulationthe capacity to manipulate symbolic representations, adhere to logical rules, and execute mental transformations independent of physical reality. Conversely, Embodied Navigation and Physical Commonsense evaluate external simulationthe ability to model interactions within the physical world. Specifically, Embodied Navigation tests models capacity to simulate agent-environment dynamics for spatial planning, whereas Physical Commonsense assesses an understanding of the intuitive physics governing real-world objects. These domains are strategically complementary; together, they exercise the five core reasoning abilities outlined in Section 1. Abstract Reasoning prioritizes Logical and 2D Spatial reasoning, while the external domains integrate 3D Spatial, Temporal, and Physical reasoning through dynamic scenarios (see Table 1 for the complete mapping). This design ensures comprehensive evaluation of the reasoning competencies essential for robust world modeling. Figure 2 shows several examples from MMGR."
        },
        {
            "title": "3.1 Abstract Reasoning",
            "content": "Maze. Designed to assess 2D spatial, logical, and temporal reasoning, this task requires models to navigate valid path from start cell (green) to goal cell (red) while avoiding obstacles. We employ DFS and Wilsons algorithms to generate 240 mazes across three difficulty levelsEasy (3 35 5), Medium (6 6 9 9), and Hard (10 1013 13)using four distinct start-goal configurations (e.g., corner-to-corner, random-to-random) (Ivanitskiy et al., 2023). Sudoku. This task evaluates constraint satisfaction and logical deduction (Seely et al., 2025). Models must complete grids such that every row, column, and subgrid contains unique digits. The dataset comprises 300 puzzles across two grid sizes (4 4 and 9 9) and three difficulty levels (Easy, Medium, and Hard), where complexity is modulated by the sparsity of initial clues. ARC-AGI. To evaluate abstract reasoning and few-shot rule induction, we utilize the ARC-AGI benchmark (Chollet, 2019). Models must infer latent transformation rules from input-output demonstration examples and apply them to unseen test cases. Our benchmark comprises 456 tasks from v1 (381 tasks) and v2 (75 tasks), classified by shape consistency (Match and Mismatch) and quantitative difficulty (Easy, Medium, and Hard). 6 Visual Math. We assess mathematical reasoning across diverse domains using five benchmarks: GSM8K (Cobbe et al., 2021) (grade school), MATH500 (Hendrycks et al., 2021) (high school), AIME 2024/2025 (Mathematical Association of America, 2024, 2025) (invitational competitions), and Omni-MATH (Gao et al., 2024) (Olympiadlevel). The resulting dataset contains 327 problems requiring logical deduction and spatial understanding."
        },
        {
            "title": "3.2 Embodied Navigation",
            "content": "3D Real-World Navigation. Utilizing cutaway dollhouse renderings from Matterport3D (Chang et al., 2017) and HM3D (Ramakrishnan et al., 2021b), this task assesses multi-room and multi-level spatial reasoning. Models operate from fixed third-person perspective to generate navigation trajectories, requiring them to interpret full 3D scene structures, including verticality and complex room connectivity. Last-Mile Navigation (Ego-centric). This setting presents 360 panoramic environment via proximal over-the-shoulder view. Models must synthesize wide-field visual context to execute short-range navigation, necessitating the interpretation of agent-centric layouts to generate goal-directed trajectories. Top-down View Navigation. Adopting fixed birds-eye perspective, this task targets global spatial planning and long-horizon prediction. Models generate trajectories on 2D overhead maps, emphasizing the ability to reason about global geometry and multi-step pathfinding. Simultaneous Localization and Generation (SLAG). SLAG integrates both 3D and top-down views, challenging models to jointly localize the agent while generating the surrounding scene layout. This requires maintaining geometric coherence and performing cross-view spatial alignment across distinct observation modalities. Dataset Configuration. We evaluate each of the four tasks on 120 samples. The dataset spans 24 configurations stratified by environmental complexity (single vs. multi-floor), view fidelity (quality 35), trajectory distance (short vs. long), and goal specification (visual marker vs. linguistic description)."
        },
        {
            "title": "3.3 Physical Commonsense",
            "content": "Physical Concept. Leveraging the VideoPhy ontology (Bansal et al., 2024), this task assesses intuitive understanding of fundamental physical interactions. We evaluate three core categories: Solid-Solid (143 captions), Solid-Fluid (146 captions), and Fluid-Fluid (55 captions). The dataset spans broad physical domains including statics, dynamics, kinematics, and hydrodynamics. Additionally, we incorporate VideoPhy v2 to expand the evaluation scope with 600 supplementary captions covering 197 unique physical actions. From the larger source corpus, we randomly sample 25 examples and ensure diversity across these interaction categories and physical domains to create balanced evaluation set. Sports. This task evaluates compositional physical reasoning within complex scenarios characterized by the intersection of multiple physical laws. The source corpus encompasses diverse activitiesspecifically Ballet (12), Skiing (13), Diving (12), and Swimming (13)challenging models to analyze phenomena such as momentum conservation, balance control, projectile motion, and fluid dynamics in goal-oriented contexts. To construct balanced evaluation set, we randomly sampled 25 diverse examples from this larger collection."
        },
        {
            "title": "4 Experimental Setup",
            "content": "To systematically evaluate zero-shot reasoning capabilities (Wiedemer et al., 2025; Guo et al., 2025; Tong et al., 2025), we benchmark state-of-the-art generative models across the ten tasks outlined in Section 3. Our analysis aims to quantify model performance and disentangle granular strengths and limitations within the five core reasoning dimensions."
        },
        {
            "title": "4.1 Data\nTable 2 provides a comprehensive statistical overview of the benchmark, which aggregates 1,853 testing\nsamples across three domains and ten tasks. To facilitate fine-grained capability analysis, we employ rigorous",
            "content": "7 difficulty stratifications and human verification. For Abstract Reasoning, complexity is modulated by grid dimensions (Maze, Sudoku), shape consistency (ARC-AGI), and mathematical scope (Math). Similarly, Embodied Navigation tasks are organized into 24 distinct configurations defined by environmental complexity, visual fidelity, trajectory distance, and goal specification."
        },
        {
            "title": "4.3 Evaluation Protocol",
            "content": "VLM-based Evaluation. Following established video benchmarking protocols (Huang et al., 2024; Liu et al., 2024b; Wiedemer et al., 2025), we employ Gemini 2.5-Pro (Comanici et al., 2025) as unified automated evaluator. The model assesses generation quality using task-specific rubrics that evaluate both the plausibility of the reasoning process and the correctness of the final result. Metrics Aggregation. We begin by reporting diverse task-specific fine-grained metrics to dissect model performance across individual reasoning dimensions. Building on these components, we define strict primary metric for each task that necessitates the simultaneous satisfaction of all sub-metrics (detailed in Table 2). We prioritize this holistic measure to address the disparity between partial success and complete correctnessa gap that typically inflates performance estimates by 1.24 when ignored."
        },
        {
            "title": "4.4 Models for Evaluation",
            "content": "Our study evaluates diverse selection of state-of-the-art multimodal generative models, spanning both video and image modalities. We include representative closed-source and open-weights models from major research laboratories, as detailed in Table 3. Table 3 Generative models evaluated in MMGR benchmark, categorized by modality. Model Video Generation Models Sora-2 (OpenAI, 2024b) Veo-3 (DeepMind, 2024) Wan-2.2 (Wan, 2025) Source Closed Closed Open Image Generation Models Nano-banana (Comanici et al., 2025) Closed Nano-banana Pro (DeepMind, 2025b) Closed Closed GPT-4o-image (OpenAI, 2024a) Open Qwen-image (Qwen, 2024)"
        },
        {
            "title": "4.5 Human Evaluation",
            "content": "To establish ground-truth performance and validate the reliability of VLM-based automatic evaluation, we conducted systematic human annotation on generated outputs. This human evaluation serves as critical complement to AutoEval, particularly for tasks requiring nuanced judgment of temporal consistency, spatial reasoning, and physical plausibility. Annotation Interface. We developed web-based annotation platform (Figure 3) featuring full video playback controls including frame-by-frame navigation and adjustable playback speed. The interface displays the original task prompt alongside the generated video and provides structured evaluation forms tailored to each 8 (a) Maze Navigation interface with failure mode detection (Maze Changed, Cross Wall) and navigation behavior metrics. (b) Math Reasoning interface with process correctness and outcome accuracy assessment. Figure 3 Human annotation interface for evaluating generated videos. The interface provides video playback controls (frame-by-frame navigation, speed adjustment), displays the original problem/condition, and presents structured evaluation forms with task-specific metrics. task type. Annotators assess multiple dimensions including task completion, process correctness, and failure modes with associated confidence ratings. Evaluation Protocol. We recruited 6 annotators with bachelor education background. The training process included 4-hour instruction session, 50-video practice phase, and calibration meetings."
        },
        {
            "title": "5.2 Hard-Level Control",
            "content": "To ensure diverse and controllable set of evaluation cases, we leverage the open-source Python library maze-dataset (Ivanitskiy et al., 2023) to programmatically generate mazes of varying structure and difficulty. We vary task parameters along three axes: Generators (2 types): We employ two maze-generation algorithmsDepth-First Search (DFS) and Wilsons Algorithmto produce topologically diverse maze layouts. Grid Sizes (10 levels): Maze difficulty is scaled across ten grid sizes, ranging from 33 to 1313. StartGoal Placement (4 schemes): Each maze is instantiated under four placement schemescorner-tocorner, corner-to-random, random-to-corner, and random-to-randomto prevent models from overfitting to single trajectory pattern. minimum startgoal distance is enforced to rule out trivial solutions. For each generator, we produce 120 mazes, comprising 40 Easy (33-55), 40 Medium (66-99), and 40 Hard (1010-1313) instances. Overall, this yields 240 mazes across the two generators. With our generation algorithm, each maze only has one solution path. Figure 4 presents representative examples across difficulty levels and startend configurations, along with their corresponding solutions. (a) Easy 55 (cc) (b) Easy 33 (rc) (c) Easy 55 (cr) (d) Easy 415 (rr) (e) Medium 99 (cc) (f) Medium 99 (rc) (g) Medium 99 (cr) (h) Medium 88 (rr) (i) Hard 1111 (cc) (j) Hard 1313 (rc) (k) Hard 1313 (cr) (l) Hard 1313 (rr) Figure 4 Maze-solving results across three difficulty levels (Easy, Medium, Hard) and four start-end configurations: cornercorner, randomcorner, cornerrandom, and randomrandom. Each subfigure shows unique DFS solution illustrating how maze size and start/end randomness affect traversal patterns. Solution paths are highlighted blue."
        },
        {
            "title": "5.3 Evaluation and Metrics",
            "content": "We evaluate generated videos and images using VisionLanguage Model (VLM)based evaluator, Gemini2.5-Pro (Comanici et al., 2025). The evaluator receives three inputs: (i) the model-generated video or image, (ii) the ground-truth maze solution image, and (iii) structured evaluation prompt (with modality-specific variants for video vs. image). Given these inputs, the VLM judges whether the model solved the task and provides fine-grained feedback across multiple failure modes. The evaluation prompt asks: Does the green square (start) reach and stop on the red square (end)? Does the green square ever touch or cross black wall? Does the layout of the black walls or the position of the red square change at any time? Using the VLMs responses, we compute one primary metric and four fine-grained metrics: Maze Changed (Failure Mode): (i) Video: 1 if the maze layout changes in any frame, 0 if unchanged throughout. (ii) Image: 1 if the maze structure differs from the solution reference, 0 otherwise. Cross Wall (Failure Mode): (i) Video: 1 if the green square crosses black wall in any frame, 0 only if it stays on white paths at all times. (ii) Image: 1 if the blue path touches or crosses black walls, 0 if fully contained within the white corridors. Action Reflection: (i) Video: 1 if the video shows exploratory behavior (e.g., backtracking or trying multiple paths), 0 for single direct route. (ii) Image: 1 if the rendered blue trajectory depicts multiple 10 attempted paths, 0 for single direct path. Target Achievement: (i) Video: 1 if the green square reaches and stops on the red square in any frame, otherwise. (ii) Image: 1 if continuous, valid blue path connects start and end. Overall Score: 1 only if Maze Changed=0 AND Cross Wall=1 AND Task Completion=1; 0 otherwise."
        },
        {
            "title": "5.4 Case Study",
            "content": "Image Generation. Figure 5a illustrates Nano-Bananas behaviors, showing not only perfectly solved outputs but also common failure modes unique to image-based generation. These include wall-crossing artifacts in the final predicted trajectory, slight distortions of maze geometry, and action-reflection artifactscases where the rendered trajectory contains redundant loops or implausible detours even though no temporal dynamics are involved. These artifacts reflect the models uncertainty when inferring long-range paths from single static instruction, resulting in inconsistent or physically implausible solution traces. Video Generation. Figure 5b shows Veo-3s successful generations. Frame-by-frame annotations reveal that Veo3 can preserve the mazes topology, keeps the green square strictly on valid white paths, and maintains consistent wall boundaries from start to finish. The model occasionally performs mild exploratory behaviorssuch as brief backtracking or short directional adjustments between early framesbefore ultimately converging on the correct route. Notably, these explorations remain structurally valid: the agent never crosses walls or distorts the environment, and the maze remains unchanged throughout the entire sequence. In contrast, Figure 5c highlights Veo-3s failure cases. Here, the model sometimes introduces structural inconsistenciesremoving or altering wall segments, inserting open passages, or shifting the geometry of the target region. In other cases, the green square traverses invalid regions (e.g., sliding across black walls during transitions) or produces contradictory intermediate frames despite ending at the correct goal cell. The examples also show how subtle frame-to-frame drifts, such as disappearing wall pixels or morphing corridors, can accumulate into integrity violations not captured by coarse success metrics. Collectively, these case studies show that both models may successfully reach the red goal but still differ dramatically in path fidelity, wall adherence, and temporal consistency. The frame-level evidenceranging from clean, stable trajectories to structurally inconsistent or wall-violating behaviorsunderscores the necessity of fine-grained maze-evaluation framework capable of capturing these nuanced, multimodal failure modes that simple goal-achievement metrics overlook."
        },
        {
            "title": "5.5 Evaluation Results",
            "content": "Key Finding: The Illusion of Competence and Physical Grounding Our analysis uncovers two critical disconnects in video generation reasoning. First, dichotomy of simulation: Veo-3 mimics the result, generating direct solution paths via pattern matching but failing to respect impermeable boundaries. Conversely, Sora-2 mimics the process, performing visible reasoning (backtracking, hesitation) but losing logical coherence (hallucinating maze structures). Second, human verification reveals an evaluation gap: Automated VLM metrics systematically overestimate model competence by missing transient physics violations. While Auto-Eval reports moderate success for Veo-3, human review reveals that the model effectively cheats by clipping through walls in fast motiona failure mode invisible to current VLMs. Consequently, true adherence to physical constraints remains near zero, suggesting current models prioritize visual plausibility over logical validity. 5.5.1 VLM-Based Evaluation Table 4 isolates the reasoning capabilities of video and image generative models by correlating path planning strategies with environmental consistency. Veo-3 displays Direct Execution reasoning style: its near-zero Action Reflection (0.00%3.00%) confirms it generates single, non-exploratory route without backtracking. While this allows for high Target Achievement (up to 62.00%), the models reliance on one-shot generation comes at the cost of physical precision, evidenced by significant Cross Wall rates (18.00%25.00%). This suggests Veo-3 solves mazes via pattern-matching (predicting the solution trajectory directly) rather than 11 (a) Success and failure cases generated by Nano-Banana. Solution paths are highlighted in blue. (b) Success cases generated by Veo-3. Solution paths are highlighted in blue. (c) Failure cases generated by Veo-3. Solution paths are highlighted in blue. Figure 5 Case studies comparing success and failure behaviors across Nano-Banana and Veo-3. 12 Table 4 Quantitative results for the 2D Maze task. We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image) across two maze generation algorithms (DFS and Wilsons) and three difficulty levels (Easy, Medium, and Hard). The highest overall scores in each setting are highlighted in bold. Model Maze Changed Cross Wall Action Reflection Target Achievement Overall Fine-grained Metrics Primary Metric Generator: Depth-First Search Level: Easy (3355) Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Medium (6699) Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Hard (10101313) Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Generator: Wilsons Algorithm Level: Easy (3355) Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Medium (6699) Video Models Veo-3 Sora-2 Wan-2. Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Hard (10101313) Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image 15.50% 67.50% 35.00% 5.00% 5.00% 95.00% 5.00% 0.50% 47.50% 10.83% 0.63% 0.00% 72.50% 2.50% 0.00% 57.50% 6.67% 0.00% 0.00% 62.50% 7.50% 3.50% 67.50% 32.50% 10.50% 10.00% 82.50% 5.83% 1.25% 62.50% 14.17% 0.00% 2.50% 75.00% 4.17% 1.25% 45.00% 10.00% 1.25% 0.00% 60.00% 2.50% 1.00% 77.50% 7.50% 15.50% 10.00% 7.50% 15.00% 0.00% 60.00% 28.33% 11.88% 30.00% 0.00% 12.50% 1.50% 60.00% 35.00% 18.33% 20.00% 5.00% 11.67% 2.50% 40.00% 8.33% 16.50% 30.00% 0.00% 9.17% 1.25% 47.50% 15.83% 13.75% 15.00% 5.00% 14.17% 0.63% 52.50% 28.33% 23.13% 17.50% 5.00% 20.00% 25.50% 7.50% 79.17% 30.00% 42.50% 5.00% 23.33% 25.63% 12.50% 90.83% 30.63% 25.00% 10.00% 28.33% 18.50% 7.50% 80.83% 24.17% 12.50% 5.00% 11.67% 21.50% 10.00% 84.17% 47.50% 32.50% 15.00% 12.50% 15.63% 12.50% 85.83% 36.88% 17.50% 5.00% 25.00% 18.75% 10.00% 89.17% 27.50% 22.50% 7.50% 27.50% 13 60.50% 12.50% 10.00% 85.00% 90.00% 72.50% 65.00% 50.75% 10.00% 23.33% 71.25% 82.50% 82.50% 44.17% 60.00% 25.00% 20.00% 60.00% 80.00% 77.50% 42.50% 61.50% 15.00% 15.00% 81.00% 85.00% 90.00% 67.50% 55.63% 20.00% 14.17% 70.00% 80.00% 80.00% 47.50% 58.75% 10.00% 13.33% 60.63% 75.00% 77.50% 38.33% 42.00% 2.50% 1.67% 15.50% 17.50% 0.00% 11.67% 38.69% 7.50% 1.67% 4.38% 2.50% 5.00% 0.00% 51.50% 10.00% 5.00% 0.83% 5.00% 0.00% 1.67% 46.50% 5.00% 1.67% 6.50% 12.50% 2.50% 20.00% 47.50% 10.00% 1.67% 1.25% 2.50% 5.00% 0.00% 45.63% 2.50% 0.83% 0.00% 5.00% 7.50% 0.00% active search, often fudging collisions to maintain momentum. In contrast, Sora-2 exhibits Performative it achieves high Action Reflection scores (40.00%78.00%) by generating visible exploratory Reasoning: behaviors like backtracking and trying multiple paths. However, this exploration is fundamentally disconnected from valid state maintenancethe model frequently hallucinates new maze structures (Maze Changed 45.00% 68.00%) while searching, and rarely achieves the target (10.00%25.00%). Wan-2.2 and the Nano-banana family demonstrate failure of physical constraint satisfaction: their high Cross Wall rates (up to 91.00%) indicate they treat walls as visual suggestions rather than impermeable boundaries, allowing them to traverse the maze without solving the topological puzzle. Finally, GPT-4o-image bypasses the reasoning task entirely, altering the problem definition (Maze Changed up to 95.00%) to fabricate successful outcome. 5.5.2 Human Evaluation To establish ground-truth performance estimates and validate the reliability of our VLM-based evaluator, we conducted human evaluation on subset of Veo-3s generated videos. Table 5 presents side-byside comparison of Auto-Eval versus Human-Eval across all maze generators and difficulty levels. The results highlight substantial divergence between automated and human assessments, with human evaluators consistently uncovering failure modes that the VLM overlooks. Table 5 Comparison of Auto-Eval (VLM-based) and Human-Eval results for Veo-3 on the 2D Maze task across two maze generation algorithms (DFS and Wilsons) and three difficulty levels (Easy, Medium, and Hard). Setting MC CW AR TA Overall MC CW AR TA Overall Auto-Eval Human-Eval Generator: Depth-First Search Easy (3355) Medium (6699) Hard (10101313) 15.50% 25.50% 1.00% 60.50% 42.00% 10.00% 80.00% 40.00% 70.00% 10.00% 0.00% 0.50% 25.63% 0.00% 50.75% 38.69% 20.00% 90.00% 90.00% 20.00% 0.00% 0.00% 18.50% 1.50% 60.00% 51.50% 20.00% 100.00% 40.00% 70.00% Generator: Wilsons Algorithm Easy (3355) Medium (6699) Hard (10101313) 3.50% 21.50% 2.50% 61.50% 46.50% 40.00% 70.00% 60.00% 40.00% 20.00% 0.00% 1.25% 15.63% 1.25% 55.63% 47.50% 20.00% 100.00% 60.00% 40.00% 0.00% 1.25% 18.75% 0.63% 58.75% 45.63% 40.00% 100.00% 70.00% 40.00% Note: MC = Maze Changed, CW = Cross Wall, AR = Action Reflection, TA = Target Achievement. The most critical discrepancy appears in the Cross Wall (CW) metric. While Auto-Eval reports moderate wallcrossing rates (16.00%26.00%), Human-Eval detects significantly higher violation rates (70.00%100.00%)a 35 increase. Notably, human evaluators identified wall-crossings in 100.00% of samples for DFS Hard, Wilsons Medium, and Wilsons Hard mazes, whereas Auto-Eval reported rates of only 16.00%19.00%. This confirms that the VLM evaluator systematically fails to capture transient wall-crossing events, likely due to insufficient temporal resolution or frame-dropping when processing fast-moving agents. These Cross Wall failures propagate directly to the Overall Score, which mandates zero wall-crossings for success. Consequently, while Auto-Eval suggests moderate competence (39.00%52.00%), Human-Eval reveals that true performance is near zero (0.00%20.00%). In four of the six configurations, Veo-3 achieved zero successful completions according to human review. This indicates that the automated metrics overestimate the models maze-solving success by factor of 25. Conversely, the Action Reflection (AR) metric exhibits an inverse pattern. Human evaluators assigned substantially higher scores (40.00%90.00%) than the VLM (0.00%3.00%). This suggests that humans are sensitive to subtle exploratory behaviorssuch as hesitations, micro-adjustments, or partial backtrackingthat the VLM fails to classify as meaningful reflection. While these behaviors do not constitute full multi-path exploration, they indicate that the model engages in implicit trajectory reasoning that automated metrics miss. Finally, the Maze Changed (MC) metric shows narrower gap, with Human-Eval reporting slightly higher rates (10.00%40.00%) than Auto-Eval (0.00%16.00%). This suggests that structural changes to maze geometry are more visually salient to VLMs than the fleeting motion artifacts involved in wall-crossings. In summary, these findings expose critical limitation in VLM-based evaluation for temporally dense tasks: automated evaluators systematically overestimate success by missing transient but fatal errors. Human calibration reveals that while Veo-3 demonstrates emerging spatial understanding, its operational reliability is substantially lower than automated metrics imply. 5.5.3 Limitations and Insights from VLM-Based Evaluation Our evaluation pipeline relies on VLM-based automated evaluator (AutoEval), instantiated as Gemini2.5-Pro. While AutoEval delivers consistent ratings for Easy and Medium tasks, its reliability degrades significantly at the Hard difficulty level. In these complex scenarios, high-velocity agent movements challenge the VLMs temporal resolution. The evaluator frequently drops frames, missing transient but critical violationsspecifically Cross Wall and Maze Changed errorsthat occur within single frames. Consequently, AutoEval systematically overestimates performance on harder tasks. To calibrate this blind spot, Human Evaluation (HumanEval) serves as an essential baseline; we argue that presenting HumanEval scores alongside AutoEval is strictly necessary for Hard settings to provide truthful performance picture. Beyond validation reliability, our analysis highlights the critical concept of evaluability. Models that explicitly visualize reasoningsuch as Nano-Banana, which renders its intended trajectory as static blue pathfundamentally transform the evaluation task. This plan visualization converts challenging temporal verification problem (tracking frame-by-frame collisions) into straightforward spatial comparison (checking static path validity), thereby enhancing transparency and trustworthiness. However, this benefit relies on structural faithfulness. When prompted to generate similar trajectory visualizations, Veo-3 frequently produced erratic, hallucinated blue curves covering area the agent never visited. Far from aiding interpretation, these hallucinations obscured the models actual reasoning and actively confused the Gemini-2.5-Pro evaluator, further degrading AutoEval accuracy. This underscores key insight: while explicit visual reasoning can improve evaluability, it is only beneficial when the visualized artifacts remain grounded in the physical reality of the environment."
        },
        {
            "title": "6 Sudoku",
            "content": "We introduce the Sudoku task to evaluate models core abilities in Constraint Satisfaction and Logical Reasoning. This task directly probes models capacity for logical inference-its ability to derive valid conclusions under structured set of logical rules. In Sudoku, the model must internalize the underlying constraints that govern valid solutions, ensuring that each symbol appears exactly once in every row, column, and subgrid. Moreover, the task explicitly tests deductive reasoning: the step-by-step application of these constraints to infer the only logically consistent values for each empty cell. Successful completion thus requires the model to integrate global structural understanding with local deductive consistency, producing fully valid and complete grid (Seely et al., 2025)."
        },
        {
            "title": "6.1 Hard-Level Control",
            "content": "To build diverse and systematically controlled evaluation set, we use an open-source Sudoku generation library (Seely et al., 2025) to create puzzles spanning range of structural and reasoning complexities. We vary task parameters along two main axes: Grid Size (2 levels): We generate puzzles in two standard configurations44 (digits 14) and 99 (digits 19). The larger grid substantially increases combinatorial difficulty and requires deeper multi-step logical inference. Puzzle Difficulty (3 levels): For each grid size, we control difficulty by varying the number of initial clues (pre-filled digits). The three levelsEasy (many clues), Medium, and Hard (few clues while ensuring unique solution)modulate the search space and constraint-satisfaction complexity. For every difficulty level (Easy, Medium, Hard ), we generate 100 puzzles: 50 for the 44 grid and 50 for the 99 grid. This results in balanced evaluation set of 300 Sudoku puzzles spanning controlled spectrum of structural sizes and reasoning challenges. Figure 6 provides representative puzzles and solutions for each difficulty level and grid size, with each puzzle displayed directly above its corresponding solution. 15 (a) 44 (Easy) (b) 44 (Easy) Solution (c) 99 (Easy) (d) 99 (Easy) Solution (e) 44 (Medium) (f) 44 (Medium) Solution (g) 99 (Medium) (h) 99 (Medium) Solution (i) 44 (Hard) (j) 44 (Hard) Solution (k) 99 (Hard) (l) 99 (Hard) Solution Figure 6 34 grid showing Sudoku puzzles and their solutions across Easy, Medium, and Hard difficulty levels for both 44 and 99 grid sizes, arranged so each puzzle is immediately followed by its solution."
        },
        {
            "title": "6.2 Evaluation and Metrics",
            "content": "We evaluate both video and image outputs using Vision-Language Model (VLM)based evaluator, Gemini2.5-Pro (Comanici et al., 2025). The evaluator receives three inputs: (i) the generated video or image, (ii) the ground-truth solved Sudoku grid, and (iii) structured evaluation prompt (with modality-specific variants). Using these inputs, the VLM assesses whether the model produces valid Sudoku solution and identifies failure modes related to rule consistency, clue preservation, and reasoning behavior. From the evaluators structured outputs, we derive one primary success metric and four fine-grained metrics: Clues Changed (Failure Mode): (i) Video: 1 if any original digits (clues) are modified, removed, or displaced in any frame; 0 if all clues remain intact across the entire sequence. (ii) Image: 1 if any given clue differs from the original puzzle image; 0 otherwise. Constraints Violation (Failure Mode): The fraction of Sudoku constraints correctly satisfied in the final output (rows, columns, and subgrids). value of 1 indicates full rule compliance; 0 indicates violation. Completion Accuracy: The fraction of correctly filled originally empty cells, computed by comparing the models final output against the ground-truth solution. Action Reflection: (i) Video: 1 if the sequence shows interpretable step-by-step reasoning (e.g., gradual cell updates without overwriting earlier entries); 0 if digits appear simultaneously or in an erratic order. (ii) Image: Not applicable. Overall Score: 1 only if Clues Changed=0 AND Constraints Violation=0 AND Completion Accuracy=1; 0 16 otherwise. Figure 7 Case Study: Success and failure cases generated by Nano-Banana."
        },
        {
            "title": "6.3 Case Study",
            "content": "Figure 7 and Figure 8 illustrate several solution trajectories generated by Nano-Banana (image generative model) and Veo-3 (video generative model) for the same 44 easy Sudoku puzzle. In the successful case, Nano-Banana solves the puzzle perfectlypreserving all initial clues, following valid reasoning steps, and reaching the correct final configuration without any constraint violations. However, when the model fails, the error patterns align closely with our diagnostic evaluation metrics: missing digits, repeated numbers within row or box, constraint violations across frames, and changes to the original clues. These failures often arise despite our detailed, instruction-focused generation prompts, suggesting that Nano-Banana still lacks the deeper abstract reasoning capacity required for consistent multi-step logical problem solving. Although Veo-3 performs worse overall in producing fully correct solutions, its video outputs reveal set of systematic and interpretable reasoning behaviors: (1) Veo-3 demonstrates strong positional bias: it almost always initiates its reasoning from the top-left box of the grid. Humans, in contrast, typically adapt their solving order based on puzzle structure, difficulty, or available constraintssuggesting that Veo-3 relies more on fixed procedural heuristic than on dynamic reasoning. (2) Veo-3 frequently engages in self-reflective edits during the generation process. These edits occasionally help the model avoid violations (e.g., at second 3, where it changes the (1,1) cell from 4 to 2 to resolve conflict), but they can also be detrimental. At second 2, for example, Veo-3 overwrites correct digit, changing the (0,0) entry from 2 to 3, instantly invalidating the entire solution. This behavior illustrates broader challenge in generative reasoning models: self-reflection without grounded logical consistency can introduce more instability than benefit. (3) Veo-3 exhibits characteristic temporal drift pattern. Its early frames often remain stable and respect the given clues, but as generation progresses, the model increasingly modifies clues, introduces inconsistencies, or oscillates between alternative partial solutions. This suggests that the model lacks persistent internal representation of constraints, leading to reasoning degradation over time, even when the prompt explicitly prohibits altering clues. Overall, these case studies highlight the gap between current generative models and true abstract reasoning ability. Both Nano-Banana and Veo-3 demonstrate surface-level competencefilling cells, correcting mistakes, or imitating stepwise processesbut neither maintains robust, constraint-aware reasoning trajectory throughout the entire solution. These observations reinforce the importance of MMGRs diagnostic metrics: beyond assessing whether the final answer is correct, evaluating how the model reasons is essential for understanding the strengths and limitations of generative reasoning models. 17 Figure 8 Case Study: Failure cases generated by Veo-3 with frame-wise analysis highlighting three key behaviors: positional bias, self-reflective edits, and temporal drift."
        },
        {
            "title": "6.4 Evaluation Results",
            "content": "Key Finding: Image Models Outperform Video Models in Symbolic Reasoning Image generation models significantly outperform video models on Sudoku tasks, as video models suffer from severe temporal instability and lack of global symbolic reasoning. While video models exhibit high action reflection (making step-by-step edits), they fail to maintain logical consistency, leading to frequent constraint violations and clue changes. Crucially, human evaluation reveals 0% overall success for Veo-3 across all conditions, highlighting that current video models strictly fail at complex symbolic reasoning and that VLM-based auto-evaluation tends to overestimate model performance. 6.4.1 VLM-Based Evaluation Table 6 reveals clear and persistent performance divide between image and video generative models across grid sizes and difficulty levels. Image models (Nano-Banana, Nano-Banana Pro, GPT-4o-image, Qwen-image) consistently achieve higher completion accuracy and overall scores, reflecting stronger symbolic consistency when directly producing completed Sudoku grids. In contrast, video models (Veo-3, Sora-2, Wan-2.2) frequently generate visually coherent step-by-step editshigh action reflectionyet still suffer from widespread clue changes, constraint violations, and low end-to-end correctness. 44 Performance. Even in the simplest setting, video models struggle. On Easy puzzles, the best video performer, Veo-3, reaches an overall accuracy of only 11.38%, while Sora-2 and Wan-2.2 effectively fail (0% and 2%). Image models perform dramatically better: Nano-Banana achieves 66.25%, GPT-4o-image 61.22%, and Nano-Banana Pro 56.12%. This gap widens at Medium and Hard levels, where video models rarely produce valid solutions (overall 9.70%). Although Veo-3 maintains high action reflection (92%96%), its completion accuracy remains low (30%38%), and clue-change rates (24%30%) indicate persistent temporal drift. 99 Performance. On full-scale Sudoku, where global structural consistency is essential, video models break down almost entirely. Overall scores remain below 5% for most models. Sora-2s occasional peak at 7.14% on 18 Table 6 Quantitative results for the Sudoku task. We evaluate video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-Banana, Nano-Banana Pro, GPT-4o-image, and Qwen-image) on two grid sizes (44, 99) and three difficulty levels (Easy, Medium, and Hard). Because image outputs do not support frame-by-frame reasoning, action-reflectionbased metrics are omitted for image generative models (marked with N/A). The highest overall scores in each setting are highlighted in bold. Model Grid Size: 44 Level: Easy Video Models Veo-3 Sora-2 Wan2. Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Medium Video Models Veo-3 Sora-2 Wan2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Hard Video Models Veo-3 Sora-2 Wan2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Grid Size: 99 Level: Easy Video Models Veo-3 Sora-2 Wan2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Medium Video Models Veo-3 Sora-2 Wan2. Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Hard Video Models Veo-3 Sora-2 Wan2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Clues Changed Constraints Violation Completion Accuracy Action Reflection Overall Fine-grained Metrics Primary Metric 27.60% 100.00% 100.00% 18.40% 35.25% 23.83% 83.50% 24.40% 100.00% 100.00% 0.63% 34.81% 31.68% 83.50% 30.12% 100.00% 99.33% 24.40% 40.53% 36.40% 83.50% 31.60% 95.74% 87.00% 10.50% 19.09% 70.72% 28.33% 34.00% 100.00% 85.33% 2.00% 14.91% 71.37% 28.33% 31.20% 92.86% 91.00% 2.40% 14.75% 71.58% 28.33% 37.21% 25.78% 17.22% 67.48% 93.38% 73.14% 46.27% 37.61% 21.71% 14.36% 52.14% 91.86% 56.39% 46.27% 30.50% 20.03% 14.79% 46.21% 89.50% 49.00% 46.27% 15.47% 8.47% 18.03% 19.87% 31.52% 19.83% 73.12% 13.66% 9.43% 16.13% 36.88% 26.19% 17.92% 73.12% 13.45% 8.65% 16.86% 17.03% 22.52% 15.74% 73.12% 42.00% 44.22% 35.67% 17.20% 1.33% 19.65% 61.50% 43.53% 45.24% 36.67% 21.30% 0.50% 24.88% 61.50% 44.88% 41.85% 38.17% 25.80% 0.83% 24.37% 61.50% 39.60% 54.85% 44.59% 31.66% 33.67% 59.67% 7.88% 40.59% 54.96% 46.44% 35.54% 28.40% 58.53% 7.88% 43.01% 59.79% 48.07% 38.16% 41.36% 57.68% 7.88% 19 92.00% 32.65% 4.67% N/A N/A N/A N/A 95.60% 61.90% 6.00% N/A N/A N/A N/A 95.18% 60.87% 6.00% N/A N/A N/A N/A 70.00% 34.04% 8.00% N/A N/A N/A N/A 72.00% 30.00% 2.00% N/A N/A N/A N/A 70.40% 35.71% 3.00% N/A N/A N/A N/A 11.38% 0.00% 2.00% 66.25% 56.12% 61.22% 6.67% 9.70% 0.00% 1.33% 51.78% 56.75% 46.28% 6.67% 8.71% 0.00% 0.00% 42.45% 57.38% 39.08% 6.67% 3.18% 4.26% 0.00% 28.80% 39.28% 12.44% 18.08% 2.77% 0.00% 0.00% 24.15% 33.99% 11.43% 18.08% 2.57% 7.14% 1.00% 19.94% 30.86% 10.00% 18.08% Hard puzzles stems from anomalously low clue-change instances rather than genuine logical success. High constraint-violation rates (39%60%) and low completion accuracy (8%18%) further highlight compounding temporal errors that worsen with puzzle size. Image models, by contrast, remain substantially more stable. Nano-Banana Pro delivers the strongest overall results across difficulties39.28%, 33.99%, and 30.86%while standard Nano-Banana remains competitive it frequently alters clues on 44 puzzles but achieves (19.94%28.80%). Qwen-image is mixed case: surprisingly strong completion accuracy on 99 (73.12%). However, its solutions often contain subtle structural inconsistencies, limiting overall correctness under our strict validator. Across all conditions, video models exhibit fundamental weakness: despite producing locally plausible temporal edits, they lack the global, long-horizon reasoning required to satisfy Sudokus symbolic constraints. Their sequential generation process introduces temporal instabilitydigit drift, unintended clue overwrites, and cumulative constraint violationsthat image models avoid by generating single, static grid. The persistent performance gap across both grid sizes and all difficulty levels underscores deeper architectural mismatch between current video-generation paradigms and tasks requiring coherent, symbolic logical reasoning. 6.4.2 Human Evaluation To establish ground-truth performance estimates and validate the reliability of our VLM-based evaluator, we conducted human evaluation on subset of Veo-3s generated videos. Table 7 presents side-by-side comparison of Auto-Eval versus Human-Eval across all grid sizes and difficulty levels. Table 7 Comparison of Auto-Eval (VLM-based) and Human-Eval results for Veo-3 on the Sudoku task across two grid sizes (44, 99) across three difficulty levels (Easy, Medium, and Hard). Grid Eval Type Clues Changed Constraints Violation Completion Accuracy Action Reflection Overall Fine-grained Metrics Primary Metric Level: Easy 44 Auto-Eval Human-Eval 99 Auto-Eval Human-Eval Level: Medium 44 Auto-Eval Human-Eval 99 Auto-Eval Human-Eval Level: Hard 44 Auto-Eval Human-Eval 99 Auto-Eval Human-Eval 27.60% 10.00% 31.60% 30.00% 24.40% 30.00% 34.00% 20.00% 30.12% 50.00% 31.20% 10.00% 42.00% 10.00% 39.60% 7.50% 43.53% 5.00% 40.59% 0.00% 44.88% 10.00% 43.01% 20.00% 37.21% 17.50% 15.47% 0.00% 37.61% 12.50% 13.66% 2.50% 30.50% 17.50% 13.45% 0.00% 92.00% 90.00% 70.00% 100.00% 95.60% 100.00% 72.00% 80.00% 95.18% 100.00% 70.40% 100.00% 11.38% 0.00% 3.18% 0.00% 9.70% 0.00% 2.77% 0.00% 8.71% 0.00% 2.57% 0.00% Clues Changed. The clues-changed metric exhibits mixed pattern across conditions. For 44 Easy puzzles, Auto-Eval reports higher rate (28%) than Human-Eval (10%), yet this reverses on Hard puzzles where Human-Eval detects more clue modifications (50% vs. 30%). On 99 grids, the discrepancy is smaller but inconsistent: Human-Eval reports lower clue-change rates on Medium (20% vs. 34%) and Hard (10% vs. 31%) levels, but comparable rates on Easy (30% vs. 32%). These divergent patterns suggest that VLM and human annotators apply different detection criteriathe VLM may flag subtle visual perturbations as clue changes, while humans focus on semantically meaningful digit alterations. Constraints Violation. The most striking discrepancy emerges in constraint violation detection. Human evaluators consistently assign substantially lower constraint violation scores across all conditions: 10% vs. 42% on 44 Easy, 5% vs. 44% on 44 Medium, 10% vs. 45% on 44 Hard. For 99 puzzles, the gap widens furtherHuman-Eval yields 8% (Easy), 0% (Medium), and 20% (Hard), compared to Auto-Evals 40%, 41%, and 43% respectively. Notably, human annotators report zero constraint violations on 99 Medium puzzles, 20 whereas Auto-Eval detects violations in over 40% of outputs. This suggests the VLM-based evaluator may over-detect violations by misinterpreting visual artifacts, blurry digits, or frame inconsistencies as rule breaches. Completion Accuracy. Completion accuracy is substantially lower under human evaluation across all conditions. On 44 grids, Auto-Eval reports accuracy of 37% (Easy), 38% (Medium), and 31% (Hard), while Human-Eval yields only 18%, 13%, and 18% respectivelyroughly half the VLMs estimates. The disparity is even more pronounced on 99 puzzles: Auto-Eval reports 15% (Easy), 14% (Medium), and 13% (Hard), but Human-Eval drops to 0% on both Easy and Hard levels, with only 3% on Medium. This near-zero completion accuracy under human judgment indicates that human annotators apply stricter criteria for recognizing correctly filled digits, likely requiring clearer visual rendering and unambiguous digit shapes that Veo-3 fails to consistently produce at the 99 scale. Action Reflection. In contrast to other metrics, action reflection scores show reasonable agreement between evaluation methods. Both evaluators recognize that Veo-3 exhibits step-by-step reasoning behavior, with Auto-Eval reporting 92%96% on 44 and 70%72% on 99, while Human-Eval assigns 90%100% on 44 and 80%100% on 99. Human annotators even rate action reflection slightly higher on several conditions, reaching perfect scores (100%) on 44 Medium, 44 Hard, 99 Easy, and 99 Hard. This consensus confirms that Veo-3 does produce visually interpretable sequential editsthe models failure lies not in lacking reasoning process, but in lacking reasoning correctness. Overall Score. Most strikingly, no Veo-3 output achieved an overall success score under human evaluationall conditions yield 0%, compared to Auto-Evals modest but non-zero scores ranging from 3% (99 Easy) to 11% (44 Easy). This complete failure under human judgment, despite the model producing visually coherent step-by-step animations, underscores that even outputs deemed partially successful by the VLM fail to meet the stringent correctness standards required for valid Sudoku solutions. Key Insights. These findings highlight two key insights: (1) VLM-based evaluation tends to be more lenient, potentially overestimating model performance on structured reasoning tasksparticularly by over-detecting constraint violations while being more generous on completion accuracy; and (2) human evaluation remains essential for validating generative model outputs in tasks requiring strict logical correctness, as the metrics that matter most (completion accuracy and overall success) show the largest divergence between evaluation methods. The consistent zero overall scores under human evaluation reinforce our main conclusion: current video generative models lack the robust constraint-satisfaction capabilities needed for reliable Sudoku solving."
        },
        {
            "title": "7 ARC-AGI",
            "content": "We evaluate models on the ARC-AGI task (Abstraction and Reasoning Corpus for Artificial General Intelligence), benchmark designed to measure Abstract Reasoning, Pattern Recognition, and Rule Induction (Chollet, 2019). ARC-AGI probes models ability to infer latent transformation rules from small set of inputoutput examples and to apply those rules to unseen test casesan ability central to fluid intelligence. Unlike tasks with explicit instructions or predefined rule sets, ARC-AGI demands that the model autonomously identify and generalize the underlying visual and structural principles governing each puzzle. As result, the task jointly tests 2D spatial reasoning (interpreting grid-based patterns) and logical reasoning (deducing and executing abstract transformations)."
        },
        {
            "title": "7.1 Hard-Level Control",
            "content": "To construct diverse and controllable evaluation suite, we build on the open-source ARC-AGI benchmark (Chollet, 2019). Our final dataset comprises 456 tasks drawn from two benchmark versions: ARC-AGI v1 (381 tasks): The publicly released training set from the original benchmark, encompassing wide variety of pattern-transformation and abstract reasoning problems. ARC-AGI v2 (75 tasks): Newly added tasks that introduce novel pattern families and higher structural complexity, expanding the benchmarks coverage. This combined set provides broad coverage of transformation types while introducing sufficient novelty and difficulty for rigorous model evaluation. All tasks undergo manual curation to ensure clean pattern design, consistent formatting, and unambiguous transformation rules. Collectively, the benchmark spans wide spectrum of transformation categoriesincluding symmetry, rotation, scaling, color manipulation, and object-level reasoningcapturing the core abstractions and reasoning skills fundamental to ARC-style tasks.2 7.1.1 Two-Level Classification System To enable fine-grained analysis of model capabilities, we classify all cases along two dimensions: Level 1: Shape Consistency Classification. We categorize each case based on whether the input and output grids maintain the same spatial structure: Match (316 cases): The output grid has the same dimensions as the input grid. Transformations occur in-place through color changes, pattern filling, or local modifications. Examples include color replacement, symmetry completion, and pattern extension within fixed boundaries. Mismatch (140 cases): The output grid dimensions differ from the input. These cases require spatial restructuring, such as cropping, extraction of sub-patterns, grid concatenation, or shape reconstruction. These are generally more challenging as they demand explicit understanding of spatial relationships beyond simple transformations. Level 2: Quantitative Difficulty Classification. Within each shape-consistency category, we further assign each case to one of three difficulty levelsEasy, Medium, or Hard based on five quantitative grid-level features: 1. Grid Size (minimum side length): < 8 cells score 0; 8-15 cells score 1; 16 cells score 2. 2. Color Count (distinct colors in input): 3 score 0; 4-6 score 1; 7 score 2. 3. Object Count (distinct connected components): 4 score 0; 5-10 score 1; > 10 score 2. 4. Occupancy Ratio (non-background cells / total cells): 0.25 score 0; (0.25, 0.55] score 1; > 0.55 score 2. 5. IO (grid change ratio, for Match cases only): 0.2 score 0; (0.2, 0.5] score 1; > 0.5 score 2. For Mismatch cases, this feature is not applicable. cases overall difficulty is determined by summing the applicable feature scores.For Match cases (5 features): Easy 3, Medium 46, Hard 7. For Mismatch cases (4 features): Easy 2, Medium 34, Hard 5. Table 8 reports the full distribution across categories. Figure 9 and Figure 10 present some selected examples. Table 8 Distribution of 456 ARC-AGI cases across shape consistency and difficulty levels, separated by v1 and v2. Percentages indicate the proportion within each shape consistency group for the corresponding benchmark version. Version Shape Consistency Easy Medium Hard Total V1 V2 Match Mismatch Total Match Mismatch Total 102 (38.8%) 34 (28.8%) 124 (47.1%) 57 (48.3%) 37 (14.1%) 27 (22.9%) 136 1 (1.9%) / 181 64 27 (50.9%) 7 (31.8%) 25 (47.2%) 15 (68.2%) 1 40 Overall Match Mismatch 103 (31.0%) 34 (25.0%) 151 (45.4%) 64 (45.7%) 62 (23.6%) 42 (29.3%) 263 118 381 53 22 75 316 140 Total 133 (29.2%) 205 (45.0%) 118 (25.9%)"
        },
        {
            "title": "7.2 Evaluation and Metrics",
            "content": "We evaluate both video and image outputs using Vision-Language Model (VLM)based evaluator, Gemini2.5 Pro (Comanici et al., 2025). The evaluator takes four inputs: (i) the demonstration examples, (ii) the 2Additional examples and visualizations are available in the official repositories: https://github.com/fchollet/ARC-AGI , https://github.com/michaelhodel/re-arc , and the automated generation toolkit https://github.com/google/ARC-GEN. 22 Figure 9 Selected examples from ARC-AGI v1, illustrating both Match and Mismatch tasks across three difficulty levels: Easy, Medium, and Hard. Figure 10 Selected examples from ARC-AGI v2, illustrating both Match and Mismatch tasks across three difficulty levels: Easy, Medium, and Hard. test input, (iii) the ground-truth output, and (iv) the generated video or image. Using these inputs, the VLM determines whether the predicted transformation is correct and identifies errors in pattern recognition, structural consistency, and rule application. From the evaluators structured responses, we compute one primary correctness metric and three fine-grained diagnostic metrics: Pattern Recognition: 1 if the evaluator confirms that the model successfully identifies the transformation pattern from the demonstrations; 0 otherwise. Grid Integrity: 1 if the generated output preserves the correct grid dimensions and structural layout; 0 if the grid is distorted or misaligned. Color Accuracy: 1 if all colors are applied correctly according to the transformation rule; 0 otherwise. Valid Solution (Primary Metric): 1 only if the generated output exactly matches the ground-truth solution; 0 otherwise. 23 Table 9 Overall quantitative results for the ARC-AGI v1 task (381 cases). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). Model Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Fine-grained Metrics Primary Metric Pattern Recog. Grid Integrity Color Accuracy Overall 17.32% 71.99% 0.61% 28.42% 61.98% 1.05% 1.31% 32.98% 94.58% 13.04% 55.79% 84.73% 10.24% 4.46% 8.22% 36.75% 0.17% 12.63% 40.42% 0.52% 0.52% 5.16% 20.18% 0.17% 9.21% 30.54% 0.00% 0.52%"
        },
        {
            "title": "7.3 Case Study",
            "content": "The case studies presented in Figure 11a and Figure 11b highlight significant failure mode in video generation models applied to abstract reasoning tasks: the inability to maintain temporal consistency for static information. In both instances, the model fails to distinguish between the invariant problem context (the EXAMPLES E1-E4) and the dynamic solution generation (the TEST T1). As the video progresses from Frame 1 to Frame 4, the demonstration exampleswhich should remain strictly fixed to define the logic of the tasksuffer from severe hallucinations, including unintended color shifts, pattern deformations, and progressive structural degradation (such as the complete erasure of grid contents in Figure 11). This drift suggests that the model treats the entire visual field as mutable video sequence rather than respecting the logical constraints of the prompt, ultimately undermining the reasoning process by destabilizing the very ground truth required to solve the puzzle."
        },
        {
            "title": "7.4 Evaluation Results",
            "content": "Key Finding: The Abstract Reasoning Divide and Temporal Instability Our evaluation reveals two critical insights about abstract visual reasoning. First, fundamental modality gap: Nano-banana Pro achieves 30.54% accuracy on ARC-AGI v1, establishing clear stateof-the-art performance and outperforming the best video model (Sora-2: 20.18%) by significant margin. Second, temporal consistency failure: video models struggle to maintain static demonstration examples, leading to context drift where the problem definition itself becomes corrupted during generation. Most critically, human evaluation exposes an evaluation reliability gapwhile VLM-based metrics report 45% valid solutions for Veo-3, human annotators find 0.00% across all 98 evaluated cases, revealing that current video models cannot reliably execute abstract transformations despite occasionally producing visually plausible outputs. 7.4.1 VLM-Based Evaluation Overall Performance Patterns. The quantitative results (Table 9 and Table 10) establish clear performance hierarchy across the 456 ARC-AGI cases. Nano-banana Pro dominates with 30.54% overall accuracy on v1 and 30.36% on v2, demonstrating both superior reasoning capability and remarkable robustness to distribution shift. Among video models, Sora-2 emerges as the clear leader with 20.18% on v1notably surpassing the base Nano-banana image model (9.21%)yet collapses to 1.33% on the harder v2 dataset, exposing critical brittleness. Veo-3 maintains modest but stable performance (5.16% v1, 4.00% v2), while Wan-2.2 effectively fails (0.17% v1, 0.00% v2). At the bottom tier, GPT-4o-image achieves 0.00% across both versions despite non-zero Grid Integrity (10.24% on v1), and Qwen-image barely registers (0.52% v1, 1.33% v2). The Modality Gap. The results reveal fundamental divide between image and video generation paradigms (a) Inconsistent demonstration examples across video frames. The model fails to maintain the static demonstration inputs (E1E4), with colors and patterns changing between frames. This behavior reflects critical failure to preserve the given problem context. (b) Progressive transformation of example demonstrations across frames. The examples (E1E4) undergo unintended color and pattern evolution from Frame 1 to Frame 4, indicating lack of temporal consistency in preserving the demonstration context during solution generation. Figure 11 Failure cases generated by Veo-3 on ARC-AGI showing violation of static demonstration preservation. In both examples, the model fails to keep the input demonstrations unchanged through time, illustrating key breakdown in temporal consistency for reasoning-based video generation. 25 Table 10 Overall quantitative results for the ARC-AGI v2 task (75 cases). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). Model Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Fine-grained Metrics Primary Metric Pattern Recog. Grid Integrity Color Accuracy Overall 17.78% 4.00% 0.00% 18.67% 62.50% 1.33% 1.33% 31.11% 16.00% 5.78% 42.67% 83.93% 2.67% 5.33% 6.22% 1.33% 0.00% 8.00% 44.64% 1.33% 1.33% 4.00% 1.33% 0.00% 2.67% 30.36% 0.00% 1.33% Table 11 Quantitative results for the ARC-AGI task across different difficulty levels (Easy, Medium, and Hard). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). Model Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Version: v1 & v2 Combined Level: Easy Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Medium Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Level: Hard Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image 18.98% 76.07% 0.00% 32.85% 62.60% 0.73% 1.46% 17.36% 58.67% 0.78% 24.77% 62.98% 0.47% 1.40% 15.38% 40.43% 0.64% 23.08% 59.30% 2.88% 0.96% 39.66% 89.74% 16.79% 64.23% 86.18% 10.95% 5.11% 32.40% 84.18% 10.54% 51.87% 86.74% 9.77% 4.65% 24.04% 59.57% 8.01% 43.27% 77.91% 4.81% 3.85% 10.46% 41.88% 0.00% 13.87% 43.90% 1.46% 0.73% 7.29% 29.08% 0.31% 10.75% 40.33% 0.00% 0.47% 5.77% 18.09% 0.00% 11.54% 38.37% 0.96% 0.96% 5.60% 22.22% 0.00% 10.22% 30.89% 0.00% 0.73% 5.12% 16.33% 0.31% 7.01% 30.39% 0.00% 0.47% 3.85% 10.64% 0.00% 7.69% 30.23% 0.00% 0.96% (Table 9 and Table 10). Image models demonstrate superior abstract reasoning: Nano-banana Pro achieves 61.98% Pattern Recognition and 84.73% Grid Integrity, substantially outperforming the best video model Sora-2 (71.99% Pattern Recognition, 94.58% Grid Integrity). However, striking anomaly emerges: Sora-2 achieves higher fine-grained scores but lower overall accuracy than Nano-banana Pro. This suggests that video models can perceive patterns and maintain structure but fail at the final integration steptranslating partial 26 Table 12 Quantitative breakdown results (Match and Mismatch) for the ARC-AGI v1 task (381 cases). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Video Models VeoMatch Mismatch Sora-2 Match Mismatch Wan-2.2 Match Mismatch Image Models Nano-banana Match Mismatch Nano-banana Pro Match Mismatch GPT-4o-image Match Mismatch Qwen-image Match Mismatch 17.74% 16.38% 67.29% 80.51% 0.51% 0.85% 24.05% 38.14% 62.24% 61.29% 0.38% 2.54% 0.76% 2.54% 35.74% 26.84% 93.46% 96.61% 13.31% 12.43% 53.82% 60.17% 89.63% 72.04% 9.51% 11.86% 4.56% 4.24% 9.51% 5.37% 35.05% 39.83% 0.00% 0.56% 11.07% 16.10% 42.74% 34.41% 0.76% 0.00% 0.76% 0.00% 5.70% 3.95% 17.76% 24.58% 0.00% 0.56% 8.40% 11.02% 31.54% 27.96% 0.00% 0.00% 0.76% 0.00% understanding into correct solutions. The gap amplifies on v2: Nano-banana Pro maintains stable performance (30.36%), while Sora-2 collapses from 20.18% to 1.33%a 93% relative declinerevealing that video models rely heavily on memorized patterns rather than generalizable reasoning. Dataset Difficulty Progression. Performance stratification by difficulty level  (Table 11)  reveals distinct scaling behaviors across model types: Nano-banana Pro (Robust Stability): Maintains remarkable consistency across Easy (30.89%), Medium (30.39%), and Hard (30.23%) cases. Grid Integrity shows only modest degradation (86.18% 86.74% 77.91%), confirming that this model has internalized abstract transformation rules rather than relying on surface-level pattern matching. Sora-2 (Difficulty-Sensitive Collapse): Exhibits pronounced degradation from Easy (22.22%) to Medium (16.33%) to Hard (10.64%). Notably, Pattern Recognition drops from 76.07% to 40.43%, indicating that video models lose the ability to identify transformation rules as complexity increases. Veo-3 (Consistent Underperformance): Remains stagnant around 5% across all difficulty levels, with Grid Integrity showing steady decline (39.66% 32.40% 24.04%). This suggests that Veo-3s failures are systematic rather than difficulty-dependent. Shape Consistency Analysis. Comparing Match cases (same input/output dimensions) against Mismatch cases (different dimensions) across ARC-AGI v1 and v2 reveals distinct model behaviors (Table 12 and Table 14): The Inverse Pattern (v1 vs. v2): In v1, both Sora-2 and the base Nano-banana model perform better on Mismatch cases than Match cases (Sora-2: 24.58% vs 17.76%; Nano-banana: 11.02% vs 8.40%). This suggests these models may leverage visual restructuring effectively when dimensions shift. However, this advantage disappears in the more complex v2, where Sora-2 collapses to 0.00% on Mismatch cases. Nano-banana Pro Stability: Demonstrates demonstrates the most robust handling of transformation types. In v1, it maintains high stability (31.54% Match vs 27.96% Mismatch). In v2, while it remains the top 27 performer, it exhibits clearer preference for Match cases (33.33%) over Mismatch (21.43%), indicating that increased task complexity reintroduces the expected difficulty penalty for dimension changes. Model-Specific Difficulty Curves: Other models show varied sensitivity to grid consistency. Veo-3 displays mixed behaviordropping slightly in v1 (5.70% to 3.95%) but surprisingly improving on Mismatch in v2 (3.77% to 4.55%), potentially due to low sample variance. Conversely, image models like Qwen-image consistently struggle with Mismatch tasks, collapsing to near-zero performance across both datasets. Dimensionality-Difficulty Interaction. granular intersectional analysis of difficulty and grid consistency (Table 13 and Table 15) exposes paradox in video model behavior distinct from standard scaling laws: The Sora-2 Anomaly (Hard Case Inversion): On v1 Hard cases, Sora-2 displays highly anomalous inverted profile, achieving nearly 4 higher accuracy on Mismatch tasks (29.63%) compared to Match tasks (7.41%). Fine-grained metrics reveal this is not structural failureGrid Integrity remains identical (96.30%) across both categories. Instead, the collapse is driven by Color Accuracy, which plummets to 18.52% on Match compared to 44.44% on Mismatch. This implies video models struggle to execute complex pixel-level color transformations when constrained to fixed canvas, but recover capability when generating new spatial structures. Nano-banana Pro (Logical Scaling): In contrast, the leading image model exhibits expected difficulty scaling. It consistently performs better on Match (fixed-dimension) tasks than Mismatch (dynamicdimension) tasks across all difficulty levels (e.g., v1 Hard Match 37.14% vs. Hard Mismatch 27.27%), confirming stable, dimension-agnostic reasoning core. Brittleness of the Generative Advantage: Sora-2s generative advantage on Mismatch tasks proves brittle. On the v2 benchmark, the anomaly vanishes entirely: Sora-2 collapses to 0.00% on Hard Mismatch cases, while Nano-banana Pro maintains robust capability (22.22%). This reinforces that while video models may occasionally exploit spatial restructuring, they lack the robust generalization of their image-based counterparts. Metric Cascade and Bottlenecks. The four evaluation metrics form consistent funnel where success becomes progressively more restrictive. For Nano-banana on v1: Grid Integrity (55.79%) Pattern Recognition (28.42%) Color Accuracy (12.63%) Overall (9.21%). This cascade reveals that approximately 51% of structurally correct outputs achieve pattern recognition, 44% of those achieve color accuracy, and 73% of color-accurate outputs reach full correctness. Color Accuracy emerges as the critical bottleneck, representing only 2025% of Grid Integrity performance across all models. This bottleneck is most severe for GPT-4o-image: despite achieving 10.24% Grid Integrity, it manages only 0.52% Color Accuracy, resulting in 0.00% overall success. The cascade pattern suggests that models can learn structural rules but struggle with precise color-based execution. Version Comparison. The transition from v1 to v2 exposes model robustness to novel pattern families: Dramatic Video Model Collapse: Sora-2 drops from 20.18% to 1.33% (93% decline), while Nano-banana drops from 9.21% to 2.67% (71% decline). Nano-banana Pro remains stable (30.54% 30.36%). Dataset Composition Effects: v2 skews harder, containing only 1 Easy Match case versus 102 in v1. The difficulty distribution shift disproportionately penalizes models that rely on memorized patterns. Grid Integrity Preservation: Interestingly, Veo-3 maintains similar Grid Integrity across versions (32.98% v1, 31.11% v2) despite failing on the primary metric, suggesting it can preserve structure without understanding transformation semantics. 7.4.2 Human Evaluation To establish ground-truth performance estimates and validate the reliability of our VLM-based evaluator, we conducted human evaluation on subset of 98 Veo-3 generated videos (60 from v1, 38 from v2). Human annotators assess the same four metrics: Pattern Recognition, Grid Integrity, Color Accuracy, and Valid Solution. Table 16 and Table 17 present side-by-side comparisons of Auto-Eval versus Human-Eval across Match and Mismatch subsets. Pattern Recognition. Human and VLM evaluators show reasonable agreement on pattern recognition ability. On v1, humans report 18.33% versus Auto-Evals 16.80%a modest 1.5 percentage point difference. On v2, the gap is slightly larger: 23.68% (Human) versus 22.67% (Auto). This consensus suggests that both evaluation methods can reliably detect whether model has identified the underlying transformation rule, even when execution fails. Grid Integrity. substantial divergence emerges in structural assessment. Humans consistently rate Grid Integrity lower than the VLM evaluator: 23.33% versus 35.70% on v1 overall (12.4 pp gap), and 31.58% versus 41.33% on v2 overall (9.8 pp gap). This systematic discrepancy suggests that the VLM evaluator may be more tolerant of minor structural deformationssuch as slight grid misalignments or cell boundary artifactsthat human annotators penalize. The gap is particularly pronounced for Match cases on v1 (26.67% Human vs 40.30% Auto), indicating that humans apply stricter standards when evaluating in-place transformations where structural preservation is expected. Interestingly, the pattern reverses for color assessment. Humans rate Color Accuracy Color Accuracy. substantially higher than the VLM: 23.33% versus 7.61% on v1 (3.1 higher), and 26.32% versus 6.67% on v2 (3.9 higher). This suggests the VLM evaluator may be overly sensitive to minor color deviationsperhaps detecting subtle RGB differences that humans perceive as acceptable matches. The implication is significant: VLM-based Color Accuracy scores may systematically underestimate model performance on this metric. Valid Solution Rate. The most critical finding is the complete failure under human evaluation: 0.00% Valid Solution rate across all 98 cases, compared to 4.72% (v1) and 2.67% (v2) from Auto-Eval. Despite achieving non-trivial scores on fine-grained metricswith Pattern Recognition reaching 38.10% and Grid Integrity reaching 47.62% on v2 Match casesno single generated video produces an exact match to the ground-truth solution. This reveals fundamental gap between partial understanding and precise execution: video models can demonstrate awareness of transformation patterns and maintain structural consistency in nearly half of cases, yet consistently fail to translate this partial competence into pixel-perfect outputs. Match versus Mismatch Performance. Human evaluation confirms and amplifies the performance gap observed in automatic evaluation. On v2, Match cases achieve 38.10% Pattern Recognition compared to only 5.88% for Mismatch casesa 6.5 difference. Grid Integrity shows an even more pronounced gap: 47.62% for Match versus 11.76% for Mismatch (4.0). On v1, the gaps are smaller but consistent: Match cases achieve 23.33% Pattern Recognition versus 13.33% for Mismatch (1.75). These results confirm that video generation models fundamentally struggle with shape-changing transformations, where producing outputs with different dimensions from inputs poses substantially greater challenge than in-place modifications. Key Insights. The human evaluation reveals two critical findings: (1) VLM-based evaluation tends to overestimate overall success while being overly strict on Color Accuracy and lenient on Grid Integritya pattern that may mask true model capabilities and limitations; and (2) Human evaluation remains essential for validating generative model outputs in tasks requiring strict logical correctness, as the complete 0.00% Valid Solution rate under human judgment starkly contrasts with the non-zero Auto-Eval scores. These findings underscore that current video generation models lack the robust constraint-satisfaction capabilities needed for reliable abstract reasoning. 7.4.3 Key Findings and Implications Three critical insights emerge from the ARC-AGI evaluation, revealing fundamental limitations in how current generative models approach abstract visual reasoning. 1. The Temporal Consistency Barrier. defining failure mode of video generation on ARC-AGI is the inability to maintain static context. As illustrated in Figure 11a and Figure 11b, video models fail to distinguish between invariant problem context (the demonstration examples E1E4) and the dynamic solution generation (test output T1). Demonstration exampleswhich should remain strictly fixed to define the transformation logicundergo progressive hallucination: color shifts, pattern deformations, and structural degradation across frames. This context drift effectively corrupts the problem definition itself, undermining any chance of correct solution generation. The implication is clear: current video architectures treat the entire visual field 29 as mutable sequence rather than respecting logical invariants, making them fundamentally unsuited for reasoning tasks that require stable reference points. 2. The Perception-Execution Gap. The metric cascade analysis reveals systematic disconnect between pattern perception and precise execution. Models frequently achieve reasonable Pattern Recognition (up to 71.99% for Sora-2) and Grid Integrity (up to 94.58%) scores, yet fail catastrophically on Overall accuracy. This suggests that current generative models can see the transformation pattern but cannot execute it correctly. The bottleneck consistently occurs at Color Accuracy, where performance drops to 2025% of Grid Integrity levels. This perception-execution gap implies that models may learn to recognize visual patterns without internalizing the precise symbolic rules governing color assignmenta fundamental limitation for tasks requiring exact constraint satisfaction. 3. The Robustness-Memorization Trade-off. The dramatic performance collapse from v1 to v2particularly Sora-2s 93% relative declineexposes critical reliance on memorized patterns rather than generalizable reasoning. In contrast, Nano-banana Pros stability across versions (30.54% 30.36%) demonstrates that image-based generation can achieve robust abstract reasoning. This dichotomy suggests that video models may be overfitting to temporal motion patterns in training data rather than learning the underlying transformation logic. For ARC-AGI specifically, where novel pattern families are explicitly designed to defeat memorization, this limitation is fatal. The finding has broader implications: video generation architectures may require fundamentally different training objectives or inductive biases to support genuine rule-based reasoning rather than sophisticated pattern matching. 30 Table 13 Quantitative breakdown results for the ARC-AGI v1 task (381 cases) across different difficulty level (Easy, Medium, and Hard). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). Model / Difficulty Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Veo-3: Match Easy Medium Hard Veo-3: Mismatch Easy Medium Hard Sora-2: Match Easy Medium Hard Sora-2: Mismatch Easy Medium Hard Wan-2.2: Match Easy Medium Hard Wan-2.2: Mismatch Easy Medium Hard Nano-banana: Match Easy Medium Hard Nano-banana: Mismatch Easy Medium Hard Nano-banana Pro: Match Easy Medium Hard 18.95% 17.74% 14.41% 19.61% 14.04% 17.28% 73.17% 63.81% 62.96% 85.29% 80.70% 74.07% 0.00% 0.54% 1.80% 0.00% 1.75% 0.00% 27.45% 20.33% 27.03% 47.06% 35.09% 33.33% 62.77% 60.71% 65.71% Nano-banana Pro: Mismatch Easy Medium Hard 60.71% 65.12% 54.55% GPT-4o-image: Match Easy Medium Hard GPT-4o-image: Mismatch Easy Medium Hard Qwen-image: Match Easy Medium Hard Qwen-image: Mismatch Easy Medium Hard 0.98% 0.00% 0.00% 0.00% 1.75% 7.41% 0.98% 0.00% 2.70% 2.94% 3.51% 0.00% 11.76% 8.06% 8.11% 6.86% 5.26% 3.70% 42.68% 33.33% 18.52% 41.18% 36.84% 44.44% 0.00% 0.00% 0.00% 0.00% 1.17% 0.00% 13.73% 8.13% 13.51% 11.76% 19.30% 14.81% 45.74% 37.50% 51.43% 35.71% 37.21% 27.27% 1.96% 0.00% 0.00% 0.00% 0.00% 0.00% 0.98% 0.00% 2.70% 0.00% 0.00% 0.00% 5.88% 5.65% 5.41% 4.90% 3.51% 3.70% 19.51% 19.05% 7.41% 29.41% 19.30% 29.63% 0.00% 0.00% 0.00% 0.00% 1.17% 0.00% 8.82% 7.32% 10.81% 11.76% 10.53% 11.11% 31.91% 29.46% 37.14% 25.00% 30.23% 27.27% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.98% 0.00% 2.70% 0.00% 0.00% 0.00% 40.52% 35.48% 23.42% 37.25% 23.39% 20.99% 90.24% 95.24% 96.30% 91.18% 100.00% 96.30% 16.34% 12.37% 8.11% 17.65% 9.36% 12.35% 56.86% 50.41% 56.76% 85.29% 52.63% 44.44% 90.43% 89.29% 88.57% 71.43% 74.42% 68.18% 10.78% 11.29% 0.00% 11.76% 10.53% 14.81% 5.88% 3.23% 5.41% 2.94% 7.02% 0.00% 31 Table 14 Quantitative breakdown results (Match and Mismatch) for the ARC-AGI v2 task (75 cases). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Video Models Veo-3 Match Mismatch Sora-2 Match Mismatch Wan-2.2 Match Mismatch Image Models Nano-banana Match Mismatch Nano-banana Pro Match Mismatch GPT-4o-image Match Mismatch Qwen-image Match Mismatch 17.61% 18.18% 5.66% 0.00% 0.00% 0.00% 18.87% 18.18% 64.29% 57.14% 0.00% 4.55% 1.89% 0.00% 6.92% 4.55% 1.89% 0.00% 0.00% 0.00% 11.32% 0.00% 50.00% 28.57% 0.00% 4.55% 1.89% 0.00% 3.77% 4.55% 1.89% 0.00% 0.00% 0.00% 3.77% 0.00% 33.33% 21.43% 0.00% 0.00% 1.89% 0.00% 32.08% 28.79% 18.87% 9.09% 4.40% 9.09% 45.28% 36.36% 88.10% 71.43% 1.89% 4.55% 5.66% 4.55% 32 Table 15 Quantitative breakdown results for the ARC-AGI v2 task (75 cases) across different difficulty levels (Easy, Medium, and Hard). We compare the performance of video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-banana, Nano-banana Pro, GPT-4o-image, and Qwen-image). * The Easy level of ARC-AGI v2 contains only one evaluation case; therefore, model solving this case correctly achieves 100% score. Model / Difficulty Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Veo-3: Match Easy Medium Hard Veo-3: Mismatch Medium Hard Sora-2: Match Easy Medium Hard Sora-2: Mismatch Medium Hard Wan-2.2: Match Easy Medium Hard Wan-2.2: Mismatch Medium Hard Nano-banana: Match Easy Medium Hard Nano-banana: Mismatch Medium Hard 0.00% 20.99% 14.67% 23.81% 15.56% 0.00% 7.41% 4.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 100.00% 18.52% 16.00% 42.86% 6.67% Nano-banana Pro: Match Easy Medium Hard Medium Hard 100.00% 66.67% 60.00% 80.00% 44.44% Nano-banana Pro: Mismatch GPT-4o-image: Match Easy Medium Hard GPT-4o-image: Mismatch Medium Hard Qwen-image: Match Easy Medium Hard Qwen-image: Mismatch Medium Hard 0.00% 0.00% 0.00% 0.00% 6.67% 0.00% 3.70% 0.00% 0.00% 0.00% 0.00% 8.64% 5.33% 4.76% 4.44% 0.00% 3.70% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 100.00% 7.41% 12.00% 0.00% 0.00% 100.00% 61.90% 35.00% 40.00% 22.22% 0.00% 0.00% 0.00% 0.00% 6.67% 0.00% 3.70% 0.00% 0.00% 0.00% 0.00% 6.17% 1.33% 4.76% 4.44% 0.00% 3.70% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 100.00%* 0.00% 4.00% 0.00% 0.00% 100.00%* 38.10% 25.00% 20.00% 22.22% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 3.70% 0.00% 0.00% 0.00% 33.33% 34.57% 29.33% 42.86% 22.22% 0.00% 22.22% 16.00% 28.57% 0.00% 33.33% 4.94% 2.67% 9.52% 8.89% 100.00% 55.56% 32.00% 57.14% 26.67% 100.00% 95.24% 80.00% 100.00% 55.56% 0.00% 3.70% 0.00% 0.00% 6.67% 0.00% 7.41% 4.00% 0.00% 6.67% 33 Table 16 Comparison of Auto-Eval (VLM-based) and Human-Eval results for Veo-3 on the ARC-AGI v1 task across two subsets (Match and Mismatch). Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Evaluation Methods Human-Eval Match Mismatch Total Auto-Eval Match Mismatch Total 23.33% 13.33% 18.33% 19.01% 11.86% 16.80% 26.67% 20.00% 23.33% 40.30% 25.42% 35.70% 30.00% 16.67% 23.33% 9.51% 3.39% 7.61% 0.00% 0.00% 0.00% 5.70% 2.54% 4.72% Table 17 Comparison of Auto-Eval (VLM-based) and Human-Eval results for Veo-3 on the ARC-AGI v2 task across two subsets (Match and Mismatch). Model / Category Pattern Recog. Grid Integrity Color Accuracy Overall Fine-grained Metrics Primary Metric Evaluation methods Human-Eval Match Mismatch Total Auto-Eval Match Mismatch Total 38.10% 5.88% 23.68% 20.75% 27.27% 22.67% 38.10% 11.76% 26.32% 7.55% 4.55% 6.67% 0.00% 0.00% 0.00% 1.89% 4.55% 2.67% 47.62% 11.76% 31.58% 41.51% 40.91% 41.33%"
        },
        {
            "title": "8 Math",
            "content": "We construct the Visual Math task to evaluate Logical Reasoning, Logical Deduction, and 2D Spatial Reasoning. This task probes models ability to understand complex mathematical problems presented visually (e.g., geometry, diagrams) and challenges its Temporal Reasoning by requiring it to generate video animating the step-by-step deduction from premises to solution (Huang et al., 2025)."
        },
        {
            "title": "8.1 Hard-Level Control\nWe evaluate models across five benchmarks that span a wide spectrum of difficulty: GSM8K (Cobbe et al.,\n2021) (grade school), MATH (Hendrycks et al., 2021) (high school competition), the AIME 2024 (Mathematical\nAssociation of America, 2024) and AIME 2025 (Mathematical Association of America, 2025) invitational\nexaminations, and Omni-MATH (Gao et al., 2024) (Olympiad-level). For Omni-MATH, we further classify\nproblems by difficulty (five levels: T0–T4) and category (eight types: Algebra, Applied Math, Calculus,\nDiscrete Math, Geometry, Precalculus, Number Theory, and Other). Table 18 summarizes the evaluation\nbenchmarks and sample counts, while Table 19 details the Omni-MATH sample distribution across difficulty\nlevels and categories. Figure 12 shows some selected examples.",
            "content": "Table 18 Summary of mathematical reasoning benchmarks used for evaluation. GSM8K contains grade school math word problems. MATH covers high school competition mathematics across multiple subjects. AIME (American Invitational Mathematics Examination) represents advanced competition-level problems. Omni-MATH provides fine-grained categorization across difficulty levels and subject areas. Dataset GSM8K MATH500 AIME 2024 AIME 2025 Omni-MATH Total Level Grade School High School Competition Invitational Competition Invitational Competition Multi-level (T0-T4) Sample Count 50 50 30 30 327 Table 19 Sample distribution of Omni-MATH dataset across difficulty levels and mathematical categories. Note: Omni-MATH provides fine-grained ontology for mathematical problem classification. Difficulty levels range from T0 (easiest, suitable for middle school) to T4 (hardest, olympiad-level problems). Categories cover the major branches of mathematics commonly tested in competitions and standardized assessments. These results represent video generation evaluations only; image generation was not performed for this dataset. Difficulty Algebra Applied Math Calculus Discrete Math Geometry Precalculus Number Other T0 (Easiest) T1 T2 T3 T4 (Hardest) 4 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 4 4 4 5 5 5 5 Total 25 20 25 24 23 1 0 1 0 0 2 Note: Omni-MATH provides fine-grained ontology for mathematical problem classification. Difficulty levels range from T0 (easiest, suitable for middle school) to T4 (hardest, olympiad-level problems). Categories cover the major branches of mathematics commonly tested in competitions and standardized assessments. These results represent video generation evaluations only; image generation was not performed for this dataset."
        },
        {
            "title": "8.2 Evaluation and Metrics",
            "content": "We use Gemini-2.5-Pro (Comanici et al., 2025) as our evaluator to assess the correctness and quality of generated solutions. The evaluator analyzes intermediate reasoning steps, final answers, and reflective behavior (for video generations). We define the following metrics: 35 Figure 12 Examples from our selected five mathematical reasoning benchmarks. Final Correctness: 1 if the final solution matches the ground truth answer, 0 otherwise. This metric verifies whether the model arrives at the correct conclusion. Intermediate Correctness: 1 if all reasoning steps leading to the solution are logically valid and mathematically sound, 0 otherwise. This evaluates the quality of the step-by-step deduction process. Action Reflection (videos only): 1 if the generated video exhibits self-correction behavior (e.g., revising incorrect steps, reconsidering approaches), 0 if the solution proceeds without reflection. This metric is not applicable to static image generations. Overall Score: 1 if and only if both Final Correctness=1 AND Intermediate Correctness=1, 0 otherwise. This is the primary metric representing complete solution correctness."
        },
        {
            "title": "8.3 Evaluation Results",
            "content": "Key Finding: The Reasoning-outcome Disconnect While image generation models maintain high consistency between reasoning process and final answers, video models exhibit severe reasoning disconnect. Video models frequently generate correct final answers (Outcome Success) despite flawed intermediate logic (Process Success), suggesting they rely on visual pattern matching rather than causal deduction. 8.3.1 Overall Performance Patterns The Modality Gap. The quantitative results  (Table 20)  establish stark performance hierarchy where image generative models consistently outperform video generative models. The gap is most visible when comparing the state-of-the-art in both modalities: on the entry-level GSM8K benchmark, Nano-banana Pro achieves near-perfect Overall Success Rate of 97.83%, whereas the best-performing video model, Sora-2, reaches only 30.00%a greater than 3 performance disparity. This gap widens on competition-level mathematics; on AIME 25, Nano-banana Pro maintains robust 66.67%, while Sora-2 collapses to 0.00%. The Illusion of Reasoning in Video. critical insight from the fine-grained metrics is the divergence between Outcome Success Rate and Process Success Rate in video models. On GSM8K, Veo-3 achieves relatively high Outcome Success Rate of 74.00% but low Process Success Rate of 12.00%. This substantial 62% delta 36 (a) t=0s (b) t=2s Figure 13 Temporal evolution of math problem solving at t=0s, 2s, 4s, 6s. Generated by Veo-3. (c) t=4s (d) t=6s indicates that video models frequently arrive at the correct final solution through hallucinated or logically invalid visual transitions, effectively guessing the answer without successfully modeling the mathematical steps. In contrast, image models like Nano-banana Pro show near-perfect alignment (97.83% for both metrics), demonstrating that their outputs rely on consistent step-by-step deduction. Action Reflection and Error Correction. Video-specific Action Reflection scores remain critically low across the board (016%), implying limited capacity for self-correction during temporal generation. Inverse Scaling: Interestingly, different video models struggle in different regimes. Sora-2 leads video models on easier tasks (30.00% on GSM8K) but degrades on hard tasks. The Wan-2.2 Anomaly: Conversely, Wan-2.2 performs poorly on simple tasks (2.00% on GSM8K) but unexpectedly outperforms other video models on the hardest benchmarks (15.56% on AIME 24), suggesting it may possess latent reasoning capabilities that are not triggered by simpler prompts, or that its training distribution favors complex visual proofs over simple arithmetic. 8.3.2 Dataset Difficulty Progression Performance degradation across datasets  (Table 20)  reveals three distinct scaling behaviors: robust retention (Nano-banana Pro), rapid collapse (GPT-4o-image), and the middle-peak stability of video models (Veo-3). GSM8K (Grade School): The Hallucination Gap. On this baseline benchmark, image models demonstrate mastery, with Nano-banana Pro hitting ceiling of 97.8%. In contrast, Veo-3 exhibits defining characteristic of current video generation: the illusion of competence. While it achieves high Outcome Success Rate of 74.00%, its Process Success Rate is only 12.00% (and Overall Success matches at 12.00%). This massive disparity suggests that on simple word problems, Veo-3 retrieves correct answers from its training data without generating the corresponding visual reasoning to support them. Sora-2 performs better here (30.00% Overall), but still trails image models by wide margin. MATH500 (High School Competition): The Video Stability Phenomenon. As problem complexity increases, 37 Table 20 Quantitative results for the Math task. We evaluate video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-Banana, Nano-Banana Pro, GPT-4o-image, and Qwen-image) on selected samples from different mathematical reasoning benchmarks. Because image outputs do not support frame-by-frame reasoning, action-reflectionbased metrics are omitted for image generative models (marked with N/A). The highest overall scores in each setting are highlighted in bold. Model Process Success Rate Outcome Success Rate Action Reflection Overall Success Rate Fine-grained Metrics Primary Metric Dataset: GSM8K Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Dataset: MATH500 Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Dataset: AIME24 Video Models Veo-3 Sora-2 Wan-2. Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Dataset: AIME25 Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Dataset: Omni-MATH Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image 12.00% 38.00% 2.00% 44.00% 97.83% 80.00% 44.00% 20.00% 34.04% 3.33% 16.00% 91.84% 29.14% 0.00% 8.33% 8.70% 15.56% 5.00% 63.64% 3.33% 1.12% 3.33% 8.70% 5.56% 1.75% 71.43% 0.00% 4.00% 4.79% 0.62% 0.41% 3.90% 65.77% 0.58% 15.65% 12.00% 16.00% 0.00% N/A N/A N/A N/A 14.00% 10.64% 0.00% N/A N/A N/A N/A 5.00% 4.35% 11.11% N/A N/A N/A N/A 1.67% 4.35% 3.33% N/A N/A N/A N/A 5.09% 6.88% 0.61% N/A N/A N/A N/A 12.00% 30.00% 2.00% 42.00% 97.83% 75.65% 44.00% 18.00% 31.91% 3.33% 16.00% 91.84% 27.34% 0.00% 1.67% 4.35% 15.56% 0.00% 31.82% 0.00% 1.12% 3.33% 0.00% 5.56% 1.75% 66.67% 0.00% 4.00% 3.89% 0.62% 0.41% 3.90% 63.06% 0.58% 14.35% 74.00% 64.00% 2.00% 88.00% 97.83% 83.48% 44.00% 52.00% 59.57% 6.00% 74.00% 91.84% 42.45% 0.00% 8.33% 13.04% 22.22% 15.00% 36.36% 10.00% 1.12% 11.67% 21.74% 10.00% 33.33% 90.48% 3.33% 4.00% 15.57% 1.88% 3.46% 39.94% 85.59% 2.33% 15.65% 38 counter-intuitive trend emerges for video models. While image models like GPT-4o-image suffer 64% relative decline (75.65% 27.34%), Veo-3 actually improves, rising from 12.00% on GSM8K to 18.00% on MATH500. Similarly, Sora-2 remains stable (31.91%). This suggests that the structured, formal nature of competition mathematicswhich often involves distinct, sequential stepsmay align better with video generation temporal priors than the varying linguistic structures of grade-school word problems. Meanwhile, Nano-banana Pro continues to dominate, maintaining 91.84% success rate. AIME & Omni-MATH: The Hard-Reasoning Frontier. On the most challenging benchmarks, model behaviors diverge sharply: The Image SOTA: Nano-banana Pro defies the difficulty curve, achieving 66.67% on AIME25 and 63.06% on Omni-MATH, proving that visual generation models can sustain complex reasoning chains. The Video Shuffle: The hierarchy among video models inverts at this level. Wan-2.2, which failed on easy tasks, spikes to 15.56% on AIME24. Furthermore, on Omni-MATH, Veo-3 emerges as the most robust video model (3.89%), significantly outperforming both Sora-2 (0.62%) and Wan-2.2 (0.41%). This indicates that while Veo-3 struggles with the exactness of simple arithmetic (GSM8K), it possesses superior capacity for generalized, albeit imperfect, reasoning on novel, high-complexity tasks compared to its peers. 8.3.3 Omni-MATH Deep Dive The fine-grained analysis of Omni-MATH reveals that difficulty and domain do not impact video and image models uniformly (Table 21 and Table 22). Difficulty Analysis: The Inverse Scaling Anomaly. Standard benchmarks usually show linear degradation as difficulty increases (T0 T4). However, both model types exhibit counter-intuitive behavior here: Video Non-Monotonicity: Veo-3 displays U-shaped performance curve. It starts at 6.06% (T0), dips to near-zero 1.47% on T3, but surprisingly recovers to 5.00% on T4 (the hardest tier). This suggests that T4 problems, while mathematically harder, may possess canonical structures (standard Olympiad templates) that video models can memorize and reproduce more effectively than the ambiguous, semi-structured problems found in intermediate tiers (T1T3). Image Inverse Scaling: Nano-banana Pro exhibits arguably the most shocking result in the dataset: it performs better on the hardest problems than the easiest ones. Its Overall Success Rate skyrockets from 26.67% on T1 to commanding 82.76% on T4. This implies the model is highly optimized for formal, high-complexity mathematical proofs rather than simpler, variable-heavy word problems. Category Analysis: The Visual vs. Abstract Divide. Domain breakdowns expose the specific cognitive limitations of video generation. Geometry: Video models perform best in Geometry (Veo-3: 8.33%), outperforming their own averages in other domains. This confirms that video generation logic aligns well with geometric construction, where reasoning steps (e.g., drawing auxiliary lines) correspond directly to visual frame transitions. The Abstract Collapse: Conversely, video models face catastrophic failure in domains requiring abstract symbolic manipulation. In Calculus, Discrete Math, Applied Math, and Precalculus, video models collapse to an Overall Success Rate of 0.00%. Notably, in Precalculus, Veo-3 achieves 21.74% Outcome Success Rate but 0.00% overall, illustrating extreme hallucinationguessing the right number through invalid visual morphing. Image Robustness: Unlike video models, Nano-banana Pro maintains high process correctness across all domains, including those where video fails (e.g., 53.33% in Calculus; 68.75% in Applied Math). This confirms that SOTA image models have successfully internalized the symbolic logic required for abstract math, whereas video models remain tethered to visual pattern matching. 39 8.3.4 Key Findings and Implications Three critical insights emerge from the evaluation, pointing to fundamental divergences in how image and video architectures process mathematical logic. 1. The Temporal Penalties of Reasoning. The results establish that video generation currently penalizes mathematical reasoning rather than enhancing it. While image-based models like Nano-banana Pro have achieved near-mastery of complex logic (66%+ on AIME), video models face temporal tax. The requirement to maintain frame-to-frame coherence appears to compete with logical consistency, leading to massive performance gapoften exceeding 46 on hard benchmarks. This suggests that current video architectures treat mathematical derivation as visual texture to be morphed, rather than semantic chain to be constructed, necessitating new architectures that decouple reasoning states from visual rendering. 2. The Hallucination of Competence in Video. defining failure mode of current video models is solution-answer dissociation. Models like Veo-3 frequently achieve high Outcome Success Rates (e.g., 74% on GSM8K) while failing Process Success (12%), indicating they reach correct answers through invalid or hallucinated visual transitions. This contrasts sharply with SOTA image models, where process and outcome metrics are tightly coupled (> 99% correlation for Nano-banana Pro). This dissociation undermines the utility of video for educational or explanatory tasks, as the visual \"proof\" provided by the video is often mathematically illusory despite the final answer being correct. 3. Domain-Specific Islands of Aptitude. Visual mathematical reasoning is not monolithic capability but highly domain-dependent. The Geometry Bias: Video models show clear inductive bias for Geometry (8.33% success), where the reasoning process (construction, transformation) is inherently spatial-temporal. The Abstract Barrier: Conversely, video models collapse to near-zero performance on abstract domains like Calculus and Discrete Math. The Inverse Scaling Paradox: Unexpectedly, models like Wan-2.2 (Video) and Nano-banana Pro (Image) perform significantly better on the hardest benchmarks (AIME, Omni-MATH T4) than on intermediate ones. This implies that high-difficulty problems often follow rigid, canonical templates that models can memorize, whereas simpler problems involving variable linguistic structures or non-standard arithmetic expose the fragility of their generalized reasoning. 8.3.5 Case Study Analysis: Veo-3 Failure Modes Visual case studies of Veo-3s generation traces reveal three structural pathologies that explain the ReasoningOutcome Disconnect observed in the quantitative results. 1. Temporal Drift and Visual Hallucination. The most pervasive failure mode is Temporal Drift, where the logical integrity of the solution degrades as the video progresses, even if the initial setup is correct. This is the primary driver of the massive gap between Outcome Success (74.00%) and Process Success (12.00%) on GSM8K. In 62% of cases, Veo-3 teleports to the correct final answer via visually fluid but mathematically invalid transitionsmorphing numbers arbitrarily or skipping essential derivation steps. This suggests the model minimizes visual prediction error (pixel consistency) at the expense of logical semantic error, prioritizing smooth-looking video over mathematically valid proof. 2. The \"Correction Paradox\" (Toxic Reflection). Counter-intuitively, the models self-correction mechanisms often act as noise injection rather than error mitigation. While Veo-3 exhibits an Action Reflection rate of 14.00% on MATH500, these edits rarely salvage the solution. Qualitative analysis shows that when the model attempts to backtrack and rewrite frame, it frequently introduces continuity errors or hallucinates new, irrelevant constraints. Rather than exhibiting genuine metacognition (detecting logical flaws), the reflection appears to be stochastic processrandomly modifying parts of the visual fieldwhich disrupts the linear chain of reasoning required for multi-step problems. 3. Spatial Attention Collapse (Positional Bias). Veo-3 displays distinct foveal bias, disproportionately 40 attending to the center of the visual field while neglecting constraints or auxiliary figures located at the periphery. This bias is particularly detrimental in Geometry tasks (where Veo-3 otherwise performs well, relative to other domains). In failed instances, the model successfully renders the central geometric construction but fails to incorporate numerical values or variable definitions positioned in the upper or lower margins. This spatial neglect results in correctly solved wrong problemslogic that is internally consistent but divorced from the specific boundary conditions of the prompt. Table 21 Quantitative breakdown results for the Omni-MATH task. We evaluate video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-Banana, Nano-Banana Pro, GPT-4o-image, and Qwen-image) on five difficulty levels (T0T4). Model T0 Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-Banana Nano-Banana Pro GPT-4o-image Qwen-image T1 Video Models Veo-3 Sora-2 Wan-2. Image Models Nano-Banana Nano-Banana Pro GPT-4o-image Qwen-image T2 Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-Banana Nano-Banana Pro GPT-4o-image Qwen-image T3 Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-Banana Nano-Banana Pro GPT-4o-image Qwen-image T4 Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-Banana Nano-Banana Pro GPT-4o-image Qwen-image Process Success Rate Outcome Success Rate Action Reflection Overall Success Rate Fine-grained Metrics Primary Metric 6.06% 3.03% 0.98% 0.00% 53.85% 0.00% 22.00% 4.55% 0.00% 0.00% 0.00% 26.67% 0.00% 8.42% 5.56% 0.00% 0.93% 4.17% 70.00% 2.78% 7.92% 1.96% 0.00% 0.00% 5.88% 70.83% 0.00% 16.67% 5.17% 0.00% 0.00% 9.52% 82.76% 0.00% 25.68% 12.12% 6.06% 4.90% 33.82% 84.62% 0.00% 22.00% 4.55% 3.12% 6.06% 19.70% 60.00% 0.00% 13.68% 16.67% 0.00% 3.70% 47.22% 96.67% 2.78% 8.91% 13.24% 0.00% 1.96% 45.59% 79.17% 2.86% 12.22% 36.67% 0.00% 0.00% 53.97% 93.10% 6.67% 22.97% 3.03% 15.15% 2.94% N/A N/A N/A N/A 1.52% 3.12% 0.00% N/A N/A N/A N/A 2.78% 2.86% 0.00% N/A N/A N/A N/A 1.47% 9.68% 0.00% N/A N/A N/A N/A 15.00% 3.45% 0.00% N/A N/A N/A N/A 6.06% 3.03% 0.98% 0.00% 53.85% 0.00% 22.00% 3.03% 0.00% 0.00% 0.00% 26.67% 0.00% 8.42% 5.56% 0.00% 0.93% 4.17% 66.67% 2.78% 7.92% 1.47% 0.00% 0.00% 5.88% 62.50% 0.00% 12.22% 5.00% 0.00% 0.00% 9.52% 82.76% 0.00% 22.97% Table 22 Quantitative breakdown results for the Omni-MATH task. We evaluate video generative models (Veo-3, Sora-2, and Wan-2.2) and image generative models (Nano-Banana, Nano-Banana Pro, GPT-4o-image, and Qwen-image) on eight categories. Because image outputs do not support frame-by-frame reasoning, action-reflectionbased metrics are omitted for image generative models (marked with N/A). Model Algebra Video Models Veo-3 Sora-2 Wan-2. Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Applied Math Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Calculus Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Discrete Math Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Geometry Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Precalculus Video Models Veo-3 Sora-2 Wan-2.2 Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Number Video Models Veo-3 Sora-2 Wan-2. Image Models Nano-banana Nano-banana Pro GPT-4o-image Qwen-image Process Success Rate Outcome Success Rate Action Reflection Overall Success Rate Fine-grained Metrics Primary Metric 4.17% 4.35% 0.00% 4.17% 61.11% 0.00% 19.70% 0.00% 0.00% 0.00% 16.00% 68.75% 4.00% 14.49% 5.00% 0.00% 0.00% 0.00% 66.67% 0.00% 15.79% 0.00% 0.00% 0.00% 0.00% 54.55% 0.00% 15.94% 8.33% 0.00% 0.00% 0.00% 78.57% 0.00% 23.64% 4.35% 0.00% 0.00% 13.04% 68.18% 0.00% 20.90% 4.17% 0.00% 1.39% 0.00% 64.29% 0.00% 2.82% 4.17% 13.04% 0.00% N/A N/A N/A N/A 8.00% 8.00% 0.00% N/A N/A N/A N/A 5.00% 10.00% 0.00% N/A N/A N/A N/A 4.00% 4.17% 0.00% N/A N/A N/A N/A 4.17% 0.00% 4.17% N/A N/A N/A N/A 8.70% 0.00% 0.00% N/A N/A N/A N/A 12.50% 0.00% 0.00% N/A N/A N/A N/A 4.17% 4.35% 0.00% 4.17% 55.56% 0.00% 19.70% 0.00% 0.00% 0.00% 16.00% 68.75% 4.00% 14.49% 0.00% 0.00% 0.00% 0.00% 53.33% 0.00% 15.79% 0.00% 0.00% 0.00% 0.00% 54.55% 0.00% 15.94% 8.33% 0.00% 0.00% 0.00% 78.57% 0.00% 18.18% 0.00% 0.00% 0.00% 13.04% 68.18% 0.00% 16.42% 4.17% 0.00% 1.39% 0.00% 64.29% 0.00% 2.82% 12.50% 8.70% 5.56% 45.83% 83.33% 4.00% 24.24% 20.00% 0.00% 5.33% 32.00% 87.50% 4.00% 14.49% 10.00% 0.00% 1.67% 35.00% 80.00% 0.00% 15.79% 12.00% 0.00% 5.33% 24.00% 72.73% 0.00% 17.39% 25.00% 0.00% 1.39% 45.83% 92.86% 0.00% 20.00% 21.74% 0.00% 1.45% 65.22% 86.36% 4.17% 16.42% 8.33% 0.00% 1.39% 50.00% 92.86% 4.00% 4.23%"
        },
        {
            "title": "9 Embodied Navigation",
            "content": "We develop the Embodied Navigation task to evaluate 3D Spatial Understanding, 2D Spatial Grounding, Temporal Reasoning, and Physical Commonsense. This task probes models ability to reason about ego-centric environments, interpreting scene geometry, anticipating future states, and respecting physical constraints, and challenges its multi-step planning ability by requiring it to generate video or image that depicts the agents trajectory as it navigates toward the goal."
        },
        {
            "title": "9.1 Task Definition",
            "content": "Figure 14 Visual illustration of the four Embodied Navigation task settings: Panoramic View Last-Mile Navigation, Top-down View Real-World Navigation, 3D Real-World Navigation, and Simultaneous Localization and Generation (SLAG). Figure 14 illustrates the four Embodied Navigation task settings evaluated in our benchmark: Panoramic View Last-Mile Navigation (L.M.Nav.) (Section 10) presents 360 panoramic environment from third-person over-the-shoulder perspective, requiring models to reason over wide-field visual context for short-range navigation. Top-down View Real-World Navigation (T.V.R.-W.Nav.) (Section 11) uses fixed birds-eye camera, emphasizing global spatial planning and long-horizon path prediction. 3D Real-World Navigation (3D R.-W.Nav.) (Section 12) adopts cutaway/dollhouse-style renderings to expose full 3D structure, challenging models to ground navigation decisions in multi-room, multi-level layouts from fixed third-person view. Simultaneous Localization and Generation (SLAG) (Section 13) combines both 3D and top-down environment views, requiring models to jointly localize the agent and generate the surrounding scene layout. Together, these settings form comprehensive testbed for evaluating spatial reasoning, geometric understanding, and scene-generation capabilities of modern video and image generative models."
        },
        {
            "title": "9.2 Hard-Level Control",
            "content": "To build consistent and controllable evaluation suite across the four embodied navigation tasksPanoramic View Last-Mile Navigation, Top-down View Real-World Navigation, 3D Real-World Navigation, and Simultaneous Localization and Generation (SLAG)we structure the dataset along four hard-level axes: Environmental Complexity, View Fidelity, Trajectory Distance, and Destination Specification. Each axis is defined through unified set of principles shared across tasks, while accommodating modality-specific differences in sensing, 43 Table 23 Distribution of evaluation samples across the 24 hard-level configurations defined by environmental complexity, view fidelity, trajectory distance, and destination specification. Counts are reported for the four embodied navigation tasks: Panoramic View Last-Mile Navigation (L.-M.Nav.), Top-down View Real-World Navigation (T.D.V.Nav.), 3D Real-World Navigation (3D R.-W.Nav.), and Simultaneous Localization and Generation (S.L.A.G.). Env. Complexity View Fidelity Distance Level Destination Type L.-M.Nav. T.D.V.Nav. 3D R.-W.Nav S.L.A.G. quality03 1 Floor quality quality05 quality03 2 Plus Floors quality04 quality05 short long short long short long short long short long short long color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description color mark location description 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 Total All 24 Configurations 120 120 120 120 spatial representation, and scene geometry. This organization ensures cross-task comparability while preserving the unique challenges intrinsic to each navigation setting. 9.2.1 Environmental Complexity We source scenes from photo-realistic indoor scans in Matterport3D (Chang et al., 2017) and HM3D (Ramakrishnan et al., 2021b), as well as rendered environments in Habitat (Savva et al., 2019). Environmental complexity varies by the structural layout visible to the agent. In Panoramic View Last-Mile Navigation, complexity is determined by the spatial arrangement captured within single panorama: floor01 scenes correspond to single-floor homes with no vertical transitions, whereas floor02plus scenes include multi-level structures with either implicit stairs not fully visible in the panorama or explicit staircases enabling vertical navigation. For the remaining three tasks, which rely on rendered 3D views or top-down maps, multi-level environments present fully connected floors and additional branching regions. These multi-floor layouts may include basements, attics, outdoor pools, and gardens. By varying structural scale while keeping other factors fixed, the benchmark ensures controlled yet diverse navigation challenges. 9.2.2 View Fidelity Because the four navigation tasks use distinct rendering modalities, we define view fidelity in task-specific yet cross-comparable manner. For Panoramic View Last-Mile Navigation, fidelity captures how much of the environment is visually accessible. Human raters evaluate the extent and spatial distribution of occlusions from foreground objectsi.e., how much of the room layout is obstructed and how many landmarks remain visible. Scores range from 3 to 5, corresponding to quality03 through quality05. For Top-down View Real-World Navigation, 3D Real-World Navigation, and SLAG, fidelity reflects more holistic assessment of scene realism 44 (a) Sample Vocabulary Distribution of Panoramic View Last-Mile Navigation. (b) Sample Vocabulary Distribution of 3D Real-World Navigation. Figure 15 Word Clouds of Semantic Target Descriptions for Destination Specification (Hard Level). It visualizes the frequency of terms used by annotators to describe target locations using natural language. The dominance of spatial prepositions (e.g., front, top, next, right) and structural landmarks (e.g., staircase, window, entrance, living [room]) indicates that annotators heavily rely on relative positioning and salient architectural features. This validates the hard-level protocol, where ambiguity in complex environments is resolved by adding spatially anchored landmarks and floor identifiers to uniquely isolate the intended area. and navigability. Raters consider factors such as the presence of holes or cracks, furnishing quality, door openness, and the plausibility of interaction with the environment. These scenes are likewise scored on the 35 scale aligned with quality03 to quality05. 9.2.3 Trajectory Distance Trajectory distance is defined as the geodesic separation between the agents starting point and the target location. We categorize trajectories into two types. Short trajectories involve relatively direct motion: they require no major turns and may include vertical movement (e.g., reaching an upper floor) without substantial directional changes. Long, In contrast,trajectories include at least one significant turn. To maintain comparability across the four tasks, long trajectories are selected so that they share partial path structure with the corresponding short cases at the same hard level. 9.2.4 Destination Specification Targets are specified either through direct visual annotation or through natural-language description of the same location. For color-based targets (color mark ), annotators highlight the target region in the input image using pure red overlay (#ff0000), ensuring that it is visually distinguishable from its surroundings. For semantic targets (location description), annotators describe the corresponding region using natural language. Although both specifications are intended to reference the same location, ambiguity may arise when multiple similar regions exist. In such cases, annotators include additional disambiguating detailssuch as floor identifiers and spatially anchored landmarksto ensure the description uniquely identifies the intended target area. These four axes allow us to systematically control difficulty across all 24 configuration slots summarized in the table, while keeping the evaluation consistent across tasks that vary in sensing modality, scene rendering, and navigation objective. Table 23 provides the detailed distribution of the Embodied Navigation samples."
        },
        {
            "title": "9.3 Evaluation and Metrics",
            "content": "Evaluating navigation-conditioned video or image generation requires metrics that jointly capture geometric fidelity and visual-semantic reasoning. Our evaluation focuses on whether the agent moves plausibly through the environment, correctly interprets the navigation instruction, adheres to the physical layout of the scene, and preserves the identity and spatial integrity of the target destination. All metrics are binary and are computed directly from the agents execution trace and the generated video frames. The definitions below consolidate the evaluation protocol used across all navigation tasks, drawing from the criteria specified in the evaluation prompt templates and supplementary rules. As illustrated in Figure 16, this framework is adaptive: 45 while several fine-grained metrics are shared across multiple tasks, others are task-specific to address unique navigation modalities. Figure 16 Evaluation metrics flow. Decomposition of the three main metrics Task Completeness, Video Quality in Physical Understanding, and Video Quality in Instruction Following into fine-grained components (e.g., Consistency, Physical Plausibility, Instruction Alignment) and their mapping to the corresponding navigation tasks. 9.3.1 Task Completeness Metrics These metrics assess whether the agent reaches or meaningfully approaches the correct destination based solely on geometric information. They intentionally ignore visual fidelity, semantic correctness, and physical plausibility, which are evaluated by separate metrics. Success Score (S.S. 2D). Measures whether the agents final position lies within the highlighted or textually specified goal region in the 2D overhead map. The score is 1 if the final coordinates fall entirely inside the goal footprint; otherwise 0. Oracle Success Score (O.S. 2D). Provides partial credit when the agent comes sufficiently close to the 2D goal during navigation. The score is 1 if the agents path ever intersects or touches the goal region, even if it does not stop there; otherwise 0. Success Score (S.S. 3D). Checks whether the agent ends inside the correct destination volume in the 3D navigation sequence. This metric is purely geometric and independent of any visual discrepancies at the destination. The score is 1 if the final 3D position is within the target volume; otherwise 0. Oracle Success Score (O.S. 3D). Grants credit when the agent enters the vicinity of the correct 3D destination at any point during its rollout. The score is 1 if the trajectory ever crosses the predefined proximity threshold around the target; otherwise 0. Trajectory Alignment Score. Evaluates whether the agents 2D projected route is consistent with its 3D motion path, focusing on major turns and spatial transitions. score of 1 indicates strong correspondence between the two trajectories; otherwise 0. 9.3.2 Video Quality in Physical Understanding These metrics assess whether the agents motion obeys basic physical principles and remains consistent with the underlying scene geometry. They focus on physical plausibility, continuity, and spatial coherence rather than destination correctness. Object Semantic Score (Obj. Sem.). Evaluates whether the agent interacts with the environment in physically valid way. The agent must not collide with, pass through, or visually intersect solid structures such as walls, furniture, or appliances. Score is 1 if no collision or penetration is observed; otherwise 0. Agent Consistency Score (Agent Con.). Measures temporal continuity and identity preservation of the navigating agent. For image generation tasks: (1) The agents trajectory must remain continuous across frames, and (2) exactly one agent should appear throughout the navigation sequence. Score is 1 if the same agent moves smoothly and consistently across all frames; otherwise 0. Spatial Alignment Score (Spa. Ali.). Checks whether the agents heading, motion direction, and elevation changes remain coherent with the expected physical layout. For image generation tasks: (1) The initial position must be visually identifiable when provided, and (2) the agents initial facing direction must align with its first movement. Score is 1 if heading, transitions, and movement direction are physically and visually consistent; otherwise 0. 9.3.3 Video Quality in Instruction Following Because video generation model can cheat in embodied navigationby fabricating visually plausible destination, altering the environment, or painting new target beneath the agentwe impose strict constraints to ensure faithful instruction following. These metrics evaluate whether the generated video preserves the intended destination and maintains static, coherent scene. Destination Integrity Score (Des. Inte.). Assesses whether the destination region is preserved and correctly interpreted by the model. According to the supplementary rules: (1) The red-marked target region must remain unchanged in size, position, texture, and overall appearance; (2) The agent must not rely on hallucinated alternatives (e.g., newly created look-alike objects or fabricated goal markers). The score is 1 if the original destination remains intact and the agent stops within that region; otherwise 0. Scene Consistency Score (Scene Con.). Evaluates whether the environment remains static throughout the video. No objects, lighting, geometry, or layout elements may appear, disappear, deform, or shift in way that violates the static-scene assumption. The score is 1 if the scene stays unchanged across all frames; otherwise 0. Across all four navigation tasks, these video-quality metrics employ unified binary (pass/fail) definition to ensure consistent evaluation. 9.3.4 Holistic Performance Across all embodied navigation tasks, we define an Overall metric that measures end-to-end success under strict, holistic criterion: sample is considered correct only if all fine-grained evaluation metrics simultaneously achieve score of 1. This requirement highlights key observation about current generative models: strong performance on isolated metrics does not necessarily translate into coherent, successful task execution. The Overall score thus captures the true difficulty of producing videos that are simultaneously geometrically correct, physically plausible, visually consistent, and instruction-faithful. We run both automatic VLM-based evaluation and human evaluation for Embodied Navigation task. The automatic setup uses Gemini-2.5-Pro (Comanici et al., 2025) with structured evaluation prompt; it receives (i) the model-generated video or image and (ii) task-specific context, and returns binary metric scores and short thinking of the justifications. Human raters are given the exact same media and prompts to ensure their labels align with the same judgment criteria."
        },
        {
            "title": "9.4 Overall Evaluation Results",
            "content": "We evaluate the embodied navigation capabilities of state-of-the-art generators by consolidating per-task performance into four metric families: Task Completeness, Physical Understanding, Instruction Following, and the strict Holistic Overall metric. The comparative results across models are detailed in Table 24, while Table 25 provides fine-grained human evaluation of Veo-3 to diagnose specific failure modes. 47 9.4.1 Comparative Analysis of Generators As detailed in Table 24, Nano-banana establishes distinct lead across most axes, demonstrating superior instruction adherence and holistic accuracy. It notably surpasses 74% holistic accuracy on both Panoramic and 3D navigation tasks. Among video models, performance is highly variable. Veo-3 generally outperforms other video generators like Sora-2 and Wan-2.2, particularly in maintaining physical plausibility (reaching 93.3% in Panoramic views). However, video models often exhibit \"completeness vs. coherence\" trade-off: while they frequently achieve decent physical understanding scores, they struggle to combine this with instruction following, resulting in significantly lower holistic scores compared to the image-based Nano-banana. GPT-4o-image generally lags behind, struggling to maintain temporal coherence across sequences. Performance varies significantly by task complexity: Panoramic View (L.M.Nav.): Nano-banana achieves its second-highest holistic score here (74.2%). Among video models, Veo-3 performs best (60.0% holistic) and actually achieves the highest physical understanding score of any model (93.3%). In contrast, Sora-2 and Wan-2.2 fail to produce holistically valid trajectories (0.0%), despite moderate physical understanding. Top-down View (T.V.R.-W.Nav.): This task proves universally difficult, yet it is the only category where video model outperforms Nano-banana. Veo-3 achieves the highest holistic accuracy (19.5%), surpassing Nano-banana (11.1%) and Sora-2 (3.4%). This suggests that Veo-3 possesses superior overhead spatial reasoning capabilities compared to its peers. 3D Real-World Navigation (3D R.-W.Nav.): This task highlights the widest gap between architecture types. Nano-banana achieves dominant holistic score of 79.2%. While Wan-2.2 (24.2%) and Veo-3 (22.5%) manage to complete some routes, Sora-2 fails completely (0.0%), indicating that current video generators struggle to maintain long-horizon coherence in 3D environments. Simultaneous Localization and Generation (SLAG): Nano-banana leads with 28.8% holistic accuracy. Contrary to simpler tasks, Sora-2 (12.9%) slightly outperforms Veo-3 (11.2%) here, while Wan-2.2 collapses almost entirely (0.8%). While video models do not fail completely (scoring > 0 on fine-grained metrics), their low holistic scores indicate critical inability to maintain the precise cross-view alignment required for SLAG. 9.4.2 Fine-Grained Analysis of Veo-3 To better understand why video models struggle despite strong visual quality, we report detailed human evaluation of Veo-3 in Table 25. The results uncover sharp disconnect between component-level physical understanding and holistic task success. Local vs. Global Success: In the Panoramic setting, Veo-3 achieves strong component scores for Object Semantic Score (87.50%) and Agent Consistency Score(92.50%). However, the Overall success rate collapses to 26.67% because the model rarely satisfies all validity checks simultaneously. The Plausible but Wrong Problem: In 3D Real-World Navigation, Veo-3 retains high Physical Quality scores (77.50% Agent Consistency) but fails almost entirely on Instruction Following (1.67% Overall). The video generation looks physically realisticobjects are stable and lighting is consistentbut the agent fails to navigate to the correct destination. Geometric Failures: The SLAG task results are particularly telling. While Scene Consistency is remarkably high (93.90%), Trajectory Alignment is nearly non-existent (6.10%). This confirms that while the model can generate temporally consistent frames, it lacks the geometric grounding necessary to align those frames with specified trajectory map."
        },
        {
            "title": "9.5 Key Observations",
            "content": "In this section, we distill the key insights that emerge from evaluating state-of-the-art video generation models across the four embodied navigation tasks. These findings offer high-level view of current strengths and limitations, setting the stage for deeper investigation. In the following sectionsSections 10.3, 11.3, 12.3 and 13.3we introduce each subtask in detail and provide comprehensive quantitative and qualitative analyses. 48 Table 24 Per-task performance across all metric families for three video generative models (Veo-3, Sora-2, and Wan-2.2) and two image generative models (Nano-banana, GPT-4o-image). Task / Model Task Completeness Physical Understanding Instruction Following Holistic Overall Panoramic View Last-Mile Navigation (L.M.Nav.) Top-down View Real-World Navigation (T.V.R.-W.Nav.) Veo-3 Sora-2 Wan2.2 Nano-banana GPT-4o-Image Veo-3 Sora-2 Wan2.2 Nano-banana GPT-4o-Image Veo-3 Sora-2 Wan2.2 Nano-banana GPT-4o-Image Veo-3 Sora-2 Wan2.2 Nano-banana GPT-4o-Image 76.2 0.4 25.0 79.2 0.0 66.1 38.1 32.2 46.5 16.5 76.7 15.4 60.4 79.9 14. 41.2 36.4 12.9 56.7 30.2 3D Real-World Navigation (3D R.-W.Nav.) Simultaneous Localization and Generation (SLAG) 93.3 84.4 71.4 91.1 32.2 69. 75.4 66.7 74.1 37.3 79.4 76.9 68.1 96.8 63.1 56.6 76.7 24.0 81.3 61.7 83.8 0.4 39. 85.4 0.0 44.1 13.6 22.5 43.8 4.7 47.1 11.2 69.2 86.8 16.2 38.8 61.6 25.8 73.5 58. 60.0 0.0 0.0 74.2 0.0 19.5 3.4 5.1 11.1 3.4 22.5 0.0 24.2 79.2 13.3 11.2 12.9 0. 28.8 16.1 Table 25 Fine-grained human evaluation of Veo-3 across the four embodied navigation tasks in MMGR: Panoramic View Last-Mile Navigation, Top-down View Real-World Navigation, 3D Real-World Navigation, and Simultaneous Localization and Generation (SLAG). Each column reports per-task pass rates for the corresponding metric, and the Overall row reflects the strict holistic success rate where all applicable checks must succeed simultaneously. Metric Panoramic View Last-Mile Navigation Top-down View Real-World Navigation 3D Real-World Navigation Simultaneous Localization and Generation Task Completeness S. S. 3D O. S. 3D S. S. 2D) O. S. 2D) Traj. Ali. 70.00 71.67 N/A N/A N/A Physical Understanding Quality Obj. Sem. Agent Con. Spa. Ali. Des. Inte. Scene Con. 87.50 92.50 96.67 60.00 64.17 Instruction Following Quality Holistic Performance Overall 26.67 N/A N/A 33.05 65.25 N/A 38.14 70.34 58.47 65.25 81.36 5.93 76.67 85.00 N/A N/A N/A 78.33 77.50 68.33 45.83 16.67 1.67 42.68 53.66 23.17 28.04 6.10 34.15 65.85 65.85 54.88 93. 0.00 Note: S.S. = Success Score; O.S. = Oracle Success Score; Traj. Ali. = Trajectory Alignment Score; Obj. Sem. = Object Semantic Score; Agent Con. = Agent Consistency Score; Spa. Ali. = Spatial Alignment Score; Des. Inte. = Destination Integrity Score; Scene Con. = Scene Consistency Score. N/A indicates the metric is not applicable to the specific task. 49 Findings 1: Strong Capability in Ego-Centric Navigation Video Generation. Our results indicate that video generation models possess surprisingly robust capability for third-person, off-the-shoulder perspectives navigation generation. Beyond simple frame coherence, these models demonstrate abilities that have been long-pursuing goal in embodied navigation (Batra et al., 2020; Anderson et al., 2018a), such as the understanding of scene layouts (Li et al., 2023; Fuentes-Pacheco et al., 2015; Henriques & Vedaldi, 2018) and semantic meanings (Cartillier et al., 2021; Chaplot et al., 2020b; Zhou et al., 2025; Irshad et al., 2021; Wani et al., 2020; Blanco et al., 2008; Konolige et al., 2011; Gomez et al., 2020; An et al., 2022), effectively recognizing goal objects and maintaining contextual consistency even when the agents body is visible within the frame. Notably, they exhibit strong capabilities in imagining spatial relationships (Koh et al., 2021; Qin et al., 2025; Sridhar et al., 2024; Bar et al., 2025; Shah et al., 2025), allowing them to consistently model the geometric interaction between the agent and the environment. This confirms that video diffusion models implicitly learn powerful geometric priors and environment continuity, which are critical for perceiving the immediate surroundings from an embodied perspective. However, such spatial imagination does not translate into functional navigation except in the Panoramic View Last-Mile Navigation task. Only when the goal is already visible, and the required trajectory is short and locally constrained, do models like Veo-3 achieve meaningful success rates. For longer-horizon or multi-view settings, their ability to reason over distance, occlusion, or multi-step spatial transformations rapidly deteriorates, exposing the gap between ego-centric perception and actionable planning. Findings 2: Free-form Generation is Insufficient for Navigation. Tasks requiring controlled movement, like Topdown View Real-World Navigation, SLAG, and Instruction-based Target Search, reveal consistent failure pattern: video models excel at producing aesthetically coherent frames but fail to follow navigation constraints. Models such as Sora-2 often generate plausible but irrelevant motion, drift off the intended path, or remain static despite instructions. Even with perfect semantic grounding (e.g., Sora-2 achieving 87.35% Object Semantic Score), Success Scores remain near zero, underscoring that generative quality does not equal navigational utility. The inability to reliably execute directed motion suggests that current video models lack mechanisms for consistent temporal control, trajectory commitment, or adherence to spatial rules imposed by the prompt. Findings 3: Cross-View Spatial Alignment Exists, But Must Be Explicitly Activated. Our SLAG experiments reveal an unexpected finding: while trajectory alignment remains highly challenging, scene consistency improves markedly when models are evaluated in cross-view alignment framework. In SLAG, aligning generated motion with 2D top-down map forces the model to maintain geometric coherence across viewpoints, and this pressure appears to activate latent spatial priors that remain unused in other tasks. Veo-3, in particular, maintains high Scene Consistency even when its navigation fails, suggesting robust internal world modeling. However, this ability does not emerge in free-form video generation or text-only prompting. It requires structured multimodal conditioning, indicating that cross-view alignment is learnable but currently underutilized capability in video models. Additional Profile of Strengths and Weaknesses. Our results suggest preliminary navigation capability profile for video generation models: Strengths: strong semantic grounding, accurate object identity preservation, coherent ego-centric spatial reasoning, and latent ability for cross-view geometric alignment. Weaknesses: poor rule-following under long-horizon instructions, inability to maintain goal-directed motion, hallucination under abstract destination descriptions (e.g., Veo-3 generating an entirely new scene matching the semantic description), and brittleness to environmental complexity. Summary. Taken together, these findings suggest that current video generation models possess strong ego-centric spatial perception, semantic grounding, and latent cross-view alignment capability, but they fail to reliably execute goal-driven navigation under free-form generation. Their cross-view understanding must be explicitly elicited through structured multimodal constraints such as SLAG, while long-horizon trajectory control and rule following remain largely unsolved. In the following sections, we provide detailed analyses for each navigation task, including fine-grained quantitative and qualitative breakdowns."
        },
        {
            "title": "10.1 Task Definition",
            "content": "The Panoramic View Last-Mile Navigation task evaluates fine-grained, embodied decision-making when target is already within the agents visual field. Distinct from large-scale route planning, last-mile navigation isolates the critical final phase of movement: precisely localizing visible goal and generating an optimal shorthorizon trajectory toward it. In this setting, the model receives single panoramic RGB observation and must infer feasible motion plan leading directly to the destination. Targets are either explicitly defined by red marker or implicitly specified via object-class descriptions, necessitating synthesis of geometric reasoning and semantic recognition. successful execution by Veo-3 is illustrated in Figure 17. Ultimately, this task probes models capacity to interpret egocentric 360 spatial layouts, estimate relative pose and depth, and execute precise actions to bridge perception and control in the final meters of navigation. Figure 17 successful case completed by Veo-3 on the Panoramic View Last-Mile Navigation task. The model navigates the final segment of the route from first-person panoramic viewpoint, demonstrating coherent mental mapping, spatial awareness, and accurate localization needed to reach the precise target destination."
        },
        {
            "title": "10.2 Evaluation and Metrics",
            "content": "Evaluating navigation-conditioned video generation presents dual challenge: quantifying geometric precision while ensuring visual-semantic fidelity. Our framework explicitly assesses whether the agent executes plausible motion, maintains structural consistency, and preserves the semantic integrity of the destination. All metrics are binary (pass/fail) and come from the rollout traces and generated frames. We adopt the shared embodied evaluation protocol in Section 9.3: all metrics are binary and labeled via the automatic VLM/human rules on the rollout traces and generated frames. The Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged; here we only note how task completeness metrics are used for Panoramic Last-Mile Navigation and how the gated scores are formed. 10.2.1 Task Completeness Metrics (Geometry Only) These metrics isolate navigational accuracy, evaluating whether the agent reaches the destination based purely on geometric coordinates. Visual fidelity and physical consistency are excluded here and assessed by subsequent metrics. Success Score 3D (S.S. 3D). Measures whether the final state of the agent coincides with the target location. This metric is strictly geometric; it is satisfied if the agent terminates its trajectory within the defined volume of the destination, regardless of visual preservation. Oracle Success Score 3D (O.S. 3D). relaxed variation of Success Score 3D that credits the agent for reaching the target at any point during the trajectory. The metric passes if the agent enters the destinations proximity threshold at any timestep, irrespective of where it ultimately stops. 10.2.2 Gate Metrics and Holistic Performance To ensure rigorous assessment, we introduce composite Gate Metrics that intersect geometric success with semantic and physical validity. These metrics serve as strict filters, disqualifying trajectories that achieve goal conditions through generative artifacts such as hallucination or geometry warping. Success (3D) with Original Destination. This metric imposes strict visual-geometric consistency check. It counters the tendency of generative models to exploit scene manipulationsuch as fabricating target at the agents feet or warping the room layoutto satisfy stopping conditions artificially. trajectory is successful under this metric only if it satisfies the Success Score 3D (S.S. 3D) while simultaneously passing the Destination Integrity (Des. Inte.) and Scene Consistency (Scene Con.) checks. Physics Validity. holistic measure of physical plausibility, ensuring that movement is grounded in reality rather than dream-like transitions. Geometric success is disregarded if the agent violates fundamental physical laws. This metric requires the simultaneous satisfaction of three conditions: (1) Object Semantic Score (Obj. Sem.) (no collisions); (2) Agent Consistency Score Agent Con.) (identity persistence); and (3) Spatial Alignment Score (Spa. Ali.) (pose coherence). Overall Success = Success Score 3D Oracle Success Score 3D Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; sample passes only when all seven binary checks are 1. This strict criterion highlights that strong performance on individual metrics does not guarantee successful, plausible end-to-end task completion."
        },
        {
            "title": "10.3 Evaluation Results",
            "content": "Key Finding: Model Capabilities & The Evaluation Gap Performance Stratification: Results are strictly binary; only Veo-3 and Nano-banana function, while Sora-2 and GPT-4o fail completely. Surprisingly, the image-based Nano-banana often outperforms Veo-3 in complex environments and instruction following. Auto-Metric Failure: massive reliability gap exists between evaluation methods. Auto-metrics rate Veo-3 at 73.33% success, while humans rate it at only 25.00%. Auto-evaluators successfully track camera motion but fail to penalize physical hallucinations and logic violations. 10.3.1 VLM-Based Evaluation The quantitative results in Table 26 reveal distinct stratification in model capabilities for the Panoramic View Last-Mile Navigation task. The performance landscape is strictly binary: Veo-3 and Nano-banana demonstrate functional navigation capabilities, while Sora-2 and GPT-4o-image fail to complete the task entirely (0.00% Overall Success). Top-Tier Performance (Veo-3 vs. Nano-banana). While Veo-3 is the primary video model of interest, Nano-banana serves as surprisingly robust baseline, frequently outperforming the video model in complex scenarios. Environmental Robustness: In simple environments (floor01 ), both models achieve parity with 73.33% Overall Success rate. However, as environmental complexity increases (floor02plus), Veo-3s performance degrades sharply to 46.67%, whereas Nano-banana maintains high success rate of 75.00%. Visual Fidelity Scaling: notable divergence occurs in how the models handle visual quality. Nano-banana exhibits positive scaling with fidelity, improving from 60.00% success at quality03 to 82.50% at quality05. In contrast, Veo-3 plateaus, peaking at only 62.50% regardless of increased visual fidelity. Instruction Following Capabilities. critical differentiator found in the results is the response to destination specifications. 52 Visual vs. Textual Guidance: When the destination is specified by visual marker (color mark ), both models perform comparably (70%). Descriptive Navigation: When the destination is defined by text instructions (location description), Veo-3 struggles, dropping to 50.00% success. Conversely, Nano-banana excels, achieving its highest success rate of 78.33%. This suggests that while Veo-3 offers superior temporal consistency (Agent Consistency >93% across most settings), Nano-banana possesses superior multimodal understanding, allowing it to map textual descriptions to visual goals more effectively. Navigation Failures (Sora-2 and GPT-4o-image). The failing models illustrate two distinct modes of error. Static vs. Dynamic Failure: GPT-4o-image achieves high Object Semantic Scores (up to 100%) but fails to generate the temporal trajectory required for navigation, resulting in 0.00% success rate. Steerability Failure: Sora-2 presents steerability problem. While it maintains decent Physical Understanding (e.g., 88.33% Object Semantic Score in floor01 ) and generates coherent video, it lacks Destination Integrity (0.00%). The model hallucinates plausible scenes but cannot be constrained to specific coordinate or target path. Impact of Trajectory Distance. As anticipated, performance for the leading video model, Veo-3, degrades with trajectory length, dropping from 66.67% (Short) to 53.33% (Long). Nano-banana remains more resilient to distance, maintaining 66.67% success rate even on long trajectories, further validating the robustness of image-based stepwise generation over holistic video generation for this specific navigation benchmark. 10.3.2 Human vs. Automated Evaluation Discrepancy Table 27 presents critical finding: Automatic evaluation metrics are significantly over-optimistic compared to Human Expert evaluation. This discrepancy exposes the limitations of current auto-evaluators in detecting physical and logical inconsistencies in video generation. The Overall Success Collapse. The most striking divergence appears in the Holistic Metric. While Auto Evaluation suggests Veo-3 is competent navigator, human judges strongly disagree. Baseline Discrepancy: In the simplest setting (floor01 ), Auto Evaluation reports 73.33% success rate, whereas Human Evaluation rates it at only 25.00%. Complexity Penalty: This gap widens in complex scenarios. For Long Trajectories, Auto claims 53.33% success rate, while humans find that only 11.67% of the trajectories are actually successful. The Hallucination Blind Spot (Physics & Consistency). The primary driver of the reliability gap is the auto-evaluators inability to penalize dream-like logic violations. Physics Validness: There is massive disconnection in physical grounding. For floor01, Auto reports In Long Trajectories, human-rated physics validity 81.67% validity, while humans report 25.00%. plummets to 16.67%, indicating that over longer durations, the model frequently clips through objects, floats, or violates lighting consistencieserrors that auto-metrics (likely based on frame-to-frame pixel similarity) fail to capture. Scene Stability: Auto-evaluators consistently overrate Scene Consistency (e.g., 98.33% for floor01). In contrast, humans rate it at 66.67%. This confirms that auto-metrics struggle to detect subtle temporal morphing of walls, textures, or landmarks that humans immediately recognize as inconsistent. Notably, for Location Description tasks, Scene Consistency drops to 48.33% in human eyes, suggesting that text-conditioning may induce greater visual instability than visual prompting. Task Completion vs. Reality. Even when the agent appears to move correctly, it often fails to satisfy the exact conditions of the destination. Auto-metrics (Destination Integrity) suggest the agent reaches the target 90.00% of the time in simple settings. Humans, however, judge this at only 55.00%, indicating that the agent often stops short, overshoots, or faces the wrong direction upon arrivalnuances the auto-evaluator misses. Table 26 Quantitative results for the Panoramic View Last-Mile Navigation benchmark. We compare performance across two video generative models (Veo-3 and Sora-2) and two image generative models (Nano-banana, and GPT-4o-image). Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Model Success Score (3D) Oracle Success Score (3D) Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Success (3D) Original Destination Physics Validness Overall Success Environmental Complexity Level: floor01 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: floor02plus Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image View Fidelity Level: quality03 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: quality04 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: quality05 Video Models Veo-3 SoraImage Models Nano-banana GPT-4o-image Trajectory Distance Level: short Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: long Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image 90.00% 0.00% 76.67% 0.00% 58.33% 0.00% 80.00% 0.00% 72.50% 0.00% 62.50% 0.00% 75.00% 0.00% 85.00% 0.00% 75.00% 0.00% 87.50% 0.00% 81.67% 0.00% 86.67% 0.00% 66.67% 0.00% 70.00% 0.00% Destination Specification Level: color mark Video Models Veo-3 Sora-2 83.33% 0.00% Image Models Nano-banana GPT-4o-image 76.67% 0.00% Level: location description Video Models Veo-3 Sora-2 65.00% 0.00% Image Models Nano-banana GPT-4o-image 80.00% 0.00% 90.00% 0.00% 80.00% 0.00% 55.00% 0.00% 80.00% 0.00% 70.00% 0.00% 62.50% 0.00% 75.00% 0.00% 90.00% 0.00% 72.50% 0.00% 87.50% 0.00% 80.00% 0.00% 86.67% 0.00% 65.00% 0.00% 73.33% 0.00% 80.00% 0.00% 80.00% 0.00% 65.00% 0.00% 80.00% 0.00% 98.33% 0.00% 91.67% 0.00% 91.67% 1.69% 90.00% 0.00% 92.50% 2.56% 80.00% 0.00% 97.50% 0.00% 92.50% 0.00% 95.00% 0.00% 100.00% 0.00% 98.33% 1.67% 95.00% 0.00% 91.67% 0.00% 86.67% 0.00% 93.33% 0.00% 86.67% 0.00% 96.67% 1.67% 95.00% 0.00% 90.00% 0.00% 75.00% 0.00% 55.00% 0.00% 78.33% 0.00% 70.00% 0.00% 62.50% 0.00% 75.00% 0.00% 80.00% 0.00% 72.50% 0.00% 87.50% 0.00% 80.00% 0.00% 85.00% 0.00% 65.00% 0.00% 68.33% 0.00% 80.00% 0.00% 73.33% 0.00% 65.00% 0.00% 80.00% 0.00% 81.67% 81.67% 88.33% 0.00% 83.33% 64.41% 86.67% 0.00% 77.50% 69.23% 82.50% 0.00% 82.50% 67.50% 87.50% 0.00% 87.50% 82.50% 92.50% 0.00% 83.33% 76.67% 91.67% 0.00% 81.67% 69.49% 83.33% 0.00% 85.00% 67.80% 81.67% 0.00% 80.00% 78.33% 93.33% 0.00% 73.33% 0.00% 73.33% 0.00% 46.67% 0.00% 75.00% 0.00% 55.00% 0.00% 60.00% 0.00% 62.50% 0.00% 80.00% 0.00% 62.50% 0.00% 82.50% 0.00% 66.67% 0.00% 81.67% 0.00% 53.33% 0.00% 66.67% 0.00% 70.00% 0.00% 70.00% 0.00% 50.00% 0.00% 78.33% 0.00% 90.00% 1.67% 80.00% 0.00% 66.67% 0.00% 80.00% 0.00% 80.00% 2.56% 62.50% 0.00% 75.00% 0.00% 90.00% 0.00% 80.00% 0.00% 87.50% 0.00% 83.33% 0.00% 86.67% 0.00% 73.33% 1.69% 73.33% 0.00% 88.33% 1.69% 80.00% 0.00% 68.33% 0.00% 80.00% 0.00% 93.33% 93.33% 96.67% 100.00% 95.00% 81.36% 96.67% 91.67% 92.50% 79.49% 92.50% 90.00% 95.00% 85.00% 100.00% 100.00% 95.00% 97.50% 97.50% 97.50% 96.67% 88.33% 98.33% 95.00% 91.67% 86.44% 95.00% 96.67% 96.67% 86.44% 95.00% 93.33% 91.67% 88.33% 98.33% 98.33% 93.33% 88.33% 88.33% 1.67% 93.33% 77.97% 90.00% 0.00% 92.50% 82.05% 87.50% 0.00% 95.00% 80.00% 87.50% 0.00% 92.50% 87.50% 92.50% 2.50% 93.33% 83.33% 93.33% 1.67% 93.33% 83.05% 85.00% 0.00% 93.33% 77.97% 85.00% 0.00% 93.33% 88.33% 93.33% 1.67% 93.33% 93.33% 88.33% 0.00% 91.67% 76.27% 86.67% 0.00% 90.00% 76.92% 82.50% 0.00% 90.00% 87.50% 87.50% 0.00% 97.50% 90.00% 92.50% 0.00% 90.00% 85.00% 91.67% 0.00% 95.00% 84.75% 83.33% 0.00% 93.33% 81.36% 81.67% 0.00% 91.67% 88.33% 93.33% 0.00% Areas of Agreement: The Camera Motion Paradox. Interestingly, Spatial Alignment is the only metric where humans frequently rate the model higher than the automatic system. For Long Trajectories, humans rated Spatial Alignment at 98.33%, exceeding the Auto rating of 95.00%. This suggests that the camera movement itself (the ego-motion) is highly convincing to human observers. The failure of Veo-3 is not in moving like an agent, but in maintaining the world around the agent. The motion is realistic; the environment is not. Conclusion. While Veo-3 demonstrates state-of-the-art potential in generating semantically correct paths, the stark contrast between the 73.33% (Auto) and 25.00% (Human) success rates serves as warning. Current automatic metrics for video generation prioritize visual similarity over physical logic, making them insufficient proxies for measuring true World Modeling capabilities. Table 27 Quantitative results for the Panoramic View Last-Mile Navigation benchmark. We compare automatic evaluations against human judgments for Veo-3 across four hard-level dimensions. Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Evaluation Success Score (3D) Oracle Success Score (3D) Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Success (3D) Original Destination Physics Validness Overall Success Environmental Complexity Level: floor01 Auto Evaluation Human Evaluation 90.00% 73.33% Level: floor02plus Auto Evaluation Human Evaluation 58.33% 66.67% View Fidelity Level: quality03 Auto Evaluation Human Evaluation 72.50% 77.50% Level: quality04 Auto Evaluation Human Evaluation 75.00% 65.00% Level: quality05 Auto Evaluation Human Evaluation 75.00% 67.50% Trajectory Distance Level: short Auto Evaluation Human Evaluation 81.67% 83.33% Level: long Auto Evaluation Human Evaluation 66.67% 56.67% Destination Specification Level: color mark Auto Evaluation Human Evaluation 83.33% 86.67% Level: location description Auto Evaluation Human Evaluation 65.00% 53.33% 90.00% 75.00% 66.67% 68.33% 80.00% 77.50% 75.00% 67.50% 80.00% 70.00% 83.33% 85.00% 73.33% 58.33% 88.33% 86.67% 68.33% 56.67% 93.33% 88.33% 95.00% 86.67% 92.50% 87.50% 95.00% 92.50% 95.00% 82.50% 96.67% 91.67% 91.67% 83.33% 96.67% 83.33% 91.67% 91.67% 93.33% 95.00% 93.33% 90.00% 92.50% 92.50% 95.00% 92.50% 92.50% 92.50% 93.33% 98.33% 93.33% 86.67% 93.33% 90.00% 93.33% 95.00% 93.33% 96.67% 91.67% 96.67% 90.00% 95.00% 90.00% 100.00% 97.50% 95.00% 90.00% 95.00% 95.00% 98.33% 93.33% 96.67% 91.67% 96.67% 90.00% 55.00% 55.00% 65.00% 70.00% 65.00% 75.00% 57.50% 72.50% 57.50% 80.00% 76.67% 65.00% 43.33% 80.00% 60.00% 65.00% 60.00% 98.33% 66.67% 91.67% 61.67% 92.50% 67.50% 97.50% 67.50% 95.00% 57.50% 98.33% 65.00% 91.67% 63.33% 93.33% 80.00% 96.67% 48.33% 90.00% 31.67% 55.00% 33.33% 70.00% 42.50% 75.00% 27.50% 72.50% 27.50% 80.00% 48.33% 65.00% 16.67% 80.00% 41.67% 65.00% 23.33% 81.67% 25.00% 83.33% 36.67% 77.50% 35.00% 82.50% 32.50% 87.50% 25.00% 83.33% 45.00% 81.67% 16.67% 85.00% 40.00% 80.00% 21.67% 73.33% 25.00% 46.67% 28.33% 55.00% 32.50% 62.50% 27.50% 62.50% 20.00% 66.67% 41.67% 53.33% 11.67% 70.00% 33.33% 50.00% 20.00%"
        },
        {
            "title": "10.4 Case Study: Qualitative Failure Modes",
            "content": "This case study, visualized in Figure 18, evaluates the world modeling capabilities of video generation models during last-mile navigation task. In this scenario, humanoid robot must traverse 3D environment to reach specific target marked on the floor. The analysis reveals that while models can generate plausible motion, they suffer from distinct breakdowns in object permanence, physical laws, and instruction grounding. Veo-3 (Temporal Identity Collapse). Initially, Veo-3 demonstrates strong spatial reasoning. The agent successfully perceives pillar as solid obstacle and navigates around it, exhibiting valid collision avoidance. However, the generation fails critically in Agent Consistency at the 5-second mark. The model suffers from doppelgänger hallucination, where second agent spontaneously manifests and approaches the camera while the initial agent retreats. This loss of agent identity results in an Oracle Success Score of 0, as the intended actor never completes the trajectory. Sora-2 (Semantic and Physical Drift). Sora-2 struggles to maintain the fundamental constraints of the scene. By Frame 2s, it exhibits severe semantic drift, abruptly shifting the visual style and ungrounding the instruction: 55 Figure 18 Qualitative Comparison on Panoramic View Last-Mile Navigation. We analyze three models (Veo-3, Sora-2, and Wan-2.2) navigating toward red target. The figure highlights distinct failure modes: Veo-3 suffers from agent inconsistency (hallucinating second agent), Sora-2 exhibits severe geometric and physical violations (style shift, clipping through railings), and Wan-2.2 struggles with scene stability (altering structural elements like doors and pillars) despite smooth agent motion. Figure 19 Top-down View Real-World Navigation task showing birds-eye view navigation. The model must interpret the abstract 2D map representation and plan path from start to goal, demonstrating 2D spatial reasoning and temporal reasoning from global perspective. successful case completed by Veo-3 on the Top-down View Real-World Navigation task. the model incorrectly remaps the red target marker from the floor to vertical wall. Furthermore, the physics engine collapses; the agent is observed floating on non-existent air platform and subsequently clipping through solid railing. This confirms that Sora-2 prioritizes visual fluidity over collision geometry or logical consistency. Wan-2.2 (Environmental Instability). In contrast to the others, Wan-2.2 maintains high Agent Consistency, producing smooth, continuous motion without teleportation or scaling artifacts. However, it fails to maintain the stage around the actor. As the camera angle shifts (Frame 2s), the model suffers from background object permanence failures, inexplicably erasing glass door and hallucinating new pillars within the room. While it preserves Destination Integrity better than Sora-2 (keeping the target relative to the agent), the volatility of the environment ultimately prevents successful navigation."
        },
        {
            "title": "11.1 Task Definition",
            "content": "The Top-down Real-World Navigation task benchmarks models ability to synthesize advanced spatial reasoning with semantic understanding in complex, human-centric environments. This task requires models to interpret diverse and partially occluded top-down layoutssuch as floor plansto generate optimal trajectories. Simultaneously, it tests robust semantic grounding by demanding goal identification based on textual descriptions of varying abstraction."
        },
        {
            "title": "11.2 Evaluation and Metrics",
            "content": "We adopt the shared embodied evaluation protocol in Section 9.3: Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged. Here we only note how task completeness metrics are used for Top-down View Real-World Navigation and how the gated scores are formed. 11.2.1 Task Completeness Metrics (Geometry Only) These metrics isolate navigational accuracy, evaluating whether the agent reaches the destination based purely on geometric coordinates. Visual fidelity and physical consistency are excluded here and assessed by subsequent metrics. 57 Success Score 2D (S.S. 2D). This metric evaluates whether the agent terminates its trajectory strictly within the designated goal region in the top-down 2D view. Success is achieved if and only if the agents final position is contained entirely within the goal footprint. Oracle Success Score 2D (O.S. 2D). This metric determines if the agent encounters the goal region at any point during the rollout, regardless of the stopping location. Credit is awarded if the agents trajectory intersects with or passes adjacent to the 2D goal boundary at any timestep. 11.2.2 Gate Metrics and Holistic Performance As in Panoramic Last-Mile Navigation, gates guard against generative shortcuts, but here the challenge comes from the 2D top-down view: success must hold on static map, and Physical Validness is stricter because object recognition and agent orientation are blurrier from overhead. Success (2D) Original Destination. This composite metric mitigates the tendency of video generation models to hallucinate solution shortcuts. It is satisfied if and only if the Success Score 2D, Destination Integrity Score, and Scene Consistency Score are met simultaneously. This ensures that successful arrival is valid and not the result of the model altering the scene layout or destination coordinates to simplify the task. Physical Validity. To enforce physical realism, this composite metric acts as strict consistency check. It is satisfied only when the Object Semantic Score, Agent Consistency Score, and Spatial Alignment Score are simultaneously met. This precludes physical anomalies such as object clipping, teleportation, or unnatural drift. Overall Success = Success Score 2D Oracle Success Score 2D Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; sample passes only when all seven binary checks are 1. Finally, across all embodied navigation tasks, we introduce the Overall metric to evaluate holistic performance. sample is considered successful only if all fine-grained evaluation metrics are satisfied. This strict criterion demonstrates that strong performance on individual sub-metrics does not guarantee successful end-to-end task completion."
        },
        {
            "title": "11.3 Evaluation Results",
            "content": "Key Finding: The Reality & Modality Gap Physics Hallucination: Video models like Veo-3 excel at path coherence but fail at physical realism. Automatic metrics miss this, inflating success rates (37.14%) compared to human judgment (10.34%) by ignoring violations like wall-clipping. The Semantic Cliff: Performance drops by >3 when instructions shift from visual markers to textual descriptions. Models struggle significantly to ground abstract linguistic commands into spatial actions. The Top-down Real-World Navigation task evaluates models ability to synthesize spatial planning with semantic understanding. The following analysis dissects model performance across environmental complexities, instruction modalities, and visual fidelities, followed by critical examination of automatic metric reliability against human judgment. 11.3.1 VLM-Based Evaluation Video vs. Image-based Navigation. Table 28 establishes Veo-3 as the state-of-the-art model for this benchmark, consistently outperforming peers in navigation success and trajectory adherence. The Temporal Advantage: In the baseline setting (floor01 ), Veo-3 achieves Success Score (2D) of 65.71%, significantly surpassing the image-based Nano-banana (41.67%) and the video-based Sora-2 (22.81%). This advantage stems from Veo-3s superior Spatial Alignment Score (91.43%), indicating that 58 video generation models with strong temporal attention can better maintain trajectory coherence than frame-by-frame image generators. The Semantic Trade-off: Interestingly, while Nano-banana lags in navigation, it remains highly competitive in static recognition. It achieves an Object Semantic Score of 88.89% in simple environmentssurpassing Veo-3 (77.14%) and Sora-2 (82.46%). This suggests that current image models excel at what is in the scene (grounding), while video models excel at how to move through it (dynamics). Resilience to Complexity and Noise. Environmental Complexity: Performance degrades universally as environments become more intricate. Transitioning from floor01 to floor02plus, Veo-3s Overall Success drops sharply from 37.14% to 13.89%. However, its Oracle Success Score remains robust (83.33%), implying that the model often identifies the correct path but fails to execute it without violating physical constraints (e.g., collisions), hypothesis supported by the low Physics Validness (33.33%). View Fidelity: The results indicate positive correlation between visual quality and navigation success for video models. Veo-3s Overall Success improves from 20.83% in lower fidelity settings (quality03 ) to 29.17% in higher fidelity (quality04 ), suggesting that clearer visual cues are critical for maintaining the driver capability in video generation. Instruction Following: The Modality Gap. distinct performance gap exists between visual and textual grounding (see Destination Specification). Visual Markers: When targets are specified via simple color mark, Veo-3 achieves robust Overall Success of 38.89%. Textual Descriptions: When targets require semantic parsing (location description), success rates collapse. Veo-3 falls to 11.43%, and Nano-banana drops to 8.33%. This confirms that mapping abstract linguistic descriptions to spatial layouts remains significant hurdle compared to direct visual matching. 11.3.2 Human vs. Automatic Evaluation Discrepancy To validate the automatic benchmarking protocols, we conducted side-by-side comparison with human evaluators for the top-performing model, Veo-3  (Table 29)  . The data reveals critical divergences in how algorithms versus humans perceive success. The Physics Hallucination Problem. Automatic metrics consistently overestimate physical realism. In floor01, the Auto Metric reports Physics Validness of 54.29%, while humans rate it at only 22.41%. This discrepancy leads to massive inflation of the Overall Success metric in automatic evaluations (37.14%) compared to the human ground truth (10.34%). This indicates that while models satisfy geometric path constraints (high Spatial Alignment), they frequently hallucinate physically impossible traversals (e.g., clipping through walls) that simple 2D projection metrics fail to penalize. The Consistency Paradox. Conversely, automatic metrics appear overly punitive regarding visual consistency. Agent Consistency: Humans rated Veo-3s agent consistency in floor01 at near-perfect 96.55%, whereas the automatic scorer only awarded 62.86%. Scene Consistency: Similarly, humans perceived the environment as stable (79.31%), significantly higher than the algorithmic assessment (45.71%). This suggests that current computer vision metrics for temporal consistency (likely based on pixel-wise or feature-wise similarity) are sensitive to minor generative artifacts that human observers naturally filter out as temporally coherent motion. Grounding Reliability. The evaluation gap widens with task abstractness. For simple color mark destinations, the Auto and Human Success Scores (2D) are relatively close (52.78% vs. 48.28%). However, for location descriptions, the auto metric claims 37.14% success rate while humans find only 18.33% of trajectories successful. This implies that automatic evaluators are prone to false positives when validating complex semantic 59 Table 28 Quantitative results for the 2D Top-down Navigation benchmark. We compare Sora-2, Veo-3, Nano-banana, and GPT-4o-image across various dimensions. Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Model Success Score (2D) Oracle Success Score (2D) Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Physics Validness Overall Success Environmental Complexity Level: floor01 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: floor02plus Video Models Veo-3 SoraImage Models Nano-banana GPT-4o-image View Fidelity Level: quality03 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: quality04 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: quality05 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Trajectory Distance Level: noturn Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: oneturn Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image 65.71% 22.81% 41.67% 10.34% 25.00% 28.81% 38.89% 16.67% 45.83% 34.21% 41.67% 21.05% 37.50% 23.08% 37.50% 12.50% 52.17% 20.51% 41.67% 7.50% 36.11% 29.31% 41.67% 20.34% 54.29% 22.41% 38.89% 6.78% Destination Specification Level: color mark Video Models Veo-3 Sora-2 52.78% 17.24% Image Models Nano-banana GPT-4o-image 58.33% 10.34% Level: location description Video Models Veo-3 Sora-2 37.14% 34.48% Image Models Nano-banana GPT-4o-image 22.22% 16.67% 65.71% 7.02% 33.33% 1.72% 33.33% 15.25% 36.11% 5.00% 50.00% 18.42% 41.67% 5.26% 41.67% 7.69% 33.33% 5.00% 56.52% 7.69% 29.17% 0.00% 38.89% 15.52% 36.11% 5.08% 60.00% 6.90% 33.33% 1.69% 58.33% 12.07% 55.56% 1.72% 40.00% 10.34% 13.89% 5.00% 45.71% 8.77% 38.89% 1.72% 45.71% 23.73% 66.67% 10.00% 43.48% 18.42% 58.33% 5.26% 45.83% 17.95% 37.50% 10.00% 47.83% 12.82% 62.50% 2.50% 54.29% 18.97% 52.78% 8.47% 37.14% 13.79% 52.78% 3.39% 63.89% 24.14% 55.56% 3.45% 26.47% 8.62% 50.00% 8.33% 54.29% 52.63% 50.00% 6.90% 33.33% 49.15% 50.00% 10.00% 29.17% 57.89% 37.50% 10.53% 54.17% 51.28% 50.00% 12.50% 47.83% 43.59% 62.50% 2.50% 38.89% 48.28% 44.44% 10.17% 48.57% 53.45% 55.56% 6.78% 61.11% 44.83% 41.67% 3.45% 25.71% 56.90% 58.33% 13.33% 37.14% 1.75% 5.56% 1.72% 13.89% 5.08% 16.67% 5.00% 20.83% 7.89% 12.50% 5.26% 29.17% 2.56% 4.17% 5.00% 26.09% 0.00% 16.67% 0.00% 25.00% 1.72% 11.11% 5.08% 25.71% 5.17% 11.11% 1.69% 38.89% 1.72% 13.89% 1.72% 11.43% 5.17% 8.33% 5.00% 85.71% 47.37% 52.78% 15.52% 83.33% 55.93% 52.78% 23.33% 79.17% 52.63% 58.33% 23.68% 83.33% 46.15% 50.00% 17.50% 91.30% 56.41% 50.00% 17.50% 83.33% 60.34% 52.78% 23.73% 85.71% 43.10% 52.78% 15.25% 91.67% 53.45% 77.78% 13.79% 77.14% 50.00% 27.78% 25.00% 77.14% 82.46% 88.89% 55.17% 61.11% 89.83% 86.11% 51.67% 66.67% 92.11% 83.33% 65.79% 70.83% 87.18% 87.50% 52.50% 69.57% 79.49% 91.67% 42.50% 61.11% 89.66% 88.89% 57.63% 77.14% 82.76% 86.11% 49.15% 80.56% 77.59% 91.67% 39.66% 57.14% 94.83% 83.33% 66.67% 62.86% 68.42% 50.00% 6.90% 44.44% 54.24% 55.56% 11.67% 33.33% 63.16% 45.83% 13.16% 58.33% 66.67% 50.00% 12.50% 69.57% 53.85% 62.50% 2.50% 52.78% 62.07% 50.00% 10.17% 54.29% 60.34% 55.56% 8.47% 69.44% 56.90% 44.44% 3.45% 37.14% 65.52% 61.11% 15.00% 91.43% 78.95% 77.78% 56.90% 80.56% 86.44% 86.11% 41.67% 75.00% 81.58% 75.00% 57.89% 95.83% 89.74% 83.33% 50.00% 86.96% 76.92% 87.50% 40.00% 83.33% 84.48% 83.33% 57.63% 88.57% 81.03% 80.56% 40.68% 91.67% 77.59% 86.11% 34.48% 80.00% 87.93% 77.78% 63.33% 60 Table 29 Quantitative results for the Top-down View Real-World Navigation benchmark. We compare automatic evaluations against human judgments for Veo-3 across the same hard-level dimensions. Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Evaluation Success Score (2D) Oracle Success Score (2D) Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Success (2D) Original Destination Physics Validness Overall Success Environmental Complexity Level: floor01 Auto Evaluation Human Evaluation 65.71% 41.38% Level: floor02plus Auto Evaluation Human Evaluation 25.00% 25.00% View Fidelity Level: quality03 Auto Evaluation Human Evaluation 45.83% 34.21% Level: quality04 Auto Evaluation Human Evaluation 37.50% 35.00% Level: quality05 Auto Evaluation Human Evaluation 52.17% 30.00% Trajectory Distance Level: short Auto Evaluation Human Evaluation 36.11% 28.81% Level: long Auto Evaluation Human Evaluation 54.29% 37.29% Destination Specification Level: color mark Auto Evaluation Human Evaluation 52.78% 48.28% Level: location description Auto Evaluation Human Evaluation 37.14% 18.33% 85.71% 72.41% 83.33% 58.33% 79.17% 57.89% 83.33% 70.00% 91.30% 67.50% 83.33% 61.02% 85.71% 69.49% 91.67% 81.03% 77.14% 50.00% 77.14% 51.72% 61.11% 25.00% 66.67% 47.37% 70.83% 20.00% 69.57% 47.50% 61.11% 45.76% 77.14% 30.51% 80.56% 34.48% 57.14% 41.67% 62.86% 96.55% 44.44% 45.00% 33.33% 63.16% 58.33% 72.50% 69.57% 75.00% 52.78% 69.49% 54.29% 71.19% 69.44% 74.14% 37.14% 66.67% 91.43% 55.17% 80.56% 61.67% 75.00% 63.16% 95.83% 57.50% 86.96% 55.00% 83.33% 50.85% 88.57% 66.10% 91.67% 65.52% 80.00% 51.67% 65.71% 72.41% 33.33% 58.33% 50.00% 65.79% 41.67% 65.00% 56.52% 65.00% 38.89% 66.10% 60.00% 64.41% 58.33% 87.93% 40.00% 43.33% 45.71% 79.31% 45.71% 83.33% 43.48% 84.21% 45.83% 77.50% 47.83% 82.50% 54.29% 83.05% 37.14% 79.66% 63.89% 94.83% 26.47% 68.33% 42.86% 34.48% 13.89% 16.67% 25.00% 31.58% 29.17% 22.50% 30.43% 22.50% 22.00% 20.34% 32.20% 30.51% 41.67% 39.66% 14.29% 11.67% 54.29% 22.41% 33.33% 11.67% 29.17% 18.42% 54.17% 10.00% 47.83% 22.50% 33.90% 13.56% 39.00% 20.34% 61.11% 16.67% 25.71% 17.22% 37.14% 10.34% 13.89% 1.67% 20.83% 5.26% 29.17% 2.50% 26.09% 10.00% 15.30% 8.47% 23.70% 3.39% 38.89% 6.90% 11.43% 5.00% grounding, likely crediting trajectories that end near the target by chance rather than by understanding the textual clue."
        },
        {
            "title": "11.4 Case Study: Qualitative Failure Modes",
            "content": "This analysis, presented in Figure 20, examines the performance of three video generation modelsVeo-3, Sora-2, and Wan-2.2on top-down embodied navigation task. Each model is instructed to generate video depicting an agent navigating from blue triangular start position to red target within static environment, enabling direct comparison of their spatial reasoning, trajectory consistency, and goal-directed behavior. Veo-3: Strong Spatial Grounding with Control and Termination Failures. Veo-3 exhibits the strongest initial spatial grounding among the three models, correctly identifying the agents starting position at the blue triangle and demonstrating partial semantic awareness of obstacles. In particular, the agent recognizes the sofa as navigable obstruction and initially routes around it. However, the generation deteriorates due to failures in movement physics and action termination. The agent displays critical orientation error, moving laterally to the left while facing forward, which results in Spatial Alignment Score of 0 at Frame 5s. Although the agent briefly enters the red-marked target region (Oracle Success Score = 1), it fails to terminate upon arrival. Instead, the trajectory continues beyond the goal, deviates unpredictably, and ultimately collides with the sofa. This post-arrival drift causes both the Success Score and Object Semantic Score to drop to 0, revealing failure to couple goal completion with action cessation. Sora-2: Severe Scene Hallucination and Physical Constraint Violations. Sora-2 fails to preserve both the spatial ground truth and the physical constraints of the input environment. From the outset, the model reconstructs an entirely new scene in which the agent no longer originates from the blue start location. Temporal consistency further collapses as the generation introduces severe hallucinations, including the spontaneous appearance of second agent at Frame 4s. The agent also violates fundamental physical boundaries by passing through solid obstacles (the black block), yielding an Object Semantic Score of 0. While an agent eventually reaches red61 Figure 20 Qualitative Comparison on Top-down View Real-World Navigation. We analyze three models (Veo-3, Sora-2, and Wan-2.2) navigating toward red target. marked region, the complete loss of scene integrity and environmental correspondence renders the navigation invalid with respect to the original prompt, highlighting failure in maintaining world-state continuity. Wan-2.2: Surface-Level Scene Consistency with Semantic Role Confusion. Wan-2.2 preserves the visual style and low-level layout of the scene more effectively than Sora-2, achieving an initial Scene Consistency Score of 1. However, this apparent stability masks deeper semantic failure. The model misidentifies the controllable agent, erroneously animating dining table as the acting entity, which immediately induces spatial misalignment. The resulting motion is disjointed and drifting, lacking coherent locomotion or intentional control. Compounding this issue, the model fails to maintain destination integrity: the red target region shifts from its original floor location to corner of the scene, causing the Destination Integrity Score to drop to 0. As result, the agent never reaches the correct target, demonstrating that visual fidelity alone is insufficient for semantically grounded navigation. 12 3D Real-World Navigation (3D R.-W.Nav.)"
        },
        {
            "title": "12.1 Task Definition\nThis task evaluates an agent’s foundational capabilities in 3D Spatial Reasoning and Visual-Semantic Grounding\nwithin real-world scanned environments (Chang et al., 2017; Ramakrishnan et al., 2021b; Savva et al., 2019;\nAnderson et al., 2018b; Zhu et al., 2017). It probes the agent’s ability to interpret egocentric visual streams and\nparse complex 3D environmental geometry using datasets such as Matterport3D, HM3D, and Habitat (Chang\net al., 2017; Ramakrishnan et al., 2021b,a; Chaplot et al., 2020a). Furthermore, the task challenges the\nmodel’s sequential decision-making by requiring the generation of valid action trajectories (e.g., move forward,\nturn left) and tests its semantic reasoning by demanding the identification of goals based on varied abstract\nor textual descriptions.",
            "content": "Figure 21 3D Real-World Navigation task showing navigation through realistic 3D environment. The model must navigate from starting position to goal location using visual cues from the third-person perspective. successful case completed by Veo-3 on the 3D Real-World Navigation task."
        },
        {
            "title": "12.2 Evaluation and Metrics",
            "content": "We adopt the shared embodied evaluation protocol in Section 9.3: all metrics are binary and labeled via the automatic VLM/human rules on rollout traces and generated frames. Physical Understanding and Instruction Following checks (Object Semantic, Agent Consistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged; here we outline how they are applied in 3D Real-World Navigation and how the gates are formed. 63 12.2.1 Task Completeness Metrics (Geometry Only) We report Success Score 3D and Oracle Success Score 3D as in Section 9.3, testing whether the agent stops inside or ever enters the destination volume. These ignore visual fidelity and physical plausibility, which the gate metrics enforce. Success Score 3D (S.S. 3D). This checks whether the agent stops at the correct location in the 3D navigation sequence. The evaluation is geometric only and does not depend on whether the destination appearance is preserved. It is satisfied if the agent stops inside the intended destination volume. Oracle Success Score 3D (O.S. 3D). This credits the agent if it ever comes within proximity threshold of the correct 3D destination, regardless of its final stop. It is satisfied if the agent reaches the destination vicinity at any time during the video. 12.2.2 Gate Metrics and Holistic Performance In 3D R.-W.Nav., the gates surface common failure modes: agents may teleport between floors, cut through occluded rooms, or alter the dollhouse layout to satisfy geometric hits. Requiring destination integrity and scene consistency prevents shortcutting via hallucinated staircases or duplicated goals, while the physics gate catches identity drift and implausible motion in the third-person view. As with the Panoramic and Top-down tasks, strict conjunctions block generative shortcuts (e.g., hallucinating nearer goals or warping 3D structure), but the 3D setting stresses multi-floor occlusions and third-person viewpoint drift: Success (3D) Original Destination. This composite metric addresses the tendency of video generation models to hallucinate solution shortcuts. It is satisfied only if the Success Score 3D, Destination Integrity Score, and Scene Consistency Score are all met simultaneously. This metric ensures that successful arrival is valid and not the result of the model altering the scene layout or destination location to simplify the task. Physics Validness. To provide holistic view of physical understanding, this composite metric serves as strict gate. It is satisfied only if the Object Semantic Score, Agent Consistency Score, and Spatial Alignment Score are all met simultaneously. This ensures the agent does not clip through objects, teleport, or drift unnaturally. Overall Success = Success Score 3D Oracle Success Score 3D Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; sample passes only when all seven binary checks are 1."
        },
        {
            "title": "12.3 Evaluation Results",
            "content": "Key Finding: The Simulation Gap in Video Generation Current video generation models (Veo-3, Sora-2) exhibit fundamental disconnect between visual plausibility and physical causality. While they often achieve high raw success rates in simple navigation tasks (8589%), they fail catastrophically on holistic metrics (03%) due to severe scene hallucination and physics violations. In contrast, image-based agents (Nano-banana) demonstrate robust simulation capabilities, maintaining near-perfect scene consistency (>95%) and physical validity. 12.3.1 VLM-Based Evaluation Table 30 benchmarks Veo-3 against Sora-2, Nano-banana, and GPT-4o-image. The results highlight distinct hierarchy in embodied navigation capabilities, separated by model modality and architectural stability. Image Models vs. Video Models (Nano-banana Dominance). Nano-banana emerges as the distinct leader, showcasing that image generation is currently far more reliable for embodied tasks than video generation. Table 30 Quantitative results for the 3D Real-world Navigation benchmark. We compare Sora-2, Veo-3, Nano-banana, and GPT-4o-image across the same hard-level dimensions. Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Evaluation Success Score (3D) Oracle Success Score (3D) Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Success (3D) Original Destination Physics Validness Overall Success Environmental Complexity Level: floor01 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: floor02plus Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image View Fidelity Level: quality03 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: quality04 Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: quality05 Video Models Veo-3 SoraImage Models Nano-banana GPT-4o-image Trajectory Distance Level: short Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image Level: long Video Models Veo-3 Sora-2 Image Models Nano-banana GPT-4o-image 85.00% 88.89% 75.00% 13.33% 68.33% 72.22% 83.33% 15.00% 65.00% 75.00% 75.00% 7.50% 82.50% 91.67% 87.50% 17.50% 82.50% 75.00% 75.00% 17.50% 73.33% 72.22% 77.78% 13.33% 80.00% 88.89% 80.56% 15.00% Destination Specification Level: color mark Video Models Veo-3 Sora-2 93.33% 88.89% Image Models Nano-banana GPT-4o-image 80.56% 16.67% Level: location description Video Models Veo-3 Sora-2 60.00% 72.22% Image Models Nano-banana GPT-4o-image 77.78% 11.67% 91.67% 97.22% 75.00% 13.33% 78.33% 77.78% 86.11% 16.67% 77.50% 83.33% 79.17% 7.50% 85.00% 95.83% 87.50% 20.00% 92.50% 83.33% 75.00% 17.50% 85.00% 83.33% 80.56% 13.33% 85.00% 91.67% 80.56% 16.67% 96.67% 91.67% 80.56% 18.33% 73.33% 83.33% 80.56% 11.67% 81.67% 72.22% 100.00% 88.33% 75.00% 69.44% 94.44% 75.00% 72.50% 79.17% 95.83% 80.00% 86.67% 83.33% 97.22% 48.33% 68.33% 75.00% 97.22% 56.67% 75.00% 83.33% 95.83% 45.00% 63.33% 97.22% 97.22% 58.33% 73.33% 83.33% 94.44% 51.67% 70.00% 91.67% 91.67% 52.50% 75.00% 54.17% 85.00% 75.00% 65.00% 83.33% 100.00% 80.00% 100.00% 57.50% 100.00% 52.50% 87.50% 79.17% 95.83% 85.00% 81.67% 80.56% 97.22% 85.00% 75.00% 61.11% 97.22% 78.33% 72.50% 79.17% 95.83% 55.00% 78.33% 77.78% 97.22% 56.67% 76.67% 80.56% 97.22% 48.33% 70.00% 95.83% 95.83% 60.00% 68.33% 86.11% 94.44% 58.33% 68.33% 94.44% 97.22% 51.67% 66.67% 58.33% 73.33% 75.00% 70.00% 88.89% 100.00% 71.67% 100.00% 46.67% 100.00% 45.00% 90.00% 83.33% 94.44% 91.67% 81.67% 83.33% 94.44% 58.33% 66.67% 91.67% 91.67% 65.00% 53.33% 77.78% 75.00% 13.33% 38.33% 58.33% 86.11% 15.00% 40.00% 66.67% 79.17% 5.00% 50.00% 70.83% 87.50% 20.00% 47.50% 66.67% 75.00% 17.50% 46.67% 63.89% 80.56% 11.67% 45.00% 72.22% 80.56% 16.67% 31.67% 69.44% 80.56% 18.33% 60.00% 66.67% 80.56% 10.00% 15.00% 44.44% 86.11% 21.67% 18.33% 36.11% 100.00% 15.00% 15.00% 41.67% 91.67% 7.50% 20.00% 41.67% 91.67% 27.50% 15.00% 37.50% 95.83% 20.00% 16.67% 41.67% 88.89% 16.67% 16.67% 38.89% 97.22% 20.00% 28.33% 36.11% 91.67% 20.00% 5.00% 44.44% 94.44% 16.67% 11.67% 0.00% 75.00% 13.33% 5.00% 0.00% 83.33% 13.33% 5.00% 0.00% 75.00% 5.00% 10.00% 0.00% 87.50% 17.50% 10.00% 0.00% 75.00% 17.50% 8.33% 0.00% 77.78% 11.67% 8.33% 0.00% 80.56% 15.00% 15.00% 0.00% 80.56% 16.67% 1.67% 0.00% 77.78% 10.00% 40.00% 55.00% 97.22% 35.00% 33.33% 60.00% 94.44% 40.00% 30.00% 67.50% 91.67% 30.00% 42.50% 45.00% 100.00% 40.00% 37.50% 60.00% 95.83% 42.50% 38.33% 55.00% 94.44% 45.00% 35.00% 60.00% 97.22% 30.00% 28.33% 51.67% 100.00% 30.00% 45.00% 63.33% 91.67% 45.00% 3.33% 0.00% 75.00% 13.33% 0.00% 0.00% 83.33% 13.33% 0.00% 0.00% 75.00% 5.00% 2.50% 0.00% 87.50% 17.50% 2.50% 0.00% 75.00% 17.50% 3.33% 0.00% 77.78% 11.67% 0.00% 0.00% 80.56% 15.00% 3.33% 0.00% 80.56% 16.67% 0.00% 0.00% 77.78% 10.00% Consistency as Foundation: Nano-banana maintains near-perfect scores in Scene Consistency (86100%) and Physics Validness (94100%) across all complexity levels. This stability allows it to achieve high Overall Success rates (7587.5%) that video models cannot approach. Resilience to Complexity: As environment complexity increases (from floor01 to floor02plus), Nanobanana actually improves its overall success from 75% to 83.33%. In sharp contrast, video models collapse; Veo-3s holistic success drops to 0% in complex multi-floor environments, unable to reconcile geometric consistency with longer navigation horizons. Video Model Trade-offs (Veo-3 vs. Sora-2). While both video models struggle with holistic success (mostly 0%3%), they exhibit different failure modes: Sora-2 (Better Geometry, Worse Adherence): Sora-2 generally outperforms Veo-3 in Spatial Alignment (e.g., 97.22% vs 63.33% in floor01 ) and Scene Consistency (44.44% vs 15.00%). However, it suffers from critical flaw in Gate Metric: Success (Original Destination), scoring 0.00% across almost all categories. This indicates that while Sora-2 generates smooth, consistent video, it hallucinates the destination or drifts significantly from the prompts specific target. Veo-3 (Better Adherence, Worse Physics): Veo-3 is more compliant with the task, scoring higher on reaching the original destination (11.67% in floor01 ). However, it achieves this by sacrificing physical laws, as evidenced by its abysmal Scene Consistency scores (dropping to 5.00% in location tasks) and heavy penalization in human evaluations for physics violations. Baseline Comparison. GPT-4o-image acts as semantic baseline. While it demonstrates strong object recognition (Object Semantic Score 8090%), it lacks the spatial reasoning to navigate, resulting in low success scores ( 1317%). This confirms that embodied navigation requires more than just seeing the scene; it requires consistent internal world model that GPT-4o-image lacks. Table 31 Quantitative results for the 3D Real-World Navigation benchmark. We compare automatic evaluations against human judgments for Veo-3 across the same hard-level dimensions. Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Evaluation Success Score (3D) Oracle Success Score (3D) Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Success (3D) Original Destination Physics Validness Overall Success Environmental Complexity Level: floor01 Auto Evaluation Human Evaluation 88.89% 85.00% Level: floor02plus Auto Evaluation Human Evaluation 72.22% 68.33% View Fidelity Level: quality03 Auto Evaluation Human Evaluation 75.00% 65.00% Level: quality04 Auto Evaluation Human Evaluation 91.67% 82.50% Level: quality05 Auto Evaluation Human Evaluation 75.00% 82.50% Trajectory Distance Level: short Auto Evaluation Human Evaluation 72.22% 73.33% Level: long Auto Evaluation Human Evaluation 88.89% 80.00% Destination Specification Level: color mark Auto Evaluation Human Evaluation 88.89% 93.33% Level: location description Auto Evaluation Human Evaluation 72.22% 60.00% 97.22% 91.67% 77.78% 78.33% 83.33% 77.50% 95.83% 85.00% 83.33% 92.50% 83.33% 85.00% 91.67% 85.00% 91.67% 96.67% 83.33% 73.33% 72.22% 81.67% 69.44% 75.00% 79.17% 72.50% 54.17% 75.00% 79.17% 87.50% 80.56% 81.67% 61.11% 75.00% 58.33% 66.67% 83.33% 90.00% 83.33% 86.67% 75.00% 68.33% 83.33% 75.00% 75.00% 85.00% 79.17% 72.50% 77.78% 78.33% 80.56% 76.67% 75.00% 73.33% 83.33% 81.67% 77.78% 53.33% 58.33% 38.33% 66.67% 40.00% 70.83% 50.00% 66.67% 47.50% 63.89% 46.67% 72.22% 45.00% 69.44% 31.67% 66.67% 60.00% 44.44% 15.00% 36.11% 18.33% 41.67% 15.00% 41.67% 20.00% 37.50% 15.00% 41.67% 16.67% 38.89% 16.67% 36.11% 28.33% 44.44% 5.00% 38.89% 11.67% 33.33% 5.00% 37.50% 5.00% 41.67% 10.00% 29.17% 10.00% 36.11% 8.33% 36.11% 8.33% 36.11% 15.00% 36.11% 1.67% 61.11% 40.00% 50.00% 33.33% 62.50% 30.00% 37.50% 42.50% 66.67% 37.50% 63.89% 38.33% 47.22% 35.00% 44.44% 28.33% 66.67% 45.00% 25.00% 3.33% 16.67% 0.00% 29.17% 0.00% 12.50% 2.50% 20.83% 2.50% 19.44% 3.33% 22.22% 0.00% 16.67% 3.33% 25.00% 0.00% 97.22% 63.33% 83.33% 73.33% 91.67% 70.00% 83.33% 65.00% 95.83% 70.00% 86.11% 68.33% 94.44% 68.33% 88.89% 70.00% 91.67% 66.67% 66 12.3.2 Human vs. Automated Evaluation Discrepancy Table 31 exposes systemic bias in current automated benchmarking. While automated metrics suggest that Veo-3 possesses moderate embodied competency, human evaluation reveals competence illusion, where high task completion rates mask fundamental failures in physical realism. The Plausibility Gap in Auto-Eval. Automated metrics exhibit severe inflation regarding agent reliability. Metric Collapse: In the floor01 environment, the Auto Evaluation reports respectable Holistic Metric (Overall Success) of 25.00%. Human evaluators, however, penalize the model rigorously, causing this metric to collapse to just 3.33%. Superficial Success: Crucially, the raw Task Completeness (Success) scores are nearly identical between machines (88.89%) and humans (85.00%). This discrepancy proves that auto-evaluators correctly identify destination arrival but completely fail to penalize the impossible trajectories used to get there. Blindness to Physics and Scene Integrity. The divergence stems from the auto-evaluators inability to detect violations of physical laws, creating physics-blind assessment loop. Scene Consistency: Human evaluators rate Scene Consistency drastically lower than the auto-evaluator (e.g., 15.00% vs. 44.44% in floor01 ). This indicates that the model frequently warps the environmentshifting walls, deleting obstacles, or altering lightingto facilitate navigation, behavior the auto-evaluator ignores. The Validity Gate: While the auto-evaluator estimates Physics Validness at 5060%, humans rate it between 3040%. Because Overall Success is gated metric (requiring both arrival and valid physics), this 20-point drop in validity effectively zeroes out the holistic success rate in human trials. Overall Metric Analysis: The Consistency Bottleneck. The holistic data reveals clear Fragile Success paradox. While the model achieves high peak performance in isolationscoring 80.56% in Success Score (3D)it fails to integrate these capabilities into coherent simulation. The 69% Drop: Only 20.83% of samples satisfy the strict holistic criterion, representing massive 69.45% degradation relative to the models best individual metric (Spatial Alignment, 90.28%). Root Cause: The dominant limiting factor is Scene Consistency, which remains dangerously low at 40.28%. This confirms that the model is better at moving the agent than it is at maintaining the world. Implication: Successful trajectories are frequently invalidated by environmental drift (e.g., vanishing objects, morphing geometry). This underscores that for high-quality motion generation is futile without the temporal stability required to keep the environment static."
        },
        {
            "title": "12.4 Case Study: Qualitative Failure Modes",
            "content": "This case study in Figure 22 evaluates the ability of three video generation modelsVeo-3, Sora-2, and Wan-2.2to simulate humanoid robot navigating multi-story indoor space to specific red target zone. The prompt requires strict adherence to physical constraints, step-by-step stair climbing, and consistent dollhouse isometric view. Veo-3 Analysis: Task Success with Physical Compromises. Veo-3 was the only model to achieve successful arrival at the destination (Oracle Success Score = 1). However, this success came with significant hallucinations and physics violations: Scene Consistency: The model failed to maintain the structural integrity of the input scene, notably hallucinating straight staircase in place of the original curved one. Physical Fidelity: While the agent initially demonstrated understanding of vertical movement, it ultimately failed the physics constraint by jumping directly from the second floor to the ground floor, resulting in the body clipping into the floor. Despite these errors, the agent maintained consistency without teleporting earlier in the sequence. Figure 22 Case Study for 3D Real-World Navigation comparing Veo-3, Sora-2, and Wan-2.2. The visualization highlights model-specific failure modes: Veo-3 achieves the destination despite physics violations (floor jumping) and scene alterations (staircase geometry); Sora-2 suffers from severe scene hallucination and style drift; and Wan-2.2 exhibits temporal inconsistency with drifting agents and moving furniture. 68 Sora-2 Analysis: Severe Hallucination and Physics Failures. Sora-2 struggled significantly with both scene adherence and physical laws: Style and Destination Drift: The model completely disregarded the input image, creating new scene with different video style. Crucially, it altered the task parameters by moving the red destination marker from the floor to the wall. Physics Violations: The agent displayed zero understanding of solid geometry (Object Semantic Score = 0), standing on air platforms near the stairs and walking directly through the railing. Consequently, the agent never arrived at the valid destination. Wan-2.2 Analysis: Temporal Instability and Agent Drifting. Wan-2.2 initially maintained the scene details well but quickly devolved into temporal incoherence: Environmental Instability: The static environment proved unstable; by Frame 1s, furniture (a table) shifted position from right to left. Agent Control: The navigation was disjointed. The agent drifted, stood on top of tables, and walked backwards down the stairs. This resulted in failure to reach the destination. While Veo-3 was the only model to complete the navigation task, all three models struggled with strict physical plausibility in 3D space. Veo-3 prioritized path completion over geometry; Sora-2 hallucinated an entirely new reality; and Wan-2.2 failed to keep the static environment stationary."
        },
        {
            "title": "13.1 Task Definition\nThe Simultaneous Localization and Generation (SLAG) task extends the paradigm of Simultaneous Localization\nand Mapping (SLAM) Durrant-Whyte & Bailey (2006) by coupling 3D spatial navigation with real-time\ngenerative mapping. Unlike traditional methods that estimate position within a static or progressively built\nmap, SLAG actively synthesizes a synchronized 2D top-down trajectory that corresponds to the robot’s\nmovement through a 3D environment. As a humanoid robot navigates complex indoor scenes—such as\nphotorealistic apartments rendered in a “dollhouse” perspective—the system dynamically generates a 2D\nrepresentation of its progress from start to goal. The core objective of SLAG is to achieve precise spatiotemporal\nalignment between physical 3D navigation and generative 2D plotting, demonstrating the synergy between\nreal-time perception and generative modeling in spatial understanding.",
            "content": "Figure 23 Simultaneous Localization and Goal-reaching (SLAG) task requiring the model to simultaneously maintain spatial awareness of its position while navigating toward goal in an unknown environment."
        },
        {
            "title": "13.2 Evaluation and Metrics\nSLAG differs from the previous tasks by requiring simultaneous 3D navigation and 2D generative mapping,\nmaking trajectory alignment the primary bottleneck. Similarly, we adopt the shared embodied evaluation\nprotocol in Section 9.3. Physical Understanding and Instruction Following checks (Object Semantic, Agent\nConsistency, Spatial Alignment, Destination Integrity, Scene Consistency) are unchanged. Here we focus on\nthe SLAG-specific task completeness set and gates.",
            "content": "13.2.1 Task Completeness Metrics (Geometry Only) Five task completeness metrics are evaluated jointly: Success Score 2D, Oracle Success Score 2D, Success Score 3D, Oracle Success Score 3D (all from Section 9.3), and Trajectory Alignment to ensure the projected 2D path matches the 3D motion. The 2D map is provided, but the destination is not; alignment hinges on correctly projecting the 3D navigation onto the given map, with 2D success conditioned on the same 3D destination. Success Score (S.S. 2D). Measures whether the agents final position lies within the highlighted or textually specified goal region in the 2D overhead map. The score is 1 if the final coordinates fall entirely inside the goal footprint; otherwise 0. Oracle Success Score (O.S. 2D). Provides partial credit when the agent comes sufficiently close to the 2D goal during navigation. The score is 1 if the agents path ever intersects or touches the goal region, even if it does not stop there; otherwise 0. Success Score (S.S. 3D). Checks whether the agent ends inside the correct destination volume in the 3D navigation sequence. This metric is purely geometric and independent of any visual discrepancies at the destination. The score is 1 if the final 3D position is within the target volume; otherwise 0. Oracle Success Score (O.S. 3D). Grants credit when the agent enters the vicinity of the correct 3D destination at any point during its rollout. The score is 1 if the trajectory ever crosses the predefined proximity threshold around the target; otherwise 0. Trajectory Alignment Score. Evaluates whether the agents 2D projected route is consistent with its 3D motion path, focusing on major turns and spatial transitions. score of 1 indicates strong correspondence between the two trajectories; otherwise 0. 13.2.2 Gate Metrics and Holistic Performance As in the prior tasks, gates block generative shortcuts, but only 3D destination gate is possible because the 2D map has no fixed ground-truth target (it is generated, not given): In SLAG, the holistic gate exposes compounded failures: models often align one panel but not the other, hallucinate extra corridors in the 2D drawing, or teleport in 3D while the 2D trace stays smooth. Because the 2D goal is synthesized, the 3D destination gate plus trajectory alignment are key to preventing such mismatches. Success (3D) with Original Destination = Success Score 3D Destination Integrity Scene Consistency. Physics Validness = Object Semantic Agent Consistency Spatial Alignment. Overall Success = Success Score 2D Oracle Success Score 2D Success Score 3D Oracle Success Score 3D Trajectory Alignment Object Semantic Agent Consistency Spatial Alignment Destination Integrity Scene Consistency; sample passes only when all ten binary checks are 1."
        },
        {
            "title": "13.3 Evaluation Results",
            "content": "The Simultaneous Localization and Generation (SLAG) task represents significant leap in embodied AI, requiring models to maintain strict temporal and spatial alignment between 3D physical navigation and 2D generative mapping. The following analysis dissects the performance of state-of-the-art models, revealing critical trade-off between the visual fluidity of video models and the logical adherence of image-based models. 70 Key Findings: Reality & Modality Gap Alignment bottleneck. Trajectory Alignment remains the weakest link for video baselines (44.07% 55.17% vs. Nano-bananas 88.89%/76.67%), and Destination Integrity stays low for them (20.34% 44.83%). Even with strong Scene Consistency (45.76%89.66%), cross-view synchronization caps holistic success. Semantic cliff. Switching from color marks to textual location descriptions collapses Nano-bananas 3D Success (81.25% 17.65%) and Holistic score (50.00% 8.82%); GPT-4o-image drops similarly (31.03% 1.67%). Video models show the same gap (Veo-3: 17.24% 5.17%, Sora-2: 19.64% 6.67%), confirming spatial language grounding is the dominant failure mode. Cross-view priors activate under pressure. Forced 3D-to-2D alignment sustains Scene Consistency (Veo3: 45.76% 57.89%, Nano-banana: 100.00% 96.67%, GPT-4o-image: 98.33% 94.83%) even when trajectories misalign, suggesting latent spatial priors surface only with structured multimodal conditioning. 13.3.1 VLM-Based Evaluation Table 32 highlights the performance disparities across Veo-3, Sora-2, Nano-banana, and GPT-4o-image. While video generation models often excel at visual fidelity, the requirement for synchronized spatial logic proves challenging. The Logic vs. Motion Trade-off. Quantitative evaluation again ranks Nano-banana as the strongest SLAG performer: it posts the top Holistic Metric on floor01 (27.78%), edging out GPT-4o-image (25.00%), while Veo-3 and Sora-2 remain far lower (11.86% and 10.34%). Trajectory Alignment Dominance: Nano-banana achieves dominant Trajectory Alignment Score of 88.89% on floor01. Video baselines trail sharply (Veo-3: 44.07%, Sora-2: 55.17%), underscoring that smooth motion does not guarantee coordinate-level alignment with the map. Gate-Induced Failures for Video Models: Despite respectable physical validity (Physics Validness: Veo-3 at 45.76%, Sora-2 at 65.52%), the Success (3D) Original Destination gate collapses to 18.64% and 22.41%, respectively, versus Nano-bananas 38.89%. Low destination integrity (20.34%32.76% for video models) is the primary culprit behind their depressed holistic scores. The Grounding Gap: Visual vs. Linguistic Complexity. Performance across instruction types shows that linguistic grounding is steeper barrier than visual grounding. Drastic Drop on Text Prompts: When the destination is specified by Color Mark, Nano-banana excels (81.25% 3D Success, 50.00% Holistic). Switching to Location Description collapses 3D Success to 17.65% and Holistic to 8.82%. Model Consistency: GPT-4o-image mirrors this trend, sliding from 44.83% to 3.33% (3D Success) and from 31.03% to 1.67% (Holistic). Video models show the same gap (Veo-3: 17.24% 5.17% Holistic; Sora-2: 19.64% 6.67%), confirming that textual spatial instructions remain the weakest link. Environmental Scaling. As complexity increases from floor01 to floor02plus: Nano-banana loses geometric reliability (3D Success 55.56% 40.00%) yet nudges its Holistic Metric upward (27.78% 30.00%) thanks to stable Scene Consistency (100.00% 96.67%) and stronger 2D alignment. Sora-2 remains limited (Holistic 10.34% 15.52%) despite high physical priors (Physics Validness 65.52% 56.90%), indicating that added floors do not resolve its instruction-following issues. GPT-4o-image degrades sharply (Holistic 25.00% 6.90%), revealing brittleness of single-image rollouts once multi-floor reasoning is required. 71 Table 32 Quantitative results for the Simultaneous Localization and Generation benchmark. We compare automatic evaluations across different models: Sora 2, Veo 3, Nano-banana, and GPT-4o-image. Evaluation Success Score (3D) Success Score (2D) Oracle Success Score (3D) Oracle Success Score (2D) Trajectory Alignment Score Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Success (3D) Original Destination Physics Validness Overall Success Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Environmental Complexity Level: floor01 Video Models Veo 3 Sora 2 Image Models Nano-banana GPT-4o-image Level: floor02plus Video Models Veo 3 Sora 2 Image Models Nano-banana GPT-4o-image View Fidelity Level: quality03 Video Models Veo 3 Sora 2 Image Models Nano-banana GPT-4o-image Level: quality04 Video Models Veo 3 Sora 2 Image Models Nano-banana GPT-4o-image Level: quality05 Video Models Veo 3 Sora Image Models Nano-banana GPT-4o-image Trajectory Distance Level: short Video Models Veo 3 Sora 2 52.54% 44.07% 29.31% 27.59% 55.56% 41.67% 35.00% 28.33% 21.05% 38.60% 29.31% 32.76% 40.00% 56.67% 12.07% 17.24% 35.00% 40.00% 27.50% 27.50% 50.00% 62.50% 30.00% 25.00% 46.15% 51.28% 26.32% 34.21% 45.83% 45.83% 25.00% 30.00% 29.73% 32.43% 34.21% 28.95% 50.00% 33.33% 15.79% 13.16% 38.98% 40.68% 40.35% 31.58% Image Models Nano-banana GPT-4o-image 54.55% 51.52% 27.12% 20.34% Level: long Video Models Veo 3 Sora 2 35.09% 42.11% 18.64% 28.81% Image Models Nano-banana GPT-4o-image 42.42% 45.45% 20.34% 25.42% Destination Specification Level: color mark Video Models Veo 3 Sora 2 53.45% 51.72% 44.64% 46.43% Image Models Nano-banana GPT-4o-image 81.25% 59.38% 44.83% 36.21% Level: location description Video Models Veo 3 Sora 2 20.69% 31.03% 15.00% 15.00% Image Models Nano-banana GPT-4o-image 17.65% 38.24% 10.00% 3.33% 67.80% 72.41% 77.78% 61.67% 52.63% 68.97% 70.00% 37.93% 52.50% 75.00% 91.67% 52.50% 69.23% 71.05% 58.33% 50.00% 59.46% 65.79% 72.22% 47.37% 64.41% 75.44% 90.91% 50.85% 56.14% 66.10% 57.58% 49.15% 65.52% 67.86% 78.12% 51.72% 55.17% 73.33% 70.59% 48.33% 20.34% 32.76% 50.00% 26.67% 31.58% 44.83% 50.00% 17.24% 22.50% 37.50% 58.33% 20.00% 33.33% 34.21% 50.00% 32.50% 21.62% 44.74% 38.89% 13.16% 28.81% 47.37% 57.58% 28.81% 22.81% 30.51% 42.42% 15.25% 32.76% 62.50% 71.88% 37.93% 18.97% 16.67% 29.41% 6.67% 45.76% 79.31% 100.00% 98.33% 57.89% 89.66% 96.67% 94.83% 45.00% 77.50% 100.00% 97.50% 48.72% 84.21% 95.83% 97.50% 62.16% 92.11% 100.00% 94.74% 62.71% 89.47% 100.00% 96.61% 40.35% 79.66% 96.97% 96.61% 67.24% 92.86% 100.00% 96.55% 36.21% 76.67% 97.06% 96.67% 18.64% 22.41% 38.89% 25.00% 15.79% 20.69% 36.67% 6.90% 15.00% 22.50% 41.67% 17.50% 25.64% 21.05% 37.50% 22.50% 10.81% 21.05% 33.33% 7.89% 20.34% 29.82% 42.42% 18.64% 14.04% 13.56% 33.33% 13.56% 25.86% 33.93% 62.50% 31.03% 8.62% 10.00% 14.71% 1.67% 45.76% 65.52% 69.44% 58.33% 31.58% 56.90% 60.00% 34.48% 37.50% 62.50% 83.33% 52.50% 35.90% 60.53% 54.17% 47.50% 43.24% 60.53% 55.56% 39.47% 42.37% 66.67% 78.79% 45.76% 35.09% 55.93% 51.52% 47.46% 34.48% 58.93% 71.88% 46.55% 43.10% 63.33% 58.82% 46.67% 11.86% 10.34% 27.78% 25.00% 10.53% 15.52% 30.00% 6.90% 10.00% 17.50% 41.67% 17.50% 15.38% 10.53% 20.83% 22.50% 8.11% 10.53% 22.22% 7.89% 15.25% 15.79% 36.36% 18.64% 7.02% 10.17% 21.21% 13.56% 17.24% 19.64% 50.00% 31.03% 5.17% 6.67% 8.82% 1.67% 59.32% 34.48% 55.56% 36.67% 26.32% 31.03% 40.00% 13.79% 35.00% 27.50% 50.00% 32.50% 51.28% 26.32% 45.83% 25.00% 43.24% 44.74% 50.00% 18.42% 45.76% 43.86% 54.55% 28.81% 40.35% 22.03% 42.42% 22.03% 60.34% 51.79% 81.25% 44.83% 25.86% 15.00% 17.65% 6.67% 49.15% 37.93% 47.22% 31.67% 42.11% 37.93% 63.33% 20.69% 40.00% 32.50% 62.50% 32.50% 58.97% 42.11% 54.17% 30.00% 37.84% 39.47% 44.44% 15.79% 45.76% 40.35% 54.55% 25.42% 45.61% 35.59% 54.55% 27.12% 56.90% 57.14% 68.75% 41.38% 34.48% 20.00% 41.18% 11.67% 44.07% 55.17% 88.89% 66.67% 33.33% 48.28% 76.67% 43.10% 35.00% 47.50% 91.67% 65.00% 43.59% 55.26% 70.83% 57.50% 37.84% 52.63% 88.89% 42.11% 44.07% 52.63% 87.88% 55.93% 33.33% 50.85% 78.79% 54.24% 43.10% 48.21% 87.50% 55.17% 34.48% 55.00% 79.41% 55.00% 72.88% 84.48% 83.33% 81.67% 50.88% 82.76% 86.67% 68.97% 60.00% 85.00% 91.67% 75.00% 66.67% 84.21% 87.50% 72.50% 59.46% 81.58% 72.22% 78.95% 61.02% 85.96% 90.91% 77.97% 63.16% 81.36% 78.79% 72.88% 56.90% 82.14% 84.38% 70.69% 67.24% 85.00% 85.29% 80.00% 52.54% 84.48% 83.33% 76.67% 42.11% 67.24% 86.67% 48.28% 45.00% 77.50% 95.83% 65.00% 46.15% 71.05% 79.17% 62.50% 51.35% 78.95% 77.78% 60.53% 54.24% 82.46% 90.91% 62.71% 40.35% 69.49% 78.79% 62.71% 46.55% 73.21% 87.50% 62.07% 48.28% 78.33% 82.35% 63.33% 72 Table 33 Quantitative results for the Simultaneous Localization and Generation (SLAG) benchmark. We compare automatic evaluations against human judgments for Veo-3 across the same hard-level dimensions. Evaluation Success Score (3D) Success Score (2D) Oracle Success Score (3D) Oracle Success Score (2D) Trajectory Alignment Score Object Semantic Score Agent Consistency Score Spatial Alignment Score Destination Integrity Score Scene Consistency Score Physics Validness Overall Success Task Completeness Physical Understanding Instruction Following Gate Metric Holistic Metric Environmental Complexity Level: floor01 Auto Evaluation Human Evaluation 52.54% 44.07% 50.00% 31.48% Level: floor02plus Auto Evaluation Human Evaluation 21.05% 38.60% 7.14% 28.57% View Fidelity Level: quality Auto Evaluation Human Evaluation 35.00% 40.00% 50.00% 21.88% Level: quality04 Auto Evaluation Human Evaluation 46.15% 51.28% 36.67% 40.00% Level: quality Auto Evaluation Human Evaluation 29.73% 32.43% 0.00% 40.00% Trajectory Distance Level: short Auto Evaluation Human Evaluation 38.98% 40.68% 47.62% 21.43% Level: long Auto Evaluation Human Evaluation 35.09% 42.11% 37.50% 25.00% Destination Specification Level: color mark Auto Evaluation Human Evaluation 53.45% 51.72% 55.56% 31.48% Level: location description Auto Evaluation Human Evaluation 20.69% 31.03% 7.14% 17.86% 59.32% 64.81% 26.32% 32.14% 35.00% 59.38% 51.28% 53.33% 43.24% 45.00% 45.76% 61.90% 40.35% 45.00% 60.34% 70.37% 25.86% 21.43% 49.15% 37.04% 42.11% 10.71% 40.00% 21.88% 58.97% 50.00% 37.84% 5.00% 45.76% 30.95% 45.61% 25.00% 56.90% 33.33% 34.48% 17.86% 44.07% 9.26% 33.33% 0.00% 35.00% 6.25% 43.59% 10.00% 37.84% 0.00% 44.07% 7.14% 33.33% 5.00% 43.10% 5.56% 34.48% 7.14% 72.88% 38.89% 50.88% 25.00% 60.00% 40.62% 66.67% 33.33% 59.46% 25.00% 61.02% 35.71% 63.16% 32.50% 56.90% 35.19% 67.24% 32.14% 52.54% 68.52% 42.11% 60.71% 45.00% 68.75% 46.15% 53.33% 51.35% 80.00% 54.24% 76.19% 40.35% 55.00% 46.55% 66.67% 48.28% 64.29% 67.80% 61.11% 52.63% 75.00% 52.50% 43.75% 69.23% 76.67% 59.46% 85.00% 64.41% 66.67% 56.14% 65.00% 65.52% 61.11% 55.17% 75.00% 20.34% 44.44% 31.58% 75.00% 22.50% 40.62% 33.33% 63.33% 21.62% 65.00% 28.81% 54.76% 22.81% 55.00% 32.76% 33.33% 18.97% 96.43% 45.76% 98.15% 57.89% 85.71% 45.00% 87.50% 48.72% 100.00% 62.16% 95.00% 62.71% 92.86% 40.35% 95.00% 67.24% 96.30% 36.21% 89.29% 45.76% 5.56% 31.58% 2.22% 37.50% 3.33% 35.90% 5.83% 43.24% 2.50% 42.37% 6.11% 35.09% 1.67% 34.48% 5.00% 43.10% 2.78% 11.86% 0.00% 10.53% 0.00% 10.00% 0.00% 15.38% 0.00% 8.11% 0.00% 15.25% 0.00% 7.02% 0.00% 17.24% 0.00% 5.17% 0.00% 13.3.2 Human vs. Automated Evaluation Discrepancy Table 33 presents critical divergence between automatic metrics and human judgment regarding the Veo-3 model. This comparison exposes the Uncanny Valley of physics simulation in current video models. The Hallucination of Quality: Scene Consistency. Humans consider the videos visually stable (98.15% on floor01, 100% on quality04 ), yet auto-metrics flag instability (Scene Consistency 45.76% on floor01 ). Veo-3 produces temporally smooth, coherent frames to the eye, but algorithms detect subtle pixel/geometry shifts that humans overlook. Physics Validness: Humans Are Harsher. On floor01, the auto-evaluator assigns Physics Validness of 45.76%, while humans give just 5.56%. Simple collision or alignment checks let runs pass automatically, but humans catch soft violations (teleportation, clipping, floating), driving the human Holistic Metric to 0.00%. Trajectory Alignment Reality Check. Alignment between the 2D map and 3D view is sharply contested: Auto scores 44.07% on floor01, whereas humans rate only 9.26%. The map may look structurally plausible to an algorithm yet fails to reflect the exact turns and timing seen in the 3D video. Overall Metric Analysis. SLAG shows severe wood barrel effect: overall success collapses to 3.64% (2/55 samples) because weak linksDestination Integrity (25.45%), Trajectory Alignment (29.09%), Physics Validnessgate everything else. Image models (Nano-banana) occasionally assemble logically consistent frames, but video models (Veo-3, Sora-2) remain dreamers, generating visually compelling yet spatially and physically unreliable navigations."
        },
        {
            "title": "13.4 Case Study: Qualitative Failure Modes",
            "content": "The Simultaneous Localization and Generation task highlights common failure mode across all three models in Figure 24: the inability to maintain semantic linkage between split-screen representations over time. Veo-3: High Fidelity, Low Logic. Veo-3 offers the most stable visual experience, maintaining high Scene Consistency as confirmed by human evaluation (98.15%). In the case study, the agent correctly identifies the 73 Figure 24 Case Study for Simultaneous Localization and Generation. The figure compares Veo-3, Sora-2, and Wan-2.2 on split-screen navigation task requiring synchronization between 3D view and 2D map. Veo-3 achieves 3D target success but suffers from 2D trajectory misalignment. Sora-2 exhibits severe scene hallucination and physics violations (clipping through walls). Wan-2.2 demonstrates agent inconsistency (teleporting) and fails to reach the final target. 74 start position and successfully reaches the red-marked goal in the 3D view. However, the Trajectory Alignment fails. As the 3D agent moves forward, the 2D dot on the map drifts independently, failing to mirror the agents path. This exemplifies the disembodied nature of the generationthe model understands it needs to generate map and video, but treats them as separate artistic tasks rather than coupled data streams. Sora-2: Severe Hallucination. Sora-2 struggles to maintain the input reality. (1) Scene Hallucination: It immediately fails Scene Consistency (Score = 0) by replacing the input floor plan with hallucinated layout. (2) Physics Violations: Consistent with the low Physics Validness scores, the agent is observed clipping through cabinet.(3) Phantom Turns: The 2D agent executes turn that the 3D agent never makes, further emphasizing the lack of cross-modal attention in the model architecture. Wan-2.2: The Teleportation Problem. Wan-2.2 exhibits the most erratic behavior, leading to complete task failure. (1) Agent Instability: The model suffers from severe Agent Consistency failures (Score = 0). At Frame 1s, the agent teleportseffectively flyingonto an indigo wall, treating vertical surface as walkable floor. (2) Navigation Failure: Unlike Veo-3, Wan-2.2 fails to reach the destination in either view. The agent stops short, resulting in zero scores for both Success (2D) and Success (3D). This model demonstrates that without strong physical priors, video generation models devolve into surrealism rather than embodied simulation."
        },
        {
            "title": "14 Physical Commonsense",
            "content": "We introduce the Physical Commonsense task to assess models foundational understanding of intuitive physics (Battaglia et al., 2013; Yi et al., 2020; Wu et al., 2015), critical prerequisite for robust world modeling. This task probes Physical Reasoning by requiring models to generate videos that combine photorealism with physical plausibility (Bear et al., 2021; Riochet et al., 2021; Piloto et al., 2022). Beyond static visual fidelity, the task evaluates whether the model captures essential principlessuch as gravity, momentum, collisions, and material properties (rigidity, fluid dynamics)and correctly models causal dynamics (Bakhtin et al., 2019; Allen et al., 2020). The evaluation spans both 3D Spatial Reasoning (spatial object interactions) and Temporal Reasoning (sequential cause-and-effect), organized along two complementary axes: (1) Physical Concepts, which tests fundamental principles using the VideoPhy ontology (Bansal et al., 2024); and (2) Sports Scenarios, which probe compositional reasoning through dynamic, high-velocity human movements."
        },
        {
            "title": "14.1 Data Sources and Task Structure",
            "content": "Physical Concepts (Fundamental Interactions). To systematically evaluate atomic physical principles, we adopt the structured ontology from VideoPhy (Bansal et al., 2024) and VideoPhy-2 (Bansal et al., 2025). We draw from diverse pool of captioned interactions, including: SolidSolid (e.g., rigid collisions, stacking), SolidFluid (e.g., splashing, buoyancy), and FluidFluid (e.g., diffusion, mixing). These prompts isolate specific physical laws, allowing us to test statics, dynamics, and kinematics in controlled environments. Sports Scenarios (Compositional Reasoning). To evaluate physical reasoning in complex, real-world contexts, we synthesize complementary dataset of sports-oriented prompts. These scenarios naturally require the integration of multiple physical laws simultaneously. The dataset spans: Precision & Arts (e.g., ballet pirouettes requiring angular momentum), Winter Sports (e.g., skiing moguls involving friction and gravity), Aquatics (e.g., diving and swimming involving fluid resistance), and Athletics. These prompts test the models ability to maintain physical consistency during complex human-object interactions."
        },
        {
            "title": "14.2 Hard-Level Control and Evaluation Taxonomy",
            "content": "To ensure rigorous assessment, we curate balanced evaluation set of 100 samples (see Table 34), stratified along three dimensions of difficulty: Interaction Type (The What): We categorize samples by the material properties involved: Solid-Solid: Interactions between rigid bodies (testing impulse, friction, and collision response). Solid-Fluid: Interactions between solids and liquids (testing displacement, splashing, and floating). 75 Fluid-Fluid: Dynamics of miscible and immiscible fluids (testing viscosity, mixing, and turbulence). Scenario Context (The Where): We distinguish between controlled physics experiments (Physical Concepts) and unconstrained, dynamic environments (Sports Scenarios), aiming to test generalization from atomic laws to complex behaviors. Interaction Complexity (The How): Across all categories, we vary the complexity level: Simple: Single-object motion or static equilibrium. Complex: Multi-object interactions with simultaneous forces. Chain-Reaction: Causal sequences where an initial action triggers cascading effects. Table 34 Distribution of Physical Commonsense evaluation samples. The set is balanced to weigh fundamental physical understanding equally against compositional real-world application. Task Axis Total Samples Physical Concepts (Atomic Interactions) Sports Scenarios (Compositional Contexts) Total 25"
        },
        {
            "title": "14.3 Evaluation and Metrics",
            "content": "Modeling physical commonsense inherently requires capturing temporal dynamics such as force propagation, momentum transfer, and continuous motion. Because static image generators lack temporal modeling capabilities and cannot represent causal interactions unfolding over time, they are fundamentally limited in assessing physical plausibility. Therefore, our evaluation focuses exclusively on video generative models, capable of producing temporally coherent sequences (Cai et al., 2025). To ensure consistent, structured judgments, we employ Vision-Language Model (VLM) evaluatorGemini2.5-Pro (Comanici et al., 2025)prompted to act as physics and video expert. For each generated video, the VLM assesses four specific dimensions of quality. To standardize the evaluation, each dimension is treated as binary metric (0/1), where score of 1 indicates the criteria are fully satisfied and 0 indicates failure or significant violation. We define the four fine-grained metrics as follows: Physics Accuracy (0/1): Evaluates whether the generated video strictly obeys fundamental physical laws. The model checks if motion adheres to gravity, momentum, and friction, and verifies that object interactions are plausible. score of 0 is assigned if there are violations such as objects floating, moving at unrealistic speeds for the context, displaying incorrect trajectories, or deviating from the scenarios Physics Focus. Motion Quality (0/1): Assesses the temporal coherence and naturalness of the movement. This metric verifies that the motion follows the expected pattern described in the scenario and remains smooth and continuous. score of 0 is assigned if the motion is jerky, exhibits unnatural accelerations, or suffers from temporal discontinuities and inconsistency. Visual Realism (0/1): Measures the visual fidelity and believability of the scene. The model checks if objects and materials appear realistic, whether lighting and shadows are consistent, and if the scene composition is plausible. score of 0 is assigned if there are significant visual artifacts, glitches, or if the scene lacks photorealism. Prompt Adherence (0/1): Determines whether the video semantically matches the user input. This metric verifies that all key elements (objects, setting) are present and that the specific action described actually occurs. score of 0 is assigned if there are significant mismatches between the generated content and the text prompt. 76 Overall Success (Aggregated Metric): To provide holistic measure of generation capability, we compute strict Overall Success score. generated video is marked as successful (1) if and only if it satisfies all four fine-grained metrics simultaneously. This rigorous aggregation ensures that high performance requires model to generate videos that are not only physically accurate but also visually coherent, smooth, and semantically correct. Figure 25 Case Study: Success case generated by Veo-3. Physically Plausible Parachute Inflation."
        },
        {
            "title": "14.4 Case Study",
            "content": "To better illustrate the strengths and limitations revealed by our Physical Commonsense evaluation, we present three representative case studies generated by Veo-3 (Figure 25, 26, and 27). Each example highlights how our metrics disentangle distinct dimensions of physical reasoning: physics accuracy, prompt adherence, motion quality, and visual realism. These cases further demonstrate that high visual fidelity alone is not indicative of correct physical behaviorunderscoring why physics-focused evaluation is necessary for robust real-world video generation. Success Case: Physically Plausible Parachute Inflation. Figure 25 shows prompt involving the inflation of parasail by two-person crew. Veo-3 performs strongly across all evaluation dimensions, achieving full scores on physics accuracy, prompt adherence, motion quality, and visual realism. The model captures the physical mechanics of inflation: air fills the canopy, internal pressure increases, the fabric transitions from wrinkled to taut, and the lines tighten appropriately as load is applied. The motion unfolds smoothly and continuously, without discontinuities or unnatural accelerations. Detailed textures, realistic lighting, and consistent shading further contribute to the clips visual plausibility. This example illustrates Veo-3s ability to correctly represent gradual force buildup, material deformation, and multi-agent coordination. Failure Case I: Missing SolidSolid Interaction in Coffee Grinding. Figure 26 highlights failure case involving metal grinder crushing coffee beans (SolidSolid interaction). While the static frames resemble real grinder setup, the temporal dynamics violate fundamental physical laws. Instead of showing realistic grinding processwhere rigid beans fracture into progressively smaller particlesthe video abruptly transitions from whole beans to uniform fine grounds, without any mechanical cause. This discontinuous state-change effect 77 Figure 26 Case Study: Failure case generated by Veo-3. Missing SolidSolid Interaction in Coffee Grinding. resembles visual morph rather than physical transformation, resulting in violations of mass conservation and material fracture mechanics. Although the model adheres partially to the prompt in terms of objects present, it fails to depict the required interaction, leading to zeros in physics accuracy, prompt adherence, motion quality, and visual realism. Failure Case II: Incorrect Angular Momentum Dynamics in Ballet Rotation. Figure 27 presents ballet scenario requiring fouetté turn, physically demanding movement involving rapid leg whipping to generate angular momentum and sustain rotation. Veo-3 fails across all physics-focused dimensions. The dancers extended leg and upper body become unnaturally distorted mid-rotation, deviating from realistic human biomechanics. The model also misinterprets the action: instead of producing sharp, continuous whipping motion that drives true fouetté, the dancer performs slow, controlled rotation with no momentum-generating movement. This breaks both prompt adherence and key physical principles such as angular momentum conservation and joint kinematic constraints. Although the visual appearance remains high-quality at glance, closer inspection reveals anatomy inconsistencies and motion artifacts that undermine physical realism. Takeaways. These case studies highlight the need to evaluate video models beyond photorealism. Veo-3 can produce visually convincing clips, but frequently fails in scenarios requiring nuanced physical reasoningespecially where material properties, continuous force propagation, or human biomechanics play central role. The structured metrics in our Physical Commonsense task make these failure modes explicit, providing robust diagnostic tool for guiding future model improvements. 78 Figure 27 Case Study: Failure case generated by Veo-3. Incorrect Angular Momentum Dynamics in Ballet Rotation."
        },
        {
            "title": "14.5 Evaluation Results",
            "content": "Key Finding: Physical Plausibility Is Decoupled from Visual Realism Sora-2 achieves the strongest physical commonsense performance with 70.00% overall success rate, substantially outperforming Veo-3 (51.02%) and Wan-2.2 (24.00%). Crucially, the results reveal consistent pattern: high visual realism does not imply correct physical reasoning. Wan-2.2 produces highly photorealistic videos (8496% Visual Realism) yet fails to follow prompts and physical constraints (24% Overall), indicating fundamental gap between appearance-level fidelity and physically grounded world modeling. Fine-grained analysis reveals Sports Scenarios are systematically easier than Physical Concepts across all models, while Solid-Solid interactions prove most challengingVeo-3 achieves 0% success on rigid body collisions compared to 75% on Solid-Fluid interactions. 79 Table 35 Quantitative results for the Physical Commonsense task. We evaluate three video generative models (Veo-3, Sora-2, and Wan-2.2) on Physical Concepts and Sports Scenarios. Model Physics Accuracy Motion Quality Visual Realism Prompt Adherence Overall Fine-grained Metrics Primary Metric Scenario Type: Physical Concepts Veo-3 Sora-2 Wan-2.2 Veo-3 Sora-2 Wan-2.2 62.50% 84.00% 58.67% 80.00% 88.00% 42.67% Scenario Type: Sports Scenarios Average Veo-3 Sora-2 Wan-2. 71.43% 86.00% 50.67% 14.5.1 VLM-Based Evaluation 54.17% 80.00% 53.33% 68.00% 72.00% 33.33% 61.22% 76.00% 43.33% 83.33% 96.00% 72.00% 92.00% 88.00% 96.00% 87.76% 92.00% 84.00% 50.00% 76.00% 38.67% 68.00% 68.00% 21.33% 59.18% 72.00% 30.00% 41.67% 76.00% 26.67% 60.00% 64.00% 21.33% 51.02% 70.00% 24.00% Table 35 reports quantitative results on the Physical Commonsense task, evaluated using Gemini-2.5-Pro (Comanici et al., 2025) as VLM-based evaluator. We compare three state-of-the-art video generation modelsVeo3, Sora-2, and Wan-2.2across two complementary scenario types: Physical Concepts and Sports Scenarios. Overall Performance Trends. Sora-2 consistently outperforms competing models, achieving the highest overall success rate (70.00%) and leading across all fine-grained metrics. In contrast, Veo-3 exhibits moderate but uneven performance (51.02%), while Wan-2.2 substantially underperforms (24.00%) despite strong visual realism. Notably, all models score highly on Visual Realism (8492%), yet exhibit large variance in Physics Accuracy (50.6786.00%) and Prompt Adherence (30.0072.00%), reinforcing that perceptual quality alone is an unreliable indicator of physical correctness. Scenario-Specific Difficulty. Across all models, Sports Scenarios are consistently easier than Physical Concepts. Veo-3 improves from 41.67% overall success on Physical Concepts to 60.00% on Sports Scenarios, while Sora-2 maintains strong performance across both (76.00% vs. 64.00%). In contrast, Wan-2.2 fails to generalize even in Sports Scenarios, achieving only 21.33% overall success. These results suggest that contemporary video models benefit from learned biomechanical motion patterns in human-centric activities, while struggling with fine-grained material interactions such as collisions, splashing, and mixing. Table 36 Fine-grained VLM-based evaluation results by Sports Scenarios attributes. Overall success rate (%) is reported across sport types (Ballet, Diving, Skiing, Swimming) and difficulty levels (Easy, Medium, Hard). Model Veo-3 Sora-2 Wan-2.2 Ballet Diving By Sport Type Skiing Swimming By Difficulty Easy Medium Hard 33.3% 50.0% 71.4% 33.3% 50.0% 85.7% 28.6% 0.0% 44.4% 83.3% 83.3% 11.1% 60.0% 60.0% 16.7% 62.5% 75.0% 33.3% 57.1% 57.1% 14.3% Table 37 Fine-grained VLM-based evaluation results by Physical Concepts attributes. Overall success rate (%) is reported across states-of-matter interaction types and difficulty levels. Model Veo-3 Sora-2 Wan-2. Solid-Solid 0.0% 100.0% 33.3% By States of Matter Solid-Fluid Fluid-Fluid Action/Other 75.0% 75.0% 25.0% 50.0% 100.0% 83.3% 40.0% 66.7% 17.8% By Difficulty Easy Hard 53.3% 22.2% 75.0% 77.8% 35.4% 11.1% 80 Sport-Specific Patterns. Fine-grained analysis  (Table 36)  reveals sport-specific challenges. Ballet proves most difficult for all models (3344% success), while Swimming achieves the highest scores (83% for Veo-3 and Sora-2). Wan-2.2 fails completely on Diving (0%) yet achieves its best performance on Ballet (44.4%), suggesting model-specific biases in motion priors. By difficulty level, Sora-2 shows the most consistent performance across Easy (60%), Medium (75%), and Hard (57%) prompts, while Wan-2.2 collapses uniformly across all difficulty levels (1433%). States-of-Matter Analysis. Table 37 reveals interaction-specific challenges. Solid-Solid interactions prove most difficult: Veo-3 achieves 0% success on rigid body collisions, while Sora-2 achieves perfect 100%. Fluid-Fluid interactions show more variance, with Wan-2.2 achieving 83.3%its highest category scorewhile Veo-3 scores only 50%. By difficulty level, all models show degradation on hard cases, with Wan-2.2 exhibiting the most severe collapse (35.4% Easy 11.1% Hard). The VisualPhysical Disconnect. Wan-2.2 demonstrates the clearest dissociation between visual quality and physical reasoning. Despite achieving the highest Visual Realism score on Sports Scenarios (96.00%), it records the lowest Physics Accuracy (42.67%) and Prompt Adherence (21.33%) in the same setting. This failure mode suggests that the model prioritizes surface-level appearance over causal and physical consistency, producing videos that look right but violate core physical principles. 14.5.2 Human Evaluation To validate VLM-based assessments, we conducted human evaluation on Veo-3 generated videos (n = 45). Human annotators assessed multiple dimensions including Physics Accuracy, Motion Quality, Visual Realism and Prompt Adherence, which we map to the same metrics used in AutoEval  (Table 35)  . Table 38 Veo-3 evaluation comparison: VLM-based evaluation (AutoEval) vs. human evaluation (HumanEval) across Physical Concepts and Sports Scenarios. Scenario Eval Mode Physical Concepts AutoEval Human Eval Sports Scenarios Average AutoEval Human Eval AutoEval Human Eval Physics Accuracy Motion Quality Visual Realism Prompt Adherence Overall 62.50% 77.27% 80.00% 91.30% 71.43% 84.44% 54.17% 86.36% 68.00% 78.26% 61.22% 82.22% 83.33% 83.64% 92.00% 84.35% 87.76% 84.00% 50.00% 72.73% 68.00% 65.22% 59.18% 68.89% 41.67% 77.27% 60.00% 82.61% 51.02% 80.00% AutoEval vs Human Eval Gap. Table 38 reveals striking discrepancy between automated and human evaluation. Human evaluation consistently rates Veo-3 higher across all metrics, with the overall success rate increasing from 51.02% (AutoEval) to 80.00% (Human Eval)a 29-point improvement. Motion Quality shows the largest gap: 82.22% (Human) vs. 61.22% (AutoEval), suggesting that temporal artifacts flagged by the VLM evaluator are often imperceptible or acceptable to human observers. Physics Accuracy similarly improves from 71.43% to 84.44%, indicating that VLM-based evaluation applies overly strict criteria that may not align with human perception of physical plausibility. Table 39 Veo-3 fine-grained comparison by Sports Scenarios attributes: AutoEval vs. HumanEval. Overall success rate (%) is reported across sport types (Ballet, Diving, Skiing, Swimming) and difficulty levels (Easy, Medium, Hard). Eval Mode AutoEval Human Eval Ballet Diving By Sport Type Skiing Swimming By Difficulty Easy Medium 33.3% 50.0% 71.4% 50.0% 100.0% 100.0% 83.3% 83.3% 60.0% 90.0% 62.5% 57.1% Hard 57.1% 100.0% Sport-Specific Insights. Fine-grained analysis  (Table 39)  reveals that human evaluators rate Veo-3s Diving and Skiing scenarios at 100% Visual Realism, compared to AutoEval scores of 50.0% and 71.4% respectively. Ballet remains consistently challenging across both evaluation modes (50.0% Human, 33.3% AutoEval), confirming 81 Table 40 Veo-3 fine-grained comparison by Physical Concepts attributes: AutoEval vs. HumanEval. Overall success rate (%) is reported across states-of-matter interaction types and difficulty levels. Eval Mode AutoEval Human Eval Solid-Solid 0.0% 66.7% By States of Matter Solid-Fluid Fluid-Fluid Action/Other 75.0% 100.0% 50.0% 100.0% 40.0% 73.3% By Difficulty Easy Hard 53.3% 22.2% 78.6% 77.8% that sustained rotational dynamics in pirouettes and fouettés pose genuine difficulties for current video generators. By difficulty level, human evaluators show an unexpected pattern: hard scenarios achieve 100% success while medium scenarios score only 57.1%, suggesting that brief, dramatic actions (ski jumps, cliff dives) are easier to generate plausibly than sustained complex motions. Physical Concept Insights. Table 40 shows that for Physical Concepts, Solid-Fluid and Fluid-Fluid interactions achieve 100% human-rated Visual Realism, while Solid-Solid interactions remain challenging at 66.7%. The AutoEval-Human gap is most pronounced for Solid-Solid: AutoEval rates Veo-3 at 0%, while humans rate it at 66.7%a complete reversal that highlights the strictness of VLM-based collision detection. Across difficulty levels, human evaluation maintains consistent performance (78.6% Easy, 77.8% Hard), whereas AutoEval shows severe degradation on hard cases (53.3% Easy, 22.2% Hard)."
        },
        {
            "title": "15 Conclusion",
            "content": "We introduce comprehensive benchmark framework MMGR to evaluate the reasoning capabilities of generative models across five complementary abilities. Our evaluation reveals critical gap between perceptual quality and reasoning capability, with stark performance hierarchies suggesting models primarily learn pattern matching rather than true symbolic reasoning. The gap between closed-source and open-source models indicates progress relies heavily on scale rather than architectural innovations. Furthermore, our analysis identifies unique temporal tax on reasoning in video generation, where the requirement to maintain frame-to-frame coherence actively competes with logical consistency. This is evidenced by video models consistently underperforming their image-based counterparts on complex logic tasks, treating mathematical derivation as visual texture to be morphed rather than semantic chain to be constructed. We also observe prevalent hallucination of competence, where models frequently generate correct final outcomes despite invalid intermediate reasoning steps, confirming that they are often memorizing answer patterns rather than executing genuine multi-step deduction. These limitations stem from three fundamental bottlenecks in current training recipes: severe scarcity of structured symbolic reasoning data compared to naturalistic physical data; architectural constraints that prioritize local visual plausibility over global, long-horizon consistency; and optimization objectives that reward perceptual fidelity rather than logical validity. To bridge the gap from image animation to true world simulation, future work must look beyond mere scaling to develop architectures that decouple reasoning states from visual rendering and integrate auxiliary objectives for causal consistency."
        },
        {
            "title": "References",
            "content": "Kelsey Allen, Kevin Smith, and Joshua Tenenbaum. Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning. Proceedings of the National Academy of Sciences, 117(47):2930229310, 2020. Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, and Jing Shao. Bevbert: Topo-metric map pre-training for language-guided navigation. arXiv preprint arXiv:2212.04385, 2022. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 36743683, 2018a. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018b. Renée Baillargeon. Object permanence in 3½and 4½-month-old infants. Developmental Psychology, 23(5):655664, 1987. Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: new benchmark for physical reasoning. 2019. Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1579115801, 2025. David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. 2018. Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects. In arXiv:2006.13171, 2020. Peter Battaglia, Jessica Hamrick, and Joshua Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, 2013. Daniel Bear, Elias Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li Fei-Fei Cao, Surya Ganguli, Daniel LK Yamins, et al. Physion: Evaluating physical prediction from vision in humans and machines. 2021. Irving Biederman. Recognition-by-components: theory of human image understanding. Psychological Review, 94(2): 115147, 1987. Jose-Luis Blanco, Juan-Antonio Fernández-Madrigal, and Javier Gonzalez. Toward unified bayesian approach to hybrid metrictopological slam. IEEE Transactions on Robotics, 24(2):259270, 2008. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. 2023. Zefan Cai, Haoyi Qiu, Haozhe Zhao, Ke Wan, Jiachen Li, Jiuxiang Gu, Wen Xiao, Nanyun Peng, and Junjie Hu. From preferences to prejudice: The role of alignment tuning in shaping social bias in video diffusion models. arXiv preprint arXiv:2510.17247, 2025. Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, and Dhruv Batra. Semantic mapnet: Building allocentric semantic maps and representations from egocentric views. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 964972, 2021. 83 Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV), 2017. Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. In ICLR, 2020a. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33: 42474258, 2020b. François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Google DeepMind. Veo: Googles most capable video generation model. https://deepmind.google/technologies/veo/, 2024. Technical report. Google DeepMind. Veo 3: Generative video with native audio and cinematic control. Technical report, Google DeepMind, May 2025a. URL https://deepmind.google/models/veo/. Google DeepMind. Gemini 3 pro image (nano banana pro): High-fidelity image generation with reasoning. Technical report, Google, 2025b. URL https://deepmind.google/models/gemini-image/pro/. Also covers Nano-banana model variants. Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, et al. Robothor: An open simulation-to-real embodied ai platform. In CVPR, 2020. Hugh Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: part I. IEEE Robotics & Automation Magazine, 13(2):99110, 2006. doi: 10.1109/MRA.2006.1638022. Russell Epstein, Alison Harris, Damian Stanley, and Nancy Kanwisher. The parahippocampal place area: Recognition, navigation, or encoding? Neuron, 23(1):115125, 1999. Jorge Fuentes-Pacheco, José Ruiz-Ascencio, and Juan Manuel Rendón-Mancha. Visual simultaneous localization and mapping: survey. Artificial intelligence review, 43(1):5581, 2015. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. James Gibson. The Ecological Approach to Visual Perception. Houghton Mifflin, 1979. Rohit Girdhar and Deva Ramanan. Cater: diagnostic dataset for compositional actions and temporal reasoning. In ICLR, 2020. Clara Gomez, Marius Fehr, Alex Millane, Alejandra Hernandez, Juan Nieto, Ramon Barber, and Roland Siegwart. Hybrid topological and 3d dense mapping through autonomous exploration for large indoor environments. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 96739679. IEEE, 2020. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \"something something\" video database for learning and evaluating visual common sense. In ICCV, 2017. Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 84 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Joao Henriques and Andrea Vedaldi. Mapnet: An allocentric spatial memory for mapping environments. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 84768484, 2018. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022b. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. Why vision language models struggle with visual arithmetic? towards enhanced chart and geometry understanding. arXiv preprint arXiv:2502.11492, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. 2024. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. Muhammad Zubair Irshad, Niluthpol Chowdhury Mithun, Zachary Seymour, Han-Pang Chiu, Supun Samarasekera, and Rakesh Kumar. Sasra: Semantically-aware spatio-temporal reasoning agent for vision-and-language navigation in continuous environments. arXiv preprint arXiv:2108.11945, 2021. Michael Igorevich Ivanitskiy, Rusheb Shah, Alex F. Spies, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. configurable library for generating and manipulating maze datasets, 2023. URL http://arxiv.org/abs/2309.10498. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. 2017. Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011. Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: world model for indoor navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1473814748, 2021. Kurt Konolige, Eitan Marder-Eppstein, and Bhaskara Marthi. Navigation in hybrid metric-topological maps. In 2011 IEEE International Conference on Robotics and Automation, pp. 30413047. IEEE, 2011. Stephen Kosslyn. Image and Mind. Harvard University Press, 1980. Kuaishou. Kling: Large-scale video generation model. https://kling.kuaishou.com/, 2024. Technical report. Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. Yann LeCun. path towards autonomous machine intelligence. Open Review, 2022. Mingxiao Li, Zehao Wang, Tinne Tuytelaars, and Marie-Francine Moens. Layout-aware dreamer for embodied visual referring expression grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13861395, 2023. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. 2024a. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. 2024b. Gary Marcus. The Algebraic Mind: Integrating Connectionism and Cognitive Science. MIT Press, 2001. Mathematical Association of America. American invitational mathematics examination (aime) 2024. https://maa. org/maa-invitational-competitions/, 2024. American Invitational Mathematics Examination problems from 2024. 85 Mathematical Association of America. American invitational mathematics examination (aime) 2025. https://maa. org/maa-invitational-competitions/, 2025. American Invitational Mathematics Examination problems from 2025. Albert Michotte. The Perception of Causality. Methuen, 1963. Original work published 1946. OpenAI. Gpt-4o system card. Technical report, OpenAI, 2024a. URL https://openai.com/index/gpt-4o-system-card/. OpenAI. Video generation models as world simulators. OpenAI Technical Report, 2024b. URL https://openai.com/ research/video-generation-models-as-world-simulators. OpenAI. Sora 2 system card: Advanced video generation with physics simulation. Technical report, OpenAI, September 2025. URL https://openai.com/index/sora-2-system-card/. AJ Piergiovanni, Vincent Casser, Michael Ryoo, and Anelia Angelova. Evolving space-time neural architectures for videos. In ICCV, 2020. Luis Piloto, Ari Weinstein, Peter Battaglia, and Matthew Botvinick. Intuitive physics grounded scene generation and understanding. In ICLR, 2022. Yiran Qin, Ao Sun, Yuze Hong, Benyou Wang, and Ruimao Zhang. Navigatediff: Visual predictors are zero-shot navigation assistants. arXiv preprint arXiv:2502.13894, 2025. Qwen. Qwen-image: 20b parameter mmdit-based visual generation model. arXiv preprint arXiv:2508.02324, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. 2021. Santhosh Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. 2021a. Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021b. URL https://arxiv.org/abs/2109.08238. Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Véronique Izard, and Emmanuel Dupoux. Intphys 2019: benchmark for visual intuitive physics understanding. 2021. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In ICCV, 2019. Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench: Evaluating creative reasoning with sudoku variants. arXiv preprint arXiv:2505.16135, 2025. Hardik Shah, Jiaxu Xing, Nico Messikommer, Boyang Sun, Marc Pollefeys, and Davide Scaramuzza. Foresightnav: Learning scene imagination for efficient exploration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52365245, 2025. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Elizabeth Spelke and Katherine Kinzler. Core knowledge. Developmental Science, 10(1):8996, 2007. Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey Levine. Nomad: Goal masked diffusion policies for navigation and exploration. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 6370. IEEE, 2024. Edward Tolman. Cognitive maps in rats and men. Psychological Review, 55(4):189208, 1948. Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, and Xipeng Qiu. Thinking with video: Video generation as promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.04570, 2025. 86 Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In CVPR, 2018. Tomer Ullman, Elizabeth Spelke, Peter Battaglia, and Joshua Tenenbaum. Mind games: Game engines as an architecture for intuitive physics. Trends in Cognitive Sciences, 21(8):586599, 2017. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. 2018a. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018b. Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. 2016. Wan. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Saim Wani, Shivansh Patel, Unnat Jain, Angel Chang, and Manolis Savva. Multion: Benchmarking semantic map memory using multi-object navigation. Advances in Neural Information Processing Systems, 33:97009712, 2020. Taylor Webb, Keith Holyoak, and Hongjing Lu. Emergent symbols through binding in external memory. 2023. Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Jiajun Wu, Ilker Yildirim, Joseph Lim, Bill Freeman, and Josh Tenenbaum. Physics 101: Learning physical object properties from unlabeled videos. In BMVC, 2015. Xin Wu, Zheng Qi, Zexiang Liu, and He Wang. 3d shape reconstruction from 2d images with disentangled attribute flow. In CVPR, 2022. Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer. Audiovisual slowfast networks for video recognition. 2020. Dawei Xu, Yifan Zhang, and Bo Han. Can llms solve arc-agi tasks? arXiv preprint arXiv:2411.00993, 2024. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. In ICLR, 2020. Jeffrey Zacks and Barbara Tversky. Event structure in perception and conception. Psychological Bulletin, 127(1): 321, 2001. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Abstract spatial-temporal reasoning via probabilistic abduction and execution. 2021. Yichen Zhong, Yunzhu Liu, Junyu Liang, Jessica Hodgins, and Antonio Torralba. Learning 3d part assembly from single image. ECCV, 2020. Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pp. 260278. Springer, 2025. Meng-Jiun Zhou, Zheng Shou, and Bernard Ghanem. Video timeline modeling for news story understanding. 2022. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. 2017."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Michigan State University",
        "Microsoft",
        "Salesforce AI Research",
        "University of Adelaide",
        "University of California, Los Angeles",
        "University of Illinois Urbana-Champaign",
        "University of Wisconsin-Madison"
    ]
}