{
    "paper_title": "Scene-Centric Unsupervised Panoptic Segmentation",
    "authors": [
        "Oliver Hahn",
        "Christoph Reich",
        "Nikita Araslanov",
        "Daniel Cremers",
        "Christian Rupprecht",
        "Stefan Roth"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ."
        },
        {
            "title": "Start",
            "content": "Scene-Centric Unsupervised Panoptic Segmentation Oliver Hahn* 1 Daniel Cremers 2,4,5 Christoph Reich* 1,2,4,5 Christian Rupprecht 3 Nikita Araslanov 2,4 Stefan Roth 1,5,6 6hessian.AI 1TU Darmstadt 2TU Munich 3University of Oxford 4MCML 5ELIZA *equal contribution Cityscapes KITTI BDD MUSES Waymo MOTS https://visinf.github.io/cups 5 2 0 2 2 ] . [ 1 5 5 9 1 0 . 4 0 5 2 : r +4.9 % PQ U +9.4 % PQ +9.4 % PQ +4.1 % PQ +4.1 % PQ +6.6 % PQ +17.1 % PQ +4.9 % PQ +4.1 % PQ +4.1 % PQ +6.6 % PQ +17.1 % PQ S 2 Pseudo Label Generation Unsupervised Panoptic Training + Stereo Frames Motion & Depth Instance Labeling Semantic Labeling Panoptic Pseudo Label Self Train Panoptic Network Input Image . Figure 1. Results and overview of our unsupervised panoptic segmentation approach CUPS. We visualize panoptic predictions (top) of the current state of the art, U2Seg [55], and the proposed CUPS on various scene-centric datasets. We utilize motion and depth cues from stereo frames (bottom left) to generate scene-centric pseudo labels. Given monocular image (bottom right) we learn panoptic network using our pseudo labels and self-training. CUPS significantly outperforms U2Seg, indicated by the gains in panoptic quality (PQ)."
        },
        {
            "title": "Abstract",
            "content": "Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for objectcentric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on In particular, we propose an apscene-centric imagery. proach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and panoptic self-training strategy yields novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4 % points in PQ. 1. Introduction Panoptic image segmentation [40] is comprehensive scene understanding task that unifies semantic and instance segmentation. Semantic segmentation classifies each pixel into categories from pre-defined semantic taxonomy, whereas instance segmentation aims to detect, segment, and classify each object instance [21, 46, 86]. Achieving semantic and an instance-level understanding of complex scenes are longstanding challenges with broad applications in robotics, autonomous driving, and medical image analysis [see 52, 86, for an overview]. Recent progress in panoptic scene understanding [17, 18, 40] has been primarily driven by supervised learning. However, obtaining the required pixellevel annotations for high-resolution imagery is time and resource-intensive [7, 21]. Although significant resources have been devoted to large-scale supervised models [41], there remains necessity to develop efficient approaches that overcome the need for annotated data. This is particularly relevant when training data is scarce or in everchanging environments [16, 74]. highly promising opportunity lies in approaching panoptic segmentation without any manual supervision. Unsupervised panoptic segmentation aims to automatically partition images into semantically meaningful regions and detect each object instance without annotations. This task is rather ambiguous due to the task-dependent and humandefined nature of semantic class boundaries and object notions. Despite these challenges, recent advances in selfTo appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 2025. 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. O k Figure 2. Comparing MaskCut [78] to our instance labeling on Cityscapes val. For scene-centric images, MaskCut attends to areas with high semantic correlation instead of instances, reflected in mask precision (at 50 % IoU threshold) of 6.5 % and 59.6 % for MaskCut and our instance labels, respectively. supervised learning (SSL) of representations [12, 34, 56] have led to remarkable successes in unsupervised scene understanding. Current unsupervised semantic segmentation methods, e.g., STEGO [30], leverage pre-trained DINO [12] features to learn lower-dimensional representation and clustering [30, 38, 60, 62]. Recent class-agnostic unsupervised instance segmentation methods [3, 63, 77, 78, 80], for example CutLER [78], use self-supervised representations to generate pseudo-instance masks for trainIn contrast, unsupervised panoptic ing detection models. segmentation remains less explored. The only approach to dateU2Seg [55]demonstrates the feasibility of this task by building upon CutLER, combining distilled MaskCut [78] with STEGO for panoptic pseudo labeling and training panoptic segmentation network. Being only the first step, U2Seg has several limitations. First, U2Seg relies on MaskCut, which assumes object-centric images. While highly effective in separating large foreground objects from background in object-centric data, MaskCut struggles with scene-centric images (cf . Fig. 2). Second, due to the inability to train on scene-centric target datasets, U2Seg is compelled to bypass the classification into thing and stuff classes. Consequently, large number of pseudoclasses are learned, letting this differentiation mainly arise from pseudo to ground-truth class matching during evaluation. Third, U2Seg uses low-resolution semantic predictions from STEGO for pseudo supervision, which hampers results on high-resolution, scene-centric data. We present CUPS: scene-Centric Unsupervised Panoptic Segmentation. Drawing inspiration from Gestalt principles [42, 81] of perceptual organizatione.g., siminvariance, and common fatewe complement ilarity, visual representations with depth and motion cues to extend unsupervised panoptic segmentation to scenecentric data. Gestalt psychology suggests that humans naturally group visual elements based on inherent perceptual cues. Similarly, we argue that aside from the visual cues used by previous unsupervised scene understanding 2 approaches [30, 55, 78], an additional signal capturing the spatial, three-dimensional properties of scenes is essential. Moreover, motion provides cue for detecting object instances and achieving distinction between thing and stuff categories, owing to the physical conception of objects being entities capable of moving or being moved. Leveraging scene-centric data for learning is promising, as it provides an abundance of rich information and is in real-world applications (e.g., predominantly present autonomous driving) [52]. We derive semantic signal by utilizing depth-enhanced inference grounded in distilled SSL visual representations. Instance-level signals are obtained through SSL 3D motion estimation, leveraging an ensemble-based motion segmentation approach. By integrating these cues, we generate high-precision panoptic pseudo labels, which enable the bootstrapping of panoptic segmentation network that is subsequently refined through self-training (cf . Fig. 1). While previous work has utilized motion and depth for unsupervised semantic or instance segmentation, integrating these cues within panoptic framework has remained unexplored. Specifically, we make the following contributions: (i) We derive high-quality panoptic pseudo labels of scenecentric images by leveraging SSL visual representations, (ii) We effectively train SSL depth, and SSL motion. panoptic segmentation network on our pseudo labels using self-enhanced copy-paste augmentations and self-training. (iii) We demonstrate state-of-the-art unsupervised panoptic segmentation results from the proposed CUPS across wide range of scene-centric datasets. Additionally, our approach leads to impressive results on the sub-tasks of unsupervised semantic and instance segmentation. Finally, CUPS reduces the gap to supervised panoptic segmentation, allowing for label-efficient learning on fraction of the training data. 2. Related Work Approaches for unsupervised segmentation tasks have been significantly influenced by the literature on self-supervised learning (SSL) and low-level vision tasks (e.g., optical flow estimation), which we review first. Self-supervised representation learning focuses on learning generic feature extractors from unlabeled data, aiming for expressive features that facilitate broad range of downstream tasks [25]. To that end, various self-supervised pretext tasks have been proposed [1, 25]. The development of Vision Transformers (ViTs) [23] shaped current pretext tasks while allowing for data-scalable training [12, 34]. Current approaches typically train ViTs on contrastive [5, 14, 15, 33], negative-free [6, 12, 13, 27, 56], clusteringbased [4, 10, 11], or masked modeling [28, 34, 54] pretext tasks. Recent state-of-the-art models (e.g., DINO [12]) offer semantically rich and dense features suitable for unsupervised scene understanding [30, 78]. 1a: Instance Pseudo Labeling tt t+1t+1 Images Left Images Left tt t+1t+ Images Right tt Image Left 1b: Semantic Pseudo Labeling (cid:95) (cid:95) Flow/Depth Network Semantic Network Flow Depth Semantic Predictions 3 3 3 3 3 E 2 2 2 2 2 F S Merging Depth-guided Semantic Inference 1c: Instance and Semantic Fusion Instance Pseudo Label Align Semantic Pseudo Label Panoptic Pseudo Label Figure 3. Stage 1: CUPS pseudo-label generation. Instance pseudo labeling applies ensembling-based SF2SE3 motion segmentation [65] to scene flow extracted from flow and depth estimates. Semantic pseudo labeling uses semantic network, distilling and clustering DINO features [12], combined with depth-guided inference. Instance and semantic fusion aligns the two signals into panoptic pseudo labels. Unsupervised optical flow is concerned with learning optical flow estimation without the need for ground-truth data. While early deep networks relied on synthetic ground-truth flow for supervision [22, 50, 67, 71], the domain gap to real videos, among other factors, has prompted the development of unsupervised deep optical flow pipelines [36, 45, 49, 51, 85]. Current unsupervised optical flow methods (e.g., SMURF [66]) offer accurate flow estimates, fast inference, and generalization to various real-world domains. Unsupervised instance segmentation aims to discover and segment object instances in images [64]. Recent work [63, 73, 7779] bootstraps class-agnostic instance segmentation networks using pseudo labels extracted from SSL features on object-centric data. TokenCut [80] applies normalized cuts [N-Cut, 61] to DINO features, providing foreground pseudo mask. CutLER [78] proposes MaskCut by iteratively applying N-Cuts, retrieving up to three pseudo masks per image. second stream of works uses motion cues to obtain an unsupervised signal for object discovery [20, 37, 47, 59, 69, 83]. SF2SE3 [65] clusters scene flow from consecutive stereo frames into independent rigid object motions in SE (3) space, improving object segmentation and motion accuracy. MOD-UV [69] uses motion segmentation for pseudo labeling and multi-stage training. Unsupervised semantic segmentation is approached by early deep learning methods via representation learning [19, 31, 35]. STEGO [30] leverages the self-supervised DINO features as an inductive prior and distills the features into lower-dimensional space before unsupervised probing. Later, [38, 60, 62] proposed improvements to the feature distillation or probing [29]. DepthG [62] extends STEGO by spatially correlating the feature maps with depth maps and furthest point sampling in the contrastive loss. DiffSeg [72] utilizes Stable Diffusion [58] and iterative attention merging for unsupervised semantic segmentation. Unsupervised panoptic segmentation is nascent research avenue following recent advancements in unsupervised semantic and instance segmentation. To the best of our knowledge, U2Seg [55] is the only method to date 3 to approach unsupervised panoptic segmentation. U2Seg leverages STEGO [30] and CutLER [78] to create panoptic pseudo labels for training panoptic network. However, its dependence on CutLERs MaskCut approach significantly In contrast, we limits its accuracy on scene-centric data. present the first unsupervised panoptic approach that learns directly from scene-centric data, addressing key limitations of U2Seg and MaskCut. 3. Method: CUPS We aim to learn panoptic segmentation network without any manual supervision. To that end, we leverage stereo video during pseudo labeling to incorporate depth and motion cues alongside visual features. Training and inference is done on single monocular images (cf . Fig. 1). Our pipeline comprises three stages: (1) Generating panoptic pseudo labels; (2) bootstrapping panoptic network with these pseudo labels; and (3) self-training of the network. 3.1. Stage 1: Pseudo-label generation We generate high-resolution panoptic pseudo labels directly on scene-centric data (cf . Fig. 3). Our key insight is that motion and depth provide cues to disambiguate the object instances and semantics in complex scenes. Specifically, we leverage scene flow from an unsupervised framework and perform motion segmentation to obtain pseudo masks for moving objects. Semantic information is derived from our depth-enhanced inference based on pre-trained SSL features [12]. Fusing these two signalsthe semantic information and the instance masksproduces panoptic pseudo labels, which contain both thing and stuff classes. Fig. 3 depicts our pseudo-label generation pipeline. 1a: Mining scene flow for precise object masks. Our first goal is to group scene flowper-pixel displacement in 3D space and timeinto coherently moving regions that likely correspond to object instances. Using two consecutive t), (Il stereo video frames, {(Il t+1)} of dimension R3HW, we estimate scene flow. We use SMURF [66] to compute unsupervised optical flow and disparity from the t+1, Ir t, Ir pair of stereo images. Given the forward flow fw and backward flow bw of dimension R2HW, as well as the est+1)} in RHW, we timated disparity {(dlr ), (dlr compute depth RHW and scene flow R3HW with respect to frame Il using the camera parameters of the dataset. We derive consistency mask {0, 1}HW using forward-backward and left-right consistency [70]. t+1, drl , drl Equipped with the scene flow and the consistency mask O, we perform motion segmentation to obtain object masks of moving objects. We employ SF2SE3 [65], motion clustering algorithm that uses and to fit variable number of rigid motions, defined in the Lie group SE (3). This results in set of SE (3)-motions with corresponding masks. However, the original SF2SE3 algorithm is stochastic due to random initialization, which can produce inconsistent motion segmentation across multiple runs. To mitigate these inconsistencies, we perform sampling-based filtering and mask refinement. While SF2SE3 ensures nonoverlapping masks, running SF2SE3 times produces potentially overlapping masks Mi,:,: {0, 1}HW, {1, ..., m}. To identify potentially incorrect masks, we compute consistency score [0, 1]m for each mask as ci = (cid:18) (cid:88) h,w Mi,:,: (cid:88) j=1 (cid:18) (cid:88) (cid:19) / Mj,:,: (cid:19) Mi,:,: , (1) h,w where is the Hardamard product. We keep object masks that occur in at least 80 % of SF2SE3 runs (i.e., ci 0.8). This straightforward consistency filtering removes potentially erroneous predictions. Conflicts between overlapping masks are resolved using matrix non-maximum suppression [76]. Finally, we isolate connected components of our object masks. Overall, this process results in highprecision moving object masks {0, 1}lHW. 1b: Depth-guided semantic pseudo labeling. Next, we proceed to extract the semantic pseudo labels. Thereby, our goal is to partition an image into semantically meaningful regions in an unsupervised manner while accounting for the high resolution and fine details present in scene-centric data. This is particularly relevant in the context of unsupervised semantic segmentation, where state-of-the-art methods typically operate at low resolutions (e.g., 320 320).1 To address this limitation, we propose to enhance semantic inference by incorporating depth guidance. We derive the semantic segmentation model by distilling lowerdimensional representation of DINO features using contrastive loss extended by depth as an auxiliary signal to guide feature correlation and sampling. The segmentation is obtained via stochastic cosine-distance K-means. The result is an unsupervised semantic segmentation model : R3HW RKHW, which encodes an image 1This is mainly due to the low resolution used in SSL pre-training [12]. R3HW into dense feature representation RKHW with semantic pseudo classes per pixel. The depth-guided semantic inference takes two semantic predictions at different resolutions, Plow and Phigh, and uses depth to compute their weighted average. Plow comes from running on downscaled input resolution. Conversely, Phigh represents semantic prediction at higher resolution. To compute Phigh, we slide window over the image, spatially concatenate the output tensors, and average the soft predictions (see supp. material for details). Intuitively, Plow provides reliable feature representations for large-scale scene elements located at close range. In contrast, Phigh encapsulates more fine-grained representation, which benefits small-scale objects at distance. Utilizing the inverse relationship between the projected object size and its distance to the camera in the pinhole model, we compute depth-based weight α at each pixel (h, w) as αh,w = (Dh,w + 1)1. (2) Note that we add 1 in the denominator for bounded range, ensuring that αh,w [0, 1]. We use the depth estimate D, obtained during instance pseudo labeling in Step 1a above. The weighted semantic prediction is then given as k,:,: = α Plow k,:,: + (1 α) Phigh k,:,:. (3) Observe that the pixels with small depth values Dh,w will derive their feature representation predominantly from Plow, whereas Phigh will contribute to the semantic representation of the pixels with large depth. To further improve the alignment of with the image structure, we perform post-processing with fully-connected conditional random field [43]. For our unsupervised semantic segmentation model S, we build upon DepthG [62]. To conform with our fully unsupervised setup, we re-train with the stereo depth from the unsupervised SMURF model. 1c: Instance and semantic fusion. We can finally obtain panoptic pseudo labels by fusing the instance and semantic pseudo labels. challenge here is distinguishing between the semantic classes belonging to stuff or thing categories in panoptic segmentation. Aggregating pixel distributions across all images, we compute the ratio of each pseudo classs frequency within the instance masks relative to its overall frequency. We designate semantic pseudo classes with high ratio above predefined threshold ψts as thing, and those below it as stuff. Next, we assign consistent semantic pseudo-label ID within each instance mask. Given semantic tensor P, we assign the most frequent semantic pseudo-class ID within each pseudo instance mask. Similarly, we assign image areas of pseudoclass IDs corresponding to pseudo-thing classes that do not have an instance mask to ignore. This results in the final pseudo labels, encompassing the training signal for all stuff regions as well as moving thing instances. Image Batch n E - S a - C a n a m Panoptic Network Predictions Pseudo Label Batch Pseudo Labels Self-Label Augmentations Photometric Aug. (cid:95) Momentum Network EMA (cid:95) Panoptic Network Predictions Self-Label (a) Stage 2: CUPS panoptic bootstrapping. (b) Stage 3: CUPS panoptic self-training. Figure 4. Overview of our CUPS training and self-training. Panoptic bootstrapping (a) optimizes the panoptic network based on the pseudo labels using self-enhanced copy-paste augmentation. Self-training of CUPS (b) is performed by obtaining augmented predictions from momentum network. We align, fuse, and filter the predictions to obtain refined self-labels. The panoptic network is trained on photometrically augmented images and pseudo labels. The momentum network prediction heads are updated using EMA weight updates. 3.2. Learning unsupervised panoptic segmentation Using our panoptic pseudo labels, we train panoptic network in two stages (cf . Fig. 4). First, we increase the coverage of the initial sparse set of pseudo labels in bootstrapping stage. Then, we use self-training on self-labels from ensembled predictions to enhance the models accuracy. Stage 2: Panoptic bootstrapping. Figure 4a provides an overview of the bootstrapping stage. We utilize the pseudo labels from Sec. 3.1, containing semantic information and sparse set of instance masks for moving objects. Despite the set sparsity, we can use this set of thing masks for network bootstrapping to accommodate the static objects as well. For this we employ the DropLoss [78], defined by Ldrop(Rj, ˆRi) = 1(cid:0) IoUmax > τ IoU(cid:1) LTh(Rj, ˆRi), (4) where LTh is the detection loss [40]. The DropLoss approach in Eq. (4) only supervises thing instances Rj with pseudo mask ˆRi whose maximum overlap IoUmax exceeds τ IoU. Importantly, the DropLoss does not penalize thing predictions that do not overlap with any pseudo mask from thing categories. This selective supervision allows the network to expand its prediction to potentially static objects not captured by the pseudo labels. For semantics, we pseudo-supervise using standard cross-entropy loss while omitting ignore pixels in the pseudo semantics. To enhance the accuracy of the panoptic network on small objects, we employ variant of copy-paste augmentation [24]. Instead of pasting masks from our pseudo labels, we copy-paste model predictions as they become confident during training. This self-enhanced copy-paste augmentation promotes the bootstrapping stage, since the network gradually discovers more potentially static objects. Stage 3: Panoptic self-training. In this final stage, illustrated in Fig. 4b, we further boost the panoptic segmentation accuracy of CUPS. We self-train the network on selflabels by ensembling and confidence thresholding of augmented predictions. Our self-training maintains momentum network as an exponential moving average (EMA) of our panoptic network. This approach is much akin to the student-teacher framework [2, 33, 82], which we apply here for panoptic segmentation. In detail, we create views of an input image using horizontal flipping and multiple image scales. The momentum network infers panoptic predictions for each view. We apply the inverse transform of each augmentation and merge the batch of predictions by averaging the soft semantic and instance predictions. Selflabels are retrieved by confidence thresholding of both the semantic and the instance signal. Given the averaged instance predictions [0, 1]JHW and their confidence prediction κ [0, 1]J, we apply the threshold γ [0, 1] and only keep instance masks for which κj > γ for the instance self-label. For the semantic self-label Lsem, we derive class-dependent threshold ζk [0, 1]. Given the averaged semantic predictions RKHW, we compute ζk from the semantic threshold ˆζ and the maximum probability of every pseudo class as ζk = ˆζ max( Pk,:,:). Given the predicted pseudo class with the highest probah,w = arg max( P:,h,w), we ignore low-confidence bility predictions using our class-dependent threshold ζk:"
        },
        {
            "title": "Lsem",
            "content": "h,w = (cid:40) if max( P:,h,w) ζk h,w ignore, otherwise. h,w (5) In summary, for each optimization step, we apply augmentations to the image and generate self-label by merging the augmentation-based predictions from the momentum network. Obtaining second prediction from photometrically perturbed image from our panoptic network, we apply standard panoptic loss w.r.t. the self-label. We update the panoptic network with gradient descent and use EMA to update the momentum network. We freeze all normalization layers and only train the heads of the network. 4. Experiments We evaluate the proposed CUPS on an extensive set of benchmarks and compare it to simple baseline and the current state-of-the-art methods for unsupervised panoptic segmentation, semantic segmentation, and class-agnostic instance segmentation. We refer to the supplemental material for additional quantitative and qualitative results. 5 Table 1. Unsupervised panoptic segmentation on Cityscapes val. Comparing CUPS to existing unsupervised panoptic methods, using PQ, SQ, and RQ, as well the PQ for thing and stuff classes (all in %, ). denotes results reported in [55]. Method Supervised [39] DepthG [62] + CutLER [78] U2Seg [55] U2Seg [55] CUPS (Ours) vs. prev. SOTA Training data Cityscapes Cityscapes & ImageNet COCO & ImageNet COCO & ImageNet Cityscapes Pseudo classes 27 800 + 27 800 + 27 27 PQ 62.3 16.1 17.6 18.4 27.8 +9.4 SQ 81.8 45.4 52.7 55. 57.4 +1.6 RQ 75.1 21.1 21.7 22.7 35.2 +12.5 PQTh PQSt 62.4 3.0 8.4 10.2 17.7 +7.5 62.1 25.7 24.2 24. 35.1 +10.8 Datasets. We train CUPS on pseudo labels generated using Cityscapes training sequences and evaluate it on Cityscapes val [21]. We also evaluate the generalization of our method through cross-domain experiments on KITTI panoptic segmentation [26, 53], BDD [84], MUSES [7], and Waymo [68]. Following the evaluation by Niu et al. [55], we use 19 semantic categories from Cityscapes for unsupervised panoptic segmentation. For the cross-domain datasets, we ensured compatibility of their label space with the Cityscapes categories. Additionally, we test CUPS in an out-of-domain setting by evaluating it on MOTS [75]. Our evaluation for unsupervised semantic segmentation follows the established protocol [19, 30, 35, 62] considering all 27 Cityscapes classes. For unsupervised class-agnostic instance segmentation, we follow previous work [9, 69] and evaluate on Waymo. We additionally demonstrate the versatility of our method w.r.t. the training set choice. Specifically, we alternatively use the raw KITTI data (instead of Cityscapes) for training, excluding all images used for evaluation. Please refer to our supplement for more details. Evaluation metrics. For evaluating the panoptic segmentation accuracy, we utilize the panoptic quality (PQ) [40] metric. We also report the segmentation quality (SQ) and recognition quality (RQ), which are part of PQ. As we train without any supervision, the semantic pseudo IDs predicted by the model need to be aligned with the ground truth for evaluation. To that end, we adapt the established evaluation in unsupervised semantic segmentation [19, 30, 35, 62] to unsupervised panoptic segmentation. In particular, we match thing and stuff classes independently based on the pixel-wise overlap of the pseudolabel IDs to the ground-truth labels across the dataset using the Hungarian algorithm [44]. After this one-to-one matching, we use maximum assignment for the remaining pseudo-label IDs. Notably, this matching solely depends on predicted pseudo semantics, avoiding extensive matching between segments, and does not introduce any hyperparameters, unlike the matching of U2Seg [55]. For unsupervised semantic segmentation, we report both the mean Intersection-over-Union (mIoU) and the all-pixel accuracy (Acc) following [19, 30, 35, 62]. Class-agnostic instance segmentation is evaluated using mask mean average precision (AP) [57], mask AP at an IoU threshold of 50 % (AP50), and AP for small, medium, and large objects [46]. Implementation details. We utilize 27 pseudo classes for pseudo-label generation (stage 1) allowing for comparison with both existing unsupervised panoptic and semantic segmentation approaches. For fair comparison, we follow Niu et al. [55] by using the same model architecture and initializationPanoptic Cascade Mask R-CNN [8, 39] with self-supervised pre-trained DINO ResNet-50 [32] backbone. CUPS pseudo-label training (stage 2) is using thing-stuff threshold ψts of 0.08, applying DropLoss, and self-enhanced copy-paste augmentation for 4 000 steps optimized with AdamW [48]. CUPS self-training (stage 3) applies multi-scale, horizontal flipping, and photometric augmentations following Chen et al. [14]. We apply self-enhanced copy-paste augmentation and EMA for 1 500 steps optimized with AdamW. We provide all implementation details in the supplement. Unsupervised panoptic segmentation baseline. To construct strong baseline equivalent of our method, we integrate the unsupervised semantic segmentation method DepthG [62] with the unsupervised class-agnostic instance segmentation of CutLER [78]. We obtain panoptic prediction by fusing the semantic and instance predictions in the same fashion as we do for our pseudo labels (cf . Sec. 3.1). Supervised upper bound. To better assess the effectiveness of CUPS, we train supervised variant of our method and report its performance as an upper bound of our framework. In line with our experimental setup, we train on Cityscapes and test on the same datasets as CUPS. 4.1. Comparison to the state of the art Our experiments assess the unsupervised panoptic segmentation accuracy of CUPS within its training domain and its generalization capabilities across diverse scene-centric datasets. We further evaluate CUPS on the two panoptic sub-tasks: unsupervised semantic and class-agnostic instance segmentation. Additionally, we analyze the impact of individual components of our method. Lastly, we investigate label-efficient learning. Unsupervised panoptic segmentation. pares CUPS with the state of the art Table 1 comin unsupervised 6 Table 2. Generalization. Comparing CUPS with unsupervised panoptic segmentation methods, using PQ, SQ, and RQ (in %, ) in terms of generalization to KITTI panoptic, BDD, MUSES, and Waymo. In addition, we analyze generalization to the OOD dataset MOTS. Method Supervised [39] DepthG [62] + CutLER [78] U2Seg [55] CUPS (Ours) vs. prev. SOTA KITTI PQ SQ 31.9 11.0 20. 25.5 +4.9 71.7 34.5 52.9 58.1 +5.2 RQ 40. 13.8 25.2 32.5 +7.3 BDD PQ SQ 33. 14.4 15.8 19.9 +4.1 76.3 41.9 57.2 60.3 +3.1 MUSES Waymo MOTS (OOD) RQ 42.0 19.2 19.2 25.9 +6. PQ SQ 38.1 10.1 20.3 24.4 +4.1 62. 30.1 45.8 48.5 +2.7 RQ 49.6 13.1 26.5 33.0 +6. PQ SQ 31.5 13.4 19.8 26.4 +6.6 70. 37.3 50.8 60.3 +9.5 RQ 40.9 17.0 23.4 PQ 73.8 49.6 50.7 SQ 86.4 78.4 79.2 RQ 84.6 60.6 64.3 33.0 67.8 +9.6 +17.1 86.4 76.9 +7.2 +12.6 Table 3. Unsupervised semantic segmentation. Comparing CUPS to existing unsupervised semantic segmentation methods on Cityscapes val, using Accuracy and mean IoU (in %, ). Table 4. Unsupervised instance segmentation. Comparing CUPS, trained on Cityscapes, to unsupervised class-agnostic instance segmentation methods on Waymo using mask APs (%, ). Method Model Acc mIoU Method Training data AP50 AP APS APM APL Supervised [39] Pano. Cascade Mask R-CNN PiCIE [19] DiffSeg [72] HP [60] PriMaPs-EM [29] STEGO [30] EAGLE [38] DepthG [62] U2Seg [55] ResNet-18 + FPN Stable Diffusion V1.4 DINO-S/8 DINO-S/8 DINO-B/8 DINO-B/8 DINO-B/8 Pano. Cascade Mask R-CNN CUPS (Ours) Pano. Cascade Mask R-CNN 94.7 65.5 67.3 80.1 81.2 73.2 79.4 81.6 79.1 83.2 76.7 12.3 15.2 18.4 19.4 21.0 22.1 23.1 21.6 26. panoptic segmentation, U2Seg [55], and our baseline, DepthG [62] + CutLER [78], evaluated on the Cityscapes validation dataset. We also report supervised upper bound. Since the evaluation code of U2Seg for Cityscapes is not publicly available, we re-evaluate U2Seg using our pseudoclass-to-class matching and report the results of [55] for reference. CUPS achieves PQ of 27.8 %, substantially improving over U2Seg (18.4 %). This demonstrates how CUPS effectively utilizes scene-centric training data to improve panoptic quality. Remarkably, CUPS achieves superior panoptic quality across all thing classes (PQTh), despite not excessively over-clustering instance classes (contrary to U2Seg with 800 thing pseudo classes) and solving the additional thing-stuff assignment task. In Tab. 2, we explore the generalization of CUPS across various scene-centric domains. CUPS consistently surpasses the baseline as well as U2Seg on all datasets. These results indicate that CUPS not only excels in the training domain, Cityscapes, but is also effective under domain shift, underscoring its robustness. By contrast, the supervised baseline suffers significant drop in accuracy under the domain shift an effect not observed for the unsupervised approaches. In addition, we assess the generalization of CUPS on MOTS, which represents an out-of-domain (OOD) testing scenario. CUPS achieves outstanding segmentation accuracy, substantially surpassing the baseline and U2Seg. These results provide strong evidence that CUPS excels in the OOD scenario as well. Supervised [39] Cityscapes 44.6 27.6 10. 45.0 73.3 U2Seg [55] CutLER [78] HASSOD [9] MOD-UV [69] COCO & ImageNet ImageNet COCO Waymo 4.3 9.1 3.9 25.1 2.3 5.2 2.0 11. CUPS (Ours) Cityscapes 30.5 12.4 0.0 0.0 0.0 4.5 2. 1.5 3.4 0.9 15.6 17.0 34.6 18.3 36.3 21.2 45.3 Unsupervised semantic segmentation. As unsupervised panoptic segmentation implicitly solves the task of unsupervised semantic segmentation, we also assess the performance of CUPS on this sub-task by comparing it to recent methods on Cityscapes. Table 3 summarizes the results. CUPS achieves state-of-the-art semantic segmentation accuracy, improving over the previous best method, In comparison to DepthG [62], by significant margin. U2Seg, CUPS improves substantially by 4.1 % in pixel accuracy and by 5.2 % in mIoU. Unsupervised instance segmentation. Our unsupervised panoptic predictions include object instances, which we benchmark against current methods in class-agnostic instance segmentation. Table 4 provides evaluation results on the Waymo dataset. Our approach achieves excellent results and outperforms prior work. Compared to the state of the art, MOD-UV [69], which directly trains on Waymo and does not provide panoptic segmentation, CUPS outperforms it in terms of AP50 and overall AP. In more detail, our method shows stronger detection accuracy for large and medium instance sizes than MOD-UV, but performs slightly worse on small instances. Nevertheless, CUPS achieves state-of-the-art level of instance segmentation accuracy despite this being side task in the overall framework. 4.2. Analyzing CUPS CUPS pseudo-label generation. In Tab. 5, we analyze the contribution of individual pseudo-label generation substeps by gradually increasing the complexity. We start by simply combining the unsupervised semantic predictions and unsupervised instance predictions. As described in 7 Table 5. Pseudo-label generation ablation, analyzing the contribution of individual generation components, using PQ, SQ, and RQ (in %, ) for pseudo labels generated on Cityscapes val. Pseudo-label configuration Vanilla semantics + SF2SE3 + Instance-aligned semantics (1c) + SF2SE3-ensembling (1a) + Depth-guided semantic inference (1b) PQ 14.3 14.9 15.9 18.1 SQ 35.5 43.8 47.0 47.3 RQ 17.8 18.2 19.5 22. Table 6. Pseudo label thing-stuff threshold analysis showing the impact of ψts using PQ (in %, ) on Cityscapes val pseudo labels. ψts PQ 0.04 17.8 0. 18.1 0.08 18.1 0.1 18.1 0. 17.7 0.14 16.7 Table 7. CUPS training analysis. We study the role of (a) the pseudo labels, (b) the number of pseudo classes, and (c) training components on the models accuracy trained on Cityscapes and validated on Cityscapes val. We also ablate (d) the training dataset by training on KITTI-raw and validating on KITTI panoptic. (a) Pseudo-label training analysis (b) Overclustering analysis Pseudo-label configuration PQ Pseudo classes PQ SQ RQ Vanilla pseudo labels CUPS pseudo labels 19.7 27.8 (c) Training components ablation Training configuration Vanilla training + DropLoss + Copy-paste aug. + Self-enhance copy-paste + Self-training (CUPS) PQ 24.1 25.3 26.3 26.6 27.8 27 (default) 40 54 27.8 57.4 35.2 30.3 64.3 37.5 30.6 65.1 37. (d) Training dataset analysis Method DepthG+CutLER U2Seg CUPS (on KITTI) Pseudo classes PQ 14.3 20.6 22.0 27 827 27 Sec. 3.1, we add the alignment of the semantic pseudo IDs based on the instance masks (1c), our ensemble-based SF2SE3 extension (1a), and depth-guided semantic inference (1b). Every component contributes to the PQ of the labels. We also analyze the thing-stuff threshold ψts in Tab. 6 and observe highly stable behavior for different thresholds. The pseudo-label analysis is conducted on pseudo labels generated on Cityscapes val for better comparison. CUPS training analysis. We analyze the contribution of different components and design choices of the CUPS training in Tab. 7. Building on Tab. 5, we show the significant performance metric differences between training on the simplest form of pseudo labels and our CUPS pseudo labels in Tab. 7a. Table 7c reveals that each training component and stage complements the final panoptic quality. Table 7b demonstrates that increasing the number of pseudo classes for pseudo-label generation and training substantially improves unsupervised panoptic segmentation performance. Finally, we train on KITTI without making any adjustments to our method and hyperparameters, see Tab. 7d. Our approach yields significantly better results than both the baseline and U2Seg w.r.t. the panoptic quality. Although 40 30 +3.9% +3.4% +3.6% +4.5% CUPS (Ours) U2Seg [55] w/o fine-tune 10 30 ( 1%) Annotated Images 60 ( 2%) Figure 5. Label-efficient learning results. We fine-tune CUPS and U2Seg on reduced amounts of annotated Cityscapes training images and compute PQ on Cityscapes val (in %, ). We report the average and standard deviation across three different subsets. the PQ is slightly inferior to that of the model trained on Cityscapeslikely due to the lower resolution and diversity of KITTIwe observe that CUPS performs well, regardless of the training data. 4.3. Label-efficient learning Ultimately, achieving high-quality, task-specific panoptic segmentation requires alignment with the desired taxonomy, which cannot be accomplished in purely unsupervised fashion. However, pre-training for unsupervised panoptic segmentation could be promising modus operandi for label-efficient learning, where minimal amount of labeled data is available to address the desired application. Here, we explore this scenario by allocating small share of annotated Cityscapes training images. We fine-tune the segmentation heads of CUPS and U2Seg, while keeping their backbones and feature pyramid networks frozen. Fig. 5 reports the average PQ for varying amount of annotated training data. Each data point averages the PQ across three runs with random non-overlapping training subset of fixed size. We observe that CUPS scales reliably across all data regimes, maintaining roughly constant and significant margin over U2Seg. Notably, using solely 60 annotated images, i.e., 2 % of the total Cityscapes labels, achieves PQ of 43.6 % which amounts to 70 % of the panoptic quality of the supervised upper bound. This experiment suggests that CUPS not only achieves outstanding accuracy in the unsupervised setting but also reduces the annotation effort in downstream tasks. 5. Conclusion We presented CUPS, the first scene-centric unsupervised panoptic segmentation framework that trains directly on rich scene-centric images. Integrating visual, depth, and motion cues, CUPS overcomes the dependence on objectcentric training data and achieves significant improvements on challenging scene-centric datasets where prior methods struggle. Our approach brings the quality of unsupervised panoptic, instance, and semantic segmentation to new level and demonstrates highly promising results in labelefficient panoptic segmentation. 8 Acknowledgments. This project was partially supported by the European Research Council (ERC) Advanced Grant SIMULACRON, DFG project CR 250/26-1 4D-YouTube, and GNI Project AICC. This project has also received funding from the ERC under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 866008). Additionally, this work has further been co-funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center [LOEWE/1/12/519/03/05.001(0016)/72] and by the State of Hesse through the cluster project The Adaptive Mind (TAM). Christoph Reich is supported by the Konrad Zuse School of Excellence in Learning and Intelligent Systems (ELIZA) through the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research. Finally, we acknowledge the support of the European Laboratory for Learning and Intelligent Systems (ELLIS) and thank Simone Schaub-Meyer and Leonhard Sommer for insightful discussions."
        },
        {
            "title": "References",
            "content": "[1] Saleh Albelwi. Survey on self-supervised learning: Auxiliary pretext tasks and contrastive learning methods in imaging. Entropy, 24(4):551, 2022. 2 [2] Nikita Araslanov and Stefan Roth. Self-supervised augmentation consistency for adapting semantic segmentation. In CVPR, pages 1538415394, 2021. 5 [3] Shahaf Arica, Or Rubin, Sapir Gershov, and Shlomi Laufer. CuVLER: Enhanced unsupervised object discoveries through exhaustive self-supervised transformers. In CVPR, pages 2310523114, 2024. 2 [4] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 2 [5] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS, pages 1550915519, 2019. 2 [6] Adrien Bardes, Jean Ponce, and Yann LeCun. VICRegL: Self-supervised learning of local visual features. In NeurIPS, pages 87998810, 2022. 2 [7] Tim Brödermann, David Brüggemann, Christos Sakaridis, Kevin Ta, Odysseas Liagouris, Jason Corkill, and Luc Van Gool. MUSES: The multi-sensor semantic perception dataset for driving under uncertainty. In ECCV, pages 21 38, 2024. 1, 6, iii [8] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detection. In CVPR, pages 6154 6162, 2018. 6, ii, vi [9] Shengcao Cao, Dhiraj Joshi, Liangyan Gui, and Yu-Xiong Wang. HASSOD: Hierarchical adaptive self-supervised obIn NeurIPS, pages 5933759359, 2023. 6, ject detection. 7 [11] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, pages 99129924, 2020. 2 [12] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 96509660, 2021. 2, 3, 4, ii [13] Xinlei Chen and Kaiming He. Exploring simple Siamese In CVPR, pages 1575015758, representation learning. 2021. [14] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv:2003.04297 [cs.CV], 2020. 2, 6, ii [15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In CVPR, pages 96409649, 2021. 2 [16] Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and Zeynep Akata. Semi-supervised and unsupervised deep visual learning: survey. IEEE Trans. Pattern Anal. Mach. Intell., 46(3):13271347, 2022. 1 [17] Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-DeepLab: simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, pages 12472 12482, 2020. 1 [18] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask In CVPR, transformer for universal image segmentation. pages 12901299, 2022. 1, vi, [19] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. PiCIE: Unsupervised semantic segmentation usIn CVPR, ing invariance and equivariance in clustering. pages 1679416804, 2021. 3, 6, 7, ii, iii [20] Subhabrata Choudhury, Laurynas Karazija, Iro Laina, Andrea Vedaldi, and Christian Rupprecht. Guess What Moves: Unsupervised video and image segmentation by anticipating motion. In BMVC, 2022. 3 [21] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Scharwächter, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, pages 32133223, 2016. 1, 6, iii [22] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Häusser, Caner Hazırbas, Vladimir Golkov, Patrick v. d. Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learning optical flow with convolutional networks. In ICCV, pages 27582766, 2015. 3 [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 1616 words: Transformers for image recognition at scale. In ICLR, 2021. 2 [10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, pages 132149, 2018. 2 [24] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, Paste and Learn: Surprisingly easy synthesis for instance detection. In ICCV, pages 13011310, 2017. 9 [25] Linus Ericsson, Henry Gouk, Chen Change Loy, and Timothy M. Hospedales. Self-supervised representation learning: Introduction, advances, and challenges. IEEE Trans. Signal Process., 39(3):4262, 2022. 2 [26] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? The KITTI vision benchmark suite. In CVPR, pages 33543361, 2012. 6, iii [27] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: new approach to self-supervised learning. In NeurIPS, pages 2127121284, 2020. 2 [28] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese In NeurIPS, pages 4067640693, masked autoencoders. 2023. [29] Oliver Hahn, Nikita Araslanov, Simone Schaub-Meyer, and Stefan Roth. Boosting unsupervised semantic segmentation with principal mask proposals. Trans. Mach. Learn. Res., 2024. 3, 7 [30] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. 2, 3, 6, 7, i, ii, iii [31] Robert Harb and Patrick Knöbelreiter. InfoSeg: Unsupervised semantic image segmentation with mutual information maximization. In GCPR, pages 1832, 2021. 3 [32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. pages 770778, 2016. 6, ii [33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 97299738, 2020. 2, 5 [34] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. 2 [35] Xu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, pages 98659874, 2019. 3, 6, ii, iii [36] Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What matters in unsupervised optical flow. In ECCV, pages 557572, 2020. 3 [37] Laurynas Karazija, Subhabrata Choudhury, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Unsupervised multiobject segmentation by predicting probable motion patterns. In NeurIPS, pages 21282141, 2022. [38] Chanyoung Kim, Woojung Han, Dayun Ju, and Seong Jae Hwang. EAGLE: Eigen aggregation learning for objectIn CVPR, centric unsupervised semantic segmentation. pages 35233533, 2024. 2, 3, 7 [39] Alexander Kirillov, Ross B. Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks. In CVPR, pages 63996408, 2019. 6, 7, ii, v, vi [40] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In CVPR, pages 94049413, 2019. 1, 5, 6, ii [41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. 1 [42] Kurt Koffka. Principles of Gestalt psychology. Routledge, 1935. [43] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected CRFs with Gaussian edge potentials. In NIPS, pages 109117, 2017. 4, ii [44] Harold W. Kuhn. The Hungarian method for the assignment problem. Nav. Res. Logist., 2(1-2):8397, 1955. 6, iii [45] Gal Lifshitz and Dan Raviv. Cost function unrolling in unIEEE Trans. Pattern Anal. Mach. supervised optical flow. Intell., 46(2):869880, 2024. 3 [46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, pages 740755, 2014. 1, 6 [47] Runtao Liu, Zhirong Wu, Stella Yu, and Stephen Lin. The emergence of objectness: Learning zero-shot segmentation from videos. In NeurIPS, pages 1313713152, 2021. 3 [48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6, ii [49] Rémi Marsal, Florian Chabot, Angélique Loesch, and Hichem Sahbi. BrightFlow: Brightness-change-aware unsupervised learning of optical flow. In WACV, pages 2061 2070, 2023. 3 [50] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, In CVPR, pages optical flow, and scene flow estimation. 40404048, 2016. 3 [51] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Unsupervised learning of optical flow with bidirectional census loss. In AAAI, pages 72517259, 2018. 3 [52] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: survey. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):35233542, 2022. 1, 2 [53] Rohit Mohan and Abhinav Valada. EfficientPS: Efficient Int. J. Comput. Vis., 129(5):1551 panoptic segmentation. 1579, 2021. 6, iii [54] Duy Kien Nguyen, Yanghao Li, Vaibhav Aggarwal, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, and Xinlei Chen. R-MAE: Regions meet masked autoencoders. In ICLR, 2024. [55] Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei Herzig, and Trevor Darrell. Unsupervised universal image segmentation. In CVPR, pages 2274422754, 2024. 1, 2, 3, 6, 7, 8, i, ii, iv, v, vi, vii, viii, ix [56] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, 10 et al. DINOv2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024. 2 Unsupervised zero-shot segmentation using stable diffusion. In CVPR, pages 35543563, 2024. 3, 7 [57] Rafael Padilla, Sergio L. Netto, and Eduardo A. B. Da Silva. survey on performance metrics for object-detection algorithms. In IWSSIP, pages 237242, 2020. [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3 [59] Sadra Safadoust and Fatma Güney. Multi-object discovery by low-dimensional object motion. In ICCV, pages 734744, 2023. 3 [60] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for unsupervised semantic segmentation. In CVPR, pages 1954019549, 2023. 2, 3, 7 [61] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 22 (8):888905, 2000. 3 [62] Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo Ropinski. Unsupervised semantic segmentation through In CVPR, depth-guided feature correlation and sampling. pages 36373646, 2024. 2, 3, 4, 6, 7, i, ii, iii, v, vi [63] Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. In BMVC, 2021. 2, 3 [64] Oriane Siméoni, Éloi Zablocki, Spyros Gidaris, Gilles Puy, and Patrick Pérez. Unsupervised object localization in the era of self-supervised ViTs: survey. Int. J. Comput. Vis., 133(2):781808, 2025. [65] Leonhard Sommer, Philipp Schröppel, and Thomas Brox. SF2SE3: Clustering scene flow into SE(3)-motions via proposal and selection. In GCPR, pages 215229, 2022. 3, 4, [66] Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, and Rico Jonschkowski. SMURF: Self-teaching multi-frame unsupervised RAFT with full-image warping. In CVPR, pages 38873896, 2021. 3, i, iii, iv [67] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In CVPR, pages 89348943, 2018. 3 [68] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, pages 24432451, 2020. 6, iii [69] Yihong Sun and Bharath Hariharan. MOD-UV: Learning In ECCV, mobile object detectors from unlabeled videos. pages 289307, 2024. 3, 6, 7 [70] Narayanan Sundaram, Thomas Brox, and Kurt Keutzer. Dense point trajectories by GPU-accelerated large displacement optical flow. In ECCV, pages 438451, 2010. 4 [71] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402419, 2022. 3, iii, iv [72] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar González-Franco. Diffuse, attend, and segment: [73] Wouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv:2206.06363 [cs.CV], 2022. 3 [74] Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, et al. Continual learning: Applications and the road forward. Trans. Mach. Learn. Res., 2024. 1 [75] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. MOTS: Multi-object tracking and segmentation. In CVPR, pages 79427951, 2019. 6, iii [76] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic and fast instance segmentation. In NeurIPS, pages 1772117732, 2020. 4 [77] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and José M. Álvarez. FreeSOLO: Learning to segment objects without annotations. In CVPR, pages 1415614166, 2022. 2, [78] Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In CVPR, pages 31243134, 2023. 2, 3, 5, 6, 7, iv, v, vi [79] XuDong Wang, Jingfeng Yang, and Trevor Darrell. Segment In NeurIPS, pages 138731 anything without supervision. 138755, 2024. 3, ix [80] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L. Crowley, and Dominique Vaufreydaz. TokenCut: Segmenting objects in images and videos with self-supervised transformer and normalized cut. IEEE Trans. Pattern Anal. Mach. Intell., 45(12):1579015801, 2023. 2, 3 [81] Max Wertheimer. Experimentelle Studien über das Sehen von Bewegung. Zeitschrift fur Psychologie, 61:161165, 1912. 2 [82] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves ImageNet classification. In CVPR, pages 1068710698, 2020. [83] Yanchao Yang, Brian Lai, and Stefano Soatto. DyStaB: Unsupervised object segmentation via dynamic-static bootstrapping. In CVPR, pages 28262836, 2021. 3 [84] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. BDD100K: diverse driving dataset for heterogeneous multitask learning. In CVPR, pages 26332642, 2020. 6, iii [85] Jason J. Yu, Adam W. Harley, and Konstantinos G. Derpanis. Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. In ECCV Workshops, pages 310, 2016. 3 [86] Tianfei Zhou, Fei Zhang, Boyu Chang, Wenguan Wang, Image Ye Yuan, Ender Konukoglu, and Daniel Cremers. segmentation in foundation model era: survey. arXiv: 2408.12957 [CV.cv], 2024. 1 11 Scene-Centric Unsupervised Panoptic Segmentation"
        },
        {
            "title": "Supplementary Material",
            "content": "Oliver Hahn* 1 Daniel Cremers 2,4,5 Christoph Reich* 1,2,4,5 Christian Rupprecht 3 Nikita Araslanov 2,4 Stefan Roth 1,5,6 6hessian.AI 1TU Darmstadt 2TU Munich 3University of Oxford 4MCML 5ELIZA *equal contribution https://visinf.github.io/cups In this appendix, we first highlight the conceptual features of our unsupervised panoptic segmentation method CUPS. We elaborate on the training and validation approach as well as on implementation details to facilitate reproducibility. Next, we conduct further analyses of different design choices and the training stages. We then provide additional quantitative and qualitative results. Finally, we discuss the limitations of current unsupervised panoptic approaches as well as our CUPS approach. A. CUPS vs. U2Seg: Conceptual Comparison Table 8 conceptually compares CUPS to U2Seg [55]. While both frameworks address the problem of unsupervised panoptic segmentation, CUPS features novel distinctions: (1) Scene-centric training. Object-centric images typically depict center-aligned foreground object on fairly homogeneous background. The photographic bias inherent to this type of imagery also implies the need for manual curation in the data collection process. By contrast, scenecentric data encapsulates the complexity of real-world environments where multiple objects coexist and interact. Furthermore, collecting scene-centric imagery is substantially cheaper, since it obviates the need for artificially isolating objects from their context. Training on scene-centric data is crucial to producing models that are capable of understanding real-world complexity and serving the needs of challenging applications, such as autonomous driving, robotic navigation, augmented reality, and assistive technologies for visually impaired individuals. Although we are not the first to leverage motion for retrieving instance cues, accomplishing this in self-supervised fashion is novel aspect in the context of unsupervised panoptic segmentation. (2) High-resolution pseudo labels. High-resolution training is crucial for capturing fine details in scene-centric data, which lower-resolution settings cannot address. Our depthguided semantic inference (cf . Sec. 3.1) provides semantic pseudo-labeling component with twice the resolution of previous methods. This enhancement allows CUPS to learn semantic cues to higher degree of detail, which can be observed in our qualitative results (cf . Fig. 7). (3) Thing-stuff separation. Our integration of motion cues enables precise distinction between semantic pseudo Table 8. conceptual comparison of CUPS and U2Seg. Unsupervised panoptic segmentation Scene-centric training High-resolution pseudo labels Thing-stuff separation U2Seg [55] CUPS (Ours) thing and pseudo stuff classes. This is because motion helps us identify thing classes as objects that move relative to the camera. In contrast, U2Seg cannot really distinguish between stuff and thing classes; this ambiguity is only resolved at test time via oracle matching of the pseudo labels with ground-truth semantic categories and object instances. The capacity of CUPS to discriminate between stuff and thing categories is an advancement toward solving unsupervised panoptic segmentation in more principled way. B. Reproducibility To facilitate reproducibility, we elaborate on the technical and implementation details. Note that our code is available at https://github.com/visinf/cups. B.1. Implementation details CUPS is implemented using PyTorch [89], PyTorch Lightning [87], and Kornia [90]. We partly build upon public codebases from previous work [30, 55, 62, 65, 66]. Stage 1. CUPS pseudo-label generation uses 27 pseudo classes and thing-stuff threshold ψts of 0.08. This setting enables comparison against existing unsupervised panoptic and semantic segmentation approaches without relying on significant overclustering (cf . [55]). Instance pseudo labeling uses motion and depth estimates from pre-trained SMURF model [66]. For our ensembling-based SF2SE3 clustering, we build upon the original implementation by Sommer et al. [65]. Semantic pseudo labeling uses pretrained SMURF [66] to generate the depth to train the semantic segmentation network following Sick et al. [62]. For depth-guided semantic inference, we first resize the input image so that its smaller side is 320 pixels, matching the standard resolution in unsupervised semantic segmentai tion. We then perform second inference pass using sliding windows on an image scale of 640 pixels with stride of half the window size. Depth-guided semantic inference uses the SMURF depth estimate to weight the two semantic segmentation predictions. The size of the sliding window is half the image width and height. Finally, we perform post-processing by further aligning the prediction to the image using fully connected conditional random field (CRF) [43, 92]. For fair comparison and to demonstrate the impact of our pseudo labeling as well as the proposed training scheme, we use the same panoptic network as U2Seg. In particular, we follow Niu et al. [55] by employing the Panoptic Cascade Mask R-CNN [8, 39] with ResNet-50 [32] backbone pre-trained using self-supervised DINO [12] for two epochs on ImageNet [91]. Stage 2. CUPS pseudo-label bootstrapping proceeds by training for 4 000 steps with AdamW [48], using learning rate of 104, and weight decay of 105. The drop-loss overlap threshold τ IoU is set to 0.4. After 1 000 steps, we start utilizing our self-enhanced copy-paste augmentation, randomly pasting between 1 and 8 objects into each image. Stage 3. CUPS self-training runs for 1 500 steps using AdamW with learning rate of 105 and no weight decay. The EMA decay for updating the momentum network is set to 0.9999. We only update the detection heads, the mask head, and the semantic head, freezing all normalization layers. For self-labeling augmentation, we use three different scales of the original image (0.75, 1.0, and 1.25), as well as horizontal flips at each scale, resulting in six views. We follow Chen et al. [14] to set up the photometric augmentation and employ our self-enhanced copy-paste augmentation also during self-training. For both pseudo-label training and self-training, we utilize four NVIDIA A100 GPUs (40 GB) with batch size of 16 per GPU. We evaluate CUPS on the native resolution of each dataset, except for unsupervised semantic segmentation (cf . Tab. 3) where we follow the common evaluation protocol [19, 30, 35, 62]. B.2. Computing panoptic quality As we train without any supervision, the semantic pseudoclass IDs are not aligned with the ground-truth semantic class IDs. Therefore, to compute the panoptic quality (PQ) [40], we need to align the pseudo-class IDs with the ground truth, distinguishing between thing and stuff semantic categories at the same time. While U2Seg [55] also utilizes the panoptic quality and proposes an elaborate matching approach, significant limitations remain. Niu et al. [55] establish semantic matching using three steps. First, predicted segments are matched with all ground-truth segments, ignoring the thing and stuff separation. Segments with an overlap of less than pre-defined threshold (hyperparameter) are discarded. Second, using the set of matched segments and both the semantic pseudo-class IDs and the ground-truth class IDs, cost matrix is constructed on per-segment basis. Third, for each semantic pseudo class, the most frequent ground-truth class ID based on the cost matrix is matched. This matching approach entails two significant limitations. First, the overlap threshold is crucial hyperparameter and can significantly impact the final PQ value. This is mainly due to the fact that the segment-wise cost matrix finds relatively few overlapping objects, and thresholding is required to consider only accurate predictions for matching. Second, the matching approach does not consider the thing and stuff separation, leading to matches between both thing and stuff categories. This is highly undesired as thing segments entail object-level masks, whereas stuff segments only capture the semantic level. Finally, code for evaluation on the Cityscapes dataset has not been published by the authors of [55]. Principles. We redefine the matching process in alignment with the following core principles: Simplicity: Introducing additional hyperparameters within the matching is undesirable, as it complicates evaluation. Semantic segmentation is pixel-wise classification, hence we aim to perform matching of the pseudo classes to ground-truth classes on the pixel level as well. More specifically, every predicted pixel should be considered in the alignment between pseudo classes and ground-truth annotations. This resembles the simplest form of approaching the problem and is common in unsupervised semantic segmentation [19, 30, 35, 62]. Clear thing and stuff separation: The distinction between thing and stuff classes is core aspect of panoptic segmentation. Consequently, it should be addressed by the method itself rather than the matching process. To ensure alignment, only pseudo classes labeled as stuff are matched with stuff ground-truth classes, and the same applies to thing classes. Approach. To this end, we propose simple but effective approach for matching. Taking inspiration from the established semantic matching for the task of unsupervised semantic segmentation [19, 30, 35, 62], we perform matching purely utilizing semantics. In particular, we obtain the semantic segmentation prediction {1, . . . , ξp}HW from the unsupervised panoptic prediction, with ξp denoting the number of pseudo classes. We use the ground-truth semantic segmentation ˆP {1, . . . , ξGT}HW, with ξGT indicating the number of ground-truth semantic classes, to construct cost matrix NξpξGT. This cost matrix counts the number of overlapping pixels of each pseudo-class ID with all ground-truth class IDs. The full cost matrix is obtained using all validation samples. To ensure no thing class ID is matched to stuff class ID or vice versa, we extract thing and stuff cost matrix from the full cost ii ξSt ξTh > ξTh GT and/or ξSt matrix A. By using the thing and stuff splits of classes in the pseudo classes as well as the ground-truth classes, we construct thing cost matrix ATh NξTh GT and stuff cost matrix ASt NξSt GT . Hungarian matching [44] is then applied to maximize overlap and establish one-to-one matching between pseudo-class IDs and ground-truth class IDs by running matching on ATh and ASt, separately. As we can have more semantic pseudo-class IDs than groundtruth class IDs (i.e., ξTh GT), we assign all remaining pseudo classes, not assigned by Hungarian matching, to the respective ground-truth class ID with the maximum overlap. This process leads to permutation of the pseudo-class IDs, maximizing the overlap with the ground-truth class IDs while adhering to the thing and stuff separation. Finally, we utilize the permuted (i.e., matched) semantics alongside the instance maskthe binary masks predicted for instancesto compute PQ. For evaluating on the task of unsupervised semantic segmentation, we skip the step of separating into ATh and ASt and perform single matching on as done by the related work in the field [19, 30, 35, 62]. > ξSt To conclude, our class matching for unsupervised panoptic quality builds on established protocols, performs straightforward and efficient matching, and adheres to the thing and stuff class split, while not introducing any hyperparameters. Interestingly, we observe that evaluating U2Seg with our matching leads to better panoptic quality than reported in the original paper (cf . Tab. 1). This suggests that we find better correspondence between pseudo and ground-truth classes. We make the evaluation code for all settings publicly available to facilitate future research. B.3. Datasets We provide further details about the datasets used to train and evaluate CUPS. contains 5 000 It Cityscapes [21] is an ego-centric driving scene dataset, which images with high-resolution 20481024 pixels. into 2 975 train, 500 is split val, and 1 525 test images with pixel-level annotations provided for grouping into 27, 19, or 7 categories. Each of the training images stems from short video sequence. We leverage all 86 275 video frames of the training split for unsupervised training and evaluate on the validation split, in line with previous work. Table 9. Comparison of motion networks for pseudo-label generation. Investigating the contribution of the correspondence matching network, using PQ, SQ, and RQ (in %, ) for pseudo labels generated on Cityscapes val. We use our full configuration and only change the motion network. Optical flow method BrightFlow [88] (unsupervised) SMURF [66] (unsupervised) SEA-RAFT [93] (supervised) RAFT [71] (supervised) PQ 17.8 18. 19.2 20.4 SQ 46.4 47.3 51.8 52.6 RQ 22.4 22. 23.4 24.7 taxonomy. We use the 200 validation images for evaluation. Furthermore, we use all 42 150 rectified KITTI images excluding the validation split and calibration scenes for unsupervised training. BDD [84] is driving scene dataset, which also contains panoptic annotations with 19 class definitions identical to those in Cityscapes. The images have resolution of 1280720 pixels. The validation set contains 1 000 images. MUSES [7] is multi-modal dataset representing adverse conditions in driving scenes. The labels use the 19 class taxonomy of Cityscapes. For evaluation, we utilize the daytime clear validation split, containing 50 images with resolution of 19201080. Waymo [68] is another driving scene dataset. We use the front camera, providing resolution of 19201280 pixels and evaluate using the 1 930 images of the 2D panoptic segmentation validation split. Waymo classes are remapped to ensure compatibility of its label space with the Cityscapes classes, resulting in 16 classes. MOTS [75] allows to assess scene-centric panoptic segmentation outside of driving scenarios. Evaluation is performed using the MOTChallenge sequences for multiobject tracking and segmentation of humans in indoor and outdoor scenes. The annotations include two classes background and person, where background is considered as stuff class and person is thing class. We evaluate on 2 862 images of resolutions 640480 or 19201080. C. Additional Results In the following, we analyze the results presented in the main paper in greater detail. C.1. CUPS pseudo-labels results The KITTI [26, 53] vision benchmark suite is comprehensive driving-scene dataset with ground truth for variety of tasks, such as semantic segmentation, optical flow estimation, depth estimation. Mohan et al. [53] introduced the KITTI panoptic segmentation dataset for urban scene understanding by providing panoptic annotations for subset of 1 055 images. The images have resolution of 1280384 pixels and adhere to the 19-class grouping of the Cityscapes Supervised vs. unsupervised optical flow. In conjunction with the pseudo-label generation analysis presented in Tab. 5, we investigate the influence of different approaches for optical flow and two-frame disparity estimation on our pseudo labels in Tab. 9. Identical to the analysis in the main paper, we generate pseudo labels on the validation split to ensure comparability with the CUPS panoptic segmentation results and CUPS analysis. iii"
        },
        {
            "title": "High Resolution",
            "content": "Depth Guided (Ours) Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle Figure 6. Depth-guided semantic pseudo-label examples. Qualitative semantic pseudo-label examples comparing low resolution Plow, high resolution Phigh, and depth-guided semantic fusion P. Table 10. Depth-guided semantic pseudo label analysis. Semantic pseudo labels evaluated on Cityscapes val (for consistency both in 19 class setting). (a) comparing the resolutions and merging approaches. (b) decomposing depth-guided semantic segmentation accuracy for different depth ranges. All metrics in %. (a) Depth-guided semantic pseudo labeling components. Method Low Resolution (Plow) High Resolution (Phigh) Mean Depth-guided (P) PQ SQ RQ 15.9 17.9 16.7 18.1 47.0 46.8 42.7 47. 19.5 22.4 20.9 22.6 (b) Analyzing different depth ranges. Distance (in m) Low Resolution (Plow) High Resolution (Phigh) Depth-guided (P) mIoU19 10 30 > 28.7 29.6 29.7 23.3 27.3 27.3 all 29.5 30. 31.1 0 10 30.7 28.6 29.2 Tab. 9 shows the direct quantitative evaluation of pseudo labels generated using different motion estimation methods against the ground truth (i.e., without the panoptic segmentation network). Alongside another unsupervised approach, BrightFlow [88], we include results obtained with supervised methods, RAFT-large [71] (a supervised analog of SMURF [66]) and SEA-RAFT-large [93]. We observe rather consistent panoptic quality of the pseudo labels across different motion estimation networks. As expected, the more accurate supervised optical flow methods can improve PQ further. The slightly weaker panoptic quality with SEA-RAFT compared to RAFT might be due to SEA-RAFT being fine-tuned on multiple diverse datasets, whereas RAFT is fine-tuned specifically on KITTI. To conclude, CUPS is already effective with unsupervised flow and depth estimation methods, while exhibiting notable Table 11. Instance pseudo label comparison. Using MaskCut instance masks (U2Seg [55]) in our CUPS pseudo-label generation. We compare using PQ, SQ, and RQ (in %, ) for pseudo labels generated on Cityscapes val. Instance pseudo-label approach MaskCut [78] SF2SE3-ensembling (Ours) PQ 9.9 18.1 SQ 41.6 47. RQ 12.4 22.6 margin for improvement in settings where some supervision of optical flow is available (and permissible). Analysis of depth-guided semantic pseudo labeling. Following Sec. 3.1, we aim to analyze our proposed depthguided semantic pseudo labeling in more detail. Table 10 shows that depth guidance fuses lowand high-resolution semantic predictions more effectively than an arithmetic mean. We use the identical experimental setting as in Tab. 5. We further analyze pseudo labels by splitting images into depth ranges. Low resolution is best for pixels closer than 10 m, both predictions perform similarly between 10 30 m, and high resolution is superior beyond 30 m. These effects stem from DINO features trained on fixed-resolution, object-centric images, causing reduced representational quality at extreme scales. In short, lowresolution predictions produce blurry outputs for distant fine details, while high-resolution (sliding-window) predictions are more accurate at large distances but introduce errors near the camera. Overall, our depth-guided fusion yields the best metric performance. We show qualitative examples in Fig. 6. Instance pseudo labeling analysis. Supporting the qualitative results presented in Fig. 2, we further analyze the performance of our SF2SE3-ensembling approach against MaskCut [78]. In particular, Tab. 11 presents pseudo-label evaluation results, replacing our SF2SE3-ensembling with iv Table 12. Per-class unsupervised panoptic segmentation on Cityscapes. Comparing CUPS to existing unsupervised panoptic methods, using PQ at the class level, as well as the mean PQ (in %, ). Method Supervised [39] w i d u o a e F P i fi T S fi T t g i e n e S R C r s i e c o ) ( M c B 96.5 72.4 85.9 16.4 30.1 48.6 48.8 67.2 86.9 34.8 86.0 65.0 60.8 79.2 58.5 77.2 59.8 54.9 59.3 62.3 DepthG [62] + CutLER [78] U2Seg [55] CUPS (Ours) 80.9 82.5 1.4 55.6 0.0 42. 85.8 6.0 64.4 3.0 2.0 0.0 0.2 0.0 0.4 0. 0.3 0.0 0.0 72.9 0.0 76.6 5.8 61.8 1.5 62.9 6.0 8.3 0.0 17.5 0.7 2.2 22.3 10.2 27.0 0. 0.2 12.4 6.2 32.1 83.7 17.1 78.2 39.1 0.0 62.9 16.3 1.2 0.0 4.7 0. 0.0 0.7 0.0 16.1 6.7 18.4 0.0 30.6 27.8 Table 13. CUPS self-training analysis. Decomposing the selftraining by analyzing the augmentation quality using PQ, SQ, and RQ (in %, ) on Cityscapes val. Table 14. DepthG [62] + VideoCutLER [94] baseline. We compare CUPS to baseline using VideoCutLER on the Cityscapes val dataset (all metrics in %, ). Training configuration CUPS w/o self-training CUPS w/o self-training + TTA CUPS (Ours) PQ 26.6 27.4 27.8 SQ RQ Runtime (ms) Method 57.5 57.2 57.4 33.5 34.9 35.2 65.9 413.4 65.2 DepthG [62] + CutLER [78] DepthG [62] + VideoCutLER [94] CUPS (Ours) PQ 16.1 16.6 27. SQ 45.4 42.6 57.4 RQ 21.1 20.5 35.2 MaskCut. All other pseudo-label generation components are kept the same. MaskCut fails to generate high-quality instance masks on scene-centric images, as PQ and RQ almost halved compared to our SF2SE3-ensembling. C.2. Unsupervised panoptic segmentation results Class-level PQ. Table 12 expands Tab. 1 by detailing class-wise PQ. CUPS demonstrates substantial improvements on most categories, particularly excelling on Car (62.9 %), Person (39.1 %), Traffic Sign (32.1 %), and Sky (78.2 %). Although CUPS has difficulties with few classes, e.g., Wall, Fence, and Rider, our baseline DepthG [62] + CutLER [78] and U2Seg [55] also struggle with segmenting these classes. The only exception is Bus, on which CUPS exhibits lower PQ than U2Seg. In the case of Rider, CUPS does not learn this as separate class, which is probably due to the motion cue used for instance pseudo labeling, which cannot easily separate Rider from their means of transportation. Accordingly, CUPS usually predicts person instead of rider or the entire unit of Bicycle and Rider is predicted as Bicycle (cf . Fig. 7a, second example). Nevertheless, CUPS significantly improves the panoptic quality for the majority of classes and narrows the gap to the supervised upper bound. Panoptic self-training vs. test-time augmentation. Following up on the ablation in Tab. 7a, we provide finergrained analysis of the self-training process in Tab. 13 by comparing against using the self-labeling augmentations as test-time augmentation (TTA) at inference time directly after Stage 2 instead of the self-training. Recall that the self-labeling augmentations involve resizing the input image to three different scales and applying horizontal flipping, followed by aggregating the predictions. Selflabeling augmentations, combined with confidence thresholding and self-enhanced copy-paste augmentations, provide self-labels for self-training (Stage 3). Note that we report TTA without thresholding in Tab. 13. While the results in Tab. 13 show that TTA improves the panoptic quality, it is not practical approach due to the significantly increased inference time. By contrast, panoptic self-training retains the original runtime of the network and even surpasses TTA in panoptic quality. DepthG+VideoCutLER baseline. Since CUPS leverages two consecutive frames to generate instance pseudo labels, it inherently exploits temporal consistency. Consequently, we combine VideoCutLER [94], an unsupervised method for video instance segmentation, with DepthG as an additional baseline. We performed the experiment using five consecutive frames as the video input to VideoCutLER. The semantic and instance predictions of DepthG and VideoCutLER are combined identically to the DepthG+CutLER baseline. As shown in Tab. 14, DepthG+VideoCutLER is slightly worse for SQ and RQ, yet better in PQ. We attribute this to the improved temporal consistency. Our CUPS approach strongly outperforms this video baseline as well. Overclustering analysis. Overclustering refers to setting the number of pseudo labels significantly higher than the number of ground-truth categories. Extending the analysis presented in Tab. 7b, we analyze the impact of overclustering along two dimensions. First, we test CUPS with an increased number of pseudo classes. Second, we run CUPS in the default setting, but evaluate it on the group-level class hierarchies defined by Cityscapes. Here, the 19-class taxonomy is mapped down to 7 broader groups of classes. Table 15. Unsupervised panoptic segmentation for CUPS on Cityscapes, KITTI, BDD, MUSES, Waymo, and MOTS. Comparing CUPS to existing unsupervised panoptic methods, using PQ, SQ, and RQ (in %, ) for different numbers of pseudo classes. By default, CUPS uses 27 pseudo classes to facilitate the comparison against both unsupervised panoptic and unsupervised semantic segmentation approaches. We also test 40 (150 % of the default) and 54 pseudo classes (200 % of the default), showcasing the impact of overclustering. Method Pseudo classes Cityscapes KITTI BDD MUSES Waymo MOTS PQ SQ RQ PQ SQ RQ PQ SQ RQ PQ SQ RQ PQ SQ RQ PQ SQ RQ Supervised [39] 62.3 81.8 75.1 31.9 71.7 40.4 33.0 76.3 42.0 38.1 62.4 49.6 31.5 70.1 40.9 73.8 86.4 84.6 DepthG [62] + CutLER [78] U2Seg [55] 27 800 + 27 16.1 45.4 21.1 11.0 34.5 13.8 14.4 41.9 19.2 10.1 30.1 13.1 13.4 37.3 17.0 49.6 78.4 60.6 18.4 55.8 22.7 20.6 52.9 25.2 15.8 57.2 19.2 20.3 45.8 26.5 19.8 50.8 23.4 50.7 79.2 64.3 CUPS (Ours) CUPS (Ours) CUPS (Ours) 27 (default) 40 27.8 57.4 35.2 25.5 58.1 32.5 19.9 60.3 25.9 24.4 48.5 33.0 26.4 60.3 33.0 67.8 86.4 76.9 30.3 64.3 37.5 28.1 63.1 35.3 21.9 57.3 28.1 28.2 52.9 35.4 27.2 62.4 33.6 74.0 88.4 82.8 30.6 65.1 37.8 28.5 60.6 36.0 21.8 62.5 27.6 22.8 45.4 29.3 27.3 65.3 32.5 78.7 89.3 87.4 Table 16. Hierarchical unsupervised panoptic segmentation on Cityscapes. Comparing CUPS to existing unsupervised panoptic methods, using PQ (all in %, ) on different class hierarchies. All datasets are analyzed on 19 and 7 ground truth classes. The number of ground-truth classes is indicated by the superscript of the metric. Method Supervised [39] DepthG [62] + CutLER [78] U2Seg [55] CUPS (Ours) Pseudo classes 27 800 + 27 27 Cityscapes KITTI BDD MUSES Waymo PQ19 62.3 16.1 18. 27.8 PQ7 79.8 44.1 43.5 63.9 PQ 31.9 10.9 20.6 25.4 PQ7 57.9 27.6 44. 57.4 PQ19 33.0 14.4 15.8 19.9 PQ 54.6 38.5 37.3 49.3 PQ19 38.1 10.1 20. 24.4 PQ7 69.4 22.1 41.4 53.5 PQ 31.5 13.4 19.8 26.4 PQ7 62.3 37.7 39. 54.7 When training CUPS with larger number of pseudo classesspecifically, 40 (150 % of the default number of pseudo classes)we observe significant improvement in the panoptic segmentation metrics (cf . Tab. 15). Further increasing the number of pseudo classes to 54 (200 % of the default number of pseudo classes) yields additional improvements but also exhibits saturation trend. However, significantly increasing the number of pseudo-classes can impede generalization, as visible on MUSES when using 54 pseudo classes. In general, we use 27 pseudo classes for fair comparison, as it is the lowest number of pseudo classes that allows for comparison to both unsupervised panoptic and unsupervised semantic segmentation. In Tab. 16, we evaluate CUPS on different class hierarchies. While the main paper demonstrates substantial gains in the standard 19-class evaluation, we show that the gains extrapolate to the setting with coarser grouping of 7 Cityscapes classes: Flat (e.g., Road, Sidewalk), Human (e.g., Person, Rider), Vehicle (e.g., Car, Truck), Construction (e.g., Building, Wall), Object (e.g., Pole, Traffic Sign), Nature (e.g., Vegetation, Terrain), and Sky. Although the accuracy improvement on the coarser label set is expected, this experiment empirically demonstrates that our analysis and conclusions hold for different granularities of the semantic taxonomy. As another remark, we follow up on our observation from Tab. 2 in the main text, where the supervised model (trained on Cityscapes) suffers noticeable drop in segmentation performance outside the training domain. In the Table 17. Panoptic segmentation architecture analysis. We evaluate CUPS after stage-1 training on the Cityscapes val datasets (all metrics in %, ). Segmentation model Mask2Former [18] Panoptic Cascade Mask R-CNN [8, 39] PQ 25.1 26.6 SQ 57.7 57.5 RQ 31.7 33. coarser setting here, this observation applies to more striking extent: CUPS nearly approaches the supervised bound and achieves competitive panoptic quality with the supervised model (e.g., only 0.3 % worse on KITTI). Analysis of panoptic segmentation architecture. Our method does not make particular assumptions regarding In printhe downstream panoptic segmentation model. ciple, CUPS can be applied to various panoptic segmentation architectures without significant changes; hyperparameter tuning may be required for optimal accuracy. As preliminary experiment, we perform stage1 training (i.e., only pseudo-label training) of CUPS using the Mask2Former [18] architecture and observe comparable panoptic segmentation accuracy relative to the Panoptic Cascade Mask R-CNN baseline. Specifically, Mask2Former achieves slightly inferior RQ but marginally superior SQ, resulting in an overall lower PQ. We attribute this weaker recognition performance to architectural differences: Mask2Former jointly predicts semantic and instance labels per mask, whereas Panoptic Mask R-CNN separates these tasks into two branches, facilitating more effective In particular, Mask2Former application of the drop loss. vi Figure 7. Qualitative unsupervised panoptic segmentation examples across all datasets after Hungarian matching. We compare CUPS (Ours) to the DepthG+CutLER baseline and U2Seg. CUPS produces more consistent and accurate panoptic segmentations."
        },
        {
            "title": "Baseline",
            "content": "U2Seg [55] CUPS (Ours) Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle (a) Cityscapes Qualitative unsupervised panoptic segmentation examples."
        },
        {
            "title": "Baseline",
            "content": "U2Seg [55] CUPS (Ours) Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle (b) KITTI Qualitative unsupervised panoptic segmentation examples."
        },
        {
            "title": "Baseline",
            "content": "U2Seg [55] CUPS (Ours) Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle (c) BDD Qualitative unsupervised panoptic segmentation examples. vii"
        },
        {
            "title": "Baseline",
            "content": "U2Seg [55] CUPS (Ours) Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle (d) MUSES Qualitative unsupervised panoptic segmentation examples."
        },
        {
            "title": "Baseline",
            "content": "U2Seg [55] CUPS (Ours) Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle (e) Waymo Qualitative unsupervised panoptic segmentation examples."
        },
        {
            "title": "Baseline",
            "content": "U2Seg [55] CUPS (Ours) Background Person (f) MOTS Qualitative unsupervised panoptic segmentation examples. viii Road Sidewalk Building Wall Fence Pole Traffic Light Traffic Sign Vegetation Terrain Sky Person Rider Car Truck Bus Train Motorcycle Bicycle Figure 8. Qualitative OOD examples for CUPS on ImageNet val. Applying the pseudo class to ground-truth matching of CUPS from Cityscapes for visualization purposes. applies the drop loss to both thing and stuff predictions, while Panoptic Cascade Mask R-CNN only drops thing masks. Our findings indicate that prior work [79] has only partially addressed the application of the drop loss to Mask2Former, thus limiting the effectiveness of the drop loss. While initial results appear promising, further investigation is necessary. D. Qualitative Results We show qualitative comparison of CUPS to the DepthG+CutLER baseline and U2Seg [55] across all datasets in Fig. 7. On Cityscapes (cf . Fig. 7a), we observe significant qualitative difference to U2Seg. We attribute this improvement to the quality of our pseudo labels, which enable predicting small instances in the background. Despite some errors, such as the Fence being incorrectly predicted in small regions of the building in the upper image, CUPS identifies substantially more classes and provides more precise instance segmentation compared to both the baseline and U2Seg. On KITTI (cf . Fig. 7b), we observe similar trend. CUPS detects and segments more objects, offering finer-grained panoptic segmentation compared to U2Seg, which tends to merge overlapping objects. For instance, in the upper example, the parked cars are incorrectly merged into single mask by U2Seg, while CUPS successfully separates them. On the BDD dataset (cf . Fig. 7c), the impact of the domain shift is evident across all methods. CUPS exhibits minor artifacts, such as predictions related to parts of the ego vehicle or dirt on the windshield. Additionally, signs on buildings are occasionally misclassified as traffic signs. In contrast, U2Seg often produces large, erroneous masks that span across the image, resembling the MaskCut artifacts in Fig. 2. Similarly, for MUSES and Waymo (cf . Figs. 7d and 7e), all methods are somewhat affected by the domain shift and challenging viewing conditions. However, CUPS consistently detects instances compared to both other approaches. For the upper Waymo example, one can observe an occasional artifact for CUPS. For example, it incorrectly classifies the shadows forming underneath the vehicles in sunny weather conditions. This is result of the instance cue being derived from unsupervised flow, which can introduce artifacts due to the apparent motion. MOTS (cf . Fig. 7f) is challenging for all approaches. Nonetheless, CUPS produces accurate predictions with fewer artifacts compared to both the baseline and U2Seg, showcasing its robustness even in complex scenarios. Overall, CUPS predicts less noisy and more accurate semantics, aligning well with the image while predicting significantly more and better instance masks. This observation is in line with our quantitative experiments (cf . Sec. 4). Additionally, we run CUPS and U2Seg on demo (validation) video sequence from Cityscapes (cf . https:// visinf.github.io/cups). For this analysis, we process each individual frame independently using the respective method and concatenate the outputs into video, as both methods are designed for per-frame processing. On this sequence, CUPS is qualitatively superior to U2Seg. Results on object-centric images. To further evaluate the generalization capabilities of our approach, we tested CUPS on randomly selected out-of-domain images, sampled from ImageNet [91]. Qualitative results, shown in Fig. 8, demonstrate that CUPS effectively generalizes to novel domains, viewpoints, and object categories. We find that objects such as tractors, forklifts, and airplanes are classified as cars, which is reasonable given the classes available in Cityscapes. Additionally, objects and surroundings in diverse scenarios are accurately segmented. For instance, despite never encountering racing car on mountain road during training, CUPS provides contextually appropriate and coherent segmentation, further highlighting the robustness of our method. ix tation by contrasting object mask proposals. In ICCV, pages 1005210062, 2021. ii [93] Yihan Wang, Lahav Lipson, Jia Deng. SEA-RAFT: Simple, efficient, accurate RAFT for optical flow. In ECCV, pages 3654, 2024. iii, iv [94] Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, Trevor Darrell. VideoCutLER: Surprisingly simple unsupervised video instance segmentation. In CVPR, pages 22755 22764, 2024. [95] Sungmin Woo, Wonjoon Lee, Woo Jin Kim, Dogyoon Lee, Sangyoun Lee. ProDepth: Boosting self-supervised multiframe monocular depth with probabilistic fusion. In ECCV, pages 201217, 2024. E. Limitations and Future Work CUPS utilizes stereo videos to extract depth cues for pseudo labeling of complex scenes. Although stereo videos are widely available and are inexpensive to record, overcoming the need for the stereo setup could further broaden the application spectrum. Future work could explore replacing the stereo input with state-of-the-art self-supervised monocular depth estimation method, such as ProDepth [95]. The evaluation of CUPS has been also largely constrained to driving datasets. This is due to the wide availability of panoptic annotation specifically for this domain. Nevertheless, we believe that CUPS has the potential for applications beyond traffic scenarios, as it relies on domainagnostic cues, such as depth and motion as well as generalpurpose visual representations. U2Seg and CUPS approach the task of unsupervised panoptic segmentation from two distinct perspectives: object-centric and scene-centric training data. Combining the strengths of both methods could open promising avenue for future research, offering more comprehensive solution to the challenges of unsupervised panoptic scene understanding. An additional direction for future work could scale such an approach by exploring more advanced panoptic segmentation networks, such as Mask2Former [18], and increasing the amount of training data."
        },
        {
            "title": "References",
            "content": "[87] William A. Falcon, The PyTorch Lightning team. PyTorch Lightning. GitHub, (2019). [88] Rémi Marsal, Florian Chabot, Angélique Loesch, Hichem Sahbi. BrightFlow: Brightness-change-aware unsupervised learning of optical flow. In WACV, pages 20602069, 2023. iii, iv [89] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 80248035, 2019. [90] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Bradski Ethan, Gary Bradski. Kornia: An open source differentiable computer vision library for PyTorch. In WACV, pages 8024 8035, 2020. [91] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis., 115(13):211252, 2015. ii, ix [92] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Luc Van Gool. Unsupervised semantic segmenx"
        }
    ],
    "affiliations": [
        "ELIZA",
        "MCML",
        "TU Darmstadt",
        "TU Munich",
        "University of Oxford",
        "hessian.AI"
    ]
}