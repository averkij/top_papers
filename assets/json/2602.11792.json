{
    "paper_title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
    "authors": [
        "Hongbo Zhang",
        "Yue Yang",
        "Jianhao Yan",
        "Guangsheng Bao",
        "Yue Zhang",
        "Yue Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 2 9 7 1 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
            "content": "Hongbo Zhang 1 2 * Yang Yue 3 Jianhao Yan 2 Guangsheng Bao 2 Yue Zhang Yue Zhang 2 Abstract Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR finetunes models based on reward feedback from selfgenerated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-kNN Distance, simple black-box detector that quantifies this collapse by sampling multiple completions for given prompt and computing the average of the smallest nearest-neighbor edit distances. MinkNN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-kNN Distance reliably distinguishes RLseen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines. The project page is available at https://stevenzhb.github. io/detect-rlvr-data/. 1. Introduction Recent advances in large language models (LLMs) have demonstrated remarkable improvements in reasoning performance, particularly on benchmarks with verifiable answers such as mathematics (Guo et al., 2025; Zeng et al., 2025; Team et al., 2025; He et al., 2025), coding (Seed et al., 2025; Liu et al., 2025), and symbolic problem solving (Xie et al., 2025). key driver behind these gains is reinforcement * Work done during an internship at Kuaishou Technology. The fifth author is Yue Zhang (章岳) and the last author is Yue Zhang (张岳). 1Zhejiang University 2School of Engineering, Westlake University 3Kuaishou Technology. Correspondence to: Yue Zhang <zhangyue@westlake.edu.cn>. Preprint. February 13, 2026. 1 learning with verifiable rewards (RLVR), in which models are trained on problems with automatically checkable outcomes, most commonly math and code tasks, and are optimized to produce correct final answers. By directly reinforcing reasoning trajectories that lead to correct solutions, RLVR enables models to refine their chain-of-thought generation beyond supervised fine-tuning. As result, RLVR has become cornerstone of modern post-training pipelines and now underpins many state-of-the-art reasoning models. Despite impressive benchmark results, growing body of evidence suggests that high reported reasoning performance does not always translate to robust generalization. When evaluation shifts to temporally newer, less exposed, or expert-level problems, model performance often drops substantially (Petrov et al., 2025; Glazer et al., 2024; Mahdavi et al., 2025). In practice, models that solve standard benchmark questions reliably may fail on newly released problems, alternative formulations of known tasks, or slightly more complex variants that require the same underlying reasoning. These patterns raise concerns that some gains reflect over-specialization during post-training, rather than consistently robust reasoning ability. This issue is exacerbated by current open-source practices around RLVR. Many recent releases provide only RLVR-tuned models, without access to the base checkpoints or RL training data, making it difficult to assess whether benchmark problems or close paraphrases were encountered during training. As result, risks of benchmark contamination and inflated generalization claims increase, while practitioners lack reliable tools to detect overlap with prior RLVR exposure. These challenges highlight the need for methods that can identify whether specific example has appeared during RLVR training. Prior work has shown that detecting training data exposure during pretraining or supervised fine-tuning is often feasible, as memorization under likelihood-based objectives leaves strong statistical traces detectable even in black-box settings (Shi et al., 2023; Zhang et al., 2024; Xie et al., 2024; Dong et al., 2024). However, detecting exposure during RLVR training presents different challenge. Unlike likelihood-based training, RLVR optimizes models through reward feedback on self-generated reasoning trajectories, which makes conventional token-level or likelihood-based signals ineffective. As demonstrated in our experiments (Section 5), traditional detection methods, which rely on perDetecting RLVR Training Data Figure 1. We study how RLVR induces structural convergence in reasoning trajectories. Left: RLVR compresses diverse reasoning paths into shared structural modes. Middle: multiple completions cluster into few reasoning structures. Right: Min-kNN Distance quantifies this collapse via nearest-neighbor edit distances, yielding low values for seen prompts and high values for unseen prompts. plexity and token-level statistics, fail to capture the changes in reasoning patterns induced by RLVR. To understand how RLVR reshapes model behavior, we analyze the evolution of generation diversity during training. As shown in Fig. 1, RLVR induces systematic convergence in reasoning trajectories: prompts seen during RL training yield increasingly similar generations, while unseen prompts retain high variability. This collapse concentrates on symbolic and algebraic reasoning components and compresses outputs into small number of recurring structural modes rather than single derivation. These stable structural patterns form distinctive behavioral signature of RLVR exposure. Building on this observation, we introduce Min-kNN Distance, simple black-box method for detecting whether an example has appeared during RLVR training. As illustrated in Fig. 1, given prompt example, Min-kNN Distance samples multiple completions from the RLVR-tuned model and quantifies their structural diversity by computing edit distances among generated outputs. Since RLVR induces convergence in reasoning diversity for seen examples, prompts that appeared during RLVR training yield consistently smaller Min-kNN Distance values than unseen prompts. The method requires only sampling access to the RLVR-tuned model and does not rely on token log probabilities or any reference models. Extensive experiments across multiple reasoning models and RLVR setups demonstrate that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones. Our contributions are as follows: We provide the first systematic analysis of how RLVR reshapes reasoning behavior, revealing convergence in the structural space of reasoning trajectories, particularly in symbolic reasoning components. Based on these findings, we introduce Min-kNN Distance, simple, black-box method for detecting RLVR exposure without requiring access to the token log probabilities. We conduct extensive experiments across diverse reasoning models, RL algorithms, and training setups, showing that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones, even under challenging conditions like paraphrasing, distillation, and varying decoding configurations. 2. Preliminary Problem Definition: We study the problem of detecting whether given example has appeared during the RLVR training stage of reasoning language model. Formally, given an RLVR-tuned model MRLVR trained on an (unknown) RL dataset DRL, the goal is to learn detector that determines whether an arbitrary query input belongs to DRL. The detector predicts membership as h(x, MRLVR) {0, 1}. This formulation follows the standard membership inference attack (MIA) setting, but differs from pretraining and supervised fine-tuning scenarios due to the reward-driven and self-generative nature of RLVR. RLVR Training Process: During RLVR training, each example consists of reasoning-intensive prompt paired with verifiable answer a. The model first generates CoT and final answer ˆa sampled from its policy πθ, that is (c, ˆa) πθ( x). The output of the model, denoted as o, is the combination of the CoT and the final answer: = (c, ˆa). 2 Detecting RLVR Training Data (a) EAD (b) NLI (c) Embedding Figure 2. Evolution of generation diversity during RLVR training under DAPO and GRPO, measured in per-input setting by three complementary metrics capturing lexical (EAD), logical (NLI), and semantic (embedding) diversity. reward r(ˆa, a) is computed based on the scalar correctness of The model the predicted answer. parameters are updated to increase the expected reward J(θ) = E(c,ˆa)πθ(x)[ r(ˆa, a) ], and typical policy gradient update takes the form θJ(θ) = (cid:2) r(ˆa, a) θ log πθ(c, ˆa x)(cid:3). E(c,ˆa)πθ(x) This optimization acts on the distribution of generated chains of thought rather than on the prompt itself, and no golden chain of thought is available for likelihood comparison. As result, RLVR repeatedly adjusts the probability of reasoning trajectories according to their observed rewards. Unlike likelihood-based pretraining, RLVR optimizes models through reward feedback on self-generated reasoning trajectories, rather than optimizing on golden trajectories, making conventional token-level signals ineffective. This motivates closer examination of how RLVR affects reasoning diversity. 3. Analyzing Reasoning Pattern under RLVR Previous work has demonstrated that RLVR models tend to reduce the coverage of reasoning, leading to narrower reasoning trajectories compared to their base models (Gandhi et al., 2025; Yue et al., 2025). While these studies highlight the loss of reasoning diversity, they primarily focus on performance metrics rather than directly analyzing the changes in output structure. Notably, there has no systematic study quantifying how reasoning diversity evolves through the RLVR training process, especially for seen and unseen samples. To fill this gap, we focus on analyzing the dynamics of reasoning diversity in the outputs of RLVR models. We investigate how the diversity of reasoning trajectories decreases during RLVR training and explore how these changes can serve as potential signal for detecting whether specific example has been included in the training data. Specifically, we train Qwen-2.5-7B-Base model (Qwen et al., 2024) using two representative RLVR algorithms, DAPO (Yu et al., 2025) and GRPO (Shao et al., 2024) (Details in App. A.1). The DAPO algorithm employs clip-higher strategy to explicitly encourage exploration during training. We conduct RLVR training on the DAPO dataset (Yu et al., 2025) for 200 rollout steps and analyze multiple intermediate checkpoints. RLVR Induces Generation Rigidity. To quantify the impact of RLVR on output diversity, we use three complementary metrics, as proposed by Kirk et al. (2023): lexical, logical, and semantic diversity. Lexical diversity is measured using expectation-adjusted distinct n-grams (EAD) (Liu et al., 2022), which estimates the proportion of distinct n-grams while correcting for length bias, averaged over = {1, 2, 3, 4, 5}. In particular, for given prompt, we calculate the distinct n-grams from completions and normalize them by the expected number of n-grams. Logical diversity is assessed using an NLI-based metric, which measures the proportion of entailment and contradiction relations across pairs of randomly selected completions. This allows us to quantify how much the models reasoning varies in terms of logical consistency. Finally, semantic diversity is evaluated through sentence-embedding diversity, defined as one minus the average pairwise cosine similarity between the embeddings of the generated outputs. Together, these metrics capture lexical, logical, and semantic diversity, respectively, providing comprehensive view of how RLVR affects different aspects of generation behavior. For each checkpoint, we sample 32 completions for 300 training prompts and compute the per-input diversity across all three metrics. As shown in Fig. 2a, we observe steady decline in all three metrics throughout RLVR training under both DAPO and GRPO. This consistent decrease suggests that the space of reasoning trajectories conditioned on fixed prompt narrows as the model undergoes RLVR, indicating that RLVR systematically leads to converged reasoning trajectories in lexical, logical, and semantic dimensions. We further analyze the diversity in cross-input setting, where we assess the diversity across completions from all 300 prompts (Fig. 8 in App. A.2). We observe similar declining trend in lexical diversity, further supporting the notion that RLVR reduces output diversity not only at the level of individual prompts but also across the global out3 Detecting RLVR Training Data Figure 3. Rigid 3-gram category counts in GRPO training checkpoints. GPT-4o labeled rigid 3-grams into four categories. Figure 4. Heatmap of rigid 3-grams that appear in at least three completions for single prompt. The clustering is achieved via hierarchical agglomerative clustering. put distribution. This global reduction in diversity reflects the collapse of the reasoning space, concentrating it into smaller set of recurring patterns, leading to more rigid outputs as training progresses. RLVR Converges Symbolic Reasoning Segments. To understand which parts of the completions are most affected by RLVR training, we inspect 3-grams that appear in at least half of the completions for given prompt. Through this analysis, we identify several repeated fragments can be categorized into three main types. The first type is restatements of the problem, where the model habitually repeats the problem statement in the early part of the output. The second type is boilerplate connective phrases, such as To solve this problem... or Lets denote the.... These phrases are structural fillers that do not contribute to the core reasoning but serve to connect the reasoning steps. The third type, symbolic or algebraic logic steps, represents the core reasoning components, typically involving algebraic manipulations, standardized mathematical transformations, or function definitions. For example, model might express equations like = + 2, apply mathematical laws, or define functions like f(x) = xˆ2 + 3x, all of which involve relatively fixed structural forms that allow little variation during reasoning. These segments play critical role in the core of the models reasoning and tend to become more standardized as RLVR training progresses. For example, Fig. 9 in App. A.4 shows representative case of these categories. We extend this observation by sampling 50 training prompts at each checkpoint and extracting the high-frequency 3grams. These 3-grams are then categorized with GPT4o(prompt is shown in App. A.3), allowing any unclassifiable 3-grams to be marked as other. The results are plotted for each checkpoint, showing the number of recurring 3-grams. As shown in Fig. 3, we observe that symbolic logic fragments increase rapidly over the course of RLVR training, while restatement and boilerplate patterns exhibit slower growth. This indicates that RLVR significantly compresses the symbolic reasoning components, especially the logical tokens, which carry the models reasoning structure. Figure 5. Distribution of the number of reasoning structure clusters across 50 training prompts. As these segments become more rigid, they form core set of standardized reasoning steps, leaving less variability in the models outputs. Thus, we conclude that RLVR primarily causes the collapse of symbolic reasoning segments. RLVR Converges Reasoning into Limited Set of Structural Modes. To explore how RLVR affects reasoning trajectories, we analyze whether it leads to single deterministic path or multiple recurring structural modes. As shown in Fig. 4, we observe that logical 3-grams exhibit clustering patterns and often co-occur with boilerplate phrases. Restatements of the problem appear across all completions. Fig. 1 (middle) provides qualitative example, where completions are grouped into distinct reasoning structures. Each structure reflects similar solution path, showing the models tendency to collapse into limited number of reasoning modes during RLVR training. To quantify these patterns, we apply hierarchical agglomerative clustering (Murtagh & Contreras, 2012) to the logic n-grams extracted from 50 training prompts and analyze the diversity of the reasoning structures across different RLVR checkpoints. The histogram in Fig. 5 shows that most prompts exhibit between two and four stable clusters of reasoning structures, with few prompts showing even more. This result indicates that RLVR compresses reasoning into limited number of tightly concentrated structural Detecting RLVR Training Data Table 1. Comparison of reasoning structure collapse between seen and unseen data. The first part shows the distribution of rigid 3-gram categories, while the second part displays the cumulative distribution of the number of reasoning structure clusters across different stages of RLVR training. Rigid Logic 3-gram Count Step Seen Unseen 856 840 100 2,240 1,684 150 3,817 2,639 4,673 3,476 Cumulative Distribution of Reasoning Structure Clusters Cluster Size 2 4 16 Seen Unseen 44.90% 67.35% 95.92% 100.00% 22.45% 55.10% 89.80% 100.00% modes rather than single canonical trajectory. The clustering analysis further supports the idea that RLVR does not lead to single deterministic solution, but rather to small set of rigid patterns, highlighting the convergence of reasoning trajectories. Greater Rigidity in Seen Data Compared to Unseen Data. Finally, we compare the reasoning structure convergence between seen and unseen data. As shown in Tab. 1, both seen and unseen data exhibit reasoning convergence, but with notable differences. For the rigid 3-gram categories, the training set shows greater increase in symbolic reasoning steps, indicating more rigid model. In contrast, the unseen data exhibits smaller increase in rigidity, suggesting lower level of rigidity for unseen prompts. Regarding reasoning structure clusters, the cumulative distribution reveals that training prompts have higher proportion of clusters with fewer reasoning structures, while unseen prompts retain more diverse reasoning paths. This suggests that RLVR induces stronger rigidity for seen data, while unseen prompts maintain more variability in reasoning. 4. Min-kNN Distance: Simple Black-Box"
        },
        {
            "title": "Detector for RLVR Exposure",
            "content": "Previous analyses have shown that RLVR training leads to collapse in the structural components of CoT reasoning, with completions for seen prompts converging into few tight clusters. This characteristic provides basis for membership inference: completions for member prompts should exhibit stronger tendency to cluster together than those for non-member prompts. Building on this observation, we introduce Min-kNN Distance, simple black-box statistic that quantifies the degree of this clustering. Given prompt x, we sample completions from MRLVR O(x) = {o1, o2, . . . , om}, oi MRLVR( x), and compute the pairwise normalized edit distance between every pair: Dij = d(oi, oj), 1 i, m. where d(, ) denotes the normalized Levenshtein edit distance between two completions, defined as the minimum number of insertions, deletions, and substitutions required to transform one sequence into the other, normalized by the length of the longer sequence. For each completion oi, we define its nearestneighbor distance: NNi(x) = min j=i Dij. Let NN(1)(x) NN(2)(x) NN(m)(x), be the sorted list of nearest-neighbor distances. We take the smallest values to form set Min-k(x) and define the detection score: Min-kNN(x) = 1 (cid:88) t=1 NN(t)(x). (1) Because RLVR collapse forces completions of seen prompts into few compact structural modes, we expect Min-kNN(xseen) < Min-kNN(xunseen). detector is obtained by thresholding Min-kNN(x). The method requires only black-box sampling access to the RLVR-tuned model and does not use the token log probabilities, or reference models. 5. Experiments 5.1. Experimental Setup Models and Datasets To comprehensively evaluate the effectiveness of Min-kNN Distance , we consider two complementary evaluation settings: publicly released RLVRtrained models and controlled RL-MIA benchmarks. For Open-source RLVR models, we evaluate on several publicly released reasoning models that are available only after RLVR training, including SimpleRL-32B (Zeng et al., 2025), DAPO-Qwen-32B (Yu et al., 2025), JustRLDeepSeek-1.5B (He et al., 2025), and Open-Reasoner-Zero7B (Hu et al., 2025). For each model, we sample 300 examples from its RL training data as member (seen) instances. To create non-member (unseen) instances, we collect 300 reasoning problems that do not appear in the RL data of any evaluated model. These problems are sourced from AIME 2024 (30 problems), AIME 2025 (30 problems), Beyond-AIME (ByteDance-Seed, 2025) (100 problems), Omni-Math (Gao et al., 2024) (100 problems), and MATH500 (Wang et al., 2024) (40 problems). This balanced set provides clean and fair benchmark for detecting RLVR 5 Detecting RLVR Training Data Table 2. AUC results for RLVR data detection across RL-tuned models. BB indicates whether method operates with sampling-only access. DS-KK and DS-SAT denote DeepSeek-Math-7B-Instruct models RLVR-trained under the K&K and SAT settings, respectively; QW-KK and QW-SAT denote QW-2.5-7B-Instruct models RLVR-trained under the same settings. The best result is shown in bold and the second best is underlined. Open-source RLVR Models RL-MIA Models Method BB DAPO JustRL ORZ SimpleRL DS-KK QW-KK DS-SAT QW-SAT PPL (Shi et al., 2023) Min-K% (Shi et al., 2023) Min-K%++ (Zhang et al., 2024) CDD (Dong et al., 2024) Recall (Xie et al., 2024) Self-Critique (Tao et al., 2025) Min-kNN Distance 0.63 0.37 0.37 0.52 0.50 0. 0.72 0.61 0.43 0.48 0.51 0.52 0.51 0.66 0.72 0.29 0.56 0.36 0.51 0.48 0.75 0.72 0.28 0.39 0.45 0.62 0. 0.76 0.59 0.53 0.56 0.49 0.54 0.54 0.80 0.56 0.44 0.53 0.46 0.52 0.53 0.68 0.44 0.53 0.66 0.50 0.56 0. 0.68 0.54 0.47 0.52 0.47 0.47 0.49 0.57 Avg. 0.60 0.42 0.51 0.47 0.53 0.51 0.70 (+17%) data in real-world open-source models. RL-MIA benchmark models. Following the RL-MIA setup of (Tao et al., 2025), we include controlled contamination settings. Specifically, we train Qwen2.5-7B-Instruct and DeepSeek-Math7B-Instruct (Shao et al., 2024) using the SAT and K&K settings. In these settings, the provided training portions are treated as member data, and additional synthetic samples are constructed as non-member instances, following the original benchmark protocol. This setting allows us to evaluate detection performance under precisely controlled RL contamination scenarios. See App. for details. Baselines We compare Min-kNN Distance against several membership inference methods, most of which were originally proposed for pretraining data detection: (1) PPL, which uses perplexity as proxy for memorization; (2) MinK% Prob (Shi et al., 2023), which detects low-probability outlier tokens; (3) Min-K%++ (Zhang et al., 2024), an improved extension of Min-K%; (4) Recall (Xie et al., 2024), which measures likelihood shifts under unrelated prefixing; and (5) CDD (Dong et al., 2024), which compares the edit distance between stochastic generations and greedy sampling, checking for sharp peaks in the distance distribution. Finally, (6) Self-Critique (Tao et al., 2025) is specifically designed for RL data contamination detection. It compares token-level entropy sequences between models initial response and its self-critique, where high similarity in entropy is interpreted as evidence of contamination. Implementation and Evaluation Metrics For each prompt, Min-kNN Distance samples 32 completions from the RLVR-tuned model, computes all pairwise edit distances among the generated outputs, and averages the smallest distances to obtain the final score. Unless otherwise stated, we set = 10, which we find to be stable across models and training regimes. The baseline experimental setup follows the methodology in Tao et al. (2025). Other details are shown in App. B.3. 6 We evaluate detection performance using the Area Under the ROC Curve (AUC), threshold-independent metric widely adopted in membership inference and data contamination studies (Shi et al., 2023; Tao et al., 2025; Zhang et al., 2024). AUC measures the probability that the detector assigns higher score to member example than to non-member example, with 50% corresponding to random guessing. 5.2. Main Results Tab. 2 reports AUC performance across all models. We also conduct cost analysis in App. C. Min-kNN Distance consistently outperforms all baseline methods. Min-kNN Distance achieves the highest AUC across all evaluated models, with an average score of 0.70, which represents 17% relative improvement over the strongest baseline. In contrast, probability-based and consistency-based baselines often exhibit unstable or nearrandom performance. Min-kNN Distance is robust across different RLVR algorithms. Min-kNN Distance consistently performs well across models trained with different RLVR algorithms, including GRPO (SimpleRL-32B), DAPO (DAPO-Qwen32B), and PPO (Open-Reasoner-Zero-7B) (Schulman et al., 2017). This stability suggests that the signal detected by Min-kNN Distance is not dependent on the RL algorithm. Min-kNN Distance generalizes across model scales. From 1.5B to 32B parameters, Min-kNN Distance consistently maintains strong detection performance, despite significant variations in model size and training dynamics. Notably, Min-kNN Distance operates in fully black-box setting, consistently outperforming methods that require access to token probabilities or internal likelihoods. These results demonstrate that RLVR-induced structural collapse provides strong, model-agnostic signal for exposure detection. Detecting RLVR Training Data Table 4. Detection performance on distillation prompts (OpenR1Distill-7B). Method PPL Self-Critique Min-kNN Distance AUC 0.70 0.65 0.76 Figure 6. Ablation studies of Min-kNN Distance. (a) Effect of k. (b) Effect of the number of sampled completions. (c) Effect of sampling temperature. Table 3. Detection performance before and after paraphrasing RLVR training prompts under the DAPO setting. Method Original Paraphrased PPL Self-Critique Min-kNN Distance 0.63 0.58 0.72 0.66 0.57 0. 5.3. Ablation We analyze the sensitivity of Min-kNN Distance to its main hyperparameters. For and the number of sampled completions, we report mean AUC over eight RLVR-trained models, while temperature is varied on SimpleRL setting. Min-kNN Distance is robust to the choice of over wide range. As shown in the left panel of Fig. 6, performance improves quickly as increases from very small values and then saturates, with noticeable degradation only when is too small to reliably capture collapsed modes. Sampling more completions consistently improves detection performance. The middle panel of Fig. 6 demonstrates steady AUC gains as the number of sampled completions increases. This is because additional samples make collapsed structural patterns easier to expose, though the improvement plateaus after certain point. Higher decoding temperatures strengthen the detection signal. As illustrated in the right panel of Fig. 6, higher temperatures yield stronger detection performance, while lower temperatures reduce output variability and partially obscure structural collapse. We also observe that models trained with more extensive RLVR tend to require smaller to reach stable performance, whereas lower temperatures make Min-kNN Distance more sensitive to k. 5.4. Analysis Robustness to Paraphrased Prompts. In practice, detector should remain effective when training prompts are paraphrased rather than exactly repeated. We therefore evaluate the robustness of Min-kNN Distance under the DAPO-Qwen-32B setting by paraphrasing 300 RLVR trainTable 5. Detection performance on code and math data under ProRL setting. Method Math Code PPL Self-Critique Min-kNN Distance 0.40 0.75 0. 0.36 0.47 0.69 ing prompts using GPT-4o and testing detection performance on these paraphrased inputs. As shown in Tab. 3, Min-kNN Distance remains highly stable under paraphrasing, with AUC decreasing only slightly from 0.72 to 0.71, and other baselines exhibit similar changes. This indicates that the signal exploited by Min-kNN Distance is not tied to surface-level prompt forms, but instead reflects structural collapse in reasoning induced by RLVR that generalizes across semantically equivalent prompts. Detecting Distillation Prompts. Recent reasoning models are often distilled from RLVR-tuned models, where the RLVR-trained teacher model generates multiple reasoning trajectories, and the student is trained on these outputs. We analyze whether prompts used during distillation can be detected. We conduct this analysis on OpenR1-Distill7B (Hugging Face, 2025). We evaluate detection performance on two sets of prompts: 300 held-out evaluation problems and 300 prompts used during the distillation process. Using the same detection setup as in our main experiments, Min-kNN Distance achieves an AUC of 0.76 on distillation prompts, outperforming other baselines (Tab. 4). These results suggest that the structural collapse induced by RLVR is partially transferred to the distilled model, allowing Min-kNN Distance to detect prompts used during distillation. Robustness across Different RL Data Sources. We further analyze the robustness of Min-kNN Distance across different types of RLVR training data. We conduct this analysis on Nemotron-Research-Reasoning-Qwen-1.5B (Liu et al., 2025), which is trained with mixture of math, coding data during RLVR. From its RL training set, we sample 100 prompts from math and 100 from code as member examples. To construct unseen data, we use 100 problems from Beyond-AIME for math and 100 validation examples from Eurus Coding (Cui et al., 2025) for code. As shown 7 Detecting RLVR Training Data 6. Related Work Detect Data Exposure in Language Models. growing body of work investigates whether large language models have been trained on specific data, driven by concerns about benchmark contamination, privacy leakage, and evaluation reliability. Most existing methods focus on pretraining or supervised fine-tuning, where data exposure shows up as statistical signals like low perplexity or probability outliers (Shi et al., 2023; Dong et al., 2024; Zhang et al., 2024; Xie et al., 2024; Carlini et al., 2021; Mireshghallah et al., 2022; Fu et al., 2023). Reinforcement learning post-training introduces different challenge. In RLVR, models are optimized via reward feedback on self-generated reasoning, making likelihood-based methods less effective. Tao et al. (2025) proposed self-critique to detect RL data contamination by identifying entropy collapse. Our work complements this by showing that RLVR induces collapse in reasoning trajectory diversity for seen prompts, using structural similarity across multiple generations for RLVR data exposure detection, with fully black-box approach. Effects of Reinforcement Learning on Generation Diversity. Reinforcement learning with preference or alignment objectives consistently reduces generation diversity compared to supervised fine-tuning or non-preference methods. This effect has been observed across lexical, syntactic, semantic, and conceptual dimensions, indicating compression of the models output space (Kirk et al., 2023; Perez et al., 2022; Slocum et al., 2025; Murthy et al., 2025). Studies show that this reduction in diversity is linked to decreased creativity and exploratory behavior. For example, Mohammadi (2024) show that RLHF lowers token-level entropy and embedding-space diversity, driving models toward fewer attractor states, which negatively impacts open-ended generation tasks. Casper et al. (2023) discuss mode collapse in RL-trained models, attributing it to reward maximization. More recent work, like Murthy et al. (2025), shows that RLHF and RLAIF methods reduce models ability to capture conceptual diversity in human responses, even with high surface-level quality. These findings suggest that diversity reduction is not merely trade-off but fundamental shift caused by reinforcement-based alignment. Unlike prior work, which focuses on diversity collapse as trade-off in alignment, our study examines how this collapse manifests structurally in reasoning trajectories and uses it to detect RLVR training data exposure. 7. Conclusion We investigate training data exposure in RL-post-trained reasoning models, identifying structural collapse in reasoning trajectories during RLVR and proposing Min-kNN Distance, black-box detection method leveraging this. Min-kNN Figure 7. Dual-stage contamination analysis. The green line shows Min-kNN Distance RL data detection performance on the lower pretraining contamination subset, selected based on the bottom-q quantile of PPL scores. The gray line shows performance on the random-control subset, matched in size to control for sample size effects on AUC. in Tab. 5, Min-kNN Distance achieves strong performance on both math (AUC 0.80) and coding (AUC 0.69), outperforming other methods. However, RL-phase contamination detection is more challenging for code, likely due to the higher diversity and flexibility in coding tasks compared to the more structured nature of math problems. Dual-Stage Contamination Analysis We follow the dualstage contamination detection experiment setup from Tao et al. (2025), using the GSM8K dataset (Cobbe et al., 2021), which is known for its pretraining contamination. RL-phase contamination is simulated by injecting portion of the test set into the RL training process and fine-tuning with PPO. Two data subsets are created for contamination detection. The first subset, representing lower pretraining contamination, is created by selecting the bottom portion of items based on pretraining-contamination scores (using PPL). The second subset is random-control subset, sampled uniformly to match the sample size of the first subset, controlling for sample size effects on AUC. As shown in Fig. 7, we compare RL-phase contamination effects across these subsets. The green line represents performance on the lower pretraining contamination subset, while the gray line shows performance on the random-control subset. The results indicate that Min-kNN Distance performs significantly better on the lower pretraining contamination subset, while the random-control subset shows reduced performance. This suggests that Min-kNN Distance is more effective at detecting RL-phase contamination when pretraining contamination is lower, validating our hypothesis that RL-phase contamination is easier to detect in data with weaker pretraining contamination signals. Moreover, MinkNN Distance can be used alongside common pretraining detection methods to help determine the training phase in which the data appeared. 8 Detecting RLVR Training Data Distance requires no access to training data or model internals. Experiments across various models and RL algorithms show Min-kNN Distanceconsistently outperforms baselines, providing robust signal for contamination detection. Limitations of our work are outlined in App. D."
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to improve the detection of RLVR exposure in AI models, crucial step toward increasing transparency in machine learning systems. By enabling better detection of data contamination, our method can help ensure that models are more reliable and fair in their evaluations. As AI continues to play larger role in critical applications, understanding and mitigating potential biases is essential. While our focus is on detection techniques, we recognize the importance of ethical considerations such as privacy and fairness, and we believe our work contributes to these broader conversations. At this time, we do not identify any specific ethical risks that require immediate attention."
        },
        {
            "title": "References",
            "content": "ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school https://huggingface. olympiads. co/datasets/ByteDance-Seed/ BeyondAIME(https://huggingface.co/ datasets/ByteDance-Seed/BeyondAIME), 2025. Carlini, N., Tramer, F., Wallace, E., Jagielski, M., HerbertVoss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pp. 26332650, 2021. Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cui, G., Yuan, L., Wang, Z., Wang, H., Zhang, Y., Chen, J., Li, W., He, B., Fan, Y., Yu, T., et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Dong, Y., Jiang, X., Liu, H., Jin, Z., Gu, B., Yang, M., and Li, G. Generalization or memorization: Data contamina9 tion and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938, 2024. Fu, W., Wang, H., Gao, C., Liu, G., Li, Y., and Jiang, T. Practical membership inference attacks against fine-tuned large language models via self-prompt calibration. arXiv preprint arXiv:2311.06062, 2023. Gandhi, K., Chakravarthy, A., Singh, A., Lile, N., and Goodman, N. D. Cognitive behaviors that enable selfimproving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Gao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li, L., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D., Quan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T., and Chang, B. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. Glazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen, E., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A., Santos, E. d. O., et al. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, B., Qu, Z., Liu, Z., Chen, Y., Zuo, Y., Qian, C., Zhang, Justrl: ScalK., Chen, W., Xiao, C., Cui, G., et al. ing 1.5 llm with simple rl recipe. arXiv preprint arXiv:2512.16649, 2025. Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H.-Y. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github. com/huggingface/open-r1. Kirk, R., Mediratta, I., Nalmpantis, C., Luketina, J., Hambro, E., Grefenstette, E., and Raileanu, R. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452, 2023. Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. Liu, S., Sabour, S., Zheng, Y., Ke, P., Zhu, X., and Huang, M. Rethinking and refining the distinct metric. arXiv preprint arXiv:2202.13587, 2022. Detecting RLVR Training Data Mahdavi, S., Li, M., Liu, K., Thrampoulidis, C., Sigal, L., and Liao, R. Leveraging online olympiad-level math problems for llms training and contamination-resistant evaluation. arXiv preprint arXiv:2501.14275, 2025. Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789, 2023. Mireshghallah, F., Uniyal, A., Wang, T., Evans, D. K., and Berg-Kirkpatrick, T. An empirical analysis of memorization in fine-tuned autoregressive language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 18161826, 2022. Mohammadi, B. the chat: The Creativity has left price of debiasing language models. arXiv preprint arXiv:2406.05587, 2024. Murtagh, F. and Contreras, P. Algorithms for hierarchical clustering: an overview. Wiley interdisciplinary reviews: data mining and knowledge discovery, 2(1):8697, 2012. Murthy, S. K., Ullman, T., and Hu, J. One fish, two fish, but not the whole sea: Alignment reduces language modIn Proceedings of the 2025 els conceptual diversity. Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 1124111258, 2025. Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., and Irving, G. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. Petrov, I., Dekoninck, J., Baltadzhiev, L., Drencheva, M., Minchev, K., Balunovic, M., Jovanovic, N., and Vechev, M. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. Qwen, A. Y., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Seed, B., Zhang, Y., Su, J., Sun, Y., Xi, C., Xiao, X., Zheng, S., Zhang, A., Liu, K., Zan, D., et al. Seed-coder: Let the code model curate data for itself. arXiv preprint arXiv:2506.03524, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Slocum, S., Parker-Sartori, A., and Hadfield-Menell, D. Diverse preference learning for capabilities and alignment. In The Thirteenth International Conference on Learning Representations, 2025. Tao, Y., Wang, T., Dong, Y., Liu, H., Zhang, K., Hu, X., and Li, G. Detecting data contamination from reinforcement learning post-training for large language models. arXiv preprint arXiv:2510.09259, 2025. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024. Xie, R., Wang, J., Huang, R., Zhang, M., Ge, R., Pei, J., Gong, N. Z., and Dhingra, B. Recall: Membership inference via relative conditional log-likelihoods. arXiv preprint arXiv:2406.15968, 2024. Xie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Zhang, J., Sun, J., Yeats, E., Ouyang, Y., Kuo, M., Zhang, J., Yang, H. F., and Li, H. Min-k%++: Improved baseline for detecting pre-training data from large language models. arXiv preprint arXiv:2404.02936, 2024. Detecting RLVR Training Data A. Analysis Details A.1. Training Details For the analysis model, we train Qwen-2.5-7B-Base using the GRPO and DAPO algorithms on the DAPO training data. The training hyperparameters are shown in Tab. 6. The model is trained for total of 200 rollout steps, which is approximately equivalent to 6 epochs. Table 6. Hyperparameters used for training Qwen-2.5-7B-Base on GRPO and DAPO."
        },
        {
            "title": "HyperParameter",
            "content": "GRPO/DAPO"
        },
        {
            "title": "Actor learning rate\nTrain batch size\nMax prompt length\nMax generation length\nTemperature\nSamples per prompt\nTensor model parallel\nMini batch size\nUse KL loss",
            "content": "1.0 106 512 2048 20480 1.0 16 4 32 No A.2. Cross-Input Generation Diversity (a) EAD (b) NLI (c) Embedding Figure 8. Evolution of generation diversity during RLVR training under DAPO and GRPO, measured in cross-input setting by three complementary metrics capturing lexical (EAD), logical (NLI), and semantic (embedding) diversity. 11 Detecting RLVR Training Data A.3. Prompt For Labeling Labeling Rigid N-gram Prompt { \"task\": ( \"You will be given: (1) problem statement, (2) one sample model answer for that problem, and (3) list of n-grams extracted from multiple model answers to the same problem. Your job is to label EACH n-gram with exactly one category from: restatement, logic, boilerplate, other. Labeling rules: 1) restatement: the n-gram repeats or paraphrases the problem statement. If the n-gram appears verbatim in the problem statement, it MUST be labeled restatement. 2) logic: the n-gram expresses problem-specific reasoning, math relations, formulas, constraints, or derived quantities. 3) boilerplate: generic reasoning template language (e.g., we need to, let us, therefore, in conclusion), or domain-agnostic filler. 4) other: everything else that does not fit the above. Important: - Use the problem statement and the sample answer as context. - The sample answer includes inline markers [[...]] around matching n-grams. - Do not invent new n-grams or change the provided strings. - Output ONLY JSON object mapping each input n-gram to one of the labels. - Include all n-grams, even if uncertain. - Keep the n-gram strings exactly as given.\" ), \"problem\": problem, \"sample_answer\": annotated_answer, \"ngrams\": ngrams, } A.4. Case on Rigidity Detecting RLVR Training Data Fig. 9 presents the final checkpoint of GRPO training, showing three answers to single prompt, with n-grams that appear more than 8 times highlighted. Figure 9. Example of repeated n-grams extracted from completions of single prompt. High-frequency n-grams naturally cluster into four categories: restatement, symbolic or algebraic logic steps, boilerplate phrases, and other tokens. The shading of the n-grams indicates their frequency, with darker colors representing higher repetition. 13 Detecting RLVR Training Data B. Experimental Details B.1. Training Details For the RL-MIA benchmark, we follow Tao et al. (2025) to train Qwen2.5-7B-Instruct and DeepSeek-Math7B-Instruct with the following hyperparameters: Table 7. Hyperparameter settings for Qwen2.5-7B-Instruct and Deepseek-math-7b-Instruct in RL-MIA. Parameter Qwen2.5-7B-Instruct Deepseek-math-7b-Instruct Actor learning rate Train batch size Max prompt length Max generation length Temperature Samples per prompt (n) Tensor model parallel (TP) Mini batch size Use KL loss 1.0 106 128 1024 4096 1.0 8 2 2 No 1.0 106 128 1024 3072 1.0 8 2 2 No B.2. Data Details Table 8. RL-MIA data splits for training and evaluation (Selected). Source Base RL Corpus (size) Injected items Train Size Occurrences Detection Tasks K&K SAT K&K train: 950 SAT train: 450 K&K test: 50 SAT test: 50 950 + 50 450 + 50 3 100 100 B.3. Implementation Details In all experiments, we use vLLM and deploy all models on 8 H800 GPUs with TP=8 or TP=4. During sampling, the default temperature is set to 0.7, and top is set to 0.95. Each models output is capped at max token length of 1024. We found that this token limit provides sufficient detection performance while also speeding up detection time. C. Cost Analysis We calculate the average detection time per sample using 32 completions on Open-Reasoner-Zero-7B, as shown in Tab. 9. While Min-kNN Distance requires more time compared to methods like PPL and DIME, with 6.65 seconds per item for 32 completions, the increased time is expected given the need to sample multiple completions for structural diversity analysis. This time is still reasonable and comparable to existing methods, making Min-kNN Distance an effective approach for RLVR data exposure detection. Table 9. Comparison of average detection latency per sample. Method Avg. Latency (s/item) PPL / Min-K / Min-K++ DIME Self-Critique CDD / Consistency Min-kNN Distance (n = 16) Min-kNN Distance (n = 32) 1.43 2.22 2.67 4.29 4.56 6. D. Limitations While Min-kNN Distance provides an effective solution for detecting RLVR exposure, there are some limitations. It relies on the assumption that RLVR-induced structural collapse is observable across various models and tasks. Additionally, the 14 Detecting RLVR Training Data method requires generating multiple completions per prompt, which can increase computational cost. Finally, Min-kNN Distance focuses on detecting structural collapse but does not directly address broader concerns like potential biases in training data."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "School of Engineering, Westlake University",
        "Zhejiang University"
    ]
}