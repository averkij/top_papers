{
    "paper_title": "Don't Pay Attention",
    "authors": [
        "Mohammad Hammoud",
        "Devang Acharya"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Transformer has become the de facto standard for large language models and a wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond a fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, a new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises a ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 5 0 3 1 1 . 6 0 5 2 : r Dont Pay Attention Mohammad Hammoud Avey AI mhh@avey.ai Devang Acharya Avey AI dacharya@avey.ai"
        },
        {
            "title": "Abstract",
            "content": "The Transformer has become the de facto standard for large language models and wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies."
        },
        {
            "title": "Introduction",
            "content": "The Transformer [98] has emerged as one of the most influential AI innovations in recent years, profoundly impacting various aspects of modern life, including work, science, and art, to mention just few. Notably, Large Language Models (LLMs) are almost universally based on the Transformer [25], which has demonstrated remarkable performance in natural language processing (NLP) [66, 55, 75] and various other fields [32, 56, 2]. The Transformers state-of-the-art performance is largely driven by its recurrence-free self-attention mechanism, which enables parallel processing of entire token sequences. Nevertheless, the computational and memory costs of self-attention grow quadratically with sequence length, making it inefficient for handling arbitrarily long sequences. Extensive research has been conducted over the years to address this limitation [94], with noticeable emphasis on linearizing attention [48, 9, 106, 99]. These linear approaches aim at approximating self-attention in more computationally efficient manner, without considerably compromising performance. Nonetheless, linear attention mechanisms have generally underperformed the original self-attention mechanism, often by significant margin in language modeling tasks [102, 47]. While recent linear models such as RWKV [70] and RetNet [91] have shown promising results, substantial progress is still needed before they can reliably and consistently surpass the Transformer [52]. In addition, these models have yet to demonstrate definitive empirical effectiveness at scale [25]. This persistent performance gap between quadratic and linear approaches has spurred renewed interest in RNN-based architectures, which offer linear scalability with sequence length but limit parallelism due to their inherently cyclical nature. Equal contribution. Figure 1: Needle-in-a-Haystack test performance comparison between Transformer++, Mamba, RWKV-7, and Avey, all using 1.5B parameters. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens) and the y-axis refers to the position of the needle (i.e., short sentence) within any of the haystacks. green cell means that the model was able to recall the needle and red one entails that it could not. Transformer++, Mamba, and RWKV-7 were all trained with 2k-token contexts, while Avey was trained with only 512-token context and was able to extrapolate to the maximum needed modeling capacity. To exemplify, state space models (SSMs) [44, 28], which are viewed as extensions of RNNs, have recently emerged as compelling class of architectures. Unlike traditional RNNs, SSMs can parameterize their state transition matrices in structured manner (e.g., via using diagonal plus low-rank decomposition) to improve computational efficiency and enhance gradient flow. specialized subclass of these models, known as structured state space sequence (S4) models [27, 28], has garnered growing attention. Yet, despite their theoretical appeal, S4 models struggled with language modeling tasks, typically trailing Transformers by several points [20, 27]. Most recently, Mamba [25] advanced S4 models by enhancing their selectivity and effectiveness while enabling high training concurrency. It demonstrated strong performance on tasks involving long-range dependencies and compared favorably to Transformers in language modeling. However, training, scaling, and interpreting Mambaand SSMs more broadly [87, 72, 31]remain challenging, while continue to be promising [13]. We posit that the primary limitation of the Transformer lies in its inability to effectively model dependencies beyond its fixed context window. While its core self-attention mechanism is inherently parallelizable, this constraint makes its quadratic complexity significant bottleneck at scale. This explains the surge of research aimed at reducing this complexity or exploring RNN-inspired alternatives. In this work, we propose more viable approach by decoupling context width from sequence length, allowing models to scale to arbitrarily long sequences. Under this paradigm shift, the quadratic training complexity becomes less of concern when small context windows are used, especially if the models maintain high parallelizability. This paper introduces Avey2, new architecture for language modeling that entirely departs from Transformer-based and RNN-like designs. Avey is flexible, sequence-length-invariant model that decouples sequence length from context width, thus enabling effective processing of longIt preserves the influence of tokens that appear outside its context window, range sequences. regardless of their positions in the sequence. This is achieved via weighted-selective-split interaction mechanism, which systematically skips irrelevant tokens beyond the context window and ensures direct interactions with only relevant ones, retaining their contributions irrespective of sequence length. Fig. 1 illustrates Aveys ability to generalize beyond its training context. popular benchmark for evaluating this capability is Needle-in-a-Haystack (NiaH) [45]. This benchmark measures models capacity to recite specific sentence (i.e., the needle) placed at an arbitrary position within large body of distractor text (i.e., the haystack). Since its introduction, NiaH has become widely used sandbox for probing the limits of long-context language models in capturing distant dependencies, and smaller models in generalizing beyond their trained context windows [22]. As shown in the figure, Transformer++ (i.e., the Transformer with an enhanced architecture and training recipe-- see Section 3.1), which was trained with 2k-token context window, could not generalize beyond that limit. In contrast, Mamba and RWKV-7 [70], also trained with 2k-token context windows, managed to generalize to nearly 8k and 16k tokens, respectively. Most notably, Avey, despite being trained 2Avey is not an acronym, but name that the authors like. 2 on context window of only 512 tokens, successfully generalized to the maximum tested sequence length of 64k tokens, demonstrating strong extrapolative capability far beyond its original training regime. To elaborate on its technical aspects, Avey is recurrenceand attention-free architecture comprising two principal components, ranker and neural processor. The ranker slices each input sequence into splits of consecutive tokens and selects the top most relevant splits for each current split being processed by the neural processor. The neural processor consists of three core units, the enricher, contextualizer, and fuser. The enricher enhances the quality of token embeddings by expanding their learnable features using position-wise neural network. The contextualizer is an embedding-wise neural network with dynamic parameterization, enabling interactions between relevant tokens across the current and top splits. Lastly, the fuser learns function that integrates the contextualized features produced by the contextualizer with some uncontextualized features bypassed by partial-embedding bypassing mechanism. To summarize, our main contributions in this paper are as follows: We propose Avey, new recurrenceand attention-free neural architecture that decouples context window from sequence length, thus enabling effective processing of long-range sequences. We show that Avey performs comparably to the Transformeroutperforming it at two model sizes and underperforming it at oneacross range of popular zero-shot NLP benchmarks, thereby establishing an initial foundational architecture with potential for more scalable and effective language modeling. In contrast to the Transformer, we demonstrate that Avey can scale far beyond its context window using the standard Single Needle-In-A-Haystack (S-NIAH) benchmark suite from RULER [38]. We show that Mamba (representing SSMs) and RWKV-7 (representing linear attention models) exhibit some ability to generalize beyond their training context windows, but their performance decline significantly as the sequence length increases far beyond them. By comparison, Avey consistently and substantially outperforms both Mamba and RWKV-7 on the S-NIAH benchmark suite. We conduct extensive ablation studies to assess the impact of each design choice in Avey. We provide comprehensive survey of related work in Appendix O. We open-source the code and pretrained checkpoints of Avey to facilitate reproducibility and foster future research3."
        },
        {
            "title": "2 Avey",
            "content": "As indicated earlier, Avey comprises two components, ranker and an autoregressive neural processor. Below, we provide detailed explanation of each component. 2.1 Ranker Avey decouples sequence length from context width, enabling the processing of arbitrarily long sequences. The sequence length refers to the total number of tokens in sequence, while the context width denotes the number of tokens that the neural processor can contextualize simultaneously. Importantly, the sequence length can be set to value that is much larger than the context width. As such, the influence of global tokens (or tokens that fall outside the context window) may diminish as more tokens are successively processed. If such global tokens are semantically relevant to local tokens (or tokens that fall within the context window), the quality of token representations will decline, and the effectiveness of the model will degrade. To this end, the ranker and neural processor jointly employ weighted-selective-split interaction mechanism, which skips irrelevant global tokens and ensures direct interactions with relevant ones, preserving their impact regardless of sequence length. As demonstrated in Fig. 2, Avey divides each 3https://github.avey.ai/avey-dpa. Figure 2: The ranker (left) partitions each input sequence into equal-sized splits and identifies the top most relevant ones (e.g., splits 1 and 3 for = 2) with respect to the current split (e.g., split 4), using the MaxSim operator. These top-k splits are then weighted by their normalized scores, where the normalized score (NS) of split is computed as the ratio of its MaxSim value to the highest MaxSim score among the splits. Finally, the weighted top-k splits are contextualized together with the current split by the neural processor (right). input sequence into equal-sized splits, each consisting of list of contiguous token embeddings. Prior to predicting the next token in the sequence (e.g., token 9 in the figure), Avey involves the ranker to identify the top (e.g., 2 in the figure) splits that are most relevant (e.g., splits 1 and 3) to the current split (i.e., split 4). The current split is defined as the one that either contains the token to be predicted (e.g., split 4 may contain only embedding 7, and Avey will aim to predict token 8) or contributes to predicting the first token in its subsequent split (e.g., split 4 serves in predicting token 9, which will belong to split 5 once predicted). To determine relevance, the ranker computes similarity score between the current split, say Sc, and each preceding split, say Sp, using the MaxSim operator [49], originally proposed and utilized in Information Retrieval. Specifically, pairwise similarities are calculated (e.g., using cosine function) between each embedding in Sc and all embeddings in Sp. For each embedding in Sc, the maximum similarity across all Sps embeddings is taken, and then the maxima of all Scs embeddings are added to yield the final MaxSim score (see Fig. 2). This score signifies how relevant Sp is to Sc. Subsequently, the preceding splits are ranked based on their MaxSim scores, and the top most relevant ones are contextualized with the current split by the neural processor, while maintaining their original order in the sequence. Before contextualization, however, the MaxSim scores of the top splits are normalized with respect to the highest MaxSim score among them (e.g., split 3s MaxSim score of 0.8 in Fig. 2 is normalized via dividing it by the maximum score among the top splits, i.e., 1.6, yielding 0.5). Each selected split is then weighted by its corresponding normalized MaxSim score, effectively scaling its contribution during contextualization. As result, the weighted-selective-split interaction mechanism does not only allow the ranker to rank splits based on relevance but also the neural processor to contextualize them accordingly, as each selected split is pre-weighted by its relevance score. This empowers the neural processor to judiciously leverage global information (i.e., splits beyond the context width) by focusing selectively on only relevant features, emphasizing informative ones and deemphasizing less useful ones, thus enhancing performance. We analyze the impact of weighting the top splits in Appendix K. Note that the ranker is invoked only once per full forward and backward pass. To elucidate, Aveys depth can be increased by stacking multiple layers within the neural processor (see Fig. 3), thereby enabling the modeling of complex, hierarchical patterns. In contrast, only single ranker is required before the stack of layers within the processor, regardless of their number. Once the ranker identifies the top relevant splits of the current split, the neural processor contextualizes them all using one or more layers. Consequently, during training, each current split is matched once against every preceding split. This results in compute cost of N/S(N/S+1) S2d or time complexity of O(N 2d), where is the sequence length, is the split size, and is the embedding dimension. This complexity assumes 2 4 Figure 3: The neural processor (top) with its three major components, the enricher, contextualizer (Cx), and fuser. The processor is unfolded into two copies for illustrative purposes only, to show how different embeddings, (e.g., e1 and e2, or more precisely, parts of their tails, i.e., e122 and e222) are contextualized by Cx (i.e., in reality, all components are shared across all embeddings and many embeddings can be input to Cx simultaneously). that scalar multiply-add operations (e.g., those used in computing cosine similarity for MaxSim) and comparisons (e.g., those utilized to determine maximum scores) are constant-time. We next discuss the neural processor. 2.2 Neural Processor The neural processor encompasses three key machineries, the enricher, contextualizer, and fuser (see Fig. 3). We describe each in detail below. 2.2.1 The Enricher The enricher aims at enriching the quality of each token representation via expanding the quantity of its learnable features, thereby enabling the contextualizer to capture more nuanced distinctions between tokens. Concretely, it is one-layer, position-wise neural network (i.e., the input to each neuron is single scalar element from an embedding), thus operating on each embedding independently, without considering neighboring embeddings. As such, it allows intra-feature interactions within the context of each individual embedding, facilitating the learning of higher-order and more expressive representations. The enricher can expand each input embedding by an arbitrary factor. We study the effect of varying the expansion factor on Aveys performance in Appendix D, and ablate the enrichers contribution in Appendix K. Equation 1 formalizes the enricher, where RCd is matrix of input embeddings (C , where is the sequence length), each of dimension d; σ is an activation function; Rdm is learnable weight matrix defining linear projection from dimension to m, where > d; and RCm denotes biases. = σ(XU + b) (1) As demonstrated in Fig. 3, the enricher feeds both, the contextualizer and the fuser. In particular, it bypasses portion of each expanded embedding directly to the fuser in technique that we refer to as partial-embedding bypassing. More precisely, the output of the enricher, RCm, is split into two parts: (1) the head Zh RCmh, which is bypassed directly to the fuser, and (2) the tail Zt RCmt, which is forwarded to the contextualizer, where = mh + mt. Consequently, varying the tail size alters the head size, which can influence Aveys performance. We investigate the impact of different tail sizes on Aveys performance in Appendix E. 5 The partial-embedding bypassing technique allows preserving raw distinctive features of each embedding, thus inducing representations with inherent diversity. This diversity may serve in alleviating issues like entropy collapse [107], where the contextualizer increasingly focuses on few tokens, and over-smoothing [109, 85, 110], where embeddings become increasingly similar, as Aveys depth is increased. We analyze the significance of partial-embedding bypassing on Aveys effectiveness in Appendix K. Lastly, Equation 1 implies that each neuron performs weighted sum of input features (i.e., the elements of an embedding), incurring 1 multiply-add operations. Since is projected to higher dimension m4, the total computational cost is m(d 1) per token. For sequence of tokens, the cost becomes m(d 1), or asymptotically O(N md). 2.2.2 The Contextualizer The contextualizer is one-layer, embedding-wise neural network (i.e., the input to each neuron is one embedding), thus operating in parallel on embeddings, where denotes the context width. More precisely, it enables inter-embedding, data-dependent interactions of only tail embeddings (i.e., Zt RCmt, as defined in Section 2.2.1), after each enrichers output embedding is split into head part (i.e., mh) and tail part (i.e., mt), and only the mt part (e.g., e12 and e22 in Fig. 3) is forwarded to the contextualizer. The mt part of each enriched embedding is further divided into two equal portions, mtl (or left portion) and mtr (or right portion), to enable judicious control of information flow through the neural processor. Specifically, mtl serves as gating mechanism for mtr, regulating how much of its contextualized feature values are propagated forward. Both mtl and mtr are learnable by the model, hence, allowing mtl to dynamically capture the significance of each mtrs feature, and emphasize or deemphasize its influence accordingly. This gating mechanism was inspired from gMLP [53] and resembles that of Gated Linear Units [15, 84, 100]. More formally, Zt RCmt is partitioned into two equal parts, Ztl RC(mt/2), which is bypassed to multiplicative element-wise operation as part of gating mechanism, and Ztr RC(mt/2), which is contextualized via neural network, where each neuron takes as input an embedding of dimension mt/2. Equation 2 defines the overall process, where RCC is learnable weight matrix representing linear cross-embedding transformation, denotes element-wise multiplication, RC(mt/2) refers to optional biases, and (Ztr) and (Z tr) are rowand column-wise normalized versions of Ztr, respectively. c(Zt) = Ztl σ (cid:0)(cid:0)V (Ztr)N (Z tr)(cid:1) Ztr + b(cid:1) (2) Equation 2 suggests that each neuron in the contextualizers network performs weighted sum of the cosine similarities between embeddings (denoted by (Ztr)N (Z tr)) and the embeddings themselves (denoted by Ztr). This introduces level of selectivity into the neural processor, as described and advocated by [25]. Specifically, it makes the parametrization of the neural processor dynamic, enabling it to disregard or focus on information during inference based on the input. We examine the influence of dynamic parametrization on Aveys performance in Appendix K. Finally, we note that the contextualizer inherently models the relationships between tokens, making the neural processor naturally aware of their positions in the sequence (i.e., positional encodings are not needed). In terms of complexity, as each neuron performs weighted sum of embeddings, each of dimension mt/2, it results in cost of (C 1)mt/2 multiply-add operations. With neurons, the cost becomes C(C 1)mt/2. For sequence of tokens, the contextualizer processes N/S splits, each contextualized with relevant splits, making = S(k + 1) and yielding total cost of (N/S)[C(C 1)mt/2] = (k + 1)[(C 1)mt/2] (after substituting with C/(k + 1)), or asymptotically O(N kCmt). 2.2.3 The Fuser The fuser is designed to learn an optimal function, referred to as fusion5, that integrates uncontextualized features (i.e., those of dimension mh, bypassed by the partial-embedding bypassing technique 4In our case, we experiment with being multiple of d, entailing that 2d (see Appendix D). 5The name is inspired from the CNN literature [40]. 6 Figure 4: The Time to First Token (TTFT) for Avey, Transformer++, Mamba, and RWKV-7 using 1.5B parameters, across varying sequence lengths. See Section 3.1 for details on the experimental methodology. presented in Section 2.2.1) with contextualized features (i.e., those of dimension mt/2, output by the contextualizer). Subsequently, it produces, for each input token, contracted representation that matches the tokens original embedding dimension (see Fig. 3). Akin to the enricher, it is one-layer, position-wise neural network, which operates on each embedding of dimension mh +mt/2 independently. Equation 3 provides mathematical definition of the fuser, where Zh RCmh (as described in Section 2.2.1) and c(Zt) RC(mt/2) (as suggested by Equation 2) are concatenated, and R(mh+mt/2)d is learnable weight matrix representing linear projection from dimension mh + mt/2 back to dimension d, where < mh + mt/26. (Z) = [Zh c(Zt)]O (3) Equation 3 entails that each neuron performs weighted sum of mh + mt/2 embedding elements, yielding cost of (mh + mt/2 1) multiply-add operations. For neurons (since the fuser projects mh + mt/2 to d), the cost is d(mh + mt/2 1). For sequence of tokens, the total cost is d(mh + mt/2 1), or asymptotically O(N md). Considering the aggregate computational costs of the ranker, enricher, contextualizer, and fuser, Avey exhibits training time complexity of O(L(2N md + kCmt) + 2d), where denotes the number of neural processor layers. As the term 2d dominates asymptotically, the overall complexity simplifies to O(N 2d). During inference, the complexity reduces to O(N ), or linear per token. We elaborate on Aveys inference time complexity in Appendix N. To characterize Aveys inference-time efficiency relative to other models, we benchmarked Time to First Token (TTFT)a key latency metric for real-time applications [36, 54, 17]on single NVIDIA H200 GPU. The evaluation included Avey, Transformer++, Mamba, and RWKV-7 across range of sequence lengths. As shown in Figure 4, Transformer++ demonstrates an approximately quadratic increase in TTFT as the sequence length grows, owing to its full self-attention mechanism, which must operate over the entire prompt before generating the first token. In contrast, Mamba and RWKV-7 scale linearly with , as they require full forward pass to construct their RNN-style hidden states prior to emitting the first token. While Avey is also theoretically expected to scale linearly, its empirical TTFT is significantly lower than that of Transformer++, Mamba, and RWKV-7. This efficiency gain stems from Aveys architectural design, which invokes its dominant contributor to inference complexity (see Appendix N), namely, the ranker, only once per forward pass. As result, the ranker imposes minimal computational overhead in practice, enabling Avey to achieve substantially lower TTFT and making it particularly well-suited for latency-sensitive, real-world applications (e.g., conversational AI and edge deployments). 6This inequality will always hold if mh + mt 2d, as is the case in our experiments (see Section 3). Table 1: Summary of studies for key design choices and corresponding experimental references. Question RMSNorm or LayerNorm? LR: to decay or not to decay? Answer RMSNorm Yes, cosine decay with peak LR of 1e3 Best values for sequence length , split size S, and top-k splits? = 512, = 64, = 7 Activation in the enricher? Activation in the contextualizer? Deeper model and narrower embeddings, or shallower model and wider embeddings? Yes, ReLU2 No Deeper model and narrower embeddings Weight ranked splits? Yes, using normalized scores Enrich embeddings before contextualization? Yes, by 4x Bypass uncontextualized features to the fuser? Yes, 50% of each enriched embedding Static or dynamic parametrization for the contextualizer? Dynamic parametrization Replace the contextualizer with self-attention? Besides enabling extrapolation past the context window, does the ranker improve performance? No Yes Experiments Appendix Appendix Appendix Appendix Appendix Appendix Appendix Appendix K, Appendix K, Appendix Appendix Appendix K"
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup We compare Avey against three leading open-source models, namely, Mamba [42], RWKV-7 [43], and Transformer++, extended to the strongest known architectural recipe of the standard Transformer [46] (see Appendix for details). All models were trained using their best-known hyperparameters under fixed budget of 100 billion tokens drawn from the FineWeb dataset [41]. Complete training and model hyperparameters for all the baselines are provided in Appendix A. To assess each models accuracy, we employed suite of widely used NLP benchmarks, including ARC-E and ARC-C [11], HellaSwag [105], PIQA [5], OBQA [61], SIQA [82], and Winogrande [80]. Additionally, we evaluated the long-context retrieval capabilities of all the models using the standard Single Needle-In-A-Haystack (S-NIAH) benchmark suite from RULER [38]. Full details of all the benchmarks and additional experimental setups are included in Appendix A. 3.2 Design Choices and Ablations We conducted over 200 experiments to investigate and ablate several key design choices in our architecture. These included, whether to: (1) utilize activation functions in the enricher and contextualizer; (2) trade off additional layers for reduced embedding dimensionality (or vice versa) under fixed parameter budget; (3) apply weights to ranked splits; (4) enrich embeddings; (5) bypass uncontextualized features to the fuser; (6) use dynamic parameterization in the neural processor; and (7) replace the neural contextualizer with self-attention. Furthermore, we evaluated Avey without the ranker to assess its contribution to downstream task performance, beyond its primary role of enabling effective extrapolation past the trained context window. Table 1 summarizes our findings and provides references to the corresponding experiments that support each conclusion. 3.3 Short-Range Benchmark Results We now evaluate Avey on standard autoregressive language modeling benchmarks, comparing it against Transformer++, Mamba, and RWKV-7 across three model sizes, small, medium, and large, as defined in Section 3.1. We utilize range of widely used zero-shot downstream evaluation tasks, all detailed in Section 3.1. Table 2 summarizes the results. With small models, Avey, Mamba, and RWKV-7 outperformed Transformer++ by average margins of 1.43%, 2.41%, and 1.82%, respectively. Mamba and RWKV-7 slightly exceeded Aveys performance, with average margins of 0.9% and 0.3%, respectively. With medium models, Avey, Mamba, and RWKV-7 again outperformed Transformer++ by averages of 0.3%, 3.4%, and 2.9%, respectively. Lastly, with large 8 Table 2: Zero-shot performance across multiple NLP tasks. Model ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average Avey-153M 24.37 Transformer++-152M 23.63 24.17 Mamba-144M 24.17 RWKV-7-168M Avey-496M 27.50 Transformer++-488M 26.73 28.64 Mamba-500M 27.13 RWKV-7-501M Avey-1.52B Transformer++-1.5B Mamba-1.4B RWKV-7-1.5B 31.26 30.00 32.13 32.94 42.33 43.17 43.53 43.01 48.95 48.09 51.02 49.37 56.55 56.29 57.74 59. 39.36 39.32 40.55 41.55 51.82 52.66 54.15 54.54 61.42 63.87 63.74 64.43 31.40 29.80 30.40 29.67 32.47 31.73 34.47 36.27 36.80 38.00 36.85 37. 68.37 67.01 68.32 68.72 72.49 72.13 73.03 73.58 75.61 76.01 76.19 76.84 39.13 38.89 39.41 39.17 40.15 39.93 40.84 39.40 42.00 42.24 42.00 41. 51.28 50.22 52.72 51.09 54.38 55.25 55.49 55.72 57.06 61.38 60.40 60.06 42.32 41.72 42.73 42.48 46.82 46.65 48.23 48.00 51.53 52.54 52.72 53. models, Avey underperformed Transformer++ by an average of 1.9%, while Mamba and RWKV-7 marginally outpaced it by 0.71% and 1.19%, respectively. The results above assume fixed training budget of 100B tokens. To better understand how Avey scales with increasing model size, we conducted additional experiments following the Chinchilla scaling laws [35], which recommend increasing the number of training tokens proportionally with model size. Consequently, we adjusted the number of training steps and tokens to align with these laws. Appendix outlines the configurations of the trained models, including the numbers of layers, embedding dimensions, training steps, learning rates, and total training tokens. The methodology of these experiments closely follows that of [25], with slight modifications (e.g., to accommodate parameter budget constraints). As demonstrated in Appendix J, Avey exhibits more favorable scaling behavior than Transformer++, Mamba, and RWKV-7, particularly when both model size and token count are increased proportionally in accordance with the Chinchilla scaling laws. Finally, to provide deeper insights into the behaviors of the compared models, we performed detailed statistical analysis, presented in Appendix L. The analysis reports the standard deviation, standard error, and 95% confidence interval for each model, computed over its final three checkpointscaptured at 5-billion-token intervals (i.e., at 90B, 95B, and 100B tokens). The results revealed meaningful variance among models. For example, in the small model regime, although Avey does not surpass Mamba in mean performance, their confidence intervals overlap substantially within the range (42.06, 43.24), indicating that the two models are statistically comparable and that Avey may outperform Mamba in some runs. similar, though narrower, overlap is observed between Avey and RWKV-7. In the large model regime, while Avey does not exceed Transformer++ in average performance, their confidence intervals also exhibit significant overlap, suggesting that Avey could potentially outperform Transformer++ in certain cases. more limited overlap is seen between Avey and Mamba, implying that although Mamba generally performs better, Avey may outperform it in some runs. 3.4 Long-Range Benchmark Results In this section, we evaluate Avey, Transformer++, Mamba, and RWKV-7 on benchmarks designed to assess performance on tasks with long-range dependencies. Specifically, we use the standard Single Needle-In-A-Haystack (S-NIAH) benchmark suite from RULER[38], as described in Section3.1. The S-NIAH suite includes multiple variants, notably S-NIAH-1 (pass-key retrieval) and S-NIAH-2 (number in haystack). S-NIAH-1 involves retrieving the specific value associated with given key (the pass-key) from lengthy distractor text (the haystack) containing many key-value pairs. The passkey, serving as the needle, appears only once, and the model must accurately recall its corresponding value regardless of its position in the haystack. S-NIAH-2 is similar to S-NIAH-1 but poses greater challenge, whereby the value to be retrieved is numerical (e.g., random 9-digit number). This task requires exact recall, where even single-digit error is considered incorrect, thereby testing the models precision in extracting structured numerical information from long haystacks. Figure 5: Performance comparison between Transformer++, Mamba, RWKV-7, and Avey on S-NIAH-1 and S-NIAH-2. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens). All models use 0.5B parameters. Similar results are shown in Appendix for other model sizes. Fig. 5 demonstrates the results of Avey, Transformer++, Mamba, and RWKV-7 on both S-NIAH-1 and S-NIAH-2 benchmarks. As described in Section 3.1, Transformer++, Mamba, and RWKV-7 were all trained with context window of 2,048 tokens. As shown, Transformer++ performs strongly on both benchmarks as long as the haystack length remains within its trained context window. Once the haystacks length exceeds its context window width, Transformer++ fails to recall the correct values associated with the keys. In contrast, Mamba and RWKV-7 demonstrate some ability to generalize beyond their training windows, but their performance also declines significantly as the haystack length increases far beyond those limits. On the flip side, Avey achieves good performance across both benchmarks, despite being trained with context window of only 512 tokens. For instance, on S-NIAH-2 with 64k-token haystack, Avey outperforms Mamba and RWKV-7 by averages of 85.25% and 23.6%, respectively. In addition, on S-NIAH-1 under the same 64k-token setting, Avey achieves an accuracy of 97.8%, while Mamba and RWKV-7 drop to 0% and 0.8%, respectively. Interestingly, Aveys performance tends to improve as the haystack length increases, highlighting its strong extrapolative capability. This behavior can be attributed to the fact that as the haystack length (i.e., sequence length ) grows, the candidate pool from which the ranker selects the top-k splits for contextualization also expands. As discussed in Appendix G, larger enables the ranker to identify and retrieve more relevant splits while discarding less informative ones, thereby improving the overall quality of contextualization and potentially enhancing performance. This effect is further supported by the results in Appendix K, where the inclusion of the ranker led to measurable performance gains. Notably, embeddings containing needlewhether in S-NIAH-1 or S-NIAH-2are not processed in isolation but rather contextualized alongside other embeddings. As such, an improved quality of contextualization driven by the ranker may contribute to more accurate value recall. However, whether this mechanism fully explains Aveys increasing performance with longer haystacks remains uncertain, and further interpretability studies are needed to better understand the underlying drivers of this behavior."
        },
        {
            "title": "4 Related Work",
            "content": "Extensive research has focused on addressing the limitations of the Transformer and exploring alternative RNN-like architectures. Appendix provides comprehensive survey of these efforts and offers comparison with Avey."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced Avey, new foundational architecture for autoregressive language modeling. Unlike traditional models, Avey relies neither on recurrence nor attention. Instead, it employs neural approach to enrich and contextualize embeddings. Additionally, it leverages ranker that enables the model to flexibly and effectively handle sequences of arbitrary lengths, despite being trained with only small context window. We hope this work lays the groundwork for future research and inspires further advances in scalable and effective language modeling."
        },
        {
            "title": "Limitations",
            "content": "We define the scope of our work and discuss limitations in Appendix P."
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449--12460, 2020. [3] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. [4] Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, and Joel Hestness. Straight to zero: Why linearly decaying the learning rate to zero works best for llms. arXiv preprint arXiv:2502.15938, 2025. [5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical In Proceedings of the AAAI conference on artificial commonsense in natural language. intelligence, volume 34, pages 7432--7439, 2020. [6] Guy Blelloch. Prefix sums and their applications. 1990. [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [8] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. [9] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. [11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [12] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [13] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. [14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:16344--16359, 2022. [15] Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933--941. PMLR, 2017. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171--4186, 2019. [17] Gregory Dexter, Shao Tang, Ata Fatahi Baarzi, Qingquan Song, Tejas Dharamsi, and Aman Gupta. Llm query scheduling with prefix reuse and latency constraints. arXiv preprint arXiv:2502.04677, 2025. 11 [18] Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, and Guiguang Ding. Repmlp: Re-parameterizing convolutions into fully-connected layers for image recognition. arXiv preprint arXiv:2105.01883, 2021. [19] Jeffrey Elman. Finding structure in time. Cognitive science, 14(2):179--211, 1990. [20] Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. [21] Daniel Fu, Elliot Epstein, Eric Nguyen, Armin Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher Ré. Simple hardware-efficient long convolutions for sequence modeling. In International Conference on Machine Learning, pages 10373--10391. PMLR, 2023. [22] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171, 2024. [23] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. framework for few-shot language model evaluation. https://github.com/EleutherAI/ lm-evaluation-harness, 2021. Accessed: 2024-05-16. [24] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. Its raw! audio generation with state-space models. In International conference on machine learning, pages 7616--7633. PMLR, 2022. [25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [26] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:1474--1487, 2020. [27] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572--585, 2021. [29] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971--35983, 2022. [30] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-- 22994, 2022. [31] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [32] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000--16009, 2022. [33] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. URL https://arxiv.org/abs/1606.08415. [34] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9 (8):1735--1780, 1997. 12 [35] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [36] Maxwell Horton, Qingqing Cao, Chenfan Sun, Yanzi Jin, Sachin Mehta, Mohammad Rastegari, and Moin Nabi. Kv prediction for improved time to first token. arXiv preprint arXiv:2410.08391, 2024. [37] Haowen Hou, Zhiyi Huang, Kaifeng Tan, Rongchang Lu, and Fei Richard Yu. Rwkv-x: linear complexity hybrid language model. arXiv preprint arXiv:2504.21463, 2025. [38] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [39] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1 (2):3, 2022. [40] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132--7141, 2018. [41] Hugging Face. Fineweb dataset. https://huggingface.co/datasets/HuggingFaceFW/ fineweb, 2023. Accessed: 2025-04-19. [42] Mamba Implementation. Mamba: Linear-time sequence modeling with selective state spaces. https://github.com/state-spaces/mamba, 2023. Accessed: 2024-05-16. [43] RWKV-7 Implementation. Rwkv-lm: The rwkv language model (rnn + transformer style). https://github.com/BlinkDL/RWKV-LM, 2023. Accessed: 2024-05-16. [44] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. [45] Greg Kamradt. Llmtest_needleinahaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Accessed: 2025-05-13. [46] Andrej Karpathy. Nanogpt: The simplest, fastest repository for training/finetuning mediumsized gpts. https://github.com/karpathy/build-nanogpt, 2023. Accessed: 2024-05-16. [47] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah Smith. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. [48] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156--5165. PMLR, 2020. [49] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39--48, 2020. [50] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [51] Li, Cai, Zhang, Chen, and Dey. What makes convolutional models great on long sequence modeling? arxiv. arXiv preprint arXiv:2210.09298, 2024. [52] Zhiyuan Li, Tingyu Xia, Yi Chang, and Yuan Wu. survey of rwkv. arXiv preprint arXiv:2412.14847, 2024. [53] Hanxiao Liu, Zihang Dai, David So, and Quoc Le. Pay attention to mlps. Advances in neural information processing systems, 34:9204--9215, 2021. [54] Jingyu Liu, Beidi Chen, and Ce Zhang. Speculative prefill: Turbocharging ttft with lightweight and training-free token importance estimation. arXiv preprint arXiv:2502.02789, 2025. 13 [55] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012--10022, 2021. [57] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. [58] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022. [59] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. [60] Luke Melas-Kyriazi. Do you even need attention? stack of feed-forward layers does surprisingly well on imagenet. arXiv preprint arXiv:2105.02723, 2021. [61] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [62] Vinod Nair and Geoffrey Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807--814, 2010. URL https://www.cs.toronto.edu/hinton/absps/reluICML. pdf. [63] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Ré. S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems, 35:2846--2861, 2022. [64] OpenAI. tiktoken: fast bpe tokeniser for use with openais models. https://github.com/ openai/tiktoken, 2022. Accessed: 2024-05-16. [65] Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670--26698. PMLR, 2023. [66] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730--27744, 2022. [67] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pages 4055--4064. PMLR, 2018. [68] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [69] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 3, 2024. [70] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, et al. Rwkv-7\" goose\" with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456, 2025. [71] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. [72] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043--28078. PMLR, 2023. [73] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. arXiv preprint arXiv:2305.04749, 2023. [74] Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [75] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):1--67, 2020. [76] Prajit Ramachandran, Barret Zoph, and Quoc Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. URL https://arxiv.org/abs/1710.05941. [77] David Romero, Anna Kuzina, Erik Bekkers, Jakub Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data (2021). arXiv preprint arXiv:2102.02611, 2021. [78] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53--68, 2021. [79] David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by back-propagating errors. nature, 323(6088):533--536, 1986. [80] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99--106, 2021. [81] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers for speech recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1--5. IEEE, 2023. [82] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [83] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 9355--9366. PMLR, 2021. [84] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [85] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and James Kwok. Revisiting over-smoothing in bert from the perspective of graph. arXiv preprint arXiv:2202.08625, 2022. [86] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-- 31327. PMLR, 2023. [87] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [88] Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015. 15 [89] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [90] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. [91] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [92] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International conference on machine learning, pages 9438--9447. PMLR, 2020. [93] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning, pages 10183--10192. PMLR, 2021. [94] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey. ACM Computing Surveys, 55(6):1--28, 2022. [95] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34: 24261--24272, 2021. [96] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. IEEE transactions on pattern analysis and machine intelligence, 45(4):5314--5321, 2022. [97] Jos Van Der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. arXiv preprint arXiv:1804.04849, 2018. [98] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [99] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [100] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019. [101] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3--19, 2018. [102] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [103] Kaichao You, Mingsheng Long, Jianmin Wang, and Michael Jordan. How does learning rate decay help modern neural networks? arXiv preprint arXiv:1908.01878, 2019. [104] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. [105] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [106] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. 16 [107] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua Susskind. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning, pages 40770-- 40803. PMLR, 2023. [108] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Advances in Neural Information Processing Systems, 2019. [109] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin arXiv preprint Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv:2103.11886, 2021. [110] Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, and Zhenzhong Lan. Value residual learning for alleviating attention concentration in transformers. arXiv preprint arXiv:2410.17897, 2024."
        },
        {
            "title": "A Experimental Methodology",
            "content": "In this section, we describe the experimental methodology employed throughout the paper. To begin with, we adopted cascaded search process to identify the best configuration of Avey. Specifically, we began with baseline version of the neural processorexcluding the rankerand sequentially explored several architectural design choices. After each empirical conclusion regarding specific architectural element, we integrated that element into the processor (one at time) and resumed the search process from the updated configuration. This cascaded process is captured chronologically in Appendices B, C, D, E, and F. To elaborate, we started with an expansion factor of 4 in the enricher, tail size of 50% (i.e., half of each expanded embedding is forwarded to the contextualizer), RMSNorm [108] as normalization technique, no activation functions in the enricher and contextualizer, global batch size of 0.5M, context width of 1024, and constant learning rate of 1e3. As we empirically verified and decided upon each architectural element, we updated the processor accordingly. For example, after determining that ReLU2 was the most effective activation function for the enricher, we integrated it into the model and proceeded with the remaining search. After finalizing the above exploratory set of experiments, we incorporated the ranker into the neural processor and conducted an extensive studycomprising over 138 training and inference runsto identify the optimal sequence length (i.e., ), split size (i.e., S), and number of top-k splits (i.e., k). The results of this sensitivity study are summarized in Appendix G. Following this, we evaluated the best normalization technique (Appendix H) as well as the optimal peak learning rate and learning rate schedule (Appendix I) for the full architecture. All the experiments described above were conducted using 145-million-parameter model trained on 10 billion tokens from the FineWeb dataset7 [41] (specifically, the sample-100BT subset of FineWeb). The results of these experiments informed the following final selection of training and model hyperparameters for Avey across three parameter scales, 153M (small), 496M (medium), and 1.52B (large). Training and Model Hyperparameters of AVEY: Training hyperparameters: Optimizer: AdamW Betas: (0.9, 0.95) Epsilon: 1e12 Peak learning rate: 1e3 Schedule: Cosine decay to 10% of the peak learning rate, with no warmup Batch size: 0.5M for the small and medium models, and 1M for the large model Gradient norm clip: 1.0 Weight decay: 0.1 (applied only to matrices) Model hyperparameters: All models: Context width: 512 Split size (S): 64 Number of top-k splits: 7 Vocabulary size: 50,304 Expansion factor: 4 Tail size: 0.5 Small model (153M parameters): Embedding dimension: 768 Number of layers: 26 Medium model (496M parameters): Embedding dimension: 768 7This dataset is released under the Open Data Commons Attribution License (ODC-By) v1.0. 18 Number of layers: 104 Large model (1.52B parameters): Embedding dimension: 2048 Number of layers: 48 For baselines, we compared Avey against three leading open-source models, namely, Mamba [42], RWKV-7 [43], and Transformer++ [46]. For Transformer++, we implemented the strongest architectural recipe known to us, incorporating rotary positional encodings [89], SwiGLU MLPs [84], and RMSNorm in place of LayerNorm [108]. All models were trained using their best-known hyperparameters (see details below, assuming three model sizes, small, medium, and large) under fixed budget of 100 billion tokens drawn from the aforementioned FineWeb dataset 8. For consistency and comparability, we used the p50k_base tokenizer [64] across all the models, as it aligns with the GPT-2-derived token counts reported for this dataset. Training and Model Hyperparameters of TRANSFORMER++: Training hyperparameters: Optimizer: AdamW Betas: (0.9, 0.95) Epsilon: 1e12 Peak learning rates: Small model: 3e3 Medium model: 1.5e3 Large model: 1.25e3 Schedule: linear warmup for 10% of steps, followed by cosine decay to 10% of the peak learning rate Batch size: 0.5M for the small and medium models, and 1M for the large model Gradient norm clip: 1.0 Weight decay: 0.1 (only applied to matrices) Model hyperparameters: All models: Context width: 2048 Vocabulary size: 50,304 Intermediate size in FFN: 4 the embedding dimension Small model (152M parameters): Embedding dimension: 768 Number of layers: 12 Number of heads: 12 Medium model (488M parameters): Embedding dimension: 1024 Number of layers: 26 Number of heads: Large model (1.5B parameters): Embedding dimension: 1664 Number of layers: 32 Number of heads: 16 Training and Model Hyperparameters of MAMBA: Training hyperparameters: Optimizer: AdamW 8More precisely, all models, including Avey, Transformer++, Mamba, and RWKV-7 were trained for 1 epoch over the sample-100BT subset of FineWeb. 19 Betas: (0.9, 0.95) Epsilon: 1e12 Peak learning rates: Small model: 3e3 Medium model: 1.5e3 Large model: 1.0e3 Schedule: linear warmup for 10% of steps, followed by cosine decay to 10% of the peak learning rate Batch size: 0.5M for the small and medium models, and 1M for the large model Gradient norm clip: 1.0 Weight decay: 0.1 (applied only to matrices) Model hyperparameters: All models: Context width: 2048 Vocabulary size: 50,304 All hyperparameters other than the ones specified are left at their default values according to [42] Small model (144M parameters): Embedding dimension: 768 Number of layers: 28 Medium model (500M parameters): Embedding dimension: 1280 Number of layers: Large model (1.4B parameters): Embedding dimension: 2048 Number of layers: 52 Training and Model Hyperparameters of RWKV-7: Training hyperparameters: Optimizer: AdamW Betas: (0.9, 0.95) Epsilon: 1e12 Peak learning rates: Small model: 6e4 Medium model: 4e4 Large model: 4e4 Schedule: Cosine decay to 10% of the peak learning rate, with no warmup Batch size: 1M for the small and medium models, and 2M for the large model Gradient norm clip: 1.0 Weight decay: 0.1 (applied only to matrices) Model hyperparameters: All models: Context width: 2048 Vocabulary size: 50,304 All hyperparameters other than the ones specified are left at their default values according to [43] Small model (168M parameters): Embedding dimension: 768 Number of layers: 12 Medium model (501M parameters): Embedding dimension: 1024 Table 3: Aveys performance without and with an activation function in the enricher. The study involves only the neural processor within Avey and trains it on 10B tokens. Configuration Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average No Activation GELU ReLU ReLU2 SiLU 37.65 30.10 31.02 30.18 30.81 22.53 23.04 22.87 24.15 22.18 35.19 37.88 37.21 38.76 38. 28.95 31.37 31.35 32.08 31.30 27.40 26.00 28.00 28.00 28.20 60.45 63.55 62.79 63.76 62.30 36.13 36.95 38.13 38.28 37.05 48.70 51.85 48.86 50.12 53.20 37.05 38.66 38.46 39.31 38. Number of layers: 30 Large model (1.5B parameters): Embedding dimension: 2048 Number of layers: 24 To compare all models, we employed suite of widely used NLP benchmarks, including ARC-E and ARC-C (for scientific reasoning and reading comprehension) [11], HellaSwag (for commonsense inference) [105], PIQA (for physical reasoning) [5], OBQA (for open-book science reasoning) [61], SIQA (for social interaction understanding) [82], and Winogrande (for coreference and commonsense reasoning) [80]. In addition, we evaluated long-context retrieval capabilities using the standard Single Needle-In-A-Haystack (S-NIAH) benchmark suite from RULER [38], which measures models ability to extract pass-keys from large distractor corpora, with sequence lengths ranging from 2k to 64k tokens. All evaluations were conducted using the widely adopted LM Evaluation Harness from EleutherAI [23], consistent with the prior work in the field. For all models, we reported performance in terms of benchmark accuracy9. Specifically, we used normalized accuracy (acc-norm) from the LM Evaluation Harness whenever available. For each model, the reported score on each NLP benchmark is the average accuracy across its final three checkpoints (taken at 90B, 95B, and 100B tokens) to account for variability due to training randomness. Complete benchmark results across these checkpoints, along with key summary statistics and discussions, are provided in Appendix L. Finally, all training and evaluation runs were executed on 208 NVIDIA H200 GPUs, with mixedprecision (bfloat16) enabled for training. The total training time for all modelsAvey, Transformer++, Mamba, and RWKV-7across the three presented model sizes and 100B training tokens is estimated to be approximately 8090 hours, assuming optimal parallelization across the 208 GPUs and using the implementations referenced above. To avoid potential sources of randomness and ensure consistency across results, we disabled Torch Compile during all design choice and sensitivity experiments. For the ablation studies and final training runs, however, Torch Compile was enabled whenever possible to accelerate training. Additionally, we fixed the random seed to 11 (arbitrarily chosen) for all training runs to further reduce variability due to stochastic effects."
        },
        {
            "title": "B Activation or No Activation in the Enricher",
            "content": "In this study, we illustrate the performance of Avey with and without an activation function in the enricher. To this end, we trained model with 145 million parameters using 10 billion tokens from the Fineweb dataset [41]. The model employs an expansion factor of 4 in the enricher (i.e., each embedding dimension is expanded fourfold by the enricher), tail size of 50% (i.e., half of each expanded embedding is forwarded to the contextualizer), RMSNorm [108] as normalization technique, and no activation function in the contextualizer. Additionally, the context width (i.e., the maximum number of tokens that can be input to and processed by the contexutalizer simultaneously) is set to 1024 and constant learning rate of 1e3 is maintained throughout training. The study excludes the ranker and focuses solely on the neural processor. Besides, it evaluates four activation functions, namely, GELU [33], ReLU [62], ReLU2 [10], and SiLU [76]. All other experimental settings follow the methodology detailed in Appendix A. Table 3 summarizes the results. As shown, ReLU2 yielded an improvement in performance versus baseline with no activation, hence, was adopted as the default activation function for the enricher throughout our experiments 9Additionally, throughout the paper, all reported perplexity values specifically refer to training perplexity. 21 Table 4: Aveys performance without and with an activation function in the contextualizer. The study involves only the neural processor within Avey, trains it on 10B tokens, and uses ReLU2 within the enricher, capitalizing on the results shown in Table 3. Configuration Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average No Activation GELU ReLU ReLU2 SiLU 30.18 30.64 30.30 31.05 30.92 24.15 22.01 23.29 22.35 23.63 38.76 37.54 37.84 38.80 36.74 32.08 31.23 31.82 30.78 31.51 28.00 27.20 26.60 27.20 27. 63.76 64.74 64.15 63.49 64.20 38.28 37.21 38.13 37.31 36.18 50.12 50.67 50.59 52.01 50.04 39.31 38.66 38.92 38.85 38.53 Table 5: The effect of the expansion factor on Aveys performance. The study involves only the neural processor within Avey, trains it on 10B tokens, adopts ReLU2 within the enricher, and does not use an activation function within the contextualizer, building upon the results portrayed in Tables 3 and 4. Expansion Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average 2 4 8 30.85 30.18 30.00 23.29 24.15 23.21 36.83 38.76 37.79 30.92 32.08 31.48 26.20 28.00 26. 63.98 63.76 63.82 37.72 38.28 36.59 51.78 50.12 50.75 38.67 39.31 38.55 presented in Sections 3.3 and 3.4. It is important to note, however, that the lowest perplexity was provided by GELU and not ReLU2 (although the difference in perplexity was very minimal). While perplexity quantifies how effectively the model predicts the next token in the training dataset, it remains proxy for overall modeling capability and does not always precisely predict downstream task performance."
        },
        {
            "title": "C Activation or No Activation in the Contextualizer",
            "content": "We now evaluate Avey with and without an activation function in the contextualizer. We use the same experimental settings outlined in Appendix and add to that ReLU2 as an activation function in the enricher, capitalizing on the findings therein. We also experiment with four activation functions, namely, GELU, ReLU, ReLU2, and SiLU. Results are summarized in Table 4. As illustrated, the best performance was achieved without any activation function in the contextualizer, thus was employed as the default configuration in all our experiments reported in Sections 3.3 and 3.4. What is the Best Expansion Factor? In this study, we vary the expansion factor in the enricher, defined as the degree to which each input embedding is expanded. Specifically, we evaluate several expansion factors, ranging from 2x to 8x as shown in Table 5, while keeping the total model parameter count constant (e.g., with 2x expansion factor we use 34 layers, while with 4x one we utilize 20 layers). The experimental setup follows the methodology outlined in Appendix B, but incorporates ReLU2 as an activation function in the enricher and omits any activation function in the contextualizer, aligning with the findings reported in Appendices and C. As depicted in the table, an expansion factor of 4x yielded the best performance, hence, was set as the default configuration in the enricher throughout all our experiments presented in Sections 3.3 and 3.4. What is the Best Tail Size? We now examine the impact of forwarding tail portion of each enriched embedding to the contextualizer. More precisely, we vary the size of this tail portion, referred to as the tail size, from 10% to 90% of each enriched embedding, as illustrated in Table 6. The study follows the experimental setup described in Appendix and employs ReLU2 as an activation function in the enricher, no activation function in the contextualizer, and an expansion factor of 4x, based on the findings presented in Appendices B, C, and D, respectively. As depicted in Table 6, the best performance was accomplished using tail size of 50%, thus was adopted as the default configuration for Avey in all our experiments reported in Sections 3.3 and 3.4. Table 6: The effect of the tail size on Aveys performance. The study involves only the neural processor within Avey, trains it on 10B tokens, adopts ReLU2 within the enricher, does not use an activation function within the contextualizer, and utilizes an expansion factor of 4x, as recommended by the findings demonstrated in Tables 3, 4, and 5. Tail Size Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average 10% 30% 50% 70% 90% 34.23 30.91 30.18 29.79 30.20 21.59 23.55 24.15 23.04 23.04 36.78 37.33 38.76 38.26 38. 30.34 31.41 32.08 31.68 32.13 27.0 29.4 28.0 28.4 27.6 63.60 64.25 63.76 63.55 64.04 37.15 37.15 38.28 37.36 37.67 50.28 51.22 50.12 49.96 50.91 38.11 39.19 39.31 38.89 39. Table 7: Aveys performance across different model configurations, including wider embedding dimensions (e.g., 1536 under 0.5B-parameter model) with shallower layers (e.g., 24 layers under 0.5B-parameter model), or narrower embedding dimensions (e.g., 768 under 0.5B-parameter model) with deeper layers (e.g., 90 layers under 0.5B-paramter model). The models with 140M, 0.5B, and 1.5B parameters are referred to as small Avey, medium Avey, and large Avey in the text. # Params Embed. # Layers Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average 140 0.5 1.5 512 768 768 1024 1536 1536 2048 2560 40 20 11 90 54 24 80 48 30 31.14 30.18 31. 23.02 23.27 23.51 19.97 19.84 20.23 22.53 24.15 23.46 23.55 24.40 23.98 25.26 25.77 26.62 37.58 38.76 38. 42.05 41.92 42.85 45.50 46.55 45.16 31.05 32.08 30.78 38.61 38.46 37.79 44.56 44.99 43.91 28.6 28.0 27. 30.2 29.2 29.4 30.2 31.6 29.2 63.87 63.76 63.93 66.10 67.19 66.97 68.61 69.42 69.10 37.46 38.28 37. 39.20 38.74 38.89 40.02 40.17 39.82 52.64 50.12 48.86 51.78 51.46 51.78 52.33 52.17 52.09 39.10 39.31 38. 41.64 41.62 41.67 43.78 44.38 43.70 Deeper Models and Narrower Embeddings, or Shallower Models and"
        },
        {
            "title": "Wider Embeddings",
            "content": "The objective of this study is to determine whether narrower embedding dimension with greater model depth yields better or worse performance than wider embedding dimension with fewer layers. The study utilizes the experimental setup described in Appendix and leverages the findings presented in Appendices B, C, D, and E. Consequently, it utilizes ReLU2 as an activation function in the enricher, no activation function in the contextualizer, an expansion factor of 4x, and tail size of 50%. To begin with, we evaluated small Avey model (referred to as small Avey) with 140 million parameters, using three different embedding dimensions, 512, 768, and 1024. These configurations resulted in 40, 20, and 11 layers, respectively, to maintain constant parameter count. Table 7 summarizes the results. As illustrated, the configuration with an embedding dimension of 768 and layer count of 20 outperformed the other two configurations. Afterwards, we assessed larger Avey model with 500 million parameters (referred to as medium Avey), using three different embedding dimensions, 768, 1024, and 1536. These configurations resulted in 90, 54, and 24 layers, respectively, while keeping the total parameter count constant. As shown in Table 7, the setup with an embedding dimension of 768 and 90 layers yielded the best performance among the three tested ones. Finally, we examined an even larger Avey model with 1.5 billion parameters (referred to as large Avey), using three different embedding dimensions, 1536, 2048, and 2560. To maintain constant parameter count across configurations, these dimensions corresponded to 80, 48, and 30 layers, respectively. As portrayed in Table 7, the configuration with an embedding dimension of 2048 and 48 layers delivered the best performance among the three considered configurations. The above results suggest trend, whereby wider embedding dimensions (e.g., 1024 in small Avey; 1563 in medium Avey; and 2560 in large Avey) paired with shallower architectures (e.g., 11 layers in small Avey; 24 layers in medium Avey; and 30 layers in large Avey) tend to underperform deeper models (e.g., 40 and 20 layers in small Avey; 90 and 54 layers in medium Avey; and 80 and 48 layers in large Avey) with narrower embeddings (e.g., 512 and 768 in small Avey; 768 and 1024 Table 8: Aveys performance under different sequence lengths, , split sizes, S, and top-k values. All models (a total of 69) were trained on 10B tokens using 140 million parameters. The sweet spot in terms of downstream task performance was at = 512, = 64, and = 7; hence, it was adopted as Aveys default configuration. 256 16 64 128 16 512 32 128 256 16 32 1024 128 256 512 1 3 5 7 9 11 13 15 1 3 5 1 3 1 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 1 3 5 7 1 1 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 1 3 5 7 1 1 Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average 43.59 31.83 27.14 25.65 24.28 22.89 23.85 22.04 36.58 31.51 30.06 30. 32.76 30.90 31.30 42.51 29.28 24.64 23.49 20.79 19.52 19.45 17.95 35.47 29.49 27.99 28.07 27.17 26.77 25.72 26.29 31.67 29.51 29.31 28.02 29.25 29. 29.26 41.64 28.08 23.69 21.48 19.83 18.34 16.80 15.33 35.07 28.54 26.25 26.29 24.79 24.33 23.44 23.14 30.84 27.82 27.89 27.48 27.37 27.38 27.44 26.85 28.62 27.08 28.07 27.27 28.30 28. 28.24 21.16 22.18 24.15 22.27 23.72 22.53 22.35 22.61 23.98 22.78 23.38 24.23 23.89 23.04 22.27 22.27 22.87 22.01 23.46 23.38 23.63 22.44 21. 22.87 22.95 22.78 22.01 23.89 22.87 23.55 22.70 23.46 23.38 24.23 24.49 23.72 22.70 22.70 21.42 22.61 22.18 23.81 22.53 21.93 23.55 23.29 22.70 23.55 22.95 24.06 23.63 21.93 22.78 23. 23.89 22.61 23.04 24.06 22.35 23.29 24.23 24.32 23.12 23.89 23.98 24.32 22.61 23.04 24.40 37.67 39.81 39.02 38.59 38.55 39.27 38.97 38.80 39.18 39.56 38.76 38. 39.18 40.11 39.65 37.42 38.76 38.05 38.13 39.02 37.88 37.54 36.66 39.94 39.18 37.71 40.07 39.02 39.65 38.97 39.23 39.35 37.92 39.77 39.98 39.90 39. 39.02 37.16 38.26 38.38 38.05 37.50 37.16 37.50 37.54 39.98 38.55 39.06 38.76 38.89 38.76 37.46 39.56 38.51 39.60 40.49 39.27 39.98 39.35 39.77 40.36 40.57 39.81 40.03 38.85 38.38 39. 39.39 30.86 32.23 32.57 31.89 32.44 32.45 31.04 32.66 32.57 33.15 33.51 33.14 33.26 33.70 33.07 31.38 31.99 32.69 31.72 31.74 32.13 31.34 31. 32.49 33.22 33.46 33.45 33.46 32.55 33.52 32.53 33.15 33.12 33.17 33.77 33.76 33.38 33.49 31.12 31.88 31.94 31.41 31.90 31.45 30.55 31.04 32.87 32.91 33.39 32.70 33.34 32.56 32.73 32. 33.31 33.39 32.97 33.54 33.48 33.05 33.05 34.01 33.56 33.65 33.22 33.92 33.22 32.68 32.98 24 27.20 26.80 26.00 28.60 28.20 27.40 26.80 28. 26.80 25.00 28.60 26.80 27.60 27.40 28.20 27.80 28.00 26.80 26.40 27.00 27.20 26.80 27.80 28.40 25.60 28.20 29.20 28.40 27.20 29.00 29.40 27.80 28.40 27.60 29. 28.20 28.80 27.00 29.80 27.20 28.80 27.00 26.80 28.60 26.80 27.60 27.60 27.60 28.60 27.60 27.80 26.40 29.00 28.40 27.20 28.60 30.00 28.80 28.00 28.60 28.20 28.60 27.80 29.00 29.60 27. 27.80 27.40 28.20 64.20 63.76 64.20 64.80 64.15 65.07 64.91 65.61 64.25 64.91 65.34 65.23 66.05 64.91 37.72 38.02 38.02 37.97 37.36 38.02 37.51 38. 37.36 38.74 37.87 37.82 38.69 38.08 65.45 39.20 63.87 64.96 63.98 64.58 64.04 64.91 63.33 64.09 64.47 65.13 65.02 64.80 64.91 64.09 65.67 64. 65.02 65.72 64.58 65.13 64.09 65.23 37.31 36.49 36.44 37.82 38.23 38.02 36.80 37.82 38.18 39.00 38.33 37.67 38.59 38.33 37.56 38.08 38.13 39.10 38.33 38.08 37.10 38. 64.25 37.51 64.47 64.64 64.09 63.38 64.47 65.13 63.11 63.06 65.23 64.74 64.64 64.80 64.53 64.36 65.23 63.60 65.18 64.74 65.02 65.72 65.29 65.72 66.05 65.45 65.61 64.69 65.89 65. 64.85 64.36 37.56 38.08 37.87 36.80 37.92 37.77 36.95 37.77 37.77 37.51 38.28 37.67 37.72 37.36 37.31 37.31 38.28 38.84 38.49 37.72 38.74 37.97 37.31 37.72 38.54 37.72 39.10 38.13 38.79 37. 65.13 37.72 51.85 51.07 53.20 50.43 51.54 51.30 49.72 52.25 50.83 52.25 52.41 51.14 50.99 50.83 51. 51.07 52.17 52.01 49.41 51.14 53.67 50.83 51.07 51.70 50.51 52.72 50.75 50.20 51.14 51.85 50.04 51.70 50.83 52.09 51.30 50.99 51.62 52.41 50.75 50.99 51.22 50.59 49.64 50.67 52.33 50. 51.14 50.28 50.04 53.12 52.09 51.38 50.67 51.38 49.88 50.20 49.09 50.75 52.64 50.67 51.07 51.30 51.30 52.09 50.67 49.88 51.30 49.88 50.59 38.66 39.12 39.59 39.22 39.42 39.43 38.76 39. 39.28 39.48 39.98 39.62 39.95 39.72 39.95 38.73 39.32 38.85 38.79 39.22 39.63 38.44 39.87 39.72 39.37 39.75 39.71 39.78 39.40 40.02 39.54 39.80 39.78 39.97 40. 39.68 39.94 39.48 38.90 39.09 39.21 38.72 38.68 38.96 38.68 38.74 39.61 39.31 39.57 39.82 39.71 38.96 39.31 39.48 39.46 39.71 39.87 39.98 40.07 39.81 39.95 40.25 40.07 40.12 40.35 39. 39.56 39.27 39.77 in medium Avey; and 1536 and 2048 in large Avey). As such, in all our experiments discussed in Sections 3.3 and 3.4, we employed deeper models with narrower embedding dimensions, namely, the best performing small Avey, medium Avey, and large Avey in Table 7. Interestingly, Table 7 also highlights that certain benchmarks benefit more from increased model capacity than others. For instance, commonsense reasoning benchmark like HellaSwag demonstrates performance improvements of 20.36% and 28.5% under 0.5B-parameter and 1.5B-parameter models, respectively, compared to 140M-parameter baseline. In contrast, question-answering benchmark such as SIQA exhibits only modest gain of 2.4% under both 0.5B-parameter and 1.5B-parameter models relative to the 140M-parameter baseline, suggesting less sensitivity to model size. What are the Best Sequence Length, Split Size, and Top-k Values? We now analyze how Aveys perplexity and overall performance are affected by variations in three key parameters, the rankers top-k selected splits, the split size S, and the sequence length . We consider three values for , 256, 512, and 1024. For each , the split size is grown geometrically, starting from 16 and doubling at each step, up to the maximum permissible value N/2. Subsequently, for any and S, the number of top-ranked splits can range from 1 (i.e., contextualizing the current split with one additional relevant split) up to N/S 1. To tame the quantity of experiments, we increase arithmetically from 1 to maximum of 15, whenever possible, using an increment of 2. Finally, we use the experimental configurations recommended in Appendices B, C, D, E, and F. As shown in Table 8, Aveys perplexity is highest when both and are very small (e.g., = 16 and = 1). While small can help filter out irrelevant embeddings and denoise contextualization, pairing it with very small can deprive the contextualizer of sufficient context to build expressive representations10. To expand the context (i.e., increase its width S(k + 1)) and enrich the resulting embeddings, either or can be increased. For example, as we increased under = 256 and = 16, perplexity decreased and benchmark performance improved. However, larger context does not always yield better outcomes, especially when involving high proportion of irrelevant embeddings. This behavior was observed when was increased beyond 3 under = 1024 and = 128, resulting in higher perplexity and diminished downstream accuracy. In contrast to and k, the sequence length determines the size of the candidate pool from which the ranker selects the top-k splits for any current split during training. larger allows the ranker to reach farther back in the sequence history, potentially retrieving more relevant splits and lowering perplexity. For example, increasing from 512 to 1024, while holding = 16 and = 15 constant, reduced perplexity by 5.4%. Nevertheless, lower perplexity does not always translate into improved downstream performance. For instance, with = 512 and = 16, = 15 yielded the lowest perplexity, yet = 5 achieved higher benchmark accuracy. As noted in Appendix B, the loss remains only proxy for overall modeling capability and does not always exactly predict downstream task performance. As demonstrated in Table 8, the best empirical performance was obtained with = 512, = 64, and = 7, which was, accordingly, adopted as Aveys default configuration."
        },
        {
            "title": "H RMSNorm or LayerNorm",
            "content": "Table 9: Aveys performance with RMSNorm and LayerNorm. model with 153 million parameters was trained on 10B tokens using both the neural processor and ranker with the best configuration from Table 8. Normalization Method Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average RMSNorm LayerNorm 28.02 30.93 24.49 23.55 39.98 39. 33.77 33.24 29.8 28.8 65.13 65.07 38.08 38.28 51.30 49.57 40.36 39. In all previous runs across the appendices, we used RMSNorm [108] as normalization technique. We now test Avey with another standard normalization method, namely, LayerNorm [1]. To begin with, we note that Avey normalizes input embeddings before each layer (as illustrated in Fig. 3) and output embeddings once after the final layer and prior to token prediction. 10Recall from Section 2.2.2 that the context width is defined as = S(k + 1). 25 We evaluate each normalization technique using Aveys complete architecture, including its neural processor and ranker. The ranker is configured according to the best-performing setting identified in Appendix (i.e., sequence length = 512, split size = 64, and number of top-ranked splits = 7). The neural processor employs the optimal configurations reported in Appendices B, C, D, E, and F. As before, we trained the resulting model with 153 million parameters on 10 billion tokens from the Fineweb dataset. Table 9 summarizes the results. As shown, RMSNorm slightly outperforms LayerNorm on average, and is therefore adopted in Avey and used consistently across all the experiments presented in Sections 3.3 and 3.4. Learning Rate: To Decay or Not to Decay? Table 10: Aveys performance with two types of schedules, constant learning rate (LR) and cosine decay, starting from different peak learning rates. All models involved the neural processor and ranker, and were trained with 153 million parameters on 10B tokens, using both the neural processor and ranker with the best configuration from Table 8. Schedule LR Perplexity ARC-C ARC-E HellaSwag OBQA PIQA SIQA Winogrande Average Constant Cosine Decay 8e-04 1e-03 3e6e-04 8e-04 1e-03 28.21 27.35 30.38 26.24 25.64 25.00 23.12 24.06 23.81 22.87 24.06 23.21 39.81 40.32 38. 40.95 39.31 41.12 33.52 33.88 32.90 33.76 34.43 34.76 28.4 29.6 28.2 29.2 29.6 27.0 64.96 65.13 63. 65.02 65.83 65.67 38.13 38.54 37.77 37.72 37.87 38.38 52.41 51.46 52.33 49.01 49.80 50.75 40.05 40.43 39. 39.79 40.13 40.13 In all previous runs across the appendices, we used constant learning rate of 1e3. In this study, we evaluate the performance of Avey under varying maximum learning rates and learning rate schedules. Specifically, we compare two schedules, cosine decay and constant learning rate (which can be viewed as cosine decay with an infinite cycle length, effectively eliminating any decay). The study adopts the experimental configurations suggested in Appendices B, C, D, E, and F. In addition, it employs Aveys complete architecture, including its neural processor and ranker, with the best configuration from Table 8. As shown in Table 10, cosine decay consistently achieves lower losses than the constant schedule across the tested learning rates. This observation aligns with findings from the Chinchilla paper [35], which indicates that when the cosine cycle length significantly exceeds the total number of training steps (by at least 25%), model performance tends to deteriorate. Notably, the longest cycle length arises when the schedule is constant. In contrast, setting it to approximately match the training duration yields the best final loss [35]. To this end, we adopt cosine decay schedule with peak learning rate of 1e3 for Avey, especially that it provides the lowest loss across all the runs. We note, however, that Table 10 also shows that the lowest loss does not correspond to the best downstream task performance. For instance, the loss of 3.308 under the constant learning rate schedule resulted in slightly better benchmark performance than the lower loss of 3.218 under the cosine decay schedule, both using the same peak learning rate. While constant learning rates can be effective for short or exploratory runs (this study uses only 10 billion tokens), it is generally the case that, as the number of training tokens increases, the learning rate must decrease to allow the optimizer to settle into lower-loss region [103]. Hence, schedules with decay are typically favored for longer or large-scale training runs [35, 4]."
        },
        {
            "title": "J Scaling Laws",
            "content": "In this section, we present scaling law study comparing how well Avey, Transformer++, Mamba, and RWKV-7 scale with increasing compute. For Avey, we use the full architecture, including both the ranker and neural processor. All models are trained at three different sizes, as defined in Appendix and summarized in Table 11. To ensure compute-optimal scaling, we proportionally increase the number of training tokens with model size, following the Chinchilla scaling laws [35]. Specifically, and in line with the methodology from [25], we use 2B, 7B, and 20B tokens to train models with approximately 150M, 500M, and 1.5B parameters, respectively. Lastly, we employ the 26 Table 11: Model configurations used in the scaling law experiments. Each model is trained at three different sizes and numbers of training tokens increased proportionally, following the Chinchilla scaling laws. Model # Layers (# Heads) Embedding Dim. Learning Rate # Tokens Avey-153M Avey-496M Avey-1.5B Transformer++-152M Transformer++-488M Transformer++-1.5B Mamba-153M Mamba-496M Mamba-1.5B RWKV-7-152M RWKV-7-488M RWKV-7-1.5B 26 104 48 12 (12) 26 (16) 32 (16) 28 42 52 12 30 24 768 768 2048 768 1024 1664 768 1280 2048 768 1024 2048 1.00e-03 1.00e-03 1.00e-03 3.00e-03 1.50e-03 1.25e-03 1.00e-03 1.00e-03 1.00e-03 3.00e-03 1.50e-03 1.25e-03 2B 7B 20B 2B 7B 20B 2B 7B 20B 2B 7B 20B Figure 6: Scaling law results, comparing how perplexity decreases as compute increases, assuming three model sizes of 150M, 500M, and 1.5B parameters and proportional increase in the number of training tokens with model size, following the Chinchilla scaling laws. same batch size across all the models to control for variability in the number of gradient update steps, especially because of training with limited number of tokens. Fig. 6 presents the scaling results. The x-axis represents the total training compute budget, calculated as the product of the number of training tokens and model parameters, which serves as proxy for the total FLOPs required to train each model. As shown, Avey exhibits the steepest decline in perplexity as compute increases. Although it begins with relatively high perplexity11, it improves more rapidly than the other models, indicating strong scaling behavior and greater benefit from additional compute. Following Avey, Transformer++ demonstrates the next-best scaling trend, outpacing Mamba and RWKV-7. While Mamba achieves relatively low perplexity at smaller compute budgets, it does not scale as effectively as Avey or Transformer++. Finally, RWKV-7 performs well at low compute but shows the flattest scaling curve, suggesting it gains the least from additional training compute. 11Avey can achieve substantially lower perplexity under alternative configurations. For example, the small model (150M parameters) with sequence length = 1024, split size = 16, and top-k = 15 achieves much lower perplexity as shown in Table 8. The configuration used in this experiment-- and in Sections 3.3 and 3.4, was selected based on its strong downstream benchmark performance, rather than optimal perplexity. As discussed in Appendix B, perplexity serves as useful proxy for modeling capability, but does not always align perfectly with downstream task accuracy. 27 Table 12: Ablation results comparing Avey variants, with individual components removed or replaced. Model Variant ppl ARC-C ARC-E Hella OBQA PIQA SIQA Wino Average Avey full (all features) Avey without dynamic parameterization Avey without bypassing Avey without embedding expansion Avey without weighting selected splits Avey without the ranker Avey with self-attention in place of neural proc. 30.00 34.31 32.55 39.94 31.17 29.48 31.39 25.17 25.00 22.61 22.44 22.78 23.72 22.61 39.90 40.66 38.38 37.92 38.55 38.59 39. 33.59 32.99 32.31 28.75 33.25 32.52 31.99 28.8 28.8 28.0 25.4 28.0 28.0 28.0 65.56 65.34 64.20 62.40 65.89 63.66 64.58 37.62 36.64 38.28 38.64 37.82 37.67 38.33 51.62 50.51 52.09 52.01 52.09 53.20 51.38 40.32 39.99 39.41 38.22 39.77 39.62 39."
        },
        {
            "title": "K Ablation Study",
            "content": "In this study, we perform series of ablation experiments on the core components of Avey, leveraging the best configurations identified in Appendices B, C, D, E, F, H, and I. All experiments are conducted using Aveys complete architecture, comprising both the ranker and neural processor, with the small model variant (153 million parameters) as the baseline. For this study, we trained this model on 10 billion tokens from the FineWeb dataset, using the training methodology and hyperparameters detailed in Appendix A. We begin by examining the effect of dynamic parameterization on both perplexity and downstream benchmark performance. As described in Section 2.2.2, dynamic parameterization allows each neuron in the contextualizer network to compute cosine similarity between its input embedding and the embeddings of all other neurons, weight those embeddings accordingly, and aggregate them via learned weighted sum. This mechanism induces selectivity, as defined in [25], into the neural processor, thereby making its parameterization dynamic (or input-dependent). Table 12 summarizes the results. Disabling this component results in 14.3% increase in perplexity and 0.8% drop in average performance, highlighting its importance. Second, we evaluate the impact of the partial-embedding bypassing technique introduced in Section 2.2.1. This method involves forwarding portion of each expanded embedding directly to the fuser, allowing raw, distinctive features to be preserved and potentially serve in promoting more diverse representations. As shown in Table 12, removing this mechanism results in an 8.5% increase in perplexity and 2.2% drop in average performance, underscoring its significance. Third, we set the expansion factor in the enricher to 1, effectively disabling the expansion of input embeddings. As illustrated in Table 12, this modification results in 33.1% increase in perplexity and 5.2% drop in average performance, corroborating the critical role of embedding expansion in the models effectiveness. Fourth, we remove the weighting of each selected split by its corresponding normalized MaxSim score, thereby preventing the contextualizer from scaling each splits contribution during contextualization. As depicted in Table 12, this adjustment leads to 3.8% increase in perplexity and 1.37% drop in average performance, indicating the importance of this technique. Fifth, we evaluate Avey without the ranker to assess its impact on downstream task performance, beyond its primary role of enabling effective extrapolation past the trained context window. As shown in Table 12, the ranker does indeed enhance the neural processors performance, primarily by improving the quality of contextualization through more meaningful cross-token interactions. We note, however, the slight increase in loss (by 0.5%), which again highlights (as in Appendix B) the discrepancy between the objectives of pertaining and downstream tasks. Finally, we replace Aveys neural processor with self-attention to assess the relative contribution of each component to Aveys overall performance, given that both are designed to pursue cross-token interactions. As illustrated in Table 12, this substitution leads to 4.6% increase in perplexity and 2.1% decline in average performance, underscoring the significance of the neural processor and suggesting that self-attention is less effective within Aveys architectural framework. Additional Short-Range Benchmark Results To mitigate the effects of fluctuations in pre-training loss and downstream benchmark scores, we reported in Section 3.3 average results across the final three checkpointstaken at 5 billion token in28 Table 13: Performance of all models across short-range benchmarks at 90B, 95B, and 100B training tokens. Model (# of Tokens) ARC-C ARC-E HellaSwag PIQA OBQA SIQA Winogrande Avg. Avey-153M (100BT) Avey-153M (95BT) Avey-153M (90BT) Transformer++-152M (100BT) Transformer++-152M (95BT) Transformer++-152M (90BT) Mamba-144M (100BT) Mamba-144M (95BT) Mamba-144M (90BT) RWKV-7-168M (100BT) RWKV-7-168M (95BT) RWKV-7-168M (90BT) Avey-496M (100BT) Avey-496M (95BT) Avey-496M (90BT) Transformer++-488M (100BT) Transformer++-488M (95BT) Transformer++-488M (90BT) Mamba-500M (100BT) Mamba-500M (95BT) Mamba-500M (90BT) RWKV-7-501M (100BT) RWKV-7-501M (95BT) RWKV-7-501M (90BT) Avey-1.52B (100BT) Avey-1.52B (95BT) Avey-1.52B (90BT) Transformer++-1.5B (100BT) Transformer++-1.5B (95BT) Transformer++-1.5B (90BT) Mamba-1.4B (100BT) Mamba-1.4B (95BT) Mamba-1.4B (90BT) RWKV-7-1.5B (100BT) RWKV-7-1.5B (95BT) RWKV-7-1.5B (90BT) 23.98 24.23 24.91 23.29 23.55 24.06 24.32 23.63 24.57 23.89 24.23 24.40 27.13 27.90 27. 25.68 27.39 27.13 29.27 28.67 27.99 26.96 27.39 27.05 30.89 32.34 30.55 30.29 30.97 28.75 32.42 32.85 32. 32.42 33.11 33.28 42.30 42.09 42.59 43.43 43.14 42.93 43.73 43.69 43.18 43.14 42.89 43.01 48.99 49.20 48. 48.02 47.90 48.36 51.26 51.39 50.42 49.83 49.24 49.03 56.36 56.94 56.36 56.19 57.07 55.60 57.87 57.91 58. 59.55 58.88 58.71 39.57 39.21 39.31 39.47 39.51 38.97 40.82 40.51 40.33 41.50 41.77 41.38 52.17 51.74 51. 52.92 52.69 52.37 54.45 54.25 53.76 54.49 54.66 54.46 61.49 61.63 61.15 64.28 63.87 63.45 64.78 64.37 64. 64.59 64.49 64.21 29.8 31.2 33.2 29.4 30.2 29.8 29.8 32.2 29.2 29.8 29.2 30.0 32.0 33.0 32. 31.6 31.6 32.0 34.0 34.8 34.6 36.0 35.6 37.2 34.4 37.6 38.4 38.8 37.0 38.2 38.4 35.4 36. 37.4 37.0 37.0 68.61 68.23 68.28 67.03 67.14 66.87 68.28 68.06 68.61 68.72 68.99 68.44 72.47 73.07 71. 72.69 72.36 71.33 73.88 72.69 72.52 73.23 73.78 73.72 75.84 75.57 75.41 76.12 76.17 75.73 76.61 76.33 76. 76.82 76.88 76.82 39.05 39.15 39.20 39.10 38.69 38.89 39.00 39.82 39.41 39.41 39.10 39.00 40.53 40.63 39. 39.56 40.07 40.17 40.38 40.89 41.25 39.30 39.15 39.76 42.07 41.76 42.17 42.27 42.07 42.37 42.48 42.02 41. 41.86 41.71 41.56 51.85 50.28 51.70 50.51 50.99 49.17 52.41 53.35 52.41 50.99 51.14 51.14 54.54 53.51 55. 55.96 54.22 55.56 54.70 55.33 56.43 55.17 55.80 56.20 56.59 58.09 56.51 61.33 61.72 61.09 62.27 60.93 61. 59.67 60.38 60.14 42.02 42.06 42.74 41.90 41.89 41.24 42.62 43.61 42.53 42.35 42.48 42.48 46.55 46.72 46. 46.06 46.12 46.16 48.28 48.29 48.14 47.71 47.95 48.20 51.09 52.42 51.51 52.75 52.70 52.19 53.55 52.69 52. 53.19 53.21 53.10 tervals (i.e., at 90B, 95B, and 100B tokens)for all models evaluated, namely, Avey, Transformer++, Mamba, and RWKV-7. In this section, we provide the detailed performance scores for each model at each checkpoint in Table 13. In addition, we summarize the mean, standard deviation, standard error, and 95% confidence interval for each model, computed across the three checkpoints, in Table 14. The illustrated statistical results reveal meaningful variance between models and across runs of the same model. For instance, while Mamba achieves the highest mean score of 42.73 among all the models in the small parameter regime (150M parameters), it also exhibits relatively wide confidence interval (42.06, 43.40) and moderate standard deviation, highlighting nontrivial variability in performance across checkpoints. Regarding variability across models, Table 14 shows overlapping confidence intervals, indicating that model rankingsparticularly which model achieves the highest mean performancecould shift under minor experimental changes (e.g., random initialization, stochastic optimization, etc.). For example, in the small model regime, while Avey does not surpass Mamba in mean performance, their confidence intervals substantially overlap in the range (42.06, 43.24), suggesting that the two models are statistically comparable and that Avey could outperform Mamba in some runs. Similarly, narrow but meaningful overlap exists between Avey and RWKV-7 in the range (42.46, 42.51), implying that Avey may occasionally match or slightly exceed RWKV-7 in certain cases. Lastly, although Mamba has the highest mean in this setting, its confidence interval also overlaps with RWKV-7, indicating that the difference in performance between the two models is not statistically significant and that RWKV-7 could match or slightly outperform Mamba in some runs. 29 Table 14: Summary statistics for each model with different sizes computed over the last three checkpoints (i.e., at 90B, 95B, and 100B training tokens). Model Mean Standard Deviation Standard Error 95% Confidence Interval Avey-153M 42.32 Transformer++-152M 41.72 42.73 Mamba-144M 42.48 RWKV-7-168M Avey-496M 46.82 Transformer++-488M 46.65 48.23 Mamba-500M 48.00 RWKV-7-501M Avey-1.52B Transformer++-1.5B Mamba-1.4B RWKV-7-1.5B 51.53 52.54 53.12 53.17 0.3683 0.1821 0.2700 0.0094 0.1895 0.0507 0.0835 0.1807 0.4497 0.3218 0.3783 0.0553 0.2126 0.1052 0.1559 0. 0.1094 0.0293 0.0482 0.1043 0.2596 0.1858 0.2184 0.0320 (41.41, 43.24) (41.27, 42.17) (42.06, 43.40) (42.46, 42.51) (46.35, 47.29) (46.52, 46.77) (48.03, 48.44) (47.55, 48.45) (50.41, 52.65) (51.74, 53.34) (52.18, 54.06) (53.03, 53.30) Figure 7: Performance comparison between Transformer++, Mamba, RWKV-7, and Avey on S-NIAH-1 and S-NIAH-2. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens). All models use 150M parameters. In the medium model regime (500M parameters), Avey outperforms Transformer++ in mean performance, but they are statistically comparable. In contrast, the performance gap between Avey and both Mamba and RWKV-7 is statistically significant at the 95% confidence level, indicating that both models clearly outperform Avey in this setting. In the large model regime (1.5B parameters), while Avey does not outpace Transformer++ in average performance, their confidence intervals overlap substantially, suggesting that Avey could potentially surpass Transformer++ in some runs. There is also limited overlap between Avey and Mamba, indicating that while Mamba generally performs better, Avey might outperform it in certain cases. In contrast, the difference between Avey and RWKV-7 is statistically significant at the 95% level, confirming that RWKV-7 consistently outperforms Avey in this setting. Finally, although RWKV-7 has slightly higher mean than Mamba (53.17 vs. 53.12), the meaningful overlap in their confidence intervals implies that the difference between them is not statistically significant, and either model could outperform the other depending on minor experimental factors. Additional Long-Range Benchmark Results In Section 3.4, we presented results for Avey, Transformer++, Mamba, and RWKV-7 under the medium parameter regime (500M parameters) on the standard Single Needle-In-A-Haystack (SNIAH) benchmark suite from RULER [38], which is designed to evaluate models abilities to handle long-range dependencies. The S-NIAH benchmark, along with two of its common variantsSNIAH-1 and S-NIAH-2was described in detail in Section 3.4. In this section, we extend our analysis by reporting results under two additional model regimes, small (150M parameters) in Fig. 7 and large (1.5B parameters) in Fig. 8. Akin to the experiment in Section 3.4, Transformer++, 30 Figure 8: Performance comparison between Transformer++, Mamba, RWKV-7, and Avey on S-NIAH-1 and S-NIAH-2. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens). All models use 1.5B parameters. Mamba, and RWKV-7 were trained with context window of 2048 tokens, while Avey was trained with shorter window of only 512 tokens. In both the small and large model regimes, under S-NIAH-1 and S-NIAH-2, Transformer++, Mamba, and RWKV-7 perform well when the haystack length is 2k, fitting within their trained context windows. Yet, Mamba consistently underperforms Transformer++ and RWKV-7, likely due to solely relying on recurrence, which somehow treats the entire input uniformly, making the model more susceptible to distractions from irrelevant tokens. In contrast, RWKV-7, which combines recurrence with attention, performs better than Mamba but remains inferior to Transformer++, potentially because the attention mechanism allows it to prioritize tokens relevant to the needle, while the recurrent component may still contribute to signal dilution. Transformer++, relying exclusively on full attention, achieves the best performance within the context window by effectively focusing on relevant tokens without interference from recurrence-based mechanisms. Nonetheless, once the haystack length exceeds the models context windows, all the three models exhibit substantial drop in performance. Mamba and RWKV-7, however, show minimal generalization beyond their training limits compared to Transformer++, as previously discussed in Section 3.4. Compared to Transformer++, Mamba, and RWKV-7, Avey generalizes far beyond its trained context window on both S-NIAH-1 and S-NIAH-2 across all parameter regimes, underscoring its strong extrapolative capabilities (as also shown in Section 3.4). Notably, this holds despite Avey being trained with context window of only 512 tokens. For example, in the small parameter regime, Avey achieves an accuracy of 97.8% on S-NIAH-1 with 64k-token haystack, while Transformer++, Mamba, and RWKV-7 drop to 0%, 0%, and 0.6%, respectively. Similarly, on S-NIAH-2 at the same haystack length, Avey attains 68.8% accuracy, whereas Transformer++, Mamba, and RWKV-7 fall to 0%, 2.8%, and 2%, respectively. Comparable trends are observed in the large parameter regime as well, as illustrated in Fig. 8. An interesting observation arises in the small parameter regime, where Avey outperforms all other models on S-NIAH-1 with haystack length of 2k, knowing that this length exceeds its trained context window width and enables it to demonstrate its strong extrapolative capability. However, this pattern does not persist in the medium (see Fig. 5 in Section 3.4) and large (see Fig. 8) parameter regimes, where Transformer++ and RWKV-7 outperform Avey on the same benchmark at 2k length, despite this length still surpassing Aveys trained context window. This suggests that these models, with their increased capacity, are able to compensate for the challenge posed by S-NIAH-1, and entails that Avey might benefit from longer training context window. In this paper, we kept Aveys context window fixed at 512 tokens across all parameter regimes. All tuning experiments related to sequence length, split size, and top splits (see Section G) were conducted exclusively using the small model size. It is plausible that with larger capacity, Avey could more effectively leverage longer sequences by retrieving and contextualizing larger set of relevant tokens while filtering out less informative ones, thereby enhancing contextual representations and further boosting performance. Investigating the relationship between sequence length and model size in Avey is an interesting direction for future work."
        },
        {
            "title": "N Complexity Analysis",
            "content": "As indicated in Sections 2.1, 2.2.1, 2.2.2, and 2.2.3, the training time complexities of the ranker, enricher, contextualizer, and fuser are O(N 2d), O(N md), O(N kCmt), and O(N md), respectively, where is the sequence length, is the split size, is the original embedding dimension, is the projected embedding dimension (with > d), mt is the tail part of forwarded to the contextualizer, is the context width (with ), and is the number of splits contextualized with each current split. This yielded an overall training time complexity of O(N 2d), assuming that scalar multiply-add operations (e.g., those used in computing cosine similarity for MaxSim) and comparisons (e.g., those used to determine maximum scores) are constant-time. During inference, the time complexities of the enricher, contextualizer, and fuser remain unchanged. However, the rankers analysis slightly changes, as at each time step (i.e., upon predicting new token), the current split is compared against all previous splits. More precisely, at each time step t, the current splitdenoted as split and incrementally filled as tokens are generatedis compared against all 1 preceding splits, each consisting of tokens. Consequently, the cost of comparing tokens in split (with S) against tokens in previous split is O(t d). Now, if we let = N/S N/S for large be the number of splits, the total inference cost can be defined as: (cid:88) (cid:88) i= t=1 (i 1) O(t d) Simplifying the inner summation yields: (cid:88) t=1 (i 1) O(t d) = (i 1) O(S d) (cid:88) t=1 = (i 1) O(S d) S(S + 1) 2 = (i 1) O(S2 d) Substituting this back into the outer summation gives: (cid:88) i=1 (i 1) O(S2 d) = O(S2 d) (cid:88) i=1 (i 1) = O(S2 d) (M 1) 2 Substituting with N/S for large results in: O(S2 d) (N/S)2 2 = O(S2 d) O(N 2/S2) = O(N 2 d) Therefore, the cost per token becomes: O(N 2d) = O(N d)"
        },
        {
            "title": "O Related Work",
            "content": "Recurrent Neural Networks (RNNs) [19, 79] are designed to process sequential data by capturing temporal dependencies, making them well-suited for tasks where input order is essential. However, their cyclical nature limits their potential for parallel computation and exposes them to vanishing and exploding gradient problems. As result, they typically struggle to effectively learn long-range dependencies. While architectures like Long Short-Term Memory (LSTM) [34] and Gated Recurrent Units (GRU) [8] mitigate these gradient-related issues, they remain slow to optimize and challenging to scale due to retaining RNNs core recurrent and sequential structure. In contrast, the Transformer [98] employs self-attention mechanism to process each sequence of tokens simultaneously. More precisely, it promotes two key design principles: (1) recurrent-free architecture, which enables parallel computation of token embeddings, while still capturing their 32 order through positional encodings, and (2) multi-head self-attention approach, which facilitates cross-token interactions to further enrich the expressiveness of embeddings. These innovations make the Transformer highly effective, as well as parallelizable and efficient to train. However, they also cause its computational and memory requirements to scale quadratically with sequence length, making it expensive and less efficient for very long sequences. To address the Transformers quadratic computation and memory costs, wide array of approaches have been proposed, including linear attention [50, 48, 9, 71, 106], sparse or local attention [104, 7, 67], context compression [74, 99, 90, 78], and modified attention computations [93, 100, 92], to mention just few. Notably, the Attention-Free Transformer (AFT) [106] offers linear drop-in replacement for the quadratic self-attention mechanism. In particular, it weights key and value vectors using learned positional biases and integrates them with query vectors via element-wise multiplication. As such, it eliminates the need to compute and store the costly attention matrix while still preserving global query-value interactionswithout requiring architectural modifications or additional tuning. Furthermore, AFT introduces variants such as AFT-local and AFT-conv, which leverage local attention patterns to reduce parameter count and further improve computational and memory efficiency. RWKV-4 [68] (the first 3 versions were experimental [52]) capitalizes on AFT and suggests combining the strengths of both Transformers and RNNs. To elaborate, unlike Transformers and akin to RNNs, it does not process each input token solely based on its own embedding, but rather as weighted sum of its embedding and that of the preceding one. To the contrary of traditional RNNs and similar to Transformers, it adopts self-attention, but an extended version of it, namely, that of AFT. This hybrid approach allows RWKV-4 to maintain some of the modeling capabilities of RNNs, while benefiting from the parallelization and scalability features of Transformers. More precisely, RWKV-4 extends AFT in two distinct ways: (1) it introduces an additional parameter to handle each current token independently, and (2) it implements per-time-step decay mechanism that selectively removes older content from the recurrent hidden state in data-dependent manner. This decay mechanism addresses central limitation of linear attention, which pertains to the lack of systematic way to discard outdated information [83, 102]. Architecturally, RWKV-4 consists of homogeneous stacked residual blocks, each encompassing two units, time-mixing and channel-mixing ones. The time-mixing unit applies linear attention across tokens, while the channel-mixing unit integrates each element of the current tokens embedding with its corresponding element from the preceding token embedding, leveraging the output of the time-mixing unit. RWKV-5 [69] enhances RWKV-4s architecture and learning decay mechanism by replacing traditional vector-valued states with more expressive multi-head, matrix-valued ones. Moreover, it reconfigures receptive states, incorporates supplementary gating mechanisms, and dynamically learns the linear interpolation between the current and preceding token embeddings instead of relying on pre-defined hyperparameters. RWKV-6 [69] promotes new application of low-rank adaptation functions [39, 52]. Specifically, it makes the linear interpolation between the current and preceding tokens data-dependent to improve the selectivity of the model in retaining and discarding information. Additionally, it replaces the static decay mechanism with dynamic one, allowing each element in the decay vector to fluctuate independently over time in response to the input. The decay strategies of RWKV-4, RWKV-5, and RWKV-6 still cannot remove values stored at specific keys. DeltaNet [83] overcomes this drawback by partially replacing the values stored at current keys with equivalent new values, enabling models to erase outdated memories and include up-to-date ones on per-key basis. However, it only allows fixed scalar fraction of value to be replaced from state via an in-context learning rate parameter, thus demonstrating rigidity in adapting to varying data contexts. RWKV-7 [70] builds upon the principles of DeltaNet and introduces vector-valued in-context learning rate instead of scalar-valued one. This allows selective replacement of state data on channel-wise basis. Furthermore, RWKV-7 employs vector-valued decay mechanism and uses additional low-rank projections to optimize the trade-off between the number of parameters, computational efficiency, and downstream performance. Lastly, it incorporates Value Residual Learning [110], which improves the propagation of initial local information via utilizing residual 33 connection between the value vectors of the current layer and those of the first layer prior to the attention operation, resulting in enhanced language modeling performance. Most recently, RWKV-X [37] proposed combining the strengths of RWKV and sparse attention, drawing inspiration from Mixture of Block Attention (MoBA) [57]. In particular, RWKV-X restricts each query to attend only to small, relevant subset of the input, thus reducing computational cost and facilitating the modeling of longer-range dependencies. More precisely, rather than allowing each token to attend to every other token in the sequence (as in traditional self-attention), it constrains each tokens attention to limited subset (hence, making it sparse), while maintaining the coupling between the input sequence and context window. Similar to RWKV, RetNet [91] adopts linear attention and promotes hybrid approach that blends Transformerand RNN-like representations, yet with decay-based memory unit. Specifically, it divides the input sequence into chunks, wherein the Transformer-like parallel representation is applied. Additionally, it enables propagating information sequentially across chunks using the RNNlike representation. Lastly, it uses multiple attention heads, each governed by distinct decay rate, and replaces LayerNorm [1] with GroupNorm [101]. Although linear attention has been proposed as promising alternative to quadratic softmax attention [48, 9, 47, 71], existing implementations of it are in practice slower than optimized versions of softmax attention [14, 12, 102]. From an accuracy standpoint, linear attention generally underperforms conventional softmax attention, sometimes by significant margin in language modeling [47, 102]. To this end, and in light of the exponentially growing complexity associated with overcoming the limitations of Transformer-based architectures, there has been renewed interest in RNN-based alternatives in recent years. Notably, Structured State Space Sequence (S4) models [27, 28], inspired by the classical state space models (SSMs) [44], have emerged as promising paradigm for sequence modeling. These models describe the temporal evolution of system using differential equations, offering continuous-time formulation of dynamics, and can be viewed as generalized versions of RNNs. An SSM as concept has broad meaning, which simply refers to the notion of any recurrent process with latent state [25]. From this perspective, the RNN-like linear attention model proposed and formulated by [48] can be interpreted as degenerate linear SSM. Interestingly, this justifies the usage of decay factor in RetNet and RWKV, especially that decay term (or forget gate) has been shown to be crucial in RNNs [34, 97, 8]. Numerous variants of SSMs [27, 29, 30, 51, 58, 65, 87] have demonstrated strong performance across range of domains, including audio and vision [24, 63, 81]. Nonetheless, these variants have struggled with language modeling, often lagging behind Transformers by several points in perplexity [27]. From an efficiency standpoint, however, SSMs have shown encouraging results in language modeling. For instance, S4 [27, 28], prominent SSM, converts the continuous-time state update equation of SSMs into discrete form, hence, enabling parallel sequence modeling. Moreover, it utilizes the HiPPO (High-Order Polynomial Projection Operator) initialization [26], which alleviates the vanishing gradient problem and facilitates processing longer sequences. Another example of SSMs is H3 [20], which improves language modeling by allowing both, the recall of earlier tokens and token-wise comparisons within sequence. H3 extends S4 by suggesting statepassing algorithm that enhances computational efficiency on modern hardware. This advancement reduces the hardware-related barriers that have traditionally limited the scalability of SSM-based architectures. Hyena [72] capitalizes on H3 by replacing its S4 layer with an MLP-parameterized global convolution [77]. S5 [87] proposes using parallel scan [59] to parallelize S4. Liquid S4 [31] augments S4 with an input-dependent state transition matrix, computed convolutionally in the frequency domain (which is computationally efficient) and mapped back to the time domain using an inverse Fourier transformation. SGConv [51], LongConv [21], MultiresConv [86], and Toeplitz Neural Network [73] all focus on the convolutional representation of S4 as well, aiming to enhance its efficiency [25]. Most recently, Mamba [25] introduced new class of SSMs known as selective SSMs, specifically designed to improve the performance of language modeling tasks. Mamba addresses key limitation 34 in SSMs, namely, their inability to selectively process inputs in an input-dependent manner (i.e., focus on or ignore specific parts of the input sequence). Consequently, it makes the SSM parameters input-dependent, but introduces technical challenge since traditional SSMs are inherently designed to be timeand input-invariant to ensure computational efficiency. To overcome this challenge, Mamba proposes hardware-efficient parallel scan (or prefix sum) algorithm [6], which enables recurrent-style computation without explicitly materializing the expanded state. This design precludes costly I/O operations across GPU memory hierarchies and accelerates both, training and inference. Tri Dao and Albert Gu [13] argue that various approaches to operating SSMs can be reframed as matrix multiplication algorithms involving specific class of structured matrices known as semiseparable matrices. They further leverage the language of tensor contractions to prove the recurrent formulation of linear attention as proposed by [48], before generalizing it to new family of structured masked attention (SMA). Subsequently, Tri Dao and Albert Gu demonstrated that SSMs and Transformers are fundamentally connected, governed by the mathematical framework of semiseparable matrices and SMA. Additionally, they developed rich state space duality (SSD) framework of theoretical connections between SSMs and various forms of attention. This framework facilitated the design of Mamba-2, an extended version of Mamba, which aims to improve its efficiency (not performance). Mamba-2 utilizes scalar data-dependent gating mechanism (like the ones proposed by [71, 91, 3]), which enables transforming its recurrent structure into matrix-multiply form, thus allowing for efficient execution on tensor cores and better support for larger hidden state sizes. The strategy of the SSD framework mirrors that of linear attention [48], which established connection between autoregressive attention mechanisms and linear RNNs via showing an equivalence between \"dual forms\" of quadratic kernelized attention and specific type of linear recurrence. Conceptually, the SSD framework seeks to transfer algorithmic and systems-level optimizations originally developed for Transformers to the realm of SSMs. Its overarching goal is to enable the development of architectures that outperform Transformers, while scaling more efficiently with sequence length. Finally, several works, including Tolstikhin et al. [95], Melas-Kyriazi [60], Touvron et al. [96], and Ding et al. [18], among others, have questioned the necessity of self-attention, particularly in the context of Vision Transformers. In contrast, Liu et al. [53] introduced gMLP, an MLP-based alternative to BERT-style Transformers [16] that (partially) eliminates self-attention but ultimately underperforms average performance on downstream NLP tasks. gMLP encompasses channel (hidden) and spatial (cross-token) projections with multiplicative gating and static parameterization. Its gating mechanism is reminiscent of Gated Linear Units (GLUs) [15, 84, 100], as well as earlier architectures such as Highway Networks [88] and LSTM-RNNs [34]. key distinction, however, is that gMLP applies gating on the spatially projected dimension and not the hidden one. The gated embedding-wise neural network in Aveys contextualizer draws inspiration from gMLP. Unlike all previously mentioned models, Avey entirely abandons self-attention and recurrence, introducing new architecture composed of ranker and dynamically parameterized neural processor. The ranker identifies the most relevant tokens for contextualization, while the neural processor contextualizes them data-dependently. This design decouples sequence length from context width, enabling efficient processing of arbitrarily long sequences without diminishing the influence of distant yet important tokens. At the core of Aveys architecture is weighted-selective-split interaction mechanism, which filters out irrelevant tokens beyond the context window and enables direct interactions only with relevant ones, thus preserving their influence irrespective of sequence length. In addition, Avey employs partial-embedding bypassing technique that retains portion of each tokens raw, distinctive features before fusing them with its contextualized ones through neural network. This technique boosts the performance of Avey (as shown in Appendix K) and might help mitigate issues such as entropy collapse [107] and over-smoothing [109, 85], especially at large-scale, when the depth of the model is increased significantly."
        },
        {
            "title": "P Limitations",
            "content": "The scope of our work is limited to textual data and does not involve other modalities such as images, audio, or genomics. Additionally, our evaluation of Avey is restricted to standard autoregressive language modeling, benchmarking it against popular open-source architectures using both pretraining metrics (perplexity) and zero-shot evaluations on established NLP benchmarks. As result, we do not investigate Aveys ability to construct bidirectional contextualized word representations, as done in BERT [16]. We leave this for future work. Finally, the paper focuses solely on effectiveness rather than efficiency. While we provide complexity analysis showing that Avey exhibits quadratic training time like Transformers, our current implementation is slower. As such, further engineering efforts are required to optimize it."
        }
    ],
    "affiliations": [
        "Avey AI"
    ]
}