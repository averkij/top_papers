{
    "paper_title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions",
    "authors": [
        "Mohammed Mehedi Hasan",
        "Hao Li",
        "Gopi Krishnan Rajbahadur",
        "Bram Adams",
        "Ahmed E. Hassan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Model Context Protocol (MCP) introduces a standard specification that defines how Foundation Model (FM)-based agents should interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear. Hence, we examine 856 tools spread across 103 MCP servers empirically, assess their description quality, and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, and then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These results indicate that achieving performance gains is not straightforward; while execution cost can act as a trade-off, execution context can also impact. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs."
        },
        {
            "title": "Start",
            "content": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions MOHAMMED MEHEDI HASAN, Queens University, Canada HAO LI, Queens University, Canada GOPI KRISHNAN RAJBAHADUR, Queens University, Canada BRAM ADAMS, Queens University, Canada AHMED E. HASSAN, Queens University, Canada The Model Context Protocol (MCP) introduces standard specification that defines how Foundation Model (FM)-based agents should interact with external systems by invoking tools. However, to understand tools purpose and features, FMs rely on natural-language tool descriptions, making these descriptions critical component in guiding FMs to select the optimal tool for given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear. Hence, we examine 856 tools spread across 103 MCP servers empirically, assess their description quality, and their impact on agent performance. We identify six components of tool descriptions from the literature, develop scoring rubric utilizing these components, and then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These results indicate that achieving performance gains is not straightforward; while execution cost can act as trade-off, execution context can also impact. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs. CCS Concepts: Software and its engineering Empirical software validation. Additional Key Words and Phrases: Model context protocol, MCP, tool description, AI agents, smells, prompt engineering ACM Reference Format: Mohammed Mehedi Hasan, Hao Li, Gopi Krishnan Rajbahadur, Bram Adams, and Ahmed E. Hassan. 2026. Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions. 1, 1 (February 2026), 43 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nModel Context Protocol (MCP) adoption for Foundation Models (FMs), such as GPT, continues\nto grow across domains, including health [16], bioinformatics [104], transportation [11], vision",
            "content": "Authors Contact Information: Mohammed Mehedi Hasan, mohammedmehedi.hasan@queensu.ca, Queens University, Kingston, ON, Canada; Hao Li, Queens University, Kingston, ON, Canada, hao.li@queensu.ca; Gopi Krishnan Rajbahadur, Queens University, School of Computing, Kingston, Ontario, Canada, grajbahadur@acm.org; Bram Adams, Queens University, Kingston, ON, Canada, bram.adams@queensu.ca; Ahmed E. Hassan, Queens University, Kingston, ON, Canada, ahmed@cs.queensu.ca. 6 2 0 2 1 2 ] . [ 2 8 7 8 4 1 . 2 0 6 2 : r Request permissions from permissions@acm.org. 2026 ACM. ACM XXXX-XXXX/2026/2-ART https://doi.org/XXXXXXX.XXXXXXX , Vol. 1, No. 1, Article . Publication date: February 2026. 2 M. Mehedi Hasan et al. Fig. 1. MCP workflow for an FM-based agent. When an agent receives user query, (1) it retrieves tool metadata (name, description, and input schema) via the MCP client; (2) the agent prompts the foundation model (FM) with the user query and retrieved metadata, whereupon the FM plans the solution, formulates the appropriate tool call, and instructs the agent to execute it; (3) the agent executes the tool call via the MCP client; and (4) the agent forwards the tool response to the FM, which synthesizes the final answer for the user. systems [88], and especially software engineering [79]. MCP provides unified interface to bridge FM-based agents with external capabilities (tools) by exposing them to the following three toolsrelated natural-language artifacts: tool name, tool description, and an input schema (with argument names and their data types) to FMs. This purely native natural-language-based alignment of MCP with the agentic ecosystem is driving its massive adoption, leading several major companies, including GitHub, Google Cloud Platform (GCP), and PayPal, to develop and maintain their own MCP servers, commonly referred to as official MCP servers [29]. In parallel, independent developers and open-source contributors have created numerous community MCP servers that integrate wide range of third-party services [29]. In MCP-enabled workflows with these official and community MCP servers, tool descriptions serve as the essential linguistic guide that directs FM behavior. These descriptions convey tools intended functionality, constraints, and usage cues, and shape tool selection, parameterization, and multi-step orchestration [32]. For example, as illustrated in Figure 1, upon receiving user query (e.g., What was the third-quarter income of Apple for 2025?), the agent retrieves the names, descriptions, and schemas of all available tools from the connected MCP servers and injects this metadata into the FMs context, along with the user query. Only then can the FM leverage these artifacts to discover the capabilities of the available tools, plan solution strategy, select an appropriate tool (get_financial_statement), infer the required parameters (ticker=\"AAPL\", financial_type=\"quarterly_income_statement\"), and issue tool call via the agents MCP client. The agent then returns the tool response to the FM, which synthesizes the final answer for the user. From this flow, it is quite clear that if the tool descriptions are defective, underspecified, or misleading, the FM may select the wrong tool, supply invalid or suboptimal arguments, or take unnecessary interaction steps, ultimately reducing the reliability of MCP-enabled systems. In other , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 3 words, the tool description is not merely documentation, but embodies dual nature, as it serves as (i) requirement-like specification that defines the tools expected behavior and parameter constraints [86], and (ii) prompt-like instruction that shapes the models contextual reasoning and decision-making [51]. This hybrid role blurs the boundary between software requirements and natural-language prompts, creating novel design surface where textual or structural imperfections can propagate in the form of specification errors and prompt misguidance. We conceptualize these imperfections as tool description smells, similar to the concept of recurring suboptimal patterns that degrade clarity, correctness, or maintainability [53, 99] in software engineering. While smells in MCP code have been reported [29], the prevalence and distribution of smells in MCP tool descriptions remain largely unexplored. Prior work on prompts has shown that instructional artifacts are composed of multiple components, including personality information, task information covering task intent, user demand, and domain information, as well as demonstration through examples, which together contribute to the accuracy and efficiency of FM [45]. We suggest that MCP tool descriptions exhibit an analogous component structure and that smells can arise at the component level. On the other hand, the efficiency of AI agents using MCP servers has recently come under strict scrutiny, as the tool metadata, repeatedly injected into the FMs context during typical interaction with an FM-based agent, is inflating token usage and increasing execution cost. While emerging techniques like Agent Skills [64] attempt to complement the MCPs use by progressive discovery of capabilities to an FM, and Tool Search [68] is enabling the FM to search for tool on demand rather than always loading them into context, they do not eliminate the reliance on MCP tools and their tool descriptions. Instead, these developments highlight fundamental tension in MCP-enabled agents: resolving tool description smells by augmenting all components may improve semantic guidance and agent performance, but it also consumes scarce context windows and increases costs. Any attempt to augment tool descriptions must therefore justify its cost and, ideally, identify compact representations that preserve effectiveness. Despite extensive work on FM tool calling challenges, including complex function calls [119], large tool sets [72], and security attacks on tool selection [80, 103], there has been no systematic investigation of the quality of tool descriptions in MCP servers or their downstream impact on agent performance. Although industry documentation and practitioner guidelines propose best practices for writing tool descriptions [65, 76, 109], it remains unclear how widely such practices are adopted in the MCP ecosystem and whether they really improve agent behavior under realistic workloads. Recognizing this lack of systematic research we carry out an extensive empirical study of MCP tool description quality and its effect on FM-based agent performance. On dataset of 103 major MCP servers comprising 856 tools, we scan tool descriptions using structured quality rubric designed to identify potential suboptimal design patterns or smells [53]. We then use FMs to automatically fix the identified smells and augment the description. Finally, we utilize the MCP Universe benchmark [48] to evaluate how these augmented descriptions influence FM-based agents performance. This evaluation is structured around the following core Research Questions (RQs). RQ-1: To what extent do MCP tools descriptions contain smells? Motivation. Despite the critical role of tool descriptions as both requirement-like specifications and prompt-like instructions, their quality in real-world MCP deployments remains largely unexamined. In particular, there is no empirical baseline characterizing which components tool descriptions typically include in practice, how frequently they exhibit smells, or how these smells differ between official and community-maintained MCP servers. Prior research shows that smells in software artifacts (e.g., code, tests, datasets, and prompts) increase change-proneness and erode , Vol. 1, No. 1, Article . Publication date: February 2026. 4 M. Mehedi Hasan et al. reliability [30, 39, 74, 117], suggesting similar risks for MCP-enabled agents. We therefore quantify the prevalence and distribution of tool description smells across both official and communitymaintained MCP servers. Findings. We find that 56% of the 856 MCP tool descriptions exhibit an Unclear Purpose smell, indicating that majority fail to articulate their intended functionality clearly to the FM. More broadly, 97.1% of tool descriptions contain at least one smell, and the majority exhibit multiple smell types, particularly Unstated Limitations, Missing Usage Guidelines, and Opaque Parameters affecting them. Tool descriptions from both official and community-maintained servers exhibit these issues, indicating that producing high-quality tool descriptions is challenging for all types of practitioners. RQ-2: How does resolving tool description smells by augmenting all tool description components impact the performance of FM-based agents? Motivation. Given that RQ-1 reveals that tool description smells are widespread, natural scientific question is whether resolving them by augmenting underspecified or missing components improves agent behavior in realistic MCP workflows. Prior software engineering research reports mixed effects from smell removal. While it improves specific quality attributes, such as energy efficiency and runtime performance in some contexts [9], it can also unintentionally alter system behavior in others [95]. In contrast, prompt enhancement techniques for FMs, such as DSPy [38], MIPROv2 [57], and GEPA [1], have consistently demonstrated performance gains. Motivated by these conflicting signals, we investigate whether augmenting MCP tool descriptions with all components improves agent performance in practice and whether such improvements entail trade-offs, if any. Findings. Augmented tool descriptions yield statistically significant increase of 5.85 percentage points in task success rate across domain-model combinations, while causing regressions in 16.67% of cases in the MCP Universe benchmark. They also improve evaluator-level performance, increasing the Average Evaluator score by 15.12%, reflecting higher-quality intermediate execution step completion. These improvements come with trade-off: the average number of execution steps increases by 67.46% (median), indicating that agents expend significantly more interaction steps with richer descriptions. Analysis of this accuracy-cost trade-off reveals that different domain-model combinations support distinct operating points, allowing practitioners to prioritize either peak accuracy or lower execution costs, depending on their deployment requirements. RQ-3: How do different components of the augmented tool description impact the performance of FM-based agents? Motivation. The results of RQ-2 demonstrate that fully augmented tool descriptions improve agent performance but incur substantial execution overhead, intensifying the tension between the semantic completeness and token efficiency. Practitioners warn that excessive detail can saturate the FMs context window,1 making fully augmented descriptions impractical for use cases where the context window is scarce. Furthermore, prior research indicates that not all components of instructional prompts contribute equally to FM behavior [113]. We therefore investigate the impact of individual tool description components through an ablation study to identify minimal effective set that preserves performance while reducing context overhead. Findings. Our results indicate that no single combination of MCP tool description components consistently yields improved performance across all domains and models. However, shorter, targeted descriptions can retain the core semantic content of fully augmented descriptions while achieving statistically equivalent performance. Across all domain-model combinations, removing the Examples 1https://jentic.com/blog/the-mcp-tool-trap , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 5 component does not statistically degrade performance. These findings indicate that practitioners can identify the most impactful components for their specific domain and model as lower-cost alternative without sacrificing effectiveness. Our study makes the following key contributions to MCP ecosystem: (1) Scoring Rubric: We consolidate best practices for writing MCP tool descriptions from multiple sources and propose the first structured scoring rubric to evaluate the quality of individual description components. (2) FM-based Smell Scanner: Using this rubric, we develop the first automated smell detector for MCP tool descriptions, released as part of our replication package 2, enabling developers to identify quality issues in their tools. (3) Tool Description Augmentor: We introduce an FM-based augmentor that systematically resolves smells by enriching tool descriptions with all the components that practitioners can leverage to fix smells. (4) Tool Description Router: We present the first tool description router that allows MCP users to experiment with multiple versions of tool description at runtime and select the variant that performs best in their workflow without changing the code of the MCP servers. (5) Empirical Findings: Through large-scale analysis and benchmark evaluation, we quantify smell prevalence, demonstrate performance effects of augmentation, and identify the impact of different description components on the performance."
        },
        {
            "title": "2 A Motivational Example\nFM-based agents rely on MCP servers to reach external capabilities. As seen in Figure 2a, how an\nagent plans and invokes those tools is shaped by the tool descriptions the FM reads. We illustrate\na common failure seen in real deployments and how a small change in description shifts agent\nbehavior.",
            "content": "Consider scenario where Alex is an AI engineer at financial institution tasked with building finance assistant that answers portfolio questions and simple what-if queries. The team selects popular Yahoo Finance MCP server and wires it to frontier FM. Phase 1: It works. During the development phase, the agent handles simple requests, for example, show the last month of prices and plot recent history. The agent: (1) forwards the user request and tool list to the FM, (2) the FM picks the historical-price tool, (3) the tool returns data, (4) the agent responds cleanly. The team ships the agent. Phase 2: The unseen inefficiency. Days later, users start asking time-bounded questions, for example, What happened around last March? Responses slow down, costs start to rise, and logs show large payloads. Traces reveal that the FM is calling get_historical_stock_prices with broad period that expands to multi-year windows, despite the users question regarding narrow time periods. This inflates the response size of the tool and downstream token usage of the FM. Phase 3: Root cause in the description. The original tool description (Fig. 2a) lists period parameter and says Either use period parameter or use start and end, but never names start or end as explicit parameters, nor gives the data type or format (e.g., whether yyyy-mm-dd or dd-mm-yyyy). Lacking concrete parameter names and guidance, the FM cannot reliably infer how to construct bounded time range and therefore defaults to the period parameter, often selecting ranges broader than required for the user query. This is not model bug; it is specification problem in the tool description. 2https://github.com/SAILResearch/mcp-tool-description-augmentation , Vol. 1, No. 1, Article . Publication date: February 2026. 6 M. Mehedi Hasan et al. (a) Original Yahoo Finance MCP tool description for get_historical_stock_prices showing only reference to start and end without explicit argument names or format specification. (b) Forked Yahoo Finance MCP tool description defining explicit arguments start_date and end_date with specified yyyy-mm-dd format. Fig. 2. Comparison of two Yahoo Finance MCP tool descriptions used by the same FM-based agent. The original version (a) provides ambiguous guidance, while the forked version (b) clarifies parameter names and formats. This difference in description quality directly influences how the FM selects parameters during tool invocation, affecting data retrieval scope, latency, and overall efficiency. Phase 4: small fix that changes behavior. The team tries forked MCP server whose tool description replaces the vague mentions of start_date and end_date with explicit arguments, specifying the expected data format yyyy-mm-dd (Figure 2b). With clear names and constraints, the FM starts issuing date-bounded calls that fetch only the needed window, reducing upstream data volume, latency, and token cost, while improving answer relevance for time-scoped queries. Phase 5: The challenge. This experience shows that seemingly minor description details can materially affect agent performance, prompting Alex and their team to reflect on the existence of broader set of unanswered questions that should be considered when integrating third-party MCP servers into production agents. In particular, teams deploying FM-based agents must understand: (1) How prevalent quality issues or smells are in MCP tool descriptions in practice, and how these issues are distributed across official and community-maintained MCP servers (RQ1). , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 7 (2) Whether systematically augmenting underspecified or missing tool description components improves FM-based agent performance in realistic MCP workflows, and what trade-offs such improvements entail (RQ2). (3) What components of tool description most strongly impact the performance of FM-based agents, and whether there exists any generalizable golden rule for deriving effective tool descriptions across different MCP servers and use cases (RQ3)."
        },
        {
            "title": "3.1 Model Context Protocol (MCP)\nTo let AI applications operate on real systems, FM-based agents rely on tools that perform external\nactions, e.g., searching the web for information retrieval, executing database queries, triggering\nAPI calls, running programs, or performing device management. Anthropic proposed the Model\nContext Protocol (MCP) so that AI agents can discover tools using a standard client-server protocol\nand invoke them with the proper arguments, reducing the need for custom glue code across models\nand frameworks. Since then, MCP has been evolving as an open integration protocol that facilitates\ncommunication between AI applications and external systems, often positioned as a ‚ÄúUSB-C port for\nAI‚Äù [4]. Since its introduction, the protocol has been adopted by major FM providers such as OpenAI,\nMicrosoft, Google, and Cloudflare [29], and currently observes over 20 million weekly downloads\nfor the Python and JavaScript SDKs of MCP servers, signaling strong community acceptance and\nusage.",
            "content": "To enable this interoperability at scale, MCP adopts client-server architecture that cleanly separates AI agents from tool implementations. In this design, an AI agent spins up one MCP client to connect to MCP servers. Each server can run locally or remotely and may expose multiple tools, resources, and prompts [32]. Discovery and capability negotiation follow JSON-RPC data layer with an initialization handshake and list/get methods that let clients enumerate available tools before execution through well-known protocol called reflection [29]. MCP supports multiple transport options including stdio for local, process-to-process connections, and streamable HTTP with optional server-sent events for remote servers and authenticated access. Within this architecture, the primary handshaking interface between MCP servers and foundation models is the tool description itself. Each MCP server exposes its tools through structured description consisting of name, natural-language description, and an input schema. Through reflection, this information is passed to the Foundation Model (FM) by MCP clients, which rely on it to select and invoke the correct tool. As shown in Figure 2, each tool in an MCP server should clearly describe its purpose (i.e., the core functionality it provides) and guide the FM on how to use it. For example, the description of the get_financial_statement tool specifies what it does, retrieving financial statements for company from Yahoo Finance, and how to use it by mentioning the types of statements that can be obtained. well-written tool description therefore conveys the tools purpose, usage guidance, and any relevant caveats or examples [65, 73]. These descriptions play central role at runtime, shaping how agents reason about and execute tool calls. As shown in Figure 1, MCP standardizes how FM-based agents interact with tools through client-server loop. The MCP client orchestrates this communication loop among the FM and available MCP servers, each of which exposes tool metadata, e.g., name, description, and input schema, to the model. (1) Discovery The client queries connected servers via reflection to list available tools and their metadata. In the finance example, it retrieves entries such as get_financial_statement or get_historical_stock_prices from the yahoo-finance-mcp-server. , Vol. 1, No. 1, Article . Publication date: February 2026. M. Mehedi Hasan et al. (2) Planning The client embeds these descriptions in the FMs context along with the user query. Using its language reasoning, the FM selects the correct tool and infers the tools arguments, e.g., ticker=\"AAPL\" and financial_type=\"quarterly_income_statement\". (3) Execution The FM issues tool-call instruction. For this, the MCP client validates parameters, seeks user consent for sensitive actions, and executes the call through the appropriate server. The FM then synthesizes the final answer from the returned data. (4) Reuse Because all interaction occurs through the MCP interface, the same server (e.g., yahoo-finance-mcp-server) can be reused across different agents and frameworks without re-implementation. Given this neatly coupled reasoning loop between the FM and tool descriptions, evaluating MCP-enabled agents requires benchmarks that can faithfully capture both planning correctness and execution behavior. In this study, we adopt the MCP-Universe benchmark [48], which is one of the most comprehensive and widely used benchmarks for evaluating MCP-based agents. MCP-Universe spans multiple domains, such as finance, data analysis, repository management, and information retrieval, and defines realistic, goal-oriented tasks that test an agents ability to choose the correct tool and execute it with the proper arguments. It provides total of 18 MCP servers with 202 tools combined. Each task is evaluated by at least one evaluator, with an average of 3.3 evaluators per task."
        },
        {
            "title": "3.2 Studies on Smells\nSoftware ‚Äúsmells‚Äù have long been studied as indicators of latent design or process issues rather\nthan explicit faults. Fowler and Beck characterize smells as weaknesses that may slow development\nor increase future error risk without being technically incorrect [23]. Because this notion links\nsmells to long-term maintenance costs, the topic has received sustained attention.",
            "content": "Early research studied code smells through taxonomies and catalogs [34, 49, 50], followed by empirical work that examined their evolution and impact [56, 83, 110]. These studies recommend practices such as limiting module size and avoiding large multi-purpose changes to improve maintainability. Detection techniques span textual heuristics [97], repository mining [59], and token-based analysis [36, 101] across multiple languages. Smells are not limited to source code. Architectural smells, such as connector envy and ambiguous interface, have been identified [25, 26], with tools like Arcan supporting automated detection [22]. Test smells, e.g., assertion roulette, mystery guest, and eager test, are also prevalent and can hinder comprehension and maintenance [6, 7, 92]. Beyond implementation artifacts, requirements and design artifacts exhibit smell-like deficiencies [21, 40, 53], and defects traced to later phases are known to be substantially more costly to remedy. With the advent of foundation models (FMs), newer categories of smells have emerged. These include smells in FM-generated code [63, 82] and unit test code [58], as well as data-related smells, such as data leakage, lack of context, or misleading instances, observed in FM-driven systems [98]. In addition, prompt smells [74] have been identified as factors that degrade FM output quality by introducing ambiguity, bias, or inconsistency in instruction formulations. Within the MCP ecosystem, recent work reports code smells in MCP servers [29]. However, to date, no empirical analyses have examined their designor specification-level smells. In this study, we focus on tool descriptions as first-class artifacts and, by analyzing the smells hidden within them, we provide empirical evidence on the impact of tool descriptions on agent performance. , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions"
        },
        {
            "title": "3.3 Refactoring the Smells and Optimization\nIn classical software engineering research, automated code smell identification and refactoring\nhave primarily relied on static analysis techniques grounded in heuristics, coupling-cohesion\nmetrics, and distance-based similarity measures to detect smell manifestations and suggest behavior-\npreserving refactorings [90, 91]. Building on these foundations, more recent refactoring surveys\nhave systematically categorized these efforts into broader families of approaches, including metrics-\nand precondition-oriented methods, clustering- and graph-based analyses, code slicing and dynamic\nanalysis techniques, as well as search-based optimization strategies [42].",
            "content": "Beyond static analysis, growing body of work has operationalized different machine learning (ML) and deep learning (DL) based techniques to identify refactoring opportunities and predict refactoring actions. These include multi-layer perceptrons (MLPs) and recurrent neural network (RNN) variants such as bidirectional long short-term memory (BiLSTM) and gated recurrent unit networks (GRU) for refactoring type prediction [54, 87], convolutional neural network (CNN)- and RNN-based models with embedding pipelines for naming-related refactorings such as Rename Method [44], and RNN encoder-decoder architectures that model refactoring as learned code transformation task [93], as summarized in the survey by Naik et al. [55]. Recent advances in FMs have expanded this space by introducing FM-based strategies for automated smell detection and correction. For instance, iSmell [106] integrates multiple smell detection toolsets through Mixture of Experts (MoE) architecture to identify and refactor code segments with smelly code. Similarly, techniques such as Co-pilot loops [115] employ agentic feedback cycles to iteratively refine code, while UTRefactor [24] targets smell remediation within unit tests. Beyond code and test refactoring, multi-agent frameworks have also been proposed to address architectural and design-level smells [60]. Parallel research has explored optimization in the context of prompt engineering. In addition to heuristic-based search methods [14], recent studies propose FM-based prompt optimization frameworks such as MAP [10], DSPy [38], EASE [107], MIPROv2 [57], and GEPA [1]. These approaches leverage differentiable optimization or feedback-guided refinement and, in several cases (e.g., GEPA), outperform reinforcement learning-based techniques in achieving higher-quality model responses. Despite the functional similarities between prompts and tool descriptions in the MCP ecosystem, no prior study has investigated the optimization or refactoring of tool descriptions. To fill this gap, we examine how FM-based augmentation can be adapted to improve the quality of MCP tool descriptions."
        },
        {
            "title": "3.4 Evaluating MCP-Enabled AI Agents\nEvaluation of agents has progressed rapidly, yet most popular benchmarks emphasize general\nagentic or language capabilities rather than the specific competence of utilizing MCP tools effectively.\nTo broaden coverage, recent studies have begun to assess orchestration and tool-use behaviors in\nrealistic settings. For example, MCP-Universe spans six domains with 231 tasks, using fine-grained\nevaluators to measure goal-directed tool sequencing [48]. LiveMCPBench provides multi-domain,\nmulti-server assessment of multi-step trajectories [52]. Additional efforts include LIVEMCP-101,\nwhich stress-tests long-horizon queries [114], MCPWorld for computer-use agents [111], MCPEval\nfor standard metrics and automated pipelines [46], and MCPToolBench++ for multi-domain and\nmultilingual evaluation [18].",
            "content": "Despite this progress, existing benchmarks maintain static MCP server specifications, i.e., tool names, descriptions, and parameters remain unchanged. Consequently, there is no empirical evidence on whether augmenting or changing the tool descriptions improves or degrades agent , Vol. 1, No. 1, Article . Publication date: February 2026. 10 M. Mehedi Hasan et al. Fig. 3. Overview of the process used to study MCP server tool descriptions. The components in bright yellow boxes represent workflows repurposed from the MCP-Universe benchmark for evaluation and benchmarking, while all other components and processes were introduced in this study. outcomes. Our study addresses this gap by evaluating agents under augmented tool descriptions and by conducting ablation studies to isolate the impact of individual description components on downstream performance."
        },
        {
            "title": "4.1.1 Official MCP documentation search. As MCP was originally proposed by Anthropic and their\ntechnical documents define the baseline tool standard, we first consult Anthropic‚Äôs official MCP\ndocumentation [65] when studying the tool descriptions. This documentation provides guidance on",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 11 the required components of tool description and outlines how these components support model comprehension."
        },
        {
            "title": "4.1.3 Open coding to identify components of tool description. We manually analyze all collected\nsources and synthesize the recurring recommendations to construct a comprehensive list of tool\ndescription components. The first two authors independently analyzed the 15 sources and uncovered\nthe components through a small, open coding exercise. Because each source can discuss multiple\ncomponents, this coding exercise constitutes a multi-label annotation task. We evaluate inter-rater\nagreement by computing Jaccard similarity for each source and averaging across sources, following\nprior multi-label annotation studies [28, 61]. The mean Jaccard similarity is 0.92, indicating excellent\nagreement. The resulting six components are grounded in official guidance from Anthropic [65],\npractitioner insights [20, 76], and academic analyses [109].",
            "content": "We observed how these tool description components each play one of two complementary roles, consistent with the dual nature discussed in Section 1. One group of components are requirementlike specification components that describe what the tool does and how it should be invoked, including its purpose, limitations, parameter explanation, and examples. These components define the strict functional contract and constraints required for valid execution. The second group of tool description components represent prompt-like instructional components, such as Guidelines and Length and Completeness, that do not introduce new functional constraints but instead serve as behavioral directives that shape how the FM interprets the description, prioritizes information, and reasons about tool selection and invocation. Below, we define each of the six components and provide an illustrative example drawn from the Sequential Thinking tool description shown in Figure 4: (1) Purpose: This component defines the tools functional core and identity. It must clearly state what the tool does, independent of specific task context. For example, in Figure 4, the purpose is established in the opening statement: tool for dynamic and reflective problem solving through thoughts. This primes the models attention mechanism to the tools fundamental capabilities. This component is mentioned in nine out of 15 sources along with the official documentation [3, 15, 17, 20, 70, 72, 84, 85, 109]. (2) Guidelines: This component addresses when and how the tool should be utilized. It provides decision-making criteria for activation and operational conduct for the FM. As shown in Figure 4, this is distributed into two distinct logical blocks: Activation Criteria (When): The When to use section explicitly lists appropriate task types (e.g., Tasks needing context retention). Operational Instructions (How): The You should section provides behavioral protocols (e.g., Start with an adjustable estimate). 3https://openai.com/index/introducing-deep-research/ 4https://github.com/SAILResearch/mcp-tool-description-augmentation , Vol. 1, No. 1, Article . Publication date: February 2026. 12 M. Mehedi Hasan et al. Tool description. tool for dynamic and reflective problem solving through thoughts. It supports flexible, evolving reasoning where each thought can build on, question, or revise previous insights. When to use: Complex problems that need stepwise reasoning Planning or analysis that may require revision Multi-step solutions or unclear initial scope Tasks needing context retention Situations requiring filtering of irrelevant info Key features: Adjustable total_thoughts Ability to revise or question past thoughts Add thoughts even after reaching an apparent end Support for uncertainty, branching, and backtracking Hypothesis generation and verification Parameters: thought (string): The current thinking step next_thought_needed (boolean): Whether another thought step is needed thought_number (integer): Current thought number total_thoughts (integer): Estimated total thoughts needed is_revision (boolean, optional): Whether this revises previous thinking revises_thought (integer, optional): Which thought is being reconsidered branch_from_thought (integer, optional): Branching point thought number branch_id (string, optional): Branch identifier needs_more_thoughts (boolean, optional): If more thoughts are needed You should: (1) Start with an adjustable estimate of needed thoughts (2) Revise previous reasoning when appropriate (3) Add thoughts freely, even at the end (4) Express uncertainty when relevant (5) Mark revisions or branches (6) Ignore irrelevant information (7) Generate and verify hypotheses (8) Iterate until satisfied (9) Provide single correct final answer (10) Set next_thought_needed to false only when truly done Fig. 4. Tool Description for the Sequential Thinking tool. Consequently, if the guidelines are unclear, overly generic, or if explicit guidance is entirely missing, we classify this deficiency as Missing Usage Guidelines smell. This component is also mentioned in four sources [3, 69, 70, 84] beyond the official documentation. (3) Limitations: tool description should describe known constraints, caveats, or corner cases where the tool may fail or be less effective. For example, limitation of calculator tool can be that it only supports up to two decimal point precision. We find this component is mentioned in only two sources apart from the official documents [69, 70]. (4) Parameter Explanation: tool description may include detailed explanations of all input parameters and their intended roles. Figure 4 demonstrates this in the Parameters section, where nine parameters are defined not just by data type (e.g., boolean, integer), but by intent (e.g., is_revision: Whether this revises previous thinking). This is the second-highest component in terms of mention, as we detect it in eight sources, in addition to official documents [3, 17, 20, 43, 69, 70, 72, 109]. (5) Length and Completeness: tool description should contain at least three to four sentences to ensure adequate detail. Complex tools warrant expanded descriptions; the Sequential Thinking tool (Figure 4) necessitates multi-section structure to fully capture its branching , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 13 Purpose: How clearly and completely does the tool description explain what the tool does? 5/5: Clearly explains function, behavior, and return data with precise language. 4/5: Explains function and behavior with only minor ambiguity. 3/5: Basic explanation present but lacks behavioral or output details. 2/5: Vague or incomplete purpose statement. 1/5: Purpose unclear or missing. Fig. 5. Scoring instrumentation for the Purpose component. To ensure granular evaluation, this 5-point Likert scoring is applied independently to each of the six components. logic, validating the need for variable length based on complexity. Apart from the official documents, we find it in three sources [17, 69, 89]. (6) Examples: tool description may include one or more illustrative examples demonstrating correct and effective usage. We observe example-related discussions in three sources beyond the official documents [20, 72, 109]. Scoring & smell derivation. While in Section 4.1.3 we identified the six core recommended 4.1.4 components of tool descriptions, here we use these components to derive scoring mechanism to measure the quality of tool descriptions. Prior studies indicate that FM-based evaluations become significantly more consistent and interpretable when guided by well-defined analytic rubrics rather than open-ended prompts [62, 102]. Following these insights, and adopting methodologies from prior work in automated FM-driven evaluation [102], we implement 5-point Likert scale [35] rubric for each component. This scale offers higher resolution than binary classification, allowing us to grade the quality of component in tool description from missing to ideal. Since tool descriptions consist of unstructured natural language, evaluating the quality of these components is not binary proposition. component might be technically present in tool description but semantically ambiguous or sub-optimal, making simple Yes/No checklist insufficient. As representative example, Figure 5 details the specific performance descriptors for the Purpose component. We designate score 3 as the minimum threshold: it represents Minimum Viable description where the basic purpose is available. Scores 4 and 5 reward increasing precision, behavioral detail, and clarity. Conversely, scores 1 and 2 capture failure modes, reflecting descriptions that are vague, incomplete, or functionally sub-optimal. We apply this same rigorous scalar definition to all six identified components; the complete set of rubrics and associated evaluation prompts is provided in the Appendix A.1. We interpret scores below the minimum threshold (Score < 3) as indicators of qualitative deficiencies. Following prior work on design smells in specifications and prompts [53, 99], we refer to recurrent, component-specific deficiencies in the tool descriptions as smells. Figure 6 illustrates this mapping. For each component, scores in the smelly zone (Score < 3) directly indicate corresponding smell: Unclear Purpose, Missing Usage Guidance, Unstated Limitation, Opaque Parameters, Underspecified or Incomplete, and Exemplar Issues. As each of these smells is deterministically derivable from the component and its score, we do not need any additional heuristic or classifier to identify the smells in the tool descriptions. , Vol. 1, No. 1, Article . Publication date: February 2026. 14 M. Mehedi Hasan et al. Fig. 6. Mapping component scores to the tool description smells. The vertical threshold establishes Score 3 as the Minimum Viable standard, at which basic requirements for given component are satisfied. The Smelly Zone (Scores < 3) captures distinct smells, with the corresponding smell names shown within the zone, when component is missing or inadequately specified, whereas the Non-smelly Zone (Scores 3) indicates that the description contains the corresponding component, ranging from bare minimum presence (Score 3) to highly precise (Scores 45) specification."
        },
        {
            "title": "We then apply the following inclusion criteria to determine which studies provide MCP servers",
            "content": "that are appropriate for our analysis: The study uses real-world, MCP servers. The study evaluates MCP servers empirically or through benchmark analysis for generalpurpose scenarios. The study publicly releases both the codebase and the MCP servers used. After applying these criteria, four studies meet all requirements [18, 46, 48, 52]. From these studies, we curate 856 tools across 103 MCP servers reported in recent literature as of August 20, , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 15 2025. To support comparative analysis of tool description smells between official and communitymaintained MCP servers, we examined server documentation to identify their maintainers. Among the 103 MCP servers, we identify that 23 are maintained by official organizations, including Anthropic and major industry contributors such as GitHub, Airbnb, PayPal, and Microsoft, while the remaining 80 are maintained by the broader open-source community."
        },
        {
            "title": "4.3.2 Multi-model LLM-as-Jury evaluation. To mitigate FM bias and ensure generalizability across\nFM families, we adopt a multi-model LLM-as-jury configuration. Instead of relying on a single\nFM, we task three distinct FMs to independently execute the rubric-based scoring for every tool\ndescription in our dataset. Following prior study, we select models from three disparate families, e.g.,\ngpt-4.1-mini, claude-haiku-3.5, and qwen3-30b-a3b, to ensure that our evaluation measures\nthe quality of the description without any preferences specific to a single model architecture [96].\nTo quantify the level of agreement among the models on the scores assigned, we compute the\nintraclass correlation coefficient (ICC), specifically ICC(2,1), which is widely used to measure\nabsolute agreement among raters evaluating the same target [81]. ICC values between 0.5 and 0.75\nindicate moderate reliability, values between 0.75 and 0.9 indicate good reliability [41]. Table 1\nsummarizes the ICC values for each component. The results demonstrate substantial agreement\nacross the majority of the components. We observed good reliability in 5 out of 6 components and",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. 16 M. Mehedi Hasan et al. Table 1. Intraclass correlation coefficient ICC(2,1) computed for each rubric component based on scores independently assigned by three LLMs of the LLM-as-Jury across all 856 tool descriptions in our dataset. The results indicate good inter-rater agreement for most components and moderate agreement for the Examples component, supporting the robustness of the evaluation."
        },
        {
            "title": "Rubric component",
            "content": "ICC (2,1)"
        },
        {
            "title": "Purpose\nGuidelines\nLimitations\nParameter Explanation\nLength and Completeness\nExamples",
            "content": "0.82 0.85 0.84 0.90 0.76 0.62 moderate reliability in Examples component. This variability is expected because the presence and quality of examples can depend on stylistic preferences or model-specific tendencies. Smell identification. To transition from the raw multi-model scores to definitive smell assign4.3.3 ments, we apply an aggregation and thresholding procedure. For each tool description and each component, we calculate the average of the three scores given by the FM evaluators and consider this as the consolidated score. Formally, for component with scores Score1, Score2, Score3: Smell Detected 1 ùëÅ ùëÅ ùëñ=1 ùëÜùëêùëúùëüùëíùëñ < 3 Since the scoring mechanism is developed by keeping score 3 as the threshold for well-formed component (as defined in Section 4.1.4), any averaged score falling below this threshold indicates that the component is sub-optimal. When this condition is met, we assign the corresponding smell from the taxonomy in Section 4.1.4. For example, for the Purpose component of the tool airbnb_search, the three evaluators assigned scores of 3, 2, and 3, yielding an average of approximately 2.7, which falls below the viability threshold and therefore triggers the Unclear Purpose smell. In contrast, for the tool find_nearby_places, all three evaluators assigned score of 5 for the Purpose component, yielding an average of 5.0 and indicating clean, non-smelly description."
        },
        {
            "title": "4.4 Resolving Smells via Tool Component Augmentation\nThe objective of the tool description augmentation is to fix the smells identified in Section 4.3 while\nretaining the original meaning and intent of the tool description. To achieve this, we design a semi-\nautomated augmentor that combines rubric-based augmentation with an FM to generate refined,\ncomprehensive, and factually consistent tool descriptions, being motivated by prior FM-based\nprompt and text optimization techniques [38, 57].",
            "content": "Initial augmentation of components. In the first stage, the augmentor collects the original tool 4.4.1 descriptions from every MCP server included in the MCP-Universe [48] and the input schema from MCP servers through an MCP client following the same procedure described in Section 4.2.2. It then applies the rubrics defined in Section 4.1 to guide the augmentation process. Using the GPT-4.1-mini model as FM, the augmentor automatically enhances each description by improving coverage across five rubric components: Purpose, Guidelines, Limitations, Parameter Explanation, and Length. The outputs from this stage are referred to as init_augmented_description, as these outputs serve as input for the next refinement step. We intentionally exclude the Examples component at this stage because the model cannot reliably generate factually grounded examples without execution traces, , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 17 and doing so would risk introducing hallucinated or incorrect examples. This issue is addressed in the next stage, where examples are constructed from actual tool executions."
        },
        {
            "title": "4.4.2 Generation of the \"Examples and Limitations\" components. Although the FM improves most\nrubric components effectively, it cannot reliably infer realistic Examples and the full set of Limita-\ntions without access to execution context. Generating these elements purely from prompts risks\nhallucination or factual inconsistency. To address this issue, we manually execute relevant tools to\ncollect authentic usage traces.",
            "content": "For each tool, we manually create set of example tasks designed to encourage the FM to invoke the tool in realistic scenarios. We follow the general strategy used in prior work [72], with manually crafted task descriptions to align with the semantics of the original tool description. Each task is phrased in natural language and is passed to the FM through Claude Desktop [66], which supports MCP integration. The FM then decides which tool to call and provides the corresponding arguments. To ensure coverage of both successful and failing behaviors, we generate at least two tasks per tool: (i) at least one task that should result in successful tool execution and produce valid response, (ii) at least one task that should result in an empty response or an error. Additionally, we generate more tasks to cover the edge cases (ranging from 1 to 3 depending on the complexity of the tools). As an illustrative example, for the get_historical_stock_prices tool in the yfinance MCP server, we construct the following tasks: (1) Retrieve Salesforce (CRM) prices from 1 Nov 2020 to 2 Dec 2020. (2) Retrieve Salesforce (CRM) prices for the same day, 2 Dec 2020. (3) Retrieve price changes from 10 Jan 2023 to 25 Jan 2025 and make multiple tool calls if needed. The first task produces successful response and serves as positive example. The second task produces an empty response because the tool requires non-zero date range and therefore serves as negative example. The third task encourages the FM to issue multiple calls due to the large date range, which helps capture multi-step behavior useful for identifying limitations related to response size and rate. We use Claude Desktop as it provides user-friendly chat interface, supports seamless integration with MCP servers, and exposes both request and response logs in the interface. From these logs, we extract input-output pairs for both successful and failing cases and store them in structured JSON file for further processing. Final consolidation of all components. In the final stage, we feed the init_augmented_description, 4.4.3 along with the collected JSON logs, into the augmentor FM. We instruct the FM to output structured JSON object comprising five explicit fields: Purpose, Guidelines, Limitations, Parameter Explanation, and Examples, mapping to distinct components of the tool descriptions as identified in Section 4.1.3. We do not include separate field for Length and Completeness. This component functions as meta-quality dimension of the overall tool description and is automatically fulfilled when the other five components are properly populated. Therefore, while it remains an essential dimension during scoring and smell detection, it does not require an independent field in the augmented representation. Consequently, the resulting five-component augmented tool descriptions are used directly in subsequent evaluations and ablation studies."
        },
        {
            "title": "4.5 Evaluating the Augmented Tool Descriptions\n4.5.1 Benchmark adoption. We use the MCP-Universe benchmark [48] for evaluating the per-\nformance of agents with augmented tool descriptions. We chose this benchmark because of its\ncomprehensive design that integrates real-world scenarios and temporal dynamics. It includes 231\ncomplex real-world tasks across six domains, while providing a total of 202 tools. Unlike several\nother benchmarks that rely solely on LLM-as-Judge evaluations [52], MCP-Universe combines",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. 18 M. Mehedi Hasan et al. this with robust execution-based evaluation mechanism. In this benchmark, task outcomes are assessed by automated evaluators that directly execute tools and verify results against ground-truth criteria. These evaluators are programmatic validation scripts defined to verify format compliance, static content matching, and dynamic validation for temporally sensitive tasks. This combination enables more accurate and reliable assessment of agent performance and ensures fair comparison across models. Each task is evaluated by at least one evaluator, with an average of 3.3 evaluators per task."
        },
        {
            "title": "4.5.2 Tool Description Router. The original MCP client provided in MCP-Universe does not support\ndynamic modification of tool descriptions at runtime, which is required for our evaluation. To\naddress this limitation, we extend the client with a configurable switching module called the Tool\nDescription Router, which allows for the dynamic selection of tool descriptions. This module\ncan load either the original descriptions or the augmented descriptions stored in the PostgreSQL\ndatabase described in Section 4.4.3, depending on a configuration provided as an argument.",
            "content": "Additionally, to support the ablation study of individual rubric components, the router also supports retrieving certain components in specified order from the augmented tool description and assembles valid description for model consumption. For example, suppose we need to run the benchmark with only the Purpose and Guideline components of the augmented tool description. In that case, those two component names can be passed as comma-separated arguments to the tool description router, which will fetch only these two components for all tools from the database, concatenate them, and present them to the FM. Full Rubric Evaluation. Using the tool description router, we evaluate the augmented tool 4.5.3 descriptions by running the MCP-Universe benchmark with all rubric components retrieved from the database. Each task in MCP-Universe is associated with one or more evaluators that determine task completion correctness, as described in Section 3. To measure the impact of augmented tool descriptions, we compare model performance against the baseline results reported in the original MCP-Universe study. We use three primary evaluation metrics: Success Rate (SR), which measures the percentage of tasks that pass all evaluators; Average Evaluator score (AE), which represents the average fraction of evaluators that pass across all tasks within domain or model; and Average # of Steps (AS), which captures the average number of steps required for an FM to complete each task. These steps reflect how the agent leverages FM and tools in real-time while solving the task, e.g., the number of calls to FMs made by the agent. To illustrate these metrics, consider two example tasks: the first task takes four steps to complete and is evaluated by three evaluators and passes all of them, while the second task takes six steps and is evaluated by four evaluators and passes only two. Hence, the SR is 50% since one of the two tasks (task-01) passed all its evaluators. The proportion of evaluators passed for task-01 is 1.0 and for task-02 is 0.5, giving an AE of 0.75. This indicates that although only 50% of the tasks fully passed, some evaluators in the failed tasks still passed as successful. Finally, AS is 5, calculated as the mean of four and six steps. Following prior similar studies [37], we analyze the accuracy-cost tradeoff of the augmented tool descriptions using Pareto curve that uses AS as proxy for the cost metrics across all six domains for each model. As there are 231 tasks in the MCP-Universe benchmark, running all the tasks against each model is costly in terms of token cost and time. Running the entire benchmark for one round with one FM can cost us around 200 to 300 million tokens, which translates to 75 to 600 USD, depending on the model provider. Hence, to balance the cost and generalization of our evaluation, we conduct , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 19 experiments on one proprietary model (GPT-4.1) and two open-weight models (Qwen3-Coder480B-A35B and GLM-4.5 355B A32B), following the configurations established in the original MCP-Universe benchmark [48]. limitation is that MCP-Universe does not report success information, the number of passed evaluators, or the number of steps at per-task level, which prevents paired statistical tests. Reexecuting these baselines to obtain per-task data would be prohibitively expensive. To overcome this, we adopt hybrid strategy: (i) for the three original models (from MCP-Universe study [48]), we compare our augmented results directly against the reported aggregated (average/percentage) baselines from MCP-Universe; and (ii) to enable more rigorous statistical comparison, we introduce smaller-sized open-weight model, Qwen3-Next-80B-A3B-Instruct. We select this model for its cost-efficiency (0.10 USD per 1M tokens vs. 0.456 USD for Kimi-K2 [2]) and strong performance on external benchmarks such as LiveCodeBenchv6 (56.56 versus 53.7) [2, 12]. For this model, we execute both the baseline (original descriptions) and treatment (augmented descriptions) runs, generating the paired per-task data necessary for significance testing. Finally, we had to apply specific adaptations to accommodate model and domain constraints. As the smaller open-source model, i.e., Qwen3-Next-80B-A3B-Instruct, has shorter context window, we use Purpose, Guidelines, and Limitation components from the augmented tool description for this model to avoid overloading the context window of this model. We exclude the Parameter Explanation component as context-length tradeoff. This exclusion remains feasible because the MCP protocol automatically provides the input schema at runtime, including parameter names and types, which preserves the minimum structural information required for tool invocation. Similarly, out of the six domains of the MCP universe, we avoid generating examples for the Browser Automation domain as the examples for the tools of this domain are too large to fit in the context window. Finally, the original MCP-Universe study utilizes the SERP API-based Google search MCP server, which provides 250 free queries per month per API key. As the Web Searching domain has 55 tasks, each of which requires multiple searches, it is not suitable for an overall study. Hence, we adopt the Google search MCP server 5, which uses the Google search API key to run the experiments uninterrupted."
        },
        {
            "title": "4.5.4 Ablation Study. Since the augmented tool description stored in the database contains the\ncomponents Purpose, Guidelines, Limitations, Parameter Explanation, and Examples (as explained\nin Section 4.4.3), we conduct an ablation study to determine which components contribute most\nsignificantly to performance. For example, Anthropic [65] considers the Examples component less\ncritical, whereas other studies suggest that it may be beneficial [109]. Similarly, the Parameter\nExplanation component may contain redundant information that overlaps with the input schema.\nMoreover, including all components may increase the input context length of the FM and affect\nefficiency.",
            "content": "To investigate these effects, we use the components command of the tool description router to selectively enable different combinations of rubric elements. We design two experimental configurations: (i) excluding the Examples component while retaining Purpose, Guidelines, Limitation, and Parameter Explanation to measure the effect of examples; and (ii) evaluating pairwise combinations of two components, where one component is always Purpose (for instance, Purpose + Limitation, Purpose + Parameter Explanation, and Purpose + Examples). We include Purpose in all combinations because it defines what the tool does, and without it, the FM cannot correctly infer the tools intent or functionality. 5https://github.com/mixelpixx/Google-Search-MCP-Server , Vol. 1, No. 1, Article . Publication date: February 2026."
        },
        {
            "title": "5 Results",
            "content": "M. Mehedi Hasan et al."
        },
        {
            "title": "5.1 RQ-1: To what extent do MCP tools‚Äô descriptions contain smells?\nMotivation. Writing MCP tool descriptions requires combining principles from software re-\nquirements specification and prompt engineering, suggesting that these descriptions may inherit\nsuboptimal design patterns or smells from both domains. Historically, smells in software engi-\nneering have been known to be related to change-proneness and bugs [30, 39], motivating an\ninvestigation of tool description smells in MCP. Although 66% of MCP servers already exhibit\ncode-level smells [29], it remains unclear whether similar issues manifest in the natural-language\ndescriptions that guide agent behavior.",
            "content": "Approach. We develop rubric for evaluating MCP tool descriptions which consists of the components of tool description derived from Anthropics design guidelines, practitioners recommendations, and prior research, as well as structured scoring scale as mentioned in Section 4.1. As FMs are the primary consumers of the tool description, to evaluate whether FMs can understand and interpret these tool descriptions, we score the 856 tools collected from MCP servers through an FM-based scanner. This scanner consists of an LLM-as-Jury with 3 FMs using the rubric, as detailed in Section 4.3. Then we identify the cases with low scores and map those to recurring smells. Given prior work showing differences between official and community-maintained MCP servers [29], we conduct pairwise nonparametric comparisons using the MannWhitney test and refine the results with Bonferroni-adjusted ùëù-values [75] to uncover any such differences. Findings. All six smell types affect the majority of MCP tool descriptions, with the most severe issues appearing in nearly 90% of tools. As shown in Figure 7, the most widespread smell categories are Unstated Limitations (89.8%), Missing Usage Guidelines (89.3%), and Opaque Parameters (84.3%). These patterns suggest that tool descriptions frequently lack critical boundary conditions, fail to indicate when or how tools should be invoked, and provide little insight into the meaning or behavioral implications of input parameters. The next tier includes Underspecified or Incomplete descriptions (79.1%) and Exemplar Issues (77.9%), which arise when descriptions are overly brief relative to tool complexity or rely on sparse or uninformative examples instead of clear explanatory text. Even for the best-performing component (i.e., Purpose), we observe the Unclear Purpose smell in 56% of tools, indicating that more than half of tool descriptions do not clearly articulate their intended functionality. Taken together, these suboptimal patterns can hinder the ability of FMs to properly solve real-world problems. Prior work shows that under-specified prompts can lead to up to 2 performance regressions across model versions [112], suggesting that such pervasive tool description smells may similarly increase brittleness and reduce reliability in MCP-enabled agent behaviors. We illustrate high-quality description of tool in Figure 4 (presented in Section 4.1.3) from the Sequential Thinking MCP server. The description clearly conveys the tools purpose, guidelines, limitations, and parameters. In contrast, Figure 9 shows three examples of low-quality tool descriptions that scored below the median quality score. These minimal descriptions provide little contextual information, leaving foundation models with insufficient cues to infer when or how to use the tools effectively. Only 2.9% of MCP tool descriptions are fully smell-free. As shown in Table 2, the number of smell-free instances drops sharply when analyzing tool descriptions with larger combinations of components considered together. While certain individual components have relatively high smell-free rates, e.g., 44.0% for Purpose alone and 10.4% with Guidelines, the proportion declines to 7.5% when analyzing tools that combine Purpose, Guidelines, and Limitations components in their description, and drops to mere 2.9% when all five components in the description are required to , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 21 Fig. 7. Prevalence of smell types in the tool descriptions of MCP servers. Table 2. Smell-free tool description counts and percentages across rubric combinations. Notation: = Purpose; = Guidelines; = Limitation; PEx = Parameter Explanation; = Examples. Rubric combination # Smell-free % Smell-free + + + + + + PEx + + + PEx + 376 89 64 26 44.0 10.4 7.5 3.0 2.9 Table 3. Statistical test results comparing median component scores between community and official MCP servers using the MannWhitney test, with Bonferroni correction applied to p-values."
        },
        {
            "title": "Score",
            "content": "Statistic p-value Adj. p-value Purpose Guidelines Limitations Parameter Explanation Length & Completeness Examples 873.50 759.50 824.00 973.00 829.00 783.00 0.18 0.17 0.42 0.61 0.46 0.24 1.00 1.00 1.00 1.00 1.00 1.00 be smell-free. This pattern indicates that many tool descriptions are highly incomplete in terms of components. Official and community MCP servers show low-quality tool descriptions across all components of our rubric. We visualize the distributions of median scores for each rubric item across official and community-maintained MCP servers in Figure 8. To compare the two groups, we , Vol. 1, No. 1, Article . Publication date: February 2026. 22 M. Mehedi Hasan et al. Fig. 8. Distribution of the median scores across the six components of the rubric among the official and community MCP servers. Tool name: create_invoice (official) Description: Creates PayPal Invoice Link. Tool name: read_mail(community) Description: Retrieves the content of specific email. Tool name: maps_place_details(community) Description: Get detailed information about specific place. Fig. 9. Example MCP tool descriptions with low quality score. apply the Mann-Whitney test with Bonferroni correction for the six components. As summarized in Table 3, none of the components show statistically significant differences between official and community servers; all raw p-values exceed 0.17 and all corrected p-values equal 1.0. Summary of RQ-1 (1) MCP tool descriptions exhibit pervasive lack of quality, with 97.1% containing at least one description smell. (2) Even the least prevalent smell, Unclear Purpose, appears in 56% of tools, indicating that over half fail to state their core functionality clearly. (3) Moreover, official and community tools reveal no statistically significant differences, suggesting that poor quality is systemic issue within the ecosystem."
        },
        {
            "title": "5.2 RQ-2: How does resolving tool description smells by augmenting all tool",
            "content": "description components impact the performance of FM-based agents? Motivation. The pervasive presence of smells in tool descriptions (as found in RQ-1) naturally motivates attempts to resolve them; however, theoretical signals regarding their impact are conflicting. While software engineering research warns that smell removal can unintentionally alter , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 23 Table 4. Wilcoxon signed-rank test results comparing tool description component scores before augmentation (BA) and after augmentation (AA), showing statistically significant median score increases across all components."
        },
        {
            "title": "Component",
            "content": "Statistic p-value Med. Score (BA) Med. Score (AA) Med. diff."
        },
        {
            "title": "Purpose\nGuidelines\nLimitations\nParameter explanation\nExamples\nLength and completeness",
            "content": "0.0 0.0 0.0 0.0 0.0 0.0 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 2.0 1.0 1.0 1.0 1.0 1.3 5.0 5.0 5.0 4.7 5.0 5.0 2.7 4.0 3.7 3.3 3.7 3.7 system behavior [95], prompt engineering studies suggest that richer descriptions can improve outcomes [38]. We therefore investigate whether systematically resolving tool description smells by augmenting all components yields net positive impact on agent performance on standard benchmarks. Approach. Following the steps mentioned in Section 4.4, we augment all components of the tool description, resulting in fully augmented description that includes purpose, guidelines, limitations, parameter explanation, and examples. As discussed in Section 4.4.3, length and completeness are implicitly addressed through these components rather than augmented independently. After augmenting the tool descriptions, we again measure the quality score of the fully augmented tool descriptions using the same multi-model LLM-as-Jury as in Section 4.3 to validate whether the fully augmented tool descriptions achieve higher quality score. We further apply the Wilcoxon signedrank test [105] to determine whether the observed improvements in quality scores are statistically significant, as this test detects systematic shifts in the central tendency of paired observations in before and after augmentation. In addition, we test whether the observed changes in SR, AE, and AS are statistically significant. As the original study did not provide these metrics for every (task, model) combination, we measure these metrics using the model introduced in our work, i.e., Qwen3-Next-80B-A3B-Instruct. Since SR is binary outcome (0/1), we assess the significance of SR changes using McNemars test in its chi-squared formulation [71]. In contrast, as AE and AS are continuous-valued metrics, we apply Wilcoxon signed-rank test to determine their statistical significance. Findings. FM-based augmentation resolves the tool description smells in the MCP-Universe benchmark by improving the median scores by 2.74.0 points in our scoring rubric. We perform the Wilcoxon signed-rank test on each tool description to compare scores before (BA) and after augmentation (AA) and report the findings in Table 4. These results confirm statistically significant improvements across all six components (ùëù < .001). We observe that for every component, the test statistic equals zero, establishing that all paired comparisons favor the augmented descriptions, with median scores rising by 2.7 to 4.0 points on the five-point Likert scale. We find that median BA scores cluster between 1.0 and 2.0, reflecting widespread pre-augmentation under-specification, whereas median AA scores converge near the ceiling value of 5.0 for all components. With augmented tool descriptions, agents achieve an absolute increase of 5.85 percentage points (median) in task success rate across all models and domains. As summarized in Table 5, we report the absolute success rate change (ŒîSR = SRafter augmentation SRoriginal) for each model and domain to avoid inflation effects caused by low baseline success rates. Across four foundation models and twenty-four benchmark runs, agents using augmented descriptions outperform , Vol. 1, No. 1, Article . Publication date: February 2026. M. Mehedi Hasan et al. Table 5. Success rate (SR) comparison of models using original vs. augmented tool descriptions across six domains. Original SR values for GPT-4.1, Qwen3-Coder-480B-A35B, and GLM-4.5 are taken from the MCP-Universe baseline study [48], whereas Original SR for Qwen3-Next-80B-A3B-Instruct is obtained by running the agent with the original tool descriptions. The ŒîSR column reports the absolute change in success rate in percentage points (SRafter augmentation SRoriginal). The statistically significant improvements in description quality (observed in Table 4) also translate into higher task success rates in more than half of the domain-model combination rows, while also resulting in performance regressions in smaller subset (16.67%) of cases."
        },
        {
            "title": "Original SR",
            "content": "SR after augmentation ŒîSR (pp) GPT-4.1 Qwen3-Coder480B-A35B GLM-4.5 Qwen3-Next-80BA3B-Instruct Finance Repo Management 3D Design Location Navigation Browser Automation Web Searching Finance Repo Management 3D Design Location Navigation Browser Automation Web Searching Finance Repo Management 3D Design Location Navigation Browser Automation Web Searching Finance Repo Management 3D Design Location Navigation Browser Automation Web Searching 40.00% 6.06% 26.32% 8.89% 23.08% 10.91% 40.00% 3.03% 26.32% 8.89% 25.64% 10.91% 50.00% 9.09% 26.32% 17.78% 15.38% 27.27% 50.00% 18.18% 0.00% 11.11% 12.82% 0.00% 57.50% 21.20% 31.60% 31.00% 25.65% 10.91% 72.50% 18.20% 21.10% 15.60% 23.07% 9.10% 67.50% Could not run 26.32% 17.78% 15.38% 18.18% 65.00% 18.18% 10.53% 13.33% 12.82% 7.27% Cross-Model Median ŒîSR Improved cases (ŒîSR > 0) Regressed cases (ŒîSR < 0) Unchanged or failed runs 17.50% 15.14% 5.28% 22.11% 2.57% 0.00% 32.50% 15.17% -5.22% 6.71% -2.57% -1.81% 17.50% N/A 0.00% 0.00% 0.00% -9.09% 15.00% 0.00% 10.53% 2.22% 0.00% 7.27% 5.85% 54.17% 16.67% 29.16% their baseline counterparts in 54.17% of cases (highlighted in green). While we have observed that the tool description scores have improved for all components (in Table 4), the performance of the agent regresses in 16.67% of cases (highlighted in red), indicating that augmenting all components does not necessarily improve the success rate in all domains and models. We observe the greatest improvement in the Finance domain for the Qwen3-Coder-480B-A35B model, whereas moderate decline occurs for GLM-4.5 in the Web Searching domain. Despite these regressions, McNemars test confirms that the 5.85 percentage point improvement in SR is statistically significant (with ùëù = 0.02) across the full benchmark. These results suggest that augmenting tool descriptions , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 25 Table 6. Comparison of baseline (Base.) and augmented (Aug.) results for success rate (SR), average evaluator score (AE), and average number of steps (AS) across models. Baseline SR, AE, and AS values for GPT-4.1, Qwen3-Coder-480B-A35B, and GLM-4.5 are taken from the MCP-Universe baseline study [48], whereas the baseline values for Qwen3-Next-80B-A3B-Instruct are obtained by running the agent with the original tool descriptions. Here, SR is aggregated over the models from Table 5. Column # Tasks AE0.80 indicates the number of tasks that passed 80% of the evaluators. The last row reports the overall median change; green marks improvement in SR or AE, and red marks degradation in AS as trade-off for performance gain."
        },
        {
            "title": "Overall AS",
            "content": "Base. Aug. Base. Aug. # Tasks AE0.80 Base. Aug. GPT-4.1 Qwen3-Coder480B-A35B GLM-4.5 Qwen3-Next-80BA3B-Instruct"
        },
        {
            "title": "Median change",
            "content": "18.18 19.91 24.68 15.58 29.44 25.97 25.25 21.21 5.85 0.41 0. 0.41 0.33 0.47 0.43 0.45 0.39 15.12 19 16 18 17 5.24 7.78 7.33 9.46 8.08 14.06 14.79 6.97 67. across all components can substantially improve task success rates in many domains, while also emphasizing the need for adaptive augmentation strategies tailored to specific domain contexts. Beyond task-level success rate, augmented tool descriptions also improve evaluatorlevel performance, increasing the Average Evaluator Score (AE) by 15.12% across all foundation models. As shown in Table 6, the overall AE, which represents the mean proportion of evaluators passing per task (where AE = 1 denotes complete success) increased consistently across all four models. This indicates that even for tasks not fully completed (i.e., not satisfying all evaluators), augmented descriptions help agents satisfy greater share of evaluation criteria. Using the Wilcoxon signed-rank procedure, we determine that the observed increase in AE is statistically significant, with ùëù < 0.01. Furthermore, we observe that 17 tasks, or 7.36% of all tasks (median across models), can achieve an AE 0.80 (i.e., passing more than 80% of evaluators) yet still fall short of final success, often because the benchmark imposes maximum iteration limit. These trends suggest that augmentation enhances intermediate reasoning and partial goal completion, aligning with prior findings [78, 118] that improved prompt clarity strengthens reasoning pathways to drive steadier, more progressive task execution. The average number of execution steps (AS) increases by 67.46% (median) across models and domains when using the fully augmented tool descriptions, compared to the original MCP-Universe baseline. As shown in Table 6, despite the absolute AS remaining relatively low (less than 15 for two models and 9 for two others), three of the four evaluated models show statistically significant increase in AS (with ùëù < 0.001 at the Wilcoxon signed-rank test). In contrast, the smaller-sized Qwen3-Next-80B-A3B-Instruct model shows resilience, reducing AS from the baseline while still improving both Success Rate (SR) and Average Evaluator Score (AE). However, focusing on the broader trend independent of domain (Figure 10), 6878% of tasks require more steps than the baseline. Among these tasks with increased AS, roughly half (4155%) show improved AE; however, only 1920% of all tasks achieve the final success. This funnel indicates that richer descriptions motivate deeper intermediate exploration that captures more requirements, even if it does not always yield perfect final output. For instance, in Location Navigation task (google_maps_task_0001), the fully augmented tool description doubled the agents steps (from , Vol. 1, No. 1, Article . Publication date: February 2026. M. Mehedi Hasan et al. (a) GPT-4.1 (b) GLM-4.5 (c) Qwen3-Next-80BA3B-Instruct Fig. 10. Task flow from increased steps (AS) to improved evaluator completion (AE) and final success (SR). For each model, the leftmost bar shows all tasks split by whether AS increased from the baseline. The horizontal dotted line indicates that only the AS-increased subset (shown in red in the lower segment of the leftmost bar) is considered in the subsequent bars. Accordingly, the AE and SR bars report the fractions of AS-increased tasks that (i) improve in AE and (ii) improve in SR, respectively. 9 to 19) compared to the baseline with Qwen3-Next-80B-A3B-Instruct. However, it increased the number of passing evaluators from 8 to 10 out of 11. Consequently, the aggregated increase in AS represents cost-performance trade-off: where agents expend more computational effort to achieve greater partial progress and higher likelihood of final completion. While the proprietary GPT-4.1 leads in peak performance among the evaluated models, relatively smaller-sized open-weight model, Qwen3-Next-80B-A3B-Instruct, demonstrates superior balance between cost and accuracy compared to substantially larger open-weight alternatives under augmented tool descriptions. As shown in Figure 11, the Pareto frontiers visualize the trade-off between Average Evaluator Score (AE) and Average Steps (AS) across six domains when agents operate with augmented tool descriptions. Models falling on the blue line represent efficient choices. Models falling below the line (e.g., Qwen3-Coder-480B in 3D Design) are suboptimal, as they consume more steps to achieve equal or lower accuracy. Across domains such as Browser Automation, Financial Analysis, Location Navigation, and Repository Management, Qwen3-Next-80B-A3B-Instruct consistently appears on or near the Pareto frontier, achieving competitive AE while maintaining relatively low AS. At the same time, GPT-4.1 frequently lies on the Pareto frontier and is often closest to the idealized upper-left region, reflecting strong accuracy with moderate execution cost. In contrast, larger open-weight models like GLM-4.5 and Qwen3-Coder-480B-A35B generally occupy dominated regions with higher AS for comparable AE. These observations indicate that under augmented tool descriptions, simple parameter scale does not guarantee higher efficiency; rather, different architectures occupy distinct niches in the cost-accuracy trade-off space. , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 27 (a) 3D design (b) Browser automation (c) Financial analysis (d) Location navigation (e) Repository management (f) Web search Fig. 11. Domain-wise Pareto frontiers. Each subfigure shows the trade-off between Average Evaluator Score (AE; proxy for accuracy) and Average Steps (AS; proxy for cost) for different models within domain. All subfigures share an identical AE axis for comparability, while the AS axis is scaled independently to reflect domain-specific execution costs. Summary of RQ- (1) Augmenting tool descriptions with all components can improve agents performance, increasing the Average Evaluator Score by 15.12% and the task success rate by 5.85 percentage points, although 16.67% of cases experience regressions. (2) As trade-off for these gains, the median number of execution steps increases by 67.46%, reflecting deeper exploration to solve problems. (3) Model scale alone does not guarantee efficiency; smaller-sized open-weight models, such as Qwen3-Next-80B-A3B-Instruct, achieve superior cost-accuracy balance compared to larger open-weight models, like Qwen3-Coder-480B-A35B or GLM-4.5."
        },
        {
            "title": "5.3 RQ-3: How do different components of the augmented tool description impact the",
            "content": "performance of FM-based agents? Motivation. Although fully augmented tool descriptions can improve agent performance, it remains unclear which description components drive these gains and which ones are redundant or even detrimental. Practitioner guidance is also inconsistent; for instance, Anthropic considers the Examples component less critical [65], whereas other studies suggest it is beneficial [109]. Furthermore, given that fully augmented tool descriptions can overload the FMs context window and conflict with the increasing adoption of progressive capability disclosure via agent skills, identifying minimal effective set of description components is essential to preserve performance , Vol. 1, No. 1, Article . Publication date: February 2026. 28 M. Mehedi Hasan et al. while reducing overhead. To this end, we conduct systematic component ablation study [27], analyzing interactions among models, domains, and component configurations across both highand low-performing settings. Approach. We evaluate the impact of individual components of the augmented tool description by conducting an ablation study across five (model, domain) combinations. In the RQ-2 results, we observe that with fully augmented tool description, i.e., one that includes all components, the SR of the agent can be improved for certain domain-model combinations, while potentially experiencing some regression in other domain-model combinations. For example, in the Finance domain, we have observed that with fully augmented tool description, all models have shown higher SR. Conversely, for 3D Design, with an augmented tool description, GPT-4.1 has achieved higher SR, whereas Qwen3-Coder-480B-A35B has shown lower SR than the baseline. To investigate the mechanics behind these divergent outcomes, we select domain-model combinations from Table 5 that cover both performance profiles. We specifically examine three combinations where full augmentation yields gains: Finance with GPT-4.1, Location Navigation with GPT-4.1, and Repository Management with Qwen3-Coder-480B-A35B. Conversely, to understand regression modes, we also include two combinations where performance degrades relative to the baseline: Web Searching with GLM-4.5 and 3D Design with Qwen3-Coder-480B-A35B. For each combination, we run two ablation variants: (i) removing the Examples component from all tool descriptions, and (ii) combining two components where one is always Purpose. We keep Purpose fixed in all combinations because it defines the core functionality of the tool, which is essential for agents to understand what the tool does and when to use it. We execute the agents under these settings while controlling the composition of the descriptions through the components flag introduced in Section 4.5.2. The results from these controlled runs allow us to isolate the relative influence of specific components on agent performance across diverse model and domain contexts. To validate whether various ablation combinations exhibit statistically significant behavioral consistency, we employ Pearsons Chi-Squared test of independence [120] complemented by the ùúô-signed coefficient, which quantifies the strength of association between the two configurations. Findings. There is no single golden combination of components that yields the best results across all domain-model pairs. As summarized in Table 7, the best-performing component combination varies across domain-model pairs. For instance, in the Finance domain with GPT-4.1, using only the Purpose and Guidelines components yields the highest success rate, surpassing the fully augmented tool description, making it the best-performing component combination (BC). In contrast, tasks in Location Navigation with the same model show the best performance with the fully augmented tool description (FR). Meanwhile, in Repository Management with Qwen3-Coder480B-A35B, performance improves modestly when only the Examples component is removed. These results reveals complex interplay where the effectiveness of specific rubric components varies across settings, depending on both the underlying model architecture and the domain-specific requirements. To analyze why the combination of Purpose and Guideline is improving the SR for the Finance domain, we examine the tool descriptions further. We observe that in the tool get_historical_stock_prices in the yfinance MCP server, the Guidelines component provides critical operational cues such as requested dates should include trading days and set end_date one day later than expected since the tool returns the previous days closing price. These explicit behavioral instructions help the model reason correctly about valid input ranges and temporal offsets, leading to higher task success when this component is used alone. In contrast, the Limitations component of the same tool includes vague or self-referential statements such as this contradiction requires disambiguation , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 29 Table 7. Success rate (SR) comparison across component combinations (rows) for each domain-model pair (columns). Green marked ones are the highest performance achieved in the respective domain-model combination. Notation: FR = Fully augmented tool description containing all components; = Purpose; = Guidelines; = Limitation; PEx = Parameter Explanation; = Examples."
        },
        {
            "title": "Rubric setup",
            "content": "FR + + + PEx + + + + PEx Finance (GPT-4.1) Location (GPT-4.1) Repo (Qwen3Coder) 3D-design (Qwen3Coder) Web Searching (GLM-4.5) 57.50% 55.00% 67.50% 47.50% 62.50% 40.00% 31.00% 26.70% 20.00% 24.50% 17.80% 20.00% 18.20% 21.21% 18.20% 18.20% 18.20% 6.06% 21.10% 21.10% 15.80% 21.05% 26.32% 15.80% 18.18% 12.73% 12.73% 16.36% 10.91% 18.18% before relying on intraday availability, which can introduce uncertainty into the models reasoning. When combined with other components, such ambiguity dilutes otherwise useful guidance and lowers performance relative to the single-component configuration. This pattern suggests that components that convey precise behavioral constraints of tool improve agent performance, whereas components containing ambiguous or contradictory statements can degrade it. Statistical analysis shows strong association between task-level outcomes produced by the fully augmented tool description (FR) and those produced by the best-performing component combination (BC). As shown in Table 8, Pearsons chi-square tests reveal significant dependencies between BC and FR across multiple domain-model combinations, including 3D Design, Financial Analysis, and Repository Management (p < 0.01). These results indicate that task success under BC and FR is associated rather than independent, meaning that both configurations tend to succeed or fail on the same tasks. The ùúô-signed coefficient, which measures agreement between paired binary outcomes, ranges approximately from 0.5 to 0.9, indicating strong correspondence in the sets of tasks solved by BC and FR. This correspondence suggests that, for given domain-model combination, BC preserves much of the functional guidance encoded in FR, despite omitting certain components. Consequently, domain-specific pruning emerges as favorable engineering trade-off that can maintain logical reliability comparable to the fully augmented description, while reducing token usage and inference latency, provided that the pruning strategy is tailored to the target domain. Removing the Examples component does not significantly degrade performance compared to either the best-performing component combination (BC) or the fully augmented tool description (FR), contradicting traditional benefits of few-shot examples. As shown in Table 7, configurations excluding Examples, such as P+G+L+PEx, achieve stable success rates across approximately 60% of the evaluated domain-model pairs, although they are not always the highest-performing configuration. To jointly assess performance differences across the three configurations (FR, BC, and P+G+L+PEx), we apply Cochrans test [13], which is appropriate for comparing three or more matched binary outcomes. Across all evaluated domain-model combinations, Cochrans test consistently yields ùëù > 0.20, indicating no statistically significant differences among the three configurations. This result suggests that the inclusion or removal of the Examples component does not materially affect task success rates, which affirms Anthropics suggestion to put less emphasis on examples, but contradicts the traditional benefit of few-shot examples in the prompts [8] of MCP tool descriptions. , Vol. 1, No. 1, Article . Publication date: February 2026. 30 M. Mehedi Hasan et al. Table 8. Comparison between the fully augmented tool description containing all components (FR) and the best-performing component combination (BC). Panel reports Pearsons chi-square tests assessing whether task success under FR and BC is statistically dependent. Higher signed ùúô values indicate stronger agreement between paired binary outcomes, implying that BC and FR solve similar tasks. Panel shows the overall confusion matrix of task outcomes between the two configurations, where value of 1 indicates tasks solved and 0 indicates tasks failed. Panel A: Pearsons chi-square tests"
        },
        {
            "title": "Model",
            "content": "p-value ùúô (signed) 3D Design Financial Analysis Location Navigation Repository Management Qwen3-Coder Web Searching Qwen3-Coder GPT-4.1 GPT-4.1 GLM-4.5 Panel B: Overall confusion matrix (all domains) BC=1 BC=0 FR=1 46 11 0.864 0.591 0.572 0.909 0.511 < 0.01 < 0.01 < 0.01 < 0.01 < 0.01 FR=0 15 120 The best-performing component combinations (BC) and their fully augmented counterparts (FR) solve overlapping but distinct sets of tasks, indicating complementary coverage across configurations. As illustrated in Figure 12, neither BC nor FR fully subsumes the task coverage of the other. Instead, each solves partially unique subset of tasks that have high overlap. For example, in Location Navigation with GPT-4.1, the two configurations share nine successfully solved tasks but diverge on eight others, suggesting that different component structures guide the model toward distinct reasoning trajectories. Hypothetically, when the results from both configurations are combined in hybrid approach, the overall success rate has the potential to increase; for instance, running the failed Location Navigation tasks with the reduced components after the fully augmented one can raise the success rate to 37.8%. While such an ensemble approach inherently increases cost (in terms of steps and latency), it opens viable research direction for maximizing accuracy in mission-critical scenarios where high success rate is more important than cost. Summary of RQ-3 (1) There is no single golden combination of tool description components; effectiveness is highly context-dependent across different models and domains. (2) Removing the Examples component does not result in statistically significant performance degradation (ùëù > 0.20), supporting Anthropics guidance on deprioritizing examples while contrasting with the traditional benefits attributed to few-shot prompting. (3) Depending on the domain and model, tool descriptions can be pruned to small set of components whose task-level outcomes are strongly associated with those of fully augmented descriptions, offering cost-efficient alternative without sacrificing logical reliability. , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 31 Fig. 12. Overlap between tasks solved by the fully augmented tool description (FR) and the best-performing reduced component configuration (BC) across five domain-model combinations. FR (yellow) denotes tasks successfully solved using the fully augmented tool description, BC (blue) represents tasks solved using the best-performing reduced component configuration, and Green intersection indicates tasks solved by both configurations."
        },
        {
            "title": "6.1 Implications for MCP Developers\nMCP developers should integrate rubric-based smell detection into their review or Contin-\nuous Integration (CI) pipelines to prevent the deployment of sub-optimal tool descriptions.\nAs RQ-1 reveals, 56% of tools suffer from Unclear Purpose and 89.3% lack Usage Guidance, effectively\nrendering them as stubs rather than functional specifications. These high smell rates indicate\nthat current ad hoc writing practices are insufficient; teams should therefore treat descriptions\nas first-class engineering artifacts. They need to consider tool description quality as a blocking\ncriterion for release and use automated scanners to detect smells.",
            "content": "MCP developers developing new MCP servers or refactoring the existing individual descriptions should prioritize small set of high-leverage tool description components rather than attempting to fix every aspect simultaneously. As RQ-1 and Table 2 show, 44% of tools are smell-free on Purpose alone, yet this drops to only 2.9% when all five major components are considered together. On the other hand, RQ-3 demonstrates that compact combinations, such as Purpose + Guidelines in Finance, can outperform tool description that contains all components. Given the practical token limitations of current models, e.g., where large descriptions consume valuable context window capacity of FMs and increase execution cost, developers should first identify and optimize the most impactful components for their tools that convey critical semantic , Vol. 1, No. 1, Article . Publication date: February 2026. 32 M. Mehedi Hasan et al. intent with minimal text. Only after establishing these core elements should additional components, such as examples or exhaustive parameter semantics, be introduced selectively, and only in domains where they demonstrably justify the additional token overhead and context window consumption. This token-aware prioritization strategy should help balance quality improvements with resource efficiency, preserving context budget for essential reasoning and reducing unnecessary model invocation costs. MCP developers can consider FM-based augmentation as refinement process, but should weigh it against scale, costs, and alternative manual or semi-automated processes. Table 4 shows that FM-based augmentation can lift median tool description scores from the 1-2 range to nearly 5.0 across all components. However, blindly relying on FMs can lead to verbose descriptions that unnecessarily consume the context window, especially depending on the scale of the MCP server (hundreds of tools vs. couple). Moreover, in the RQ-3, we have seen that FMs can sometimes produce confusing instructions in some components, e.g., in the Limitations of the get_historical_stock_prices tool. Hence, for developers maintaining servers with few tools, manual refinement may suffice. On the other hand, for larger servers, developers should utilize the semi-automated framework described in Section 4.4 to augment the tool description while critically reviewing the output to ensure the augmented text contains critical operational cues and remains concise. The goal of FM-based augmentation should be to resolve ambiguity without inflating the token footprint to point where the agents token cost cannot justify the operational efficiency anymore."
        },
        {
            "title": "6.2 Implications for Ecosystem Maintainers\nTo enable dynamic context management and reduce token overhead, MCP protocol main-\ntainers should extend MCP specifications beyond the current monolithic description field\nto structured schema definitions for individual components. The current MCP specification\ntreats the tool description as a monolithic text blob, obscuring distinct semantic elements such as\nPurpose, Guidelines, Limitations, Parameter Explanation, and Examples. Additionally, RQ-3 demon-\nstrates that different subsets of components, e.g., Purpose + Guidelines or Purpose + Guidelines\n+ Limitations + Parameter Explanation, can preserve core semantic information across different\ndomains, driving equivalent or better success profiles than both baseline and fully augmented tool\ndescriptions. Hence, by introducing dedicated fields for each component (e.g., distinct JSON fields\nfor Purpose, guidelines, and examples), protocol designers can empower agents to dynamically\nassemble the most effective description profile at runtime. An agent low on context space could\nrequest only the Purpose of tools for initial selection and lazily load guidelines or examples only\nwhen a specific tool is invoked, significantly optimizing token window usage. This structural\ndecoupling will allow MCP developers and users both to selectively load, experiment with, or\noptimize specific components of the description based on the immediate token budget and domain\nrequirements, rather than being forced to consume a fixed text blob.",
            "content": "Registry maintainers should integrate rubric-based smell detection and quality scoring as built-in services across MCP registries and marketplaces. As RQ-1 shows, smells are pervasive across both official and community servers, indicating that no maintainer group consistently produces high-quality descriptions. This finding implies that quality cannot be reliably delegated to individual teams. MCP registries, such as Glama, Smithery, or Cloudflare Workers, existing review processes primarily focus on infrastructure setup or package-level checks, and rarely assess the quality of tool descriptions. Ecosystem maintainers can respond to the pervasive smells by running FM-based scans over published servers, with smell summaries and augmentation-aware quality badges, and issuing warnings when tool or server falls below agreed thresholds. Because the FM-based scanner shows stable agreement across models, these diagnostics can function similarly , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 33 to security advisories in registries such as npm, encouraging developers to submit higher-quality descriptions or maintain multiple augmented profiles to reduce integration friction."
        },
        {
            "title": "6.3 Implications for MCP Users\nMCP users should treat tool descriptions as mutable client-side configurations and use\nthem as a cost-effective leverage point, rather than immediately defaulting to larger and\nmore expensive frontier models. As shown in Section 5.2, the smaller-sized Qwen3-Next-80B-\nA3B-Instruct model, when equipped with augmented tool descriptions, achieves performance\nparity with or even surpasses the significantly larger Qwen3-Coder-480B-A35B in domains such\nas Finance, Repository Management, and Location Navigation. This implies that high-quality\ntool descriptions can serve as an architectural catalyst, enabling the use of smaller, more cost-\neffective models without sacrificing reliability in specific domains, which MCP users should identify\nand utilize.",
            "content": "In current practice, however, MCP users typically consume default tool descriptions as fixed artifacts authored and distributed by MCP server developers or vendors, without modification. Our methodology demonstrates that this constraint is not fundamental. By re-purposing the Tool Description Router described in Section 4.5.2, MCP users can override the default tool description at runtime without modifying server code. This client-side customization will enable MCP users to adapt tool descriptions to their specific domain, model, and context window constraints, providing practical mechanism for description-level augmentation until the MCP protocol natively supports equivalent capabilities. MCP users should enforce explicit resource caps, such as maximum step counts, thinking budget, rate limit, or token limits, tailored to the complexity of the domain when deploying augmented tool descriptions. As RQ-2 and Table 6 demonstrate, while augmented descriptions boost the overall Success Rate (SR) by 5.85 percentage points, they simultaneously inflate the Average Steps (AS) by 67.46%. This substantial increase in execution overhead implies that richer descriptions improve reasoning at the expense of computational efficiency. Consequently, teams should implement budgeted policies before enabling full augmentation, allowing higher caps in domains where SR gains justify the overhead (for example Finance or Repository Management) and enforcing lower caps or compact configurations such as Purpose + Guidelines in domains where gains are small or negative (for example Web Searching in Table 5) to preserve runtime performance."
        },
        {
            "title": "6.4 Implications for Researchers\nResearchers should investigate holistic mechanisms that improve agent convergence with\nminimal cost, recognizing that augmented tool descriptions are one of several levers in a\nbroader efficiency landscape. RQ-2 and RQ-3 of this study show that augmented tool descriptions\nsignificantly increase task success and evaluator coverage; however, these gains often come with\nhigher step counts or increased token costs. To optimize the flow, emerging approaches such as\nMCP Zero‚Äôs active tool discovery [19] aim to reduce context size by loading only the tools an agent\nis likely to need. Similarly, Anthropic has proposed reactive tool search [67] to enable agents\nto find the correct tools for a task through a semantic search. Moreover, recent code-mode or\nprogrammatic tool execution pipelines proposed by Cloudflare [94] and Anthropic [67] enable FMs\nto generate code that orchestrates tool calls directly, thereby avoiding the transport of intermediate\ntool responses back into the model and reducing both the number of steps and token usage.",
            "content": "However, the role of description quality within these efficiency mechanisms remains empirically unexplored. It is unknown whether the rubric-augmented descriptions can improve the recall of dynamic tool discovery of the agents or increase the syntactic correctness and stability of generated , Vol. 1, No. 1, Article . Publication date: February 2026. 34 M. Mehedi Hasan et al. orchestration code. If they do, these approaches may achieve the high success rates observed in our experiments without incurring the resource penalties associated with traditional multi-step loops. Researchers should therefore investigate how behaviorally precise, rubric-aligned description variants influence tool search accuracy, static planning quality, and code-generation reliability within these emerging cost-saving architectures. Researchers should extend tool description component ablations to progressive-disclosure mechanisms, e.g., agent skills, and empirically test whether current challenges of these emerging techniques are rooted in under-specified metadata. Agent skills expose only minimal metadata (e.g., name and description) and load full instructions on demand, aiming to reduce context usage while enabling autonomous invocation. Yet, practitioner reports suggest that models often ignore available skills unless explicitly prompted, indicating gap between theoretical progressive discovery and observed behavior6. In this regard, we observed that behaviorally precise components, especially those that encode critical operational cues and explicit usage constraints, can materially shift agent behavior in the RQ-3 results. These observations may suggest that discoverability failures in agent skills may stem from missing or weakly expressed critical cues rather than insufficient metadata volume. Consequently, future research can adopt similar ablation-style approach on skill metadata to identify the smallest set of high-impact semantic signals that reliably trigger invocation under strict token constraints, preserving the efficiency goals of progressive disclosure while improving its practical effectiveness."
        },
        {
            "title": "7.1 External validity\nOur dataset comprises 856 tools across 103 MCP servers collected from prior empirical and evalua-\ntion studies, and therefore potentially excludes MCP servers that have not yet been evaluated or\ndocumented in the literature. As a result, some categories of MCP servers, including proprietary\ninternal deployments or recently introduced servers, may be underrepresented or absent from\nour dataset. However, the dataset includes a mix of community-maintained open source servers,\nofficially managed open source servers (e.g., GitHub and Playwright), and a small number of offi-\ncially managed closed MCP servers (e.g., PayPal), covering multiple governance and deployment\nsettings. Moreover, we extract tool descriptions through a dynamic MCP client based reflection\nmechanism rather than static source code analysis. This design makes the smell detection and\naugmentation pipeline agnostic to implementation language and source code availability, and in\nprinciple applicable to both open source and closed source MCP servers.",
            "content": "While we detect and optimize smells across 856 tools from 103 MCP servers, performance evaluation is conducted only on the subset of tools and servers included in the MCP-Universe benchmark. Specifically, MCP-Universe covers 202 tools drawn from 18 MCP servers, which represent strict subset of the full corpus analyzed in RQ1 and RQ2. In addition, due to the high token cost of running the full benchmark, we do not evaluate all models included in MCP-Universe. To mitigate these limitations, we adopt MCP-Universe because it inherently covers diverse set of tasks spanning multiple domains, MCP servers, and evaluators, enabling robust execution-based assessment without additional sampling. We also intentionally select heterogeneous model set, covering frontier proprietary model (GPT-4.1), large open-weight models (Qwen3-Coder-480BA35B and GLM-4.5), and smaller-sized model (Qwen3-Next-80B-A3B-Instruct), to capture variation across model families while balancing evaluation cost. 6https://scottspence.com/posts/claude-code-skills-dont-auto-activate, https://www.reddit.com/r/ClaudeAI/comments/ 1qbc30u/comment/nz9lom3/ , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 35 The feasibility of evaluating the fully augmented tool description depends on the available context window of the models and the verbosity of the tool descriptions. For Qwen3-Next-80BA3B-Instruct, we exclude the Parameter Explanation and Examples components to avoid context overflow. Similarly, for the Browser Automation domain, the excessive length of execution examples necessitates excluding examples for all models. As result, the evaluation for Qwen3-Next-80BA3B-Instruct reflects partial augmentation setting, and the full benefits of the proposed pipeline may not be realizable on resource-constrained models or highly verbose tool domains. We justify our choices by prioritizing components that are not otherwise available to the FM. Specifically, while Parameter Explanation is omitted, the MCP protocol still provides the input schema to the FM, partially compensating for its absence. In addition, our ablation study in RQ-3 shows that excluding Examples has limited impact on performance in most settings, supporting this design choice under context constraints."
        },
        {
            "title": "7.3 Internal validity\nFor three models adopted from the MCP-Universe benchmark, the original study does not report\nper-task SR, AE, or AS, and we do not re-execute these baseline configurations due to the prohibitive\ncomputational cost. Consequently, comparisons for these models rely on aggregate baseline metrics\nreported in prior work, which may differ from our execution environment. In addition, agentic tool-\nuse workflows are inherently non-deterministic, and observed performance differences may partially\nresult from stochastic variation rather than systematic effects of tool description augmenting. To\nmitigate this threat, we introduce an additional, smaller and very low-cost model, Qwen3-Next-\n80B-A3B-Instruct, for which we execute both baseline runs using the original tool descriptions\nand augmented runs under identical conditions. This allows direct before-and-after comparison\nwithin the same environment. For this model, we further assess the statistical significance of\nchanges in SR, AE, and AS using appropriate paired statistical tests.",
            "content": "The original MCP-Universe study uses SERP API-based Google Search MCP server with strict query limits. To ensure the uninterrupted execution of Web Searching tasks, we utilize an alternative Google Search MCP server. Differences in the underlying API platform may impact absolute performance in the Web Searching domain, regardless of the tools description quality. They may also explain the lack of performance improvement observed across all three MCP-Universe models. To mitigate this issue, we evaluate the Web Searching domain for the newly introduced model, Qwen3-Next-80B-A3B-Instruct, using the same Google Search MCP server for both the baseline and augmented tool descriptions. As result, the statistical analyses for this model control for instrumentation differences, ensuring that measured differences in performance reflect description augmenting effects, as opposed to search API variability. , Vol. 1, No. 1, Article . Publication date: February 2026. 36 M. Mehedi Hasan et al. The ablation study explicitly selects five domain-model combinations and examines five component configurations for each pair, resulting in total of 25 runs. All ablation settings retain the Purpose component, which we treat as mandatory for correct tool interpretation. Additionally, we do not evaluate all possible permutations of components, as each combination would require separate benchmark run, which would incur substantial computational costs and time. These design choices may introduce selection bias, as the estimated importance of individual components is derived from subcases already known to be sensitive to augmenting effects. To mitigate this risk, we include domain-model combinations exhibiting both performance improvements and regressions. We also conduct an ablation that retains all components except Examples, indicating that the observed findings are not limited to narrowly selected configurations."
        },
        {
            "title": "8 Conclusion\nThis research provides an extensive empirical characterization of MCP tool descriptions and\nevaluates the extent to which these descriptions affect FM-based agent execution. By analyzing\n856 tools from both official and community-maintained servers and by conducting rubric-guided\naugmentation, benchmarking, and controlled studies, we establish tool descriptions as a critical but\nunder-engineered artifact of agentic systems.",
            "content": "Our findings reveal that over 97% of MCP tools suffer from ecosystem-wide description smells, yet augmenting these artifacts with all components serves as powerful architectural lever, enabling smaller-sized open-weight models to achieve performance parity with larger frontier models. Rubric-aligned descriptions with all components enhance agent performance, resulting in 15.12% increase in the Average Evaluator Score and 5.85 percentage point improvement in task success rates. However, these gains require more execution steps and higher costs, and are not universal across all domain-model combinations, highlighting the need for cost-aware and context-sensitive augmentation. On that front, our ablation study shows that compact combinations of high-impact components can achieve behavioral alignment comparable to fully augmented descriptions while reducing cost overhead. Collectively, these findings call for shift toward treating tool descriptions as configurable engineering artifacts, motivating structured, component-aware protocol designs that support dynamic, cost-aware context management in MCP-enabled agents. Future work should investigate how description quality impacts emerging cost-reduction techniques, such as dynamic tool search and code-mode execution of MCP, and whether rubric-aligned descriptions can enhance tool retrieval and orchestration, enabling future agents to achieve high reliability without incurring the cost penalties of traditional multi-step loops. Disclaimer We employed ChatGPT-5.2, Anthropic Opus-4.5, and Gemini-3 Pro-Preview solely for copy-editing and table layout preparation, consistent with IEEE and ACM policies governing AI use in scholarly work."
        },
        {
            "title": "References",
            "content": "[1] Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, et al. 2025. Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457 (2025). [2] Moonshot AI. 2025. Kimi-K2-Instruct-Moonshot AI. https://huggingface.co/moonshotai/Kimi-K2-Instruct Accessed: 2025-12-09. [3] Towards AI. 2025. Tool Descriptions Are Critical: Making Better LLM Tools + Research Capability. https://towardsai. net/p/artificial-intelligence/tool-descriptions-are-critical-making-better-llm-tools-research-capability Accessed: 2025-12-09. , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 37 [4] Anthropic. 2025. Introducing the Model Context Protocol. https://www.anthropic.com/news/model-context-protocol, last visited: Nov 10. [5] Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern√°ndez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. 2025. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 238255. [6] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and David Binkley. 2012. An empirical analysis of the distribution of unit test smells and their impact on software maintenance. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 5665. [7] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and Dave Binkley. 2015. Are test smells really harmful? an empirical study. Empirical Software Engineering 20, 4 (2015), 10521094. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 18771901. [9] Diego Cedrim, Alessandro Garcia, Melina Mongiovi, Rohit Gheyi, Leonardo Sousa, Rafael De Mello, Baldoino Fonseca, M√°rcio Ribeiro, and Alexander Ch√°vez. 2017. Understanding the impact of refactoring on smells: longitudinal study of 23 software projects. In Proceedings of the 2017 11th Joint Meeting on foundations of Software Engineering. 465475. [10] Yuyan Chen, Zhihao Wen, Ge Fan, Zhengyu Chen, Wei Wu, Dayiheng Liu, Zhixu Li, Bang Liu, and Yanghua Xiao. 2023. Mapo: Boosting large language model performance with model-adaptive prompt optimization. In Findings of the Association for Computational Linguistics: EMNLP 2023. 32793304. [11] Gaurab Chhetri, Shriyank Somvanshi, Md Monzurul Islam, Shamyo Brotee, Mahmuda Sultana Mimi, Dipti Koirala, Biplov Pandey, and Subasish Das. 2025. Model Context Protocols in Adaptive Transport Systems: Survey. arXiv preprint arXiv:2508.19239 (2025). [12] Alibaba Cloud. 2025. Qwen3-Next-80B-A3B-Instruct - Qwen. https://huggingface.co/Qwen/Qwen3-Next-80B-A3BInstruct Accessed: 2025-12-09. [13] J√©r√©mie Cohen, Martin Chalumeau, Robert Cohen, Dani√´l Korevaar, Babak Khoshnood, and Patrick MM Bossuyt. 2015. Cochrans test was useful to assess heterogeneity in likelihood ratios in studies of diagnostic accuracy. Journal of clinical epidemiology 68, 3 (2015), 299306. [14] Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley Malin, and Sricharan Kumar. 2025. Automatic prompt optimization via heuristic search: survey. arXiv preprint arXiv:2502.18746 (2025). [15] Speak Easy. 2025. MCP Core Concepts. https://www.speakeasy.com/mcp/core-concepts/tools Accessed: 2025-12-09. [16] Abul Ehtesham, Aditi Singh, and Saket Kumar. 2025. Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework. arXiv preprint arXiv:2506.13800 (2025). [17] Github User: eonist. 2025. MCP command best practice. https://github.com/eonist/conduit/issues/191 Accessed: 2025-12-09. [18] Shiqing Fan, Xichen Ding, Liang Zhang, and Linjian Mo. 2025. Mcptoolbench++: large scale ai agent model context protocol mcp tool use benchmark. arXiv preprint arXiv:2508.07575 (2025). [19] Xiang Fei, Xiawu Zheng, and Hao Feng. 2025. Mcp-zero: Active tool discovery for autonomous llm agents. arXiv preprint arXiv:2506.01056 (2025). [20] Gil Feig. 2025. 3 insider tips for using the Model Context Protocol effectively. https://www.merge.dev/blog/mcpbest-practices Accessed: 2025-10-27. [21] Henning Femmer, Daniel M√©ndez Fern√°ndez, Stefan Wagner, and Sebastian Eder. 2017. Rapid quality assurance with requirements smells. Journal of Systems and Software 123 (2017), 190213. [22] Francesca Arcelli Fontana, Ilaria Pigazzini, Riccardo Roveda, Damian Tamburri, Marco Zanoni, and Elisabetta Di Nitto. 2017. Arcan: tool for architectural smells detection. In 2017 IEEE International Conference on Software Architecture Workshops (ICSAW). IEEE, 282285. [23] Martin Fowler. 2018. Refactoring: improving the design of existing code. Addison-Wesley Professional. [24] Yi Gao, Xing Hu, Xiaohu Yang, and Xin Xia. 2025. Automated Unit Test Refactoring. Proceedings of the ACM on Software Engineering 2, FSE (2025), 713733. [25] Joshua Garcia, Daniel Popescu, George Edwards, and Nenad Medvidovic. 2009. Identifying architectural bad smells. In 2009 13th European Conference on Software Maintenance and Reengineering. IEEE, 255258. [26] Joshua Garcia, Daniel Popescu, George Edwards, and Nenad Medvidovic. 2009. Toward catalogue of architectural bad smells. In International conference on the quality of software architectures. Springer, 146162. [27] Isha Hameed, Samuel Sharpe, Daniel Barcklow, Justin Au-Yeung, Sahil Verma, Jocelyn Huang, Brian Barr, and Bayan Bruss. 2022. BASED-XAI: Breaking ablation studies down for explainable artificial intelligence. arXiv preprint arXiv:2207.05566 (2022). , Vol. 1, No. 1, Article . Publication date: February 2026. 38 M. Mehedi Hasan et al. [28] Mohammed Mehedi Hasan, Hao Li, Emad Fallahzadeh, Gopi Krishnan Rajbahadur, Bram Adams, and Ahmed Hassan. 2025. An empirical study of testing practices in open source AI agent frameworks and agentic applications. arXiv preprint arXiv:2509.19185 (2025). [29] Mohammed Mehedi Hasan, Hao Li, Emad Fallahzadeh, Gopi Krishnan Rajbahadur, Bram Adams, and Ahmed Hassan. 2025. Model context protocol (mcp) at first glance: Studying the security and maintainability of mcp servers. arXiv preprint arXiv:2506.13538 (2025). [30] Mohammad Mehedi Hassan and Akond Rahman. 2022. As code testing: Characterizing test quality in open source ansible development. In 2022 IEEE Conference on Software Testing, Verification and Validation (ICST). [31] Thomas Hirsch and Birgit Hofer. 2022. systematic literature review on benchmarks for evaluating debugging approaches. Journal of Systems and Software 192 (2022), 111423. [32] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278 (2025). [33] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. 2023. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv:2308.00675 (2023). [34] Marcel Jerzyk and Lech Madeyski. 2023. Code smells: comprehensive online catalog and taxonomy. In Developments in Information and Knowledge Management Systems for Business Applications: Volume 7. Springer, 543576. [35] Ankur Joshi, Saket Kale, Satish Chandel, and Kumar Pal. 2015. Likert scale: Explored and explained. British journal of applied science & technology 7, 4 (2015), 396. [36] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. 2002. CCFinder: multilinguistic token-based code clone detection system for large scale source code. IEEE transactions on software engineering 28, 7 (2002), 654670. [37] Sayash Kapoor, Benedikt Stroebl, Zachary Siegel, Nitya Nadgir, and Arvind Narayanan. 2024. Ai agents that matter. arXiv preprint arXiv:2407.01502 (2024). [38] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023). [39] Foutse Khomh, Massimiliano Di Penta, and Yann-Gael Gueheneuc. 2009. An exploratory study of the impact of code smells on software change-proneness. In 2009 16th Working Conference on Reverse Engineering. IEEE, 7584. [40] Foutse Khomh, St√©phane Vaucher, Yann-Ga√´l Gu√©h√©neuc, and Houari Sahraoui. 2009. bayesian approach for the detection of code and design smells. In 2009 Ninth International Conference on Quality Software. IEEE, 305314. [41] Terry Koo and Mae Li. 2016. guideline of selecting and reporting intraclass correlation coefficients for reliability research. Journal of chiropractic medicine 15, 2 (2016), 155163. [42] Guilherme Lacerda, Fabio Petrillo, Marcelo Pimenta, and Yann Ga√´l Gu√©h√©neuc. 2020. Code smells and refactoring: tertiary systematic review of challenges and observations. Journal of Systems and Software 167 (2020), 110610. [43] Matthew LeRay. 2025. 4 Tips for Developing Model Context Protocol Server. https://speedscale.com/blog/4-tips-fordeveloping-model-context-protocol-server Accessed: 2025-12-09. [44] Jiahui Liang, Weiqin Zou, Jingxuan Zhang, Zhiqiu Huang, and Chenxing Sun. 2021. deep method renaming prediction and refinement approach for Java projects. In 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS). IEEE, 404413. [45] Yao-Yang Liu, Zhen Zheng, Feng Zhang, Jin-Cheng Feng, Yi-Yang Fu, Ji-Dong Zhai, Bing-Sheng He, Xiao Zhang, and Xiao-Yong Du. 2026. comprehensive taxonomy of prompt engineering techniques for large language models. Frontiers of Computer Science 20, 3 (2026), 2003601. [46] Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, et al. 2025. Mcpeval: Automatic mcp-based deep evaluation for ai agent models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 373402. [47] Keila Lucas, Rohit Gheyi, Elvys Soares, M√°rcio Ribeiro, and Ivan Machado. 2024. Evaluating large language models in detecting test smells. arXiv preprint arXiv:2407.19261 (2024). [48] Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. 2025. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. arXiv preprint arXiv:2508.14704 (2025). [49] Mika Mantyla, Jari Vanhanen, and Casper Lassenius. 2003. taxonomy and an initial empirical study of bad smells in code. In International Conference on Software Maintenance, 2003. ICSM 2003. Proceedings. IEEE, 381384. [50] Ra√∫l Marticorena, Carlos L√≥pez, and Yania Crespo. 2006. Extending taxonomy of bad code smells with metrics. In Proceedings of 7th International Workshop on Object-Oriented Reengineering (WOOR). Citeseer, 6. [51] Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. 2025. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334 (2025). , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 39 [52] Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, and Le Sun. 2025. Livemcpbench: Can agents navigate an ocean of mcp tools? arXiv preprint arXiv:2508.01780 (2025). [53] Naouel Moha, Yann-Ga√´l Gu√©h√©neuc, Laurence Duchien, and Anne-Francoise Le Meur. 2009. Decor: method for the specification and detection of code and design smells. IEEE Transactions on Software Engineering 36, 1 (2009), 2036. [54] Michael Mohan, Des Greer, and Paul McMullan. 2016. Technical debt reduction using search based automated refactoring. Journal of Systems and Software 120 (2016), 183194. [55] Purnima Naik, Salomi Nelaballi, Venkata Sai Pusuluri, and Dae-Kyoo Kim. 2024. Deep learning-based code refactoring: review of current knowledge. Journal of Computer Information Systems 64, 2 (2024), 314328. [56] Steffen Olbrich, Daniela Cruzes, Victor Basili, and Nico Zazworka. 2009. The evolution and impact of code smells: case study of two open source systems. In 2009 3rd international symposium on empirical software engineering and measurement. IEEE, 390400. [57] Krista Opsahl-Ong, Michael Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab. 2024. Optimizing instructions and demonstrations for multi-stage language model programs. arXiv preprint arXiv:2406.11695 (2024). [58] Wendk√ªuni Ou√©draogo, Yinghua Li, Kader Kabor√©, Xunzhu Tang, Anil Koyuncu, Jacques Klein, David Lo, and Tegawend√© Bissyand√©. 2024. Test smells in LLM-Generated Unit Tests. arXiv preprint arXiv:2410.10628 (2024). [59] Fabio Palomba, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, Denys Poshyvanyk, and Andrea De Lucia. 2014. Mining version histories for detecting code smells. IEEE Transactions on Software Engineering 41, 5 (2014), 462489. [60] Gabriele Pandini, Antonio Martini, Adela Nedisan Videsjorden, and Francesca Arcelli Fontana. 2025. An exploratory study on architectural smell refactoring using Large Languages Models. In 2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C). IEEE, 462471. [61] Michael Parker, Caitlin Anderson, Claire Stone, and YeaRim Oh. 2024. large language model approach to educational survey feedback analysis. International journal of artificial intelligence in education (2024), 138. [62] Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Arnav Ramamoorthy, Pratyush Ghosh, Aaryan Raj Jindal, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, et al. 2025. Rubric is all you need: Improving llm-based code evaluation with question-specific rubrics. In Proceedings of the 2025 ACM Conference on International Computing Education Research V. 1. 181195. [63] Debalina Ghosh Paul, Hong Zhu, and Ian Bayley. 2025. Investigating The Smells of LLM Generated Code. arXiv preprint arXiv:2510.03029 (2025). [64] Anthropic PBC. 2025. Agent Skills: What are skills. https://agentskills.io/what-are-skills Accessed: 2026-01-18. [65] Anthropic PBC. 2025. Claude: How to implement tool use. https://docs.claude.com/en/docs/agents-and-tools/tooluse/implement-tool-use Accessed: 2025-10-27. [66] Anthropic PBC. 2025. Getting Started with Local MCP Servers on Claude Desktop. https://support.claude.com/en/ articles/10949351-getting-started-with-local-mcp-servers-on-claude-desktop Accessed: 2025-10-28. [67] Anthropic PBC. 2025. Introducing advanced tool use on the Claude Developer Platform. https://www.anthropic. com/engineering/advanced-tool-use Accessed: 2025-12-01. [68] Anthropic PBC. 2025. Tool search tool. https://platform.claude.com/docs/en/agents-and-tools/tool-use/tool-searchtool Accessed: 2026-01-18. [69] Anthropic PBC. 2025. Writing Effective Tools for Agents: Complete MCP Development Guide Writing Effective Tools for Agents. https://modelcontextprotocol.info/docs/tutorials/writing-effective-tools Accessed: 2025-12-09. [70] Anthropic PBC. 2025. Writing effective tools for agents with agents. https://www.anthropic.com/engineering/ writing-tools-for-agents Accessed: 2025-12-09. [71] Matilda QR Pembury Smith and Graeme Ruxton. 2020. Effective use of the McNemar test. Behavioral Ecology and Sociobiology 74, 11 (2020), 133. [72] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023). [73] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2025. Tool learning with large language models: survey. Frontiers of Computer Science 19, 8 (2025), 198343. [74] Krishna Ronanki, Beatriz Cabrero-Daniel, and Christian Berger. 2024. Prompt smells: An omen for undesirable generative AI outputs. In Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI. 286287. [75] Graeme Ruxton and Guy Beauchamp. 2008. Time for some priori thinking about post hoc testing. Behavioral ecology 19, 3 (2008), 690693. [76] Zack Saadioui. 2025. Maximizing Your MCP Experience: Tips for Effective Tool Descriptions. https://www.arsturn. com/blog/maximizing-your-mcp-experience-tips-for-effective-tool-descriptions Accessed: 2025-10-27. , Vol. 1, No. 1, Article . Publication date: February 2026. 40 M. Mehedi Hasan et al. [77] Ahmed Sadik and Siddhata Govind. 2025. Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3. arXiv preprint arXiv:2504.16027 (2025). [78] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927 (2024). [79] Anjana Sarkar and Soumyendu Sarkar. 2025. Survey of LLM Agent Communication with MCP: Software Design Pattern Centric Review. arXiv preprint arXiv:2506.05364 (2025). [80] Jiawen Shi, Zenghui Yuan, Guiyao Tie, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun. 2025. Prompt Injection Attack to Tool Selection in LLM Agents. arXiv preprint arXiv:2504.19793 (2025). [81] Patrick Shrout and Joseph Fleiss. 1979. Intraclass correlations: uses in assessing rater reliability. Psychological bulletin 86, 2 (1979), 420. [82] Mohammed Latif Siddiq, Lindsay Roney, Jiahao Zhang, and Joanna Cecilia Da Silva Santos. 2024. Quality assessment of chatgpt generated code and their use by developers. In Proceedings of the 21st international conference on mining software repositories. 152156. [83] Dag IK Sj√∏berg, Aiko Yamashita, Bente CD Anda, Audris Mockus, and Tore Dyb√•. 2012. Quantifying the effect of code smells on maintenance effort. IEEE Transactions on Software Engineering 39, 8 (2012), 11441156. [84] Reddit User: sjoti. 2025. Good MCP design is understanding that every tool response is an opportunity to prompt the model. https://www.reddit.com/r/mcp/comments/1lq69b3/good_mcp_design_is_understanding_that_every_tool Accessed: 2025-12-09. [85] Anastasios (Tasos) Stamoulakatos. 2025. Building effective AI agents: brief guide and best practices. https://medium.com/wpp-ai-research-labs/building-effective-ai-agents-a-guide-to-the-future-of-llms-andour-proposed-best-practices-06f6ca7c250f Accessed: 2025-12-09. [86] Ion Stoica, Matei Zaharia, Joseph Gonzalez, Ken Goldberg, Koushik Sen, Hao Zhang, Anastasios Angelopoulos, Shishir Patil, Lingjiao Chen, Wei-Lin Chiang, et al. 2024. Specifications: The missing link to making the development of llm systems an engineering discipline. arXiv preprint arXiv:2412.05299 (2024). [87] Bal√°zs Szalontai, Andr√°s Vad√°sz, Zsolt Rich√°rd Borsi, Ter√©z V√°rkonyi, Bal√°zs Pint√©r, and Tibor Gregorics. 2021. Detecting and fixing nonidiomatic snippets in python source code with deep learning. In Proceedings of SAI Intelligent Systems Conference. Springer, 129147. [88] Aditi Tiwari, Akshit Bhalla, and Darshan Prasad. 2025. Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions. arXiv preprint arXiv:2509.22814 (2025). [89] Reddit User: tleyden. 2025. MCP Best Practices: Mapping API Endpoints to Tool Definitions. https://www.reddit. com/r/mcp/comments/1oo39hm/mcp_best_practices_mapping_api_endpoints_to_tool/ Accessed: 2025-12-09. [90] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of move method refactoring opportunities. IEEE Transactions on Software Engineering 35, 3 (2009), 347367. [91] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2011. Identification of extract method refactoring opportunities for the decomposition of methods. Journal of Systems and Software 84, 10 (2011), 17571782. [92] Michele Tufano, Fabio Palomba, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, Andrea De Lucia, and Denys Poshyvanyk. 2016. An empirical investigation into the nature of test smells. In Proceedings of the 31st IEEE/ACM international conference on automated software engineering. 415. [93] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk. 2019. On learning meaningful code changes via neural machine translation. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2536. [94] Kenton Varda and Sunil Pai. 2025. Code Mode: the better way to use MCP. https://blog.cloudflare.com/code-mode/ Accessed: 2025-12-01. [95] Roberto Verdecchia, Giuseppe Procaccianti, Patricia Lago, et al. 2018. Empirical evaluation of the energy impact of refactoring code smells. In 5th International Conference on Information and Communication Technology for Sustainability. ICT4S2018. EasyChair, 365383. [96] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 2024. Replacing judges with juries: Evaluating llm generations with panel of diverse models. arXiv preprint arXiv:2404.18796 (2024). [97] Tijana Vislavski, Gordana Rakiƒá, Nicol√°s Cardozo, and Zoran Budimac. 2018. LICCA: tool for cross-language clone detection. In 2018 IEEE 25th international conference on software analysis, evolution and reengineering (SANER). IEEE, 512516. [98] Antonio Vitale, Rocco Oliveto, and Simone Scalabrino. 2025. catalog of data smells for coding tasks. ACM Transactions on Software Engineering and Methodology 34, 4 (2025), 132. [99] Andreas Vogelsang, Alexander Korn, Giovanna Broccia, Alessio Ferrari, Jannik Fischbach, and Chetan Arora. 2025. On the impact of requirements smells in prompts: The case of automated traceability. In 2025 IEEE/ACM 47th International , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 41 Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). IEEE, 5155. [100] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, et al. 2024. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 94409450. [101] Pengcheng Wang, Jeffrey Svajlenko, Yanzhao Wu, Yun Xu, and Chanchal Roy. 2018. CCAligner: token based large-gap clone detector. In Proceedings of the 40th International Conference on Software Engineering. 10661077. [102] Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, and Xin Xia. 2025. Can llms replace human evaluators? an empirical study of llm-as-a-judge in software engineering. Proceedings of the ACM on Software Engineering 2, ISSTA (2025), 19551977. [103] Zihan Wang, Rui Zhang, Yu Liu, Wenshu Fan, Wenbo Jiang, Qingchuan Zhao, Hongwei Li, and Guowen Xu. 2025. Mpma: Preference manipulation attack against model context protocol. arXiv preprint arXiv:2505.11154 (2025). [104] Florensia Widjaja, Zhangtianyi Chen, and Juexiao Zhou. 2025. BioinfoMCP: Unified Platform Enabling MCP Interfaces in Agentic Bioinformatics. arXiv preprint arXiv:2510.02139 (2025). [105] Robert Woolson. 2007. Wilcoxon signed-rank test. Wiley encyclopedia of clinical trials (2007), 13. [106] Di Wu, Fangwen Mu, Lin Shi, Zhaoqiang Guo, Kui Liu, Weiguang Zhuang, Yuqi Zhong, and Li Zhang. 2024. ismell: Assembling llms with expert toolsets for code smell detection and refactoring. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. 13451357. [107] Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang Low. 2024. Prompt optimization with EASE? efficient ordering-aware automated selection of exemplars. Advances in Neural Information Processing Systems 37 (2024), 122706122740. [108] Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, and Weinan Zhang. 2025. survey of llm-based deep search agents: Paradigm, optimization, evaluation, and challenges. arXiv preprint arXiv:2508.05668 (2025). [109] Weikai Xu, Chengrui Huang, Shen Gao, and Shuo Shang. 2025. LLM-Based Agents for Tool Learning: Survey: W. Xu et al. Data Science and Engineering (2025), 131. [110] Aiko Yamashita and Leon Moonen. 2012. Do code smells reflect important maintainability aspects?. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 306315. [111] Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, et al. 2025. MCPWorld: Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents. arXiv preprint arXiv:2506.07672 (2025). [112] Chenyang Yang, Yike Shi, Qianou Ma, Michael Xieyang Liu, Christian K√§stner, and Tongshuang Wu. 2025. What Prompts Dont Say: Understanding and Managing Underspecification in LLM Prompts. arXiv preprint arXiv:2505.13360 (2025). [113] Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Jason Wu. 2023. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. arXiv preprint arXiv:2306.01150 (2023). [114] Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, et al. 2025. Livemcp-101: Stress testing and diagnosing mcp-enabled agents on challenging queries. arXiv preprint arXiv:2508.15760 (2025). [115] Beiqi Zhang, Peng Liang, Qiong Feng, Yujia Fu, and Zengyang Li. 2024. Copilot-in-the-loop: Fixing code smells in copilot-generated python code using copilot. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. 22302234. [116] Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al. 2025. From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents. arXiv preprint arXiv:2506.18959 (2025). [117] Zhimin Zhao, Abdul Ali Bangash, Filipe Roseiro C√¥go, Bram Adams, and Ahmed Hassan. 2025. On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards. IEEE Transactions on Software Engineering (2025). [118] Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797 (2023). [119] Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. 2025. ComplexFuncBench: exploring multi-step and constrained function calling under long-context scenario. arXiv preprint arXiv:2501.10132 (2025). [120] Minhaz Fahim Zibran. 2007. Chi-squared test of independence. Department of Computer Science, University of Calgary, Alberta, Canada 1, 1 (2007), 17. , Vol. 1, No. 1, Article . Publication date: February 2026."
        },
        {
            "title": "A Appendix",
            "content": "M. Mehedi Hasan et al. A.1 Prompts used by the LLM-Jury Judge Tool Description Quality Using Six-Component Rubric You are grading tool description. Score each component from 1 to 5, then provide an overall quality score (0100), justification, and improvement recommendations. Scoring Rubric (15 scale for each component). (1) Purpose (What the tool does) 5/5: Clearly explains function, behavior, and return data with precise language. 4/5: Explains function and behavior with minor ambiguity. 3/5: Basic explanation present but lacks behavioral details. 2/5: Vague or incomplete purpose statement. 1/5: Purpose unclear or missing. (2) Usage Guidelines (When to use or not use) 5/5: Explicitly states appropriate use cases and when not to use; includes disambiguation if the tool name is ambiguous. 4/5: States when to use with minimal guidance on when not to use. 3/5: Implies usage context but lacks explicit boundaries. 2/5: Usage context unclear or overly generic. 1/5: No usage guidance provided. (3) Limitation (Caveats and boundaries) 5/5: Clearly states what the tool does not return, scope boundaries, and important constraints. 4/5: Mentions main limitations but misses some edge cases. 3/5: Vague or incomplete limitation statements. 2/5: Minimal or implied limitations only. 1/5: No limitations or caveats mentioned. (4) Parameter Explanation (Input clarity) 5/5: Every parameter is explained with type, meaning, behavioral effect, and required or default status. 4/5: Most parameters are explained with minor omissions. 3/5: Basic parameter information is present but lacks behavioral impact. 2/5: Parameters are listed without meaningful explanation. 1/5: Parameters are not explained or only provided in schema form. (5) Examples vs. Description Balance 5/5: Description is self-sufficient; examples, if any, supplement rather than replace the explanation. 4/5: Mostly descriptive with minor reliance on examples. 3/5: Even mix of description and examples. 2/5: Over-relies on examples with minimal prose. 1/5: Only examples are provided with no descriptive explanation. (6) Length and Completeness 5/5: Four or more sentences of substantive, well-structured prose covering all aspects. 4/5: Three to four sentences with good coverage. 3/5: Two to three sentences that are somewhat complete. 2/5: One to two sentences that are too brief. 1/5: Single phrase or fragment. , Vol. 1, No. 1, Article . Publication date: February 2026. Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions 43 Input. {tool_payload} Output Format (JSON). { \"scores\": { \"purpose\": 1-5, \"usage_guideline\": 1-5, \"limitation\": 1-5, \"parameter_explanation\": 1-5, \"examples_balance\": 1-5, \"length_completeness\": 1-5 } \"label\": \"Good\" \"Bad\", \"reason\": \"One sentence justification\", \"improvement_needed\": [ \"comma separated list of specific weak areas with scores <= 3\" ] } Labeling rules: description is labeled Bad if: Any of the six rubric dimensions score below 3, or Examples replace the description instead of supporting it. description is labeled Good only if: All six dimensions score 3 or higher, and All requirements in components 1 through 6 are satisfied. , Vol. 1, No. 1, Article . Publication date: February 2026."
        }
    ],
    "affiliations": [
        "Queens University, Canada"
    ]
}