{
    "paper_title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
    "authors": [
        "Zecheng Tang",
        "Quantong Qiu",
        "Yi Yang",
        "Zhiyi Hong",
        "Haiya Xiang",
        "Kebin Liu",
        "Qingqing Dang",
        "Juntao Li",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method."
        },
        {
            "title": "Start",
            "content": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers Zecheng Tang * 1 2 Quantong Qiu * 1 2 Yi Yang 1 2 Zhiyi Hong 1 2 Haiya Xiang 1 2 Kebin Liu 3 Qingqing Dang 3 Juntao Li 1 2 Min Zhang 1 6 2 0 2 4 2 ] . [ 1 7 6 3 7 1 . 1 0 6 2 : r Abstract The quadratic complexity of standard attention mechanisms poses significant scalability bottleneck for large language models (LLMs) in longcontext scenarios. While hybrid attention strategies that combine sparse and full attention within single model offer viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8A800 GPUs, our method enables models to achieve both strong performance and efficient inference (see Figure 1). Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method. 1. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in processing long-context sequences (Liu et al., 2025a;b; Mei et al., 2025). However, the quadratic computational and memory complexity of standard full attention (FA) mechanisms (Vaswani et al., 2017) poses significant scalability bottleneck as the context window continues to expand. Sparse attention (SA) mechanisms (Child, 2019; Zaheer et al., 2020) represent an effective strategy for mitigating this limitation by selectively attending to subset of critical tokens, thereby substantially reducing computational overhead and improving inference throughput. To balance the trade-off between reduced computational cost and preserved model quality, i.e., achieving performance 1Soochow University, China 2LCM Laboratory 3Baidu Inc, China. Correspondence to: Juntao Li <ljt@suda.edu.cn>. Figure 1. Comparison between Elastic Attention (ours) and existing approaches on LongBench-V2 (Bai et al., 2025). (XA+SSA) and (FA+SSA) denote our different settings. comparable to FA, existing methods typically adopt hybrid modeling strategies that integrate both SA and FA within single model (Zhang et al., 2025a; Xiao et al., 2025). Yet, downstream task performance is susceptible to the proportion between those two attention computation modes (see our preliminary study in 2.2), and identifying an appropriate proportion typically requires extensive task-specific validation or tuning, which severely limits robustness and general applicability (Peng et al., 2025a). In this work, we show that downstream tasks naturally fall into two categories: (1) sparsity-robust tasks (e.g., summarization), whose performance remains stable across wide range of sparsity levels, and (2) sparsity-sensitive tasks (e.g., question answering), which suffer substantial degradation beyond certain sparsity threshold. Such an observation suggests that model should only differentiate between these two task regimes and allocate the appropriate sparsity, instead of painstakingly learning distinct sparsity configurations for each task. Going beyond this, we propose Elastic Attention, which enables the model to automatically adjust its overall sparsity during the prefill stage to accommodate the two aforementioned task categories. The core of our approach is lightweight Attention Router module integrated into existing transformer architectures. Preprint. On progress work. Specifically, the Attention Router operates analogously to 1 Test-time Adaptive Sparsity Ratios for Efficient Transformers Mixture-of-Experts (Shazeer et al., 2017): it uses the input hidden states to perform head-wise routing, dynamically assigning each attention head to either FA or SA computation mode, thereby enabling dynamic adjustment of the models overall sparsity. Notably, the Attention Router introduces negligible overhead, adding only 0.27M parameters per layer (assuming head dimension of 128), which preserves both inference efficiency and computational cost. To optimize the Attention Router, we adopt continuous relaxation scheme based on the Gumbel-Softmax (Jang et al., 2016) strategy, which alleviates the traininginference discrepancy arising from discrete routing decisions. Furthermore, to stabilize optimization, we employ straight-through estimator (STE) (Bengio et al., 2013) trick for gradient propagation. During inference, different attention heads within the same layer may be routed to different computation modes. We design fused kernel that enables all routed heads to be processed simultaneously in single forward pass. We evaluate our method on 3 widely-used and cutting-edge LLMs (Qwen3-series (Yang et al., 2025) and Llama-3.1-8BInstruct (Grattafiori et al., 2024) models), considering both streaming (Xiao et al., 2024b) and block-sparse (Guo et al., 2024) SA patterns. Across diverse set of long-context benchmarks (real-world, synthetic retrieval, and long-form reasoning tasks), our method not only learns to allocate different sparsity levels to different tasks but also yields superior performance compared to baselines. Notably, this gain is achieved under limited training budgets, requiring only 12 hours of training on 8A800 GPUs without modifying the backbone models parameters. Furthermore, we conduct an in-depth analysis of the Attention Router, studying the impact of its architectural design and parameter capacity, as well as providing detailed training monitoring metrics. 2. Preliminary 2.1. Retrieval Heads and Sparse Heads Retrieval Heads and FA Retrieval heads are class of attention heads specialized in capturing contextually relevant tokens from long sequences, enabling modern LLMs to operate effectively in long-context processing tasks (Wu et al., 2024). In practice, retrieval heads typically adopt the standard FA computation mode. Given the Query (Q), Key (K), and Value (V ) states, the computation of FA is : Or = Softmax(cid:0)QK (cid:1) V, (1) Figure 2. Trend of model performance as the hybrid model sparsity ratio (ΩMSR) increases. We report model performance as relative percentage score with respect to that of FA. of SA can be defined as: Os = Softmax (cid:16) (cid:17) , (2) where and are partial elements in and . Hybrid Head Mechanism While SA substantially reduces computation, it may lead to performance degradation (Lu et al., 2025b). To balance efficiency and performance, recent methods adopt hybrid heads design that mixes retrieval and sparse heads within the same model (Xiao et al., 2025; Yu et al., 2025; Bhaskar et al., 2025). Specifically, for Transformer with layers and keyvalue heads per layer1, the h-th head in the ℓ-th layer is assigned computation type: π(ℓ,h) {FA, SA}, and the final output per layer (O(ℓ)) can be written as: (cid:16) O(ℓ,1), O(ℓ,2), . . . , O(ℓ,H)(cid:17) O(ℓ) = Concat , O(ℓ,h) = (cid:40) Or, π(ℓ,h) = FA, Os, π(ℓ,h) = SA, (3) Sparsity Ratio Computation Under the hybrid-head mechanism, given model fθ, we define two types of model sparsity ratios from two perspectives: (1) the proportion of sparse attention heads, and (2) the proportion of tokens attended to by those heads. Definition 2.1 (Model Sparsity Ratio, ΩMSR). ΩMSR measures the fraction of sparse heads in the model: where we omit the scaling factor for simplicity. ΩMSR(fθ) = (cid:88) (cid:88) (cid:105) (cid:104) π(ℓ,h) = SA , (4) 1 1 l=1 where I[] is the indicator function. h=1 1Here we adopt grouped query attention (GQA) (Ainslie et al., 2023) definition instead of multi-head attention (MHA), as GQA is widely used in modern LLM architectures. 2 Sparse Heads and SA Compared to the retrieval heads, sparse heads reduce computational cost by leveraging SA mode, which retains only fraction of the most relevant and (e.g., 20% of key and value states). The computation Test-time Adaptive Sparsity Ratios for Efficient Transformers Figure 3. Illustration of our proposed Elastic Attention. (a) shows the adapted model block with frozen backbone parameters; (b) details the dynamic assignment of heads via the Attention Router module; (c) presents the lightweight design of the Attention Router. Definition 2.2 (Effective Sparsity Ratio, ΩESR). ΩESR measures the effective token-level sparsity of attention along the sequence dimension, taking both the sparse heads and their corresponding sparsity patterns into account: FA). For instance, at an ΩMSR = 20%, the resulting model may achieve 73.85% of the backbone models performance (ΩMSR = 0%). We provide background of retrieval head identification and implementation details in Appendix C. ΩESR(fθ) = 1 H (cid:88) h=1 ρ(L,h), (5) where ρ(L,h) [0, 1) denotes the pruning ratio of each head in the last layer, with ρ(ℓ,h) = 0 for heads with FA, since no tokens are pruned, and ρ(ℓ,h) = ρSA for heads with SA2. 2.2. Impact of ΩMSR on Downstream Tasks While existing hybrid head mechanisms have achieved strong performanceefficiency trade-offs, they suffer from critical limitation: the ΩMSR is fixed at inference time, and its relationship with downstream task performance remains unexplored. In this section, we investigate the impact of varying ΩMSR on different downstream tasks. Settings We experiment with the Llama3.1-8B-Instruct model (Grattafiori et al., 2024) and follow Wu et al. (2024) to identify retrieval heads in the model and rank them by their activation frequency. Based on this ranking, we progressively replace retrieval heads with sparse heads, thereby increasing the ΩMSR. We evaluate the above resulting model on LongBench (Bai et al., 2024), which consists of 6 distinct long-context downstream tasks. For different values of ΩMSR, we report the model performance as percentage relative to that of the backbone model (full 2Different SA strategies correspond to specific ρSA, e.g., if SA only attends to 10% tokens along the sequence dimension, then ρSA = 0.9, meaning that 90% of tokens are pruned. Results As shown in Figure 2, the tasks can be broadly categorized into two categories. The first category, which we refer to as sparsity-robust tasks, exhibits performance that is largely insensitive to changes in ΩMSR. This is because these tasks can be solved using only coarse-grained contextual information, e.g., summarization. The second category, sparsity-sensitive tasks, shows sharp decline in performance once ΩMSR exceeds certain threshold. Such tasks require retrieving fine-grained evidence from the context to produce accurate outputs, as is typical in question-answering tasks. Therefore, regardless of the number of task types, they can all be mapped into one of these two categories. Under this setting, the model only needs to determine whether task requires fine-grained information and then apply the corresponding attention computation mode accordingly. 3. Elastic Attention We provide an overview of our approach in Figure 3. The sub-figure 3(a) illustrates the overall architecture integrating our Elastic Attention module, whose core component is an Attention Router mechanism. Notably, except for the Elastic Attention components, all backbone model parameters are frozen during training. 3.1. Introduce Attention Router Information Flow of Attention Router As illustrated in sub-Figure 3(b), the attention router operates in manner 3 Test-time Adaptive Sparsity Ratios for Efficient Transformers two challenges: (i) how to make the softmax-based routing distribution closely approximate hard routing decisions, and (ii) how to handle the non-differentiability introduced by hard routing decisions. To address the first issue, we adopt continuous relaxation scheme based on the Gumbel Softmax (Jang et al., 2016) with annealing, which gradually sharpens the routing distribution during training toward discrete assignments. Specifically, we feed to the Gumbel Softmax and obtain soft routing matrix r(ℓ) soft RH2, where each column corresponds to the routing probabilities of all heads in layer ℓ. Hard routing decisions are then obtained by applying arg max over the second dimension: r(ℓ,h) hard = argmax c{0,1} r(h,c) soft , {1, . . . , H}. (6) hard = r(ℓ,h) r(ℓ,h) For the second issue, since arg max operation is nondifferentiable and would block gradient flow, we employ straight-through estimator (STE) strategy (Bengio et al., 2013), where r(ℓ,h) (cid:104) hard can be rewritten as: r(ℓ,h) soft gradient detach , (7) which enables gradients to propagate through the soft routing distribution in the backward process while preserving hard routing behavior in the forward pass. We provide more details and theoretical explanations in Appendix F. hard + r(ℓ,h) soft (cid:17)(cid:105) (cid:16) Training Objective We use the ΩMSR metric to calculate the predicted sparsity during training and compare it against the task-specific target sparsity t. Notably, instead of enforcing fixed for each task, we impose task-dependent nontight constraints with predefined lower and upper bounds since the optimal sparsity for given task is unknown. To ensure that introducing sparsity does not degrade the language modeling capability, we build min-max optimization term upon the standard language modeling objective with sparsity regularization term. Given an input and corresponding label Y, the training objective is: max λ1,λ2 min Llanguage(X ) (cid:125) (cid:123)(cid:122) language modeling (cid:124) + λ1Ldiff (X ) + λ2L (cid:124) (cid:123)(cid:122) sparsity regularization diff (X ) , (cid:125) (cid:40) Llanguage = CE Loss (Y fθ(X )) , Ldiff = ΩMSR (fθ(X )) t, (8) where CE Loss denotes the standard cross-entropy loss for language modeling, and λ1, λ2 are task-specific trainable Lagrange multipliers optimized via gradient ascent (Bhaskar et al., 2025), which decouple the sparsityperformance trade-offs across tasks and mitigate optimization conflicts. 3.3. Efficient Deployment Since each layer adopts hybrid-heads computation, where different types of heads cannot be efficiently parallelized at 4 Figure 4. Comparison of our fused kernel with Torch-based sequential implementation for layer-wise hybrid attention. analogous to Mixture-of-Experts (MoE) (Shazeer et al., 2017) gating mechanism: it determines, based on the input hidden states, which attention computation mode (i.e., FA or SA) is assigned to each keyvalue head. Specifically, the Key hidden states (xK RsHd , where denotes sequence length, denotes head dimension) are fed into the attention router. Then, it produces hard binary decision (r(ℓ,h) hard {0, 1}) for each head, indicating whether this head employs FA (r(ℓ,h) hard = 1) computation mode. Each head then adopts its allocated attention computation mode, and the resulting head outputs are concatenated along the feature dimension to form the final output. hard = 0) or SA (r(ℓ,h) Architecture Design of Attention Router We show the architecture of the attention router in sub-Figure 3(c), which consists of two components: task MLP and router MLP. The task MLP is designed to infer task-specific characteristics from the input hidden states, while the router MLP leverages these task-aware representations to determine the attention computation mode for each head. More concretely, given the Key hidden states (xK), the attention router first applies pooling along the sequence dimension to obtain task representation .3 The pooled latent is then fed into the task MLP and the router MLP to produce head-wise routing logit matrix (z RH2), which is then converted into binary decision r(ℓ,h) hard for each head via softmax-based classifier. RHd 3.2. Optimization-based Head Identification Attention Router Optimization Notably, to ensure consistency between training and inference, where inference uses hard routing decisions (i.e., binary classification), we also adopt hard routing during training. This introduces 3In practice, we avoid pooling over the full sequence, as long context introduces substantial redundancy that degrades the quality of task representation. We investigate this in Appendix H.5. Test-time Adaptive Sparsity Ratios for Efficient Transformers Table 1. Performance on LongBench-E (Bai et al., 2024). We report average performance (Perf.) and ΩMSR per task category. The 1st and the 2nd performance in each comparison group are highlighted with bold font and underlined, respectively. Method Qwen3-4B + InfLLM-V2 + DuoAttention + PruLong + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) Qwen3-8B + InfLLM-V2 + DuoAttention + PruLong + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) Llama-3.1-8B-Instruct + InfLLM-V2 + DuoAttention + PruLong + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) S-Doc QA M-Doc QA Summ In-Context Synthetic Code Avg. Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR 43.69 43.30 41.73 41. 42.20 44.40 45.57 42.20 45.45 46.05 46.15 44.01 48.75 43.77 48.65 47.69 49.92 49.40 - - 0.70 0. 0.66 0.68 - - 0.70 0.70 0.64 0.75 - - 0.70 0.70 0.64 0.71 Qwen3-4B backbone model 28.46 29.95 28.47 28.53 28.50 28.49 - - 0.70 0.70 0.76 0.82 66.21 64.19 64.59 64.80 65.73 65. Qwen3-8B backbone model 28.34 29.58 28.16 28.30 28.19 28.30 - - 0.70 0.70 0.72 0.83 66.64 64.55 66.42 66. 67.52 66.23 - - 0.70 0.70 0.68 0.71 - - 0.70 0.70 0.65 0.76 - - 0.70 0. 0.73 0.76 - - 0.70 0.70 0.71 0.80 Llama-3.1-8B-Instruct backbone model - - 0.70 0.70 0.63 0. 30.26 30.08 29.93 30.02 30.14 30.30 - - 0.70 0.70 0.73 0.80 68.16 67.32 67.35 67.86 67.99 68. - - 0.70 0.70 0.74 0.79 38.48 34.97 35.72 37.58 38.86 39.42 51.59 42.33 44.52 46.88 46.54 49. 51.85 46.30 45.21 43.05 48.92 52.94 49.59 38.54 47.17 47.23 48.43 44.35 50.16 45.87 47.50 48.66 48.07 50. 56.00 42.03 54.56 53.56 54.00 49.66 - - 0.70 0.70 0.71 0.74 - - 0.70 0.70 0.65 0. - - 0.70 0.70 0.64 0.72 54.38 59.58 53.91 53.20 54.34 54.29 61.20 59.58 62.38 62.24 62.95 60. 55.81 64.30 56.47 61.07 60.70 56.49 - - 0.70 0.70 0.82 0.87 - - 0.70 0.70 0.78 0. - - 0.70 0.70 0.79 0.87 48.45 46.68 46.95 47.19 48.08 47.59 52.16 49.03 50.67 51.34 51.51 51. 53.28 50.73 51.82 52.11 53.35 52.71 - - 0.70 0.70 0.73 0.76 - - 0.70 0.70 0.69 0. - - 0.70 0.70 0.69 0.77 runtime, we implement fused attention kernel that jointly computes retrieval heads and sparse heads simultaneously. In this work, we focus on single-GPU deployment, without introducing inter-device communication overhead, e.g., head-wise parallelism. In practice, attention kernels are launched over three-dimensional grid spanning the sequence dimension, attention heads, and batch. When the input sequence length is sufficiently large, parallelism along the sequence dimension dominates the overall execution. Since the number of concurrently active blocks is bounded by the available streaming multiprocessors, the GPU effectively schedules and executes almost all sequence blocks of one head before progressing to the next head. We report the speedup over Torch-based sequential, layer-wise hybrid attention implementation in Figure 4, which indicates that our fused kernel yields prefill-time acceleration during inference. We show more details in Appendix E. 4. Experiment 4.1. Setting Training and Data We select Qwen3-(4B/8B) (Yang et al., 2025) and Llama-3.1-8B-Instruct (Grattafiori et al., 2024) as the backbone LLMs, and build training dataset by combining five sources: ChatQA2-Long-SFT-data (Xu et al., 2024), MuSiQue (Trivedi et al., 2022), CoLT-132K (Li et al., 2025), GovReport (Huang et al., 2021), and XSum (Narayan et al., 2018), covering both sparsity-sensitive tasks (SingleDoc QA and Multihop QA) and sparsity-robust tasks (code completion, summarization, and in-context learning). The resulting dataset spans sequence lengths ranging from 8K to 64K tokens, comprising approximately 0.74B tokens in total. For sparsity-robust and sparsity-sensitive task categories, we empirically set the = 1.0 and = 0.7, respectively. We conduct training with 8A800 GPUs, with each run completing within 12 hours. We provide more training details in Appendix and list hyperparameters in Table 6. Evaluation We compare our method against representative training-based sparsity approaches, including DuoAttention (Xiao et al., 2025), PruLong (Bhaskar et al., 2025), and InfLLM-V2 (Zhao et al., 2025). For the attention computation mode of sparse heads, we consider Streaming Sparse Attention (SSA) (Xiao et al., 2024b) and XAttention (XA) (Xu et al., 2025). For XA, we set the threshold τ = 0.9, while all other hyperparameters are kept at their default values. We denote different head computation configurations using the {Retrieval Head mode}{Sparse Head mode} notation, e.g., FA-SSA means retrieval heads use FA mode and sparse heads use SSA mode. Due to space constraints, experiments on MoBA (Lu et al., 2025a) and NSA (Yuan et al., 2025), along with detailed baseline configurations, are shown in Appendix G. All the evaluations are conducted with LOOM-Eval evaluation framework (Tang et al., 2025). 4.2. Evaluation Results Real-world Long-context Tasks We first evaluate our method on LongBench-E (Bai et al., 2024), real-world long-context benchmark comprising 14 tasks across 6 cat5 Table 2. Model performance on RULER (Hsieh et al., 2024) and LongBench-v2 (Bai et al., 2025). We report the average Perf. and ΩMSR. Test-time Adaptive Sparsity Ratios for Efficient Transformers Models Qwen3-4B + InfLLM-V2 + DuoAttention + PruLong + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) Qwen3-8B + InfLLM-V2 + DuoAttention + PruLong + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) Llama-3.1-8B-Instruct + InfLLM-V2 + DuoAttention + PruLong + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) 8K 16K 32K 64K 128K 256K Perf. ΩMSR Easy Hard Perf. ΩMSR RULER LongBench-v2 Qwen3-4B backbone model 60.05 43.39 50.53 51.32 50.79 56.88 70.98 44.94 61.42 60.98 67.03 69.42 53.19 28.57 45.86 42.70 47.83 49. 43.27 27.01 45.46 48.76 47.32 43.47 Qwen3-8B backbone model 63.23 25.93 56.61 61.64 64.55 65.08 82.39 53.47 70.10 73. 77.41 82.34 65.84 34.96 54.68 59.74 61.17 64.57 66.71 32.95 56.28 58.82 61.75 63.41 66.00 51.02 58.30 58. 61.81 63.27 75.74 52.58 65.94 69.90 71.74 73.87 Llama-3.1-8B-Instruct backbone model 89.46 60.98 66.99 56.83 80.20 87. 70.79 35.90 40.13 30.88 56.30 68.23 80.12 32.29 57.45 33.74 68.16 78.87 72.34 47.27 42.92 21.64 56.47 68. 83.47 59.10 62.92 48.82 72.85 81.82 86.82 74.40 75.17 73.99 79.24 85.38 85.62 68.53 78.85 82.69 82.81 85. 92.83 80.93 77.85 69.35 83.42 92.00 87.49 78.59 76.82 77.03 83.35 86.56 89.69 77.01 81.03 87.20 86.62 85. 92.88 89.30 87.20 83.54 89.93 92.82 - - 0.70 0.70 0.66 0.67 - - 0.70 0.70 0.65 0. - - 0.70 0.70 0.65 0.72 32.67 28.00 28.67 27.60 34.00 32.00 39.33 29.32 40.67 38.67 37.33 30. 32.00 29.32 28.67 28.00 28.00 30.67 22.18 24.06 24.44 22.33 24.44 25.94 27.82 28.67 25.56 27.82 31.20 30. 33.08 28.67 27.44 28.92 29.70 30.83 25.96 25.48 25.96 24.35 27.88 28.12 31.97 29.09 31.01 31.73 33.41 30. 32.69 29.09 27.88 28.61 29.09 30.77 - - 0.70 0.70 0.70 0.72 - - 0.70 0.70 0.66 0. - - 0.70 0.70 0.68 0.75 egories with varying context lengths. We compare Elastic Attention with other baselines in Table 1. Within each comparison group, all methods share the same backbone model and are trained with the same data. We can observe that our method consistently achieves the best average performance, overcoming or approaching the backbone (fully FA) while enabling more efficient inference. Across different tasks, Elastic Attention can dynamically allocate different sparsity levels, e.g., achieving an average ΩMSR of around 0.85 on code tasks and 0.68 on QA. Notably, we find that our method underperforms some baselines on sparsity-robust tasks (e.g., Code and Summ). This stems from our higher sparsity and InfLLM-V2s usage of FA in specific cases4. Length Extrapolation Capability Testing We evaluate our method on RULER (Hsieh et al., 2024), aiming to examine model length extrapolation performance across different context length regimes. Notably, the maximum training context length is 64K tokens, and we evaluate models on lengths ranging from 8K to 256K tokens. As shown in Table 2 (left group), our approach achieves the best performance across all evaluated context lengths while maintaining favorable sparsity levels, with ΩMSR values consistently in the range of 0.65 0.7. Besides, we observe that for 8B-scale models at context lengths exceeding 64K tokens, the FAXA 4InfLLM-V2 does not have well-defined ΩMSR, as it employs FA when the context length is below 8K tokens and switches to hybrid attention computation mode for longer contexts. configuration consistently yields the best performance. This is because FAXA attains lower ΩESR than the other configurations, allowing it to retain and exploit more relevant information. This advantage becomes particularly pronounced at extremely long contexts, e.g., at 256K tokens, FAXA still maintains strong performance. Long-form Reasoning Task We further evaluate our models on LongBench-V2 (Bai et al., 2025), widely recognized long-context reasoning benchmark with context lengths ranging from 8K to 2M words. As shown in Table 2 (right group), our approach consistently delivers strong results across both Easy and Hard settings among all compared methods, as well as achieves the best average performance (either FA-SSA or FA-XA setting). Interestingly, we observe that the FAXA configuration does not yield strong performance on Qwen3-8B, which may be attributed to model-specific characteristics of this model and could benefit from more fine-grained hyperparameter tuning. 5. Ablation Study In this section, we analyze the Elastic Attention preliminary on the Qwen3-4B and Llama3.1-8B-Instruct models. We investigate the design of its core component, i.e., the Attention Router module ( 5.1); study the impact of different target sparsity ( 5.2); analyze our methods trade-off between performance and inference efficiency ( 5.3); and show the scalability of Elastic Attention ( 5.4). 6 Test-time Adaptive Sparsity Ratios for Efficient Transformers Figure 5. Visualization of task representation similarity. (Left) before Task MLP, the pooled hidden states exhibit high pairwise cosine similarity across different tasks; (Right) after passing through the Task MLP, the inter-task similarity significantly decreases. Table 3. Comparison among different MLP hidden dimensions. Hidden Size 2 4 (default) 6 8 LongBench RULER LongBench-V2 Avg. 48.07 48.08 48.07 48.47 60.67 61.81 62.08 65. 26.92 27.88 26.20 25.48 45.22 45.92 45.45 46.40 5.1. Design of Attention Router Effect of Task MLP We analyze the role of the Task MLP within the Attention Router. Specifically, we take the Task MLP from the first layer of the model and calculate the pairwise cosine similarity of task-specific hidden states (xK) before and after they are processed by the Task MLP. Notably, lower cosine similarity indicates greater separation between task representations, reflecting improved task discrimination. As shown in Figure 5, we compare representations across three tasks and observe that, after passing through the Task MLP, inter-task similarity is significantly reduced. This result indicates that the Task MLP effectively disentangles task-specific features and produces more discriminative representations for the subsequent routing decisions performed by the Router MLP. We provide more details and results of other models in Appendix H.1. Impact of MLP Intermediate Hidden Size We study the impact of the intermediate hidden size in the Taskand the Router-MLP layers within the Attention Router. Specifically, we experiment with the intermediate dimension from 2 8, where 4 corresponds to the default setting used in our main experiments. As shown in Table 3, all settings achieve very similar average performance across 3 benchmarks. Although the 8 setting attains the highest overall score, we adopt the 4 setting in our main experiments, as it provides more favorable trade-off between performance gains and additional parameter overhead. Attention Mode Routing Analysis The attention computation modes assigned by the Attention Router exhibit strong and consistent patterns at the head level. We observe that, in LLMs, certain heads primarily leverage FA comFigure 6. Overview of routing activation frequency of each head in Qwen3-4B. Red indicates heads that are consistently routed to FA (i.e., retrieval heads) across all 6 tasks in LongBench-E, while blue denotes heads that are consistently routed to SA. Figure 7. Comparison of performance and test-time ΩMSR among different training sparsity target settings. The bar chart denotes the performance and the line chart denotes ΩMSR in each task. putation mode, while others are frequently assigned with SA. As shown in Figure 6, we analyze the distribution of attention computation modes for each head across the 6 LongBench-E sub-tasks. We find that subset of heads, which are predominantly located in the middle to higher layers, are consistently activated in FA mode. This observation is consistent with prior findings in Wu et al. (2024), indicating that these heads function as retrieval heads. Partial heads that are highlighted in lighter colors exhibit switching behavior across tasks, and the remaining heads are consistently routed to SA. More results are shown in Appendix H.2. 5.2. Impact of Target Sparsity Allocation We study the impact of target sparsity (Eq. 8) to model performance. Specifically, we fix the target sparsity of sparsity-robust tasks (trob) to 1, while progressively decreasing target sparsity for sparsity-sensitive tasks (tsen) from 0.7 to 0.4. As shown in Figure 7, as tsen decreases, the resulting (ΩMSR) allocated by the model exhibits slightly greater task-level differentiation across different tasks. Yet, Test-time Adaptive Sparsity Ratios for Efficient Transformers (a) Performance on RULER. (b) Inference Acceleration. (c) Statistics of ΩESR. Figure 8. Comparison of performance and inference speedup on the RULER benchmark across different methods. We adopt Llama-3.18B-Instruct as the backbone model and compare with training-based methods (FASSA), as well as other cutting-edge sparse attention methods. We report ΩESR, as it provides fair comparison of the effective proportion of attended tokens across different approaches. we can find that ΩMSR does not strictly match the target t. This is because we adopt task-dependent and non-tight constraints, which do not force the model to exactly satisfy the prescribed sparsity. We provide full training curves with explanations in Appendix H.4. Besides, when tsen is set too low (e.g., 0.4), i.e., allocating higher proportion of FA computation, the overall performance can even surpass that of the backbone. Yet, from an inference-efficiency perspective, we adopt tsen = 0.7 and trob = 1.0 in our main experiments, which achieves favorable balance between strong overall performance and high inference efficiency. 5.3. Overall Performance and Inference Efficiency We compare different methods on RULER across multiple context-length regimes in terms of both task performance and inference efficiency. Implementation details are provided in Appendix D.2. As shown in Figure 8, our method consistently achieves the best performance among all compared approaches. In addition, its inference speedup increases with longer context lengths, as the model dynamically allocates higher ΩMSR for longer inputs. In contrast, several baselines are constrained by architectural designs. For example, NSA and InfLLM-V2 impose strict divisibility requirements on the number of KV attention heads (e.g., multiples of 16), which are not well aligned with Llama-3.18B. Moreover, MoBA and InfLLM-V2 must reserve part of the computation budget to pre-compute sequence-level features, leading to GPU out-of-memory failures at 256K context length. Finally, we analyze ΩESR, computed based on the actual number of attended tokens. We observe that, as sequence length increases, training-based hybrid models maintain an approximately constant ΩESR, whereas our method achieves consistently lower ΩESR and slightly outperforms comparable baselines (e.g., PruLong), indicating superior scalability in effective sparsification. We show more details and comparison results in Appendix H.3. Table 4. Results of implementing retrieval heads with XA. Method LongBench RULER LongBench-V2 Avg. Qwen3-4B + XA-SSA Qwen3-8B + XA-SSA Llama-3.1-8B + XA-SSA 48.45 48.14 52.16 49.25 53.28 50.71 66.00 63. 75.74 66.31 83.47 70.27 25.96 25.96 31.97 32.69 32.69 29.09 46.80 45. 53.26 49.42 56.48 50.02 5.4. Scalability of Elastic Attention We assess the scalability of Elastic Attention by implementing retrieval heads with XA, yielding the XASSA setting where the entire model operates under full SA regime. As shown in Table 4, the XASSA setting can still preserve strong performance. For instance, for Qwen3-4B, the average performance gap across three long-context benchmarks even remains within 1 point. Although 8B-scale models exhibit performance degradation, this trade-off is accompanied by substantial improvement in inference speed, owing to the fully sparse attention structure of the model. 6. Conclusion We proposed Elastic Attention, test-time adaptive sparse attention that dynamically adjusts model sparsity based on the input. We show that effective sparsity allocation can be achieved by distinguishing between sparsity-robust and sparsity-sensitive task regimes. Thus, Elastic Attention works by introducing lightweight Attention Router that performs head-wise routing between FA and SA modes based on the input task regimes. Notably, Elastic Attention brings negligible overhead, eliminating the modification of pretrained backbones. Experiments across 3 widely accepted long-context benchmarks on different cutting-edge LLMs demonstrate the superiority of our methods. 8 Test-time Adaptive Sparsity Ratios for Efficient Transformers"
        },
        {
            "title": "References",
            "content": "Ainslie, J., Lee-Thorp, J., De Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 172. URL https://aclanthology.org/2024. acl-long.172. Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X., Cao, S., Xu, J., Hou, L., Dong, Y., et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 36393664, 2025. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv:2004.05150, 2020. Bengio, Y., Leonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Bhaskar, A., Wettig, A., Gao, T., Dong, Y., and Chen, D. Cache me if you can: How many kvs do you arXiv preprint need for effective long-context lms? arXiv:2506.17121, 2025. Child, R. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL https://arxiv.org/ abs/2405.21060. Glorioso, P., Anthony, Q., Tokpanov, Y., Whittington, J., Pilault, J., Ibrahim, A., and Millidge, B. Zamba: compact 7b ssm hybrid model, 2024. URL https:// arxiv.org/abs/2405.16712. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Guo, J., Tang, H., Yang, S., Zhang, Z., Liu, Z., and Han, S. Block Sparse Attention. https://github.com/ mit-han-lab/Block-Sparse-Attention, 2024. He, Z., Zhang, Y., Zhang, C., Jiang, H., Yang, Y., and Qiu, L. Trianglemix: Accelerating prefilling via decoding-time contribution sparsity, 2025. URL https://arxiv. org/abs/2507.21526. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419 1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 112. URL https://aclanthology.org/2021. naacl-main.112. Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Ji, X., Zhang, H., Fu, F., and Cui, B. Sale : Low-bit estimation for efficient sparse attention in long-context llm prefilling, 2025. URL https://arxiv.org/abs/ 2505.24179. Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025. Gao, Y., Zeng, Z., Du, D., Cao, S., So, H. K.-H., Cao, T., Yang, F., and Yang, M. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Ku, J., Nguyen, E., Romero, D. W., Brixi, G., Yang, B., Vorontsov, A., Taghibakhshi, A., Lu, A. X., Burke, D. P., Brockman, G., Massaroli, S., Re, C., Hsu, P. D., Hie, B. L., Ermon, S., and Poli, M. Systems and algorithms for convolutional multi-hybrid language models at scale, 2025. URL https://arxiv.org/abs/ 2503.01868. 9 Test-time Adaptive Sparsity Ratios for Efficient Transformers Lai, X., Lu, J., Luo, Y., Ma, Y., and Zhou, X. Flexprefill: context-aware sparse attention mechanism for In The Thirteenth efficient long-sequence inference. International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=OfjIlbelrT. Li, J., Zhu, H., Liu, H., Shi, X., Zong, H., Dong, Y., Zhang, K., Jiang, S., Jin, Z., and Li, G. aixcoder-7b-v2: Training llms to fully utilize the long context in repository-level code completion. arXiv preprint arXiv:2503.15301, 2025. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., Abend, O., Alon, R., Asida, T., Bergman, A., Glozman, R., Gokhman, M., Manevich, A., Ratner, N., Rozen, N., Shwartz, E., Zusman, M., and Shoham, Y. Jamba: hybrid transformer-mamba language model, 2024. URL https://arxiv.org/ abs/2403.19887. Liu, J., Zhu, D., Bai, Z., He, Y., Liao, H., Que, H., Wang, Z., Zhang, C., Zhang, G., Zhang, J., et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025a. Liu, X., Li, R., Huang, M., Liu, Z., Song, Y., Guo, Q., He, S., Wang, Q., Li, L., Liu, Q., et al. Thus spake long-context large language model. arXiv preprint arXiv:2502.17129, 2025b. Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019. URL https://arxiv.org/abs/ 1711.05101. Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S., He, W., Yuan, E., Wang, Y., Huang, Z., Yuan, H., Xu, S., Xu, X., Lai, G., Chen, Y., Zheng, H., Yan, J., Su, J., Wu, Y., Zhang, N. Y., Yang, Z., Zhou, X., Zhang, M., and Qiu, J. Moba: Mixture of block attention for longcontext llms, 2025a. URL https://arxiv.org/ abs/2502.13189. Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S., He, W., Yuan, E., Wang, Y., et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025b. Mei, L., Yao, J., Ge, Y., Wang, Y., Bi, B., Cai, Y., Liu, J., Li, M., Li, Z.-Z., Zhang, D., et al. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334, 2025. Narayan, S., Cohen, S. B., and Lapata, M. Dont give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018. Peng, D., Fu, Z., Ye, Z., Song, Z., and Wang, J. Accelerating prefilling for long-context llms via sparse pattern sharing. arXiv preprint arXiv:2505.19578, 2025a. Peng, D., Fu, Z., Ye, Z., Song, Z., and Wang, J. Accelerating prefilling for long-context llms via sparse pattern sharing, 2025b. URL https://arxiv.org/abs/ 2505.19578. Ren, L., Liu, Y., Lu, Y., Shen, Y., Liang, C., and Chen, W. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint, 2024. URL https://arxiv.org/abs/2406.07522. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Tang, Z., Wang, H., Qiu, Q., Ji, B., Sun, R., Zhou, K., Li, J., and Zhang, M. Loom-scope: comprehensive and efficient long-context model evaluation framework. arXiv preprint arXiv:2507.04723, 2025. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wu, W., Wang, Y., Xiao, G., Peng, H., and Fu, Y. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv, 2024a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview. net/forum?id=NG7sS51zVF. 10 Test-time Adaptive Sparsity Ratios for Efficient Transformers Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., and Chen, J. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025b. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C. W., Wang, Z., and Chen, B. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Zhao, W., Zhou, Z., Su, Z., Xiao, C., Li, Y., Li, Y., Zhang, Y., Zhao, W., Li, Z., Huang, Y., Sun, A., Han, X., and Liu, Z. Infllm-v2: Dense-sparse switchable attention for seamless short-to-long adaptation, 2025. URL https: //arxiv.org/abs/2509.24663. Xiao, G., Tang, J., Zuo, J., Yang, S., Tang, H., Fu, Y., Han, S., et al. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. In The Thirteenth International Conference on Learning Representations, 2025. Xu, P., Ping, W., Wu, X., Liu, Z., Shoeybi, M., and Catanzaro, B. Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities. arXiv preprint arXiv:2407.14482, 2024. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. In Proceedings of the 42nd International Conference on Machine Learning (ICML), 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yu, Y., Liu, J., Wu, Q., Wang, H., and Pei, J. Sliding window attention adaptation. arXiv preprint arXiv:2512.10411, 2025. Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y., Wang, L., Xiao, Z., Wang, Y., Ruan, C., Zhang, M., Liang, W., and Zeng, W. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2307823097, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1126. URL https:// aclanthology.org/2025.acl-long.1126/. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Zhang, C., Bai, Y., Li, J., Gui, A., Wang, K., Liu, F., Wu, G., Jiang, Y., Bu, D., Wei, L., et al. Efficient context scaling with longcat zigzag attention. arXiv preprint arXiv:2512.23966, 2025a. 11 Test-time Adaptive Sparsity Ratios for Efficient Transformers A. Code and Model We open-source our code and model as follows: Code: https://github.com/LCM-Lab/Elastic-Attention ı Model: https://modelscope.cn/collections/LCM group/Elastic-Attention B. Related Work B.1. Sparse Attention Mechanisms To mitigate the quadratic complexity of standard attention, existing research has broadly evolved along two trajectories: inference-time heuristics and training-aware sparsification. Inference-time heuristics typically employ static patterns, such as fixed sliding windows or strides (Xiao et al., 2024b; He et al., 2025; Beltagy et al., 2020), to restrict the receptive field. To capture more dynamic dependencies, content-aware approaches have been proposed. Token eviction policies discard uninformative tokens based on accumulated importance scores (Zhang et al., 2023; Li et al., 2024; Liu et al., 2024), while kernel-based estimators identify salient blocks to bypass redundant computations (Jiang et al., 2024). Complementarily, prefill optimizers leverage importance-driven selection to accelerate long-context processing (Lai et al., 2025; Xu et al., 2025; Zhang et al., 2025b; Peng et al., 2025b; Ji et al., 2025). Despite their effectiveness, these heuristic methods often hinge on sensitive hyperparameters, limiting their robustness across varying tasks. In contrast, internalize sparsity within the optimization objective to align training with sparse inference. primary direction involves learnable selection. For instance, SeerAttention (Gao et al., 2024), NSA (Yuan et al., 2025) and MoBA (Lu et al., 2025a) employ learnable gates and hierarchical constraints to approximate ground-truth attention patterns. To bridge the gap between dense pre-training and sparse adaptation, InfLLM-v2 (Zhao et al., 2025) introduces dense-sparse switchable mechanism via parameter-free pooling, while DSA (DeepSeek-AI, 2025) utilizes lightning indexer with two-stage training strategy to efficiently filter top-k key-value pairs. However, most of these methods focus on fine-grained block-level selection within fixed attention mechanism, rather than dynamically adapting the attention mode itself based on input complexity. B.2. Hybrid Efficient Architectures To balance efficiency with performance, hybrid architectures strategically integrate Full Attention (FA) with linear-complexity operators. The dominant paradigm, inter-layer hybridization, interleaves linear layers (e.g., SSMs or RNNs) with standard attention layers to recover associative recall capabilities (Ku et al., 2025; Dao & Gu, 2024). Notable large-scale implementations, such as Jamba (Lieber et al., 2024), utilize fixed block-wise ratios, while variants optimize memory via shared global blocks (Glorioso et al., 2024) or sliding windows (Ren et al., 2024). More recently, intra-layer hybridization has emerged to refine granularity. For example, PruLong (Bhaskar et al., 2025) and DuoAttention (Xiao et al., 2024a) combine FA and Streaming Sparse Attention (SSA) within individual layers, assigning different heads to different modes. LongCat (Zhang et al., 2025a) proposes the LoZA mechanism, constructing static ZigZag topology by replacing low-sensitivity Multi-head Latent Attention (MLA) modules with linear-complexity SSA. Nevertheless, critical limitation of these approaches is their reliance on static topologies or pre-defined ratios determined prior to inference. Such rigid designs lack the flexibility to distinguish between sparsity-robust and sparsity-sensitive tasks dynamically, often leading to suboptimal resource allocation for diverse inputs. C. Retrieval Scoring and Sparsification Setup In this section, we provide detailed description of the retrieval head identification and the progressive sparsification strategy mentioned in Section 2.2. C.1. Retrieval Score Calculation Following the methodology proposed by Retrieval Head (Wu et al., 2024), we identify and rank attention heads based on their ability to retrieve specific information from long contexts. We employ the Needle-in-a-Haystack probing method using the Llama-3.1-8B-Instruct (Grattafiori et al., 2024) model, where specific key information (the needle) is inserted into 12 Test-time Adaptive Sparsity Ratios for Efficient Transformers Table 5. Performance retention rates across various model sparsity ratios. The values represent the percentage of performance relative to the Full Attention baseline (Sparsity 0.0), where 100.00 indicates parity. Rows denote the model sparsity ratio, and columns denote the evaluation tasks. Sparsity Single-Doc QA Multi-Hop QA Summarization Few-Shot Synthetic Code Full (ΩMSR = 0.0) 100.00 100.00 100. 100.00 100.00 100.00 ΩMSR = 0.1 ΩMSR = 0.2 ΩMSR = 0.3 ΩMSR = 0.4 ΩMSR = 0.5 ΩMSR = 0.6 ΩMSR = 0.7 ΩMSR = 0.8 ΩMSR = 0.9 ΩMSR = 1.0 85.37 71.32 61.94 59.36 57.95 56.95 56.08 56.04 56.02 56.46 96.23 73.84 69.07 66.48 64.33 59.66 61.91 59.55 63.40 61. 99.38 98.00 96.19 94.75 93.02 94.68 92.52 93.73 94.00 93.42 96.54 93.83 91.83 87.01 87.33 87.05 88.51 89.07 90.19 90.19 92.27 81.26 59.28 47.61 45.31 44.75 41.25 47.98 48.37 47.98 99.07 98.49 98.10 95.58 99.54 98.66 97.73 99.86 98.48 99.46 long context (the haystack). For given attention head in layer ℓ, denoted as Hℓ,h, we calculate its Retrieval Score (Sℓ,h) by measuring the attention mass allocated to the needle tokens. Formally, let be the input sequence length, O(ℓ,h) Rss be the attention matrix, and Ineedle be the set of indices corresponding to the needle tokens. The score is computed as: Sℓ,h = 1 (cid:88) (cid:88) O(ℓ,h) T,j xD jIneedle (9) where represents the validation dataset, and O(ℓ,h) T,j denotes the attention weight from the last token (query position ) to the needle position j. higher Sℓ,h indicates that the head frequently and strongly activates on relevant information in long contexts. C.2. Progressive Sparsification Strategy Based on the computed scores Sℓ,h, we rank all attention heads across the model in descending order. As defined in the main text, the Model Sparsity Ratio (ΩMSR) represents the proportion of heads converted to local attention. To simulate the varying levels of sparsity reported in our experiments (e.g., ΩMSR = 20%), we employ thresholding mechanism: 1. We determine the number of heads to preserve as Full Attention (FA) via = (1 ΩMSR) (L H). 2. The top-k heads with the highest retrieval scores are retained as Retrieval Heads (keeping FA) to ensure global information integration. 3. The remaining heads (the bottom ranked ones) are replaced with Streaming Sparse Attention (SSA) heads. D. Implementation Details In this section, we provide comprehensive overview of the training configurations, baseline implementation details, and system-level optimizations tailored for efficient long-context processing. D.1. Training Configuration and Hyperparameters Model Architecture. We validate the scalability of our approach across diverse model sizes, ranging from lightweight architectures like Qwen3-4B to widely-adopted mid-class models such as Qwen3-8B (Yang et al., 2025) and Meta-Llama-3.1Instruct (Grattafiori et al., 2024). To preserve the pre-trained models general capabilities, we adhere to parameter-efficient fine-tuning protocol: the pre-trained backbone is frozen, and only the Attention Router parameters are optimized. Regarding task representation, we employ boundary-pooling strategy that exclusively aggregates the first 100 and last 100 tokens 13 Test-time Adaptive Sparsity Ratios for Efficient Transformers Table 6. Hyperparameters: General configuration (Left) and Baseline-specific settings (Right). (a) General Config (b) Baseline-Specifics"
        },
        {
            "title": "Value",
            "content": "Param MoBA NSA InfLLMv2 Model & Training Base Model Sequence length Precision Global Batch Size Training Steps Mask / Reg. LR Warmup Ratio AdamW Momentum (β1, β2) Weight Decay Learning Rate Schedule Qwen, Llama 65536 bfloat16 48 300 5e4 / 1e3 0.2 (0.9, 0.95) 0.1 Cosine"
        },
        {
            "title": "Sparsity Config",
            "content": "Sink / Local Size Block / Chunk Size Stride / Threshold Selection Mode 128 / 2048 64 / 16384 16 / 0.9 Inverse Block Size Top-k Window Size Kernel Size Kernel Stride Q/K/V Proj Gate Compress K/V Structure & Kernel 1024 8 - - - 64 128 512 32 16 Learnable Params - - Extra Config 64 64 2048 32 16 - - Compress Type Use NoPE Dense Len Pooling - - Linear - - Pooling False 8192 of the sequence. These segments are selected as they typically encapsulate critical system instructions and user queries essential for accurate task identification. Optimization Setup. All models are trained with sequence length of = 65, 536 tokens using bfloat16 precision to accommodate long-context dependencies. The training utilizes the AdamW optimizer (Loshchilov & Hutter, 2019) (β1 = 0.9, β2 = 0.95) on distributed cluster employing Fully Sharded Data Parallel (FSDP) with Hybrid Sharding strategy. We adopt decoupled learning rate strategy to balance router convergence and sparsity regularization: Router Parameters: learning rate of 5 104 is applied to the attention router to facilitate rapid adaptation to retrieval patterns. Regularization Coefficients: higher learning rate of 1 103 is assigned to the sparsity regularization terms. Specifically, the dual regularization coefficients λ1 and λ2 are randomly initialized and optimized alongside the router parameters. We utilize cosine decay learning rate schedule following linear warmup phase spanning the first 20% of total training steps. D.2. Baseline Implementation Details To rigorously evaluate the efficacy of our approach, we benchmark against comprehensive suite of state-of-the-art sparse attention mechanisms. These are categorized into training-free methods and training-based adaptation methods. Training-Free Methods. We employ XAttention 5 (Xu et al., 2025) as the primary training-free baseline. This category relies on heuristic-based sparsity without parameter updates. 5https://github.com/mit-han-lab/x-attention 14 Test-time Adaptive Sparsity Ratios for Efficient Transformers Algorithm 1 Comparison of Serial Dispatch (Baseline) vs. Parallel BSA (Ours) (a) PyTorch Baseline (b) Ours (Parallel via BSA) Input: Q, K, V, Router 1: R(x) 2: Step 1: Serial Split Ifull {h rh = 0} Isp {h rh = 1} Qfull Q[:, Ifull] Step 2: Separate Comp. Ofull FlashAttn(. . . ) Osp SlidingWin(. . . ) Step 3: Merge Results O[:, Ifull] Ofull O[:, Isp] Osp Input: Q, K, V, Router 1: R(x) 2: Map(r) Step 1: Unified Execution BSA Krn(Q, K, V, m) Inside Kernel: par for do if m[h]==SP then O[h] Sparse(. . . ); else O[h] Full(. . . ); 3: end if end par for Training-Based Methods. For methods requiring training adaptation, including InfLLM v2 6 (Zhao et al., 2025), MoBA 7 (Lu et al., 2025a), NSA 8 (Yuan et al., 2025), PruLong 9 (Bhaskar et al., 2025), and DuoAttention 10 (Xiao et al., 2025), we implement unified fine-tuning protocol to ensure strict fairness. Unlike our method, which freezes the backbone entirely, most competing methods require updating projection layers. For InfLLM v2, MoBA, and NSA, we restrict the trainable scope to the query-key-value projection weights (Wqkv) and their respective method-specific parameters, freezing the remaining backbone. All baselines are trained under the same environment and dataset, and the hyperparameters are strictly adhered to their original setups, such as block size of 1024 for MoBA versus 64 for NSA, and the specific dense context length (8K) for InfLLM v2. Detailed hyperparameter comparisons are provided in Table 6 (Right). D.3. Sparsity and Kernel Configuration To achieve efficient streaming inference, we employ Block-Sparse-Attention (Guo et al., 2024). This configuration governs the granularity and retention policy of the attention mechanism: Block Size: Set to 64, defining the minimum unit of sparsity. Chunk Size: Set to 16,384, enabling the processing of ultra-long sequences. Sink Token Strategy: We enforce sink token size of 128 to preserve the attention sink phenomenon, ensuring stability during streaming generation. Specific kernel parameters, including stride, normalization, and selection modes, are detailed in the Sparsity Config section of Table 6 (Left). E. Efficient Deployment of Elastic Attention critical system bottleneck in hybrid attention pertains to the efficient scheduling of heterogeneous attention workloads within single batch. Algorithm 1(a) illustrates the conventional paradigm, termed Serial Dispatch, as adopted by libraries like Flash-Attn11 (Dao, 2024). This method entails explicit data rearrangement (highlighted in red), requiring the materialization of input tensors according to routing decisions r. 6https://modelscope.cn/models/OpenBMB/MiniCPM4-8B/file/view/master/modeling_minicpm.py 7https://github.com/MoonshotAI/MoBA 8https://github.com/XunhaoLai/native-sparse-attention-triton 9https://github.com/princeton-pli/PruLong 10https://github.com/mit-han-lab/duo-attention 11https://github.com/Dao-AILab/flash-attention Test-time Adaptive Sparsity Ratios for Efficient Transformers Table 7. LongBench-E results comparison. The 1st and the 2nd performance in each comparison group are highlighted with bold font and underlined, respectively. Method Qwen3-4B + InfLLM-V2 + DuoAttention + PruLong + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Qwen3-8B + InfLLM-V2 + DuoAttention + PruLong + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Llama-3.1-8B-Instruct + InfLLM-V2 + NSA + DuoAttention + PruLong + MoBA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Single-Document QA Multi-Document QA Summarization Few-shot Learning Synthetic Code MF-en Qasper HotpotQA 2WikiMQA GovReport MultiNews TREC TriviaQA SAMSum PCount PRe Lcc RB-P 52.16 49.13 48.85 48.88 42.41 42.46 50.36 49.13 52.06 49.21 49.92 46.35 49.59 51.06 48.81 44.29 48.93 51.45 48.18 45. 53.44 48.60 42.86 52.68 50.74 48.92 53.80 53.48 53.65 51.81 35.21 37.51 34.60 34.28 35.80 33.90 32.80 35.27 36.74 34.63 41.22 38.05 41.32 41.04 40.65 36.96 38.03 40.84 39.83 33. 44.06 38.93 41.80 44.62 44.63 44.33 43.84 46.35 45.14 44.79 44.81 40.26 42.25 45.14 38.76 38.10 44.54 47.87 45.65 47.32 58.98 49.51 51.82 53.87 48.26 42.26 57.32 53.34 59.38 51. 59.62 50.74 40.79 52.01 49.70 46.36 59.98 55.50 60.21 53.53 Qwen3-4B backbone model 33.47 33.99 33.26 33.36 33.82 31.69 33.79 33.35 33.45 33.14 23.45 25.92 23.68 23.70 25.11 23.07 23. 23.65 23.55 23.75 Qwen3-8B backbone model 33.27 34.49 33.14 33.20 35.19 32.93 33.41 33.14 33.30 33.21 23.42 24.67 23.19 23.39 25.47 23.00 23.37 23.24 23.29 23. 32.15 29.68 29.18 30.01 29.84 31.53 33.17 29.85 33.18 30.02 44.21 35.14 37.22 39.90 38.29 32.63 40.65 39.74 40.60 30.95 70.67 67.67 66.33 66.33 68.67 68.00 69.00 69.33 68.67 68. 71.33 67.00 70.33 70.33 67.67 74.33 69.33 72.33 69.00 70.00 Llama-3.1-8B-Instruct backbone model 44.08 41.85 39.94 38.41 36.41 41.45 43.47 42.33 45.67 39.27 34.50 34.40 34.57 34.01 34.25 34.79 34. 34.50 34.56 34.56 26.02 25.76 25.29 25.84 25.78 26.65 26.05 25.77 26.03 25.98 71.00 69.00 68.00 69.67 70.00 69.67 72.33 69.67 71.67 70.67 88.22 86.14 87.70 88.31 85.69 82.49 87. 87.23 87.33 87.64 86.77 87.03 86.83 87.68 84.13 86.74 87.69 88.67 87.78 87.83 90.54 89.92 89.69 90.33 91.45 89.91 90.39 91.70 91.37 90.59 39.74 38.76 39.73 39.77 39.34 39.82 39. 40.62 39.79 40.12 41.83 39.61 42.11 41.83 40.85 40.36 41.77 41.56 41.92 40.73 42.94 43.04 42.27 42.06 42.13 40.76 42.99 42.60 42.62 42.24 2.33 4.30 2.11 4.79 2.33 5.67 3. 2.00 4.39 5.50 2.00 12.07 0.00 0.33 14.00 2.67 2.00 0.00 2.33 0.67 12.67 6.33 2.33 10.13 9.80 7.33 8.33 9.33 8.31 9.07 96.84 72.78 92.24 89.67 72.52 41.93 74. 94.86 84.32 93.00 98.33 79.67 95.00 97.00 83.89 55.33 83.72 96.14 99.56 92.78 99.33 77.73 28.00 99.00 97.33 64.67 73.33 98.67 91.00 73.67 57.93 62.56 57.57 55.44 58.61 62.44 59. 57.80 50.47 59.15 66.31 66.86 67.43 67.30 68.21 69.81 65.71 68.44 65.40 68.56 63.85 59.78 65.18 64.69 68.55 69.47 62.92 67.72 63.22 68.12 50.84 56.59 50.25 50.95 50.63 53.77 49. 50.87 58.11 50.98 56.08 52.30 57.32 57.18 56.89 54.57 55.53 57.47 55.74 55.13 47.78 68.81 50.12 48.24 53.59 59.48 50.13 53.69 49.75 53.49 Avg. 48.45 46.68 46.95 47.19 45.09 43.02 46.44 48.08 47.59 48.14 52.16 49.03 50.67 51.34 50.47 46.12 50.13 51.51 51.66 49.25 53.28 50.73 44.03 51.82 52.11 49.69 51.00 53.35 52.71 50. Consequently, it introduces two major sources of inefficiency that contradict the high-throughput requirements of longcontext inference: (1) Memory Overhead, incurred by allocating and copying non-contiguous tensor fragments (e.g., separating retrieval heads from sparse heads); and (2) Kernel Launch & Scheduling Overhead. As noted in the main text, parallelism along the sequence dimension dominates execution in long-context scenarios. Launching separate kernels for different head groups fragments this workload, incurring latency from multiple kernel invocations and disrupting the GPUs ability to globally schedule thread blocks across available streaming multiprocessors. To overcome these limitations, we employ the Block Sparse Attention (BSA) Kernel12 (Guo et al., 2024) (Algorithm 1(b)). Obviating the need for tensor splitting, we pass routing decisions directly to the kernel as lightweight metadata m. As depicted in the green block, the kernel leverages thread-block level branching: each thread block dynamically retrieves its assigned heads type from and executes the corresponding attention logic. This design enables unified kernel launch for all heads. By keeping the grid dimensions intact (Batch Heads Sequence Blocks), we effectively eliminate redundant memory copies and avoid workload fragmentation, allowing the GPU hardware scheduler to optimally distribute sequence blocks. One can refer to the anonymous code provided in Appendix ?? for more details. F. Theoretical Explanation of Attention Router Optimization The primary challenge in optimizing an attention router lies in the non-differentiable nature of discrete selection. To enable the model to learn which attention heads are essential for given task, we employ continuous relaxation technique based on the Gumbel-Softmax (specifically, the Gumbel-Sigmoid for binary decisions) and the Straight-Through Estimator (STE). 12https://github.com/mit-han-lab/Block-Sparse-Attention Test-time Adaptive Sparsity Ratios for Efficient Transformers F.1. Latent Representation and Logit Generation Given the Key hidden states xK RsHd extracts task-aware representation via pooling operation and two-stage MLP: , where is the sequence length and is the hidden dimension, the router first = Pooling(xK), x RHd The routing logits for each head are then computed as: = MLProuter(MLPtask(x K)), (10) (11) where RH represents the unnormalized preference for specific attention mode (e.g., FA vs. SA) for each head. F.2. Differentiable Sampling via Gumbel-Sigmoid To simulate the stochasticity of discrete routing while maintaining differentiability, we apply the reparameterization trick (Bhaskar et al., 2025). We introduce i.i.d. noise samples Uniform(0, 1) and transform them into Gumbel noise g: = log( log(u + ϵ) + ϵ), (12) where ϵ is small constant for numerical stability. The discrete binary decision is then relaxed into continuous approximation ˆzsoft using temperature-dependent Sigmoid function: ˆzsoft = σ (cid:19) (cid:18) + τ = 1 1 + exp (cid:0) z+g τ (cid:1) (13) Here, τ (0, ) is the temperature parameter. When τ , the distribution becomes uniform; as τ 0, the output ˆzsoft approaches discrete Bernoulli distribution. In Section F.3, we introduce the strategy for parameter τ . F.3. Temperature Annealing Schedule The optimization process utilizes an annealing schedule for τ to bridge the gap between the continuous relaxation and the discrete reality. We define the decay as: τ (t) = max(τmin, τinit exp(r p)), (14) where is the training step and is the decay rate (we set = 0.6). In the early stages of training, high τ encourages exploration by providing dense gradients; in later stages, low τ forces the router to converge toward the hard binary decisions used during actual deployment. G. Additional Evaluation Results In this section, we present detailed performance for the benchmarks discussed in the main paper. G.1. Evaluation Results on LongBench Table 7 and Table 8 provides the complete evaluation results on the LongBench-E. We report the performance metrics across all sub-tasks, categorized into 6 categories: Single-Document QA (S-Doc QA), Multi-Document QA (M-Doc QA), Summarization (Summ), In-context Learning (In-Context), Synthetic, and Code Tasks. G.2. Evaluation Results on LongBench-V2 and RULER This subsection details the experimental settings and evaluation results for LongBench-V2 and the RULER benchmark. For RULER, as detailed in Table 9, our evaluation on the RULER benchmark covers comprehensive range of context lengths, extending from 8K to 262K tokens. The comparative performance results are summarized in Table 10. 17 Test-time Adaptive Sparsity Ratios for Efficient Transformers Table 8. Performance on LongBench-E. We report average performance (Perf.) and ΩMSR per task category. The 1st and the 2nd performance in each comparison group are highlighted with bold font and underlined, respectively. Note that ΩMSR is not calculated for XA-SSA as it does not employ retrieval heads. Method Qwen3-4B + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Qwen3-8B + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Llama-3.1-8B-Instruct + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) S-Doc QA M-Doc QA Summ In-Context Synthetic Code Avg. Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR Perf. ΩMSR 43.69 38.16 39.13 41.58 42.20 44.40 41.92 45.57 44.73 40.63 43.48 46.15 44.01 39.43 48.75 46.63 42.33 48. 49.92 49.40 48.30 - - - - 0.66 0.68 - - - - - 0.64 0.75 - - - - - 0.64 0.71 - Qwen3-4B backbone model 28.46 29.46 27.38 28.76 28.50 28.49 28.45 - - - - 0.76 0.82 - 66.21 64.57 63.44 65.45 65.73 65.26 65.25 Qwen3-8B backbone model 28.34 30.33 27.97 28.39 28.19 28.30 28.27 - - - - 0.72 0.83 - 66.64 64.22 67.14 66.26 67.52 66.23 66.19 - - - - 0.68 0.71 - - - - - 0.65 0.76 - - - - - 0.73 0.76 - - - - - 0.71 0.80 - Llama-3.1-8B-Instruct backbone model - - - - 0.63 0.69 - 30.26 30.72 29.93 30.26 30.17 30.30 30.27 - - - - 0.73 0.80 - 68.16 66.78 66.65 68.57 67.99 68.55 67.83 - - - - 0.74 0.79 - 38.48 34.30 34.81 38.85 38.86 39.42 38. 51.59 43.28 37.45 48.99 46.54 49.99 41.38 51.85 43.91 40.37 51.73 48.92 52.94 46.40 49.59 37.42 23.80 39.01 48.43 44.35 49. 50.16 48.95 29.00 42.86 48.07 50.95 46.73 56.00 36.00 15.17 40.83 54.00 49.66 41.37 - - - - 0.71 0.74 - - - - - 0.65 0.77 - - - - - 0.64 0.72 - 54.38 54.62 58.10 54.52 54.34 54.29 55. 61.20 62.55 62.19 60.62 62.95 60.57 61.85 55.81 64.48 57.65 56.53 60.70 56.49 60.81 - - - - 0.82 0.87 - - - - - 0.78 0.86 - - - - - 0.79 0.87 - 48.45 45.09 43.02 46.44 48.08 47.59 48. 52.16 50.47 46.12 50.13 51.51 51.66 49.25 53.28 49.69 44.03 51.00 53.35 52.71 50.71 - - - - 0.73 0.76 - - - - - 0.69 0.80 - - - - - 0.69 0.77 - Table 9. Detailed configuration for the RULER benchmark evaluation. We evaluate across exponentially increasing context windows up to 256k tokens. PARAMETER CONFIGURATION DETAILS Context Windows 8k, 16k, 32k, 64k, 128k, 256k Sample Size 50 samples per task-length pair Evaluation Tasks Retrieval (NIAH): single {1-3}, multikey {1-3}, multiquery, multivalue QA & Extraction: qa {1,2}, fwe H. Ablation Study H.1. Task-Discriminative Geometry of the Attention Router To elucidate the operational logic of the Attention Router, we examine the geometric structure of the latent space learned by its internal projection layers. We hypothesize that, despite being optimized solely for the sparsity-performance trade-off without explicit task supervision, the router implicitly learns task-discriminative representations to facilitate optimal retrieval head allocation. RHd Formally, let denote the pooled hidden state of an input sequence. The MLPtask maps this context to latent routing representation XK = MLPtask(x K). To quantify the distinctness of these representations while mitigating the representation anisotropy inherent in pre-trained models, we employ Pairwise Conditional Rescaling strategy. This approach normalizes the local subspace spanned by specific task pair (u, v) to measure their geometric relationship independent of global variances. We define the pairwise similarity metric Muv as follows. Let Xk be the set of latent representations corresponding to task k. We first construct the local support set Suv = Xu Xv and compute the feature-wise standard deviation σuv within this subset: σuv = (cid:115) 1 (cid:88) Suv xSuv (x x) + ϵ, 18 (15) Test-time Adaptive Sparsity Ratios for Efficient Transformers Table 10. Additional results on RULER and LongBench-v2. Models Qwen3-4B + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Qwen3-8B + MoBA + NSA XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) Llama-3.1-8B-Instruct + MoBA + NSA + XAttention + Elastic Attention (FA-SSA) + Elastic Attention (FA-XA) + Elastic Attention (XA-SSA) 8K 16K 32K 64K 128K 256K Avg. Perf ΩMSR Easy Hard Avg. Perf Avg. ΩMSR RULER LongBench-v2 87.49 81.74 86.82 85.93 83.35 86.56 81.43 89.69 85.17 78.64 83.88 86.62 85.07 71.48 92.88 89.05 73.27 93. 89.93 92.82 89.07 86.82 71.85 76.94 84.60 79.24 85.38 80.68 85.62 75.26 65.82 84.88 82.81 85.12 74.49 92.83 67.14 50.68 89. 83.42 92.00 82.99 Qwen3-4B backbone model 70.98 42.88 57.62 67.01 67.03 69.42 65.69 53.19 12.78 25.96 48.76 47.83 49.48 49. 43.27 8.67 9.18 38.42 47.32 43.47 53.30 66.00 44.35 45.29 61.23 61.81 63.27 63.70 Qwen3-8B backbone model 82.39 51.64 43.04 80. 77.41 82.34 69.03 65.84 34.68 32.21 63.11 61.17 64.57 64.56 66.71 29.64 24.17 62.49 61.75 63.41 55.73 75.74 55.68 45.18 72. 71.74 73.87 66.31 60.05 40.74 44.18 60.32 50.79 56.88 51.12 63.23 47.88 45.77 63.23 64.55 65.08 56.35 Llama-3.1-8B-Instruct backbone model 89.46 30.12 21.39 86.11 80.20 87.80 77.74 70.79 6.13 22.52 66.89 56.30 68.23 58.86 80.12 1.15 15.30 75.55 68.16 78.87 64. 72.34 0 11.42 41.6 56.47 68.51 47.68 83.47 37.38 30.58 75.36 72.85 81.82 70.27 - - - - 0.66 0.67 - - - - - 0.65 0.76 - - - - - 0.65 0.72 - 32.67 22.00 26.00 26.00 34.00 32.00 26. 39.33 28.00 34.67 32.67 37.33 30.68 38.00 32.00 10.00 21.33 26.67 28.00 30.67 30.67 22.18 26.32 25.56 24.81 24.44 25.94 25. 27.82 24.44 27.44 26.69 31.20 30.77 29.70 33.08 12.78 25.19 28.2 29.70 30.83 28.20 25.96 24.76 25.72 25.24 27.88 28.12 25. 31.97 25.72 30.05 28.85 33.41 30.74 32.69 32.69 11.78 23.80 27.64 29.09 30.77 29.09 - 0.70 0.72 - - 0.66 0.78 - - - 0.73 0.75 - where denotes the element-wise product, and the calculation assumes zero-centered activations. Using these statistics, we define projection ϕuv(x) = σuv (where denotes element-wise division). The metric Muv is then calculated as the cosine similarity between the projected task centroids: Muv = CosSim (cid:32) 1 Xu (cid:88) xXu ϕuv(x), 1 Xv (cid:88) xXv (cid:33) ϕuv(x) . (16) As illustrated in Figure 9b, the latent space exhibits distinct modularity. The observation that similarity scores approach zero (Muv 0) implies that MLPtask maps different problem types to orthogonal subspaces of the local manifold. This geometric orthogonality confirms that the router implicitly functions as semantic discriminator, disentangling task representations into independent basis directions to apply specialized sparsity policies without requiring explicit task labels. H.2. Attention Mode Routing Analysis Comparing the multi-task heatmaps reveals fundamental divergence in attention mechanisms between model families. For Qwen3 (Fig. 11a), we observe structural universality: specific subset of heads remains consistently active (dark red) or sparse (dark blue) across all tasks, suggesting task-agnostic attention topology. In contrast, Llama3.1 exhibits context-dependent sparsity. While the multi-task aggregate (Fig. 11b) shows no universally active heads, the single-task breakdown (Fig. 11c) confirms that strong activations exist but shift dynamically depending on the input context. This indicates that Qwen3 relies on fixed retrieval heads, whereas Llama3.18B-Instruct adaptively reallocates attentional resources based on task requirements. Figure 10. Router latency analysis. The router incurs negligible overhead (avg. 0.196 ms). Our design ensures length-invariant 19 stability, maintaining constant speed from 512 to 1M tokens. Test-time Adaptive Sparsity Ratios for Efficient Transformers (a) Hidden states Similarity before MLPtask. (b) Hidden states Similarity after MLPtask. Figure 9. Pairwise cosine similarity of routing representations ztask. The prevalence of near-zero scores (Muv 0) indicates that the router maps distinct tasks to orthogonal subspaces on the local manifold. This confirms that the model implicitly disentangles task semantics into independent directions without supervision. (a) Multi-Task in Qwen3-8B. (b) Multi-Task in LLama3.1-8B-Instruct. (c) Single Task in Llama3.1-8B-Instruct. Figure 11. Extended Head Robustness Analysis. Similar to Figure 6, these heatmaps visualize the frequency of full-attention activation for each head. (a) and (b) show the multi-task global robustness for Qwen3-8B and Llama3.1-8B-Instruct, respectively, indicating how attention patterns generalize across tasks. (c) presents the robustness analysis for Llama3.1-8B-Instruct in single-task setting. Darker blue indicates heads that are universally sparse, while darker red indicates heads that are universally active. We observe that during training, the effective sparsity levels gradually diverge across different tasks. Despite sharing the same non-tight constraint t, task-dependent differences emerge. This behavior is enabled by the Lagrangian constraint, which dynamically adjusts the weighting of task losses, allowing each task to tolerate different gaps between the achieved sparsity ΩMSR and the target t. H.3. Length Extrapolation and Sparsity Dynamics Analysis We evaluate the length extrapolation capability on the RULER benchmark (8K-256K) using the Llama-3.1-8B-Instruct backbone. Figure 12 visualizes the interplay between performance, inference speedup, and ΩESR. As the context length extends to 256K, baselines like MoBA and NSA suffer catastrophic degradation (near-zero accuracy). In contrast, our Elastic Attention (FA-XA) demonstrates superior robustness, maintaining high score of 68.51. Even our highly efficient variant, Elastic Attention (XA-SSA), achieves score of 47.68, significantly outperforming the standard Xattention baseline (35.82) while operating at much higher sparsity. Figure 12 (b) & (c) highlight critical efficiency advantage. Baselines like NSA and InfLLM-V2 achieve high sparsity (> 0.95) but suffer from limited or regressed speedups (< 1.0) due to heavy dynamic selection overheads or kernel constraints. Conversely, our XA-SSA configuration achieves an extreme sparsity of 0.995 while delivering massive 20 Test-time Adaptive Sparsity Ratios for Efficient Transformers (a) RULER Performance. (b) Efficiency Analysis. (c) Sparsity Rate. Figure 12. Analysis of length extrapolation capability and sparsity dynamics on the RULER benchmark (8K-256K). We adopt Llama-3.18B-Instruct as the backbone model to compare our Elastic Attention variants (FA-XA and XA-SSA) with including MoBA and NSA. (a) reports the RULER performance scores; (b) and (c) illustrate the trade-off between inference speedup and (ΩESR), highlighting the superior Pareto frontier established by our method. 3.28 speedup, verifying the minimal overhead of our router. Meanwhile, Elastic Attention (FA-XA) strikes balanced trade-off, securing 1.51 acceleration while retaining essential information. In summary, Elastic Attention establishes superior Pareto frontier. Elastic Attention (FA-XA) prioritizes information retention (ΩESR 0.65) to effectively mitigate context collapse, while Elastic Attention (XA-SSA) maximizes throughput through adaptive extreme sparsity. Both configurations consistently outperform comparison methods in their respective regimes of accuracy and efficiency. H.4. Loss Curve and Monitoring metrics To validate the training stability and the dynamic routing capabilities of our proposed Elastic Attention, we visualize the detailed training dynamics in Figure 13. This visualization decomposes the optimization process into four key perspectives: the primary language modeling loss, the sparsity regularization loss, the evolution of the routed sparsity metric (ΩMSR), and the adaptive coefficients (λ). Optimization Stability. As illustrated in Figure 13a and Figure 13b, the joint optimization of the language modeling objective and the Attention Router parameters is stable. The LM loss decreases rapidly and stabilizes around 2.1, confirming that the integration of the lightweight Attention Router and the injection of sparsity do not hinder the backbone models convergence. Concurrently, the sparsity regularization loss drops significantly within the first 100 steps (from 0.16 to 0.06), indicating that the continuous relaxation scheme (Gumbel-Softmax) effectively guides the router to satisfy the prescribed sparsity constraints. Differentiation in Elastic Attention Allocation (ΩMSR). Figure 13c provides empirical evidence for the motivation described in Section 1: downstream tasks naturally exhibit different sensitivity to attention sparsity. Task-Dependent Routing: Starting from neutral initialization, the Attention Router automatically learns to differentiate between tasks. Consistent with our hypothesis, sparsity-sensitive tasks (such as Code and In-Context learning) converge to higher ΩMSR values (approx. 0.800.85), indicating higher allocation of Full Attention (FA) computation to preserve performance. Sparsity-Robust Efficiency: Conversely, sparsity-robust tasks like Q&A task plateau at lower values, closer to the target threshold (dashed line, representing tsen). This confirms that Elastic Attention successfully identifies tasks that can tolerate higher sparsity levels, thereby improving inference throughput without unnecessary computation. Adaptive Coefficients (λ). Finally, Figure 13d plots the evolution of the Lagrangian multipliers (λ), which dynamically scale the penalty for violating sparsity targets. We observe that λ5 (In-Context) increases most aggressively, suggesting that the model prioritizes fulfilling the density requirements for this sensitive task. This adaptive mechanism ensures that 21 Test-time Adaptive Sparsity Ratios for Efficient Transformers (a) Language Modeling Loss during the training process (b) Sparsity Regularization Loss during the training process (c) ΩMSR during the training process (d) Adaptive Coefficients (λ) during the training process Figure 13. Decomposition of Training Objectives for Elastic Attention. We visualize the training dynamics of the Attention Router, separating the total loss into (a) the primary language modeling objective and (b) the sparsity regularization term. Subfigures (c) and (d) illustrate the task-level differentiation in sparsity allocation (ΩMSR) and adaptive coefficients (λ), demonstrating how the model automatically distinguishes between sparsity-robust and sparsity-sensitive tasks. the trade-off between computational cost and model quality is balanced automatically, removing the need for the manual task-specific tuning. H.5. Impact of Input Truncation on Task Identification To optimize the trade-off between routing efficiency and accuracy, we investigate the sensitivity of the Attention Router to the input sequence length. Specifically, we analyze how varying the truncation budget influences the routers ability to distinguish between task types and allocate appropriate sparsity patterns. Figure 14 illustrates the performance and sparsity trends across six downstream tasks as the truncation budget varies from 50 tokens (pooling boundaries) to the full sequence. The Boundary-Pooling Hypothesis. Our default strategy employs boundary-pooling mechanism that aggregates only the first and last 100 tokens. This design is predicated on the observation that task-specific instructions (System Prompts) typically reside at the beginning of the context, while specific user queries appear at the end. The intermediate content often consists of long context (e.g., documents to be summarized) which, while necessary for the generation phase, acts as noise for the routing phase. Analysis of Signal-to-Noise Ratio. As shown in Figure 14, we observe that performance generally saturates around truncation length of 100-200 tokens. Notably, incorporating the full sequence does not lead to performance improvements and, in some cases (e.g., Multi-Document QA), results in suboptimal routing decisions. We attribute this to dilution of the task identification signal: as the router processes more tokens from the document body, the distinct semantic signatures of 22 Test-time Adaptive Sparsity Ratios for Efficient Transformers Figure 14. Impact of router input truncation length on downstream performance and ΩMSR. We compare varying truncation budgets (L {50, . . . , 800, All}) applied to the concatenation of the sequences prefix and suffix. Results indicate that increasing the input length beyond 100 tokens yields negligible performance gains and may degrade router selectivity due to lower signal-to-noise ratio. the instructions become obscured by the high variance of the content. Consequently, the router struggles to classify the task type accurately, leading to convergence in sparsity patterns (as seen in the flattened sparsity lines for longer lengths) without corresponding gain in generation quality. These findings validate our selection of 100-token boundary window as robust configuration that captures essential task descriptors while filtering out content-induced noise. I. Error Analysis In Table 15, 16, and 17, we present representative model outputs comparing our method with other baselines. Due to the extensive length of the contexts, only partial input context is shown. We observe that the primary source of performance improvement stems from our methods ability to accurately identify and respond to the key contextual segments relevant to the query. 23 Test-time Adaptive Sparsity Ratios for Efficient Transformers Case 1: Policy Decision Making (Long-Context) Context: The shaded areas of the map indicate ESCAP members and associate members.* The Economic and Social Commission for Asia and the Pacific (ESCAP) is the most inclusive intergovernmental platform in the Asia-Pacific region. The Commission promotes cooperation among its 53 member States and 9 associate members in pursuit of solutions to sustainable development challenges. ESCAP is one of the five regional commissions of the United Nations. The ESCAP secretariat supports inclusive, resilient, and sustainable development in the region by generating action-oriented knowledge, and by providing technical assistance and capacitybuilding services in support of national development objectives, regional agreements, and the implementation of the 2030 Agenda for Sustainable Development. *The designations employed and the presentation of material on this map do not imply the expression of any opinion whatsoever on the part of the Secretariat of the United Nations con... Question: Which policy mix should the government pursue to best balance fiscal sustainability, private sector engagement, and the energy transition, while maintaining political and social stability? Correct Prediction (Ours / Ground Truth): Gradual Energy Transition with National Green Investment Bank: Create national green investment bank... Maintain existing fossil fuel subsidies for the next five years to ensure energy price stability while gradually scaling up renewable energy... while postponing the introduction of carbon tax. Incorrect Baselines: Qwen3-8B, PruLong, DuoAttn, InfLLM-V2: Aggressive Fiscal Realignment...: Introduce substantial carbon tax on oil production... Gradually phase out fossil fuel subsidies over the next five years... (Error: Premature carbon tax and subsidy removal contradicts stability goals) MoBA: Immediate Fossil Fuel Subsidy Removal...: Remove all fossil fuel subsidies immediately... Introduce carbon pricing mechanism within two years... (Error: Immediate removal ignores social stability constraint) NSA: Blended Finance and Export-Led...: Establish public-private blended finance fund... Implement modest carbon tax... (Error: Incorrect focus on export markets and carbon tax timing) Figure 15. Qualitative comparison on complex policy reasoning task. Our model correctly identifies the Gradual approach required for stability, whereas baselines hallucinate Aggressive or Immediate measures that contradict the stability constraint. 24 Test-time Adaptive Sparsity Ratios for Efficient Transformers Case 2: Legal Reasoning & Document Understanding Context: Current to June 20, 2024 Last amended on June 20, 2024 `A jour au 20 juin 2024 Derni`ere modification le 20 juin 2024 Published by the Minister of Justice at the following address: http://laws-lois.justice.gc.ca Publie par le ministre de la Justice `a ladresse suivante : http://lois-laws.justice.gc.ca CANADA CONSOLIDATION Financial Administration Act CODIFICATION Loi sur la gestion des finances publiques R.S.C., 1985, c. F-11 L.R.C. (1985), ch. F-11 Current to June 20, 2024 Last amended on June 20, 2024 `A jour au 20 juin 2024 Derni`ere modification le 20 juin 2024 OFFICIAL STATUS OF CONSOLIDATIONS CARACT `ERE OFFICIEL DES CODIFICATIONS Subsections 31(1) and (2) of the Legislation Revision and Consolidation Act, in force on June 1, 2009, provide as follows: Les paragraphes 31(1) et (2) de la Loi sur la revision et la codification des textes legislatifs, en vigueur le 1er juin 2009, prevoient ce qui suit : Published consolidation is evidence Codifications comme element de preuve 31 (1) Ever... Question: The Special Economic Measures Act (SEMA) and the Financial Administration Act (FAA) have distinct roles... how might their application differ in hypothetical scenario where Canadian citizens business assets are seized...? Which interpretation best reflects the nuanced interplay? Correct Prediction (Ours / Ground Truth): SEMA allows the government to freeze the assets as part of broader sanctions framework without immediate concern for forfeiture, whereas the FAA would require legal process to determine whether the assets should be permanently seized and reallocated for public use. Incorrect Baselines: Qwen3-8B, PruLong, DuoAttn, MoBA, NSA: SEMA allows for freezing... while the FAA would require an assessment of the legal grounds for the potential forfeiture... ensuring compliance with Canadian law... (Error: Generic response. Fails to identify the specific outcome of public use reallocation.) InfLLM-V2: The FAA would necessitate legal review to determine whether the assets should be seized for governmental use, while SEMA would enable freezing... delaying any immediate decision... (Error: Incorrectly focuses on procedural delays rather than the substantive distinction in asset destination.) Figure 16. Comparison on bilingual legal document. Our model accurately extracts the specific legal provision regarding asset reallocation for public use (FAA), whereas baselines provide generic descriptions of legal assessments or compliance without specific details. 25 Test-time Adaptive Sparsity Ratios for Efficient Transformers Case 3: Long-Context Narrative Understanding (The Count of Monte Cristo) Context: Chapter 1: Marseilles The Arrival. On the 24th of February, 1810, the look-out at Notre-Dame de la Garde signalled the three-master, the Pharaon from Smyrna, Trieste, and Naples. As usual, pilot put off immediately, and rounding the Chateau dIf, got on board the vessel between Cape Morgion and Rion island. Immediately, and according to custom, the ramparts of Fort Saint-Jean were covered with spectators; it is always an event at Marseilles for ship to come into port, especially when this ship, like the Pharaon, has been built, rigged, and laden at the old Phocee docks, and belongs to an owner of the city. The ship drew on and had safely passed the strait, which some volcanic shock has made between the Calasareigne and Jaros islands; had doubled Pomegue, and approached the harbor under topsails, jib, and spanker, but so slowly and sedately that the idlers, with that instinct which is the forerunner of evil, asked one another what misfortune could have happene... Question: Who were the ones who framed Edmond Dantes by writing and sending letter filled with lies about him to the public prosecutor? Correct Prediction (Ours / Ground Truth): Danglars, Fernand and Caderousse. Incorrect Baselines: Llama3.1-8B-Instruct, DuoAttn, NSA: Fernand, Villefort and Danglars (Error: Entity Confusion - Mistook the prosecutor Villefort for conspirator) InfLLM-V2: Fernand and Caderousse (Error: Incomplete - Failed to retrieve the mastermind Danglars) PruLong: Caderousse, Villefort and Fernand (Error: Entity Confusion - Incorrectly included Villefort) Figure 17. Qualitative comparison on narrative entity tracking. The task requires identifying the specific characters who conspired to frame the protagonist. Our model accurately retrieves the correct trio, whereas baselines consistently hallucinate Villefort (the public prosecutor) into the group, failing to distinguish between the plotters and the judicial figure involved later."
        }
    ],
    "affiliations": [
        "1",
        "2",
        "3"
    ]
}