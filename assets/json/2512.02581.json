{
    "paper_title": "GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies",
    "authors": [
        "Chubin Zhang",
        "Zhenglin Wan",
        "Feng Chen",
        "Xingrui Yu",
        "Ivor Tsang",
        "Bo An"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 1 8 5 2 0 . 2 1 5 2 : r GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Chubin Zhang1*, Zhenglin Wan2*, Feng Chen2, Xingrui Yu3, Ivor Tsang3, Bo An2 1Beijing University of Posts and Telecommunications, China 2Nanyang Technological University, Singapore 3Centre for Frontier AI Research, A*STAR, Singapore Abstract: Reinforcement learning (RL) faces persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), framework that optimizes tractable latent policy while utilizing conditional generative decoder to synthesize actions. two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches normalized return above 870, more than 3 that of the strongest baseline. These results demonstrate that separating optimization from generation provides practical path to policies that are both stable and highly expressive. Date: Dec 1, 2025 Correspondence: Xingrui Yu (yu_xingrui@a-star.edu.sg) Code: https://github.com/bennidict23/GoRL 1. Introduction Reinforcement learning (RL) has driven remarkable progress in robotics (Kober et al., 2013, Wan et al., 2025b), games (Mnih et al., 2015, Silver et al., 2016), and continuous control (Lillicrap et al., 2016, Haarnoja et al., 2018, Yu et al., 2024) by optimizing policies through continual interaction with the environment. Among the various paradigms, policy-gradient methods are dominant choice for continuous control, largely because they remain stable and differentiable under tractable policy parameterizations such as Gaussian or Beta distributions (Williams, 1992, Schulman et al., 2015, 2017). These simple parametric forms yield analytical likelihoods and smooth gradients, enabling reliable optimization across diverse tasks. However, their unimodal structure imposes an inherent limitation: such policies often struggle to represent the complex or multimodal action patterns required in challenging environments. In practice, forcing unimodal Gaussian policy to fit multimodal action distribution typically produces mode-covering effect, where the learned density spreads probability mass into low-density regions between modes (Wang et al., 2023a) (Figure 1). This behavior becomes especially harmful when high-reward actions concentrate around well-separated modes. Therefore, while likelihood-based parametric policies offer stability and theoretical clarity, they can fall short in expressiveness, exposing persistent tension between stable optimization and rich action modeling. *Co-first author & Equal Contribution. GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies To address this expressiveness bottleneck, recent work has turned to generative modeling. Diffusion models (Ho et al., 2020, Dhariwal and Nichol, 2021) and flow matching (FM) (Lipman et al., 2023) can represent rich multimodal action distributions by casting policy learning as conditional generation from noise to actions (Wan et al., 2025a). Building on this progress, several approaches have achieved strong results in behavior cloning and offline RL, where training relies on fixed datasets with stationary stateaction distributions (Chen et al., 2021, Emmons et al., 2022). For instance, Diffusion Policy attains impressive visuomotor performance by directly generating action trajectories conditioned on observations (Chi et al., 2023), and Diffusion-QL improves offline RL by coupling diffusion-based action generation with value learning (Wang et al., 2023b). However, these successes depend on static expert or offline data and do not readily extend to settings without demonstrations, where learning must proceed purely from reward feedback. In online RL, the stateaction distribution shifts as the policy improves, violating the fixed-data assumption under which diffusion and FM models are typically trained. Consequently, bringing expressive generative policies into such dynamic settings introduces critical challenge: preserving expressiveness while maintaining the stability required for online optimization. Figure 1: Illustration of the mode-covering problem: when the optimal policy (blue) is multimodal, unimodal policy (red) assigns high probability to the low-reward region between modes, leading to suboptimal behavior. Empirically, generative policies that excel in offline settings often become fragile when deployed in online RL, where policies must adapt continuously through environmental interaction (Henderson et al., 2018, Ilyas et al., 2020, Kang et al., 2023). Their likelihoods are typically intractable, and gradient estimation requires backpropagating through deep generative sampling chains. This makes optimization noisy and highly sensitive to distribution shift; in practice, the tight coupling between sampling and optimization can lead to unstable learning dynamics, degraded performance, or even collapse (Wang et al., 2023a, Kang et al., 2023). These observations point to deeper issue: the expressiveness that empowers generative models offline does not readily translate into stable online learning, revealing persistent tension between stability and expressiveness in online RL. Theoretically, we analyze the root causes of these instabilities in Appendix A. To alleviate this issue, Flow Policy Optimization (FPO) (McAllister et al., 2025) applies PPO-style updates by replacing the intractable likelihood ratio with lower-bound surrogate derived from the flow-matching loss. However, this surrogate can deviate from the true ratio and inherits the high variance of the flow-matching objective, causing FPO to collapse in later stages of training on long-horizon tasks (see Section 4). This limitation highlights the broader challenge of reliably optimizing expressive generative policies in online RL, raising central question: Can we design an online reinforcement learning framework for generative policies that maintains training stability while preserving expressiveness? We address this challenge by proposing GoRL (Generative Online Reinforcement Learning), general framework that disentangles policy modeling from policy learning. The key idea is to separate the component that must remain stable during optimization (a tractable latent prior) from the component that enables expressive action generation (a conditional decoder). Directly optimizing high-capacity generative policies in action space often leads to unstable gradients and feedback loops; by contrast, performing optimization in tractable latent space retains stability while preserving the decoders expressiveness. Concretely, GoRL decomposes the policy into learned prior and generative transport map: latent policy ğœ‹ğœƒ(ğœ€ ğ‘ ) learns state-conditioned prior over latent variables, while conditional decoder ğ‘”ğœ‘(ğ‘ , ğœ€) (parameterized, GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies for instance, by diffusion or flow matching (Ho et al., 2020, Lipman et al., 2023)) transports this prior to an expressive action distribution. Optimization in GoRL follows two-timescale update schedule: we alternate between improving the latent prior via standard policy gradients with the decoder fixed, and refining the transport map via supervised generative training with the latent policy frozen. Crucially, the decoder update requires careful design to drive genuine improvement. If we were to train the decoder using the evolving latent prior as input, it would mainly reinforce its current mapping without acquiring additional expressive capacity. To avoid this stagnation loop, we train the decoder to map samples from fixed Gaussian prior to the high-reward actions discovered by the latent policy. This fixed prior provides stable external anchor, forcing the decoder to absorb the encoders exploration progress and to establish more capable transport map from well-conditioned base distribution to the improved action space. Consequently, the encoder and decoder iteratively enhance one another, creating reinforcing cycle where stability and expressiveness grow in tandem. This principle is conceptually general and algorithm-agnostic, compatible with any onor off-policy RL algorithm for the encoder and any generative architecture for the decoder. To the best of our knowledge, GoRL is the first framework for online RL that simultaneously offers stable policygradient optimization together with expressive generative action modeling, while remaining fully algorithmand model-agnostic. We summarize our main contributions as follows: We empirically and theoretically identify the core obstacles to integrating expressive generative policies into online RL. In particular, we show how likelihood intractability and tightly coupled updates destabilize training, revealing fundamental structural tension between expressiveness and stable optimization. We propose GoRL, generic and algorithm-agnostic framework that disentangles optimization from generation. By performing all policy-gradient updates in tractable latent space and delegating multimodal action synthesis to separate generative decoder anchored by fixed prior, GoRL resolves the stability expressiveness tension. We demonstrate that GoRL consistently outperforms both Gaussian baselines and modern generative methods across range of continuous-control tasks. Notably, on HopperStand, it achieves normalized return above 870, more than 3 that of the strongest baseline. These results position GoRL as simple yet general framework for integrating expressive generative models into stable online reinforcement learning. 2. Background and Preliminaries 2.1. Likelihood-Based Policy Optimization Modern policy-gradient methods typically rely on policies with tractable probability densities, since stable optimization requires evaluating log-likelihoods or likelihood ratios. representative example is Proximal Policy Optimization (PPO) (Schulman et al., 2017), which maximizes the clipped surrogate objective [ â„’PPO(ğœƒ) = ( min ğ‘Ÿğœƒ(ğ‘ğ‘ )ğ´(ğ‘ , ğ‘), clip(ğ‘Ÿğœƒ(ğ‘ğ‘ ), 1 ğ›¿, 1 + ğ›¿)ğ´(ğ‘ , ğ‘) )] , where ğ‘Ÿğœƒ(ğ‘ğ‘ ) = ğœ‹ğœƒ(ğ‘ğ‘ )/ğœ‹ğœƒold(ğ‘ğ‘ ) is the likelihood ratio. Computing this ratio efficientlyas well as the logdensities required by entropy-regularized methodsrequires closed-form distributions that are cheap to evaluate. Consequently, standard implementations typically adopt simple unimodal families such as diagonal Gaussians. While these parameterizations guarantee smooth gradients and reliable updates, they impose clear expressivity bottleneck: unimodal policies cannot represent the multimodal action distributions that often arise in challenging 3 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies control tasks (Shafiullah et al., 2022). For completeness, general RL background (MDPs, policy gradients, and notation) is provided in Appendix B. 2.2. Generative Models as Expressive Policies To overcome the unimodal limitation, recent work reformulates policies as conditional generative models: ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€), ğœ€ ğ’© (0, ğ¼), where ğ‘”ğœ‘ is high-capacity generator that transforms simple Gaussian noise ğœ€ into structured actions conditioned on the state. This view replaces explicit density modeling with conditional generation, enabling rich and potentially multimodal action distributions that are difficult for Gaussian actors to capture. Two prominent instantiations are: Diffusion policies. Diffusion models generate actions by iteratively denoising Gaussian noise through reversetime process (Ho et al., 2020). In policy form, ğ‘”ğœ‘ is multi-step denoising chain that maps ğœ€ to an action, and has shown strong performance in behavior cloning and offline RL (Chi et al., 2023, Wang et al., 2023b). Flow-Matching policies. Flow Matching offers deterministic alternative by learning velocity field that transports base distribution into the action distribution via an ODE (Lipman et al., 2023): ğ‘‘ğ‘ğ‘¡ ğ‘‘ğ‘¡ = ğ‘£ğœ‘(ğ‘ğ‘¡, ğ‘ , ğ‘¡), ğ‘¡ [0, 1], with actions obtained by integrating this flow from ğ‘¡ = 0 to ğ‘¡ = 1. FM typically yields faster sampling and smooth supervised gradients, and has been adapted to online settings as an expressive policy class (McAllister et al., 2025, Wan et al., 2025a). Detailed architectural and training settings for both model classes are given in Appendix C. 2.3. Why Generative Policies Are Hard to Optimize Online Deploying diffusionor FM-based policies in online RL introduces structural conflict between the requirements of stable policy optimization and the mechanics of deep generative modeling. Standard policy-gradient algorithms assume that (i) log-probabilities are tractable and (ii) gradients can be propagated through the policy with low variance. Generative policies violate both assumptions. Likelihoods are intractable or prohibitively expensive. Algorithms such as PPO rely on frequent evaluation of likelihood ratios, while maximum-entropy methods require log-densities for regularization. For diffusion and FM policies, however, log ğœ‹ğœ‘(ğ‘ğ‘ ) is not directly available in closed form. In diffusion, recovering the action likelihood involves accounting for the full reverse-time chain, which is expensive and numerically delicate (Wang et al., 2023b). In flow-based models, exact likelihoods require integrating divergence (Jacobian-trace) terms along the ODE trajectory (Chen et al., 2018, Grathwohl et al., 2019). As result, standard likelihood-based updates become challenging without biased surrogates or approximations. Deep sampling chains make gradients noisy. Even when one bypasses likelihoods and treats ğ‘”ğœ‘ as reparameterized sampler, policy optimization must backpropagate value gradients through the generation process. For diffusion and FM, this process is deep computation graph (a long denoising chain or an ODE solver), which GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Figure 2: Overview of the GoRL framework. (a) Latent optimization: The decoder ğ‘”ğœ‘ is frozen while the encoder ğœ‹ğœƒ is optimized in the latent space using standard policy gradients, with KL penalty toward ğ’© (0, ğ¼). (b) Decoder refinement: The encoder is frozen and the decoder ğ‘”ğœ‘ is updated via supervised learning on recent rollouts, mapping the fixed Gaussian prior over ğœ€ to actions using an expressive generative loss (e.g., flow matching). amplifies noise from the critic and from online distribution shift. Empirically, prior work reports that these deep gradient pathways lead to high-variance updates and unstable training dynamics in online RL (Wang et al., 2023b, Ma et al., 2025, McAllister et al., 2025). In summary, the difficulty is not that generative policies lack expressive power, but that their expressiveness comes at the expense of the analytical tractability and gradient stability that classical online policy gradients rely on. In other words, diffusion and flow-based actors trade tractable likelihoods for deep sampling processes, and this trade-off clashes with stable likelihood-ratio optimization in online RL. We formalize this intuition in Appendix by analyzing how diffusion and FM policies violate the classical routes to policy gradients. These observations motivate the disentangled design principle introduced next in Section 3. 3. Methodology We propose GoRL (Generative Online Reinforcement Learning), general framework that explicitly disentangles optimization from generation. The core principle is to confine policy search to tractable latent space, while delegating the complexity of action modeling to conditional generative decoder. This structural separation lets stable policy-gradient updates coexist with highly expressive, likelihood-free generative policies. Figure 2 provides an overview of the framework. 3.1. LatentGenerative Factorization We formalize the disentangling principle through latentgenerative factorization of the policy: ğœ‹(ğ‘ğ‘ ) = ğœ‹ğœ‘(ğ‘ğ‘ , ğœ€) ğœ‹ğœƒ(ğœ€ğ‘ ) ğ‘‘ğœ€. (1) 5 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Here, the encoder ğœ‹ğœƒ(ğœ€ ğ‘ ) governs the agents decision intent via tractable latent distribution responsible for optimization and exploration. The decoder ğœ‹ğœ‘(ğ‘ ğ‘ , ğœ€) is the conditional action distribution induced by generator ğ‘”ğœ‘ (possibly implicit), which maps the state ğ‘  and latent variables ğœ€ to concrete actions in the environment. Crucially, because policy gradients are computed only with respect to ğœ‹ğœƒ, optimization remains stable and tractable even when ğœ‹ğœ‘ has no explicit likelihood. Conversely, the decoder can adopt any high-capacity architecturesuch as diffusion or flow matchingwithout compromising gradient tractability. Priortransport view. Eq. (1) can be interpreted through priortransport lens: the policy is decomposed into (i) state-conditioned prior over latent variables and (ii) conditional transport map. Specifically, the encoder ğœ‹ğœƒ(ğœ€ ğ‘ ) specifies this learned prior in the latent space. The decoder, instantiated as generator ğ‘”ğœ‘(ğ‘ , ğœ€), defines transport map ğœ€ ğ‘ that pushes the prior forward to the action distribution. From this perspective, policy learning optimizes the latent prior ğœ‹ğœƒ, whereas decoder learning refines the transport map ğ‘”ğœ‘ to realize high-reward actions from the latent intent. 3.2. Alternating Optimization Training proceeds in two-timescale process that balances stability and expressiveness. We employ an alternating schedule where the encoder and decoder are updated in distinct, mutually reinforcing phases. Initially, the decoder is instantiated as an identity-like mapping, ensuring that the composite policy behaves similarly to standard Gaussian actor. This warm-up guarantees smooth gradients and stable early exploration. Encoder update (policy optimization). In this phase, the decoder ğ‘”ğœ‘ is frozen and the encoder ğœ‹ğœƒ(ğœ€ğ‘ ) is updated to maximize return under the current mapping ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€). Iterative re-initialization. Because the decoder ğ‘”ğœ‘, which maps latent variables to actions, evolves between stages, the optimal latent parameters from the previous stage quickly become obsolete. Therefore, at the beginning of each encoder phase, we re-initialize ğœ‹ğœƒ to the prior ğ’© (0, ğ¼). Since the updated decoder is trained to map this prior to improved actions, this reset acts as behavioral warm start, immediately aligning the policy with the new transport map before further optimization. With this initialization, the encoder improves the latent prior using standard policy gradients: ğœƒğ½ = ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğœ€ğœ‹ğœƒ(ğ‘ ) [ ğœƒ log ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ´(ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€)) ] . (2) Latent regularization. Although latent-space gradients are well-defined, the encoder distribution may drift significantly during optimization. If ğœ‹ğœƒ deviates too far from the Gaussian regime on which the decoder was trained, the decoder receives out-of-distribution (OOD) inputs, leading to unpredictable actions. To mitigate this, we regularize the encoder toward the standard Gaussian prior: ( â„’reg = ğ›½ KL ğœ‹ğœƒ(ğœ€ğ‘ ) ğ’© (0, ğ¼) ) . (3) This regularizer keeps latent intents within the decoders reliable support, enabling stable long-horizon updates. Decoder update (generative refinement). After the encoder phase, we freeze ğœ‹ğœƒ and refine the decoder. key design choice is the source distribution. If we were to train the decoder using samples from the evolving latent prior, it would simply learn to reconstruct the current behavior, leading to stagnation loop. To break this loop, we feed the decoder fresh samples from the fixed Gaussian prior ğœ€ ğ’© (0, ğ¼). This forces the decoder to absorb the encoders exploration progress by learning new transport from the stable prior to the high-reward actions 6 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Algorithm 1 GoRL: Two-Timescale Alternating Optimization 1: Input: Total stages ğ‘€ , interaction budgets {ğ‘1, . . . , ğ‘ğ‘€ }, decoder epochs ğ¾dec. 2: Initialize decoder ğ‘”ğœ‘ (e.g., identity-like) and encoder ğœ‹ğœƒ. 3: Initialize experience buffer ğ’Ÿ . 4: for stage ğ‘š = 1, . . . , ğ‘€ do 5: Phase 1: Latent optimization Freeze ğœ‘. Re-initialize encoder ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ’© (0, ğ¼). ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  0 while ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  < ğ‘ğ‘š do Collect rollouts ğœ with ğœ€ ğœ‹ğœƒ( ğ‘ ), ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€). Store ğœ in ğ’Ÿ and ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  ğ‘ ğ‘¡ğ‘’ğ‘ğ‘  + ğœ . Update ğœƒ using Eq. (2) with â„’reg on ğ’Ÿ (e.g., PPO on recent rollouts or SAC on replay buffer). end while Phase 2: Decoder refinement Freeze ğœƒ. for ğ‘˜ = 1, . . . , ğ¾dec do Sample mini-batch from ğ’Ÿ (e.g., recent high-reward data). Sample latents from the fixed prior ğœ€ ğ’© (0, ğ¼) matching the batch size. Update ğœ‘ by minimizing Eq. (4) on the batch. end for 20: 21: end for collected in the recent rollout dataset ğ’Ÿrollout. The decoder is updated by minimizing generative objective (e.g., flow matching): min ğœ‘ E(ğ‘ ,ğ‘) ğ’Ÿrollout, ğœ€ğ’© (0,ğ¼) [ â„’gen(ğ‘”ğœ‘(ğ‘ , ğœ€), ğ‘) ] . (4) This update consolidates the high-reward behaviors discovered by the encoder into the generative mapping, preparing the system for the next round of latent optimization. Algorithm 1 summarizes this procedure. Empirically, this alternating scheme yields smooth evolution of policy capacity: the agent begins with Gaussianlike behavior and progressively acquires multimodal, high-return action patterns as the decoder becomes more expressive (as illustrated in Figure 7 in Section 4). 3.3. Instantiation GoRL is framework rather than fixed algorithm. It requires only that the encoder ğœ‹ğœƒ(ğœ€ ğ‘ ) be tractable latent policy so that standard policy-gradient updates can be computed in latent space, and that the decoder ğ‘”ğœ‘(ğ‘ , ğœ€) be trainable with an expressive likelihood-free generative objective. With optimization and generation separated, GoRL can combine any onor off-policy latent-policy optimizer (e.g., PPO (Schulman et al., 2017), SAC (Haarnoja et al., 2018)) with any conditional generative decoder (e.g., diffusion, flow matching (Lipman et al., 2023)) without changing the alternating scheme. For concreteness, we describe the instantiation used in our experiments: the encoder is optimized with PPO and the decoder is trained with conditional flow matching. With ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€), PPO is applied to the latent policy via the clipped surrogate [ â„’PPO(ğœƒ) = E(ğ‘ ,ğœ€)ğœ‹ğœƒold ( min ğ‘Ÿğœƒ(ğœ€ğ‘ ) ğ´(ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€)), clip(ğ‘Ÿğœƒ(ğœ€ğ‘ ), 1 ğ›¿, 1 + ğ›¿) ğ´(ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€)) )] , (5) 7 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies where ğ‘Ÿğœƒ(ğœ€ğ‘ ) = ğœ‹ğœƒ(ğœ€ğ‘ )/ğœ‹ğœƒold(ğœ€ğ‘ ) is the latent-space likelihood ratio. The decoder is refined on recent rollouts using the conditional flow-matching loss (Lipman et al., 2023) â„’FM(ğœ‘) = E(ğ‘ ,ğ‘)ğ’Ÿrollout, ğœ€ğ’© (0,ğ¼), ğœ ğ’° [0,1] [ ğ‘£ğœ‘((1 ğœ )ğœ€ + ğœ ğ‘, ğœ ; ğ‘ ) (ğ‘ ğœ€)2 2 ] , (6) where ğ‘£ğœ‘ denotes conditional vector field used in flow matching and ğœ is timestep sampled uniformly from [0, 1]. Minimizing this objective learns transport map from the fixed Gaussian prior over ğœ€ to the on-policy high-reward action distribution. This combination is not essential: other latent-policy optimizers or generative decoders can be substituted directly. GoRL thus provides plug-and-play way to couple stable latent-space optimization with highly expressive action generation. 3.4. Latent-Space Optimization and Performance Guarantees The latentgenerative factorization in Eq. (1) admits clean analytical view when the decoder is fixed. In this case, we show that (i) policy gradients computed in latent space are unbiased estimators of the true policy gradient of the induced action policy, and (ii) small updates in latent space, as measured by divergence between successive latent policies, yield bounded changes in the return of the induced policy. We formalize these properties below. Lemma 3.1. Unbiased latent policy gradient. Fix ğœ‘, and let ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) be the action distribution induced by sampling ğœ€ ğœ‹ğœƒ( ğ‘ ) and executing ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€). Then [ ] ğœƒğ½ (ğœƒ, ğœ‘) = ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğœ€ğœ‹ğœƒ(ğ‘ ) ğœƒ log ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ´ğœ‹ğœƒ,ğœ‘(ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€)) , which matches Eq. (2). Thus, latent-space gradients are unbiased estimators of the true policy gradient of ğœ‹ğœƒ,ğœ‘. Implication. Lemma 3.1 shows that, with frozen decoder, GoRL preserves standard policy-gradient semantics while requiring only the tractable latent likelihood log ğœ‹ğœƒ(ğœ€ ğ‘ ). Proof. Let ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) be the pushforward of ğœ‹ğœƒ(ğœ€ ğ‘ ) under ğ‘”ğœ‘. By the policygradient theorem, ğœƒğ½ (ğœƒ, ğœ‘) = ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğ‘ğœ‹ğœƒ,ğœ‘(ğ‘ ) [ ğœƒ log ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) ğ´ğœ‹ğœƒ,ğœ‘(ğ‘ , ğ‘) ] . Since ğ‘”ğœ‘ does not depend on ğœƒ, the dependence of ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) on ğœƒ arises only through the latent distribution ğœ‹ğœƒ(ğœ€ ğ‘ ). Rewriting the expectation over actions as an expectation over latents via ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€) therefore gives ğœƒğ½ (ğœƒ, ğœ‘) = ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğœ€ğœ‹ğœƒ(ğ‘ ) [ ( ğœƒ log ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ´ğœ‹ğœƒ,ğœ‘ ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€) )] , which is exactly the claimed expression. Lemma 3.2. Performance under small latent divergence. Let ğœ‹ğœƒ(ğœ€ ğ‘ ) and ğœ‹ğœƒ(ğœ€ ğ‘ ) satisfy [ )] ( Eğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ ğ· ğœ‹ğœƒ( ğ‘ ) ğœ‹ğœƒ( ğ‘ ) ğ›¿, where ğ· upper-bounds total variation and ğ‘‘ğœ‹ğœƒ,ğœ‘ is the discounted state-visitation distribution under the induced policy. Under the fixed decoder ğœ‘, the returns satisfy ğ½ (ğœƒ, ğœ‘) ğ½ (ğœƒ, ğœ‘) 1 1 ğ›¾ ğ‘ ,ğœ€ğœ‹ğœƒ (ğ‘ ) [ ğ´ğœ‹ğœƒ,ğœ‘(ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€)) ] ğ¶ ğ´max ğ›¿. Thus, if latent updates are small in divergence, any performance loss is bounded by ğ‘‚(ğ›¿). 8 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Sketch. This follows from the performancedifference lemma applied to the pushed-forward action policies (Kakade and Langford, 2002), using that latent divergence controls the induced action divergence when ğœ‘ is fixed. See Appendix for details. Together, Lemma 3.1 and Lemma 3.2 explain why GoRL can optimize only the encoder during online interaction: latent updates remain unbiased and inherit trust-region style stability, while the decoder can be refined separately to increase expressiveness. 4. Experiments We empirically evaluate GoRL on six continuous-control tasks from the DMControl Suite (Tassa et al., 2018) in the standard on-policy, from-scratch training setting. We structure our analysis around three core questions: (i) Can GoRL improve online RL performance and stability compared to Gaussian and existing generative-policy baselines? (ii) How important are the key design components, particularly latent regularization and the alternating optimization schedule? (iii) Does GoRL learn more expressive and potentially multimodal action distributions than conventional policies? Unless otherwise specified, results are averaged over five random seeds; we report mean returns with shaded regions indicating one standard deviation. 4.1. Settings 4.1.1. Environments We conduct experiments on six standard DMControl benchmarks: CheetahRun, FingerSpin, FingerTurnHard, FishSwim, HopperStand, and WalkerWalk. These tasks span wide range of dynamics, from smooth locomotion (WalkerWalk, CheetahRun) to highly nonlinear contact dynamics (HopperStand, FishSwim), where multimodal control strategies can be particularly beneficial. All methods are trained purely from on-policy interaction for total of 180M environment steps per task. Figure 3 provides visual overview of these domains. Figure 3: Visual overview of the DMControl tasks. The benchmark covers diverse control challenges: high-speed locomotion (CheetahRun), bipedal gait control (WalkerWalk), object manipulation (FingerSpin, FingerTurnHard), and fine-grained stabilization with complex contacts (HopperStand, FishSwim). 4.1.2. Baselines We compare GoRL against representative unimodal and generative baselines: Gaussian PPO: standard diagonal Gaussian policy optimized with Proximal Policy Optimization (PPO) (Schulman et al., 2017). 9 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies FPO (McAllister et al., 2025): flow-based policy gradient method that employs flow-matching surrogate objective. DPPO (Ren et al., 2024): diffusion-based PPO method that models the denoising process as two-layer MDP with explicit Gaussian likelihoods at each denoising step. All methods use comparable neural architectures and identical training budgets to ensure fair comparison. Detailed hyperparameters and architectural specifications are provided in Appendix E. 4.1.3. Training Details We instantiate GoRL using PPO for the encoder and either Flow Matching (GoRL(FM)) or Diffusion (GoRL(Diff)) for the decoder. We follow the two-timescale training scheme described in Section 3: 60M-step warm-up phase (using fixed approximate identity decoder), followed by three generative refinement stages at 60M, 120M, and 150M steps. At the beginning of each refinement stage, we freeze the encoder and train the decoder on the most recent on-policy rollout buffer using Eq. (4) with latent inputs sampled from the fixed prior ğ’© (0, ğ¼). Subsequently, we fix the decoder and run new encoder phase. Consistent with our behavioral warm start strategy, the encoder is reset to the prior ğ’© (0, ğ¼) at the start of each phase and then optimized using the latent PPO objective (Eq. (5)) combined with KL regularization. Unless otherwise noted, decoder training runs for 50 epochs per stage with learning rate of 3 104. To ensure fair comparison, we use identical PPO hyperparameters for the encoder across all methods. All experiments are run on four NVIDIA RTX A5000 GPUs. For reproducibility, complete list of hyperparameters is provided in Appendix E. Figure 4: Learning curves across six DMControl tasks. Curves are smoothed using Gaussian filtering (ğœ = 100.0) for visual clarity. Shaded regions denote standard deviation across five seeds. 4.2. Main Results The learning curves across all six tasks are presented in Figure 4. Both GoRL(FM) and GoRL(Diff) consistently demonstrate faster improvement and higher final returns compared to the baselines. The performance gap is 10 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies particularly striking on HopperStand: while standard baselines plateau below 300 on average, GoRL variants continue to learn, achieving normalized returns exceeding 870. This result highlights key advantage: while unimodal policies can learn basic balancing, they fail to master the complex, high-reward control strategies unlocked by GoRLs expressive decoder. Similar dominance is observed on FishSwim and FingerSpin, where GoRL establishes clear lead early in training. critical observation from Figure 4 is the instability of FPO. We observe that FPO suffers from severe performance collapse in the mid-to-late training stages on several tasks (notably WalkerWalk, FingerSpin, and FishSwim), resulting in extremely high variance. We attribute this fragility to the flow-matching surrogate objective, which can become misaligned with the true policy gradient under online distribution shift. Furthermore, unlike Gaussian PPO, standard flow matching does not provide direct handle on policy entropy, making it difficult to maintain sufficient exploration and recover from premature collapse. In contrast, GoRL circumvents these issues by confining optimization to tractable latent space, where standard entropy regularization (inherent to PPO) ensures sustained exploration and stability throughout training. Table 1 summarizes the final returns. GoRL(Diff) and GoRL(FM) each achieve the best performance on three tasks, indicating that the framework is robust to the choice of generative decoder. Collectively, these results confirm that decoupling optimization from generation allows the agent to leverage the full expressiveness of generative models without suffering from the instability typically associated with their online training. Table 1: Final normalized returns (mean std) over five seeds. Method CheetahRun FingerSpin FingerTurnHard FishSwim HopperStand WalkerWalk PPO FPO DPPO 724.83 155.67 599.15 297.45 559.79 99.97 539.03 146.63 56.05 124.53 694.06 191.59 738.70 114.45 752.08 55.39 633.84 88.21 433.70 73.63 204.66 191.49 143.52 26.25 286.09 273.07 3.94 1.79 2.14 0.81 825.65 79.70 29.00 4.32 345.59 64. GoRL(Diff ) GoRL(FM) 902.24 2.20 883.40 19.94 844.74 59.43 903.92 104.08 884.59 26.95 860.83 14.93 608.61 22.07 641.01 13.10 874.63 38.79 908.96 30.45 733.66 223.76 919.61 60. 4.3. Ablation and Mechanism Analysis 4.3.1. Effect of Latent Regularization Figure 5 examines the role of the KL regularization coefficient ğ›½ in encoder training. We sweep ğ›½ over {0, 104, . . . , 101}. With no regularization (ğ›½ = 0), learning destabilizes early. This failure occurs because the latent policy tends to drift away from the prior distribution during optimization; consequently, the decoder receives out-of-distribution (OOD) latent inputs that lie outside its training manifold, leading to unpredictable behavior. Conversely, excessive regularization (ğ›½ 5 102) over-constrains the policy, suppressing the variance needed for exploration and limiting the expressiveness of the final behavior. moderate ğ›½ [103, 5 103] strikes the necessary balance, anchoring the latent policy within the decoders reliable support while permitting sufficient flexibility. We use ğ›½ = 103 as the default. Figure 5: Ablation of latent regularization on CheetahRun. Varying the KL coefficient ğ›½ significantly affects stability. 11 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies 4.3.2. Impact of Staged Decoder Refinement Figure 6 isolates the contribution of the decoders progressive evolution. We compare the encoders performance when paired with decoders frozen at different refinement stages: Stage 0 (Identity), Stage 1, Stage 2, and Stage 3. We observe clear monotonic improvement: the identity decoder (Stage 0) limits the agent to Gaussian-like performance, while each subsequent stage systematically raises the asymptotic return. Crucially, we observe that performance gains begin to saturate by Stage 3 (particularly evident on CheetahRun, where Stage 2 and Stage 3 curves overlap), suggesting that the alternating optimization process effectively converges to high-capacity policy within just few refinement cycles. This pattern confirms that the schedule creates virtuous cycle: the encoder generates improved data, which refines the decoder, which in turn expands the policys representational capacity for the next phase of exploration. Figure 6: Impact of staged decoder refinement. Performance improves systematically as the decoder evolves from an identity mapping (Stage 0) to generative map. Results are shown for CheetahRun (left) and HopperStand (right). On CheetahRun, gains saturate by Stage 3, indicating that the alternating schedule converges efficiently to high-capacity policy. 4.4. Qualitative Analysis: Evolution of Action Multimodality To verify that GoRL progressively captures complex distributions, we visualize the evolution of action density at fixed state in HopperStand throughout training (Figure 7). We sample 10,000 actions from the trained policy at 60M, 120M, and 180M steps, projecting them onto their first principal component to estimate the density. As shown in Figure 7, the Gaussian PPO baseline remains restricted to single unimodal peak across all stages. FPO, while growing slightly broader over time, fails to develop clear modal separation even at 180M. In contrast, GoRL(FM) exhibits distinct evolutionary pattern: at 60M, it behaves similarly to unimodal Gaussian (consistent with our identity-like initialization for stability); by 180M, it has evolved clearly bimodal structure with two separated peaks. This qualitative behavior is consistent with our design goal: the policy starts from simple, stable regime and progressively acquires multiple distinct high-return action modes through alternating encoderdecoder refinement. 5. Related Work Parametric and latent-space policies. Standard policy-gradient methods (e.g., PPO, TRPO) rely on tractable parametric families such as Gaussians to enable stable likelihood-ratio updates (Schulman et al., 2017, 2015). While robust in practice, these unimodal policies can struggle to represent the multimodal action distributions required in complex control tasks (Shafiullah et al., 2022). To increase expressiveness while retaining tractable 12 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Figure 7: Evolution of action distributions on HopperStand. Gaussian KDE plots of policy outputs at fixed state. Initially (60M), GoRL behaves similarly to the unimodal Gaussian baseline, ensuring stability. By 180M, GoRL evolves clear bimodal structure to capture distinct high-reward strategies, whereas baselines (PPO, FPO) remain restricted to unimodal or unstructured distributions. likelihoods, prior work in offline RL has proposed optimizing policies in learned latent action spaces. Methods such as PLAS (Zhou et al., 2020) and its flow-based extensions (Akimov et al., 2022) train conditional VAE or flow on static dataset and perform policy search in the latent space to stay within dataset support. More recent approaches use latent diffusion for offline trajectory modeling (Venkatraman et al., 2024) or steer pretrained diffusion models via latent-space RL (Wagenmaker et al., 2025). Unlike model-based methods that learn latent dynamics (e.g., Dreamer (Hafner et al., 2020)), these approaches focus on latent policies. However, they remain largely designed for offline or imitation scenarios. GoRL adopts the latent-policy idea in an online setting from scratch, introducing two-timescale schedule to cope with the non-stationarity induced by continual data collection. Generative policies in imitation and offline RL. Generative models have become standard tool for representing expressive policies in imitation and offline RL. Diffusion-based policies model complex action or trajectory distributions and achieve strong performance on D4RL-style benchmarks and visuomotor manipulation (Chi et al., 2023, Wang et al., 2023b, Hansen-Estruch et al., 2023, Fu et al., 2020). Flow Matching (Lipman et al., 2023) provides deterministic, simulation-free alternative for supervised policy learning. In parallel, diffusion-based decision-making libraries such as CleanDiffuser (Dong et al., 2024) standardize implementations of expressive generative policies and make them easier to apply in practice. In most of these works, the generative model is trained once to mimic static dataset or expert demonstrations, and then used as fixed sampler at deployment. Stability issues arising from online distribution shift and repeated policy improvement are therefore not directly addressed. Online reinforcement learning with generative models. More recently, several methods have sought to adapt generative policies to online RL by modifying the optimization objective to accommodate intractable likelihoods. For diffusion policies, algorithms such as DDiff PG (Li et al., 2024) and SDAC (Ma et al., 2025) backpropagate gradients through the diffusion sampling chain or use score-matching-style surrogates, while QVPO (Ding et al., 2024) derives Q-weighted variational lower bound. For flow-based policies, FPO (McAllister et al., 2025) replaces the PPO likelihood ratio with flow-matching surrogate, and ReinFlow (Zhang et al., 2025a) injects noise along the flow to recover approximate likelihoods. Recent work has further proposed specialized techniques to stabilize training: FlowRL (Lv et al., 2025) applies Wasserstein regularization to flow policies, while SAC 13 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Flow (Zhang et al., 2025b) reparameterizes the velocity network to mitigate gradient explosion in off-policy updates. Despite these innovations, these approaches generally optimize the generative model directly in action space, which requires policy gradients to traverse deep sampling computations or rely on surrogate objectives, often leading to high-variance updates and instability. In contrast, GoRL structurally decouples optimization from generation: it retains simple, tractable latent policy for stable latent-space updates (instantiated with PPO), and delegates expressiveness to decoder trained via supervised regression on recent rollouts. Summary. To our knowledge, GoRL is the first framework to combine (i) algorithm-agnostic latent-space policy gradients that require only tractable latent likelihoods, (ii) decoder-agnostic expressive generator (e.g., diffusion or flow matching) trained from fixed latent prior, and (iii) an alternating two-timescale training procedure, in fully online RL setting from scratch. Existing approaches either optimize the generative policy itself in action space with specialized objectives, or restrict latent-space optimization to offline scenarios or to steering fixed, pretrained decoders. GoRL fills this gap by showing that simple latentdecoder factorization is sufficient to recover stable policy-gradient updates while still exploiting the multimodality of modern generative models in continuous-control domains. 6. Conclusion In this paper, we presented GoRL, framework that reconciles the tension between optimization stability and policy expressiveness in online reinforcement learning. By structurally disentangling optimization from generation, GoRL confines policy-gradient updates to tractable latent space while delegating the complexity of multimodal action synthesis to separately trained generative decoder. This factorization preserves the theoretical guarantees of likelihood-based optimization (such as PPO) while allowing the overall policy class to inherit the modeling power of modern generative models. Our experiments across diverse continuous-control tasks show that this two-timescale design yields improved stability, asymptotic performance, and mode coverage compared to both unimodal Gaussian baselines and prior coupled generative methods. Limitations and Future Work. While effective, the two-timescale process introduces additional computational overhead compared to end-to-end methods, as it requires maintaining and updating two separate networks; exploring more unified or amortized update schemes is natural direction. Moreover, our study focuses on on-policy optimization with low-dimensional state features. Extending the latentdecoder decomposition to offpolicy algorithms (e.g., SAC), high-dimensional visual control, or offline-to-online fine-tuning settings represents promising frontier. Finally, understanding how this disentangled structure interacts with safety constraints and robustness in real-world control remains an important open question."
        },
        {
            "title": "References",
            "content": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning, 2017. Dmitry Akimov, Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov, and Sergey Kolesnikov. Let offline RL flow: Training conservative agents in the latent space of normalizing flows. arXiv preprint arXiv:2211.11096, 2022. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Anikait Kumar, and Pieter Abbeel. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, 2021. 14 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, 2018. Cheng Chi, Joy Hsu, Siddhant Haldar, Annie Xu, Andy Zeng, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Conference on Robot Learning, pages 14961515, 2023. Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021. Shutong Ding, Ke Hu, Zhenhao Zhang, Kan Ren, Weinan Zhang, Jingyi Yu, Jingya Wang, and Ye Shi. Diffusionbased reinforcement learning via q-weighted variational policy optimization. In Advances in Neural Information Processing Systems, 2024. Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yi Ma, Pengyi Li, and Yan Zheng. Cleandiffuser: An easy-to-use modularized library for diffusion models in decision making. In Advances in Neural Information Processing Systems, Datasets and Benchmarks Track, 2024. Scott Emmons, Mingyang Zeng, Chelsea Finn, and Sergey Levine. Policy diffusion: diffusion model for behavior cloning and offline reinforcement learning. In NeurIPS Deep RL Workshop, 2022. Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. In Advances in Neural Information Processing Systems, 2018. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. Amir Gholami, Kurt Keutzer, and George Biros. ANODE: Unconditionally accurate memory-efficient gradients for neural odes. In International Joint Conference on Artificial Intelligence (IJCAI), 2019. Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD: Freeform continuous dynamics for scalable reversible generative models. In International Conference on Learning Representations, 2019. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International Conference on Machine Learning, pages 18611870, 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL: Implicit Q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In AAAI Conference on Artificial Intelligence, 2018. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Are deep policy gradient algorithms truly policy gradient algorithms? In International Conference on Learning Representations, 2020. 15 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, pages 267274, 2002. Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. In Advances in Neural Information Processing Systems, 2023. Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: survey. The International Journal of Robotics Research, 32(11):12381274, 2013. Zechu Li, Rickmer Krohn, Tao Chen, Anurag Ajay, Pulkit Agrawal, and Georgia Chalvatzaki. Learning multimodal behaviors from scratch with diffusion policy gradient. arXiv preprint arXiv:2406.00681, 2024. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, et al. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016. Yaron Lipman, Hengrui Chen, Haggai Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. Transactions on Machine Learning Research, 2023. arXiv:2210.02747. Lei Lv, Yunfei Li, Yu Luo, Fuchun Sun, Tao Kong, Jiafeng Xu, and Xiao Ma. Flowrl: Flow-based policy for online reinforcement learning. arXiv preprint arXiv:2506.12811, 2025. Haitong Ma, Tianyi Chen, Kai Wang, Na Li, and Bo Dai. Soft diffusion actor-critic: Efficient online reinforcement learning for diffusion policy. arXiv preprint arXiv:2502.00361, 2025. David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053, 2025. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015. Allen Z. Ren, Justin Lidard, Lars L. Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, and Max Simchowitz. Diffusion policy policy optimization. arXiv preprint arXiv:2409.00588, 2024. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. In International Conference on Machine Learning, pages 18891897, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Nur Muhammad Shafiullah, Rahul Kidambi, Dhruv Singh, Aravind Rajeswaran, and Sergey Levine. On the expressivity of policies in reinforcement learning. In International Conference on Machine Learning, 2022. David Silver, Aja Huang, Chris J. Maddison, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484489, 2016. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 1999. GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Yuval Tassa et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, and Glen Berseth. Reasoning with latent diffusion in offline reinforcement learning. In International Conference on Learning Representations, 2024. Andrew Wagenmaker, Jong Wook Jeong, Claire Tomlin, Sergey Levine, Anca Dragan, and Joseph E. Gonzalez. Steering your diffusion policy with latent space RL. In International Conference on Machine Learning, 2025. Zhenglin Wan, Jingxuan Wu, Xingrui Yu, Chubin Zhang, Mingcong Lei, Bo An, and Ivor Tsang. Fm-irl: Flow-matching for reward modeling and policy regularization in reinforcement learning. arXiv preprint arXiv:2510.09222, 2025a. Zhenglin Wan, Xingrui Yu, David Mark Bossens, Yueming Lyu, Qing Guo, Flint Xiaofeng Fan, Yew-Soon Ong, and Ivor Tsang. Diversifying policy behaviors with extrinsic behavioral curiosity. In International Conference on Machine Learning, 2025b. Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022. Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. In International Conference on Learning Representations, 2023a. Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. In International Conference on Learning Representations, 2023b. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(34):229256, 1992. Xingrui Yu, Zhenglin Wan, David Mark Bossens, Yueming Lyu, Qing Guo, and Ivor Tsang. Imitation from diverse behaviors: Wasserstein quality diversity imitation learning with single-step archive exploration. arXiv preprint arXiv:2411.06965, 2024. Tonghe Zhang, Chao Yu, Sichang Su, and Yu Wang. Reinflow: Fine-tuning flow matching policy with online reinforcement learning, 2025a. Manuscript. Yixian Zhang, Shuang Yu, Tonghe Zhang, et al. SAC flow: Sample-efficient reinforcement learning of flow-based policies via velocity-reparameterized sequential modeling. arXiv preprint arXiv:2509.25756, 2025b. Wenxuan Zhou, Sujay Bajracharya, and David Held. PLAS: Latent action space for offline reinforcement learning. In Conference on Robot Learning, pages 17191735. PMLR, 2020. 17 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies"
        },
        {
            "title": "Appendix",
            "content": "A Why Generative Policies Are Hard to Update with Standard Policy Gradients 19 A.1 Three classical routes to policy gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.2 Diffusion policies violate all three routes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.3 Flow-matching policies violate all three routes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.4 Implications for online RL and connection to GoRL . . . . . . . . . . . . . . . . . . . . . . . . . . Reinforcement Learning Background 21 B.1 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2 Policy Gradient Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Generative Policy Details C.1 Diffusion-Based Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.2 Flow Matching Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Proofs of Theoretical Guarantees 23 D.1 Notation and Induced Action Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Proof of Lemma 3.1 (Unbiased Latent Policy Gradient) . . . . . . . . . . . . . . . . . . . . . . . . 23 D.3 Proof of Lemma 3.2 (Performance under Small Latent Divergence) . . . . . . . . . . . . . . . . . 24 D.4 Discussion: Stability and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Training and Implementation Details E.1 Environments and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.2 Shared PPO Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.3 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.4 GoRL Specifics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 E.5 Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 E.7 Action Distribution Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 18 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies A. Why Generative Policies Are Hard to Update with Standard Policy Gradients In this appendix, we provide structural explanation for why expressive generative policiesnotably diffusion and flow-matching (FM) policiesare difficult to optimize with standard online policy gradients. The key message is simple: classical policy-gradient estimators are stable only when at least one of three tractability routes is available. Diffusion and FM policies break all three routes in practice, which makes direct action-space optimization expensive, noisy, and often unstable. A.1. Three classical routes to policy gradients Let ğœ‹ğœƒ(ğ‘ ğ‘ ) be stochastic policy and define the objective where ğ‘„ is critic. Since ğœƒ appears inside the sampling distribution, ğœƒğ½ (ğœƒ) is not directly available. In continuous control, three gradient constructions are commonly used. ğ½ (ğœƒ) = E(ğ‘ ,ğ‘)ğœ‹ğœƒ [ ğ‘„(ğ‘ , ğ‘) ] , (7) Route I: likelihood-ratio (score-function) gradients. Using the log-derivative trick, ğœƒğ½ (ğœƒ) = E(ğ‘ ,ğ‘)ğœ‹ğœƒ [ ğœƒ log ğœ‹ğœƒ(ğ‘ ğ‘ ) ğ‘„(ğ‘ , ğ‘) ] , (8) which underlies REINFORCE and modern actorcritic algorithms. This route assumes that log ğœ‹ğœƒ(ğ‘ ğ‘ ) is tractable and cheap to evaluate. Route II: explicit reparameterization (pathwise) gradients. ğ‘”ğœƒ(ğ‘ , ğœ‰) with exogenous noise ğœ‰ ğ‘(ğœ‰), then If actions admit differentiable sampler ğ‘ = ğœƒğ½ (ğœƒ) = ğ‘ ğ‘‘ğœ‹ğœƒ , ğœ‰ğ‘(ğœ‰) [ ğ‘ğ‘„(ğ‘ , ğ‘) ğ‘”ğœƒ(ğ‘ ,ğœ‰) ğœƒ ] , ğ‘ = ğ‘”ğœƒ(ğ‘ , ğœ‰). (9) This estimator is stable when ğ‘”ğœƒ is shallow and easy to differentiate. Route III: implicit reparameterization via CDFs. When no explicit ğ‘”ğœƒ exists, implicit reparameterization differentiates through the CDF ğ¹ğœƒ (Figurnov et al., 2018). Sampling is written as ğ‘¢ ğ’° [0, 1] and ğ¹ğœƒ(ğ‘ ğ‘ ) = ğ‘¢, so ğ‘ is defined implicitly. Gradients require evaluating ğ¹ğœƒ (and conditional CDFs in multiple dimensions), which is feasible only if the CDF is numerically accessible. Thus, stable policy gradients rely on either tractable likelihoods, or short differentiable samplers, or tractable CDFs. We next show why diffusion and FM policies violate these assumptions. A.2. Diffusion policies violate all three routes conditional diffusion policy generates an action by denoising Gaussian noise through long stochastic chain (Ho et al., 2020, Song et al., 2021): ğ‘¥ğ‘‡ ğ’© (0, ğ¼), ğ‘¥ğ‘¡1 = ğ‘“ğœƒ(ğ‘¥ğ‘¡, ğ‘ , ğ‘¡, ğœ–ğ‘¡), ğ‘¡ = ğ‘‡ , . . . , 1, ğ‘ = ğ‘¥0. 19 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Route breaks: likelihoods are intractable. Applying the likelihood-ratio estimator requires log ğœ‹ğœƒ(ğ‘ ğ‘ ). For diffusion policies, this corresponds to the log-density of the reverse-time SDE (or its discretization), which involves solving an SDE/ODE and accumulating Jacobian/divergence terms along the entire denoising path (Song et al., 2021). This computation is far more expensive and numerically fragile than the closed-form likelihood used by PPO/TRPO-style updates, making Route impractical in online RL. Route II becomes high-variance: gradients flow through deep stochastic chains. Diffusion policies are must be obtained by backpropagating through ğ‘‡ formally reparameterized by base noise (ğ‘¥ğ‘‡ , {ğœ–ğ‘¡}), but ğ‘ ğœƒ denoising steps. In online RL, this deep stochastic computation graph amplifies noise from both the critic gradient ğ‘ğ‘„(ğ‘ , ğ‘) and the denoising dynamics. As result, pathwise gradients typically have high variance and require aggressive stabilization (small learning rates, truncated chains, strong regularization). Route III is unavailable: no tractable CDF. Diffusion models are defined via score fields and sampling dynamics, not via closed-form densities or CDFs. Hence implicit CDF-based gradients are not accessible. A.3. Flow-matching policies violate all three routes Flow-matching (FM) and continuous normalizing flow (CNF) policies generate actions through an ODE transport (Grathwohl et al., 2019, Lipman et al., 2023): ğ‘‘ğ‘¥ğ‘¡ ğ‘‘ğ‘¡ = ğ‘£ğœƒ(ğ‘¥ğ‘¡, ğ‘ , ğ‘¡), ğ‘¥0 = ğœ‰ ğ‘0, ğ‘ = ğ‘¥1 = Î¦ğœƒ,ğ‘  01(ğœ‰). Thus ğœ‹ğœƒ( ğ‘ ) is the pushforward of ğ‘0 by Î¦ğœƒ,ğ‘  01 . Route breaks or is prohibitively costly. For CNFs, the log-density satisfies (Grathwohl et al., 2019) ( log ğ‘ğ‘¡(ğ‘¥ğ‘¡ ğ‘ ) = tr ğ‘‘ ğ‘‘ğ‘¡ log ğœ‹ğœƒ(ğ‘ ğ‘ ) = log ğ‘0(ğœ‰) ğ‘£ğœƒ ğ‘¥ (ğ‘¥ğ‘¡, ğ‘ , ğ‘¡) 1 0 ) , ( tr ğ‘£ğœƒ ğ‘¥ (ğ‘¥ğ‘¡, ğ‘ , ğ‘¡) ) ğ‘‘ğ‘¡. (10) (11) (12) So even one likelihood evaluation requires (i) solving the ODE for ğ‘¥0:1 and (ii) estimating the trace integral, usually via Hutchinson probes and Jacobianvector products. Differentiating log ğœ‹ğœƒ adds backward ODE solve. In pure FM, no likelihood is defined at all, so Route is inapplicable. Route II is natural in form but forces ODE sensitivities. FM policies already provide reparameterized sampler ğ‘ = Î¦ğœƒ,ğ‘  01(ğœ‰), ğœ‰ ğ‘0, giving ğœƒğ½ (ğœƒ) = Eğ‘ ,ğœ‰ [ ğ‘ğ‘„(ğ‘ , ğ‘) Î¦ğœƒ,ğ‘  01(ğœ‰) ğœƒ ] . (13) The difficulty lies in the sensitivity Î¦/ğœƒ. Computing it requires differentiating through the ODE solver: either by unrolling the numerical steps (BPTT) or by continuous adjoints/sensitivities as in neural ODEs (Chen et al., 2018). Both options introduce backward solve of similar cost to the forward flow, and can suffer numerical mismatch between the continuous adjoint and the discretized forward solver (Gholami et al., 2019). Consequently, the pathwise estimator remains expensive and can be unstable in long-horizon online training. GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Implicit gradients require (conditional) CDFs. FM/CNF policies specify distributions Route III is unavailable. through dynamics, not tractable CDFs; recovering them would be at least as hard as computing likelihoods. Thus Route III is not practical. A.4. Implications for online RL and connection to GoRL Diffusion and FM policies therefore share common structural mismatch with classical online policy gradients: likelihoods are expensive or undefined, reparameterization gradients require differentiating through deep generative dynamics, and CDFs are inaccessible. This explains the instability widely observed when optimizing generative policies directly in action space in online RL. GoRL resolves this mismatch by removing the generator from the policy-gradient estimator. All gradient-based policy optimization is carried out on tractable latent policy ğœ‹ğœƒ(ğœ€ ğ‘ ), which satisfies Route with cheap likelihood ratios, while the expressive decoder ğœ‹ğœ‘(ğ‘ ğ‘ , ğœ€) is trained separately using likelihood-free generative objectives under fixed latent prior. In this way, stable online policy gradients and expressive multimodal action generation can coexist without forcing diffusion or FM likelihoods into the optimization loop. B. Reinforcement Learning Background For completeness, we provide brief review of the reinforcement learning (RL) formalism and the standard policy gradient theorem that underpins our method. B.1. Markov Decision Processes We consider standard continuous RL problem modeled as Markov Decision Process (MDP), defined by the tuple (ğ’®, ğ’œ, ğ‘ƒ , ğ‘Ÿ, ğ›¾, ğœŒ0), where: ğ’® Rğ‘‘ğ‘  is the continuous state space. ğ’œ Rğ‘‘ğ‘ is the continuous action space. ğ‘ƒ : ğ’® ğ’œ (ğ’®) denotes the state transition dynamics, where ğ‘ƒ (ğ‘  ğ‘ , ğ‘) is the probability density of transitioning to state ğ‘  given state ğ‘  and action ğ‘. ğ‘Ÿ : ğ’® ğ’œ is the reward function. ğ›¾ [0, 1) is the discount factor. ğœŒ0 (ğ’®) is the initial state distribution. policy ğœ‹ is mapping from states to probability distribution over actions, denoted as ğœ‹(ğ‘ğ‘ ). The goal of the agent is to learn policy ğœ‹ğœƒ parameterized by ğœƒ that maximizes the expected discounted cumulative return: ğ½ (ğœƒ) = Eğœ ğœ‹ğœƒ ] ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ ğ‘¡, ğ‘ğ‘¡) , [ ğ‘¡=0 (14) where ğœ = (ğ‘ 0, ğ‘0, ğ‘ 1, ğ‘1, . . . ) denotes trajectory sampled under the policy dynamics: ğ‘ 0 ğœŒ0, ğ‘ğ‘¡ ğœ‹ğœƒ(ğ‘ ğ‘¡), and ğ‘ ğ‘¡+1 ğ‘ƒ (ğ‘ ğ‘¡, ğ‘ğ‘¡). B.2. Policy Gradient Theorem The standard Policy Gradient Theorem (Sutton et al., 1999) provides gradient estimator for maximizing the objective ğ½ (ğœ‹ğœƒ) with respect to the policy parameters ğœƒ: ğœƒğ½ (ğœƒ) = ğ‘ ğ‘‘ğœ‹ğœƒ , ğ‘ğœ‹ğœƒ(ğ‘ ) [ğœƒ log ğœ‹ğœƒ(ğ‘ ğ‘ ) ğ´ğœ‹ğœƒ (ğ‘ , ğ‘)] , (15) 21 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies where ğ‘‘ğœ‹ğœƒ (ğ‘ ) = (1 ğ›¾) ğ‘¡=0 ğ›¾ğ‘¡ğ‘ƒ (ğ‘ ğ‘¡ = ğ‘  ğœ‹ğœƒ) is the discounted state visitation distribution, and ğ´ğœ‹ğœƒ (ğ‘ , ğ‘) = ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) ğ‘‰ ğœ‹ğœƒ (ğ‘ ) is the advantage function. Here, ğ‘„ğœ‹ğœƒ (ğ‘ , ğ‘) is the state-action value function, and ğ‘‰ ğœ‹ğœƒ (ğ‘ ) is the state value function. In practice, algorithms like PPO (Schulman et al., 2017) approximate this gradient using empirically collected trajectories and optimize surrogate objective (as described in Eq. 5 of the main text) to perform stable, monotonic policy updates. C. Generative Policy Details In this appendix, we describe the architectural and training details for the two generative decoders used in GoRL: diffusion policies and flow-matching policies. In our framework, these models serve as the conditional decoder ğ‘”ğœ‘(ğ‘ , ğœ€), mapping latent variable ğœ€ ğ’© (0, ğ¼) to an action ğ‘, conditioned on the state ğ‘ . C.1. Diffusion-Based Policies Diffusion models (Ho et al., 2020) generate data by reversing gradual noising process. We adopt the Denoising Diffusion Probabilistic Model (DDPM) formulation adapted for continuous control (Chi et al., 2023, Wang et al., 2022). Forward Process. The forward process progressively adds Gaussian noise to an action ğ‘0 over ğ‘‡ timesteps. We define the noise-corrupted action ğ‘ğ‘¡ as: ğ‘ğ‘¡ = ğ›¼ğ‘¡ğ‘0 + 1 ğ›¼ğ‘¡ğœ‰, ğœ‰ ğ’© (0, ğ¼), (16) where ğ›¼ğ‘¡ follows fixed variance schedule. Reverse Process (Decoder). The generative process corresponds to the decoder ğ‘”ğœ‘. It reverses this chain, starting from the latent noise ğœ€ (which corresponds to ğ‘ğ‘‡ ) and iteratively denoising it to recover the action ğ‘0. The reverse transition ğ‘ğœ‘(ğ‘ğ‘¡1 ğ‘ğ‘¡, ğ‘ ) is parameterized by noise prediction network ğœ–ğœ‘(ğ‘ğ‘¡, ğ‘¡, ğ‘ ), which estimates the noise component ğœ‰. The decoder ğ‘”ğœ‘(ğ‘ , ğœ€) is defined as the complete sampling chain initialized with ğ‘ğ‘‡ = ğœ€. Training Objective. The network ğœ–ğœ‘ is trained to predict the noise added to ğ‘0. Crucially, we sample state-action pairs from the recent on-policy rollout buffer ğ’Ÿ. The loss function is: â„’Diff(ğœ‘) = ğ‘¡ğ’° {1,ğ‘‡ }, (ğ‘ ,ğ‘0)ğ’Ÿ, ğœ‰ğ’© (0,ğ¼) ğœ‰ ğœ–ğœ‘(ğ‘ğ‘¡, ğ‘¡, ğ‘ )2] [ , (17) where ğ‘ğ‘¡ is constructed from ğ‘0 and ğœ‰ using the forward process. C.2. Flow Matching Policies Flow Matching (FM) (Lipman et al., 2023) offers continuous-time alternative based on Ordinary Differential Equations (ODEs). Probability Path and Vector Field. FM defines probability path that interpolates between base distribution (noise) and the data distribution. We use the Conditional Flow Matching (CFM) objective with an Optimal Transport path, defined as linear interpolation between the latent noise ğœ€ and the target action ğ‘1: The unique vector field generating this linear path is ğ‘¢ğ‘¡(ğ‘ğ‘¡ ğ‘1, ğœ€) = ğ‘1 ğœ€. ğ‘ğ‘¡ = (1 ğ‘¡)ğœ€ + ğ‘¡ğ‘1, ğ‘¡ [0, 1]. (18) 22 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Decoder Definition. The decoder ğ‘”ğœ‘(ğ‘ , ğœ€) is defined as the solution to the ODE evaluated at time ğ‘¡ = 1, i.e., ğ‘‘ğ‘ğ‘¡ ğ‘‘ğ‘¡ = ğ‘£ğœ‘(ğ‘ğ‘¡, ğ‘¡, ğ‘ ), ğ‘0 = ğœ€, ğ‘”ğœ‘(ğ‘ , ğœ€) = ğ‘1 = ğœ€ + 1 ğ‘£ğœ‘(ğ‘ğ‘¡, ğ‘¡, ğ‘ ) ğ‘‘ğ‘¡. (19) In practice, this integral is approximated using numerical solver (e.g., Euler or RK45). Training Objective. The neural network ğ‘£ğœ‘(ğ‘ğ‘¡, ğ‘¡, ğ‘ ) is trained to regress the target vector field ğ‘¢ğ‘¡. Using samples from the rollout buffer ğ’Ÿrollout, the loss is: â„’FM(ğœ‘) = ğœ ğ’° [0,1], (ğ‘ ,ğ‘)ğ’Ÿrollout, ğœ€ğ’© (0,ğ¼) ğ‘£ğœ‘(ğ‘ğœ , ğœ , ğ‘ ) (ğ‘ ğœ€)2] [ , (20) where ğ‘ğœ = (1 ğœ )ğœ€ + ğœ ğ‘. This objective provides stable, low-variance gradients for the decoder parameters ğœ‘. D. Proofs of Theoretical Guarantees This appendix provides full proofs for the latent-space guarantees stated in Section 3. Throughout this section, the decoder parameters ğœ‘ are treated as fixed, and we analyze updates of the latent policy (encoder) ğœ‹ğœƒ(ğœ€ ğ‘ ). D.1. Notation and Induced Action Policy Given stochastic encoder ğœ‹ğœƒ(ğœ€ ğ‘ ) and deterministic decoder ğ‘”ğœ‘(ğ‘ , ğœ€), the induced action policy (pushforward distribution) is defined as: ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) := ( ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ›¿ ğ‘ ğ‘”ğœ‘(ğ‘ , ğœ€) ) ğ‘‘ğœ€. (21) Sampling ğ‘ ğœ‹ğœƒ,ğœ‘( ğ‘ ) is procedurally equivalent to sampling ğœ€ ğœ‹ğœƒ( ğ‘ ) and computing ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€). We denote by ğ‘‘ğœ‹ the discounted state-visitation distribution of policy ğœ‹, and by ğ´ğœ‹(ğ‘ , ğ‘) = ğ‘„ğœ‹(ğ‘ , ğ‘) ğ‘‰ğœ‹(ğ‘ ) the advantage function. For brevity, we write ğ´ğœƒ,ğœ‘(ğ‘ , ğ‘) := ğ´ğœ‹ğœƒ,ğœ‘(ğ‘ , ğ‘). D.2. Proof of Lemma 3.1 (Unbiased Latent Policy Gradient) Lemma (Unbiased Latent Policy Gradient). Fix ğœ‘, and let ğœ‹ğœƒ,ğœ‘ be the induced action policy. Then ğœƒğ½ (ğœƒ, ğœ‘) = ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğœ€ğœ‹ğœƒ(ğ‘ ) [ ğœƒ log ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ´ğœƒ,ğœ‘ ( ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€) )] . Proof. Consider the induced action policy ğœ‹ğœƒ,ğœ‘. By the standard policy gradient theorem applied to ğœ‹ğœƒ,ğœ‘, we have [ ğœƒ log ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) ğ´ğœƒ,ğœ‘(ğ‘ , ğ‘) ğœƒğ½ (ğœƒ, ğœ‘) = ] . ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğ‘ğœ‹ğœƒ,ğœ‘(ğ‘ ) Sampling ğ‘ ğœ‹ğœƒ,ğœ‘( ğ‘ ) is equivalent to sampling ğœ€ ğœ‹ğœƒ( ğ‘ ) and setting ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€). Moreover, ğ‘”ğœ‘ does not depend on ğœƒ, so the dependence of ğœ‹ğœƒ,ğœ‘(ğ‘ ğ‘ ) on ğœƒ comes entirely from the latent policy ğœ‹ğœƒ(ğœ€ ğ‘ ). This allows us to rewrite the score term as ğœƒ log ğœ‹ğœƒ(ğœ€ ğ‘ ) and express the gradient as ğœƒğ½ (ğœƒ, ğœ‘) = ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğœ€ğœ‹ğœƒ(ğ‘ ) [ ğœƒ log ğœ‹ğœƒ(ğœ€ ğ‘ ) ğ´ğœƒ,ğœ‘ ( ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€) )] , which is exactly the claimed latent-space estimator. 23 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies D.3. Proof of Lemma 3.2 (Performance under Small Latent Divergence) Lemma (Performance under Small Latent Divergence). Let ğœ‹ğœƒ(ğœ€ ğ‘ ) and ğœ‹ğœƒ(ğœ€ ğ‘ ) be two encoders such that their average divergence is bounded: Eğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ [ ğ·TV ( ğœ‹ğœƒ( ğ‘ ) ğœ‹ğœƒ( ğ‘ ) )] ğ›¿, (22) where ğ·TV(ğ‘, ğ‘) := 1 Then ğ½ (ğœƒ, ğœ‘) ğ½ (ğœƒ, ğœ‘) where ğ¶ = 2ğ›¾ (1ğ›¾)2 . 2 ğ‘ ğ‘1 is the Total Variation distance and ğœ‘ is fixed. Assume ğ´ğœƒ,ğœ‘(ğ‘ , ğ‘) ğ´max for all (ğ‘ , ğ‘). 1 1 ğ›¾ ğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ , ğœ€ğœ‹ğœƒ (ğ‘ ) ( [ ğ´ğœƒ,ğœ‘ ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€) )] ğ¶ ğ´max ğ›¿, (23) Proof. Step 1: Performance Difference Lemma. For any two policies ğœ‹ and ğœ‹, the standard performance difference lemma (Kakade and Langford, 2002) states: ğ½ (ğœ‹) ğ½ (ğœ‹) = 1 1 ğ›¾ Eğ‘ ğ‘‘ğœ‹ ğ‘ğœ‹(ğ‘ ) [ ğ´ğœ‹(ğ‘ , ğ‘) ] . (24) Applying this to our induced policies ğœ‹ = ğœ‹ğœƒ,ğœ‘ and ğœ‹ = ğœ‹ğœƒ,ğœ‘ and using that ğ‘ = ğ‘”ğœ‘(ğ‘ , ğœ€) with ğœ€ ğœ‹ğœƒ( ğ‘ ), we obtain ğ½ (ğœƒ, ğœ‘) ğ½ (ğœƒ, ğœ‘) = 1 1 ğ›¾ Eğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ ğœ€ğœ‹ğœƒ (ğ‘ ) ( [ ğ´ğœƒ,ğœ‘ ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€) )] . (25) Step 2: Bounding the Distribution Shift. Define ğ¹ (ğ‘ ) = assumption ğ´ğœƒ,ğœ‘(ğ‘ , ğ‘) ğ´max, we have ğ¹ (ğ‘ ) ğ´max for all ğ‘ . The error from replacing ğ‘  ğ‘‘ğœ‹ğœƒ,ğœ‘ is ğœ€ğœ‹ğœƒ (ğ‘ )[ğ´ğœƒ,ğœ‘(ğ‘ , ğ‘”ğœ‘(ğ‘ , ğœ€))]. By the bounded advantage by ğ‘  ğ‘‘ğœ‹ğœƒ,ğœ‘ = Eğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ [ğ¹ (ğ‘ )] Eğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ [ğ¹ (ğ‘ )] ğ´max ğ‘‘ğœ‹ğœƒ,ğœ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ 1 . Step 3: Relating Action Divergence to Latent Divergence. Since ğ‘”ğœ‘ is fixed function, the data processing inequality implies that, for every state ğ‘ , ( ğ·TV ğœ‹ğœƒ,ğœ‘( ğ‘ ), ğœ‹ğœƒ,ğœ‘( ğ‘ ) ) ( ğ·TV ğœ‹ğœƒ( ğ‘ ), ğœ‹ğœƒ( ğ‘ ) ) . (26) Using standard trust-region bounds for discounted MDPs (Kakade and Langford, 2002, Schulman et al., 2015, Achiam et al., 2017), the state-visitation distribution shift is bounded as ğ‘‘ğœ‹ğœƒ,ğœ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ 1 2ğ›¾ 1 ğ›¾ Eğ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ [ ğ·TV ( ğœ‹ğœƒ,ğœ‘( ğ‘ ), ğœ‹ğœƒ,ğœ‘( ğ‘ ) )] . Combining the two inequalities and the latent divergence assumption (22) yields ğ‘‘ğœ‹ğœƒ,ğœ‘ ğ‘‘ğœ‹ğœƒ,ğœ‘ 1 2ğ›¾ 1 ğ›¾ ğ›¿. (27) (28) Substituting this into the bound on from Step 2 and then back into the performance difference expression gives (23) with ğ¶ = 2ğ›¾ . (1ğ›¾)2 24 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies D.4. Discussion: Stability and Regularization Lemma 3.2 guarantees that as long as successive encoder policies remain close in latent space, any performance degradation of the induced policy is bounded by ğ‘‚(ğ›¿). In particular, when the expected advantage term is positive and sufficiently large, the update yields strict improvement in return. In practice, PPO updates explicitly control the divergence between ğœ‹ğœƒ and ğœ‹ğœƒ per update via clipped likelihood ratios. Our additional KL regularization toward the prior ğ’© (0, ğ¼) serves complementary global purpose: it prevents the latent policy from drifting into regions of the latent space that are out-of-distribution (OOD) for the fixed decoder. By anchoring the encoders output distribution to the decoders training support, we ensure that the theoretical guarantees of the latent update translate into valid action improvements. E. Training and Implementation Details This appendix provides the experimental and implementation details required to reproduce our results and to ensure fair comparison with Gaussian and generative-policy baselines. E.1. Environments and Preprocessing We evaluate on six DeepMind Control Suite tasks implemented using standard MuJoCo-based setup: CheetahRun, FingerSpin, FingerTurnHard, FishSwim, HopperStand, and WalkerWalk. Table 2 reports the observation and action dimensions. Table 2: Observation and action dimensions of DMControl tasks. Task Action dim. Observation dim. CheetahRun HopperStand WalkerWalk FingerSpin FingerTurnHard FishSwim 6 4 6 2 2 5 17 15 24 9 9 24 All methods are trained from scratch for 180M environment steps with 2048 parallel environments and episode length 1000. Observations are normalized online using running mean and variance shared across methods. Rewards are scaled by constant factor of 10.0. Policy outputs are squashed through tanh to fit the action bounds. Advantages are normalized per batch to zero mean and unit variance. E.2. Shared PPO Hyperparameters Gaussian PPO, FPO, DPPO, and the encoder of GoRL follow the same PPO training pipeline whenever applicable. The shared hyperparameters are listed in Table 3. The PPO clipping threshold differs across methods, following standard practice for expressive policies (see below). E.3. Baselines Across all methods, actor/decoder networks use comparable MLPs (typically 4 layers, 3264 units) with SiLU activations, and critics share common 5-layer MLP backbone, ensuring that performance differences stem from GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies Table 3: Shared PPO hyperparameters across all methods. Hyperparameter Value Optimizer Learning rate Batch size per update Rollout horizon Optimization epochs per batch Discount factor ğ›¾ GAE parameter ğœ† Value loss coefficient Entropy coefficient Adam 1 103 (PPO / GoRL encoder) 1024 30 16 0.995 0.95 0.25 0.01 (set to 0 for FPO/DPPO) the policy formulation rather than network capacity. Gaussian PPO. The actor is diagonal Gaussian policy with 4-layer MLP (width 32, SiLU). The critic is 5-layer MLP (width 256, SiLU). The log standard deviation is parameterized by softplus and clipped to [103, 10]. FPO. We implement Flow Policy Optimization following the original paper (McAllister et al., 2025). The policy is conditional flow-matching model sampled with 10 ODE steps and trained with flow-matching surrogate objective. For each action, we sample 8 (ğœ , ğœ€) pairs. The policy network is 4-layer MLP (width 32, SiLU) that takes (ğ‘ , ğ‘ğ‘¡, ğœ -embed) as input and outputs velocity field. We use learning rate of 3 104 and clipping ğœ–clip = 0.05, which are the best-performing values reported in the FPO hyperparameter sweep. DPPO. DPPO (Ren et al., 2024) parameterizes the policy as conditional diffusion model whose denoising process is modeled as two-layer MDP. In our implementation, we use 10 denoising steps with cosine noise schedule and optimize the diffusion policy with PPO on this denoising MDP using analytic Gaussian likelihoods at each step. The main hyperparameters are learning rate of 3 104, clipping ğœ–clip = 0.2, and diffusion noise scale ğœğ‘¡ = 0.05. E.4. GoRL Specifics Encoder. The encoder ğœ‹ğœƒ(ğœ€ ğ‘ ) is diagonal Gaussian latent policy mirroring the Gaussian PPO actor. We set the latent dimension ğ‘§dim = action dim. for all tasks. During encoder updates, the decoder is frozen and PPO is applied in latent space (Eq. 2). To prevent latent drift, we regularize the latent Gaussian parameters toward standard normal prior. In practice, this is implemented as an ğ¿2 penalty on the mean and log-scale, which is proportional to the KL divergence term: with default ğ›½ = 103. We sweep ğ›½ in the ablation study. â„’reg = ğ›½ KL(ğœ‹ğœƒ(ğœ€ğ‘ ) ğ’© (0, ğ¼)), Decoder. We instantiate the decoder as either conditional flow-matching model (GoRL(FM)) or conditional diffusion model (GoRL(Diff)). Both are trained on recent on-policy rollouts, but with latent inputs sampled 26 GoRL : An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies from fixed ğ’© (0, ğ¼) prior to avoid non-stationary feedback (Eq. 4). The decoder network uses 4-layer MLP (width 64, SiLU). Each decoder refinement stage runs for 50 epochs with learning rate of 3 104 and batch size 8192. Two-time-scale schedule. Training follows two-time-scale alternation between (i) encoder interaction and latent-space PPO updates with frozen decoder, and (ii) decoder refinement with frozen encoder. Concretely, we run four encoder stages with environment interaction budgets 60M, 60M, 30M, and 30M steps, respectively (total 180M). Stage 0 starts from an identity-like decoder and trains the encoder for 60M steps to obtain stable early rollouts. After each encoder stage, we freeze the encoder and refine the decoder on the most recent on-policy rollout buffer using Eq. 4 with latent inputs sampled from fixed ğ’© (0, ğ¼) prior. This yields three decoder updates at the 60M, 120M, and 150M milestones, producing decoders for Stage 1, Stage 2, and Stage 3, respectively. The identity warm-up therefore provides stable exploration and clean initial dataset, while later shorter stages allow the encoder to adapt to progressively more expressive decoders without introducing non-stationary feedback. E.5. Evaluation Protocol We report mean and standard deviation over five random seeds. Policies are evaluated every 6M environment steps using 128 parallel environments. All curves in the main paper show normalized returns with shaded regions indicating one standard deviation. E.6. Ablations Latent regularization sweep. We sweep ğ›½ {0, 104, 5 104, 103, 5 103, 102} on CheetahRun, keeping all other settings fixed. Decoder iteration study. For each decoder snapshot (Stage 0/1/2/3), we freeze the decoder and train only the encoder for 60M environment steps, then report the resulting performance. E.7. Action Distribution Visualization To visualize multimodality, we fix representative state from HopperStand, sample 10,000 actions from each trained policy, project actions to 1D using PCA, and estimate densities via Gaussian KDE with bandwidth factor of 0.8. The same pipeline is applied to all methods for fair comparison."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications, China",
        "Centre for Frontier AI Research, A*STAR, Singapore",
        "Nanyang Technological University, Singapore"
    ]
}