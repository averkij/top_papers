{
    "paper_title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
    "authors": [
        "Zirui Tang",
        "Boyu Niu",
        "Xuanhe Zhou",
        "Boxiu Li",
        "Wei Zhou",
        "Jiannan Wang",
        "Guoliang Li",
        "Xinyi Zhang",
        "Fan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 9 1 8 1 . 8 0 5 2 : r ST-Raptor: LLM-Powered Semi-Structured Table Question Answering Zirui Tang Shanghai Jiao Tong University tangzirui@sjtu.edu.cn Boxiu Li Shanghai Jiao Tong University lbxhaixing154@sjtu.edu.cn Guoliang Li Tsinghua University liguoliang@tsinghua.edu.cn Boyu Niu Shanghai Jiao Tong University nby2005@sjtu.edu.cn Wei Zhou Shanghai Jiao Tong University weizhoudb@sjtu.edu.cn Xuanhe Zhou Shanghai Jiao Tong University zhouxh@cs.sjtu.edu.cn Jiannan Wang Simon Fraser University jnwang@sfu.ca Xinyi Zhang Renmin University of China xinyizhang.info@ruc.edu.cn Fan Wu Shanghai Jiao Tong University fwu@cs.sjtu.edu.cn Figure 1: Example analytical questions over real-world semi-structured tables (e.g., Excel spreadsheets). Abstract Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, tree-based framework for semi-structured table question answering (semi-structured table QA) using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree by identifying headers, content values, and their implicit relationships. Second, we define set of basic tree operations to guide LLMs in executing common QA tasks. Xuanhe Zhou is the corresponding author. Given user question, ST-Raptor decomposes it into simpler subquestions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate two-stage verification mechanism: (1) forward validation checks the correctness of execution steps, while (2) backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor."
        },
        {
            "title": "1 Introduction\nSemi-Structured Tables are a type of data structure that represents\nthe flexible and complex layouts commonly found in real-world data\nacross a variety of applications, such as Word Tables for financial\nreports [1], Excel spreadsheets for medical records [2], and PDF\nTables for e-commerce transaction orders [3]. They often serve as\nthe major type in these applications, e.g., accounting for up to 80%\nof patient records in Electronic Medical Record (EMR) systems [2].\nFor better understanding, in Figure 1, we showcase five exam-\nple semi-structured tables with corresponding user questions from",
            "content": "1 diverse scenarios (e.g., human resource management, financial management, and personal information). Considering the bottom-right table (TD-Tech): (ùëñ) the top portion covers the companys fundamental details, while (ùëñùëñ) the lower portion contains basic information and performance ratings of employees per department. Different shades of blue in TD-Tech highlight the nested levels of the table, reflecting relationships like hierarchical headers (e.g., the Basic Info header linked to lower-level headers like Company and TEL) and header-to-content (e.g., both Department and Level headers own the content values of As). Even human analysts may need to carefully analyze the layout characters to fully understand such semi-structured tables. This layout flexibility makes semi-structured table QA extremely challenging and distinguishes it from other common QA tasks on structured data (e.g., relational tables [12]) and unstructured data (e.g., textual documents or multimedia files [7]). In Figure 1, we showcase semi-structured table questions that commonly require the following analysis strategies: (ùëñ) Identify the headers based on the user question to locate the areas of relevant table cells. For instance, ùëÑ8 first identifies the Level header in the Employee Info sub-table, and then recognizes the A+ cells. Meanwhile, it is essential to distinguish that the content value under the Department header is different from Level in the original question. (ùëñùëñ) Leverage the identified cells and the original question to analyze the surrounding table structures for capturing nested relationships and extracting additional information. For instance, for ùëÑ9, the merged cell A+ applies to two employees to get the right answer 2. (ùëñùëñùëñ) Explore all potentially relevant header and content cells required to form an accurate answer. For instance, for ùëÑ10, relevant information about the company is needed for summarization. Existing works (including powerful LLMs like GPT4-o[29] and DeepSeek-R1 [13]) face significant limitations in semi-structured table QA. First, NL2SQL methods [8, 10, 16, 34, 35, 41] generate SQL statements executed on relational tables to get the final answer. However, it requires converting semi-structured tables into fully structured formats, causing substantial information loss. Second, methods like NL2Code [43] generate python code to operate on pandas dataframes. However, they struggle to understand many complex semi-structured tables and conduct precise information retrieval. more promising approach involves converting tables into images for processing with Vision Language Models (VLM) [20, 47]. But it has three main limitations: (ùëñ) Table2image causes precise loss and often misleads to irrelevant table areas; (ùëñùëñ) It requires extensive fine-tuning on QA tasks and has poor generalization ability; (ùëñùëñùëñ) It cannot work for relatively large tables those with over 100+ rows (see more analysis in Section 2.3). There are several challenges in achieving automatic semistructured table QA. First, understanding the structure of semistructured tables involves two main problems: (ùëñ) how to distinguish header and content cells that could distribute in any areas of the table, which involves semantic understanding (e.g., in the bottomright table of Figure 1, the cells under department and level headers) and cannot be handled by rule-based matching approaches [9]; (ùëñùëñ) how to understand the nested or containment relationships across the header and content cells, where the same question to different cell relationships could lead to different answers. For instance, in the bottom-right table of Figure 1, when splitting the merged cell of department A, the answer to the question what is the second department would be department instead of (C1). Second, due to the complexity of semi-structured table layouts, answering questions over such tables can be challenging, often requiring variety of analytical tricks. For instance, we may need to apply both left-to-right and top-to-down lookup strategies (e.g., identifying departments in the bottom-right table of Figure 1 by referencing the top-level title header, the left Employee Info header, and the nested Department header). Conversely, we may sometimes need to examine the content cells to identify the relevant headers (i.e., bottom-up lookup), which makes the semi-structured table QA procedure even more complicated (C2). Third, an effective validation mechanism for semi-structured table QA remains absent, which is especially crucial for resolving issues such as hallucination in LLMs [10, 34]. In related tasks like NL2SQL, many methods do not validate the accuracy of generated answers and thus produce the final result in single shot. Others merely check whether executing the SQL statements yields result tables sufficient to answer the question. However, in semi-structured table QA, the retrieved cells can (ùëñ) still involve complex semi-structured layouts (e.g., single cell may contain multi-row text or even nested sub-table) and (ùëñùëñ) be derived through multiple lookups, making it difficult for general LLMs to verify the accuracy of these retrieved semi-structured table cells (C3). To address these challenges, we propose novel semi-structured table QA framework (ST-Raptor). First, we introduce graph model (HO-Tree) to represent semi-structured table layouts, with nodes denoting table headers / content values and edges capturing their hierarchical and containment relationships. This model also includes nine basic tree operations, covering most common QA tasks and addressing the structural complexity challenge (for C1). Second, we present an effective HO-Tree construction strategy: (1) multi-modal LLM identifies semi-structured table headers, (2) heuristic rules separate the semi-structured table into basic table units based on the identified headers, and (3) depth-first search (DFS) algorithm constructs the HO-Tree. This stage addresses the difficulty of accurately representing the complex implicit relations of semi-structured tables. Third, we propose question decomposition method with two key techniques: (1) semantic alignment between the input question and the derived operation pipeline, and (2) column-type-aware tagging approach that annotates discrete, continuous, and unstructured columns (e.g., listing [man, woman] for sex column) to enhance data retrieval accuracy (for C2). Finally, we introduce two-stage QA verification mechanism to ensure solution stability. The first stage checks constraints (e.g., non-empty, question-related) to validate the generated operations and their execution results. The second stage provides confidence score by comparing the original questions with those derived from the final answers (for C3). We summarize our contributions as follows: (1) We present novel framework that enables effective and robust semi-structured table QA using large language models. (2) We provide tree-based representation method for semi-structured tables (HO-Tree), and design basic tree operations in the model to support common QA tasks. 2 (3) We propose DFS-based algorithm that combines VLM and heuristic rules to construct HO-Tree from semi-structured table. (4) We design question decomposition strategy that ensures semantic alignment with the generated operation pipelines, and introduce column-type-aware tagging strategy to improve lookup accuracy on relatively large semi-structured tables. (5) We propose two-stage QA verification mechanism that conducts constraint examinations and compares the pipelines of the origin question and those derived from the final answer. (6) We curate the SSTQA dataset, featuring 102 diverse semi-structured tables and 764 representative queries commonly found in real-world scenarios. (7) We conduct thorough evaluations to verify ST-Raptor can effectively tackle the structural and semantic complexities of semistructured tables, resulting in improved QA accuracy and reliability."
        },
        {
            "title": "2.1 Semi-Structured Tables\nSemi-structured tables can be far more complex than structured\nones due to the combination of diverse table layouts. However, com-\npared with other semi-structured data, they still adhere to stricter\nlayout constraints and may suffer information loss when stored in\nformats like JSON (e.g., splitting merged cells). To preserve their\nstructure, richer formats like HTML [31] are required. Furthermore,\nquestions over such tables often demand precise layout-aware rea-\nsoning (e.g., interpreting layout constraints) and operator ground-\ning (e.g., identifying the intersection of relevant rows and columns).\nCore Elements. In a semi-structured table ùëá , a table cell is the\natomic unit of a semi-structured table (e.g., the intersection of one\nrow and column). A table header consists of one or more rows\nor columns that label the table body. A merged cell spans multi-\nple adjacent rows or columns to convey hierarchical information.\nA subtable is a semantically and structurally self-contained table\nembedded within a parent table with at least one level of nested\nheaders, rows, and internal layout.\nTable Layouts. The semi-structured table ùëá (in standard form and\nignoring issues likes data cleaning [6]) can be composed of four\ntypical layouts (independent with each other):\n(L.1) Header-Single-Value. The simplest layout pairs a header\nùêª = ‚Ñé1 with a single content value ùëâ = ùë£1, arranged either vertically\nor horizontal: ùëáùë° = { ùêª = ‚Ñé1, ùëâ = ùë£1, ùêª ‚Üí ùëâ } . For instance, in\nTable 1, the atomic header ‚ÄúName‚Äù is associated with the single value\n‚ÄúAlbert‚Äù, demonstrating this minimal semi-structured table layout.\n(L.2) Header-Multiple-Values. A more prevalent structure in semi-\nstructured table is an atomic or hierarchical header ùêª = ‚Ñé1 accom-\npanied by a list of values ùëâ = [ ùë£1, ùë£2, . . . , ùë£ùëõ ]. We represent this as\nùëáùë° = { ùêª = ‚Ñé1, ùëâ = [ ùë£1, ùë£2, . . . , ùë£ùëõ ], ùêª ‚Üí ùëâ } . For instance, Table 1\npresents an example in which the atomic header ‚ÄúName‚Äù corresponds\nto multiple values, specifically the list [ ‚ÄúAlbert‚Äù, ‚ÄúTim‚Äù, ‚ÄúJack‚Äù ].\n(L.3) Orthogonal-Subtables. An orthogonal-subtable layout con-\nsists of two or more subtables, whose top-level headers appear\nat the same level, either horizontally or vertically. The subtables",
            "content": "L1: HeaderSingle-value L2: HeaderMultiple-Values Table 1: Semi-Structured Table Layouts Cells marked in blue in examples represent table headers. Formal Representation Layout ùëáùë° = { ùêª = ‚Ñé1, ùëâ = ùë£1, ùêª ùëâ } . ùëáùë° = { ùêª = ‚Ñé1, ùëâ = [ ùë£1, ùë£2, . . . , ùë£ùëõ ], ùêª ùëâ } . (cid:110) ùëá = ùëá = [ ùëá1,ùëá2, . . . ,ùëáùëõ ], ùêªùëñ ùëáùëñ, 1 ùëñ ùëõ (cid:110) ùêª = [ ùêª1, ùêª2, . . . , ùêªùëõ ], L3: Orthogonal Tables Example (cid:111) . L4: HeaderOrthogonal-Tables ùêª = ùêª p, ùëá = [ ùëá1,ùëá2, . . . ,ùëáùëõ ], ùëáùë° = ùêª ùëáùëñ, 1 ùëñ ùëõ each contain the same number of rows, forming parallel structure, while the content values of these subtables are weakly related or unrelated (e.g., personal information for employees in two separate departments). Suppose we have ùëõ such subtables ùëá1, ùëá2, . . . , ùëáùëõ. We represent their orthogonal combination as ùëá = (cid:111) (cid:110) ùêª =[ ùêª1, ùêª2, . . . , ùêªùëõ ], ùëá =[ ùëá1,ùëá2, . . . ,ùëáùëõ ], ùêªùëñ ùëáùëñ, 1 ùëñ ùëõ . For instance, as demonstrated in Table 1, the three atomic-headersingle-value tables are combined in Orthogonal-Table layout. (L.4) Header-Orthogonal-Subtables. Header-Orthogonal-Subtables consists of an atomic or hierarchical header paired with one or more orthogonal-subtables ((L.3)), which means all orthogonal-subtables in (L.4) share the same header. Formally, let the grouped subtables be ùëá1,ùëá2, . . . ,ùëáùëõ. We represent (cid:111) this layout as ùëáùë° = . For instance, in Table 1, the header Info groups two orthogonal subtables (i.e., Name and Age subtables) that each follows the Header-Multiple-Value layout. (cid:110) ùêª =ùêª p,ùëá =[ ùëá1,ùëá2, . . . ,ùëáùëõ ], ùêª pùëáùëñ, 1 ùëñ ùëõ subtable ùëásub ùëá follows one or combination of the above layouts. Note that: (1) we do not consider irregular layouts, such as content values presented without corresponding headers [4]; (2) unlike structured tables, common semi-structured tables (e.g., Word tables, transaction records) are relatively small (e.g., tables with over 100 rows are already considered large); (3) we assume the tables are error-free, and issues such as table cleaning (e.g., missing value imputation [5]) are beyond the scope of this paper. Example 2.1. For the bottom-right semi-structured table in Figure 1, we can extract two orthogonal subtables sharing common header TD Tech (ùêø.4 ùêø.3). Within the Employee Info subtable, we can extract four header-multiple-values subtables (ùêø.3[ùêø21, . . . , ùêø24]). In this way, we can recursively traverse the entire structures through ùêø.4ùêø.3[ùêø.4{ùêø.11, . . . , ùêø.14}, ùêø.4ùêø.3[ùêø.2[1], . . . , ùêø.2[4]]]. Furthermore, the one-to-many relationships between these subtables motivate us to adopt tree-based strategy to represent semistructured tables (see Section 4)."
        },
        {
            "title": "2.2 Semi-Structured Table QA\nGiven a semi-structured table ùëá , the QA tasks aim to answer an\ninput question ùëÑ expressed in natural language based on ùëá [36].",
            "content": "Definition 1 (Semi-Structured Table QA). Let ùëá be semistructured table with multi-layered organization, and let ùëÑ be question that may reference one or multiple subtables in ùëá . SemiStructured Table QA is defined as mapping (ùëá , ùëÑ) ùê¥, where the answer ùê¥ is derived by identifying the subtables {ùëásub ùëásub ùëá } relevant to ùëÑ. 3 Figure 2: Error Distribution GPT-4o is evaluated on both HTML and structured (JSON) formats; ReAcTable (NL2SQL) converts semistructured tables into structured representations; while TableLLaVA utilizes LVM to process them as images. QA Tasks. Commonly, QA tasks in this problem often require understanding the layouts of target semi-structured tables. Here we showcase three typical QA tasks together with relevant layouts. (1) Numerical Computation. The question ùëÑ2 in Figure ?? requests the age of the oldest employee (Header-Multiple-Values in L.2). Correctly answering it involves (ùëñ) locating the target column Age within the hierarchical header Info, (ùëñùëñ) extracting all age values from the corresponding records, and (ùëñùëñùëñ) computing the maximum value from the retrieved data. Such QA requires accurate header identification and data retrieval, upon which we can easily apply basic computational functions to get accurate results. (2) Information Extraction. The question ùëÑ1 in Figure ?? relies on the Employee Info layout (Header-Orthogonal-Tables in L.4). To derive the correct answer 2, we must extract the records of employees meeting the rating condition and aggregate them. Failure to properly interpret the semantic relationship between the merged cell A+ and its associated employees may lead to wrong answer. (3) Summarization. The question ùëÑ3 in Figure ?? requests summary of the companys basic information (Orthogonal-Tables in L.3). Addressing this question involves two critical steps: (ùëñ) identifying and extracting the semantically relevant table segments corresponding to the question, and (ùëñùëñ) leveraging the reasoning capabilities of LLMs to generate coherent summary from the retrieved data. The key challenge lies in robustly interpreting complex table layouts to ensure precise alignment between the question intent and the extracted data to generate accurate summaries."
        },
        {
            "title": "2.3 Limitations of Existing Methods\nIn this section, we discuss the limitations and challenges of existing\napproaches that could potentially be adapted to semi-structured table\nQA. As shown in Table 2, these methods can be categorized based\non their table representation strategies (e.g., table serialization [37],\nHTML/JSON [19]) and the question comprehension techniques\n(e.g.,[33]). Figure 2 illustrates the error distribution when applying\ntypical methods to semi-structured table QA.\n(Limitation 1) Poor Table Layout Understanding. The fail-\nure in layout understanding indicates that the model incorrectly\ncaptures structural evidence, primarily due to the limitations of\nsemi-structured table representation. Among existing represen-\ntation methods, structured table representation (i.e., converting\nsemi-structured tables into fully structured formats) leads to major\nloss of layout information. Alternative approaches (e.g., HTML,\nJSON, Spreadsheet [37]) employ common serialization strategies\nthat partially preserve structural information in textual form. How-\never, due to the inherent one-dimensional nature of these formats,",
            "content": "4 LLMs face significant challenges in effectively interpreting complex table layouts. In contrast, image-based methods exhibit the lowest layout understanding error among wrong answers (31.58% compared to 35.48% for GPT-4o with structured input, 44.82% for GPT-4o with HTML input, and 61.67% for NL2Code agent), but own relatively low overall accuracy caused by the following two limitations. This observation motivates us to design proper semistructured table representation method that simultaneously stores the structural information and content of semi-structured tables. (Limitation 2) Inaccurate Table Data Retrieval. As shown in Figure 2, data retrieval errors constitute substantial proportion of failures. Specifically, the four methods (i.e., GPT-4o with structured input, GPT-4o with HTML input, NL2Code agent, and visionlanguage model) exhibit retrieval error rates of 55.17%, 54.84%, 76.67%, and 61.84%, respectively, which are primarily due to their inability to identify question-relevant tabular data. Among these methods, GPT-4o achieves superior performance, attributable to its advanced contextual comprehension capabilities. In contrast, the agent-based approach, which operates on structured table representations using external tools, incurs significant information loss during structural transformation. We observe that (1) these methods lack robust data retrieval mechanisms, and (2) vanilla LLMs struggle to accurately locate target table content, likely due to inherent limitations in semantic understanding. This indicates that significant improvements remain achievable in accurately locating and retrieving question-relevant content in semi-structured tables. (Limitation 3) Question Comprehension Errors. Question comprehension errors occur when model misinterprets the semantics of question and consequently retrieves incorrect answers from semi-structured tables, often due to inadequate integration of table layout and content. VLMs demonstrate the weakest performance, mainly due to their weak understanding of rich-text images. In contrast, both GPT-4o and agent-based methods demonstrate better performance, benefiting from the advanced reasoning capabilities of LLMs. However, their remaining errors are predominantly attributable to the failure to align the semantics of the input question with complex semi-structured table layouts, motivating us to design tailored question decomposition and table semantic alignment techniques for semi-structured tables."
        },
        {
            "title": "3 ST-Raptor Overview",
            "content": "Architecture. Figure 3 shows the architecture of ST-Raptor, which consists of four main modules: (1) Table2Tree converts the given semi-structured table into Hierarchical Orthogonal Tree (HOTree), which effectively represents the header / content relationships within the original table (see Section 4.2). Note that the Table2Tree module is utilized only once when handling multiple questions related to the same table. (2) Question2Pipeline transforms complex questions into simpler sub-questions, from which corresponding operation pipeline are generated for each sub-question (see Section 5.1). (3) AnswerGenerator then executes these operations to obtain intermediate results or produce the final answer (see Section 5.2). (4) AnswerVerifier adopts two-stage validation strategy. In the forward stage, it checks whether the execution results are non-empty and consistent with the question; otherwise, the operation is regenerated or terminated early. In the backward stage, Table 2: Comparison of Relevant Methods for Semi-Structured Table QA More stars indicates better performance."
        },
        {
            "title": "Representation",
            "content": "HTML/JSON/Spreadsheet HTML HTML/JSON JSON/Spreadsheet Structured Structured Structured Image HO-Tree Structure Information"
        },
        {
            "title": "Method",
            "content": "NL2SQL NL2Code LLM NL2Code NL2SQL / NL2Code LLM Agent Multimodal LLM ST-Raptor Structure Understanding - - - - - Data Retrieval - - Supported Table Scale - -"
        },
        {
            "title": "Main Challenge",
            "content": "Answer Accuracy Fail to operate the table. - Fail to operate the table. - Fail to understand table structure. Fail to understand table structure. Structural information loss. Structural information loss. Structural information loss. Fail to process big tables. Modeling and operation. into two parts: (1) Metadata, which provides high-level semantic abstraction (e.g., headers), and (2) Data, which contains the actual content values (e.g., the header sex associated with [female, male, female]). Both metadata and data components may exhibit hierarchical and orthogonal structures. Given this inherent one-tomany organization, we model them using trees: one for metadata and one for data. In each, nodes store metadata or content values, while edges encode containment or orthogonal relationships. Definition 2 (Hierarchical Orthogonal Tree (ùêªùëÇ-ùëáùëüùëíùëí)). Given semi-structured table ùëá , we model ùëá into HO-Tree that links the metadata with corresponding content values by pointing the leaf node in the ùëÄùëá ùëüùëíùëí to each level of ùêµùëá ùëüùëíùëí, representing the association between metadata and the corresponding table column: (1) Meta Tree (ùëÄùëá ùëüùëíùëí). It represents the structural information and content of the tables metadata. Each path from the root to leaf corresponds to an abstract description of specific column. Nodes in ùëÄùëá ùëüùëíùëí are denoted as ùëÄùëÅùëúùëëùëí; (2) Body Tree (ùêµùëá ùëüùëíùëí). It represents the structural information and content of the table body. Each node corresponds to single cell value, each path from the root to leaf represents row, and each level of the tree corresponds to columnaligned with path in ùëÄùëáùëüùëíùëí. Nodes in ùêµùëá ùëüùëíùëí are denoted as ùêµùëÅùëúùëëùëí. single ùêªùëÇ-ùëá ùëüùëíùëí is represented as ùëá = {ùëÄùëá ùëüùëíùëí = ùëÄ, ùêµùëáùëüùëíùëí = ùêµ, ùëÄ ùêµ}. Within semi-structured table, collection of such ùêªùëÇ-ùëá ùëüùëíùëíùë† may exist, each comprising an ùëÄùëá ùëüùëíùëí and ùêµùëáùëüùëíùëí. Moreover, hierarchical containment may exist among these trees, where one ùêªùëÇ-ùëá ùëüùëíùëí can serve as the value of node within another ùêµùëáùëüùëíùëí. Example 4.1. The right part of Figure 4 illustrates an example of HO-Tree, where TD Tech serves as the top-level cell. This cell is stored in single-node ùëÄùëá ùëüùëíùëí, which points to the remaining structure stored in ùêªùëÇ-ùëá ùëüùëíùëí. The subsequent ùëÄùëá ùëüùëíùëí contains two ùëÄùëÅùëúùëëùëí instances (i.e., Basic Info and Employee Info), each referencing distinct ùêµùëÅùëúùëëùëí, with each ùêµùëÅùëúùëëùëí storing sub ùêªùëÇ-ùëáùëüùëíùëí."
        },
        {
            "title": "4.2 HO-Tree Construction\nBased on the definition, constructing an HO-Tree from a given semi-\nstructured table requires precisely identifying (1) meta-information\n(table headers), (2) content values, as well as (3) the relationships\nbetween subtables. Two main challenges remain. First, headers and\ncontent cells can appear in arbitrary positions, making it difficult to\ndistinguish between them. Second, understanding the nested or con-\ntainment relationships among these cells is non-trivial, especially\ngiven the flexible and irregular structure of semi-structured tables.\nThus, next we introduce the detailed steps in HO-Tree construction.",
            "content": "5 Figure 3: The ST-Raptor Architecture. similar questions are generated from the output, and their similarity to the original question is used to score answers reliability (see Section 6). System Workflow. When new semi-structured table and its associated questions arrive, Table2Tree first preprocesses the table into an HO-Tree and serializes the object into local file. The Question2Pipeline then decomposes each question into subquestions and iteratively interacts with the AnswerGenerator to generate answers for each subquestion. The answer to the final subquestion serves as the question answer. The AnswerVerifier is involved throughout each step of operation execution, identifying and discarding incorrect intermediate results, based on which ST-Raptor iterates until generating correct answer."
        },
        {
            "title": "Representation",
            "content": "As discussed in Section 2.1, semi-structured tables consist of complex combinations of basic table layouts (e.g., hierarchical headers with nested subtables). Accurately performing question answering over such tables remains challenging even for advanced LLMs like GPT4o (see Section 2.3). Thus, in this section, we study how to effectively represent the layout relationships within semi-structured tables to enable accurate question answering."
        },
        {
            "title": "4.1 Hierarchical Orthogonal Tree\nFollowing the recursive definition in Section 2.1, a semi-structured\ntable composed of layouts ùêø.1‚Äìùêø.4 (Table 1) can be decomposed",
            "content": "Figure 4: An example of constructing three-level nested Hierarchical Orthogonal Tree (ùêªùëÇ-ùëá ùëüùëíùëí) Different shades of blue highlight the tables nested levels. For example, in the bottom-right table, the metadata, marked in dark blue, is constructed top-down into tree of depth two, while the unshaded data section structured into left-to-right tree of depth five. Table 3: Table of Atomic Operations. ùê∂ùêª ùêø (ùëâ ) ùêπùê¥ùëá (ùëâ ) ùê∏ùëãùëá (ùëâ1, ùëâ2 ) Operation Formal Representation Description Children Father Value Condition ùê∂ùëúùëõùëë (ùê∑, ùêπùë¢ùëõùëê ) Calculation ùëÄùëéùë°‚Ñé (ùê∑, ùêπùë¢ùëõùëê ) Compare Execute Align Reason ùê∂ùëöùëù (ùê∑1, ùê∑2, ùêπùë¢ùëõùëê ) ùêπùëúùëüùëíùëéùëê‚Ñé (ùê∑, ùêπùë¢ùëõùëê ) ùê¥ùëôùëñùëîùëõ (ùëÉ, ùêªùëÇ-ùëá ùëüùëíùëí ) ùëÖùëíùëé (ùëÑ, ùê∑ ) Get children based on the given value. Get Father based on the given value. Get the cross of two values. Filter values based on the function. Calculate result based on the function. Compare values based on the function. Apply function to the given values. Operation-Table alignment. Reasoning based on LLMs. Algorithm 1: HO-Tree Construction (HOTC) Input: Semi-Structured Table ùëá Output: Extracted HO-Tree ùêªùëÇùëáùëüùëíùëí 1 ùëÄùëíùë°ùëéùêºùëõùëì ùëú ùëÄùëíùë°ùëéùêºùëõùëì ùëúùê∑ùëíùë°ùëíùëêùë° (ùëá ); 2 ùëáùëôùëñùë†ùë° ùëá ùëéùëèùëôùëíùëÉùëéùëüùë° (ùëá , ùëÄùëíùë°ùëéùêºùëõùëì ùëú); 3 ùêªùëÇùëá ùëüùëíùëíùëôùëñùë†ùë° []; 4 for ùëáùë†ùë¢ùëè in ùëáùëôùëñùë†ùë° do 5 switch ùë°ùë¶ùëùùëí (ùëáùë†ùë¢ùëè ) do case ùêø1, ùêø2, ùêø4 do 6 7 9 ùêªùëÇùëá ùëüùëíùëíùëôùëñùë†ùë° .ùëùùë¢ùë†‚Ñé_ùëèùëéùëêùëò (ùê∂ùëúùëõùë†ùëáùëüùëíùëí (ùëáùë†ùë¢ùëè )); case ùêø3 do ùêªùëÇùëá ùëüùëíùëíùëôùëñùë†ùë° .ùëùùë¢ùë†‚Ñé_ùëèùëéùëêùëò (ùêªùëÇùëáùê∂ (ùëáùë†ùë¢ùëè )); end 10 11 end 12 return ùê∂ùëúùëõùë†ùëáùëüùëíùëí (ùêªùëÇùëáùëüùëíùëíùëôùëñùë†ùë° ); To overcome these limitations, we adopt hybrid method that combines rule-based matching with LLM-based reasoning. As shown in Figure 4, given semi-structured table in Excel format, we first convert it to HTML, render it using headless browser, and capture high-resolution screenshot as input image for the VLM. Then, we prompt the VLM to output all possible keys that would be present in JSON-formatted representation of the table as the candidate meta-information cell values. Subsequently, we calculate similarity scores between candidates and all table cells using embeddingbased similarity metrics. Cells exceeding predefined threshold are identified as meta-information, and their positions guide the subsequent table partitioning process."
        },
        {
            "title": "4.2.2 Table Partition Principles",
            "content": "We introduce three principles to guide the HO-Tree construction process by interpreting different layouts in semi-structured tables: (Principle 1) Top-Level Header Identification. If merged cell spans an entire row or column, it is treated as header in HeaderOrthogonal-Tables layout (ùêø.4), and adjacent cells (below or to the right) are interpreted as subtable. (Principle 2) Header-Content Differentiation. When both topaligned and left-aligned headers are present, the one with more cells is selected to construct the ùëÄùëá ùëüùëíùëí, while the other is integrated into the ùêµùëá ùëüùëíùëí. (Principle 3) Orthogonal Table Identification. When OrthogonalTables (ùêø.3) are detected, we segment them and process each subtable recursively and sequentially. As illustrated in Figure 4, applying these principles enables table partitioning based on meta-information locations and the recursive construction of HO-Trees for semi-structured tables."
        },
        {
            "title": "4.2.3 DFS-based Tree Model Construction",
            "content": "Unlike structured tables with fixed schemas, semi-structured tables often exhibit complex and irregular nesting, posing challenges for meta-information extraction. Rule-based approaches fail to capture such nuances, especially when similar structural patterns represent different semantics. Serializing tables (e.g., as images or structured formats) allows prompting models for metadata extraction or format generation. However, LLMs trained on 1D text struggle with 2D tables [23], often exhibiting issues like hallucination and the Lost in the Middle effect [27], undermining consistency and reliability. An HO-Tree is then constructed for each subtable according to its identified layout type: for layouts ùêø.1 and ùêø.2, the HO-Tree is built directly; otherwise, recursive DFS is applied. For example, in Figure 4, the vertically structured semi-structured tables demonstrate top-down alignment of metadata and content. Based on the detected metadata, the table is partitioned into list of subtables following predefined principles. An HO-Tree is then constructed for each subtable according to its identified layout type: for layouts ùêø.1 and ùêø.2, the HO-Tree is built directly; 6 otherwise, recursive DFS is applied. Take the vertically structured semi-structured tables where metadata and content are aligned topdown in Figure 4 as an example: (1) In the ùëÄùëáùëüùëíùëí, each root-toleaf path represents column, and the tree grows vertically to reflect vertical relationships among metadata. (2) In the ùêµùëá ùëüùëíùëí, each path represents row, with horizontally aligned attributes, so the tree grows horizontally. Each leaf node in the ùëÄùëáùëüùëíùëí points to corresponding level in the ùêµùëá ùëüùëíùëí, linking metadata to the associated content column. During DFS backtracking, we model Orthogonal-Tables and Header-Orthogonal-Tables layouts (i.e., ùêø.3 and ùêø.4). In such cases, node in the ùêµùëáùëüùëíùëí may recursively contain another HO-Tree as its value. Algorithm 1 outlines the HO-Tree construction process. VLMs identify table headers via the ùëÄùëíùë°ùëéùêºùëõùëì ùëúùê∑ùëíùë°ùëíùëêùë° function (Section 4.2.1). Using the extracted metadata and predefined principles, the table is partitioned into subtables through the ùëá ùëéùëèùëôùëíùëÉùëéùëüùë° function (Section 4.2.2). To address structural complexity, each subtable is transformed into an HO-Tree via the ùê∂ùëúùëõùë†ùëáùëüùëíùëí function (Section 4.2.3) and recursively merged using depth-first search to reconstruct the full structure of the original semi-structured table. Example 4.2. Figure 4 illustrates the construction of three-level nested HO-Tree. The Basic Info subtable is initially misidentified by the VLM as Basic Information and corrected by alignment. Metadata is used to identify the ùêø.4 ùêø.3 layout, guiding the partition of the Basic Info subtable into ùëÄùëÅùëúùëëùëí and sub-ùêªùëÇ-ùëá ùëüùëíùëí (constructing ùêø.4). Finally, the leaf nodes of the ùëÄùëáùëüùëíùëí in subtree link to corresponding levels of the ùêµùëá ùëüùëíùëí (constructing ùêø.3). 5 Pipeline-based Question Answering With the HO-Tree model in place, it poses three key challenges in question answering over semi-structured tables. First, such questions often require multi-hop reasoning rather than one-shot retrieval, and unlike NL2SQL tasks, lack standardized operation set for pipeline construction. Second, answering necessitates hybrid traversal strategies (e.g., left-to-right, top-down, bottom-up) to locate relevant content. Third, the large number of cells in some semistructured tables complicates the precise identification of questionrelevant information. To address these challenges, we propose pipeline-based QA strategy centered on set of tree-specific operations that cover most QA scenarios: (1) question decomposition method for complex multi-hop queries, (2) an operation generation mechanism with parameter-content alignment, and (3) column-type-aware tagging mechanism that annotates columns based on data characteristics, enabling efficient and accurate retrieval from large, complex tables."
        },
        {
            "title": "5.1 Basic Operations over HO-Tree\nWe design a suite of atomic operations to enable structured and\ninterpretable QA over the ùêªùëÇ-ùëáùëüùëíùëí. These operations support both\nprecise tree traversal and auxiliary tasks correspond to common\nsemi-structured table sub-tasks. The operations fall into four cate-\ngories: (1) Data Retrieval Operations, which retrieve relevant values\nfrom the tree; (2) Data Manipulation Operations, which process or\ntransform retrieved data; (3) Alignment Operations, which align\noperation parameters with table content; (4) Semantic Reasoning\nOperations, which invoke LLMs for contextual inference.",
            "content": "7 For any user question, we first decompose it into one or more sub-questions, each resolved through sequence of operations to retrieve relevant information or derive the answer. Ideally, the generated operation pipeline includes: (1) retrieval to collect nonredundant data, (2) manipulation to structure the data into model comprehensible form, (3) alignment to ensure question-content alignment, and (4) reasoning to produce the final answer. In worstcase scenarios, the model may retrieve nearly the entire table and rely heavily on reasoning, highlighting the need for fine-grained, modular operation design. Data Retrieval Operation. These operations are responsible for extracting relevant table content from the ùêªùëÇ-ùëá ùëüùëíùëí based on arguments derived from the user question. Children Retrieval (ùê∂ùêªùêø(ùëâ )) This operation retrieves all successor nodes of any node whose value matches ùëâ . If multiple such nodes exist, each set of successors is returned separately. Use the ùêªùëÇ-ùëá ùëüùëíùëí in Figure 4 as an example, ùê∂ùêªùêø(ùêµùëéùë†ùëñùëê ùêºùëõùëì ùëú) returns the ùêªùëÇ-ùëá ùëüùëíùëí containing the company information. Father Retrieval (ùêπùê¥ùëá (ùëâ )) This operation is used to obtain the set of ancestor nodes of given tree node. For instance, in Figure 4, ùêπùê¥ùëá (ùê∑ùëíùëùùëéùëüùë°ùëöùëíùëõùë°) would return the ùêªùëÇ-ùëá ùëüùëíùëí containing the employee information. Value Retrieval (ùê∏ùëãùëá (ùëâ1, ùëâ2)) This operation is used to find nodes in certain layer of ùêµùëá ùëüùëíùëí pointed to by ùëÄùëá ùëüùëíùëí leaf node, while giving them common ancestor ùêµùëÅùëúùëëùëí as filtering criterion. Specifically, one of ùëâ1 and ùëâ2 needs to be ùëÄùëÅùëúùëëùëí value, and the other needs to be ùêµùëÅùëúùëëùëí value. Assume that ùëâ1 is the ùëÄùëÅùëúùëëùëí value and ùëâ2 is the ùêµùëÅùëúùëëùëí value, the operation returns set of node values that satisfy the following: (1) The ùêµùëÅùëúùëëùëí is in the ùëÄùëáùëüùëíùëí column ùëâ1 (2) ùêµùëÅùëúùëëùëí with value ùëâ2 is an ancestor of the ùêµùëÅùëúùëëùëí in ùëÄùëá ùëüùëíùëí column. Data Manipulation Operation is used to perform specific operations on the data, including filtering based on conditions, performing calculations, and making comparisons. Condition (ùê∂ùëúùëõùëë (ùê∑, ùêπùë¢ùëõùëê)) filters data set ùê∑ using predicate function ùêπùë¢ùëõùëê and returns the filtered values or new ùêªùëÇ-ùëáùëüùëíùëí. For instance, ùê∂ùëúùëõùëëùëñùë°ùëñùëúùëõ(ùê∏ùëãùëá (ùêøùëñùëôùë¶, ùê∫ùëüùëéùëëùëí), ùëëùëí ùëì (ùë•) : ùëüùëíùë°ùë¢ùëüùëõ ùë• < 60) retrieves Lilys grades that are less than 60. Calculation (ùëÄùëéùë°‚Ñé(ùê∑, ùêπùë¢ùëõùëê)) applies numerical computation function ùêπùë¢ùëõùëê over data set ùê∑. For instance, ùëÄùëéùë°‚Ñé(ùê∂ùêªùêø(ùëÉùëüùëñùëêùëí), ùëëùëí ùëì (ùë•) : ùëüùëíùë°ùë¢ùëüùëõ ùë†ùë¢ùëö(ùë•)) returns the sum of the column Price. Compare (ùê∂ùëöùëù (ùê∑1, ùê∑2, ùêπùë¢ùëõùëê)) compares two data sets ùê∑1 and ùê∑2 using function ùêπùë¢ùëõùëê, and returns the boolean result. For instance, ùê∂ùëúùëöùëùùëéùëüùëí (ùê∏ùëãùëá (ùêøùëñùëôùë¶, ùê∫ùëüùëéùëëùëí), ùê∏ùëãùëá (ùê∂ùëñùëõùëëùë¶, ùê∫ùëüùëéùëëùëí), ùëëùëí ùëì (ùë•1, ùë•2) : ùëüùëíùë°ùë¢ùëüùëõ ùë•1 > ùë•2 returns the truth value of the statement Lilys grade is greater than Cindys. Execute (ùêπùëúùëüùëíùëéùëê‚Ñé(ùê∑, ùêπùë¢ùëõùëê)) applies function ùêπùë¢ùëõùëê to each element in data set ùê∑ and returns the resulting set. For instance, ùêπùëúùëüùëíùëéùëê‚Ñé(ùê∏ùëãùëá (ùêøùëñùëôùë¶, ùê∫ùëüùëéùëëùëí), ùëëùëí ùëì (ùë•) ùëüùëíùë°ùë¢ùëüùëõ ùë•10) retrieves all Lilys grades and returns the values after minus ten. Alignment Operation aims to ensure consistency between operation parameters and the content of the ùêªùëÇ-ùëá ùëüùëíùëí. Align (ùê¥ùëôùëñùëîùëõ(ùëÉ, ùêªùëÇ-ùëá ùëüùëíùëí)) This operation aligns the parameters ùëÉ in given operation with the nodes in the ùêªùëÇ-ùëáùëüùëíùëí using Figure 5: Question Decomposition and Pipeline Generation. an embedding-based similarity model [39]. The process involves computing embeddings for the parameters and table content: (1) ùê∏ùëé = ùê∏ùëöùëèùëíùëë (ùëé1, ùëé2, . . . , ùëéùëõ) ùê∏ùëê = ùê∏ùëöùëèùëíùëë (ùëê1, ùëê2, . . . , ùëêùëö) ùëÜùëñùëöùëÄùëéùë°ùëüùëñùë• = ùê∂ùëúùë†ùëÜùëñùëö(ùê∏ùëé, ùê∏ùëê ) (3) where ùëéùëñ represents the ùëñ-th parameter and ùëê ùëó denotes the content of the ùëó-th node in the tree, with ùëö representing the total number of nodes. Cosine similarity (ùê∂ùëúùë†ùëÜùëñùëö) is used to identify the most semantically aligned table node for each parameter, which is then used for downstream operations. (2) Example 5.1. For the operation ùê∂ùêªùêø(ùêºùëëùëíùëõùë°ùëñ ùëì ùëñùëêùëéùë°ùëñùëúùëõ), if the metainformation of the table is [ID, Name, Age], the alignment operation ùê¥ùëôùëñùëîùëõ(ùêºùëëùëíùëõùë°ùëñ ùëì ùëñùëêùëéùë°ùëñùëúùëõ, ùêªùëÇùëá ùëüùëíùëí) attempts to output ID to align the operation parameter to the table content. Semantic Reasoning Operation leverages LLMs for high-level reasoning over retrieved data. Reason (ùëÖùëíùëé(ùëÑ, ùê∑)) This operation takes the original question ùëÑ and the data ùê∑ obtained from prior operations, and uses an LLM to generate the final answer. In cases where the question is decomposed into multiple sub-questions, semantic reasoning can also be used to aggregate intermediate answers into final response."
        },
        {
            "title": "Generation",
            "content": "Figure 5 illustrates the overall process of question decomposition, operation generation, and step-by-step data retrieval via both topdown and bottom-up strategies. Step 1: Question Decomposition. As shown in Figure 5, upon receiving user question, we first utilize the semantic understanding capabilities of an LLM to decompose complex multi-hop questions into multiple simpler, single-step sub-questions. Specifically, we prompt the LLM with the input question, sampled table content as semantic supplements, and example decomposition cases that are dynamically retrieved based on question similarity to generate sub-questions. These sub-questions are often interdependent: some can be directly resolved using the ùêªùëÇ-ùëáùëüùëíùëí, while others rely on intermediate results produced by earlier sub-questions. For example, question such as What is the total salary for 2021 and 2022? 8 can be decomposed into three sub-questions: (1) retrieve the 2021 salary, (2) retrieve the 2022 salary, and (3) compute the sum of the two. For table operation generation, we prompt the LLMs with predefined set of operations and illustrative examples. Step 2: Relevant Data Retrieval. Each sub-question is then answered independently through combination of top-down and bottom-up retrieval. ST-Raptor always starts with top-down retrieval and switches to bottom-up retrieval when the former fails. The left section of Figure 5 shows this iterative process. For each sub-question, the LLM first generates an operation statement based on the sub-question content and the meta-information extracted from the ùêªùëÇ-ùëá ùëüùëíùëí. Executing this operation yields intermediate results (i.e., sub-trees), which are then used to inform subsequent operation generation. We employ the Align operation for two purposes: (1) OperationTable Alignment, applied upon each operations generation to align sub-questions with the corresponding table content, and (2) Relevant Content Match, invoked when the number of data nodes is large, to extract key entities from the question and constrain the search space within the HO-Tree. Step 3: Answer Generation Once the relevant sub-data is retrieved, each sub-question is resolved via the Reason operation, and all sub-answers are aggregated to produce the final answer."
        },
        {
            "title": "5.3 Relatively Large Table QA Enhancement\nThe abundance of available BNodes poses challenges in selecting\naccurate operation parameters. To address this, we introduce a data\ngrouping strategy that leverages inherent structural and semantic\nfeatures, combining top-down grouping based on data characteris-\ntics with parallel grouping via tree structure traversal.\n1. Characteristic-based Grouping. The top-down grouping pro-\ncess is performed during HO-Tree construction, clustering data\nbased on column characteristics. As shown in Figure 6, column\nvalues are classified by data type (e.g., Numeric, Datetime, Unstruc-\ntured String) and then further categorized as follows: Discrete ‚Äî\ncolumns with a limited set of values (e.g., grades [‚ÄòA‚Äô, ‚ÄòB‚Äô, ‚ÄòC‚Äô, ‚ÄòD‚Äô]\nor binary options [Yes, No]); Continuous ‚Äî columns with nu-\nmeric values spanning a range (e.g., height or temperature); and\nUnstructured ‚Äî columns with free-form text (e.g., comments or\ndescriptions). A rule-based classifier performs this categorization",
            "content": "when asking for company information and the given data only contain employee information, we stop the process). This involves directly matching generated parameters against table cells to ensure semantic and syntactic alignment. After executing each operation over the HO-Tree, the general LLM evaluates whether the resulting data sufficiently answers the corresponding sub-question. If the result is found to be inadequate, new operation statement is generated to continue the reasoning process. Notably, real-world queries often lack sufficient information to be answered directly from the table, which may cause the model to hallucinate incorrect answers. To mitigate this issue, the forward verification mechanism allows the model to halt the pipeline and return an unanswerable signal when it detects that the answer cannot be reliably inferred. Furthermore, when operation statements are found to misalign with the table content, the system triggers regeneration process to refine and correct the statements, thereby improving both the data retrieval accuracy and efficiency. Backward Verification. In the second stage, we evaluate the correctness of the generated operation pipeline by verifying it against alternative reasoning paths. Notably, different questions over the same semi-structured table may yield identical answers via distinct yet similar pipelines (e.g., Who is the highest-paid employee? vs. Who leads the technical department?). Leveraging this property, we perform backward verification by generating alternative questions with the same answer, deriving corresponding pipelines, and measuring their similarity to the original. The average similarity serves as an implicit indicator of pipeline and answer reliability. Specifically, we employ few-shot learning, retrieving questionsimilar examples to guide the generation of alternative questions to solve the problem that questions with the same answers may show tremendous difference in their operation pipeline."
        },
        {
            "title": "7.1 Experiment Setup",
            "content": "LLMs. We use InterVL2.5 26B [11] as the vision language model, Deepseek-V3 [14] as the general LLM, and Multilingual-E5-Large [39] as the semantic embedding model. Evaluated Methods. The evaluated methods include: (1) NL2SQL. OpenSearch-SQL [41] employs dynamic few-shot learning strategy (Query-CoT-SQL) and introduces an SQL-Like intermediate language to optimize reasoning chains. (2) Foundation Model. GPT-4o [30], the cutting-edge LLM developed by OpenAI, and DeepSeekV3 [14], strong Mixture-of-Experts LLM developed by DeepSeek-AI is evaluated. (3) Fine-tuning based Methods. TableLLaMA [44] is fine-tuned version of LLaMA-2 7B tailored for tabular data processing across eight table-specific tasks. The model handles various table types, including Wikipedia tables and spreadsheets. TableLLM [45] is 13B-parameter LLM designed for tabular data manipulation tasks, which is fine-tuned on diverse mix of table-centric datasets in processing document tables and spreadsheets, making it well-suited for real-world office scenarios. (4) Agent based Methods. ReAcTable [46] is an agent-driven approach that integrates reasoning and action-based decision-making for table question answering. It iteratively generates operations, updates the table, and constructs reasoning chain as proxy for Figure 6: Characteristic-based Grouping Mechanism to reduce complexity in tables with numerous similar cells. With clearly defined rules, it achieves near-perfect accuracy. Each category is then grouped using tailored strategies: (1) Discrete values are clustered by exact matches (e.g., students with Grade A); (2) Continuous values are sorted and partitioned into fixed intervals to preserve numeric order (e.g., stress levels by range); and (3) Unstructured values are grouped via embeddingbased clustering to capture semantic similarity. As shown in Figure 6, column with values like Grade (A+, A, A-, ...) is classified as Discrete, allowing identical values to be grouped. Queries such as fetch all students with Grade can then be resolved efficiently within the corresponding group. 2. Group-based Data Retrieval. The parallel-direction grouping leverages structural relationships within the ùêªùëÇ-ùëáùëüùëíùëí. After locating the target node(s) based on content, the tree is traversed to retrieve related information by: (1) searching upwards for ancestor nodes and (2) searching downwards for descendant nodes. This bi-directional traversal allows for the reconstruction of minimal sub-tree that contextualizes the target node, revealing the complete information associated with the same entity (e.g., retrieving all attributes of specific student or transaction)."
        },
        {
            "title": "6 Two-Stage QA Verification\nRobust validation mechanisms are essential for the reliability of\nsemi-structured table QA, particularly given that existing solutions\n(e.g., NL2SQL [24, 41]) often lack comprehensive answer verifica-\ntion. The main challenge arises from the fact that, different from\nstructured table QA, the retrieved result cells in semi-structured\ntable QA often exhibit complex layouts and may be derived through\nmulti-step lookup processes, which is tricky for general LLMs to\nverify. To address this problem, we propose a two-stage verification\nframework that integrates both forward and backward validation\nto enhance the robustness and trustworthiness of the final answers.\nForward Verification. The first stage focuses on validating the\ncorrectness of intermediate operations and execution results dur-\ning the QA process. Specifically, at each step of generating and\nexecuting operations, we verify whether the parameters produced\nby LLM are consistent with the actual contents of the table (e.g.,",
            "content": "9 Table 4: Characteristics of SSTQA Benchmark."
        },
        {
            "title": "WikiTQ\nTEMPTABQA\nINFOTABS\nSSTQA",
            "content": "1.2970 2.0000 2.0000 2.5196 0.0091 0.1780 0.0548 0.0544 Cell Count 178.3564 44.8350 23.6683 147.4608 Avg. length of Contents 1.9568 3.6696 2.0769 2.7287 intermediate thought processes through prompting LLMs and incontext learning. TAT-LLM [48] extracts relevant segments from the context, generates logical rules or equations, and then applies these rules or executes the equations to derive the final answer through LLM prompting. (5) VLM based Methods. TableLLaVA [47] extends the training of LLaVA-7B/13B on 150K table recognition samples, allowing the model to align table structures and elements with textual modality. We choose the 7B version in our experiment. mPLUG-DocOwl1.5 [20] is fine-tuned VLM with 8B parameters. It incorporates spatial-aware vision-to-text module designed to represent high-resolution, text-rich images while preserving structural information and reducing the length of visual features. Input Formats. NL2SQL and agent-based methods take structured tables as input, typically stored in databases or CSV files. Finetuning-based approaches operate on structured tables in Markdown format, while VLM-based methods accept table images as input. Foundation models generally utilize the HTML representation of tables. ST-Raptor currently supports Excel as the input format and is compatible with all lossless table representations like HTML. Benchmarks. We evaluate nine baselines and ST-Raptor on three benchmarks: (ùëñ) WikiTQ, featuring Wikipedia tables with complex natural language questions, (ùëñùëñ) TempTabQA, targeting on temporal question answering over semi-structured tables, and (ùëñùëñùëñ) SSTQA, our proposed dataset detailed in Section 7.2. Since the table formats in other datasets do not fully adhere to the definition of semistructured tables, we select subset of tables that meet the criteria and denote them as WikiTQ-ST and TempTabQA-ST, respectively."
        },
        {
            "title": "7.2 SSTQA Benchmark\nExisting semi-structured table datasets face two key limitations: (1)\nthey consist of small, structurally simple tables that fail to evaluate\na model‚Äôs capacity to comprehend complex semi-structured tables;\nand (2) their queries are misaligned with practical applications, lim-\niting real-world utility. For example, although WikiTQ [31] includes\na large number of semi-structured tables from Wikipedia, these\ntables typically exhibit simple layouts with merged cells and are\nconverted into structured formats as part of the benchmark prepro-\ncessing. Meanwhile, TempTabQA [17] consists solely of shallowly\nnested tables with fewer than five columns, lacking the structural\ncomplexity commonly observed in real-world datasets.",
            "content": "To fill the gap, we introduce SSTQA, dataset specifically designed to evaluate models ability to conduct Semi-Structured Table Question Answering task in real-world scenarios. As shown in Table 4, WikiTQ has the highest cell count but the shallowest nesting depth. In contrast, SSTQA exhibits the deepest nesting and the relatively large table size among the existing semi-structured table benchmarks. Data Collection. The 102 tables in SSTQA are carefully curated from over 2031 real-world tables coverage across 19 representative real scenarios (e.g., administrative and financial management) by 10 Table 5: Overall Performance Comparison."
        },
        {
            "title": "Methods",
            "content": "OpenSearch-SQL [41] TableLLaMA [44] TableLLM [45] ReAcTable [46] TAT-LLM [48] TableLLaVA [47] mplug-DocOwl1.5 [20] GPT-4o [30] DeepSeekV3 [14] ST-Raptor (Ours) WikiTQ-ST TempTabQA-ST Acc 38.89 35.01 62.40 68.00 23.21 20.41 39.8 60.71 69.64 71.17 Acc 4.76 32.70 9.13 35.88 61.86 6.91 39.80 74.83 63.81 77.59 SSTQA ROUGE-L Acc 24.00 40.39 7.84 37.24 39.78 9.52 29.65 66.45 63.22 72.39 23.87 26.71 2.93 7.49 19.26 5.92 28.43 43.86 46.17 52. considering tables featuring semi-structured formats, such as nested cells, multi-row/column headers, irregular layouts, which ensures the representativeness both in structure and information. For question-answer pair generation, we employ two-stage approach. First, we augment the question set by extracting information from tables as answers, then generating corresponding questions to enhance QA pair alignment. Second, we sample question templates and prompt LLM to generate open-ended questionanswer pairs based on the table and template. To ensure data quality, we implement two-step verification process. Initially, LLM validates the alignment between tables, queries, and answers. This is followed by manual inspection to verify answer correctness by 11 professional annotators, which results in high-quality dataset of 764 meticulously curated table-based QA pairs. Table Complexity. We categorize table difficulty based on weighted combination of three key features: (i) nesting depth (0.5), (ii) structural irregularity, including the number of header rows and column spans (0.3), and (iii) average cell content length (0.2). After z-score normalization and feature aggregation, SSTQA tables are grouped into 59 simple, 33 medium, and 10 hard instances. Table + QA Complexity. Table-question difficulty varies with the combination of table structure and query complexity. Accordingly, we categorize Table+QA tasks into three levels: (i) simple, where answers can be directly retrieved from the table; (ii) medium, requiring logical inference or conditional operations; and (iii) hard, where answers are not explicitly present and demand semantic reasoning. We obtain 299 simple, 284 medium, and 178 hard cases. The corresponding experimental results are presented in section 7.3. Evaluation Metrics. We adopt two primary evaluation metrics: Answer Accuracy (Acc), following prior work [23, 44], and ROUGEL to accommodate summarization-style questions in SSTQA. To address the limitations of exact string matching, we further employ general-purpose LLMs to compare model predictions and groundtruth answers, enabling more nuanced evaluation."
        },
        {
            "title": "7.3 Overall Performance Comparison\nWe conduct a comprehensive evaluation of ST-Raptor against nine\nstate-of-the-art table question answering (QA) methods, spanning\nfive technical paradigms, i.e., NL2SQL, fine-tuning based methods,\nagent based methods, VLM based methods, and foundation LLMs.\nThe general accuracy of these methods across three semi-structured\ntable QA benchmarks is shown in Table 5. Then, we further classify\nthe difficulty of semi-structured tables into Simple, Medium, and\nHard tiers, and visualize the accuracy variation upon different table\ndifficulties, which is shown in Figure 7.",
            "content": "Meanwhile, the experimental results highlight significant performance variations among the methods. NL2SQL-based approaches perform the worst on the SSTQA and TempTabQA dataset but achieve better results on WikiTQ, as the SQL generation paradigm is ill-suited for non-relational data, making it ineffective for semistructured tables. TableLLM ranks second-lowest on SSTQA due to two main limitations: (1) its training is restricted to structured datasets, reducing its generalizability to semi-structured formats, and (2) it struggles with large-scale tables and complex layouts due to limited context length and one-dimensional semantic reasoning. Its improved performance on WikiTQ can be attributed to task-specific fine-tuning on this dataset. Agent-based methods perform better on SSTQA due to their integration of external tools, but they fail to operate directly on semi-structured tables. The transformation of semi-structured data into structured formats results in the loss of critical layout information, reducing their effectiveness. TAT-LLM exhibits unexpectedly strong performance on the TempTabQA dataset. We attribute this to its fine-tuning on large volume of financial data, which shares similarities with the temporal question types prevalent in TempTabQA, thereby contributing to its effectiveness. Vision-language models excel at layout recognition through visual encoding but underperform in text-dense scenarios due to limited textual comprehension, especially in tables requiring strong semantic understanding. Foundation models, while not explicitly designed for table-related tasks, achieve the second-best performance on SSTQA dataset. This is attributed to their robust contextual reasoning and semantic interpretation abilities, enabling accurate answer inference especially when tabular layout understanding is secondary to interpreting textual content. Accuracy under Different Table Difficulties. We categorize tables in SSTQA benchmark into three levels of difficulty (i.e., Simple, Medium, Hard) by layout complexity and content length. Figure 7 presents the comparative performance evaluation across these difficulty levels. Three key observations emerge from our analysis. First, both ST-Raptor and foundation models exhibit progressively decreasing performance as table difficulty increases, underscoring the inherent challenges posed by complex layouts and largescale semi-structured tables. In contrast, other methods demonstrate only marginal performance variation across difficulty levels. We posit that these models primarily address questions with less table structure comprehension. Consequently, performance differences among them are largely driven by architectural variations rather than structural modeling capabilities. Second, although ST-Raptor shows modest performance decline on hard-level tables, it consistently outperforms all methods by substantial margin (e.g., exceeding the second-best model by over 20% on the SSTQA dataset). The reasons are three-fold: (1) the hierarchical HO-Tree representation, which facilitates efficient processing of large tables; (2) the question decomposition mechanism, which simplifies complex queries into tractable sub-questions; and (3) the operation-table alignment strategy, which ensures accurate and context-aware data retrieval. Third, performance differences across models are less pronounced. This can be attributed to three key factors: (1) the smaller table sizes, which reduce structural complexity; (2) the predominance of semantically driven questions that require less explicit layout Figure 7: Evaluation Results under Different Table Difficulty. Overall Accuracy. Experiments show that ST-Raptor consistently outperforms all nine baselines across the evaluated WikiTQST, TempTabQA-ST and SSTQA benchmarks. Table 5 shows ST-Raptor achieves the highest accuracy, exceeding the second-best method by 10.23% on SSTQA benchmark. This consistent outperformance can be attributed to three folds. First, ST-Raptor leverages the HO-Tree to represent semi-structured tables, enabling explicit structural modeling while decoupling layout understanding from question answering. This design allows the general-purpose LLM to operate without directly parsing complex table layouts. Instead, given JSON-formatted header information, ST-Raptor generates atomic operations and executes them on the HO-Tree for relevant data retrieval. In contrast, most methods, excluding vision-language models (VLMs) which can directly perceive layout information from table images, struggle to capture two-dimensional semantics using linear text representations. Second, ST-Raptor incorporates novel question decomposition mechanism that breaks complex queries into simpler sub-questions, followed by precise operation-table alignment. This improves both operation generation accuracy and execution reliability, thereby improving overall question answering performance. Third, ST-Raptor dynamically combines top-down and bottomup retrieval strategies based on question characteristics, enabling robust handling of diverse semi-structured table QA scenarios. When the top-down retrieval fails or question lacks explicit header references, bottom-up retrieval is employed. This flexible approach allows the system to effectively navigate complex table structures, outperforming other methods in challenging scenarios. Additionally, we observe performance increase (around 3%) even on the simple tables of WikiTQ-ST and TempTabQA-ST. For WikiTQ, where the majority of tables are fully structured, ST-Raptor outperforms the second-best model by around 2%. This modest improvement reflects ST-Raptors specialization for semi-structured tables with complex nested hierarchies. In contrast, on TempTabQA, where all tables are semi-structured but exhibit only shallow nesting and small sizes, ST-Raptor achieves around 3% improvement over the second-best approach. While our model could effectively models such structures, the overall retrieval pipeline remains relatively simple, limiting the performance gap over LLMs like Deepseek-V3 which can directly interpreting relevant information. Table 6: Analysis of Table + QA Difficulty on SSTQA. Table 7: Ablation Study on ST-Raptor Modules Methods (Acc) DeepseekV3 GPT-4o ST-Raptor Simple Medium 61.83% 92.94% 59.75% 86.64% 62.66% 93.97% Hard 47.19% 43.26% 58.43% reasoning; and (3) the datasets focus on temporal question answering within single scenario, which limits question diversity and diminishes the impact of advanced structural modeling. Analysis on Table + QA Difficulty. We categorize the Table+QA tasks into three difficulty levels. As shown in Table 6, ST-Raptor outperforms all baselines across these levels. While both foundation models and ST-Raptor perform well on simple cases, accuracy drops as difficulty increases. Notably, ST-Raptor demonstrates superior performance on hard cases, attributed to its HO-Tree-based representation and operation-pipeline-driven QA strategy. We categorize Table+QA tasks into three difficulty levels  (Table 6)  . ST-Raptor consistently outperforms all baselines across these levels. While both foundation models and ST-Raptor perform well on simple cases, accuracy drops as difficulty increases. Notably, ST-Raptor excels on hard cases, benefiting from its HOTree representation and operation-pipeline-driven QA strategy."
        },
        {
            "title": "7.4 Fine-grained Analysis\nIn this section, we discuss the quality of meta-information detection,\nanalyze question answering latency, and examine the impact of\npipeline errors.\nQuality of Meta Information Detection. We evaluate the Ta-\nble2Tree module‚Äôs accuracy in converting semi-structured table into\nHO-Tree. Experiment results show that the untuned VLM achieves\n93.14% on SSTQA, 94.32% on TempTabQA and 92.31% on WikiTQ,\nwhich is sufficiently high for accurate HO-Tree construction.\nAnalysis of Backward Verification.",
            "content": "To assess the potential negative impact of generating suboptimal question alternatives on question answering accuracy, we quantify the number of such bad alternatives and their corresponding answers on the SSTQA dataset. Experimental results show false negative rate of 4.78% under the few-shot learning setting, indicating that misjudgments in backward verification have minimal impact on table QA performance. Latency Analysis. The runtime of ST-Raptor is primarily influenced by the cost of accessing the LLM, largely due to network latency. As ST-Raptor performs question answering via pipelinebased operation generation, the runtime per query is inherently unstable. ST-Raptor requires around 30 seconds per question (ignoring bias caused by factors like network communications), with 2.89 pipeline operations on average. This is substantially faster than the agent-based method, which incurs higher latency due to greater number of operations with more API calls, and slightly slower than the fine-tuning approach, which benefits from local deployment and direct reasoning. Effects of Pipeline Mistakes. Pipeline Mistakes Analysis. Mistakes in the ST-Raptor pipeline primarily arise from two sources. First, mistakes in meta-information detection by the VLM can lead to incorrect HO-Tree representations, resulting in more complex data retrieval paths (e.g., locating related data dispersed across different subtrees) and reduced overall efficiency (e.g., from 10 to"
        },
        {
            "title": "Model",
            "content": "Full Model (DeepseekV3) GPT-4o DeepseekV3 w/o Table2Tree w/o Question Decomposition w/o Operation-Table Alignment w/o Data Manipulation Operation w/o Answer Verifier"
        },
        {
            "title": "SSTQA",
            "content": "Acc 72.39% 62.12% 62.26% 57.24%(-15.15%) 68.06%(-4.33%) 71.07%(-1.32%) 65.09%(-7.30%) 66.10%(-6.29%) ROUGE-L 52.19% 43.86% 46.17% 41.55%(-10.64%) 48.09%(-4.10%) 50.86%(-1.33%) 47.13%(-5.06%) 47.46%(-4.73%) seconds). Second, semantic misinterpretations by the LLM, such incorrectly splitting combined address-phone entry into separate fields, can trigger unnecessary lookups and potentially yield incorrect answers, leading to additional verification and iteration, and thereby diminishing efficiency."
        },
        {
            "title": "7.5 Ablation Study on ST-Raptor Modules\nIn this section, we perform an ablation study on ST-Raptor from\nfive perspectives. Results are reported in Table 7.\nWithout Table2Tree. We disable the Table2Tree module and in-\nstead apply ST-Raptor directly to raw semi-structured tables, which\nevaluates the importance of explicit table layout modeling. The\nremoval leads to the most significant degradation (an absolute ac-\ncuracy drop of 15.15%), demonstrating the critical role of HO-Tree-\nbased structural representation in handling complex semi-structured\ntables. This also highlights that foundation models alone struggle\nto capture intricate layout semantics without explicit structural\nguidance.\nWithout Question Decomposition. We remove the question\ndecomposition module, requiring ST-Raptor to process complex\nqueries in a single step. This results in a 4.33% accuracy drop, con-\nfirming the necessity of decomposition for effective multi-hop rea-\nsoning. Without decomposition, the ST-Raptor fails to isolate inter-\nmediate steps, leading to compounding errors in reasoning chains.\nWithout Operation-Table Alignment. We omit the operation-\ntable alignment mechanism to test whether the LLM in ST-Raptor\ncan inherently align operations with table content. A 1.32% per-\nformance decline is observed, indicating that while LLMs possess\nsemantic reasoning ability, explicit alignment could still improve\nexecution precision. This suggests that structural grounding re-\nmains beneficial even for advanced models with strong language\nunderstanding capabilities.\nWithout Data Manipulation Operations. We restrict ST-Raptor\nto data retrieval, alignment, and reasoning operations, disabling\ndata manipulation functions. This leads to a 7.30% accuracy drop,\nunderscoring the frequent necessity of manipulation operations\nand validating the completeness of our atomic operation set. Many\nquestions inherently require operations such as filtering and calcu-\nlation, which cannot be bypassed through reasoning alone.\nWithout Answer Verifier. To evaluate the impact of self-verification,\nwe remove the answer verifier module. Accuracy drops by 6.29%,\nsuggesting that the verifier plays a vital role in detecting and cor-\nrecting execution errors, thereby enhancing output reliability. This\nmodule is especially useful when wrong intermediate results or\nfinal answer are generated during multi-step execution.",
            "content": "Table 8: Case Study on SSTQA Dataset. Table Id 5 19 15 Layout Representation Table Difficulty Simple ùêø.4 ùêø.3 [ùêø.21, . . . , ùêø.212 ] Simple ùêø.4 ùêø.3 [ùêø.21, . . . , ùêø.23 ] Simple ùêø.4 ùêø.3 [ùêø.21, ùêø.22, ùêø.23 ] 20 Simple 4 Medium ùêø.4 ùêø.3 [ {ùêø.4 ùêø.3 [ùêø.21, . . . , ùêø.26 ] }1, {ùêø.4 [ùêø.21, . . . , ùêø.26 ] }2 ] ùêø.3 [ {ùêø.4 [ùêø.21, ùêø.22, ùêø.23 ] }1, . . . , {ùêø.4 [ùêø.21, ùêø.22, ùêø.23 ]6 } ] 95 Medium ùêø.4 ùêø.3 [ùêø.21, . . . , ùêø.28 ] 100 Medium ùêø.4 {ùêø.4 ùêø.3 [ùêø.11, ùêø.12, . . . , ùêø.134 ] } 87 Medium ùêø.4 ùêø.3 [ùêø.21, . . . , ùêø.210 ] ùêø.4 [ùêø.1, {ùêø.4 {ùêø.3 [ùêø.11, ùêø.12 ] } }1, {ùêø.4 {ùêø.3 [ùêø.21, . . . , ùêø.24 ] } }2, . . . ] ùêø.3 [ {ùêø.4 [ùêø.21, ùêø.22 ] }1, . . . , {ùêø.4 [ùêø.21, ùêø.22 ]4 } ] ùêø.4 ùêø.3 [ùêø.21, . . . , ùêø.28 ] Hard"
        },
        {
            "title": "Hard",
            "content": "ùêø.3 [ùêø.1, ùêø.21, . . . , ùêø.26 ] 1 10 91 30 Question Summarize the reimbursement activities of Tian Xiaohong. What are the components of employee compensation? What documents must Continuity and Availability Planner submit to the IT Service Management Committee? What are the categories of variable manufacturing overhead? How many items are there in drawing technology? Which employees in the table have 18 years of service? What is the net cash flow generated from investing activities? What are the evaluation criteria for work attitude? How many secondary indicators are included under the performance metrics efficiency indicators? How many phases are included in the Change Phase Code Table? What are the beginning and ending balances of total assets? How many categories are there for service sub items with service number XX-R-I-4? TableLLaMA ReAcTable mPLUGDocOwl1.5 GPT-4o ST-Raptor (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) Collectively, these results demonstrate that each module in ST-Raptor addresses distinct yet complementary challenges in semi-structured table QA, and their synergistic integration is vital for effectively tackling the layout-intensive questions in SSTQA."
        },
        {
            "title": "7.6 Case Study on SSTQA Dataset\nFor each table difficulty level in the SSTQA dataset, we select four\nrepresentative question-answering cases. Table 8 presents the ab-\nstract semi-structured table layout representations, the correspond-\ning questions, and the results from five selected methods. The layout\nrepresentations follow the table definitions introduced in Section 3,\nwhere ùêø.1 denotes a Header-Single-Value structure, ùêø.2 denotes\nHeader-Multiple-Value, ùêø.3 denotes Orthogonal Tables, and ùêø.4 de-\nnotes Header-Orthogonal-Tables. A rightward arrow indicates the\nconstruction of a layout. For example, ùêø.4 ‚Üí ùêø.2 indicates that\nheaders are added to the ùêø.2 layout.",
            "content": "Two key observations emerge from the Table 8: (1) In terms of structural complexity, tables with complex layouts (e.g., Tables 20, 4, 1, 10) often lead to errors for most methods except ST-Raptor. (2) Regarding questions, those requiring math operations (e.g., Tables 4, 1, 30) demand deeper understanding of table structure, where ST-Raptor consistently excels other methods. Regarding limitations, ST-Raptor may occasionally irregular layout patterns, such as erroneously treating horizontally merged content cells as headers, which can negatively affect the QA accuracy. Besides, both ST-Raptor and the baselines face challenges in decomposing questions involving complex pipelines (e.g., resembling multi-level nested SQL queries), requiring techniques such as specialized LLM fine-tuning. Nonetheless, such cases are infrequent in semi-structured table QA tasks. 8 Related Works"
        },
        {
            "title": "8.1 Structured Table QA.\nMainstream approaches can be categorized into NL2SQL, NL2Code\nand vision-language model based methods. NL2SQL [16, 24, 41]",
            "content": "13 focuses on translating natural language queries into structured SQL commands by leveraging techniques such as (1) schema linking, which aligns user intents with database schema to resolve ambiguities, and (2) content retrieval, which dynamically extracts relevant information from the database to refine query generation. VLM based methods [20, 47] transform the table into image for analysis and question answering."
        },
        {
            "title": "8.2 Semi-Structured Table QA.\nSemi-structured tables bring a large challenge for table understand-\ning and render traditional Text2SQL strategy ineffective. To address\nthe issue, numerous excellent research efforts have been carried\nout [21, 26, 32, 38]. For instance, Wang et al. proposed an end-to-end\nsystem [38] that uses semi-structured tables as knowledge sources\nby first finding the most similar tables and then selecting the most\nrelevant table cells to derive the answer. Additionally, Liu et al.\nproposed the GrabTab method [26] featuring a Component Delib-\nerator that efficiently leverages multiple table components without\nrequiring complex post-processing, for addressing the challenge\nof recognizing complex and irregular table structures. Inspired by\nNL2SQL techniques, Lu et al. [28] propose to convert natural lan-\nguage queries into NoSQL ones, but only produce intermediate\nresults, lacking the end-to-end semi-structured table QA capabil-\nity like ST-Raptor. Moreover, Gupta et al. built a TEMPTABQA\ndataset [18] from 1,208 Wikipedia Infobox tables to evaluate the tem-\nporal reasoning capabilities, and found that even top-performing\nLLMs fall behind human performance by over 13.5 F1 points. Some\nworks conduct information extraction over semi-structured data.\nTWIX [25] assumes that many semi-structured data is generate\nfrom one similar layout template and proposes a method that first\nreconstruct the template than extract the content. However, it lacks\nsupport for merged cells and cannot be readily transformed into\nHO-Trees within our problem scope. Another approach is convert\nthe semi-structured data into structured formats for downstream",
            "content": "analysis [6]. However, this conversion process can introduce information loss and reduce answer accuracy. Overall, these findings highlight that despite significant advancements, substantial challenges remain in effectively addressing semi-structured table QA."
        },
        {
            "title": "9 Conclusion\nIn this paper, we introduced ST-Raptor, a tree-based framework\naimed at addressing the critical challenges of automating question\nanswering over semi-structured tables. Central to our approach is\nthe Hierarchical Orthogonal Tree (HO-Tree), a formal representa-\ntion capable of capturing complex table layouts, including hierarchi-\ncal headers, merged cells, and implicit relationships. We designed\na set of basic tree operations over HO-Trees to enable LLMs to\nperform layout-aware tasks. Given a user question, ST-Raptor de-\ncomposes it into simpler subquestions, constructs corresponding\ntree-operation pipelines, and executes them to retrieve relevant\ninformation or derive the final answer. To ensure both execution\ncorrectness and answer reliability, we proposed a two-stage verifica-\ntion mechanism combining forward constraint checking and back-\nward answer validation. Additionally, we constructed the SSTQA\nbenchmark, consisting of 764 questions over 102 real-world semi-\nstructured tables. Experimental results demonstrate that ST-Raptor\noutperforms all baselines by up to 20% in answer accuracy.",
            "content": "14 References [1] [n.d.]. https://www.frontiersin.org/research-topics/21489/knowledge-discoveryfrom-unstructured-data-in-finance [2] [n.d.]. https://enterprises.upmc.com/resources/insights/health-caresunstructured-data-challenge/ [3] [n.d.]. https://pages.cs.wisc.edu/jbeckham/TR/cnet.pdf [4] Serge Abiteboul. 1997. Querying semi-structured data. In Database Theory ICDT 97, Foto Afrati and Phokion Kolaitis (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 118. [5] Majed Alwateer, El-Sayed Atlam, Mahmoud Mohammed Abd El-Raouf, Osama A. Ghoneim, and Ibrahim Gad. 2024. Missing Data Imputation: Comprehensive Review. Journal of Computer and Communications 12, 11 (2024), 5375. https: //doi.org/10.4236/jcc.2024.1211004 [6] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher R√©. 2025. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. arXiv:2304.09433 [cs.CL] https://arxiv.org/abs/2304.09433 [7] Camille Barboule, Benjamin Piwowarski, and Yoan Chabot. 2025. Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends. arXiv:2501.02235 [cs.CL] https://arxiv.org/abs/2501.02235 [8] ≈Åukasz Borchmann and Marek Wydmuch. 2025. Query and Conquer: ExecutionGuided SQL Generation. arXiv:2503.24364 [cs.CL] https://arxiv.org/abs/2503. 24364 [9] Douglas Burdick, Marina Danilevsky, Alexandre V. Evfimievski, Yannis Katsis, and Nancy Wang. 2020. Table Extraction and Understanding for Scientific and Enterprise Applications. Proc. VLDB Endow. 13, 12 (2020), 34333436. https: //doi.org/10.14778/3415478. [10] Kaiwen Chen, Yueting Chen, Nick Koudas, and Xiaohui Yu. 2025. Reliable Textto-SQL with Adaptive Abstention. Proceedings of the ACM on Management of Data 3, 1 (Feb. 2025), 69:169:30. https://doi.org/10.1145/3709719 [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2025. Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling. arXiv:2412.05271 [cs.CV] https://arxiv.org/abs/2412.05271 [12] E. F. Codd. 1970. relational model of data for large shared data banks. Commun. ACM 13, 6 (June 1970), 377387. https://doi.org/10.1145/362384.362685 [13] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501. 12948 [14] DeepSeek-AI, Aixin Liu, Bei Feng, et al. 2025. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL] https://arxiv.org/abs/2412. [15] Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea. 2024. Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs. In Findings of the Association for Computational Linguistics: ACL 2024. Association for Computational Linguistics, Bangkok, Thailand, 407426. https://doi.org/10.18653/v1/2024.findings-acl.23 [16] Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, and Yu Li. 2025. Preview of XiYan-SQL: Multi-Generator Ensemble Framework for Text-to-SQL. arXiv:2411.08599 [cs.AI] https://arxiv.org/abs/2411.08599 [17] Vivek Gupta, Pranshu Kandoi, Mahek Vora, Shuo Zhang, Yujie He, Ridho Reinanda, and Vivek Srikumar. 2023. TempTabQA: Temporal Question Answering for Semi-Structured Tables. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 24312453. https://doi.org/10.18653/v1/2023.emnlp-main.149 [18] Vivek Gupta, Pranshu Kandoi, Mahek Vora, Shuo Zhang, Yujie He, Ridho Reinanda, and Vivek Srikumar. 2023. TempTabQA: Temporal Question Answering for Semi-Structured Tables. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 24312453. https://doi.org/10.18653/v1/2023.emnlp-main.149 [19] Jonathan Herzig, Pawe≈Ç Krzysztof Nowak, Thomas M√ºller, Francesco Piccinno, and Julian Martin Eisenschlos. 2020. TAPAS: Weakly Supervised Table Parsing via Pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, 43204333. https://doi.org/10.18653/v1/2020.acl-main.398 [20] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024. mPLUGDocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding. arXiv:2403.12895 [cs.CV] https://arxiv.org/abs/2403. [21] Sujay Kumar Jauhar, Peter Turney, and Eduard Hovy. 2016. Tables as Semistructured Knowledge for Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Katrin Erk and Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin, Germany, 474483. https://doi.org/10.18653/v1/P16-1045 [22] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed Prompting: Modular Approach for Solving Complex Tasks. arXiv:2210.02406 [cs.CL] https: //arxiv.org/abs/2210.02406 [23] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2023. TableGPT: Table-tuned GPT for Diverse Table Tasks. arXiv:2310.09263 [cs.CL] https://arxiv.org/abs/2310.09263 [24] Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, and Hangyu Mao. 2024. PETSQL: Prompt-Enhanced Two-Round Refinement of Text-to-SQL with Crossconsistency. arXiv:2403.09732 [cs.CL] https://arxiv.org/abs/2403.09732 [25] Yiming Lin, Mawil Hasan, Rohan Kosalge, Alvin Cheung, and Aditya G. Parameswaran. 2025. TWIX: Automatically Reconstructing Structured Data from Templatized Documents. arXiv:2501.06659 [cs.DB] https://arxiv.org/abs/ 2501.06659 [26] Hao Liu, Xin Li, Mingming Gong, Bing Liu, Yunfei Wu, Deqiang Jiang, Yinsong Liu, and Xing Sun. 2024. Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation. Proceedings of the AAAI Conference on Artificial Intelligence 38, 4 (Mar. 2024), 36033611. https: //doi.org/10.1609/aaai.v38i4. [27] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. arXiv:2307.03172 [cs.CL] https://arxiv.org/abs/2307.03172 [28] Jinwei Lu, Yuanfeng Song, Zhiqian Qin, Haodi Zhang, Chen Zhang, and Raymond Chi-Wing Wong. 2025. Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation. arXiv:2502.11201 [cs.DB] https://arxiv.org/abs/2502.11201 [29] OpenAI. 2024. GPT-4o System Card. arXiv:2410.21276 [cs.CL] https://arxiv.org/ abs/2410.21276 [30] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge arXiv:2303.08774 [cs.CL] Akkaya, et al. 2024. GPT-4 Technical Report. https://arxiv.org/abs/2303.08774 [31] Panupong Pasupat and Percy Liang. 2015. Compositional Semantic Parsing on Semi-Structured Tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong and Michael Strube (Eds.). Association for Computational Linguistics, Beijing, China, 14701480. https://doi.org/10.3115/v1/P15- [32] Panupong Pasupat and Percy Liang. 2015. Compositional Semantic Parsing on Semi-Structured Tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong and Michael Strube (Eds.). Association for Computational Linguistics, Beijing, China, 14701480. https://doi.org/10.3115/v1/P15-1142 [33] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, Fei Huang, and Yongbin Li. 2022. Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions. arXiv preprint arXiv:2208.13629 (2022). https://arxiv.org/abs/2208.13629 [34] Ge Qu, Jinyang Li, Bowen Li, Bowen Qin, Nan Huo, Chenhao Ma, and Reynold Cheng. 2024. Before Generation, Align it! Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation. In Findings of the Association for Computational Linguistics: ACL 2024. Association for Computational Linguistics, Bangkok, Thailand, 54565471. https://doi.org/10.18653/v1/2024.findings-acl.324 [35] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 98959901. https://doi.org/10.18653/v1/2021. emnlp-main.779 [36] Tal Schuster, Adam Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William Cohen, and Donald Metzler. 2024. SEMQA: Semi-Extractive Multi-Source Question Answering. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association for Computational Linguistics, Mexico City, Mexico, 13631381. https://doi.org/10.18653/v1/2024.naacl-long.74 [37] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table Meets LLM: Can Large Language Models Understand Structured Table Data? Benchmark and Empirical Study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining (WSDM 2024). Association for Computing Machinery, M√©rida, Mexico, 123134. https://doi.org/10.1145/3616855.3635752 [38] Hao Wang, Xiaodong Zhang, Shuming Ma, Xu Sun, Houfeng Wang, and Mengxiang Wang. 2018. Neural Question Answering Model Based on Semi-Structured Tables. In Proceedings of the 27th International Conference on Computational Linguistics, Emily M. Bender, Leon Derczynski, and Pierre Isabelle (Eds.). Association for Computational Linguistics, Santa Fe, New Mexico, USA, 19411951. https://aclanthology.org/C18-1165/ [39] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual E5 Text Embeddings: Technical Report. arXiv:2402.05672 [cs.CL] https://arxiv.org/abs/2402.05672 [40] Zhongyuan Wang, Richong Zhang, Zhijie Nie, and Jaein Kim. 2024. ToolAssisted Agent on SQL Inspection and Refinement in Real-World Scenarios. arXiv:2408.16991 [cs.CL] https://arxiv.org/abs/2408.16991 [41] Xiangjin Xie, Guangwei Xu, Lingyan Zhao, and Ruijie Guo. 2025. OpenSearchSQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment. arXiv:2502.14913 [cs.CL] https://arxiv.org/abs/2502.14913 [42] Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. arXiv:2301.13808 [cs.CL] https://arxiv. org/abs/2301.13808 [43] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2023. Large Language Models Meet NL2Code: Survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, 74437464. https://doi.org/10.18653/v1/2023.acl-long. [44] Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. 2024. TableLlama: Towards Open Large Generalist Models for Tables. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association for Computational Linguistics, Mexico City, Mexico, 60246044. https: //doi.org/10.18653/v1/2024.naacl-long.335 [45] Xiaokang Zhang, Sijia Luo, Bohan Zhang, Zeyao Ma, Jing Zhang, Yang Li, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, and Jie Tang. 2025. TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios. arXiv:2403.19318 [cs.CL] https://arxiv.org/ abs/2403.19318 [46] Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen Deep, and Jignesh M. Patel. 2023. ReAcTable: Enhancing ReAct for Table Question Answering. arXiv:2310.00815 [cs.DB] https://arxiv.org/abs/2310.00815 [47] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. 2024. Multimodal Table Understanding. arXiv:2406.08100 [cs.CL] https://arxiv.org/abs/2406.08100 [48] Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, and Tat-Seng Chua. 2024. TAT-LLM: Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. arXiv:2401.13223 [cs.CL] https://arxiv.org/abs/2401."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Shanghai Jiao Tong University",
        "Simon Fraser University",
        "Tsinghua University"
    ]
}