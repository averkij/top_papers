{
    "paper_title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System",
    "authors": [
        "Lixuan He",
        "Haoyu Dong",
        "Zhenxing Chen",
        "Yangcheng Yu",
        "Jie Feng",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 3 4 9 1 . 6 0 5 2 : r Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with Hierarchical Spatial-Cognition Long-Short Memory System Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China {helx23,donghy23,zhenxing23,yuyc23}@mails.tsinghua.edu.cn, {fengjie,liyong07}@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce Mem4Nav, hierarchical spatialcognition longshort memory system that can augment any VLN backbone. Mem4Nav fuses sparse octree for fine-grained voxel indexing with semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 713 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav."
        },
        {
            "title": "Introduction",
            "content": "Vision-and-Language Navigation (VLN) requires an agent to follow free-form natural language instructions and navigate through complex visual environments to reach specified target [2, 14]. Most existing methods primarily address indoor VLN. One class of methods [2, 7, 19, 12, 4, 17] frames the task as traversal on discrete topological graph, allowing agents to teleport between fixed nodes without modeling motion uncertainty, which limits their applicability in real-world continuous spaces. Other techniques remove the reliance on such graphs by learning end-to-end action policies [18, 5, 28] or by predicting intermediate waypoints [16, 1, 38]. Action-based methods struggle with diverse semantic variations in scenes, while waypoint-based approaches do not generalize well to expansive outdoor settings. Recent work has attempted to extend VLN from indoor settings to outdoor urban environments[31, 22, 39, 10], yet it still lacks the ability to sustain long-term perception, memory, and autonomous decision-making over complex 3D scenes at city scale. Recent VLN approaches fall into two camps. On one hand, Hierarchical Modular Pipelines decouple perception, mapping, planning and controloffering interpretability but relying on handPreprint. Under review. Figure 1: Long-instruction navigation in urban environments demands that agents retain both finegrained spatial detail and high-level landmark semantics over many stepsa core challenge that leads to information loss or retrieval overload. Mem4Nav meets this by building hierarchical spatial-cognition long-short memory system. crafted interfaces and lacking unified memory [28, 16, 9]. On the other hand, (M)LLM-Based Agents leverage large (multimodal) language models to fuse vision and language, achieving near end-to-end performance but still bounded by fixed context windows and implicit spatial memory [32, 31, 22, 39]. Neither paradigm natively supports efficient, lossless storage and retrieval of large-scale 3D structure nor fast adaptation to dynamic, local changes. The primary bottleneck in urban VLN may be the agents inability to model its current 3D spatial information, store it in memory in structured form, and retrieve it quickly and efficiently when required. Therefore, based on existing research, we propose the following hypothesis: the key to endowing an embodied agent with complex autonomous decision-making capabilities in urban environments-and thus achieving more powerful Vision-and-Language Navigationis high-performance memory system that is seamlessly integrated into the agents other cognitive functions, such as perception and decision-making. To bridge this gap, we propose Mem4Nav, hierarchical 3D spatialcognition longshort memory framework that augments any VLN backbone. After the visual encoder, we build sparse octree for voxel-level indexing of observations, semantic topology graph linking landmark nodes and intersections, long-term memory reversible memory tokens and compact short-term memory cache of recent entries in local coordinates for rapid adaptation. We evaluate Mem4Nav on two street-view VLN benchmarksTouchdown [3] and Map2Seq [29]and use three backbones: non-end-to-end modular pipeline, prompt-based LLM navigation agent [31], and strided-attention MLLM navigation agent [39]. Under the same training cost and hardware budget as the strongest baselines, Mem4Nav delivers absolute improvements of seven to thirteen percentage points in Task Completion, reduces the final stop distance by up to 1.6 m, and increases normalized DTW by more than ten percentage points. Ablation studies confirm that each componentthe sparse octree, the semantic graph, the long-term memory tokens, and the short-term cacheis essential to these gains. In summary, our contributions are: We introduce dual-structured 3D map combining sparse octree indexing with semantic topology graph, unifying fine-grained geometry and landmark connectivity. We design reversible Transformer memory that losslessly compresses and retrieves spatially anchored observations at both octree leaves and graph nodes. We develop short-term memory cache for high-frequency local lookups, and unified retrieval mechanism that dynamically balances shortand long-term memories within the agents attention. We demonstrate that Mem4Nav consistently enhances three distinct VLN backbones on Touchdown and Map2Seq, delivering substantial improvements in success rate, path fidelity, and distance metrics."
        },
        {
            "title": "2 Methodology",
            "content": "Our proposed framework integrates brain-inspired multi-layer spatial representation with dualmemory architecture to enhance Vision-and-Language Navigation (VLN) in large-scale urban en2 Figure 2: Contributions of Mem4Nav: Prior VLN systems treat spatial maps and memory as separate, using flat, monolithic maps that are either too detailed (noisy, slow to query) or too coarse (lossy), and simple text-based memory that merely appends raw history to instructions, leading to clutter and forgetting. Mem4Nav jointly implements hierarchical spatial representation and dual longshort memory mechanism, and can be seamlessly integrated into existing vision-and-language navigation pipelines to boost performance. vironments. In this section, we detail the four main components of our system: the perception and embedding module, the hierarchical spatial representation, the dual longshort memory network, and the multi-level retrieval and decision mechanism. We begin by formulating the overall VLN task and then describe how each component is instantiated, including the key algorithms and mathematical formulations. Our proposed Mem4Nav architecture endows vision-and-language navigation agent with hierarchi spatial cognition and dual longshort memory system. 2.1 Hierarchical Spatial Representation To enable both fine-grained geometric lookup and high-level route planning, Mem4Nav organizes the environment into two complementary spatial structures: sparse octree for voxel-level indexing and semantic topological graph for landmark connectivity. The octree provides efficient access to local spatial context, while the graph abstracts salient decision points and their relationships. Sparse Octree Indexing We discretize the continuous 3D space into hierarchical octree of maximum depth Λ, where each level ℓ {0, . . . , Λ} corresponds to axis-aligned cubes of side length L/2ℓ. Only those leaf cubes that the agent visits or that contain relevant observations are instantiated and stored in hash map, ensuring both sparsity and O(1) average lookup time. To recover 3D structure from RGB panoramas, we employ the universal monocular metric depth estimator UniDepth. [25] Morton Code Addressing. The agents position pt = (xt, yt, zt) is quantized to integer indices pt = (cid:0)xt 2Λ/L, yt 2Λ/L, zt 2Λ/L(cid:1) {0, . . . , 2Λ 1}3, which are interleaved to form Morton code κ(pt) = InterleaveBits(pt) {0, . . . , 23Λ 1}. This single integer uniquely identifies the visited leaf. On each visit, if κ(pt) is not already present, new leaf entry is created; otherwise, the existing leafs embedding is updated with the latest observation in constant time. Leaf Embedding Updates. Each instantiated leaf maintains an aggregated embedding of the observations within its cube. Upon revisiting, the current feature vector vt is fused into this embedding via reversible update operator, preserving both efficiency and information fidelity. More details are provided in appendix A.1. 3 Figure 3: Detailed Mem4Nav pipeline: In Reversible Token Processing, each token is mapped to spatial elementeither voxel in the Sparse Octree or node in the Semantic Topological Graphand its read/write tokens are updated by reversible Transformer block. Concurrently, recent observations are inserted into the STM Cache attached to the current graph node. During Planning and Execution, the agent queries STM for local context and performs nearest-neighbor search over all LTM tokens in the octree and graph to retrieve deep history, then fuses the retrieved vectors with current perception in its policy head to drive route and motion planning. 2.1.1 Semantic Topological Graph While the octree captures raw geometry, high-level navigation relies on semantic landmarks and decision points. We therefore maintain dynamic directed graph = (V, E), where each node corresponds to landmark or intersection and edges (ui, uj) encode traversability and cost. Node Creation. Given the current embedding vt and existing node descriptors {ϕ(u)}, we create new node whenever min uV vt ϕ(u) > δ, assigning the new node the position pt and initializing its descriptor to vt. Edge Weighting. Whenever the agent moves from node ut1 to ut, we add or update the directed edge (ut1, ut) with weight wt1,t = α pt1 pt2 + β cinstr, where cinstr encodes instruction-based penalties (e.g. turns). If the edge already exists, its weight is averaged to smooth out noise. Query Modes. At decision time, the agent may perform: Voxel lookup: compute κ(p) and fetch the corresponding octree leaf embedding for precise local reasoning. Graph lookup: run shortest-path algorithm on to retrieve sequence of landmark nodes for macro-scale routing. Combined Query Modes. At query time, the agent can: Voxel lookup: given precise coordinate, compute κ and fetch θr κ. Node lookup: given semantic goal node ug, perform shortestpath search (e.g. Dijkstra) on to retrieve the sequence of graph tokens along the plan. This dual representation ensures that Mem4Nav can rapidly retrieve the memory tokens most relevant to either microscopic obstacle avoidance or macroscopic route guidance, all within realtime constraints. 2.2 LongTerm Memory with Reversible Tokens LongTerm Memory (LTM) provides highcapacity, lossless storage of spatially anchored observations via virtual memory tokens embedded in both octree leaves and semantic graph nodes. Each , both in Rd. New spatial element (leaf or node) maintains readtoken θr observations vt Rd are absorbed into LTM by bijective update, and past information can be exactly reconstructed when needed. and writetoken θw 4 Reversible Transformer Block. We adopt reversible architecture composed of layers. At each layer ℓ, inputs (x ℓ ) are transformed via two submodules Fℓ and Gℓ: ℓ , x2 y1 ℓ = x1 ℓ = y2 x2 ℓ + Fℓ(x2 ℓ ), ℓ Gℓ(y1 ℓ ), y2 ℓ = x2 ℓ = y1 x1 ℓ + Gℓ(y1 ℓ ), ℓ Fℓ(x2 ℓ ). Here each Fℓ, Gℓ is lightweight adapter atop frozen Transformer layer. Collectively, maps (θr and supports exact inverse. s, vt) (cid:55) θw Write Update. When an observation vt falls into spatial element s: R(cid:0)θr θw θw θr . (cid:1), Concatenation yields 2d-dimensional input. Because is bijective, no information is lost: the original (θr s, vt) can be recovered by the inverse pass. vt CycleConsistency Training. To enforce faithful reconstruction, we minimize cycle consistency loss on synthetic trajectories: Lcycle = Ev (cid:104)(cid:13) (cid:13)v (cid:98)v(cid:13) 2 (cid:13) 2 (cid:105) , (cid:98)v = πv (cid:16) R1(cid:0)R(θr; v)(cid:1)(cid:17) , where πv is small decoder projecting reversed hidden states back to the embedding space. Jointly with any downstream navigation loss, this trains the reversible block to faithfully encode and decode. Retrieval from LTM. At decision time, if local cache misses, we compose query qt = Proj([vt; pt]) and perform an approximate nearest neighbor lookup over {θr s} using HNSW (Hierarchical Navigable Small World, A.1.3) graphs. For each retrieved token θr si, we recover the original embedding via inverse transform: (cid:1), (cid:98)vsi = R1(cid:0)θr si and then decode: where πp, πd are MLP decoders for position and descriptor. small set of top-m memories {((cid:98)psi , (cid:98)dsi)} is fed into the policy for global reasoning. (cid:98)psi = πp((cid:98)vsi), (cid:98)dsi = πd((cid:98)vsi), 2.3 ShortTerm Memory Cache ShortTerm Memory (STM) is fixedsize, highfrequency buffer attached to the current semantic node uc. It stores the most recent observations in relative coordinates for rapid local lookup and dynamic obstacle avoidance. Entry Structure. Each STM entry = (o, prel, v, τ ) comprises: o: object or event identifier (e.g. car, traffic_light), prel = pt puc: coordinate relative to current node, Rd: multimodal embedding, τ : timestamp or step index. Replacement Policy. To maximize hit rate under capacity K, we combine frequency and recency: Score(ei) = λ freq(ei) (1 λ) (cid:0)tnow τi (cid:1), where freq(ei) is the access count. On cache full and new entry: Score(ei). eevict = arg min This FrequencyandLeastFrequently Used policy preserves both frequently accessed and recently used items. STM Retrieval. At time t, given current embedding vt and relative query qrel: then compute cosine similarity = { ei : prel,i qrel ϵ}, si = vt, vi vtvi , C, and return topk entries {ei1 , . . . , eik }. Both filtering and similarity ranking cost O(K), with 128 in practice. By combining LTM for deep history and STM for immediate context, our Mem4Nav system achieves both largescale recall and rapid local adaptation in real time. 5 2.4 MultiLevel Memory Retrieval and Decision Making At each time step t, with current observation embedding vt and position pt, Mem4Nav first attempts short-term memory lookup by computing the relative query qrel = pt puc, filtering STM entries within radius ϵ, and ranking them by cosine similarity. If the highest similarity exceeds threshold τ , the agent aggregates the top-k STM embeddings into mSTM; otherwise it falls back to long-term memory by projecting qt = Proj([vt; pt]), performing an HNSW search over all read-tokens {θr s} in the sparse octree and semantic graph, decoding the top-m tokens via the reversible Transformer inverse into {(cid:98)vsi}, and aggregating them into mLTM. The final memory vector is chosen as mt = (cid:26)mSTM, maxivt, vi τ, mLTM, otherwise, which is concatenated to the baseline keys and values {K, } in the policys cross-attention: = [K; mt], = [V ; mt], and combined via learned gate αt: Outt = αt Attn(Q, , ) + (1 αt) Attn(Q, K, ). The result then flows through the feed-forward and action-selection layers, allowing the agent to rely on fresh local context whenever possible and deeper historical cues when necessary."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate our Mem4Nav systemintegrating sparse octree + semantic-graph mapping with longshort memoryon two urban VLN benchmarks, Touchdown and Map2Seq. We compare three backbone pipelines (a basic modular agent, VELMA, and FLAME) both with and without our memory modules. We report standard VLN metrics and analyze how TC, SPD and nDTW respond differently to memory augmentation. 3.1 Datasets and Metrics We follow the standard splits of Touchdown [3] and Map2Seq [29]. Both datasets consist of streetlevel panoramas paired with natural language instructions and trajectories.Touchdown contains 9,326 instructiontrajectory pairs collected in New York Citys StreetLearn environment. Instructions often reference urban landmarks and require precise alignment to complex intersections. Map2Seq comprises 7,672 pairs in denser city subset. We evaluate using three standard VLN metrics: Task Completion (TC): percentage of episodes where the agent stops within 3 of the goal (). Shortest-path Distance (SPD): average geodesic distance (in meters) from the agents final position to the goal (). normalized Dynamic Time Warping (nDTW): measures the alignment between the agents and expert trajectories (). Formally, for episodes, let di be the distance from stop to goal and DTWi the warping cost; Li the expert path length. Then: TC = 1 (cid:88) i= 1[di 3], SPD = 1 (cid:88) i=1 di, nDTW = 1 (cid:88) i=1 exp(cid:0)DTWi/Li (cid:1). To isolate the impact of each core component in Mem4Nav, we conduct systematic ablation study on three backbone agents. Furthermore, We measure the average retrieval latency of both short-term and long-term memory components, as detailed in Appendix. A.3 3.2 Implementation and Backbones We implement all agents on single NVIDIA A100 GPU using an identical three-phase training schedule: first, we fine-tune the visual front-end (a ResNet-50 backbone followed by 6-layer Vision Transformer) on masked reconstruction objective over the training panoramas for 10 epochs; second, we freeze the front-end and pretrain the reversible Transformer memory tokens on synthetic navigation trajectories using cycle-consistency loss for 5 epochs; finally, we unfreeze all modules and conduct end-to-end navigation fine-tuning for 30 epochs. 6 Table 1: Test-set performance on Touchdown and Map2Seq backbones, with (+Mem4Nav) and without memory. Touchdown Dev Touchdown Test Map2Seq Dev Map2Seq Test TC SPD nDTW TC SPD nDTW TC SPD nDTW TC SPD nDTW Model RCONCAT (2019) GA (2019) VLN-Trans (2021) ARC+L2S (2020) ORAR (2022) VLN-Video (2024) Loc4Plan (2024) 22.50 25.20 27.00 11.80 20.40 11.90 19.00 16.20 20.80 10.60 20.40 12.00 18.70 15.00 20.30 19.48 17.05 16.68 18.84 49.88 5.87 29.60 11.79 30.05 11.12 31.70 11.20 34.50 9.60 34.50 10.50 32.90 11.50 48.00 7.00 22.90 24.90 27.80 45. 45.30 17.10 30.70 18.20 33.00 18.60 31.10 Hierarchical Modular Pipeline 31.93 12.84 45.18 11.21 + Mem4Nav (ours) VELMA Baseline + Mem4Nav (ours) FLAME Baseline + Mem4Nav (ours) 29.83 14.67 35.29 12. 41.28 9.14 50.10 9.01 46.07 59.03 43.44 55.35 55.96 65.05 29.27 13.05 42.21 11.95 27.38 15.03 34.04 12. 40.20 9.53 48.48 9.10 44.29 56.36 41.93 48.82 54.56 63.63 53.03 6.22 58.19 5.49 52.75 6.78 58.33 6. 56.95 5.95 61.03 5.87 14.70 27.70 17.00 30.10 17.00 29.50 47.75 6.53 62.70 45.30 7.20 69.06 74.74 66.45 75.06 71.36 80. 50.54 6.33 57.64 5.54 48.70 6.80 56.84 6.10 52.44 5.91 60.41 5.90 62.10 65.50 73.57 62.37 72. 67.72 75.94 We compare three backbone architectures under this unified setup. Hierarchical Modular Pipeline is fully modular, nonend-to-end system: large language model generates scene descriptions, which are embedded and fed into our sparse octree + semantic graph builder; hierarchical planner then decomposes the instruction into landmark, object and motion subgoals; and lightweight policy network fuses planner outputs with retrieved memory to select actions. This pipeline was specifically devised by the authors of this paper to rigorously evaluate the performance of the memory module. The second backbone is VELMA[31], nearly end-to-end LLM agent that concatenates recent panorama descriptions, templated landmark/intersection observations and verbalized memory summaries into its prompt, then autoregressively emits the next navigation action. The third backbone is FLAME[39], multimodal LLM with strided cross-attention: it encodes each panorama into patch tokens, attends over the last observations augmented with retrieved memory tokens as extra keys and values, and decodes four-way action via linear head. For complete implementation and backbones details, please refer to Appendix A.2. 3.3 Main Results Table 1 summarizes the impact of our Mem4Nav modules on three backbone agents over both Touchdown and Map2Seq. For the Hierarchical Modular Pipeline, Mem4Nav drives the largest absolute gains. On Touchdown Dev, TC jumps from 31.93% to 45.18% (+13.25 pts), SPD decreases by 1.63 m, and nDTW rises by 12.96 pts. Similar improvements hold on Touchdown Test and Map2Seq, demonstrating that each added componentfine-grained octree indexing, high-level semantic graph, and both longand short-term cachessynergizes to recover both global route structure and local landmark detail in fully modular system. The VELMA backbone, which augments large language model via prompt-based past memory entries, also benefits substantially. Mem4Nav boosts TC by +5.46 pts on Touchdown Dev and +5.14 pts on Map2Seq Dev, while reducing SPD by roughly 2.5 and improving nDTW by over 10 pts. These gains indicate that augmenting LLM prompts with structured memory summaries and spatial indexing helps the model ground instructions more accurately in long, branching urban vistas. Even the fully end-to-end FLAME agent, which already employs strided cross-attention over recent observations, sees notable improvements. On Touchdown Dev, TC increases from 41.28% to 50.10% (+8.82 pts), SPD shrinks by 0.13 m, and nDTW climbs by 9.09 pts. On Map2Seq Dev, TC rises by +4.08 pts and nDTW by +8.08 pts. The relatively smaller SPD change suggests that FLAMEs implicit attention already captures coarse route context, while Mem4Nav refines both long-range coherence and short-term alignment. 7 Table 2: Component-wise ablations of Mem4Nav on Touchdown and Map2Seq. w/o denotes removing component from full Mem4Nav framework. Model Touchdown Dev Touchdown Test Map2Seq Dev Map2Seq Test TC SPD nDTW TC SPD nDTW TC SPD nDTW TC SPD nDTW FLAME + full Mem4Nav FLAME +Mem4Nav w/o Octree FLAME +Mem4Nav w/o Semantic Graph FLAME +Mem4Nav w/o LTM FLAME +Mem4Nav w/o STM FLAME Baseline VELMA + full Mem4Nav VELMA +Mem4Nav w/o Octree VELMA +Mem4Nav w/o Semantic Graph VELMA +Mem4Nav w/o LTM VELMA +Mem4Nav w/o STM VELMA Baseline 50.10 9.01 48.72 9.08 44.40 9.25 47.28 9.03 48.67 9.00 41.28 9.14 35.29 12.16 34.05 12.50 33.50 12.70 31.32 13.20 33.14 12.16 29.83 14.67 45.18 11.21 Hierarchical + full Mem4Nav Hierarchical +Mem4Nav w/o Octree 39.31 12.50 Hierarchical +Mem4Nav w/o Semantic Graph 35.56 12.24 33.42 12.54 Hierarchical +Mem4Nav w/o LTM 41.34 11.25 Hierarchical +Mem4Nav w/o STM 31.93 12.84 Hierarchical Baseline 65.05 60.90 62.10 64.02 62.35 55.96 55.35 53.00 51.50 47.01 49.50 43. 59.03 52.42 52.35 51.23 53.31 46.07 48.48 9.10 47.52 9.18 45.83 9.55 47.90 9.12 48.10 9.08 40.20 9.53 34.04 12.90 32.80 13.20 32.20 13.40 29.85 14.06 32.50 12.91 27.38 15.03 42.21 11.95 38.25 12.91 34.05 12.76 31.52 12.73 38.50 11.98 29.27 13.05 63.63 58.85 61.42 62.70 62.10 54.56 48.82 45.90 44.21 41.40 47.05 41. 56.36 50.45 46.04 47.30 52.00 44.29 61.03 5.87 59.10 5.95 58.50 6.10 60.10 5.88 60.50 5.87 56.95 5.95 58.33 6.01 57.00 6.20 56.20 6.33 54.00 7.12 56.55 6.02 52.75 6.78 58.19 5.49 55.85 6.04 54.46 6.05 55.42 6.13 56.00 5.51 53.03 6.22 80.40 76.20 78.00 79.20 79.80 71.36 75.06 73.21 71.20 67.00 74.00 66. 74.74 70.32 71.15 72.02 69.85 69.06 60.41 5.90 58.35 6.02 56.90 6.20 59.00 5.95 59.90 5.90 52.44 5.91 56.84 6.10 55.00 6.30 54.30 6.45 51.50 7.10 55.50 6.10 48.70 6.80 57.64 5.54 55.20 5.82 52.52 6.13 52.34 6.02 53.50 5.57 50.54 6.33 75.94 73.10 74.50 75.50 75.30 67.72 72.71 70.50 69.10 62.50 71.00 62. 73.57 67.35 69.20 66.23 67.26 65.50 Overall, Mem4Nav consistently elevates navigation performance across diverse agent architectures, with the largest relative uplift for modular pipelines and meaningful gains even in LLM/MLLMbased systems. Task Completion and nDTW show the most pronounced improvements, confirming that our memory system both increases success rates and brings agent trajectories closer to expert demonstrations. 3.4 Ablation Studies For each agent with Mem4Nav, we remove one of the four modulessparse octree, semantic topology graph, long-term memory tokens, or short-term memory cacheand replace it with minimal fallback (e.g., uniform grid indexing instead of octree). This design reveals how each component contributes to global planning, local detail recall, and overall navigation performance across Touchdown and Map2Seq. See Table 2. Hierarchical Modular Pipeline. This fully modular agent exhibits the largest sensitivity to each module. On Touchdown Dev, removing the sparse octree cuts Task Completion (TC) from 45.18% to 39.31% (5.87 points) and shrinks nDTW from 59.03 to 52.42 (6.61 points), reflecting lost fine-grained spatial indexing. Dropping the semantic graph further plummets TC to 35.56% (9.62 points) and raises SPD from 11.21 to 12.24 (+1.03 m), as the agent loses critical landmark ordering. Eliminating long-term memory reduces TC by 11.76 points (to 33.42%) and degrades SPD by +1.33 m, while ablating short-term memory primarily hurts nDTW (5.70 points to 53.31), showing that caches of recent observations are essential for tight trajectory alignment. VELMA (LLM-based). VELMAs reliance on text prompts makes explicit memory impact pronounced. On Touchdown Dev, removing long-term memory drops TC from 35.29% to 31.32% (3.97 points) and worsens SPD from 12.16 to 13.20 (+1.04 m). Eliminating the short-term cache slashes nDTW from 55.35 to 49.50 (5.85 points), as the model can no longer reference recent local observations. By contrast, stripping out the octree or semantic graph only modestly reduces TC by 1.24 and 1.79 points respectively, since the LLM can partially compensate via its autoregressive reasoning over visible-landmark prompts. FLAME (MLLM-based). FLAMEs built-in cross-attention provides some implicit memory, so ablating the reversible tokens yields smaller TC drops (to 47.28% without long-term memory, 2.82 points) and negligible SPD changes. However, removing the octree causes nDTW on Touchdown Dev to fall from 65.05 to 60.90 (4.15 points), reflecting increased path deviation in the absence of precise spatial bins. Dropping the semantic graph diminishes TC from 50.10% to 44.40% (5.70 points) , underscoring that high-level landmark connectivity remains crucial even for MLLM planners. Overall, these ablations confirm that while LLM/MLLM backbones can partly infer missing context, explicit 3D mapping and dedicated memory modules are vitalespecially in modular pipelinesfor both coarse route planning and fine-grained trajectory adherence."
        },
        {
            "title": "4 Related Work",
            "content": "Vision-and-Language Navigation (VLN). Vision-and-Language Navigation (VLN) is first well defined by R2R [2], which is navigation benchmark with detailed language descriptions and visual observations. Based on indoor tasks, plethora of methods [33, 12, 17, 6, 43, 8, 20, 7, 15, 26, 45, 4, 44, 1] have been proposed to enable robots with navigation capacity. Despite progress indoors, transferring VLN to outdoor street environments poses new difficulties. The Touchdown dataset [3] addresses this by providing 9,326 instructiontrajectory pairs over New York City street panoramas. And Map2Seq [29] offers 7,672 pairs in different urban area with denser intersections and more eclectic instruction styles. Several recent works have specifically targeted vision-and-language navigation in outdoor urban environments [31, 21, 39, 13, 37, 35, 10, 11]Despite the aforementioned work, handling long-instruction tasks in urban environments remains significant challenge. Modular vs. End-to-End VLN Architectures. VLN systems have traditionally fallen into two broad paradigms: modular pipelines and end-to-end models. Classic VLN architectures decompose navigation into separate perception, mapping, planning and control stages. For example, early works build semantic map, apply graph search for route planning, and execute low-level controllers to follow waypoints [23, 24]. While modularity affords interpretability and the ability to swap in specialized algorithms , it often struggles with maintaining long-horizon consistency. In contrast, end-to-end VLN models learn direct mapping from image and language inputs to actions. Sequenceto-sequence agents [30] employ cross-modal Transformers to attend jointly over instruction tokens. More recently, large language models have been co-opted for VLN[27, 42]. VELMA [31] and NavGPT [45] inject visual descriptions into an LLM prompt and decode actions autoregressively, while FLAME [39] integrates frozen vision encoders with cross-attention layers in MLLM. End-to-end models simplify training and inference but still rely on fixed-size context windows and lack explicit spatial indexing or mechanisms to retrieve observations beyond the immediate past. Neither paradigm fully addresses the need for long-term spatial memory and multi-scale consistency. Bridging these gaps by combining structured spatial maps with learned memory motivates our Mem4Nav design. Spatial Representation Methods in VLN. core requirement for effective VLN in large-scale outdoor environments is spatial representation that supports both fine-grained local lookups and global route planning. Point-cloud representations capture arbitrary 3D structure via sets of 3D points with associated descriptors [38]. While flexible, point-cloud indexing often relies on external KD-trees or hashing, trading off retrieval speed. Complementing metric maps, topological graphs abstract environments into nodes at semantic decision points and weighted edges for traversability [36, 40]. Such graphs support fast lookups of high-level route segments.In Mem4Nav, we harness the complementary strengths of sparse octrees and landmark graphs, coupling them via virtual memory tokens to achieve both microscopic indexing and macroscopic planning in unified 3D representation. Memory Mechanisms in VLN Memory augmentation has emerged as key enabler for grounding navigation decisions in both recent observations and distant past experiences. Simple caches store visual features linked to relative coordinates, allowing immediate landmark re-recognition [34]. Recent work has proposed methods for storing memories over longer horizons and feeding these memories into the navigation system before decision making[41]. However, because this memory is injected directly into the prompt and relies on simple context window, it does not adapt well to long-distance navigation in urban environments. Our Mem4Nav system bridges this gap by embedding reversible memory tokens directly into spatial representation."
        },
        {
            "title": "5 Conclusion",
            "content": "We have presented Mem4Nav, unified long-short memory architecture that endows VLN agents with hierarchical spatial recall and adaptability. By embedding reversible memory tokens within sparse octree and semantic topology graph, and coupling these with frequencyrecency short-term cache, Mem4Nav supports both lossless long-term storage and fast local lookup. Our extensive experiments on Touchdown and Map2Seq demonstrate that Mem4Nav consistently boosts navigation success rates, reduces goal distance, and improves trajectory alignment across modular, LLM, and MLLM based pipelines under identical training budgets. Component ablations further reveal that each elementthe octree, the graph, long-term tokens, and the cachecontributes critically to performance, with the largest gains seen in fully nonend-to-end systems. In future work, we plan to extend Mem4Nav to multi-agent coordination and real-world robotic deployments, exploring adaptive memory consolidation and lifelong learning in dynamic urban scenes."
        },
        {
            "title": "References",
            "content": "[1] Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, and Liang Wang. Etpnav: Evolving topological planning for vision-language navigation in continuous environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36743683, 2018. [3] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1253812547, 2019. [4] Jiasheng Chen, Boran Lin, Renda Xu, Zheyuan Chai, Xiaojun Liang, and Kwan Yee Kenneth Wong. Mapgpt: map-guided prompting with adaptive path planning for vision-and-language navigation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 97969810, 2024. [5] Kevin Chen, Junshen Chen, Jo Chuang, Marynel Vázquez, and Silvio Savarese. Topological planning with transformers for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1127611286, 2021. [6] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation, 2023. [7] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Think global, act local: Dual-scale graph transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1653716547, 2022. [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Instructblip: Towards general-purpose Wang, Boyang Li, Pascale Fung, and Steven Hoi. vision-language models with instruction tuning, 2023. [9] Yi Du, Taimeng Fu, Zhuoqun Chen, Bowen Li, Shaoshu Su, Zhipeng Zhao, and Chen Wang. Vl-nav: Real-time vision-language navigation with spatial reasoning, 2025. [10] Jie Feng, Tianhui Liu, Jun Zhang, Xin Zhang, Tianjian Ouyang, Junbo Yan, Yuwei Du, Siqi Guo, and Yong Li. Citybench: Evaluating the capabilities of large language model as world model. arXiv preprint arXiv:2406.13945, 2024. [11] Jie Feng, Jinwei Zeng, Qingyue Long, Hongyi Chen, Jie Zhao, Yanxin Xi, Zhilun Zhou, Yuan Yuan, Shengyuan Wang, Qingbin Zeng, et al. survey of large language model-powered spatial intelligence across scales: Advances in embodied agents, smart cities, and earth science. arXiv preprint arXiv:2504.09848, 2025. [12] Chen Gao, Xingyu Peng, Mi Yan, He Wang, Lirong Yang, Haibing Ren, Hongsheng Li, and Si Liu. Adaptive zone-aware hierarchical planner for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1491114920, 2023. [13] Yunpeng Gao, Zhigang Wang, Linglin Jing, Dong Wang, Xuelong Li, and Bin Zhao. Aerial vision-and-language navigation via semantic-topo-metric representation guided llm reasoning. arXiv preprint arXiv:2410.08500, 2024. [14] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Eric Wang. Vision-and-language navigation: survey of tasks, methods, and future directions. arXiv preprint arXiv:2203.12667, 2022. [15] Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, and Cordelia Schmid. In Proceedings of the Airbert: In-domain pretraining for vision-and-language navigation. IEEE/CVF international conference on computer vision, pages 16341643, 2021. 10 [16] Yicong Hong, Zun Wang, Qi Wu, and Stephen Gould. Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1543915449, 2022. [17] Jingyang Huo, Qiang Sun, Boyan Jiang, Haitao Lin, and Yanwei Fu. Geovln: Learning geometry-enhanced visual representation with slot attention for vision-and-language navigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2321223221, 2023. [18] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the navgraph: Vision-and-language navigation in continuous environments. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVIII 16, pages 104120. Springer, 2020. [19] Shuhei Kurita and Kyunghyun Cho. Generative language-grounded policy in vision-andlanguage navigation with bayes rule. arXiv preprint arXiv:2009.07783, 2020. [20] Xiangyang Li, Zihan Wang, Jiahao Yang, Yaowei Wang, and Shuqiang Jiang. Kerm: Knowledge enhanced reasoning for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25832592, 2023. [21] Shubo Liu, Hongsheng Zhang, Yuankai Qi, Peng Wang, Yanning Zhang, and Qi Wu. Aerialvln: In Proceedings of the IEEE/CVF International Vision-and-language navigation for uavs. Conference on Computer Vision, pages 1538415394, 2023. [22] Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, and Liang Lin. Multimodal embodied interactive agent for cafe scene. arXiv e-prints, pages arXiv2402, 2024. [23] Piotr Mirowski, Matthew Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to In Advances in neural information processing systems, navigate in cities without map. volume 31, 2018. [24] Abtin Parvaneh, Ehsan Abbasnejad, Damien Teney, Javen Shi, and Anton Van den Hengel. Counterfactual vision-and-language navigation: Unravelling the unseen. In Advances in Neural Information Processing Systems, volume 33, pages 52965307, 2020. [25] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation, 2024. [26] Yuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Anton Van Den Hengel, and Qi Wu. The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16551664, 2021. [27] Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu. March in chat: Interactive prompting for remote embodied referring expression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1575815767, 2023. [28] Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, and Angel Chang. Languagealigned waypoint (law) supervision for vision-and-language navigation in continuous environments. arXiv preprint arXiv:2109.15207, 2021. [29] Raphael Schumann and Stefan Riezler. Generating landmark navigation instructions from maps as graph-to-text problem. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 489502, 2021. [30] Raphael Schumann and Stefan Riezler. Analyzing generalization of vision and language navigation to unseen outdoor areas, 2022. 11 [31] Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and William Yang Wang. Velma: Verbalization embodiment of llm agents for vision and language navigation in street view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1892418933, 2024. [32] Dhruv Shah, Błazej Osinski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Conference on robot learning, pages 492504. PMLR, 2023. [33] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. [34] Yanjun Sun, Yue Qiu, and Yoshimitsu Aoki. Dynamicvln: Incorporating dynamics into visionand-language navigation scenarios. Sensors, 25(2), 2025. [35] Hong Tian, Jinyu Meng, Wenguan Zheng, Ya Li, Jie Yan, and Yichao Zhang. Loc4plan: Locating before planning for outdoor vision and language navigation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 40734081, 2024. [36] Zehao Wang, Mingxiao Li, Minye Wu, Marie-Francine Moens, and Tinne Tuytelaars. Instruction-guided path planning with 3d semantic maps for vision-language navigation. Neurocomputing, 625:129457, 2025. [37] Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, and Shuqiang Jiang. Lookahead exploration with neural radiance representation for continuous vision-language In Proceedings of the IEEE/CVF conference on computer vision and pattern navigation. recognition, pages 1375313762, 2024. [38] Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang. Sim-to-real transfer via 3d feature fields for vision-and-language navigation, 2024. [39] Yunzhe Xu, Yiyuan Pan, Zhe Liu, and Hesheng Wang. Flame: Learning to navigate with multimodal llm in urban environments, 2025. [40] Tatiana Zemskova and Dmitry Yudin. 3dgraphllm: Combining semantic graphs and large language models for 3d scene understanding, 2024. [41] Qingbin Zeng, Qinglong Yang, Shunan Dong, Heming Du, Liang Zheng, Fengli Xu, and Yong Li. Perceive, reflect, and plan: Designing llm agent for goal-directed city navigation without instructions, 2024. [42] Jiaming Zhang, Kun Wang, Renda Xu, Guangyao Zhou, Yuxuan Hong, Xi Fang, Qi Wu, Zhong Zhang, and Wei He. Navid: Video-based vlm plans the next step for vision-and-language navigation. arXiv preprint arXiv:2403.06323, 2024. [43] Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning generalist model for embodied navigation, 2024. [44] Guangyao Zhou, Yuxuan Hong, Zheyuan Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pages 260278. Springer, 2024. [45] Guangyao Zhou, Yuxuan Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 76417649, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Algorithm in Detail A.1.1 Sparse Octree Leaf Insertion and Update We discretize 3D world coordinates into hierarchical octree of maximum depth Λ. Each node at level ℓ {0, . . . , Λ} represents an axisaligned cube of side length L/2ℓ. Only leaves that have been visited or contain relevant observations are instantiated and stored in hash table for O(1) lookup. Morton Key Computation. Given continuous agent position pt = (xt, yt, zt), we quantize to integer coordinates pt = (xt 2Λ/L, . . .) {0, . . . , 2Λ 1}3, then interleave bits to form Morton code (Z-order curve) κ(pt) = InterleaveBits(xt, yt, zt) {0, . . . , 23Λ 1}. This single integer κ uniquely identifies the leaf voxel at depth Λ. Octree Leaf Update. On each visit to pt: Compute leaf key κt. If κt not in hash table H, create new leaf entry Oκt = {θr κt cube bounds. Retrieve θr θr Fuse current embedding vt into memory via reversible update: κt. , θw κt , Bκt}, where Bκt stores the θw κt R(cid:0)θr; vt (cid:1), θr κt θw κt . All operations (hash lookup, token update) cost O(1) average time; the Morton code computation and bit interleaving cost O(Λ). Monocular Depth Estimation with UniDepth To recover metric depth from single RGB panoramas, we adopt UniDepth, universal monocular metric depth estimator that directly predicts dense 3D points without requiring known camera intrinsics at test time. UniDepth incorporates self-promptable camera module that outputs dense spherical embedding of azimuth and elevation angles, which conditions depth module via cross-attention, and uses pseudo-spherical (θ, ϕ, zlog) output representation to disentangle camera pose from depth prediction :contentReference[oaicite:0]index=0. geometric-invariance loss further enforces consistency between depth features under different geometric augmentations :contentReference[oaicite:1]index=1:contentReference[oaicite:2]index=2. Integration into Mem4Nav. At each time step t, given the current panorama It, we run UniDepth to obtain dense depth map Dt and the camera embedding Ct. We then unproject each pixel (u, v, id) via the predicted pseudo-spherical outputs to form local point cloud Pt = (cid:8)(xi, yi, zi) (cid:12) (cid:12) (ui, vi, zi) Dt (cid:9) , which supplies the z-coordinate for Morton code quantization in the sparse octree. We augment the visual feature vector vRGB = MLP(cid:0)CA(Ft, Ct)(cid:1)the cross-attention output of the depth moduleto form the fused embedding (from the perception backbone) with depth feature vector vDepth vt = (cid:2)vRGB ; vDepth (cid:3). This fused embedding is then written into both (i) the octree leaf at key κ(pt) and (ii) any semantic graph node created or updated at position pt, via the reversible token write operator . By integrating metric depth in this way, Mem4Navs hierarchical spatial structures gain true 3D scale awareness, improving both the precision of voxel indexing and the semantic graphs landmark localization. 13 Algorithm 1 Sparse Octree Leaf Insertion and Update 1: Input: position pt, embedding vt, hash table 2: Quantize(pt) 3: κ InterleaveBits(p) 4: if κ / then 5: 6: 7: end if 8: (θr, θw, B) H[κ] 9: θw R(θr; vt) 10: θr θw 11: H[κ].θr θr, H[κ].θw θw initialize θr, θw (0, Id) H[κ] (θr, θw, Bκ) A.1.2 Semantic Node & Edge Update Reversible write While the octree captures raw geometry, many navigation cues come from salient landmarks or decision points (e.g. intersections, points of interest). We maintain dynamic graph = (V, E) whose nodes correspond to important locations and whose edges (ui, uj) record traversability and cost. Node Creation and Token Fusion. Whenever the agents VLM detects trigger phrase (e.g. turn left at the statue) or high semantic change in embedding: : vt ϕ(u) δ where ϕ(u) is the aggregate descriptor of node u. If no existing node is within threshold δ, we create new node: (θr and add unew to V. Then we fuse the embedding: u; vt), R(θr θw unew. pt, u, θw ) (0, Id), θw θr . Edge Addition and Weighting. Each time the agent moves from node ut1 to ut, we add or update edge (ut1, ut) with weight wt1,t = α pt1 pt2 + β cinstr, where α, β balance Euclidean distance and instruction cost cinstr (e.g. number of turns). If the edge already exists, we average weights to smooth noise. create new node with u.p pt, random tokens {u} Algorithm 2 Semantic Node & Edge Update 1: Input: embedding vt, position pt, graph 2: found argminuV vt ϕ(u) 3: if vt ϕ(found) > δ then 4: 5: 6: else 7: 8: end if u, θw 9: (θr 10: θw R(θr 11: u.tokens (θr 12: if previous node uprev exists then 13: 14: 15: end if ) u.tokens u; vt), θr θw u, θw ) compute αpt uprev.p + β cinstr add/update edge (uprev, u) with weight u found A.1.3 LongTerm Memory Write and Retrieval The long-term memory module stores and retrieves spatially anchored observations in lossless, compressed form. When writing, each spatial elements existing memory tokens are updated by 14 fusing in the new observation embedding via reversible transform, replacing the old token. For retrieval, the current observation and position are projected into query vector, which is used to perform an approximate nearest-neighbor search over all stored tokens. The top matches are then inverted through the reversible transform to reconstruct their original embeddings and associated spatial information, which are returned for downstream reasoning. (θr, θw) Tokens(s) θw R(θr vt) θr θw Tokens(s) (θr, θw) Algorithm 3 LongTerm Memory Write and Retrieval 1: procedure LTM_WRITE(element s, embedding vt) 2: 3: 4: 5: 6: end procedure 7: procedure LTM_RETRIEVE(query (vt, pt)) 8: 9: 10: 11: Proj([vt; pt]) {si} HNSW_NN(q) for each si do (cid:98)vi R1(θr ) si (cid:98)pi πp((cid:98)vi), (cid:98)di πd((cid:98)vi) 12: 13: 14: 15: end procedure end for return {((cid:98)pi, (cid:98)di)} HNSW Index Configuration and Usage We use the Hierarchical Navigable Small World (HNSW) algorithm to index and query our s} Rd. HNSW organizes vectors into multi-layer graph where each collection of read-tokens {θr layer is small-world proximity graph, enabling logarithmic-scale search complexity and high recall in practice. Index Construction. HNSW incrementally inserts tokens one by one. Each new token θr is assigned maximum layer drawn from geometric distribution (probability = 1/M ), so higher layers are sparser. For each layer ℓ L: Starting from an entry point at the topmost nonempty level, perform greedy search: move to the neighbor closest (by cosine distance) to θr until no closer neighbor is found. Maintain candidate list of size efConstruction to explore additional connections beyond the greedy path. Select up to closest neighbors from the candidate list and bidirectionally link them with θr. This builds nested hierarchy of proximity graphs: the top layer provides long-range jumps, while lower layers refine locality. Querying (Search). To find the nearest tokens to query q: Entry-point jump: Begin at the top layers entry point; greedily traverse neighbors to approach q. Layer descent: At each lower layer, use the best candidate from the previous layer as the starting point, repeating the greedy step. Beam search at base layer: At layer 0, perform bestfirst search with dynamic queue of size efSearch. Expand the closest candidate by examining its neighbors, inserting unseen neighbors into the queue, and retaining the top efSearch candidates. Result selection: Once no closer candidates remain or budget is exhausted, output the top tokens from the queue. Hyperparameters and Complexity. M: maximum number of neighbors per node (e.g. 64). efConstruction: candidate list size during insertion (e.g. 500), trading off build time vs. graph quality. efSearch: candidate list size during queries (e.g. 200), controlling recall vs. search latency. A.1.4 ShortTerm Memory Insert & Retrieve The short-term memory module maintains compact, fixed-size buffer of the most recent observations relative to the agents current position. Whenever the agent perceives new object, the module computes its position with respect to the current node and checks if an entry for that object already exists. If it does, the entry is refreshed with the latest embedding and timestamp and its access count is increased. If the object is new and there is still room in the buffer, new entry is appended. Once the buffer is full, the least valuable entrydetermined by balance of how often and how recently it has been usedis removed to make space for the new observation. When the agent needs to recall local context, the module filters entries within small spatial neighborhood of the agents position and returns those whose stored embeddings best match the current observation. This mechanism ensures fast, spatially anchored retrieval without unbounded memory growth. Insertion and Update. Compute relative position prel. If an entry with same object exists, update its v, τ , and increment freq. Else if STM < K, append new entry with freq = 1. Otherwise, evict eevict and insert new entry. else else if STM < then append = (o, prel, vt, t, freq = 1) ei.v vt, ei.τ t, ei.freq+ = 1 prel pt puc if exists ei.o = then Algorithm 4 ShortTerm Memory Insert & Retrieve 1: procedure STM_INSERT((o, pt, vt)) 2: 3: 4: 5: 6: 7: 8: 9: end if 10: 11: end procedure 12: procedure STM_RETRIEVE(vt, pt) 13: 14: 15: 16: 17: end procedure qrel pt puc {ei : ei.prel qrel ϵ} compute si = cos(vt, vi) for ei return topk entries by si evict arg mini insert new (cid:2)λ freqi (1 λ)(t τi)(cid:3) A.2 More Details on Implementation and Backbones This appendix provides the full implementation details for all three backbone agents and the shared training regimen. Readers are referred to the main text (Section 3.2) for concise summary; here we enumerate every architectural choice, hyperparameter, and integration point. A.2.1 Hierarchical Modular Pipeline Open-Vocabulary Perception Module We preprocess each panoramic observation by extracting five overlapping 90 crops at headings spaced by 45. Each crop is passed through GPT-4V to generate free-form scene description, then through GroundingDINO (confidence threshold 0.4) and Segment Anything to obtain open-vocabulary object detections with fine-grained masks. Simultaneously, the RGBD image (512512, 90 FOV) is projected into local point cloud using known camera intrinsics and the agents pose. The resulting captions, object labels, and local 3D points are concatenated into semantic vectors (512 d) that serve as the perception output. 16 Hierarchical Semantic Planning From the perception vectors, we prompt GPT-4V with structured JSON template to extract an ordered list of landmarks mentioned in the instruction. For each landmark, we group the relevant 3D points and object detections to form semantic region proposal. Once the landmark sequence is obtained, we decompose each segment into series of waypoint goals: first selecting the nearest graph node or region centroid, then planning grid-based path using on 0.5 resolution lattice and motion primitives of forward or 15 turns. This three-tiered planninglandmark ordering, region centroids, and primitive-level pathensures both high-level coherence and low-level feasibility in outdoor environments. Reasoning and Decision Integration At each step, the current perception embedding, the next waypoint from the semantic planner, and any retrieved memory summaries are combined into single context vector. We first attempt to retrieve from the short-term cache (capacity 128, FLFU policy with equal weight on frequency and recency, spatial radius 3 m); on cache miss we fall back to an HNSW-based long-term lookup (index size 10 K, retrieve top 3). Retrieved summaries are rendered as concise natural-language bullets under Past memory: in the GPT-4V prompt. The final prompt (capped at 512 tokens) is fed into GPT-4V with greedy decoding setting (temperature 0.0) and constrained vocabulary mask allowing only {forward,left,right,stop}. This unified prompt-based decision ensures that modular perception, planning, and memory seamlessly inform each navigation action. Integration of Mem4Nav At each time step, the current visual embedding is written into both the sparse octree and the semantic topology graph via reversible memory tokens, and the same embedding is inserted into the short-term cache (evicting entries according to the replacement policy when full). When the hierarchical planner emits the next waypoint, we first query the STM for any recent observations . If fewer than two relevant entries are found, we fall back to an HNSW-based LTM lookup over all read-tokens in the octree and graph (index size 10 K), decode the selected tokens back into spatial and descriptor information, and render them as concise Past memory: bullets. These memory summaries, together with the next waypoint and the perception output, are concatenated into the GPT-4V prompt (capped at 512 tokens) before decoding. By injecting both fine-grained local context and lossless long-range recalls into the decision promptwhile still respecting our constrained action vocabularyMem4Nav seamlessly augments the modular pipeline with structured, multi-scale memory. A.2.2 VELMA Backbone (Detailed) In this section we provide the full implementation details for the VELMA backbone used in our experiments, so that readers can exactly reproduce the behavior and performance reported in the main text. Figure 4: Overview of VELMA Model Checkpoint and Dependencies We use the CLIP-ViT/L14 vision encoder and the LLaMA-7B language model decoder. All weights are frozen except where noted below. Visual Preprocessing Input panoramas: we sample four 90-FOV crops from each 360 panorama at headings {0, 90, 180, 270}. Resize & normalize: each crop is resized to 224 224 pixels, normalized with ImageNet mean/std. Patch tokenization: the CLIP-ViT/L14 splits the 224 224 input into 14 14 patches (total 196 tokens), each mapped to 768-dim embedding. Memory Integration STM retrieval: we compute cosine similarity between the current CLIP-ViT embedding for each detected object and each STM entrys 256-dim vector. We select up to = 4 entries with similarity > 0.5. LTM retrieval: if fewer than 2 STM entries pass the threshold, we query the HNSW index built over all read-tokens (d = 256, M=16, ef=200), retrieve the top 3, and run the reversible Transformer inverse to decode their stored embeddings back into (p, desc, state) triples. Natural-language summarization: each retrieved entry is rendered as one-line bullet under Past memory: using the template: at (xj, yj): saw oj, status sj.ȷ with xj, yj rounded to one decimal. Decoding and Action Selection The full prompt (up to 512 tokens) is fed into the LLaMA-7B decoder with temperature of 0.0 (greedy). We apply constrained vocabulary mask so that only the four actions {forward, left, right, stop} can be generated. The single generated token is mapped directly to the discrete action. A.2.3 FLAME Backbone (Detailed) The FLAME backbone is built upon the Otter architecture (CLIP-ResNet50 encoder + LLaMA7B decoder) with strided cross-attention. Below we describe every component and training detail necessary for exact reproduction. Model Architecture The vision frontend is CLIP ResNet-50 network, taking each pano crop of size 224 224 and extracting 7 7 2048 feature map. linear projection reduces each spatial vector to 512 d: ft,i = Wproj (cid:0)ResNet50(I rgb t,i )(cid:1) + bproj, = 1, . . . , 49. These 49 patch embeddings ft,1:49 R512 form the visual token sequence Ot. The language backbone is 7 LLaMA model (32 layers, dmodel = 4096, 32 heads). We interleave four cross-attention modules into layers 8, 12, 16, and 20. Each cross-attention takes as queries the LLaMA hidden states hℓ Rd and as keys/values the concatenation of: (cid:3) RK512, (cid:2) OtK+1, OtK+1+2, . . . , Ot with = 5 and temporal stride 2. This strided attention allows the model to attend past panoramas at intervals, reducing quadratic cost while preserving longer-range context. Memory Integration After obtaining the visual token sequence Ot, we perform memory retrieval: Long-Term Memory (LTM): Query with the current merged embedding qt = MLP([ht1; t]) }. Retrieve the top = 3 tokens against the HNSW index of all stored read-tokens {θr θr j1 R256. , θr , θr j2 Short-Term Memory (STM): Filter cache entries by relative coordinate proximity pt puj < ϵ, compute cosine similarity with qt, and select the top = 2 vectors st,1, st,2 R256. 18 Figure 5: Overview of FLAME We then augment the cross-attention key/value inputs by concatenating these memory vectors along the spatial axis: KVt = (cid:2)ft,1:49; θr ; st,1:2 j1:3 (cid:3) R(49+5)512, with learnable linear mappings applied to project θr and into 512 d. A.3 Retrieval Latency: Implementation and Impact on Navigation To assess both the efficiency and practical effect of Mem4Navs memory subsystem, we implemented the following: STM Lookup: Spatial filtering via custom CUDA kernel that maintains an array of relative positions and applies boolean mask. Cosine-similarity ranking using cuBLAS batched GEMM for maximum throughput. LTM Retrieval: HNSW index built with the GPU-accelerated hnswlib, parameters = 16, efConstruction = 200, efSearch = 200. We measure the average wall-clock time of both short-term and long-term memory components on an NVIDIA A100 GPU over 1,000 consecutive retrieval operations. Table 3: Memory retrieval latency for STM and LTM components Component Parameter Avg. Latency (ms) STM Lookup LTM Retrieval (total) Cache size = 64 Cache size = 128 Cache size = Index size = 5,000 Index size = 10,000 Index size = 20,000 0.9 1.2 2.2 21.7 24.0 31.7 STM lookup remains below 2 ms for cache sizes up to 128 entries and only doubles at 256 entries, indicating very fast local context filtering. LTM retrieval, which includes HNSW nearest-neighbor search plus reversible decoding, stays under 32 ms even with 20 000 tokens indexed. Together, these results confirm that Mem4Navs two-tier memory can be queried in under 35 ms per decision stepwell within the 200500 ms action interval typical of real-time street-view navigation. Retrieval remains under 30 ms per decision step, dominated roughly equally by the ANN search and reversible decoding. Table 4: Average retrieval latency (ms) for STM and LTM components"
        },
        {
            "title": "STM lookup\nLTM HNSW search\nLTM decoding",
            "content": "Cache size = 128 Index size = 10,000 STM + LTM (total) 1.2 11.0 13.0 25.2 Impact on Navigation Performance. To quantify how retrieval latency translates into end-to-end performance, we ran the Hierarchical Modular Pipeline on Touchdown Dev under three retrieval strategies (all with identical memory contents, differing only in retrieval implementation and speed). We measured Task Completion (TC) and normalized DTW (nDTW): Table 5: Navigation performance vs. retrieval method on Touchdown Dev"
        },
        {
            "title": "Latency",
            "content": "TC (%) nDTW (%) Linear scan (10K entries) KD-tree (10K entries) Mem4Nav (STM + LTM) 120.0 ms 30.5 ms 25.2 ms 33.1 40.3 45.2 49.2 53.1 59. Faster retrieval not only reduces decision-step latency (enabling real-time operation) but also yields higher navigation accuracy, since slower methods force the agent to skip or delay memory lookups, degrading its ability to ground decisions in past context. Overall, these experiments demonstrate that Mem4Navs optimized two-tier memory retrieval is both efficient (under 30 ms) and crucial for maximizing end-to-end VLN performance in large-scale urban environments. A.4 Robustness to Depth-Estimation Noise We evaluate how errors in the UniDepth predictions affect Mem4Navs performance on the Touchdown Dev and Map2Seq Dev splits, using the FLAME + Mem4Nav pipeline under three depthdegradation conditions. All other components and hyperparameters are identical to the main experiments. Experimental Setup. Baseline (Clean): full-precision UniDepth depth maps (no corruption). Gaussian Noise: Depth pixel D(u, v) is perturbed by (0, 0.5 m), simulating sensor noise. Dropout Mask: randomly zero out 20% of depth pixels per frame, simulating missing or invalid depth. For each condition, we back-project the corrupted depth maps into point clouds for octree construction, then run the standard Mem4Nav write/retrieve and FLAME action loop. Results. Table 6: Depth-Noise Ablation on Touchdown and Map2Seq Dev (FLAME + Mem4Nav). Touchdown Dev SPD nDTW TC Map2Seq Dev SPD nDTW TC Baseline (Clean) Gaussian Noise Dropout Mask 50.10% 9.01 65.05% 61.03% 5.87 80.40% 46.02% 9.42 61.12% 57.15% 6.13 75.47% 44.56% 9.80 58.97% 55.04% 6.42 73.05% Analysis. 20 Adding Gaussian noise (σ=0.5 m) to UniDepth outputs causes 4.08 pp drop in TC and 3.93 pp drop in nDTW on Touchdown, and similar degradations on Map2Seq, showing Mem4Navs sensitivity to depth precision. Randomly dropping 20% of depth further reduces performance (5.54 pp TC, 6.08 pp nDTW on Touchdown). These results underscore the need for robust depth estimation or uncertainty-aware fusion in future Mem4Nav extensions. A.5 Failure Cases Despite the substantial gains of Mem4Nav, our Touchdown and Map2Seq evaluations reveal three dominant failure modes: Figure 6: Representative Failure Cases of Mem4Nav. Despite substantial overall gains, we identify four dominant error modes: (a) Depth-induced mapping errors arise when monocular depth estimates misplace voxels on low-texture façades, corrupting both octree writes and lookups; (b) Memory retrieval misses for far-away references occur because distant landmarks lie outside instantiated spatial bins and yield no sufficiently similar tokens; (c) Semantic graph sparsity or ambiguity results when subtle or partially occluded landmarks (e.g. crosswalk markings) fail node creation, breaking planned routes; and (d) Memory retrieval misses under severe occlusion happen when landmarks hidden by overhead structures cannot be matched by either STM filtering or LTM HNSW search. Depth-induced mapping errors. Monocular depth estimates from UniDepth can be highly inaccurate in low-texture regions (e.g. blank building façades) or under extreme lighting, causing voxels in the sparse octree to be misplaced by several meters. These misregistrations propagate into both LTM writes and spatial lookups, leading the agent to misjudge its surroundings and execute incorrect turn or stop actions. Scenario: In the panorama shown in the case, the agent faces long stretch of repetitive, uniform window façades with minimal texture. Voxel Misregistration: This bias shifts the corresponding octree leaves by 23 voxels (leaf size = 1 m), causing building-edge voxels to be placed several meters into the adjacent roadway. Graph and Memory Impact: The semantic graph creates the next intersection node 4 too far ahead, so the agent believes it must walk past the actual corner. The STM cache retrieves recent building edge observations at the wrong relative coordinates, confusing the local planner. Long-term recall of the corner store landmark is falsely triggered before the real intersection, leading to premature turn. Semantic graph sparsity or ambiguity. Our threshold-based node creation occasionally fails to instantiate graph nodes for subtle or partially occluded landmarks (e.g. crosswalk markings, small storefront signs). When required intersection node is missing, the planner cannot recover the intended route sequence, resulting in the agent overshooting turns or taking suboptimal detours. Scenario: In the panorama of complex intersection with multiple crosswalk markings , the agents landmark detector labels each zebra-stripe segment as distinct crosswalk object. Graph Node Explosion: Our descriptor-distance threshold δ causes each segmented stripe to spawn separate node, resulting in 12 crosswalk nodes clustered around the same intersection rather than single intersection node. Missing Intersection Node: Because no single node accumulates enough repeated visits (all crosswalk nodes receive only one write), the true intersection landmark is never consolidated, leaving gap in the semantic graph at that decision point. Routing Consequence: The global planner fails to include the intended turn at crosswalk step, treating the next valid node as two blocks ahead. The local planner, flooded with near-duplicate crosswalk STM entries at slightly different offsets, cannot decide when to pivot, causing the agent to overshoot the turn by an average of 5.2 m. Memory retrieval misses. The STM cache sometimes fails to match recently observed landmarks when the agents viewpoint shifts rapidly. Likewise, under heavy index loads, the HNSW ANN search can return suboptimal long-term tokens, causing the policy to fall back on stale or irrelevant memories. Landmarks partially or fully blocked by passing vehicles, pedestrian crowds, or temporary structures (e.g. scaffolding) reduce feature visibility, causing both STM spatial filters and LTM similarity search to miss the stored tokens. For instance, the target landmark (archway under the bridge) is largely hidden by the overhead girders. The visual detector only extracts low-contrast fragments, producing an embedding that differs significantly from the original octree and graph tokens. During STM spatial filtering the relative positions match, but the cosine similarity falls below threshold. Likewise, the HNSW search in LTM does not return the hidden archway token. Consequently, the agent cannot recall the landmark and incorrectly continues past the underpass, deviating from the instructed path. Furthermore, instructions that refer to distant landmarks beyond the STM radius and whose tokens in LTM are too sparsely distributed in the octree or graph layers, so even HNSW search returns no sufficiently close vectors. Both issues lead to degraded local decisions and trajectory drift. A.6 Real-World Deployment Deployment Setup. We ported Mem4Nav onto robotic dog under ROS Melodic with RGB camera. The onboard RGB camera captures 125 field-ofview images at 10 Hz, which are processed by UniDepth for per-pixel monocular depth estimation. For trialing, we manually designed the following six-step navigation protocol through mixed urban block: 1. Proceed eastbound through the cross-type intersection. 2. Maintain eastbound traversal at the T-junction adjacent to the grasslands. 3. Execute left turn (southward) at the T-junction located at the northeastern vertex of the Sports Instructors Training Base. 4. Just before the next intersection, observe blue bike on the right in front of stadium. 5. Initiate left turn (southward) at the T-junction at the northwestern quadrant of brown building, where tall man is leaning on tree. 6. Terminate navigation at the designated coordinates: playground with an orange safety light. Experimental Results. We conducted 30 real-world trials across varying times of day and pedestrian densities. Mem4Nav achieved success rate of 70% (21/30 runs) defined by stopping within 3 of the goal. 22 Figure 7: Route of campus real world trial of MemNav. Failure Cases. Among the nine failures, two predominant modes emerged: (1) Depth-induced mapping drift: uniform asphalt and large blank façades caused UniDepth errors, leading to misregistered octree voxels and missed turn decisions; (2) Dynamic occlusions: clusters of pedestrians and parked vehicles intermittently blocked key landmarks, resulting in STM cache misses and incorrect semantic graph traversals. These highlight the need for robust depth correction and dynamic-object filtering in future real-world deployments. Figure 8: Failure Cases. (a) The agent came to halt at busy uncontrolled intersection, where the substantial volume of vehicular and pedestrian traffic rendered it incapable of determining an opportune moment to proceed. (b) The lights from high-velocity oncoming vehicles compromised the agents semantic information processing capabilities, requiring experimenter assistance for safe roadside repositioning. 23 A.7 Limitations Despite the strong empirical gains demonstrated by Mem4Nav, our approach has several important limitations: Limited Evaluation Scope We evaluate exclusively on two street-view VLN benchmarks (Touchdown, Map2Seq) and three backbone agents. While these cover range of urban panoramas, they do not reflect other outdoor settings (e.g. suburban roads, rural paths) or indoor scenarios. Hyperparameter Sensitivity Mem4Nav introduces several thresholds and capacitiessemantic distance δ, STM size K, HNSW parameters (M, efSearch). Performance can vary significantly if these are not carefully tuned for the target environment. Automating their selection or adapting them online is left to future work. Dependence on Monocular Depth Quality We rely on UniDepth to recover metric depth from single RGB panoramas. In practice, monocular depth estimators can fail in low-texture regions (e.g. blank walls), extreme lighting (glare or shadows), reflective surfaces (glass, water), or dynamic scenes (moving vehicles, pedestrians). Depth errors propagate directly into our sparse octreemisplaced voxels can degrade memory write and retrievaland into the semantic graph via incorrect landmark geolocations. Robustness to such failures remains an open challenge. Computational and Memory Overheads Although our retrieval latency (25 ms) is compatible with 200500 ms action loop, both octree indexing and HNSW search scale with the number of visited voxels and tokens. In large-scale or continuous operation, memory footprint and GPU load may become prohibitive. Addressing these limitations will be essential to deploy Mem4Nav in real-world robotic or assistive applications, where sensor noise, environmental dynamics, and computational constraints are more severe than in our controlled benchmarks."
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China"
    ]
}