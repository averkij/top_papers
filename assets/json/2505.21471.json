{
    "paper_title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration",
    "authors": [
        "Zijun Liu",
        "Zhennan Wan",
        "Peng Li",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang",
        "Yang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, $\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls $\\textit{within or exceeds the context window}$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 7 4 1 2 . 5 0 5 2 : r Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration Zijun Liu1*, Zhennan Wan1*, Peng Li2, Ming Yan3, Ji Zhang3, Fei Huang3, Yang Liu1,2 1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University 2Institute for AI Industry Research (AIR), Tsinghua University 3Tongyi Lab, Alibaba Group zj-liu24@mails.tsinghua.edu.cn, wanzn21@mails.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as new paradigm to handle massive input in distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop multi-agent framework, EXTAGENTS, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longercontext training. Benchmarked with our enhanced multi-hop question answering test, Bench+, and other public test sets including long survey generation, EXTAGENTS significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls within or exceeds the context window. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have recently witnessed dramatic progress in both parameter scale and context length, culminating in context windows that now span more than book-length of text [24]. Yet even these impressive limits remain insufficient for many real-world tasksmulti-hop question answering with Internet, reasoning over enterprise knowledge bases, or writing long survey papers based on massive academic researchwhere more external knowledge input could often result in better outcomes. Especially, recent research on post-training LLMs to generate long chains of thoughts on reasoning [5, 6] and information seeking [710] tasks, have shown that scaling the amount of retrieved knowledge within the context window could lead to better task performance [11]. Figure 1: Performance of scaling external knowledge input with EXTAGENTS and LLMMapReduce [1] on Bench+. *Equal contribution. Code and data are available at https://github.com/THUNLP-MT/ExtAgents. Preprint. For larger input beyond the context window length, the situation is more complicated. When such knowledge is crudely truncated or compressed, essential evidence is lost and downstream performance suffers. natural solution is to train ever longercontext models [1215], but this is economically prohibitive and experimentally brittle: (i) the quadratic complexity of attention [16] becomes intractable; and (ii) longercontext training data is scarce. Consequently, practitioners turn to retrievalaugmented generation (RAG) [1720] or context compression [2125] pipelines. Unfortunately, both strategies inevitably introduce an information loss: RAG is limited by ranking errors, while compressors may discard subtle cues that are only useful once the reasoning chain unfolds. Recent approaches [2628, 1] let LLM-based agents collaborate to process long contexts distributedly, reaching state-of-the-art performance on long-context tasks. In this work, we take step further by asking question: Could LLMs consistently leverage task performance by scaling the amount of external knowledge input beyond the context window? Achieving high scalability of external knowledge implies two requirements: (i) scalable context extension method needs to accept the massive input, and (ii) the knowledge should be effectively integrated in reasoning processes to improve the performance. Since it is impractical to re-train short-context LLMs, we mainly focus on the scalability of inference-time knowledge integration beyond context windows. We focus on few tasks that require massive external knowledge, including multi-hop question answering (QA), both over long documents and large knowledge bases, and long survey generation. We found current benchmarks on long-context tasks constructed with biases, that quantity of queries could be answered by sweeping small context window over the attached document. For comprehensive validation, we enhance the existing long-context benchmark, Bench [29], with an automated pipeline, to obtain long-document-based multi-hop QA test set, Bench+, alongside with public multi-hop QA [30] and long survey generation [31] benchmarks. In preliminary experiments, we find that the current state-of-the-art LLM-based multi-agent system [1] fails to consistently improve task performance with scaled external knowledge input, and even degrades the performance compared to truncated input (Figure 1). We systematically analyzed existing multi-agent methods, and then spotted two core bottlenecks in shared components of these methods: (i) knowledge synchronization that agents comprehend the distributed contexts and provide condensed information for reasoning, where the bottleneck is the bandwith of accessible agents for each agent. and (ii) knowledge-integrated reasoning, where the bottleneck is the ratio of irrelevant information in the reasoning process. To overcome the bottlenecks, we develop multiagent framework, EXTAGENTS. Following prior distributional paradigm, the framework partitions the full input into agent-specific context chunks, each sized to fit small window. EXTAGENTS simplifies the roles of agents into two: Seeking Agents and Reasoning Agent; featuring two key components: global knowledge synchronization, where Seeking Agents to globally exchange and update salient intermediate results instead of locally sharing entire context chunks [27, 1], and knowledge-accumulating reasoning, which gradually integrates and increases the updated knowledge from Seeking Agents to Reasoning Agent throughout multiple rounds of reasoning. We demonstrate the effectiveness and efficiency of EXTAGENTS with comprehensive experiments on the aforementioned benchmarks. We show that EXTAGENTS consistently improves task performance with scaled external knowledge input, outperforming the state-of-the-art non-training methods and achieves increasing performance when the input exceeds context windows. We show the generalization of EXTAGENTS across different QA and long generation tasks, and its compatibility with different LLM families. We also measure the efficiency gain of EXTAGENTS from high parallelism. In summary, our contributions are: We introduce and define the problem of scaling external knowledge input beyond context windows, filling critical gap in current LLM deployment. We also construct an enhanced long-context multi-hop QA benchmark, Bench+, for corresponding evaluation. We systematically study existing LLM-based multi-agent systems for context window extension, and overcome their bottlenecks by proposing novel framework, EXTAGENTS. We demonstrate the effectiveness and efficiency of EXTAGENTS on QA and survey generation tasks. With external knowledge input scaling beyond context windows, it consistently improves task performance and significantly outperforms the state-of-the-art methods."
        },
        {
            "title": "2 Related Work",
            "content": "Context Window Extension Methods for LLMs We categorize existing methods into three: Retrieval-Based Methods: For massive input breaking the context window, retrieval-augmented generation (RAG) [17, 18] is common solution to chunk the input into smaller pieces and retrieve relevant contexts through indexing [32], searching [20], or ranking [33]. The granularity ranges from token- [34] to document-level [19]. Recently, iterative retrieval is shown to be effective for multi-hop tasks [26] and with scaled retrieved documents [11]. Compression-Based Methods: Orthogonal to RAG, long contexts can be compressed into smaller representations, including parametric states [35, 2325, 36] and non-parametric summaries [37, 21, 22, 38], which are then fed into LLMs. However, the compression is often lossy due to the limited context window of compressor model, and is generally limited compared to LLM-based multi-agent systems [39] that process contexts distributedly. LLM-Based Multi-Agent Collaboration Methods: LLM-based multi-agent systems have emerged as new paradigm to handle massive input in distributional manner. We analyze existing methods [27, 28, 1, 39] in Section 3.2 in detail. Though the approach could be viewed as mixing retrieval and compression, it contains more nuanced decision making process. LLM-Based Multi-Agent Collaboration on General Tasks For general tasks, LLM-based multiagent systems [4046] have been proposed to collaboratively process task queries and workloads, resulting in improved performance. Different applications has been explored, including coding [47], science research [48], interactive decision making [49], embodied game playing [50], etc. Recent studies also post-train LLMs to enhance the collaboration process in various tasks [5154]."
        },
        {
            "title": "3 Scaling External Knowledge Input Beyond Context Windows of LLMs",
            "content": "3.1 Problem Definition Since real-world applications [7, 55, 56] frequently demand the integration of extensive external knowledge whose scale could dramatically surpass current context window limitations of LLMs, the need for systematic approach to effectively handle scalable external knowledge input is paramount. Formal View For each task query Q, given external knowledge source could be long document attached (K = fQ(q)) or serveral document pieces retrieved from large knowledge bases (K = fC(q)). In both cases, the knowledge source could be partitioned into chunks = {d1, d2, . . . , dN }, where di is chunk with length di and the total input length = (cid:80)N i=1di. For long-documentbased tasks, this could be done by simple splitting, and, as discussed in Appendix A, more sophisticated chunking methods are also available for future work; for knowledge bases C, chunk di could be retrieved document piece with further aggregation or splitting. The former is often used in QA tasks oriented to long documents [29], while the latter is common in open-domain knowledge-intensive tasks [30]. The query is processed by an LLM θ with maximum context length L, maxi{di} < (e.g., 32k, 128k, or more tokens), under the guidance of pre-defined prompts and workflows πθ, to give out answer y. In this work, we focus on tasks where the total length of knowledge source is much larger than the context window L, i.e., L. Figure 2: The illustration of scaling external knowledge input for context window extension methods for LLMs. Ideally, knowledgeintensive tasks, including QA and long generation, should benefit from scaled input. = πθ (cid:0)q, K(cid:1) with L. (1) The overall objective is to maximize the task performance with respect to the amount of Objective external knowledge input. For tasks with ground-truth answers y, the objective is formulated as: (cid:2)Scorepair (cid:0)y, y(cid:1)(cid:3) with fixed max{K}, θ, EqQ,K{fQ(q),fC(q)} (2) maximize π 3 Table 1: Comparisons of existing LLM-based multi-agent methods for context window extension and our EXTAGENTS on the bottlenecks in knowledge synchronization (sync.) and reasoning processes. Method Sync. Bandwidth Reasoning Context (Mr) Parallelized Component Chain of Agents [28] LongAgent [27] LLMMapReduce [1] ExtAgents (ours) 2 2 O( ) {mN,N } {mi,t}1iN,1tT {mi,T }1iN None Sync. Sync. Top2s ({mi,t}1iN ) Sync. & Reasoning where Scorepair(, ) is task-specific reference-based metric (e.g., F1 for QA, LLM-as-aJudge [57]). For tasks without clear ground truths, e.g., open-ended generation, the objective is maximize π EqQ,K{fQ(q),fC(q)} (cid:2)Scoresingle (cid:0)y(cid:1)(cid:3) with fixed max{K}, θ, (3) where Scoresingle() is reference-free metric (e.g., LLM with rating principles [31]). The control of maximum input length max{K} is achieved by appending truncation to fQ() or fC(). Noticeably, the setting of scaling external knowledge input is different from expanding the scale of retrieval knowledge bases [58], which does not increase inference costs of LLMs but of the retriever. We argue this is orthogonal to our primary goal towards the scalability of LLM-based agents. 3.2 Review of Existing Multi-Agent Methods In this section, we review existing LLM-based multi-agent systems for context window extension, including Chain of Agents [28], LongAgent [27], and LLMMapReduce [1]. These methods spin up team of LLM-based agents. Each agent is attributed local context chunk di, and the team is allowed to (1) exchange short messages and (2) collectively decide on the answer y. Specifically, iterative retrieval with multiple LLM instances [26, 11] could be viewed as special case of multi-agent systems, since each retrieval iteration incorporates new chunks of external knowledge. We conclude that existing multi-agent methods share two-stage pattern of knowledge synchronization and reasoning, where the former designed to comprehend the distributed contexts and provide related knowledge for the latter to produce the final answer. We follow Liu et al. [43] to incorporate timesteps for modeling agent communication, analyze the stages, and identify core bottleneck in each stage: 1. Knowledge Synchronization: At timestep , each agent ai,t digests its local chunk di and messages MGi,t1,t1 {mj,t1aj,t1 Gi,t1} from neighbourhood Gi,t1 = {aik1,t1, . . . , ai,t1, . . . , ai+k2,t1} of size Gi,t1, with the maximum maxi,t{Gi,t} termed bandwidth. Some original chunks from agents in Gi,t1 may also be included (DGi,t1,t {diai,t1 Gi,t1}). It then emits an updated information with single-turn prompting: mi,t = ai,t(q, DGi,t1,t, MGi,t1,t1). With smaller bandwidth, more timesteps are needed to synchronize the knowledge across all agents. We also find that the bandwidth of Chain of Agents and LongAgent is 2, and the bandwidth of LLMMapReduce is O( ), where is the expected length of single message. The values of bandwidth generally reflect the reported performance [1] of existing methods, and we conjecture that larger bandwidth leads to better performance. (4) 2. Knowledge-Integrated Reasoning: An agent ar collects subset of messages as the reasoning context Mr {mi,t}i,t and produces the task answer following workflow: = ar (cid:0)q, Mr (cid:1). (5) According to Jiang et al. [21], the ratio of irrelevant information in the reasoning process is key factor affecting the performance. Chain of Agents and LLMMapReduce default to put as much information as possible into the reasoning context, which may lead to information overload. Detailed description and comparison of implementation are elaborated in Appendix D. For practice, LLMMapReduce [1] reports the best performance among the three multi-agent methods on long-context tasks, and is the most feasible for scalable external knowledge input. Figure 3: Overview of EXTAGENTS: Our framework consists of multiple agents with fixed context windows, that collaboratively process (a) scalable external knowledge inputs beyond the context limit. It features (b) global knowledge synchronization, and (c) knowledge-accumulate reasoning processes. Moreover, EXTAGENTS support (d) both multi-hop QA and long survey generation tasks. For instance, Chain of Agents is limited to linear topology, sensitive to the order of chunks and essembling of reading paths; LongAgent conducts pairwise knowledge synchronization between two agents, which is expensive and inefficient for large-scale input. Due to the resource constraints and high costs of long-context inputs, we use LLMMapReduce as the main baseline in our experiments. 3.3 Challenge: Evaluation and Implementation of Scalable Approaches Bench+ Zhang et al. [29] proposed Bench, benchmark for evaluating LLMs on long-context tasks, which could be seamlessly extended to evaluate the scalability of LLM-based agent systems on external knowledge input. Nevertheless, we found that portion of samples have been constructed with biases, where the answer-related information is concentrated in small part of the attached document. These queries could be easily answered by sweeping small context window over the document. Thus, we adopt an automated pipeline to construct new long-document-based multi-hop QA test set, Bench+, requiring the benchmarked system to aggregate information across large segments of each document. Specifically, we use gpt-4o-mini-2024-07-18 [59] to examine whether the answer could be obtained in each 8k token segment of the document. If so, we discard the sample. The process has been performed on the En.QA subset of Bench, and the average length of samples is reduced  (Table 2)  , showing that some long samples from the original benchmark are biased. Besides, we also include Zh.QA subset for bi-lingual evaluation. However, we found that the enhanced dataset contains too few samples, causing instability in evaluation. Concretely, the sizes of En.QA and Zh.QA subsets are 157 and 56, respectively. Thus, for stable evaluation, we evaluate all methods on samples from Bench+ together with samples longer than 128k from the original Bench. The En.QA subset for evaluation contains 294 samples, whereas the Zh.QA subset contains 184 samples. Table 2: Statistical information of the En.QA subset in Bench [29] and Bench+. Bench Bench+ Bench Bench+ 1302k 904k #Tokens (Avg.) 194k 188k Benchmark #Samples 351 157 189 56 Zh.QA En.QA Subset Preliminary Experiments We test LLMMapReduce on our Bench+ benchmark with gpt4o-mini-2024-07-18, and find that the method fails to consistently improve task performance with gradually increasing external knowledge input from 8k tokens, as shown in Figure 1. When scaling the input beyond the context window of 128k tokens, the performance shows no advantage over directly inputting the truncated context, which does not meet our expectation for scalable approach."
        },
        {
            "title": "4 EXTAGENTS: A Scalable Solution via Multi-Agent Collaboration",
            "content": "To systematically address the scalability bottlenecks identified in existing LLM-based multi-agent methods (Section 3.2), we introduce EXTAGENTS, novel multi-agent framework specifically designed for effective and scalable inference-time knowledge integration beyond context windows (Figure 3). The primary objectives of EXTAGENTS are: (i) to process massive input within limited context windows, (ii) to achieve performance equivalent to or surpassing that of hypothetical infinite-context LLM, and (iii) to maintain efficiency through high parallelism. 4.1 Agent Profiles We adopt the distributional paradigm of LLM-based multi-agent systems [27, 28, 1] to partition the full input into agent-specific context chunks, each sized to fit small window (not necessarily equal to the context window of LLMs). EXTAGENTS simplifies agent roles into two clearly defined profiles, corresponding to the two stages of knowledge synchronization and reasoning, compatible to process any amount of external knowledge input (prompting templates in Appendix E): Seeking Agents: Implemented with prompting, Responsible for comprehending their assigned knowledge chunks and generating concise yet informative summaries and intermediate results. Optionally, Seeking Agents can also perform chunk exclusion when the knowledge is irrelevant to the task query, or rate the relevance and usefulness of the context chunk to the task query. Reasoning Agent: Tasked with integrating and synthesizing knowledge accumulated from Seeking Agents to generate the final answer. Reasoning Agent identifies the answerability and updates the outputs by accumulating knowledge across multiple rounds, ensuring comprehensive reasoning. Compatible for both multi-hop QA and long generation with switched task prompts. 4.2 Global Knowledge Synchronization Existing methods synchronize knowledge locally with limited agent interaction bandwidth, potentially restricting performance (Section 3.2). To overcome this, EXTAGENTS implements global knowledge synchronization. Each Seeking Agent summarizes its local context chunk into succinct messages and posts them onto shared scratchpad accessible by all other agents. Unlike previous methods [27, 1], which restrict agent interactions to local neighborhoods, our approach grants every agent global visibility, thus maximizing synchronization bandwidth and ensuring efficient propagation of salient information. Formally, each Seeking Agent ai,t at synchronization timestep updates its message as: mi,t = a(EA) i,t (q, di, Mt1), Mt1 = {mj,t1}N j=1 where Mt1 represents the global set of messages from the previous timestep, and EA stands for EXTAGENTS. This ensures comprehensive visibility and enhances collective comprehension. However, when the number of Seeking Agents is large, the amount of information exchanged can be overwhelming, breaking the context window. To mitigate this, each agent ai,t rates the relevance j=1 R+. The and usefulness of the messages in Mt to the task query, outputing scores {hj,t}N rating could be done by appending prompt, or by using separate metric tool, e.g., retrieval scores. Topk (Mt) = arg max ˆMtMt, ˆMt=k (cid:88) hj,t. j{jmj,t ˆMt} (6) By selecting the top-k pertinent messages with as large as possible, we maintain the global bandwidth and fit within the context window for knowledge synchronization at each timestep: mi,t = a(EA) i,t (q, di, Topk(Mt)), with + di + Topk(Mt) < (7) Alike LLMMapReduce, all Seeking Agents can run simultaneously in parallel at each timestep during knowledge synchronization, substantially reducing latency with high parallelism. 4.3 Knowledge-Accumulating Reasoning After each knowledge synchronization timestep, the reasoning process is initiated. To tackle the reasoning bottleneck caused by irrelevant or redundant information overload, Reasoning Agent 6 Table 3: Performance on Multi-Hop QA tasks with the optimal setting and the corresponding input length (#tokens). Method Direct Input Direct Input DRAG IterDRAG LLMMapReduce EXTAGENTS (ours) Direct Input DRAG IterDRAG LLMMapReduce EXTAGENTS (ours) HotpotQA En.QA Zh.QA Input F1 Input F1 Input DeepSeek-R1-Distill-Llama-8B 32k .097 .159 32k .143 32k gpt-4o-mini-2024-07-18 128k .204 128k .482 128k . .182 128k .204 128k .534 1024k .374 .382 128k 1024k .436 .482 128k 1024k Llama-3.1-8B-Instruct 128k .254 .237 32k .349 32k .368 128k .315 128k .412 1024k .254 .291 256k 256k .345 .347 128k 256k EXTAGENTS (N = 1) EXTAGENTS gpt-4o-2024-08-06 128k 1024k .553 .597 Figure 4: Ablation studies on the global knowledge synchronization (GKS) and knowledge-accumulating reasoning (KAR) on Hotpot QA. incrementally integrates the most pertinent messages via an iterative selection process, rather than directly inputting all synchronized information. Formally, at each reasoning iteration (1 S), Reasoning Agent selects the top-2s messages. The accumulated reasoning context is defined as: M(s) = Top2s (cid:0){mi,T }N i=1 (cid:1) . (8) The reasoning process is performed under Equation (5), but Reasoning Agent first checks the answerability of the query based on given information M(s) and will only output the answer if the query is answerable, which then halts the whole process. This iterative reasoning ensures that the Reasoning Agent progressively benefits from increased context without being overwhelmed. For QA tasks, the reasoning process is terminated when the answer is produced or the maximum number of iterations is reached, which then starts new round of knowledge synchronization. For long survey generation, we adopts the drafting method [31], where the outline is generated first, and then the reasoning process is performed to fill in each section. In this case, after filling up section, the newly started process will take the previous section into the task query for continuous generation. The structured separation of Seeking and Reasoning Agents in EXTAGENTS, combined with global synchronization and incremental knowledge accumulation, inherently enables significant parallel processing. When using separate tool for rating messages, each round of the knowledge-accumulating reasoning process could independently select information from globally synchronized messages, which can also exploit parallel computation. For instance, the top (2s1 + 1) 2s messages can be synchronized in parallel to the reasoning process on the top 2s1 messages, forming an interleaved asynchronous pipeline. Consequently, EXTAGENTS maintains high parallelism and scalability."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Settings Benchmarks We evaluate on the following benchmarks: (i) Bench+, our enhanced multi-hop QA benchmark with bi-lingual long document attached to each query (Section 3.3), featuring both Zh.QA and En.QA bi-lingual subsets, (ii) HotpotQA [30], containing multi-hop queries requiring factual knowledge from Wikipedia, and (iii) AutoSurvey [31], generating long surveys with preretrieved papers. The evaluation metrics are F1 for multi-hop QA and LLM-as-a-Judge for long survey generation. We also provide the result on original Bench in Appendix E.2. We found existing principles [31, 39] for LLM-based judges have very low discriminative power on generated surveys, and thus we use the prompting template from Liu et al. [60] to judge the result against baselines. We also include the Helmet correctness score [61] and additional quantitative measurement (e.g., #citations, citation density, duplication rate, etc.) as supplementary metrics for QA and long 7 Figure 5: Experiment of scaling external knowledge input on multi-hop QA tasks with gpt-4o-mini. (a) The top row shows the performance of EXTAGENTS and retrieval methods on HotpotQA. (b) The middle and the bottom row show the performance of EXTAGENTS and LLMMapReduce on En.QA and Zh.QA, respectively. The rightmost subfigure of each part is the baseline result. generation tasks, respectively. We control the whole input length for each methods in the range of {8k, 16k, 32k, 64k, 128k, 256k, 512k, 1024k} tokens with the maximum context window 128k, and also control the chunk sizes of each agent for LLM-based multi-agent methods. Specifically, for minority of samples with contexts longer than 1024k, we input all contexts for the 1024k setting. Methods On multi-hop QA tasks, we compare EXTAGENTS with (i) Direct Input, the baseline method that directly inputs the truncated context into LLMs according to the benchmark settings, (ii) LLMMapReduce [1], the state-of-the-art multi-agent method for long-context tasks, (iii) DRAG and (iv) IterDRAG [11], inference-time scalable retrieval methods for multi-hop QA based on external knowledge bases. On the long survey generation task, we compare EXTAGENTS with AutoSurvey [31], only substituting the generation process in the pipeline for fair comparison. concurrent work [39] proposes long survey generation method with task-specific techniques in multiple aspects, including skeleton evolving, content trees, etc., which we decide is not directly comparable to our method. We use the same LLM as the backbone model for all methods, including gpt-4o-mini-2024-07-18 [59] and Llama-3.1-8B-Instruct [62]. We test these methods with the optimal configuration except for the input length and multi-agent chunk size. For stable reproduction, we report the median results of three runs. For HotpotQA, we use BM25 retriever [63], and for AutoSurvey, we use the original retrieval method. Other details are elaborated in Appendix E. 8 Table 4: Experimental results on long survey generation tasks. Benchmark LLM-as-a-Judge (1 10) #Citations Citation Density Duplication Rate AutoSurvey EXTAGENTS (ours) 6.75 7.63 113 191 1.00 1.09 2.41 1.80 5.2 Results and Analysis Performance on Multi-Hop QA We plot the experimental results of scaling external knowledge input on multi-hop QA tasks in Figure 5. Detailed illustration is in Appendix E. The increasing trend of performance w.r.t. the input length indicates the scalability of each context window extension method. Empirically, EXTAGENTS consistently outperforms the baselines across all input lengths, achieving the significantly better performance on both HotpotQA and Bench+ benchmarks. Moreover, the performance consistently improves with the increase of external knowledge input, demonstrating the scalability of EXTAGENTS. We also summarize the performance and input length of each method within its optimal setting in Table 3. EXTAGENTS achieves the best performance on all three multihop QA benchmarks by effectively utilizing more external knowledge compared to other methods. As another paradigm of inference-time scaling, long reasoning chains do not benefit the use of large quantity of external knowledge (see DeepSeek-R1-Distill-Llama-8B in Table 3). Performance on Long Survey Generation We test EXTAGENTS on long survey generation tasks compared to AutoSurvey with gpt-4o-mini. EXTAGENTS could incorporate more papers during generation, and achieves better performance with more citations and lower duplication rate, as shown in Table 4. By aggregating eight pairwise scores from LLM-as-a-Judge, we find that EXTAGENTS achieves an overall higher score than AutoSurvey with significant margin. However, the evaluation of long surveys is challenging even for human experts. Thus, we also include part of generated texts in Appendix for qualitative comparisons. The results indicate that EXTAGENTS is capable of generating long surveys with high quality and low redundancy. Latency and Cost Analysis In HotpotQA benchmark, we measure the latency of EXTAGENTS and direct input under the same amount of 128k external knowledge input in single process with Llama-3.1-8B-Instruct on 4 A100 GPUs, and estimate the average latency with higher number of parallel processes, in Table 5. Under reasonable amount of parallel processes, ideally, EXTAGENTS could achieve significant speedup, demonstrating the efficiency and high parallelism of our framework. Table 5: Latency analysis of EXTAGENTS under different numbers of processes. Method #Processes Latency (s) Direct Input EXTAGENTS (estimated) 1 1 4 16 17.80 19.08 4.94 1.41 Ablation Studies We conduct ablation studies on the global knowledge synchronization (GKS) and knowledge-accumulating reasoning (KAR) in EXTAGENTS with gpt-4o-mini, as shown in Figure 4. The results show that removing either KAR leads to significant drop in performance, especially as the amount of external knowledge increases. This demonstrates that the gradual knowledge accumulation components are critical for achieving strong scalability and effective knowledge integration, breaking the bottleneck of information overload. Removing GKS leads to slight drop in performance, indicating that the total rounds of global knowledge synchronization is not the main bottleneck of EXTAGENTS. Compared to other retrieval methods, EXTAGENTS still achieves better performance with multi-round global knowledge synchronization, demonstrating that the bottleneck of this stage is not the number of rounds, but the amount of information exchanged (i.e. bandwith). Compatibility across LLM Families We further test EXTAGENTS with gpt-4o-2024-08-06 on HotpotQA benchmark, and find that the performance is significantly improved with the stronger LLM comFigure 6: Results of EXTAGENTS with gpt-4o-202408-06 on HotpotQA benchmark with strong scalability. pared to weaker models, e.g., gpt-4o-mini in Figure 5, as shown in Figure 6. More detailed analysis is in Appendix E. The reason might be that the stronger LLMs are more capable of collaboration and comprehension. Overall, stronger LLMs are more likely to benefit more from the scalability of EXTAGENTS, implying promising future work on the framework with even stronger LLMs."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "We introduced EXTAGENTS, multi-agent framework that enables scaling external knowledge input beyond context windows of LLMs without additional training. By clearly separating global knowledge synchronization and knowledge-accumulating reasoning, EXTAGENTS resolves the core bottlenecks of existing distributed approaches, consistently enhancing performance on multi-hop QA and long-form generation benchmarks, while preserving high parallel efficiency. Future work includes adaptive orchestration of agents based on task complexity, extending the framework to cross-modal and tool-augmented reasoning, and theoretical analyses of information robustness and safety mechanisms, collectively moving towards more scalable knowledge-centric AI."
        },
        {
            "title": "References",
            "content": "[1] Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Rongqiao An, Qi Shi, Zhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, and Maosong Sun. Llmmapreduce: Simplified longsequence processing using large language models. Computing Research Repository, arXiv:2410.09342, 2024. URL https://arxiv.org/abs/2410.09342. [2] DeepSeek-AI. Deepseek-v3 technical report. Computing Research Repository, arXiv:2412.19437, 2025. URL https://arxiv.org/abs/2412.19437. [3] OpenAI. Openai gpt-4.5 system card. OpenAI Technical Report, 2025. URL https://cdn.openai. com/gpt-4-5-system-card-2272025.pdf. [4] Anthropic. Claude 3.7 sonnet system card. Anthropic Technical Report, 2025. URL https://assets. anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf. [5] OpenAI. Openai o3 and o4-mini system card. OpenAI Technical Report, 2025. URL https://cdn. openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card. pdf. [6] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Computing Research Repository, arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. [7] OpenAI. Deep research system card. OpenAI Technical Report, 2025. URL http://cdn.openai.com/ deep-research-system-card.pdf. [8] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. Computing Research Repository, arXiv:2501.05366, 2025. URL https://arxiv.org/abs/2501.05366. [9] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. Computing Research Repository, arXiv:2503.05592, 2025. URL https://arxiv.org/abs/2503.05592. [10] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Computing Research Repository, arXiv:2503.09516, 2025. URL https://arxiv.org/abs/2503.09516. [11] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented In The Thirteenth International Conference on Learning Representations, 2025. URL generation. https://openreview.net/forum?id=FSjIrOm1vz. [12] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. Computing Research Repository, arXiv:2306.15595, 2023. URL https://arxiv.org/abs/2306.15595. 10 [13] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=wHBfxhZu1u. [14] Chejian Xu, Wei Ping, Peng Xu, Zihan Liu, Boxin Wang, Mohammad Shoeybi, Bo Li, and Bryan Catanzaro. From 128k to 4m: Efficient training of ultra-long context large language models. Computing Research Repository, arXiv:2504.06214, 2025. URL https://arxiv.org/abs/2504.06214. [15] Ning Shang, Li Lyna Zhang, Siyuan Wang, Gaokai Zhang, Gilsinia Lopez, Fan Yang, Weizhu Chen, and Mao Yang. Longrope2: Near-lossless llm context window scaling. Computing Research Repository, arXiv:2502.20082, 2025. URL https://arxiv.org/abs/2502.20082. [16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 60006010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. [17] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. RetrievalIn Proceedings of the 34th International augmented generation for knowledge-intensive nlp tasks. Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. [18] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. Computing Research Repository, arXiv:2312.10997, 2024. URL https://arxiv.org/abs/2312.10997. [19] Yuxuan Chen, Dewen Guo, Sen Mei, Xinze Li, Hao Chen, Yishan Li, Yixuan Wang, Chaoyue Tang, Ruobing Wang, Dingjun Wu, Yukun Yan, Zhenghao Liu, Shi Yu, Zhiyuan Liu, and Maosong Sun. Ultrarag: modular and automated toolkit for adaptive retrieval-augmented generation. Computing Research Repository, arXiv:2504.08761, 2025. URL https://arxiv.org/abs/2504.08761. [20] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems. Computing Research Repository, arXiv:2310.08560, 2024. URL https://arxiv.org/abs/2310.08560. [21] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16581677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.91. URL https://aclanthology.org/2024.acl-long.91/. [22] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, and Zhicheng Dou. Are long-llms necessity for long-context tasks? Computing Research Repository, arXiv:2405.15318, 2024. URL https://arxiv.org/abs/2405.15318. [23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. [24] Yan Wang, Dongyang Ma, and Deng Cai. With greater text comes greater necessity: Inference-time training helps long text generation. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=dj9x6JuiD5. [25] Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, and Sheng Guo. OmniKV: Dynamic context selection for efficient long-context LLMs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=ulCAPXYXfa. [26] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.557. URL https: //aclanthology.org/2023.acl-long.557/. 11 [27] Jun Zhao, Can Zu, Xu Hao, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, and Xuanjing Huang. LONGAGENT: Achieving question answering for 128k-token-long documents through multi-agent collaboration. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1631016324, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.912. URL https://aclanthology.org/2024.emnlp-main.912/. [28] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. Chain of agents: Large language models collaborating on long-context tasks. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=LuCLf4BJsr. [29] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. Bench: Extending long context evaluation beyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1526215277, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.814. URL https://aclanthology.org/2024.acl-long.814/. [30] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. [31] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. Autosurvey: Large language models can automatically write surveys. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=FExX8pMrdT. [32] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained ISSN 1046-8188. doi: language models: survey. ACM Trans. Inf. Syst., 42(4), February 2024. 10.1145/3637870. URL https://doi.org/10.1145/3637870. [33] Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang, Renhai Chen, Hua Xu, and Hongwei Sun. Xl3m: training-free framework for llm length extension based on segment-wise inference. Computing Research Repository, arXiv:2405.17755, 2024. URL https: //arxiv.org/abs/2405.17755. [34] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. InfLLM: Training-free long-context extrapolation for LLMs with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=bTHFrqhASY. [35] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-infinite: Zero-shot extreme length generalization for large language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39914008, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/2024.naacl-long.222/. [36] Zeyuan Yang, Fangzhou Xiong, Peng Li, and Yang Liu. Rethinking long context generation from the continual learning perspective. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend AlKhalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International Conference on Computational Linguistics, pages 19221933, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025.coling-main.131/. [37] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. Computing Research Repository, arXiv:2310.05029, 2023. URL https://arxiv.org/abs/2310.05029. [38] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: graph rag approach to query-focused summarization. Computing Research Repository, arXiv:2404.16130, 2025. URL https://arxiv.org/abs/2404.16130. 12 [39] Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, and Maosong Sun. Llmmapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources. Computing Research Repository, arXiv:2504.05732, 2025. URL https://arxiv.org/abs/2504.05732. [40] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative agents for mind exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=3IyL2XWDkG. [41] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1173311763. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/du24e.html. [42] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversations. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=BAakY1hNKS. [43] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. dynamic LLM-powered agent network for task-oriented agent collaboration. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=XII0Wp1XA9. [44] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. GPTSwarm: Language agents as optimizable graphs. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6274362767. PMLR, 2127 Jul 2024. URL https: //proceedings.mlr.press/v235/zhuge24a.html. [45] Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. Computing Research Repository, arXiv:2410.11782, 2025. URL https://arxiv.org/abs/ 2410.11782. [46] Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. Masrouter: Learning to route llms for multi-agent systems. Computing Research Repository, arXiv:2502.11133, 2025. URL https://arxiv.org/abs/2502.11133. [47] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VtmBAGCN7o. [48] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Computing Research Repository, arXiv:2504.08066, 2025. URL https://arxiv.org/abs/ 2504.08066. [49] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=O0nBMRlkc8. [50] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=EHg5GDnyq1. [51] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Jiang, Chengfei Lv, and Huajun Chen. AutoAct: Automatic agent learning from scratch for QA via self-planning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of 13 the Association for Computational Linguistics (Volume 1: Long Papers), pages 30033021, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.165. URL https://aclanthology.org/2024.acl-long.165/. [52] Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, and Igor MorIn The Thirteenth datch. Multiagent finetuning: Self improvement with diverse reasoning chains. International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=JtGPIZpOrz. [53] Zhitao He, Zijun Liu, Peng Li, May Fung, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Enhancing language multi-agent learning with multi-agent credit re-assignment for interactive environment generalization. Computing Research Repository, arXiv:2502.14496, 2025. URL https://arxiv.org/abs/ 2502.14496. [54] Junwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. Marft: Multi-agent reinforcement finetuning. Computing Research Repository, arXiv:2504.16129, 2025. URL https://arxiv.org/abs/ 2504.16129. [55] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. Computing Research Repository, arXiv:2504.12516, 2025. URL https: //arxiv.org/abs/2504.12516. [56] ByteDance Inc. Deerflow: high-performance flow-based diffusion model. https://github.com/ bytedance/deer-flow, 2025. Accessed: 2025-05-15. [57] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLMIn Thirty-seventh Conference on Neural Information as-a-judge with MT-bench and chatbot arena. Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum? id=uccHPGDlao. [58] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=iAkhPz7Qt3. [59] OpenAI. Gpt-4o system card. Computing Research Repository, arXiv:2410.21276, 2024. URL https: //arxiv.org/abs/2410.21276. [60] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inferencetime scaling for generalist reward modeling. Computing Research Repository, arXiv:2504.02495, 2025. URL https://arxiv.org/abs/2504.02495. [61] Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. HELMET: How to evaluate long-context models effectively and thoroughly. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=293V3bJbmE. [62] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz 14 Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin 15 Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. Computing Research Repository, arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. [63] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019. [64] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. Dense retrieval: What retrieval granularity should we use? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1515915177, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.845. URL https://aclanthology. org/2024.emnlp-main.845/. [65] André V. Duarte, João DS Marques, Miguel Graça, Miguel Freire, Lei Li, and Arlindo L. Oliveira. LumberChunker: Long-form narrative document segmentation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 64736486, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.377. URL https://aclanthology.org/2024.findings-emnlp. 377/. [66] Jihao Zhao, Zhiyuan Ji, Yuchen Feng, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, and Zhiyu Li. Metachunking: Learning efficient text segmentation via logical perception. Computing Research Repository, arXiv:2410.12788, 2024. URL https://arxiv.org/abs/2410.12788."
        },
        {
            "title": "A Limitations",
            "content": "Model Alignment EXTAGENTS inherits both the strengths and weaknesses of its underlying LLMs: while substantial scalability on external knowledge input has shown in EXTAGENTS, the framework offers no principled defense on adversarial models, e.g., the aggregated evidence might be factually incorrect, biased, or policyincompliant. Misaligned or adversarial Seeking Agents can propagate errors to every Reasoning Agent, amplifying harmful content or systemic biases. Incorporating alignmentaware scoring, preference-based post-training, or toolbased content filters could alleviate the problem but introduce training costs, which is left to future work. Broader Tasks Our experiments concentrate on knowledgeintensive multihop QA over long documents and large-scale knowledge bases, and long survey generation; tasks that demand interactive decisionmaking, long-horizon planning, or multi-modal derivations (e.g., embodied manipulation, automated research, or multimodal reasoning) remain unexplored. Such scenarios may require substantially different model architecture, synchronization schedules, memory management, and evaluation metrics, and the current design may not transfer without additional orchestration logic or domainspecific agents. Integration with Chunking Techniques EXTAGENTS partitions long inputs into fixedsize slices to simplify agent contexts, but makes no attempt to optimize those boundaries. Advanced chunking strategiessemantic segmentation, overlap windows, or hierarchical compressioncould further reduce information loss [6466], yet also introduce new coordination overhead and hyperparameter choices. systematic study of how adaptive chunking interacts with agent synchronization and reasoning quality is beyond the scope of this work."
        },
        {
            "title": "B Broader Impact",
            "content": "EXTAGENTS should be viewed as step towards more scalable and efficient knowledge-centric LLM workflow. Thus, different domains may be impacted either positively or negatively depending on the specific use case. For instance, in the educational domain, our work could enhance personalized learning and research productivity by allowing teachers and students to access and reason over extensive knowledge beyond textbooks, potentially democratizing expert-level insights and reducing barriers to advanced inquiry. However, there might also be negative implications: the improved scalability in integrating large-scale external knowledge may unintentionally amplify misinformation or biased viewpoints, as automated retrieval and reasoning processes could propagate inaccuracies present in the underlying data sources, especially for medical or economical industries. To mitigate these risks, further development of verification mechanisms and post-training techniques to align agent-produced knowledge is recommended."
        },
        {
            "title": "C Qualitative Analysis",
            "content": "C.1 Case: Multi-Hop Question Answering The case below demonstrates EXTAGENTS superior ability to connect disparate pieces of information across multiple documents to answer complex, multi-step question accurately, where other methods fail to synthesize the necessary facts or get sidetracked by irrelevant details. Example from HotpotQA: Question Answering on William Hodges Voyage Question: William Hodges visited polar region as part of voyage, the region is how big? Answer: 14 million km Excerpts from retrieved documents: William Hodges RA (28 October 1744 6 March 1797) was an English painter. He was member of James Cooks second voyage to the Pacific Ocean, and is best known for the sketches and paintings of locations he visited on that voyage, including Table Bay, Tahiti, Easter Island, New Zealand, Dusky Sound and the Antarctic. 17 The Antarctic (US English, UK English or and or) is polar region around the Earths South Pole, opposite the Arctic region around the North Pole. The Antarctic comprises the continent of Antarctica, the Kerguelen Plateau and other island territories located on the Antarctic Plate or south of the Antarctic Convergence. The Antarctic region includes the ice shelves, waters, and all the island territories in the Southern Ocean situated south of the Antarctic Convergence, zone approximately wide varying in latitude seasonally. The region covers some 20 percent of the Southern Hemisphere, of which 5.5 percent (14 million km) is the surface area of the Antarctic continent itself. EXTAGENTS Extracted information (partial): William Hodges, an English painter, accompanied James Cook on his second voyage to the Pacific Ocean between 1772 and 1775. During this voyage, he traveled to various locations, including Antarctic regions. However, the chunk does not provide the exact size of the polar region Hodges visited. It mentions that Hodges is known for his sketches and paintings of locations he visited on that voyage, including those in the Antarctic, but does not specify the size of the region. William Hodges was likely referring to the polar regions explored during his voyages. Specifically, he could relate to the Antarctic region, which is noted as large landmass and describes being covered by about 98% ice, averaging in thickness. The Antarctic continent itself encompasses an area of approximately 14 million square kilometers (or around 5.5% of the Southern Hemisphere). Prediction: Approximately 14 million square kilometers. DRAG prediction: The region is thought to have covered about 1.5 million square kilometers. IterDRAG prediction: The specific size of the broader region William Hodges visited is not mentioned in the context; however, one specific area he explored, Hodges Knoll, is 2.2 km in northeast-southwest direction and 1 km wide. The case below highlights EXTAGENTS effectiveness in pinpointing the correct answer by intelligently scoring and prioritizing the most relevant textual evidence, especially when multiple, potentially conflicting pieces of information are present within the source material. Example from En.QA in Bench+: Question Answering on Davions Fortune Question: Where is the Davions fortune hidden? Answer: On the island of Brayan Annabel Excerpts from original text: Davion followed him with an excited look. \"and now,\" he said, when he saw that Pascal had read the last line, \"put the two fragments together, and judge for yourself.\" Pascal obeyed, and the conjointed pieces gave the following: \"This 25th day of April, 1498, be...ing invited to dine by his Holiness Alexander VI., and fearing that not...content with making me pay for my hat, he may desire to become my heir, and re...serves for me the fate of Cardinals Caprara and Bentivoglio, who were poisoned...I declare to my nephew, Guido Spada, my sole heir, that have bu...ried in place he knows and has visited with me, that is, in...the caves of the small Island of Brayan Annabel all poss...ssed of ingots, gold, money, jewels, diamonds, gems; that alone...know of the existence of this treasure, which may amount to nearly two mil...lions of Roman crowns, and which he will find on raising the twentieth ro...ck from the small creek to the east in right line. Two open...ings have been made in these caves; the 18 treasure is in the furthest a...ngle in the second; which treasure bequeath and leave en...tire to him as my sole heir.\" EXTAGENTS Extracted information and corresponding scores (partial): Davions fortune is hidden in coffer located in cave on the Island of Brayan Annabel. The coffer is described as being bound with iron and engraved with the arms of the Spada family. It contains piles of golden coins, bars of unpolished gold, and significant amount of diamonds, pearls, and other gems. The treasure is concealed beneath large circular rock that Pascal discovers and manages to move, revealing the coffer underneath. Score: 100.0 Prediction: Island of Brayan Annabel LLMMapReduce Extracted information (partial): The Davions fortune is hidden in treasure chest buried in the caves on the Island of Brayan Annabel. The Davions fortune is hidden in diamond that belonged to character named Armando Pascal, who bequeathed it to Corbin, the innkeeper, before his death. Davions fortune is hidden in grotto at Brayan Annabel, of which Brisa knows the secret. The Davions fortune is hidden in chest concealed under the closet in the little house in the Allees de Meillan, at Marseilles. Prediction: The Davions fortune is hidden in multiple locations: treasure chest in the caves on the Island of Brayan Annabel, diamond bequeathed to Corbin, grotto at Brayan Annabel, and chest under closet in house in Allees de Meillan, Marseilles. The case below shows EXTAGENTS strength in accurately identifying the main subject by discerning the most pertinent information through its scoring mechanism, successfully avoiding misdirection from other related but less central entities mentioned in the text in different languages. Example from Zh.QA in Bench+: Question Answering on Who Infiltrated the Scarlet Research Institute Question: 谁混入猩红研究院开启他的卧底计划 Answer: 贾易 Excerpts from original text: 我决定让我的学生贾易开始接触生物科学领域希望他不会让我失望以他的学 习能力如果努力的话不需要太久就能精通这方面的内容并混入猩红研究院 贾易已经通过了猩红研究院的两轮考核即将混入猩红研究院开启他的卧底计 划希望这次能从中得到有用的信息 贾易已经混入猩红研究院了我等待着他传来的好消息 EXTAGENTS Extracted information and corresponding scores (partial): 在故事中混入猩红研究院开启卧底计划的角色是贾易他是封棋的学生负责 保护可能被暗杀的人类天才并试图从猩红研究院获取有用的信息贾易的任务是 潜入猩红研究院调查其内部情况尤其是与符文改造人计划相关的秘密Score: 100.0 19 在文章中混入猩红研究院并开启卧底计划的是黑影黑影是灵能族的叛徒 曾经在灵能族中拥有强大的实力文章提到黑影在科技研究院布局未来并且在战 争线中与封棋等人产生了交集封棋在与黑影的对抗中逐渐意识到黑影的真实身 份和目的Score: 90.0 Prediction: 贾易 LLMMapReduce Extracted information (partial): 在小说中混入猩红研究院开启卧底计划的是贾易他是未来的封棋的学生之 一在封棋未进入未来梦境之前已经通过了猩红研究院的考核成为了组织的成 员负责保护被暗杀的人类天才 根据文章内容混入猩红研究院开启卧底计划的是黑影在封棋与黑影的对话 中黑影透露自己来自一个由多个弱小族群组成的联盟负责刺杀人类新生代天才 等行动同时黑影也提到自己和他的族群并不像科技研究院或虎魄研究院那般强 大而是被迫在外打拼只能通过暗杀等方式潜入人类社会表明其立场和背景 根据您提供的文章混入猩红研究院并开启卧底计划的角色是黑影黑影是掌 控科技研究院的潜伏者与黑玄派系的灵能族成员有关暗中培养力量目的是为 了对抗族地灵能族有关黑影的信息在文章的多处被提及其作用和动机与整个情 节的发展密切相关 Prediction: 黑影 C.2 Case: Long Survey Generation In the following, we present comparison of one subsection generated by our method and AutoSurvey for the same topic. The survey generated by EXTAGENTS presents more related and more diverse range of innovative LLM applications in education, making the overall argument more persuasive and resulting in overall 68 pages compared to 58 pages from AutoSurvey. Subsection in Long Survey Generated by EXTAGENTS 1.6 Case Studies Demonstrating LLM Impact on Education The integration of Large Language Models (LLMs) into educational contexts has resulted in transformative changes, showcasing their potential to enhance teaching, learning, and administrative processes. Numerous case studies illustrate the successful implementation of LLMs in various educational settings, providing insights into their effectiveness and the tangible benefits they deliver. One notable case study focused on using LLMs to simulate student learning behaviors, where researchers leveraged LLMs to create virtual student models that replicate real learner patterns based on demographic data. This experiment involved 145 participants and revealed that the simulated results aligned closely with the actual studentslearning behaviors across diverse demographics. This application demonstrates how LLMs can enhance inclusivity in curriculum design by providing insights into how different student characteristics influence learning outcomes [40]. In the realm of personalized learning, case studies have highlighted the development of LLM-driven intelligent tutoring systems. These systems adapt to individual student needs, providing real-time support tailored to specific learning styles and paces. In one study, LLMs were implemented as personalized tutors in mathematics, demonstrating that such systems could significantly improve comprehension and engagement among learners. The ability to receive tailored guidance directly addresses students challenges, ultimately enhancing educational efficacy [5]. 20 Automated grading systems powered by LLMs have also gained traction in educational institutions. These systems evaluate students assignments consistently and objectively, thereby saving educators time and minimizing biases often associated with manual grading processes. In practical studies, LLMs have shown reliable scoring that correlates well with human assessment, allowing educators to focus more on quality instruction rather than administrative tasks [41]. Interactive tools facilitated by LLMs, such as chatbots, have revolutionized student engagement models. significant case involved an LLM-based tool providing on-demand programming assistance to students in an introductory computer science course. Over period of 12 weeks, the tool managed more than 2,500 queries, primarily related to immediate help with assignments. The findings suggested that students who engaged frequently with the tool had higher success rates in the course, indicating that LLM-powered assistance can significantly enhance the learning experience in large classroom settings [42]. Furthermore, LLMs have been effectively employed to enhance language acquisition among learners. One study examined how situational dialogue models fine-tuned on LLMs facilitate conversational practice for language students. The models allowed for rich, simulated dialogues that mirrored authentic conversations, leading to significant improvements in fluency and confidence. Participants who engaged with these LLM-driven dialogues exhibited enhanced communication skills compared to those who did not, showcasing the crucial role of LLMs in supporting language learning [43]. In the context of automated content generation, LLMs have proven capable of producing educational resources quickly and effectively. noteworthy implementation is the development of AutoPlan, which leverages LLMs to create interactive decision-making tasks. This intelligent system significantly increases the efficiency of educational environments centered on decision-making and planning, illustrating how LLMs can enhance problem-solving instruction by breaking down complex tasks into manageable components [44]. Moreover, LLMs have demonstrated their capability to contribute to informal learning situations through chat-based environments. Researchers have explored how learners utilize LLMs to seek answers outside traditional classroom boundaries, supporting knowledge acquisition and empowering students to take charge of their own learning. The results indicate that LLM interactions not only facilitate knowledge gain but also promote lifelong learning habits that are essential in rapidly changing world [45]. Another compelling case involved utilizing LLMs to support health literacy in educational settings. comprehensive assessment using various prompts with several LLMs, including ChatGPT and Google Bard, demonstrated their effectiveness in tailoring health-related content for different reading levels. The findings revealed that LLMs could produce responses at appropriate reading levels for younger audiences, thus enhancing health communication capabilities and aligning with the goal of promoting understanding among diverse learner populations [46]. unique implementation highlights how LLMs can serve as reading companions for students. Through study involving user-centric assistant named Claude, researchers found tangible improvements in reading comprehension and engagement among students utilizing the AI tool compared to traditional study methods. This innovative use of LLMs showcases their potential as supplemental educational resources, helping learners tackle complex literature and academic texts effectively while fostering enhanced interaction with reading materials [47]. Moreover, the case of employing LLMs in hybrid workplace decision support exemplifies their adaptability and decision-making capabilities. In educational contexts, these systems guide students in creating effective study plans and managing their time better. By analyzing users decision-making processes, LLMs assist learners in balancing various factors, thus enhancing their academic experience [48]. Finally, research has begun to explore the ethical implications and potential biases associated with LLM implementations in education. One study examined how LLMs were evaluated for ethical considerations in real-world educational practices, underscoring the importance 21 of transparent LLM development processes. The findings emphasized that ethical reflection and intervention could significantly enhance educational outcomes while addressing biases, highlighting the dual need for technological innovation and ethical responsibility when deploying LLMs in educational settings [49]. These case studies collectively illustrate the transformative impact of LLMs in various educational contexts. They highlight the models dual role as supportive tools for educators and personalized learning facilitators for students. From dialogic interactions and intelligent tutoring systems to automated content generation and tailored health communication, the versatility of LLM applications is evident. However, they also reveal the complexities and challenges of integrating AI into educational systems, necessitating balanced approach that prioritizes ethical considerations, pedagogical integrity, and student engagement. As the landscape of education continues to evolve, the integration of LLMs holds immense potential for enriching the learning experience. Continuous research and iterative refinements will be essential to harnessing the full capabilities of LLMs, ensuring they complement traditional educational practices while addressing the diverse needs of learners. The future of education may present unprecedented opportunities through the effective deployment of these advanced AI systems, paving the way for personalized, engaging, and impactful learning experiences that prepare students for the challenges of the 21st century. Subsection in the Long Survey Generated by AutoSurvey 1.6 Case Studies Demonstrating LLM Impact on Education Large Language Models (LLMs) have taken center stage in educational innovation, demonstrating significant potential to enhance teaching and learning experiences across various contexts. This subsection presents variety of case studies that illustrate successful deployments of LLM technologies in educational settings, showcasing their effectiveness and the tangible benefits achieved in real-world scenarios. One notable application of LLMs is highlighted in the study titled \"Future-proofing Education: Prototype for Simulating Oral Examinations Using Large Language Models.\" This research explored prototype system designed to simulate oral examinations in higher education. Educators and students evaluated this system, which aimed to automate and enhance the examination process. The outcomes showed that the prototype provided personalized feedback to students while significantly reducing the workload on educators, demonstrating the LLMs capability to streamline assessment processes and improve educational efficiency [27]. Another innovative implementation involved using LLMs to simulate student learning behavior. In the paper \"Leveraging generative artificial intelligence to simulate student learning behavior,\" researchers revealed how they utilized LLMs to craft virtual students with distinct demographics. The findings from three experiments showcased the ability of LLMs to replicate intricate learning behaviors and experiences, unveiling correlations between course materials, engagement levels, and understanding. This simulation approach empowers educators to design curricula that adapt dynamically to diverse student needs, thus enhancing inclusivity and educational effectiveness [29]. Additionally, the integration of LLMs within role-playing scenarios has proven effective in fostering engagement and active learning among students. The study titled \"Role-Playing Simulation Games using ChatGPT\" illustrated how incorporating ChatGPT into role-playing simulations could enhance the quality of teaching by providing realistic environments for learners to practice skills. This approach led to increased student interest and participation, exemplifying the potential of LLMs to create immersive and interactive learning experiences [30]. LLMs have also shown promise in automating the grading process, crucial area of concern in educational assessments. In \"Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning,\" the study discusses how LLMs can provide adaptive feedback on mathematical problem-solving tasks. By assessing students answers, LLMs can identify misconceptions and offer tailored guidance, potentially leading to better learning outcomes. Such systems emphasize the transition toward more personalized assessment methods in education [31]. Moreover, the implementation of LLMs in content generation showcases their value in educational resource development. The research titled \"Prototyping the use of Large Language Models (LLMs) for adult learning content creation at scale\" examined how LLMs could assist in creating quality learning materials for adult education. By leveraging human-in-the-loop approach, the study found that LLMs could produce high-quality content quickly, marking significant advancements in Generative AIs application for education. This capacity to automate content creation not only reduces the burden on educators but also ensures resource availability and accessibility [32]. In collaborative learning context, LLMs have been utilized to foster peer interaction and engagement. The paper titled \"The Use of Multiple Conversational Agent Interlocutors in Learning\" explored how LLMs could simulate conversations among various personas in an educational environment. This method assists learners in problem-solving by exposing them to different viewpoints and areas of expertise, enriching the collaborative learning experience. Such interactive dynamics can enhance comprehension and engagement among students, urging creative exploration of subject matter [33]. Implementing LLMs within existing educational frameworks also presents advantages for developing personalized learning experiences. The study \"Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges\" provides insights into how LLMs can adapt to individual student requests, addressing diverse learning obstacles. By utilizing LLMs capable of interpreting and responding to students needs, educational institutions can design more effective and customized learning modules, reinforcing the models role as facilitator in education [34]. Furthermore, the integration of LLMs into LMS (Learning Management Systems) has revealed additional benefits. The research \"Identifying Critical LMS Features for Predicting At-risk Students\" discussed how integrating LLMs into LMSs could analyze student interaction data to identify those at risk of underperforming. By using predictive analytics, educators can intervene proactively, thus promoting student success through personalized academic support [35]. Finally, ethical considerations, crucial when implementing LLMs in educational environments, have been addressed in the study \"The teachers are confused as well: Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education.\" The authors conducted stakeholder interviews to explore the ethical challenges faced by both educators and students in using LLMs. Their findings underscore the importance of establishing guidelines and fostering digital literacy among students to ensure balanced approach to utilizing LLMs responsibly in education [25]. Overall, these case studies collectively illustrate the transformative impact of LLMs in education. By automating assessments, enhancing content generation, enriching collaborative learning experiences, and facilitating personalized education, LLMs hold the potential to revolutionize pedagogical practices. As educational institutions continue to explore and refine LLM applications, efforts to address ethical concerns and optimizing their deployment will be critical to fully realizing their capabilities in fostering an effective and inclusive learning environment. The integration of LLMs can pave the way for innovative educational solutions and provide solid foundation for future research and development in the field. Details of Review on Existing Multi-Agent Methods In this section, we provide the implementation of existing LLM-based multi-agent systems for context window extension, including Chain of Agents [28], LongAgent [27], and LLMMapReduce [1] in our framework in Section 3.2. We also explain the comparison results in Table 1. 23 Table 6: Performance on Multi-Hop QA tasks in Helmet correctness scores with the optimal setting and the corresponding input length (#tokens). The settings are the same as Table 3. Method HotpotQA En.QA Zh.QA Helmet Input Helmet Input Helmet Input Direct Input 1.56 32k 0.69 32k 0.66 32k DeepSeek-R1-Distill-Llama-8B Direct Input DRAG IterDRAG LLMMapReduce ExtAgents (ours) Direct Input DRAG IterDRAG LLMMapReduce ExtAgents (ours) gpt-4o-mini-2024-07-18 1.83 1.53 1. 128k 128k 128k 1.41 128k 1.04 128k 1. 1024k 1.12 1.20 256k 1024k 1.04 1.10 128k 256k Llama-3.1-8B-Instruct 0. 0.96 1.20 1.14 128k 32k 32k 128k 0.89 128k 1. 1024k 0.78 1.09 256k 1024k 0.79 0.85 256k 256k EXTAGENTS (N = 1) EXTAGENTS 1.73 1.86 128k 1024k gpt-4o-2024-08-06 Gi,t1,t = {di} and M(CoA) i,t1 = {ai1,t1, ai,t1}, the bandwidth is 2. Knowledge Synchronization Here, we explain how existing methods synchronize knowledge across agents according to Equation (4). For Chain of Agents, each agent ai, incorporate the message from previous agent ai1,t1 in linear topology, and the message is passed to the next agent ai+1,t1 in the next timestep. So in Equation (4), D(CoA) Gi,t1,t1 = {mi1,t1}. Since G(CoA) It is also clear that the process could not be parallelized. For LongAgent, the leader agent identify conflicts between two agents ai,t1 and aj,t1, (j = i), and the individual message and original chunk are passed to each other. So in Equation (4), D(LA) Gi,t1,t = {di, dj} and M(LA) Gi,t1,t1 = {mi,t1, mj,t1}. Thus, G(LA) i,t1 = {ai,t1, aj,t1}, and the bandwidth is 2. For each timestep, the two agents could function in parallel. Though, the total timesteps is O( ), which is the number of synchronization decisions made by the leader agent. However, due to the capacity of the leader agent, is rather small and most agents barely access all other contexts. Specifically, the task query might be changed to generated sub-queries by the leader agent during the process. For LLMMapReduce, agents function in parallel and aggregate their messages in groups with adjacent agents. The group size is O( ) for agent to process messages within the context window at the next timestep. And M(MR) Gi,t1,t1 = ϕ. Thus, the bandwidth is O( m ). Each group could be processed in parallel at the same timestep. Knowledge-Integrated Reasoning Here, we explain how existing methods integrate knowledge in reasoning according to Equation (5). For Chain of Agents, the reasoning agent only takes the message from the last agent aN,T as the reasoning context M(CoA) = {mN,T }, (T = ). For LongAgent, the leader agent takes all messages from all agents in sequential as the reasoning context M(LA) ), the context would not exceed the context window. For LLMMapReduce, the reduce agent takes the last messages from all agents as the reasoning context M(MR) = {mi,T }1iN . The method guarantees that the reasoning context is within the context window by recursively aggregating the messages in groups with adjacent agents. The last two methods tend to include as much information as possible in the reasoning context, which may lead to overload with irrelevant, noisy information. = {mi,t}1iN,1tT . Since = O( r"
        },
        {
            "title": "E Implementation Details",
            "content": "E.1 Experiment Details All datasets and models are used under their individual licenses without intention to violate any terms. 24 Table 7: Performance on Bench with the optimal setting and the corresponding input length (#tokens). Other settings are the same as Table 3. Method En.QA Zh.QA F1 Input F1 Input DeepSeek-R1-Distill-Llama-8B Direct Input .104 32k .144 32k gpt-4o-mini-2024-07Direct Input LLMMapReduce EXTAGENTS (ours) .189 .385 .421 128k 128k 1024k Llama-3.1-8B-Instruct Direct Input LLMMapReduce EXTAGENTS (ours) .267 .287 . 128k 256k 512k .206 .443 .491 .316 .347 .352 128k 128k 1024k 128k 128k 256k We utilized several large language models in our experiments. Closed-source models included gpt-4o-mini-2024-07-18 and gpt-4o-2024-08-06, accessed via API. For these models, the sampling temperature was set to 0. Open-source models employed were Llama-3.1-8B-Instruct and Llama-3.23B-Instruct. These models were deployed on four NVIDIA A100 80GB GPUs, and the sampling temperature was set to 0.1. The maximum input context length was 128,000 tokens for the closedsource models and 131,092 tokens for the open-source models. Baseline implementations are adjusted slightly for each task. For En.QA and Zh.QA test sets, the direct input method is implemented using the official InfiniteBench repository [29]. The LLMMapReduce is re-implemented by us to align with the our settings with EXTAGENTS. For HotpotQA, we re-implement the DRAG and IterDRAG methods due to the lack of official code, utilizing the prompts provided in the appendix of Yue et al. [11]. For the survey generation task, the AutoSurvey baseline was adapted from the official implementation [31], with the reflection and refinement removed from the original pipeline. For the metrics, the citation density is calculated as the number of citations divided by the number of thousand tokens in the generated survey. The duplication rate is calculated as the number of duplicate citations divided by the total number of citations. The LLM-as-a-Judge method on long survey generation is implemented with gpt-4.1-mini-2025-04-14 [3], with temperature set to 1. For EXTAGENTS, in En.QA and Zh.QA tests, information extracted from different chunks is ranked based on scores rated by Seek Agents, based on the task query. For the HotpotQA test, the information is ranked based on the retrieval priority of chunks. The chunk exclusion mechanism is disabled in long-document QA tasks. For AutoSurvey task, we set the number of chunks of input retrieved papers to 4, and thus assigning 4 Seeking Agents to input papers. Other hyper-parameters are set to the default values. The overall process for our methods involves maximum of = 5 synchronization timesteps. We only employ the knowledge accumulation strategy at = 1. This strategy processes information incrementally, starting with the top 1, then top 2, top 4, top 8, and finally all information, following power-of-two sequence. The iteration halts once the answer is obtained. E.2 Additional Experimental Results Helmet Correctness Scores on Multi-Hop QA We provide corresponding Helmet correctness scores [61] on multi-hop QA tasks complementary to Table 3 in Table 6. F1 scores may misjudge the performance of LLMs, especially when the response is long and the answer is not concise enough. In our experiments, we observe that the trend of Helmet correctness scores are consistent with the F1 scores, indicating that the performance of EXTAGENTS is robust and reliable. Results on the Original Bench We provide the result on the original Bench in Table 7. We observe the same trend as in Bench+ in Table 3, that EXTAGENTS achieves the highest performance with the longest input contexts. And other methods, including inference-time scaling with long CoT, fail to utilize longer contexts beyond the context window and reach inferior results. Detailed Results on Multi-Hop QA We provide the detailed results on multi-hop QA tasks in Figure 7 and Figure 8. The results are consistent with the main findings in Figure 5 with enriched results. 25 Figure 7: Detailed experimental results of scaling external knowledge input on multi-hop QA tasks with gpt-4o-mini, Complementary to Figure 5 with the same subfigure arrangement. Results on Weaker & Stronger LLMs We also test the performance of EXTAGENTS on weaker LLM, Llama-3.2-3B-Instruct, besides stronger LLM, gpt-4o-2024-08-06, on HotpotQA benchmark. The results are shown in Figure 9 and Figure 6, respectively. We observe that EXTAGENTS achieves consistent performance improvements over scaled external knowledge input, and the performance gap is larger on the stronger model, potentially due to the better collaboration capability of stronger LLMs. Figure 9: Results of EXTAGENTS with Llama-3.2-3B-Instruct on HotpotQA benchmark. 26 Figure 8: Experiment of scaling external knowledge input on multi-hop QA tasks with Llama-3.18B-Instruct. The arrangement of subfigures is the same as Figure 5. Detailed Costs Analysis We demonstrate the average costs of EXTAGENTS and baseline methods on gpt-4o-mini-2024-07-18 in Table 8. The costs are calculated based on the average number of tokens in the input and output, where the cost of 1M token input is $0.15 and 1M token output is $0.60. The extra cost of EXTAGENTS is due to the global knowledge synchronization, which introduces larger bandwidth and according costs. Table 8: Average costs ($) of EXTAGENTS and baseline methods on gpt-4o-mini-2024-07-18. Method HotpotQA En.QA Zh.QA DRAG IterDRAG LLMMapReduce 0.019 0. EXTAGENTS (ours) 0.021 0.022 0.025 0.021 0. E.3 Prompt Templates We enumerate the prompt templates used in experiments in the order of tasks. Prompt Templates for HotpotQA Knowledge synchronization: First iteration We are working on long-text question answering, and you are responsible for one chunk. Read the following chunk and extract as much information as possible related to the question. Ensure your extracted information provides clear context and is logically complete. If no information, just output \"NO INFORMATION\". Your chunk: {Retrieved documents} Question:{question} Knowledge synchronization: Other iterations We are working on long-text question answering, and you are responsible for one chunk. This is the {iteration} round of Q&A. And we have the previously extracted information from all 27 chunks in the previous round. Based on the previously extracted information and question, extract new information from the chunk. Do not repeat the previously extracted information. If no new information, just output \"NO INFORMATION\". Your chunk: {Retrieved documents} Previously extracted information: {Previously extracted information} Question: {question} Knowledge-accumulating reasoning: No need to terminate We have the following extracted information from different chunks of the text: {Extracted information} Based on the extracted information, decide whether you can confidently answer the question. If you can, combine and reduce this information into final answer, as short as possible, word or phrase. If you cannot, just output \"NO ANSWER\". Question: {question} Knowledge-accumulating reasoning: Need to terminate We have the following extracted information from different chunks of the text: {Extracted information} Based on the extracted information, combine and reduce this information into final answer, as short as possible, word or phrase. Question: {question} Prompt Templates for En.QA in Bench+ Knowledge synchronization: First iteration Read the following article and extract as much information as possible related to the question. {Context} Question: {question} Knowledge synchronization: Other iterations We are working on long-text question answering, and you are responsible for one chunk. This is the {iteration} round of Q&A. And we have the previously extracted information from all chunks in the previous round. Based on the previously extracted information and question, extract new information from the chunk. Do not repeat the previously extracted information. Your chunk: {Context} Previously extracted information: {Extracted information} Question: {question} Knowledge synchronization: Ranking information Based on the extracted information and question, provide score (0-100) for how useful the extracted information is for answering this question. Extracted information: {extracted information} Question: {question} Please follow this format: 28 Score: (0-100) Knowledge-accumulating reasoning: No need to terminate We have the following extracted information from different chunks of the text: {Extracted information} Based on the extracted information, decide whether you can confidently answer the question. If you can, combine and reduce this information into final answer, as short as possible, word or phrase. If you cannot, just output \"NO ANSWER\". Question: {question} Knowledge-accumulating reasoning: Need to terminate We have the following extracted information from different chunks of the text: {Extracted information} Based on the extracted information, combine and reduce this information into final answer, as short as possible, word or phrase. Question: {question} Prompt Template for Zh.QA in Bench+ Knowledge synchronization: First iteration 请阅读以下文章并尽可能提取与问题相关的信息 {Context} 问题{question} Knowledge synchronization: Other iterations 我们正在进行长文本问答任务你负责处理其中一个文本块这是第{iteration}轮问 答我们在之前几轮已经对所有文本块中提取了信息请基于先前提取的信息和问 题从当前文本块中提取新信息不要重复已提取的信息 你的文本块 {Context} 先前提取的信息 {Extracted information} 问题{question} Knowledge synchronization: Ranking information 根据提取的信息和问题给出一个分数0-100评估提取的信息对回答该问题的 有用程度 提取的信息{extracted information} 问题{question} 请遵循以下格式 Score: (0-100) Knowledge-accumulating reasoning: No need to terminate 我们有以下从不同文本块中提取的信息 {Extracted information} 根据提取的信息请判断是否能确定地回答该问题如果能将这些信息合并并 简化为最终答案请尽量简短地回答只使用一个或多个词语如果不能直接输 出\"NO ANSWER\". 29 问题{question} Knowledge-accumulating reasoning: Need to terminate 我们有以下从不同文本块中提取的信息 {Extracted information} 根据提取的信息将这些信息合并并简化为最终答案请尽量简短地回答只使用 一个或多个词语 问题{question}"
        }
    ],
    "affiliations": [
        "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University",
        "Institute for AI Industry Research (AIR), Tsinghua University",
        "Tongyi Lab, Alibaba Group"
    ]
}