{
    "paper_title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
    "authors": [
        "Jiarui Zhang",
        "Ollie Liu",
        "Tianyu Yu",
        "Jinyi Hu",
        "Willie Neiswanger"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks."
        },
        {
            "title": "Start",
            "content": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Jiarui Zhang , Jinyi Hu University of Southern California, Tsinghua University , Tianyu Yu , Ollie Liu , Willie Neiswanger 4 2 0 2 1 1 ] . [ 1 7 3 7 8 0 . 2 1 4 2 : r Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP)particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, benchmark designed to evaluate an MLLMs ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with data curriculum. Notably, we find that data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks. Website: euclid-multimodal.github.io Model Weights & Datasets: huggingface.co/euclid-multimodal Code Repository: github.com/euclid-multimodal/Euclid 1. Introduction Multimodal large language models (MLLMs) have rapidly progressed in recent years, demonstrating remarkable potential in understanding and reasoning about the visual world through the powerful capabilities of large language models (LLMs) (Liu et al., 2024c,a, Achiam et al., 2023, Team et al., 2023, Hu et al., 2023, Tong et al., 2024a, Wang et al., 2024a). These models have showcased strong performance in tasks such as visual question answering (VQA) (Goyal et al., 2017), image captioning (Lin et al., 2014), and multimodal reasoning (Liu et al., 2023). As one recent example, LLaVA-NeXT-34B (Liu et al., 2024b) achieves an impressive 83.7% accuracy on the VQAv2 benchmark (Goyal et al., 2017), comprehensive benchmark on natural image question answering. While MLLMs achieve impressive results on tasks like VQA, their performance relies on high-level semantic extraction (Tong et al., 2024b); in contrast, they often fall short on low-level visual perception (LLVP)i.e., the ability to accurately describe the geometric details of an image, such as the points, lines, angles, shapes, and spatial relationships among its constituent objects. This limitation becomes especially apparent in tasks requiring precise descriptions, such as mathematical visual problem solving (Zhang et al., 2024a, Lu et al., 2023), scientific visual understanding (Yue et al., 2024, Fu et al., 2024a), abstract visual reasoning (Jiang et al., 2024, Ahrabian et al., 2024), and even simple visual comprehension (Rahmanzadehgervi et al., 2024, Wang et al., 2024b). For example, when interpreting graph diagram, precise recognition of edges is essential for extracting reliable information, and in geometry problem-solving, accurate identification of Corresponding author(s): Jiarui Zhang, jzhang37@usc.edu; Willie Neiswanger, neiswang@usc.edu Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions relationships between line segments and points is fundamental (Fu et al., 2024a). Beyond abstract tasks, LLVP is also vital in real-world applications, including spatial understanding for robotics, medical image analysis for accurate diagnosis, quality control in manufacturing to detect subtle defects, autonomous driving systems that rely on exact object localization or distance estimation, and augmented reality applications that demand precise overlay of virtual objects onto the real world. In this paper, we aim to study the challenges of LLVP in MLLMs, take steps to understand the root cause of their performance, and improve the models capabilities in this area. We begin by developing benchmark dataset specifically designed to evaluate precise geometric perception, which we call Geoperception. As focused test bed, this benchmark targets 2D geometry tasks. Using this benchmark, we demonstrate the limitations of leading closed and open MLLMs, followed by comprehensive empirical study to explore strategies for significantly improving MLLMs performance on geometric perception tasks. Our findings show the benefits of key factors such as model architecture, training techniques, and data strategies, including the use of synthetic data and multi-stage training with data curriculum. Notably, we find that data curriculum enables models to learn challenging geometry LLVP tasks, which they fail to learn from scratch, even when trained on very large dataset. Using these lessons learned, we then train family of modelsusing carefully designed curriculum of synthetic datathat are specifically optimized for strong LLVP, which we call Euclid. We evaluate this family of models, and show that it excels on variety of low-level geometric perception tasks. Our main technical contributions are as follows: Geoperception Benchmark: We introduce new benchmark dataset, Geoperception, derived from the Geometry-3K corpus (Lu et al., 2021), specifically designed to evaluate MLLMs ability to accurately perceive surface-level geometric information without requiring complex inference or reasoning. Our benchmark reveals shortcomings in precise geometric perception across all leading vision-language MLLMs, both closed and open-source. Empirical Study and Synthetic Data Engine: To investigate the root cause of this performance, we conduct detailed empirical exploration of MLLM architecture and training strategies. To aid in our investigation, we develop synthetic data engine capable of generating high-fidelity visual descriptions of geometric elements. This study leads to key insights, such as the importance of certain architectural choices and the use of curriculum-based, multi-stage training with progressively more complex visual descriptions for improving low-level visual perception. Euclid Model Family: Leveraging the insights from our exploration and our synthetic data engine, we train Euclid, series of MLLMs tailored for high-quality geometric LLVP. Although purely trained on synthetic multimodal data with simple geometry shapes, Euclid generalizes strongly to the real-world geometry images from Geoperception benchmark, for instance, outperforming the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain benchmark tasks and 10.65% across the tasks. 2. Background and Related Work We provide an overview of prior efforts that assess and improve low-level perception and geometric reasoning in MLLMs, and highlight our contributions in data synthesis, evaluation, and training. Vision-Language MLLMs. While recent iterations of LLMs feature standardized model architecture and pretraining recipe, MLLMs still often differ in design choices for infusing visual inputs. One popular design 2 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions is to align continuous visual features with the embedding space of backbone LLM (Liu et al., 2024a,b, Dubey et al., 2024, McKinzie et al., 2024, Tong et al., 2024a, Beyer et al., 2024, AI, 2023, Wang et al., 2024a); another approach involves tokenizing visual inputs to be trained jointly with language tokens (Team et al., 2023, Team, 2024a). These modules are often infused with decoder-only LLM, but others have explored encoder-decoder architectures to integrate more varied collection of modalities (Alayrac et al., 2022, Mizrahi et al., 2024, Ormazabal et al., 2024, Bachmann et al., 2024). Our study focuses on decoder MLLMs with continuous visual encoder, and we carry out an empirical study to explore the effect of synthetic dataset mixture, training recipe, and encoder design (Liu et al., 2022, Radford et al., 2021, Zhai et al., 2023, Oquab et al., 2023). Geometry-Oriented MLLMs. At the core of these choices is the hardness in designing module adept in general visual reasoning (McKinzie et al., 2024, Tong et al., 2024a). In this work, we explore the optimal design of MLLMs specialized in low-level visual perception, crucial aspect for (among other applications) multimodal mathematical understanding (Lu et al., 2023, Zhang et al., 2024a). This paper supplements prior efforts in improving mathematical reasoning (Gao et al., 2023, Zhang et al., 2024b, Zhuang et al., 2024, Li et al., 2024, Peng et al., 2024, Shi et al., 2024b) with detailed study on the effect of dataset mixture, curriculum, and visual encoder, to reach recipe that elicits strong performance on geometric tasks (Kazemi et al., 2023) that require low-level perception. Evaluating LLVP. Many benchmarks (Rahmanzadehgervi et al., 2024) have reported that frontier-class MLLMs struggle with visual perception tasks, which are prerequisites for applications that emphasize low-level geometric perception (Chen et al., 2024, Fu et al., 2024c), including mathematical (Yue et al., 2024, Lu et al., 2023, Zhang et al., 2024a, Jiang et al., 2024) and spatial reasoning (Chen et al., 2024, Fu et al., 2024b). These findings collectively identify that MLLMs exhibit language prior (Lin et al., 2023)a preference of textual inputs over visual inputsleading to performance gap between modalities (Wang et al., 2024b, Zhang et al., 2024a, Fu et al., 2024a). Meanwhile, there lacks high-quality benchmark that evaluates low-level geometric perception in MLLMs, and the Geoperception benchmark represents first effort to narrow this gap. This type of efforts have led to significant improvements in certain capabilities of MLLMs, such as compositionality of objects (Yuksekgonul et al., 2022, Kong et al., 2023). Improving LLVP. Many prior works study data-driven approaches to improve low-level perception skills. For example, Gao et al. (2023), Li et al. (2024), Zhuang et al. (2024) employ standardized supervised finetuning recipe, and optionally adjust the training data mixture. This type of training data is often synthesized from text-only math problems (Lu et al., 2021, Trinh et al., 2024) or via rule-based systems (Kazemi et al., 2023). In parallel, Vishniakov et al. (2023), Shi et al. (2024a), Tong et al. (2024b) have explored the design space of visual encoders for general-purpose vision-language reasoning. We identify best practices over the union of these design spaces, and then train small MLLMs with strong performance in low-level perception tasks. Lastly, several works (Schick et al., 2024, Surís et al., 2023, Hu et al., 2024) have opted to augment an MLLM with external APIs that process low-level features with specialized vision modules, such as object detection (Redmon et al., 2016), segmentation (Kirillov et al., 2023), and depth estimation (Yang et al., 2024). While these agentic frameworks (Wu et al., 2023) present promising alternative that directly addresses the shortcomings of visual encoders, they are limited by their scalability to novel use cases, and may be insufficient for precise tool routing that requires low-level perception as primer (Picard et al., 2023, Wu et al., 2024, Buehler, 2024). 3 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 1: Four examples from our Geoperception dataset. The questions are sourced from the Geometry-3K corpus (Lu et al., 2021), which compiles problems from two widely-used high school textbooks. We perform filtering, validation, and generate question-and-answer text for each image. 3. Geoperception Benchmark Recently, there has been growing number of multimodal benchmarks across diverse domains beyond natural image understanding, including mathematical reasoning (Zhang et al., 2024a, Lu et al., 2023) and abstract visual reasoning (Jiang et al., 2024, Chia et al., 2024). Many of these prior works have realized the importance of accurate low-level visual perception. Specifically, Marvel (Jiang et al., 2024) introduces perception questions for various abstract reasoning patterns, and finds that the main bottleneck of MLLMs performance on abstract visual reasoning is that they fail to accurately transcribe visual information into concepts; Mathverse (Zhang et al., 2024a) and IsoBench (Fu et al., 2024a) both test MLLMs on equivalent question represented by language and visual modalities, respectively. Both works find that language-only input always outperforms vision-language input, and that the vision component of MLLMs always fails to utilize low-level visual features. VDLM (Wang et al., 2024b) transcribes raster images into vector graphics and uses LLMs to reason over the SVG code. They find that although SVG code is not straightforward to understand, using LLMs to reason over SVG is consistently more effective than directly using MLLMs on original raster images. Blind-test (Rahmanzadehgervi et al., 2024) and BLINK (Fu et al., 2024c) also share similar findings with the works above. Benchmark for Geometric LLVP. Although such shortcomings of MLLMs are commonly recognized, there is lack of comprehensive benchmark that purely focuses on these abilities of MLLMs. Our goal is to construct benchmark focusing solely on the perception ability of MLLMs, which is also representative enough of real-world applications. When humans perceive and memorize visual information, it is wellrecognized that this procedure relies crucially on searching for the closest and simplest corresponding geometric shapes (Sablé-Meyer et al., 2022). We posit that geometric perception is fundamental and broadly representative LLVP ability in many applications. Hence, we select geometry understanding as our domain of dataset construction. Benchmark Tasks. Over two thousand years ago, Euclid introduced five axioms that underpin all further geometric reasoning. These axioms involve establishing and extending lines using points (Axioms 1 and 2), constructing circles from point and radius (Axiom 3), and defining perpendicularity (Axiom 4) and parallelism (Axiom 5). Additionally, Euclid provided common notions regarding the properties of equality. To capture these aspects, we define five tasks in our Geoperception dataset: PointLiesOnLine, 4 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions PointLiesOnCircle, Parallel, Perpendicular and Equal, and additionally define AngleClassification and LineComparison tasks to assess the models understanding of angle and length measurements, resulting in total of seven tasks. In geometric diagrams, perpendicularity, parallelism, and equality are often indicated by annotation symbols. Thus, we classify Parallel, Perpendicular, and Equal as annotated geometry understanding. Meanwhile, PointLiesOnLine, PointLiesOnCircle, AngleClassification, and LineComparison fall under primitive geometry shape understanding, which includes both logical (PointLiesOnLine, PointLiesOnCircle) and numerical (AngleClassif-ication, LineComparison) tasks. Data Filtering. Geoperception is sourced from the Geometry3K (Lu et al., 2021) corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-school textbooks. However, certain points in these logical forms are absent in the corresponding diagrams. To resolve this, we use GPT-4o-mini MLLM to confirm the presence of all points listed in the logical forms. This process filters the 3,002 diagrams to retain 1,584, where at least one logical form fully represents its points in the diagram. random inspection of 100 annotations reveals only two errors, indicating high annotation accuracy. Table 1: Statistics of the seven tasks in our Geoperception dataset, including the number of questions and images."
        },
        {
            "title": "Predicate",
            "content": "# # I"
        },
        {
            "title": "PointLiesOnLine\nPointLiesOnCircle\nParallel\nPerpendicular\nEquals",
            "content": "924 1901 322 359 101 106 1266 456 4436 1202 AngleClassification 2193 1389"
        },
        {
            "title": "LineComparison",
            "content": "Converting Logical Forms Into Questions. We convert logical forms into question-and-answer pairs for each of the seven tasks in Geoperception. In the Equals task, for example, we directly convert the logical form (e.g., Equals(LengthOf(Line(Q, T)), 86)) into question-answer pair (e.g., Q: What is the length of line QT as annotated? A: 86). For PointLiesOnLine, two points on the line are chosen to form the question, with the remaining points on the line as the answer. Similarly, for PointLiesOnCircle, we ask which points lie on the circle, using its center as the basis for the question. For Parallel and Perpendicular, we represent each line by two points and query which other lines are parallel or perpendicular to it. In AngleClassification, we ensure the queried angle is in the range of [10, 80] [100, 170] degrees to avoid ambiguity. For LineComparison, we ensure that the shorter line is less than 70% of the length of the longer line. Since multiple equivalent questions can be generated for single logical form (e.g., line containing five points generates 5P2 equivalent questions), we randomly select one to avoid redundancy. Table 1 summarizes the question statistics for each task, as well as the number of images involved. Four examples from Geoperception are illustrated in Fig. 1. Evaluation Details. We evaluate seven leading MLLMs, both open source and closed source. The open source models include Molmo-7B-D (Deitke et al., 2024), Cambrian-1-8B (Tong et al., 2024a), Qwen2VL-7B (Wang et al., 2024a), Llama-3.2-11B (Dubey et al., 2024), and Pixtral-12B (AI, 2023). The closedsource models include GPT-4o-mini (Achiam et al., 2023), GPT-4o (Achiam et al., 2023), Claude-3.5-Sonnet (Anthropic, 2024), Gemini-1.5-flash (Team et al., 2023), and Gemini-1.5-pro (Team et al., 2023). Additionally, GPT-4o-mini without image input is used for generating the random baseline, employing the same textual instructions. To prevent stretching, all images are padded to square dimensions before being fed into the models. During evaluation of given question by an MLLM, let denote the ground truth set of answers, 5 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions and let denote the predicted set of answers; then the evaluation score is defined as Evaluation score = G 0 if G, otherwise. (1) Current MLLMs struggle to perceive low-level geometry annotations and relationships. We show comparison of all models on Geoperception in Table 2. Despite the simplicity of Geoperception for humans, it remains considerable challenge for even the most advanced commercial MLLMs. Notably, all models fall short of achieving 30% accuracy on the PointLiesOnLine task and do not outperform the text-only GPT-4o mini model in AngleClassification task. Closed source models generally outperform open source ones, with Gemini-1.5-pro attaining the highest average score of 56.98%, followed by gemini-1.5-flash at 54.76%. Among open source models, Pixtral-12B achieves the best performance with an overall score of 41.95%. It is worth noting that Cambrian-1 (Tong et al., 2024a), which is reported to be trained on Geo-170K (Gao et al., 2023), geometry multimodal instruction tuning dataset built on the logical annotation of Geometry-3K, the same source with our Geoperception, still faces challenges in our Geoperception task, despite being trained on the dataset having the same images and augmented text annotations. Table 2: Performance (average evaluation score) of different models on Geoperception benchmark tasks. POL: PointLiesOnLine, POC: PointLiesOnCircle, ALC: AngleClassification, LHC: LineComparison, PEP: Perpendicular, PRA: Parallel, EQL: Equals. As the Random Baseline method, we use GPT-4o-mini, given the same textual instruction but without an image. The best model for each task is bolded. Model Random Baseline Molmo-7B-D (Deitke et al., 2024) Llama-3.2-11B (Dubey et al., 2024) Qwen2-VL-7B (Wang et al., 2024a) Cambrian-1-8B (Tong et al., 2024a) Pixtral-12B (AI, 2023) Logical Numerical Annotations POL 1.35 POC 2.63 LHC ALC 59.92 51.36 PEP 0.23 PRA 0. EQL 0.02 Overall 16.50 Open Source 11.96 16.22 21.89 15.14 24.63 35.73 37.12 41.60 28.68 53. 56.77 59.46 46.60 58.05 47.33 Closed Source 16.79 52.08 63.27 61.48 51.43 1.06 8.38 26.41 22.96 21.96 0.00 22.41 30.19 30.74 36.64 0.81 49.86 54.37 31.04 58. 44.74 4.25 9.80 GPT-4o-mini (Achiam et al., 2023) 9.80 48.84 61.19 44.69 60.30 24.80 16.43 71.49 55.63 GPT-4o (Achiam et al., 2023) 63.92 66.34 21.41 42.95 68.34 Claude 3.5 Sonnet (Anthropic, 2024) 25.44 49.89 Gemini-1.5-Flash (Team et al., 2023) 29.30 67.75 66.28 63.44 29.98 57.96 79.05 38.81 76.65 52.15 69.80 Gemini-1.5-Pro (Team et al., 2023) 69.51 74.39 70.73 76.69 24.42 17.59 35.08 40.62 35.44 41.95 35.45 49.68 51.30 54.76 56.98 4. Empirical Study on MLLM Design Space Although large-scale web-crawled image-text pairs cover variety of domains, including geometry, the textual descriptions often lack the necessary specificity and depth. To address this issue, current studies in this domain (Gao et al., 2023, Shi et al., 2024b, Zhang et al., 2024b) typically construct geometry or 6 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions mathematical domain dataset and apply the same training strategy used for general-purpose MLLMs. For example, Math-LLaVA (Shi et al., 2024b) and multi-math (Peng et al., 2024) rely on GPT-4V or GPT-4os vision ability to generate most of the question and answer pairs and image captions, which is essentially model distillation. However, as evidenced by Table 2, GPT-4o and Gemini-1.5-Pro often struggle to answer even basic perception questions, limiting the performance potential of resulting models. Furthermore, while works such as G-LLaVA (Gao et al., 2023), MAVIS (Zhang et al., 2024b), and Math-PUMA (Zhuang et al., 2024) utilize human crafted logical forms or synthetic multimodal data to ensure the reliability of textual annotations, they often conflate low-level perception with problem-solving, and train models to directly solve multimodal geometry problems, without verifying if the models low-level perception abilities are sufficient. As an evidence, the best models in MAVIS (Zhang et al., 2024b) and Math-PUMA (Zhuang et al., 2024) evaluation results on Mathverse (Zhang et al., 2024a) still have substantial gap of 26.8% and 28.7% between text-dominant versions and vision-only versions of problems1, respectively. Furthermore, attempts to train MLLMs on low-level visual perception tasks (Wang et al., 2024b, Rahmanzadehgervi et al., 2024) have also struggled to achieve satisfactory in-domain performance or generalize effectively. In this section, we aim to address these challenges. We hypothesize the inability of todays MLLMs to effectively perceive basic geometric annotations and relationships stems from two factors: 1. The lack of high-fidelity geometric visual perception training data. 2. The problem of their model architectures and training strategy. Next, we will introduce our geometry dataset generation engine to overcome the lack of data, and then use generated dataset to study the optimal training strategy. Figure 2: Three geometry logical shapes, of increasing complexity, used in our empirical study. Our geometry image generation engine is able to produce infinite visual instances for each of these logical shapes. All letters are randomly sampled from the alphabet and reassigned to each of the points before drawing. Geometry Image Generation Engine. To provide sufficient high-fidelity training datasets, we develop synthetic dataset generation engine to programmatically produce geometry shapes. Our geometry shape generation engine is built on AlphaGeometry (Trinh et al., 2024). Given an input formal language describing geometry shape, the geometry engine will first check the validity of the geometry shape. Then it will create numerical positions for all points following the restrictions given by the input. After the creation of all points, it will connect the line as specified in the input. To avoid inductive bias during training (e.g., point is always on top of triangle), letters are first picked from letter pool (e.g., all 26 capital letters) and then randomly assigned to each point. In addition to the original image generation engine, we introduce three visualization enhancements: (1) additional inputs to control the connections between points, number of letters in the letter pool, presence of each points, and annotation about length and angles; (2) increased randomness in creating numerical instances from conceptual shapes; 1In Mathverse, text-dominant is the version where the problem is mainly represented by text, while in the vision-only version an equivalent problem is represented purely by image. 7 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 3: LLM size experiments. Training loss and testing accuracy curve comparing three choices of LLM size with fixed visual encoder and multimodal connector. Training losses are window-smoothed using window size of 10 for better visibility. and (3) adjustments to the canvas range to ensure visibility of all geometry components. Examples of our geometry generation engine, showing three geometries of increasing complexity, are shown in Fig. 2 Exploration of MLLM design space. With sufficient training dataset, we now revisit the MLLMs architectural and training design space. We choose 2 primitive geometry tasks from Geoperception as the test bed for the exploration: logical task, PointLiesOnLine and numerical task, LineComparison. For each task, we carefully create three tasks with incremental difficulty levels. We name them as difficulty level easy, medium and hard. Based on the insight from our preliminary experiments, to increase the difficulty levels, for PointLiesOnLine, we increase the complexity of geometry shapes as is shown in Fig. 18, for LineComparison, we increase the total number of letters in letter pool while mixing geometry shapes. During our preliminary experiments, we find that sometimes the model fails to converge due to instability. To this end, for all experiment moving forward, we run the training for three times and report the best run among them (i.e., having the lowest overall training loss or testing accuracy). We start with typical setting of MLLMs: CLIP-ViT-L/14 (Radford et al., 2021) as the visual encoder and two layer MLP as multimodal connector and the latest Qwen-2.5 series (Team, 2024b) as LLM. During training, we actively tune the MLP and LLM, while keeping visual encoder frozen. We use the mixture of three difficulty levels as the training set. Lesson 1: Under the same training dataset, scaling LLM sizes does not lead to better performance. It is commonly acknowledged that under the same training dataset, scaling up LLM can lead to better MLLM performance (Liu et al., 2024a). To this end, we first vary the sizes of LLMs, Qwen-2.5 (Team, 2024b) in range of 0.5B, 1.5B, and 3B while keep other components consistent. The result is shown in Fig. 3. For LineComparison, Qwen-2.5-1.5B performs the best while Qwen-2.5-3B learns most slowly. For PointLiesOnLine, Qwen-2.5-1.5B and Qwen-2.5-3B performs almost the same. Qwen-2.5-0.5B learns relatively slower, but still reach almost the same final performance with two other models. In conclusion, we do not observe an obvious trend that larger LLMs can learn such low-level visual perception task faster or 8 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 4: Vision encoder experiments. Training loss and testing accuracy (on 1500 instances holdout test set) curve comparing eight visual encoders, with fixed multimodal encoder and LLM. For fair comparison, all visual encoder transcribe an image into 256 visual tokens. Training losses are window-smoothed using window size of 10 for better visibility. better. Moving forward, we will use Qwen-2.5-1.5B to continue our exploration. Model Params Objective Table 3: Summary of Visual Encoders Lesson 2: CNN architecture performs better than ViT. We then study the choice of visual encoder architectures, including two families of architectures: Vision Transformer (ViT) (Dosovitskiy, 2020) and ConvNeXT (Liu et al., 2022); as well as two visual representation learning objectives: language-supervised learning (Radford et al., 2021) and self-supervised learning (Oquab et al., 2023). We control the number of visual tokens to 256 for all of our vision encoders. The result is shown in Fig. 4. We find that ConvNeXt-XXLarge and ConvNeXt-Large consistently learns the geometric concept the fastest among all of the visual encoders. Notably, ConvNeXT-Large shows superior learning performance with the vision transformers which are 3-5 times larger. We hypothesize that CNN architecture extract visual features globally, effectively preserving low-level visual features. In contrast, ViT architectures split images into discrete patches, making it more challenging to retain the original low-level visual information. Self-supervised learning (SSL) visual encoders, DINO-v2, struggles to learn the geometry concept; we hypothesis this is due to the weak vision-language representation in these models. Surprisingly, although the SigLIP-family is widely-recognized as better visual encoder (Tong et al., 2024a), we find that their performance in learning basic visual geometry attributes is limited. ConvNeXt Large@512 ConvNeXt XXLarge@512 ViT-g/14@224 ViT-H/14@224 ViT-L/14@224 SigLIP@224 (ViT) DINOv2 Giant@224 (ViT) DINOv2 Large@224 (ViT) 200M 847M 1.01B 632M 303M 428M CLIP-like Self-Sup 1.14B 304M Self-Sup CLIP CLIP CLIP CLIP CLIP Lesson 3: Tuning vision encoder does not provide significant help. We next study the effect of tuning versus freezing the visual encoder. In Fig. 5, we show the testing accuracy curves of tuning and freezing visual encoders. We find that compared with using frozen encoder, tuning the visual encoder does not help 9 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 5: Tuning/freezing vision encoder experiments. Testing accuracy (on 1500 instances holdout test set) curve comparing freezing versus tuning the visual encoder during training. the model learn low-level geometry relationships faster or better. In what follows, we will freeze the encoder for simplicity. Lesson 4: Curriculum learning unleashes full potential. Finally, we study training data composition. In our preliminary experiment Fig. 19, we observe that the model fails to converge on difficulty level 3 of PointLiesOnLine and difficulty level 2 and 3 of LineComparison. However, when using mixed training set of all three difficulty levels, the model achieves convergence, despite using the same amount of data for each difficulty levels. We hypothesize that including easier levels aids the model in learning more complex levels. To test this hypothesis, we report the test accuracy for three difficulty levels separately during the mixed training of ConvNeXtXXLarge, in Fig. 6, on both tasks. We notice that the testing accuracy for easier tasks increase earlier and more quickly than difficulty tasks. In PointLiesOnLine tasks, we notice an apparent plateau for hard level tasks until the model has trained on approximately 20K samples. During this period, the testing accuracy for easy and medium continue to increase. This suggests that learning easier shapes can significantly help the model tackle more challenging shapes, comparing with directly learning the challenging ones, this finding align with the principles of curriculum learning. Figure 6: Separate testing accuracy curves on difficulty levels easy, medium, and hard, shown over the course of training on mixture of all difficulty levels. While mixed training enables effective spontaneous curriculum learning, we investigate whether structured curriculum can further enhance model efficiency on challenging shapes. To this end, we train the model sequentially from simple to more complex shapes and compare testing accuracy just on hard level tasks. During training, we monitor the models performance and dynamically adjust the distribution of Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 7: Curriculum learning experiments. Test accuracy on difficulty level hard of three training strategies: purely training on difficulty level hard, mixed training of difficulty levels easy/medium/hard, and curriculum training. training data (i.e., the curriculum stage) based on this performance. Specifically, the model starts by training on the easy level data. and is evaluated when it finishes training round, using testing accuracy from the current level of data. Upon evaluation, if the model achieves an accuracy exceeding predefined threshold θ, the framework advances the level to the next. Formally, the update rule for advancing stages is given by: if accuracy > θ + 1. (2) The model is trained on total of rounds and steps within each round. To avoid forgetting, we apply data smoothing at each stage. Specifically, we smooth our dataset distribution over all stages using an exponential attenuation function: where α denotes the attenuation rate. Eq. (3) ensures that stages proximal to the current stage receive higher sampling probabilities. ratio = exp (α stage c) , (3) 11 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions We refer to this as our curriculum training strategy. Specifically, the accuracy threshold for advancing training stage θ is set to 0.99. We train all the models for = 30 rounds, each round with = 50 steps. The results are shown in Fig. 7. Firstly, we find that all of the models fail to converge when trained purely on hard level for PointLiesOnLine task. In contrast, the mixed training strategy shown by the red curve, consistently reaches faster convergence on hard level. Curriculum training strategy, shown by the purple curve, proves more efficient than mixed training. 5. Euclid: Family of MLLMs for Geometric Visual Perception In this section, we take all of the lessons we learned in the previous sections and train Euclid, family of MLLMs specifically designed for strong geometric LLVP. On-the-fly progressive training. We use the same strategy as the curriculum training in Section 4, but scale our training to all tasks in Geoperception. For each task, we create stages of training dataset shapes with progressively increasing geometric complexity. Specifications. For models, we select the best visual encoder architecture we found in our investigation, ConvNeXt, including ConvNeXt-Large@512 and ConvNeXt-XXLarge@512, and keep the same multimodal connector (2 layers MLP) and LLM (Qwen2.5-1.5B-instruct). The accuracy threshold for advancing training stage θ is set to 0.99. All models are trained on = 3 stages with manually curated geometry shapes and = 50 rounds with = 500 steps in each round, and the batch size is 64 for each training step. The total training dataset volume for both of the models is 1.6M. Table 4: Performance comparison between Euclid and the best leading open source and closed source MLLMs on the seven tasks. Note that Euclid is not trained on any of the in-distribution data from the benchmark tasks below. The best model for each task is bolded. Logical Numerical Annotations Model Random Baseline Pixtral-12B (AI, 2023) Gemini-1.5-Pro (Team et al., 2023) Euclid-ConvNeXt-Large Euclid-ConvNeXt-XXLarge PEP EQL PRA ALC POC POL 0.25 0.00 0.02 59.92 2.63 0.43 36.64 58.41 21.96 47.33 53.21 24.63 38.81 76.65 52.15 24.42 69.80 57.96 34.45 80.54 64.94 42.23 86.37 57.76 31.94 82.98 61.45 90.56 90.82 46.96 70.52 LHC 51.36 51.43 79.05 88. Average 16.37 41.95 56.98 64.93 67.89 Evaluation results. The results are shown in Table 4. Overall, although only trained on very simple synthetic geometry shapes, and using only 1.5B language model, Euclid significantly outperforms current leading MLLMs in most of the tasks, showing strong generalization abilities on real-world geometry LLVP. Notably, in the PointLiesOnLine task, which is particularly challenging for existing MLLMs, Euclid achieves up to 82.98% accuracy, more than three times the performance of Gemini-1.5-Pro. On all both numerical tasks, LineComparison and AngleClassification, Euclid keeps superior performance. However, on three annotation tasks, Euclids performance is limited. We hypothesis this is due to the limited setting of our annotation types and styles, making the model hard to generalize to diverse human geometry annotations. 12 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Error analysis. We take deep look into Euclids prediction on Geoperception, we find that our models performance is hindered when diagrams are heavily annotated. An example is shown in Fig. 8, where line is annotated by x, confusing the model from choosing the correct point. Incorporating training data with more diverse annotation types, geometry shape and can better distinguish different diagram annotation types could potentially help the model with such scenarios. 6. Conclusion and Future Work In this work, we highlight the importance of accurate low-level visual perception(LLVP) in MLLMs. To this end, we first introduce Geoperception, large-scale multimodal benchmark focused exclusively on geometry-domain visual perception. We evaluate leading MLLMs on Geoperception, find that even top models such as Gemini-1.5-Pro struggle significantly it, although it is straightforward for humans. We then conduct an empirical study to explore the design space of MLLM training and architectures using the dataset generated by geometric high-fidelity synthetic-data engine that we develop. Our study indicate that convolutional neural network visual encoders outperform vision transformers in our tasks; tuning the visual encoder generally enhances performance; and employing curriculum-based training approach yields much more model potential than direct task training. Based on insights from this study, we develop Euclid, model trained purely on highfidelity synthetic generated data, which generalizes effectively to real-world geometric shape understanding tasks, surpassing the leading MLLMs by substantial margin. Figure 8: An error case where Euclid fails to predict the correct point on line, potentially distracted by the annotation x. Future work. Our work examines the potential of using synthetic multimodal data to improve MLLM performance in low-level geometric perception tasks. However, there are still directions that remain underIncorporating more diverse dataset, including varied explored: (1) Automatic curriculum learning. geometric shapes and different domain dataset, introduces challenges in defining the learning order. Rule based definition and manual curation may become impractical, necessitating automated strategies like hard negative sampling to organize the curriculum based on training loss or testing accuracy. This approach could streamline the process, reduce human effort, provide more suitable and efficient curriculum learning orders. (2) Using more-diverse training dataset. Currently, the text portion of our synthetic multimodal training data uses restricted set of templates, and the model trained on such templates could fail to generalize to other question types; it could therefore be beneficial to increase the diversity of our training images as well as the instruction-following formats. (3) Generalizing to other task domains. In this work, our study is focused on data from 2D geometry, as it provides focused test bed of fundamental tasks. We believe the lessons we learn from this domain can be effectively generalized to broader set of downstream domains that benefit from high-quality LLVP."
        },
        {
            "title": "Reproducibility Statement",
            "content": "In Section 3, we provide comprehensive description of the procedure for generating the Geoperception benchmark. This includes employing GPT-4o-mini for dataset filtering and detailing the conversion of logical forms into questions and answers. Evaluation prompts for MLLMs on different types of Geoperception questions are presented in Appendix B. For model architecture exploration, we specify the visual encoders and provide corresponding Hugging Face links in Table 3. Additionally, we outline the LLMs and multimodal connector architectures used. For our Euclid model, we include all geometry shape code used for training, along with demonstration diagrams and pseudo-code for generating training questions and answers. 13 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, and Jay Pujara. The curious case of nonverbal abstract reasoning with multi-modal large language models. arXiv preprint arXiv:2401.12117, 2024. Mistral AI. Pixtral 12b. https://mistral.ai/news/pixtral-12b/, 2023. Accessed: 2024-09-27. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. Anthropic. The claude 3 model family: Opus, Sonnet, Haiku, March 2024. URL https: //www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_ Claude_3.pdf. Roman Bachmann, Oğuzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, and Amir Zamir. 4m-21: An any-to-any vision model for tens of tasks and modalities. arXiv preprint arXiv:2406.09406, 2024. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Markus Buehler. Cephalo: Multi-modal vision-language models for bio-inspired materials analysis and design. Advanced Functional Materials, page 2409531, 2024. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models, 2024. URL https://arxiv.org/abs/2409.17146. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 14 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations. In First Conference on Language Modeling, 2024a. URL https://openreview.net/forum?id= KZd1EErRJ1. Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, and Lawrence Chen. Tldr: Token-level detective reward model for large vision language models. arXiv preprint arXiv:2410.04734, 2024b. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024c. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et al. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint arXiv:2308.12038, 2023. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. Yifan Jiang, Jiarui Zhang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, and Jay Pujara. Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning. arXiv preprint arXiv:2404.13591, 2024. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, and Greg Ver Steeg. Interpretable diffusion via information decomposition. arXiv preprint arXiv:2310.07972, 2023. Zhihao Li, Yao Du, Yang Liu, Yan Zhang, Yufang Liu, Mengdi Zhang, and Xunliang Cai. Eagle: Elevating geometric reasoning through llm-empowered visual instruction tuning. arXiv preprint arXiv:2408.11397, 2024. 15 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, and Deva Ramanan. Revisiting the role of language priors in vision-language models. arXiv preprint arXiv:2306.01879, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https://llava-vl.github.io/ blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36, 2024. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, et al. Reka core, flash, and edge: series of powerful multimodal language models. arXiv preprint arXiv:2404.12387, 2024. Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Cyril Picard, Kristen Edwards, Anna Doris, Brandon Man, Giorgio Giannone, Md Ferdous Alam, and Faez Ahmed. From concept to manufacturing: Evaluating vision-language models for engineering design. arXiv preprint arXiv:2311.12668, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. Mathias Sablé-Meyer, Kevin Ellis, Josh Tenenbaum, and Stanislas Dehaene. language of thought for the mental representation of geometric shapes. Cognitive Psychology, 139:101527, 2022. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024. Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024a. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024b. Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1188811898, 2023. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024a. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Qwen Team. Qwen2.5: party of foundation models, September 2024b. URL https://qwenlm.github. io/blog/qwen2.5/. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024b. 17 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu. Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy. arXiv preprint arXiv:2311.09215, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, and Heng Ji. Textbased reasoning about vector graphics. arXiv preprint arXiv:2404.06479, 2024b. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. Sifan Wu, Amir Khasahmadi, Mor Katz, Pradeep Kumar Jayaraman, Yewen Pu, Karl Willis, and Bang Liu. Cadvlm: Bridging language and vision in the generation of parametric cad sketches. arXiv preprint arXiv:2409.17457, 2024. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024a. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024b. Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024. 18 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions"
        },
        {
            "title": "Appendix",
            "content": "A. Geoperception Benchmark Details In Table 5, we provide more details on the Geoperception benchmark, such as the number of logic forms present before and after filtering, the number of questions, and the number of images. AngleClassification and LineComparison are directly derived from points coordinates without filtering. Predicate # LF Before Filter # LF After Filter # # I"
        },
        {
            "title": "PointLiesOnLine\nPointLiesOnCircle\nParallel\nPerpendicular\nEquals",
            "content": "6988 1966 222 1111 6434 2567 1240 123 680 4123 924 1901 322 359 101 106 1266 456 4436 1202 Table 5: Statistics of the five predicates in our Geoperception dataset. Including number of logic forms before filter, after filter and the number of questions and images. 19 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 9: Examples of our Geoperception dataset. 20 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions B. Prompts for the Geoperception Dataset Evaluation romp Templat for he PointLiesOnLine Task Answer me directly just with the all points lie on the line mentioned in the question (do not include the point mentioned in the question). Answer template: (If only one point) The other point is: \"your point\". Or (if multiple points) The other points are: \"your points\". For example: The other point is: Or The other points are: A, B, Figure 10: Template for he PointLi esOnL ine tasks"
        },
        {
            "title": "Prompt Templat e for t he PointLiesOnCircle Task",
            "content": "Answer me directly just with the all points lie on the circle mentioned in the question. Answer template: (If only one point) The point is: \"your point\". Or (If multiple points) The points are: \"your points\". For example: The point is: Or: The points are: A, B, Figure 11: Template for he PointLi esOnC ircle tasks"
        },
        {
            "title": "Prompt Templat e for t he Par al lel Task",
            "content": "Answer me directly just with the all lines which are parallel to the line mentioned in the question (do not include the line mentioned in the question). Answer template: (If only one line) The line is: \"your line\". Or (If multiple lines) The lines are: \"your lines\". For example: The line is: BC Or: The lines are: BC, DE Figure 12: Template for he Par allel task Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions romp Templat for he Pe rpendicul ar Task Answer me directly just with the all lines which are perpendicular to the line mentioned in the question (do not include the line mentioned in the question). Answer template: (If only one line) The line is: \"your line\". Or (If multiple lines) The lines are: \"your lines\". For example: The line is: BC Or: The lines are: BC, DE Figure 13: Template for he erpe nd icular tasks romp Templat for he Equals Task Answer me directly just with the annotations presented on the image. Answer template: The annotation is: \"your annotation\". For example: The annotation is: 2x+ Or: The annotations is: 90 Figure 14: Template for he Equals task s"
        },
        {
            "title": "Prompt Templat e for t he A ngle Classification Task",
            "content": "Answer me directly just with the classification of the angle mentioned in the question. Answer template: The angle is: \"your angle\". For example: The angle is: acute Or: The angle is: obtuse Figure 15: Template for he ngl lassif ication tasks"
        },
        {
            "title": "P romp t Templat e for t he LineComparison Task",
            "content": "Answer me directly just with the longer line mentioned in the question. Answer template: The longer line is: \"your line\". For example: The longer line is: BC Or: The longer line is: DE Figure 16: Template for he ineComparison task 22 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions C. Details for Training Data Engine In this section, we provide all geometry shapes we use for Euclid training, including the pseudocode for generating text describing the geometry shapes and diagram examples. C.1. Pseudocode for Training Textual Dataset Synthesis Algorithm 1 Data Synthesis for the PointLiesOnLine Task 1: Input: data_info, points_set 2: Output: data 3: for points_set data_info do 4: for (A, B) permutations(points_set, 2) do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: all_rest_points [p for in points_set if not in [A, B]] for rest_points permutations(all_rest_points) do verb_agreement is if len(rest_points) == 1 else are rest_points [f\"{p}\" for in rest_points] rest_points sorted(rest_points) question What is the point lying on line + + + ? answer The point lying on line + + + + verb_agreement + + , .join(rest_points) gt .join(rest_points) data {question: question, answer: answer, gt: gt} end for end for 15: 16: end for Algorithm 2 Data Synthesis for the PointLiesOnCircle Task 1: Input: data_info 2: Output: data 3: point_set random.choice(list(data_info.items())) 4: center_point point_set[0] 5: target_points point_set[1] 6: target_points sorted(target_points) 7: question What are the point lying on circle + center_point + ? 8: answer The point lying on circle + center_point + are + , .join(target_points) 9: gt .join(target_points) 10: data {question: question, answer: answer, gt: gt} 23 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Algorithm 3 Data Synthesis for the ngleClassification Task 1: Input: data_info 2: Output: data 3: angle data_info 4: angle_options [f{angle[1][0]}{angle[1][1]}{angle[1][2]}, f{angle[1][2]}{angle[1][1]}{angle[1][0]}] 5: angle_letter random.choice(angle_options) 6: angle_class acute if angle[0] < 90 else obtuse 7: question Is angle + angle_letter + acute or obtuse? 8: answer Angle + angle_letter + is + angle_class 9: gt angle_class 10: data {question: question, answer: answer, gt: gt} Algorithm 4 Data Synthesis for the LineCompari son Task 1: Input: data_info 2: Output: data 3: names [data_info[0][1], data_info[1][1]] 4: lengths [data_info[0][0], data_info[1][0]] 5: if lengths[0] > lengths[1] then 6: 7: else 8: 9: end if 10: data [ 11: { question: longer_name, shorter_name names[0], names[1] longer_name, shorter_name names[1], names[0] Which line is longer, + longer_name + or + shorter_name + ?, The longer line is + longer_name, Which line is longer, + shorter_name + or + The longer line is + longer_name, answer: gt: longer_name }, { question: longer_name + ?, answer: gt: longer_name } 12: 13: 14: 15: 16: 17: ] Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Algorithm 5 Data Synthesis for the Par allel Task 1: Input: data_info 2: Output: data 3: points_set data_info 4: for line_points points_set do 5: for (A, B) permutations(line_points, 2) do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: all_rest_lines [p for in points_set if != line_points] gts [.join( f{p} for line in all_rest_lines for in line) ] rest_point_pairs [] for rest_line all_rest_lines do C, random.sample(rest_line, 2) rest_point_pairs.append([C, D]) end for all_possible_answer , .join( [f{C}{D} for C, in rest_point_pairs] ) verb_agreement is if len(rest_point_pairs) == 1 else are question What is the line parallel to line + + + ? answer ( According to the diagram, the line parallel to + + + verb_agreement + all_possible_answer ) gt , .join(gts) data { question: question, answer: answer, task: task, gt: gt } end for 28: 29: end for 25 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Algorithm 6 Data Synthesis for the Perpendicular Task 1: Input: data_info 2: Output: data 3: source_lines, target_lines data_info 4: all_possible_answer [] 5: gts target_lines 6: for target_line target_lines do 7: C, random.sample(target_line, 2) all_possible_answer.append(f{C}{D}) Randomly choose two points from each target line 13: 14: 15: 16: 17: 18: 19: 8: 9: end for 10: verb_agreement is if len(all_possible_answer) == 1 else are 11: for (A, B) permutations(source_line, 2) do 12: question What is the line perpendicular to line + + + ? answer ( According to the diagram, the line perpendicular to + + + verb_agreement + , .join(all_possible_answer ) gt , .join(gts) data { question: question, , answer: answer, ask: task, gt: gt } 20: 21: end for 26 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Algorithm 7 Data Synthesis for the Equal Task 1: Input: data_info 2: Output: data 3: statement, content data_info.split(;) 4: if statement == angles_value then 5: 6: 7: 8: 12: 13: 14: 18: 19: 20: 21: 22: 23: 27: 28: 29: 30: 31: 32: angle_letter, angle_measure content.split(=) angle_letter random.choice([angle_letter, angle_letter[::-1]]) question What is the measure of angle + angle_letter + as annotated? answer Angle + angle_letter + is annotated as + angle_measure gt angle_measure 9: 10: else if statement == segments_value then 11: segment_letter, segment_length content.split(=) segment_letter random.choice([segment_letter, segment_letter[::-1]]) question What is the length of line + segment_letter + as annotated? 15: 16: else if statement == angles then 17: answer Line + segment_letter + is annotated as + segment_length gt segment_length angle1, angle2 content.split(=) angle1 random.choice([angle1, angle1[::-1]]) angle2 random.choice([angle2, angle2[::-1]]) query_angle random.choice([angle1, angle2]) answer_angle angle2 if query_angle == angle1 else angle1 question What is the angle in the diagram that is equal to angle + query_angle answer Angle + query_angle + is equal to angle + answer_angle gt answer_angle 24: 25: else if statement == segments then 26: segment1, segment2 content.split(=) segment1 random.choice([segment1, segment1[::-1]]) segment2 random.choice([segment2, segment2[::-1]]) query_segment random.choice([segment1, segment2]) answer_segment segment2 if query_segment == segment1 else segment1 question What is the segment in the diagram that is equal to segment + query_segment answer Segment + query_segment + is equal to segment + answer_segment gt answer_segment 33: 34: end if 35: data { 36: 37: } question: question, answer: answer, task: task, gt: gt Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions C.2. Geometry Shapes Used for Euclid Training Geome try Shape Gener at ion Code PointLiesOnLine (stage 1) = triangle C; = midpoint (stage 1) = triangle C; = midpoint C; = circle C (stage 2) = triangle C; = midpoint B; = midpoint (stage 2) = triangle C; = midpoint B; = midpoint C; = circle C (stage 3) = triangle C; = midpoint C; = midpoint C; = intersection_ll E (stage 3) = triangle C; = midpoint C; = midpoint C; = intersection_ll E; = circle C PointLiesOnCircle (stage 1) = segment B; = on_circle (stage 1) = segment B; = on_circle B; = on_circle (stage 1) = segment B; = on_circle B; = on_circle B; = on_circle (stage 1) = segment B; = on_circle B; = on_circle B; = on_circle B; = on_circle (stage 1) = segment B; = on_circle B; = on_circle B; = on_circle B; = on_circle B; = on_circle (stage 2) = segment B; = on_circle B; = midpoint (stage 2) = segment B; = on_circle B; = midpoint (stage 2) = segment B; = on_circle B; = midpoint B; = on_circle (stage 2) = segment B; = on_circle B; = midpoint B; = on_circle B; = on_circle (stage 2) = segment B; = on_circle B; = midpoint B; = on_circle B; = on_circle B; = on_circle (stage 2) = segment B; = on_circle B; = midpoint B; = on_circle B; = on_circle B; = on_circle B; = on_circle (stage 3) = segment B; = on_circle B; = midpoint B; = midpoint (stage 3) = segment B; = on_circle B; = midpoint B; = midpoint C; = on_circle (stage 3) = segment B; = on_circle B; = midpoint B; = midpoint C; = on_circle B; = on_circle (stage 3) = segment B; = on_circle B; = midpoint B; = midpoint C; = on_circle B; = on_circle B; = on_circle (stage 3) = segment B; = on_circle B; = midpoint B; = midpoint C; = on_circle B; = on_circle B; = on_circle B; = on_circle B (stage 3) = segment B; = on_circle B; = midpoint B; = on_circle B; = on_circle B; = on_circle B; = on_circle B; = midpoint (stage 3) = segment B; = on_circle B; = midpoint B; = midpoint (stage 3) = segment B; = on_circle B; = midpoint B; = lc_tangent (stage 3) = segment B; = on_circle B; = midpoint B; = on_circle B; = on_circle B; = on_circle B; = lc_tangent AngleClassification (stage 1) = triangle (stage 3) = triangle C; = midpoint (stage 3) = triangle C; = midpoint C; = midpoint C; = intersection_ll B LengthComparison (stage 1) = triangle (stage 2) = triangle C; = midpoint (stage 3) = triangle C; = midpoint B; = midpoint Parallel (stage 1) = triangle C; = midpoint B; = midpoint (stage 1) = triangle C; = midpoint B; = midpoint (stage 1) = triangle C; = midpoint B; = midpoint (stage 2) = triangle C; = parallelogram D (stage 3) = triangle C; = midpoint B; = midpoint C; = midpoint Perpendicular (stage 1) = triangle C; = foot (stage 1) = r_triangle (stage 1) = segment B; = eq_triangle B; = eq_triangle B; = on_circle (stage 2) = triangle C; = foot C; = foot (stage 2) = r_triangle C; = foot (stage 2) = triangle C; = circle C; = foot B; = foot (stage 3) D = rectangle D; = intersection_ll D (stage 3) = triangle C; = incenter C; = foot C; = foot C; = foot (stage 3) = r_triangle C; = foot C; = foot (stage 3) = triangle C; = foot C; = foot B; = foot Equal (stage 1) = triangle C; = midpoint (stage 1) = triangle C; = midpoint B; = circle C (stage 1) = triangle C; = angle_bisector C, on_line (stage 2) = triangle C; = midpoint B; = midpoint (stage 2) = triangle C; = midpoint B; = midpoint C; = circle C (stage 2) = triangle C; = midpoint B; = midpoint (stage 3) = triangle C; = circle C; = on_circle C, angle_bisector Figure 17: Geometry Sh ape Gener ation Code for Eu lid ai ni ng 28 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 18: Examples of the geometry diagrams used to train Euclid, the diagrams are generated by our dataset engine. 29 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions D. Additional Experimental Results Figure 19: Result of our preliminary experiments, we use standard setting of MLLMs: an OpenAI-CLIP@224 as visual encoders (Radford et al., 2021), two-layer MLP as multimodal connector and Qwen-2.5-1.5B as language model. We find that the model can reach convergence in some of the easy tasks, while struggle to learn hard tasks. We also find mixed training is better than separate training, given the same amount of training data in each difficulty level. 30 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 20: The complete result of the effect of LLM size. The finding is similar with Fig. 3. 31 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 21: The complete result of the effect of different visual encoders. The finding is similar with Fig. 4. 32 Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions Figure 22: The complete result of the effect of tuning visual encoders. The finding is similar with Fig. 5."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "University of Southern California"
    ]
}