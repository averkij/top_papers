{
    "paper_title": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging",
    "authors": [
        "Gabriele Lozupone",
        "Alessandro Bria",
        "Francesco Fontanella",
        "Frederick J. A. Meijer",
        "Claudio De Stefano",
        "Henkjan Huisman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE"
        },
        {
            "title": "Start",
            "content": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging Case Study on Alzheimers Disease Gabriele Lozuponea,b,, Alessandro Briaa, Francesco Fontanellaa, Frederick J.A. Meijerc, Claudio De Stefanoa, and Henkjan Huismanb, for the Alzheimers Disease Neuroimaging Initiative aDepartment of Electrical and Information Engineering (DIEI), University of Cassino and Southern Lazio, Via G. Di Biasio 43, Cassino, 03043, FR, Italy bDiagnostic Image Analysis Group, Radboud University Medical Center, Geert Grooteplein 10, Nijmegen, 6500HB, Netherlands cDepartment of Medical Imaging, Radboud University Medical Center, Geert Grooteplein 10, Nijmegen, 6500HB, Netherlands 5 2 0 2 1 1 ] . [ 1 5 3 6 8 0 . 4 0 5 2 : r Abstract This study presents Latent Diffusion Autoencoder (LDAE), novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimers disease (AD) using brain MR from the ADNI database as case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for 6month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as promising framework for scalable medical imaging applications, with the potential to serve as foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE Keywords: Alzheimers disease, Diffusion Models, Foundation Models, Representation Learning 1. Introduction Recently, Diffusion Probabilistic Models (DPMs) have shown remarkable performance in image synthesis and dataset distribution modelling (Dhariwal and Nichol, 2021a, Nichol et al., 2021). The DPMs stable training process allowed to achieve state-of-the-art sample quality surpassing deep generative models like generative adversarial networks (GANs) (Goodfellow et al., 2014) and variational autoencoders (VAEs) (Kingma, 2013, Rezende et al., 2014). Most of the existing literature explores DPMs synthesis and editing capability, with limited focus on their representational capability (Abstreiter et al., 2021, Hudson et al., 2024, Xiang et al., 2023). One of the first works to propose diffusion-based approaches for Main contributor. Corresponding author: Email address: gabriele.lozupone@unicas.it (Gabriele Lozupone) 1*Data used in preparation of article were obtained from this the Alzheimers Disease Neuroimaging database (ADNI) Initiative (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in the analysis or writing of this report. complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/ uploads/how_to_apply/ADNI_Acknowledgement_List.pdf. representation learning was Preechakul et al. (2022). The authors introduced Diffusion Autoencoders (DAEs) to produce meaningful and decodable representation by means of an encoder to capture high-level semantics and diffusion-based decoder for the low-level stochastic variations. For the first time, Diffusion-based model surpassed GANs in feature disentanglement and enabled image attribute manipulation, preserving image quality and training stability. Traditional DPMs can act as encoder-decoder by converting an input image x0 into spatial latent variable xT by running the diffusion process backwards. The latent representation, however, lacks semantics and properties such as disentanglement, compactness, and the abilIn contrast, DAEs enity to perform semantic interpolation. sure that representations are both compact and disentangled, while also facilitating meaningful linear interpolation in the latent space. This makes the DAE approach strong candidate for structured semantic learning. Subsequently, some studies investigated different representation learning strategies based on encoder-decoder architectures like Zhang et al. (2022) that explored DAEs representation learning from pretrained DPMs (PDAE) and the more recent SODA architecture (Hudson et al., 2024) that introduces layer modulation to improve semantic attribute disentanglement further in the semantic space. ReprePreprint submitted to Medical Image Analysis April 14, sentation learning with diffusion models has received limited attention overall and even less in medical imaging. In 3D MRI brain domain diffusion-based approaches are mainly proposed for unconditional and conditional generation (Peng et al., 2023, Pinaya et al., 2022) and for disease progression prediction using pre-computed brain anatomical features and MRI sequences (Puglisi et al., 2024, Yoon et al., 2023). Therefore, unsupervised representation learning using diffusion models to learn general semantic representation that captures the complex 3D brain anatomical structure remain an unexplored direction. Contributions. This work introduces Latent Diffusion Autoencoders (LDAE) as an efficient framework for unsupervised and meaningful representation learning. The proposed approach builds upon principles established in Diffusion Autoencoders (DAE) (Preechakul et al., 2022), Pretrained Diffusion Autoencoders (PDAE) (Zhang et al., 2022), and Latent Diffusion Models (LDMs) (Rombach et al., 2022). Unlike conventional diffusion-based autoencoders that operate in the original image space, LDAE applies the diffusion process in compressed latent space. This formulation enhances computational efficiency and scalability, making diffusion-based representation learning tractable for 3D medical imaging. Method. Our approach consists of three key stages: (i) perceptual autoencoder (AE) that compresses high-dimensional MRI scans into lower-dimensional latent space; (ii) pretraining diffusion model on the compressed latent representations; and (iii) LDAE unsupervised representation learning with an encoder-decoder to fill the posterior mean gap, following the strategy introduced in PDAE (Zhang et al., 2022). To the best of our knowledge, this is the first latent diffusion autoencoder framework, demonstrating that meaningful semantic representations can be learned even when the diffusion process is performed in compressed space. Experiments and Results. To validate the effectiveness of the proposed LDAE, we conduct several experiments using longitudinal 3D brain MRI data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database. Our evaluation aims to validate two key hypotheses: (i) LDAE learns semantically rich representations that capture clinically relevant attributes related to Alzheimers disease (AD) and ageing consenting unsupervised representation learning via latent-space DAE, and (ii) LDAE offers high-quality generation and reconstruction improving computational efficiency over conventional DAE. For hypothesis (ii), we compare LDAE to voxel-space DAEs and demonstrate significant improvement in efficiency, achieving 20 speedup in inference time while surpassing reconstruction quality (SSIM: 0.962, MSE: 0.001, LPIPS: 0.076). To assess hypothesis (i), we perform linear-probe evaluations on the learned semantic embeddings. LDAE achieves good performance in downstream tasks such as AD diagnosis (ROCAUC: 89.48%, Accuracy: 83.65%) and age prediction (MAE: 4.16 years, RMSE: 5.23 years), indicating that the learned latent codes encode clinically meaningful information. We further validate semantic interpretability through latent attribute manipulation, which consent brain MR anatomical structures alteration related to disease and age progression. Finally, semantic and stochastic interpolation experiments show LDAEs capacity to predict missing intermediate scans in longitudinal series, achieving robust performance even at longer temporal gaps, supporting its ability to capture the temporal trajectory of neurodegeneration (e.g., SSIM: 0.97 for 6-month intervals and SSIM > 0.93 for 24-month gaps; SSIM is computed between the reconstructions done by the autoencoder to isolate the interpolation quality from the upper-bound limitation due to the compression model). The remainder of the paper is organized as follows: Section 2 introduces DPMs and background concepts utilized in the work, Section 3 describes in detail the multi-stage approach investigated in this manuscript. Section 4 presents the experimental settings, and Section 5 the generation, manipulation, interpolation and downstream tasks results. Discussions of the findings and conclusions are provided in Section 6. 2. Background DPMs are generative models that model target distribution by learning denoising process at varying noise levels (SohlDickstein et al., 2015). This concept is inspired by nonequilibrium thermodynamics, in which physical system starts from structured, low-entropy state that is gradually diffused or driven toward more disordered, high-entropy equilibrium state over time. In principle, the system can be steered back toward more ordered configuration, although this typically requires precise control and information about the underlying dynamics. In diffusion-based generative models, we begin with real data and then apply stochastic diffusion of noise stepby-step. Each step slightly corrupts the data by adding Gaussian noise to arrive at highly noisy, nearly featureless distribution that is mathematically close to pure Gaussian distribution N(0, I). 2.1. Denoising Diffusion Probabilistic Models Denoising Diffusion Probabilist Models (DDPMs) proposed in Ho et al. (2020) defined the diffusion process as Markov chain that starts from the data distribution q(x0) and sequentially corrupts in steps it to N(0, I) with Markov diffusion kernels q(xtxt1). The kernels are defined by fixed variance schedule {βt}T i=1 αi. This formulation allows to directly sample xt from x0 for arbitrary with q(xtx0) = N(xt; αt x0, (1 αt)I). The overall process can be expressed by: t=1 where αt = 1βt and αt = (cid:81)t q(xtxt1) = T(cid:89) q(x1:T x0) = (cid:16) xt; (cid:112) 1 βtxt1, βtI (cid:17) , q(xtxt1). (1) t=1 We are interested in learning the reverse process, i.e., the distribution p(xt1xt). As shown by Sohl-Dickstein et al. (2015), these probability functions are difficult to model unless the gap between 1 and is infinitesimally small (t ). In 2 Figure 1: Overview of the proposed 3D LDAE framework for unsupervised representation learning in brain medical imaging. The framework consists of three key components: (i) compression model (E and D), which encodes high-dimensional MRI brain scans (x0) into lower-dimensional latent representation (z0), facilitating efficient processing; (ii) Latent Diffusion Model (LDM), trained to learn the distribution of the compressed representations through diffusion process, progressively transforming z0 into zT and vice versa via U-Net-based denoising network (UNetθ); and (iii) the semantic encoder-decoder model (Gψ and Encϕ), which learns meaningful latent representation (ysem) from the input scan and utilizes it to guide the reverse diffusion process via gradient estimator (Gψ). This approach enables structured semantic learning, facilitating interpretable image synthesis, counterfactual generation, and disease-specific attribute disentanglement. practice, sufficiently large = 1000 is chosen and in such case, good approximation pθ(xt1xt) can be modeled as N(µθ(xt, t), σt) in which parameters θ can be learned with an UNet (Ho et al., 2020). The model is trained with the loss function: Lϵ = ϵθ(xt, t) ϵ, where ϵ is the noise added to x0 to obtain xt. This is simplified formulation of the variational lower bound on the marginal log-likelihood commonly used in DDPMs training (Dhariwal and Nichol, 2021b, Nichol and Dhariwal, 2021, Preechakul et al., 2022, Song et al., 2020a). 2.2. Denoising Diffusion Implicit Models Denoising Diffusion Implicit Models (DDIMs) proposed in (Song et al., 2020a) introduce non-Markovian forward process that, unlike standard DDPMs, offers increased flexibility, enabling faster inference with fewer steps. In particular, the latent variable xt1 can be derived from xt by leveraging ϵθ from pretrained DDPM as follows: (cid:32) xt 1 αtϵθt (xt, t) (cid:33) xt1 = + αt1 (cid:32) (cid:113) αt (cid:33) + (2) 1 αt1 σ2 ) ϵθt (xt, t) + σtϵt. where ϵt N(0, I), and σt determines the degree of stochasticity in the forward process. By choosing σt = 0, the generative process will be fully deterministic, which is named implicit in DDIMs. The DDIM posterior distribution becomes: q(xt1xt, x0) = (cid:32) αt1x0 + (cid:112) 1 αt1 xt αtx0 1 αt (cid:33) , 0 . (3) 3 while maintaining the DDPM marginal distribution: q(xtx0) = N(xt; αtx0, (1 αt)I). (4) Since σt = 0 implies deterministic generative process, the DDIM can encode x0 into decodable noise map xt. In Preechakul et al. (2022), the authors show that this process yields an accurate reconstruction but xt lacks high-level semantics, not consenting semantically-smooth interpolation between samples. 2.3. Diffusion Autoencoders In the objective of obtaining semantically meaningful latent code, the authors of DAE (Preechakul et al., 2022) designed conditional DDIM image decoder that approximate p(xt1xt, ysem) in which ysem is non-spatial vector of dimension = 512 learned from semantic encoder that maps x0 into ysem = Encϕ(x0). The encoder and the DDIM decoder are trained in conjunction by optimizing: Lϵ = T(cid:88) t= Ex0,ϵt (cid:104) ϵθ(xt, t, ysem) ϵt2 2 (cid:105) . (5) with respect to θ and ϕ by conditioning an UNet with the Encϕ output using adaptive group-wise normalization (AdaGN) layers as proposed in Dhariwal and Nichol (2021a). By training the two models simultaneously, the encoder Encϕ is forced to learn as much information as possible to help the DDIM in the denoising process. The authors of PDAE (Zhang et al., 2022) clarified this behaviour by showing that there exists gap between the posterior mean predicted by an unconditional DPMs (µθ(xt, t)) and the true one ( µt(xt, x0)). The posterior mean gap is caused by an information loss that, in theory, can be recovered by conditioning on some that contain all information about x0. By letting = ysem learnable vector produced by an encoder, it will be forced to learn as much information as possible to fill the gap and consequently as much information as possible from x0. Following these principles, it is possible to train DAE from pretrained DPMs, achieving better training efficiency and stability (Zhang et al., 2022). 2.4. Classifier-Guided Sampling Method The classifier-guided method allows to condition the generation of DDPM towards some information, e.g. classes or prompts (Sohl-Dickstein et al., 2015, Song et al., 2020b). It consists of training classifier pψ(yxt) on noisy data. The gradient xt log pψ(yxt) can then be leveraged to guide the generation towards samples correlated to information in y. The conditional reverse process can be approximated using Gaussian distribution, resembling the unconditional case, but with an adjusted mean: pθ,ψ(xt1xt, y) (cid:16) xt1; µθ(xt, t) + Σθ(xt, t) xt log pψ(yxt), Σθ(xt, t) (cid:17) . (6) For DDIMs, score-based conditioning trick (Song et al., 2020a, Song and Ermon, 2019) can be applied to define new function approximator for conditional sampling: ˆϵθ(xt, t) = ϵθ(xt, t) (cid:112) 1 αt xt log pψ(yxt). (7) Based on this concept the authors of Zhang et al. (2022) employed gradient estimator Gψ(xt, ysem, t) to simulate xt log p(ysemxt) that assemble conditional DPM as decoder. The decoder is conditioned on the semantic encoder output that forces it to learn more information to improve the generation of frozen and unconditional pretrained DDPM. More details will be discussed in Section 3.3 since we used the same concepts but on compressed data representation as discussed in Section 3.1 and Section 3.2. 3. Methods The overall scheme of the proposed framework is shown in Fig.1. This section provides detailed description of the training stages and the network architecture choices. Since the framework is designed to learn two different types of latent representations from the original data, we will refer throughout the remainder of the manuscript to the latent space generated by the compression model as the compressed space (Section 3.1) and to the latent space produced by the semantic encoder as the semantic space (Section 3.3) 3.1. Compression Model To make the training tractable, given the 3D nature of the input data, we followed the principles of LDMs as already done by Pinaya et al. (2022), Puglisi et al. (2024). As in the original LDMs (Rombach et al., 2022), we trained an AE based on (Esser et al., 2021) as our perceptual compression model, an essential step to scale to high-resolution images. This model consists of an AE trained by combination of perceptual loss (Zhang et al., 2018) and patch-based adversarial objective (Dosovitskiy and Brox, 2016, Yu et al., 2021). These losses guarantee that reconstructions remain within the image manifold by enforcing local realism while also preventing the blurriness that arises from relying solely on pixel-space losses like L2 or L1 objectives. To avoid arbitrarily high variance in the compressed space, we used the Kullback-Leibler (KL) regularization, which imposes slight KL penalty that encourages the compressed space to stay close to normal distribution, similar to Variational Autoencoders (VAEs) (Kingma, 2013, Rezende et al., 2014). Hence, given 3D brain scan x0 RHWD1, compression encoder encodes x0 into compressed representation z0 = E(x0) Rhwdc in which = H/h = W/w = D/d is the downsampling factor and the decoder reconstructs the image from the compression: x0 = D(z0) = D(E(x0)). 3.2. Latent Diffusion Model - Pretraining The trained perceptual compression model provides compact representation of the input scan by factor along each dimension, leading to volume with 3 times fewer voxels. This model discards imperceptible high-frequency details while retaining crucial semantics embedded in low-frequency structures, making it an efficient yet expressive space for generative modelling. However, to ensure stability and improve the effectiveness of diffusion training, we normalize the learned latents as suggested in Appendix G. of Rombach et al. (2022). Specifically, after encoding with E, we rescale the latent representation to have unit variance across the first batch, ensuring standardized latent space that facilitates diffusion learning. In this stage, we train DDPM without conditioning on the input image, resulting in time-conditioned U-Net operating in the compressed space. The reweighted lower bound is: LLDM = T(cid:88) t= EE(x),ϵt (cid:104) ϵθ(zt, t) ϵt2 2 (cid:105) in which ϵt Rhwdc N(0, I), zt = 1 αtϵt and U(0, ) during training with = 1000. The reverse DDIM process using Eq.2 will sample new ˆz0 from the compressed latent distribution p(z) that can be decoded to image space trough D. αtz0 + 3.3. Representation Learning - LDAE Now that we have pretrained the LDM we can employ semantic encoder that maps x0 into ysem = Encϕ(x0) and gradient estimator for the compressed space guidance simulation 4 Figure 2: Architecture of the Semantic Encoder used in the LDAE framework. The input 3D brain MRI scan x0 RHWD is sliced along the axial plane into sequence of 2D slices, each processed independently through shared 2D CNN backbone (e.g., ConvNeXt-Small) to extract slice-level embeddings ei Rd. These embeddings are then aggregated via two-stage attention mechanism: SoftAttention computes global summary vector as weighted mean over the sequence, and CrossAttention simulates self-attention by querying with the original embeddings E, yielding final global non-spatial semantic vector ysem Rd used to guide the reverse diffusion process. Gψ(zt, ysem, t). Similarly to Zhang et al. (2022) the gradient estimator Gψ is used to simulate zt log p(ysemzt) and the conditional decoder will approximate: pθ,ψ(zt1zt, ysem) = (cid:16) zt1; µθ(zt, t)+ (cid:17) +Σθ(zt, t) Gψ(zt, ysem, t), Σθ(zt, t) . (8) where SNR(t) = αt and γ is hyperparameter controlling the 1 αt balance between early-stage and late-stage weighting. Following their recommendation, we set γ = 0.1 as it down-weights the loss for both very low and very high diffusion steps while encouraging the model to focus on learning richer representations in intermediate stages. LDAE is trained as regular DPM by optimizing the following variational lower bound derived objective: 3.4. Encoder Design LLDAE(ψ, ϕ) = Ex0,t,ϵ αt + 1 αt βt (cid:34) λt (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ϵ ϵθ(zt, t)+ Σθ(zt, t) Gψ(zt, Encϕ(x0), t) (9) 2(cid:35) . (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Note that: (i) the pretrained LDM is frozen during this phase, so Eq. 9 dont optimize parameters θ but only ψ and ϕ; and (ii) the gradient estimator operates on the compressed latent distribution (p(z)) while the encoder on the original image (x0). This adopted optimization, similar to PDAE, forces the predicted mean shift Σθ(zt, t) Gψ(zt, Encϕ(x0), t) to fill the latent posterior mean gap µt(zt, z0) µθ(zt, t) by learning as much information as possible ysem from x0. Following Zhang et al. (2022), we adopt their proposed weighting scheme of the diffusion loss (Eq. 9), which has been shown to improve training stability and representation learning. Instead of using constant weighting factor (λt = 1), they suggest signal-to-noise ratio (SNR)-based scheme: (cid:32) λt = 1 1 + SNR(t) (cid:33)1γ (cid:32) SNR(t) 1 + SNR(t) (cid:33)γ , (10) 5 To enable efficient training and convergence in unsupervised representation learning, we adopted an encoder design inspired by our previous diagnostic framework proposed in Lozupone et al. (2024). In that work, we observed that leveraging 2D CNN architectures improved convergence speed and performance when training with limited data. Our encoder leverages 2D CNN for feature extraction while back-propagating the error signal at the volume level. Specifically, we treat axial slices as sequential input of 2D images, which are embedded by 2D CNN. Formally, given sequence of axial slices = x1, x2, . . . , xL, the embedding function fcnn() maps each slice to feature representation: ei = fcnn(xi), ei Rd, = 1, . . . , where is the embedding dimension. Since self-attention transforms sequence e1, . . . , eL into another sequence and we want non-spatial compact vector ysem, we approximate this mechanism through combination of soft-attention and crossattention mechanisms. Soft-attention computes weighted mean representation of the sequence, where the weights indicate the relative importance of each embedding: = SoftAttention(E), = [e1, e2, . . . , eL] RLd This yields general representation R1d, computed as: 3.6. Gradient Estimator as Stochastic Encoder and CondiQ = L(cid:88) i=1 αiei, αi = exp(wi) j=1 exp(w j) (cid:80)L where wi are learnable attention weights. Cross-attention operation is applied to approximate self-attention one by using the global representation obtained from soft-attention as the query. Instead of directly computing self-attention on the full sequence, we compute: MultiHeadAttention(Q, K, V) where R1d is the global summary, and both the key and value matrices are the original sequence = = RLd. The resulting single-head attention matrix has the shape: Attention(Q, K, V) = softmax (cid:33) (cid:32) QKT R1d This formulation ensures that the final representation ysem encapsulates the most relevant features from the sequence while maintaining global dependencies, and allows yielding state-ofthe-art 2D CNN architectures as base embedding networks, with the additional flexibility to utilize pre-trained weights. 3.5. Gradient-Estimator Design The gradient estimator Gψ(zt, ysem, t) is implemented as modified U-Net architecture as proposed in (Zhang et al., 2022). It shares the same downsampling path and time embedding modules as the pre-trained LDM introduced in Section 3.2, enabling reuse of the learned representations from the unconditional model. However, to enable conditional guidance based on the semantic encoding ysem, we add second, dedicated upsampling path. As result, Gψ consists of shared encoder and two decoder branches: 1. The first branch (original) corresponds to the unconditional denoiser ϵθ trained during LDM pretraining. 2. The second branch is newly initialized, including its middle blocks, upsampling path, and output layers, and uses the same skip connections of the frozen encoder. This branch is trained from scratch using Eq. 9. Following the conditioning strategy proposed in (Zhang et al., 2022), we use AdaGN (Dhariwal and Nichol, 2021b) to inject both timestep and semantic condition ysem into the new decoder. AdaGN applies learned affine transformation to the normalized feature maps: AdaGN(h, t, ysem) = ys sem (cid:0)ts GroupNorm(h) + tb (cid:1) + yb sem, sem, yb where [ys sem] and [ts, tb] are obtained via linear projections of ysem and t, respectively. This design allows the model to retain the low-level representations learned during the unconditional pretraining, while enabling the new branch to learn how to guide the denoising process using the high-level semantic codes. 6 tional Decoder Beyond approximating the denoising function ϵθ, the gradient estimator Gψ(zt, ysem, t) plays dual role in our LDAE framework: it can act as both stochastic encoder and conditional decoder. This dual capability was demonstrated in the original DAE framework (Preechakul et al., 2022) and is essential to enable both efficient reconstruction and semantic-level control. Stochastic Encoder via DDIM Inversion. To encode compressed sample z0 = E(x0) into its stochastic latent code zT , we employ deterministic DDIM-like backward using the semantic code ysem = Encϕ(x0) and the gradient estimator Gψ: zt+1 = αt+1 fθ(zt, t, ysem) + (cid:112) 1 αt+1 ϵθ(zt, t, ysem), (11) where fθ(zt, t, ysem) is the DDIM predictor and ϵθ is approximated via the conditional gradient estimator Gψ. This process encodes the residual information not captured by ysem into zT , enabling near-exact reconstructions. Conditional Decoder for Reconstruction and Generation. Conversely, the same gradient estimator can decode noisy latent code zT into clean latent z0 via the conditional DDIM reverse (generative) process. This decoding can be performed in two modes: 1. Reconstruction: starting from the inferred zT obtained via inversion and ysem, enabling near-exact reconstructions. 2. Generation: starting from pure noise zT N(0, I) and guiding the process with ysem, producing samples that shares semantic attributes with x0. This dual capability allows the model to interpolate in latent space, perform counterfactual generation, and reconstruct missing intermediate timepoints, as explored in our semantic evaluation experiments (see Section 4). 3.7. Controlling Semantic Attributes via Latent Linear Directions Once semantically rich encoder is trained (Sections 3.3 3.4), we can manipulate the latent representation ysem Rd to alter specific features in the reconstructed 3D scan as proposed in Preechakul et al. (2022). Concretely, suppose we train linear classifier ℓ(ysem) = ysem + to distinguish, for example, AD from CN participants. The set of points ysem satisfying wysem + = 0 forms hyperplane in Rd. By definition, the weight vector is orthogonal to this hyperplane, meaning that w(ysem,2 ysem,1) = 0 for any two points ysem,1 and ysem,2 on the decision boundary. Hence, defines principal direction in ysem-space along which movement most strongly affects the classifiers output (ℓ(ysem)). Intuitively, translating ysem in the direction of (i.e., ysem ysem+αw) will add the corresponding AD-related features to the reconstructed scan, whereas moving in the opposite direction (α < 0) will subtract them. Empirically, this directional manipulation can disentangle specific symptoms or morphological traits in the semantic representation, thereby giving control over generated 3D reconstructions. 3.8. Semantically meaningful interpolation To assess the semantic smoothness of the learned semantic space, we performed interpolation leveraging both the semantic and stochastic codes. This enables qualitative and quantitative evaluation of how well the latent space captures continuous and clinically meaningful transformations across brain MRI scans. Following the DAE framework Preechakul et al. (2022), we perform linear interpolation in the semantic space and spherical linear interpolation in the stochastic one. Given two input scans x1 and x2, their corresponding semantic and stochastic representations are denoted as y1 , respectively. The interpolated latent representations at interpolation factor [0, 1] are computed as: and y2 sem, sem, z2 LERP(y1 sem, y2 SLERP(z1 sem; t) = (1 t) y1 ; t) = sin((1 t)θ) sem , z2 sin(θ) + y2 sem z1 + sin(tθ) sin(θ) z2 where the angle θ between z1 and z2 is given by: , z2 z1 z2 z1 θ = arccos (12) (13) (14) The interpolated pair (y(t) construction procedure described in Section 3.6. sem, z(t) ) is then decoded using the re4. Experiments 4.1. Dataset and Preprocessing We conduct our experiments on the ADNI database containing longitudinal 3D brain volumes of subjects across various stages of cognitive decline, ranging from cognitively normal (CN) to mild cognitive impairment (MCI) and Alzheimers disease (AD). The subjects demographic information of the dataset used is reported in Table 1, which provides an essential context for our analysis and findings. BIDS Conversion. We first converted the raw ADNI data into the Brain Imaging Data Structure (BIDS) format (Gorgolewski et al., 2016). This conversion enables structured and standardized neuroimaging data handling and seamless integration with Python-based neuroimaging tools such as PyBIDS (Yarkoni et al., 2019). We performed this conversion using the Clinica platform (Routier et al., 2021, Samper-Gonzalez et al., 2018), an open-source software framework specifically designed for reproducible clinical neuroscience research. During this phase, the Clinica ADNI-to-BIDS converter automatically applies quality control check, selecting the preferred scan for each visit and discarding scans that fail predefined quality checks. 7 Table 1: Population statistics across CN, AD, and MCI groups. Including age, mini-mental state examination (MMSE) and global clinical dementia rating (CDR) scores Subjects Samples Age MMSE CDR CN AD 965 748 MCI 1193 2064 4603 75.43 6.83 29.06 1.20 0.03 0.17 76.36 7. 21.79 4.44 0.95 0.52 74.72 7.74 27.51 2.24 0.46 0.22 Preprocessing Pipeline. The preprocessed dataset was prepared using standard neuroimaging pipeline composed of the following sequential steps: 1. Bias Field Correction: Intensity non-uniformities were corrected using the N4ITK algorithm (Tustison et al., 2010). 2. Skull Stripping: Brain tissue was extracted using the deep learning-based brain extraction models proposed in Cullen and Avants (2018) from ANTsPyNet package. 3. Affine Registration: Each brain-extracted volume was affinely aligned to the MNI152 ICBM 2009c nonlinear symmetric template (Fonov et al., 2011, 2009) using the SyN algorithm from the ANTs toolkit (Avants et al., 2008, 2014). Inference-Time Normalization. We employed the MONAI framework (Cardoso et al., 2022) for batch preprocessing at inference time. The following transformations were applied: 1. Voxel Spacing Normalization: All images were resampled to uniform voxel spacing of 1.5mm isotropic using B-spline interpolation. 2. Spatial Resizing: Volumes were resized using cropping or padding to target shape (e.g., 128 160 128). 3. Intensity Normalization: Intensities were rescaled to the [0, 1] range using min-max normalization. 4.2. Experimental Setup AutoencoderKL Training. To enable efficient diffusion training in compressed representation space, we fine-tuned 3D perceptual autoencoder to compress full-resolution MRI scans into compact latent representation. We used the AutoencoderKL implementation from the MONAI Generative (Pinaya et al., 2023) framework. The model was initialized from checkpoint pre-trained on the UK Biobank dataset (Sudlow et al., 2015) and fine-tuned on volumes resampled to resolution of 128 160 128. The training setup is summarized in Table 2. Latent Diffusion Model Pretraining. Following the compression model training stage, we pretrained DDPM directly in the compressed latent space. This stage models the distribution of latent representations z0 R3162016, significantly reducing the computational burden compared to operating in voxel space. The training follows the standard DDPM formulation with linear noise schedule and uses 3D UNet architecture trained to predict the added Gaussian noise in the latent space. Figure 3: Qualitative reconstructions from AutoencoderKL. Top row: original scan slices. Middle row: reconstructed outputs from compressed latent codes. Bottom row: reconstruction error. The reconstructions preserve global and local anatomical features despite the 170 compression. Figure 4: Reconstruction from semantic code and stochastic latent. Reconstruction results for two representative subjects from the test set: CN subject (left block) and an AD subject (right block). First column: original brain MR scan. Second column: reconstruction obtained using both the semantic embedding ysem = Encϕ(x0) and the stochastic latent zT obtained via DDIM inversion of the encoded compressed latent z0 = E(x0). Columns 35: reconstructions obtained by keeping ysem fixed and sampling different z(i) N(0, I). Despite the stochastic variation, the reconstructions retain global anatomical and disease-relevant structure, indicating that ysem captures high-level semantics while zT encodes low-level variability. In the AD subject, expected pathological traits (e.g., ventricular enlargement) are consistently preserved, whereas in the CN subject, normal cortical volume and structure remain stable across samples. The Exponential Moving Average (EMA) of model weights update was applied during training for improved stability and sample quality. full list of architectural and training parameters is provided in Table 3. Representation Learning via 2.5D Semantic Encoder. To guide the reverse diffusion process with clinically meaningful features, we trained semantic encoder network to extract compact latent code ysem from the original 3D brain volume x0. This latent code serves as conditioning vector during denoising via the gradient estimator Gψ(zt, ysem, t), as detailed in Section 3.3. The encoder follows 2.5D strategy, where axial slices are independently processed using 2D CNN backbone, and then aggregated using combination of soft attention and cross-attention mechanisms to produce global, nonspatial embedding. We employed pre-trained ConvNeXtSmall model as the slice-level feature extractor, adapted to accept single-channel inputs (grayscale MRI slices), with embedding dimension = 768 and input sequence length = 128. The grayscale adaptation was done by summing the pre-trained convolutional filters of the backbones first layer. The semantic encoder and gradient estimator were optimized jointly to minimize the weighted diffusion loss described in Equation 9, using the SNR-based weighting scheme proposed by Zhang et al. (Zhang et al., 2022). The gradient estimator Gψ was implemented as modified U-Net architecture, reusing the encoder and time embedding modules from the pre-trained unconditional LDM  (Table 3)  . new decoder branch, composed of middle, upsampling and output blocks, was initialized with the same configuration of the LDM. full list of architecture and training parameters for this stage is provided in Table 4. Data Splits and Model Selection Strategy. We adopted consistent 90-10 train-test split at the subject level for all training stages. Within the 90% training partition, random 1% subset was reserved for validation purposes. This minimal validation split was chosen to preserve maximal training data availabil8 Table 2: AutoencoderKL architecture and training configuration. Table 4: Representation Learning Stage: Configuration of the Semantic Encoder and Gradient Estimator Parameter Input Resolution Latent Size Channels Residual Blocks per Level Normalization KL Weight Adversarial Weight Perceptual Weight Perceptual Net Discriminator Layers / Channels Optimizer Learning Rate (Generator) Learning Rate (Discriminator) Batch Size (Effective) Training Time Pretrained Init Hardware 1 128 160 128 3 16 20 16 [64, 128, 128, 128] 2 Group Normalization 1 107 0.025 0.001 SqueezeNet (fake-3D ratio: 0.5) 3 / 64 Adam 5 105 1 104 8 (1 GPUs 1, grad. accum. 8) 20 epochs / 3 days UK Biobank (Pinaya et al., 2022) 1 NVIDIA A100 80GB Table 3: Latent Diffusion Model (LDM) pretraining configuration Parameter Input Latent Shape Channels Residual Blocks per Level Attention Resolutions (factors) Dropout Timesteps Beta Schedule Optimizer Learning Rate EMA Decay Batch Size (Effective) Training Time Hardware 3 16 20 16 [256, 512, 768] 2 [2, 4] 0.1 1,000 Linear: βt [104, 2 102] Adam 2.5 105 0.999 128 (2 GPUs 64, grad. accum. = 2) 500 epochs / 18 hours 2 NVIDIA A100 SXM4 40GB ity, as the main objective was image reconstruction rather than classification. Validation was conducted using image-level reconstruction metrics. For the compression model and the representation learning stage, model selection was based on the best SSIM score observed on the validation set. In the representation learning stage, we specifically monitored the SSIM between the original image and the reconstruction generated using the semantic embedding ysem and pure noise as stochastic input. This metric served as proxy for the quality and completeness of the semantic information extracted by the encoder. No validation-based checkpointing was used in the latent diffusion model pretraining stage. Instead, the model parameters at the final epoch were retained for downstream use, as unconditional diffusion sampling was the primary goal, and model weights updates was performed trough an EMA strategy. Linear Probe Experimental Setup. To evaluate the semantic quality of the learned representations, we performed linear probe experiments on two downstream tasks: AD diagnosis and age prediction. For this purpose, we trained linear models on top of the fixed semantic embeddings extracted from the trained LDAE encoder. Given an input 3D brain MRI scan x0, we first 9 Semantic Encoder Architecture Slicing plane Backbone Pretrained Init Input Modality Input Sequence Length Multihead Attention Output Representation Gradient Estimator Gψ 2.5D Attention-based Encoder Axial ConvNeXt-Small ImageNet 1 128 128 160 128 slices 8 heads, attention dropout 0.1 Global, non-spatial vector ysem Architecture Shared Components New Components Condition Injection Weighting Scheme Modified U-Net (same configuration of LDM) Downsampling blocks from pretrained LDM Middle, upsampling, and output blocks AdaGN SNR based, γ = 0.1 Training Configuration Optimizer Learning Rate EMA Decay Effective Batch Size Training Duration Hardware Adam 2.5 105 0.999 16 (2 GPUs 2 grad. accum. 4) 200 epochs / 2 days 2 NVIDIA A100 SXM4 40GB projected each image into the semantic space using the encoder Encϕ(x0), obtaining semantic vector ysem R768. These embeddings were computed for all samples in the dataset. The dataset was initially split at the subject level into 90% training and 10% test sets. From the 90% training portion, we held out 1% for validation and used the remaining 89% for training. We further split this 89% into 70% training and 30% validation subsets for the linear probe experiments. The original 10% test set was kept as fixed benchmark for final evaluation. For the AD vs. CN classification task, we trained linear classifier consisting of single fully connected layer with binary crossentropy loss. We trained linear regressor using mean squared error (MSE) loss for the age prediction task. The classifier was trained using the Adam optimizer for 200 epochs with 1103 learning rate. The regressor was trained using stochastic gradient descent (SGD) for 1000 epochs with the same learning rate. Note that all linear models were trained on the fixed semantic vectors without finetuning the encoder. These linear probe experiments aim to quantify the extent to which the learned semantic codes ysem encode clinically meaningful and linearly separable features relevant to disease classification and age estimation. Interpolation Experiment for Missing Scan Generation. We conducted semantic and stochastic interpolation experiment on subset of the held-out test set to evaluate the models ability to generate plausible missing follow-up scans. This experiment simulates longitudinal scan prediction by reconstructing intermediate brain scans from subjects early and late visits. For each subject with at least three visits in the test set, we selected all valid (start, target, end) triplets such that the target timepoint lies temporally between the start and end. For Table 5: Comparison of voxel-space DAE and latent-space LDAE in terms of input resolution, model size, training duration, and reconstruction efficiency. Model Input Resolution Model Parameters Training Duration Hardware Inference Time (T = 100) SSIM () (T = 100) LPIPS () (T = 100) DAE LDAE 112 128 112 128 160 130M 920M 140 epochs / 1 week 500+200 epochs / 3 days 2 A100 40GB 2 A100 40GB 120 seconds 6 seconds 0.892 0.962 0.038 0. Table 6: Comparison of SSIM, LPIPS, and MSE for various models at different sampling steps T. Since training full-resolution 3D Diffusion Autoencoder (DAE) was computationally prohibitive, we resized the scans to 1x112128112. However, we trained the AutoencoderKL at higher resolution 1x128160128, allowing it to encode images into 3x162016 latent space. This enabled efficient training of LDDIM and LDAE, which operate in the compressed latent space but rely on the autoencoder for final image reconstruction. As result, AutoencoderKL reconstruction quality acts as bottleneck for LDDIM and LDAE performance. Model Latent dim SSIM () LPIPS () T=10 T=20 T=50 T=100 T=10 T=20 T=50 T=100 T=10 T=20 T=50 T=100 MSE () AutoencoderKL (@14M) 5,120 0.962 0.075 0.001 LDDIM (@486M) a) Encoded xT LDAE (@920M) a) No encoded xT , from ysem b) Encoded xT DAE (@130M) a) No encoded xT , from ysem b) Encoded xT 5,120 0.959 0.962 0. 0.962 0.077 0.075 0.075 0.075 0. 0.001 0.001 0.001 768 5,888 0.872 0.953 0.871 0. 0.870 0.962 0.869 0.962 0.156 0.081 0.155 0.077 0.155 0.076 0.155 0. 0.005 0.001 0.005 0.001 0.005 0.001 0.005 0.001 768 1,605,632 0.272 0. 0.287 0.397 0.281 0.684 0.283 0.892 0.460 0.433 0.256 0.132 0.182 0. 0.170 0.030 0.017 0.007 0.016 0.002 0.016 0.001 0.016 0.001 (cid:17) (cid:16)n 3 sem, z(t) each triplet, we computed the interpolation factor α based on the temporal offset of the target with respect to the interval defined by start and end. We then interpolated linearly between the semantic codes (ysem) and spherically between the stochastic codes (zT ) to generate the latent pair (y(t) ) corresponding to the target intermediate timepoint t. This experiment was conducted on subset of 30 subjects from the test set, yielding approximately 1400 valid triplet configurations. The number of possible triplets for subject with sessions grows approximately as , under the constraint that the target scan lies strictly between the start and end. This implies that even modest increases in subjects considered for the evaluation will increase significantly the number of generations to compute. The generated latent representation was decoded using the DDIM-based reverse process described in Section 3.8. The resulting image ˆx0 was compared to the ground truth scan xtarget using SSIM and MSE. We report average SSIM and MSE metrics stratified by the temporal gap between start and end scans (time gap), the minimum distance of the predicted target from the endpoints (prediction gap), and the relative position of the target within the interval (normalized to [0, 1]). This allows us to assess how interpolation accuracy varies with temporal context. 0 5. Results In this section, we present experimental results evaluating the proposed LDAE framework. The results are structured to progressively validate the components of the pipeline and support the two main hypotheses introduced in Section 1. 5.1. AutoencoderKL: Perceptual Compression and Reconstruction Quality We begin by evaluating the perceptual compression model based on AutoencoderKL. This autoencoder compresses each 10 3D brain MR from 1 128 160 128 into latent representation of size 3 16 20 16, reducing the volume by factor of approximately 170. Despite this compression, the model achieves high-fidelity reconstructions, with an SSIM of 0.962 and an MSE of 0.001 on the external test set (see Table 6). Qualitative examples are shown in Figure 3. These results highlight the models capacity to retain high-frequency anatomical details in the compressed latent space. It is important to note that this reconstruction accuracy establishes an upper bound for the performance of the subsequent latent diffusion models (LDDIM and LDAE), as the final decoded output always passes through the AutoencoderKL decoder. 5.2. Reconstruction Quality As shown in Figure 4 and Table 6, the proposed LDAE when using both the semantic encoder and gradient estimator Gψ as described in Section 3.6achieves reconstruction quality on par with AutoencoderKL and LDDIM. For instance, at = 50, LDAE obtains SSIM = 0.962, LPIPS = 0.076, and MSE = 0.001, matching the performance of both the LDDIM and the upper bound imposed by the AutoencoderKL. Since LDAE operates in the compressed latent space, it allows efficient model parameters scalability. Specifically, the full LDAE model has approximately 920M parameters, compared to 130M of the voxel-space DAE. Despite this, LDAE remains more efficient (see Table 5). LDAE was trained at higher resolution (128 160 128) for 200 epochs over 2 days on 2 A100SXM4-40GB GPUs. In contrast, the DAE required 1 week of training on the same hardware and had to operate on downsampled volumes (112128112) due to memory constraints, limiting its capacity and overall performance. At inference time, the efficiency gap is even more pronounced: LDAE requires approximately 6 seconds per reconstruction at = 100 steps on an A100 GPU, while the full-resolution DAE takes approximately 2 minute per scan due to the lack of compression and increased I/O overhead. These results validate our second hypothesis: LDAE enables high-fidelity 3D brain MRI reconstruction with full semantic controllability and significantly improved computational efficiency, both during training and inference. 5.3. Semantic Guidance Enables Reconstruction from Pure Noise To assess the semantic richness of the learned representation, we perform reconstructions using only the semantic code ysem = Encϕ(x0) and randomly sampled stochastic code zT N(0, I). This setup evaluates whether the semantic code alone can guide the reverse diffusion process to generate anatomically plausible MR that contains structure information of x0. Figure 4 shows qualitative reconstructions for two randomly selected subjects: CN individual (left) and an AD patient (right). For both cases, reconstructions are generated by fixing the semantic code ysem = Encϕ(x0) and sampling multiple stochastic codes zT . Across samples, the reconstructions preserve global brain morphology, indicating that the semantic code captures the high-level anatomical and pathological attributes of the subject, while the stochastic component contributes only to low-level variability. In the CN subject, cortical thickness and brain volume are preserved across stochastic samples. In contrast, the AD subject reconstructions consistently display expected atrophic patterns, such as enlarged ventricles and reduced hippocampal volume, despite variation in image details. These observations suggest that ysem encodes disease-relevant structural information. This behavior is consistent with prior findings in DAE Preechakul et al. (2022), where the semantic representation governs identity and structure, and the stochastic latent controls fine-grained appearance. 5.4. Linear Probe Evaluation on Alzheimers Disease Classification and Age Prediction Figure 5: LDA projection of semantic representations (ysem) extracted from the LDAE encoder. Each point corresponds to 3D brain scan colored by diagnostic class (AD or CN). The clear separation suggests that the learned semantic space captures clinically meaningful features relevant to Alzheimers disease. 11 Figure 6: Progressive manipulation of an AD subject toward the CN class. The manipulation strength α ranges from 0.0 (original reconstruction) to 5.0. Structural changesespecially hippocampal recovery and ventricle shrinkagebecome more evident with larger α. To quantitatively assess the semantic quality of the representations learned by our LDAE framework, we conducted linear probe experiments on two downstream tasks: (i) AD vs. CN classification, and (ii) age prediction. For this purpose, we trained linear classifier (for AD vs. CN) and linear regressor (for age) on top of the semantic vectors (ysem) extracted from the pre-trained LDAE encoder. As baselines, we also evaluated (i) baseline DAE trained in the original voxel space with joint optimization of encoder and diffusion decoder and (ii) fully supervised semantic encoder trained end-to-end with access to ground-truth diagnostic labels. As shown in Table 7, the LDAE embeddings achieved good performance on both tasks, with 83.65% accuracy and 89.48% AUC on the AD classification task, and mean absolute error (MAE) of 4.16 years and RMSE of 5.23 years on the age prediction task. To further understand the semantic structure of the learned embeddings, we applied Linear Discriminant Analysis (LDA) to project the 768-dimensional semantic vectors into 2D. As shown in Figure 5, the resulting plot exhibits distinct clustering of AD and CN subjects, despite the encoder never being trained with diagnostic labels. This confirms that the semantic space ysem captures disease-relevant information in linearly separable manner. These findings support Hypothesis 1, demonstrating that the semantic encoder learns semantically rich representations. Moreover, the effectiveness of such representations on both classification and regression tasks confirms their generality across clinically relevant phenotypes. 5.4.1. Semantic Manipulation via Latent Directions To qualitatively evaluate the interpretability and controllability of the learned semantic space, we conducted semantic manipulation experiment following the strategy discussed in Section 3.7. Once linear classifier is trained to distinguish between AD and CN subjects using the semantic representations ysem, its weight vector defines the principal direction along which the most discriminative semantic variation lies. Since the classifier is trained with label 0 assigned to AD and 1 to CN, Table 7: Linear probe evaluation using semantic representations learned by the encoders of DAE and LDAE compared to the same encoder trained with supervision. Metrics are reported separately for Alzheimers disease classification (left) and age prediction (right). Model Accuracy Precision LDAE (Ours) DAE (Baseline) Supervised Encoder 0.8365 0.7468 0.8464 0.8469 0.7768 0.8690 AD vs. CN Recall 0.9102 0.8504 0. F1-score 0.8774 0.8119 0.8716 MCC 0.6369 0.4312 0.6806 Age Prediction ROC AUC MAE RMSE 0.8948 0.7800 0.9067 4.16 4.93 4.34 5.23 6.11 4.63 Figure 7: Semantic manipulation examples along the direction defined by the vector orthogonal to the classifiers decision boundary. In the ADCN case (top), we observe reduction of hippocampal atrophy; in the CNAD case (bottom), atrophy becomes more prominent. moving along αw amplifies AD-related features (manipulating CN towards AD). In contrast, moving in the opposite direction αw reduces them (manipulating AD towards CN). Given subjects scan, we perform the manipulation as follows: 1. Extract the semantic vector ysem = Encϕ(x0). 2. Encode the stochastic latent representation zT using LDAEs DDIM inversion conditioned on ysem. 3. Perform attribute manipulation by modifying the semantic code: ymanip = ysem +αw (or αw depending on direction). 4. Reconstruct the manipulated image by decoding from (zT , ymanip) via the LDAE reverse process. Figure 6 shows the effect of different manipulation strength, we progressively increased α from 0.0 to 5.0 for an AD subject. The anatomical changes become increasingly pronounced as α grows, particularly in hippocampal and ventricular regions, highlighting smooth and meaningful trajectory in the latent space. Figure 7 shows two representative examples: in the first case (top), an AD subject is manipulated along the αw direction towards CN-like representation, and in the second (bottom), CN subject is manipulated along the αw direction towards an AD-like representation. Both manipulations use scaling factor α = 1.5. 5.5. Interpolation for Missing Scan Generation To further validate the semantic consistency and interpolation capability of the learned latent spaces, we conducted interpolation experiments simulating missing follow-up scan generation. This setup shows LDAEs application toward solving the problem of reconstructing longitudinal scans that were not acquired in real studies, frequent issue in medical datasets. This experiment leverages the ability of the LDAE framework to in12 (a) Mean Squared Error (MSE) as function of prediction gap. Larger gaps correspond to more difficult interpolation tasks. (b) Structural Similarity Index (SSIM) as function of prediction gap. The model shows robust perceptual consistency across all temporal ranges. Figure 8: Quantitative evaluation of semantic interpolation across different prediction gaps. While MSE increases with the temporal distance between input scans, SSIM remains high, indicating perceptual fidelity even in challenging scenarios. terpolate between subjects earlier and later visits to predict an intermediate scan. Figure 9: Qualitative example of latent interpolation for missing scan generation on single subject with four longitudinal scans acquired at 0, 6, 12, and 24 months. The images at months 0 and 24 serve as endpoints (α = 0 and α = 1) for interpolation in the latent space. Intermediate scans at 6 months (α = 0.25) and 12 months (α = 0.5) are synthesized via linear interpolation in the semantic space and spherical interpolation in the stochastic space. 0 To quantitatively assess interpolation accuracy, we compare the generated scan ˆxtarget against the autoencoder reconstructed scan D(E(xtarget )) using SSIM and MSE. This ensures the evaluation is performed entirely within the autoencoders output space, isolating the quality of interpolation in the compressed latent spaces from the reconstruction upperbound imposed by the compression model . The difficulty of this task is evaluated based on the prediction gap, defined as the minimum temporal distance between the target scan and its two neighbors: min(target start, end target). smaller prediction gap implies target temporally close to either neighbor (easier), while larger gap makes accurate interpolation more challenging. Figure 8b, the LDAE maintains strong generation performance even for wider prediction gaps. Notably, SSIM remains above 0.93 and MSE below 0.004 even at larger temporal gaps (up to 24 months), highlighting the semantic smoothness and temporal awareness encoded in the learned representations. In addition, Figure 9 provides qualitative examples of interpolated scans across different temporal gaps. The generated images appear visually coherent and anatomically plausible, preserving the subject identity and global brain structure. These results further support the hypothesis that LDAE captures semantically meaningful representation and potentially temporal progression trend. 6. Discussion and conclusion In this study, we introduced LDAE, novel diffusion-based architecture specifically tailored for efficient and meaningful unsupervised representation learning, with focused application on brain MRI scans related to AD. Semantic Representation Learning. The results demonstrate that LDAE effectively captures structural brain changes on brain MR related to AD and aging. The linear-probe evaluations quantitatively confirm semantic representation capabilities. Although the generative results have been qualitatively validated by clinical expert, future works should include validation of the proposed methods in clinical studies. clinical validation is essential and can be effectively realized through close collaboration with clinical experts. Such partnerships ensure that the methodologies align with real-world medical practices, meet clinical needs, and ultimately enhance their applicability and reliability in healthcare settings. Moreover, systematic multi-attribute disentanglement analyses are essential to ensure robust and interpretable attribute manipulation, given potential correlations (e.g., disease state and age). We report the results over approximately 1400 triplet configurations across 30 test subjects. As shown in Figure 8a and Generative Capabilities. Regarding generative capabilities, the compressed latent space enables efficient and high-quality 13 reconstruction. Nonetheless, reconstruction quality is inherently limited by the perceptual compression autoencoders fidelity. It will be crucial to achieve lossless reconstruction on full resolution brain scan in order to use LDAE for missing scan generation and manipulation in clinical practice. Additionally, the current interpolation methods (LERP and SLERP) presume linearity in brain evolution trajectories. Incorporating advanced regression or forecasting models trained explicitly within the semantic space could better approximate realistic brain aging progressions. Potential Clinical Impact. In longitudinal studies, LDAEs could be used to capture and model subtle temporal changes of anatomical structures on medical imaging; the method can be applied for evaluating the progression of neurodegenerative diseases, or tumor growth, over time. This capability is critical for understanding disease trajectories, monitoring therapeutic responses, and personalizing treatment plans. Additionally, LDAEs are valuable in augmenting sparse datasets, common challenge in medical research. By generating realistic synthetic data that preserves the underlying distribution of the original dataset, LDAEs can help train machine learning models more effectively. This is particularly beneficial in rare disease studies, where obtaining large datasets is often infeasible. Another significant application lies in enhancing explainability through semantic latent manipulation. By isolating and controlling specific features within the latent space, researchers and clinicians can better understand the relationships between imaging patterns and disease characteristics. General limitations and Future Work. The success of LDAE frameworks in medical imaging depends on rigorous validations across diverse populations, imaging scanners, and disease phenotypes. Variability in imaging protocols, demographic factors, and disease presentations can significantly impact model performance. Therefore, comprehensive cross-validation is essential to ensure the generalizability and robustness of these frameworks in real-world clinical settings. Foundation Model capability. These findings position LDAE as promising framework for scalable and interpretable medical imaging applications and as potential Foundation Model for 3D medical image analysis. Future work should explore its transferability to other tasks and modalities, investigate domain adaptation strategies, and benchmark its performance in diverse clinical settings to validate its generalization capacity and pretraining utility. Declaration of generative AI and AI-assisted technologies in the writing process During the preparation of this work the authors used ChatGPT in order to improve language and readability. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication. Acknowledgements Project ECS 0000024 Ecosistema dellinnovazione - Rome Technopole financed by EU in NextGenerationEU plan through MUR Decree n. 1051 23.06.2022 PNRR Missione 4 Componente 2 Investimento 1.5 - CUP H33C22000420001. Data collection and sharing for the Alzheimers Disease Neuroimaging Initiative (ADNI) is funded by the National Institute on Aging (National Institutes of Health Grant U19AG024904). The grantee organization is the Northern California Institute for Research and Education. In the past, ADNI has also received funding from the National Institute of Biomedical Imaging and Bioengineering, the Canadian Institutes of Health Research, and private sector contributions through the Foundation for the National Institutes of Health (FNIH) including generous contributions from the following: AbbVie, Alzheimers Association; Alzheimers Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. References Abstreiter, K., Mittal, S., Bauer, S., Scholkopf, B., Mehrjou, A., 2021. Diffusion-based representation learning. arXiv preprint arXiv:2105.14257 . Avants, B.B., Epstein, C.L., Grossman, M., Gee, J.C., 2008. Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. Medical image analysis 12, 2641. Avants, B.B., Tustison, N.J., Stauffer, M., Song, G., Wu, B., Gee, J.C., 2014. The insight toolkit image registration framework. Frontiers in neuroinformatics 8, 44. Cardoso, M.J., Li, W., Brown, R., Ma, N., Kerfoot, E., Wang, Y., Murrey, B., Myronenko, A., Zhao, C., Yang, D., et al., 2022. Monai: An open-source framework for deep learning in healthcare. arXiv preprint arXiv:2211.02701 . Cullen, N.C., Avants, B.B., 2018. Convolutional neural networks for rapid and simultaneous brain extraction and tissue segmentation. Brain Morphometry , 1334. Dhariwal, P., Nichol, A., 2021a. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34, 87808794. Dhariwal, P., Nichol, A., 2021b. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34, 87808794. Dosovitskiy, A., Brox, T., 2016. Generating images with perceptual similarity metrics based on deep networks. Advances in neural information processing systems 29. Esser, P., Rombach, R., Ommer, B., 2021. Taming transformers for highresolution image synthesis, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883. Fonov, V., Evans, A.C., Botteron, K., Almli, C.R., McKinstry, R.C., Collins, D.L., Group, B.D.C., et al., 2011. Unbiased average age-appropriate atlases for pediatric studies. Neuroimage 54, 313327. 14 data distribution. Advances in neural information processing systems 32. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B., 2020b. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 . Sudlow, C., Gallacher, J., Allen, N., Beral, V., Burton, P., Danesh, J., Downey, P., Elliott, P., Green, J., Landray, M., et al., 2015. Uk biobank: an open access resource for identifying the causes of wide range of complex diseases of middle and old age. PLoS medicine 12, e1001779. Tustison, N.J., Avants, B.B., Cook, P.A., Zheng, Y., Egan, A., Yushkevich, P.A., Gee, J.C., 2010. N4itk: improved n3 bias correction. IEEE transactions on medical imaging 29, 13101320. Xiang, W., Yang, H., Huang, D., Wang, Y., 2023. Denoising diffusion autoencoders are unified self-supervised learners, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15802 15812. Yarkoni, T., Markiewicz, C.J., De La Vega, A., Gorgolewski, K.J., Salo, T., Halchenko, Y.O., McNamara, Q., DeStasio, K., Poline, J.B., Petrov, D., et al., 2019. Pybids: Python tools for bids datasets. Journal of open source software 4, 1294. Yoon, J.S., Zhang, C., Suk, H.I., Guo, J., Li, X., 2023. Sadm: Sequence-aware diffusion model for longitudinal medical image generation, in: International Conference on Information Processing in Medical Imaging, Springer. pp. 388400. Yu, J., Li, X., Koh, J.Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., Wu, Y., 2021. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627 . Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O., 2018. The unreasonable effectiveness of deep features as perceptual metric, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595. Zhang, Z., Zhao, Z., Lin, Z., 2022. Unsupervised representation learning from pre-trained diffusion probabilistic models. Advances in neural information processing systems 35, 2211722130. Fonov, V.S., Evans, A.C., McKinstry, R.C., Almli, C.R., Collins, D., 2009. Unbiased nonlinear average age-appropriate brain templates from birth to adulthood. NeuroImage 47, S102. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial nets. Advances in neural information processing systems 27. Gorgolewski, K.J., Auer, T., Calhoun, V.D., Craddock, R.C., Das, S., Duff, E.P., Flandin, G., Ghosh, S.S., Glatard, T., Halchenko, Y.O., et al., 2016. The brain imaging data structure, format for organizing and describing outputs of neuroimaging experiments. Scientific data 3, 19. Ho, J., Jain, A., Abbeel, P., 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 68406851. Hudson, D.A., Zoran, D., Malinowski, M., Lampinen, A.K., Jaegle, A., McClelland, J.L., Matthey, L., Hill, F., Lerchner, A., 2024. Soda: Bottleneck diffusion models for representation learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2311523127. Kingma, D.P., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 . Lozupone, G., Bria, A., Fontanella, F., Meijer, F.J., De Stefano, C., 2024. Axial: Attention-based explainability for interpretable alzheimers localized diagnosis using 2d cnns on 3d mri brain scans. arXiv preprint arXiv:2407.02418 . Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M., 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 . Nichol, A.Q., Dhariwal, P., 2021. Improved denoising diffusion probabilistic models, in: International conference on machine learning, PMLR. pp. 8162 8171. Peng, W., Adeli, E., Bosschieter, T., Park, S.H., Zhao, Q., Pohl, K.M., 2023. Generating realistic brain mris via conditional diffusion probabilistic model, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 1424. Pinaya, W.H., Graham, M.S., Kerfoot, E., Tudosiu, P.D., Dafflon, J., Fernandez, V., Sanchez, P., Wolleb, J., Da Costa, P.F., Patel, A., et al., 2023. Generative ai for medical imaging: extending the monai framework. arXiv preprint arXiv:2307.15208 . Pinaya, W.H., Tudosiu, P.D., Dafflon, J., Da Costa, P.F., Fernandez, V., Nachev, P., Ourselin, S., Cardoso, M.J., 2022. Brain imaging generation with latent diffusion models, in: MICCAI Workshop on Deep Generative Models, Springer. pp. 117126. Preechakul, K., Chatthee, N., Wizadwongsa, S., Suwajanakorn, S., 2022. Diffusion autoencoders: Toward meaningful and decodable representation, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1061910629. Puglisi, L., Alexander, D.C., Rav`ı, D., 2024. Enhancing spatiotemporal disease progression models via latent diffusion and prior knowledge, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 173183. Rezende, D.J., Mohamed, S., Wierstra, D., 2014. Stochastic backpropagation and approximate inference in deep generative models, in: International conference on machine learning, PMLR. pp. 12781286. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B., 2022. Highresolution image synthesis with latent diffusion models, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695. Routier, A., Burgos, N., Dıaz, M., Bacci, M., Bottani, S., El-Rifai, O., Fontanella, S., Gori, P., Guillon, J., Guyot, A., et al., 2021. Clinica: An open-source software platform for reproducible clinical neuroscience studies. Frontiers in neuroinformatics 15, 689675. Samper-Gonzalez, J., Burgos, N., Bottani, S., Fontanella, S., Lu, P., Marcoux, A., Routier, A., Guillon, J., Bacci, M., Wen, J., et al., 2018. Reproducible evaluation of classification methods in alzheimers disease: Framework and application to mri and pet data. NeuroImage 183, 504521. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S., 2015. Deep unsupervised learning using nonequilibrium thermodynamics, in: International conference on machine learning, PMLR. pp. 22562265. Song, J., Meng, C., Ermon, S., 2020a. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 . Song, Y., Ermon, S., 2019. Generative modeling by estimating gradients of the"
        }
    ],
    "affiliations": [
        "Department of Electrical and Information Engineering (DIEI), University of Cassino and Southern Lazio, Italy",
        "Department of Medical Imaging, Radboud University Medical Center, Netherlands",
        "Diagnostic Image Analysis Group, Radboud University Medical Center, Netherlands"
    ]
}