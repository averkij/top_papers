{
    "paper_title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception",
    "authors": [
        "Hui Zhang",
        "Zijian Wu",
        "Linyi Huang",
        "Sammy Christen",
        "Jie Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"
        },
        {
            "title": "Start",
            "content": "Robust Dexterous Grasping of General Objects from Single-view Perception Hui Zhang1, Zijian Wu2, Linyi Huang2, Sammy Christen1 and Jie Song2,3 2HKUST (Guangzhou), China 3HKUST, Hong Kong (China) 1ETH Zurich, Switzerland Email: huizhang@ethz.ch Equal Contribution 5 2 0 2 7 ] . [ 1 7 8 2 5 0 . 4 0 5 2 : r Fig. 1: Our method achieves robust dexterous grasping from single-view object point clouds. It performs adaptive motions to disturbances such as object movement and external forces (a), and can grasp various objects with random poses, diverse shapes, sizes, materials, and masses, including shiny, heavy, deformable, thin, and transparent objects (b). AbstractRobust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose mixed curriculum learning strategy, which first utilizes imitation learning to distill policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust DexGrasp/. I. INTRODUCTION Grasping is one of the most essential skills for robot manipulation, as it forms the foundation for further tasks, such as handing objects to person or placing an object in target position. Previous research on dexterous grasping has primarily focused on scenarios involving fully observable or pre-known object models [15, 39], transferring human demonstrations to robots [7, 50, 23], or executing predicted static grasping poses or fingertip positions [2, 5, 48]. However, fully scanning an object or recording object-specific human demonstrations limits the efficient deployment to novel objects. Additionally, grasping based on static pose execution restricts adaptability to disturbances, such as external forces or unobserved object displacement, either before or after the object is grasped. In this paper, we present framework for zero-shot dexterous grasping of variety of novel objects from single-view visual inputs. More importantly, like humans, our system can adapt in real time to disturbances during grasping, such as unobserved object state changes or external forces. Specifically, we adopt hand-centric object shape representation that uses dynamic distance vectors between finger joints and object surfaces. This representation emphasizes potential fingerobject interactions rather than global object shapes, making it robust to variations across different objects and uncertainties due to limited viewpoints. Besides, learning adaptive grasping motions in response to external disturbances relies on continuous feedback from observations. However, the accessible perception is limited due to the lack of tactile sensing and the use of single-view camera that suffers from object occlusion caused by the robot hand during grasping. To address this, we train teacher policy using privileged real-time visual-tactile perception and propose mixed curriculum learning approach to train the student policy. In particular, the training starts with imitation learning to efficiently distill the grasping capability of the teacher policy. Then the training gradually transitions to reinforcement learning to encourage exploration for adaptation to disturbances, which are introduced by observation noise and dynamic randomization during the student policy training. We conduct comprehensive experiments to assess the generalization and robustness of our method. Our approach shows strong generalization in grasping novel objects randomly placed on table. It achieves success rates of 97.0% on 247,786 simulated Objaverse [12] objects and 94.6% on 512 real-world daily objects, all of which were unseen during training. Additionally, our method enables real-time adaptive motions in both simulation and real-world environments, showing superior robustness against disturbances like object movement and external forces. An ablation study on key components demonstrates their effectiveness and necessity. In summary, our contributions are 1) framework for zeroshot dynamic dexterous grasping of various unseen objects from single-view perception. 2) We demonstrate the effectiveness of sparse hand-centric object shape representation using joint distance vectors for real robot dexterous grasping, showing robustness to shape variance and uncertainty. 3) mixed curriculum learning method that combines imitation learning for efficient distillation of privileged policy and learning for adaptive motion learning under reinforcement disturbances, all from limited observations. 4) Trained in simulation, our method generalizes exceptionally across 247,786 unseen objects in simulation and 512 unseen objects on real robot, while exhibiting robust adaptive motions to disturbances such as object displacement and external forces. II. RELATED WORK For better comparison of existing dexterous grasping methods with ours, we list the differences in Tab. I. Method GraspXL [46] Agarwal et al.[1] SpringGrasp [5] DextrAH-G [26] DexGraspNet2.0 [48] Ours Single-view Observation Non-static Grasping Pose Zero-shot General. Unseen Obj. Num. (Sim) Unseen Obj. Num. (Real) 503,409 2 - - 1319 247,786 - 6 14 30 32 512 TABLE I: Comparison with existing dexterous grasping works A. Optimization-based Dexterous Grasping Dexterous grasping is long-standing research topic in robotics [13, 21, 36]. Traditional methods typically focus on predicting contact points or fingertip positions by optimizing properly designed analytical metrics for stable grasping, such as the differentiable approximations of shape closure [39] and force closure [25] metrics. These works mostly require accurate object models [11, 13, 28, 33] which limits their generalization ability in real-world deployment. Recently, some works have explored dealing with object shape uncertainty with compliant control algorithms [5, 20, 24, 29, 30]. Although enabling grasping under imperfect object shape observation, these methods usually utilize analytical dynamic models to calculate the joint torque commands according to static target grasping poses or fingertip positions, which limits their robustness to unmodeled disturbances such as inaccurate joint actuator models or external forces. Furthermore, the reliance on static poses limits their dexterity, as the hand cannot adaptively change the grasping poses when the object moves unexpectedly. B. Data-driven Dexterous Manipulation To effectively learn the complex and dexterous interaction between the hand and the objects for different manipulation tasks, some works utilize imitation learning to learn from human demonstrations [6, 22, 43] or robot demonstrations collected on real robots [8, 32, 42, 45]. However, these demonstrations are usually expensive to collect, which leads to limited available data and further limits their generalization ability for out-of-distribution settings. Jiang et al. [18] enrich the collected demonstrations by behavior cloning in simulation. Nevertheless, the generalization is still limited to basic attributes like the object positions and colors, without further capability to deal with completely novel objects. Compared with these works, our method has specific focus on grasping, and can effectively grasp various unseen objects without reliance on any demonstrations. Instead of relying on collected demonstrations, some works utilize synthetic datasets to learn dexterous grasping. Specifically, Zhang et al. [49] sample grasping poses according to object point clouds. Zhang et al. [48] train diffusion model to generate the grasping poses from their optimizationbased synthetic dataset [39]. However, due to the lack of synthetic large-scale dynamic dexterous grasping datasets, these methods usually focus on generating static grasping poses and executing the poses in an open-loop manner, which suffers from robustness and adaptation ability to external disturbances. In contrast, our method predicts real-time joint Fig. 2: Overview of our framework. We first train teacher policy with reinforcement learning (RL) which can access privileged information including ground-truth contacts and impulses, fully observable real-time object point clouds, and noisefree robot joint angles. Then we train student policy with access only to initial single-view object point clouds and noisy robot joint angles. The student policy is trained with mixed curriculum learning framework, which initially utilizes imitation learning (IL) for efficient teacher policy distillation, and gradually transitions to RL for exploration to deal with disturbances. The contact and impulse reconstruction loss remains active during the whole student training process. actions according to current status, leading to adaptive motions to disturbances and more robust grasping. C. RL-based Dynamic Dexterous Grasping RL has recently been verified to be effective in dealing with disturbances under various environments, especially for robot locomotion [9, 14, 27, 31]. Through exploration in simulation under disturbances, the policies can learn dynamic actions and perform real-time adaptation to unexpected disturbances. However, due to the complex hand-object dynamical interactions of dexterous grasping which are challenging to explore and difficult to precisely simulate, current RL-based dexterous grasping methods usually suffer from poor generalization and sim-to-real gap. Qin et al. [31] achieves dexterous grasping from single-view perception, but is limited in category-level generalization on grasping unseen objects. Overall, most of the current works with RL for dynamic dexterous grasping are still constrained to simulation [10, 38, 44, 46, 47]. Some RL-based methods leverage human data to simplify the exploration and successfully deploy on real robots. For example, some works [1, 26, 35] simplify the RL exploration with lower-dimensional action space utilizing the PCA components extracted from human grasping data, which limits the hand dexterity. Besides, [1] relies on multi-camera system and demonstrates only category-level generalization ability, while [26, 35] use analytical dynamic models to calculate control commands, leading to limited perceptionbased adaptation ability to unmodeled disturbances as admitted in their paper. Huang et al. [15] leverage human grasping poses to guide functional robot grasping, while relying on known object meshes. Overall, the requirements of specific human data, multiple cameras, accurate dynamic models, and known object meshes, limit their potential to scale up or perform adaptive motions to unmodeled disturbances. Our method, in contrast, can be successfully deployed on real robots with single camera, and can achieve zero-shot generalization to 512 unseen daily objects without any human data or known object meshes, while performing real-time proprioceptionbased adaptive motions to external disturbances. A. Overview III. METHOD We aim to tackle the challenging task of dexterous grasping for various unseen objects based on single-view visual perception. Given the single-view point cloud of an object, our goal is to control robotic arm with dexterous hand to grasp the object while performing adaptive motions to disturbances. Fig. 2 illustrates the pipeline of our method. We first train teacher policy using reinforcement learning, with access to real-time, fully observable object point clouds and ground-truth hand-object contacts and impulses, which we refer to as visual-tactile policy. Then we train student policy with the perception that is accessible on the real robot. Specifically, due to the severe object occlusion caused by the robot hand during grasping, the student policy only utilizes the static object point clouds captured before grasping (initial single-view object point clouds) in our framework, and the proprioception contains noisy joint states without tactile sure the hand heads to object centers and encloses the objects from narrow edge while avoiding singularity problems. With the defined 6D wrist pose, we can calculate the initial arm joint angles with inverse kinematics (IK) [19]. Furthermore, if the IK solver can not find feasible solution or lead to selfcollision, we will set to be the same direction from the arm base to and re-initialize the arm joints. C. Visual-tactile Policy Training We first train policy with reinforcement learning to control the robot arm and dexterous hand, which has access to realtime full object point clouds and ground-truth hand-object binary contacts and impulses of each finger link. 1) Observation: The observation space of the policy is represented with st = (at1, qt, Ot, ct), where is the current time step, at1 is the action of the previous step, qt is the current arm and finger joint angles, Ot is the real-time fully observable object point cloud, ct = {bt, ft} includes the binary contact states bt and impulses ft of each finger link with the object. We omit the notation for simplicity in this section. Given a, and O, we extract the following features with function Φ(a, q, O) = (d, h, T, q). Specifically, is the vector from each finger joint to the closest point of O. is the vertical distance of each arm and hand joint to the table. is the wrist orientation and position. is the tracking error, which means the difference between current joint angles qt and previous action at1. The extracted features are then fed to the policy together with and c. 2) Reward Function: To incentivize the policy to learn robust and safe grasping, we design the reward function with the formulation = rdis + rcontact + rheight + rreg (1) which comprises the hand-object distance reward rdis, link contact and impulse rewards rcontact, link height reward rheight, and an regularization term rreg. The hand-object distance reward rdis is inspired by [46], which aims to penalize the distance from the hand links to the object with the formulation rdis = (cid:80)L i=1 wd di2, (2) where wd number of hand links. is the corresponding weights and indicates the The contact reward rcontact aims to encourage desired handobject contacts while penalizing all the undesired contacts (self-collision, robot-table collision, and arm-object collision) with the formulation rcontact = (cid:80)L ) (cid:80)L+M j=1 bj(wcu + wf ) (3) i , wcu where wcd are the weights for desired and undesired binary contact states and impulses, bi and bj are the desired binary contact states of the ith hand link and undesired and binary contact states of the jth hand or arm link, are the magnitude of desired hand impulses on the ith link and undesired hand or arm impulses on the jth link, and is the number of arm links. i=1 bi(wcd , wf + wf , wf Fig. 3: Pre-grasping pose of our method. The finger angles are initialized to get partially opened hand, while the arm joints are initialized according to the end-effector 6D pose by inverse kinematics. Specifically, the heading direction (the red arrow) of the hand points to the object point cloud center from fixed starting point, and the palm direction (the green arrow) is determined to enclose the objects from narrow edge while avoiding singularity problems. The hand is then set 25cm away from along x. perception. The student policy training is driven by mixed curriculum learning method, which starts with imitation learning to efficiently distill the grasping capability of the teacher policy, and gradually transitions to reinforcement learning for exploration to learn adaptive motions to disturbances and observation noises. The policies output target joint positions for the low-level PD controllers of the hand and arm. B. Pre-grasp Pose Initialization good pre-grasp pose can significantly reduce the difficulty for the policy to learn stable grasping as it helps constrain the exploration space. Thus, we propose an intuitive and effective module to generate the pre-grasping poses, which is shown in Fig. 3. The hand is initialized to open the fingers partially. To initialize the arm joints, we define heading direction for the hand (the red arrow in Fig. 3) and set it to point from fixed starting point in the world frame towards c, which is the center of the single-view object point clouds. Then we define palm direction (the green arrow in Fig. 3) along the final arm link. We randomly sample 10 directions for which are orthogonal to x, and choose the best direction by minimizing ly = wlengthllength + q4 1.572, where llength is the length of the object point cloud projection along y, and q4 is the second final arm joint. wlength = 5 is the weight. The wrist orientation of the hand can be defined with the two directions and y, while the wrist position is set so that the hand is 25 cm away from along x. Intuitively, the pre-grasp poses make The link height reward rheight penalizes the hand and arm link vertical distances to the table when they are smaller than 2 cm to avoid collision rheight = (cid:80)L+M i=1 wh log(min{hi, 0.02}), (4) where wh table of the ith link. is the weight and hi is the vertical distance to the rreg is defined to penalize object movement and extreme robot movement with the formulation rreg = wh Th2 + wo To2 + wllo + wq qa2, (5) where Th and To are the linear and angular velocities of the hand and object, lo is the object displacement, and qa is the arm joint velocities. Fig. 4: Hardware setup D. Student Policy Training with Limited Perception E. Sim-to-real Transfer With the teacher policy trained using real-time visual-tactile perception, we further train student policy with access to initial single-view object point clouds and noisy proprioception without tactile perception. 1) Observation: The visual-tactile policy requires real-time fully observable object point clouds Ot and contact states ct, which are privileged information inaccessible on the real robots. For hardware deployment, the student policy should utilize the initial single-view point cloud ˆO0 and noisy joint state proprioception ˆqt. Specifically, we utilize an LSTMbased encoder to reconstruct the contact states ˆct = {ˆbt,ˆft} from joint state and action histories. Intuitively, the actions can be used to infer the joint actuator torques, while the misalignment between the actuator torques and joint state changes can indicate external forces induced by contacts. 2) Mixed Curriculum Learning: While the teacher policy utilizes visual-tactile perception to learn stable grasping without disturbances, the student policy should learn to grasp from limited perception and perform adaptive motions to disturbances. To deal with this challenge, we propose mixed curriculum learning method to train the student policy. Specifically, the training is initially driven by imitation learning, with contact reconstruction loss Lre = wre(ˆbt bt2 + ˆft ft2) to encourage the reconstruction of ct, and an action imitation loss Lact = wactˆat at2 for the student policy to imitate the teacher policy behavior. This can help the student policy efficiently distill the grasping capability of the teacher policy. Based on this, the training gradually transitions from imitation learning to reinforcement learning with wact decreasing by an adaptive factor λ and RL rewards increasing by 1λ (wre is fixed). This helps the student policy keep exploring to learn adaptation capability to deal with the disturbances caused by observation noises and actuator execution inaccuracies. The student policy network is initialized with the teacher policy weights to further accelerate the training process. All weight values and λ can be found in the supplementary material. Our work aims at robust dexterous grasping for real robots. To narrow down the sim-to-real gap, we apply domain randomization when training the student policy to simulate various environment attributes, observation noises, and actuator inaccuracies. Besides, we also simulate the action delay caused by the inference time. 1) Domain Randomization: Real robots usually suffer from noisy observations such as in the proprioception of joint states. In addition, some physical parameters are difficult to accurately estimate such as the friction coefficients. To solve these limitations, we adopt domain randomization which is often used to increase the robustness of sim-to-real transfer [15, 17]. Specifically, we randomize the friction coefficients and proprioception of robot joint angles when training the student policy. Besides, the actuator dynamics, especially for the finger joint actuators, suffer from unstable stiffness and damping caused by factors like overheating. Hence, we also randomize the PD gains of the low-level joint PD controllers. More details about the parameter values and randomization can be found in the supplementary material. 2) Action Delay: During hardware deployment, the inference time of the policy will cause some action delay. Specifically, the low-level controller (e.g., the joint PD controllers) will follow the previous actions from the high-level controller the policy) during the inference time. To deal with (e.g., this sim-to-real gap, we randomly delay the high-level action update when training the policies. IV. EXPERIMENTS A. Experimental Setup 1) Hardware Setup: We utilize UR5 [37] robot arm paired with an Allegro [41] dexterous robot hand, as depicted in Fig. 4. We position stationary RealSense D435i camera above the manipulation table to capture single-view object point clouds. The policy runs at 5 Hz as high-level controller, while the low-level PD controllers for the hand and arm operate at 100 Hz. Small Objects Medium Objects Large Objects Total Object Number Success Rate 38,493 0.949 100,435 0. 108,858 247,786 0.976 0.970 TABLE II: Large-scale evaluation (Sim) Category Suc. Rate Category Success Rate Picnic Models Fruit & Vegetable Models Animal Models Wooden Models Bottles & Boxes Deformable Objects 0.902 0.962 0.907 0.940 0.970 0.957 Average Building Blocks Tool Models Toy Cars Snacks Real Tools Other Daily Objects 0.946 0.963 0.875 0.979 0.974 0.893 0.971 TABLE III: Large-scale evaluation (Real) 2) Training Details: We employ Raisim [16] as the physics simulation and use PPO [34] to train our policies. Using single NVIDIA RTX 3090 GPU and 128 CPU cores, the training of the teacher and student policies takes approximately 30 hours in total. The policies are trained with the 35 objects, which we get from [3] and scanned real objects from [15]. 3) Metric: Since our focus is on robust grasping, we use the grasping success rate (Suc. Rate) as our evaluation metric. Specifically, grasp is considered successful if the object can be lifted to height of 0.1 meters and remains stable without falling for at least 3 seconds. B. Large Scale Evaluation To demonstrate the strong generalization ability of our method, we evaluate it using 512 real objects and 247,786 simulated objects, which are all unseen during training. 1) Simulation Evaluation: We begin by evaluating our method using objects from the Objaverse [12] dataset. Following [46], we scale the objects into three sizes: small, medium, and large, and filter out objects that are infeasible to grasp from the table (diameter < 1cm, height < 2cm, or width > 15cm), resulting in test set of 247,786 objects in total. More details on preprocessing are provided in the supplementary material. Each object is placed randomly on the table for grasping, and the results are presented in Tab. II. Our method achieves an overall success rate of 0.970. It shows higher success rates with larger objects due to the large link sizes of the Allegro hand, while maintaining success rate of 0.949 for small objects. This demonstrates the excellent generalization ability across different object sizes of our method. 2) Hardware Evaluation: In addition to simulation evaluation, we assess the effectiveness of our method in grasping variety of novel objects in the real world. Specifically, we collect 512 real objects with diverse shapes, weights, materials, and sizes from different categories, which are illustrated in Fig. 5. The category labels are listed in Tab. III with the corresponding order, and more detailed information about the objects is available in supplementary material. Each object is evaluated in three random poses on the table. The average success rate for each category is shown in Tab. III. Without any prior knowledge of the objects, our Fig. 5: Real objects used for large-scale evaluation method achieves an overall success rate of 0.946 using only single-view depth observation. It demonstrates exceptional effectiveness and generalization across variety of real objects. Notably, although the policy is trained only with solid objects in simulation, it effectively grasps deformable objects, which we attribute to the reconstructed contacts and impulses of the finger links. Most failure cases are caused by noisy point clouds for thin, small objects, and the finger joint torque limitations for heavy, slippery objects. C. Comparisons One of the main advantages of RL-based grasping is its ability to adapt in real time to disturbances, potentially enhancing robustness. To verify this, we compare our method with two state-of-the-art optimization-based dexterous grasping methods under different conditions: no external disturbances, random object movement, and external forces. 1) Baselines: (1) DexGraspNet [39]. Since DexGraspNet optimizes grasping poses without considering tables, we incorporate table collision loss to generate collision-free poses for fair comparison (see the supplementary material for more details). We generate 64 static grasping poses for each object and employ the optimal one with PD controller. The pre-grasp pose is initialized with the fingers partially opened from the target pose, and the wrist 25cm away from the target along the line from the object point cloud center to the hand. To facilitate clear comparison between the performances in simulation and the real world, we utilize 30 ShapeNet [4] objects with diverse shapes for simulation evaluation, and 3D print these objects for hardware evaluation, as shown in Fig. 6. DexGraspNet leverages full object point clouds, which we obtain with known object meshes and FoundationPose [40] for their method during hardware experiments, Method Success Rate SpringGrasp [5] Ours 0.771 0.957 TABLE IV: Comparison with SpringGrasp (Real) Method No Disturb. < 3cm Move. < 5cm Move. 2.5N Force DexGraspNet [39] (Sim) Ours (Sim) DexGraspNet [39] (Real) Ours (Real) 0.667 0.953 0.607 0.920 0.500 0.893 0.447 0. 0.453 0.753 0.373 0.767 0.553 0.920 0.480 0.840 TABLE V: Comparison with DexGraspNet (Sim & Real) the objects in random directions before grasping and after the point cloud has been captured, with the moving distances randomly sampled within two distinct ranges: 3 cm and 5 cm. The results are presented in Tab. V. Our method outperforms DexGraspNet in both simulation and real-world scenarios, and exhibits smaller performance drops compared to the results without disturbances, which verifies our superior robustness to object movement. Compared with the performance drop from no disturbance to < 3 cm settings, our performance drop from < 3 cm to < 5 cm settings is slightly more pronounced, as many objects have dimensions smaller than 5 cm, resulting in unreliable initial single-view object observations. 4) Robustness to External Forces: We further evaluate the robustness of our method against external forces applied to objects after they have been grasped, comparing the performance with DexGraspNet. In the simulation, we apply an external force of 2.5 with random directions at random point on the object. In the hardware experiment, we place 250 weight at randomly selected location on the object to introduce controlled external disturbances, leading to 2.5 downward external force at random point. The results are shown in Tab. V. Similarly, our method achieves superior success rates with smaller performance degradation across both simulation and real-world environments, indicating enhanced robustness against external forces. We observe that thin and heavy objects are more susceptible to external forces for both methods, particularly in hardware experiments. This is attributed to the difficulties in firmly grasping thin objects, and the sensitivity of heavy objects to additional weights due to the output torque limit of the finger joint actuators. 5) Qualitative Evaluation: To further validate the adaptability of our RL-based grasping method, we present qualitative demonstrations of its robustness to disturbances in Fig. 7. Our method can adjust the grasping poses in response to unexpected collisions caused by noisy joint states, inaccurate object observations, or imprecise actuator execution. Additionally, it effectively handles unobserved object displacements and helps maintain stable grasp even when large external forces are applied. More qualitative results are presented in our supplementary video. D. Ablation and Analysis We conduct an ablation study to evaluate the impact of different components of our method in both simulation and Fig. 6: Objects used for comparisons and ablation while our method is still evaluated with original single-view perception. Each object is tested across five random poses in both simulation and the real world. (2) SpingGrasp [5]. This method optimizes fingertip positions, initial hand poses, and controller gains, allowing the hand to grasp the object compliantly. Since SpringGrasp lacks simulation setup and uses different hardware, we utilize the same (or highly similar but more challenging) objects as in their paper to conduct our real-world experiments, and directly compare our results with those reported in their paper. The objects we use are shown in the supplementary material. Focusing on robust grasping, we compare the success rates of strictly successful grasps, while labeling partially successful grasps defined in SpringGrasp (where the object is lifted but subsequently slides) as failures for both methods. SpringGrasp evaluates each object with five poses, while we assess our method using ten random poses for each object for more comprehensive evaluation. 2) Evaluation without External Disturbances: We first compare the performance of different methods without additional disturbances in both simulation and the real world, with the results presented in Tab. IV and Tab. V. It is important to note that the hardware experiments still experience disturbances due to noisy joint state observations, object point clouds, and inaccuracies in actuator executions (e.g., caused by overheating). Our method consistently outperforms the baselines in both simulation and real-world experiments. Notably, it demonstrates smaller sim-to-real performance gap compared to DexGraspNet, underscoring the enhanced robustness of our RL-based approach to observation noise and inaccuracies in dynamic models. Most failures of DexGraspNet occur with thin objects, as its emphasis on contact-rich grasping poses can be infeasible for such objects lying flat on the table. Additionally, the higher success rate of our method compared to SpringGrasp further validates the effectiveness and advantages of our approach. 3) Robustness to Object Movement: We evaluate the robustness of our method against object movement in both simulation and real-world scenarios, comparing it to DexGraspNet. Specifically, we introduce external disturbances by moving Fig. 7: Our method can adapt the poses for stable grasping when unexpected collision occurs due to internal disturbances (a), deal with unobserved object movement (b), and maintain robust grasps when the object slips due to large external forces (c). the real world. We use the same 30 ShapeNet objects as in Section IV-C, with five random poses for each object. 1) Simulation Ablation: In simulation, we first verify the effectiveness of our proposed mixed curriculum learning method. Specifically, we assess variants such as training the student policy without the RL rewards (W.o. RL rewards), without action imitation losses (W.o. IL losses), and using fixed ratio for the RL rewards and imitation loss (W.o. Curriculum). Secondly, we evaluate the necessity of privileged teacher policy by considering variant that directly trains the student policy from scratch using only the RL rewards and contact reconstruction loss (W.o. Priv. learning). To further validate the effectiveness of our student policy, we also compare the results with the visual-tactile teacher policy, which has access to privileged information (Teacher policy). The results are presented in Tab. VI. Compared to our full method, the variants without RL rewards and IL loss exhibit similar decline in performance, underscoring the necessity of both components. The marginally lower success rate of the policy trained with fixed RL rewards/IL loss ratio indicates the contribution of the reward ratio curriculum. Overall, starting with larger IL loss facilitates rapid distillation of the grasping capabilities from the teacher policy, while subsequently increasing RL rewards encourages effective exploration for adaptation to disturbances. This illustrates the effectiveness of our mixed curriculum learning approach. The student policy trained from scratch does not perform on par with our original setting, while the training becomes Setting Suc. Rate Setting Success Rate W.o. RL rewards W.o. IL loss W.o. Priv. learning Ours 0.907 0.933 0.773 0.953 W.o. Curriculum 0.913 Teacher policy 0.960 TABLE VI: Ablation (Sim) Method Ours Full Point Cloud Success Rate 0.920 0.933 TABLE VII: Ablation (Real) much more sensitive to hyperparameters. This highlights the importance of first training teacher policy with privileged perception to provide an effective initialization and guide the training for the student policy. The comparable success rates between our student and teacher policies further validate the effectiveness of our mixed curriculum learning method. 2) Real-world Ablation: We also compare our original setting with variant that utilizes real-time, fully observable object point clouds in the real world (Full Point Cloud), which we get from FoundationPose [40] along with known object meshes. In contrast, our approach uses initial single-view object point clouds captured by the camera without object meshes, which can be directly applied to novel objects for zero-shot generalization. The results are shown in Tab. VII, where the highly comparable performances between the two settings highlight the effectiveness of our sparse object-centric representation in extracting meaningful shape features from single-view object observations. V. CONCLUSION In this paper, we present an RL-based framework for robust dexterous grasping from single-view perception. The exceptional generalization ability of our method is verified with 512 real objects and 247,786 simulated objects, leading to success rates of 94.6% and 97.0%, respectively. Compared with previous methods, our RL-based dynamic grasping method exhibits significant advantages in the robustness and adaptation ability to disturbances caused by observation or actuator noises, object movement, and external forces, which is quantitatively and qualitatively demonstrated in our experiment. Overall, our method verifies highly sparse hand-centric object shape representation plus proprioception is effective for grasping various objects with adaptation ability to disturbances. VI. LIMITATIONS Our method has some limitations which we hope to address in the future. First, focusing on learning adaptive motions to disturbances based on real-time proprioception, our method utilizes the static object point clouds before they are grasped (initial single-view object point clouds), leading to open-loop visual inputs. This makes it difficult to deal with significant object movement. Grasping under highly dynamic environments with the object continuously moving would be an interesting direction for future work. Second, due to the limitation of the hand size, it is difficult for the robot to grasp very small objects with diameter less than 1.5 cm. Apart from the limitations, another interesting direction for future work is to utilize our method as robust low-level skill for dexterous robots to grasp various objects, and integrate high-level environment understanding with our grasping capability to complete diverse practical tasks, such as grasping certain object indicated by language instructions from the table, handing it over to person, or putting it to desired position. REFERENCES [1] Ananye Agarwal, Shagun Uppal, Kenneth Shaw, and Deepak Pathak. Dexterous Functional Grasping. In Conference on Robot Learning (CoRL), 2023. [2] Maria Attarian, Muhammad Adil Asif, Jingzhou Liu, Ruthrash Hari, Animesh Garg, Igor Gilitschenski, and Jonathan Tompson. Geometry Matching for MultiEmbodiment Grasping. In Conference on Robot Learning (CoRL), 2023. [3] Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M. Dollar. The YCB object and Model set: Towards common benchmarks for manipulation research. International Conference on Advanced Robotics (ICAR), 2015. [4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012, 2015. [5] Sirui Chen, Jeannette Bohg, and C. Karen Liu. SpringGrasp: An optimization pipeline for robust and compliant dexterous pre-grasp synthesis. Robotics: Science and Systems (RSS), 2024. [6] Yuanpei Chen, Chen Wang, Yaodong Yang, and Karen Liu. Object-Centric Dexterous Manipulation from HuIn Conference on Robot Learning man Motion Data. (CoRL), 2024. [7] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter Fox. Dextransfer: Real world multi-fingered dexterous arXiv grasping with minimal human demonstrations. preprint arXiv:2209.14284, 2022. [8] Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and Xiaolong Wang. Open-TeleVision: Teleoperation with Immersive Active Visual Feedback. Conference on Robot Learning (CoRL), 2024. [9] Suyoung Choi, Gwanghyeon Jeongsoo Park, Hyeongjun Kim, Juhyeok Mun, Jeong Hyun Lee, and Jemin Hwangbo. Learning quadrupedal locomotion on deformable terrain. Science Robotics, 2023. Ji, [10] Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin Hwangbo, Jie Song, and Otmar Hilliges. D-grasp: Physically plausible dynamic grasp synthesis for handIn Computer Vision and Pattern object interactions. Recognition (CVPR), 2022. [11] Matei T. Ciocarlie, Corey Goldfeder, and Peter K. Allen. Dexterous Grasping via Eigengrasps: Lowdimensional Approach to High-complexity Problem. 2007. URL https://api.semanticscholar.org/CorpusID: 6853822. [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: Universe of Annotated 3D Objects. arXiv preprint arXiv:2212.08051, 2022. [13] C. Ferrari and J. Canny. Planning optimal grasps. International Conference on Robotics and Automation (ICRA), 1992. [14] Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, and Jianyu Chen. Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning. Robotics: Science and Systems (RSS), 2024. [15] Linyi Huang, Hui Zhang, Zijian Wu, Sammy Christen, and Jie Song. FunGrasp: Functional Grasping for Diverse arXiv preprint arXiv:2411.16755, Dexterous Hands. 2024. [16] Jemin Hwangbo, Joonho Lee, and Marco Hutter. Percontact iteration method for solving contact dynamics. Robotics and Automation Letters (RA-L), 2018. [17] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 2019. [18] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning. arXiv preprint arXiv:2410.24185, 2024. [19] Felix Jonathan. python UR5 ikSolver. [20] Moslem Kazemi, Jean-Sebastien Valois, J. Andrew Bagnell, and Nancy Pollard. Robust Object Grasping using Force Compliant Motion Primitives. Robotics: Science and Systems (RSS), 2012. [21] Seungsu Kim, Ashwini Shukla, and Aude Billard. Catching Objects in Flight. Transactions on Robotics (T-RO), 2014. [22] Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo Seo, Georgios Pavlakos, and Yuke Zhu. OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation. Conference on Robot Learning (CoRL), 2024. [23] Kailin Li, Puhao Li, Tengyu Liu, Yuyang Li, and Siyuan Huang. ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning. In Computer Vision and Pattern Recognition (CVPR), 2025. [24] Miao Li, Kaiyu Hang, Danica Kragic, and Aude Billard. Dexterous grasping under shape uncertainty. Robotics and Autonomous Systems, 2016. [25] Tengyu Liu, Zeyu Liu, Ziyuan Jiao, Yixin Zhu, and SongChun Zhu. Synthesizing diverse and physically stable grasps with arbitrary hand structures using differentiable force closure estimator. Robotics and Automation Letters (RA-L), 2021. [26] Tyler Ga Wei Lum, Martin Matak, Viktor Makoviychuk, Ankur Handa, Arthur Allshire, Tucker Hermans, Nathan D. Ratliff, and Karl Van Wyk. DextrAH-G: Pixels-to-Action Dexterous Arm-Hand Grasping with In Conference on Robot Learning Geometric Fabrics. (CoRL), 2024. [27] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science Robotics, 2022. [28] A.T. Miller and P.K. Allen. Graspit! versatile simulator IEEE Robotics & Automation for robotic grasping. Magazine, 2004. [29] Martin Pfanne, Maxime Chalon, Freek Stulp, Helge Ritter, and Alin Albu-Schaffer. Object-Level Impedance Control for Dexterous In-Hand Manipulation. Robotics and Automation Letters (RA-L), 2020. [30] Steffen Puhlmann, Jason Harris, and Oliver Brock. RBO Hand 3: Platform for Soft Dexterous Manipulation. Transactions on Robotics (T-RO), 2022. [31] Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Hao Su, and Xiaolong Wang. DexPoint: Generalizable Point Cloud Reinforcement Learning for Sim-to-Real Dexterous Manipulation. Conference on Robot Learning (CoRL), 2022. [32] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. Robotics: Science and Systems (RSS), 2018. [33] Maximo A. Roa and Raul Suarez. Grasp quality measures: review and performance. Auton. Robots, 2015. [34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [35] Ritvik Singh, Arthur Allshire, Ankur Handa, Nathan Ratliff, and Karl Van Wyk. DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands. arXiv preprint arXiv:2412.01791, 2024. [36] Taro Takahashi, Toshimitsu Tsuboi, Takeo Kishida, Yasunori Kawanami, Satoru Shimizu, Masatsugu Iribe, Tetsuharu Fukushima, and Masahiro Fujita. Adaptive grasping by multi fingered hand with tactile sensor based on robust force and position control, 2008. [37] Universal Robots. UR5. https://www.universal-robots. com/products/ur5-robot/. [38] Weikang Wan, Haoran Geng, Yun Liu, Zikang Shan, Yaodong Yang, Li Yi, and He Wang. UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-Aware Curriculum and Iterative GeneralistIn International Conference on Specialist Learning. Computer Vision (ICCV), 2023. [39] Ruicheng Wang, Jialiang Zhang, Jiayi Chen, Yinzhen Xu, Puhao Li, Tengyu Liu, and He Wang. DexGraspNet: Large-Scale Robotic Dexterous Grasp Dataset for International General Objects Based on Simulation. Conference on Robotics and Automation (ICRA), 2023. [40] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects. In Computer Vision and Pattern Recognition (CVPR), 2024. [41] Wonik Robotics. Allegro robot hand. https://www. wonikrobotics.com/robot-hand. [42] Shiqi Yang, Minghuan Liu, Yuzhe Qin, Ding Runyu, Li Jialong, Xuxin Cheng, Ruihan Yang, Sha Yi, and Xiaolong Wang. ACE: Cross-platfrom VisualExoskeletons for Low-Cost Dexterous Teleoperation. arXiv preprint arXiv:2408.11805, 2024. [43] Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe Qin, and Xiaolong Wang. Learning continuous grasping function with dexterous hand from human demonstrations. Robotics and Automation Letters (RA-L), 2023. [44] Haoqi Yuan, Bohan Zhou, Yuhui Fu, and Zongqing Lu. Cross-Embodiment Dexterous Grasping with Reinforcement Learning. arXiv preprint arXiv:2410.02479, 2024. [45] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple In Robotics: Science and Systems 3D Representations. (RSS), 2024. [46] Hui Zhang, Sammy Christen, Zicong Fan, Otmar Hilliges, and Jie Song. GraspXL: Generating Grasping In European Motions for Diverse Objects at Scale. Conference on Computer Vision (ECCV), 2024. [47] Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, and Otmar Hilliges. ArtiGrasp: Physically Plausible Synthesis of Bi-Manual In International Dexterous Grasping and Articulation. Conference on 3D Vision (3DV), 2024. [48] Jialiang Zhang, Haoran Liu, Danshi Li, Xinqiang Yu, Haoran Geng, Yufei Ding, Jiayi Chen, and He Wang. DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes. Conference on Robot Learning (CoRL), 2024. [49] Lei Zhang, Kaixin Bai, Guowen Huang, Zhaopeng Chen, and Jianwei Zhang. Multi-fingered Robotic Hand Grasping in Cluttered Environments through HandarXiv preprint object Contact Semantic Mapping. arXiv:2404.08844, 2024. [50] Shuqi Zhao, Xinghao Zhu, Yuxin Chen, Chenran Li, Xiang Zhang, Mingyu Ding, and Masayoshi Tomizuka. DexH2R: Task-oriented Dexterous Manipulation from arXiv preprint arXiv:2411.04428, Human to Robots. 2024."
        },
        {
            "title": "APPENDIX",
            "content": "Weights Value A. Implementation Details 1) Object Point Cloud Extraction: To obtain the singleview object point clouds from the RGBD camera output, we filter out all the points captured from the camera that are lower than the table height. With the object depth information, we calculate the object point cloud by averaging the objects depth over 60 frames to minimize noise and remove outliers from the corresponding point clouds, which takes 1.28 on average. 2) Domain Randomization: As explained in the main paper, we randomize the environment parameters when training the student policy for robust sim-to-real transfer. The randomized parameters are listed in Tab. VIII. wd (fingertip) wd (the other hand links) wcd (fingertip) wcd (the other hand links) wf (fingertip) wf (the other hand links) wcu wf wh wo wl wq wre wact λ 2.0 0.5 6.0 1.5 4.0 1.0 -1.0 -0.5 -1.0 -15.0 -5.0 -1.0 1.0 1.0 1.0 - iter num/ Variable Randomization TABLE X: Weights of the Reward Function Friction Coefficient Hand Gain Hand Gain Arm Gain Arm Gain Hand Joint Angles Arm Joint Angles Arm/Hand Link Position Arm/Hand Link Orientation {0.5, 0.6, 0.7, 0.8,0.9} [0.9, 1.1] * 600 [0.9, 1.1] * 20 [0.5, 1.05] * 1.6e4 [0.5, 1.05] * 600 [-0.02, +0.02]rad + GT [-0.005, +0.005]rad + GT [-0.01, +0.01]m + GT [-0.02, +0.02]rad + GT TABLE VIII: Domain randomization parameters. 3) Training Parameters: We use PPO [34] to train our policy. An overview of important parameters and reward function weights are provided in Tab. IX and Tab. X. Hyperparameters PPO Epochs Steps per epoch Environment steps per episode Batch size Updates per epoch Simulation timestep Simulation steps per action Discount factor γ Max. gradient norm Value loss coefficient Entropy coefficient Hidden units Hidden layers Value 1.5e4 70 63 2000 20 0.01s 20 0.996 0.5 0.5 0.0 128 2 TABLE IX: Hyperparameters 4) Initial Hand Joint Angles: To get partially opened hands, the initial hand joint angles are set to be q0 = [0.2, 0.6, 0.2, 0.5, 0.2, 0.6, 0.2, 0.5, 0.2, 0.6, 0.2, 0.5, 1.3, 0.0, -0.1, 0.2] B. Experiment Details 1) Large Scale Object Details: In simulation, we utilize the Objaverse [12] objects processed to graspable sizes by [46], which contain different scales: small, medium, and large. Furthermore, as [46] focuses on grasping motion generation without hand-table collision, we filter out the objects that are not feasible to be grasped from the table (height < 2cm or width > 17cm), leading to 247,786 objects in total. For hardware experiments, we choose 512 objects from 12 categories as shown in the main paper, which contains objects with various shapes, materials, masses, and sizes. The details of each category are listed in Tab. XI. 2) Baselines: As explained in the main paper, we add table-collision loss to DexGraspNet for fair comparison. Specifically, Given the object pose and corresponding table surface with height htable, we add loss term with the formulation ltable = (cid:80)N i=0 htable hi2 Ihtable>hi, where hi is the height of the ith hand joints and is the joint number (including virtual joints for fingertips). We show the objects used in our experiment for the comparison with SpringGrasp [5] and the original objects used in their paper in Fig. 8. The geometries of the objects are either the same or highly similar. As both their and our methods use only depth information, the object textures make not much difference to the performance (our objects are even more challenging due to the reflecting surface, transparent material, or similar color with the background, which leads to more noisy depth observation). Fig. 8: Objects used in our comparison experiment with SpringGrasp [5] (a) and in their original experiment (b). Category Num. Mass (g) Scale (mm) Material Picnic Models Building Blocks Fruit & Vegetable Models Tool Models Animal Models Toy Cars Wooden Models Snacks Bottles & Boxes Real Tools Deformable Objects Other Daily Objects 41 54 35 16 18 16 78 38 67 50 31 68 9-257 10-180 7-196 20-270 26-165 40-117 25-218 22-570 15-550 16-610 8-142 19-454 50*50*40 - 280*170*100 40*30*30 - 200*200*100 70*50*50 - 330*80*80 70*40*15 - 400*140*130 100*50*30 - 230*120*100 90*35*30 - 110*100*70 50*30*30 - 170*90*90 60*40*40 - 350*130*70 35*35*30 - 240*170*55 40*40*30 - 130*100*90 70*50*50 - 220*180*90 40*30*30 - 270*130*100 Plastic Plastic/Styrofoam Plastic/Styrofoam/Rubber Plastic Rubber Plastic Wood - Plastic/Glass/Paper Metal/Plastic/Rubber Rubber/Cloth/Sponge/Styrofoam - Total 7-610 35*30*15 - 400*200*130 - TABLE XI: Physical Attributes of Real Objects"
        }
    ],
    "affiliations": [
        "ETH Zurich, Switzerland",
        "HKUST (Guangzhou), China",
        "HKUST, Hong Kong (China)"
    ]
}