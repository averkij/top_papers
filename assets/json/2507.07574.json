{
    "paper_title": "Beyond the Linear Separability Ceiling",
    "authors": [
        "Enrico Vompa",
        "Tanel Tammet",
        "Mohit Vaishnav"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by the linear separabilty of their visual embeddings on abstract reasoning tasks. This work investigates this \"linear reasoning bottleneck\" by introducing the Linear Separability Ceiling (LSC), the performance of a simple linear classifier on a VLM's visual embeddings. We find this bottleneck is widespread and stems not from poor perception, but from failures in the language model's reasoning pathways. We demonstrate this is a solvable alignment issue. The required intervention, however, is task-dependent: activating existing pathways suffices for semantic concepts, while complex relational reasoning requires adapting core model weights. Using postfix tuning as a methodological control, we find strong evidence for powerful, dormant reasoning pathways within VLMs. However, for complex relational tasks requiring deeper adaptation, explicitly improving representation quality causes the model to fail on new prompt formats despite its embeddings remaining well separated. Ultimately, this work provides a new lens for VLM analysis, showing that robust reasoning is a matter of targeted alignment, not simply improved representation learning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 7 5 7 0 . 7 0 5 2 : r a"
        },
        {
            "title": "BEYOND THE LINEAR SEPARABILITY CEILING",
            "content": "Enrico Vompa, Tanel Tammet, and Mohit Vaishnav Applied Artificial Intelligence Group, Tallinn University of Technology, Estonia {envomp, tanel.tammet, mohit.vaishnav}@taltech.ee"
        },
        {
            "title": "ABSTRACT",
            "content": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by the linear separabilty of their visual embeddings on abstract reasoning tasks. This work investigates this linear reasoning bottleneck by introducing the Linear Separability Ceiling (LSC), the performance of simple linear classifier on VLMs visual embeddings. We find this bottleneck is widespread and stems not from poor perception, but from failures in the language models reasoning pathways. We demonstrate this is solvable alignment issue. The required intervention, however, is task-dependent: activating existing pathways suffices for semantic concepts, while complex relational reasoning requires adapting core model weights. Using postfix tuning as methodological control, we find strong evidence for powerful, dormant reasoning pathways within VLMs. However, for complex relational tasks requiring deeper adaptation, explicitly improving representation quality causes the model to fail on new prompt formats despite its embeddings remaining well separated. Ultimately, this work provides new lens for VLM analysis, showing that robust reasoning is matter of targeted alignment, not simply improved representation learning."
        },
        {
            "title": "INTRODUCTION",
            "content": "A critical challenge in advancing state-of-the-art Visual-Language Models (VLMs) (Radford et al., 2021) lies in understanding the root of their frequent failures on abstract tasks. Is the bottleneck poor visual perception or flawed reasoning? This unresolved question creates persistent gap between machine and human cognition, particularly on visual puzzles (WÃ¼st et al., 2024) and tasks that VLMs can solve in text but not in visual formats (Park et al., 2025). While previous studies have explored this perception-reasoning interface, they have not definitively pinpointed the primary cause of failure (Vaishnav & Tammet, 2025; Lin et al., 2025). To address this ambiguity, this paper introduces diagnostic framework centered on the Linear Separability Ceiling (LSC): measure of the maximum performance linear classifier can achieve on VLMs raw visual embeddings. The LSC provides clear baseline for the quality of the initial visual representations, establishing benchmark that the models end-to-end reasoning must surpass to demonstrate added value. Our focus on computational non-linearity is complemented by recent work on representational geometry by Engels et al. (2025) that challenges the Linear Representation Hypothesis (LRH). They find that concepts like days of the week and months of the year form irreducible, circular features in activation space. This representational non-linearity implies need for the computational non-linearity we investigate; as they show, models use these exact circular features to perform tasks involving modular arithmetic. Their work thus offers concrete, mechanistic example of the structured features our hypothesized reasoning pathways may operate on. Applying this framework reveals our central finding: widespread \"linear reasoning bottleneck,\" where the generative performance of most leading VLMs is statistically equivalent or lower than the LSC on their own visual embeddings. The few models that surpass this ceiling do so via two distinct pathways: either by refining embeddings into more linearly separable format or by applying effective non-linear reasoning. 1 The discovery that reasoning, not perception, is often the main bottleneck motivates our investigation into unlocking this latent potential. We show the reasoning bottleneck is solvable alignment issue. Through targeted parameter-efficient fine-tuning (PEFT) (Hu et al., 2022), we enable models to significantly surpass their initial LSC. The success of postfix tuning (Li & Liang, 2021; Lester et al., 2021), methodological control that cannot alter visual representations, provides strong evidence that VLMs possess powerful, dormant reasoning pathways."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Abstract visual reasoning in VLMs. The evaluation of Vision-Language Models (VLMs) has evolved through series of increasingly complex benchmarks. Foundational benchmarks focused on single-image understanding, including image captioning (Lin et al., 2014), Optical Character Recognition (OCR) (Mishra et al., 2019), and Visual Question Answering (VQA) (Hudson & Manning, 2019; Goyal et al., 2019). To assess multi-image understanding with language Meng et al. (2025) developed the Multimodal Multi-Image Understanding (MMIU) dataset. To evaluate core visual understanding beyond language, researchers use purely visual reasoning benchmarks that require identifying abstract concepts and relations. This category includes Bongard tasks (Bongard, 1970), which we focus on. But also Ravens Progressive Matrices (RPMs) (Zhang et al., 2019), Visual Progressive Matrices (V-PROM) (Teney et al., 2020), Compositional Visual Reasoning (CVR) (Zerroug et al., 2022), and the Abstract Reasoning Corpus (ARC) (Chollet, 2019; Unsal & Akkus, 2025). VLM architectures and modality fusion. key architectural differentiator in VLMs is the strategy for fusing modalities (Gadzicki et al., 2020; Shukor et al., 2025). While early-fusion models like Chameleon (Team, 2024) create joint representation from raw inputs, the now-dominant latefusion approach first processes images with dedicated vision encoder. In its most common form, exemplified by LLaVA (Liu et al., 2023), the resulting visual embeddings are mapped into the language models space via simple projection layer. more deeply integrated variant is crossattention fusion, pioneered by Flamingo (Alayrac et al., 2022) and BLIP (Li et al., 2022), which insert cross-attention layers within the LLM, allowing textual tokens to dynamically attend to visual features throughout the model for more iterative fusion (Lin et al., 2021). Our framework is designed for late-fusion models where initial visual representation can be isolated. Mechanistic interpretability aims to reveal the internal workings of neural networks, explaining how specific outputs are generated. Common approaches include probing, which uses simple auxiliary classifiers on intermediate outputs to identify encoded information; logit lens analysis, which examines layer-by-layer prediction changes; and analysis of sparse autoencoders or attention mechanisms. (Liu et al., 2025a; Golovanevsky et al., 2025) Our framework applies the probing paradigm to assess how concept separability evolves across the models internal representations. Linear representation hypothesis (LRH) posits that concepts within neural networks are represented by linear structures in activation space. This is commonly observed as compositional linearity, where the embeddings of composite concepts can be decomposed into the sum of their constituent parts (Trager et al., 2023). This principle appears to be fundamental aspect of transformer decoders, where even the transformations between sequential layers exhibit near-perfect linearity (Razzhigaev et al., 2024). Theoretical work supports these empirical findings, suggesting that the next-token prediction objective inherently biases models towards learning representations that are linear transformations of latent concepts (Liu et al., 2025b). Representation alignment and the modality gap research has identified critical bottlenecks that can occur either in the initial visual encoding or in how the language model component subsequently extracts and utilizes these visual features for complex reasoning tasks (Chia et al., 2024; Zhang et al., 2024). Compounding these challenges is the modality gap, widely observed phenomenon where the learned representations for different modalities exhibit distinct structures or occupy separate regions within the joint embedding space, even when they are semantically aligned (Yaras et al., 2022). This systematic separation, particularly noted in contrastively trained models, can hinder cross-modal alignment and the seamless integration of information (Qiu et al., 2024), potentially impacting the discriminative power of these representations. Our framework allows us to quantitatively assess the impact of such issues by establishing the linear separability ceiling as baseline. This concept of representational performance ceiling is not new; for instance, prior work has identified contrastive learning ceiling in semantic textual similarity (Zhang & Li, 2024). Training and adapting vision encoders in VLMs are typically pre-trained with methods such as contrastive objectives (Chen et al., 2020), exemplified by CLIP (Radford et al., 2021), or combined contrastive and generative objectives during pretraining, characteristic of CoCa (Yu et al., 2022). Building on this, some approaches experiment with contrastive losses to augment NTP to align VLMs (Ouali et al., 2025; Wu et al., 2025; Ak et al., 2024). To efficiently implement such strategies, PEFT methods like Low-Rank Adaptation (LoRA) (Hu et al., 2022) and prompt tuning (Li & Liang, 2021; Zhou et al., 2022) are widely adopted for efficiently adapting VLMs (Lester et al., 2021; Jahan et al., 2025). We investigate whether strategies that explicitly co-optimize for enhanced representational discriminability alongside generative accuracy enable VLMs to better leverage their internal visual representations."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Our analysis centers on abstract visual reasoning using bongard-style tasks, which require model to infer rule from positive and negative examples to classify query image. representative example of this task structure is shown in Figure 1. Figure 1: An example of the bongard HOI task. Model must infer common rule, in this case, \"a person is performing jump on motorcycle,\" from the positive examples and determine if the query image follows this rule, which the negative examples do not. Datasets. We conduct experiments on two such datasets: Bongard OpenWorld (Wu et al., 2024) and Bongard Human-Object-Interaction (HOI) (Jiang et al., 2022). The OpenWorld dataset was divided into train (500), validation (100), and test (500) sample splits, ensuring distinct semantic components across them. For HOI, we utilized its original splits, where 4000 balanced samples across concepts were chosen from train split. Its validation (4 100) and test sets (4 200) balanced samples, which are categorized by concept novelty: seen/unseen object/action. Each sample consisted of 6 positive images, 6 negative images and 1 query image. Models. Baseline performance was established on multiple SoTA VLMs: Phi 3.5 vision 4.2B (Microsoft, 2024), Pixtral 12B (MistralAI, 2024), Gemma3 4B and 27B (Google, 2025), InternVL3 14B (Chen et al., 2024), and Qwen 2.5 VL 7B and 72B (Alibaba, 2025). Subsequently, we applied PEFT to Gemma3 4B, Phi and Pixtral. Evaluation. For all fine-tuning experiments, we saved model checkpoints every epoch and selected the one with the best performance on the validation set. Baselines and best-performing checkpoints were evaluated on their respective test splits. Models demonstrate consistent performance across all conceptual novelty splits (see Appendix I), so we report the average for brevity."
        },
        {
            "title": "4 A FRAMEWORK FOR DECOMPOSING VLM REASONING",
            "content": "To diagnose whether VLM failures on abstract reasoning tasks stem from flawed perception or reasoning, we introduce diagnostic framework centered on non-parametric linear probe. This framework assesses the discriminability of models visual embeddings, independent of its generative process, allowing us to disentangle the contributions of representation quality from non-linear reasoning. First, we benchmark performance across various prompt structures, including interleaved vs. labeled image presentation and direct vs. Chain-of-Thought (CoT) prompting (see Appendix A)."
        },
        {
            "title": "4.1 METHODOLOGY",
            "content": "We first extract multi-token image embeddings at two key stages: the vision stage, which captures initial representations from the vision encoder before contextualization, and the final stage, which captures contextualized representations from the LLMs final hidden state. We aggregate these sequences into single vectors, vi, using mean pooling and L2 normalization. While other methods like max pooling were explored, mean pooling is an effective, computationally efficient approximation of slerp (Reimers & Gurevych, 2019), which preserves the angular properties measured by cosine similarity (Shoemake, 1985; Karras et al., 2019). To quantify the discriminative quality of these embeddings, we probe them using nearest-centroid classifier. We selected this nearest-centroid strategy as it proved more performant than nearest-neighbor approach because it mitigates the influence of outlier examples as demonstrated in Appendix (Hastie et al., 2009). We then compute prototype vectors for the positive (P ) and negative (N ) example sets by averaging their respective image vectors (e.g., cP = mean(vp1 , . . . , vpk )). query image (Q) is then classified based on its cosine similarity to these centroids (i.e., comparing cos(vQ, cP ) to cos(vQ, cN )). 4.2 DECOMPOSITION OF PERFORMANCE Our framework dissects VLM performance by statistically comparing three key accuracies (Figure 2): the end-to-end generative accuracy (accgen), the linear probe accuracy on initial vision embeddings (accvision), and on final contextualized embeddings (accf inal). Grounded in the linear representation hypothesis, which posits that concepts are encoded as linear structures within models activation space, we define accvision as the linear separability ceiling (LSC). The LSC quantifies the baseline discriminative power of the visual representations, primary goal of contrastive pre-training. While we recognize this linear assumption is simplification that may not capture all complex concept geometries, it establishes the benchmark model must surpass to demonstrate effective non-linear reasoning. We also note that accuracy can also be misleading under class imbalance, as the embeddings discriminative power often exhibits class-dependent variability (Appendix K). Figure 2: Flowchart illustrating the derivation of the accvision (LSC), accf inal, and accgen metrics. We classify model performance based on statistical comparison (non-overlapping 95% confidence intervals) between accgen and the LSC. Generative performance is surpassed if accgen is superior to the LSC, linear reasoning bottleneck if accgen is statistically indistinguishable or lower than LSC. For models that surpass the LSC, we diagnose the mechanism by comparing accgen to accf inal: if they are indistinguishable, success is due to non-linear representation refinement; if accgen is superior, it points to post-representation non-linear reasoning. Chi-squared (Ï2) test assesses the statistical dependence between the generative model and our linear probes."
        },
        {
            "title": "5 LINEAR REASONING BOTTLENECK",
            "content": "Our framework, applied to 8 SoTA VLMs across two datasets and tested with 8 prompt formats each, reveals widespread linear reasoning bottleneck. As shown in Figure 3, models generative performance (accgen) struggles to surpass their LSC. Full results are in Appendix C. Figure 3: generative accuracy vs. linear separability for all baseline models. The diagonal dashed line is the LSC benchmark (y = x). Points above the line indicate that models generative performance has surpassed its LSC. The plot shows that most evaluations struggle to consistently surpass the benchmark, demonstrating prevalent linear reasoning bottleneck. Pathways to success and failure. However, some models surpass the LSC, revealing distinct success pathways. For instance, Pixtral 12B and Qwen2.5-VL 72B consistently achieve generative performance statistically superior to their LSC. In contrast, other models like Gemma3 27B fail to do so reliably. Table 1 highlights the dramatically different mechanisms behind these outcomes. Table 1: Performance comparison on the OpenWorld dataset, revealing two primary reasoning pathways. The first, used by Pixtral, increases the linear separability of representations (accf inal > LSC). The second is post-representation pathway, where Gemma3 27B represents an extreme case operating on embeddings with no linear separability (accf inal 50%), while Qwen2.5-VL 72B is an intermediate case with only partial drop. Model Direct acc (%) CoT acc (%) LSC (accvision, %) Pixtral 12B Gemma3 27B Qwen2.5-VL 72B 79.4 3.5 93.2 2.2 93.6 2.1 84.2 3.2 87.2 2.9 93.2 2.2 76.0 3.7 88.6 2.8 86.8 3.0 Final rep. (accf inal, %) 88.0 2.8 50.0 74.8 3.8 Our analysis reveals two pathways to surpass the LSC. The first, non-linear representation refinement, is exemplified by Pixtral, which transforms its initial embeddings into more linearly separable format (accf inal > LSC). The process suggests that next-token prediction itself can function as representation learner, enhancing separability in way that shares deep theoretical underpinnings with contrastive learning (Alshammari et al., 2025). While its final generative accuracy is not direct readout of these embeddings, its success is characterized by strong statistical alignment between its generative output and the classifications from linear probe on these improved final representations. 5 The second observed route is post-representation non-linear reasoning pathway, where the LLM remaps the initial visual embeddings into more complex representational geometry (accf inal < LSC). Gemma3 27B is an extreme example of this, where its final embeddings collapse to chancelevel separability (accf inal 50%) and its performance is brittle across different prompts. In contrast, Qwen2.5-VL 72B provides stable and highly effective implementation of this pathway. Although its final representation accuracy also drops relative to its LSC, the model achieves consistent, SoTA performance. This suggests Qwen successfully leverages sophisticated non-linear reasoning mechanism that can operate on these less-separable embeddings. This pathway highlights that high LSC is not sufficient on its own; success depends on robust reasoning process that can navigate non-linear representational spaces. The disconnect between separability and generation. Generally, better linear separability in models vision embeddings (LSC) correlates with better generative performance. Ï2 test shows statistically significant dependence between the LSC probe and generative performance in 97 out of 128 instances with it being an inverse dependency only for 2 instances. deeper look, however, reveals complex disconnect between embedding separability and generative output. While models processing attunes its final representations to its generative logic (significant in 103 out of 128 instances), the relationship is often counterintuitive, with 29 instances showing significant inverse dependence. The models reasoning can therefore be disconnected from, or even opposed to, the logic of linear probe on the same embeddings. While high final separability is not prerequisite for success, this suggests hypothesis for creating more robust and interpretable models: train them to improve the linear discriminability of their final embeddings while ensuring that structure is positively aligned with the generative process."
        },
        {
            "title": "6 SURPASSING THE LSC",
            "content": "Our baseline analysis in Section 5 revealed widespread linear reasoning bottleneck where VLMs struggle to surpass their LSC. Through targeted fine-tuning, we systematically probe intervention points within the VLM, from the vision-language interface to the LLMs reasoning pathways, to pinpoint and resolve the bottleneck. 6.1 EXPERIMENTAL SETUP To pinpoint the most effective intervention points, we employ several parameter-efficient fine-tuning (PEFT) and minimal tuning methods. Each is trained with two distinct objectives to assess its impact on generative accuracy and representation quality. Full hyperparameters for all methods are detailed in Appendix and Appendix F. - Interface adaption: To test if the LSC can be surpassed by minor adjustments at the modality interface, we tune isolated components: either the vision-language projection layer or an additive vision bias vector on initial patch embeddings. - Prompt tuning: By adding trainable soft prompts to the input sequence while keeping the VLMs core weights frozen, we test whether activating dormant reasoning pathways is enough when provided with an optimal input prompt. - LoRA: We adapt the model by injecting trainable, low-rank matrices into the attention layers of both the vision encoder and language model. This evaluates how adapting core weights can surpass the linear reasoning bottleneck and improve abstract reasoning. Objectives. We train the PEFT parameters using two objectives. Next-token prediction (LNT), the standard language modeling objective of maximizing the likelihood of the ground-truth output tokens. And InfoNCE + NTP (Lcombined) objective, which augments LNT with an explicit contrastive loss (Lsim) by means of weighted sum. Lcombined = wnLNT + wcLsim This contrastive term explicitly encourages the final embeddings of query image to be closer to its true category centroid than the alternative. The motivation is to directly improve the linear separability of the final representations, thereby raising the separability ceiling for the task while co-optimizing for generative correctness, which is further detailed in Appendix E. 6 Evaluation. To evaluate structural robustness, we test performance on unseen prompt formats. The in-distribution (ID) evaluation uses the standard interleaved prompt structure seen during training. For the out-of-distribution (OOD) test, we use labeled prompt structure that groups all images by category at the end of the prompt. This OOD setup challenges how well the optimization targets generalize beyond the syntactic structure on which they were trained. Furthermore, OpenWorld test set consists entirely of new concepts unseen in train set, verifying conceptual generation."
        },
        {
            "title": "6.2 PINPOINTING THE LOCATION FOR INTERVENTION",
            "content": "To pinpoint the source of performance gains, we conducted targeted ablation study on the Phi model with the OpenWorld dataset. The results, summarized in Table 2, reveal that the location of the intervention is the critical factor in surpassing the linear reasoning bottleneck. formats. interventions Structural overfitting. Minimalist at the vision-language interface, such as vision bias and projector tuning, lead to structural overfitting rather than robust reasoning, as seen in Table 2. They fail to generalize to new Projector prompt tunings performance is entirely dependent on prompt syntax, and vision bias tuning succeeds only by degrading the visual representations (lowering the LSC). Since these interface-level adjustments encourage superficial learning, we discarded them and froze the projector for all subsequent experiments. Table 2: Reported accuracies for the Phi model on the OpenWorld dataset. The ID evaluation uses the interleaved prompt structure seen during training. For the OOD test, we use labeled prompt structure that groups all images by category at the end of the prompt. In the generative columns, bold indicates the performance is not significantly lower than the LSC, while an underline indicates the LSC is surpassed. Tuning method ID (%) OOD (%) LSC (%) 59.0 Direct baseline Vision bias (LNT) 75.0 Vision projector (LNT) 90.2 94.2 Postfix (LNT) 90.4 Prompt (LNT) 89.8 LoRA (LNT) 89.8 LoRA (LNT llm-only) 79.4 68.4 73.8 90.2 83.4 88.6 89.0 84.0 73.8 84.0 84.2 84.2 84.2 84.2 The bottleneck is reasoning, not perception. Our results pinpoint the bottleneck to the LLMs reasoning pathways, not the vision encoders perception. An ablation study confirmed this (Appendix G), applying LoRA to the vision encoder yielded no performance gain over adapting the LLM alone. This suggests any adaptations within the vision encoder are form of naÃ¯ve loss minimization (Prieto et al., 2025), merely scaling existing features rather than learning more discriminative ones. The core issue is therefore not perceptual deficit but deeper failure in how the LLM processes its visual inputs. 6.3 ACTIVATION VS. ADAPTATION With the bottleneck identified in the LLMs reasoning module, we find that targeted interventions can robustly unlock latent abilities. Our findings (Appendix H) reveal that the required method for unlocking these pathways depends on the reasoning task, highlighting distinction between two approaches: activating latent skills versus adapting core weights. The success of postfix tuning, methodological control that cannot alter visual representations because its learnable tokens are placed after the images, provides conclusive evidence that VLMs possess powerful, dormant reasoning pathways. For semantic concepts (OpenWorld), performance was an activation issue; prompt tuning is as effective as LoRA, suggesting that the models inherent skills only need to be steered by an optimal input, process similar to prompt engineering (Burns et al., 2023; Brown et al., 2020). For complex relational reasoning (HOI) activation was insufficient. Prompt tuning struggles where LoRA succeeds, demonstrating that these harder tasks require fundamental adaptation of the models internal weights. This reveals core principle for these models: activating existing skills suffices for simple concepts, but unlocking relational reasoning demands deeper model adaptation."
        },
        {
            "title": "6.4 COMPARING OBJECTIVES",
            "content": "Table 3 consolidates the performance of different PEFT strategies, comparing their generative and embedding similarity-based classification accuracy against relevant baselines. Table 3: Summary of PEFT performance on Bongard tasks. Reported accuracies presented as accuracy E. In the generative columns, bold indicates the generative performance is not significantly lower than the LSC, while an underline indicates the LSC is surpassed. Superscripts on separability scores denote significant dependence (p < 0.05) with the predictions of the corresponding generative method (G). Dataset Method Generative (%) LSC (%) Repr. acc. final (%) Model Phi OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Pixtral OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Gemma3 4B OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) 59.0 4.3 89.8 2.7 96.2 1.7 52.1 3.5 77.4 2.9 78.9 2.8 72.4 3.9 93.4 2.2 95.0 1. 57.8 3.4 78.0 2.9 79.6 2.8 76.0 3.7 92.4 2.3 95.6 1.8 56.5 3.4 84.2 2.5 84.2 2.5 84.0 3.2G 84.2 3.2G 84.2 3.2G 71.9 3.1 71.1 3.1G 71.8 3.1G 76.0 3.7G 76.6 3.7 74.4 3.8 62.7 3.4G 61.6 3.4G 63.1 3.3G 89.8 2.7 89.8 2.7G 89.8 2.7 74.1 3.0 74.1 3.0G 74.1 3.0G 76.4 3.7G 74.0 3.8 93.2 2.2G 60.5 3.4G 61.1 3.4G 79.4 2.8G 87.2 2.9G 87.2 2.9G 96.2 1.7G 70.2 3.2G 74.9 3.0G 77.8 2.9G 50.0 4.4G 50.0 4.4 96.6 1.6G 50.0 4.4G 50.0 4.4 83.2 2.6G Generative performance surpassing separability ceiling. Fine-tuning with the LNT objective allows generative accuracy to significantly surpass the LSC  (Table 3)  . This occurs because the generative model can implement complex, non-linear decision boundary that our linear separability probe cannot. The LSC is therefore not an absolute performance limit but benchmark for linear reasoning, which models can learn to bypass by activating more sophisticated classification pathways. Modality alignment and raising the separability ceiling. While the LNT objective fosters postrepresentation non-linear reasoning pathways that bypass the LSC, the combined objective (Lcombined) works by directly improving the embedding space itself through non-linear representation refinement. Its contrastive loss term (Lsim) explicitly optimizes the final representations to be more linearly separable, which raises the representation accuracy and aligns the generative process with this improved structure. This alignment is confirmed by the consistent, significant dependence between generative outcomes and final-layer similarity predictions for all LoRA models trained with Lcombined  (Table 3)  , consistency the LNT objective fails to achieve. This convergence of generative and linear probe accuracy indicates the performance bottleneck shifts from reasoning back to perception. The primary barrier is now the quality of the visual representations themselves. Given that applying LoRA to adapt the vision encoders feature extraction process yielded no improvement to the LSC, pointing to more fundamental limitation. This suggests the remaining perception issue is likely architectural and cannot be resolved by further training with the methods and objectives explored in this study. Both objectives offer largely equivalent performance on relational HOI task, with Lcombined being better on semantic OpenWorld task. For visual analysis of how each objective alters the models internal attention mechanisms to produce these outcomes, see Appendix M."
        },
        {
            "title": "6.5 MEASURING GENERALIZATION",
            "content": "Conceptual Generalization. The performance gains from fine-tuning stem from genuine conceptual understanding rather than memorization. This is demonstrated on the Bongard-HOI dataset, where our models show robust generalization by maintaining high accuracy on test splits with entirely unseen objects and actions (Appendix I). In fact, top models often achieve their peak performance on these most challenging splits. This finding is reinforced by strong results on the OpenWorld test set, which consists entirely of new concepts by design. Structural generalization. Beyond conceptual understanding, we evaluated for structural generalization by testing model robustness to changes in prompt format (Appendix H). We found critical trade-off: while the standard LNT objective yields structurally robust models, the Lcombined objective, which improves representation quality, can be brittle. This fragility is evident on the HOI dataset, where models trained with the combined objective fail on out-of-distribution (OOD) prompts despite producing highly separable final-layer representations. This indicates the generative pathway has overfit to the prompts syntax, failing to interpret its own well-structured embeddings in new format. The superior OOD performance of postfix tuning over methods that include prefix further confirms this sensitivity to prompt structure. Generalization under domain shift. rigorous cross-domain evaluation revealed that successful generalization depends on aligning the fine-tuning objective with the models intrinsic reasoning strategy (Appendix J). For models like Pixtral and Phi, which naturally excel at representation refinement, the Lcombined objective, which explicitly enhances representations, proved most resilient. Remarkably, when trained on OpenWorld, Pixtrals generative performance on HOI (71.0%) also surpassed its LSC (63.1%), showing that its learned skill had successfully transferred. Conversely, for Gemma3, which relies on post-representation pathway, the standard LNT objective provided more robust (though still limited) generalization. This highlights that true generalization is not achieved by single best method, but by matching the intervention to the models architectural strengths."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This work demonstrates that the reasoning failures of VLMs in abstract visual tasks are not an issue of poor visual perception but solvable misalignment in the subsequent reasoning process, revealing that these models possess far greater latent reasoning capabilities than their out-of-the-box performance suggests. Our analysis reveals that the linear separability ceiling, measure of the discriminative power of models embeddings to linear classifier, is best understood not as an absolute performance limit, but as an observational ceiling that baseline models consistently fail to surpass. The linear reasoning bottleneck, defined by this ceiling, is therefore prevalent but surmountable alignment issue. We show there is no single path to success; performance can be unlocked either by refining the models existing post-representation reasoning pathways to bypass the ceiling, or by restructuring its representations to make them more linearly separable and thus raise the ceiling itself. Our key contributions include: (1) novel diagnostic framework centered on the linear separability ceiling, which formalizes and quantifies the linear reasoning bottleneck in visual tasks; (2) an empirical demonstration that this bottleneck stems not from fundamental capability deficit but from an alignment failure, proving that VLMs possess powerful dormant reasoning pathways, finding supported by the success of postfix tuning which activates these pathways without altering the original visual representations; (3) demonstration of the shifting performance bottleneck, showing that after successfully closing the linear reasoning bottleneck, the primary limitation shifts back to the quality of the visual representations themselves, making perception the next frontier for improvement; and (4) the discovery that objectives designed to improve representation quality can introduce structural brittleness, revealing an often overlooked trade-off between performance and structural robustness. This framework opens several avenues for future work, from developing training methods that avoid the trade-off between performance and structural robustness, to using the LSC as live diagnostic during model development. Ultimately, our work provides both validated methods for enhancing VLM reasoning and novel insights into the representational properties of multimodal models, paving the way for more capable and interpretable AI."
        },
        {
            "title": "REFERENCES",
            "content": "Kenan Emir Ak, Jay Mohta, Dimitris Dimitriadis, Saurav Manchanda, Yan Xu, and Mingwei Shen. Aligning vision language models with contrastive learning. ECCV Workshop on Unlearning and Model Editing, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, MikoÅ aj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and KarÃ©n Simonyan. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. Alibaba. Qwen2.5-vl technical report. arXiv, 02 2025. Shaden Naif Alshammari, John Hershey, Axel Feldmann, William Freeman, and Mark Hamilton. I-con: unifying framework for representation learning. ICLR, 2025. Bongard. Pattern recognition(book on automatic recognition of visual patterns and mechanization of creativity, presenting computer program termed arithmetic). 1970. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS, 2020. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. ICLR, 2023. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. ICML, 2020. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CVPR, 2024. doi: 10.48550/arXiv.2312.14238. Yew Ken Chia, Vernon Toh, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. ACL, 2024. FranÃ§ois Chollet. On the measure of intelligence. arXiv, 11 2019. Joshua Engels, Eric Michaud, Isaac Liao, Wes Gurnee, and Max Tegmark. Not all language model features are one-dimensionally linear. ICLR, 2025. Konrad Gadzicki, Razieh Khamsehashari, and Christoph Zetzsche. Early vs late fusion in multimodal convolutional neural networks. IEEE, 2020. Michal Golovanevsky, William Rudman, Vedant Palit, Carsten Eickhoff, and Ritambhara Singh. What do vlms notice? mechanistic interpretability pipeline for gaussian-noise-free text-image corruption and evaluation. NAACL, 2025. Google. Gemma 3 technical report. arXiv, 03 2025. Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. CVPR, 2019. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical learning, Second Edition : Data mining, inference, and Prediction. Springer, 2 edition, 2009. 10 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 2022. Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. CVPR, 2019. Prottasha Nusrat Jahan, Upama Roy Chowdhury, Shetu Mohanto, Tasfia Nuzhat, Abdullah As Sami, Md Shamol Ali, Shohanur Islam, Hafijur Raman, Md Kowsher, and Ozlem Ozmen Garibay. Peft a2z: Parameter-efficient fine-tuning survey for large language and vision models. arXiv, 2025. Huaizu Jiang, Xiaojian Ma, Weili Nie, Zhiding Yu, Yuke Zhu, and Anima Anandkumar. Bongard-hoi: Benchmarking few-shot visual reasoning for human-object interactions. CVPR, 2022. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. CVPR, 2019. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. EMNLP, 2021. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. ICML, 2022. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. ACL-IJCNLP, pp. 45824597, 2021. doi: 10.18653/v1/2021.acl-long.353. Hezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Qing Song, and Wei Yuan. Cat: Cross attention in vision transformer. ICCV, 2021. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and Lawrence Zitnick. Microsoft coco: Common objects in context. ECCV, 2014. Zihao Lin, Samyadeep Basu, Mohammad Beigi, Varun Manjunatha, Ryan Rossi, Zichao Wang, Yufan Zhou, Sriram Balasubramanian, Arman Zarei, Keivan Rezaei, Ying Shen, Barry Menglong Yao, Zhiyang Xu, Qin Liu, Yuxiang Zhang, Yan Sun, Shilong Liu, Li Shen, Hongxuan Li, Soheil Feizi, and Lifu Huang. survey on mechanistic interpretability for multi-modal foundation models. CoRR, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. Yiming Liu, Yuhui Zhang, and Serena Yeung-Levy. Mechanistic interpretability meets vision language models: Insights and limitations. ICLR Blogposts, 2025a. Yuhang Liu, Dong Gong, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton , and Javen Qinfeng Shi. predict therefore am: Is next token prediction enough to learn humaninterpretable concepts from data? arXiv, 03 2025b. Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. ICLR, 2025. Microsoft. Phi-3 technical report: highly capable language model locally on your phone. arXiv, 04 2024. doi: 10.48550/arXiv.2404.14219. Microsoft. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv, 03 2025. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. ICDAR, 2019. MistralAI. Pixtral 12b. arXiv, 10 2024. Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Georgios Tzimiropoulos, and Brais Martinez. Vladva: Discriminative fine-tuning of lvlms. CVPR, 2025. 11 Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Generalizing from simple to hard visual reasoning: Can we mitigate modality imbalance in vlms? ICML, 2025. Lucas Prieto, Melih Barsbey, Pedro , and Tolga Birdal. Grokking at the edge of numerical stability. ICLR, 2025. Longtian Qiu, Shan Ning, and Xuming He. Mining fine-grained image-text alignment for zero-shot captioning via text-only training. AAAI, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. ICML, 2021. Nikhil Raghuraman, Adam Harley, and Leonidas Guibas. Support-set context matters for bongard problems. TMLR, 2024. Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Nikolai Gerasimenko, Ivan Oseledets, Denis Dimitrov, and Andrey Kuznetsov. Your transformer is secretly linear. ACL, 2024. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. EMNLP-IJCNLP, 2019. Ken Shoemake. Animating rotation with quaternion curves. Proceedings of the 12th annual conference on Computer graphics and interactive techniques - SIGGRAPH 85, 1985. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. NeurIPS, 2022. Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, and Alaaeldin El-Nouby. Scaling laws for native multimodal models. arXiv, 04 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv, 05 2024. doi: 10.48550/arXiv.2405.09818. Damien Teney, Peng Wang, Jiewei Cao, Lingqiao Liu, Chunhua Shen, and Anton van den Hengel. V-prom: benchmark for visual reasoning using visual progressive matrices. AAAI, 2020. Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, and Stefano Soatto. Linear spaces of meanings: Compositional structures in vision-language models. ICCV, 2023. Mert Unsal and Aylin Akkus. Easyarc: Evaluating vision language models on true visual reasoning. CVPR, 2025. Mohit Vaishnav and Tanel Tammet. cognitive paradigm approach to probe the perception-reasoning interface in vlms. arXiv, 05 2025. Rujie Wu, Xiaojian Ma, Qing Li, Wei Wang, Zhenliang Zhang, Song-Chun Zhu, and Yizhou Wang. Bongard-openworld: Few-shot reasoning for free-form visual concepts in the real world. ICLR, 2024. Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, and Nick Haber. Symmetrical visual contrastive optimization: Aligning vision-language models with minimal contrastive images. ACL, 2025. Antonia WÃ¼st, Tim Tobiasch, Lukas Helff, Devendra Singh Dhami, Constantin Rothkopf, and Kristian Kersting. Bongard in wonderland: Visual puzzles that still make ai go mad? Sys2Reasoning, 2024. Can Yaras, Siyi Chen, Peng Wang, and Qing Qu. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. NeurIPS, 2022. 12 Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. TMLR, 2022. doi: 10.48550/ arXiv.2205.01917. Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, and Thomas Serre. benchmark for compositional visual reasoning. NeurIPS, 2022. Bowen Zhang and Chunping Li. Pcc-tuning: Breaking the contrastive learning ceiling in semantic textual similarity. EMNLP, 2024. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: dataset for relational and analogical visual reasoning. CVPR, 2019. Gengyuan Zhang, Yurui Zhang, Kerui Zhang, and Volker Tresp. Can vision-language models be good guesser? exploring vlms for times and location reasoning. WACV, 2024. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. IJCV, 2022."
        },
        {
            "title": "A BASELINE PROMPT DEFINITIONS",
            "content": "Strategies evaluated in Table 5 varied primarily in how images were presented relative to descriptive text, the position of the query image, and whether CoT reasoning was asked. Interleaved strategy. You are presented bongard task. There are {image_count} pictures total. First {cat_imgs} samples belong to cat_2, which follow 1 common rule. Here they are: {cat2_imgs}. Following {cat_imgs} distinctly do not follow that rule and are cat_1. Here they are: {cat1_imgs}. Last image is query image you need to categorize either as cat_2 or cat_1 based on the rule, which is here: {query_img}. If it follows the rule, its cat_2. If it doesnt follow the rule, its cat_1. Interleaved query first strategy. You are presented bongard task. There are {image_count} pictures total. First image is query image you need to categorize either as cat_2 or cat_1 based on the rule. Here is query image: {query_img}. {cat_imgs} samples belong to cat_2, which follow 1 common rule. Here they are: {cat2_imgs} . Following {cat_imgs} distinctly do not follow that rule and are cat_1. Here they are: {cat1_imgs} . If query image follows the rule, its cat_2. If it doesnt follow the rule, its cat_1. Labeled strategy. You are presented bongard task. There are {image_count} pictures total. First {cat_imgs} samples belong to cat_2, which follow 1 common rule. Following {cat_imgs} distinctly do not follow that rule and are cat_1. Last image is query image you need to categorize either as cat_2 or cat_1 based on the rule. If query image follows the rule, its cat_2. If it doesnt follow the rule, its cat_1. Here are the cat2 images: {cat2_imgs} , cat1 images: {cat1_imgs} , query image: {query_img} . Labeled query first strategy. You are presented bongard task. There are {image_count} pictures total. First image is query image you need to categorize either as cat_2 or cat_1 based on the rule. Following {cat_imgs} samples belong to cat_2, which follow 1 common rule. Last {cat_imgs} distinctly do not follow that rule and are cat_1. If query image follows the rule, its cat_2. If it doesnt follow the rule, its cat_1. Here is the query image: {query_img} , cat2 images: {cat2_imgs} , cat1 images: {cat1_imgs}. One of the following string variables is appended to the output of the core prompt. Direct conclusion prompt string. Your task is to: 1. Provide your conclusion for the query image if it can be categorized as either cat_1 or cat_2 based on the analysis and the rule. The format of your output should be as follows: Conclusion: cat_1 or cat_2 Conclusion should be 1 category only without extra symbols! CoT prompt string. Your task is to: 1. Determine the rule or criterion that distinguishes the cat_2 samples from the cat_1 ones. 2. Analyse the query image (last image). 3. Provide your conclusion for the query image if it can be categorized as either cat_1 or cat_2 based on the analysis and the rule. Ensure that the output is clear, well-formatted, and free of unnecessary explanations. The format of your output should be as follows: Analysis: (Your analysis here) Rule: (The distinguishing rule here) query image: (query image details) Conclusion: cat_1 or cat_2 Conclusion should be 1 category only without extra symbols!"
        },
        {
            "title": "B SINGLE VS BATCHED CONTEXT",
            "content": "We performed preliminary analysis comparing nearest-centroid classifier approach against simpler nearest-neighbor classifier. This allows us to demonstrate that aggregating visual information into conceptual prototype is more effective than treating each piece of visual evidence in isolation. We refer to these two distinct classification strategies as: Single context, nearest-neighbor approach where the query image is classified based on its cosine similarity to the single closest example image vector from the combined positive and negative sets. This method is sensitive to the features of individual, and potentially outlier, examples; Batched context, nearest-centroid approach selected for the LSC. The query image is classified based on its cosine similarity to prototype vector (centroid) representing the entire set of positive or negative examples. This method is designed to capture the underlying abstract rule of the set. The results of this comparison are presented in Table 4. Table 4: Comparison of classification accuracy for single vs. batched context."
        },
        {
            "title": "Context",
            "content": "Sim. acc (vision, %) Sim. acc (final, %)"
        },
        {
            "title": "Model",
            "content": "Phi OpenWorld HOI Pixtral OpenWorld HOI Gemma3 4B OpenWorld HOI Gemma3 27B OpenWorld HOI InternVL 14B OpenWorld HOI Qwen2.5-VL 7B OpenWorld HOI Qwen2.5-VL 72B OpenWorld HOI Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched Single Batched 78.8 3.6 84.0 3.2 64.9 3.3 71.9 3. 70.4 4.0 76.0 3.7 62.4 3.4 62.9 3.3 77.8 3.6 89.6 2.7 69.1 3.2 74.1 3.0 78.2 3.6 88.6 2.8 68.4 3.2 73.4 3. 69.4 4.0 70.2 4.0 63.7 3.3 65.1 3.3 81.0 3.4 88.2 2.8 69.1 3.2 72.1 3.1 79.0 3.6 87.0 2.9 69.2 3.2 72.0 3. 78.8 3.6 87.0 2.9 66.4 3.3 69.6 3.2 78.6 3.6 88.2 2.8 68.0 3.2 72.4 3.1 81.6 3.4 76.6 3.7 69.8 3.2 70.2 3. 78.4 3.6 78.8 3.6 69.8 3.2 64.8 3.3 74.0 3.8 78.4 3.6 64.4 3.3 68.0 3.2 75.4 3.8 81.2 3.4 66.8 3.3 65.9 3. 71.2 4.0 70.2 4.0 60.6 3.4 58.5 3.4 The data indicates that the batched context method provides significantly more accurate and stable measure of linear separability. By averaging embeddings to create conceptual prototype, the classifier becomes more resilient to individual outlier examples and better reflects the abstract visual rule. For this reason, all LSC scores reported in the main body of this paper are calculated using this batched context methodology."
        },
        {
            "title": "C SEPARABILITY CEILING IN VLMS",
            "content": "The Table 5 (next page) presents our baseline results, comparing each models generative accuracy to its LSC (accvision) and its final representation accuracy (accf inal). While generative performance is not lower than their LSC in 84 out of 128 instances, only handful of models manage to surpass this ceiling. This success is achieved through two distinct mechanisms. In 4 instances, models like Pixtral use non-linear representation refinement, where the models processing increases the linear separability of the embeddings (accf inal > LSC). More commonly, in 10 instances, models like Qwen-VL-72B employ post-representation non-linear pathway, where generative performance exceeds the LSC despite decrease in the linear separability of the final embeddings (accf inal < LSC). Ï2 test highlights complex relationship between embedding separability and generative output. The initial vision embeddings (LSC) show statistically significant dependence on generative performance in 97 out of 128 instances, with only 2 of those being an inverse relationship. This suggests that the initial visual features are generally, and logically, aligned with the final output. However, the connection becomes more complicated after the models reasoning process. While the final embeddings have significant dependence on the output in 103 instances, surprising 29 of these are an inverse dependence. This underlines the disconnect between simple linear discriminability and the complex, non-linear reasoning happening within these models. 16 Table 5: Generative performance vs. the LSC. Superscripts on embedding scores denote significant (p<0.05) dependence (negative indicating inverse) with generative predictions (D=Direct, C=CoT). Model Phi Dataset Prompt Strategy Direct acc (%) CoT acc (%) OpenWorld HOI Pixtral OpenWorld HOI Gemma3 4B OpenWorld HOI Gemma3 27B OpenWorld HOI InternVL 14B OpenWorld HOI InternVL3 78B OpenWorld HOI Qwen2.5-VL 7B OpenWorld HOI Qwen2.5-VL 72B OpenWorld HOI 80.6 3.5 68.0 4.1 78.8 3.6 64.0 4.2 59.9 3.4 55.5 3.4 57.4 3.4 55.8 3.4 83.8 3.2 81.0 3.4 84.2 3.2 76.0 3.7 66.9 3.3 67.2 3.3 66.6 3.3 65.4 3. 80.2 3.5 51.4 4.4 83.0 3.3 52.4 4.4 62.6 3.4 50.7 3.5 63.9 3.3 51.7 3.5 88.6 2.8 71.4 4.0 87.2 2.9 67.2 4.1 75.2 3.0 65.0 3.3 75.1 3.0 59.6 3.4 80.6 3.5 70.4 4.0 71.8 3.9 73.0 3.9 63.6 3.3 60.4 3.4 58.8 3.4 62.7 3. 69.9 4.1 79.4 3.8 67.9 6.7 76.6 4.8 62.1 3.4 62.0 3.4 60.4 3.7 63.0 3.5 84.6 3.2 78.2 3.6 87.2 2.9 80.6 3.5 67.4 3.2 60.8 3.4 68.4 3.2 63.0 3.3 89.2 2.7 91.8 2.4 93.2 2.2 92.2 2.4 78.5 2.8 77.2 2.9 79.2 2.8 77.5 2. Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first Interleaved Interleaved query first Labeled Labeled query first 59.0 4.3 52.4 4.4 79.4 3.5 57.2 4.3 52.1 3.5 51.5 3.5 58.8 3.4 55.1 3. 72.4 3.9 71.2 4.0 79.4 3.5 62.4 4.2 57.8 3.4 53.5 3.5 60.6 3.4 53.4 3.5 76.0 3.7 50.2 4.4 68.6 4.1 52.0 4.4 56.5 3.4 50.4 3.5 54.2 3.5 50.7 3.5 91.8 2.4 60.6 4.3 93.2 2.2 54.6 4.4 75.1 3.0 54.4 3.5 76.2 2.9 52.2 3. 79.4 3.5 80.4 3.5 55.8 4.4 66.6 4.1 58.2 3.4 64.2 3.3 52.0 3.5 56.5 3.4 73.0 3.9 88.4 2.8 64.4 4.2 84.6 3.2 59.0 3.4 66.1 3.3 54.0 3.5 66.5 3.3 86.4 3.0 51.2 4.4 92.2 2.4 54.8 4.4 63.0 3.3 50.4 3.5 70.4 3.2 50.6 3. 93.6 2.1 91.6 2.4 93.6 2.1 91.8 2.4 79.9 2.8 77.1 2.9 80.2 2.8 76.0 3.0 17 Sim. acc (vision, %) 84.0 3.2D 84.0 3.2D 84.0 3.2 84.0 3.2D 71.9 3.1C 71.9 3.1D,C 71.9 3.1C 71.9 3.1D Sim. acc (final, %) 76.4 3.7D 66.0 4.2D,C 78.2 3.6 65.0 4.2D,C 60.5 3.4D 56.5 3.4D,C 60.6 3.4C 56.1 3.4D,C 76.0 3.7D,C 75.8 3.8D,C 76.0 3.7D 75.8 3.8D,C 62.7 3.4D,C 63.2 3.3C 62.7 3.4D,C 63.2 3.3C 89.8 2.7C 89.8 2.7D,C 89.8 2.7C 89.8 2.7D,C 74.1 3.0C 74.1 3.0D,C 74.1 3.0C 74.1 3.0 88.6 2.8D,C 88.6 2.8D,C 88.6 2.8D 88.6 2.8D,C 73.4 3.1D,C 73.4 3.1D,C 73.4 3.1D,C 73.4 3.1D,C 70.2 4.0 70.2 4.0 70.2 4.0D 70.2 4.0D,C 65.1 3.3D,C 65.1 3.3D,C 65.1 3.3D 65.1 3.3D,C 75.6 3.8C 75.6 3.8D,C 75.6 3.8 75.6 3.8D,C 66.1 3.3D,C 66.1 3.3D,C 66.1 3.3D 66.1 3.3D 87.8 2.9D,C 87.8 2.9D,C 87.8 2.9D,C 87.8 2.9D,C 72.2 3.1D,C 72.2 3.1C 72.2 3.1D,C 72.2 3.1C 86.8 3.0D,C 86.8 3.0D,C 86.8 3.0D,C 86.8 3.0D,C 72.1 3.1D,C 72.1 3.1D,C 72.1 3.1D,C 72.1 3.1D,C 87.2 2.9D,C 76.2 3.7C 88.0 2.8D,C 75.0 3.8D,C 70.2 3.2D,C 62.6 3.4D,C 70.5 3.2C 61.1 3.4D,C 50.0D,C 50.0D,C 50.0D 50.0D,C 50.0D,C 50.0D,C 50.0D,C 50.0D,C 50.0D,C 50.0D,C 50.0C 50.0D,C 50.0D,C 50.0D,C 50.0C 50.0D,C 61.6 4.3 59.4 4.3C 62.4 4.2D 54.0 4.4D,C 52.0 3.5D 52.8 3.5D,C 53.1 3.5D 51.5 3.5D,C 72.4 3.9C 69.8 4.0C 73.4 3.9D 67.2 4.1D,C 57.6 3.4C 58.6 3.4D,C 60.4 3.4D,C 57.1 3.4D,C 58.6 4.3D,C 54.0 4.4D,C 57.0 4.3C 54.8 4.4D,C 52.2 3.5D,C 51.1 3.5D,C 50.5 3.5D,C 51.7 3.5D,C 59.0 4.3 68.2 4.1D,C 74.8 3.8D,C 65.2 4.2C 53.9 3.5C 59.4 3.4D,C 63.7 3.3D,C 56.2 3.4D,C"
        },
        {
            "title": "D HYPERPARAMETER DETAILS",
            "content": "LoRA and projector tuning employed learning rate of 1e4 and batch size of 25. The LoRA experiments utilized rank of = 8 and scaling factor of Î± = 8. These hyperparameters are consistent with established practices for fine-tuning VLMs and training LoRA adapters (Hu et al., 2022; Liu et al., 2023; Microsoft, 2025). Prompt tuning method reported in the main paper employs hybrid approach, prepending and appending learnable soft prompt embeddings to the input sequence concurrently. The final structure consists of prefix prompt, 100 learnable embeddings prepended to the standard interleaved prompt. And instructional postfix prompt, fixed, human-readable instruction designed to encourage metalearning, followed by 100 learnable embeddings. The fixed instruction text is: Also construct specific instruction which you think will help the most for this kind of task. Think of an algorithm to follow, what kind of components to look for on the images, how they could be combined to follow some rule. Be creative! :D Return the classification before anything else and then the the specific instruction word-by-word and nothing else! Here is the specific instruction which helps you lot: Postfix tuning method employs postfix part of prompt tuning only. Hyperparameter search revealed that an aggressive configuration with small batch size (2) and high learning rate (0.01), which we refer to as the fast configuration, was highly effective for vision bias vector and postfix tuning. Prompt tuning uses the default learning rate of 1e4 and batch size of 25."
        },
        {
            "title": "E COMBINED LOSS DETAILS",
            "content": "Inspired by the effectiveness of similarity measures in assessing embedding quality (Section 4), we explored an additional objective for tuning. Instead of relying solely on the standard next-token prediction loss (LNT), we incorporated similarity-based contrastive loss (Lsim) designed to directly enhance the discriminability of the final embeddings with respect to the task structure during tuning. Specifically, let VT be the normalized mean-pooled embedding vector derived from the final hidden states corresponding to the query image . Similarly, let CP and CN be the normalized meanpooled centroids derived from the concatenated final hidden states of all images in the positive set = {p1, ..., pk} and negative set = {n1, ..., nk}, respectively. These embeddings are obtained after the model processes the full input sequence, including the tunable soft prompt tokens. We calculate the cosine similarities between the query image embedding and the set centroids: sP = VT CP and sN = VT CN . The similarity-based loss, Lsim, is then formulated as cross-entropy loss over these similarities, akin to InfoNCE, encouraging the query image embedding to be closer to the centroid of its true category set: Lsim = CrossEntropy([ sP Ï , sN Ï ], y) where Ï (fixed to 0.07) is temperature hyperparameter scaling the logits, and is the target label (y = 0 if the ground truth category for is the positive set , and = 1 if it is the negative set ). This contrastive loss term was combined with the standard next-token prediction loss, weighted by hyperparameters wn and wc: Lcombined = wnLNT + wcLsim The gradients from this combined loss Lcombined were used to update the parameters. This approach directly optimizes final embeddings to be well-clustered according to the Bongard tasks positive/negative distinction, complementing the language modeling objective."
        },
        {
            "title": "F COMBINED LOSS HYPERPARAMETER SEARCH",
            "content": "To find hyperparameters for Lcombined = wnLNT + wcLsim, we fixed the wn to 1 and scanned through wc values with different schedules. Constant schedule had same wc throughout different epochs, which proved unstable for larger values. Linear schedule had target value where every epoch the wc = 1+cur_epoch total_epochs target_value. Using linear schedule, scan was conducted over [0.1, 0.2, 0.4, 0.8, 1.6, 2.4, 3.2] target values. For Phi and Gemma3 the best value was 0.4 and for Pixtral best value was 1.6, which is shown on Figure 4. Figure 4: Pixtral contrastive loss hyperparameter scan Afterwards, target values were tested with different learning rates and since they proved stable, we picked these values. However, as seen with target value of 1.6 with learning rate 0.0001, the accuracy peaked quite early on. To address this early peaking and ensure more controlled dynamic where the contrastive loss provides an initial strong signal that gradually yields to the NTP objective for final model refinement, we introduced cosine learning schedule which has two separate schedules for wn and wc, governed by the overall training progress. Let = current_step+1 represent the fraction of training completion, where current_step is the current training iteration (e.g., batch or epoch, 0-indexed) and total_steps is the total number of such iterations for the run. total_steps The weight for the next-token prediction loss, wn, is designed to smoothly increase, following the formula: wn(p) = 1 cos (cid:17) (cid:16) Ï 2 This schedule ensures that the next-token prediction (NTP) objective becomes increasingly dominant as training nears completion, aligning with the goal for NTP to be the primary objective by the end of the run. Concurrently, the weight for the contrastive loss, wc, is determined by separate schedule: wc(p) = cos (cid:17) (cid:16) Ï 2 min (2p, 1) Here, represents constant scaling factor (0.4 for Phi and Gemma3 and 1.6 for Pixtral). This formulation for wc incorporates two dynamic components based on the training progress p. cosine decay term, cos (cid:0) Ï 2 p(cid:1), which smoothly transitions the base weight from 1 down to 0. linear ramp-up term, min (2 p, 1), which effectively scales the influence of the cosine term. This ramp-up is active during the first half of the training duration, remaining at 1 for the second half. The product of these terms, further scaled by C, allows the contrastive loss to have an initial warm-up period and then gradually diminishing influence as the training increasingly prioritizes the NTP objective towards its conclusion. Both prompt tuning and LoRA training was conducted using cosinel schedule. For prompt tuning progress was calculated using epochs and for LoRA, progress was calculated using batches."
        },
        {
            "title": "G LORA ON VISION ENCODER",
            "content": "To further investigate the claim that the primary performance bottleneck resides within the LLMs reasoning pathways, we conducted an ablation study. We compare the final generative accuracy of our standard approach, where LoRA is applied to both the vision encoder and language model, against configuration where same LoRA is applied only to the language models attention layers. This analysis was performed across all three models, both datasets, and both training objectives to ensure the robustness of our findings. Table 6: Ablation of Vision Encoder LoRA. This table compares the final generative accuracy (%) when LoRA is applied to both the vision encoder and LLM versus only the LLM. The comparison is shown for both the standard next-token prediction (LNT) and the combined contrastive (Lcombined) objectives. The results show no statistically significant performance difference across these configurations."
        },
        {
            "title": "Method",
            "content": "Phi OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Pixtral OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Gemma3 4B OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Both (VE + LLM) 59.0 4.3 89.8 2.7 96.2 1.7 52.1 3.5 77.4 2.9 78.9 2.8 72.4 3.9 93.4 2.2 95.0 1.9 57.8 3.4 78.0 2.9 79.6 2.8 76.0 3.7 92.4 2.3 95.6 1.8 56.5 3.4 84.2 2.5 84.2 2. LLM-only 59.0 4.3 89.8 2.7 96.6 1.6 52.1 3.5 77.2 2.9 78.2 2.9 72.4 3.9 93.6 2.1 94.8 1.9 57.8 3.4 78.9 2.8 79.5 2.8 76.0 3.7 92.4 2.3 95.6 1. 56.5 3.4 84.2 2.5 84.2 2.5 The results presented in Table 6 are remarkably consistent. In every tested scenario, there is no statistically significant performance difference between applying LoRA adapters to the LLM alone versus applying them to both the vision encoder and the LLM. This provides evidence that the LoRA adapters on the vision encoder are redundant for improving final task performance. It confirms that the gains from fine-tuning are not derived from adapting the initial feature extraction process, but rather from unlocking and refining the latent reasoning capabilities within the language model. This holds true even for the Lcombined objective, demonstrating that co-adapting the vision encoder is unnecessary for the LLM to learn how to structure its final representations more effectively."
        },
        {
            "title": "H PEFT COMPARISON",
            "content": "To understand how to best surpass the linear reasoning bottleneck, we compared two PEFT methods: prompt tuning, which seeks to activate latent abilities by finding an optimal input, and LoRA, which adapts the models core weights. We evaluated these methods using two distinct training objectives: the standard next-token prediction loss (LNT) and combined objective that includes contrastive loss (Lcombined) to explicitly improve representation quality. To evaluate structural robustness, we test performance on unseen prompt formats. The in-distribution (ID) evaluation uses the standard interleaved prompt structure seen during training. For the out-of-distribution (OOD) test, we use labeled prompt structure that groups all images by category at the end of the prompt. This OOD setup challenges how well the optimization targets generalize beyond the syntactic structure on which they were trained. Table 7: Comparison of PEFT methods. The in-distribution (ID) evaluation uses the standard interleaved prompt structure seen during training. For the out-of-distribution (OOD) test, we use labeled prompt structure that groups all images by category at the end of the prompt. In the generative columns,bold indicates the performance is not significantly lower than the corresponding LSC, while an underline indicates the ceiling is surpassed. Superscripts on embedding classification scores indicate statistically significant (p < 0.05) dependence between the item-level predictions of the similarity-based classifier and generative method: for the ID method and for the OOD method. Default configurations use batch size of 25 and learning rate of 1e4. The fast configurations use batch size of 2 and learning rate of 0.01. Model Dataset Method Phi OpenWorld Direct baseline HOI Postfix tuning (LNT) Prompt tuning (LNT) Prompt tuning (Lcombined) LoRA (LNT) LoRA (Lcombined) Direct baseline Postfix tuning (LNT) Prompt tuning (LNT) Prompt tuning (Lcombined) LoRA (LNT) LoRA (Lcombined) Pixtral OpenWorld Direct baseline HOI Postfix tuning (LNT) Prompt tuning (LNT) Prompt tuning (Lcombined) LoRA (LNT) LoRA (Lcombined) Direct baseline Postfix tuning (LNT) Prompt tuning (LNT) Prompt tuning (Lcombined) LoRA (LNT) LoRA (Lcombined) Gemma3 4B OpenWorld Direct baseline HOI Postfix tuning (LNT) Prompt tuning (LNT) Prompt tuning (Lcombined) LoRA (LNT) LoRA (Lcombined) Direct baseline Postfix tuning (LNT) Prompt tuning (LNT) Prompt tuning (Lcombined) LoRA (LNT) LoRA (Lcombined) Generative (ID, %) Generative (OOD, %) 59.0 4.3 94.2 2.0 90.4 2.6 94.4 2.0 89.8 2.7 96.2 1.7 52.1 3.5 63.2 3.3 58.5 3.4 65.4 3.3 77.4 2.9 78.9 2.8 72.4 3.9 93.6 2.1 93.6 2.1 94.4 2.0 93.4 2.2 95.0 1. 57.8 3.4 66.4 3.3 58.6 3.4 70.8 3.2 78.0 2.9 79.6 2.8 76.0 3.7 86.0 3.0 86.0 3.0 82.2 3.4 92.4 2.3 95.6 1.8 56.5 3.4 56.1 3.4 54.1 3.5 58.9 3.4 84.2 2.5 84.2 2.5 79.4 3.5 90.2 2.6 83.4 3.3 86.0 3.0 88.6 2.8 94.2 2.0 58.8 3.4 59.1 3.4 52.6 3.5 62.1 3.4 71.9 3.1 71.2 3.1 79.4 3.5 93.6 2.1 79.2 3.8 88.2 2.8 90.0 2.6 94.6 2. 60.6 3.4 66.4 3.3 54.0 3.5 58.9 3.4 74.4 3.0 62.0 3.4 68.6 4.1 86.0 3.0 71.2 4.0 64.6 4.2 94.0 2.1 96.2 1.7 54.2 3.5 56.1 3.4 51.6 3.5 54.0 3.5 81.8 2.7 67.4 3.2 LSC (%) 84.0 3.2I 84.2 3.2 84.2 3.2 84.2 3.2I 84.2 3.2I 84.2 3.2I 71.9 3.1 71.9 3.1I,O 71.9 3.1I 71.9 3.1I 71.1 3.1I,O 71.8 3.1I,O 76.0 3.7I,O 76.6 3.7 76.6 3.7I 76.6 3.7 76.6 3.7 74.4 3.8 62.7 3.4I,O 62.7 3.4I,O 62.7 3.4I 62.7 3.4I 61.6 3.4I,O 63.1 3.3I 89.8 2.7 89.8 2.7 89.8 2.7I 89.8 2.7I 89.8 2.7I 89.8 2.7O 74.1 3.0 74.1 3.0 74.1 3.0O 74.1 3.0I 74.1 3.0I,O 74.1 3.0I Final rep. (ID, %) 76.4 3.7I 77.4 3.7I 74.6 3.8I 94.2 2.0I 74.0 3.8 93.2 2.2I 60.5 3.4I 60.9 3.4I 59.9 3.4I 68.8 3.2I 61.1 3.4I 79.4 2.8I 87.2 2.9I 87.6 2.9 78.4 3.6 95.0 1.9I 87.2 2.9I 96.2 1.7I 70.2 3.2I 71.0 3.1I 62.6 3.4I 73.5 3.1I 74.9 3.0I 77.8 2.9I 50.0 4.4I 50.0 4.4 50.0 4.4 50.8 4.4I 50.0 4.4 96.6 1.6I 50.0 3.5I 50.0 3.5I 50.0 3.5I 50.0 3.5I 50.0 3.5 83.2 2.6I Final rep. (OOD, %) 78.2 3.6 78.8 3.6O 76.6 3.7O 93.2 2.2 78.4 3.6O 91.4 2.5O 60.6 3.4 61.8 3.4O 61.9 3.4O 62.9 3.3O 63.1 3.3 79.4 2.8O 88.0 2.8O 87.6 2.9 83.0 3.3O 93.4 2.2O 89.6 2.7 95.4 1.8O 70.5 3.2 71.0 3.1O 63.1 3.3O 72.2 3.1O 73.2 3.1O 77.6 2.9O 50.0 4.4O 50.0 4.4 50.2 4.4O 50.4 4.4O 50.0 4.4 96.8 1.5O 50.0 3.5O 50.0 3.5O 50.0 3.5O 50.0 3.5O 50.0 3.5 82.1 2.7O Table 7 presents results from these explorations. Key finding here is that the performance gains achieved by the Lcombined objective for prompt tuning are, for the most part, not unique to that loss function. As shown, the fast configuration achieves generative accuracy that is highly comparable 22 to the main papers prompt tuning (Lcombined) baseline. This suggests that multiple pathways exist to surpass the LSC. While the Lcombined objective provides more robust method for aligning the embedding space, aggressive hyperparameter tuning with standard language modeling objective can be similarly effective at unlocking models latent reasoning capabilities. Also, LoRAs trained with the LNT objective demonstrate stronger structural generalization. Across all models and datasets, these versions largely maintain their performance when evaluated on the OOD prompt format, showing no statistically significant degradation. In contrast, LoRAs trained with the Lcombined objective sometimes exhibit structural brittleness. In half of the tested scenarios (specifically, all models on the HOI dataset), their generative performance drops significantly on the OOD prompt structure, even while the corresponding final-layer similarity evaluation remains high. similar, and even clearer, pattern emerges when comparing prompt-based tuning methods. Postfix tuning, which only appends learnable tokens, proves to be highly robust, maintaining its performance on OOD formats. In contrast, prompt tuning, which includes learnable prefix, is more susceptible to syntactic overfitting and shows more significant performance drop on OOD prompts. This reinforces that interventions at the start of the sequence are more likely to create dependency on prompt syntax than those that only modify the end. This suggests that while explicitly optimizing for representation quality is effective, it can inadvertently encourage the generative model to rely on structural shortcuts, harming its ability to generalize to new prompt formats."
        },
        {
            "title": "I RESULTS ACROSS HOI SPLITS",
            "content": "Table 8 provides detailed breakdown of model performance across the four distinct test splits of the Bongard-HOI dataset. These results are benchmarked against two state-of-the-art methods: Test-Time Prompt Tuning (TPT) (Shu et al., 2022), which adapts prompts for vision-language models at test time, and SVM-Mimic (Raghuraman et al., 2024), Transformer-based approach that learns from SVM-derived support sets. Table 8: Performance on HOI - detailed breakdown Model Method Objective Seen obj. Seen act. (%) Seen obj. Unseen act. (%) Unseen obj. Seen act. (%) Unseen obj. Unseen act. (%) Avg acc. (%) Phi Pixtral Baseline gen. Baseline sim. Prompt tuning gen. Prompt tuning sim. Prompt tuning gen. Lsim Prompt tuning sim. Lsim LNT LoRA gen. LNT LoRA sim. Lcombined LoRA gen. Lcombined LoRA sim. Baseline gen. Baseline sim. Prompt tuning gen. Prompt tuning sim. Prompt tuning gen. Lsim Prompt tuning sim. Lsim LNT LoRA gen. LNT LoRA sim. Lcombined LoRA gen. Lcombined LoRA sim. Gemma3 4B Baseline gen. Baseline sim. Prompt tuning gen. Prompt tuning sim. Prompt tuning gen. Lsim Prompt tuning sim. Lsim LNT LoRA gen. LNT LoRA sim. Lcombined LoRA gen. Lcombined LoRA sim. CLIP-RN50 TPT SVM-Mimic 51.5 61.5 60.5 61.5 66.5 63.5 78.0 63.5 78.0 80. 60.0 66.5 60.5 63.0 73.0 76.0 77.0 73.0 78.5 79.5 58.5 50.0 53.0 50.0 56.5 50.0 83.5 50.0 83.0 84.0 66.4 69.6 55.0 60.0 59.0 59.0 69.0 73.5 77.5 59.0 79.5 77.0 61.5 73.5 58.5 62.0 74.0 73.5 79.5 77.5 81.5 78.5 56.0 50.0 56.5 50.0 61.5 50.0 84.5 50.0 84.0 86. 68.5 70.8 51.0 57.5 52.5 57.0 62.5 70.5 75.0 60.0 75.5 76.5 51.5 69.5 56.5 62.5 64.0 70.0 75.5 71.0 74.5 74.5 53.0 50.0 53.0 50.0 59.5 50.0 85.0 50.0 85.0 82.5 66.0 78.1 51.0 63.0 62.0 62.0 63.5 67.5 79.0 62.0 82.5 83. 58.0 71.5 59.0 63.0 72.0 74.5 80.0 78.0 84.0 78.5 58.5 50.0 54.0 50.0 58.0 50.0 84.0 50.0 85.0 80.0 65.5 71.2 52.1 60.5 58.5 59.9 65.4 68.8 77.4 61.1 78.9 79.4 57.8 70.2 58.6 62.6 70.8 73.5 78.0 74.9 79.6 77.8 56.5 50.0 54.1 50.0 58.9 50.0 84.2 50.0 84.2 83. 66.6 72.5 All three models fine-tuned with LoRA significantly outperform not only their own baselines but also the previous state-of-the-art methods, TPT and SVM-Mimic. The most striking finding is the consistent high performance across all splits. There is no significant accuracy degradation when models are tested on unseen objects, unseen actions, or both simultaneously. In fact, for both Phi and Pixtral, the highest generative accuracy with the combined loss is achieved on the most difficult split (unseen obj. / unseen act.), providing strong evidence that the models are learning the abstract relational concept rather than memorizing training examples."
        },
        {
            "title": "J DOMAIN GENERALIZATION",
            "content": "In this evaluation, models were fine-tuned on one dataset (e.g., OpenWorld) and evaluated on the other (e.g., HOI) to assess their ability to generalize beyond the training domains specific concepts and structures. The dataset column in the table indicates the evaluation dataset, where the evaluation is done on the other dataset. Table 9: Cross-domain generalization performance. Reported accuracies presented as accuracy E. bold indicates surpassing of LSC. Superscripts on similarity scores denote significant dependence (p < 0.05) with the predictions of the corresponding generative method (G)."
        },
        {
            "title": "OpenWorld Direct baseline",
            "content": "HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Pixtral OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Gemma3 4B OpenWorld Direct baseline HOI LoRA (LNT) LoRA (Lcombined) Direct baseline LoRA (LNT) LoRA (Lcombined) Generative (OOD, %) LSC (%) 79.4 3.5 92.6 2.3 93.8 2.1 58.8 3.4 62.1 3.4 67.6 3.2 79.4 3.5 88.8 2.8 83.4 3.3 60.6 3.4 64.9 3.3 71.0 3.1 68.6 4.1 87.2 2.9 83.4 3. 54.2 3.5 69.0 3.2 65.8 3.3 84.0 3.2 84.2 3.2G 84.2 3.2G 71.9 3.1 71.1 3.1G 71.8 3.1G 76.0 3.7G 76.6 3.7G 74.4 3.8G 62.7 3.4G 61.6 3.4G 63.1 3.3G 89.8 2.7 89.8 2.7G 89.8 2.7 74.1 3.0 74.1 3.0G 74.1 3.0G Final rep. (OOD, %) 78.2 3.6 76.0 3.7G 87.8 2.9G 60.6 3.4 59.5 3.4G 66.9 3.3G 88.0 2.8G 89.0 2.7G 87.4 2.9G 70.5 3.2 72.5 3.1G 68.4 3.2G 50.0G 50.0 83.2 3.3G 50.0G 50.0 67.4 3.2G Across all models, key observation in cross-domain generalization is that the generative accuracy remains tightly coupled with the final representation classification accuracy when trained with the combined loss ((Lcombined). Conditional generalization of Phi. It successfully surpasses its LSC when trained on the broader HOI dataset and tested on OpenWorld, but this success is not bidirectional. Robust generalization of Pixtral. It consistently surpasses its LSC when transferring between domains when trained with Lcombined, succeeding via representation refinement. This ability to significantly improve performance in both transfer directions points to more fundamentally sound approach for generalizable reasoning."
        },
        {
            "title": "K CLASS IMBALANCE INVESTIGATION",
            "content": "Evaluating with unbalanced classes can lead to misleading estimate of the LSC. This can be most easily seen when we split the accuracy into two, positive and negative only ground labels, as done in Table 10. Table 10: Class imbalances across datasets and models Model Dataset Method Phi Openworld Direct (interleaved) Hoi LoRA (LNT) LoRA (Lcombined) Direct (interleaved) LoRA (LNT) LoRA (Lcombined) Pixtral Openworld Direct (interleaved) Hoi LoRA (LNT) LoRA (Lcombined) Direct (interleaved) LoRA (LNT) LoRA (Lcombined) Gemma3 4B Openworld Direct (interleaved) Hoi LoRA (LNT) LoRA (Lcombined) Direct (interleaved) LoRA (LNT) LoRA (Lcombined) gen. acc. pos. (%) gen. acc. neg. (%) gen. acc. avg. (%) LSC (pos., %) LSC (neg., %) LSC (avg., %) 75.2 91.2 94.4 97.5 76.2 82. 80.8 92.8 95.6 67.8 80.5 80.5 69.6 92.0 95.6 24.5 86.5 81.5 42.8 88.4 98.0 6.8 78.5 75. 64.0 94.0 94.4 47.8 75.5 78.8 82.4 92.8 95.6 88.5 82.0 87.0 59.0 89.8 96.2 52.1 77.4 78. 72.4 93.4 95.0 57.8 78.0 79.6 76.0 92.4 95.6 56.5 84.2 84.2 86.8 86.8 87.2 75.0 74.2 75. 78.4 79.2 76.4 64.8 62.5 67.5 92.4 92.4 92.4 77.5 77.5 77.5 81.2 81.6 81.2 68.8 68.0 68. 73.6 74.0 72.4 60.8 60.8 58.8 87.2 87.2 87.2 70.8 70.8 70.8 84.0 84.2 84.2 71.9 71.1 71. 76.0 76.6 74.4 62.7 61.6 63.1 89.8 89.8 89.8 74.1 74.1 74.1 The data in Table 10 demonstrates how average accuracies, whether generative or similarity-based, can mask significant performance disparities between classes. An aggregated LSC is composed of differing per-class separability scores (e.g., for Gemma3 on HOI, 77.5% for positive vs. 70.8% for negative). This class-dependent variability means the overall LSC may not fully capture the models representational quality for all distinct classes. Therefore, this detailed per-class separability analysis serves as valuable diagnostic tool. It effectively uncovers such representational biases that overall average metrics might otherwise conceal."
        },
        {
            "title": "L EXAMPLE SEMANTIC CONCEPTS",
            "content": "The Bongard OpenWorld dataset primarily utilizes semantic concepts based on objects, scenes, actions, or attributes: - Elderly person using cell phone. - Evening sunset on the desert dunes. - People playing water polo in the swimming pool. - People are in hurry. - boat tied with rope in the water. - player shooting on the hockey field. - cute dog wearing cozy sweater. - Solar panels on the house roof. - The shepherd herds flocks of sheep. - closeup of dandelion. - Vehicle tires (e.g., steel frame for car tires, motorcycle tires). - Chimney on the house roof. The HOI dataset utilizes semantic concepts based on human-object interactions: - person carrying surfboard. - person riding surfboard. - person holding an apple. - person swinging tennis racket. - Multiple people sitting on bench. - person brushing with toothbrush. - person adjusting or tying tie. - person using mouse. - person throwing frisbee. - person holding and about to eat an apple. - person peeling or cutting an apple. - person lying on bench."
        },
        {
            "title": "M VISUALIZING THE MECHANISM OF PEFT VIA ATTENTION MAPS",
            "content": "The quantitative results in the main paper demonstrate that PEFT can effectively bridge and surpass the LSC. To understand how these interventions alter the models internal computations, this appendix provides qualitative, mechanistic explanation by visualizing the models attention patterns before and after fine-tuning. Attention maps reveal the core mechanism of the transformer architecture, showing which parts of the input sequence (text and image tokens) are attended to when building updated representations. higher attention score (brighter color) indicates stronger influence. By comparing these maps, we can directly observe how fine-tuning reshapes the information flow within the model to solve the reasoning task, providing visual evidence that complements the quantitative analysis. M.1 DISTINCT ARCHITECTURAL SIGNATURES AND TUNING MECHANISMS Our analysis reveals that each model family possesses unique baseline attention strategy, which dictates the mechanism of improvement unlocked by fine-tuning. Gemma3: overcoming representational degradation As established in the main paper, Gemma3 models exhibit high LSC but suffer from severe representational degradation, where the final-layer embeddings (accf inal) become poorly separated. The attention maps in Figure 5 reveal the architectural reason and the solution. Gemma3 uses an efficient sliding window attention for the most part (narrow diagonal band) (layer 24 behaved differently for some reason). - With LNT tuning, the model learns the task but the attention mechanism is not visibly altered; it refines computations within its existing pathways. - With Lcombined tuning, the contrastive loss induces change that makes the models reasoning process visible. In the final layers, the attention map develops more structured, bright vertical stripes. Each well-defined stripe represents targeted read operation, where the model globally accesses the compressed representation of visual features. This structured, crossimage comparison provides clear mechanistic explanation for how the model overcomes its baseline shortcoming. The explicit contrastive pressure forces it to develop and execute this comparison strategy, repairing its degraded final representations and aligning with the recovery of accf inal seen in Table 3. (a) Gemma3 4B after LoRA tuning (LNT) (b) Gemma3 4B after LoRA tuning (Lcombined) Figure 5: Attention maps for Gemma3 4B. The contrastive loss in (b) visibly forces global crossimage attention in the final layers, pattern not as pronounced in (a). This visualizes the mechanism that improves the models internal representations. 28 Phi and Pixtral: representation refinement and feature enhancement Phi and Pixtral employ standard causal attention mask across all layers (Figures 6 and 7). Their path to success is not about fixing shortcoming but refining existing processes. Phi possessing higher-quality LSC from the outset, Phis fine-tuning is primarily process of refinement and noise reduction. The attention maps visualize this as the patterns within each image block becoming more structured and less diffuse. Instead of scattered focus across all visual patches, the model learns to consistently attend to the most salient features while ignoring irrelevant ones. This refined intra-image feature extraction allows for cleaner signal to be passed to the final layers, enabling the model to better leverage its already-strong representations and surpass its LSC. (a) Phi after LoRA tuning (LNT) (b) Phi after LoRA tuning (Lcombined) Figure 6: Attention maps for the Phi model after LoRA tuning with (a) the LNT objective and (b) the Lcombined objective. Both training methods result in similar outcome: refinement of attention over the models already high-quality initial representations. The patterns become visibly more structured and less diffuse, visualizing mechanism of noise reduction and more targeted feature aggregation that enables the model to surpass its LSC. Pixtrals success, even at baseline, stems from its inherent ability to perform representation refinement. The attention maps reveal the mechanism: an intensified focus on intra-image processing in later layers, visible as more structured diagonal blocks. Crucially, this visual signature is nearly identical when fine-tuning with either the standard LNT or the Lcombined objective. This equivalence demonstrates that standard next-token prediction is sufficient to fully engage Pixtrals innate pathway for extracting and enhancing salient features. (a) Pixtral after LoRA tuning (LNT) (b) Pixtral after LoRA tuning (Lcombined) Figure 7: Attention maps for the Pixtral model after LoRA tuning with (a) the LNT objective and (b) the Lcombined objective. The attention patterns are nearly identical, showing that both objectives activate the same underlying mechanism."
        }
    ],
    "affiliations": [
        "Applied Artificial Intelligence Group, Tallinn University of Technology, Estonia"
    ]
}