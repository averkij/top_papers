{
    "paper_title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time",
    "authors": [
        "Yixuan Han",
        "Fan Ma",
        "Ruijie Quan",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 2 7 5 2 2 . 9 0 5 2 : r Preprint. Under review DYNAMIC EXPERTS SEARCH: ENHANCING REASONING IN MIXTURE-OF-EXPERTS LLMS AT TEST TIME Yixuan Han1 Fan Ma1 Ruijie Quan2 Yi Yang1 1 Zhejiang University 2 Nanyang Technological University"
        },
        {
            "title": "ABSTRACT",
            "content": "Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), TTS strategy that elevates expert activation into controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have achieved remarkable progress across wide range of domains (Hurst et al., 2024; OpenAI, 2024; Anthropic, 2023; Guo et al., 2025; Dubey et al., 2024), yet their reasoning capability remains significant challenge (Wei et al., 2022; Wang et al., 2023; Qi et al., 2025; Yao et al., 2023; Kang et al., 2024). Recent advances (Brown et al., 2024; Beeching et al., 2024) highlight Test-Time Scaling (TTS) as promising paradigm to enhance reasoning without additional training: by allocating more computation at inference, TTS generates multiple candidate solutions guided by verifier and votes one as final solution. This inference-time paradigm enhances reasoning ability independently of model size, providing an efficient alternative to parameter scaling. Existing TTS methods typically improve reasoning by expanding the solution space through inferencetime search strategies (Wu et al., 2024; Beeching et al., 2024; Qi et al., 2025; Snell et al., 2024; Chen et al., 2024; Liu et al., 2025). They rely on stochastic sampling to introduce diversity, increasing the likelihood of finding the correct answer. However, such diversity is obtained only at the output level, while the internal computation of the model is treated as architecture-agnostic. These methods implicitly assume that model architecture plays no role, and therefore treat different architectures in the same way. This assumption becomes problematic in light of recent trends, where many state-of-the-art LLMs adopt the Mixture-of-Experts (MoE) architecture (Fedus et al., 2022). In MoE, large pool of specialized experts exists, but only small subset is activated for each input token. Leveraging this flexibility opens up new dimension for TTS beyond sampling-based diversity alone. From this perspective, we discover an underexplored opportunity for the MoE models: the number of activated experts can be flexibly adjusted at inference, potentially influencing the models reasoning behavior. To validate this intuition, we study the effect of adjusting expert activation on the quality and diversity of solutions. As shown in Figure 1, varying the number of activated experts yields Corresponding author. 1 Preprint. Under review Figure 1: Impact of varying the number of activated experts in MoE models. (a) Quality: different activated expert counts yield comparable overall accuracy. (b) Diversity: the Jaccard Similarity between the sets of problems correctly solved activating different numbers of experts. little change in overall accuracy, but substantially alters the subsets of problems the model solves, with low Jaccard Similarity across configurations. In other words, different expert counts lead to complementary solutions, highlighting an additional source of diversity beyond output sampling. Motivated by this observation, we propose Dynamic Experts Search (DES), new TTS strategy that leverages the structural flexibility of MoE architectures now prevalent in state-of-the-art LLMs. Specifically, DES incorporates two key design choices. The first, Dynamic MoE, introduces explicit control over the number of activated experts during inference, treating it as tunable parameter rather than fixed default. The second, Expert Configuration Inheritance, keeps this number consistent along reasoning trajectory, allowing the verifier to compare configurations fairly and guide the search toward more effective ones. Together, these designs elevate expert activation into an additional controllable dimension of the search space: each run maintains fixed number of activated experts across layers for coherence, while different runs vary this number to explore alternative reasoning trajectories. In this way, DES substantially increases the likelihood of finding correct answers. To validate this design, we conduct extensive experiments across diverse MoE models and verifiers on range of reasoning tasks. In our experiments, DES consistently outperforms existing TTS strategies, achieving higher accuracy and precision across math, code, and knowledge benchmarks. Beyond raw performance, DES demonstrates two key advantages: it improves reasoning without increasing computational cost, and it generalizes effectively across different model scales and verifier choices. These results highlight that treating expert activation as controllable dimension provides practical and scalable pathway to stronger reasoning in MoE-based LLMs. More broadly, DES establishes paradigm of architecture-aware TTS and illustrates how structural flexibility in modern LLMs can be systematically harnessed to advance reasoning capabilities. Our contributions can be summarized as follows: We identify the limitation of existing TTS strategies that rely solely on output-level sampling and reveal expert activation in MoE models as new axis of exploration for reasoning. We propose DES, new TTS strategy designed for MoE models that treats expert activation as controllable dimension of the search space. DES integrates two design choices to explore different expert activation, increasing the probability of obtaining the correct answer. Extensive experiments across diverse MoE models, verifiers, and reasoning benchmarks show that DES outperforms existing TTS methods at comparable cost, highlighting architectural diversity as practical and scalable pathway to stronger reasoning in LLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Test-Time Scaling. Scaling the test-time computation of LLMs has been shown to be an effective method to enhance model performance (Brown et al., 2024). In prior work, Deepseek-R1 (Guo et al., 2025) promoted the generation of long chain-of-thought (Long CoT) during training, thereby guiding the model to increase its computational efforts at inference time, which effectively improves reasoning capabilities. variety of techniques operating exclusively during inference have also been explored, including majority voting (Wang et al., 2023), search-based strategies (Wu et al., 2024; Yao et al., 2023; Xie et al., 2023; Wan et al., 2024; Qi et al., 2025), and iterative refinement (Qu et al., 2 Preprint. Under review 2024). These approaches involve generating multiple candidate responses for given question and subsequently selecting the final answer through voting or optimization. Subsequent works (Kang et al., 2024; Wu et al., 2024; Snell et al., 2024) have extended this method by incorporating processlevel rewards to guide the search. Beeching et al. (2024) proposed enhancing TTS through search methods that explicitly promote diversity among sampled outputs. Furthermore, Liu et al. (2025) performed comprehensive evaluation of various TTS methods, demonstrating that the optimal scaling strategy is based on the policy models, PRMs, and difficulty levels of the problem. Xu et al. (2025) proposed search method based on foresight sampling, improving performance through token-level reward feedback foresight. However, common limitation of these works is their reliance on temperature sampling as the sole source of output diversity. Our method leverages the scalability of MoE architectures to achieve broader exploration, effectively improving reasoning performance. Mixture-of-Experts. MoE (Shazeer et al., 2017) paradigm has gained significant attention in recent years due to its ability to improve model specialization and computational efficiency (Jin et al., 2025; Wang et al., 2025). MoE architectures, which were originally proposed by Jacobs et al. (1991); Shazeer et al. (2017), have been widely explored to tackle complex tasks utilizing multiple expert models in modular and collaborative manner (Huang et al., 2025). MoE architectures have shown substantial promise in scaling LLMs, where computational efficiency is paramount. Recent advances in MoE have been made in variety of domains. GShard (Lepikhin et al., 2021) and Switch Transformer (Fedus et al., 2022) demonstrated the scalability of MoE models by using top-k routing strategies, allowing large models to be trained with sparse activation of experts. In terms of optimizing MoE architectures, several studies (Huang et al., 2024; Li et al., 2023; Zeng et al., 2024; Jin et al., 2025) have explored the dynamic selection of experts to enhance diversity and specialization, and some works explored how to adequately use the capacity of MoE models (Lewis et al., 2021; Roller et al., 2021; Zhou et al., 2022). Beyond optimization, MoEs inherent flexibility also has been utilized to decouple token-level predictions and improve diversity by Huang et al. (2025) in speculative decoding. By incorporating MoE architecture into an advanced search strategy, we explore new avenues to enhance its effectiveness in test-time scaling, with focus on expanding the exploration space, refining the search process, and strengthening the reasoning capacity of models."
        },
        {
            "title": "3.1 PROBLEM DEFINITION",
            "content": "Test-Time Scaling. Given question q, TTS strategy utilizes policy model πθ and verifier Rϕ to solve it under given computational budget (e.g., generate candidate solutions) with maximum number of reasoning steps . Let = {s0, s1, , sT } be the state set with st denoting the result after the steps reasoning. Similarly, let = {A0, , AT }, where At = {a(0) , , a(m1) } denotes the candidate action set at the t-th step and a(m) denoting the m-th candidate intermediate step generated at t-th step. The process begins with the initial state s0 = q. Policy model πθ iteratively generates actions a(i) πθ( st), [0, m) at each timestep t. Then the verifier produces scalar reward r(m) ) for each action, and the state evolves with the best action through deterministic concatenation: st+1 = [st, a(argmax(r(i) )) maximum steps or an explicit termination signal (<EOS>) is produced. ]. This iterative interaction ends when reach = Rϕ(st, a(m) t Vanilla MoE Inference. MoE architectures implement sparse computation by selectively activating subset of expert networks at each layer (Shazeer et al., 2017). typical MoE layer consists of router and set of lightweight feed-forward networks (experts) (Fedus et al., 2022). During inference, the router selects fixed number of top-scoring experts for each input token based on its representation. This enables token-level specialization while maintaining computational efficiency. Building on this, we identify two often overlooked properties of MoE inference: (1) small variations in the number of activated experts have negligible impact on overall accuracy, and (2) different activation counts can solve distinct, partially non-overlapping sets of problems. 3 Preprint. Under review Figure 2: Comparison of other search-based method and our Dynamic Experts Search when applied on MoE models. denotes the correct step and represents an incorrect one."
        },
        {
            "title": "3.2 DYNAMIC EXPERTS SEARCH",
            "content": "In this section, we propose Dynamic Experts Search (DES), which dynamically explores different expert activation counts within MoE models. As illustrated in Figure 2, TTS search strategies solve question step-by-step(i.e., generating an intermediate step at each timestep t). However, when applied to MoE models, existing approaches activate only fixed number of experts throughout the process, whereas our DES explores varying expert counts. Our DES consists of Dynamic MoE and Expert Configuration Inheritance. Together, these two components form unified strategy for dynamic expert selection during multi-step reasoning. Below, we describe each module in detail. Dynamic MoE. To introduce expert variability into the TTS search process, we augment the MoE model with the ability to control the number of activated experts. In standard MoE settings, this number is typically fixed and hard-coded across all layers. In contrast, our Dynamic MoE mechanism treats the activation count as tunable parameter during inference. We define the policy model πθ to take as input state and an expert activation counts k, and generate the next intermediate reasoning step conditioned on state by activating experts for each forward pass. Formally, the intermediate steps are sampled from πθ( s, k). Expert Configuration Inheritance. When solving question step by step, DES leverages Dynamic MoE to explore multiple expert activation counts. As evidenced by the observations in Figure 1, each question tends to have an optimal expert activation count. Consequently, uniformly exploring all activation counts throughout the entire search process inevitably wastes part of the computation budget on suboptimal choices. To address this, we introduce Expert Configuration Inheritance, which enables the search to progressively focus on the most promising activation count. Specifically, it ensures that for given state st = [st1, at1], the activation count used to expand st is the same as that used to expand st1 (i.e., to generate at1). This inheritance mechanism ensures that the number of experts remains consistent along each reasoning trajectory. At the first step, the model uniformly explores predefined set of expert activation counts. As the search progresses, the verifier assigns lower scores to less promising activation counts, making them unlikely to be inherited in subsequent steps. In this way, more computational resources are allocated to activation counts that lead to higher rewards, enabling focused exploration around promising activation counts and increasing the likelihood of discovering correct answers. Dynamic Experts Search. Given question with computation budget (i.e., generate rollouts per problem), at timestep = 0 we initialize the search with predefined set of expert activation counts {k0, , kn1} with ki = kj when = and construct the initial candidate set C0 = {(s0, k0), , (s0, kn1)}. (1) Here, (s0, ki) denotes that given state s0 = q, the next reasoning step will be generated by activating ki experts for each forward pass of Dynamic MoE model πθ. To satisfy the computation budget , each activation count ki is allocated to generate intermediate steps. Consequently, we can obtain 4 Preprint. Under review Algorithm 1: Dynamic Experts Search Input :Question q, computational budget , candidates counts retained per step , dynamic MoE policy model πθ, verifier Rϕ, maximum steps , different numbers of activated experts {k0, , kn1} with ki = kj when = 1 Initialize s0 q, timestep 0, priority queue C0 [ ]; 2 for = 0 to 1 do Add (s0, ki) to C0 4 while < and no end-of-sequence token in Ct do 5 for (s, k) Ct do initialize candidates 6 7 Sample actions {a(j)} for = 0 to Ct 1 do Ct 1 j=0 Add ([s, a(j)], k) into t; πθ(s, k); Sample using experts 9 Ct+1 {(s, k) + 1; 10 11 (ˆs, ˆk) arg max Rϕ(s); (s,k)CT Rϕ(s) opM (Rϕ(C t))}; Update candidates 12 return ˆs; Return solution with highest reward set of candidate intermediate steps, each paired with its corresponding expert activation count: A0 = {(a(j) 0 , ki) [0, n), [0,"
        },
        {
            "title": "N\nn",
            "content": "), a(j) 0 πθ( q, ki)}, (2) where a(j) 0 πθ( q, ki) denotes the j-th single sample drawn from πθ( q, ki) when = 0. Each intermediate step in A0 can serve as the first step to solve q. We concatenate s0 with each candidate intermediate step to form candidate states paired with corresponding expert activation count: Then we score every states in these scores where is predefined parameter: 0 = {([s0, a], k) (a, k) A0}. (3) 0 using the verifier Rϕ and retain the top states according to 0))} = {(s(0) , k0), , (s(M 1) C1 = {(s, k) 0 Rϕ(s) opM (Rϕ(C (4) In this process, the Expert Configuration Inheritance mechanism records the number of activated experts used to generate each intermediate step and propagates this information into C1, ensuring that each candidate preserves its activation count when generating the subsequent step. The set C1 then serves as the starting state for the next reasoning step. Similarly, at timestep t, the set Ct received from timestep 1 undergoes the operations described in Equations 2 3 4 to generate Ct+1, and this procedure continues until either the maximum reasoning step is reached or the end-of-sequence token (\"<EOS>\") is generated. The detailed process of DES is shown in the Algorithm 1. Upon completion of the search, we obtain complete answers to the question q. Then single answer is selectedthe one achieving the highest verifier score or the highest majority voteas the final answer. , kM 1)}. Discussion. DES introduces structured and adaptive approach to inference-time exploration that differs fundamentally from existing TTS strategies (as shown in Figure 2). We highlight the following key distinctions: (1) Traditional TTS methods overlook that expert selection within MoE models could be exploited to broaden the capability boundaries of MoE models. In contrast, DES leverages Dynamic MoE to vary the number of activated experts, which introduces structural diversity in the computational pathway of models and enlarges the exploration space for identifying correct answers. (2) Through Expert Configuration Inheritance, DES maintains consistent expert activation counts across each reasoning path. This design enables implicit configuration-level credit assignment and allows the model to gradually converge to the most effective expert activation count for the task."
        },
        {
            "title": "4.1 SETUP",
            "content": "Models and Benchmarks. We select four MoE-based policy models: Qwen3-30B-A3B (Qwen, 2025), Ling-lite-1.5 (Ling, 2025), OLMoE-1B-7B-Instruct (Muennighoff et al., 2025) 5 Preprint. Under review Table 1: Accuracy (Acc ) and Precision (Prec ) of different strategies on benchmarks when using Qwen2.5-Math-PRM-7B as policy model. For implementation, we generate = 32 rollouts for each problem. Additional results for other models are provided in the Appendix A. Strategy Metric MATH500 AIME24 AIME25 HumanEval LiveCodeBench (v6-lite) LiveBench (reasoning) Best-of-N BeamSearch DVTS DES(Ours) Best-of-N BeamSearch DVTS DES(Ours) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Qwen3-30B-A3B-Instruct 83.33 73.02 83.33 72.08 86.67 71.98 86.67 72.29 66.67 56. 63.33 54.79 70.00 57.92 70.00 58.44 Ling-lite-1.5 26.67 17.19 23.33 16. 26.67 20.00 26.67 19.27 23.33 13.65 13.33 11.77 23.33 15.94 16.33 12. 92.07 92.23 89.02 93.48 93.90 92.85 94.51 93.75 79.88 82.13 82.32 83. 82.93 84.60 83.54 84.18 92.40 91.38 93.00 92.16 87.40 83.60 93.20 92. 80.00 77.72 81.60 78.46 82.80 77.19 83.80 77.27 35.11 31.99 37.40 31. 32.06 32.03 33.59 32.94 19.08 17.65 19.84 19.13 20.61 18.89 19.84 17. 90.50 79.34 91.50 79.11 90.00 79.17 91.50 81.58 22.00 13.58 20.50 12. 21.00 13.73 23.00 14.39 Figure 3: Comparison of different search strategies applied to OLMoE-1B-7B-Instruct & Llama3.1-8B-PRM-Deepseek-Data. (a) Results on GSM8K. (b) Results on MATH-500. and DeepSeek-V2-Lite-Chat (DeepSeek-AI, 2024). We utilize two widely used process reward models which are designed capable of scoring each intermediate reasoning step within multi-step solution as verifier: Qwen2.5-Math-PRM-7B (Zhang et al., 2025) and Llama3.1-8B-PRM-Deepseek-Data (Wang et al., 2024). In this work, we conduct evaluations on several widely used reasoning benchmarks in the math, code and knowledge domains to examine the reasoning performance of DES in the 0-shot setting. For the math domain, we evaluate on MATH500 (Lightman et al., 2024), AIME24 (AI-MO, 2024) and AIME25 (AI-MO, 2025) (GSM8K (Cobbe et al., 2021), SVAMP (Saha et al., 2021) are used for models under 16B), all of which consist of diverse mathematical reasoning problems written in natural language. For the code domain, we evaluate on HumanEval (Chen et al., 2021) and LiveCodeBench(v6-lite) (Jain et al., 2025), both of which cover wide range of programming tasks. For the knowledge domain, we evaluate on LiveBench(reason) (White et al., 2025), which require knowledge-based reasoning. Baselines and Metrics. We compare DES with three baseline search strategies. Best-of-N (Brown et al., 2024) samples complete responses and selects the solution with highest reward scored by process reward model as the final answer. Beam Search (Snell et al., 2024) generates candidates per step, retains the top-M scored by verifier, and expands each with new steps iteratively. After reaching the maximum reasoning step or the termination signal being produced, Beam Search selects Preprint. Under review Table 2: Accuracy (Acc ) of different modes for Qwen3-30B-A3B. We evaluate four modes: with think-mode, with DES, with instruction tuning and with DES + instruction tuning. For DES, we generated = 32 rollouts for each problem and use Qwen2.5-Math-PRM-7B as verifier. Model Metric MATH500 AIME24 AIME HumanEval LiveCodeBench (v6-lite) LiveBench (reasoning) Think DES Instruct tune Think DES Instruct tune Think DES Instruct tune Think DES Instruct tune Think DES Instruct tune Acc(%) 79.60 23.33 13. 87.19 Acc(%) 92.40 83.33 70.00 88. Acc(%) 84.80 43.33 20.00 74.39 Acc(%) 92.00 70.00 63.33 91.46 Acc(%) 93. 86.67 70.00 94.51 20.61 37.40 21. 31.29 33.59 41.50 63.00 57.50 81. 91.50 Figure 4: Left: Average number of activated experts on various datasets using DES. Right: Average number of activated experts at each timestep of DES on various datasets. final answer via scoring by verifier. Diverse Verifier Tree Search (Beeching et al., 2024) enhances diversity by partitioning the search into independent subtrees, each searched separately using Beam Search. We report Accuracy (Acc ) for different search strategies, which refers to the proportion of correctly solved problems when the final answer is determined by majority voting (i.e., selecting the most frequently occurring response). We also report Precision (Prec ) for each search strategy on each benchmark, which indicates the fraction of correct answers among the candidate solutions returned by the search process. In addition, we include the Average number of generated tokens per problem (#Gen.Tok ) as measure of the computational cost."
        },
        {
            "title": "4.2 MAIN RESULT",
            "content": "Comparisons to Different Search Strategies. To assess the effectiveness of our proposed DES, we conduct comparative experiments against standard TTS search strategies across benchmark datasets. We report results in terms of Accuracy, Precision and #Gen.Tok as defined in Section 4.1. Specifically, Table 1 presents the results for Qwen3-30B-A3B-Instruct and Ling-lite-1.5, using Qwen2.5-Math-PRM-7B as the verifier. Additional results on other model pairs are provided in Appendix A. As shown, DES consistently outperforms TTS baselines in both Accuracy and Precision, highlighting its advantage in guiding the model to allocate computational resources toward more effective exploration during search. This leads to higher proportion of correct answers and overall improved performance. Comparisons to Thinking Mode. Recently, some reasoning models (e.g., Qwen3-30B-A3B) have introduced thinking mode, which generates long chains of thought (i.e., large number of tokens) to analyze and solve problems in detail. Compared to the non-thinking mode, the thinking mode typically incurs substantially higher computational costs. To provide comprehensive comparison between thinking mode and our proposed DEStwo approaches that improve model performance by increasing computational expenditurewe conduct experiments on Qwen3-30B-A3B under 7 Preprint. Under review Table 3: Ablation study on the initial numbers of activated experts when applying DES. Results are obtained on Qwen3-30B-A3B-Instruct, with = 32 rollouts per problem. Metric 2-9 3-10 4-11 5-12 6-13 78-15 9-16 10-17 Initial numbers of activated experts Avg.#Experts 5. 6.79 7.62 8.44 9.47 10.53 11. 11.95 13.11 Table 4: Average number of generated tokens per problem (#Gen.Tok) for different search strategies. Each problem is solved with = 32 rollouts. Strategy Metric MATH500 AIME24 AIME25 HumanEval LiveCodeBench (v6-lite) LiveBench (reasoning) Qwen3-30B-A3B-Instruct & Qwen2.5-Math-PRM-7B Best-of-N #Gen.Tok BeamSearch #Gen.Tok DVTS #Gen.Tok DES(Ours) #Gen.Tok 32.9k 33.9k 22.7k 33.9k 165.5k 190.8k 169.6k 199.1k 172.5k 189.5k 168.4k 197.0k 20.3k 20.5k 27.4k 20.9k 69.9k 73.9k 83.5k 87.0k 144.2k 134.8k 134.8k 135.1k different modes (see Table 2). The results show that DES achieves comparable or better performance than the thinking mode, highlighting its advantage in balancing effectiveness and efficiency. Comparisons across Different Computational Budgets. We evaluate how model performance scales with the computational budget by comparing OLMoE-1B-7B-Instruct & Llama3.1-8B-PRM-Deepseek-Data on GSM8K and MATH500 under different search strategies. As shown in Figure 3, our method consistently outperforms the baselines across nearly all budgets. Moreover, the performance gap widens as increases, indicating that our approach raises the upper bound of achievable accuracy and demonstrates strong scalability in resource-rich settings."
        },
        {
            "title": "4.3 ABLATION STUDY",
            "content": "Average Number of Activated Experts. To ensure that the performance gains of our method are not merely due to activating more experts, we measure the average number of activated experts during the search process. Figure 4(a) illustrates the distribution of the overall average number of activated experts when using Qwen3-30B-A3B-Instruct & Qwen2.5-Math-PRM-7B (whose default is 8 experts) across multiple datasets, while Figure 4(b) shows the average number at each timestep. The results show that DES does not activate more experts than the default configuration of the policy models. This suggests that the observed performance improvements stem from the effectiveness of the search strategy rather than merely from activating additional experts. Effect of Initial Numbers of Activated Experts. For policy models with default of 8 activated experts, we vary the initial number of activated experts in [4, 5, 6, 7, 8, 9, 10, 11] in experiments shown above. Under this setting, we find that the average number of activated experts during the search process does not exceed the default. To further investigate this phenomenon, we conduct an ablation study on different initial values. As shown in Table 3, the observed average consistently aligns with the mean of the initial values(e.g., 7.5 for [4, 5, 6, 7, 8, 9, 10, 11]). This indicates that the average number of activated experts in the search process is largely determined by the initial setting. Moreover, the results suggest that all candidate numbers of activated experts are uniformly explored during search, rather than the model simply favoring larger values. Computation Comparison Between Baselines and DES. In the performance comparison experiments, both the baseline strategies and DES generate the same number of rollouts. However, an identical number of rollouts does not strictly imply equal computational cost across search strategies. To enable more comprehensive comparison, we additionally report the average number of generated tokens per problem (#Gen.Tok) for each strategy (see Table 4). Since the search strategies do not alter the model architecture, the computational cost (e.g., FLOPs) per token remains the same; thus, the number of generated tokens is strictly proportional to the overall computation cost. As shown in Table 4, DES incurs comparable computational cost to the baselines, indicating that the observed performance gains are not attributable to increased computation. 8 Preprint. Under review Table 5: Ablation study on Dynamic MoE which introduces Exploration on different numbers of activated experts (Exp.on.Num). We performed on MATH500 benchmark using OLMoE-1B-7B-Instruct & Llama3.1-8B-PRM-Deepseek-Data. Metric Acc(%) Strategy Computational budget 8 32 64 BeamSearch w/o Exp.on.Num 21.00 20.40 BeamSearch Exp.on.Num 23.00 24.20 24.60 25.60 28.40 29. 128 27.40 31.00 Figure 5: Ablation study on Inheritance of the number of activated experts (Inh.of.Num) conducted on the MATH500 benchmark, using OLMoE-1B-7B-Instruct as the policy model and Llama3.1-8B-PRM-Deepseek-Data as the verifier. The left subfigure reports Accuracy of DES with and without Inh.of.Num, while the right subfigure presents pass@N . Effect of Dynamic MoE. To investigate the influence of Exploration on different numbers of activated experts (Exp.on.Num) introduced by Dynamic MoE, we simply compare vanilla BeamSearch and BeamSearch with Dynamic MoE on MATH500. Unlike vanilla BeamSearch activated fixed number of experts, BeamSearch with Dynamic MoE uniformly varies different numbers of activated experts to generate multiple candidate next reasoning steps at each timestep. As shown in Table 5, introducing the exploration of different numbers of activated experts leads to stable performance improvement with different computational budget . Effect of Experts Configuration Inheritance. The Inheritance of the activated experts number is designed to guide computation toward more suitable configurations, thereby improving the effectiveness of exploration during the search process. To assess the impact of Inheritance of the activated experts number (Inh.of.Num), we conduct an ablation study comparing DES with and without this setting. In the variant without Inh.of.Num, different numbers of experts are uniformly explored at each timestep when generating candidates. As shown in Figure 5, enabling Inh.of.Num improves not only accuracy but also the pass@N metric, which measures the probability of obtaining correct answer within attempts under fixed computational budget. These results demonstrate that Inh.of.Num effectively steers the search process in more promising direction and substantially increases the likelihood of generating correct answers."
        },
        {
            "title": "5 LIMITATION",
            "content": "While DES achieves performance improvements, it also shares common limitations with other TTS strategies. First, it requires guidance from an external verifier during the search process, which introduces additional communication overhead. Second, the performance of DES is partly dependent on the quality and reliability of the verifier, as misaligned evaluations can hinder final performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We propose Dynamic Experts Search (DES), novel Test-Time Scaling (TTS) strategy that harnesses the modular structure of MoE models to unlock complementary reasoning capabilities. By dynamically adjusting the number of activated experts during inference, DES introduces expert configuration 9 Preprint. Under review as controllable dimension within the search space, enabling capacity-aware exploration. Extensive experiments across multiple reasoning benchmarks demonstrate the consistent advantage of DES over existing TTS baselines. Beyond performance gains, our work offers new perspective on Test-Time Scaling by highlighting the untapped flexibility within certain distinctive architectures. We believe DES paves the way for more adaptive and structurally-aware reasoning in large language models."
        },
        {
            "title": "REFERENCES",
            "content": "AI-MO. Aime 2024, 2024. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-aime. AI-MO. Aime 2025, 2025. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-aime. Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/ introducing-claude/. Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models, 2024. URL https://huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process. In Neural Information Processing Systems, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Preprint. Under review Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haiduo Huang, Fuwei Yang, Zhenhua Liu, Yixing Xu, Jinze Li, Yang Liu, Xuanwu Yin, Dong Li, Pengju Ren, and Emad Barsoum. Jakiro: Boosting speculative decoding with decoupled multi-head via moe, 2025. Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng. Harder tasks need more experts: Dynamic routing in moe models. CoRR, abs/2403.07652, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computation, pp. 7987, 1991. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In International Conference on Learning Representations, 2025. Peng Jin, Bo Zhu, Li Yuan, and Shuicheng YAN. Moe++: Accelerating mixture-of-experts methods with zero-computation experts. In International Conference on Learning Representations, 2025. Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, et al. MindStar: Enhancing math reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models, 2021. Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, and Hong Xu. Adaptive gating in mixture-of-experts based language models. In Empirical Methods in Natural Language Processing, 2023. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In International Conference on Learning Representations, 2024. Ling. Every flop counts: Scaling 300b mixture-of-experts ling llm without premium gpus. arXiv preprint arXiv:2503.05139, 2025. 11 Preprint. Under review Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. OLMoe: Open mixture-of-experts language models. In International Conference on Learning Representations, 2025. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Zhenting Qi, Mingyuan MA, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller LLMs stronger problem-solver. In International Conference on Learning Representations, 2025. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching In Advances in Neural Information Processing language model agents how to self-improve. Systems, 2024. Qwen. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse models. Neural Information Processing Systems, 2021. Amrita Saha, Mohnish Dubey, and Eduard Hovy. Svamp: dataset for evaluating verbal reasoning in math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021, 2021. Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, and Jun Wang. AlphaZero-like tree-search can guide large language model decoding and training. In International Conference on Machine Learning, 2024. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang In Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations, 2023. Ziteng Wang, Jun Zhu, and Jianfei Chen. Remoe: Fully differentiable mixture-of-experts with reLU routing. In International Conference on Learning Representations, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-limited LLM benchmark. In International Conference on Learning Representations, 2025. 12 Preprint. Under review Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In Advances in Neural Information Processing Systems, 2024. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. In Advances in Neural Information Processing Systems, 2023. Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, and Zhiyong Wu. ϕ-decoding: Adaptive foresight sampling for balanced inference-time exploration and exploitation, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, 2023. Zihao Zeng, Yibo Miao, Hongcheng Gao, Hao Zhang, and Zhijie Deng. Adamoe: Token-adaptive routing with null experts for mixture-of-experts language models. In Submitted to ACL Rolling Review - June 2024, 2024. under review. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. In Submitted to ACL Rolling Review - February 2025, 2025. under review. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M. Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. In Advances in Neural Information Processing Systems, 2022. 13 Preprint. Under review"
        },
        {
            "title": "SUMMARY OF THE APPENDIX",
            "content": "This appendix contains additional details for the ICLR 2026 submission, titled \"Enhancing LLM Reasoning in Test-Time Scaling via Dynamic Experts Search\". The appendix is organized as follows: Section reports additional experiments results and analysis. Section describes the broader impact of Dynamic Experts Search. Section introduces details of the implementation. Section shows the licenses of datasets, codes and models used in this paper. Section claims the use of large language models (LLMs)."
        },
        {
            "title": "A ADDITIONAL EXPERIMENTS AND ANALYSIS",
            "content": "A.1 EXPERIMENTS USING OTHER MODELS We further evaluate DeepSeek-V2-Lite-Chat and OLMoE-1B-7B-Instruct on benchmarks from different domains, using Qwen2.5-Math-PRM-7B as the verifier. Due to their relatively small sizes, these two models struggle with challenging benchmarks such as AIME AI-MO (2024; 2025). Therefore, we instead adopt two alternative mathematical reasoning benchmarks (SVAMP Saha et al. (2021) and GSM8K Cobbe et al. (2021)) for evaluation. As shown in Table 6, our method substantially outperforms other search strategies. These results confirm that DES effectively steers the search process toward correct solutions by improving the likelihood of retrieving accurate answers, while also underscoring its generalizability across diverse architectures and model scales. Table 6: Accuracy (Acc ) and Precision (Prec ) of different strategies on benchmarks using Qwen2.5-Math-PRM-7B as policy model. For the implementation of all search methods, we generated = 64 rollouts for each problem. Strategy Metric SVAMP GSM8K MATH500 HumanEval LiveCodeBench (v6-lite) LiveBench (reasoning) Best-of-N BeamSearch DVTS DES(Ours) Best-of-N BeamSearch DVTS DES(Ours) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) 85.00 45. 83.00 53.28 86.00 53.45 87.67 54.49 74.00 33.58 84.67 57.51 80.67 50. 83.67 60.47 DeepSeek-V2-Lite-Chat 68.80 35.14 69.39 46.17 72.60 42.20 82.80 49. 30.40 14.79 35.80 24.58 34.00 19.19 41.80 27.01 40.24 40.37 41.46 44. 30.49 40.13 47.56 46.06 OLMoE-1B-7B-Instruct 57.60 22.73 74.20 48.74 70.00 37. 75.40 52.86 17.00 7.96 25.00 16.62 23.78 12.33 27.00 17.62 34.76 27. 34.76 27.36 33.54 27.36 34.76 26.91 10.69 8.14 11.45 8.36 12.21 8. 12.98 8.60 4.58 2.14 4.58 2.16 5.34 1.88 6.10 1.92 13.50 9. 13.00 8.71 12.00 8.97 16.50 10.00 10.50 6.74 10.50 6.98 11.50 7. 11.50 7.86 A.2 ADDITIONAL ABLATION STUDY To illustrate the generalizability of DES across different verifier, we also select another alternative process reward model (Llama3.1-8B-PRM-Deepseek-Data) as verifier and evaluate the base14 Preprint. Under review line and DES. As shown in Table 7, when using different verifier, DES still outperforms the baseline search method, indicating the improvements brought by DES are not specific to particular verifier. Table 7: Accuracy (Acc ) and Precision (Prec ) of different strategies on benchmarks using Llama3.1-8B-PRM-Deepseek-Data as policy model. For the implementation of all search methods, we generated = 64 rollouts for each problem. Strategy Metric SVAMP GSM8K MATH500 HumanEval LiveCodeBench (v6-lite) LiveBench (reasoning) DeepSeek-V2-Lite-Chat 68.80 34.83 67.60 44.13 71.00 43.23 87.60 54.89 41.00 14.86 46.60 27. 42.40 22.59 48.00 28.67 45.12 40.61 40.85 40.26 42.68 40.27 55.49 40. OLMoE-1B-7B-Instruct 57.60 22.48 76.00 49.87 72.60 38.93 79.60 52.75 17.40 7. 27.40 18.67 25.20 13.99 29.40 19.49 45.73 27.36 45.73 26.91 45.73 27. 45.73 26.91 Best-of-N"
        },
        {
            "title": "DVTS",
            "content": "DES(Ours) Best-of-N BeamSearch DVTS DES(Ours) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) Acc(%) Prec(%) 85.33 46.06 86.33 57.12 89.33 54.46 91.67 61.47 71.67 32. 86.33 60.00 84.00 50.31 87.00 60.85 A.3 WHY DOES DES WORK? 12.21 8.52 13.74 8. 12.21 8.48 11.45 8.43 8.40 2.16 6.87 1.93 7.63 2.09 6.10 1. 12.00 9.25 14.00 9.85 12.50 9.96 16.50 10.29 12.50 6.14 12.00 5. 11.50 6.56 12.50 6.98 Dynamic MoE Effectively Enhances the Probability of Reaching Correct Answers. To demonstrate the effectiveness of Dynamic MoE, we isolate the impact of exploring different numbers of activated experts (denoted as Exp.on.Num) within the beam search framework in ablation study. Specifically, at each step of beam search, we uniformly allocate rollouts across varying numbers of activated experts (e.g., when the total number of rollouts is 128, we generate 128 8 = 16 rollouts for each expert count in {4, 5, 6, 7, 8, 9, 10, 11}), without incorporating Diversity-Aware Selection or Experts Configuration Inheritance. Compared to vanilla beam search, this strategy expands the search space and makes it possible to reach an appropriate expert configuration capable of generating the correct answer. As result, the proportion of correct answers among the generated candidates is significantly improved. Experts Configuration Inheritance Avoids Inefficient Computation. We adopt Experts Configuration Inheritance to maintain consistency in the expert configuration across steps when generating complete answer. At each step, inappropriate expert configurations tend to produce low-scoring responses that are subsequently discarded during the search process. Those suboptimal configurations will be filtered out naturally by eliminating low-scoring responses at each step. As result, in the later stages of search, only effective expert configurations are retained, concentrating computation on more promising candidates and thereby increasing the likelihood of generating the correct answer. In our ablation study, we report the pass@N metric to demonstrate that the final candidate set has higher probability of containing the correct answer."
        },
        {
            "title": "B BROADER IMPACT",
            "content": "Academic Impact. DES significantly enhances the reasoning capabilities of small-scale models (e.g., 7B model) through an effective test-time search strategy that leverages additional computation during 15 Preprint. Under review inference. In contrast, achieving similar improvements via pre-training typically requires scaling up model size and incurring substantial training costs. From this perspective, DES offers more efficient approach to boosting model performance without retraining or modifying model parameters. This highlights promising research direction in utilizing computation more strategicallyat inference rather than training timeto improve model capabilities, thereby reducing the barriers to deploying strong reasoning models in resource-constrained settings. It also sheds light on the potential of unlocking existing model capacity through smarter inference-time strategies, without the need for costly model retraining. Besides, DES dynamically adjusts the expert configuration during inference, enhancing performance without modifying model parameters. This design offers compelling insights into the synergy between Mixture-of-Experts (MoE) architectures and test-time scaling. By demonstrating how MoE models can be effectively scaled post-training, DES introduces practical pathway for deploying models more efficiently. We believe this approach will inspire future research in controllable reasoning and adaptive inference, further unlocking the potential of MoE architectures in academic applications. Social Impact. The ultimate objective of DES is to substantially enhance the reasoning capabilities of language models through test-time scaling, rather than relying on scaling up model size. By enabling small models (e.g., 7B model) to perform competitively on complex reasoning tasks, DES offers pathway to making powerful AI models more accessible and deployable in memory-constrained environments(e.g., personal devices). This shift reduces the dependency on large-scale cloud infrastructures and enables real-time and offline inference without internet connectivity, effectively eliminating latency caused by network transmission. This paradigm holds the potential to democratize access to advanced AI capabilities, especially in regions or scenarios where computational resources and reliable network connections are limited. It empowers individuals to benefit from intelligent assistants locally and securely, fostering greater privacy and responsiveness in AI applications."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 PROMPTS To enable the model to solve reasoning problems, we use the following prompts. We record the prompt as chat format [ ] {\"role\":\"system\", \"content\": [system_prompt]}, {\"role\":\"user\", \"content\": [question]}, {\"role\":\"assistant\", \"content\":} For mathematical task, [system_prompt] is Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide concise solution with minimal explanation. - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] 16 Preprint. Under review ... Regardless of the approach, always conclude with: Therefore, the final answer is: $boxed{answer}$. hope it is correct. Where [answer] is just the final number or expression that solves the problem. For code-generation tasks, [system_prompt] is You are code assistant. ### Instruction: You will be given question (problem specification) and will generate correct Python program that matches the specification. For knowledge-domain tasks, [system_prompt] is Solve the following question step by step. C."
        },
        {
            "title": "INFERENCE DETAILS",
            "content": "Deployment. In our experiments, we set the candidates counts retained per step = 4 where denotes the predefined total number of final candidate solutions, maximum steps = 10 for all benchmarks. The initial candidate numbers of activated experts is [4, 5, 6, 7, 8, 9, 10, 11] (s.t. = 8), where all policy models have default of 8 activated experts. The temperature is set to 0.8 for all search strategies. All experiments were conducted on 4 NVIDIA RTX A6000 GPUs. Inference Framework. All policy models are served using the vLLM Kwon et al. (2023) inference framework to ensure efficient and scalable generation and all reward models are served using the tranformers Wolf et al. (2020) framework. Answer extraction for mathematical tasks. To extract the models predicted answer from the raw output string, we implement post-processing function that handles various formats and common answer patterns. The function locates the final answer using segments following final answer is $ or enclosed within boxed{}. Answer extraction for mathematical tasks. To extract the models predicted answer from its raw output in mathematical tasks, we implement post-processing function that accounts for diverse output formats and common answer patterns. The function identifies the final answer either from segments following final answer is $ or from content enclosed within boxed{}, ensuring consistent and accurate extraction for evaluation. Answer extraction for code-generation tasks. In code-generation tasks, models typically produce their outputs within markdown-style code blocks, often prefixed with language identifier such as python and ended with . To obtain the executable answer, we extract the content inside these code blocks. Specifically, we identify the opening and closing code block delimiters () and retrieve all text in between, ignoring any surrounding explanations or comments. This ensures that only the generated code is used as the final answer, which can then be executed or evaluated. Answer extraction for knowledge-domain tasks. For tasks in the knowledge domain, the model is prompted to produce its final answer enclosed within <solution> tag, i.e., <solution> ... </solution>. During evaluation, we extract the content between these tags using regular expression pattern such as <solution>(.*?)</solution> and treat it as the models predicted answer. This approach ensures consistent and precise extraction of the answer from the models textual output. 17 Preprint. Under review"
        },
        {
            "title": "D LICENSE",
            "content": "The code will be publicly accessible upon acceptance. We use standard licenses from the community. We include the following licenses for the codes, datasets and models we used in this paper. 1. Benchmarks SVAMP: MIT GSM8K: MIT MATH500: MIT AIME: Apache HumanEval: MIT LiveCodeBench: MIT LiveBench: Apache 2. Models Qwen3-30B-A3B: Apache Ling-lite-1.5: MIT OLMoE-1B-7B-Instruct: Apache DeepSeek-V2-Lite-Chat: Deepseek Llama3.1-8B-PRM-Deepseek-Data: Llama Qwen2.5-Math-PRM-7B: Qwen THE USE OF LARGE LANGUAGE MODELS (LLMS) During the preparation of this manuscript, Large Language Models were used as general-purpose writing assistant tool. Specifically, LLMs were employed to polish the language and refine the clarity of the text. The authors take full responsibility for the content of the paper."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Zhejiang University"
    ]
}