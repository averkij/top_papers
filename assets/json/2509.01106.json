{
    "paper_title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
    "authors": [
        "Huang Fang",
        "Mengxi Zhang",
        "Heng Dong",
        "Wei Li",
        "Zixuan Wang",
        "Qifeng Zhang",
        "Xueyun Tian",
        "Yucheng Hu",
        "Hang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering."
        },
        {
            "title": "Start",
            "content": "Robix: Unified Model for Robot Interaction, Reasoning and Planning Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li"
        },
        {
            "title": "ByteDance Seed",
            "content": "Equal Contribution, Project Lead"
        },
        {
            "title": "Abstract",
            "content": "We introduce Robix, unified model that integrates robot reasoning, task planning, and natural language interaction within single vision-language architecture. Acting as the high-level cognitive layer in hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering. Date: September 3, 2025 Correspondence: liwei.85@bytedance.com, lihang.lh@bytedance.com Project Page: https://robix-seed.github.io/robix/ 5 2 0 2 1 ] . [ 1 6 0 1 1 0 . 9 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "The goal of generalist robots is to assist humans in diverse daily tasks within open, dynamic environments. Realizing this vision requires more than executing isolated commandsit demands the ability to engage in natural human interaction and reason through complex, long-horizon tasks. For example, when cleaning dining table, robot must not only recognize dishes and utensils, but also interpret nuanced instructions such as only clear the plates if people are finished eating, respond to corrections like leave that glass, and adapt to novel scenarios such as properly sorting stacked tableware. To meet these requirements, general-purpose robot system should adopt hierarchical architecture in which high-level cognitive layer handles complex multimodal reasoning, adaptive task planning, and natural human-robot interaction, while 1 Figure 1 demo of Robix, showcasing (1) complex instruction understanding with commonsense reasoning; (2) real-time interruption handling; (3) task-status monitoring and dynamic replanning; and (4) proactive dialogue to clarify ambiguous instructions or infer user intent. 2 low-level controller layer executes the atomic motor actions issued by the high-level layer. This division of responsibilities allows the robot to reason at macro level while acting at micro level, enabling human-like adaptability in real-world scenarios. Existing hierarchical approaches typically employ large language models (LLMs) or vision language models (VLMs) as the high-level cognitive layer for task planning, which decompose long-horizon tasks into executable subtasks for the low-level controller [1, 8, 17, 29, 53, 60, 84, 88]. However, these methods focus solely on task decomposition, overlooking human-robot interaction and embodied reasoning, which are essential for general-purpose robotic systems. Taking one step further, recent work [11] constructs modular pipelines that combine reasoning, planning, and interaction through hand-designed workflows. While workflow-based systems are easy to develop, their inflexibility and brittleness remain notable limitationsrooted primarily in rigid modularization and over-reliance on hand-engineered designs. In this work, we introduce Robix, unified high-level cognitive layer that seamlessly integrates reasoning, task planning, and natural language interaction within single model. Unlike modular frameworks, Robix adopts an end-to-end visionlanguage architecture natively designed for interactive task execution. At its core, Robix leverages chain-of-thought reasoning and formulates interactive task execution as unified reasoning-action sequence, effectively functioning as the brain of generalist robot system. Figure 1 illustrates Robix in an interactive table-organization task, demonstrating flexible capabilities such as understanding complex instructions, handling real-time interruptions, monitoring task progress, and engaging in proactive dialogue to clarify ambiguous commands or infer user intent. Modeling such complex interactive task execution within single VLM is challenging. Although general VLMs have achieved strong performance in digital domains, extending them to physical robots is far more demanding: robots must continuously perceive and act in dynamic environments, interpret ambiguous instructions, adapt to real-time feedback, and make sequential decisions under strict physical and temporal constraints. Addressing this gap requires overcoming two major limitations of existing models: (1) limited embodied reasoningthe ability to ground objects and spatial concepts in the physical world and integrate these signals for adaptive planning and task-centric reasoning [64]; (2) lack of flexible multimodal interactionhindered both by its inherent complexity and by the scarcity of corresponding training data. To address these challenges, Robix is trained with three-stage strategy: Continued pretraining on general VLMs to enhance foundational embodied reasoning capabilities. We curate large-scale dataset covering various robot-relevant tasks, such as 3D spatial understanding, visual grounding, and task-centric reasoning, enabling the model to strengthen its grounded planning and reasoning abilities. Supervised finetuning to endow the model with complex interactive capabilities. We employ comprehensive data synthesis to incorporate chain-of-thought reasoning and model interactive task execution as unified reasoning-action sequence. The synthetic data covers full spectrum of capabilities, including complex instruction understanding, long-horizon planning, task status monitoring, dynamic replanning, real-time interruption handling, and human-robot dialogue. Reinforcement learning to further refine the reasoning ability and strengthen the consistency between reasoning and actions, particularly in long-horizon, interactive tasks. We comprehensively evaluate Robix on embodied reasoning and interactive task execution. Across 31 benchmarks covering robot-relevant abilities (3D spatial understanding, visual grounding, task-centric reasoning) and general-purpose skills (general VQA, multimodal reasoning), Robix achieves obvious improvements on most robot-relevant tasks while maintaining strong general-purpose performance. On curated interactive-task benchmark spanning in-distribution and out-of-distribution (OOD) settings and diverse instruction types (multi-stage, constrained, open-ended, invalid, interrupted), Robix consistently outperforms commercial (e.g., GPT-4o, Gemini-2.5-Pro) and open-source (e.g., Qwen2.5-VL, RoboBrain-2.0) baselines; notably, Robix-32B exceeds the strongest baseline, Gemini-2.5-Pro, by 3.0 and 11.8 percentage points in accuracy on the two OOD settings. We further assess five real-world scenariostable bussing, grocery shopping, checkout packing, tableware organization & shipment, and dietary filteringusing task-progress metrics in hierarchical robot system under two low-level control modes (human teleoperation and an automatic VLA controller). Across 3 Figure 2 Illustration of the hierarchical robot system. Robix serves as the high-level cognitive layer, interpreting tasks and reasoning over multimodal inputs to generate language responses and action plans. The low-level controller layer executes the corresponding atomic commands, enabling interaction with both humans and the physical environment. both modes, Robix-32B surpasses Gemini-2.5-Pro by 1.6 and 4.3 percentage points on task progress and markedly outperforms all other baselines by 28.1 64.6 percentage points. Our experiments demonstrate that Robix couples strong embodied reasoning with flexible high-level planning and interaction, advancing toward general-purpose embodied intelligence. We summarize Robixs main features as follows: Unified model. Robix is single vision-language model that unifies robot reasoning, task planning, and human-robot interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally in an end-to-end manner. Flexible interaction. Within this unified framework, Robix supports proactive dialogue to clarify ambiguity or infer user intent, real-time interruption handling that seamlessly incorporates feedback, and context-aware commonsense reasoning for complex, open-ended tasks. Robust performance. We assess Robix in two setups: (i) on curated interactive-task benchmark covering both inand out-of-distribution scenarios with diverse instruction types, and (ii) across five real-world scenarios in hierarchical robot system with both human teleoperation and an automatic VLA model as the low-level controller. These evaluations demonstrate that Robix consistently delivers strong performance across all settings."
        },
        {
            "title": "2 The Robix Model",
            "content": "Figure 2 illustrates the hierarchical robot system, where Robix serves as the high-level cognitive layer responsible for planning and interaction. The low-level controllertypically implemented as vision-languageaction (VLA) modelexecutes the atomic commands generated by Robix, enabling the robot system to directly interact with the physical environment. At each iteration, Robix directly processes visual observations from robot-mounted cameras and user utterances, selectively producing atomic action commands for the low-level controller and appropriate verbal responses. This iterative reasoning-action loop allows Robix to perform deliberate reasoning and generate contextually grounded behaviors. The sequential decision-making process can be formally modeled as: (cid:0)tn, an, rn (cid:12) (cid:12) (o1, u1, t1, a1, r1), . . . , [(oni, uni, tni, ani, rni)]N Here, each step involves predicting the next thought tn, action an, and optional verbal response rn, conditioned on the current observations on, optional user instruction un and the interaction history. These intermediate i=1, on, un (cid:1) . (1) 4 Figure 3 Overview of Robixs pretraining data, curated to strengthen core embodied reasoning capabilities (3D spatial understanding, visual grounding, and task-centric reasoning) while also supporting general multimodal understanding and reasoning. The figure showcases the diversity of the data, establishing solid foundation for embodied models. thoughts provide structured reasoning trace that guides decision-making and enables nuanced, context-aware interaction with humans and the environment. To balance memory usage and maintain inference efficiency under token budget constraints (e.g., 32k context length), we retain only the latest visual observations as explicit input. The full sequence of prior thoughts and actions is stored in short-term memory, allowing Robix to reason over recent history without exceeding capacity limits. As shown in Figure 1, Robix unifies the entire interactive task execution processincluding instruction understanding, task planning, task status monitoring, real-time user feedback integration, proactive dialogue, and dynamic replanningthrough grounded, multi-faceted reasoning. Unlike prior modular frameworks for task planning or human-robot interaction, Robix offers significantly greater flexibility, allowing the robot to adapt its behavior in real time to dynamic environmental changes, thereby achieving human-like adaptability."
        },
        {
            "title": "3 Training Recipe",
            "content": "We develop Robix-7B and Robix-32B by continually training Qwen2.5-VL-7B and 32B [3] on approximately 200 billion tokens using three-stage training pipeline. First, we perform continued pretraining to enhance the models capabilities in robot-relevant perception and reasoning (Section 3.1). Next, we apply supervised finetuning to model the complex human-robot interaction and long-horizon task planning as sequential decision-making process grounded in chain-of-thought reasoning (Section 3.2). Finally, we leverage reinforcement learning to further improve the embodied reasoning ability and enhance the alignment between reasoning and action in interactive long-horizon tasks (Section 3.3)."
        },
        {
            "title": "3.1 Continued Pretraining\nA foundational capability of general-purpose embodied models is embodied reasoning—the ability to ground\nobjects and spatial concepts in the physical world and integrate these signals for downstream robotic tasks [64].\nOur objective is to develop a vision-language model with embodied reasoning at its core, capable of generalizing",
            "content": "5 across diverse embodied scenarios while maintaining strong foundational multimodal understanding. To support this, we construct large-scale pretraining corpus comprising 200 billion high-quality and diverse tokens, targeting both robot-relevant and general-purpose multimodal capabilities, as illustrated in Figure 3. In particular, we emphasize 3D spatial understanding, visual grounding, and task-centric reasoning, while also incorporating general visual understanding, multimodal reasoning, and instruction tuning data. Below, we detail the data sources and task types used in our continued pretraining. 3D Spatial Understanding. Current VLMs generally lack strong spatial understanding capabilities, which is crucial for embodied scenarios such as navigation and manipulation planning. To equip the model with 3D spatial understanding from 2D images, we curate over 30 million instruction pairs (about 40B tokens) spanning five key task types: (1) Multi-view correspondence learning 2D point correspondences across stereo or multi-view images of the same scene; (2) 3D bounding box detection predicting metric 3D bounding boxes from monocular images with open-vocabulary object descriptions; (3) Relative depth sorting inferring the depth ordering of objects within single image; (4) Absolute depth estimation estimating absolute object depth using semantic masks and annotated depth maps; (5) Egomotion prediction modeling camera motion over time to support temporal and spatial reasoning. The majority of the data are collected from or derived based on publicly available sources, such as ScanNet [12], ScanNet++ [79], 3RScan [66], CA-1M [35], SUN RGB-D [61], and ARKitScenes [4]. Integrating these five spatial reasoning tasks effectively improves the models spatial awareness in embodied tasks. Visual Grounding. Visual grounding enables multimodal models to interpret user instructions and locate target objects in images. We use two grounding formatsbounding boxes and center pointsand train on four types of data: 2D bounding box annotations, point annotations, counting, and visual prompt. We normalize all coordinate values to the range [0, 1000], allowing consistent grounding predictions across varying image resolutions. Our dataset comprises over 50 million instruction pairs (about 70B tokens), covering the following tasks: (1) 2D bounding box annotations: predict bounding boxes from open-vocabulary descriptions, or generate textual descriptions given bounding box coordinates; (2) Point annotations: predict object center points from descriptions or identify objects based on given coordinates. (3) Counting: derived from bounding box and point data, supporting both boxand point-based counting via two-stage localization and counting pipeline; (4) Visual Prompt: prompts contain both textual instructions and visual annotations (e.g., points, bounding boxes, arrows), enabling the model to learn multimodal fusion and context-aware understanding grounded in visual cues. Together, these tasks significantly enhance the models grounding abilities in both language-to-image and image-to-language directions, and improve its capacity for grounded planning in embodied settings. Task-centric Reasoning. To directly strengthen the models reasoning and planning abilities in embodied scenarios, we construct large-scale embodied task-centric reasoning dataset based on publicly available robot and egocentric datasets, including AgiBot [6], BridgeData V2 [67], Droid [32], Egodex [27], RoboVQA [52], HoloAssist [70], and Ego4D [22]. We curate over 5 million examples (about 10B tokens) targeting three key reasoning functions: (1) Task Status Verificationdetermining whether task or subtask has been successfully completed; (2) Action Affordanceassessing whether an action is feasible in the current context; (3) Next Action Predictionidentifying the most plausible next step to achieve the intended goal. To enrich the reasoning process, we further use Seed-1.5-VL-thinking [24] to generate step-by-step thought traces for our QA pairs via carefully designed prompts (details are shown in Appendix B). This thought-augmented supervision enables the model to learn deliberate, high-level decision-making in dynamic and open-ended environments. General Multimodal Reasoning. To enhance the models general reasoning capabilities, we curate diverse set of over 6 million multimodal instruction-image pairs (about 10B tokens) spanning STEM problem solving, agent-based decision making, and visual inference tasks. Specifically, we include: (1) STEM Reasoning Data: Multimodal problem-solving examples in mathematics, physics, chemistry, and biology, combining textual questions with diagrams, equations, and visual content. (2) Multimodal Agent Data: GUI-based agent demonstrations that involve step-by-step planning, error correction, and reflective reasoning. (3) Visual Inference Data: Tasks that require grounded visual reasoning, including spotting differences between paired images and generating HTML/CSS code from user interface screenshots. Together, these datasets equip the model with robust abstract reasoning and cross-modal problem-solving abilities, supporting its generalization 6 to complex tasks in open environments. General Multimodal Understanding. To preserve and enhance broad vision-language understanding, we curate large-scale dataset of over 50 million imagetext pairs (over 80B tokens) that serve as the foundation for multimodal comprehension. (1) VQA: diverse set of imageand video-based question answering tasks covering visual perception, factual knowledge, grounding, temporal reasoning, spatial understanding, and counting. (2) Captioning: Dense captions for both images and videos, supporting the models understanding of static scenes and multi-frame temporal dynamics. (3) OCR: To improve text recognition, we include large-scale annotated and synthetic datasets covering scene text, documents, tables, charts, and flowcharts. Together, these datasets establish robust foundation for training general-purpose vision-language models. Instruction Tuning. To further enhance the models instruction-following and reasoning abilities, we construct high-quality instruction tuning dataset comprising 1 million examples. These examples span wide range of tasks and are built by extracting curated subsets from previously collected data, integrating both general instructions and chain-of-thought examples from open-source and internal sources. We refine the instructions using Seed-1.5-VL [24] for quality filtering and ensure better alignment between instructions, images, and responses. This instruction-tuned dataset significantly improves the models ability to follow open-world multimodal instructions and engage in multi-turn, grounded reasoning. We adopt two-stage training strategy leveraging the large-scale, diverse corpus described above. In stage 1, we continue pretraining Qwen2.5-VL [3] on the full datasetcomprising approximately 5% text-only dataupdating all model parameters to enhance general multimodal and embodied reasoning capabilities. Training follows full cosine learning rate schedule, starting at 1 105 and decaying to 1 106, with linear warm-up over the first 10% of total steps. We use sequence length of 32,768 tokens, with effective batch sizes of 1536 and 3008 the sequence length for the 7B and 32B models, respectively. In stage 2, we perform instruction tuning on curated instruction-following data to align the model with multimodal prompts and improve instruction adherence. The vision encoder is frozen during this phase, while all other parameters remain trainable. The learning rate is fixed to the final value from Stage 1 (1 106) and remains constant throughout Stage 2. Optimizer states are carried over from Stage 1, and no additional warm-up is used. Both stages are optimized using AdamW [33, 41], with β1 = 0.9, β2 = 0.99, and weight decay of 0.01. Training on this diverse and comprehensive corpus significantly improves the models embodied reasoning, multimodal understanding, and its ability to generalize to long-horizon, interactive tasks in real-world settings."
        },
        {
            "title": "3.2 Supervised Finetuning",
            "content": "The supervised fine-tuning (SFT) stage adapts the preceding pretrained model into the robots high-level cognitive module while retaining its original capabilities. central challenge lies in the scarcity of largescale, multi-turn egocentric-vision datasets that integrate human-robot interaction with task planning. To address this, we design data-synthesis pipeline that transforms existing task-planning datasets into humanrobot interaction trajectories. Two properties of the resulting SFT data are crucial for out-ofdistribution generalization: (1) diverse humanrobot interactions and (2) high-quality reasoning traces. The overall pipeline is shown in Figure 4; the interaction and reasoning synthesis modules are detailed below. 3.2."
        },
        {
            "title": "Interaction Synthesis",
            "content": "The interaction synthesis is mainly based on two data sources: Teleoperated Robot Demonstrations. We utilize both internal teleoperation data (previously used to train GR-3 [7]) and the open-source AGIBot dataset [6]. Each contains episodes of robots performing diverse tasks (e.g., table bussing, breakfast preparation). Human annotators segment each demonstration into clips, where each clip corresponds to an atomic actione.g., put the tissue into the trash bin. Simulation & AIGC Data. We programmatically generate diverse object organization scenarios in our in-house simulator, and further employ state-of-the-art text-to-image models [21] to synthesize complex scenes with items not yet supported in the simulator. To ensure quality, we apply both model-based and human-in-the-loop filtering to discard low-quality generations. 7 Figure 4 Our data synthesis pipeline. The leftmost panel shows the data sources; the center illustrates how diverse human-robot interactions are synthesized from these sources; the rightmost panel presents example snapshots of the generated reasoning traces. Building on the above sources, we define seven types of human-robot interaction instructions and devise dedicated data-synthesis strategies for each. Training on this data equips Robix with flexible interaction capabilities, including understanding of complex instructions, real-time interruption handling, and proactive dialogue. Multi-Stage Instruction. Teleoperated trajectories are annotated with task name (e.g., clean up the table and pack the food). We select trajectories containing at least ten atomic actions and synthesize corresponding user instruction from the task name, e.g., The dining table is mess. Please clean the table and pack the food on the plate. Constrained Instruction. We partition each teleoperation trajectory into non-overlapping segments where feasible. For example, table-bussing task can be decomposed into subtasks such as garbage collection, tableware collection, and food packing. Based on these subtasks, we synthesize tailored user instructions, e.g., Clean up the table while leaving the food on the table and Remove the trash without moving other items. Open-Ended Instruction. We generate random scenes in simulation and prompt state-of-the-art LLMs to produce open-ended, commonsense instructions conditioned on each scene (e.g., Place the drink with the least sugar into the carton for scene containing Sprite, Coke, orange juice, and soda water). To include items currently not supported by the simulator (e.g., hamburgers, spaghetti, watermelons), we pair such instructions with images synthesized by advanced text-to-image models. Because current text-to-image models still struggle with instruction following and image quality, we apply automated and human-in-the-loop filtering, remaining only 10% of the original dataset after filtering. Anytime Interruption. We curate various user interruption utterances (e.g., Stop! , Hold on. still need it, Wait, put the fork into the sink first) and randomly inject them into task flows. We then synthesize robot responses with timing-aware heuristics: if the interruption occurs before grasping, the robot halts or adjusts the plan; if it occurs after grasping, the robot returns the item to the table and replans. Anytime interruption is critical for long-horizon tasks, as mid-task feedback and error correction can substantially improve system robustness. Invalid Instruction. To mitigate hallucination and prevent robots from engaging in dangerous actions, we synthesize four types of invalid instructions as follows: (1) Instructions asking the robot to manipulate items that do not exist in the scenario; (2) Instructions requiring physically impossible actions, such as Put the table into the rubbish bin; (3) Instructions demanding abilities beyond the robots current capabilities, e.g., Open the coke for me; (4) Unsafe or dangerous commands, e.g., Throw the knife onto the sofa. For these invalid or illegal instructions, we design corresponding response strategies to enable the robot to refuse compliance with the users requests. Ambiguous Instruction. To enable our model to clarify ambiguous instructions, we construct scenes with multiple similar items (e.g., apple, orange, pear) and synthesize underspecified instructions(e.g., Put fruit into the basket). Training with these data enable the model to seek clarification when neededa capability essential for robust robot systems. Chat Instruction. We develop some heuristics to randomly insert short human-robot dialogue segments at context-appropriate times. For example, when the robot is collecting trash from the table, the user may ask want some fruit. What kind of fruit is on the table? . This type of instruction requires the robot to respond verbally rather than performing any physical manipulation."
        },
        {
            "title": "3.2.2 Reasoning Synthesis",
            "content": "To incorporate chain-of-thought reasoning, we prompt state-of-the-art VLMs to generate high-quality reasoning traces emphasizing (1) scene understanding, (2) task status reflection, (3) long-term instruction following, and (4) next-step analysis. Scene understanding. This part of reasoning enables the robot to accurately identify task-relevant, operable objects in the current scene, with emphasis on those within the robots field of view. Task status reflection. Robots should be capable of reflecting on their prior actions and repeating tasks when initial attempts fail. Furthermore, they need to identify key milestones in long-horizon tasks and proactively request human assistance upon encountering irrecoverable errors. This capability is also critical for handling user interruptions, as robots must maintain awareness of their current status to plan subsequent actions (e.g., tracking whether gripper is holding an item). Long-term instruction following. This module is designed to help robot persist the initial goal and intermediate user instructions across long-horizon tasks, ensuring the primary objective is completed and mid-task instructions continue to guide actions many steps later (e.g., After cleaning the table, grab me drink from the fridge). Next-step analysis. In the final phase of reasoning, the robot should analyze potential actions for the next step when the overall task remains incomplete. This analysis include assessing target reachability and whether executing the action advances overall task completion. Inspired by UI-TARS [50], we adopt ActRe [78] and Thought Bootstrapping [50] to synthesize high-quality reasoning traces. Unlike conventional LLM reasoning, robot reasoning must remain concise to support real-time interaction. Accordingly, we prompt Seed-1.5-VL to produce succinct traces (within 200 tokens). We also apply model-based filtering pipeline to discard hallucinated or logically inconsistent reasoning. These high-quality, multi-faceted chain-of-thought traces enable Robix to execute robust long-horizon task planning with task-status monitoring and dynamic replanning."
        },
        {
            "title": "3.3 Reinforcement Learning",
            "content": "Following the supervised fine-tuning (SFT) stage, the model exhibits promising agentic capabilities in adaptive task planning and natural human-robot interaction. However, several limitations in robot reasoning and planning persist, notably: (1) irrational reasoning, such as generating conflicting thoughts, lacking common sense, or partially disregarding user instructions; and (2) thought-action inconsistency, where the models proposed plan diverges from its preceding thought in intent or content. For example, in table-cleaning task, the SFT model correctly infers that tissue left on the table should be discarded in rubbish bin. Yet, in the subsequent plan, it incorrectly suggests handling paper cup instead. These issues negatively impact the models effectiveness in real-world task execution. 9 To mitigate these problems, we adopt reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO) [23, 54], to enhance both the reasoning capacity and the coherence between thought and action. Our approach is based on two core strategies: (1) co-training with general visual reasoning data, and (2) reward design targeting thought-action consistency. Co-Training with General Visual Reasoning Data. The RL stage utilizes two primary data sources: robot interaction data and general visual reasoning datasets. Training on robot interaction data improves the models robustness and generalization to out-of-distribution (OOD) scenarios. Meanwhile, incorporating general visual reasoning data strengthens the models inherent reasoning capabilities. This co-training strategy helps alleviate irrational reasoning and enhances overall task understanding and solving. The general visual reasoning datasets include wide range of cognitive challenges, such as task completion verification, action affordance evaluation, and object localizationcovering broad spectrum of reasoning skills relevant to real-world robot applications. Reward Design for Thought-Action Consistency. To explicitly encourage alignment between the models thought and action, we introduce thought-action consistency reward in addition to standard rewards for output formatting and action accuracy. At each decision step, the models generated thought and corresponding action are extracted and evaluated by an external LLM (Qwen-2.5-32B [74] in our experiments). This auxiliary reward model is prompted to assess whether the action is logically consistent with the preceding thought. negative reward is given if the assessment indicates inconsistency. The system prompt of the reward model is listed in Appendix A.5. To maximize the effectiveness of RL training, we also employ data filtering procedure designed to retain only samples that can provide meaningful gradient information for GRPO. The key idea is to discard questions whose candidate answers exhibit low reward variance, as such samples contribute little to policy improvement. Specifically, for each question in the dataset, we generate multiple candidate answers using the SFT model and remove those with low variance in their rewards: (cid:26) Dnew = (xn, n) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "Var",
            "content": "(cid:18)(cid:110) R(y(i) , n) (cid:111)M (cid:19) i= > τ, y(i) πSFT( xn) (cid:27) , (2) based on the ground-truth n where denotes the original dataset, R(y(i) n) is the reward function assigning scalar score to the i-th generated answer y(i) for question xn, and πSFT is the base policy for RL. The definitions of input xn and output follow Equation (1): xn consists of the current observation, instruction, and trajectory, while yn comprises the models thought, an optional action, and an optional robot response. In our experiments, we set the number of samples to 8 and the variance threshold τ to 0. All RL training is performed using the verl framework [55]. , Through the combination of co-training with diverse reasoning data and targeted reward design, our reinforcement learning strategy substantially improves the models generalization to novel tasks and enhances the consistency between reasoning and planning."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct extensive experiments to comprehensively evaluate the performance of Robix, focusing on the following key questions: Does Robix enhance fundamental embodied reasoning capabilities? Can Robix effectively model the full process of interactive task execution in an end-to-end manner? Does Robix generalize well to out-of-distribution tasks? How does the full robotic system perform when integrating Robix with VLA model on real-world tasks? We first evaluate fundamental embodied reasoning capabilities of our model and baseline methods on public benchmarks (Section 4.1). Next, we assess the planning and interaction abilities of our model and baseline methods with both offline, pre-defined test sets (Section 4.2) and online robotic tasks (Section 4.3). 10 Benchmark VSIBench BLINK CV-Bench EmbSpatial SAT VSR SpatialBench DA-2k LVIS-MG Refcocoval Refcoco+val Refcocogval Refcocouval VisualWebBench Pixmo-Point Where2Place Agibot-ER EgoTaskQA OpenEQA-hm3d OpenEQA-scannet ERQA RoboVQA MME MMbench RealworldQA SimpleVQA EgoSchema VideoMME NextQA MathVista MMMU Robix 7B Base Robix 32B Base Qwen 2.5 VL 7B Qwen 2.5 VL 32B Cosmos Reason1 7B RoboBrain 2.0 32B Gemini 2.5 Pro OpenAI GPT 4o Seed 1.5-VL Seed 1.5-VL Think 44.6 87.6 86.5 77.4 71.1 83.3 64.7 72. 70.2 89.2 83.8 87.1 87.1 63.2 29.5 41.9 61.0 28.6 48.8 58.0 42.5 53.6 50.9 83.5 87.4 79.0 79.6 83.7 65.4 77.1 79.2 91.5 86.2 89.0 88.6 68.9 47.3 45. 62.6 33.6 51.1 58.9 43.5 48.3 3D Spatial Understanding 39.1 78.3 81.6 72.3 74.9 83.6 66.8 68.6 33.9 69.7 76.0 64.6 60.7 79.9 61.6 65.8 Visual Grounding 54.2 89.3 83.2 87.3 86.9 66.1 41.3 39. 18.2 77.1 69.2 75.0 74.2 51.5 6.0 11.4 45.2 82.4 83.7 76.6 80.3 84.0 68.8 56.9 - - - - - 20.5 46.0 73.6 Embodied Task-centric Reasoning 55.4 31.3 46.8 52.8 42.8 60.0 38.0 28.4 40.0 50.9 39.3 55.4 54.3 31.5 48.7 55.9 46.0 55.7 38.2 86.5 79.0 70.1 52.8 82.2 61.7 65. 30.6 88.9 82.6 85.6 85.5 59.4 43.5 33.0 48.2 23.7 42.2 51.3 38.0 54.0 43.4 87.6 85.7 77.2 79.5 82.5 66.0 83.0 63.8 74.6 - - - 87.3 11.3 39.9 67.1 37.0 63.8 74.3 55.0 33.9 42.5 80.5 79.7 70.6 62.4 77.5 63.7 78.2 - - - - - 80.2 10.8 26.9 49.9 30.0 63.4 71.3 47.8 34.5 34.0 79.0 77.9 68.0 67.5 78.1 64.0 86.5 73.8 92.8 89.3 90.1 88.9 88.0 13.7 18.4 63.6 27.8 58.8 66.1 39.8 37.0 39.5 82.0 82.4 73.5 72.7 80.1 66.3 87.5 72.5 92.6 89.7 90.4 88.5 87.3 9.8 25.4 60.5 31.1 60.8 68.3 47.5 35.1 Multimodal Understanding & Reasoning 2332.8 87.6 70.7 44.7 66.2 63.7 82.5 68.0 51.6 2427.2 89.1 69. 49.0 73.4 67.6 81.6 69.6 58.9 2273.1 86.8 65.9 47.7 67.6 65.3 82.9 68.3 51.9 2425.5 89.2 68.4 45.2 74.0 68.8 81. 75.6 57.3 2150.0 85.1 67.8 42.5 59.8 61.9 79.5 62.1 47.0 2462.3 88.6 63.9 41.9 70.8 66.9 78.7 68.7 59.2 2491.3 90.1 78.0 62.0 74.1 86.9 83.5 82. 81.7 2271.9 84.3 76.2 50.1 69.8 71.9 72.7 63.8 70.7 2314.4 88.0 77.0 63.1 64.3 77.6 62.6 83.0 73.6 2470.8 89.9 78.4 63.4 77.2 77.9 68.6 85.6 77. Collected by ourselves via API in July 2025. Invalid results due to failures in following format requirements. Table 1 Performance of Robix on public vision-language benchmarks compared to prior models. The left side shows Robix and state-of-the-art open-source baselines, while the right side presents closed-source large commercial models. The highest score in each benchmark is highlighted in bold within each group."
        },
        {
            "title": "4.1 Fundamental Perception & Reasoning Evaluation",
            "content": "We evaluate Robix after continued pretraining (denoted as Robix-Base) on comprehensive set of public benchmarks against state-of-the-art multimodal models including Qwen-2.5-VL-7B&32B [3], RoboBrain-2.032B [63], Cosmos-Reason1-7B [2], Gemini-2.5-Pro [64], OpenAI GPT-4o [30], Seed-1.5-VL and Seed-1.5-VLThink [24]. The evaluation spans (1) robotics-relevant embodied reasoning (3D spatial understanding, visual grounding, task-centric reasoning) and (2) general multimodal understanding and reasoning. Table 1 presents the detailed results. 3D Spatial Understanding. We evaluate Robixs 3D spatial understanding across 8 spatial reasoning benchmarks: VSIBench [75], BLINK [20], CV-Bench [65], EmbSpatial [16], SAT [51], VSR [37], SpatialBench [16], and DA-2k [76]. Details of these benchmarks are provided in Appendix A.1. As shown in Table 1, Robix-7B and Robix-32B outperform their backbones (Qwen2.5-VL-7B/32B) on 7 of 8 spatial reasoning tasks, with average accuracies of 73.4 and 75.8 compared to 66.9 and 70.7, respectively. They also surpass embodied models Cosmos-Reason1-7B (64.0) and RoboBrain-32B (72.2), and exceed the best commercial baseline, Gemini-2.5-Pro, on 5 of 8 tasks. Visual Grounding. We evaluate Robixs visual grounding capabilities on eight benchmarks covering both 11 bounding-box and center-point tasks, including LVIS-MG [25], RefCOCO [80], VisualWebBench [39], PixmoPoint [14], and Where2Place [81]. Robix consistently outperforms its backbone across all benchmarks and surpasses state-of-the-art commercial models on most tasks. Notably, Robix-7B and Robix-32B improve the absolute F1 score on LVIS-MG by 39.6 and 25.0 points over Qwen2.5-VL-7B and 32B, respectively. Robix-32B also outperforms commercial models on most tasks. These results highlight Robixs strong performance in object localization, pointing and fine-grained visual understanding. Task-centric Reasoning. Embodied reasoning reflects models ability to understand and reason about robotic tasks. We evaluate Robix across 5 diverse open benchmarks, including ERQA [34], RoboVQA [52], OpenEQA (HM3D & ScanNet) [43], and EgoTaskQA [31]. In addition, we introduce Agibot-ER, real-world task reasoning benchmark derived from the Agibot dataset [6], which includes manually annotated test sets of 97, 120, and 381 samples for the three key reasoning tasksTask Status Verification, Action Affordance, and Next Action Prediction, respectively. Full details of the benchmark are provided in Appendix B. We report the average results of the three tasks on this benchmark. Robix consistently outperforms its backbone models as well as Cosmos-Reason1-7B and RoboBrain-2.0-32B across most benchmarks. On Agibot-ER, Robix delivers substantial gains over its backbones, improving absolute accuracy by 12.8 and 7.2 points for the 7B and 32B versions, respectively. It further surpasses Cosmos-Reason1-7B and RoboBrain-2.0-32B by 23 and 8.3 points, demonstrating superior performance in embodied, task-centric reasoning. General Multimodal Understanding & Reasoning Multimodal understanding is core capability of visionlanguage models and primary focus of VLM development. To evaluate both static image and dynamic video understanding, we assess Robix on suite of general VQA benchmarksimage-based (MME [18], MMBench [40], RealWorldQA [71], SimpleVQA [9]) and video-based (EgoSchema [44], VideoMME [19], NextQA [73]). We further test general reasoning on MathVista [42] and MMMU [82], which cover complex mathematical and multimodal problem-solving tasks. Robix preserves the performance of its backbone on most benchmarks, demonstrating the benefit of training with diverse, high-coverage multimodal data, but still trails large-scale commercial modelsunderscoring the need to scale both data and model size for stronger general-purpose multimodal reasoning. Overall, Robix greatly enhances robotics-relevant perception and reasoningparticularly in 3D spatial understanding and visual groundingwhile maintaining strong performance on general multimodal tasks. These gains deepen its understanding of spatial and temporal properties, enabling more effective reasoning and planning in real-world environments."
        },
        {
            "title": "4.2 Offline Evaluation",
            "content": "The offline evaluation enables fully automated assessment of planning and interaction capabilities using predefined evaluation sets. To thoroughly evaluate both interactive long-horizon planning and out-ofdistribution (OOD) generalization, we design three dedicated evaluation sets: AGIBot Evaluation Set. We manually select 16 high-frequency daily tasks from the AGIBot dataset (e.g., making sandwich, washing dishes with dishwasher, arranging sofa, washing clothes with washing machine, arranging flowers) and ensure none appear in the training data. This set primarily evaluates the models long-horizon task planning capability on OOD tasks. Details are provided in Appendix A.2. Internal Out-of-Distribution (OOD) Benchmark. We manually design 16 scripts covering task planning and diverse humanrobot interaction scenarios, including table organization, dietary filtering, checkout packing, grocery shopping, and shoe cabinet organization. These scripts are enacted by human participantsone acting as the user and the other executing actions via robot teleoperation or Universal Manipulation Interface (UMI) [10] deviceand subsequently annotated by trained annotators. The benchmark includes tasks and items absent from the training data and is intended to evaluate interactive task execution in unseen scenarios. Internal In-Distribution (ID) Benchmark. This evaluation set is randomly sampled from our synthesized data and categorized by task type and user instruction into six groups: (1) multi-stage instructions, (2) constrained instructions, (3) invalid instructions, (4) user interruptions, (5) fail-and-replan, and (6) openended instructions. Each category targets evaluation of the models corresponding instruction following 12 AGIBot Internal OOD Multi-Stage Constrained Interrupt Open-Ended Invalid Replan Internal ID # episodes # data 16 142 16 225 9 119 25 233 110 15 60 100 Table 2 Statistics of the offline evaluation sets. AGIBot Internal OOD Multi. Const. Interrupt Open. Invalid Replan Plan Accuracy F1 score F1 score Internal ID Gemini-2.5-Pro GPT4-o Seed-1.5-VL Seed-1.5-VL-Think Qwen-2.5-VL-72B Qwen-2.5-VL-32B Qwen-2.5-VL-7B GLM-4.1-9B-Think RoboBrain-2.0-32B RoboBrain-2.0-7B Robix-7B-SFT-wo-R Robix-7B-SFT Robix-7B-RL Robix-32B-SFT Robix-32B-RL 52.6 45.9 37.4 49.6 36.7 43.3 31.1 34.1 29.6 0.3 55.2 57.8 59.6 64.0 64.4 83.8 77.0 73.2 80.4 69.2 71.6 54.7 51.7 63.5 31. 69.9 77.1 85.4 83.5 86.8 79.3 76.1 75.4 73.9 71.3 60.5 37.5 22.8 58.2 36.0 82.5 85.8 93.2 89.3 96.6 87.1 84.4 76.0 82.7 65.1 62.2 41.5 45.8 51.7 33.1 89.0 91.1 90.3 93.0 96.0 55.9 44.8 41.1 42.9 55.2 48.0 20.5 14.0 41.2 25. 91.5 84.2 78.6 89.7 92.5 60 66.7 46.7 46.7 26.7 26.7 6.7 6.7 0.0 0.0 60.0 86.7 86.7 80.0 93.3 98.3 79.2 100 74.2 87.0 70.2 47.5 86.0 43.6 0.0 100 100 95.9 100 100 83.7 73.7 36.8 75.1 40.0 37.0 10.9 37.2 29.9 22. 90.5 88.4 87.0 95.1 96.2 Table 3 Offline evaluation results. Robix-7B-SFT-wo-R refers to our SFT model without chain-of-thought reasoning, while Robix-7B-RL denotes the full trained policy obtained by applying RL after SFT. For AGIBot, Internal OOD (Out-of-Distribution), and Internal ID (In Distribution)MultiStage/Constrained/Interrupt/OpenEnded, we report plan accuracy; for Internal IDInvalid/Replan, we report F1 score. The best result for each evaluation set is shown in bold, and the best among baselines is underlined. and task planning capabilities. Data Format. The overall statistics of each evaluation set are shown in Table 2. Each episode is structured as multi-turn dialogue and evaluated using teacher-forcing approach, i.e., the model observes an error-free interaction and planning history when predicting the next step action. Because an observationinstruction pair may permit multiple valid next actions, we annotate candidate action list for each step to capture all acceptable options. Examples of the offline evaluation format are provided in Appendix A.3. Evaluation metrics. For AGIBot, Internal OOD, Internal IDMultiStage/Constrained/Interrupt/OpenEnded, we report action prediction accuracy by matching the predicted action against candidate action list, with similarity judged by Seed-1.5-VL (see Appendix A.5 for the prompt). For Internal IDInvalid/Replan, which are binary classification tasks, we report the F1 score. Baseline methods. We compare against widely used commercial and open-source VLMs, including Gemini-2.5Pro, GPT-4o, Seed-1.5-VL, Seed-1.5-VL-Think, Qwen2.5-VL-7B/32B/72B, GLM-4.1V-9B-Thinking [26], and RoboBrain-2.0-7B/32B. All baselines are adapted to the multi-turn observationthinkaction format using the prompts in Appendix A.5. For each model, we test both English and Chinese prompts and report the better result. Gemini-2.5-Pro and GPT-4o perform better with English prompts, whereas the other models achieve higher accuracy with Chinese prompts. All evaluations are conducted using greedy decoding. Results. The offline evaluation results for each model are presented in Table 3. Key observations include: Robix-32B-RL ranks first on all evaluation sets, demonstrating strong task planning and humanrobot 13 Figure 5 Online evaluation results with human labeler operating UMI device as the low-level controller. interaction capabilities, and substantially outperforming all open-source and commercial VLMs on both ID and OOD benchmarks. Chain-of-thought reasoning is critical for both OOD generalization and complex instruction following. Robix-7B-SFT without reasoning (Robix-7B-SFT-wo-R) exhibits drop of over 7 percentage points in accuracy on the Internal OOD benchmarks compared to its reasoning-enabled counterpart, and suffers 26.7-point decline on the IDOpenEnded tasks. RL is critical, boosting Robix-32Bs performance on nearly all evaluation sets. On the challenging Internal OOD benchmarks, Robix-7B-RL and Robix-32B-RL improve accuracy by 8.3 and 3.3 points, respectively, compared to their SFT counterparts. As shown in the case study (Appendix C), RL primarily enhances the SFT models by (i) reducing irrational reasoning steps, (ii) improving thought-action consistency, and (iii) minimizing formatting errors. Gemini-2.5-Pro is the strongest baseline, ranking first on most evaluation sets among baseline methods. Our evaluation suggests it is currently the leading foundation model for embodied AI applications."
        },
        {
            "title": "4.3 Online Evaluation",
            "content": "While offline evaluation is cost-effective, it is limited to static environments and cannot assess models ability to interact with the dynamic physical world. To address this, we deploy our model and baselines within hierarchical robot system across diverse real-world settingsincluding kitchens, meeting rooms, and grocery storesand conduct online evaluations to measure their effectiveness as high-level planning and interaction modules for daily tasks. We design two sets of experiments: Online evaluation of VLMs. Assess the planning and interaction capabilities of VLMs in isolation, without the influence of low-level controllers. Online evaluation of the VLM-VLA robot system. Assess the end-to-end system performance by pairing the VLM with an automatic VLA model as the low-level controller. In the first set of experiments, VLMs serve as the high-level planning and interaction module, while human labelers equipped with Universal Manipulation Interface (UMI) [10] device act as the low-level controller, enabling evaluation under fully reliable control setting. In the second set, we use our in-house VLA model GR-3 [7], as the low-level controller and deploy the integrated VLMVLA system on the ByteMini robot [7]. Robix is deployed with customized inference optimization techniques [86, 87] to reduce response latency. 14 Figure 6 Online evaluation on the ByteMini robot with GR-3 model as the low-level controller."
        },
        {
            "title": "4.3.1 Online Evaluation of VLMs",
            "content": "Experimental settings. We designed five taskstable bussing, checkout packing, dietary filtering, grocery shopping, and tableware organization & deliveryspanning diverse environments such as kitchens, meeting rooms, and grocery stores. To increase realism, some tasks deliberately incorporate user interruptions as well as fail-and-replan scenarios. For details on the initial states and brief descriptions of each task, please refer to Appendix A.4. We further annotated each task with subtasks to assess completion (e.g., for table bussing: tissue is in the rubbish bin, plate is in the basket). We compare Robix-32B 1 with four baselines that performed well in the offline evaluation: Gemini-2.5-Pro, GPT-4o, Seed-1.5-VL-Think, and Qwen2.5-VL-32B. To reduce experimental variability, each taskmodel pair is repeated four times, and we report the average results. Following [57], we use task progressthe percentage of subtasks completed by the end of the taskas the evaluation metric. Trained human annotators assess task progress to ensure reliability and consistency. Results. The online evaluation results are shown in Figure 5. Both Robix-32B and Gemini-2.5-Pro rank first in 3 of the 5 tasks, with Robix-32B achieving slightly higher average task progress (92.6% vs. 91%), demonstrating its superior performance in dynamic real-world environments. Robix-32B also outperforms Qwen2.5-VL-32B by large margin (92.6% vs. 28%), underscoring the effectiveness of our training pipeline. Gemini-2.5-Pro remains the strongest baseline, showing strong capabilities in following complex instructions. However, deploying large foundation models directly for planning and interaction introduces major challengehigh response latency. In our experiments, Gemini-2.5-Pro sometimes required over 30 seconds to respond. While customized deployment may help reduce latency, we contend that current large-scale commercial VLMs remain too computationally heavy for real-time interaction, even on advanced hardware."
        },
        {
            "title": "4.3.2 Online Evaluation of the VLM-VLA system",
            "content": "Experimental Settings. We select three tasks from the online evaluationtable cleaning, dietary filtering, and checkout packingas the evaluation set, excluding the remaining two tasks that require actions beyond GR-3s current capabilities. To better isolate the high-level cognitive layers performance, we also remove particularly challenging items to reduce frequent manipulation failures. Following the VLM online evaluation protocol, each taskmodel pair is evaluated four times, and we report average results using task progress as the metric. All experiments are conducted with the GR-3 model and the ByteMini robot [7]. Results. We compare Robix-32B with the two strongest baselines from both offline and online evaluations: Gemini-2.5-Pro and GPT-4o. Figure 6 shows the results across the three real-world tasks. The findings mirror those in Section 4.3.1: Robix-32B achieves an average task progress of 92.5%, exceeding Gemini-2.5-Pro and GPT-4o by 4.3 and 28.1 percentage points, respectively. We find that baseline methodsparticularly GPT-4osometimes generate actions that are semantically correct but unrecognizable to the VLA. For 1By default, Robix-32B refers to Robix-32B-RL. 15 instance, the VLA can execute put the Oreo into the shopping basket but fails to interpret put the biscuit box into the shopping basket. Such VLMVLA misalignment mainly accounts for the online performance drop observed in Gemini-2.5-Pro and GPT-4o."
        },
        {
            "title": "5 Related Work",
            "content": "Robotic Task Planning. Solving complex, long-horizon tasks in open environments demands robust high-level planning. Vision-Language Models (VLMs) have advanced robotic task planning by grounding high-level instructions in perceptual context [47, 68, 77]. Unlike Large Language Models (LLMs), which often generate ungrounded or physically infeasible plans due to lack of environmental perception [29, 60], VLMs integrate visual understanding with language reasoning to enable open-vocabulary instruction following and closed-loop planning. Systems such as COME-robot [88], VILA [28], and REPLAN [59] leverage GPT-4V to generate executable plans directly from raw visual observations and iteratively refine them based on environmental feedback, improving robustness through situated reasoning and failure recovery. Despite these advances, VLM-based approaches face persistent challenges: they struggle to maintain long-term consistency, exhibit limited embodied reasoning for grounding objects and spatial concepts in the physical world, and fail to fully integrate these signals for adaptive, task-centric planning. Addressing these issues is essential for scaling VLM-based planning to real-world, long-horizon embodied tasks. Moreover, most existing methods focus solely on task planning while overlooking the humanrobot interaction capabilities during task execution that are crucial for truly generalist robotic system. Human-Robot Interaction. Existing work on humanrobot interaction primarily focuses on enabling seamless, natural communication through real-time feedback and corrections. Early model-based systems grounded language in symbolic environment representations [45, 48, 49, 62], whereas recent learning-based methods adopt hierarchical architectures to directly interpret and act on user feedback [5, 13, 15, 38, 46, 56, 58, 72]. Examples include OLAF [38], which uses GPT-4 to re-label actions and update visuomotor policies from corrections; YAY Robot [56], which integrates feedback into an iterative training loop but is limited by prompt diversity; RT-H [5], which supports language-based intervention but restricts corrections to fixed spatial moves; and RACER [13], which combines VLM supervisor with physics simulation for recovery guidance. Hi Robot [57] advances these approaches by grounding real-time corrections in the robots own observations, enabling interpretation and execution of complex instructions beyond prior systems capabilities. However, achieving flexible interaction alongside adaptive task planning requires strong reasoning capacitya challenge Robix addresses by leveraging chain-of-thought reasoning to unify complex task planning and humanrobot interaction within single model. Robix further introduces novel interaction capabilities, including proactive dialogue to clarify ambiguous instructions or infer user intent, and context-aware commonsense reasoning. Embodied Reasoning. Embodied reasoning is the capacity of visionlanguage models (VLMs) to ground objects, spatial concepts, and physical interactions in the real world, and to integrate these signals into downstream robotic tasks [64]. Unlike abstract symbolic reasoning, it is inherently action-oriented, requiring agents to interpret dynamic environments, plan context-aware behaviors, and adapt through feedback. Recent advances span model design, data curation, and task-specific optimization. Embodied-Reasoner [85] learns observationthoughtaction trajectories enriched with spatial reasoning and self-reflection for visual search. Gemini Robotics-ER [64] embeds embodied reasoning into its core VLM, achieving strong generalization across tasks such as 3D perception, pointing, state estimation, and affordance prediction. Data-driven approaches include Cosmos-Reason1 [2], which curates datasets emphasizing task-centric reasoning, and RoboBrain-2.0 [63], which synthesizes spatialtemporal reasoning datasets augmented with thought traces for causal chain learning. Task-specific methods include EvolveNav [36], which improves visionlanguage navigation via formalized CoT fine-tuning and self-reflective post-training; and ECoT [83], which trains visionlanguageaction models for multi-step reasoning over plans, sub-tasks, motions, and grounded visual features before action generation. However, effectively leveraging embodied reasoning to develop generalist robotic systems capable of interactive, long-horizon task execution remains underexplored. Robix addresses this gap by integrating robot reasoning, task planning, and natural language interaction to enable seamless interaction with both humans and physical environments, advancing toward general-purpose embodied intelligence."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents Robix, unified vision-language model that integrates robot reasoning, adaptive task planning, and human-robot interaction. Serving as the high-level cognitive layer of hierarchical robot system, Robix enables robots to execute interactive, long-horizon tasks in open environments with high flexibility. It demonstrates flexible interaction capabilities, including proactive dialogue to clarify ambiguous instructions or infer user intent, real-time interruption handling, and context-aware commonsense reasoning. Experimental results show that Robix delivers strong performance on real-world robotic tasks and exhibits robust generalization in out-of-distribution settings. Limitations & Future Work. Similar to other state-of-the-art multimodal models, Robix has several limitations. In highly dynamic tasks with frequent scene transitions, it may produce hallucinations, flawed reasoning, or exhibit gaps in physical commonsense. Additionally, Robix relies on short-term context windows to process interaction history, functioning as form of short-term memory. Long-term interactive scenarios, however, require more advanced memory mechanismsspecifically, long-term memory with dynamic updates, efficient retrieval, and effective utilization, akin to context engineering in large language models. Addressing these challenges will be primary focus of future work."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We thank Wanli Peng, Yongyu Yan, and Tingshuai Yan for their assistance with model deployment and inference optimization. We are also grateful to Baifeng Xie, Lihao Liu, and Yangang Zhang for their support in utilizing the internal simulation platform. We further thank the GR-3 team for providing the teleoperation data, GR-3 model, and ByteMini robot used in our experiments. Finally, we thank Xiao Ma for his valuable suggestions on the writing of this paper."
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. [2] Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Yuri Feigin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry, Brandon Joffe, Arik Schwartz, and Elad Shulman. ARKitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [5] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, et al. Rt-h: Action hierarchies using language. arXiv preprint arXiv:2403.01823, 2024. [6] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [7] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. Gr-3 technical report. arXiv preprint arXiv:2507.15493, 2025. [8] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv:2412.04453, 2024. [9] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. arXiv preprint arXiv:2502.13059, 2025. [10] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. In Proceedings of Robotics: Science and Systems (RSS), 2024. [11] Simone Colombani, Dimitri Ognibene, and Giuseppe Boccignone. One to rule them all: natural language to bind communication, perception and action. arXiv preprint arXiv:2411.15033, 2024. [12] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [13] Yinpei Dai, Jayjun Lee, Nima Fazeli, and Joyce Chai. Racer: Rich language-guided failure recovery policies for imitation learning. arXiv preprint arXiv:2409.14674, 2024. [14] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. [15] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. 2023. [16] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2024. [17] Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, and Ranjay Krishna. Manipulate-anything: Automating real-world robots using vision-language models. arXiv preprint arXiv:2406.18915, 2024. [18] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296, 2024. [19] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. [20] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [21] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Linjie Yang, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, and Weilin Huang. Seedream 2.0: native chinese-english bilingual image generation foundation model, 2025. [22] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [25] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 18 [26] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. [27] Ryan Hoque, Peide Huang, David Yoon, Mouli Sivapurapu, and Jian Zhang. Egodex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025. [28] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842, 2023. [29] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. [30] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [31] Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human tasks in egocentric videos. Advances in Neural Information Processing Systems, 35:33433360, 2022. [32] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [33] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. [34] Anastasia Kirillova, Eugene Lyapustin, Anastasia Antsiferova, and Dmitry Vatolin. Erqa: Edge-restoration quality assessment for video super-resolution. arXiv preprint arXiv:2110.09992, 2021. [35] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2222522233, 2025. [36] Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, et al. Evolvenav: Self-improving embodied reasoning for llm-based vision-language navigation. arXiv preprint arXiv:2506.01551, 2025. [37] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [38] Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov, and Ching-An Cheng. Interactive robot learning from verbal correction. arXiv preprint arXiv:2310.17555, 2023. [39] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024. [40] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [42] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [43] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1648816498, 2024. 19 [44] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [45] Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. Learning to parse natural language commands to robot control system. In Experimental robotics: the 13th international symposium on experimental robotics, pages 403415. Springer, 2013. [46] Sabrina McCallum, Max Taylor-Davies, Stefano Albrecht, and Alessandro Suglia. Is feedback all you need? leveraging natural language feedback in goal-conditioned rl. In NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning, 2023. [47] Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, and Zhongxue Gan. Replanvlm: Replanning robotic tasks with visual language models. IEEE Robotics and Automation Letters, 2024. [48] Namasivayam, Himanshu Singh, Vishal Bindal, Arnav Tuli, Vishwajeet Agrawal, Rahul Jain, Parag Singla, and Rohan Paul. Learning neuro-symbolic programs for language guided robot manipulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 79737980. IEEE, 2023. [49] Siddharth Patki, Andrea Daniele, Matthew Walter, and Thomas Howard. Inferring compact representations for efficient natural language understanding of robot instructions. In 2019 International Conference on Robotics and Automation (ICRA), pages 69266933. IEEE, 2019. [50] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [51] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, et al. Sat: Dynamic spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [52] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 645652. IEEE, 2024. [53] Rutav Shah, Albert Yu, Yifeng Zhu, Yuke Zhu, and Roberto Martín-Martín. Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation. arXiv preprint arXiv:2410.06237, 2024. [54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [55] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [56] Lucy Xiaoyang Shi, Zheyuan Hu, Tony Zhao, Archit Sharma, Karl Pertsch, et al. Yell at your robot: Improving on-the-fly from language corrections. arXiv preprint arXiv:2403.12910, 2024. [57] Lucy Xiaoyang Shi, brian ichter, Michael Robert Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. In International Conference on Machine Learning, 2025. [58] Utsav Singh, Pramit Bhattacharyya, and Vinay Namboodiri. Lgr2: Language guided reward relabeling for accelerating hierarchical reinforcement learning. arXiv preprint arXiv:2406.05881, 2024. [59] Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Alán Aspuru-Guzik, and Animesh Garg. Replan: Robotic replanning with perception and language models. arXiv preprint arXiv:2401.04157, 2024. [60] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. [61] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567576, 2015. 20 [62] Agnes Swadzba, Sven Wachsmuth, Constanze Vorwerg, and Gert Rickheit. computational model for the alignment of hierarchical scene representations in human-robot interaction. In IJCAI, pages 18571863, 2009. [63] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. [64] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [65] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [66] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76587667, 2019. [67] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. [68] Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, and Chen Feng. Vlm see, robot do: Human demo video to robot action plan via vision language model. arXiv preprint arXiv:2410.08792, 2024. [69] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [70] Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, et al. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2027020281, 2023. [71] xAI. Realworldqa: benchmark for real-world spatial understanding. 2024. URL https://huggingface.co/ datasets/xai-org/RealworldQA. [72] Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, and David Hsu. Robi butler: Remote multimodal interactions with household robot assistant. arXiv e-prints, pages arXiv2409, 2024. [73] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to In Proceedings of the IEEE/CVF conference on computer vision and pattern explaining temporal actions. recognition, pages 97779786, 2021. [74] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [75] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [76] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [77] Zhutian Yang, Caelan Garrett, Dieter Fox, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. Guiding long-horizon task and motion planning with vision language models. arXiv preprint arXiv:2410.02193, 2024. [78] Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. React meets actre: Autonomous annotation of agent trajectories for contrastive self-training. In First Conference on Language Modeling, 2024. [79] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [80] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. 21 [81] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. [82] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [83] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. [84] Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. Hirt: Enhancing robotic control with hierarchical robot transformers. arXiv preprint arXiv:2410.05273, 2024. [85] Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, et al. Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. arXiv preprint arXiv:2503.21696, 2025. [86] Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, et al. Triton-distributed: Programming overlapping kernels on distributed ai systems with the triton compiler, 2025. URL https://arxiv.org/abs/2504.19442. [87] Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, et al. Tilelink: Generating efficient compute-communication overlapping kernels using tile-centric primitives. arXiv preprint arXiv:2503.20313, 2025. [88] Peiyuan Zhi, Zhiyuan Zhang, Yu Zhao, Muzhi Han, Zeyu Zhang, et al. Closed-loop open-vocabulary mobile manipulation with gpt-4v. arXiv preprint arXiv:2404.10220, 2024."
        },
        {
            "title": "A Experiment details",
            "content": "A.1 Fundamental Evaluation Prompts The prompt templates for all benchmarks are listed below. In each template, {question} is replaced with the samples actual question, {options} with its multiple-choice answer options, <image> with the computed ViT embeddings of the input image, and <video> with the ViT embeddings of the video frames (for video benchmarks). VSIBench. We use the official metric of VSIBench [75]. <video> {question} Options: Answer with the options letter from the given choices directly. {options} BLINK. Following RoboBrain 2.0 [63], we evaluate BLINK [20] on two sub-tasksspatial relation understanding and depth perceptionand report the average accuracy across them. <image> {question} {options} Answer with the options letter from the given choices directly. CV-Bench. We follow the official evaluation protocol of CV-Bench [65] and report average accuracy over the three defined sub-tasks. <image> {question} {options} Answer with the options letter from the given choices directly. EmbSpatial. We use the official metric of EmbSpatial-Bench [16] and report the average accuracy. <image> {question} Please answer with single option letter: {options} SAT. We use the official metric of SAT [51] and report the average accuracy across its five sub-tasks. <image> {question} {options} Please only answer with the option letter. VSR. We use the official metric of VSR [37] and report the average accuracy. <image> {question} Answer with single word yes or no. 23 SpatialBench. We use the official metric of SpatialBench [16]. <image> {question} Please answer with single word: yes or no (for existence task). DA-2k. Since some VLMs tend to repeatedly output the same option on DA-2k [76], we shuffle the answer choices and report the average accuracy. <image> {question} Only provide the answer: point1 or point2. LVIS. We report results on LVIS [25] using the F1-score 2. <image> Ground all {object} in this image. Please provide all bounding box coordinates in the format: <box_start>(x1, y1),(x2, y2)<box_end>. Each bounding box should contain: - (x1, y1): the coordinates of the upper-left corner - (x2, y2): the coordinates of the lower-right corner All coordinates must be **normalized to the range [0, 1000]**, where: - refers to the horizontal axis (image width) - refers to the vertical axis (image height) If **no object is detected**, simply output: **No instance found.** RefCOCO & RefCOCO+ & RefCOCOg & RefCOCOu. We use the official metric of RefCOCO [80]. <image> What are the coordinates of the {object} in this image? Please provide the bounding box coordinates of the {object} in the format: start>(x1, y1),(x2, y2)<box_end>, where (x1, y1) for upper-left, and (x2, y2) for lower-right. All coordinates should be normalized to [0, 1000] scale, where corresponds to the horizontal axis (image width), corresponds to the vertical axis (image height). <box_Pixmo-Point & Where2Place. For these two benchmarks [14, 81], we compute the proportion of predicted points that fall within the referring objects. <image> Locate several points for the {object}. You can mark them using <point>x y</point>. Please provide point coordinates in the format: The coordinates of the point (x, y) must be **normalized to the range [0, 1000]**, where: - refers to the horizontal axis (image width) - refers to the vertical axis (image height) <point>(x1, y1)</point> ... 2Since the baseline models (e.g., Qwen-2.5-VL, Cosmos-Reason1, RoboBrain-2.0, etc.) cannot follow unified output format on the visual grounding benchmarksincluding LVIS, RefCOCO, Pixmo-Point, and Where2Placewe revise their prompts to adapt to their output formats. 24 VisualWebBench. We use the official metric of VisualWebBench [39]. <image> {question} ERQA. We report accuracy on ERQA [64]. <image>...<image> {question} {options} Please answer directly with only the letter of the correct option and nothing else. RoboVQA & EgoTaskQA & OpenEQA-hm3d & OpenEQA-scannet. We compute accuracy by using GPT-4o to compare model responses with the ground-truth labels, and the evaluation prompt is shown below. <video> {question} Prompt for GPT-4o (RoboVQA & EgoTaskQA). You are an AI assistant tasked with evaluating whether response matches the correct answer to given question. Evaluation Rules (1) Output 1 if the response matches the answer exactly or with synonymous/equivalent wording. - Synonyms, paraphrases, or different surface forms of the same meaning count as matches. - Minor wording differences (e.g., put tomato into fridge vs. in the fridge) count as matches. the person is putting tomato (2) Output 0 if the response is incorrect, contradictory, or refers to different entity, object, or attribute. - If the answer and response describe different objects, actions, or states, mark as 0. - If the response introduces additional details that change the meaning of the answer, mark as 0. Output 1 if the response conveys essentially the same meaning as the If the response overlaps but misses or alters essential details (e.g., Special Cases - Similar meaning: answer and does not omit or add critical information (e.g., answer:put meat on the table, response:The person moved meat from the fridge to the counter.). - Partial matches: answer:put meat and tomato on the table vs. - Granularity differences: (e.g., answer:woman, response:Jessica), output 1. - Yes/No questions: mismatch outputs 0, regardless of explanation. - Ambiguity: If the response cannot be reasonably interpreted as equivalent to the answer, output 0. Only output 1 if the polarity matches (yes <-> yes, no <-> no). response:put meat on the table), output 0. If the response is more specific but still semantically equivalent Any Examples Example 1 Question: Did the attribute of plant changed because of the action getting something from something? Answer: Response: Yes, the attribute of plant got watered from no to yes after the action getting something from something. Your output: 1 yes 25 what status of fork changed while the person do the first action did before Example 2 Question: he/she put something to something? Answer: Response: fork was in drawer before the person put fork to sink. Your output: 0 cleanliness Example 3 Question: What is the person doing before he/she close something? Answer: Response: The person is putting tomato in the fridge. Your output: 1 Put tomato to fridge Example 4 Question: What is the first action the person did in the video? Answer: Response: The person pulled out chair. Your output: 0 Work on sofa Example 5 Question: How did the person changed the spatial relationships of meat? Answer: Response: The person moved meat from the fridge to the counter. Your output: 1 Put meat to table Example 6 Question: what status of fridge changed while the person do the first action did after he/she point to something? Answer: Response: The fridge was closed before the person point to something, and after that the fridge changed to open. Your output: 1 openess Example 7 Question: which object changed its status when the person do the last action in the video? Answer: Response: spoon Your output: 0 fork Example 8 Question: What is the action that just happened? Answer: Response: The person puts the can on the table. Your output: 0 Place can in the tray Please place the fruits in the bowl then place the kitchen supplies put blue packet in the bowl 7. put yellow packet in the bowl. put white packet in the bowl 2. put blue packet in the bowl 5. put white packet in the put blue packet Whats the last 20 steps: Example 9 Question: current goal is: into the holder. bowl 3. put yellow packet in the bowl 4. in the bowl 6. immediate next step? Answer: Response: put brush in the holder Your output: 0 Put duster in the black stand 1. Your Turn: Question: {question} Answer: Response: {prediction} {answer} 26 Your output: Prompt for GPT-4o (OpenEQA-hm3d & OpenEQA-scannet). You are an AI assistant tasked with evaluating whether response matches the correct answer to given question, considering both the primary answer and any extra correct answers. ##Evaluation Rules (1) Output 1 if the response matches the answer or any of the extra answers exactly or with synonymous/equivalent wording. - Synonyms, paraphrases, or different surface forms of the same meaning count as matches. - Minor wording differences (e.g., Wood panel vs. (2) Output 0 if the response is incorrect, contradictory, or refers to different entity, object, or attribute than the answer and all extra answers. - If the answer and response describe different objects, actions, or states, mark as 0. - If the response introduces additional details that change the meaning of the answer, mark as 0. Wood) count as matches. put meat and tomato on the table vs. response: Output 1 if the response conveys essentially the same meaning as the If the response overlaps but misses or alters essential details (e.g., ##Special Cases - Similar meaning: answer and does not omit or add critical information (e.g., answer: fan). - Partial matches: answer: 0. - Granularity differences: (e.g., answer: woman, response: Jessica), output 1. - Yes/No questions: mismatch outputs 0, regardless of explanation. - Ambiguity: If the response cannot be reasonably interpreted as equivalent to the answer, output 0. Only output 1 if the polarity matches (yes <-> yes, no <-> no). If the response is more specific but still semantically equivalent put meat on the table), output Any ceiling fan, response: ##Examples: Example 1: Question: Is it overcast? Answer: Extra Answers: Response: yes Your output: no [\"doesnt look like it\", \"no\", \"its sunny\"] Example 2: Question: Who is standing at the table? woman Answer: Extra Answers: Response: Jessica Your output: 1 [\"a woman\", \"a lady\", \"woman\"] Example 3: Question: Are there drapes to the right of the bed? Answer: Extra Answers: Response: yes Your output: 1 yes [\"yes, there are drapes\", \"yeah\", \"the drapes are to the right of the king bed\"] Example 4: Question: What material is the ceiling in the living room? Answer: Extra Answers: Response: wood Wood panel null 27 Your output: Example 5: Question: What is in between the two picture frames on the blue wall in the living room? The TV Answer: Extra Answers: Response: air conditioner Your output: 0 null Example 6: Question: Is the house doorway open or closed? Answer: Extra Answers: null Response: The house doorway is open. Your output: 1 Open Example 7: Question: Is my backyard safe to let me dog out in? Answer: Extra Answers: Response: yes Your output: 1 Yes, its fenced. null Example 8: Question: What is hanging from the ceiling in the bedroom? Answer: Extra Answers: Response: fan Your output: 1 ceiling fan null In the bedroom by the door Example 9: Question: Where is the full body mirror? Answer: Extra Answers: the bedroom right next to the door\"] Response: The full body mirror is in the bedroom. Your output: [\"next to the bedroom door\", \"just inside the bedroom\", \"in the bedroom\", \"in Example 10: Question: What is leaning in the corner by the coat rack? Answer: Extra Answers: Response: chair Your output: 0 An umbrella null Your Turn: Question: {question} Answer: Extra Answers: Response: {prediction} Your output: {answer} {extra_answers} AgiBot-ER. We curate 97, 120, 381 test samples for three sub-tasks, i.e., Task Status Verification, Action Affordance, and Next Action Prediction, respectively. We report the average accuracy of the three tasks. 28 Task Status Verification <image><image><image><image><image><image> {question} Your answer can only be \"yes\" or \"no\" Action Affordance <image><image><image> {question} Your answer can only be \"yes\" or \"no\". Next Task Prediction <image><image><image><image><image><image> {question} {options} Answer with the options letter from the given choices directly. MME. We report the accuracy+ metric for MME [18]. <image> {question} Please answer yes or no. MMBench. We demonstrate the official metric provide by MMBench [40]. <image> {question} {options} Answer with the options letter from the given choices directly. RealWorldQA. We use the official metric of RealWorldQA [71]. <image> {question} {options} Please answer directly with only the letter of the correct option and nothing else. SimpleVQA. We compute accuracy by leveraging GPT-4o to compare responses against the ground-truth labels. <image> {question} EgoSchema & VideoMME & NextQA. We set the number of video frames to 128 and evaluate accuracy on the three video understanding benchmarks. <video> {question} {options} Answer with the options letter from the given choices directly. MathVista. We use the official metric of MathVista [42]. 29 <image> {question} MathVision. We use the official metric of MathVision [69]. <image> {question} {options} Please only output your final answer or single letter (if it is multiple choice question). MMMU. We use the official metric of MMMU [82]. <image> {question} {options} Answer with the options letter from the given choices directly. A.2 The AGIBot offline evaluation set We select 16 high-frequency daily tasks from the AGIBot dataset for offline evaluation: Heat food in the microwave oven, Make sandwich, Wash dishes with dishwasher, Wash the dishes and silverware, Discard the trash on the coffee table, Pick up the item to wipe away the stain, Place items from the meeting room table into the storage box, Arrange sofa, Wash clothes in the washing machine, Remove clothes from the washing machine, Arrange flowers, Boil water in the kettle, Packing in the supermarket, Scan and package the goods, Restock the hanging basket area, and Restock supermarket snacks. To ensure fairness, the data for these tasks are excluded from both the pretraining and fine-tuning datasets. A.3 Offline evaluation examples We present snapshot of our offline evaluation sets. [ ... { \"role\": \"content\": \"<system_info_start>Your current views are<image><image><image><system_- \"user\", info_end><human_start>I want some fruit, put all fruits into the basket.<human_end>\" }, { \"role\": \"content\": \"<think_start>...<think_end><plan_start>put the banana into the \"assistant\", plastic box<plan_end><response_start>...<response_end>\", \"other_actions\": [ \"put the grapes into the plastic box\" ] }, { \"role\": \"content\": \"<system_info_start>Your current views are<image><image><image><system_- \"user\", info_end><human_start>Wait, dont want banana.<human_end>\" }, { \"role\": \"content\": \"<think_start>...<think_end><plan_start>put the banana on the \"assistant\", table<plan_end><response_start>...<response_end>\", \"other_actions\": [ ] 30 (a) (b) (c) (d) (e) Figure 7 Example initial states of the five tasks used in our online evaluation. (a) Table bussing: removing used dishes, utensils, and food items; (b) Checkout packing: organizing and placing purchased items into bags or boxes at checkout; (c) Dietary filtering: selecting or excluding food and drink items based on dietary constraints; (d) Grocery shopping: retrieving and collecting grocery items according to user instructions; (e) Tableware organization & shipment: sorting, packing, and transporting tableware to designated locations. }, ... ] The above example illustrates two consecutive rounds within trajectory. We query VLMs in manner analogous to multi-turn dialogue interactions. For the first round, planning is considered correct if the VLM output semantically matches either: <plan_start>put the banana into the plastic box<plan_end> or <plan_start>put the grapes into the plastic box<plan_end>. For the second round, we evaluate models under teacher-forcing setting, incorporating the ground-truth from the first round into the dialogue history. Since the other_actions list is empty for this step, planning is deemed correct only if the VLM output semantically matches: <plan_start>put the banana on the table<plan_end>. A.4 Online evaluation tasks Example initial states of the five tasks used in our online evaluation are shown in Figure 7. Below, we provide an overview of each task. Table bussing: removing used dishes, utensils, and food items. In our experiment, the user requests the robot to clear the table, but interrupts with the command still need it while the robot is placing glass into the plastic box. We also introduce failure event when the robot attempts to place spoon into the box. Checkout packing: organizing and placing purchased items into bags or boxes at checkout. In our experiment, the user asks the robot to move all items on the checkout counter into the shopping basket. An interruption occurs with the command am allergic to caffeine while the robot is placing coffee into the basket. Dietary filtering: selecting or excluding food and drink items based on dietary constraints (e.g., no caffeine, vegetarian). In our experiment, the user issues series of instructions: (1) Put the food with the highest energy into the plastic box, then add drink. Note that am allergic to caffeine.; (2) Discard all drinks containing caffeine into the rubbish bin; (3) Place all fruits and vegetables into the plastic box ; and (4) Clean up all remaining items on the table. Grocery shopping: retrieving and collecting grocery items according to user instructions. In our experiment, the user first asks the robot to recommend snack suitable for road trip. Following the recommendation, the user requests less sweet snack for the cart, then asks for non-alcoholic drink, and finally makes conditional request: Are there any nuts on the shelf? If so, put some into the shopping cart. Tableware organization & shipment : sorting, packing, and transporting tableware to designated locations. In our experiment, the user instructs the robot to discard all trash and transport the tableware to the sink. Specifically, the robot must first dispose of garbage in the rubbish bin, then place all tableware into plastic box, and finally carry the box to the sink. During this process, the user interrupts with the command Keep it on the table while the robot is placing stainless steel cup into the box. 31 A.5 Offline & Online Evaluation Prompts We present the prompts used in our experiments, including the judge prompt for offline evaluation, the baseline model prompts, and the reward model prompt for reinforcement learning. Judge Prompt for Offline Evaluation # Role You are judge to decide the similarity of two sentences. # Inputs You will be provided with two sentences, each sentence represent an action from robot. # Output Format Your output should be single number that is either 0 or 1, where 0 means the two sentences are semantically different, and 1 means the two sentences are semantically the same. Output in the following format: \"[SOME ANALYSIS]. The final answer is: [NUMBER]\" # Examples User: 1. 2. Assistant: Two sentences are exactly the same. The final answer is: put the fork into the basket put the fork into the basket 1.0 put the iron fork into the woven basket put the fork into the basket User: 1. 2. Assistant: Two sentences are the same semantically. The final answer is: 1. Grab the bottled french fries on the table with the right arm. Grab the bottled french fries. User: 1. 2. Assistant: Two sentences are the same semantically. The final answer is: 1.0 Move forward slightly Move forward User: 1. 2. Assistant: Two sentences are the same semantically. The final answer is: 1.0 put the coffee into the plastic box put the bottle of coffee into the plastic box User: 1. 2. Assistant: Two sentences are the same semantically. The final answer is: 1. put the glass into the plastic box put the glass cup into the plastic box User: 1. 2. Assistant: Two sentences are the same semantically. User: 1. 2. Assistant: put the fork into the basket put the knife into the basket The final answer is: 1. 32 The objects (fork v.s. knife) are different semantically. The final answer is: 0.0 put the fork into the basket pick up the fork User: 1. 2. Assistant: The actions are different semantically. The final answer is: 0.0 put the fork into the basket navigate to the table User: 1. 2. Assistant: Neither actions nor objects matches. The final answer is: put the fork into the basket pick up the stainless steel cup User: 1. 2. Assistant: Neither actions nor objects matches. The final answer is: 0. 0.0 English Prompt for Baseline Models. We use the following prompt to configure VLMs as the interaction and planning modules of the robot system in both offline and online experiments. # Role You are robot with two grippers developed by Bytedance, and your name is Roobio. You are deployed in home environment, and your job is to have natural interactions with users and complete some tasks required by the user. You have basic movement, perception, and manipulation capabilities, and can navigate to the following areas: shoe cabinet, sofa, and washing machine. dining table, sink, refrigerator, dishwasher, microwave, bread maker, cupboard, Three images: User instructions (optional): user may give you some instructions, user instructions are in captured by the cameras from your head/left gripper/right gripper # Input In each round, you will be provided with the following information. 1. respectively. These images reflect your current visual perception at the current moment. 2. chinese. # Output To distinguish reasoning, planning, response and query. fields. 1. include this field is for your reasoning process, it can You must output with the following <think_start> ... <think_end>: - The key items in the current scene. - Whether the previous action is complete. - Reasoning about your next action. <plan_start> ... <plan_end> (optional): 2. plan your next action based on your reasoning process. The action should be helpful to complete users task. 3. you response must be in *chinese*. You should respond to the user in the following situations. <response_end> (optional): this field is your response to user, this field is for task planning, You should <response_start> ... - When the user is chatting with you; - When the user gives new instruction on completing task, you should always respond to the user whenever you receive new instruction. relevant to the task. The response should be concise, polite, and - When you need to ask the user for more information. For example, if the user ask you to give him/her drink, and you observe both orange juice and apple juice, then you should ask 33 the user which one he/she prefers. You should avoid asking the user for more information too frequently. The text inside <think_start> ... # Guidelines 1. should be in english while the text in <response_start> ... chinese. 2. 3. format: When the user interrupt your action, you need to stop the current action and re-plan. Your task planning inside <plan_start> ... <plan_end> should be in the following <think_end> and <plan_start> ... <plan_end> <response_end> must be in into the ... - put the ... - pick up the ... - navigate to the ... - open the ... - close the ... Only include one action at time, planning such as <plan_start> put the into the B, 4. put the into the B<plan_end> is invalid. # Examples User: <system_info_start>Your current views are...<system_info_end><human_start>Robot, clean up the table for me.<human_end> Assistant: <think_start>The user asks me to clean up the table, can observe.... start>put the ... User: <system_info_start>The views you see after finishing previous action are...<system_info_end> Assistant: <think_start>I should continue cleaning up the table, there are ... ...<think_end><plan_start>put the ... into the ...<plan_end> ... into the ...<plan_end><response_start>...<response_end> <think_end><plan_left on the table, Chinese Prompt for Baseline Models We also developed corresponding Chinese prompt for models that perform better with Chinese. # 角色 你是字节跳动研发的双臂机器人你被部署在一个开放式厨房环境中负责与人类用户进行自然互动并完成日常操 作任务你具备基础的移动感知与操控能力可前往以下区域执行操作餐桌旁水池旁冰箱旁洗碗机旁 微波炉旁烤面包机旁水池旁的台面鞋柜旁沙发旁和洗衣机旁 # 输入信息 在每轮交互中你将接收到以下信息 1三张图像分别来自你头部摄像头左夹爪摄像头右夹爪摄像头反映你在当前时刻的视觉感知 2用户指令可选人类用户可能通过语音或文本向你发出任务指令 # 输出格式 你必须输出以下结构化内容用于指导任务执行与用户交互输出字段包括 1<think_start> ... <think_end>你的内部思考与任务状态判断包括 - 当前场景中包含的关键物品 - 是否完成上一步操作 - 当前场景变化的观察 - 下一步应执行的关键动作与理由 - 如遇异常如抓取失败应在此说明并调整策略 2<plan_start> ... 务操作步骤应使用英文简洁描述 3<response_start> ... <response_end>可选你对用户的语音或文本形式的自然语言回复内容 需简洁礼貌并与任务状态相关当当前信息不足以明确执行用户任务时你可以主动向用户提问以获取更多信 <plan_end>可选你对下一步具体动作的操作规划目标是完成当前用户任 34 息但请遵循非必要不提问的原则避免频繁打断用户 # 任务规则 在于用户的交互与执行任务的过程中你需要遵循以下规则 1<think_start> ... 文输出<plan_start> ... 2用户可能会打断你的操作你需要基于当前画面和用户给你的最新指令决定下一步的操作 3<plan_start>...<plan_end>中的操作推荐遵循以下格式 <think_end>和<response_start> ... <plan_end>中的任务规划以英文输出 <response_end>中的文本以中 into the ... - put the ... - pick up the ... - navigate to the ... - open the ... - close the ... 4<plan_start>...<plan_end>中的任务规划一次只能规划下一步的内容不能规划之后多步的内容例 如put the to the B, put the to the D...这样多个操作不能放到一次规划中但是它可以出现在你的思 考过程中 # 例子 User: <system_info_start>你当前看到的画面为...<system_info_end><human_start>帮我清理桌面<human_end> Assistant: <think_start>用 户 让 我 清 理 桌 面 当 前 画 面 中 我 能 看 到.... into the ...<plan_end><response_start>收到<response_end> User: <system_info_start>你完成操作后所看到的场景为...<system_info_end> Assistant: <think_start>我应该继续收拾桌面当前我能看到桌面上还有...<think_end><plan_start>put the ... into the ...<plan_end> ... <think_end><plan_start>put the ... Reward Model Prompt for RL # Role You are consistency checker for robots planning. Your task is to evaluate the robots thinking process and the resulting final action. # Input 1. 2. The robots thinking process, enclosed by <think_start> and <think_end>. The robots final action, enclosed by <plan_start> and <plan_end>. # Task You need to determine: 1. 2. decision derived from the thinking process in <think_start> and <think_end>. Whether the thinking process within <think_start> and <think_end> is reasonable. Whether the final decision within <plan_start> and <plan_end> is consistent with the # Output Your output must follow this format: [Your reasoning process] + \"The final answer is: [NUMBER].\" Here, [NUMBER] is an integer: 1 indicates consistency; -1 indicates inconsistency; 0 indicates that it is impossible to judge. # Notes 1. consistent. 2. Ignore ambiguity in names. Items that could potentially be consistent should be considered Assign -1 only if the thinking process is completely unreasonable. 35 Assign -1 only if its completely impossible for the thinking process and the plan to be 3. consistent. 4. considered consistent (i.e., the plan does not need to reflect all thoughts). If the plan is only part of the strategy outlined in the thinking process, it is still Embodied Task-centric Reasoning B.1 Data Synthesis Publicly available robot datasets often contain long video demonstrations with temporally annotated clips and corresponding individual actions, or they can be segmented automatically into clips using VLMs. To equip Robix with reasoning and planning capabilities in embodied scenarios, we design three dedicated synthesis pipelines, each targeting distinct sub-task: task status verification, action affordance, and next action prediction. The details of these pipelines are provided below. Task Status Verification. The status of task is categorized as either complete or incomplete. To construct task verification data labeled as complete, we use the entire video clip as the visual input. Reasoning traces are obtained by prompting strong VLM (Seed-1.5-VL in thinking mode) with the question Is the {action} complete?. We then extract the reasoning content from the response and retain only those instances where the predicted status is complete. Conversely, to represent incomplete task status, we randomly truncate each video clip to between one-half and one-third of its original duration, producing partial visual inputs. As before, we retain only those cases where the predicted status is identified as incomplete. Action Affordance. To obtain high-quality labels for action affordance, we distill both labels and reasoning traces from powerful VLMs. However, we find that relying on single VLM is insufficient for generating accurate labels reliably. Therefore, we employ various VLMs to produce candidate labels and take the intersection of their outputs as the final annotation. The corresponding reasoning traces are extracted from the Seed-1.5-VL-Think model. Next Action Prediction. The robot datasets are typically annotated with action sequences. Thus, the ground-truth next action can be directly determined once the previous action is specified. To synthesize the corresponding reasoning trace, we leverage Seed-1.5-VL by prompting it with both the video and the following textual input: You are an intelligent robot executing {the task}. To complete this task, you are performing the action as shown in the video. Your next action is {next action}. Please reflect on the rationale behind your next action and output your response from the perspective of robotic agent. B.2 Agibot-ER Benchmark We construct an embodied task-centric reasoning benchmark based on the Agibot dataset [6], which provides manual annotations of tasks, atomic action sequences, and the corresponding video clips for each action. Specifically: Task Status Verification: We randomly select 100 samples from the synthesized dataset and further verify the label accuracy, resulting in 97 test samples. Action Affordance: We select 120 video clips and corresponding individual actions from 50 different tasks. To ensure accuracy, we manually annotate these samples rather than relying on VLM-based annotation. Next Action Prediction. The ground-truth next action is obtained directly from the robot datasets. We randomly sample 400 instances and construct distractor options by leveraging powerful LLM to generate incorrect candidates that differ subtly from the correct onesuch as variations in action type, target object, or destination. To ensure quality, we manually verify all synthesized options, resulting in 381 validated test samples. All test samples are eliminated from the training dataset."
        },
        {
            "title": "C Case studies of RL",
            "content": "Our reinforcement learning stage yields several improvements over the SFT model, including: (1) reduced irrational reasoning, (2) improved thought-action consistency, and (3) fewer formatting errors. In the following, we present comparative case studies between Robix-32B-RL and Robix-32B-SFT to illustrate the effectiveness of reinforcement learning. Reduced irrational reasoning # Current task Put all the drinks on the table into the carton. # Robix-32B-RL output <think_start>Based on the current scene, see that there is already bottle of soda inside the carton, and the remaining items on the table are milk, Oreos, and gum. Recalling my last action, have already put the coke into the carton. Now, the only beverage left on the table is milk. To complete the task of putting all beverages into the carton, the next step should be to put the milk into the carton. <plan_start>put the milk into the carton<plan_end> <think_end> # Robix-32B-SFT output <think_start>Based on the current scene, see that there is already bottle of soda inside the carton, and the remaining items on the table are milk, Oreos, and gum. Recalling my last action, have already put the coke into the carton. Now, all the beverages on the table have been put into the carton, and the task goal has been achieved. Next, should proactively ask the user if there are any other tasks that need assistance. <response_start>The task has been processed. <response_end> Is there anything else can help you with? <think_end> Improved thought-action consistency # Current Task Put the dishes in the sink # Robix-32B-RL output <think_start>I have grasped the storage basket with my dual grippers. bowls, forks, and spoons. dishes to the sink. the sink area and deliver the dishes to the designated location.<think_end> <plan_start>navigate to the sink<plan_end> have now finished collecting the tableware. The basket contains Next, should navigate to According to the users instructions, the next step is to take these According to the users instructions, the dishes need to be taken to the sink. # Robix-32B-SFT output <think_start>I have grasped the storage basket with my dual grippers, ready to transport the dishes. the dishes that need to be washed have been collected in the storage basket. is to transport the storage basket to the sink area, completing the task of collecting and transporting the dishes.<think_end> <plan_start>navigate to the cupboard<plan_end> The next step All Fewer formatting errors # Current Task Organize the table # Robix-32B-RL output <think_start>From the current scene, the paper cup is still on the table, near the rubbish bin, and is grasped by my gripper, which indicates that the previous attempt to put the paper cup into the rubbish bin was not successful. Besides the paper cup, there is also plate with some leftover food residue on the table. need to re-attempt putting the paper cup into the rubbish bin to ensure the trash is handled correctly.<think_end> <plan_start>put the paper cup into the rubbish bin<plan_end> To continue organizing the table, first # Robix-32B-SFT output <think_start>From the current scene, the paper cup is still on the table, near the rubbish bin, and is grasped by my gripper, which indicates that the previous attempt to put the paper cup into the rubbish bin was not successful. with some leftover food residue on the table. need to re-attempt putting the paper cup into the rubbish bin to ensure the trash is handled correctly.<think_end> { Besides the paper cup, there is also plate To continue organizing the table, first \"action\": \"put\", \"object\": \"paper cup\", \"target\": \"rubbish bin\" }"
        }
    ],
    "affiliations": [
        "bytedance.com"
    ]
}