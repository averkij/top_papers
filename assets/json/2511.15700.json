{
    "paper_title": "First Frame Is the Place to Go for Video Content Customization",
    "authors": [
        "Jingxi Chen",
        "Zongxia Li",
        "Zhichao Liu",
        "Guangyao Shi",
        "Xiyang Wu",
        "Fuxiao Liu",
        "Cornelia Fermuller",
        "Brandon Y. Feng",
        "Yiannis Aloimonos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 0 7 5 1 . 1 1 5 2 : r a"
        },
        {
            "title": "First Frame Is the Place to Go for Video Content Customization",
            "content": "Jingxi Chen1, Zongxia Li1, Zhichao Liu1, Guangyao Shi2, Xiyang Wu1, Fuxiao Liu1, Cornelia Fermüller1, Brandon Y. Feng3, Yiannis Aloimonos1 1University of Maryland 2University of Southern California 3Massachusetts Institute of Technology http://firstframego.github.io Figure 1. FFGo is lightweight add-on that invokes the innate capabilities of pre-trained video generation models, such as Wan2.2 [30], to treat the first frame as compositional blueprint, enabling natural subject mixing and interaction throughout the video. Given single input image with multiple elements and guiding text prompt, FFGo generates coherent, customized videos across diverse applications including robotic manipulation, driving/aerial/underwater simulation, multi-product demonstration, and filmmaking. It requires no architectural modifications, and achieves strong subject-mixing performance with just 2050 LoRA fine-tuning examples."
        },
        {
            "title": "Abstract",
            "content": "What role does the first frame play in video generation models? Traditionally, its viewed as the spatial-temporal starting point of video, merely seed for subsequent animation. In this work, we reveal fundamentally different perspective: video models implicitly treat the first frame as conceptual memory buffer that stores visual entities for *Equal contributions Corresponding author. later reuse during generation. Leveraging this insight, we show that its possible to achieve robust and generalized video content customization in diverse scenarios, using only 2050 training examples without architectural changes or large-scale finetuning. This unveils powerful, overlooked capability of video generation models for reference-based video customization. 1 1. Introduction Recent advances in video generation models [1, 2, 4, 21, 26, 30, 34, 3941] have made them powerful tools for content creation, filmmaking, simulation, and other applications involving creative visual experiences. key application of these models is reference-based video generation, where one or more reference inputs are used to compose and synthesize visually consistent videos. This capability is essential for real-world scenarios such as film production and simulation, where customization and controllability are critical. Unlike standard Text-to-Video (T2V) models that rely solely on textual prompts, reference-based generation allows users to guide video synthesis through visual references, enabling finer control over generated video contents to be customized for the specific user guidance. The simplest case is using single reference image, known as the Image-to-Video (I2V) paradigm. I2V models animate given image to generate videos that maintain visual consistency with the reference content. While effective as basic form of video customization, this single-image setup constrains both the spatial diversity and content composition of the generated video. To overcome these limitations, recent research has focused on developing multi-reference video generation models [5, 7, 13, 17, 31], which can incorporate multiple reference images to achieve richer and more flexible video content customization. Existing multi-reference video generation models generally follow two strategies: 1) Architectural modification of pre-trained video generation models to accommodate additional reference inputs. 2) Fine-tuning on large-scale, task-specific video customization datasets, such as inserting humans into animations or product demonstration videos. However, fine-tuning often leads to performance degradation and loss of generalization in the adapted models. Since video customization typically occurs post-training, the diversity and quality of the adaptation datasets are far lower than those used during pre-training. Consequently, fine-tuned models tend to overfit to specific customization scenarios, forgetting the broad generative priors learned during pre-training, effectively transforming once generalist video models into narrow, task-specialized systems. Is it possible to incorporate content from multiple reference images into pre-trained video generation models without modifying their architecture or relying on large-scale video training datasets? To address this question, we investigate existing video generation models and uncover previously overlooked yet fundamental capability: their innate ability to incorporate multiple reference concepts into the generation process without any architectural modifications or large-scale fine-tuning. As in Figure 2, standard video generation models have potentials to naturally embed visual concepts from multiple references into the first frame, functioning as memory buffer, and then fuse them consistently through scene transitions during generation. However, this ability is difficult to invoke directly: prompt engineering alone often leads to unstable outcomes and struggles to precisely preserve object identities or control visual composition. To transform this observation into practical capability for multi-referencebased video customization, we propose simple yet effective approach that reliably activates this innate ability. Using only 20-50 training video examples for lightweight training, our method enables the model to select visual concepts in the first frame and transition scenes coherently, achieving generalized multi-reference video customization. Importantly, this is done without modifying the model architecture or compromising the rich generative priors of the pre-trained video generation model. To summarize our contributions in this work: We investigate the innate and general ability of video generation models, showing that the first frame not only serves as the spatiotemporal start of generation but also acts as conceptual buffer, which enables multi-reference-based video content customization. We develop simple yet effective pipeline that leverages Vision-Language Models (VLMs) for high-quality training data curation and achieves invocation through just 2050 examples via few-shot LoRA adaptation. We evaluate and compare our proposed invocation addon, FFGo, with state-of-the-art video generation models across diverse applications, including filmmaking, generalized multi-object interaction, driving/aerial/underwater simulation, and robotic manipulation, etc. 2. Related Work Video Generation and Video Content Customization. [1, 21, 26, 30, 34, 39] are Video generation models powerful class of generative models that synthesize videos conditioned on user-provided text prompts, primarily for creative content generation. Most state-of-the-art models are based on diffusion frameworks [25, 28] with U-Netbased denoising backbones, and more recently, Diffusion Transformers (DiT). key limitation of these pre-trained models is their reliance solely on text prompts, which restricts controllability, particularly in real-world applications such as product demonstrations or simulations, where visual references are often required for precise customization. To address this, recent works in video content customization [5, 7, 10, 12, 13, 17, 33] explore extending video generation models to accept additional visual inputs. These approaches typically require: 1) architectural modifications, and 2) large-scale training on specialized datasets of customized videos. However, both requirements come with significant drawbacks. Architectural changes often compromise model efficiency and compatibility, while task-specific fine-tuning on limited domains may degrade the general First Frame First Frame Veo 3 Sora 2 First Frame Wan 2. Text Prompt <transition> + Text Prompt Figure 2. In this figure, we illustrate general yet under-explored observation: video generation models possess an innate ability to perform subject mixing via scene transitions from mixed-subject first frame. As shown, the red-boxed results (without the transition phrase: <transition>) contrast with the blue-boxed results (with carefully chosen <transition> e.g., The camera view suddenly zoom in to show\") revealing significant differences in composition. However, this phenomenon faces three key limitations that hinder practical use: 1) The prompt engineering process for <transition> is highly manual, time-consuming, and model/video-dependent. 2) Scene transitions are often unstable. 3) Object identity is often lost, resulting in changes in appearance or the disappearance of reference objects. knowledge acquired during large-scale pretraining, where data is more diverse, higher in quality, and broader in scope. The Innate Abilities of Pre-trained Generative Models Pre-trained generative models are typically trained on massive and diverse datasets, which endows them with generalpurpose capabilities that often extend beyond their original design goals. These emergent properties, what we refer to as innate abilities, remain under-explored in current research, despite growing evidence of their utility in real-world applications. For example, recent work [11] demonstrates that with just few fine-tuning samples and using Low-Rank Adaptation (LoRA) [9] based fine-tuning, pre-trained image generation model can be prompted to generate gridaligned images with coherent content. In the video domain, study [34] havs shown that pre-trained image-to-video (I2V) models can perform various frame-level perception tasks such as edge detection, segmentation, and super-resolution, despite not being explicitly trained for them. Motivated by these findings, we explore whether similar innate abilities exist in pre-trained video generation models related to video content customization, and how they might be invoked through minimal adaptation, without architectural changes or large-scale retraining. Vision-language models (VLMs) for multimodal data curation. Vision-language models (VLMs) have become essential across wide range of multi-modal applications, demonstrating strong capabilities in video understanding [8, 15, 20], video captioning [14, 37, 42], and visual recognition tasks such as object detection and 3D scene understanding [3, 16, 23, 24, 29]. Recent progress in unified VLMs, e.g., Gemini-2.5-Pro [6], Qwen2.5-Omni [36], GPT4o [24] have further extended these capabilities by enabling seamless processing and generation across images, videos, and text within shared representation space. These models operate over unified multimodal input space, capable of processing images, text instructions, and videos, and generating outputs across the same modalities, including images, textual responses, and videos [18, 32, 38, 43]. In particular, unified VLMs demonstrate strong instruction-following capabilities for image editing tasks [35] and video understanding tasks [8], enabling us to leverage them for curating high-quality training data. 3. Proposed Approach 3.1. Pipeline Overview Overview of our pipeline is shown in Figure 3. It consists of three main components: 1) Dataset Curation, which utilizes VLMs to generate high-quality paired training data inputs for corresponding videos. 2) Few-shot LoRA Adaptation, which invokes the models innate fusion and transition abilities, and 3) Clean Customized Video Inference, which enables generalized multi-reference video generation. 3 Figure 3. The overview of our proposed pipeline FFGo, consists of 1) Dataset Curation for getting the high quality finetuning data from existing videos, 2) Few-shot LoRA Adaptation for training/inference to invoke the I2V models innate ability in fusing the subjects in the first frame and perform scene trasition to generate video Vmix following subjects in the first mixing frame Imix and the text prompt. 3.2. Dataset Curation Previous subject mixing models such as SkyReels-A2 [7], VACE [13] modified model architectures to take in multiple reference images and uses millions of training samples to train models to mix subjects, where the subjects are mainly with humans and does not generalize to other applications such as autonomous driving, robot manipulation. Instead of modifying the models architecture and using millions of training data to learn subject mixing, we leverage the models pre-trained abilities to learn subject mixing without modifying its architecture. To do so, we need to prepare dataset where the input aligns with the base model (Wan2.2 [30])s input, which is an image and text prompt. Training Data Selection. We gather two thousand videos from varies sources include Veo 3 showcases [34], HOIGen1M [19] and licensed short videos. Each of the videos is ranging from six to twenty seconds long, we crop all the videos to keep the first 81 frames. Next, we carefully go over subset of the videos, and pick the high quality ones that have clear interaction or combinations of different elements, with clear and complete boundaries that can be easily segmented. Then for each video we think it is high quality, we write down the elements we want to segment out from the video in text form. For example, in video where man wearing Chrismas hat is making cake, we write down the element names the man, cake, Chrismas hat, which will be used to segment out later. We use the first frame of the selected videos to extract elements and process to be the input for the training data. We sample total of 50 carefully selected videos with human annotations of objects to be segmented out. Element Extraction using unified VLMs. Given an image I, and set of elements in textual representation O, our goal is to use unified VLM to (1) recognize and extract and generate each of the individual from I, (2) remove all from and generate complete background of I. Specifically, we prompt Gemini-2.5-Pro [6] to do this process. At them end, we use SAM 2 [27] to remove the white backgrounds of the individually extracted or generated elements and keep an RGBA layer for each O, where only the element has RGB layer. Given the set of elements in RGBA and background, we combine them into single image, with the background always on the right and the elements always on the left with automatic resize to fit the elements. Prompt generation using VLMs. Given set of RGBA elements, and ground truth video, we prompt Gemini-2.5Pro to generate prompt that not only describe the video, but also focuses on the appearance and interaction of these elements. Thus, we get final training dataset with combined images with their respective background and elements and 4 caption that describes how these elements fit into the video and background. 3.3. Few-shot LoRA Adaptation As shown in Figure 3, with only 2050 training examples, we apply simple LoRA-based few-shot adaptation to pre-trained model, enabling it to learn subject mixing by fusing subjects in the first frame and performing scene transition. Given pre-trained I2V model gθ, input image I, and text prompt C, the standard generation process is: = gθ(I, C), where is video of frames. In our adapted pipeline: The input image is replaced by subject-mixed image Imix. The prompt is modified by appending special identifier to indicate transition Ctrans =< transition > +C. LoRA-based weight update θ is applied to the model. LoRA learns an updated weight matrix θ defined as θ = θ + θ, where θ represents the original pre-trained weights and θ is learnable low-rank update. Instead of learning full-rank matrix of size k, LoRA factorizes the update as: θ = θ + αAB, where α is scaling factor, Rdr , Rrk, and is low-rank dimension much smaller than both and k. The target output video Vmix exhibits both subject mixing and temporal scene transition. Specifically, the video has frame length = {Fc, Fg} , where: Fc represents the temporal compression frames, namely the temporal compression ratio (e.g., Fc = 4 in the Wan2.2), Fg contains the generated subject-mixed content, following sudden transition after frames Fc. Thus, the adapted video generation process becomes: Vmix = gθ+θ(Imix, Ctrans). 3.4. Clean Customized Video Inference During the inference, to generate content customized video, the processing is very simple, since the model will generate the Vmix has frame length = {Fc, Fg} with both subject mixing and temporal scene transition. The users can easily cut off the first Fc frames to get the clean subject mixing videos. As an example, in Wan2.2 as the base model, to generate = 81 frame video, the first Fc = 4 frames will be discarded and the rest Fg = 77 frames will be the clean customized content videos. 4. Experimental Results The results and comparisons are best viewed through video examples. We include video results in our project webpage. 4.1. Datasets and Implementation Details Few-shot Training Data. We carefully selected 50 videos featuring object-object, human-human, or human-object interactions with clearly defined boundaries suitable for segmentation. These were curated from pool of 2,000 videos sampled from HOIGen-1M [19], licensed video clips, and Veo 3 samples [34]. Each curated video contains 81 frames. The dataset spans four main categories: human-object interaction (60%), human-human interaction (14%), element insertion (20%), and robot manipulation (6%). Details of the curated training set are provided in the Supplementary Materials. LoRA Adaptation Training Details We use Wan2.2-I2VA14B [30] as our base model. For LoRA adaptation, we train with LoRA rank of 128 and introduce unique transition phrase, <transition>: ad23r2 the camera view suddenly changes. This phrase serves as prompt to trigger the models innate ability for subject selection and scene transition from the first frame. Since Wan2.2-I2V-A14B employs two separate denoising transformers for lowand high-noise regimes, we train each independently for 5 hours using 2 NVIDIA H200 GPUs, with batch size of 4. 4.2. Evaluation Strategy In this section, we outline our evaluation strategy to demonstrate the effectiveness of our proposed model and its performance relative to baseline methods. Test set spanning diverse application scenarios. To rigorously evaluate the effectiveness and generalization of our approach in video content customization, we curate test set of 50 video materials spanning diverse applications such as robot manipulation, filmmaking, aerial/driving/underwater simulation, and product demonstrations. Compared to prior works [7, 13], our test set offers two key advantages: (1) it covers broader range of real-world customization scenarios beyond the typical human-human or human-object interactions; and (2) it supports up to 5 reference subjects, exceeding the 3-subject limit (e.g., object1, object2, scene) seen in previous works. Baseline Models We compare our method against three strong baselines built on the Wan architecture with 14 billion parameters: Wan2.2-14B-I2V [30], VACE [13], and SkyReels-A2 [7]. Wan2.2-14B-I2V is our base I2V model, to which we apply our lightweight adaptation for invoking its innate subject mixing and scene transition capabilities. As shown in Figures 5, 6 and 7, our adaptation significantly enhances its performance in video content customization while preserving the original generation quality (Figure 4). VACE and SkyReels-A2 represent state-of-the-art video customization models trained on millions of high-quality examples. Despite their scale, we demonstrate that our method, leveraging the overlooked role of the first frame, can outperform both using only 50 training videos across wide range of scenarios. 4.3. Qualitative Comparison In this section, we qualitatively compare our proposed method with baseline models across diverse scenarios. 5 Model Wan2.2-I2V-A14B SkyReels-A2 VACE Ours (FFGo) Overall Quality Object Identity 2.09 2.34 3.00 4.28 3.32 2.89 3.50 4.53 Scene Identity Avg. Rank % Ranked 1st 3.27 3.02 2.50 1. 3.4% 4.3% 11.1% 81.2% 3.01 3.43 3.66 4.58 Table 1. User Study Results: We report ratings and rankings from 200 annotations across 40 users. FFGo consistently outperforms all baseline models across evaluation aspects, despite being trained on only 50 examples, demonstrating significantly better generalization across diverse application scenarios. Comparison with the Base Model. We first compare our adapted model with the base model (Wan2.2-I2V-A14B) to demonstrate the effectiveness of our approach in video content customization, as shown in Figures 5, 6, and 7. For the base model, we use the mixed image input along with fixed transition phrase (<transition>) that empirically performs best. As observed, the base model often animates elements independently, and the referred objects tend to disappear post-transition. In contrast, our adapted model consistently preserves object identities and performs coherent scene transitions, indicating substantial improvement in handling reference-based video customization. Preserving Pre-trained Knowledge in the Base Model. As demonstrated earlier, our add-on significantly enhances the base models performance in reference-based video customization. Crucially, it also preserves the base models original pre-trained knowledge. Since our method is designed to invoke the models innate capabilities rather than overwrite them, it retains the core generative priors embedded through pre-training. This preservation is illustrated in Figure 4. In the rare successful cases where the base model maintains all object identities and executes coherent scene transition, our results closely mirror the base output, in this instance, replicating the motion and positioning of the wingsuit performer and the car. This fidelity underscores an important advantage: our method enables customization without compromising the valuable general knowledge encoded during pre-training. Given that post-training data for customization is often narrower in scope and lower in quality than pre-training corpora, it is critical for video content customization approaches to integrate new reference inputs while preserving the strengths of the base model. Our add-on achieves exactly this, offering lightweight yet effective pathway to transform generalpurpose I2V models into powerful, user-controllable video customization systems. Comparison with State-of-the-Art Video Customization Baselines. We compare our method with two Wan-based state-of-the-art baselines: VACE and SkyReels-A2. 1) As shown in Figures 5 and 7, both baselines are trained on millions of videos for specific customization tasks involving the composition of three elements: human, object, and scene. However, this design leads to overfitting, limiting their generalization to novel application scenarios. In conFigure 4. As shown in the figure, in rare cases where the base model Wan2.2-I2V-A14B successfully performs scene transition while preserving all reference object identities, the output closely resembles ours. This demonstrates that our add-on approach effectively retains the base models pre-trained generative capabilities. trast, our method, trained on just 50 examples, activates the pre-trained base models innate capabilities while preserving its general knowledge, enabling superior performance across diverse use cases. 2) Figure 6 highlights key limitation of these baselines: their architecture only supports up to three reference inputs. As result, they fail to handle scenarios with five references. In contrast, our approach has no such architectural constraint, as our multi-reference capability stems from the the utilization of the first frame as conceptual memory buffer, not architectural modification. 4.4. Quantitative Comparison To quantitatively evaluate our method and compare it with baselines, we conducted user study on the full test set across all methods. Details of the user study setup are provided in the Supplementary Materials. We built an online interface where users are shown the prompt and reference image containing all foreground objects and background context. The system then displays four generated videos in randomized order. Users were asked to: 1) Rank the four videos from best (1st place) to worst (4th place). 2) Rate each video on three aspects: Object Identity: with score range from 1 (worst) to 5 (best), How well are foreground object identities preserved from the reference image?. Scene Identity: with score range from 1 (worst) to 5 (best), How accurately is the background retained? and Overall Quality: with score range from 1 (worst) to 5 (best), 6 Figure 5. Qualitative comparison with baseline methods. This test scenario involves generalized multi-object interactions. As shown in the figure, our method best preserves the identities of input objects and the scene, while generating customized video with coherent motion that aligns with the text prompt description. Figure 6. Qualitative comparison with baseline methods. This scenario evaluates performance with an excessive number of references, five in total (four objects and one scene). VACE and SkyReels-A2, due to their architecture-based limitations, support only up to three references and fail to include all four reference objects in the generated video. In contrast, our model successfully fuses all four objects into coherent, customized video with natural interactions. Notably, our model also enables precise selection via text prompt (e.g., blue iPhone), preserving key visual traits such as the triple-camera design while modifying appearance (e.g., changing the color to blue). 7 Figure 7. Qualitative comparison with baseline methods. This scenario evaluates generalized human-object interactions involving multiple humans. While both VACE and SkyReels-A2 excel in customized single human-object video generation, they struggle in more complex multi-human scenarios where interactions are mediated by shared objects. In such cases, both baselines fail to maintain object integrity and coherent interaction. In contrast, our method reliably generates consistent videos with preserved object identity and realistic multi-human interactions. How realistic and coherent is the video overall?. Table 1 presents the results. Our method (FFGo) outperformed all baselines across every metric, rank, object identity, scene identity, and overall quality, despite using only lightweight adaptation. Notably, over 80% of users selected our results as their top choice, indicating strong alignment with real user preferences. Importantly, FFGo transforms the base model Wan2.2-I2V-A14B from the lowest-performing baseline to the top performer in user evaluations. This highlights the strength of our add-on approach, which achieves state-of-theart customization performance without architectural changes or large-scale training. 5. Limitations While our adaptation effectively invokes the innate ability of pre-trained I2V models for video content customization via the first frame, several limitations remain. Although it is theoretically possible to incorporate an arbitrary number of reference subjects in the first frame, in practice, increasing the number of subjects reduces the resolution available to each, making identity preservation more difficult. Another challenge is selective control: as the number of reference subjects grows, it becomes harder to reliably target specific objects using only text prompts. Empirically, we find our method performs well up to four subjects plus reference scene (five references in total), beyond which identity preservation and prompt-based selection degrade. We believe these limitations are not fundamental and can be addressed through engineering improvements. For example, using multiple start frames as an extended conceptual memory buffer could allow for higher-capacity reference encoding. We leave such enhancements to future work. 6. Conclusions In this work, we propose fundamentally different perspective on the role of the first frame in video generation models. Contrary to the standard view that treats it merely as the spatiotemporal starting point of an animation, we show that the first frame functions as conceptual memory buffer, capable of storing and fusing disjoint reference subjects for downstream generation. Building on this insight, we introduce lightweight, add-on method to invoke this overlooked innate ability for video content customization. Without modifying the model architecture or requiring large-scale finetuning, our few-shot adaptation turns base video generation model into state-of-the-art video customization system. We demonstrate strong performance across wide range of real-world scenarios and validate our approach through comprehensive user study, showing clear alignment with user preferences."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [2] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motioncontrollable video diffusion models using real-time warped noise. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1323, 2025. 2 [3] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities, 2024. 3 [4] Jingxi Chen, Brandon Feng, Haoming Cai, Tianfu Wang, Levi Burner, Dehao Yuan, Cornelia Fermuller, Christopher Metzler, and Yiannis Aloimonos. Repurposing pre-trained video diffusion models for event-based video interpolation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1245612466, 2025. 2 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60996110, 2025. 2 [6] Google DeepMind. Gemini 2.5: Our most intelligent ai model. Google DeepMind, 2025. 3, [7] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, and Yahui Zhou. Skyreels-a2: Compose anything in video diffusion transformers, 2025. 2, 4, 5 [8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 3 [9] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3 [10] Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, and Yu-Chiang Frank Wang. Videomage: Multi-subject and motion customization of text-to-video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1760317612, 2025. 2 [11] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 3 [12] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66896700, 2024. [13] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing, 2025. 2, 4, 5 [14] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. survey of state of the art large vision language models: Benchmark evaluations and challenges. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 15871606, 2025. 3 [15] Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee Boyd-Graber. Videohallu: Evaluating and mitigating multimodal hallucinations on synthetic video understanding. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 3 [16] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan BoydGraber, Haitao Mi, and Dong Yu. Self-rewarding visionlanguage model via reasoning decomposition, 2025. 3 [17] Tingting Liao, Chongjian Ge, Guangyi Liu, Hao Li, and Yi Zhou. Character mixing for video generation. arXiv preprint arXiv:2510.05093, 2025. 2 [18] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 3 [19] Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, Xiaodong He, and Wu Liu. Hoigen-1m: large-scale dataset for human-object interaction video generation, 2025. 4, 5, [20] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. 3 [21] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 2 [22] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5 (5):5, 2017. 13 [23] Anish Madan, Neehar Peri, Shu Kong, and Deva Ramanan. Revisiting few-shot object detection with vision-language models. In Advances in Neural Information Processing Systems, pages 1954719560. Curran Associates, Inc., 2024. 3 [24] OpenAI. Gpt-4o system card, 2024. 3 [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [26] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [27] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman 9 Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 3 [36] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5omni technical report, 2025. 3 [37] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning, 2023. 3 [38] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models, 2025. [39] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [40] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. [41] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2024. 2 [42] Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Krähenbühl, and Liangzhe Yuan. Distilling vision-language models on millions of videos, 2024. 3 [43] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. 3 Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 4 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [29] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced largescale video generative models, 2025. 1, 2, 4, 5 [31] Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang, Pengfei Wei, Zerong Zheng, Ming Zhou, Yuan Zhang, and Mingyuan Gao. Dreamactor-h1: High-fidelity human-product demonstration video generation via motion-designed diffusion transformers. arXiv preprint arXiv:2506.10568, 2025. 2 [32] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. 3 [33] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. 2 [34] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners, 2025. 2, 3, 4, 5, 11 [35] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Work 3. Proposed Approach . 3.1. Pipeline Overview . . 3.2. Dataset Curation . . . . . 3.3. Few-shot LoRA Adaptation . 3.4. Clean Customized Video Inference . . . . . . . . . . . . . . 4. Experimental Results 4.1. Datasets and Implementation Details . 4.2. Evaluation Strategy . . . 4.3. Qualitative Comparison . . 4.4. Quantitative Comparison . . . . . . . . . . . . . . . 5. Limitations 6. Conclusions A. Video Results B. Details about Training and Testing Set B.1. Training Dataset Curation Details . . B.2. Test Set Curation . . . . . . . . . C. Details about User Study C.1. User Study Platform . C.2. User Interface Details . . . . . . . . . . . . . . D. More Training and Inference Details A. Video Results 2 3 3 4 5 5 5 5 5 5 6 8 8 11 11 11 12 12 12 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . to our refer http : / / Please page: firstframego . github . io for results, which clearly demonstrate the effectiveness of our method and its comparison with baseline models. project video B. Details about Training and Testing Set B.1. Training Dataset Curation Details Our training corpus is sourced from three datasets: one randomly selected folder from HOIGen-1M [19] ( 2,000 clips), all five Veo 3 demonstration videos [34], and 200 licensed short videos. This yields 2,205 candidate clips. We manually curate the data to select videos with clearly separable foregrounds, humans or manipulable objects, set against uncluttered backgrounds. Only clips depicting cleanly segmentable singleor multi-object interactions are retained. This filtering results in 50 high-quality training examples (Figure 10), distributed across four scene types: humanobject interaction (60%), humanhuman interaction (14%), element insertion (20%), and robot manipulation (6%). Training Data Processing. After curation, all clips are standardized to 81 frames for consistent training. From each video, we extract the first frame as reference image and manually tag all foreground entities of interest, e.g., cake, party hat, male presenter, mouse. Using prompt-to-prompt workflow with Gemini-2.5-Pro, we then perform: Object Extraction: Apply Prompt 8 to generate highfidelity renditions of each tagged entity, preserving their original appearance and scale. We refine results using SAM 2 or Adobe Photoshop to isolate each object as an RGBA layer. Background Cleanup: Use Prompt 9 to produce clean companion image with all tagged objects removed, yielding pristine background plate. This paired set of object cut-outs and object-free backgrounds forms the compositional basis of the training first frame. Caption Generation. We use Gemini-2.5-Pro to generate rich, element-aware captions for each training sample, based on the individual object cut-outs, clean background plate, and the full 81-frame video. These inputs are paired with structured prompt template  (Fig. 11)  to ensure consistency and relevance. Element Composition for First Frame. For each training clip, we synthesize 1280720 reference canvas: all foreground cut-outs are vertically tiled on the left half, while the clean background is centered on the right (see Fig. 10). This composite serves as both the conditioning input and the initial frame, guiding the video generation model to blend the elements into coherent sequence. B.2. Test Set Curation We manually curated diverse test set of foreground objects and backgrounds from our self-collected images. Each object was segmented using SAM 2 or Adobe Photoshop and saved as an RGBA cut-out. These cut-outs were then composited with their respective backgrounds on 1280720 canvas, following the same layout used in training. For each object-background pair, we drafted an initial prompt and refined it using Gemini-2.5-Pro with the template shown in Fig. 12. This process produced 50 high-quality prompts paired with composite reference images, forming our final test set."
        },
        {
            "title": "Object Extraction Task Prompt Template",
            "content": "Prompt Given the input image, extract the subset {IDENTIFIED OBJECT} (i.e., only the specified foreground objects) return them alone with no resizing, compression, or background so the output resolution exactly matches the original image. Figure 8. Prompt for extracting identified foreground objects using unified VLM."
        },
        {
            "title": "Object Removal Task Prompt Template",
            "content": "Prompt Given the input image, remove the subset {IDENTIFIED OBJECTS} entirely. Return the edited image onlyit must preserve the source resolution (no scaling or compression) and contain neither the specified objects nor any artifacts of their removal. Figure 9. Prompt and specifications for the object removal task. Figure 10. Our training dataset comprises four categories: humanobject interaction (60%), humanhuman interaction (14%), element insertion (20%), and robot manipulation (6%). C. Details about User Study Our recruitment post and task instructions are shown in To ensure smooth user study and annotation experience, we developed an HTML-based interface for participants to annotate and submit data. In this section, we describe the hiring platform, the job posting, and the design of the annotation interface. C.1. User Study Platform We recruit participants through Prolific,1 research platform designed for user studies. Prolific offers an AI user study beta program that targets participants with experience in generative AI annotation. To ensure quality, we apply screening filters to select participants with prior video annotation experience and fluent English proficiency, as understanding nuanced textual prompts is crucial for this task. We hire 40 participants, each tasked with annotating five video sets, where each set contains generated outputs from four different models. The annotation process takes approximately 15 minutes per participant. Each is compensated $5.50, reflecting the expected time and effort. 1https://www.prolific.com/ Figure 13. C.2. User Interface Details Participants first arrive at login screen, where they enter their unique Prolific ID to match their responses with task-completion records. After authentication, they are presented with the textual prompt used to generate the videos, along with composite reference image showing the required foreground objects and background. Below, four candidate videos are displayed in randomized order to eliminate position bias. Participants then rank the videos based on overall quality, as shown in Figure 14a. Next, participants scroll down to rate each video on three criteria, Object Identity, Scene Identity, and Overall Quality, using 5-point Likert scale (Figure 14b). D. More Training and Inference Details We train LoRA modules of rank 128 for both highand lownoise regime transformers in the base model Wan2.2-I2VA14B. Training videos are resized to resolution of 1344 768 with 81 frames. We use batch size of 4 and optimize"
        },
        {
            "title": "Training Data Prompt Generation Prompt Template",
            "content": "Task Description You are given video and several images. Generate descriptive caption for the video that prominently features the components shown in the images. Wrap your final text in <caption>. . . </caption> tags. The caption must highlight the significance and role of these components throughout the video, while omitting filler such as The scene unfolds with whimsical and heartwarming narrative, emphasizing the simple joys of life through the Teddy Bears endearing actions. Examples of Descriptive Captions 1. Film quality, professional quality, rich details. The video begins to show the surface of pond, and the camera slowly zooms in to close-up. The water surface begins to bubble, and then blonde woman is seen coming out of the lotus pond soaked all over, showing the subtle changes in her facial expression. 2. professional male diver performs an elegant diving maneuver from high platform. Full-body side view captures him wearing bright red swim trunks in an upside-down posture with arms fully extended and legs straight and pressed together. The camera pans downward as he dives into the water below. Figure 11. Prompt template used to generate captions for our training data. Video-Prompt Enhancement Output Task Description You will be given prompt and several images for video generation. You task is to make the prompt richer in description so the model can understand better. Enclose your caption within <caption></caption> tags. The caption must emphasize the significance and role of these components (and some description of each component) throughout the video. Your caption should exclude unnecessary information such as The scene unfolds with whimsical and heartwarming narrative, emphasizing the simple joys of life through the Teddy Bears endearing actions. Example of Descriptive Caption 1. Film quality, professional quality, rich details. The video begins to show the surface of pond, and the camera slowly zooms in to close-up. The water surface begins to bubble, and then blonde woman is seen coming out of the lotus pond soaked all over, showing the subtle changes in her facial expression. 2. professional male diver performs an elegant diving maneuver from high platform. Full-body side view captures him wearing bright red swim trunks in an upside-down posture with arms fully extended and legs straight and pressed together. The camera pans downward as he dives into the water below. Prompt to Optimize {Insert your test prompt to optimize here} Figure 12. Prompt template for test prompt enhancement. with AdamW [22], setting the learning rate to 1 104, ϵ = 1 1010, and weight decay of 3 102. During inference, videos are generated at resolution of 1280 720 with 81 frames, following the standard output format of Wan2.2-I2V-A14B based models."
        },
        {
            "title": "Annotation Task Instructions",
            "content": "You will be presented with five sets of short, AI-generated videos (5 s, no audio). Each set contains: Prompt textual description of the intended video (scene, objects, motion). Reference Image split into two halves: Left side: foreground objects that should appear in the video. Right side: background scene to be integrated with the objects. Generated Videos (4 total) four model outputs attempting to fuse the objects with the background."
        },
        {
            "title": "Your Task for Each Set",
            "content": "Step 1: Overall Ranking Watch all four videos carefully. Rank them from best to worst based on overall quality and faithfulness to the prompt. Assign unique ranks (1 = best, 4 = worst). Step 2: Aspect Ratings After ranking, rate each video on 15 scale (1 = very poor, 5 = excellent): Object Identity How well do objects retain their identity? Scene / Background Identity How well is the background preserved? Video Quality Overall realism and temporal coherence. Notes Evaluate all four videos in every set before submitting answers. There are five sets in total (20 videos). Figure 13. Recruitment post for our user study. (a) Users rank the overall quality of the four candidate videos. (b) Users rate three specific aspects with Likert scale 1-5. Figure 14. Web-based annotation interface used in our user study. Part (a) collects global quality ranking, while part (b) gathers detailed aspect-wise ratings for each video."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "University of Maryland",
        "University of Southern California"
    ]
}