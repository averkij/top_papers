{
    "paper_title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
    "authors": [
        "Yang Liu",
        "Jiaye Yang",
        "Weikang Li",
        "Jiahui Liang",
        "Yang Li",
        "Lingyong Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications."
        },
        {
            "title": "Start",
            "content": "LM-LEXICON: Improving Definition Modeling via Harmonizing Semantic Experts Yang Liu1 Jiahui Liang2 1BIGAI Jiaye Yang2 Yang Li2 2Baidu Inc. Weikang Li3 Lingyong Yan2 3Peking University liuyang@bigai.ai https://lm-lexicon.github.io https://huggingface.co/LM-Lexicon 6 2 0 2 5 1 ] . [ 1 0 6 0 4 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce LM-LEXICON, an innovative definition modeling approach that incorporates data clustering, semantic experts learning, and model merging using sparse mixtureof-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language modLMels are trained as domain experts, LEXICON achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications."
        },
        {
            "title": "Introduction",
            "content": "Defining terms (Figure 1) is the first step toward constructing lexicon for language (Pustejovsky and Boguraev, 1993). Precise definitions should be formed as summarized and human-readable sentences that capture the main sense of term. Modern language use demands continuous updates to include new terms, novel senses, meaning shifts, and domain knowledge (Hogeweg and Vicente, 2020), yet traditional lexicon construction remains laborintensive (Ahlswede, 1985). To address this challenge, definition modeling (DM) has emerged as promising approach, where definitions are automatically generated based on the target term and its context (Giulianelli et al., 2023, inter alia). Equal contribution. Correspondence to: liuyang@bigai.ai Figure 1: Four examples of the term, context (input), and definition (output) for definition modeling task. While existing DM approaches yield reasonable results, they face several key limitations. First, current methods struggle to capture subtle and rare word senses, resulting in incomplete semantic coverage (Huang et al., 2021; Giulianelli et al., 2023; Periti et al., 2024). Second, even frontier large language models (LLMs), despite their strong language understanding capabilities, tend to generate definitions that are either overly generic or excessively specific (Jhirad et al., 2023; Yin and Skiena, 2023; Almeman et al., 2024). Third, existing methods often fail to handle terms that exhibit different meanings across domains (e.g., technical vs. general usage), phenomenon known as semantic heterogeneity (Huang et al., 2021). Recent attempts such as domain adaptation (Zhang et al., 2022) or multi-task learning (Kong et al., 2022) have shown limited success. These challenges point to an inherent bottleneck in current LLMs: their dense architectures force polysemantic representation to highly share the same neurons (i.e., superposition) (Elhage et al., 2022), making it difficult to maintain precise, domain-specific meaning representations (Bricken et al., 2023). Due to the lack of sparsification mechanisms, this architectural constraint affects their ability to generate accurate definitions when words have distinct meanings across different domains. To mitigate these issues, we propose LMLEXICON (Language Model as Lexicon), which learns to perform DM covering multiple domains, adapting diverse definition genres with scalable mixture-of-experts (MoE) architecture. Unlike prior work, such as BTX (Sukhbaatar et al., 2024) and LLAMA-MOE (Zhu et al., 2024), our method incorporates data clustering, semantic expert-specialized MoE, and domain-level sequence routing, obtaining impressive performance gains in DM benchmarks. As depicted in Figure 2, instead of training directly on raw definition corpora, our method trains multiple semantic experts parallely, merges them by composing their weights, and routes test samples with the introduced semantic-aware router during inference. Our contributions can be summarized as follows: We propose LM-LEXICON, framework for definition modeling by harmonizing inherent heterogeneity in lexical semantics. It allows specialized semantic experts to be integrated for domain updates, enabling generalization to new domains, or collapsing back to single expert for efficient inference. We design domain-level sequence routing LM-LEXICON. This method policy in routes representation of samples informed by fine-grained information via semantic domains identified with pre-hoc auto clustering. Extensive experiments across five benchmarks validate the effectiveness of LMLEXICON. Notably, in automatic evaluation, LM-LEXICON shows up to 10% improvement over strong baselines. Furthermore, LM-LEXICON excels across most criteria in human evaluation, particularly outperforming frontier LLMs in semantic-intensive scenarios, where even many-shot setups fail to produce appropriate definitions."
        },
        {
            "title": "2 Related Work",
            "content": "Upcycling to Mixture-of-Experts. On the aspect of model efficiency and expressiveness, Fedus et al. (2022); Jiang et al. (2024); Shao et al. (2024) focus on designing efficient MoE architecture with tokenlevel router. From the expert specialization aspect, Li et al. (2022) introduced Branch-Train-Merge (BTM) that learns expert LMs specialized to different domains and Sukhbaatar et al. (2024) developed Branch-Train-MiX (BTX), which composes set of specialized LMs by their feed-forward networks. In addition, Zoph et al. (2022); Jiang et al. (2024); Petridis et al. (2024); Ma et al. (2024) revealed the efficacy of expert specialization at the lexicon, structured syntactic, and semantic domain level, respectively. However, these works adopt conventional routing schemes, such as token-level TopK routing, rather than exploring those better suited for semantic-intensive scenarios. Definition Modeling. Several early studies on DM (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018; Ishiwatari et al., 2019, inter alia) leveraged pre-trained word embeddings as global or local contexts of term, to generate definitions of the given target word. Then Huang et al. (2021); Kong et al. (2022); Zhang et al. (2022); Giulianelli et al. (2023); Periti et al. (2024) propose methods for DM using Transformer-based Seq2Seq LMs (e.g., T5) and Causal LMs. In the era of LLM, Jhirad et al. (2023) and Yin and Skiena (2023) used LLMs such as GPT-3.5 and GPT-4 to perform DM with in-context learning tailored to diverse domains. Periti et al. (2024) explored training causal LMs to generate definitions with instruction tuning; however, they still lack detailed quality evaluation and comphrehensive comparison with baselines."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present the details of our proposed LM-LEXICON framework. 3.1 introduces the formulation to illustrate the main idea. In 3.2, we illustrate the design of semantic expert specialization, followed by model merging in 3.3."
        },
        {
            "title": "3.1 Overview of",
            "content": "LM-LEXICON Given seed model that has been pre-trained, our goal is to improve its multi-domain performance in lexical semantics. As shown in Fig. LM-LEXICON consists 2, the framework of (1) semantic expert speof two components: cialization and (2) MoE model merging. The Figure 2: Diagram of LM-LEXICON (i.e., Specialize-then-Synthesize) framework. proposed method contains three stages, training data partitioning, parallel expert training, and separate experts merging, i.e., the Specialize-thenSynthesize framework. Considering the heterogeneity of glosses, we split the training data into semantically distinctive clusters to facilitate expert learning. To model various domains, we use separate models to learn domain-specific knowledge asynchronously. To perform the DM task generally, we merge these experts into single MoE model for further fine-tuning."
        },
        {
            "title": "Experts",
            "content": "Dataset Construction. Training data consists of triplets c, t, d, where represents the context in which the term is used (either sentence or phrase), denotes the term itself, and is its reference definition. concatenated sequence is then formatted using the prompt template p(, ) as input. Specifically, we follow Giulianelli et al. (2023) to use := <BOS>{{c}} WHAT IS THE DEFINITION OF {{t}}<EOS> as the prompt template. Clustering. LM-LEXICON begins with the training data partitioning since merging without it could lead to group of homogeneous experts. To cluster training data, we calculate the embeddings of p(c, t) in each training sample with nvidiaembed-v2 (Lee et al., 2025), and then cluster with balanced k-means (Malinen and Fränti, 2014). This process results in clusters in terms of lexical semantics, each related to semantic domain such as adjectives and proper nouns (see Fig. 3), corresponding to partitioned training datasets := {D1, . . . , DN }. It also produces cluster centroids {v1, v2, . . . , vn}. In the present study, we perform pre-experiments to determine the number of clusters and select = 4 as the best cluster numbers by the cluster cohesion and separation in the DM scenario (See Appendix C.1), as well as considering the training and inference efficiency. Experts Training. Initializing from seed model M, we train LMs: {M1, . . . , MN } as experts, with each model Mi being trained on the corresponding dataset Di, using the negative loglikelihood (NLL) loss in Eq. 1: LNLL = E(c,t,d)D (cid:104) log P( ˆd p(c, t)) (cid:105) . (1) Here, ˆd denotes the definition predicted by the model, given the prompt p(, ). We employ lossmasking strategy to omit the tokens of prompt during loss computation, ensuring that gradients are only propagated through tokens in the part of predicted definition. When expert training finished, we end up with different LMs, with each specialized in domain Di."
        },
        {
            "title": "3.3 Merging Experts into a Unified MoE",
            "content": "After all domain experts are obtained, previous works either average the final output distributions of experts to generate next token (Gururangan et al., 2023) or select experts by determining which domain the input belongs to at the test time (Li et al., 2022). Differently, we perform MoE Upcycling by merging the weights of experts, aiming at mixing model capabilities across diverse domains. Model Merging. We combine semantic experts into unified MoE to exploit the parametric domain capability (Sukhbaatar et al., 2024; Zhou et al., 2025). In the composition, LM-LEXICON brings together the feed-forward networks (FFNs) of the expert models as expert layers in MoE and averages the remaining parameters. Specifically, if FFNℓ i(x) is the FFNs at the ℓ-th layer of the i-th expert Mi, then the combined MoE layer for input representation at layer ℓ will be computed as: FFNℓ MoE(x) = (cid:88) i= G(x) FFNℓ i(x). (2) where G() is semantic domain-level router. During both training and inference, the input representation will be routed to the nearest centroid by computing its pairwise cosine similarity with each semantic label (i.e., the centroid of domain cluster), as illustrated in 3.2. G() usually has sparse output and hence switches on only some experts. In LM-LEXICON, we start from topk (k = 2) routing (Shazeer et al., 2017), where G(x) = Softmax(TopK(W ℓx)), where ℓ is linear transformation in router. For multihead selfattention (MHA) sublayers and the remaining parameters (e.g., embedding layer), we average the weights of domains. The merging process of MoE model is provided in Algorithm 1. The above merging model into MoE introduces router with new parameters ℓ, which requires further learning to make optimal choices. To enhance semantic-aware experts after merging, we continue to slightly fine-tune the router and selected expert layers to coordinate them in the semantic representation space (Bai et al., 2025)."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Datasets. We use the benchmarks introduced in Ishiwatari et al. (2019)(see Table 1), which consist Algorithm 1 Compose MHA and MLP modules for each decoder layer ℓ in LM-LEXICON. Input: Domain Experts := {e1, e2, . . . , en}. Output: LM-LEXICON-MOE (M) 1: procedure MODULES-COMPOSER(E) 2: 3: INIT STATE DICT for ei do ITERATE EACH EXPERT 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: GetExpertIdx(ei) /* Retrieve MHA and MLP weights */ θmha, θmlp HookWeights(ei) for θ {θmha, θmlp} do if IsRouterLayer(θ) then /* Get formatted layer name */ FormatName(θ, i) M[n] θ else AVERAGE θ OF MODULE M[n] M.get(n, 0) + θ/E return of four small datasets and 3D-EX from Almeman et al. (2023) (see details in A). WordNet (Noraset et al., 2017) is an online dataset1 of terms, definitions, and examples. Oxford (Gadetsky et al., 2018) is built on the widely used online oxford dictionary2. Wikipedia3 (Ishiwatari et al., 2019) is introduced to test the model capacity on the description of phrases, rather than words. Urban (Ni and Wang, 2017)4 contains terms of internet slang and urban words. 3D-EX (Almeman et al., 2023) is the largest English definition modeling dataset5 which comprises many well-known DM resources, including the four mentioned datasets. Note that we perform clustering only on 3D-EX and use the resulting four clusters for finetuning and merging semantic experts. Compared Baselines. Llama-3-8B (Dubey et al., 2024) is used as the seed model for asynchronous expert training. We select three categories of strong baseline methods for comparison purposes. 1https://wordnet.princeton.edu 2https://en.oxforddictionaries.com 3https://www.wikidata.org 4https://www.urbandictionary.com 5https://github.com/F-Almeman/3D-EX"
        },
        {
            "title": "Urban",
            "content": "3D-EX genre domain publish year train # # # test valid"
        },
        {
            "title": "WordNet",
            "content": "formal synset 2017 13, 883 1, 752 1,"
        },
        {
            "title": "Oxford",
            "content": "formal lexicon 2018 97, 855 12, 232 12, 232 web encyclopedia 2018 887, 455 44, 003 57, 232 idiom slang 2017 411, 384 57, 883 36, # glo. per term 1.75 1.19 # tok. per term 1.00 0.00 5.79 3.44 # tok. per ctx. 6.64 3.78 # tok. per glo. 0.00 / 0.00 % overlap rate 2.99 4.41 1.00 0.00 19.02 9.18 11.41 7.13 80.72 / 0.09 5.86 78.25 1.85 0.93 19.68 6.31 5.97 4.51 0.00 / 0.00 2.11 2.92 1.44 0.72 11.36 6.02 11.02 6.86 20.62 / 20.56 misc. multi 2023 1, 309, 312 513, 789 450, 6.00 53.78 1.45 0.78 18.82 9.99 8.97 6.76 0.00 / 0.00 Table 1: For datasets used in this paper, we report the mean and standard deviation of per-term, per-context, and per-gloss statistics. We report the number of terms of samples denoted for train, valid, and test splits in each dataset. The lexical overlap of each dataset is computed with test. Specifically, the % is computed by intersection rate of term occurrence and the % is computed by intersection rate of pair-wise term gloss. test / train Supervised Seq2seq LM: We reproduce Rerank-T5 (Huang et al., 2021), Contrast-T5 (Zhang et al., 2022), SimpDefiner (Kong et al., 2022), MDM-T5 (Zhang et al., 2023), and Flan-T5-Def (Giulianelli et al., 2023). Supervised Causal LM: We report the indistribution results of LlamaDictionary (Periti et al., 2024), which is finetuned on Llama-38B-Instruct, and assess its out-of-distribution performance for the unseen domains. Frontier Causal LM: We test GPT-4Turbo (Achiam et al., 2023), Gemini-1.5-Pro (Reid et al., 2024), and Claude-3-Opus (Anthropic, 2024) with random exemplar selection (Random-ICL) and retrieval-based exemplar ranking (Retrieval-ICL) based on Wu et al. (2023) in many-shot settings. Training and Evaluation Details. We run instruction tuning on four clusters obtained from 3D-EX respectively. The models trained on four clusters of 3D-EX are merged through 3.3. After merging, we proceed to fine-tune the MoE model to learn routers using the full 3D-EX dataset. In addition, we perform instruction tuning on the four real-world datasets. The hyperparameters can be found in the Tab. 12. We run three times with seeds to report the mean results and the standard deviation, with seed si {21, 42, 84}. All experiments are conducted on 8 NVIDIA H100. Model sizes and training FLOPs are reported in Table 6. Figure 3: Four-cluster UMAP plot of 10K random definitions of terms in 3D-EX (4). Each cluster is assigned manually with [label] by their major constituents. We employ metrics including (1) lexical n-grambased: BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Lavie and Agarwal, 2007); (2) semantic-based: BERTSCORE (Zhang et al., 2019), MOVERSCORE (Zhao et al., 2019), and MAUVE (Pillutla et al., 2021). We reuse the implementation of BLEU in Huang et al. (2021), ROUGE and BERTSCORE used in Giulianelli et al. (2023), as well as the rest of metrics for evaluation. To further evaluate the effectiveness of our method, we perform human evaluation described in 4.2."
        },
        {
            "title": "4.2 Main Results",
            "content": "6We develop ad-hoc heuristic parser for proprietary models WordNet Oxford Wiki Urban 3D-EX BLEU ROUGE BLEU ROUGE BLEU ROUGE BLEU ROUGE BLEU ROUGE Rerank-T5 (2021) Contrast-T5 (2022) SimpDefiner (2022) MDM-T5 (2023) Flan-T5-Def (2023) LlamaDict (2024) GPT-4-TURBO (cid:44) + Random-ICL (cid:44) + Retrieval-ICL CLAUDE-3-OPUS (cid:44) + Random-ICL (cid:44) + Retrieval-ICL GEMINI-1.5-PRO (cid:44) + Random-ICL (cid:44) + Retrieval-ICL 30.91 30.81 28.91 31.18 31.96 33.86 30.95 27.46 28.63 18.57 23.42 25.24 LM-LEXICON-DENSE (8B) 30.99 26.27 20.47 32.55 40.45 43. 32.61 29.74 27.84 21.76 26.27 27.88 25.56 22.51 23.48 24.16 21.34 22.77 21.93 20.44 19.99 15. 25.51 28.10 28.00 28.18 29.59 27.68 32.39 36.46 30.82 34.35 34.21 25.99 35.97 36.98 55.61 55.26 44.03 54.33 13.82 14. 31.63 35.40 23.30 14.59 36.87 35.59 57.25 42.27 49.26 55.83 23.97 25.29 45.89 40.68 35.22 15. 48.13 43.71 17.77 17.53 13.54 17.53 5.33 15.70 11.08 22.53 1.59 5.93 8.44 8.85 18.25 16.34 15.37 17.18 10.61 14. 12.19 26.53 3.08 7.19 9.59 9.18 34.43 34.27 32.08 32.67 26.43 24.56 25.93 29.73 18.57 17. 29.4 32.99 38.57 37.62 31.57 32.38 25.12 26.11 34.48 37.66 28.49 24.67 38.02 39.14 Avg. Results 32.85 / 34.61 32.07 / 30.13 28.40 / 29.25 31.97 / 33.12 19.77 / 26.50 22.50 / 29.17 24.30 / 31.19 27.11 / 33.79 18.41 / 25.76 14.41 / 19.08 24.72 / 31.59 26.15 / 31.37 (cid:44) + Zero-shot (cid:44) + BoN-Oracle (cid:44) + BoN-ORM 36.99 0.59 47.90 0.30 37.73 37.83 0.45 44.19 0.80 37.94 0.38 26.09 0.60 30.07 0.06 26.74 0.18 34.55 0.57 42.78 0.11 35.18 0. 57.9 2.44 62.07 0.11 59.33 0.12 59.56 1.50 68.62 0.19 59.46 0.37 26.09 0.27 36.16 0.69 26.73 0. 28.35 0.28 38.87 0.47 28.54 0.46 35.01 0.22 48.78 0.89 34.83 0.20 43.32 0.27 49.71 2.21 42.68 0. 34.63 / 38.79 44.99 / 48.83 37.07 / 40.76 0.26 LM-LEXICON-MOE (48B) (cid:44) + Zero-shot (cid:44) + BoN-Oracle (cid:44) + BoN-ORM 40.09 0.12 47.39 0.16 40.33 0.18 40.51 0.28 40.31 0.23 40.69 0.26 23.35 0.25 30.87 0.24 24.18 0.37 32.94 0.49 43.24 0.25 33.79 0.64 60.31 0.55 51.62 1.14 60.88 0. 55.52 0.33 61.88 0.30 57.66 0.73 31.26 0.85 35.23 0.42 31.08 0.17 33.81 2.26 35.69 0.26 33.26 0.22 45.69 1.25 54.84 0.12 45.86 0.38 46.07 1.06 50.50 0.11 46.38 0.26 40.14 / 41.77 43.99 / 46.32 40.46 / 42.35 Table 2: Main results on five benchmarks6. We highlight the highest scores among LM-LEXICON and compared methods; * denotes the significance test, where < 0.005 between our method and Rerank-T5 (prior SoTA). denotes that we reproduce the in-distribution results with supervised training, and indicates that the lines of results are not directly comparable with other settings. All *-ICL settings employ the best setting with 32-shot in practice. LM-LEXICON. Competitive Performance of Table 2 presents the performance comparisons among baselines and existing SoTA methods for DM, including LM-LEXICON-DENSE models (trained on four real-world datasets) and LM-LEXICON-MOE, the proposed MoE model. LM-LEXICON outperforms strong supervised methods and frontier models with distinct advantage. Specifically, (1) LM-LEXICON obtains nearly 10% extra BLEU and ROUGE improvements on 3D-EX over the prior SoTA. (2) It performs exceptionally on smaller datasets as well, for example, LM-LEXICON achieves the highest scores ({31.26%, 33.81%} on {BLEU, ROUGE}) among all compared methods on Urban dataset, indicating the efficacy of our method to model rare word senses and usages. (3) The comparison between the many-shot learning of best perfomant LM-LEXICON demonstrates frontier LMs and that our method surpasses significantly larger dense models, by {23.44%, 9.14%} on {Wiki, WordNet} in BLEU for instance. (4) It is also observed that the Oxford dataset has lower performance with our method. possible reason is that short term and relatively long context in Oxford makes it harder for the model to predict accurate definitions. Furthermore, compared to other benchmarks, the Oxford dataset exhibits significantly high term over- & LM-LEXICON to extract our focused part of the generation. lap rate of around 80% along with near-zero termdefinition overlap rate. This stark contrast underscores the strong polysemy inherent in Oxfords terms. Consequently, models trained on Oxford struggle to generalize effectively when encountering previously seen terms used in different contexts. Overall, LM-LEXICON shows clear advantage that confirms the effectiveness of introduced semantic expert specialization and semantic-focused sparsifying upcycling into LM-LEXICON. Human Evaluation. The human evaluation was conducted using random subset of 300 samples from the 3D-EX, comparing definitions generated by our model (LM-LEXICON-MOE) and the baselines (LM-LEXICON-DENSE and three proprietary models). We focus on comparing with proprietary models as they represent the current state-of-theart in practical deployment and are the primary competitors in real-world lexicon construction scenarios. To obtain fine-grained understanding of model-specific characteristics, we further propose five criteria: (1) accuracy measures how correctly the definition captures the core semantic meaning of the word; (2) clarity evaluates the definitions comprehensibility and transparency in conveying meaning, focusing on how easily readers can understand the concept; (3) conciseness assesses whether the definition achieves optimal length without reAblation on Different Data Partition Designs. Since LM-LEXICON integrates the knowledge acquired by experts from various data partitions, our first focus is on the impact of data partition methods. To this end, we considered three settings: (1) no split; (2) random split; and (3) lexical split. For random split, we follow Li et al. (2022) to slice the data into four balanced subsets and specialise an expert for each of them. For lexical split, we perform partition by TF-IDF (Sparck Jones, 1972). As shown in Table 3, we observed that the original setting with semantic embedding clustering outperforms lexical-based partition with about +7% gains in BLEU and +1% gains in ROUGE on 3DEX. The results imply that learning from semantictargeted data clusters may help capture more precise senses and use more appropriate words to compose definitions. Lastly, it enables LMLEXICON to develop more robust experts for various domains."
        },
        {
            "title": "ROUGE",
            "content": "p-value LM-LEXICON 45.690.3 46.070.1 + w/ no split + w/ random split + w/ lexical split 35.130.2 36.241.4 38.130. 43.460.3 43.580.8 44.120.6 2.9e5 1.6e5 1.3e4 Table 3: Ablation on data partition method. routing used in Comparison among Routing Policies. Other LMthan domain-level LEXICON as default, we experiment on (1) top-1 token-level; (2) top-2 token-level; and (3) sequencelevel routing. For token-level routing, we follow the implementation of Fedus et al. (2022) and Jiang et al. (2024). For sequence-level routing, we follow Pham et al. (2023)."
        },
        {
            "title": "ROUGE",
            "content": "p-value LM-LEXICON 45.690.3 46.070.1 + w/ top-1 token-level + w/ top-2 token-level + w/ sequence-level 43.120.4 45.380.2 44.470. 43.790.5 45.210.1 44.820.3 1.9e3 8.6e1 2.7e3 Table 4: Ablation on different routing policies. Table 4 presents that the domain-level routing ( LM-LEXICON) is the most effective, even surpassing one of the popular scheme, the top-2 token-level routing, indicating that semantic routing via specified domain cluster is more beneficial for semantic-intensive tasks. Figure 4: Best-of-N repeated sampling results (BLEU) on five benchmarks evaluated by oracle verifier. dundancy or omission; (4) context appropriateness measures how well the definition reflects associated contexts, situations, and pragmatic constraints of the words; (5) grammar and fluency evaluates the grammatical correctness and naturalness of the definition. We employ three graduate students majoring in linguistics and lexicography, who were instructed to assess each of the above criteria on 5-point scale, where 1 indicates the poorest quality and 5 represents the highest quality (Figure 12). The model names were kept anonymous from human evaluators to avoid possible bias, whereas the reference definitions remained accessible to them. Figure 5 (right) presents the human evaluation results across five criteria, showing the average scores for each model7. LM-LEXICON-MOE consistently outperforms other models in most dimensions, with particularly strong performance of accuracy (4.6). While all models demonstrate competent performance with scores above 3.8, LM-LEXICON-MOE shows notable advantages in capturing contextual nuances and maintaining clarity and conciseness in definitions. The proprietary models perform similarly well but show slightly lower scores in terms of context appropriateness and conciseness than other criteria. We provide detailed analysis of representative example coon in Appendix E."
        },
        {
            "title": "4.3 Ablation Study and Extra Investigation",
            "content": "In this section, we further conduct an in-depth analysis of LM-LEXICON, regarding: (1) data partition method, (2) routing policy, and (3) number of experts. In addition, we explore the impact of test-time scaling. Finally, we examine the scaling effect of ICL for proprietary LLMs. 7Details on annotators agreement can be found in D. Figure 5: Scaling performance gains and human evaluation results. The left figure: Scaling test performance on 3D-EX, with varying number of experts. The right figure: Human evaluation results across five criteria. Different Number of Semantic Experts. Except for the above four-experts LM-LEXICON-MOE, to investigate the impact of the number of semantic experts, we compare varied number of semantic experts (N = 1, 2, 4, 8). Notably, when = 1, LM-LEXICON collapses back to dense model and expands to sparse model with > 1 experts. As shown in Figure 5 (left), we find that across all settings of , the performance of our method consistently increases and outperforms the others, which are composed of fewer experts. For example, the model of = 1 returns 41.38% while = 8 yields 46.86% in BLEU. This tendency implies the scalability of our method, using more semantic experts. This trend can be extended by integrating more fine-grained semantic experts (Dai et al., 2024), but we leave this direction for future work. Impact of Test-time Scaling. In light of Stiennon et al. (2020) and Cobbe et al. (2021), we are curious on how to boost performance further via testtime scaling, notably ground truth-based (i.e., Oracle) verifier and Best-of-N (BoN) sampling with an outcome reward model (ORM). For oracle verifier, it uses reference as verification to provide binary feedbacks. For an ORM, it employs scalar feedback to select the optimal generation from candidates. As depicted in Table 2 (BoN-ORM), interestingly, the oracle verifier is able to boost task performance (avg. BLEU > 2%) for LM-LEXICONDENSE. However, it exhibits more limitations for LM-LEXICON-MOE; we speculate it is due to the diversity diminishment of models, as illustrated in Brown et al. (2024). Intuitively, optimal results are achieved with oracle verifier  (Fig. 4)  through repeated sampling with 128 completions per test sample. Intergating with the ORM or Oracle verifier, LM-LEXICONs generation quality shows consistent improvements across five benchmarks with the increase in the number of generations. This outcome aligns with the findings on math reasoning tasks (Cobbe et al., 2021; Brown et al., 2024)."
        },
        {
            "title": "5 Conclusion",
            "content": "LM-LEXICON, an apIn this paper, we present proach that combines domain experts upcycling with sparse MoE model, which can generate appropriate definitions of terms in various domains and genres. We show that it significantly outperforms frontier LLMs and strong supervised baselines. We hope LM-LEXICON could be extended to more domains and other semanticintensive tasks in the future."
        },
        {
            "title": "Limitations",
            "content": "Extrapolation to More Tasks. While we believe our observations and conclusions are comprehensive within our experimental settings, our work only focus on the task of definition modeling in English in this work. Future work could benefit from our findings in extending to other domains and similar tasks in semantic-intensive scenarios. Training Efficienty and Cost. Our method performs supervised fine-tuning of expert LMs that are initialized from seed model. The training process can be thoroughly offline and asynchronous; however, it still needs an essential and sufficient computation budget to some extent. We encourage people to further explore parameterefficient training methods based on LM-LEXICON. Desiderata for Stronger Verifiers. Our results from Section 4.3 highlight the significance of improving sample verification methods tailored for definition modeling, and even more general language generation, which are currently unavailable or highly limited. Most existing verification methods have been developed only to solve easily verifiable reasoning tasks, such as mathematical (Li et al., 2025), software engineering (Yang et al., 2024), and logical reasoning problems (Liu et al., 2025). We believe that equipping models with the ability to assess their own generations will allow test-time compute methods to be scaled further."
        },
        {
            "title": "Ethics Statement",
            "content": "This research was conducted with careful consideration of ethical implications. All data used in this study was collected from public sources with appropriate permissions. We have taken measures to ensure privacy protection and prevent misuse of our model. The computational resources were used responsibly, and we have documented all potential biases and limitations. Our annotation process followed fair labor practices with appropriate compensation for annotators."
        },
        {
            "title": "Acknowledgement",
            "content": "We are deeply grateful to all the reviewers for their valuable feedback and thoughtful efforts in helping us improve this manuscript. We would also like to thank Ziang Wu for his contributions to the early exploration and discussions that shaped this work, and Ivan Fung for his support of the computational resources that made this project possible."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Thomas E. Ahlswede. 1985. tool kit for lexicon building. In 23rd Annual Meeting of the Association for Computational Linguistics, pages 268276, Chicago, Illinois, USA. Association for Computational Linguistics. Fatemah Almeman, Hadi Sheikhi, and Luis Espinosa Anke. 2023. 3D-EX: unified dataset of In Proceeddefinitions and dictionary examples. ings of the 14th International Conference on Recent Advances in Natural Language Processing, pages 6979, Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria. Fatemah Yousef Almeman, Steven Schockaert, and Luis Espinosa Anke. 2024. WordNet under scrutiny: Dictionary examples in the era of large language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1768317695, Torino, Italia. ELRA and ICCL. AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. Jun Bai, Minghao Tong, Yang Liu, Zixia Jia, and Zilong Zheng. 2025. Understanding and leveraging the expert specialization of context faithfulness in In Proceedings of the mixture-of-experts LLMs. 2025 Conference on Empirical Methods in Natural Language Processing, pages 2192721942, Suzhou, China. Association for Computational Linguistics. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, and 6 others. 2023. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. Https://transformercircuits.pub/2023/monosemanticfeatures/index.html. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Damai Dai, Chengqi Deng, Chenggang Zhao, R.x. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.k. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12801297, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy models of superposition. Transformer Circuits Thread. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. J. L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378382. Gadetsky, Yakubovskiy, and Vetrov. 2018. Conditional generators of words definitions. In ACL 201856th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers), pages 266271. Mario Giulianelli, Iris Luden, Raquel Fernandez, and Andrey Kutuzov. 2023. Interpretable word sense representations via definition generation: The case of semantic change analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31303148, Toronto, Canada. Association for Computational Linguistics. Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah Smith, and Luke Zettlemoyer. 2023. Scaling expert language models with unsupervised domain discovery. arXiv preprint arXiv:2303.14177. Lotte Hogeweg and Agustin Vicente. 2020. On the nature of the lexicon: The status of rich lexical meanings. Journal of Linguistics, 56(4):865891. Han Huang, Tomoyuki Kajiwara, and Yuki Arase. 2021. Definition modelling for appropriate specificity. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 24992509, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Shonosuke Ishiwatari, Hiroaki Hayashi, Naoki Yoshinaga, Graham Neubig, Shoetsu Sato, Masashi Toyoda, and Masaru Kitsuregawa. 2019. Learning to describe unknown phrases with local and global conIn Proceedings of the 2019 Conference of texts. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 34673476, Minneapolis, Minnesota. Association for Computational Linguistics. James Jhirad, Edison Marrese-Taylor, and Yutaka Matsuo. 2023. Evaluating large language models understanding of financial terminology via definition modeling. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 93100. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024. Mixtral of experts. arXiv:2401.04088. Cunliang Kong, Yun Chen, Hengyuan Zhang, Liner Yang, and Erhong Yang. 2022. Multitasking framework for unsupervised simple definition generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 59345943, Dublin, Ireland. Association for Computational Linguistics. Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228231, Prague, Czech Republic. Association for Computational Linguistics. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2025. Nv-embed: Improved techniques for training llms as generalist embedding models. Preprint, arXiv:2405.17428. Leeroo-AI. 2024. Mergoo: library for easily merging multiple llm experts, and efficiently train the https://github.com/Leeroo-AI/ merged llm. mergoo. Accessed: 2024-07-23. Jiaqi Li, Xinyi Dong, Yang Liu, Zhizhuo Yang, Quansen Wang, Xiaobo Wang, Song-Chun Zhu, Zixia Jia, and Zilong Zheng. 2025. ReflectEvo: Improving meta introspection of small LLMs by learning self-reflection. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1694816966, Vienna, Austria. Association for Computational Linguistics. Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah Smith, and Luke Zettlemoyer. 2022. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Yang Liu, Jiaqi Li, and Zilong Zheng. 2025. Rulereasoner: Reinforced rule-based reasoning via domain-aware dynamic sampling. arXiv preprint arXiv:2506.08672. Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations. Jiawei Ma, Po-Yao Huang, Saining Xie, Shang-Wen Li, Luke Zettlemoyer, Shih-Fu Chang, Wen-Tau Yih, and Hu Xu. 2024. Mode: Clip data experts via clustering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2635426363. Mikko I. Malinen and Pasi Fränti. 2014. Balanced kmeans for clustering. In Structural, Syntactic, and Statistical Pattern Recognition, pages 3241, Berlin, Heidelberg. Springer Berlin Heidelberg. Ke Ni and William Yang Wang. 2017. Learning to explain non-standard english words and phrases. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 413417. Thanapon Noraset, Chen Liang, Larry Birnbaum, and Doug Downey. 2017. Definition modeling: Learning to define word embeddings in natural language. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, and 1 others. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Francesco Periti, David Alfter, and Nina Tahmasebi. 2024. Automatically generated definitions and their utility for modeling word meaning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1400814026, Miami, Florida, USA. Association for Computational Linguistics. Savvas Petridis, Ben Wedin, Ann Yuan, James Wexler, and Nithum Thain. 2024. ConstitutionalExperts: Training mixture of principle-based prompts. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 574582, Bangkok, Thailand. Association for Computational Linguistics. Hai Pham, Young Jin Kim, Subhabrata Mukherjee, David P. Woodruff, Barnabas Poczos, and Hany Hassan. 2023. Task-based MoE for multitask multilingual machine translation. In Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL), pages 164172, Singapore. Association for Computational Linguistics. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:48164828. James Pustejovsky and Branimir Boguraev. 1993. Lexical knowledge representation and natural language processing. Artificial Intelligence, 63(1):193223. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 16. IEEE. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across arXiv preprint millions of tokens of context. arXiv:2403.05530. Zhihong Shao, Damai Dai, Daya Guo, Bo Liu, and Zihan Wang. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. ArXiv, abs/2405.04434. Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations. Zhengyan Shi, Adam Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. 2024. Instruction tuning with loss over instructions. arXiv preprint arXiv:2405.14394. Karen Sparck Jones. 1972. statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):1121. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. LearnIn Ading to summarize with human feedback. vances in Neural Information Processing Systems, volume 33, pages 30083021. Curran Associates, Inc. Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Roziere, Jacob Kahn, Shang-Wen Li, Wen tau Yih, Jason Weston, and Xian Li. 2024. Branch-train-mix: Mixing expert LLMs into mixture-of-experts LLM. In First Conference on Language Modeling. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and 1 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 3845. Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, and Yu Qiao. 2023. OpenICL: An open-source framework for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 489498, Toronto, Canada. Association for Computational Linguistics. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Yunting Yin and Steven Skiena. 2023. Word definitions from large language models. arXiv preprint arXiv:2311.06362. Hengyuan Zhang, Dawei Li, Shiping Yang, and Yanran Li. 2022. Fine-grained contrastive learning for definition generation. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 10011012. Linhan Zhang, Qian Chen, Wen Wang, Yuxin Jiang, Bing Li, Wei Wang, and Xin Cao. 2023. Exploiting correlations between contexts and definitions with multiple definition modeling. arXiv preprint arXiv:2305.14717. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563578. Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, and Jianhua Lu. 2025. MergeME: Model merging techniques for homogeneous and heterogeneous MoEs. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 23152328, Albuquerque, New Mexico. Association for Computational Linguistics. Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. 2024. LLaMA-MoE: Building mixture-of-experts from LLaMA with continual pre-training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1591315923, Miami, Florida, USA. Association for Computational Linguistics. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906."
        },
        {
            "title": "A Additional Experiment Details",
            "content": "This is section in the appendix. Introduce dataset components, hyperparameter settings, and other experimental details. Data Processing. Raw 3D-EX (see fig. 6) consists of ten lexicon sources of <t, c, d> triplets, we use the word-level split on each of the sources to train, validate and test our models in this paper. We developed the following steps to undergo the preprocessing procedure for the raw 3D-EX dataset. We discard instances that do not meet any of the following conditions: ① TERM must be of string type, ② DEFINITION must be of string type, ③ EXAMPLE must not be empty, and ④ DATASET_NAME must not be empty. To enhance the models ability to interpret words in various contexts, we split the sample entries with multiple example contexts into separate data instances for each context. This approach increases the number of samples the model sees during training. 3D-EX Constituents Dist. (%) Sci-definition 5.44% Wiktionary 4.65% CHA 24.6% MultiRD 20.95% Wikipedia 31.32% Hei++ 0.02% CODWOE 2.14% WordNet 1.45% Websters Dict. Urban 4.76% 4.68% Figure 6: 3D-EX constituents distribution. In addition, we observed many examples in the existing datasets that share the same term-context pair but with different definitions, which may cause negative effects on model learning if there exist many semantics-divergent examples. To summarize and display the potential impacts, we report the salient statistics about this finding of these datasets shown in the following Table 5."
        },
        {
            "title": "Dataset Split",
            "content": "# All # Div. % Div. / All 2,723 368 333 19.61 21.00 18."
        },
        {
            "title": "Urban",
            "content": "3D-EX"
        },
        {
            "title": "Strain\nSvalid\nStest",
            "content": "13,883 1,752 1,775 82,479 10,285 10,306 Strain 887,455 Svalid 44,003 Stest 57,232 Strain 411,382 Svalid 57,883 Stest 38,371 34 2 0 186 16 1,424 152 122 Strain 1,309,312 35,632 Svalid 513,789 12,551 Stest 7,599 450,078 0.04 0.02 0.00 0.02 0.04 0.02 0.35 0.26 0.32 2.72 2.44 1. Table 5: Divergent examples statistics of each dataset. # All: number of all examples; # Div.: number of all divergent examples; % Div. / All: ratio of divergent examples in all examples. Clustering Setup. Compared with Gururangan et al. (2023), we consider to mine the intrinsit semantic meaning of term associated with their context, instead of using lexical statistics clustering method, like TF-IDF. We argue that the method building on dense semantic clustering would help upcycling models to learn specialized sense interpretation-oriented experts, towards robust system for definition modeling. We run kmeans++ clustering of the Elkan variation method with 1, 000 max iteration, 1e8 tolerance of convergence, and fixed seed of 42. Considering the computation and memory bounds, we first use 4 as the number of clusters to form and the number of centroids to generate. We further ablate this factor in the section 4.3. Training Details. LM-LEXICON was trained for 3 epochs with global batch size of 8,192 tokens (gradient accumulation 1, batch size per device 8, max sequence length 128) on 8 H100-PCIe80GB GPUs and learning rate of 1e-6, minimum learning rate of 3e-7 with cosine annealing scheduler, as well as the warm-up steps with 6% ratio of the total training steps. We used global dropout of 0.2 (Srivastava et al., 2014) and weight decay of 0.1 with AdamW optimizor (Loshchilov and Hutter, 2018), and performed early stopping to obtain the best model by the highest validation bleu. Moreover, We run three times for each training setup to report the mean results and their standard deviation of metrics, with seed si {21, 42, 84}, respectively. We use Hugging Face Transformers (Wolf et al., 2020) and Pytorch (Paszke et al., 2019) to develop the training pipeline. We run the branch training on each cluster of data points obtained from the clustering results. As depicted in tab. 12, We set up the following hyperparameters to train LM-LEXICON and vanilla finetuned LLAMA-3-8B models in this paper. We used the standard negative log-likelihood (NLL) loss to train LM-LEXICON. Contrary to Shi et al. (2024), to avoid the loss of the input sequence tokens overshadowing the actual output token loss, the loss is only computed over the result tokens (Eq. 1), limiting the potential to overfit to the input prompt and context. This loss calculation method resulted in faster training and robuster results overall. Given definition generation problem p(c, t) and its golden reference d, we define outcome reward model as the following: ORM (P R) assigns single value to to indicate whether predicted ˆd is correct. Given specific dataset D, we follow Cobbe et al. (2021) to use negative log-likelihood loss (Eq. 3) to frame the reward modeling as binary classification objective. LORM = log σ (rϕ(x, yw) rϕ(x, yl)) (3) Where yw is the preferred generation (i.e., chosen response) and yl is the alternate generation (i.e., rejected response) conditioned on the input := p(c, t). To train ORM built on training set, we leverage the golden reference as the preferred definition yw and one of the model generations as the alternate definition yl to express preferences for each x, denoted as yw yl x, where yw and yl denotes the preferred and dispreferred completion, respectively. σ is the sigmoid function and rϕ(, ) represents the parameterized reward function for the concatenated input and generation y. To enhance computing efficiency, we employ the ratio of 1 : 32 to conduct repeated sampling and rerank the generations by their log-likelihood (aka. confidence) to acquire the top-eight items as candidate set of alternate generations for each input x. Inference Setup. As shown in Table 2, for each setting in Zero-shot, BoN-Oracle, and BoNORM, we orchestrate three separate runs for each setting, using the same decoding parameters but with different random seeds to ensure robustness and consistency in the results. Specifically, for the models LM-LEXICON-DENSE and LM-LEXICONMOE, specifically, we use the temperature of 0.6, top-k of 50, top-p of 0.9, and repetition penalty of 1.05, ensuring uniformity across all evaluations. For all benchmarks included in our test, as the number of samples increases, the coverage metric corresponds to the use of an oracle verifier. This verifier checks which fraction of DM problems in the test set can be approximated using any of the samples that were generated to be as similar as possible to the ground truth. The selection of the most similar generation is achieved through an iterative comparison with the golden definition, ensuring robust matching process. In the case of the oracle verification process by the oracle verifier, we validate whether any output chosen prediction is the most similar by comparing it with golden references of the sample in the test set. In contrast, for the verification process of ORM verifier, the selection of the most similar generation is then performed solely by the ORM verifier itself, without relying on external feedback, ground-truth comparison, or oracle input. Miscellaneous. We developed our MoE language modeling codebase based on Leeroo-AI (2024) and implemented several routing policies and proposed MoE architectures. Aiming at more efficent evlauation, we follow (Huang et al., 2021) and refactor their implementation with concurrent metrics computation to boost the inference procedure in large models, please see the details in our released code."
        },
        {
            "title": "B Carbon Footprint",
            "content": "The cost of fine-tuning LLM is lower than that of pre-training them. Nevertheless, we think it is critical to quantify and record the environmental consequences of our research. Table 6 lists the materials required for single run, which is conducted using our own infrastructure. We calculate the carbon footprint estimation using carbon intensity of 0.141 kg/kWh and 700W consumption per GPU8."
        },
        {
            "title": "Model",
            "content": "Hardware FLOPs Time (h) CO2eq (kg) LM-LEXICON-DENSE LM-LEXICON-MOE 8H100 8H100 4.2e18 5.4e18 36.4 32.8 11.4 14. Table 6: Details about the training required resources."
        },
        {
            "title": "C Additional Evaluation Results",
            "content": "C.1 Data Clustering Results"
        },
        {
            "title": "Cluster Ci",
            "content": "Distanceintra-cluster C0 (Adjective) C1 (Scientific) C2 (Proper Noun) C3 (Person Name)"
        },
        {
            "title": "Average",
            "content": "0.176 0.168 0.173 0.185 0.175 Table 7: Intra-cluster Distances (i.e., the cluster cohesion) . Cluster (Ci, Cj) Distanceinter-cluster C0, C1 C0, C2 C0, C3 C1, C2 C1, C3 C2, C"
        },
        {
            "title": "Average",
            "content": "0.694 0.713 0.765 0.681 0.707 0.720 0.713 Table 8: Inter-cluster Distances (i.e., the cluster separation): C0 denotes the domain of Adjective, C1 denotes the domain of Scientific, C2 denotes the domain of Proper Noun, and C3 denotes the domain of Person Name. We show the clustering results including cluster cohesion and cluster separation in the following Table 7 and 8, respectively. C.2 In-Context Learning Evaluation We show the scaling in-context learning experimental results as shown in Figure. 7. C.3 Generation Examples of LM-LEXICON As depicted in Figure 8, 9, 10, and 11, we provide cherry-picked example for each domain cluster as shown in Figure 3 in definition modeling. 8Statistics: https://app.electricitymaps.com/map. Figure 7: Scaling the in-context learning results of frontier causal LMs on WordNet with k-shot demonstrations, where scales logarithmically from 0 to 128. Prior SoTA denotes the Rerank-T5 proposed by Huang et al. (2021). Cluster-1 Example: [Term] Combtooth Blenny Cluster-3 Example: [Term] Michael Maclennan [Query] the crested blenny is species of Combtooth [Query] Godivas is Canadian television comedyBlenny found around New South Wales, Australia, ... drama series created by Michael Maclennan with Julia What is the definition of Combtooth Blenny? Keatley of Keatley Entertainment ... What is the defini- [Source] Wikipedia tion of Michael Maclennan? [Reference] Combtooth Blenny: perciform marine fish [Source] Wikipedia of the family blenniidae. [Reference] Michael Maclennan: Canadian playwright, screenwriter, and producer of television shows. Figure 8: Example of C1 (proper noun) from 3D-EX. Figure 10: Example of C3 (person name) from 3D-EX. Cluster-2 Example: [Term] brave Cluster-4 Example: [Term] Lymphedema-distichiasis Syndrome [Query] two patients with Lymphedema-distichiasis [Query] familiarity with danger makes brave man Syndrome illustrate that both Milroys ... What is the braver but less daring - herman melville ... What is the definition of Lymphedema-distichiasis Syndrome? definition of brave? [Source] WordNet [Source] Sci-definition [Reference] Lymphedema-distichiasis Syndrome: [Reference] brave: possessing or displaying courage; lymphedema distichiasis syndrome is condition that able to deal with danger or fear without flinching. affects the normal function of the lymphatic system. Figure 9: Example of C2 (adjective) from 3D-EX. Figure 11: Example of C4 (scentific) from 3D-EX."
        },
        {
            "title": "E Comparison of Different Definitions",
            "content": "To assess the agreement among the annotators, we employed Fleisss Kappa (Fleiss, 1971), which is statistical measurement to assess the reliability of the agreement between multiple raters. Fleisss Kappa account for the possibility of agreement occurring by chance. It is calculated using the following formula: κ = Po Pe 1 Pe where: Po is the observed agreement among the raters, and Pe is the expected agreement by chance. Table 9 presents Fleisss Kappa coefficients for human evaluation agreement on each criterion and model. The following is representative case of the generated definitions from five models including three proprietary models, LM-Lexicon-Dense and LMLexicon-MoE: Word: \"coon Context: \"Ill be gone coon when the battle starts Reference: \"an eccentric or undignified rustic (from WordNet) In the demonstration in Table 10, definition generated by LM-Lexicon-MoE (our method) is most closely aligned with the reference definitions core meaning, which captures the derogatory connotation and reflects the \"undignified\" aspect. In contrast, definitions produced by the other models fail to capture fundamental aspects of the reference definition, for instance, Claude-3-Opus and GPT-4Turbo completely deviate from the basic meaning of the target word in the context. In general, there are some notable patterns: Closed-source models (e.g., GPT-4 and Claude) generate overly verbose definitions, including redundant contextual information and multiple interpretations. These models tend to overemphasize derivative or secondary meanings, resulting in broad, general definitions that deviate from the core semantic meaning. Models trained on general corpora often struggle with uncommon word usages due to limited training examples, leading to inaccurate definitions or contextual misinterpretation. This comparison demonstrates the superiority of LM-Lexicon-MoEs targeted, controlled approach over general-purpose language models for definition modeling."
        },
        {
            "title": "F Code for",
            "content": "LM-LEXICON We use the Alg. 2 and 3 provided below to train LM-LEXICON-MOE on the 3D-EX and the other four legacy datasets used in this paper. We exposed this Pytorch-style code as an implementation to extend our method to any potential domain. Additionally, to illustrate the differences and novelty of our method, we provide comparison with the relative methods as shown in Table 11."
        },
        {
            "title": "Criteria",
            "content": "LM-Lexicon-MoE LM-Lexicon-Dense Claude-3-Opus Gemini-1.5-Pro GPT-4-Turbo Accuracy Clarity Conciseness Context Appropriateness Grammar & Fluency 0.85 0.82 0.80 0.82 0.78 0.78 0.75 0.74 0.76 0. 0.80 0.78 0.77 0.79 0.75 0.79 0.76 0.75 0.78 0.74 0.77 0.74 0.73 0.76 0.73 Table 9: Fleisss Kappa coefficient statistics for the agreement of human evaluation on each criterion and model."
        },
        {
            "title": "Model",
            "content": "Claude-3-Opus Gemini-1.5-Pro GPT-4-Turbo"
        },
        {
            "title": "Generated Definition",
            "content": "The phrase gone coon means someone who is certainly defeated or about to die. The word coon is deeply offensive racial slur targeting people of African descent. It has long and painful history rooted in racist caricatures and stereotypes. While it once appeared casually in entertainment and everyday language, it is now considered completely unacceptable to use. Coon is colloquial abbreviation for raccoon, nocturnal mammal native to North America. Coon was sometimes used to refer to person who is in deep trouble or doomed, often in the phrase gone coon, meaning someone who is as good as dead or has no chance of survival. LM-Lexicon-Dense (Ours)"
        },
        {
            "title": "A person who is afraid",
            "content": "LM-Lexicon-MoE (Ours)"
        },
        {
            "title": "A person who is deemed to be despicable or contemptible",
            "content": "Table 10: Comparison of generated definition by models. MOE (2017) (Vanilla) BTM (2022) (Merge) BTX (2024) (Linear router) LM-LEXICON (Ours) Dense experts are trained independently (upcycling) Experts are specialized in different domains Experts are chosen by learned router per input token Adaptive router via domain-wise routing Semantic experts adapted to diverse domains Table 11: comprehensive comparison of the most relative sparse mixture-of-experts frameworks in recent years, including MoE (Vanilla), BTM (Merge), BTX (Linear Router), and LM-LEXICON. Our method demonstrates advancements in semantic-centric specialized expert and adaptability across domains. Algorithm 2 Pytorch code for semantic experts merger. def merge_semantic_experts(experts, router_layers): \"\"\" Mergeexpertmodelsintoaunifiedmodel. Args: -experts(ModuleList):Expertstomerge. -router_layers(ModuleList):Routerlayers. Returns: -state_dict(Dict[str,Tensor]):Mergedmodelweights. \"\"\" state_dict = dict() expert_nums = len(experts) count_total_router_layers = 0 for idx, expert in enumerate(experts): # load each expert model model_id = expert[\"model_id\"] model = load_base_model(model_id) if hasattr(model, \"_tied_weights_keys\"): tied_weights_keys.extend(model._tied_weights_keys) count_router_layers = 0 count_averaged_layers = 0 # iterate over all the layers of the model for layer_name, param in model.state_dict().items(): is_merge_layer = True for router_layer in router_layers: if is_layer_suitable_for_router(router_layer, layer_name): is_merge_layer = False wb = layer_name.split(\".\")[-1] new_layer_name = layer_name.split(f\"{wb}\")[0] new_layer_name = f\"{new_layer_name}experts.{ix}.{wb}\" assert new_layer_name not in state_dict state_dict[new_layer_name] = param count_total_router_layers += 1 count_router_layers += 1 if is_merge_layer: # average the rest of layers by mean of weights prev_weight = state_dict.get(layer_name) if prev_weight is None: prev_weight = torch.tensor(0) else: if not prev_weight.shape == param.shape: # adjust the shape of weight prev_weight, param = shape_adjuster( prev_weight, param, idx ) try: # sometimes data is empty / non weights state_dict[layer_name] = prev_weight + (param / expert_nums) except Exception as _: print(layer_name, param) state_dict[layer_name] = param count_averaged_layers += 1 return state_dict Algorithm 3 Pytorch code for modeling LM-LEXICON-MOE Layer class SemanticMoeLayer(nn.Module): def __init__( self, in_features: int, out_features: int, bias: bool, num_experts: int, num_experts_per_tok: int = 2, routing_policy: str, ): \"\"\"SemanticMixture-of-ExpertsLayer. Args: -in_features(int):InputFeatures -out_features(int):OutputFeatures -bias(bool):Usebiasornot. -num_experts(int):TotalnumbersofexpertsthatRouterLayerwouldhandle -num_experts_per_tok(int):Numberofactiveexpertspertoken. -routing_policy(str):RoutingPolicy. \"\"\" super().__init__() self.routing_policy = routing_policy if routing_policy == \"token-level\": # top-k token-level routing self.gate = nn.Linear(in_features, num_experts, bias=False) self.experts = nn.ModuleList( [nn.Linear(in_features, out_features, bias) for _ in range(num_experts)] ) self.num_experts_per_tok = num_experts_per_tok self.in_features = in_features self.out_features = out_features elif routing_policy in [\"soft-sequence-level\", \"hard-sequence-level\"]: # soft/hard sequence-level routing self.gate = nn.Linear(in_features, num_experts, bias=False) self.num_experts = num_experts self.experts = nn.ModuleList( [nn.Linear(in_features, out_features) for _ in range(num_experts)] ) elif routing_policy == \"domain-level\": # domain-level routing self.gate = nn.Linear(in_features, num_experts, bias=False) self.num_experts = num_experts self.experts = nn.ModuleList( [nn.Linear(in_features, out_features) for _ in range(num_experts)] ) def forward(self, inputs: torch.Tensor, domain_labels: torch.Tensor): if self.routing_policy == \"token-level\": gate_logits = self.gate(inputs) weights, selected_experts = torch.topk( gate_logits, self.num_experts_per_tok ) weights = F.softmax(weights, dim=2, dtype=torch.float).to(inputs.dtype) results = torch.zeros( (inputs.shape[0], inputs.shape[1], self.out_features), device=inputs.device, dtype=inputs.dtype, ) # continue this table as below ... # continue the above table ... weights = weights.to(inputs.device) for ix, expert in enumerate(self.experts): batch_idx, tok_idx, expert_idx = torch.where(selected_experts == ix) results[batch_idx, tok_idx] += expert( inputs[batch_idx, tok_idx] ) * weights[batch_idx, tok_idx, expert_idx].unsqueeze(-1) elif self.routing_policy == \"soft-sequence-level\": # soft sequence-level routing gate_logits = self.gate(inputs) gate_logits_mean = gate_logits.mean(dim=1) weights = F.softmax(gate_logits_mean, dim=-1) results = torch.zeros( (inputs.shape[0], inputs.shape[1], self.out_features), device=inputs.device, dtype=inputs.dtype, ) for ix, expert in enumerate(self.experts): results += expert(inputs) * weights[:, ix].unsqueeze(-1) elif self.routing_policy == \"hard-sequence-level\": # hard sequence-level routing (only one selected expert is responsible for the entire sequence) gate_logits = self.gate(inputs) gate_logits_mean = gate_logits.mean(dim=1) _, selected_experts = torch.topk(gate_logits_mean, 1) results = torch.zeros( (inputs.shape[0], inputs.shape[1], self.out_features), device=inputs.device, dtype=inputs.dtype, ) for ix, expert in enumerate(self.experts): results += expert(inputs) * (selected_experts == ix).float().unsqueeze( -1 ) elif self.routing_policy == \"domain-level\": # domain-level routing (only one selected expert is responsible for the entire sequence) gate_logits = self.gate(inputs) results = torch.zeros( (inputs.shape[0], inputs.shape[1], self.out_features), device=inputs.device, dtype=inputs.dtype, ) for ix, expert in enumerate(self.experts): results += expert(inputs) * (domain_labels == ix).float().unsqueeze(-1) return results Computing Infrastructure 8 H100-80GB GPU (PCIe)"
        },
        {
            "title": "Base model",
            "content": "LM-Lexicon-Dense (Llama-3-8B) DS ZERO-3 3 524,288 tokens 128 5e 6 AdamW 0.9, 0.95 Training strategy Epochs Global batch size Max sequence length Max learning rate Optimizer Adam beta weights Learning rate schedule Cosine decay to 0 Weight decay Warm-up ratio Gradient clipping Global dropout Random seeds 0.01 10% 1.0 0.1 {21, 42, 84}"
        },
        {
            "title": "Base model",
            "content": "LM-Lexicon-MoE (4 Llama-3-8B) NAIVE PP 1 131,072 tokens 128 1e 6 AdamW 0.9, 0.95 Training strategy Epochs Global batch size Max sequence length Max learning rate Optimizer Adam beta weights Learning rate schedule Cosine decay to 0 Weight decay Warm-up ratio Gradient clipping Global dropout Random seeds 0.01 10% 1.0 0.1 {21, 42, 84} Table 12: Hyper-parameters of LM-LEXICON-DENSE and LM-LEXICON-MOE training. DS ZERO-3 (left-hand table) denotes stage-3 ZeRO parallelism implemented by DeepSpeed (Rajbhandari et al., 2020). NAIVE PP (rightHugging Face Transformers (Wolf et al., 2020). hand table) denotes naive pipeline parallelism implemented by Figure 12: Human evaluation guideline."
        }
    ],
    "affiliations": [
        "BIGAI",
        "Baidu Inc.",
        "Peking University"
    ]
}