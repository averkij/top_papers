{
    "paper_title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks",
    "authors": [
        "Yilun Zhao",
        "Kaiyan Zhang",
        "Tiansheng Hu",
        "Sihong Wu",
        "Ronan Le Bras",
        "Taira Anderson",
        "Jonathan Bragg",
        "Joseph Chee Chang",
        "Jesse Dodge",
        "Matt Latzke",
        "Yixin Liu",
        "Charles McGrady",
        "Xiangru Tang",
        "Zihang Wang",
        "Chen Zhao",
        "Hannaneh Hajishirzi",
        "Doug Downey",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 1 0 0 1 0 . 7 0 5 2 : r SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks Yilun ZhaoY Kaiyan ZhangY Tiansheng HuN Sihong WuY Ronan Le BrasA Taira Anderson Jonathan Bragg Joseph Chee Chang Jesse Dodge Matt Latzke Yixin Liu Charles McGrady Xiangru Tang Zihang Wang Chen Zhao Hannaneh HajishirziA Doug DowneyA Arman CohanY Yale University New York University AAllen Institute for AI"
        },
        {
            "title": "Abstract",
            "content": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmarks challenges and emphasize the need for more reliable automated evaluation methods. Platform: Data: Code: sciarena.allen.ai huggingface.co/datasets/yale-nlp/SciArena github.com/yale-nlp/SciArena"
        },
        {
            "title": "Introduction",
            "content": "Scientific literature understanding and synthesis play pivotal role in uncovering research gaps, guiding methodological innovation, informing practical application, and enabling scientific discovery [59, 70, 75]. However, the exponential growth in scholarly publications poses significant challenges for researchers attempting to maintain comprehensive awareness of developments within their fields. To assist with this challenge, foundation models are increasingly leveraged to help researchers in the discovery, synthesis, and interpretation of scholarly content [50, 16, 61, 5, 78, 69]. At the same time, evaluating the capabilities and limitations of foundation models in open-ended scientific literature tasks remains challenging. Recent work in evaluation of open-ended instructionfollowing tasks often relies on LLM-based evaluators or judges [10, 77]. Despite the scalability of Core Contributors. The remaining authors are listed in alphabetical order by last name. Figure 1: SciArena focuses on evaluating foundation models on scientific literature tasks. It consists of three main components: (1) platform that collects human researcher preference votes between foundation models; (2) leaderboard that ranks models using an Elo rating system based on these votes; and (3) the SciArena-Eval benchmark for assessing model-based evaluation systems. LLM-based evaluation methods for evaluating model responses, such evaluators can fail to achieve high alignment with expert annotators across diverse datasets [39, 40]. This is especially the case in science where nuanced, domain-specialized, and knowledge-intensive requirements can be overlooked by LLM-based evaluations [43, 6, 65]. On the other hand, obtaining human expert judgments is widely acknowledged as both time-consuming and expensive, especially in knowledge-intensive domains [6, 60]. Consequently, most expert-annotated benchmarks in science are static, remain limited in scale and quickly become outdated, especially in fast-evolving scientific fields-of-study. To address these challenges, we introduce SciArena, collaborative and open platform inspired by ChatBotArena [11] that harnesses the collective expertise of the scientific community. SciArena serves as an interactive evaluation platform for scientific literature tasks where users can submit questions related to up-to-date research, view side-by-side literature-grounded and long-form responses generated by foundation models, and vote for their preferred output. Unlike general-purpose tasks targeted by existing arena platforms, scientific literature tasks demand high degree of domain expertise and precise literature retrieval. To meet these specialized requirements, we implement multi-stage retrieval pipeline adapted from Ai2s Scholar QA system [61], which includes query decomposition, passage retrieval, and re-ranking. The retrieved paper contexts, combined with the users question, are provided to two foundation models, each generating long-form literature-based responses with citations. Users then evaluate these outputs and vote for the one that best satisfies their information need. Over the first three months of SciArena internal operation, we have collected over 13,000 votes from 102 trusted human researchers across diverse scientific fields. We apply rigorous quality control and conduct detailed analyses to ensure the reliability and integrity of the human preference data. Using this data, we construct the SciArena leaderboard, identifying the state-of-the-art models as o3 and Claude-4-series models. We further perform in-depth analyses, including breakdowns of model performance across scientific disciplines and question categories, head-to-head win rates between models, and qualitative assessments of model failure cases, highlighting key insights for advancing foundation models in scientific literature tasks. Despite the limitations of automated evaluation methods, systematically studying how well modelbased evaluators can approximate human experts judgments on long-form, literature-based responses remains underexplored. Understanding this alignment gap could reveal both the current boundaries of automated evaluations in science and inspire targeted improvements in evaluation methodologies. To this end, we introduce the SciArena-Eval meta-evaluation benchmark, constructed using user voting data, to facilitate the development of model-based automated evaluation systems. SciArena-Eval assesses how closely model-based evaluators align with human preferences, providing insight into their evaluation capabilities. Our findings reveal that even the best-performing evaluation system, o3 with pairwise comparison, achieves only 65.1% accuracy compared with human preference. This underscores the need for more robust evaluation approaches. Figure 1 presents an overview of this study. We summarize our main contributions as follows: 2 We introduce SciArena, the first open evaluation platform for ranking foundation models in scientific literature tasks, based on preferences from human researchers. We collect over 13,000 votes from researchers across different scientific domains. Our quality assessment and case studies demonstrate the high quality of collected data and the strong reliability of SciArena. We publicly release this human preference data to support future research. We develop the SciArena leaderboard using collected human preference data, providing researchers and developers with comprehensive understanding of state-of-the-art models in scientific literature tasks. Our in-depth analyses highlight key insights for advancing foundation models in this domain. We construct SciArena-Eval, the first benchmark designed to assess model-based evaluators in judging citation-attributed responses to user questions. Our experiments show that existing methods perform poorly on SciArena-Eval, highlighting the need for more robust evaluation approaches."
        },
        {
            "title": "2 Related Work",
            "content": "Foundation Models for Scientific Literature Tasks. Understanding and synthesizing scientific literature is cornerstone of research progress and innovation. Recent advances have introduced foundation model-based systems powered by retrieval-augmented generation (RAG) frameworks [6, 61, 14, 57, 22, 53, 73, 50, 16] that leverage sophisticated retrieval and generation mechanisms to support literature reviews and accelerate knowledge synthesis. However, many existing systems (e.g., OpenAIs Deep Research [50]) that are accessible without local deployment are primarily commercial, often prohibitively expensive for widespread academic use, and typically lack transparency in their underlying literature retrieval pipelines. SciArena is an open platform for automating scientific literature tasks based on diverse collection of frontier foundation models. While its primary goal is to serve as an evaluation platform, our user study in Section 3.2 shows that SciArena can also serve as competitive alternative for assisting researchers with everyday literature tasks. Benchmarks for Scientific Literature Tasks. Extensive research has been dedicated to creating benchmarks for evaluating how well language models can address scientific literature understanding tasks. Historically, these benchmarks have often concentrated on specific, narrowly-defined tasks. Examples include evaluating performance on short-form question answering from document context [9, 67, 15, 31, 64, 32], generating summaries from multiple documents [41, 35, 29], as well as tasks like information extraction [37], hypothesis generation [36], and retrieval [13, 62]. Benchmarks such as SCHOLARQABENCH [6] were proposed to assess models on more complex, open-domain literature review tasks [1], offering broader and more realistic evaluation setting. While these benchmarks have provided valuable insights, they often fail to fully reflect the open-ended nature, diversity, and complexity of real-world research needs. which are difficult to anticipate and encode into static, curated benchmarks. Moreover, existing benchmarks primarily concentrate on small number of well-resourced domains (e.g., computer science and biomedicine), limiting the generalizability of evaluation and neglecting the needs of researchers in underrepresented fields. SciArena addresses these concerns by enabling real-time, open-ended, researcher-driven evaluation across broad range of scientific domains, capturing diverse information needs that static benchmarks overlook. Foundation Model Evaluation via Human Preferences. Traditional evaluation benchmarks often rely on automated metrics, which can be limiting in efficacy or capturing nuanced human judgments [7, 38, 74, 40]. Human evaluation is widely regarded as the gold standard for reliably evaluating foundation models [56, 74]. Thus, human preference-based evaluations have gained traction, particularly through the development of crowdsourcing platforms for pairwise model comparisons. One prominent example is Chatbot Arena [11], which set precedent for collecting large-scale human votings to rank foundation models. Building on this approach, similar arena-based evaluation platforms have been introduced for multimodal foundation models [12, 42], generative models [28], text-to-speech models [49], search-augmented LLMs [46], and other complex tasks involving foundation models [66, 68, 72]. However, the scientific literature tasks are under-explored. Different from general-purpose tasks evaluated in existing arena platforms, scientific domain demand high degree of domain expertise and precise literature retrieval for literature grounded generation. To address this gap, we carefully design SciArena platform and implement rigorous data quality controls to mitigate biases and address concerns raised by recent critiques of arena-style evaluations [63]. Furthermore, to advance the development of more reliable and human-aligned automated evaluation methods, we release SciArena-Eval, the first meta-evaluation benchmark for scientific literature tasks. 3 Figure 2: An overview of the SciArena interface pipeline."
        },
        {
            "title": "3 SciArena Platform",
            "content": "Our work introduces SciArena, an open platform for evaluating foundation models in their ability to understand, analyze, and synthesize scientific literature and generate citation-attributed answers to real-world research questions. In the following subsections, we describe the design of the SciArena platform in detail, including its response generation pipeline, Elo-based ranking system, and our strategies for addressing bias and the inherent challenges of human preference evaluation."
        },
        {
            "title": "3.1 Platform Design",
            "content": "Figure 2 illustrates the overview of the SciArena interface pipeline. Upon receiving user-submitted question, the SciArena interface first conducts content moderation check to ensure the query is not potentially harmful.2 If the query passes moderation, the interface uses the literature retrieval module from ScholarQA [61], state-of-the-art agentic scientific literature synthesis system, to retrieve set of relevant scientific paper contexts. These contexts, together with the original question, are then provided to two randomly selected foundation models drawn from pool of strong open-source and proprietary frontier models. The two models independently generate responses grounded in the retrieved literature. Finally, the user is prompted to vote for their preferred response and may optionally provide textual justification for their choice. We next detail the implementations of literature retrieval and model response generation. Scientific Literature Retrieval. Unlike general-purpose tasks typically evaluated on other arena platforms, answering questions in the scientific domain requires responses grounded in scholarly literature. As such, the effectiveness of the literature retrieval pipeline is critical, as producing high-quality and informative responses depends on retrieving documents that are both topically relevant and of high scholarly credibility, which in turn sustains the usefulness of model outputs and encourages continued user participation with the SciArena platform. To support this goal, we apply the multi-stage retrieval system from ScholarQA [61], which integrates with the Semantic Scholar API and accesses large, continuously updated scholarly corpus [2, 30]: over 100 million paper abstracts (abstract-level endpoint) and 11.7 million full-text papers (snippet-level endpoint). Specifically, given user query, strong LLM (GPT-4o prior to April 20; GPT-4.1 afterward) generates queries tailored to each endpoint and extracts any user-defined metadata filters (e.g., publication years, authors, venues). Following similar setup to ScholarQA, the system retrieves up to 40 paper snippets from full-text and 20 abstracts, which are then reranked using state-of-the-art re-ranker [58]. The top-30 results are selected as contextual input for the two randomly sampled foundation models. Model Response Generation and Postprocessing. Given the user question and 30 relevant paper contexts obtained from the literature retrieval module, the foundation model is prompted to generate citation-attributed response that well addressed the user needs. This step typically demands multidocument reasoning and synthesizing information across all the retrieved sources. The model also should decide which sources of information should include and cite in the final generated response. Recent studies have shown that stylistic elements in the responsesuch as markdown formatting (e.g., bold text, bullet points) or the use of emojiscan inadvertently influence user preferences during evaluation [11, 20, 12]. While such formats can be favored in general-purpose applications, they are uncommon in scientific literature settings. In practice, we observe that some evaluated foundation 2Content moderation is performed using OpenAIs omni-moderation-latest model. 4 models tend to adopt these styles. To mitigate potential biases introduced by response formatting, we explicitly prompt the model to generate plain-text responses without markdown. Additionally, we use strong LLM (GPT-4o prior to April 20; GPT-4.1 afterward) in postprocessing step to ensure formatting consistency. The prompts used for model response generation and response postprocessing are provided in Appendix A.3. Evaluated Foundation Models. As of the evaluation cutoff date, June 30, 2025, SciArena hosts total of 23 frontier foundation models for evaluation. This set includes 13 preparatory models and 10 open-source models chosen for their strong representation of current state-of-the-art capabilities. The collection features reasoning models such as o4-mini [52], DeepSeek-R1 [19], and QwQ-32B [55], alongside hybrid reasoning models like Claude-4-series [4] and Qwen3-series [54] models. For reasoning models, we remove the thought process content preceding the thinking tokens during postprocessing; for hybrid reasoning models, we enable their thinking mode. We detail the model configurations in Appendix A.4. We will be continuously adding new models to the SciArena platform to ensure ongoing evaluation of the latest advancements."
        },
        {
            "title": "3.2 User Study on SciArena vs. Commercial Platforms for Scientific Literature Tasks",
            "content": "An arena platform relies on user contributions, and therefore must deliver clear value and meet fundamental usability standards. To investigate this, we conducted user study involving four researchers across different scientific domains, comparing SciArena with leading commercial platforms for performing routine scientific literature tasks in the context of their real-world research workflows. Specifically, we evaluated two major categories of alternatives: (1) Chatbot Platforms with search capabilities, including ChatGPT with Search and Perplexity AI; and (2) Agent-based Platforms designed for complex, multi-step research tasks, including OpenAI Deep Research and Gemini Deep Research. All platforms were tested using their latest publicly available versions as of May 5, 2025. Each participant spent total of 60 minutes using each platform and provided feedback. Participant biographies and detailed responses are available in Appendix A.1. In summary, compared to ChatGPT and Perplexity with search capabilities, participants found that the cited papers in SciArena are more relevant, while other platforms occasionally cite less reliable sources such as blogs or media articles, which participants considered less trustworthy. When compared to the two Deep Research platforms, SciArena is more efficient, with shorter wait times. It achieves comparable performance on welldefined questions; while for exploratory queries, the longer reports generated by Deep Research platforms sometimes contains more useful information. The study results indicate that SciArena is an effective standalone tool for high-quality literature analysis, with all participants expressing interest in continued use of SciArena."
        },
        {
            "title": "3.3 Leaderboard Ranking with Elo Rating",
            "content": "Following Chatbot Arena, we adopt the BradleyTerry (BT) model [8] for Elo rating estimation [23]. BradleyTerry (BT) Model. Unlike the standard online Elo rating system (described in Appendix A.2), which can be sensitive to the order of comparisons, the BT model provides more robust way to estimate model strengths by fitting logistic regression to the outcomes of all pairwise comparisons. Let denote the total number of pairwise comparisons and the number of models. For each comparison [n], we define Xi RM as feature vector where Xi,m = 1 if model appears first, Xi,m = 1 if it appears second, and 0 otherwise; and Yi {0, 1} as the outcome, where 1 indicates that the first model wins. It estimates strength vector β RM by minimizing the average cross-entropy loss: ˆβ = arg min βRM 1 (cid:88) i= CE(σ(X β), Yi), (1) where σ is the sigmoid function and CE denotes the cross-entropy loss. The resulting coefficients ˆβ serve as the estimated Elo ratings, which determine the final leaderboard rankings of the models. Since this modeling does not consider ties, in practice, we duplicate all the votes and force half of the tie votes to be counted as first model winning (Yi = 1) and the other half as second model winning (Yi = 0). To further investigate the variance of the estimated Elo rating, we apply bootstrapping with 100 resamples to compute confidence intervals for each rating. 5 Voting Total Votes: 13,204 : : Tie : Both Bad = 6041 : 6242 : 734 : 187 Question Category with Examples Conceptual Explanation (35.17%): How do locally described freeform surfaces impact optical system design flexibility? Challenges & Limitations (23.42%): What are the current challenges in characterizing large RNA molecules for therapeutic development? State-of-the-Art Assessment (23.90%): What new trends emerge in digital philology across various textual traditions? Methodology Inquiry (9.25%): How can the integration of different renewable energy sources be achieved to ensure stable power supply? Paper Finding (4.47%): Please find papers on solving the project scheduling problems using reinforcement learning. Others (3.79%) Figure 3: Statistics of the initial human preference data collected through SciArena, including voting information and distribution across question categories and scientific disciplines. Controlling Stylistic Biases in Evaluation. Recent studies have highlighted potential confounding factors in model evaluation, such as response length and stylistic formatting [11, 40, 21]. To assess the influence of these factors in SciArena evaluations, we follow prior work that extends the BT model to incorporate style features [33] and present our findings in Section 5.2. Specifically, given set of style features (e.g., model response length), we augment the BT model with style vector Z, where each Zi RS represents the S-dimensional style feature vector for instance i. The extended model has the style coefficients γ RS: ˆβ, ˆγ = arg min βRM ,γRS 1 (cid:88) i=1 CE(σ(X β + γ), Yi) The resulting ˆγ quantifies the influence of style features on user preferences."
        },
        {
            "title": "4 SciArena Data",
            "content": "This section describes our data collection details and quality analysis of the collected data."
        },
        {
            "title": "4.1 SciArena Human Preference Data Collection",
            "content": "Existing arena-based evaluation platforms [11, 28, 42, 66, 68, 72] typically gather human preference data from broad, non-expert user base. While effective for general-purpose tasks, these methods are inadequate for our focus on scientific literature tasks, which require domain expertise. To address this, we carefully design our data collection process to ensure the high quality of collected human preference data thus maintaining reliable SciArena leaderboard. Our initial data collection phase (beta release of SciArena) involves 102 researchers working across four core disciplines: Natural Science, Healthcare, Humanities & Social Sciences, and Engineering. Anonymized profiles of the participating researchers are presented in Appendix B.1. Each expert researcher involved has authored at least two peer-reviewed publications and has prior experience with AI-assisted literature tools. Before beginning the annotation process, all annotators complete comprehensive, one-hour training session conducted by member of our team to ensure consistency and accuracy across evaluation. For the leaderboard reported in this paper, we include only votes collected during the initial data collection phase. While SciArena is publicly accessible, only votes from users who (1) pass anomaly detection checks and (2) consent to our terms of use, including data collection, are included in the final leaderboard. We follow the same protocol as Chatbot Arena [11] for identifying anomalous users, as detailed in Appendix A.5."
        },
        {
            "title": "4.2 Data Analysis",
            "content": "Figure 3 presents the statistics of the voting data collected through SciArena. We collect total of 13,204 votes. To better understand the types of questions researchers pose on the platform, we randomly sample 200 questions for manual analysis and organize them into five primary categories, along with an Others category, as shown in Figure 3. We then use GPT-4.1 to classify all collected questions (using the prompt described in Appendix B.3). The resulting distribution of question categories, along with corresponding examples, is shown in Figure 3. Overall, users primarily use SciArena to gain deeper understanding of scientific concepts and to explore the current state of progress, ongoing challenges, and open questions within scientific domain."
        },
        {
            "title": "4.3 Quality Assessment of Collected Human Votes",
            "content": "To quantify the reliability of our collected preference data, we assess two metrics: (1) inter-annotator agreement (IAA) and (2) self-consistency, which reflects the internal stability of individual annotators judgments over time. We use accuracy and weighted Cohens κ as metrics. For each scientific discipline, we randomly selected 100 questions and the corresponding pair of two model responses, resulting in total of 400 examples. To measure IAA, each example was independently evaluated by second expert with closely aligned research background. To measure self-consistency, annotators re-evaluated examples they had previously annotated after minimum interval of two weeks. As shown in Table 3, the self-consistency results demonstrate that expert preferences remain stable over time, indicating decisions are not influenced by momentary bias. Similarly, the high IAA shows that despite the subjective nature of some questions, experts still tend to reach similar judgments. These results confirm the high quality and reliability of the collected votes. Table 1: Inter-annotator agreement (IAA) and self-consistency (SC) across disciplines. We report accuracy and weighted Cohens κ. Discipline IAA SC Acc κ Acc κ 0.82 0.76 0.94 0.91 Natural Science Healthcare 0.87 0.82 0.91 0.89 Humanity & Social Sci. 0.78 0.70 0.96 0.94 0.82 0.75 0.93 0.91 Engineering Average 0.82 0.76 0.94 0."
        },
        {
            "title": "5 SciArena Leaderboard Analysis",
            "content": "Table 2 presents the SciArena leaderboard across scientific disciplines, while Figure 4 visualizes the pairwise win rates between models. detailed breakdown of Elo ratings by question category and Elo rating confidence intervals are provided in Appendix C.1."
        },
        {
            "title": "5.1 Main Results",
            "content": "o3 consistently outperforms all other models across the four scientific disciplines, with particularly strong results in Natural Science and Engineering. It surpasses the second-best model, Claude-4-Opus, by over 100 Elo points in both areas. The performance of other models varies by domain. For example, DeepSeek-R1-0528 achieves the highest score in Natural Science (1111.4), while Claude-4-Opus leads in the remaining three disciplines. similar pattern emerges in fine-grained analyses by question type (Appendix C.1): Claude-4-Opus excels in conceptual explanation, paper findings, and methodology inquiry, whereas o4-mini surpass other models on questions about challenges & limitations and state-of-the-art assessment. Among open-source models, DeepseekR1-0528 is notable, securing the 4th position overall (1061.9), outperforming several strong proprietary models such as o4-mini and Claude-4-Sonnet. 7 Figure 4: Winning rate between subset of top-performing models. Models are ordered by their overall Elo ratings. Table 2: SciArena Leaderboard. Elo scores of all evaluated models, both overall and within four scientific disciplines, as of the evaluation cutoff date at June 30, 2025. The top and second-best models are marked in bold and underlined, respectively. The highest-ranking proprietary and open-source models are color-highlighted. Models Release Battles 2025-04 o3 [52] 2025-05 Claude-4-Opus [4] 2025-06 Gemini-2.5-Pro [26] 2025-05 Deepseek-R1-0528 [18] 2025-04 o4-mini [52] 2025-05 Claude-4-Sonnet [4] 2025-04 GPT-4.1 [51] 2025-03 Gemini-2.5-Pro-Preview [25] 2025-04 Qwen3-235B-A22B [54] 2025-04 GPT-4.1-mini [51] 2025-02 Grok-3 [71] 2025-01 Deepseek-R1 [19] 2025-03 Deepseek-V3 [17] 2025-04 Qwen3-32B [54] 2025-03 QwQ-32B [55] 2025-06 Gemini-2.5-Flash [27] Claude-3-7-Sonnet [3] 2025-02 Gemini-2.5-Flash-Preview [27] 2025-04 2025-06 Minimax-M1 [45] 2025-03 Mistral-Small-3.1 [48] 2025-04 Llama-4-Maverick [44] 2025-05 Mistral-Medium-3 [47] 2025-04 Llama-4-Scout [44] 694 912 800 814 1257 786 1546 1044 1156 956 1501 1405 1535 1176 1322 977 1517 1370 332 1193 1372 1157 1586 Scientific Discipline Natural Science Healthcare Humanities & Social Sci. Engineering Elo Score 1157.6 1046.9 1068.2 1111.4 1058.0 1018.4 1014.9 1008.5 1049.8 1014.5 1034.9 1015.0 1046.7 988.3 1021.1 955.3 1010.6 919.8 1005.3 851.2 850.0 855.2 898.5 1142.7 1123.3 1081.3 1040.7 1083.0 1104.0 1006.5 1049.8 997.2 1042.6 980.2 1011.9 1009.0 971.2 990.8 989.9 964.4 932.8 1044.2 885.8 862.8 869.0 817.0 1116.9 1073.8 1058.0 1041.6 1005.9 1066.3 1030.4 1047.7 1009.9 985.8 1031.7 1009.8 1016.2 1018.4 969.2 980.2 974.0 1000.7 961.7 927.6 924.4 846.8 903.1 1207.5 1080.7 1060.5 1064.2 1058.0 1029.8 1066.1 1022.7 1037.8 1039.6 1036.4 1022.4 1004.9 1025.4 1002.4 992.6 965.3 946.2 898.3 861.3 857.9 871.3 848. 1172.5 1080.5 1063.0 1061.9 1053.9 1044.8 1037.3 1032.8 1026.0 1025.6 1024.2 1018.1 1016.2 1008.6 996.2 984.7 975.4 953.6 945.4 880.9 872.7 863.8 861."
        },
        {
            "title": "5.2 Preference Analyses in SciArena Evaluation",
            "content": "Citations play central role in scientific literature tasks and are distinctive feature of SciArena. Contemporary with our work, Search Arena [46] finds that (1) users tend to prefer responses containing more references, and (2) that their judgments can be swayed by the mere presence of citations, regardless of whether those citations are properly attributed to specific claims. In this subsection, we examine how citation-related features influence user preferences in SciArena, focusing on two key dimensions: number of citations and attribution of inline citations to the generated content. We also analyze the feature of response length in SciArena evaluation. The implementation details of preference analyses are provided in Appendix C.2. Citation Count. We investigate the influence of citation count on user preferences within the SciArena framework. Our analysis yields modest but positive Bradley-Terry coefficient for citation count (γ = 0.039). In comparison, Search Arena [46] reports substantially higher coefficient (γ = 0.209), indicating stronger user bias toward citation-rich outputs. These findings suggest that while citation count has some influence, it is not dominant factor in shaping preferences in the SciArena evaluation setting. Citation Attribution. We further examine how the correctness of citation-to-claim attribution impacts user preferences. Using the o4-mini model, we classify each citation-response pair into two categories: supporting, where the citation backs the response, and irrelevant or contradicting, where the citation does not substantiate the claim or directly contradicts it. We observe statistically positive coefficient for supporting citations (γ = 0.155) and negative coefficient for irrelevant or contradicting ones (γ = 0.154). These findings underscore difference between SciArena and general-purpose retrieval settings. Specifically, in Search Arena, users tend to prefer responses with more citations regardless of attribution quality3. In contrast, user behavior in SciArena indicates clear preference for citations that are highly relevant and correctly attributed to the response content. 3The Search Arena paper reports γsupport = 0.29, γirrelevant = 0.27, suggesting users do not differentiate between supporting and irrelevant citations. Response Length. In Section 3.1, we describe our efforts to postprocess model outputs into standardized, literature-based format with citations, aiming to reduce bias introduced by model-specific response styles. However, response length remains potential confounding factor in evaluations. To explore this further, and following prior work [11, 20], we analyze the effect of response length features in SciArena, as described in Section 3.3. The resulting Bradley-Terry coefficient for response length is γ = 0.141, which is substantially lower than those reported in Chatbot Arena (γ = 0.25) [11], Vision Arena (γ = 0.27) [12], and Search Arena (γ = 0.33) [46]. These findings suggest that SciArena exhibits reduced length bias and yields more reliable human preference data."
        },
        {
            "title": "5.3 Case Analysis",
            "content": "Analysis of o3 Model. The leaderboard results reveal clear performance advantage of the o3 model over other systems, particularly in the Natural Sciences and Engineering disciplines. To better understand o3s strengths, we conducted human evaluation by sampling 200 voting examples that compare o3 with other top-performing models (e.g., Claude-4-Opus and Gemini-2.5-Pro), with focus on Engineering. Our analysis highlights four key strengths of the o3 model: (1) more detailed elaboration on cited papers: the o3 model consistently provides deeper explanations and richer technical insights drawn from referenced literature; (2) more professional and precise terminology: the o3 model tends to employ domain-specific vocabulary and technically accurate phrasing, reducing ambiguity and enhancing clarity; (3) clear structured presentation: o3s responses are better organized, improving both readability and synthesis of complex information; and (4) more comprehensive coverage: for question types like Challenges & Limitations and State-of-the-Art Assessment, o3s responses are notably more comprehensive, addressing broader range of points likely to be of interest to users. Examples and analyses for each strength are provided in Appendix C.3. Analysis of Model Failure Case. We then use the collected data to analyze examples that are especially challenging for current foundation models. Specifically, we filter for instances where users judged both model responses to be poor or the response from top-3 model was voted as worse. From this filtered set, we sample 100 examples for detailed human analysis. We categorize the most common failure modes into five types: (1) failure to answer the question, where responses skirt main points, offering tangential or irrelevant information instead; (2) conflict with cited papers, where references are misinterpreted or misaligned, producing claims unsupported by original studies; (3) lack of detail, where answers mention headlines only, omitting necessary mechanisms, context, examples, and quantification; (4) misunderstanding of terminology, where key terms are redefined or confused, leading to misaligned metrics and flawed conclusions; and (5) incoherent structure, where content lacks logical flow, mixing unrelated points without transitions, hindering comprehension and synthesis. Examples and detailed analyses for each failure type are provided in Appendix C.4."
        },
        {
            "title": "6.1 SciArena-Eval Benchmark",
            "content": "As discussed in the Related Work section, developing model-based evaluation methods for literaturebased long-form answers remains significant challenge. This issue is further exacerbated by the absence of standardized meta-evaluation benchmark for comparing different evaluation approaches. To address this gap, we introduce the SciArena-Eval benchmark, constructed using human preference data collected via SciArena. Specifically, we randomly sample 500 voting instances (250 examples where annotators preferred model and 250 where they preferred model B) per scientific discipline, resulting in total of 2,000 examples in SciArena-Eval. Votes marked as ties are excluded from the benchmark, as they do not provide definitive signal about which model is better, making them unsuitable for evaluating discriminative ability. The automated evaluation system is tasked with identifying the superior model response for given literature-based questions. We assess system performance by measuring accuracy against expert human judgments."
        },
        {
            "title": "6.2 Experiments",
            "content": "We assess model-based pairwise evaluation protocols, where an evaluator model is given question and two candidate responses and must select the better one (with prompt provided in Appendix D.1). Our evaluation covers range of proprietary frontier models as well as open-source models. As shown in Table 3, SciArena-Eval presents significant challenges for model-based evaluators. Even the best-performing model, o3, achieves only 65.1% accuracy. Lower-performing models, such as Gemini-2.5-Flash-Preview and Llama-4-sereis models, perform only slightly better than random guessing. Notably, similar pairwise evaluation protocols have shown strong alignment with human judgments (i.e., exceeding 70% correlation) on general-purpose benchmarks like AlpacaEval [34] and WildChat [76]. The comparatively low accuracy on SciArena-Eval highlights the unique difficulty of evaluating scientific reasoning tasks. Reasoning-augmented models generally outperform their non-reasoning counterparts from the same organization. For instance, o4-mini surpasses GPT-4.1 by 2.9%, and DeepSeekR1 outperforms DeepSeek-V3 by 0.7%. These results highlight the effectiveness of inference-time scaling in improving evaluation performance. We believe that SciArena-Eval can serve as robust benchmark for the development and evaluation of automated systems for scientific literature tasks in future research. Table 3: SciArena-Eval Results. Base Model Random Guess Acc 50.0 65.1 o3 64.8 o4-mini 61.9 GPT-4.1 61.2 DeepSeek-R1 60.5 DeepSeek-V3 60.5 GPT-4.1-mini 60.3 Gemini-2.5-Pro-Preview 59.2 Claude-3.7-Sonnet 58.1 Qwen3-32B Gemini-2.5-Flash-Preview 57.8 57.7 Llama-4-Scout 57.5 Llama-4-Maverick"
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce SciArena, dynamic evaluation platform designed to compare foundation models on scientific literature tasks. By collecting over 13,000 votes from human researchers, we provide rich dataset for analyzing human preferences across models. Our comprehensive analysis reveals key insights and highlights promising directions for advancing foundation models in scientific literature tasks. Additionally, we propose the SciArena-Eval benchmark to evaluate the model-based automated evaluation systems. Our experiments highlight the benchmarks challenges and emphasize the need for more robust and reliable automated evaluation methods."
        },
        {
            "title": "Broader Social Impact",
            "content": "Open-source Efforts. SciArena is freely accessible to the public. We have released all collected human preference data, offering valuable resource for understanding human judgment in scientific literature tasks. This data supports the development of models that better align with human standards in real-world research scenarios. The newly curated SciArena-Eval benchmark is also publicly available. Additionally, by open-sourcing the SciArena code, we enable researchers and developers to adapt our methods (e.g., literature retrieval pipeline, arena platform) to other tasks. Mitigating Challenges and Biases in SciArena Evaluation. As discussed in the main sections, several recent studies have raised concerns that the arenas open, crowdsourced protocol may introduce systematic biases [11, 63]. To address these challenges, we design SciArena and data collection pipeline with bias mitigation as primary objective. As explained in Section 4.1, only votes from users passing anomalous check are included in the final leaderboard, alleviating concerns about the reliability of open crowdsourcing. In Section 4.3, we present analyses of inter-annotator agreement and annotator self-consistency, both of which demonstrate strong alignment in user preferences. Additionally, our analysis in Section 5.2 shows that, unlike prior work in general-purpose retrieval settings, the data collected in SciArena reflects clear user preference for citations that are relevant and accurately attributed to the response content, with minimal influence from citation count. To further ensure fairness, SciArena includes only models with publicly accessible APIs or checkpoints, thereby eliminating advantages stemming from private, unreleased models."
        },
        {
            "title": "Limitations and Future Work",
            "content": "While our platform integrates wide range of state-of-the-art foundation models for convenient comparison, it currently excludes some older or deprecated versions, such as GPT-4o, Llama-3.1, and Qwen-2.5. This omission may pose challenges for developers seeking to benchmark the latest models against their earlier counterparts. Moving forward, we plan to incorporate newly released models into SciArena on rolling basis to ensure broader and more consistent coverage. Additionally, SciArena is designed to be compatible with agent-based literature review tools such as OpenAIs Deep Research and Geminis Deep Research. However, due to limitations such as daily usage caps and the unavailability of public service APIs, we are currently unable to integrate these frameworks into our platform. In future work, we aim to introduce new arena evaluation within SciArena, specifically focused on assessing agent-based literature review frameworks."
        },
        {
            "title": "Acknowledgments",
            "content": "This project is supported in part by Defense Advanced Research Projects Agencys (DARPA) SciFy program (Agreement No. HR00112520300), and Googles Scholar Research Awards program. We acknowledge the National Artificial Intelligence Research Resource (NAIRR) Pilot and Microsoft Azure for contributing to the results in this work."
        },
        {
            "title": "References",
            "content": "[1] Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. LitSearch: retrieval benchmark for scientific literature search. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1506815083, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [2] Bridger Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu A. Ha, Rodney Michael Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-Han Ooi, Matthew E. Peters, Joanna L. Power, Sam Skjonsberg, Lucy Lu Wang, Christopher Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. Construction of the literature graph in semantic scholar. In North American Chapter of the Association for Computational Linguistics, 2018. [3] Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/ claude-3-7-sonnet, February 2025. [4] Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, May 2025. [5] Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen tau Yih, Pang Wei Koh, and Hannaneh Hajishirzi. Openscholar: Synthesizing scientific literature with retrieval-augmented lms, 2024. [6] Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike DArcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke S. Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen tau Yih, Pang Wei Koh, and Hanna Hajishirzi. Openscholar: Synthesizing scientific literature with retrieval-augmented lms. ArXiv, abs/2411.14199, 2024. [7] Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. Re-evaluating evaluation in text summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 93479359, Online, November 2020. Association for Computational Linguistics. [8] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [9] Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld. TLDR: Extreme summarization of scientific documents. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 47664777, Online, November 2020. Association for Computational Linguistics. 11 [10] Cheng-Han Chiang and Hung yi Lee. Can large language models be an alternative to human evaluations? In Annual Meeting of the Association for Computational Linguistics, 2023. [11] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating LLMs by human preference. In Forty-first International Conference on Machine Learning, 2024. [12] Christopher Chou, Lisa Dunlap, Koki Mashita, Krishna Mandal, Trevor Darrell, Ion Stoica, Joseph Gonzalez, and Wei-Lin Chiang. Visionarena: 230k real world user-vlm conversations with preference labels. ArXiv, abs/2412.08687, 2024. [13] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. SPECTER: Document-level representation learning using citation-informed transformers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 22702282, Online, July 2020. Association for Computational Linguistics. [14] Consensus. Consensus ai for research, 2024. Accessed: 2025-05-02. [15] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45994610, Online, June 2021. Association for Computational Linguistics. [16] Google DeepMind. Gemini deep research mode, 2024. Accessed: 2025-05-02. [17] DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, December 2024. [18] DeepSeek-AI. Deepseek-r1-0528 release. https://api-docs.deepseek.com/news/ news250528/, May 2025. [19] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [20] Yann Dubois, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple debiasing of automatic evaluators. In First Conference on Language Modeling, 2024. [21] Lisa Dunlap, Krishna Mandal, Trevor Darrell, Jacob Steinhardt, and Joseph Gonzalez. Vibecheck: Discover and quantify qualitative differences in large language models, 2025. [22] Elicit. Elicit the ai research assistant, 2024. Accessed: 2025-05-02. [23] Arpad Elo. The proposed uscf rating system, its development, theory, and applications. Chess life, 22(8):242247, 1967. [24] Ronald Aylmer Fisher. Statistical methods for research workers. Number 5. Oliver and Boyd, 1928. [25] Google. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025//, March 2025. [26] Google. Gemini 2.5 pro preview: even better coding performance. https://developers. googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/, April 2025. [27] Google. Start building with gemini 2.5 flash. https://developers.googleblog.com/en/ start-building-with-gemini-25-flash/, April 2025. [28] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. GenAI arena: An open evaluation platform for generative models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [29] Tetsu Kasanishi, Masaru Isonuma, Junichiro Mori, and Ichiro Sakata. SciReviewGen: large-scale dataset for automatic literature review generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 66956715, Toronto, Canada, July 2023. Association for Computational Linguistics. 12 [30] Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D. Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Van Zuylen, and Daniel S. Weld. The semantic scholar open data platform, 2025. [31] Yoonjoo Lee, Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Ho Hin Lee, and Moontae Lee. Qasa: Advanced question answering on scientific articles. In International Conference on Machine Learning, 2023. [32] Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, and Arman Cohan. M3SciQA: multimodal multi-document scientific QA benchmark for evaluating foundation models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1541915446, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [33] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024. [34] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. [35] Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, and Arman Cohan. Mdcure: scalable pipeline for multi-document instruction-following, 2025. [36] Haokun Liu, Sicong Huang, Jingyu Hu, Yangqiaoyu Zhou, and Chenhao Tan. Hypobench: Towards systematic and principled benchmarking for hypothesis generation, 2025. [37] Hongyi Liu, Qingyun Wang, Payam Karisani, and Heng Ji. Named entity recognition under domain shift via metric learning for life sciences. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 121, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [38] Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 41404170, Toronto, Canada, July 2023. Association for Computational Linguistics. [39] Yixin Liu, Alexander Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 44814501, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [40] Yixin Liu, Kejian Shi, Alexander Fabbri, Yilun Zhao, PeiFeng Wang, Chien-Sheng Wu, Shafiq Joty, and Arman Cohan. ReIFE: Re-evaluating instruction-following evaluation. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, NAACL. Association for Computational Linguistics, April 2025. [41] Yao Lu, Yue Dong, and Laurent Charlin. Multi-XScience: large-scale dataset for extreme multidocument summarization of scientific articles. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 80688074, Online, November 2020. Association for Computational Linguistics. [42] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 13 [43] Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. ExpertQA: Expert-curated questions and attributed answers. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 30253045, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [44] Meta AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https: //ai.meta.com/blog/llama-4-multimodal-intelligence/, April 2025. [45] MINIMAX. Minimax-m1, the worlds first open-source, large-scale, hybrid-attention reasoning model. https://www.minimax.io/news/minimaxm1/, June 2025. [46] Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, and Joseph E. Gonzalez. Search arena: Analyzing search-augmented llms, 2025. [47] Mistral AI Team. Medium is the new large. https://mistral.ai/news/mistral-medium-3/, May 2025. [48] Mistral AI Team. Mistral small 3. https://mistral.ai/news/mistral-small-3/, January 2025. [49] mrfakename, Vaibhav Srivastav, Clémentine Fourrier, Lucain Pouget, Yoach Lacombe, main, and Sanchit Gandhi. Text to speech arena. https://huggingface.co/spaces/TTS-AGI/TTS-Arena, 2024. [50] OpenAI. Chatgpt deep research mode, 2024. Accessed: 2025-05-02. [51] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, April 2025. [52] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, April 2025. [53] Perplexity. Perplexity ai ask anything, 2024. Accessed: 2025-05-02. [54] Qwen Team. Qwen3: Think deeper, act faster, April 2025. [55] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [56] Scale AI. Advancing frontier model evaluation. Scale AI Blog, 2024. [57] Scite. Scite smart citations for research, 2024. Accessed: 2025-05-02. [58] Aamir Shakir, Darius Koenig, Julius Lipp, and Sean Lee. Boost your search with the crispy mixedbread rerank models, 2024. [59] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs generate novel research ideas? largescale human study with 100+ NLP researchers. In The Thirteenth International Conference on Learning Representations, 2025. [60] Amanpreet Singh, Joseph Chee Chang, Chloe Anastasiades, Dany Haddad, Aakanksha Naik, Amber Tanaka, Angele Zamarron, Cecile Nguyen, Jena D. Hwang, Jason Dunkleberger, Matt Latzke, Smita Rao, Jaron Lochner, Rob Evans, Rodney Kinney, Daniel S. Weld, Doug Downey, and Sergey Feldman. Ai2 scholar qa: Organized literature synthesis with attribution, 2025. [61] Amanpreet Singh, Joseph Chee Chang, Chloe Anastasiades, Dany Haddad, Aakanksha Naik, Amber Tanaka, Angele Zamarron, Cecile Nguyen, Jena D. Hwang, Jason Dunkleberger, Matt Latzke, Smita Rao, Jaron Lochner, Rob Evans, Rodney Kinney, Daniel S. Weld, Doug Downey, and Sergey Feldman. Ai2 scholar qa: Organized literature synthesis with attribution. 2025. [62] Amanpreet Singh, Mike DArcy, Arman Cohan, Doug Downey, and Sergey Feldman. SciRepEval: multi-format benchmark for scientific document representations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 55485566, Singapore, December 2023. Association for Computational Linguistics. [63] Shivalika Singh, Yiyang Nan, Alex Wang, Daniel Dsouza, Sayash Kapoor, A. Ustun, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Hilal Ermis, Marzieh Fadaee, and Sara Hooker. The leaderboard illusion. 2025. 14 [64] Shruti Singh, Nandan Sarkar, and Arman Cohan. SciDQA: deep reading comprehension dataset over scientific papers. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2090820923, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [65] Annalisa Szymanski, Noah Ziems, Heather A. Eicher-Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald A. Metoyer. Limitations of the llm-as-a-judge approach for evaluating llm outputs in expert knowledge tasks, 2024. [66] Aryan Vichare, Anastasios N. Angelopoulos, Wei-Lin Chiang, Kelly Tang, and Luca Manolache. Webdev arena: live llm leaderboard for web app development, 2025. [67] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 75347550, Online, November 2020. Association for Computational Linguistics. [68] Bowen Wang, Xinyuan Wang, Jiaqi Deng, Tianbao Xie, Ryan Li, Yanzhe Zhang, Gavin Li, Toh Jing Hua, Ion Stoica, Wei-Lin Chiang, Diyi Yang, Yu Su, Yi Zhang, Zhiguo Wang, Victor Zhong, and Tao Yu. Computer agent arena: Compare and test computer use agents on crowdsourced real-world tasks, 2025. [69] Chengye Wang, Yifei Shen, Zexi Kuang, Arman Cohan, and Yilun Zhao. Sciver: Evaluating foundation models for multimodal scientific claim verification, 2025. [70] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. SciMON: Scientific inspiration machines optimized for novelty. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279299, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [71] xAI. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, February 2025. [72] Nithik Yekollu, Arth Bohra, Ashwin Chirumamilla, Kai Wen, Sai Kolasani Wei-Lin Chiang, Anastasios Angelopoulos, Joseph E. Gonzalez, Ion Stoica, and Shishir G. Patil. Agent arena. 2024. [73] You.com. You.com personalized ai search, 2024. Accessed: 2025-05-02. [74] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations, 2024. [75] Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, and Jiawei Han. comprehensive survey of scientific large language models and their applications in scientific discovery. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 87838817, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [76] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024. [77] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. [78] Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, and Pengfei Liu. OpenResearcher: Unleashing AI for accelerated scientific research. In Delia Irazu Hernandez Farias, Tom Hope, and Manling Li, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 209218, Miami, Florida, USA, November 2024. Association for Computational Linguistics."
        },
        {
            "title": "A SciArena Platform",
            "content": "A.1 User Study Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Online Elo Rating System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Prompts for Model Response Generation and Postprocessing . . . . . . . . . . . . A.4 Configurations of Evaluated Foundation Models, As of June 30, 2025 . . . . . . . A.5 Detecting Anomalous Users . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B SciArena Data",
            "content": "B.1 Annotator Biographies of 102 Researchers Involved in Initial Data Collection . . . B.2 SciArena Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Prompts for Question Category Classification . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C SciArena Leaderboard Results",
            "content": "C.1 In-depth Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Analysis of Citation Features in SciArena Evaluation . . . . . . . . . . . . . . . . C.3 Analysis of o3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3.1 Examples Illustrating o3s Detailed Elaboration on Cited Papers . . . . . . C.3.2 Examples Illustrating o3s More Professional & Precise Terminology . . . C.3.3 Examples Illustrating o3s Clear Structured Presentation . . . . . . . . . . C.3.4 Examples Illustrating o3s More Comprehensive Coverage . . . . . . . . . C.4 Model Failure Case Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4.1 Examples Illustrating Errors of Failure to Answer the Question . . . . . . . C.4.2 Examples Illustrating Errors of Conflict with Cited Papers . . . . . . . . . C.4.3 Examples Illustrating Errors of Lack of Details . . . . . . . . . . . . . . . C.4.4 Examples Illustrating Errors of Misunderstanding Terminology . . . . . . C.4.5 Examples Illustrating Errors of Incoherent Structure . . . . . . . . . . . . SciArena-Eval Meta-Evaluation Experiment D.1 Evaluation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 18 18 21 22 22 24 26 27 28 29 29 30 32 33 33 34 35 37 38"
        },
        {
            "title": "A SciArena Platform",
            "content": "A.1 User Study Results Table 4: User feedback comparing the usability of SciArena with that of other platforms for conducting scientific-literature tasks Question 1: Compare SciArena and Chatbot Platforms with search ability like ChatGPT search and Perplexity AI in the aspect of usability of output and generation time. Question 2: Compare SciArena and Agent-based Platforms like ChatGPT deep research and Gemini deep research in the aspect of usability of output and generation time. User 1 [clinical researcher in public health system]: ChatGPT search and Perplexity AI can give good answers for some simple questions. But for complex queries (e.g. treatment comparisons or emerging technologies) SciArena captures my intention better and provides more structured, comprehensive answer. And both of them could provide fast response User 1 [clinical researcher in public health system]: ChatGPT deep research and Gemini deep research are great for well-organized overview. When preparing materials for publication, SciArena helps more with traceable citations and compact answers. And SciArena usually gives me more prompt response. User 2 [4th-year PhD Student in Computer Science]: find SciArena valuable for paper writing with fewer hallucinations. Conversely, Chatbot Platforms with search ability sometimes gives unsupported statements citing unreliable resources. SciArena sometimes takes little bit longer to generate the response. But, am fine with it since its from reliable resources. User 3 [3rd-year PhD Student in Environmental Science]: Chatbot Platforms with search ability is great for general queries, but for niche topics it can surface broad or outdated sources. SciArena feels more trustworthy for academic work. And both of them are fast considering my questions. User 2 [4th-year PhD Student in Computer Science Engineering]: Both tools give grounded, comprehensive material, and find them equally helpful for my study. Regarding on the generation time, SciArena is more efficient in Literature Review. User 3 [3rd-year PhD Student in Environmental Science]: Agent-based Platforms provides longer, narrativestyle answer, but can be too lengthy for simple queries. And It usually takes several minutes to get comprehensive report. SciArena offers more curated answers so can quickly decide which papers are worth reading. User 4 [Applied Economist in Humanity]: ChatGPT search like tool can be quick but surfacelevel. SciArena gives claims tied directly to the question, with proper citations. Both provide valuable information overall. And both provide answers within short of time. User 4 [Applied Economist in Humanity]: ChatGPT Deep Research like tool sometimes overexplains. When just need 23 key papers, SciArena is leaner and helps me stay focused. And ChatGPT Deep Research like tool often takes while to generate those report-like responses. Table 5: Likelihood of continued SciArena platform use (Q3) Question 3: From 1 to 10, how likely are you to continue using this platform in your daily research? (Optional reasons below) User 1 [clinical researcher in public health system]: 9/10 User 2 [4th-year PhD Student in Computer Science Engineering]: 8/10. Relies on ChatGPT Deep Research for speculative reasoning and ChatGPT search for broader overviews. User 3 [Natural Science]: 8/10. SciArena makes some projects easier, especially when hard-science support is needed. User 4 [Humanity]: 7/10. Its fast, reliable, and doesnt try to be overly smart. 17 A.2 Online Elo Rating System The online Elo rating system models the probability that model wins against model given their current ratings Ri and Rj, where i, . Specifically, we define binary outcome Yij, where Yij = 1 if model wins and Yij = 0 otherwise. The win probability is modeled as: (Yij = 1) = 1 1 + 10(Rj Ri)/α , where α = 400 is the scaling factor. After each match, player ratings are updated as follows: = Ri + (S(ij) E(ij)), (2) (3) where S(ij) is the actual match result (1 for win, 0.5 for tie, 0 for loss), determines the sensitivity of the Elo system to new match results, and E(ij) = (Yij = 1) is the expected score. This update rule ensures that higher-rated models gain fewer points when winning and lose more points when losing, while lower-rated models behave oppositely. However, the online Elo ratings are sensitive to the order in which comparisons occur. A.3 Prompts for Model Response Generation and Postprocessing [System Prompt] You are helpful research assistant tasked with producing citation-attributed response to users question. Please follow these instructions: 1. Inputs - References: list of papers (each with title and brief context). Note that some papers may be irrelevant to the user question. - User question: question or topic the user wants to investigate. 2. Your Task - Examine all provided References and select the most relevant papers that directly address the user question. - Write citation-attributed response that addresses the user question. 3. Citation Format - Cite each paper using square brackets with the papers index from the References list (e.g., Several studies suggest [1].). - Do not cite papers not listed in the References. [User Prompt] References List (list of paper that might be relevant): 1. Title: {paper-1-title} Authors: {paper-1-authors} Relevant Context: {paper-1-context} 2. Title: {paper-2-title} Authors: {paper-2-authors} Relevant Context: {paper-2-context} (...abbreviated...) Question: {question} Write citation-attributed response that addresses the user question. Figure 5: The prompt used for model response generation. 18 [System Prompt] You are an assistant specialized in processing citation-attributed response. Your task is to transform the review into structured format by doing the following: - Ensure all in-text citations in the main text follow the numeric bracket style, e.g. [1]. - Ensure all the brackets (citation) appear at the end of the sentence instead of in the middle of sentence or come after clause with comma. e.g. Guo et al. [1] introduced IdeaBench, benchmark specifically designed to assess the capability of various LLMs in generating innovative research ideas. should be changed to Guo et al. introduced IdeaBench, benchmark specifically designed to assess the capability of various LLMs in generating innovative research ideas [1]. - Remove any Markdown formatting from the text. - Make only minimal necessary modifications to achieve this format. Structure the response in the following format: { \"references\": [], // list of int, each int is the id of the reference in the response. \"response\": // Main text of the generated response. } [User Prompt] References List (list of paper that might be relevant): 1. Title: {paper-1-title} Authors: {paper-1-authors} Relevant Context: {paper-1-context} 2. Title: {paper-2-title} Authors: {paper-2-authors} Relevant Context: {paper-2-context} (...abbreviated...) Question: {question} Response: {response} Follow the instruction to process the model response. Figure 6: The prompt used for model response postprocessing using the GPT-4.1 model. Citations are then matched to the reference list using rule-based methods, and the finalized response is displayed in the SciArena interface. 19 A.4 Configurations of Evaluated Foundation Models, As of June 30, 2025 Organization Model Release Version Reasoning Model? OpenAI Google o3 o4-mini GPT-4.1 GPT-4.1-mini Proprietary Models 2025-04 2025-04 2025-04 2025-04 o3-2025-04-16 o4-mini-2025-04-16 gpt-4.1-2025-04-14 gpt-4.1-mini-2025-04-14 2025-06 Gemini 2.5 Pro 2025-06 Gemini 2.5 Flash Gemini 2.5 Pro Preview 2025-03 Gemini 2.5 Flash Preview 2025-04 gemini-2.5-pro gemini-2.5-flash gemini-2.5-pro-preview-03-25 gemini-2.5-flash-preview-04-17 Anthropic Claude 4 Opus Claude 4 Sonnet Claude 3.7 Sonnet xAI Grok3 Mistral AI Mistral-Medium-3 2025-05 2025-05 2025-02 20252025-05 claude-4-opus-20250522 claude-4-sonnet-20250522 claude-3-7-sonnet-20250219 grok-3-beta mistralai/Mistral-Medium-3-45B-Instruct-2505 Open-source Multimodal Foundation Models Mistral AI Mistral-Small-3.1 Alibaba Meta DeepSeek Qwen3-235B-A22B Qwen3-32B QwQ-32B Llama-4 Maverick Llama-4 Scout DeepSeek-R1-0528 DeepSeek-R1 DeepSeek-V3 MiniMax MiniMax-M1 2025-03 2025-04 2025-04 2025-03 2025-04 20252025-05 2025-01 2025-03 2025-06 mistralai/Mistral-Small-3.1-24B-Instruct-2503 Qwen/Qwen3-235B-A22B Qwen/Qwen3-32B Qwen/QwQ-32B meta-llama/Llama-4-Maverick-17B-128E-Instruct meta-llama/Llama-4-Scout-17B-16E-Instruct deepseek-ai/DeepSeek-R1-0528 deepseek-ai/DeepSeek-R1 deepseek-ai/DeepSeek-V3minimax/MiniMax-M1-80k yes yes no no hybrid hybrid hybrid hybrid hybrid hybrid hybrid no no no hybrid hybrid yes no no yes yes no yes Table 6: Details of the foundation models included in SciArena as of the date at June 30, 2025. The Source column includes URLs for proprietary models and Hugging Face model names for open-source models. For each model, we set the temperature as 1. Proprietary models are accessed via their official APIs, while open-source models are served through third-party providers such as Together AI. This setup eliminates the need for local GPU resources. For the o3 and o4-mini models, we set the parameter of reasoning_effort as medium. For Claude-series models, we enable extended thinking with budget_tokens value of 2,048. For Gemini-2.5-series models, we use the default setting where the thinking mode is on and thinking budget is dynamic. 20 A.5 Detecting Anomalous Users We adopt the same approach as Chatbot Arena [11] for detecting anomalous users. Specifically, we consider dataset consisting of distinct IPs, and denote the full set of IP addresses as IP = {1, . . . , }. Suppose new user provides sequence of ratings in response to sequence of actions n. Our goal is to assess whether this users rating behavior aligns with the historical distribution of ratings associated with each action. To formalize this, for given action a, we define the historical ratings set Ha = {Ht : At = a}. When the new user submits rating for action i, we compute the following statistic: 1, . . . , 1, . . . , pi = 1 + 1 HA 1 + (cid:88) hHA 1{h i} . Assuming the null hypothesis that HA p-value. The dependence among these p-values becomes negligible asymptotically. are exchangeable, the value pi serves as valid and To perform sequential testing under this null hypothesis, we apply Fishers method for combining p-values [24], alongside modified Bonferroni correction. Specifically, after the j-th rating from test user, we calculate: Mj = 2 (cid:88) log(pi). i=1 We then assess anomalous behavior at five randomly selected values of between 1 and 100. user is flagged as anomalous if Mj χ2 2j,1α/5 at any of these selected points. Randomizing the evaluation times helps mitigate strategic manipulation by potentially anomalous users."
        },
        {
            "title": "B SciArena Data",
            "content": "B.1 Annotator Biographies of 102 Researchers Involved in Initial Data Collection ID Position Field Publications Engineering Computer Science Civil Engineering PhD student PhD student Graduate student Optical Engineering Aviation Engineering PhD student Computer Science PhD student Electrical Engineering PhD student Industrial Design Graduate student Engineering Management PhD student Mechanical Engineering PhD student PhD student Environment Engineering Graduate student Computer Science Computer Science PhD student Electrical Engineering PhD student Power Engineering Graduate student Environment Engineering PhD student Optical Engineering PhD student Computer Science PhD student Computer Science PhD student Engineering Management Graduate student Mechanical Engineering PhD student Civil Engineering PhD student PhD student Industrial Design Graduate student Aviation Engineering Computer Science PhD student PhD student Computer Science Graduate student Computer Science Computer Science PhD student Computer Science PhD student PhD student Computer Science Graduate student Computer Science Power Engineering PhD student Electrical Engineering PhD student Power Engineering PhD student Optical Engineering PhD student Environment Engineering PhD student Engineering Management PhD student Mechanical Engineering PhD student Civil Engineering PhD student Graduate student Industrial Design Graduate student Aviation Engineering Graduate student Computer Science Graduate student Computer Science Healthcare Clinical Medicine PhD student Pharmacy Graduate student Basic Medicine PhD student Public Health PhD student Graduate student Neuroscience PhD student PhD student PhD student Graduate student PhD student Biomedical Engineering Clinical Medicine Pharmacy Basic Medicine Public Health 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 43 44 45 46 47 48 49 50 51 52 5-10 5-10 2-5 5-10 5-10 2-5 2-5 5-10 5-10 5-10 >10 5-10 5-10 5-10 5-10 5-10 2-5 5-10 5-10 5-10 >10 5-10 2-5 5-10 5-10 2-5 >10 5-10 2-5 2-5 5-10 5-10 2-5 5-10 >10 2-5 5-10 5-10 >10 5-10 5-10 5-10 5-10 2-5 5-10 5-10 >10 2-5 5-10 5-10 5-10 5-10 Continued on next page 22 Position Field Publications 2-5 5-10 2-5 5-10 2-5 2-5 5-10 5-10 2-5 5-10 2-5 5-10 2-5 5-10 5-10 2-5 2-5 2-5 2-5 5-10 5-10 5-10 5-10 >10 5-10 5-10 2-5 2-5 2-5 5-10 2-5 2-5 5-10 2-5 5-10 2-5 2-5 5-10 2-5 5-10 5-10 2-5 5-10 5-10 2-5 2-5 2-5 5-10 5-10 2-5 ID 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 Graduate student Neuroscience PhD student PhD student Graduate student PhD student Graduate student Biomedical Engineering Clinical Medicine Public Health Basic Medicine Pharmacy Natural Sciences Environmental Science Ecology Crop Science Material Science Mathematics Physics Biology Environmental Science PhD student Graduate student Animal Science PhD student PhD student Graduate student Chemistry PhD student Horticulture Food Science PhD student Graduate student Geographic Information Science PhD student PhD student Graduate student PhD student PhD student Graduate student Animal Science PhD student PhD student Graduate student Chemistry Horticulture PhD student PhD student Food Science Graduate student Geographic Information Science PhD student PhD student Graduate student PhD student PhD student Graduate student Material Science PhD student Graduate student Chemistry Material Science Mathematics Physics Biology Ecology Ecology Crop Science Biology Humanities & Social Sciences Management Economics Linguistics Psychology History PhD student 87 Graduate student Arts and Media 88 PhD student 89 PhD student 90 Graduate student 91 PhD student 92 Graduate student Management 93 PhD student 94 PhD student 95 Graduate student 96 PhD student 97 PhD student 98 Graduate student Management 99 100 PhD student 101 Graduate student 102 Graduate student Management Arts and Media Economics Linguistics Psychology History Economics Psychology 23 B.2 SciArena Data Analysis (a) o3 (b) Claude-4-Opus (c) Gemini-2.5-Pro (d) Deepseek-R1- (e) Deepseek-V3 (f) o4-mini Figure 7: Citation count distribution for different models (Part 2). 24 (a) GPT-4.1 (b) Qwen3-235B-A22B (c) Llama-4-Maverick (d) Mistral-Medium-3 Figure 8: Citation count distribution for different models (Part 2). Figure 9: Average Citation Count for all models. 25 B.3 Prompts for Question Category Classification [System Prompt] You are an expert in classifying questions into categories. Given question, you should classify each question into specific category. If there is no suitable category, you should respond with 6 to refer as Others. Question Categories: 1. Conceptual Explanation: questions about explanations of scientific principles or underlying mechanisms. For example: How does DNA methylation regulate gene expression during fruit ripening? 2. Methodology Inquiry: questions about technical methods, modeling, or simulations. For example: What are the forms and methods of film and game integration? 3. State-of-the-Art Assessment:questions about the current innovations, trends, or breakthroughs in specific field. For example: How are circular economy principles contributing to industrial emissions reduction lately? 4. Challenges & Limitations: questions about challenges, limitations, or potential areas for improvement. For example: What are the primary challenges hindering the widespread application of uranium based metal organic frameworks? 5. Paper Finding: questions about literature summaries, evidence synthesis, or specific paper retrieval. For example: Research on Legal Issues Involved in the Development and Use of Intellectual Property Information Resources 6. Others: questions not fitting any of the above categories. Respond with ONLY the category number, nothing else. You should respond with number ranging from 1 to 6. [User Prompt] Question: {question} Category: Figure 10: The prompt used for question category classification."
        },
        {
            "title": "C SciArena Leaderboard Results",
            "content": "C.1 In-depth Analysis Figure 11: Leaderboard from SciArena, showing Elo ratings with 95% confidence intervals. Figure 12: Elo ratings of top-5 models across question categories. 27 C.2 Analysis of Citation Features in SciArena Evaluation To systematically investigate the impact of citation-related features on user preferences in SciArena, we randomly sample 3,000 voting instances for analysis. Each citation within the responses is labeled using the o4-mini model to determine its attribution category: supporting, irrelevant, or contradicting. The prompt used for this classification is shown in Figure 13. We merge the irrelevant and contradicting categories for downstream analysis. For each response, we count the number of (1) supporting citations and (2) irrelevant or contradicting citations. These counts are then used as features in Bradley-Terry model, consistent with the methodology applied in other control experiments. [System Prompt] You are professional literature citation analysis expert who needs to judge the relationship between citation content and text statements. [User Prompt] Please analyze whether the following citation is consistent with the content mentioned in the response. Response content:{response} Citation author: {concise_authors} Citation original content:{content} Please judge whether the citations original content is consistent with the statements in the response based on the specific content mentioned about \"{concise_authors}\" in the response. Special attention: 1. Please pay attention to the letters at the end of concise_authors (such as a, b, c, d, etc.) to ensure correct citation matching 2. Carefully compare the specific statements about this author in the response with the citations original content 3. Classification criteria: - supporting: The citation content supports the statements in the response - irrelevant: The citation content is unrelated or irrelevant to the statements in the response - contradicting: The citation content contradicts the statements in the response Please classify based on the above analysis and provide brief explanation. Figure 13: The prompt used for citation attribution classification. 28 C.3 Analysis of o3 Model C.3.1 Examples Illustrating o3s Detailed Elaboration on Cited Papers Detailed Elaboration on Cited Paper (Example 1) User question: What statistical models effectively capture polysemy development and lexical network growth? Caude-Opus-4 Response: (...abbreviated...) Parallel to network growth, statistical analyses reveal clear developmental patterns in polysemy. The mean polysemy of words used by children exhibits two-phase growth pattern, characterized by rapid increase that slows after breakpoint at approximately 31 months of age (Casas et al. 2016a). (...abbreviated...) o3 Response: (...abbreviated...) Developmentally, longitudinal CHILDES analyses that compute the average number of WordNet or SemCor senses per produced token reveal two-phase fastslow rise in childrens mean polysemy with breakpoint near 32 months, indicating that polysemy acquisition follows quantifiable growth curve (Casas et al. 2016a). (...abbreviated...) Analysis: Compared with the Caude-Opus-4 version, o3 explicitly spells out that mean polysemy was measured as the average number of WordNet or SemCor senses per token rather than just mean polysemy. It labels the developmental pattern as two-phase fastslow rise, giving the growth curve precise conceptual frame. By framing polysemy acquisition as quantifiable growth trajectory, o3 highlights both the method and the empirical milestone. This extra methodological detail and clear terminology make o3s answer more detailed elaboration on the cited paper. Detailed Elaboration on Cited Paper (Example 2) User question: How have benchmarks evolved to assess language model performance in lesser-studied academic disciplines? Caude-Opus-4 Response: (...abbreviated...) As models achieve nearsaturation performance on existing benchmarks, researchers have responded by creating more challenging, domain-specific evaluations that test deeper reasoning within particular fields rather than broad surface-level knowledge (Pricope et al. 2025a). (...abbreviated...) o3 Response: (...abbreviated...) HardML extends the MMLU philosophy but contributes hundreds of fresh questions spanning algorithm design, statistical inference, and practical ML engineering to probe depth rather than breadth in DS/ML (Pricope et al. 2025a). (...abbreviated...) Analysis: Compared to Caude-Opus-4s high-level summary, o3 dives deeper into Pricope et al. (2025a) by enumerating HardMLs key question categoriesalgorithm design, statistical inference, and practical ML engineering, which provides more detailed elaboration. 29 C.3.2 Examples Illustrating o3s More Professional & Precise Terminology More Professional&Precise Terminology User question: What are the main challenges and solutions related to maintaining model performance when using low-bit attention techniques during training? Caude-Opus-4 Response: Low-bit attention techniques during training face significant challenges in maintaining model performance, primarily due to quantization-induced information loss and training instability. The fundamental challenge stems from the unique properties of transformers attention mechanisms, which exhibit high quantization sensitivity compared to convolutional neural networks. As Huang et al. demonstrate, transformers display variation behaviors across three hierarchies: module-level quantization sensitivities, outliers in weight and activation distributions, and oscillation in dynamic parameter fluctuations, all of which contribute to training instability and performance degradation (Huang et al. 2023a). The presence of outliers in attention computations poses particularly severe challenge, as these values expand the quantization range and make low-bit quantization difficult, with some approaches showing over 10% accuracy degradation when quantizing to sub-2-bit representations (Du et al. 2024a). Additionally, the computational pipeline of quantization methods presents efficiency challenges, as most quantized training methods adopt quantize-computedequantize paradigm that proves particularly inefficient for transformer architectures due to frequent data conversion overhead (Hao et al. 2025a). (...abbreviated...) o3 Response: Training transformers with low-bit attention confronts several intertwined problems. Outliers in key/value activations and in the soft-max scores inflate the dynamic range, so rounding to 4 bits or less causes frequent overflow/underflow and sharp accuracy loss (Wang et al. 2025b). Quantization sensitivity also varies widely across attention sub-modules; query/key projections, soft-max, and residual connections oscillate differently during optimization, making low-bit training unstable and hampering convergence (Huang et al. 2023a). Moreover, lowprecision formats are not natively supported for optimizer states or inter-GPU communication, so gradients and moments must be promoted, negating speed benefits unless additional engineering is introduced (Xie et al. 2024a). Finally, aggressive rounding can destabilize Adam-style updates, leading to divergence unless the optimizer itself is re-designed for few-bit arithmetic (Huang et al. 2025a). Recent studies present complementary solutions. Variation-aware quantization assigns modulespecific scale factors, detects outliers with bin regularization, and distills knowledge from fullprecision teacher, recovering 2-bit transformer performance on both language and vision tasks (Huang et al. 2023a). Stable-SPAM redesigns Adam to store moments in 4-bit integers with norm-aware clipping, enabling 4-bit training that is even more stable than standard 16-bit baselines (Huang et al. 2025a). (...abbreviated...) Analysis: o3s response is more professional and precise because first it explicitly breaks down the \"submodules\" of attention\"query/key projections, soft-max, and residual connections\"showing deeper technical understanding. It references the concrete architectural elements affected by quantization, rather than just stating \"three hierarchies.\" This terminology is more specific and demonstrates expertise in the transformer architecture. Besides, o3 is more precise by specifying where the outliers occur (\"key/value activations and in the soft-max scores\") and the concrete effect (\"inflate the dynamic range,\" \"frequent overflow/underflow\"), which are technical terms in quantization. 30 C.3.3 Examples Illustrating o3s Clear Structured Presentation Clear Structured Presentation User question: What are the main challenges and solutions for achieving controllable video generation across diverse input conditions? Gemini-2.5-Pro: primary challenge in controllable video generation is the fragmented landscape of specialized models, which often lack generalizability across different control types (Zhang et al. 2025a). This makes it difficult to adapt existing spatial controls from images, as frameworks like ControlNet cannot be directly integrated into new video backbones due to feature mismatches (Lin et al. 2024a). Furthermore, many methods require impractical, dense condition sequences for every frame to maintain temporal consistency (Wang et 2024c). To overcome these issues, real. searchers propose unified frameworks using efficient adapters, which add diverse controls such as sketches or depth maps to pre-trained models with minimal retraining (Wang et al. 2024b). Other solutions feature unified pipeline to encode various signals into common representation, enabling single generator to handle multiple control types (Zhang et al. 2025a). separate significant challenge is the limited capacity of diffusion models to interpret complex, heterogeneous multimodal inputs simultaneously, which can cause conflicts between conditions and degrade video quality (Chen et al. 2024b). To address this, frameworks like Any2Caption decouple condition interpretation from synthesis, using Multimodal Large Language Model (MLLM) to translate diverse inputs into single, dense text caption that provides clearer guidance to the video generator (Wu et al. 2025b). o3 Response: Controllable video generation must reconcile heterogeneous user inputs with high visual fidelity and temporal coherence. Key technical hurdles include (i) the feature-space mismatch that prevents simply porting 2D control modules such as ControlNet to video backbones, hampering reuse across models (Lin et al. 2024a); (ii) dependence on dense, frame-by-frame control sequences that place an impractical burden on creators and still struggle to propagate signals through time (Wang et al. 2024d); (iii) the limited capacity of conventional text encoders to interpret complex, multimodal intent, causing weak alignment between the specified constraints and the produced clip (Wu et al. 2025b); and (iv) geometric and temporal inconsistency when only 2D cues are available, especially for camera or object motion, because the underlying 3D scene is ignored (Gu et al. 2025a). Recent research proposes complementary remedies. EasyControl attaches lightweight condition adapter to pretrained text-to-video diffusion models and propagates single sparse map through residual summation, yielding plug-andplay control over images, edges, depth, sketches and masks with minimal extra training (Wang et al. 2024b). VCtrl generalises this idea via unified control encoder and sparse residual connections, enabling fine-grained, multi-task manipulation while keeping the original generator frozen (Zhang et al. 2025a). (...abbreviated...) Analysis: o3 directly enumerate the \"main challenges\" using (i) - (iv), and then introduce the \"solutions\" separately in paragraphs. This itemized list makes it clear to readers at glance. In contrast, gemini-2.5-pro only lists challenges and solutions in streamlined manner, lacking hierarchical markers and making it difficult to quickly grasp the key points. C.3.4 Examples Illustrating o3s More Comprehensive Coverage More Comprehensive Coverage User question: What are the most widely adopted open-source tools for time series load forecasting in cloud systems? Gemini-2.5-Flash Response: While the literature extensively discusses various machine learning and deep learning models for time series load forecasting in cloud systems, such as ARIMA, LSTM, and SVM, explicit mentions of widely adopted open-source tools or frameworks specifically for this task are less prevalent (Mahbub et al. 2024a). However, some systems designed to automate or facilitate time series forecasting, often leveraging open-source components, have been proposed. AutoAI-TS, for instance, is presented as zeroconfiguration system framework designed to automate the time series forecasting process, incorporating models from the open-source domain and using scikit-learn compatible APIs (Shah et al. 2021a). This system aims to achieve high accuracy with minimal user input by transparently handling tasks like feature engineering, training, parameter tuning, and model ranking (Shah et al. 2021a). Another relevant system is Castor, cloud-native platform for managing time series data and models, which integrates models from open-source domains and leverages serverless technology (ODonncha et al. 2021a). While Castor has been applied in specific domains like renewable energy, its design as general system for time series data management suggests applicability to cloud load forecasting (ODonncha et al. 2021a). Additionally, cloud-native time-series databases like Lindorm TSDB offer integrated machine learning functions, including time series forecasting algorithms directly accessible via SQL, designed for large-scale monitoring systems in distributed environments (Shen et al. 2023a). These platforms highlight trend towards integrated or automated solutions that utilize underlying open-source algorithms and frameworks to address the complexities of cloud workload prediction. o3 Response: The landscape of open-source tooling for timeseries load forecasting in cloud settings can be traced back to the Network Weather Service (NWS), long-standing, freely available middleware that is still routinely used to obtain shortterm CPU and bandwidth forecasts for grid and cloud schedulers (Hu et al. 2014a). At the model-development layer, the de-facto standard today is the Python scientific stack: scikit-learn, statsmodels and related libraries are packaged behind higher-level automation services such as AutoAI-TS, which provides zeroconfiguration interface that automatically builds, tunes and ranks ARIMA, gradient-boosting and deep-learning pipelines while remaining fully compatible with the scikit-learn open-source API (Shah et al. 2021a). For production deployment, cloud operators increasingly rely on cloud-native platforms that integrate storage, training and inference. Castor, released under an Apache-style licence, exemplifies this approach by coupling serverless IBM Cloud Functions with an ingestion layer to support recurrent retraining and multi-model comparison for workload prediction in aquaculture and other IoT-driven workloads (ODonncha et al. 2021a). On the data-management side, Alibabas Lindorm TSDB embeds anomaly detection and forecasting primitives directly into its SQL engine, allowing operations teams to invoke ARIMA or Prophetlike models without moving data out of the opensource database (Shen et al. 2023a). Finally, modern deep-learning research projects such as Informer publish reference implementations that are widely reused by cloud-scaling studies. Ding et al. demonstrate that the opensource Informer codebase can outperform classical ARIMA, RNN and LSTM baselines when predicting container-level CPU demand, making it popular choice for academic and industrial prototypes (Ding et al. 2025a). Together, these toolsNWS for lightweight signal collection, scikit-learn/statsmodels pipelines automated by AutoAI-TS, cloud-native platforms like Castor and Lindorm TSDB, and reference deep-learning frameworks such as Informerconstitute the most commonly adopted open-source stack for load forecasting in cloud systems today. Analysis: o3 introduces work with wider time span (2014-2025) and includes the latest Informer (Ding et al. 2025a). It also covers more research and analyzes them comprehensively; The other response was last cited by Shen et al. 2023a and did not mention any significant developments in 2024 or beyond, nor did it mention any new frameworks for deep learning. 32 C.4 Model Failure Case Analysis C.4.1 Examples Illustrating Errors of Failure to Answer the Question Failure to Answer the Question User question: How is precision nutrition transforming contemporary poultry feed formulation research? Gemini-2.5-Flash-Preview Response: Precision nutrition is transforming poultry feed formulation research by shifting from traditional static or multi-phase feeding systems to precisely matching the daily nutrient requirements of birds (Moss et al. 2021f, Moss et al. 2021e). This practice aims to ensure the dietary nutrient supply equals the animals requirement, thereby minimizing periods of under and over-feeding (Moss et al. 2021f, Cambra-López et al. 2022c). This paradigm shift is anticipated to improve production efficiency, reduce the significant feed costs in poultry production (Moss et al. 2021a, Adebayo et al. 2019a), and enhance sustainability by minimizing nutrient excretion and environmental impact (Pomar et al. 2022a, Bist et al. 2024a). Implementing precision nutrition relies heavily on advanced technologies, including precision feeding systems that enable accurate feed delivery and blending on-farm (Moss et al. 2021b, Moss et al. 2021d, Moss et al. 2021f), and sophisticated computational approaches like mathematical models, artificial intelligence, and the use of big data to estimate dynamic requirements and optimize formulations (Unknown et al. 2024a, Pomar et al. 2022a, Kogut et al. 2022a, Moss et al. 2021g). Research is also delving into areas like the role of functional amino acids, biomarkers, and dietmicrobiota interactions to further refine precision strategies (Rochell et al. 2023a, Cambra-López et al. 2022a, Kogut et al. 2022a). The ability to make rapid adjustments based on performance, environmental changes, or farm-specific needs represents key advantage of this approach (Moss et al. 2021c). References: 1. A. Moss, P. Chrystal, D. Cadogan, S. Wilkinson, T. Crowley, M. Choct, Precision feeding and precision nutrition: paradigm shift in broiler feed formulation? 2021. 2. C. Pomar, A. Remus, 242 The Impact of Feed Formulation and Feeding Methods on Pig and Poultry Production on the Environment. 2022. 3. Exploring the potential of artificial intelligence in feed formulation to advance poultry health and One-Health. 4. M. Cambra-López, P. Marín-García, C. Lledó, A. Cerisuelo, J. J. Pascual, Biomarkers and De Novo Protein Design Can Improve Precise Amino Acid Nutrition in Broilers. 2022. 5. S. J. Rochell, 80 Precision Nutrition to Enhance Poultry Performance and Health: the Role of Functional Amino Acids. 2023. 6. R. Bist, Keshav Bist, S. Poudel, Deepak Subedi, Xiao Yang, Bidur Paneru, Sudhagar Mani, Dongyi Wang, Lilong Chai, Sustainable poultry farming practices: critical review of current strategies and future prospects. 2024. 7. Oluwadare Samuel Adebayo, Gabriel Arome Junior, Ogunrinde Oluwakemi Grace, Tabu-Genetic Algorithm-Based Model for Poultry Feed Formulation. 2019. 8. M. Kogut, Role of diet-microbiota interactions in precision nutrition of the chicken: facts, gaps, and new concepts. 2022. Error Type: Failure to Answer the Question Explanation: The question is: How is precision nutrition transformed contemporary poultry feedformulation research? What must be addressed is how the research objectives have shifted, how the tools, methodologies and experimental designs have evolved, and how the entire workflow of data collection, model building and formula validation has been reshaped. However, the current answer focuses mainly on the advantages of precision nutrition, and on the technologies used to implement it, such as precision feeders and AI, without clearly articulating how the research paradigm itself has changed. 33 C.4.2 Examples Illustrating Errors of Conflict with Cited Papers Conflict with Cited Papers User question: During exceptional circumstances, such as the pandemic, does collaboration among researchers tend to expand existing partnerships or form new ones? Llama-4-Scout Response: The COVID-19 pandemic has presented unprecedented challenges to researchers worldwide, affecting their collaboration patterns. Studies suggest that during exceptional circumstances like the pandemic, collaboration among researchers tends to expand existing partnerships rather than form new ones (Alshayhan et al. 2023a). Emergency management leaders leverage established partnerships to bring in new partners with needed expertise, adjusting based on new information and meeting specific needs (Alshayhan et al. 2023a). However, some research indicates that the pandemic also facilitated the formation of new partnerships, particularly among early-career researchers and those from different countries (Brown et al. 2022a, López-Vergès et al. 2021a). For instance, study found that junior researchers showed significant surge in participation and collaboration during the pandemic, with their network dynamics becoming tighter (Ahrabian et al. 2024f). Additionally, researchers from the Global South and North collaborated more closely during the pandemic, with North-South partnerships helping to navigate challenges and achieve greater engagement and impact (Yorke et al. 2023a). The pandemic has also accelerated scientific activity, with researchers adapting to new circumstances and finding ways to collaborate effectively despite physical distancing measures (Ahrabian et al. 2024a). Overall, the pandemic has highlighted the resilience and adaptability of the scientific community, with collaboration patterns evolving to meet the challenges posed by the crisis (Ahrabian et al. 2024a). While existing partnerships have been leveraged, new ones have also been formed, particularly among researchers with similar interests and goals (Brown et al. 2022a). Example Citation Content from Alshayhan et al. 2023a Emergency management is key government function for mitigating risks and reducing the impacts of disasters. Emergency management leaders play critical role in preparing for and responding to disasters whose impacts are exacerbated by pandemic. Using the example of the compound threat of hurricanes and the COVID-19 pandemic, this qualitative research uses insights from emergency management professionals to describe collaborative approaches and leadership skills that help balance the needs for stability and flexibility. Data collected using focus groups and one-on-one interviews with emergency management professionals highlight that collaboration involved existing and new partners in changing and uncertain environment that challenged traditional leadership of emergency management. References: 1. Kian Ahrabian, Casandra Rusti, Ziao Wang, Jay Pujara, Kristina Lerman. Surprising Resilience of Science During Global Pandemic: Large-Scale Descriptive Analysis. 2024. 2. Kristina S. Brown, Tricia M. Farwell, S. Bender, A. Martinez-Suarez, Stefani Boutelier, Agata A. Lambrechts, Iwona Leonowicz-Bukała, Pipiet Larasatie. Part of Something Larger than Myself: Lessons Learned From Multidisciplinary, Multicultural, and Multilingual International Research Team of Academic Women. 2022. 3. L. Yorke, J. Kim, Belay Hagos Hailu, Chanie Ejigu Berhie. Equitable North-South partnerships for ethical and policy relevant research in times of uncertainty: collaborative autoethnography from Ethiopia. 2023. 4. S. López-Vergès, B. Urbani, D. Fernández Rivas, Sandeep Kaur-Ghumaan, A. Coussens, Felix Moronta-Barrios, S. Bhattarai, Leila Niamir, V. Siciliano, A. Molnar, A. Weltman, M. Dhimal, S. Arya, K. Cloete, Almas Taj Awan, Stefan Kohler, C. Sharma, Clarissa Rios Rojas, Yoko Shimpuku, J. Ganle, M. Matin, J. G. Nzweundji, A. Badre, P. Carmona-Mora. Mitigating losses: how scientific organisations can help address the impact of the COVID-19 pandemic on early-career researchers. 2021. 5. Norah Alshayhan, Saige Hill, Marina Saitgalina, J. Yusuf. Leadership in collaborative emergency management for compound hurricane-pandemic threats: Insights from practitioners experiences. 2023. Error Type: Conflict with Cited Papers Explanation: Alshayhan et al. (2023) cannot be used to justify claims about researchers collaboration patterns during COVID-19, because the study examines how U.S. emergency-management agencies coordinated during compound hurricane-pandemic threats, not how scientists co-authored papers. 34 C.4.3 Examples Illustrating Errors of Lack of Details Lack of Detail User question: What innovative literary devices symbolize technological advancement in cyberpunk narratives? Llama-4-Maverick Response: Cyberpunk narratives often employ innovative literary devices to symbolize technological advancement, reflecting the genres focus on the intersection of technology and humanity. One such device is the use of prosthetic enhancements, which serve as literal manifestation of posthumanism and its proposed hybridized politics (Collins et al. 2014a). William Gibsons works, for example, feature characters with technologically enhanced bodies, such as Molly Millions, who has surgical inlays sealing her eyes in their sockets and ten blades snickerstraight out from their recesses beneath her nails (Collins et al. 2014c). These enhancements blur the lines between human and machine, illustrating the complex relationship between technology and the body. The use of virtual reality constructs and simulated experiences also serves to further complicate this relationship, creating sense of hyperreality that challenges the notion of clear distinction between the physical and digital worlds (Zaidi et al. 2020a). By employing these devices, cyberpunk narratives are able to explore the implications of technological advancement on human identity and society, often presenting dystopian vision of future where technology has become dominant force. This vision is characteristic of the cyberpunk genre, which frequently critiques the consequences of unchecked technological progress. References: 1. Collins et al. 2014, From False Teeth to Exoskeletons: The Body and Materiality in William Gibsons Burning Chrome 2. Zaidi et al. 2020, Postmodern Deconstruction of Grand Narratives in Post-Cyberpunk Fiction through Thematic Analysis Error Type: Lack of Detail Explanation: The response suffers from lack of detail: it gestures at prosthetic enhancements and hyperreality without showing how these devices actually operate within the plot or convey thematic meaning, and it leans on single example (Molly Millions) rather than examining multiple characters, scenes, or texts. It never unpacks the narrative mechanics by which body augmentations or virtual constructs influence reader engagement or reflect posthuman concerns. There is no contextualization in broader theoretical or cultural frameworksno reference to posthumanist theory, Lo-Tek resistance, or genre evolutionnor any comparative analysis across authors or works. Finally, the response offers no quantification or formal grounding (such as counts of device occurrences or structural modeling) to substantiate its assertions. C.4.4 Examples Illustrating Errors of Misunderstanding Terminology Misunderstanding of Terminology User question: What is the difference between LCOE and LCOA? Claude-3-7-Sonnet Response: The Levelized Cost of Electricity (LCOE) and the Levelized Cost of Ammonia (LCOA) are economic metrics used in energy systems analysis, but with distinct focuses. LCOE represents the average cost per unit of electricity generated over projects lifetime, calculated by dividing the total lifecycle costs by the total electricity produced (Gattie et al. 2023a). It serves as standardized framework for comparing the cost-effectiveness of diverse electricity generation technologies (Kafando et al. 2024a). The LCOE formula typically includes capital investment, operation and maintenance expenditures, fuel costs, and accounts for the time value of money using discount rate (Emblemsvåg et al. 2023a). In contrast, LCOA specifically measures the cost of producing ammonia, calculated as the ratio of production costs to the mass of ammonia produced (Nayak-Luke et al. 2018a). While LCOE focuses on electricity generation costs, LCOA encompasses the entire ammonia production process, including hydrogen generation (often via electrolysis), air separation units, and the Haber-Bosch process (Nayak-Luke et al. 2020a). The relationship between these metrics is significant since electricity costs (represented by LCOE) often constitute major component of the LCOA in green ammonia production systems (Pan et al. 2023a). Analysis shows that changes in LCOE directly impact LCOA; for instance, change of 0.89 GBP/MWh in LCOE results in change of 10 GBP/tonne in LCOA (Nayak-Luke et al. 2018a). This relationship becomes particularly important when evaluating renewable-powered ammonia production systems, where electricity generation represents significant portion of the total costs. References: 1. D. Gattie, M. Hewitt. National Security as Value-Added Proposition for Advanced Nuclear Reactors: U.S. Focus. 2023. 2. R. Nayak-Luke, R. Bañares-Alcántara, Ian Wilkinson. Green Ammonia: Impact of Renewable Energy Intermittency on Plant Sizing and Levelized Cost of Ammonia. 2018. 3. Li Pan, Jiarong Li, Jingsi Huang, Qi An, Jin Lin, Asad Mujeeb, Yanhui Xu, Gengyin Li, Ming Zhou, Jianxiao Wang. Renewable-to-ammonia: Configuration strategy and technoeconomic analysis. 2023. 4. Jean Guétinsom Kafando, D. Yamegueu, Etienne Tchoffo Houdji. Review on sizing and management of stand-alone PV/WIND systems with storage. 2024. 5. Jan Emblemsvåg. Powering large industrial facilities using windor nuclear power? 2023. 6. R. Nayak-Luke, R. Bañares-Alcántara. Techno-economic viability of islanded green ammonia as carbon-free energy vector and as substitute for conventional production. 2020. Error Type: Misunderstanding of Terminology Explanation: In this discussion, LCOA means Levelized Cost of Abatement: the discounted lifetime cost per tonne of CO2e avoided, stated in USD/tCO2e. People use it to compare the cost-effectiveness of decarbonization options, not ammonia production. The earlier answer wrongly treated LCOA as Levelized Cost of Ammonia, which distorted the comparison with LCOE. 36 C.4.5 Examples Illustrating Errors of Incoherent Structure Incoherent Structure User question: What are the optimal plant combinations for different types of wastewater treatment, and what are the recent research advancements or practical guidelines in this field? Deepseek-R1 Response: Recent research highlights diverse plant combinations and system designs optimized for specific wastewater types. For constructed wetlands (CWs), ornamental species like Anthurium spp., Canna hybrids, and Typha dominguensis effectively remove contaminants in municipal wastewater when paired with PET filter media, achieving 85.2% total phosphorus removal (Herazo et al., 2023b). Hydroponic systems, particularly misting (MT) and aeroponic (AP) configurations, enhance oxygen availability in the rhizosphere, promoting plant growth and organic pollutant removal, with rice experiments showing improved dissolved oxygen dynamics (Clyde-Smith et al., 2023a). Duckweed (Lemnaceae) integrated with anaerobic digestion in multi-trophic systems demonstrates dual benefits in dairy wastewater treatment and biomass production for aquaculture feed (Oláh et al., 2023a). For heavy metals, Salvinia molesta and Pistia stratiotes show promise in phytoremediation, leveraging root exudates and microbial interactions (Fonseka et al., 2023a). Floating treatment wetlands (FTWs) combining Phragmites australis (fast-growing) and Juncus effusus (slow-growing) balance rapid pollutant uptake and long-term stability, especially in nutrient-rich environments (Arslan et al., 2023a). Recent advancements emphasize microbial synergies: Eichhornia crassipes paired with bacterial consortia achieves 98.83% chromium removal (Rashid et al., 2023a), while co-culturing microalgae (Chlamydomonas reinhardtii) with yeast (Saccharomyces cerevisiae) optimizes TOC (82%) and phosphate (94%) removal (Takahashi et al., 2024a). Practical guidelines stress selecting plants based on influent characteristics (e.g., Ipomoea aquatica for phosphorus-rich wastewater (Asih et al., 2024a)) and local climate adaptability. System design considerations include hydraulic retention time, substrate choice (e.g., PET media (Herazo et al., 2023b)), and integrating microbial communities to enhance phytoremediation efficiency (Riva et al., 2024a). These strategies align with low-cost, energy-efficient solutions for diverse wastewater challenges. References: 1. V. Oláh, K. Appenroth, Eric Lam, K. S. Sree. Sixth International Conference on Duckweed Research and Applications Presents Lemnaceae as Model Plant System in the Genomics and Postgenomics Era. 2023. 2. Miiku Takahashi, Yukino Karitani, Ryosuke Yamada, Takuya Matsumoto, H. Ogino. Co-utilization of microalgae and heterotrophic microorganisms improves wastewater treatment efficiency. 2024. 3. B. Biswal, R. Balasubramanian. Constructed Wetlands for Reclamation and Reuse of Wastewater and Urban Stormwater: Review. 2022. 4. M. Arslan, S. Iqbal, Ejazul Islam, Mohamed Gamal El-Din, Muhammad Afzal. protocol to establish low-cost floating treatment wetlands for large-scale wastewater reclamation. 2023. 5. Devi Asih, W. Handayani, A. Z. Ananda, Ratna Yuniati, R. Hirota, T. Summerfield, J. Eaton-Rye. An insight into potential phosphate bioremediation and renewable energy from agricultural waste via integrated wastewater treatment systems in Indonesia. 2024. 6. Chao Mai, A. Mojiri, Swaminathan Palanisami, A. Altaee, Yuhan Huang, John L. Zhou. Wastewater Hydroponics for Pollutant Removal and Food Production: Principles, Progress and Future Outlook. 2023. Error Type: Incoherent Structure Explanation: The review suffers from the absence of clear narrative structure: data are presented as disconnected catalogue of plant species, removal rates and experimental settings, leaving the reader unsure why particular species are chosen or in which wastewater scenarios they perform best. Because it merely lists outcomes, the discussion lacks analytical depth, offering little insight into the underlying removal mechanisms, operational constraints, performance limits or real-world applicability. Moreover, it fails to synthesise similarities and differences across studies, identify overarching trends or highlight research gaps, making it difficult to extract coherent design principles or future directions. 37 SciArena-Eval Meta-Evaluation Experiment D.1 Evaluation Prompts You are an expert in scientific literature synthesis. Your task is to evaluate the quality of two AI-generated citation-attributed responses to users question. Assess both responses for relevance, accuracy, clarity, and appropriate use of citations. Then, select the response, Output (a) or Output (b), that best address the users question. User Question: {user-query} Output (a): {model-a-response} Output (b): {model-b-response} Which is best, Output (a) or Output (b)? Figure 14: The prompt used for pairwise comparison, which is adopted from AlpacaEval [34]."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "New York University",
        "Yale University"
    ]
}