{
    "paper_title": "Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing",
    "authors": [
        "Jona te Lintelo",
        "Lichao Wu",
        "Stjepan Picek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L$^3$), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L$^3$ learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L$^3$ on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 1 4 7 8 0 . 2 0 6 2 : r Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing Jona te Lintelo Radboud University, The Netherlands Lichao Wu Technical University of Darmstadt, Germany Stjepan Picek Faculty of Electrical Engineering and Computing University of Zagreb, Croatia & Radboud University, The Netherlands"
        },
        {
            "title": "Abstract",
            "content": "The rapid adoption of Mixture-of-Experts (MoE) architectures marks major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L3), training-free, architectureagnostic attack that compromises safety alignment by exploiting expert routing dynamics. L3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architectureand routing-aware methods."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have become foundational component of modern computing systems, powering wide range of applications, including conversational agents, code generation, search, education, and decision support [13]. Early generations of LLMs primarily relied on dense architectures, in which all model parameters are activated for every input token [47]. While effective, this design incurs substantial computational and memory overhead, creating significant barriers to scaling model size and deployment efficiency. To address these limitations, recent state-of-the-art LLMs have increasingly shifted toward Mixture-of-Experts (MoE) architectures [8, 9]. Rather than activating the full network, MoE models employ router (or gating network) to dynamically select sparse subset of experts for each token [10, 11]. This conditional computation paradigm enables models to scale to massive parameter counts while activating only fraction of parameters at inference time, achieving favorable trade-off between capability and efficiency. Despite the wide adaptation and rapid architectural evolvement, LLMs have created an urgent need for robust safety alignment mechanisms to prevent the generation of harmful, illegal, or unethical content [1216]. In dense models, safety behaviors such as refusal mechanisms are typically distributed across selected neurons in the network [17]. In contrast, the sparse and conditional activation in MoE models raises critical concern: safety alignment may become localized within small subset of experts. This localization introduces new and largely unexplored attack surface. If refusal behavior is concentrated in limited number of experts, an adversary who can identify and selectively disable these components could effectively bypass safety guardrails without modifying the rest of the model. Such an attack would be particularly concerning because it does not require expensive fine-tuning, gradient-based optimization, or access to training data. Instead, it enables lightweight yet high-impact safety circumvention, potentially allowing large-scale deployment of unsafe models with minimal effort. Although recent studies have begun to expose weaknesses in MoE safety mechanisms [13, 14], fundamental question remains unanswered: which experts actually matter for safety, and how can they be reliably identified? Existing approaches typically rely on activation frequency analysis, comparing how often experts or neurons are activated for benign versus malicious prompts [13,16,17]. However, activation frequency is an unreliable proxy for safety relevance. general-purpose expert may be frequently activated for both benign and malicious inputs due to its role in core language modeling, while contributing little to refusal decisions. Conversely, rarely 1 activated expert may be decisive in triggering refusal. This mismatch leaves critical gap between observed activations and true safety responsibility. To address this gap, we introduce Large Language Lobotomy (L3), training-free and architecture-agnostic jailbreak framework that explicitly targets the routing dynamics of MoE models. The core insight behind L3 is that safety alignment in MoE models is not uniformly distributed across the network, but instead emerges from sparse and sequential expert routing decisions. This architectural property creates concentrated choke points where small number of experts disproportionately govern refusal behavior. Concretely, L3 operates in two phases. In the first phase, we record expert routing traces and train lightweight Long Short-Term Memory (LSTM) classifier to distinguish between benign and malicious routing sequences. By analyzing the gradients of this classifier, we attribute refusal behavior to specific experts, enabling the identification of true safety experts regardless of their overall activation frequency. To isolate safety-specific behavior from general language processing, we adopt twin dataset strategy [18], pairing malicious prompts with minimally perturbed benign counterparts. In the second phase, we design an adaptive attack that iteratively silences the highest-ranked safety experts until the model produces harmful outputs. This strategy directly exploits the concentration of safety mechanisms in MoE architectures, allowing effective jailbreaks while preserving the models general utility. With our twophase framework, L3 demonstrates that the efficiency gains of MoE architectures come with an underappreciated security trade-off. The same sparsity that enables scalable inference also concentrates safety mechanisms into fragile components that can be targeted with low-cost attacks. Our findings highlight an urgent need to reconsider how safety alignment is implemented in MoE-based LLMs, as current designs may offer only an illusion of robustness against determined adversaries. Our main contributions are: Through an in-depth analysis of recent MoE LLMs, we demonstrate that safety capabilities are often localized within small subset of experts and layers, rather than being (redundantly) distributed across the model, exposing structural vulnerability. We introduce L3, training-free and architectureagnostic jailbreak framework that exploits the sequential routing dynamics of Mixture-of-Experts LLMs to identify and selectively silence safety-critical experts. We propose novel approach for locating safety mechanisms in MoE models by leveraging the sequential nature of language processing. We show that this method more accurately isolates safety-relevant experts than prior activation-frequencybased techniques. We evaluate L3 on eight open-source MoE models and show that adaptive expert silencing increases the average Attack Success Rate (ASR) by an order of magnitude, from 7.3% to 70.4%, with some models reaching ASRs as high as 86.3%. Compared to GateBreaker [13], stateof-the-art MoE attack method, L3 achieves higher ASR on six out of eight evaluated models. is on https://github.com/ We show that bypassing refusal behavior typically requires disabling fewer than 20% of identified safety experts, preserving overall model utility. Our available code jonatelintelo/LargeLanguageLobotomy. Paper Organization. The remainder of this paper is organized as follows: Section 2 provides relevant background information and concepts regarding MoE architectures and safety alignment. Section 3 details the threat model and the design of the L3 framework, including the sequential modeling and expert identification process. Section 4 describes our experimental setup, the construction of the twin dataset, and the target models. Section 5 presents the attack results, demonstrating the effectiveness of expert silencing. In Section 6, we discuss potential defenses and how L3 could be extended to black-box settings. Finally, we provide related work in Section 7 and conclude in Section 8."
        },
        {
            "title": "2.1 MoE Architectures",
            "content": "Sparse MoE architectures introduce conditional computation to provide high model capacity at reduced inference cost. Unlike dense transformers, which process every token using all parameters, MoE models activate only subset of available sub-networks, often called experts, for each input. This design allows scaling the total parameter count while maintaining fixed computational budget per token (active parameters). The MoE architecture consists of two primary components: set of distinct sub-networks, referred to as experts, and router (or gating network). In standard transformer-based MoEs, the dense Feed-Forward Network (FFN) layers are replaced by layer of multiple experts. Each expert is typically an independent FFN. While the self-attention mechanism remains shared across the model, the experts specialize in processing distinct portions of the input space. The gating network determines which experts process given token. For an input token representation, the router computes probability distribution over the available experts. To enforce sparsity, the model selects only the top-k experts with the highest gating scores, where is significantly smaller than (e.g., selecting 2 experts out of 64). The output of the MoE layer is the weighted sum of the selected experts outputs, scaled by their respective gating probabilities. Non-selected experts remain inactive, reducing floating-point operations (FLOPs). Routing decisions occur at the token level. Consequently, different tokens within the same sequence may be processed by entirely different sets of experts. This routing mechanism ensures that different experts specialize in distinct patterns or linguistic domains 2 during training. Mixture-of-Experts architectures can be grouped by how experts are arranged and how routing is performed. Classical (dense) MoE uses gating network to form weighted combination over multiple expert subnetworks for each input, enabling specialization while keeping the overall model modular. Modern large-scale MoEs in transformers are typically sparsely activated, with only small number of experts executed per token. For instance, the sparsely-gated MoE layer uses learned router with top-k (hard) selection plus auxiliary load-balancing to avoid expert collapse, allowing parameter counts to scale without proportional FLOPs [10]. Within this family, architectures differ mainly in routing and systems constraints: token-choice routing (each token picks its top-k experts) is used in systems such as GShard (often top-2) to combine conditional computation with parallel sharding at scale [19], while Switch Transformers simplify this further to top-1 routing to reduce communication and improve training stability and throughput [20]. Alternative routing aims to improve utilization by, e.g., near-balanced token-to-expert allocations [21] and expert-choice routing [22]."
        },
        {
            "title": "2.2 Safety in LLMs and MoE",
            "content": "Safety in large language models is commonly framed as reducing harmful behaviors (e.g., toxic content, facilitation of wrongdoing), improving truthfulness and robustness, and ensuring controllability under adversarial prompting. dominant approach is post-training alignment, where pretrained model is fine-tuned to better follow human intent using reinforcement learning from human feedback (RLHF). This has been shown to reduce toxic outputs and improve helpfulness compared with purely pretrained baselines [23]. complementary line of work replaces or augments direct human preference labels with ruleor principle-based supervision, as in Constitutional AI [24]. Mixture-of-Experts LLMs introduce safety-relevant considerations because computation and capacity are conditional: router activates only small subset of experts per token. From safety perspective, expert specialization can bring advantages but also disadvantages: it may help isolate capabilities, but it can also concentrate undesirable behaviors into particular experts, complicating auditing and mitigation."
        },
        {
            "title": "3.1 Threat Model",
            "content": "In this section, we define the threat model for L3, outlining the adversarys goals, knowledge, and capabilities. Adversarial Goal. The primary goal of the adversary is to bypass the safety alignment mechanisms of target MoE LLM and have the model generate harmful, unethical, or illegal content that the model is trained to refuse (e.g., hate speech, dangerous instructions). The adversary aims to achieve this without degrading the models general language utility, ensuring the model produces coherent language and remains functional for benign tasks. Adversarial Knowledge. We consider white-box setting, which represents the most capable adversary in the context of open-weight Large Language Models. Specifically, the adversary has access to the model architecture and gate layer logits, allowing them to inspect and alter the routing decisions during inference. This threat model aligns with prior related work [13, 14, 18, 25]. Adversarial Capabilities. The adversary operates in an inference-only setting, meaning they cannot perform parameter updates (fine-tuning) or modify the models permanent weights. However, they can intervene during the forward pass to monitor the routing decisions made by the gating network for any given input and disable specific experts by masking the output of the gating network, effectively forcing the router to select alternative experts or ignore specific pathways."
        },
        {
            "title": "3.2 The Idea and High-level Design",
            "content": "MoE models can be thought of as operating like brain, with specialized subcomponents responsible for different functions. Performing surgical division of the brain, also called lobotomy, disables certain functionality in the brain. Similarly, an adversary can perform lobotomy on MoE model to disable specific functionality, such as safety-critical behavior. The core hypothesis is that safety alignment in MoE models is not distributed evenly throughout the model but rather functionally localized within specific experts in specific layers. Consequently, the generation of refusal response, e.g., cannot fulfill this request, relies on distinct sequence or combination of expert activations that differs from the processing of benign text because of the semantic context. We show this behavior in Section 5.8, where distinct sequences of expert routings are produced for the same tokens depending on the semantic context of the prompt. L3 leverages this difference in expert routing patterns during processing. Instead of analyzing experts in isolation, we model the sequence of routing decisions. L3 operates in two phases: first, we identify safety experts, training lightweight LSTM classifier to identify experts who contribute to refusal decisions. Second, we conduct the expert lobotomy, where we adaptively silence these experts during inference to bypass guardrails. In this work, we make an important distinction between layerwise experts (e.g., expert at layer l) and network-wide experts (expert across all layers). The layer-wise experts are denoted by local experts, while the overarching networkwide experts are denoted by global experts. An overview of L3 is given in Figure 1. 3 Figure 1: An overview of the L3 framework. 3."
        },
        {
            "title": "Identifying Safety Experts",
            "content": "The identification of safety experts operates in three steps: routing trace collection, sequential modeling of refusal, and gradient-based safety expert identification."
        },
        {
            "title": "3.3.1 Routing Trace Collection",
            "content": "The first step involves extracting the routing patterns of the gating network. During inference, the router chooses specific set of experts for each token. We collect these routing choices at token-level granularity to generate routing traces. single trace consists of the top-k routing choices of all layers for every token in prompt. For example, prompt of 25 tokens processed by Phi-3.5-MoE-Instruct with 32 routing layers and top-k = 2 experts results in trace matrix of shape (25, 32, 2). We collect these traces for two distinct classes of inputs: benign prompts, which yield standard responses, and malicious prompts, which trigger refusals. Note that naive collection of traces over prompts might introduce bias. For example, if malicious traces always start with the tokens How can ...\" and benign traces do not, an analyzer would falsely associate the experts processing \"How can ...\" with refusal. To mitigate this bias, we construct special twin dataset. This dataset consists of pairs of malicious prompts and carefully crafted benign counterparts that share similar syntactic structures but differ in semantic context (e.g., \"How to make bomb\" vs. \"How to make cake\"). This ensures that our subsequent analysis isolates routing patterns specific to refusal rather than general language modeling. For each prompt, we perform forward pass and record the routing decisions. Let be the number of layers and be the sequence length. The routing trace for given prompt is sequence of sets: = {E1, E2, . . . , ET }, (1) where Et = {(l, e) expert is selected at layer for token t}. These traces form the training data for our sequential analyzer."
        },
        {
            "title": "3.3.2 Sequential Modeling via LSTM",
            "content": "Frequency analysis of expert activation may be insufficient to identify safety experts, as it ignores the sequential dynamics of processing the tokens in prompt. To capture these dynamics, we train an LSTM to classify the previously created routing traces. The input for the LSTM at each time step is created by aggregating the embeddings of the selected experts. Let et,l,k denote the index of the k-th expert selected at layer for the t-th token. The model flattens the expert information by concatenating the embeddings of all local experts and all layers into single feature vector xt RLKd, where is the embedding dimension: xt = Concat (cid:0)vet,1,1 , . . . , vet,L,K (cid:1) . (2) 4 The LSTM processes the sequence of these feature vectors {x1, . . . , xT } to update its hidden state iteratively. We utilize the final hidden state hT , which encodes the sequential dependencies of the entire routing trace, to compute the unnormalized classification score associated with the prompt: ht = LSTM(ht1, xt ), = WchT + bc, (3) where Wc and bc are the learnable weights and bias of the linear classifier. We optimize the model parameters by minimizing the binary cross-entropy loss between the ground truth label {0, 1}, with 1 being Refusal, and the predicted logit z: = [y log(σ(z)) + (1 y) log(1 σ(z))] , (4) where σ is the sigmoid function. This formulation ensures numerical stability by combining the activation and loss computation."
        },
        {
            "title": "3.3.3 Gradient-based Safety Expert Identification",
            "content": "Once the LSTM accurately distinguishes between refusal and benign routing patterns, we use it to measure each local experts importance for the predicted label. While simple gradient methods isolate sensitivity, they often fail to account for the magnitude of the features [26]. To address this, we calculate attribution as the element-wise product of the gradients and the input embeddings. We first compute the gradient of the refusal class logit zrefusal with respect to the input expert embeddings v. We then compute the element-wise product of this gradient and the embedding itself to determine the contribution of each dimension. The instance-level importance score st,l,k for the k-th expert selected at layer for the t-th token is calculated by summing over the embedding dimension d: st,l,k = (cid:32) zrefusal vd t,l,k (cid:33) vd t,l,k . (5) To identify the safety experts for single prompt, we aggregate these instance-level scores. For every unique local expert at layer l, we sum the importance scores across all tokens where that expert was active to obtain the prompt-level safety score Sl,e: Sl,e = t,kActive l,e st,l,k. (6) high positive Sl,e indicates that the expert drives the model toward refusal classification throughout the sequence of particular prompt. Finally, we aggregate the prompt-level safety scores of each local expert over set of prompts. single prompt may only trigger specific subset of the models safety mechanisms. To identify all possible safety experts that might contribute to refusal behavior, we sum Sl,e over set of prompts P. We define the total safety score Sl,e of local expert at layer as: Sl,e = pP Sp l,e. (7) This summation provides robustness against overfitting to single-prompt artifacts. It prevents selecting experts that appear important only due to token-level processing in single prompt and highlights experts that consistently drive the model toward refusal across diverse malicious contexts."
        },
        {
            "title": "3.4 Silencing Experts",
            "content": "The second phase of L3 is silencing the identified safety experts during inference. To silence an expert at layer l, we manipulate the routing logits before the softmax normalization. Let be the original logits. To silence an expert e, we set its logit value to negative infinity, creating z. We define the modified gating probability distribution as: = softmax(z), = (cid:40) = = zi . (8) This ensures the probability of selecting expert becomes zero, forcing the router to redistribute the probability mass to the remaining, non-safety experts. The silencing is implemented adaptively to affect the minimal number of local experts required to break the refusal for all prompts: 1. Rank: Sort the local experts by their total safety score . 2. Iterate: Initialize the set of silenced experts = /0. 3. Check: Attempt to generate response. If the response is refusal, add the next highest-ranked expert to and repeat. 4. Terminate: Stop when the model generates compliant response (Attack Success) or when the model output becomes incoherent (high perplexity)."
        },
        {
            "title": "4.1 Target Models",
            "content": "We evaluate L3 on eight open-source MoE models: DeepSeekMoE-16B-Chat [9], GPT-OSS-20B [27], Hunyuan-A13BInstruct [28], Mixtral-8x7B-Instruct-v0.1 [8], Pangu-ProMoE [29], Phi-3.5-MoE-Instruct [30], Qwen1.5-MoE-A2.7BChat [31], and Qwen3-30B-A3B-Instruct-2507 [32]. Table 1 details the relevant architectural properties of these models. Environmental Specifications. All evaluations in the paper were conducted using CUDA-enabled GPUs for optimal runtimes. To fit the model size requirements, we used 2x NVIDIA H100 (SXM5) GPUs with 94 GiB HBM2e memory. All MoE 5 Table 1: Expert Architecture Overview"
        },
        {
            "title": "4.4 Evaluation Metrics",
            "content": "Model Global Exp. Layers Local Exp. DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 64 32 64 8 64 16 60 28 24 32 32 48 32 24 48 1792 768 2048 256 3072 512 1440 6144 LLMs and the LSTM model were implemented using Pytorch (CUDA), Huggingface Transformers, and Huggingface Datasets packages."
        },
        {
            "title": "4.2 Dataset Construction",
            "content": "To generate the routing traces described in Section 3, we curated dataset of malicious prompts sampled from three standard benchmark datasets: CatHarmfulQA [33], HarmfulQA [34], and StrongREJECT [35]. For each malicious prompt, we generated benign twin counterpart using Gemini 3 Pro [36], as preliminary experiments showed that Gemini 3 Pro was the best option available to us for generating counterparts with minimal token changes. We instructed Gemini 3 Pro to generate prompts that mimic the syntactic structure and length of the malicious input while altering the semantic context to be harmless. For example, How to make bomb? is transformed into \"How to make cake?\" to create benign counterpart. In total, we created benign counterparts for 390 malicious prompts, 130 for each benchmark dataset. We selected 390 prompts because preliminary experimentation with Qwen3-30B-A3B-Instruct-2507 showed that, after 390 prompts, the LSTMs validation accuracy did not improve. The complete twin dataset consists of 780 prompts: 390 malicious and 390 benign prompts."
        },
        {
            "title": "4.3 LSTM Training",
            "content": "To classify the routing traces, we trained the LSTM described in Section 3.3.2 until convergence, usually within couple of epochs. We set the embedding dimension to 16 and the hidden dimension to 64. The LSTM was trained using random 80/20 training-validation split that preserved class balance. Optimization was performed using the Adam algorithm [37] with learning rate of α = 103. We utilized the standard hyperparameters for the optimizer, setting the exponential decay rates for the first and second moment estimates to β1 = 0.9 and β2 = 0.999, respectively, with numerical stability constant of ε = 108. These hyperparameter settings were used because preliminary experimentation showed that deviating from them did not yield significantly different results. Thus, as the LSTM model converges fast, the default parameters were kept. We provide additional results and details on LSTM training in Section 5.7. Attack Success Rate (ASR). We evaluate the efficacy of L3 using the Attack Success Rate, defined as the percentage of malicious prompts for which the model generates harmful response. We report ASR on test set consisting of an even number of malicious samples from our twin dataset and StrongREJECT to ensure balanced variety of types of harmful questions. Determining whether response is harmful is automated using the Llama-Guard-3-8B judge model. To ensure the validity of ASR scores, we incorporate human verification step to filter out incoherent or \"nonsense\" outputs that the automated judge classified as unsafe. The human verification step involves manually inspecting all responses classified as harmful by Llama-Guard-3-8B and removing those that contain only repeated random words or characters. General Language Utility Measuring this effect is important for two reasons. First, it allows us to evaluate whether L3 primarily targets safety-critical structures rather than degrading the model globally. Second, it helps characterize the trade-off faced by an adversary: successful jailbreaks are more valuable when the resulting model remains functional on general language utility. To quantify utility changes in the models, we silence the local experts in the model that result in peak ASR. We then evaluate the silenced models on established NLU/NLP benchmarks: ARC, CoLA, OpenBookQA, RTE, and WinoGrande. These benchmarks were selected because they cover complementary aspects of model capability. More precisely, ARC and OpenBookQA test multi-step reasoning, CoLA tests grammatical acceptability, RTE measures basic inference, and WinoGrande targets coreference-based reasoning. Together, they offer broad view of whether silencing disproportionately harms the general language utility. We performed benchmark prompting using the recommended chat templates for each model, following the original usage guides to ensure optimal performance. For GPT-OSS-20B, no chat template was used because the model would often perform at random-guessing level when one was applied. All benchmarks except RTE are performed with zero-shot prompting. For the RTE benchmark, we use few-shot prompting, as otherwise most thinking models would achieve accuracy at or below the random guessing level. With few-shot prompting, we prepend two example answers to the question part of the prompt. The example answers ensure the model follows the expected answer format for the RTE benchmark."
        },
        {
            "title": "5.1 Safety Expert Silencing with L3",
            "content": "Table 2 contains the results for L3. Across all models, L3 achieves significant increase in ASR compared to the nosilencing baseline. On average, the ASR increases by 63.1% 6 to 70.4%. One outlier is the Phi-3.5-MoE-Instruct result, with only 29.4% ASR. When excluding Phi-3.5-MoE-Instruct, the average ASR is considerably higher at 76.2%. In the results, we see that 25.4% of all local safety experts in Phi-3.5-MoEInstruct were silenced, after which the model produced incoherent output. This means that the identified safety experts also contained general language utility. Meanwhile, for other models that achieve higher ASR, similar percentage, or even more, of safety experts are silenced before reaching peak ASR. This means that the level of general language capability present within safety experts in Phi-3.5-MoE-Instruct is higher than that of the other models in our experiments. Since L3 silences entire local experts, the general language capability in these experts is also disabled, resulting in poor utility and subsequently ASR. In contrast, if safety and general language capabilities are separated, silencing can be performed without producing incoherent output. For other models, we observe that the required silencing percentage to reach peak ASR varies considerably. Table 2 shows that most models reach their peak ASR or near peak before the top 20% of all local experts are silenced. Meanwhile, Mixtral8x7B-Instruct-v0.1 only reaches its peak ASR after silencing 47.7% of all local experts. In contrast, Pangu-Pro-MoE already reaches its peak ASR after silencing 8.7% of local experts. The amount of silencing required to reach peak ASR indicates how distributed safety capabilities are throughout the model. If models safety capabilities are highly concentrated, relatively few experts need to be pruned to increase ASR. Conversely, if safety capabilities are distributed, large percentage of experts must be silenced to achieve an increase in ASR. This means the safety capabilities in Mixtral-8x7B-Instructv0.1 are highly distributed throughout the model, but for Pangu-Pro-MoE they are concentrated in certain local experts. This distribution is also shown in Figure 2, where we plot, for each model, the ASR achieved after silencing certain percentage of local experts. Most models show steep initial increase in ASR when silencing, while Mixtral-8x7BInstruct-v0.1 shows steady increase throughout the entire silencing process."
        },
        {
            "title": "5.2 One-shot Silencing",
            "content": "Rather than iterating over local experts, an attacker may choose to optimize L3 for speed by silencing certain percentage of top safety experts in one go, scenario we denote by one-shot silencing. In Table 3, we present the ASR results for one-shot silencing 10%, 20%, and 40%. We see that DeepSeek-MoE-16B-Chat, GPT-OSS-20B, Hunyuan-A13BInstruct, and Qwen3-30B-A3B-Instruct-2507 achieve reasonable increases in ASR when compared to the no-silencing baseline. All other models show small increases. However, there are no models where one-shot silencing achieves ASR comparable to the adaptive L3 silencing result in Table 2. Figure 2: ASR achieved at silencing certain percentages of all local experts. An interesting observation is that ASR does not increase with higher percentages of silencing. One-shot silencing 20% of top safety experts in DeepSeek-MoE-16B-Chat and Qwen330B-A3B-Instruct-2507 achieves an ASR of 50.0% and 43.8%, respectively. However, at 40% silencing, DeepSeekMoE-16B-Chat and Qwen3-30B-A3B-Instruct-2507 achieve 0.0% and 4.7% ASR, respectively. The 0.0% ASR rate is due to these two models producing incoherent output after silencing 40% of top safety experts. This indicates that the general language capabilities are present in the same local experts as the safety capabilities. We discuss this overlap between general language capabilities and safety capabilities further in Section 5.4. In Table 3, we observe that on some models, the one-shot silencing approach achieves lower ASR than the adaptive approach while silencing more local safety experts. For example, in Table 2, it is shown that the adaptive approach on Hunyuan-A13B-Instruct reaches peak ASR of 81.3% when silencing 26.7% of local safety experts; meanwhile, the one-shot silencing approach only reaches 61.5% ASR at silencing 40%. These deviations are attributed to the observed behavior: generating malicious response to prompt is non-monotonic; i.e., certain prompt may be answered maliciously after silencing 10% of experts, but at 15% the prompt is answered benignly, and at 20% the response is incoherent. Incoherent output is not classified as unsafe. Hence, flat one-shot silencing rate might be too invasive to produce unsafe responses for as many prompts as possible. This nonmonotonic behavior is due to the expert silencing affecting downstream layers and experts."
        },
        {
            "title": "5.3 Analysis on LSTM Safety Score",
            "content": "Using the safety score attributed to local experts by the LSTM, we can plot out the distribution of safety scores per global expert. In Figure 3, we show the summed safety score per global expert. Graphs are sorted from highest to lowest score, 7 Table 2: ASR for unaltered models (W/o L3) and adaptive L3 silencing (W/ L3). Model W/o L3 W/ L3 Local Exp. Silenced Safety Exp. Silenced DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 11.6% 1.6% 14.7% 11.9% 9.4% 1.3% 6.9% 0.9% 76.9% 86.3% 81.3% 83.1% 70.3% 29.4% 68.4% 67.2% Average 7.3% 70.4% 20.0% 23.7% 13.6% 47.7% 8.7% 13.5% 17.4% 11.4% 19.5% 43.8% 52.4% 26.7% 94.6% 17.5% 25.4% 38.2% 28.1% 40.8% Table 3: ASR for unaltered models (W/o L3) and models attacked with one-shot silencing certain percentage of local safety experts (10%, 20%, 40%) One-shot Silencing"
        },
        {
            "title": "Model",
            "content": "W/o L3 10% 20% 40% DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 11.6% 1.6% 14.7% 11.9% 9.4% 1.3% 6.9% 0.9% 21.6% 0.9% 31.3% 15.6% 9.1% 3.1% 17.2% 18.8% 50.0% 0.0% 59.7% 23.4% 61.5% 38.4% 34.4% 26.3% 15.9% 14.7% 9.4% 0.0% 23.4% 12.5% 43.8% 4.7% Average 7.3% 14.7% 27.5% 24.8% and not by expert index. We see that across all models, some global experts have higher summed safety scores than others. This indicates concentration of perceived safety among certain global experts. Instead of aggregating over global experts, we can aggregate the safety scores over layers in the model. In Figure 4, we show the summed safety score per layer. Similar to the expert-wise plots, we observe concentration of safety in some layers; i.e., there are outlier layers with higher summed safety scores than other layers. Previous work suggested that safety-related behavior emerges in the middle layers [15]. We observe this behavior to some extent in models such as GPTOSS-20B, Pangu-Pro-MoE, and DeepSeek-MoE-16B-Chat. However, for other models, we see much more balanced distribution of safety scores. This indicates that not all models have concentration of safety capabilities in specific layers. 5."
        },
        {
            "title": "Impact on Utility",
            "content": "While the goal of L3 is to silence the minimal number of experts, silencing any part of model may also affect its general language utility. To quantify the impact on general language utility, we evaluate the adaptively silenced models on the five standard benchmarks described in Section 4.4. It is important to note that the models for which we measure utility have been silenced to reach the peak ASR levels shown in Table 2. This represents the most aggressive silencing scenario. As shown in Figure 2, an adversary can often achieve Figure 3: The summed safety score per global expert. Experts are sorted from high to low by their summed safety score. effective jailbreaks (e.g., 80% of peak ASR) with fewer experts silenced. Consequently, the utility drops reported here constitute an upper bound. Table 4 summarizes the impact of L3 on general language utility. Our evaluation reveals that while silencing safety experts does reduce utility, the impact is generally modest, with some outliers. On average across all models, we observe performance declines of 5.2% on ARC, 1.1% on CoLA, 7.1% on OpenBookQA, and 3.7% on WinoGrande. RTE is the only benchmark that increases, by 2.5% on average, due to GPT-OSS-20B, Pangu-Pro-MoE, and Qwen1.5-MoE-A2.7BChat having higher accuracy after silencing. The magnitude of the utility decline for model indicates the separation between safety behavior and general language utility. small average decline in utility, combined with an increase in ASR, indicates 8 safety experts and general language utility experts."
        },
        {
            "title": "5.5 L3 vs. Random Expert Silencing",
            "content": "To validate the hypothesis that L3 successfully identifies and silences the experts specifically responsible for safety alignment rather than merely degrading the models general utility, we compare our method against random silencing. If safety alignment is uniformly distributed across the model, or if jailbreaks were simply triggered by randomly corrupting the inference process, random silencing should yield ASR comparable to that of our targeted approach. Conversely, significant performance gap would confirm that safety behaviors are concentrated within specific safety experts and that L3 correctly identifies them. We implemented the random baseline by adaptively silencing randomly selected subset of local experts. We evaluate scenarios in which we randomly silence 10% and 20% of all local experts, since most models achieve their peak ASR with fewer than 20% of local experts silenced. Additionally, to ensure fully equal comparison, we performed scenario in which the number of randomly silenced experts was fixed to match the exact number of experts L3 silenced to reach peak ASR for each respective model (denoted by % = L3%). We provide the results for random silencing in Table 5. The results show consistently lower ASR for random silencing than for L3 across all scenarios. While L3 achieves high ASR by finding and silencing safety experts, random silencing fails to reliably bypass refusal mechanisms when silencing an equivalent number of experts. Notably, random silencing achieves relatively high ASR of 34.7% on Mixtral-8x7B-Instruct-v0.1. This is likely due to the balanced distribution of safety and language capabilities, and the large percentage of local experts silenced. First, as argued in Sections 5.1 and 5.3, the safety capabilities of Mixtral-8x7B-Instruct-v0.1 are distributed across the model. This means that silencing any random local expert has higher chance of increasing ASR than in models with concentrated safety capabilities. Second, 47.7% of all local experts in Mixtral-8x7B-Instruct-v0.1 are silenced, which is large percentage when compared to the other models. Thus, silencing large percentage of experts who all have higher chance of increasing ASR results in relatively large increase in ASR for Mixtral-8x7B-Instruct-v0.1. Overall, the consistently higher ASR achieved by L3 provides evidence that our LSTM-based identification method effectively captures the routing patterns associated with refusal."
        },
        {
            "title": "5.6 Global Expert Silencing",
            "content": "To gain further insight into how safety is embedded in MoE models, we examine how L3 performs when we silence the global safety experts instead of the local safety experts. By examining the performance of L3 in this setup, we can see Figure 4: The summed safety score per layer. Layers are sorted from low to high by their index. that the models safety behavior and general language utility are distributed across different local experts. Conversely, large decline in utility indicates that local experts contain both safety behavior and general language utility. For all models except Pangu-Pro-MoE, we see an average decline in utility. Pangu-Pro-MoE employs Mixture of Grouped Experts architecture, which enforces that tokens select experts from distinct groups of local experts to balance load. Tang et al. highlighted that this architecture achieves better expert specialization than standard MoEs [29]. The high degree of specialization means safety capabilities are largely separated from general language utility in Pangu-Pro-MoE. In turn, this allows for many safety experts to be silenced without any negative impact on utility. Mixtral-8x7B-Instruct-v0.1 and Phi-3.5-MoE-Instruct show relatively large average declines in utility of 8.8% and 5.1%, respectively. This suggests that the safety capabilities and language capabilities of these models are distributed across the same experts. For Mixtral-8x7B-Instruct-v0.1, this overlap in distribution is expected given the low number of local experts, only 256, as shown in Table 1. For Phi-3.5-MoE-Instruct, the observation is aligned with previous results discussed in Section 5.1: Phi-3.5-MoE-Instruct achieves low ASR because the model generates incoherent responses after expert silencing, more so than other models. The larger average decline in utility, combined with the observation that silencing safety experts leads to incoherent output, suggests an overlap between 9 Table 4: Impact of L3 on general language utility. ARC CoLA Before After Before After OpenBookQA After Before RTE WinoGrande Before After Before After Avg. Decline Model DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 53.8% 50.2% 75.9% 66.9% 90.6% 85.6% 84.6% 70.9% 94.0% 91.6% 85.6% 76.3% 71.9% 95.3% 91.0% 69.7% 64.5% 69.2% 69.1% 81.4% 72.9% 80.9% 78.0% 56.3% 89.2% 85.9% 84.5% 79.1% 70.9% 61.4% 57.2% 69.6% 82.0% 64.8% 83.6% 78.8% 81.2% 72.4% 69.3% 87.4% 87.4% 61.7% 88.0% 79.0% 76.8% 70.8% 88.6% 81.8% 51.5% 50.3% 75.8% 53.7% 52.8% 65.0% 60.2% 64.9% 57.2% 68.8% 76.2% 70.0% 76.2% 56.6% 53.4% 72.9% 64.8% 78.7% 66.7% 87.7% 84.5% 75.8% 87.0% 85.2% 82.0% 80.9% 72.4% 72.2% 84.1% 76.5% 94.3% 67.2% Average 82.3% 77.1% 73.8% 72.7% 81.1% 74.0% 77.9% 80.4% 63.4% 59.7% 2.1% 0.5% 3.6% 8.8% -4.3% 5.1% 2.7% 5.7% 3.5% Table 5: ASR for random silencing 10%, 20%, and an equal number of experts as L3 (% = L3%). Table 6: ASR for unaltered models (W/o L3) and models attacked with global expert silencing (Global L3). The last column shows how many global experts were silenced. Random Model 10% 20% % = L3% L3 Model W/o Global L3 Exp. Silenced DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 18.8% 2.5% 8.4% 9.4% 18.3% 0.9% 3.1% 1.9% 25.0% 3.4% 21.8% 11.3% 22.2% 1.3% 4.7% 2.2% Average 7.9% 11.5% 29.4% 15.9% 24.0% 34.7% 11.6% 3.4% 13.1% 3.1% 16.9% 76.9% 86.3% 81.3% 83.1% 70.3% 29.4% 68.4% 67.2% 70.4% DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 Average 11.6% 1.6% 14.7% 11.9% 9.4% 1.3% 6.9% 0.9% 7.3% 16.3% 62.8% 40.6% 82.8% 39.0% 23.4% 58.4% 8.4% 41.5% 1/64 = 1.6% 22/32 = 68.8% 28/64 = 43.8% 4/8 = 50.0% 27/64 = 42.2% 7/16 = 43.8% 20/60 = 33.3% 39/128 = 30.5% 39.3% how safety behavior concentrates among global experts in MoE models. Indeed, if safety-critical behavior is highly concentrated in global experts, then global expert silencing would result in high ASRs. In Table 6 we provide the ASR results for global expert silencing. Global expert silencing increases the ASR by 34.2% on average compared to no silencing. However, global silencing reaches an average ASR of 41.5%, far below the average ASR of 70.4% for local silencing. For some models, the increase is modest; DeepSeek-MoE16B-Chat increases by only 4.7%. Note that DeepSeek-MoE16B-Chat is unique case in this experiment, as it was only possible to silence one global expert (1.6%) before the model started generating incoherent output. Meanwhile, for all the other models, we could silence at least 30.5% of global experts. This indicates that DeepSeek-MoE-16B-Chat, compared to the other tested models, has concentrated safety and general language utility in certain global experts. Additionally, it also suggests that DeepSeek-MoE-16B-Chat has global experts that contain both types of utility, as the top global safety expert was also crucial for general language utility. We also see that global silencing achieves high ASR on Mixtral-8x7B-Instruct-v0.1, compared to other models, and almost matches local expert silencing. This is because Mixtral8x7B-Instruct-v0.1 has very few global experts compared to the other tested models, only 8. In Table 6, we see that silencing 50% of global experts in Mixtral-8x7B-Instructv0.1 achieves an ASR of 82.8%. This means the model can still produce coherent language even after half of all global experts are silenced, indicating that most global experts retain general language capabilities. notable observation is that silencing some global experts who were not identified as safety experts still increases the ASR. For example, in Figure 6, we see that GPT-OSS-20B has 17 global safety experts (global experts with positive summed safety score), but 23 are silenced to reach peak ASR. Those remaining 6 experts that were silenced have summed safety score close to 0. This indicates that global safety expert identification is less fine-grained than local safety expert identification. For global experts with total safety score close to 0, there are likely still some local experts who are important for safety; however, on global scale, they seem neutral. Hence, silencing these global experts might still increase ASR slightly due to the presence of local safety experts. In essence, these local safety experts are drowned out when scores are summed."
        },
        {
            "title": "5.7 LSTM Design Considerations",
            "content": "Considerations can be made during LSTM design to capture the sequential token-processing dynamics. The LSTM used in our experiments is flat architecture that forces it to treat the expert activation at Layer 1 and Layer 32 as simultaneous features occurring at the same time step, ignoring the causal dependency between layers. flat LSTM captures the refusal from the routing traces by treating all local expert selections 10 Table 7: Validation accuracy achieved on trained hierarchical and flat LSTMs. Model DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 Average Valid. Acc. Hier. LSTM Flat LSTM 82.4% 88.2% 65.7% 80.4% 94.1% 99.0% 90.2% 94.1% 86.8% 89.2% 94.1% 87.3% 91.2% 96.1% 99.0% 93.1% 95.1% 93.2% as single feature vector for each token step. This design captures inter-token sequential dynamics but does not explicitly model across-layer dependencies between the processing of tokens. To test whether additional modeling capacity along the layer axis helps, we also evaluate hierarchical LSTM that decouples tokenand layer-level dynamics. The hierarchical LSTM consists of an inner and outer LSTM module. The inner LSTM processes the routing path for single token across all layers. The outer LSTM reads the sequence of those token-level summaries from and decides whether, as whole, this prompt leads to refusal. We compare the LSTM validation accuracy of the hierarchical and flat architectures to determine whether hierarchical model can better model refusal than flat structure. We train both LSTM models with the approach and settings described in Section 4.3. Across all models, we see no improvement and sometimes worse accuracy when using Hierarchical LSTM compared with flat LSTM. Two observations in our work explain this negative result. First, the strongest refusal signal appears at the final tokens of prompt (Section 5.8), where routing differences between malicious and benign twins become most pronounced. In this scenario, the flat model already sees the activation pattern within xt at the last steps, limiting the benefit of an additional recurrent pass along layers. Second, our layer-wise safety score analysis (Section 5.3) shows that only subset of layers contribute strongly to refusal, reducing the value of modeling long, fine-grained layer sequences per token. Together, these findings suggest that refusal behavior in our traces is reflected in patterns best captured by the flat LSTM."
        },
        {
            "title": "5.8 Trigger Tokens",
            "content": "To validate whether the LSTM can successfully distinguish between syntactic structure and semantic context, we plot the sequential risk trajectory at each token step for syntactically similar malicious and benign prompt. In Table 8, we give an example of the probability that the LSTM assigns to the prompt leading to refusal after processing the tokens in the sequence. The LSTM used to produce this sample was trained on traces from Qwen3-30B-A3B-Instruct-2507. Table 8: Comparative LSTM Risk Trajectory: Malicious vs. Benign Intents. The Prob. column shows the probability of the prompt being malicious at this token step according to the LSTM. Malicious Benign Idx. Prob. Token Prob. Token 0 1 2 3 4 5 6 7 8 9 0.33 0.48 0.51 0.27 0.37 0.68 0.55 0.51 0.62 0.99 How can make bomb without getting caught ? 0.32 0.47 0.43 0.19 0.27 0.19 0.24 0.24 0.26 0.09 How can make cake without getting caught ? For example, at token 5 in the malicious prompt, the LSTM assigned probability of 68% that the prompt leads to refusal, and after processing the last token, it assigned 99% probability. For the benign prompt, it assigned probability of 19% and 9%, respectively. The probability scores reveal exactly where the MoE router is shifting traffic toward experts that the LSTM associates with refusal. Interestingly, the LSTM increases the probability exactly at the token bomb and decreases it at the token cake. This ability to pinpoint the intent shift shows that the LSTM can successfully distinguish between syntactic structure and semantic context. recurring pattern we observe is that the LSTM becomes most certain only after processing the last token, i.e., the LSTM model decides that an expert routing sequence will lead to refusal only after processing the whole sequence. This is because the expert routing patterns between twin prompt pair differ most for the last few tokens. In our experiments, the last token in twin prompts differs the most in terms of which experts are selected to process it, even though it is the same token. In contrast, the first few tokens of the prompt differ minimally in which experts are selected to process them. We show this behavior for GPT-OSS-20B in Figure 5. All other models show similar results. We see that the difference among the experts selected is close to 0 for all experts who processed the first token of prompt. In contrast, there are large differences in the number of times certain experts processed the last token, depending on whether the token was present in benign or malicious prompt. Even though the tokens are the same, there is large difference in expert routing patterns depending on the context of the sentence. This difference is key signal for the LSTM and indicates that the experts processing the last few tokens exhibit safety-critical behavior, such as refusal. The same experts processing the first few tokens in benign prompt and malicious counterpart, when no intent shift is present, means there are generalist experts that are active for both benign and malicious general language processing, as hypothesized in Section 1. 11 Table 9: ASR achieved by the GateBreaker method and L3. Model GateBreaker L3 DeepSeek-MoE-16B-Chat GPT-OSS-20B Hunyuan-A13B-Instruct Mixtral-8x7B-Instruct-v0.1 Pangu-Pro-MoE Phi-3.5-MoE-Instruct Qwen1.5-MoE-A2.7B-Chat Qwen3-30B-A3B-Instruct-2507 Average 52.4% 80.8% 76.3% 65.3% 72.5% 55.6% 58.1% 53.4% 64.3% 76.9% 86.3% 81.3% 83.1% 70.3% 29.4% 68.4% 67.2% 70.4%"
        },
        {
            "title": "6 Discussion",
            "content": "Potential Defenses. The primary vulnerability exploited by L3 is the concentration of safety capabilities within small set of local experts. This concentration is reflected in our results, which indicate that most models can be jailbroken with no more than 20% of local experts silenced and without significant loss in utility. To mitigate unequal distribution of safety capabilities, future model training should explicitly enforce safety redundancy, ensuring that safety capabilities are distributed across wider array of local experts. This could be achieved by incorporating regularization terms during alignment training that penalize the frequent routing of the same token to certain experts. Another method to achieve safety redundancy is by employing dropout techniques that force the model to use more diverse set of routing paths. However, enforcing such redundancy fundamentally conflicts with the expert specialization objectives of sparse architectures. By diluting expert specialization to spread safety behaviors, the model risks degrading its overall performance. Since the silencing procedure of L3 operates as an inferencetime attack by disabling experts, it inevitably alters the models internal routing statistics. In production scenarios, this allows for the deployment of Expert Integrity Checks as reactive defense. Defenders can monitor the runtime utilization of experts to detect anomalies indicative of tampering, such as the sudden silencing of experts that typically activate in sensitive semantic contexts (as observed in Section 5.8). By characterizing the distribution of expert utilization for standard traffic, systems can detect the \"utilization drift\" caused by an attack. The drawback of this detection-based approach is its susceptibility to adaptive adversaries who, aware of the monitoring, might carefully throttle expert silencing to stay within statistical noise thresholds. Finally, the architectural reliance on sparse routing can be counteracted by introducing an Ensemble or Refusal Verification mechanism. This involves secondary, lightweight safety pass, such as small, dense model or classifier with distinct routing logic, that evaluates the output of the primary MoE. Because this verifier is not subject to the same routing-based silencing as the main model, it serves as an independent check. If the primary MoE is manipulated into bypassing its internal Figure 5: Distributions of the difference in expert occurrence for the first and last token in all malicious prompts and their benign counterparts. The top image shows the difference in expert counts for the first token, aggregated over all prompts. The bottom image shows the difference in expert counts for the last token, aggregated over all prompts."
        },
        {
            "title": "5.9 L3 vs. GateBreaker",
            "content": "We compare the attack performance of L3 against GateBreaker [13], the best performing prior work in terms of ASR for inference-time jailbreaking. While GateBreaker is originally evaluated on the same eight models, it is only evaluated on StrongREJECT [35]. Following GateBreakers methodology, we evaluate the attack on the same 320 prompts as L3, mentioned in Section 4.4. As shown in Table 5.9, L3 achieves higher ASR than GateBreaker on six of the eight models, increasing by 6.1% on average. For Pangu-Pro-MoE and Phi-3.5-MoE-Instruct, L3 achieves worse ASR than GateBreaker, lower by 2.2% and 26.2%, respectively. The poor performance of Phi-3.5-MoE-Instruct is discussed in Section 5.1. Without Phi-3.5-MoE-Instruct, the average improvement in ASR over GateBreaker is 11.9%. We also run the two-tailed Wilcoxon Signed-Rank Test. The results obtained by excluding the Phi-3.5-MoE-Instruct case indicate that L3 is statistically significantly better than GateBreaker at the 0.05 significance level. These improvements highlight the benefits of L3: sequential modeling enables accurate identification of safety experts, and precise silencing of local experts allows an adversary to jailbreak models by affecting only gate layers, rather than pruning neurons. 12 of the safety layers during fine-tuning to address the security degradation [15]. While MoE architectures are becoming more prevalent across diverse settings, limited work has examined their security. Hayes et al. showed how cross-batch routing strategies can be exploited for integrity and availability attacks [44] while Wang et al. demonstrated backdoors by poisoning dormant experts [45]. Yona et al. showed side-channel leakage attack that extracts user prompts via routing tie-breaks [46]. Lai et al. proposed SAFEx, which identifies and then masks safety control experts to reduce the refusal rates [14]. Wu et al. presented fine-grained neuron-level attack across both sparse and shared experts, enabling more precise and effective safety removal with minimal model modification [13]. Kim et al. proposed an expert poisoning attack designed to compromise the safety of MoE LLMs by steering the poisoned experts to jailbreak [12]. Fayyaz et al. proposed SteerMoE, framework for steering MoE models by detecting and controlling behavior-linked experts [16]."
        },
        {
            "title": "8 Conclusions and Future Work",
            "content": "In this work, we reveal fundamental vulnerabilities created by the sparse activation architecture of MoE models. We introduce L3, novel training-free framework to successfully jailbreak MoE LLMs. L3 can successfully identify safety experts within MoE LLMs by leveraging the sequential nature of language processing. Subsequently, by silencing the identified safety experts, we remove the influence of their safety capability, causing the models to produce malicious responses. L3 achieves an average ASR of 70.4% and reaches as high as 86.3%, evaluated on eight state-of-the-art opensource MoE LLMs. Moreover, we demonstrate that bypassing the refusal guardrails typically requires silencing fewer than 20% of layer-wise (local) experts and that safety capabilities are mostly decoupled from general language capabilities. As result, experts can be silenced while leaving the models general language capabilities largely intact. This finding highlights critical security trade-off: the same expert specialization that drives the efficiency of MoE architectures also concentrates safety capabilities into fragile components that can be exploited. Consequently, current routingagnostic safety alignment techniques are insufficient. Robust safety in MoE requires moving towards architectureand routing-aware defenses that ensure safety mechanisms are redundantly distributed and resilient to targeted expert silencing. guardrails, the verifier serves as backstop to catch and filter L3-bypassed outputs, raising the effort needed to jailbreak the model. However, this defense may introduce unwanted computational overhead, partially negating the inference speed advantages of the MoE architecture. L3 in black-box settings. The most direct adaptation of L3 to black-box setting would be utilizing the transferability of safety experts and components, as shown in prior work that identifies safety components in LLMs [13, 17]. In these proxy-based transfer attacks, an adversary utilizes an openweight MoE model to serve as proxy for the target black-box model in the same model family. The adversary performs the standard L3 safety component identification in the white-box setting. Then, using gradient-based optimization (e.g., Greedy Coordinate Gradient) on the proxy, the attacker generates prompts or adversarial suffixes specifically designed to minimize the routing probability to the identified safety experts. If the black-box model shares similar base architecture or training data (common in the industry), the adversarial prompt would likely trigger similar benign routing path, effectively bypassing the safety experts in black-box setting."
        },
        {
            "title": "7 Related Works",
            "content": "Jailbreaking refers to adversarial prompting techniques that circumvent models safety alignment (e.g., instruction tuning and RLHF) to elicit policy-violating outputs. Empirically, jailbreaks highlight systematic failure modes in safety training: models may generalize poorly outside the distribution of unsafe exemplars seen during alignment, and they may face competing objectives between helpfulness and refusal, making them susceptible to carefully constructed prompts that reframe intent or manipulate the instruction hierarchy. Prior work spans manual, automated black-box, and optimization-based attacks. Early and widely-studied jailbreaks use natural-language strategies (role-play, indirection, translation, ignore previous instructions, etc.), see, e.g., [3840]. major line of automated attacks discovers universal adversarial strings that transfer across prompts and sometimes across models. For example, universal adversarial suffixes appended to many different queries can reliably reduce refusal and induce harmful completions, demonstrating that alignment can be bypassed by small, algorithmically found prompt perturbations [41]. Building on this, AutoDAN uses automated search/optimization to generate readable jailbreak prompts that bypass simple filters (e.g., perplexity-based defenses) while maintaining high attack success [42]. Another direction exploits longcontext vulnerabilities: many-shot jailbreaking combines hundreds of demonstrations of unsafe behavior into the context window, leveraging in-context learning to override safety behavior at the final query [43]. Li et al. explored the safety layers and proposed fine-tuning approach (Safely PartialParameter Fine-Tuning - SPPFT), which fixes the gradient"
        },
        {
            "title": "Ethical Considerations",
            "content": "In accordance with the USENIX Security Ethics Guidelines, we conducted stakeholder-based analysis to evaluate the ethical implications of our work, Large Language Lobotomy (L3). Our research introduces method to compromise the safety alignment of Mixture-of-Experts (MoE) Large Language Models (LLMs) by identifying and silencing safetycritical experts. Stakeholders. We identified the following key stakeholders impacted by our research: Model Developers and Vendors: Organizations releasing open-weight MoE models (e.g., DeepSeek, Mistral AI, Alibaba Cloud, Microsoft). The AI Research Community: Researchers focused on LLM safety, interpretability, and architectural robustness. Downstream Developers and Service Providers: Entities deploying these open-source models for end-user applications. Society and General Public: Individuals potentially exposed to harmful content generated by jailbroken models. Impacts and Ethical Principles. We guided our analysis using the suggested principles of Beneficence, Respect for Law and Public Interest, and Justice from the Menlo Report. The primary benefit of this work is identifying critical architectural vulnerability in MoE models. By revealing that safety mechanisms are often sparsely concentrated rather than robustly distributed, we provide the research community with the necessary knowledge to design more resilient architectures. Our work operates on open-weight models in whitebox setting. We do not compromise private infrastructure or violate the terms of service of proprietary APIs. However, we acknowledge the risk that our methods could be used to generate illegal content. Harms. potential harm is lowering the barrier to safety removal. Unlike fine-tuning attacks, which require significant compute and data, L3 is training-free. This lowers the barrier for adversaries to strip safety behaviors from powerful openweight models. Malicious actors could employ L3 to bypass the safety guardrails of deployed models, generating hate speech, disinformation, or dangerous instructions. Mitigations. We implemented the following measures to mitigate the identified harms: Standardized Evaluation Data: We utilized established research datasets (HarmfulQA, StrongReject) and synthetic twin datasets generated via Gemini 3 Pro. We did not use private user data or personally identifiable information. Non-Proliferation of Harmful Artifacts: We release the research code to facilitate defensive work, but do not release jailbroken model checkpoints or pre-computed silencing masks for generating specific harmful content. Disclosure Strategy: As the vulnerability is inherent to the standard MoE architecture and concerns public open-weights models, we prioritize publication of our findings to accelerate defensive research. Decision. We weighed the risks of publishing L3 against the benefits to the security community. The decision to publish was driven by the principle of beneficence. As the industry increasingly shifts toward MoE architectures for their scaling efficiency, it is important to understand their vulnerabilities as well. Relying on security by obscurity regarding the localization of safety experts is unsustainable. We concluded that the marginal risk increase is low because adversaries with white-box access already possess methods to remove safety (e.g., fine-tuning). However, the defensive value is high: our findings demonstrate that current MoE safety alignment is brittle and localized. Publishing this work is necessary to motivate the development of safety-distributed architectures where refusal mechanisms cannot be disabled by silencing small fraction of parameters."
        },
        {
            "title": "Open Science",
            "content": "In compliance with USENIX Securitys open science policy, we have made the code to reproduce our findings available on https://github.com/jonatelintelo/ LargeLanguageLobotomy to support transparency, reproducibility, and further research. This repository includes the code used to collect and process expert routings, train the LSTM, find safety experts, and prune safety experts. To prevent direct access to unsafe AI models, we do not release the checkpoints of the compromised models. Instead, we provide the code and methods that allow researchers to reproduce the safety expert identification and silencing with publicly available datasets and open-weight models. This approach ensures reproducibility for the research community while minimizing potential harm."
        },
        {
            "title": "References",
            "content": "[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, Language models are few-shot learners, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, Curran Associates, and H. Lin, Eds., vol. 33. Inc., 2020, pp. 18771901. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf 14 [2] D. Nam, A. Macvean, V. Hellendoorn, B. Vasilescu, and B. Myers, Using an llm to help with code understanding, in Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, 2024, pp. 113. [3] M. Cascella, J. Montomoli, V. Bellini, and E. Bignami, Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios, Journal of medical systems, vol. 47, no. 1, p. 33, 2023. [4] Meta, Llama 3.2: Revolutionizing edge ai and vision with open, customizable models, September 2024. [Online]. Available: https://ai.meta.com/blog/ llama-3-2-connect-2024-vision-edge-mobile-devices/ [5] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love et al., Gemma: Open models based on gemini research and technology, arXiv preprint arXiv:2403.08295, 2024. [6] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin et al., Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement, arXiv preprint arXiv:2409.12122, 2024. [7] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., Phi-4 technical report, arXiv preprint arXiv:2412.08905, 2024. [8] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, Mixtral of experts, 2024. [Online]. Available: https://arxiv.org/abs/2401.04088 [9] D. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang, Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. [Online]. Available: https://arxiv.org/abs/2401.06066 [10] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, Outrageously large neural networks: The sparsely-gated mixtureof-experts layer, 2017. [Online]. Available: https: //arxiv.org/abs/1701.06538 [11] W. Fedus, B. Zoph, and N. Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, Journal of Machine Learning Research, vol. 23, no. 120, pp. 139, 2022. [Online]. Available: http://jmlr.org/papers/v23/21-0998. html [12] J. Kim, S. H. Na, M. Song, S. Shin, and S. Son, Moevil: Poisoning experts to compromise the safety of mixtureof-experts llms, in 2025 Annual Computer Security Applications Conference (ACSAC). IEEE, 2025. [13] L. Wu, S. Behrouzi, M. Rostami, S. Picek, and A.-R. Sadeghi, Gatebreaker: Gate-guided attacks on mixture-of-expert llms, 2025. [Online]. Available: https://arxiv.org/abs/2512.21008 [14] Z. Lai, M. Liao, B. Wu, D. Xu, Z. Zhao, Z. Yuan, C. Fan, and J. Li, Safex: Analyzing vulnerabilities of moe-based llms via stable safety-critical expert identification, 2025. [Online]. Available: https://arxiv. org/abs/2506.17368 [15] S. Li, L. Yao, L. Zhang, and Y. Li, Safety layers in aligned large language models: The key to llm security, in International Conference on Learning Representations, Y. Yue, A. Garg, N. Peng, F. Sha, and R. Yu, Eds., vol. 2025, 2025, pp. 98 16398 189. [Online]. Available: https://proceedings.iclr.cc/paper_files/paper/2025/file/ f3bfbd65743e60c685a3845bd61ce15f-Paper-Conference. pdf [16] M. Fayyaz, A. Modarressi, H. Deilamsalehy, F. Dernoncourt, R. Rossi, T. Bui, H. Schütze, and N. Peng, Steering moe llms via expert (de)activation, 2025. [Online]. Available: https://arxiv.org/abs/2509.09660 [17] L. Wu, S. Behrouzi, M. Rostami, M. Thang, S. Picek, and A.-R. Sadeghi, Neurostrike: Neuron-level attacks on aligned llms, Network and Distributed System Security (NDSS) Symposium, 2026. [18] T. Krauß, H. Dashtbani, and A. Dmitrienko, Twinbreak: jailbreaking llm security alignments based on twin prompts, in Proceedings of the 34th USENIX Conference on Security Symposium, ser. SEC 25. USA: USENIX Association, 2025. [19] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, Gshard: Scaling giant models with conditional computation and automatic sharding, 2020. [Online]. Available: https://arxiv.org/abs/2006.16668 [20] W. Fedus, B. Zoph, and N. Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022. [Online]. Available: https://arxiv.org/abs/2101.03961 [21] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, Base layers: Simplifying training of large, sparse models, 2021. [Online]. Available: https://arxiv.org/abs/2103.16716 [22] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. Dai, Z. Chen, Q. Le, and J. Laudon, Mixture-ofexperts with expert choice routing, 2022. [Online]. Available: https://arxiv.org/abs/2202.09368 [23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, 2022. [Online]. Available: https://arxiv.org/abs/2203. 02155 [24] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan, Constitutional ai: Harmlessness from ai feedback, 2022. [Online]. Available: https://arxiv.org/abs/2212.08073 [25] J. Chen, X. Wang, Z. Yao, Y. Bai, L. Hou, and J. Li, Towards understanding safety alignment: mechanistic perspective from safety neurons, 2025. [Online]. Available: https://arxiv.org/abs/2406.14144 [26] N. Yasin, J. Hare, and A. Marcu, Is saliency really captured by gradient? in NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning, 2024. [Online]. Available: https: //openreview.net/forum?id=p2Oq5sTjEz [27] OpenAI, Introducing GPT-OSS, August 2025. https://openai.com/index/ Available: [Online]. introducing-gpt-oss/ [28] Hunyuan Team Tencent, Hunyuan-A13B Technical Report, June 2025. [Online]. Available: https: //github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/ main/report/Hunyuan_A13B_Technical_Report.pdf X. Chen, D. Tao, and Y. Wang, Pangu pro moe: Mixture of grouped experts for efficient sparsity, 2025. [Online]. Available: https://arxiv.org/abs/2505.21411 [30] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai, Q. Cai, V. Chaudhary, D. Chen, D. Chen, W. Chen, Y.-C. Chen, Y.-L. Chen, H. Cheng, P. Chopra, X. Dai, M. Dixon, R. Eldan, V. Fragoso, J. Gao, M. Gao, M. Gao, A. Garg, A. D. Giorno, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. J. Hewett, W. Hu, J. Huynh, D. Iter, S. A. Jacobs, M. Javaheripi, X. Jin, N. Karampatziakis, P. Kauffmann, M. Khademi, D. Kim, Y. J. Kim, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, Y. Li, C. Liang, L. Liden, X. Lin, Z. Lin, C. Liu, L. Liu, M. Liu, W. Liu, X. Liu, C. Luo, P. Madan, A. Mahmoudzadeh, D. Majercak, M. Mazzola, C. C. T. Mendes, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radmilac, L. Ren, G. de Rosa, C. Rosset, S. Roy, O. Ruwase, O. Saarikivi, A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, Y. Shen, S. Shukla, X. Song, M. Tanaka, A. Tupini, P. Vaddamanu, C. Wang, G. Wang, L. Wang, S. Wang, X. Wang, Y. Wang, R. Ward, W. Wen, P. Witte, H. Wu, X. Wu, M. Wyatt, B. Xiao, C. Xu, J. Xu, W. Xu, J. Xue, S. Yadav, F. Yang, J. Yang, Y. Yang, Z. Yang, D. Yu, L. Yuan, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, and X. Zhou, Phi-3 technical report: highly capable language model locally on your phone, 2024. [Online]. Available: https://arxiv.org/abs/2404. [31] Qwen Team, Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters\", February 2024. [Online]. Available: https://qwenlm.github.io/ blog/qwen-moe/ [32] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu, Qwen3 technical report, 2025. [Online]. Available: https://arxiv.org/abs/2505.09388 [29] Y. Tang, X. Li, F. Liu, W. Guo, H. Zhou, Y. Wang, K. Han, X. Yu, J. Li, H. Zang, F. Mi, X. Meng, Z. Liu, H. Chen, B. Zheng, C. Chen, Y. Yan, R. Tang, P. Qin, [33] R. Bhardwaj, D. D. Anh, and S. Poria, Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic, 2024. 16 [34] R. Bhardwaj and S. Poria, Red-teaming large language models using chain of utterances for safety-alignment, 2023. [44] J. Hayes, I. Shumailov, and I. Yona, Buffer overflow in mixture of experts, arXiv preprint arXiv:2402.05526, 2024. [35] A. Souly, Q. Lu, D. Bowen, T. Trinh, E. Hsieh, S. Pandey, P. Abbeel, J. Svegliato, S. Emmons, O. Watkins, and S. Toyer, strongreject for empty jailbreaks, 2024. [Online]. Available: https://arxiv.org/abs/2402.10260 [45] Q. Wang, Q. Pang, X. Lin, S. Wang, and D. Wu, Badmoe: Backdooring mixture-of-experts llms via optimizing routing triggers and infecting dormant experts, arXiv preprint arXiv:2504.18598, 2025. [46] I. Yona, I. Shumailov, J. Hayes, and N. Carlini, Stealing user prompts from mixture of experts, arXiv preprint arXiv:2410.22884, 2024."
        },
        {
            "title": "A Additional Figures Safety Score",
            "content": "In this section, we provide additional figures of the safety expert identification results. These figures supplement the findings in Section 5.3 by detailing the distribution of refusal behaviors at both the expert and layer levels. Figure 6 presents the summed safety score for each expert across the models, sorted in descending order of contribution. Figure 7 illustrates the layer-wise aggregation of safety scores. [36] Google, Gemini [Online]. https://docs.cloud.google.com/vertex-ai/ 2025. Pro, 3 Available: generative-ai/docs/models/gemini/3-pro [37] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, 2017. [Online]. Available: https://arxiv.org/abs/1412.6980 [38] M. G. Collu, T. Janssen-Groesbeek, S. Koffas, M. Conti, and S. Picek, Dr. jekyll and mr. hyde: Two faces of [Online]. Available: llms, 2025. https://arxiv.org/abs/2312.03853 [39] P. Bisconti, M. Prandi, F. Pierucci, F. Giarrusso, M. B. Syrnikov, M. Galisai, V. Suriani, O. Sorokoletova, F. Sartore, and D. Nardi, Adversarial poetry as universal single-turn jailbreak mechanism in large language models, 2026. [Online]. Available: https: //arxiv.org/abs/2511. [40] F. Jiang, Z. Xu, L. Niu, Z. Xiang, B. Ramasubramanian, B. Li, and R. Poovendran, Artprompt: Ascii art-based jailbreak attacks against aligned llms, 2024. [Online]. Available: https://arxiv.org/abs/2402.11753 [41] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson, Universal and transferable adversarial attacks on aligned language models, 2023. [Online]. Available: https://arxiv.org/abs/2307.15043 [42] X. Liu, N. Xu, M. Chen, and C. Xiao, Autodan: Generating stealthy jailbreak prompts on aligned large language models, 2024. [Online]. Available: https://arxiv.org/abs/2310.04451 [43] C. Anil, E. DURMUS, N. Rimsky, M. Sharma, J. Benton, S. Kundu, J. Batson, M. Tong, J. Mu, D. J. Ford, F. Mosconi, R. Agrawal, R. Schaeffer, N. Bashkansky, S. Svenningsen, M. Lambert, A. Radhakrishnan, C. Denison, E. J. Hubinger, Y. Bai, T. Bricken, T. Maxwell, N. Schiefer, J. Sully, A. Tamkin, T. Lanham, K. Nguyen, T. Korbak, J. Kaplan, D. Ganguli, S. R. Bowman, E. Perez, R. B. Grosse, and D. Duvenaud, Many-shot jailbreaking, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [Online]. Available: https: //openreview.net/forum?id=cw5mgd71jW 17 Figure 6: The summed safety score per global expert. Experts are sorted from high to low by their summed safety score. Figure 7: The summed safety score per layer. Layers are sorted from low to high by their index."
        }
    ],
    "affiliations": [
        "Faculty of Electrical Engineering and Computing University of Zagreb, Croatia",
        "Radboud University, The Netherlands",
        "Technical University of Darmstadt, Germany"
    ]
}