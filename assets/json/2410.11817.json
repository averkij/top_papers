{
    "paper_title": "Improving Long-Text Alignment for Text-to-Image Diffusion Models",
    "authors": [
        "Luping Liu",
        "Chao Du",
        "Tianyu Pang",
        "Zehan Wang",
        "Chongxuan Li",
        "Dong Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: a text-relevant part that measures T2I alignment and a text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to a common overfitting problem during fine-tuning. To address this, we propose a reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and Kandinsky v2.2. The code is available at https://github.com/luping-liu/LongAlign."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 7 1 8 1 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint version",
            "content": "IMPROVING LONG-TEXT ALIGNMENT FOR TEXT-TO-IMAGE DIFFUSION MODELS Luping Liu1,2, Chao Du2, Tianyu Pang2, Zehan Wang2,4, Chongxuan Li3, Dong Xu1 1The University of Hong Kong; 2Sea AI Lab, Singapore; 3Renmin University of China; 4Zhejiang University luping.liu@connect.hku.hk; duchao@sea.com; tianyupang@sea.com; wangzehan01@zju.edu.cn; chongxuanli@ruc.edu.cn; dongxu@hku.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes segment-level encoding method for processing long texts and decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: text-relevant part that measures T2I alignment and text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to common overfitting problem during fine-tuning. To address this, we propose reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning 512 512 Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-α and Kandinsky v2.2. The code is available at https://github.com/luping-liu/LongAlign."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021a;b) have significantly enhanced text-to-image (T2I) generation (Schuhmann et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022). While text-based conditioning provides flexibility and user-friendliness, current models struggle with long and complex text descriptions that often span multiple sentences and hundreds of tokens (Chen et al., 2023a; Zheng et al., 2024). Effectively encoding such lengthy text conditions and ensuring precise alignment between text and generated images remains critical challenge for generative models. To encode text descriptions, contrastive pre-training encoders such as CLIP (Radford et al., 2021) are widely used in T2I diffusion models. However, as text length increases, the maximum token limit of CLIP becomes significant constraint, making it infeasible to encode long descriptions. Recent works have explored large language model (LLM)-based encoders like T5 (Raffel et al., 2020), leveraging their ability to handle longer sequences (Saharia et al., 2022; Chen et al., 2023a; Hu et al., 2024). Nevertheless, contrastive pre-training encoders retain key advantage: their text encoders are specifically trained to align with images, potentially offering superior alignment between text representations and generated images (Saharia et al., 2022; Li et al., 2024b). Beyond encoding, current T2I diffusion models struggle to accurately follow long-text descriptions, often generating images that only partially reflect the intended details, as demonstrated in Figure 1. Work done during Luping Liu and Zehan Wangs associate memberships at Sea AI Lab."
        },
        {
            "title": "Preprint version",
            "content": "Figure 1: Generation results of our long Stable Diffusion and baselines. We highlight three key facts for each prompt and provide the evaluation results at the end. In each evaluation line, the four group results are arranged in order of model presentation, with representing SD-1.5, and so on. Additionally, three or maintain the order of the key facts corresponding to each prompt. Inspired by advances in aligning LLMs (Ouyang et al., 2022; Rafailov et al., 2024), one potential solution is preference optimization, which generates and utilizes preference feedback when only parts of target are satisfied. Recent works have explored collecting human preferences from T2I users and leveraging them to train preference models (Kirstain et al., 2023; Wu et al., 2023c;b), enabling preference optimization in T2I diffusion models. However, since these models are typically fine-tuned from CLIP, they face the same token limit constraints. Moreover, existing human preferences blend text alignment with visual factors such as photorealism or aesthetics, which only partially support the goal of accurately aligning long and detailed text descriptions. In this paper, to support long-text inputs for the two scenarios mentioned, we explore segment-level encoding, which involves dividing lengthy texts into shorter segments (e.g., sentences), encoding each one separately, and then merging the results for subsequent tasks. The main challenge is effectively combining these segment outputs to merge their diverse information without causing confusion. To address this, for text encoding in diffusion models, we opt for concatenating segment embeddings and explore optimal adjustments to the unintended repetition of special token embeddings from different segments. For preference models, we implement segment-level preference training loss alongside segment-level encoding, allowing preference models to handle long inputs while generating both segment-level scores and an overall average score. To enable preference optimization for long-text alignment, we analyze the scoring mechanisms of preference models and use these models (with segment-level encoding) to fine-tune T2I diffusion models. We find that the desired T2I alignment scores can be separated from general human preference scores. Specifically, preference scores can be divided into two components: text-relevant part that assesses T2I alignment and text-irrelevant part that evaluates other factors (e.g., aesthetics). In addition, we discover that the remaining text-irrelevant part leads to common overfitting issue (Wu et al., 2024) during fine-tuning. To mitigate this, we propose reweighting strategy that assigns different weights to these two components, which reduces overfitting and enhances alignment. By integrating the methods mentioned above, we propose LongAlign. Our experiments show that segment-level encoding and training enable preference models to effectively handle long-text inputs and generate segment-level scores. Additionally, our preference decomposition method allows these models to produce T2I alignment scores alongside general preference scores. After fine-tuning the 512 512 Stable Diffusion v1.5 (Rombach et al., 2022) using LongAlign for about 20 hours on"
        },
        {
            "title": "6 A100 GPUs, the obtained long Stable Diffusion (longSD) significantly improves alignment (see\nFigure 1), outperforming stronger foundation models in long-text alignment, such as PixArt-α (Chen\net al., 2023a) and Kandinsky v2.2 (Razzhigaev et al., 2023). Our contributions are as follows:",
            "content": "We propose segment-level encoding method that enables encoding models with limited input lengths to effectively process long-text inputs. We propose preference decomposition that enables preference models to produce T2I alignment scores alongside general preference, enhancing text alignment fine-tuning in generative models. After about 20 hours of fine-tuning, our longSD surpasses stronger foundation models in long-text alignment, demonstrating significant improvement potential beyond the model architecture."
        },
        {
            "title": "2 BACKGROUND",
            "content": "We provide an overview of diffusion models and T2I models. Next, we discuss preference models for fine-tuning T2I diffusion models, followed by an introduction to the reward fine-tuning process."
        },
        {
            "title": "2.1 DIFFUSION MODEL\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021a;b) construct a\ntransformation from a Gaussian distribution to a target data distribution through a multi-step diffu-\nsion denoising process. Given a data distribution x0 ∼ q(x0), the diffusion process satisfies:",
            "content": "xt = αtx0 + βtϵ, (1) where ϵ (0, 1), {0, 1, . . . , }, is the maximum timestep, α2 the speed of adding noise. The loss function is typically defined as follows: + β2 = 1, and βt controls Lt = Ex0,ϵϵ ϵθ(αtx0 + βtϵ, t)2, (2) where the model ϵθ aims to predict the noise ϵ added to the clean data x0. Once ϵθ is learned, according to DDIM (Song et al., 2021a), the denoising process from = to = 0 satisfies: = (xt βtϵθ(xt, t))/αt, xt1 = αt1x + (cid:113) β2 t1 σ2 ϵθ(xt, t) + σtϵ, (3) where xT (0, 1) and is the predicted clean image at timestep t. When σt 0, this is the DDIM denoising process. When σt = (βt1/βt) /α2 t1), this is the DDPM denoising process. In this paper, we choose σt 0. The corresponding DDIM process can be accelerated using numerical solvers such as PNDM (Liu et al., 2022) and DPM-Solver (Lu et al., 2022). (1 α2 (cid:113) Stable Diffusion. Among different T2I diffusion models, Stable Diffusion (Rombach et al., 2022) plays crucial role, which integrates VAE (Kingma & Welling, 2013), CLIP, and diffusion model ϵθ. During training, the pretrained VAE compresses the image into latent z, while the pretrained CLIP model encodes the text prompt p. The diffusion model ϵθ then attempts to fit this new distribution of latent variables z, conditioned on the text p. In the sampling process, the diffusion model ϵθ first generates the latent based on the text prompts and then uses the VAE to decode the latent to obtain the final image x. For simplicity, we ignore the VAE in the following. 2.2 PREFERENCE MODEL Preference optimization (Ouyang et al., 2022; Rafailov et al., 2024) has shown its effectiveness in aligning LLMs with humans using preference feedback. To facilitate this for T2I diffusion models, prior work (Kirstain et al., 2023; Wu et al., 2023b) fine-tunes these models with T2I preference models that evaluate human preferences for an image given text prompt p, represented as R(x, p). We focus on preference models fine-tuned from pretrained CLIP models. To prepare the preference dataset for fine-tuning, prompts are paired with two generated images, and the preferred image is annotated. For xi preferred over xj (denoted as j), the preference training loss is: Lij = exp(R(xi, p)) exp(R(xi, p)) + exp(R(xj, p)) , (4) where R(x, p) = CX (x) CP (p) is the dot product of the image embeddings CX (x) and the text embeddings CP (p). After training, the preference model can be used to evaluate preferences or finetune generative models. Similar to CLIP, these preference models dont support long-text inputs."
        },
        {
            "title": "2.3 REWARD FINE-TUNING",
            "content": "When fine-tuning T2I diffusion models with the preference models mentioned above, previous works (Black et al., 2023; Fan et al., 2024) typically treat preference models as reward signals defined by Lr = 1 ExT ,pR(x 0, p). However, fine-tuning the generator ϵθ of diffusion models using these signals poses two challenges. First, backpropagating gradients through the entire iteration process is problematic. Second, overfitting is concern. Previous works (Prabhudesai et al., 2023; Clark et al., 2023) have employed gradient checkpointing and LoRA (Hu et al., 2021) to facilitate gradient backpropagation, but these methods skip consecutive time steps at the beginning or end of the diffusion iteration to accelerate computation, which inevitably introduces optimization bias. Recently, DRTune (Wu et al., 2024) shows new method that allows for training on uniform subset of all sampling steps. Specifically, when we apply the preference model R(x 0, p) to calculate the reward signal Lr = 1 ExT ,pR(x 0, p), the corresponding gradient is as follows: θ(1 ExT ,pR(x 0, p)) ExT ,p(x R)(x 0, p) (cid:32)T 1 (cid:88) i=1 (βi1/αi1 βi/αi)(θϵθ)(sg(xi), i, p) . (5) (cid:33) DRTune truncates the gradient of all {sg(xi)}, allowing it to optimize uniform portion of the remaining gradient {(θϵθ)(sg(xi), i, p)}. In this paper, we also utilize this as our gradient backpropagation method. However, overfitting remains concern that cannot be fully addressed by early stopping alone, necessitating further analysis and additional solutions."
        },
        {
            "title": "3 SEGMENT-LEVEL TEXT ENCODING",
            "content": "In this section, we introduce our segment-level encoding method, designed for the long-text encoding of both diffusion models and preference models. This approach divides the text into segments, encodes each one individually, and then merges the results. For diffusion models, we explore an embedding concatenation strategy during merging. For preference models, we present segment-level loss alongside the new encoding to handle long inputs and generate detailed preference scores. 3.1 TEXT ENCODING OF DIFFUSION MODEL For text encoding in diffusion models, contrastive pre-training encoders like CLIP (Rombach et al., 2022) are commonly used. However, as the input text length increases, CLIPs maximum token limitation becomes significant issue. As result, recent works (Saharia et al., 2022; Chen et al., 2023a; Hu et al., 2024) have shifted to using LLMs like T5 instead of CLIP, overlooking CLIPs distinct advantage (Radford et al., 2021) in image-text alignment pretraining. To leverage CLIPs capabilities for long text, we introduce segment-level encoding. Our method divides the text into segments (e.g., sentences), encodes each one into embeddings like the original T2I diffusion models, and then merges these embeddings. Figure 9 illustrates this segment-level text encoding. In the merging process, we initially use direct embedding concatenation, which results in poor generated images (see Figure 10). This occurs because each segment includes special tokens such as <sot>, <eot>, and <pad> during individual encoding, leading to the unintended repeated presence of their embeddings in the concatenated embedding. To address this, we conduct ablation experiments on whether to keep, remove, or replace special token embeddings. The final embedding excludes the <pad> embeddings and introduces new unique <pad*> embedding to meet the target length. It retains all <sot> embeddings while removing all <eot> embeddings, resulting in the format <sot> Text1. <sot> Text2. ... <pad*>. More details about this experiment can be found in Appendix A. We can now use both CLIP and T5 for long-text encoding in T2I diffusion models. The next challenge is accurately representing all text segments in the generated images. To tackle this issue, we further fine-tune diffusion models with large-scale long texts paired with their corresponding images. We start with supervised training employing ℓ2 loss. However, we observe clear optimization limit during training, with even the best version falling short of perfection (see the longSD(S) column in Table 2). This problem prompts us to explore additional preference optimization for long-text alignment alongside general supervised training, inspired by its success in LLMs (Ouyang et al., 2022). 3.2 SEGMENT PREFERENCE MODEL In the context of preference optimization in T2I diffusion models, previous studies (Kirstain et al., 2023; Wu et al., 2023b) have employed CLIP-based human preference models to better align T2I"
        },
        {
            "title": "Preprint version",
            "content": "Figure 2: (a) Schematic results for text embeddings. (b) Statistics of the projection scalar η for three models. (c) The relationship between the original score and the two scores after decomposition using our Denscore. In the three score tables, the diagonal represents the scores for paired data, while the off-diagonal positions indicate the scores for unpaired data. diffusion models with human preferences. Since our objectivetext alignmentis key component of human preference, and CLIP-based structures are effective for large-scale training, we aim to adapt such preference models for our long-text alignment task. The first step towards achieving the above goal is to enable CLIP-based preference models to accept long-text inputs. As shown in Section 2.2, these models introduce new human preference training objective into the CLIP framework. However, they still have the same limited maximum input length and may struggle to reflect the varying impacts of different segments (e.g., sentences) with single final score. To solve these problems, we split the long-text condition into segments, denoted as {ˆpk}K k=1. Then, the new segment-level preference training loss is: exp((cid:80)K k=i R(xi, ˆpk)/K) Lseg ij = . (6) exp((cid:80)K k=i R(xj, ˆpk)/K) k=i R(xi, ˆpk)/K) + exp((cid:80)K The above loss enables weakly supervised learning (Zhou, 2018) during training, eliminating the need for additional segment-level annotations. In comparison to its single-value counterpart, the new preference model supports long-text inputs and generates more detailed segment-level scores {R(x, ˆpk)}k, along with an average final score (cid:80)K k=i CP (ˆpk)/K. Thus, computing this average score is equivalent to first segment-level encoding the text input and (p) = (cid:80)K using the average embedding, denoted as seg k=i CP (ˆpk)/K, to compute the score. We refer to this score as Denscore, which will also function as reward signal in the following section. If there is no confusion, we may omit the segment label in seg k=i R(xi, ˆpk)/K = CX (x) (cid:80)K (p)."
        },
        {
            "title": "4 PREFERENCE DECOMPOSITION",
            "content": "In this section, we explore preference optimization using the preference models mentioned above. We find that their preference scores comprise text-relevant component and text-irrelevant component, with the latter often causing overfitting in fine-tuning diffusion models. To address this, we propose reweighting strategy for both components that reduces overfitting and enhances alignment. 4.1 ORTHOGONAL DECOMPOSITION As shown in Section 2.2, the CLIP-based preference model R(x, p) = CX (x) CP (p) evaluates an image against text with respect to human preferences. Some preferences concern whether the text condition is accurately represented in the image x, while others focus on visual factors such as photorealism and aesthetics, making them irrelevant to the text. We find direct structural correspondence between these two types of preferences within the text embedding CP (p). Specifically, different CP (p) display common direction, as illustrated in Figure 2 (a). This common direction corresponds to text-irrelevant preferences, while the remainder reflects text-relevant preferences. To support this statement, we use text embeddings from large prompt dataset to compute the common text-irrelevant direction: := EpP CP (p)/EpP CP (p). We then decompose the text embedding CP (p) into two orthogonal parts: (p) + ηV, where η = CP (p) is the projection scalar of CP onto V. For the value of η, we test CLIP, Pickscore and our Denscore, presenting the results in Figure 2 (b). We observe that CP exhibits strong positive scalar projection onto"
        },
        {
            "title": "Preprint version",
            "content": "Figure 3: Retrieval results with low or high text-irrelevant scores, using three CLIP-based models. (η > 0.4 for CLIP and η > 0.6 for the others), forming core in representation space. The presence of is referred to as the cone effect (Gao et al., 2019; Liang et al., 2022), which results from both model initialization and contrastive training. To better understand the differing roles of these two components in the preference score R(x, t), we experiment with 5k image-text dataset. Figure 2 (c) presents score table illustrating the relationship between CX (x) CP (p), CX (x) (p) and CX (x) ηV. more detailed version of this figure and real data statistics for the three scores can be found in Appendix B.2. For the second score CX (x) (p), our experiments in Section 5.2 and Appendix B.3 show that it eliminates the influence of text-irrelevant components and focuses on measuring T2I alignment, which aligns with our objective. We refer to this aspect of Denscore as Denscore-O. For the third score CX (x)ηV = η(CX (x)V), the scalar η is analyzed in the above two paragraphs. The remaining text-irrelevant term CX (x)V still needs further clarification. We provide images with low or high text-irrelevant scores in Figure 3 to visualize the score standard. Notably, the three models assign low scores to images that share similar characteristics, marked by dull visuals and large blank spaces. Conversely, while the high-scoring images selected by CLIP lack distinct features, the two preference models show strong alignment. Their high-scoring images feature rich details, well-organized layouts, and overall higher quality. This highlights the role of training on Equation 4, which provides clear training objectivegeneral human preferencefor text-irrelevant scores. In contrast, CLIPs original contrastive training does not incorporate this aspect. 4.2 GRADIENT-REWEIGHT REWARD FINE-TUNING To use the preference model R(x, p) as reward signal for fine-tuning T2I diffusion models, there are two main challenges. One challenge is backpropagating the gradient through the entire multistep iteration process. This can be addressed by the method mentioned in Section 2.3. However, previous works still struggle with overfitting, where fine-tuned images exhibit similar patterns (see Figure 6). While these patterns may enhance reward scores, they often lead to unsatisfactory images. We find that this is primarily because the text-irrelevant parts constitute significant portion of the optimization direction. Specifically, we fine-tune the generator ϵθ using the reward signal Lr = 1 ExT ,pR(x θ(1 ExT ,pR(x 0)) = Ep(CP (p)T ExT θ(CX (x 0, p), with the gradient calculated as follows: 0, p)) = θExT ,p(CP (p) CX (x = Ep((ηV + (p))T ExT θ(CX (x 0))), 0))) (7) where the item ηV + (p) controls the gradient direction of ϵθ. According to Section 4.1, the textirrelevant component comprises large portion of the entire item, overwhelming the gradient and producing similar output patterns regardless of the text input p. To mitigate this overfitting problem, we fine-tune the generator ϵθ using reweighted gradient: θ(1 ExT ,pR(x 0, p)) Ep((ω(ηV) + (8) where ω is the reweighting factor for the common direction V. This addresses the overfitting problem mentioned above. Additionally, this analysis clarifies why the original CLIP is ineffective for reward fine-tuning (see Figure 6). The issue arises because its text-irrelevant component is not well-trained (see Section 4.1), resulting in an undefined optimization direction. (p))T ExT θ(CX (x 0))),"
        },
        {
            "title": "Preprint version",
            "content": "Table 1: R@1 results for 5k text-to-image retrieval using different CLIP-based models. CLIP-H Single Average HPSv2 Single Average Pickscore Single Average Denscore Single Average CP (p) (p) 86.10 85.80 80.40 85.14 42.34 67.94 16.72 64.28 54.00 67.60 31.84 64. 83.96 87.24 75.90 91.86 Figure 4: FID and Denscore results for diffusion models with different text encodings. By combining the methods from the two sections above, we arrive at our complete LongAlign method. Detailed algorithms are available in Appendix and C."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "In this section, we first provide our experimental setup, including models, training strategies, and evaluation metrics. We then present the results of our segment preference models. Next, we detail our long-text encoding results for diffusion models using various encoding methods. The advantages of gradient reweighting fine-tuning across different reward signals are then discussed. Finally, we present the generation results using our entire LongAlign alongside those of the baselines. 5.1 EXPERIMENTAL SETUP Model. Our experiments cover three types of models: text encoders, Unets, and preference models. Specifically, we utilize the pretrained CLIP and T5 models as our text encoders. To align the embedding dimensions, we append two-layer MLP to T5s output, following LaVi-Bridge (Zhao et al., 2024a). The Unet is derived from the pretrained Stable Diffusion v1.51. Instead of full fine-tuning, we fine-tune the Unets using LoRA with rank of 32 and update both the ResNet blocks and attention blocks within the Unets. For the preference model, we select two pretrained models: Pickscore and HPSv2. Additionally, we introduce our new segment-level preference model, Denscore. All three models are fine-tuned from pretrained CLIP models. Training. (1) For training the Unet, we utilize dataset of approximately 2 million images, including 500k from SAM (Kirillov et al., 2023), 100k from COCO2017 (Lin et al., 2014), 500k from LLaVA (a subset of the LAION/CC/SBU dataset), and 1 million from JourneyDB (Sun et al., 2024). We randomly reserve 5k images for evaluation. All images are recaptioned using LLaVA-Next (Liu et al., 2023) or ShareCaptioner (Chen et al., 2023b) and resized to 512 512 pixels. We optimize the model using the AdamW optimizer with learning rate of 3105, 2k-step warmup, and total batch size of 192. Training is conducted on 6 A100-40G GPUs for 30k steps over 12 hours. (2) For the reward fine-tuning (RFT) stage of the Unet, we use the same settings as before but with batch size of 96 and 4k total training steps over 8 hours. (3) For training the segment preference model, we use the same settings as for Pickscore (Kirstain et al., 2023), employing CLIP-H on Pickscores training data, along with LLaVA-Next captions and our new segment-level loss function. More details about training the preference model can be found in Appendix B.1. Evaluation. We evaluate our methods using the FID, Denscore and Denscore-O metrics on the 5kimage evaluation dataset. Additionally, we employ GPT-4o (OpenAI, 2024) to evaluate 1k images against baselines, mitigating the risk of overfitting to Denscore. The GPT-4o evaluation template can be found in Appendix D. All our experiments employ UniPC (Zhao et al., 2024b) with 25-step sampling, maintaining consistent classifier-free guidance factor as per the original papers. 1https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-"
        },
        {
            "title": "Preprint version",
            "content": "Table 2: FID and Denscore results for 512512 image generation using different foundation models. PlayG-2 is from Li et al. (2024a), and KanD-2.2 is from Razzhigaev et al. (2023). Model SD-1.5 SD-2.1 PlayG-2 PixArt-α KanD-2.2 longSD (S) longSD (S+R) 24.96 FID-5k Denscore-O 29.20 20.29 Denscore 25.80 30.15 20.91 23.92 28.80 21.22 22.36 33.48 22.78 20.04 33.30 22.70 20.09 31.29 21.72 19.63/24.28 32.83/35.26 22.74/23. Figure 5: FID and Denscore results for diffusion models using different gradient reweighting factors. 5.2 SEGMENT PREFERENCE MODEL Here, we compare the pretrained CLIP and single-value preference models: Pickscore and HPSv2, with our new segment-level preference model, Denscore. According to Section 4.1, the textirrelevant embedding reflects the text-irrelevant preference, while the text-relevant embedding (p) reflects the T2I alignment, which is our objective. To assess the ability of the text-relevant part (p), we provide the R@1 retrieval accuracy for the four models on the 5k evaluation dataset in Table 1. Here, we either utilize the embedding encoding from the first 77 tokens in single-pass approach (Single) or take the average of segment-level embeddings Cseg (p) (Average). The final score is computed using the image embedding CX (x) with either the full text embedding CP or solely the text-relevant part (p). We find that: (1) Across all preference models, the text-relevant embedding outperforms the original embedding, as it eliminates the impact of text-irrelevant factors. (2) Only Denscore with segment-level training yields better segment-level retrieval accuracy, while the other models dont benefit from, and are confused by, the extra information provided by averaging multiple embeddings. (3) Although T2I alignment is only partial optimization objective in Denscore, the Denscore model outperforms CLIP. This highlights the importance of segment-level encoding strategies and preference decomposition. In Appendix B.3, to better evaluate Denscores performance under varying text lengths, we employ different maximum sentence prompts to obtain R@1 retrieval accuracy. Additionally, we conduct an experiment in the same appendix to identify specific misaligned segments in long inputs, showing that segment-level scoring provides more detailed information. 5.3 LONG-TEXT ENCODING Here, we provide FID and Denscore results for supervised fine-tuning (SFT), comparing various text encoding methods: CLIP with concatenation (CLIP-cat), T5 with an additional two-layer MLP (T5-mlp)2, and combination of CLIP and T5 (CLIP+T5). The results in Figure 4 show that CLIPcat and T5-mlp perform similarly, while the CLIP+T5 model significantly outperforms them. This 2We utilize LaVi-bridges (Zhao et al., 2024a) pretrained MLP, which means we only include the short-tolong fine-tuning."
        },
        {
            "title": "Preprint version",
            "content": "Figure 6: Generation results using different reward signals, with and without gradient reweighting. The corresponding text conditions can be found in Appendix F. suggests that the CLIP+T5 model is preferable, as it leverages the strengths of both CLIPs imagetext paired pretraining and T5s pure long-text encoding capabilities. This finding is consistent with the results of previous works (Saharia et al., 2022; Li et al., 2024b), while these earlier models still face input length limitations due to CLIPs maximum token count. 5.4 REWARD FINE-TUNING To simplify and accelerate ablation studies of reward fine-tuning (RFT), we leverage both LCMLoRA (Song et al., 2023; Luo et al., 2023) and DRTune (Wu et al., 2024) to speed up fine-tuning using only CLIP-cat encoding for optimal strategy identification in this subsection. Weighting Factor. In Figure 5, we provide the FID and Denscore results for different gradient reweight factors ω and training steps. According to the first three experiments with different reweight factors, the FID results exhibit parabolic shape, with ω = 0.3 achieving the best FID results. In the last three experiments with different training steps, only ω = 0.3 maintains relatively stable FID results while simultaneously improving both Denscore and Denscore-O results. In contrast, the other two options improve only one of the Denscore or Denscore-O metrics, accompanied by significant increase in FID, indicating an apparent overfitting problem. Additionally, please note that the optimal value of ω can vary depending on the model and training strategy used. Reward Signal. In Figure 6, we provide visual results showing the benefits of gradient reweighting on reward signals from various preference models, including CLIP, HPSv2, Pickscore, and our Denscore. Notably, this method also partially addresses the limitation of using reward signals from pretrained CLIP. However, CLIP cannot yet outperform preference models because its text-irrelevant component is not well-trained, which aligns with our analysis in Section 4.2. 5.5 GENERATION RESULT Foundation Model. In this subsection, we apply our entire LongAlign approach to train our long Stable Diffusion (longSD) and compare it against other baselines. Here, we present the results of SFT (S) at 28k steps and SFT+RFT (S+R) with ω = 0.3, at 1.25k and 3.75k steps. As shown in Table 2, SFT+RFT clearly outperforms both the original SD-1.5 and the basic SFT version. Compared to other advanced foundation models, our longSD model surpasses them in terms of FID and Denscore metrics. Furthermore, we use GPT-4o (OpenAI, 2024) to evaluFigure 7: GPT-4o evaluation results of ate 1k longSD(S+R) results, comparing them to baselines T2I alignment across different models. and mitigating the risk of overfitting. The new results (see Figure 7) are consistent with our previous scores. We also provide visualizations in Figure 8. All these findings highlight the effectiveness of our method for generating high-quality images from long texts, demonstrating significant potential beyond altering the model structure."
        },
        {
            "title": "Preprint version",
            "content": "Figure 8: The generation results show comparison of performance before and after our fine-tuning on SD 1.5. The corresponding text conditions are provided in Appendix F. Method Table 3: Evaluation for comparison in the P2I diffusion framework. Alignment Strategy. In addition to varying model structures, there are other methods that improve T2I alignment. Some of these approaches incorporate the assistance of LLMs, such as Ranni (Feng et al., 2024) and RPG-Diffusers (Yang et al., 2024), while others employ improved training strategies, such as Paragraph-to-Image (P2I) diffusion (Wu et al., 2023a). LLM-based methods that involve additional LLM assistance increase computational requirements and encounter significant out-of-distribution (OOD) issues with long-text inputs. More details on OOD issues can be found in Appendix E. Regarding improved training strategies, our approach is orthogonal to P2I diffusion, so we compare the original P2I diffusion with its fine-tuned version using our method. The results in Table 3 also show clear improvements in terms of FID, Denscore, and GPT-4o evaluations. FID-5k 20.36 Denscore-O 34.45 23.43 Denscore 240 GPT-4o 19.78 34.78 23.47 21.60 38.71 25.39 583 20.84 38.51 25.41 536 +ours +ours 1024 P2I P2I"
        },
        {
            "title": "6 RELATED WORK",
            "content": "Diffusion-based Generation. The popularity of diffusion models has surged, driven by breakthroughs in fast sampling methods (Song et al., 2021a; Lu et al., 2022; Zhang & Chen, 2022) and text-conditioned generation (Saharia et al., 2022; Chen et al., 2023a). Moreover, cascaded and latent space models (Ho et al., 2022; Rombach et al., 2022) have enabled the creation of high-resolution images. These advancements have also unlocked various applications beyond T2I generation, including the ability to create content with consistent style (Hertz et al., 2023) and image editing tools (Hertz et al., 2022; Brooks et al., 2023). Furthermore, advancements in foundation models like U-ViT (Bao et al., 2023) and DiT (Peebles & Xie, 2023) suggest even greater potential. Text-to-Image Evaluation. Traditional metrics, such as Inception Score (IS) (Salimans et al., 2016) and Fréchet Inception Distance (FID) (Heusel et al., 2017), have limitations in evaluating the quality of T2I generation. To address these limitations, three approaches have emerged. One approach employs Perceptual Similarity Metrics like LPIPS (Zhang et al., 2018), which utilize pretrained models to assess image similarity from human perspective. Another approach involves using detection models (Huang et al., 2023) to extract and analyze key object alignments. third approach fine-tunes human preference models (Xu et al., 2024; Kirstain et al., 2023; Wu et al., 2023b) on datasets of human preferences for images generated from specific prompts, effectively turning them into proxies for human evaluation. Reward Fine-tuning. Training text-to-image models with reward signal can be effective in targeting specific outcomes. Two primary approaches have emerged. One leverages reinforcement learning to optimize rewards that are difficult to calculate using traditional methods, such as DPOK (Fan et al., 2024) and DDPO (Black et al., 2023). Another approach, exemplified by DiffusionCLIP (Kim et al., 2022), DRaFT (Clark et al., 2023) and AlignProp (Prabhudesai et al., 2023), involves backpropagation through sampling. This method leverages human preferences and other differentiable reward signals to optimize diffusion models for specific targets. Recently, DRTune (Wu et al., 2024) further improves training speed by stopping the gradient of the diffusion models input."
        },
        {
            "title": "7 DISCUSSION",
            "content": "In this paper, we propose LongAlign to enhance long-text to image generation. We examine segment-level text encoding strategies for processing long-text inputs in both T2I diffusion models and preference models. In addition, we enhance the role of preference models by analyzing their structure and decomposing them into text-relevant and text-irrelevant components. During reward fine-tuning, we propose gradient reweighting strategy to reduce overfitting and enhance alignment. In our experiments, we utilize the classical SD-1.5 and effectively fine-tune it to outperform stronger foundation models, demonstrating significant potential beyond designing new model structures. The limitation of this paper is that our method still does not fully capture the generation of the exact number of entities specified by prompts, partly due to the constraints of CLIP. In the future, we will investigate more powerful training strategies beyond CLIP-based models."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2266922679, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023a. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023b. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-toimage diffusion for accurate instruction following. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 47444753, 2024. Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration problem in training natural language generation models. arXiv preprint arXiv:1907.12009, 2019. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pp. 68406851, 2020."
        },
        {
            "title": "Preprint version",
            "content": "Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23 (47):133, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24262435, 2022. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2023. Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024a. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, ..., and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024b. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:1761217625, 2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information Processing Systems, 2022. Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. OpenAI. Gpt-4o system card, 2024. URL https://openai.com/index/ gpt-4o-system-card/."
        },
        {
            "title": "Preprint version",
            "content": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 22562265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben In InternaPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2021b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023."
        },
        {
            "title": "Preprint version",
            "content": "Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024. Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model. arXiv preprint arXiv:2311.14284, 2023a. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023b. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20962105, 2023c. Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models. arXiv preprint arXiv:2405.00760, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering textto-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, and Kwan-Yee Wong. Bridging different language models and generative vision models for text-to-image generation. arXiv preprint arXiv:2403.07860, 2024a. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024b. Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion. arXiv preprint arXiv:2403.05121, 2024. Zhi-Hua Zhou. brief introduction to weakly supervised learning. National science review, 5(1): 4453, 2018."
        },
        {
            "title": "A CONCATENATION STRATEGY",
            "content": "For the CLIP encoder in T2I diffusion models, we first show the pipeline of our new segmentlevel encoding in Figure 9. In this pipeline, segment-level encoding and concatenation may seem straightforward, but the optimal concatenation strategy remains unclear, as shown in Figure 10. This is because each segment contains special tokens, such as <sot>, <eot>, and <pad>, leading to their corresponding embeddings unintended repeated presence in the final concatenated embedding. This raises the question of whether to retain, remove, or replace them in the final embeddings. For <pad> tokens in each segment, we omit them in the final embeddings since they lack alignment information. However, we sometimes need to introduce new <pad> embedding to ensure aligned token sequence lengths. To address this, we assign unique embedding to each required <pad> tokens position. Specifically, our unique <pad> embedding is the average value of all <pad> tokens of an empty sentence, which we denote as <pad*>. For <sot> and <eot> tokens, we then experiment with pretrained diffusion models to find the optimal strategy. As shown in Figure 10, our results indicate that the optimal approach is to keep all <sot> token embeddings and remove all <eot> token embeddings. Therefore, our final embeddings used in this paper take the form <sot> Text1. <sot> Text2. ... <pad*>. Figure 9: The visualization of our new segment-level text encoding for diffusion models is presented. Figure 10: Generation results under different embedding concatenation strategies."
        },
        {
            "title": "B SEGMENT PREFERENCE MODEL",
            "content": "Here, we provide detailed analysis of our segment preference models with respect to training, visualization, and evaluation. B.1 TRAINING We train segment preference model by incorporating long and detailed text conditions generated by LLaVA-Next alongside our segment-level preference loss function. Based on the analysis in Section 4.1, we choose to combine the segment-level loss for refining text-relevant aspects with the original loss for improving aspects unrelated to text, such as aesthetics. The loss function Lseg-a ij (where denotes addition) is: Lseg-a ij =Ex,{ ˆpk}σ (cid:32) (cid:88) (CX (xi) CP (ˆpk)/K) (cid:88) (CX (xj) CP (ˆpk)/K) + (cid:33) k=1 Ex,pσ(CX (xi) CP (p) CX (xj) CP (p)), k= (9) where σ(x) = 1 CP (ˆpk) with influencing the text-irrelevant part. The new loss function Lseg-o em+en = σ(m n). In addition, we substitute 1+ex is the sigmoid function and (ˆpk) to help the new segment-level loss focus on the T2I alignment part and avoid ij (where means orthogonal) is: em Lseg-o ij =Ex,{ ˆpk}σ (cid:32) (cid:88) (CX (xi) (ˆpk)/K) (cid:88) (CX (xj) (ˆpk)/K) + (cid:33) k=1 Ex,pσ(CX (xi) CP (p) CX (xj) CP (p)), k=1 (10) where {ˆpk} represents the segments split from the long text generated by LLava-Next, and is the original short text. If we prioritize alignment, Lseg-a ij is the better choice; if we want to balance both alignment and aesthetics, Lseg-o ij to train our new segment preference model. ij is the preferable option. In this paper, we choose Lseg-o B.2 VISUALIZATION After training the new segment preference models, we want to analyze the behavior of three scores CX (x) CP (p), CX (x) (p) and CX (x) ηV. We are interested in the relationships among them, and additionally, in the differences in these scores between different CLIP-based models. Here, we choose CLIP, Pickscore and our Denscore as our experimental targets, as they all use the same model structure, while the last two are fine-tuned on Pickscores human preference dataset. We present the results in Figures 11 and 12. The first figure uses 20 data pairs to visualize the relationships and differences, while the second figure displays the actual statistics based on 5,000 data pairs. For the original score CX (x) CP (p), only the original CLIP behaves in such way that its score can be close to zero when the image and text inputs are unpaired, while the other two models still assign relatively high scores to such unpaired inputs. This is because the preference models assess the image not only based on text alignment but also on other purely visual factors, such as aesthetics. For the text-relative score CX (x) (p), after removing the influence of the text-irrelevant part, all three models provide nearly zero scores for unpaired input data, which supports our analysis that this score focuses on the T2I alignment and explains why this score achieves the best retrieval results for preference models, as shown in Table 1. For the third score CX (x)ηV, we have analyzed the scalar η in the main paper, which is determined entirely by the text input and is image-irrelevant. Here, we focus on the text-irrelevant score CX (x) V. We find that this score is strongly positive for the two preference models. This corresponds to pure visual factors in preference, as shown in Figure 3. On the other hand, for the original CLIP, this score is nearly zero. This indicates that the common direction of text embedding is almost orthogonal to the image embeddings, which do not contribute to the final score of CLIP."
        },
        {
            "title": "Preprint version",
            "content": "Figure 11: Logit results for different models, both before and after orthogonal decomposition. Figure 12: The real data statistics for the diagonal paired data and the off-diagonal unpaired data. B.3 EVALUATION To better support the above analysis, we design two additional experiments to test the effectiveness of our score alongside the experiment in Section 5.2. Firstly, to evaluate Denscores performance under varying text lengths, we employ different maximum sentence prompts to obtain R@1 retrieval accuracy, as shown in Table 4. We find that our Denscore, using segment-level training, consistently outperforms the others, except in the one-sentence setting. In this one-sentence setting, our segment-level training becomes less meaningful, but our results still outperform other existing preference models. Additionally, to identify specific misaligned segments in long inputs, we conduct an experiment, illustrated in Figure 13, to show that segment-level scoring provides more detailed information. In some cases, certain segments align better with the first image, while other segments align better with the second image. This makes the overall score relatively meaningless, while segment-level scores continue to perform well."
        },
        {
            "title": "Preprint version",
            "content": "Table 4: R@1 results for 5k text-to-image retrieval with varying maximum numbers of sentences. max number 1 2 3 6 8 CLIP HPSv2 Pickscore Denscore 53.06 41.86 42.34 52.72 70.90 53.66 53.86 72.70 76.70 56.48 57.56 78. 79.62 59.14 60.22 83.10 83.00 62.58 63.60 88.16 84.12 63.96 63.54 89."
        },
        {
            "title": "C DECOMPOSED PREFERENCE OPTIMIZATION",
            "content": "Here, we provide the pseudocode in Algorithm 1 for the entire decomposed preference optimization pipeline discussed in this paper. Algorithm 1 Decomposed Preference Optimization for T2I Diffusion Models 1: Input: Long-text input , Initial T2I diffusion model , Preference model 2: Output: Fine-tuned T2I diffusion model ˆM 3: Segment(T ) {Step 1: Divide into segments (e.g., sentences) as = {s1, s2, . . . , sn}} 4: for each segment si do 5: 6: end for 7: Concatenate(E1, E2, . . . , En) {Step 3: Concatenate segment embeddings (as shown in Ei Encode(si) {Step 2: Encode segment si (as shown in Section 3.1 and Figure 9)} Section 3.1 and Figure 9)} 8: (E) {Step 4: Generate image from embeddings using the T2I diffusion model} 9: Eimage, Esegment R(I, S) {Step 5: Compute the Preference Embeddings of segments and image (as shown in Section 3.2)} (cid:80)n 10: Eoverall 1 11: (Etext-relevant, Etext-irrelevant, ηscaler) Decompose(Eoverall) {Step 6: Decompose overall score into i=1 Esegment {Compute overall average Embedding} relevant and irrelevant components (as shown in Section 4.1)} 12: Lloss Eimage Etext-relevant + ωirrelevantEimage ηscalerEtext-irrelevant {Step 7: Calculate adjusted loss (as shown in Section 4.2)} 13: ˆM FineTune(M, Lloss) {Step 8: Fine-tune T2I model using computed loss (as shown in Section 2.3)} 14: Return ˆM GPT-4O EVALUATION The input for evaluation consists of prompt and two images generated for this prompt using different models. We utilize the multimodal evaluation capabilities of GPT-4o and employ the following Python code for evaluation: def eval_fn(prompt, image1, image2): template = f\"\"\" {prompt} Which image is more consistent with the above prompt? Please respond with \"first\", \"second\" or \"tie\", no explanation needed. \"\"\" completion = client.chat.completions.create( model=\"gpt-4o-2024-05-13\", messages=[{ \"role\": \"system\", \"content\": \"You are an image generation evaluation expert.\", \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": template}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data: image/jpeg;base64,{image1}\"}}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data: image/jpeg;base64,{image2}\"}}, ]}])"
        },
        {
            "title": "Preprint version",
            "content": "Figure 13: Identifying the best-aligned image and segment pairs using segment-level preference scores."
        },
        {
            "title": "Preprint version",
            "content": "E OUT-OF-DISTRIBUTION PROBLEM Some of these approaches incorporate the assistance of LLMs, such as Ranni and RPG-Diffusers. LLM-based methods that involve additional LLM assistance increase computational requirements and encounter significant out-of-distribution (OOD) issues with long-text inputs. There are several reasons why existing LLM-based methods struggle with long-text inputs. Long texts contain more facts than shorter ones, and LLMs rely on injecting different subprompts into specific subareas. Some facts may be disjoint, while others overlap (e.g., detailed descriptions of single object), complicating the separation of information in pixel space. Additionally, some LLMbased methods fine-tune the LLM model that may not include datasets with long and detailed inputs. As result, these models often cannot manage such situations for generating effective layout plans to assign different subprompts across different areas. While current LLM-based methods are not fully effective, we believe this OOD problem can be resolved in the future. Additionally, LLM-based methods complement our approach: while we concentrate on training more powerful foundation models, these techniques can further enhance results during the sampling stage. Here are some examples of OOD problems: Ranni. In our experiment with Ranni, approximately half of the prompts resulted in errors before generating the final output. We selected examples from the remaining successful prompts. The example prompt is generated from the first image in Figure 14. Although Ranni generates the layout initially, it creates too many subareas that do not contribute clearly to the final result, as shown in the second image in Figure 14. Figure 14: The reference image and the corresponding generated image using Ranni. The complete input prompt is The image is digital artwork featuring female character standing in rocky environment. The character is dressed in fantasy-style armor with predominantly dark color scheme, highlighted by accents of blue and purple. The armor includes corset-like bodice, skirt, and arm guards, all adorned with intricate designs and patterns. The characters hair is styled in high ponytail, and she has serious expression on her face. The armor is illuminated by purple glow, which appears to emanate from the characters body, creating contrast against the darker elements of the armor and the surrounding environment. The glow also casts soft light on the characters face and the armor, enhancing its details and textures. The background consists of rocky landscape with purple hue, suggesting magical or otherworldly setting. The rocks are jagged and uneven, with some areas appearing to be on fire, adding to the dramatic and intense atmosphere of the image. There are no visible texts or logos in the image, and the style of the artwork is realistic with focus on fantasy elements. The image is likely intended for gaming or fantasy-themed context, given the characters attire and the overall aesthetic. RPG-Diffusers. For RPG-Diffusers, the example prompt is generated from the first image in Figure 15. However, they assign two subareas: the left side and the right side of the image. Using these subpromptsThe black leather sofa is stylishly centered in the frame, showcasing its plush texture and elegant design, the distinct cushions maintain the streamlined look of the piece, offering comfort against the stark contrast of the white background. and The three luxurious black cushions are artfully arranged, emphasizing the sofas inviting nature while seamlessly blending into the cohesive monochrome theme of the setting.results in repetition of soft in the generated image, as shown in the second image in Figure 15."
        },
        {
            "title": "Preprint version",
            "content": "Figure 15: The reference image and the corresponding generated image using RPG-Diffusers. The complete input prompt is The image presents scene dominated by black leather sofa, which is the central object in the frame. The sofa, with its three cushions, is positioned against stark white background, creating striking contrast. The cushions, like the sofa, are black and appear to be made of leather, suggesting uniform color scheme throughout the piece. The sofa is designed with wooden frame, adding touch of warmth to the otherwise monochrome setting. The frame is visible on both the front and back of the sofa, providing stability and support. The arms of the sofa, like the frame, are made of wood, maintaining the overall aesthetic of the piece. The sofa is set against white background, which accentuates its black color and wooden frame, making it the focal point of the image. There are no other objects in the image, and no text is present. The relative position of the sofa is central, with ample space around it, further emphasizing its importance in the image. The image does not depict any actions, but the stillness of the sofa suggests sense of tranquility."
        },
        {
            "title": "F TEXT CONDITION FOR VISUAL RESULT",
            "content": "In this section, we provide the text conditions for the visual results in Figure 1 and Figure 6. The text conditions are as follows: The text conditions for Figure 1 are as follows: 1. The image presents 3D rendering of horse, captured in profile view. The horse is depicted in state of motion, with its mane and tail flowing behind it. The horses body is composed of network of lines and curves, suggesting complex mechanical structure. This intricate design is further emphasized by the presence of gears and other mechanical components, which are integrated into the horses body. The background of the image is dark blue, providing stark contrast to the horse and its mechanical components. The overall composition of the image suggests blend of organic and mechanical elements, creating unique and intriguing visual. 2. The image presents close-up view of human eye, which is the central focus. The eye is surrounded by vibrant array of flowers, predominantly in shades of blue and purple. These flowers are arranged in semi-circle around the eye, creating sense of depth and perspective. The background of the image is dark blue sky, which contrasts with the bright colors of the flowers and the eye itself. The overall composition of the image suggests theme of nature and beauty. Folder 1 Image 836 3. The image presents detailed illustration of submarine, which is the central focus of the artwork. The submarine is depicted in three-quarter view, with its bow facing towards the right side of the image. The submarine is constructed from wood, giving it rustic and aged appearance. It features dome-shaped conning tower, which is common feature on submarines, and large propeller at the front. The submarine is not alone in the image. It is surrounded by variety of sea creatures, including fish and sharks, which are swimming around it. These creatures add sense of life and movement to the otherwise static image of the submarine. The background of the image is light beige color, which provides neutral backdrop that allows the submarine and the sea creatures to stand out. However, the background is not devoid of detail. It is adorned with various lines and text, which appear to be map or chart of some sort. This adds an element of intrigue to the image, suggesting that the submarine might be on mission or an expedition. Overall, the image is detailed and intricate piece of art that captures the essence of submarine"
        },
        {
            "title": "Preprint version",
            "content": "voyage, complete with the submarine, the sea creatures, and the map in the background. Its snapshot of moment in time, frozen in the image, inviting the viewer to imagine the stories and adventures that might be taking place beneath the surface of the water. 4. In the image, theres charming scene featuring green frog figurine. The frog, with its body painted in vibrant shade of green, is the main subject of the image. Its wearing straw hat, adding touch of whimsy to its appearance. The frog is positioned in front of white window, which is adorned with green plant, creating harmonious color palette with the frogs body. The frog appears to be looking directly at the camera, giving the impression of friendly encounter. The overall image exudes sense of tranquility and simplicity. 5. The image portrays female character with fantasy-inspired design. She has long, dark hair that cascades down her shoulders. Her skin is pale, and her eyes are striking shade of blue. The characters face is adorned with intricate gold and pink makeup, which includes elaborate patterns and designs around her eyes and on her cheeks. Atop her head, she wears crown made of gold and pink roses, with the roses arranged in circular pattern. The crown is detailed, with each rose appearing to have glossy finish. The characters attire consists of gold and pink dress that is embellished with what appears to be feathers or leaves, adding to the fantasy aesthetic. The background of the image is dark, which contrasts with the characters pale skin and the bright colors of her makeup and attire. The lighting in the image highlights the characters features and the details of her makeup and attire, creating dramatic and captivating effect. There are no visible texts or brands in the image. The style of the image is highly stylized and artistic, with focus on the characters beauty and the intricate details of her makeup and attire. The image is likely digital artwork or concept illustration, given the level of detail and the fantastical elements present. 6. The image captures scene of large, modern building perched on cliff. The building, painted in shades of blue and gray, stands out against the backdrop of cloudy sky. The cliff itself is mix of dirt and grass, adding touch of nature to the otherwise man-made structure. In the foreground, group of people can be seen walking along path that leads up to the building. Their presence adds sense of scale to the image, highlighting the grandeur of the building. The sky above is filled with clouds, casting soft, diffused light over the scene. This light enhances the colors of the building and the surrounding landscape, creating visually striking image. Overall, the image presents harmonious blend of architecture and nature, with the modern building seamlessly integrated into the natural landscape. The text conditions for Figure 6 are as follows: 1. The image is digital artwork featuring female character standing in rocky environment. The character is dressed in fantasy-style armor with predominantly dark color scheme, highlighted by accents of blue and purple. The armor includes corset-like bodice, skirt, and arm guards, all adorned with intricate designs and patterns. The characters hair is styled in high ponytail, and she has serious expression on her face. The armor is illuminated by purple glow, which appears to emanate from the characters body, creating contrast against the darker elements of the armor and the surrounding environment. The glow also casts soft light on the characters face and the armor, enhancing its details and textures. The background consists of rocky landscape with purple hue, suggesting magical or otherworldly setting. The rocks are jagged and uneven, with some areas appearing to be on fire, adding to the dramatic and intense atmosphere of the image. There are no visible texts or logos in the image, and the style of the artwork is realistic with focus on fantasy elements. The image is likely intended for gaming or fantasy-themed context, given the characters attire and the overall aesthetic. 2. The image presents scene of elegance and luxury. Dominating the center of the image is brown Louis Vuitton suitcase, standing upright. The suitcase is adorned with gold handle and gold lock, adding touch of opulence to its appearance. Emerging from the top of the suitcase is bouquet of pink and white roses, interspersed with green leaves. The roses, in full bloom, seem to be spilling out of the suitcase, creating sense of abundance and luxury. The entire scene is set against white background, which accentuates the colors of the suitcase and the roses. The image does not contain any text or other discernible objects. The relative position of the objects is such that the suitcase is in the center, with the bouquet of roses emerging from its top. 3. The image captures the grandeur of the Toledo Town Hall, renowned landmark in Toledo, Spain. The building, constructed from stone, stands tall with two prominent towers on either"
        },
        {
            "title": "Preprint version",
            "content": "side. Each tower is adorned with spire, adding to the overall majesty of the structure. The facade of the building is punctuated by numerous windows and arches, hinting at the intricate architectural details within. In the foreground, pink fountain adds splash of color to the scene. few people can be seen walking around the fountain, their figures small in comparison to the imposing structure of the town hall. The sky above is clear blue, providing beautiful backdrop to the scene. The image is taken from low angle, which emphasizes the height of the town hall and gives the viewer sense of being in the scene. The perspective also allows for detailed view of the building and its surroundings. The image does not contain any discernible text. The relative positions of the objects confirm that the town hall is the central focus of the image, with the fountain and the people providing context to its location."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Sea AI Lab, Singapore",
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}