{
    "paper_title": "<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs",
    "authors": [
        "Sergey Pletenev",
        "Daniil Moskovskiy",
        "Alexander Panchenko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems."
        },
        {
            "title": "Start",
            "content": "<think> So lets replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs Sergey Pletenev1,2, Daniil Moskovskiy1,2, Alexander Panchenko2,1 1AIRI, 2Skoltech, {S.Pletenev, Daniil.Moskovskiy, A.Panchenko}@skol.tech 5 2 0 2 0 1 ] . [ 1 8 5 3 8 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with drop in performance of up to 30% in joint metrics. The root cause is identified as critical lexical diversity gap: LLMs generate toxic content using small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems. Warning: The paper contains text that readers may find offensive or disturbing."
        },
        {
            "title": "Introduction",
            "content": "The rapid adoption of Large Language Models for synthetic data generation has revolutionized many NLP tasks (Sun et al., 2023; Ye et al., 2022). However, their effectiveness in sensitive and nuanced domains, such as text detoxification, is not well explored yet. Text detoxification, the task of rewriting toxic text into neutral form while preserving meaning (Logacheva et al., 2022), requires training data that reflects the vast diversity of real-world harmful language. Type Example Sentence Human-Generated would vote the s**t out of you we need to go kick their as**s man go somewhere and f**k yourself Unique Insults Used: 3 LLM-Generated would f***ing cast my vote for you We gotta f***ing smash those a**es Man, get the f**k out of here! Unique Insults Used: 1 (f**k) Table 1: comparison of toxic language generated by humans versus an LLM for similar underlying sentences. Human examples from the ParaDetox dataset demonstrate greater lexical diversity. In contrast, LLMs tend to overuse single, high-frequency insult. This paper addresses critical question: Can LLMs fully replace human annotators when generating toxic language for parallel dataset intended for detoxification? Although the application of LLMs for text detoxification shows promise (Mukherjee et al., 2024), it presents fundamental challenge: generating authentic, varied, and nuanced toxic language is arguably more difficult than neutralizing it. As shown in Table 1, human-generated toxicity often uses variety of insults, while LLMs tend to fall into repetitive patterns. We conduct comprehensive study using various LLMs (Llama 3, Qwen3) to synthesize toxic data. Our findings reveal that: Models trained on fully synthetic data significantly underperform those trained on humanannotated data. Corresponding author. LLMs exhibit lexical diversity gap, generText Toxification Prompt Rewrite the following text into toxic language and add profanity if possible. You must match the target style and preserve the original meaning as much as possible. Here are few examples: {few shot} Neutral text: {neutral text}. Toxic text: Figure 1: Side-by-side evaluation results. Figure 2: System prompt for toxic data generation. ating repetitive and narrow range of toxic expressions. Relying on such data risks creating ineffective detoxification systems that fail on real-world text. Our work serves as cautionary analysis, highlighting the current limitations of LLMs for generating high-quality toxic data and reaffirming the value of human annotation in this critical domain. We have made the code for evaluation and generation publicly available"
        },
        {
            "title": "2 Related Work",
            "content": "Recent research has focused on distilling the capabilities of large LLMs into smaller, more efficient models. For Text Style Transfer (TST), this often involves using LLMs to generate pseudo-parallel data (Zhang et al., 2024). In the context of text detoxification, Moskovskiy et al. (2024) successfully used activation-patched LLMs (Arditi et al., 2024) to create high-quality neutral rewrites from existing toxic sentences. Their work showed that models trained on data with human-toxic, synthetic-neutral pairing can achieve performance comparable to fully humanannotated datasets. Our work investigates the inverse and more challenging task: generating the toxic half of the pair from neutral source. We explore whether this approach, which could theoretically produce infinite training data, is viable substitute for human data collection. 1https://github.com/A1exRey/Lessons-from-GeneratingToxic-Texts"
        },
        {
            "title": "3 Methodology",
            "content": "Our methodology is designed to test the viability of fully synthetic data for text detoxification. We generate toxic text from neutral sources using several LLMs, train standard detoxification model on this data, and evaluate its performance against human-annotated baseline. Synthetic Data Generation. We explore toxification from two types of source text: 1. ParaDetox (Logacheva et al., 2022): The neutral portion of this dataset serves as clean, non-toxic source. 2. SST-2 (Socher et al., 2013): We use the negative reviews from this dataset to test more challenging scenariolayering toxicity onto an existing negative sentiment. We use suite of activation-patched LLMs to generate toxic paraphrases, including Llama 3 (8B, 72B), Qwen3 (8B, 32B), and Cogito v1 (8B), model with explicit reasoning capabilities. This allows us to assess performance across different model scales and architectures. The prompt used for generation is shown in Figure 2. In order to increase the variety and quality of generation of each of the models, we used the min = 0.1 (Nguyen et al., 2025). According to the author, such generation methodology increases the variety of responses, which is important in the context of our research. Model Training and Evaluation. Following prior work (Moskovskiy et al., 2024; Logacheva et al., 2022), we fine-tune bart-large model on each of our generated synthetic datasets. We then evaluate these models on the original, humanannotated ParaDetox test set. Source Data Generator Model Size Reasoning STA SIM FL (J) Human ParaDetox SST-2 Llama 3 Qwen3 Cogito Llama 3 Qwen3 Cogito v1 8B 72B 8B 32B 8B 8B 72B 8B 32B 8B 0. 0.634 0.865 0.481 0.827 0.850 0.794 0. 0.620 0.634 0.645 0.623 0.854 0.844 0.847 0.854 0.434 0.451 0.428 0. -0.047 -0.030 -0.053 -0.026 0.868 0.619 0.862 0. -0.022 0.864 0.826 0.812 0.868 0.481 0.559 0.544 0.490 0.764 0. 0.800 0.787 0.322 0.362 0.349 0.338 -0.159 -0.119 -0.132 -0.143 0. 0.472 0.801 0.334 -0.147 Table 2: Detoxification performance of BART models. The Reasoning column indicates generator models with explicit reasoning capabilities. The overall best performance in each metric is bolded. (J), highlights the best ( green ) and worst ( red ) performance drops within each source data group. We use the standard evaluation pipeline from Dementieva et al. (2023), measuring Style Transfer Accuracy (STA), Similarity (SIM), Fluency (FL), and Joint metric (J) that combines all three. To add qualitative dimension, we also conduct sideby-side human evaluation using GPT-4.1 as judge to compare the outputs of our best synthetic models against the human-data baseline."
        },
        {
            "title": "4 Results",
            "content": "Our results consistently demonstrate that detoxification models trained on synthetic toxic data fail to match the performance of those trained on humanannotated data. We find the primary cause to be significant gap in lexical diversity."
        },
        {
            "title": "4.1 Performance on Synthetic Data",
            "content": "As shown in Table 2, the baseline model trained on human data achieves the highest score of 0.481. All models trained on synthetic data underperform this baseline. The (J) column quantifies this performance drop, which is most severe for models trained on data derived from SST-2 (up to -0.159). This degradation is largely driven by sharp fall in the SIM score, indicating that layering toxicity onto already-negative text often distorts the original meaning."
        },
        {
            "title": "4.2 The Lexical Diversity Gap",
            "content": "To understand the cause of this performance drop, we analyzed the diversity of toxic terms in the Human Data Llama 3 (8B) Qwen3 (32B) s**t (6080) f**k (3328) f***ing (2678) a** (1483) b***h (889) f***ing (8223) s**t (5140) f**k (3266) a** (1707) s****d (1618) f***ing (15413) d**n (3297) s**t (3286) f**k (2949) h**l (2813) Table 3: Top 5 most frequent toxic terms in humanannotated data versus representative LLM-generated data. Note the over-representation of single slur in the LLM output. training data. Table 4 shows clear correlation between training data diversity and model effectiveness. The human-annotated data contains the most diverse vocabulary (390 unique insults), and the model trained on it is the most effective at detoxification (leaving only 34 unique insults on the test set). In contrast, the synthetic datasets are less diverse, which directly impacts the downstream models ability to generalize. This lack of diversity is not just about the number of unique terms but also their distribution. As shown in Table 3, human data has more balanced distribution of frequent slurs. In contrast, the LLMgenerated data is highly skewed, with Qwen3-32B using the term f***ing over 15,000 timesmore than double the frequency of the most common term in the human data. This repetition leads to models that are over-fitted to narrow set of expressions. Source Generator Train Diversity () Test Failures () Human ParaDetox SST-2 Llama 3-8B () Llama 3-72B () Qwen3-8B () Qwen3-32B () Cogito v1-8B () Llama 3-8B () Llama 3-72B () Qwen3-8B () Qwen3-32B () Cogito v1-8B () 390 342 293 320 367 310 353 326 363 386 371 34 45 37 45 36 36 42 47 49 40 Table 4: Analysis of training data diversity vs. model effectiveness. Train Diversity measures unique insults in the training data ( higher is better). Test Failures measures unique insults remaining after detoxification ( lower is better). The bold values show the baseline is superior on both metrics."
        },
        {
            "title": "4.3 Human Evaluation",
            "content": "To assess the practical impact of the lexical diversity gap, we conducted side-by-side evaluation using GPT-4.1 as an expert judge. Figure 1 shows the win rates for models trained on synthetic data versus the human-annotated baseline, excluding ties. The results confirm significant qualitative difference. The baseline model was consistently preferred, achieving win rates between 51% and 62% across all comparisons. The most pronounced gap was for the Llama 3 70B SST-2 model, where the baseline was preferred in 62% of non-tied decisions. This outcome reinforces our central thesis: the repetitive and stereotypical nature of the LLMgenerated toxic data leads to detoxification models that are less nuanced and effective in practice, flaw readily identified in qualitative comparisons."
        },
        {
            "title": "5 Conclusion",
            "content": "While it is technically possible to use LLMs to generate toxic text for detoxification training, our findings show that this approach is not yet viable replacement for human annotation. We identified critical lexical diversity gap: current LLMs produce toxic language that is repetitive and lacks the variety of human expression. This gap leads to detoxification models with significantly lower performance and poor generalization to real-world scenarios. Our work highlights the importance of data diversity in sensitive domains and suggests that future research should focus on methods to enhance the stylistic complexity of LLM-generated text before it can be reliably used for tasks like detoxification. Potential Risks & Ethical Considerations We acknowledge that bypassing the safety mechanisms of LLMs, as done in this research via activation patching, can be misused to generate harmful content. Our work is intended to improve text detoxification systems by demonstrating the current limitations of synthetic data. We warn that the technologies explored herein could be applied for malicious purposes, and we advocate for responsible research and development in this area."
        },
        {
            "title": "References",
            "content": "Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. Refusal in language models is mediated by single direction. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Daryna Dementieva, Daniil Moskovskiy, David Dale, and Alexander Panchenko. 2023. Exploring methods for cross-lingual text style transfer: The case of text detoxification. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, IJCNLP 2023 -Volume 1: Long Papers, Nusa Dua, Bali, November 1 - 4, 2023, pages 1083 1101. Association for Computational Linguistics. Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko. 2022. ParaDetox: Detoxification with parallel data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68046818, Dublin, Ireland. Association for Computational Linguistics. Daniil Moskovskiy, Sergey Pletenev, and Alexander Panchenko. 2024. LLMs to replace crowdsourcing for parallel data creation? the case of text detoxiIn Findings of the Association for Comfication. putational Linguistics: EMNLP 2024, pages 14361 14373, Miami, Florida, USA. Association for Computational Linguistics. Sourabrata Mukherjee, Atul Kr. Ojha, and Ondrej Dusek. 2024. Are large language models actually good at text style transfer? In Proceedings of the 17th International Natural Language Generation Conference, INLG 2024, Tokyo, Japan, September 23 - 27, 2024, pages 523539. Association for Computational Linguistics. Minh Nhat Nguyen, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, and Ravid ShwartzZiv. 2025. Turning up the heat: Min-p sampling for creative and coherent llm outputs. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA. Association for Computational Linguistics. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. 2023. Text In Findclassification via large language models. ings of the Association for Computational Linguistics: EMNLP 2023, pages 89909005, Singapore. Association for Computational Linguistics. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. 2022. ZeroGen: Efficient zero-shot learning via dataset generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1165311669, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Chiyu Zhang, Honglong Cai, Yuezhang Li, Yuexin Wu, Le Hou, and Muhammad Abdul-Mageed. 2024. Distilling text style transfer with self-explanation from In Proceedings of the 2024 Conference of llms. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, NAACL 2024, Mexico City, Mexico, June 18, 2024, pages 200211. Association for Computational Linguistics."
        }
    ],
    "affiliations": [
        "AIRI",
        "Skoltech"
    ]
}