{
    "paper_title": "FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing",
    "authors": [
        "Advait Gupta",
        "Rishie Raj",
        "Dang Nguyen",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as \"Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A$^*$ search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A$^*$ search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent \"FaSTA$^*$'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A$^*$ search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA$^*$ is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 1 9 0 2 . 6 0 5 2 : r FaSTA: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing Advait Gupta Rishie Raj Dang Nguyen Tianyi Zhou University of Maryland, College Park {advait25,rraj27,dangmn,tianyi}@umd.edu Project: https://github.com/tianyi-lab/FaSTAR"
        },
        {
            "title": "Abstract",
            "content": "We develop cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as Detect the bench in the image while recoloring it to pink. Also, remove the cat for clearer view and recolor the wall to yellow. It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local search per subtask to find costefficient toolpatha sequence of calls to AI tools. To save the cost of on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding human-like fast-slow toolpath agent FaSTA: fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate."
        },
        {
            "title": "Introduction",
            "content": "Various practical applications require multi-turn image editing, which demands applying sequence of heterogeneous operationsobject detection, segmentation, inpainting, recoloring, and moreeach ideally handled by specialized AI tool. While it is challenging for existing text-to-image models to tackle these long-horizon tasks directly, they can be broken down into sequence of easier subtasks. Hence, tool-using agent has the potential to address each subtask by careful planning of toolpath, i.e., sequence of tool calls. Since AI tools output quality and cost can vary drastically across different tasks and even samples, the planning often heavily depends on accurate estimation of the quality and cost of all applicable tools per step. This raises challenges to the exploration efficiency due to the great number of possible toolpaths and expensive computation of AI tools. Large language model (LLM) agents usually excel at fast planning that breaks down an imageediting task into sequence of high-level subtasks without extensive exploration, thanks to their strong prior. However, they often misestimate AI tools cost and quality due to lack of up-to-date knowledge and the specific properties of each subtask. Moreover, they also suffer from hallucinations, e.g., choosing an expensive diffusion model instead of simple filter when the latter suffices to complete the subtask. Compared to LLM agents, classical search requires slow, expensive exploration on dependency graph of tools, which in return can accurately produce an optimal, verifiable toolpath. Preprint. Under review. Table 1: Top-5 frequently used subroutines extracted by inductive reasoning in FaSTA from 100 CoSTA toolpaths, ranked by their total selection frequency irrespective of the corresponding subtask. Subroutine Subtask Frequency YOLO [37] SAM [19] SD Inpaint [32] Grounding DINO [24] SAM [19] SD Inpaint [32] Object Recoloration YOLO [37] SAM [19] SD Inpaint [32] Object Recoloration YOLO [37] SAM [19] SD Inpaint [32] Object Replacement Grounding DINO [24] SAM [19] SD Erase [32] Object Removal Object Removal 57/100 49/100 48/100 25/100 22/100 Can we combine the strengths of LLM agents in planning efficiency and search in editing accuracy to achieve an efficient, cost-sensitive solution for multi-turn image editing? recent work, CoSTA [9], proposes to leverage an LLM agent for high-level subtask planning, producing pruned subgraph of tools on which an search can be performed efficiently to find cost-quality balanced toolpath. Despite its effectiveness in challenging image editing tasks and advantages over other baselines, the search remains computational bottleneck. Unlike humans who can learn reusable actions or create tools from past experiences and accumulate such knowledge over time, CoSTA is test-time only approach, so its exploration on previous tasks cannot be reused to improve or accelerate the planning for future tasks. In this paper, we study how to further reduce the cost of toolpath search by reusing the knowledge learned from explored tasks, common feature of human learning. Inspired by photo editing applications that allow users to record their frequently used actions for future reuse, we propose to extract the repeatedly incurred subroutines of tool calls from the successful toolpaths of explored tasks. This is achieved automatically by performing inductive reasoning on LLMs given previous toolpaths. Each subroutine is represented by symbolic rule under identifiable conditions, e.g., if object_area θ and mask_ratio > ϕ, then YOLO [37] SAM [19] SD Inpaint [32] for Object Recoloration. Table 1 summarizes the top five subroutines extracted by LLMs from CoSTA toolpaths on 100 tasks, while Figure 1 illustrates their reuse rates. They show an unexplored reusability of subroutines for image editing tasks. Motivated by this observation, we propose neurosymbolic LLM agent with learnable memory, Fast-Slow Toolpath Agent (FaSTA), that keeps mining symbolic subroutines from previous experiences and reuses them in the exploration and planning for future tasks. Its exploration stage can be explained as novel and critical improvement to existing in-context reinforcement learning (ICRL): instead of recording all previous paths and drawing contexts from them for future exploration, FaSTA condenses these paths to few reusable subroutines by inductive reasoning and uses these principles to guide future tasks more effectively. As an imitation of the fast-slow planning in human cognition, the reusable subroutines enable fast planning of FaSTA in the planning stage. In addition to the subtask-level planning in CoSTA, the LLM agent in FaSTA further chooses subroutine for each subtask. Only when there does not exist any subroutine for the subtask, or when the subroutines output fails to pass the quality check by VLMs, FaSTA will resort to the slow planning, i.e., search on low-level subgraph of tools for the subtask. Hence, FaSTA can entirely avoid the expensive low-level search if the fast plan succeeds. Moreover, as more tasks have been explored and more subroutines collected, most incoming tasks can be handled by fast planning, and only very unique or rare tasks require slow planning. Our main contributions can be summarized as: 1. memory of Symbolic Subroutine learned by LLMs: We extract reusable subroutines as symbolic rules from explored tasks toolpaths by inductive reasoning on LLMs. They serve as high-level actions and significantly reduce the exploration cost for future tasks. 2. Fast-Slow Planning of Toolpaths: We develop neurosymbolic toolpath agent FaSTA that benefits from fast planning (LLMs subtask planning and selection of symbolic subroutines) to produce toolpath efficiently, and only invokes slow search when the fast planning fails. 3. FaSTA achieves better costquality trade-offs across diverse tasks than most baselines. Compared to CoSTA, it can save the cost by 49.3% with the price of merely 3.2% quality degradation. 2 Figure 1: Inductive Reasoning of Reusable Subroutines. Left: Reuse rate (% of applicable subtasks where subroutine was utilized) of the top-5 learned subroutines. Right: Success rate (%) of fast planning (subroutines only, without search) on subtasks for held-out test set of tasks. It increases exponentially as more reusable subroutines are extracted from an increasing number of explored tasks. Figure 2: Top: Online learning (induction) and refinement of reusable subroutines from explored toolpaths for previous tasks. Bottom: Adaptive fast-slow planning framework in FaSTA. Given new task, FaSTA first uses an LLM to generate high-level plan of subtasks and then select subroutine per subtask, yielding fast plan. Only when the subroutines output does not pass the quality check by VLMs, slow planning by search on the subtasks tool subgraph will produce toolpath for the subtask."
        },
        {
            "title": "2 Related Work",
            "content": "[33]; Multi-turn Image Editing. Generative models excel at image synthesis and single-edits [35, 33, 26, 12], (e.g., [32]; [3]), but multi-turn editing from composite instructions presents unique challenges. Controllability techniques (e.g., ControlNet [47], sketch-guidance [36], layout-synthesis [4, 23]) improve short-sequence precision. However, iteratively applying models like InstructPix2Pix [2] or those from MagicBrush [45] often lacks long-sequence coherence without sophisticated planning. Agentic systems (e.g., GenArtist [40], CLOVA [6]) decompose tasks for tool [29]; [17]; 3 use. Nevertheless, efficient execution, especially for recurrent operation patterns, is key challenge, with lack of learning from past patterns causing redundant computational effort. Tool-use Agent and Planning. LLMs as reasoning agents coordinating tools show broad impact [46, 8, 27, 22], especially in image editing for orchestrating specialized tools [10, 6, 40, 34]. MLLM agents (e.g., CoSTA [10], GenArtist [40], SmartEdit [16]) plan by decomposing goals; yet, others like Visual ChatGPT [42] and MM-REACT [43] often explore tool calls greedily, ignoring budgets, while dialog systems enable iterative refinement [13]. Agent planning often leverages LLM emergent reasoning [44, 14, 15, 11], sometimes augmented by explicit reasoning steps [44, 41]. However, generating efficient, optimal low-level tool sequences for complex image editing is challenging, as LLMs may falter without further guidance or search [15, 5]. CoSTA [10] improves this with LLM-planned search, but intensive search costs persist without experience reuse. Robust learning and symbolic reuse of common, successful tool sequences are largely missing, hindering efforts to reduce planning costs and build scalable, adaptive image editing agents."
        },
        {
            "title": "3 Preliminaries: CoSTA∗ and Efficient Toolpath Search",
            "content": "We build FaSTA based upon recent framework, CoSTA [9], to find cost-efficient toolpaths for multi-turn image editing. CoSTA uses an LLM agent to prune high-level subtask graph on which low-level search is performed to determine the final toolpath. We provide brief overview of CoSTA below. More comprehensive details are available in Appendix D. Foundational Components of CoSTA. CoSTA utilizes three pre-defined knowledge structures: Tool Dependency Graph (TDG) to map prerequisite input/output dependency relationships between AI tools. It can be automatically generated or human-crafted. Model Description Table (MDT) to catalog AI tools, their supported subtasks (e.g., object detection, recoloration), and input/output. Benchmark Table (BT) with cost/quality data for (subtask, tool) pairs collected from published works. It is used to initialize the heuristics in search. These elements collectively enable mapping from high-level task requirements to specific tool sequences and their predicted performance. CoSTA Planning Stages. The planning in CoSTA unfolds in three main stages: 1. Subtask-Tree Generation: An LLM interprets the users natural language instruction and input image to produce subtask tree, Gss. This tree decomposes the overall task into smaller subtasks. CoSTA allowed this tree to have parallel branches representing alternative subtask plans.1 2. Tool Subgraph Construction: The subtask tree is then used to identify relevant tools from the MDT and their dependencies from the TDG. This forms final Tool Subgraph for each task, Gts, which contains all tools for all subtasks required to accomplish the final editing task. 3. Cost-Sensitive Search: CoSTA performs an search on Gts to find an optimal toolpath. The search is guided by cost function (x) = g(x) + h(x), where g(x) is the accumulated actual cost (considering time and VLM-validated quality) and h(x) is the estimated heuristic value (derived from the BT) estimating the cost-to-go till leaf node. VLM checks after tool executions allow for dynamic updates and path corrections upon failure."
        },
        {
            "title": "4 Fast-Slow Toolpath Agent (FaSTA∗)",
            "content": "4.1 From CoSTA to FaSTA While CoSTA [9] offers solid foundation for tackling multi-turn image editing, its reliance on search, especially for complex tasks with large Tool Subgraphs, can be computationally intensive. Moreover, CoSTA is test-time planning method that cannot learn from existing experiences to 1We found that single branch is sufficient in practice and reduces CoSTAs complexity (see Appendix E). 4 accelerate future tasks planning. This may lead to inefficient, repeated search of the same subroutines, given our observation of many recurring ones across tasks. To further reduce the cost of CoSTA on search and avoid exploring the same subroutines repeatedly, we equip CoSTAs hierarchical planning with online learning of symbolic subroutines frequently used in explored tasks toolpaths, and choose from them to address similar subtasks in later tasks, resulting in novel, efficient In-Context Reinforcement Learning (ICRL) [25] framework. FaSTA still follow CoSTAs initial step of decomposing each task into chain of subtasks, but saves considerable computation by novel fast-slow planning that lazily triggers search for each subtask only when the selected subroutine fails. First, we introduce system for online inductive reasoning of subroutines (Section 4.2), where FaSTA learns from past successful (and unsuccessful) editing experiences. It identifies frequently used, effective subsequences of tool calls for subtasks (subroutines) and the general conditions under which they perform well. Second, these learned subroutines form the backbone of an adaptive fast-slow execution strategy (Section 4.3). Instead of immediately resorting to detailed search, FaSTA first attempts to apply Fast Plan composed of these proven subroutines. If this fast plan is unsuitable for subtask or fails quality check, FaSTA then dynamically engages more meticulous slow planning of tool calls for each subtask by search. This fast-slow planning allows FaSTA to handle many tasks rapidly by selecting from learned subroutines. However, it retains the ability to perform deeper, more complex searches when faced with novel or challenging scenarios. The goal is to achieve better balance of computational cost and high-quality, making complex image editing more practical and efficient. 4.2 Online Learning and Refinement of Reusable Subroutines Our first core contribution is neurosymbolic method for automatically discovering and refining reusable subroutines from execution data (i.e., traces representing the detailed execution data logged for each subtask) during online operation. subroutine Ps = (t1, t2, . . . , tk) represents frequently observed, ordered sequence of tool calls ti Vtd that effectively accomplish specific subtask under certain conditions Cs. The goal is to learn and maintain dynamic library of rules = {(Pj, Cj, sj)}M j=1 mapping subtasks and context features to cost-effective subroutines, stored in Subroutine Rule Table (see Appendix for the structure). Our approach diverges from standard ICRL, which often grapples with cumbersome raw experience logs and inefficient generalization. FaSTA employs an LLM for an explicit inductive reasoning step, analyzing execution traces not just to sample past experiences, but to synthesize compact, symbolic (subroutine, activation rule, subtask) knowledge leading to more interpretable and generalizable rules. Crucially, to ensure the generalizability of subroutines and fair evaluation, the inductive reasoning of subroutines is performed on held-out set of new diverse tasks (e.g., random internet images with new complex prompts) excluded from the benchmark. This online learning of symbolic rules and subroutines aims to create an interpretable and off-the-shelf library of action rules that can be periodically augmented and refined based on new experiences (Algorithm 1, Appendix F). The online learning and adaptation cycle in FaSTA involves the following key stages: 1. Data Logging: FaSTA continuously records detailed execution data τ (or traces) from each subtask. The motivation is to capture rich, contextualized data about what paths were taken, under what conditions (e.g., object size from YOLO, mask details from SAM, LLM-inferred context like background complexity), and with what outcomes (cost, quality, failures). This logged data serves as the raw experiences for subroutine learning. Full details are provided in Appendix K. 2. Periodic Refinement: To balance continuous learning with operational stability, the refinement process is triggered periodically (every = 20 tasks), using the most recent batch of accumulated traces (Trecent). This ensures the system adapts to evolving patterns without excessive computational overhead. 3. Inductive Reasoning by LLM: This is the core knowledge synthesis step. The LLM is prompted with Trecent and the current rule set to perform inductive reasoning, which analyzes these experiences, identifies recurring successful subroutines, and infers the contextual conditions (activation rules Cj) under which they are effective, or proposes modifications to existing rules. This allows FaSTA to generate new hypotheses about efficient strategies. Appendix provided the detailed prompts. 4. Verification and Selection of Subroutines: Recognizing LLM-proposed rules as hypotheses, this stage rigorously validates each change before integrate it into to ensure beneficial, robust subroutines and prevent degradation from flawed rules. Validation uses specialized test datasets against baseline (CoSTA or current FaSTA). Net Benefit score (balancing cost/quality) determines acceptance, with an LLM-based retry mechanism for refinement. Further details and evaluation protocols are provided in Appendix F.4 and Appendix J, respectively. More details of inductive reasoning can be found in Appendix F. This online adaptation loop allows FaSTA to continuously learn from its operational data, building and refining library of costeffective, validated subroutines that enhance its fast planning capabilities. Figure 1 shows the reuse rate of learned subroutines and how they improve the success rate of fast planning with increasing experiences. More details are provided in Appendix G. 4.3 Adaptive Fast-Slow Planning Execution Path Table 2: Adaptive Fast-Slow Planning Fallback Statistics. It shows the percentage of subtasks that can be addressed by subroutines vs. those requiring fallback to low-level search. Following the generation of the subtask chain (Section 3) and the online refinement of the subroutine rule library (Section 4.2), FaSTA employs its second main contribution: an adaptive fast-slow planning and execution strategy. This approach aims to drastically reduce execution cost by prioritizing an efficient fast plan of learned subroutines, while retaining the robustness of search for novel or failed subtasks via localized slow plan fallback. Figure 2 illustrates this process, and Algorithm M.2 details the execution flow. The process involves two key phases: fast plan generation and the adaptive fast-slow execution. High-Level (Subroutine Success) Low-Level Fallback (No Subroutine or Check Fails) Percentage 91% 9% Initially, FaSTA generates high-level Fast Plan Msubseq without search. 1. Fast Planning. An LLM (GPT-4o, with prompt in Appendix O) takes an input image, user prompt, subtask plan s1:N , and the subroutine rule set to select an optimal subroutine Psi or None for each subtask si. The selection considers the activation rules Cj associated with each potential subroutine Pj and the current image context. If the context satisfies Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling activation rules for multiple subroutines Pj applicable to si, the one minimizing the cost-quality tradeoff score Cavg(Pj)α (2 Qavg(Pj))2α is chosen, where Cavg(Pj) and Qavg(Pj) are the averaged cost and quality of subroutine Pj in different existing toolpaths, and α is used-defined trade-off coefficient as in CoSTA [9]. This process yields the fast plan Msubseq = (Ps1, . . . , PsN ). 2. Adaptive Fast-Slow Execution. The fast plan Msubseq is then executed sequentially. Fast Plan Attempt: For each planned subtask si, FaSTA calls the tools within Psi sequentially and check the quality of each tool execution using VLM as in CoSTA ([9, Appendix I, Fig. 12] lists details regarding the VLM check criteria). Slow Planning Trigger: If the fast plan cannot find any subroutine for si (i.e., si =None), or if any tool execution in Psi fails to pass the quality check, FaSTA switches to the slow planning for the current subtask si. Hereby, the detailed low-level subgraph Glow(si) for si is first constructed as per CoSTA [9, Sec. 4.2]. Subsequently, an search is performed on this Glow(si) to find an optimal path to complete si, employing the same cost function and search algorithm as defined in CoSTA [9, Sec. 4.34.5]. This adaptive fast-slow planning ensures that the more efficient fast plan is executed by default. In contrast, the robust but more expensive slow planning by is invoked lazily only for those subtasks where the fast plan fails. The fallback statistics of fast-slow planning is reported in Table 2."
        },
        {
            "title": "5 Experiments",
            "content": "We conduct extensive experiments to evaluate the effectiveness of FaSTA. We aim to answer: (1) How does our method compare to the state-of-the-art CoSTA in terms of execution cost and 6 Table 3: Detailed Quality Comparison (Average Human Evaluation Score) across Task Types and Complexities. Baseline results are from CoSTA [9]. Both FaSTA and CoSTA adopt balanced cost-quality trade-off by setting α = 1. Task Type FaSTA (Ours) CoSTA[9] VisProg [11] CLOVA[6] GenArtist[40] Instruct Pix2Pix[2] MagicBrush[45] Task Category Image-Only Tasks Text+Image Tasks 1-2 subtasks 3-4 subtasks 5-6 subtasks 7-8 subtasks 2-3 subtasks 4-5 subtasks 6-8 subtasks Overall Accuracy Image Tasks Text+Image Tasks All Tasks 0.92 0.91 0.92 0.91 0.91 0.92 0.91 0.91 0.91 0.91 0.94 0.93 0.93 0.95 0.93 0.94 0.94 0.94 0.93 0. 0.88 0.76 0.62 0.46 0.61 0.50 0.38 0.69 0.49 0.62 0.91 0.77 0.63 0.45 0.63 0.51 0.36 0.70 0.50 0. 0.93 0.85 0.71 0.61 0.67 0.61 0.56 0.78 0.61 0.73 0.87 0.74 0.55 0.38 0.48 0.42 0.31 0.64 0.40 0. 0.92 0.78 0.51 0.46 0.62 0.40 0.26 0.67 0.43 0.59 output quality? (2) How robust is the fast-slow planning approach, and what is the impact of its core components? All experiements have been conducted on single NVIDIA A100 GPU. 5.1 Experimental Setup Dataset. We evaluate our method on the benchmark dataset curated and released alongside CoSTA [9]. This dataset comprises 121 image-prompt pairs, featuring complex multi-turn editing instructions involving 1-8 subtasks per image (amounting to 550 total image manipulations or turns across the dataset), covering both imageonly and mixed image-and-text manipulations [9, Appendix D]. Its diversity and complexity make it suitable for assessing the cost-saving potential and robustness of our adaptive planner. Baselines. Our primary baseline is the original CoSTA algorithm [9], which represents the state-of-the-art cost-sensitive planner using search on pruned tool subgraph. In our ablation studies and discussions, we refer to this as the \"Low-Level Only\" approach. We also acknowledge other agentic and non-agentic baselines evaluated in the CoSTA paper (e.g., GenArtist [40], CLOVA [6], InstructPix2Pix [2], MagicBrush [45]), primarily to contextualize the Pareto optimality results, while noting their limitations in handling the full range of tasks or performing cost-sensitive optimization. Figure 3: Execution time (seconds) per image. FaSTA and CoSTA costs vary with tradeoff coefficient α. Baseline costs are from CoSTA [9]. Evaluation Metrics. We adopt the evaluation methodology from CoSTA [9] for fair comparisons: Quality (Human Evaluation): Task success is measured by human evaluation. Following [9], each subtask si within task is assigned score A(si) {0, x, 1}, where (0, 1) represents partial correctness based on predefined rules. The task accuracy A(T ) is the average of its subtask scores, and the overall accuracy is the average A(T ) across the dataset [9]. We rely on human evaluation due to the limitations of automated metrics like CLIP [28] in capturing nuanced errors in complex, multi-step, multimodal editing tasks. More details about the reasons for resorting to human evaluation can be found in Sec. 5.2 of [9]. More details about the evaluation process and rules for assigning partial scores are mentioned in Appendix I. Cost (Execution Time): Efficiency is measured by the total execution time in seconds for each inference, including any necessary retries. 5.2 Main Results Performance Analysis. As shown in Table 3 and Figure 3, FaSTA achieves average quality remarkably close to the original CoSTA method, with only minimal drop of approximately 3.2% in accuracy. However, the benefits in efficiency are substantial. Our approach reduces the 7 Figure 4: Qualitative comparison of FaSTA with CoSTA [9] and other leading image editing agents for complex multi-turn tasks. FaSTA achieves visual results identical to CoSTA and significantly surpasses other baselines in accuracy and coherence. Notably, FaSTA delivers this high quality at roughly half the execution cost of CoSTA, highlighting its superior efficiency. average execution cost by over 49.3%, achieving costs nearly half of CoSTA. This demonstrates the effectiveness of the adaptive slow-fast strategy: by leveraging learned subroutines for common cases (fast planning), we drastically cut down on expensive exploration within low-level tool subgraphs, while the fallback mechanism (\"slow planning\") ensures that quality is not significantly compromised when subroutines are unsuitable or fail. Pareto Optimality Analysis. Figure 5 illustrates the Pareto frontiers achieved by varying the cost-quality tradeoff coefficient α. FaSTA consistently achieves superior frontier compared to CoSTA (Low-Level Only) and dominates the other baselines evaluated in [9]. For any given quality level achieved by CoSTA, our method offers significantly lower cost, and for any given cost budget, our method yields comparable or slightly lower quality. This highlights the adaptability of our approach in catering to different user preferences regarding the balance between execution speed and output fidelity, while consistently operating at more efficient frontier than searching the low-level graph alone. Figure 5: Cost-Quality Pareto Frontier. FaSTA with various α values against CoSTA and other baselines. FaSTA achieves superior frontier, offering better cost-quality trade-offs. Qualitative Analysis Figure 4 provides qualitative comparisons of FaSTA against baselines, illustrating its ability to handle complex multi-turn editing tasks effectively. These examples showcase the practical benefits of our adaptive fast-slow approach in achieving desired edits. For more detailed qualitative analysis, including specific case studies demonstrating subroutine effectiveness, and comparisons of different tool paths under specific conditions, please refer to Appendix C. We also do similar qualitative comparison of FaSTA with the very recent Gemini 2.0 Flash Preview Image Generation on some tasks from the CoSTA benchmark dataset. These results can be seen in Figure 13. 5.3 Ablation Studies Impact of Subroutine Verification. To evaluate the significance of our subroutine verification step (Section 4.2, Step 4), we compared FaSTAs performance with and without this validation. We 8 first used an LLM to propose subroutine changes based on initial task traces. These proposals were then either inducted directly or only after passing our full verification process. The outcomes are in Table 4. The verification step proved crucial: FaSTA with verified subroutines yielded much fewer fallbacks to low-level search. This improvement stems from ensuring only reliable subroutines are adopted, which prevents wasted execution on flawed proposals and avoids misdirecting the agent when low-level search is genuinely needed. Table 4: (Left) Impact of Subroutine Verification; (Right) Fast/Slow planning only vs. FaSTA Method Low-Level Fallback (%) FaSTA (w/o Verification) FaSTA (w/ Verification) 28% 9% Method Avg. Quality Avg. Cost (s) Fast plan Only Slow plan Only FaSTA (Ours) 0.84 0.93 0.91 27.5 46.8 29.5 Fast planning only vs. FaSTA. To demonstrate the importance of the low-level fallback (slow planning), we evaluated variant that uses only the high-level subroutines for subtasks where subroutines are possible. If subroutine failed its VLM check or if no subroutine was selected (None), the planner simply failed for that subtask, without activating the low-level graph. As shown in Table 4, relying solely on the high-level subroutines leads to significant drop in quality compared to our full adaptive fast-slow planning approach. While the cost is slightly lower (due to avoiding any low-level search), the brittleness is unacceptable. Figure 6 illustrates typical failure case. While the High-Level Only approach fails the task, FaSTA successfully recovers by falling back to the low-level search, demonstrating the critical role of the \"slow planning\" component for robustness and overall task success. Slow planning only vs FaSTA. This ablation represents the original CoSTA algorithm (using the refined single-path prompt). As shown in Table 4, CoSTA achieves marginally higher output quality than FaSTA. However, this slight quality gain comes at the expense of significantly higher average execution costs. The broader performance comparison, detailed in Table 3 and Figures 3 and 5, further highlights our superior cost-efficiency. 5.4 Summary and Limitations Our results indicate that FaSTAs adaptive fast-slow execution strategy significantly improves computational efficiency while largely maintaining the high quality of the original CoSTA approach. Ablation studies confirm the necessity of both the low-level fallback mechanism and the subroutine verification process for robustness and efficiency. Despite its promising performance, FaSTA has few limitations. Its initial performance on entirely new tasks may be suboptimal until sufficient experiences are gathered for effective subroutine learning (cold start). Furthermore, overall performance and the ability to capture nuanced rules for complex tasks remain dependent on the evolving capabilities of the underlying LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced FaSTA, neurosymbolic agent designed to significantly enhance the efficiency of complex, multi-turn image editing tasks. By integrating an online subroutine mining mechanism that leverages LLM-based inductive reasoning in modified version of In-Context Reinforcement Learning, FaSTA learns and refines library of effective tool-use sequences and their activation conditions from experiences. This learned knowledge underpins an adaptive fastslow execution strategy, where Fast Plan of subroutines made by LLMs is prioritized, with localized search (Slow Plan) serving as robust fallback for novel or challenging subtasks. Our experiments demonstrate that FaSTA achieves image quality comparable to the state-of-the-art CoSTA but at substantially reduced computational cost, effectively addressing key bottleneck in prior methods. We believe that FaSTAs approach of combining learned symbolic shortcuts with principled search offers promising direction for developing more agile, cost-sensitive, and continually improving AI agents for complex tasks."
        },
        {
            "title": "References",
            "content": "[1] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text detection. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 93659374. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00959. [2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. [4] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance, 2023. [5] Xiaohan Fu, Shuheng Li, Zihan Wang, Yihao Liu, Rajesh K. Gupta, Taylor Berg-Kirkpatrick, and Earlence Fernandes. Imprompter: Tricking llm agents into improper tool use, 2024. URL https://arxiv.org/abs/2410.14923. [6] Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. CLOVA: closed-loop visual assistant with tool usage and update. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1325813268. IEEE, 2024. doi: 10.1109/CVPR52733.2024.01259. [7] Google Cloud. Google Cloud Vision API, 2024. URL https://cloud.google.com/vision. Accessed: January 29, 2025. [8] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving, 2024. URL https://arxiv.org/abs/2309.17452. [9] Advait Gupta, NandaKiran Velaga, Dang Nguyen, and Tianyi Zhou. Costa: Cost-sensitive toolpath agent for multi-turn image editing, 2025. URL https://arxiv.org/abs/2503. 10613. [10] Advait Gupta, NandaKiran Velaga, Dang Nguyen, and Tianyi Zhou. Costa: Cost-sensitive toolpath agent for multi-turn image editing, 2025. URL https://arxiv.org/abs/2503. 10613. [11] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning In IEEE/CVF Conference on Computer Vision and Pattern Recognition, without training. CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1495314962. IEEE, 2023. doi: 10.1109/CVPR52729.2023.01436. [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. URL https://arxiv. org/abs/2208.01626. [13] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation, 2024. [14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022. URL https: //arxiv.org/abs/2201.07207. [15] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey, 2024. URL https://arxiv.org/abs/2402.02716. [16] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. Smartedit: Exploring complex instruction-based image editing with multimodal large language models, 2023. 10 [17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks, 2018. [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. Segment anything. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 39924003. IEEE, 2023. doi: 10.1109/ ICCV51070.2023.00371. [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything, 2023. [20] Rakpong Kittinaradorn, Wisuttida Wichitwong, Nart Tlisha, Sumitkumar Sarda, Jeff Potter, Sam_S, Arkya Bagchi, ronaldaug, Nina, Vijayabhaskar, DaeJeong Mun, Mejans, Amit Agarwal, Mijoo Kim, A2va, Abderrahim Mama, Korakot Chaovavanich, Loay, Karol Kucza, Vladimir Gurevich, Márton Tim, Abduroid, Bereket Abraham, Giovani Moutinho, milosjovac, Mohamed Rashad, Msrikrishna, Nishad Thalhath, RaitaroHikami, and Shakil Ahmed Sumon. cwittwer/easyocr: Easyocr, July 2022. [21] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. Deblurgan: Blind motion deblurring using conditional adversarial networks, 2018. [22] Xinzhe Li. review of prominent paradigms for llm-based agents: Tool use (including rag), planning, and feedback learning, 2024. URL https://arxiv.org/abs/2406.05804. [23] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation, 2023. [24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2024. [25] Giovanni Monea, Antoine Bosselut, Kianté Brantley, and Yoav Artzi. Llms are in-context bandit reinforcement learners, 2025. URL https://arxiv.org/abs/2410.05362. [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022. URL https://arxiv.org/abs/2112.10741. [27] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8), January 2025. ISSN 2095-2236. doi: 10.1007/s11704-024-40678-2. URL http://dx.doi.org/10.1007/s11704-024-40678-2. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 2021. [29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. [30] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer, 2020. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1067410685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. 11 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. [34] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners: multi-llm agent, 2024. URL https://arxiv.org/abs/2401.07324. [35] Zineb Sordo, Eric Chagnon, and Daniela Ushizima. review on generative ai for text-toimage and image-to-image generation and implications to scientific images, 2025. URL https://arxiv.org/abs/2502.21151. [36] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models, 2022. [37] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-offreebies sets new state-of-the-art for real-time object detectors, 2022. [38] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In IEEE/CVF International Conference on Computer Vision Workshops, ICCVW 2021, Montreal, QC, Canada, October 11-17, 2021, pages 19051914. IEEE, 2021. doi: 10.1109/ICCVW54120.2021.00217. [39] Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, and Thomas S. Huang. Deepfont: Identify your font from an image, 2015. [40] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing, 2024. [41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [42] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models, 2023. URL https://arxiv.org/abs/2303.04671. [43] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action, 2023. URL https://arxiv.org/abs/2303.11381. [44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https: //arxiv.org/abs/2210.03629. [45] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing, 2024. [46] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges, 2024. URL https://arxiv.org/abs/2401.07339. [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 12 Figure 6: Failure case for High-Level Only execution versus FaSTA. For the task Replace the cat with rabbit, the initially selected high-level subroutine fails to produce satisfactory result, leading to failed output for the High-Level Only approach. In contrast, FaSTA detects this failure, activates its low-level fallback mechanism for the Object Replacement subtask, and performs search to find correct tool sequence, successfully completing the task. Figure 7: Qualitative examples of FaSTAs performance on sample tasks from the MagicBrush dataset [45]. These tasks were processed using the Subroutine Rule Table learned from nonbenchmark data. Notably, all examples shown were successfully completed by FaSTA relying entirely on its fast plan composed of learned subroutines, without needing to resort to the slow planning via A* search for any subtask. Qualitative Generalization to MagicBrush Dataset To assess the generalizability of FaSTAs learned subroutines and its adaptive planning strategy, we tested it on set of sample tasks derived from different benchmark, the MagicBrush dataset [45]. The Subroutine Rule Table (R) used for these evaluations was the one learned through the online inductive reasoning process described in Section 4.2, which utilized diverse non-benchmark image/prompt pairs, ensuring no direct exposure to MagicBrush data during the subroutine learning phase. Our objective here was to observe if the subroutines and the Fast Plan generation logic could effectively handle tasks from dataset with potentially different characteristics and prompt styles without requiring immediate fallback to low-level A* search for every step. Figure 7 presents qualitative results for several such tasks. As illustrated in Figure 7 and noted in its caption, FaSTA was able to successfully complete these diverse editing tasks from the MagicBrush dataset. Significantly, for all the examples shown, the tasks were accomplished entirely through the \"Fast Plan\" execution. This indicates that the learned subroutines and their activation rules, derived from different data sources, were sufficiently general to 13 apply effectively to these new instances, and the LLM was able to compose successful Fast Plan without needing to trigger the low-level A* search fallback for any subtask. This provides positive evidence towards the generalizability of our subroutine learning and adaptive planning approach beyond the specific characteristics of the data used during the online refinement cycles. Benchmark Dataset Rationale: CoSTA* vs. MagicBrush For evaluating FaSTA, we primarily utilized the benchmark dataset introduced with CoSTA* [9]. While the MagicBrush dataset [45] is prominent benchmark for instruction-guided image editing, the CoSTA* dataset was chosen due to its specific characteristics that better align with the capabilities we aim to demonstrate with FaSTA, particularly its efficiency in handling complex, multi-step tasks. The key distinctions motivating our choice are: Task Complexity and Depth: To best showcase FaSTAs advantages in cost-saving through learned subroutines and adaptive planning, complex examples involving greater number of sequential subtasks are essential. The CoSTA* dataset was specifically curated to include such multi-turn editing instructions. In contrast, many tasks in the MagicBrush benchmark, while diverse, often involve simpler, more direct edits that might not fully stress or benefit from sophisticated fast-slow planning approach to the same extent. Multimodal Capabilities: significant aspect of modern image editing involves text-in-image manipulation. The CoSTA* dataset includes substantial portion of tasks requiring multimodal processing (e.g., text replacement, text removal within an image context). The MagicBrush benchmark, as per its original release, primarily focuses on visual edits and does not extensively cover these text-in-image editing scenarios. FaSTA, like CoSTA, is designed to handle such multimodal tasks, making the CoSTA* dataset more suitable for comprehensive evaluation of its capabilities. Number of Manipulations/Turns: While the CoSTA* dataset has 121 image-prompt pairs, which might be fewer than the test set size of some other benchmarks like MagicBrush, the tasks in the CoSTA* dataset are designed to be multi-turn. As noted in Section 5, these tasks involve 1-8 subtasks per image, amounting to 550 total distinct image manipulations (or \"turns\") across the dataset. This provides rich set of complex sequences for evaluating planning efficiency. Table 5: Conceptual Comparison of Dataset Characteristics Relevant to FaSTA. Characteristic CoSTA* Dataset [9] MagicBrush Dataset [45] Primary Focus Max Subtasks per Task Max Tool Subgraph Depth Text-in-Image Editing Complex, Multi-Turn, Multimodal 8 22 Supported Instruction-Guided Visual Edits 3 7 Not supported Table 5 provides conceptual comparison of key characteristics relevant to our evaluation goals. While MagicBrush is invaluable for evaluating general instruction-following in image editing models, the CoSTA* datasets emphasis on longer, multi-faceted tasks involving broader range of subtask types (including multimodal ones) makes it more fitting benchmark to demonstrate the specific cost-saving and adaptive planning strengths of FaSTA. The goal of FaSTA is not just to perform an edit, but to do so efficiently by learning and reusing common multi-step patterns, capability best tested by complex sequential decision-making problems."
        },
        {
            "title": "C Detailed Qualitative Analysis and Subroutine Effectiveness",
            "content": "This section provides more in-depth qualitative examination of FaSTAs performance, focusing on the benefits of learned subroutines and the efficiency gains of the adaptive fast-slow execution strategy. Demonstrating Subroutine Relevance and Efficiency. To illustrate why learning subroutines is beneficial and how FaSTA achieves cost savings, we examined specific cases where input image and 14 Figure 8: Example demonstrating FaSTAs subroutine effectiveness. FaSTA uses learned rules to select optimal paths (e.g., SD Search&Recolor for the small ball in row 1, avoiding SD Inpaints potential failure), achieving results identical to CoSTA at significantly lower average cost (15.21s vs. 25.32s for these examples) by preventing unnecessary exploration of suboptimal paths. prompt conditions matched learned activation rules for particular subroutines P. Figure 8 depicts such scenarios for recoloring tasks, comparing the outcomes and behavior of FaSTA, CoSTA, and all potential tool paths for recoloration. Under specific conditions, certain tool paths are prone to failure or are suboptimal. FaSTA, by leveraging its learned activation rules, can preemptively select an effective subroutine, thus avoiding unnecessary exploration of these less suitable paths. For instance, consider the first row in Figure 8, where the task is to recolor small ball to blue. Paths involving SD Inpaint (such as G-DINOSAMSD Inpaint [32]or YOLOSAMSD Inpaint) often exhibit poor performance or fail VLM quality checks when the target object is very small. FaSTA incorporates an activation rule for its chosen subroutine (in this case, one that selects SD Search&Recolor) that accounts for this factor. Thus, FaSTA directly employs SD Search&Recolor and successfully recolors the ball. In contrast, CoSTA, lacking this specific learned rule for this context, might explore an SD Inpaint-based path first due to its general applicability or heuristic score. Upon failure of this path (indicated by failed VLM check), CoSTA would then use its search to backtrack and explore alternative paths, eventually finding the SD Search&Recolor sequence and completing the task, but at the cost of the initial failed attempt and additional search time. This pattern, where FaSTA makes more informed initial path choice due to its learned subroutine rules, is key to its efficiency. The figure shows that while both FaSTA and CoSTA can arrive at the same high-quality final output, FaSTA does so more directly. For the examples presented in Figure 8, this translated to FaSTA achieving the correct edits at an average execution cost of 15.21s, whereas CoSTA took an average of 25.32s. This highlights how FaSTA, by encoding successful, context-dependent tool sequences as subroutines and applying them based on learned activation rules, effectively halves the execution cost in these scenarios by minimizing costly trial-and-error exploration of paths known to be suboptimal or likely to fail under specific conditions. Detailed Overview of CoSTA Components and Planning This appendix provides more detailed explanation of the core components and planning stages of the CoSTA agent [9], which form the foundation upon which FaSTA builds and modifies. The descriptions here are rephrased from the original CoSTA paper to ensure clarity and avoid direct repetition. 15 D.1 Foundational Knowledge Structures in CoSTA CoSTA leverages three primary pre-defined knowledge structures to inform its planning and search processes: Model Description Table (MDT) The MDT serves as comprehensive catalog of all AI tools available to the agent. For each tool (e.g., YOLO [37], SAM [18], Stable Diffusion [31]), the MDT specifies: The types of subtasks it can perform (e.g., \"Object Detection\", \"Image Segmentation\", \"Object Recoloration\"). CoSTA considered 24 tools supporting 24 distinct subtasks. Its input requirements (e.g., an image, bounding boxes, segmentation masks). The outputs it produces (e.g., bounding boxes, edited image, extracted text). This structured information is essential for mapping abstract subtasks to concrete tool invocations and for constructing the Tool Dependency Graph. An excerpt of CoSTAs MDT structure is shown in [9, Table 1], with the full table in their appendix. Tool Dependency Graph (TDG) The TDG, denoted Gtd = (Vtd, Etd), is directed graph that captures the operational dependencies between the AI tools listed in the MDT. Vtd is the set of all available AI tools. An edge (v1, v2) Etd exists if the output of tool v1 can serve as valid input for tool v2 in the context of certain subtasks. The TDG represents the potential workflow sequences. CoSTA can automatically construct this graph by analyzing the input/output specifications of tools in the MDT, reducing manual effort and facilitating updates as new tools are added [9, Appendix C]. visualization of the TDG used in CoSTA is provided in [9, Fig. 4]. Benchmark Table (BT) The BT is critical resource for the searchs heuristic function. It stores pre-computed or empirically measured performance data for tool-subtask pairs (vi, sj). For each pair, the BT typically includes: Execution Time C(vi, sj): The average time taken by tool vi to perform subtask sj. Quality Score Q(vi, sj): An objective measure of the typical output quality of tool vi for subtask sj, normalized to [0,1] scale for comparability across different subtasks. This data is sourced from existing benchmarks where available, or through dedicated offline evaluations if necessary, as detailed in [9, Sec 3.3]. The complete BT for CoSTA is shown in [9, Table 11]. D.2 CoSTA Planning and Execution Stages The CoSTA agent follows three-stage process to address given multi-turn image editing task: 1. Task Decomposition and Subtask-Tree Generation Given an input image and natural language instruction u, CoSTA employs an LLM to decompose the complex request into sequence of more manageable subtasks. This decomposition results in subtask tree, Gss = (Vss, Ess). Each node vi Vss corresponds to specific subtask si (e.g., \"remove car,\" \"recolor bench to pink\"). Edges (vi, vj) Ess represent dependencies, indicating that subtask si must be completed before sj. The LLM uses prompt template fplan(x, u, S) which includes the image, instruction, and list of supported subtasks S. The original CoSTA framework allowed for the generation of trees with multiple parallel paths if subtasks were independent, representing different valid execution orders. This subtask tree provides high-level plan for addressing the users request. 16 2. Tool Subgraph Construction The abstract subtask tree Gss is then translated into concrete Tool Subgraph Gts = (Vts, Ets), which is the actual graph the search will operate on. For each subtask node si in Gss, the MDT is consulted to find all tools (si) capable of performing si. The TDG is then used to backtrack from these tools to include all necessary prerequisite tools and their interconnections, forming minimal tool subgraph Gi td for that subtask. The final Gts is formed by taking the union of all such Gi td and connecting them according to the dependencies specified in Gss. This subgraph contains all feasible tool sequences for the given task. This construction prunes the global tool graph to smaller, task-relevant search space. 3. Cost-Sensitive Search for Optimal Toolpath CoSTA employs an search algorithm on the Tool Subgraph Gts to find an optimal toolpath that balances execution cost and output quality, according to user-defined trade-off parameter α. Priority Function: The search prioritizes nodes (representing tool executions) based on the function (x) = g(x) + h(x). Actual Execution Cost g(x): This term represents the cumulative cost-quality score of the path taken so far to reach node x. It is computed in real-time as the execution progresses. For path of tool-subtask executions (vj, sj), g(x) is defined as: 2α α (cid:88) (cid:89) g(x) = c(vj, sj) 2 q(vj, sj) j=1 j=1 where c(vj, sj) is the actual measured execution time of tool vj for subtask sj, and q(vj, sj) is the real-time quality score of its output, validated by VLM. The parameter α controls the emphasis on cost versus quality (higher α prioritizes lower cost). These values include only the nodes in the current path and each node is initialized with g(x) = and updated if lower values is found. The start node is initialized with g(x) = 0. Heuristic Cost h(x): This term is an admissible estimate of the cost-to-go from the current node (representing tool-task pair (vi, si)) to goal/leaf node in Gts. It is pre-calculated using values from the Benchmark Table (BT) and considers the trade-off parameter α. For node x, h(x) is recursively defined based on its neighbors y: h(x) = min yNeighbors(x) (cid:0)(hC(y) + C(y))α (2 Q(y) hQ(y))2α(cid:1) where hC(y) and hQ(y) are the cost and quality components of the heuristic for neighbor (initialized to 0 and 1 respectively for leaf nodes), and C(y) and Q(y) are the benchmark time and quality for tool y. VLM Feedback and Retries: After each tool execution, its output is evaluated by VLM. If the quality score falls below predefined threshold, CoSTA can trigger retry mechanism (e.g., by adjusting the tools hyperparameters) and updates g(x) with any additional costs. If retries fail, the search naturally explores alternative paths with better (x) scores, allowing robust recovery from tool mispredictions or failures. This search process aims to find toolpath that optimally satisfies the users preference for cost versus quality."
        },
        {
            "title": "E Rationale for Using Subtask Chain Generation",
            "content": "The original CoSTA agent [9] utilized an LLM prompt that could generate subtask tree Gss. This tree structure allowed for multiple parallel branches, representing alternative valid execution orders for subtasks that were independent of each other. While flexible, exploring these multiple branches during the subsequent search phase could potentially increase computational cost, especially for tasks with many independent subtasks. We hypothesized that current Large Language Models (LLMs) and Vision-Language Models (VLMs) might possess improved reasoning capabilities to determine single, logical sequence of subtasks 17 based on the user prompt and image context directly. This improved reasoning could potentially determine single, optimal, or at least highly plausible, logical sequence for the required subtasks upfront. Such linear sequence, which we term \"subtask chain,\" would simplify the initial planning structure compared to branching tree. To validate the feasibility of using simpler chain structure without sacrificing outcome quality, we conducted preliminary experiment. We compared the performance of the original CoSTA algorithm using two different prompts: the original prompt generating multi-path subtask trees, and modified prompt designed to generate only single-path subtask chain. We ran both versions on representative subset of 50 diverse tasks from the benchmark dataset [9], covering range of subtask counts and types. The final output quality, assessed using human evaluation metrics defined in [9], was found to be consistently very similar between the two approaches, with scores varying by only 1.08% on average across the tested examples. Given this negligible impact on final quality, we adopted the simpler single-path prompt to generate subtask chain for FaSTA, aiming to potentially reduce planning complexity. The specific prompt used for subtask chain generation in FaSTA can be found in Appendix N. Further analysis focused on quantifying the cost impact of this prompt change within the CoSTA framework itself, independent of FaSTAs subroutine mechanism. We compare the average execution cost of CoSTA using the old (multi-path tree) prompt versus the new (single-path chain) prompt across full dataset. Table 6: Impact of Subtask Tree Prompt on CoSTA. CoSTA Variant CoSTA (Old Prompt - Multi-Path) CoSTA (New Prompt - Single Path) Avg. Cost (s) 58.2 46.8 Table 6 presents these cost results. Using the single-path (chain) prompt yielded noticeable reduction in CoSTAs average execution time (from 58.2 to 46.8 seconds). This confirms that leveraging the LLM to generate more constrained initial plan structure offers some efficiency benefits even for the original search approach. However, it is crucial to contextualize these savings. While the refined prompt contributes to efficiency, the primary driver of the substantial cost reduction observed in the full FaSTA system (29.5s) compared to the baseline CoSTA (even with the new prompt, 46.8s) is the adaptive fast-slow execution strategy (Section 4.3) that effectively utilizes learned subroutines."
        },
        {
            "title": "F Detailed Online Subroutine Induction and Refinement Process",
            "content": "This appendix provides detailed technical breakdown of the online adaptation and refinement loop used by FaSTA to learn and update its Subroutine Rule Table (R), as introduced in Section 4.2. F.1 Step 1: Data Logging For every subtask execution where multiple paths/tools possible, FaSTA logs comprehensive trace τ . Each trace is structured to capture critical information necessary for subsequent learning. The components of trace typically include: Subtask Identification (sj): The specific type of subtask being addressed (e.g., Object Recoloration, Text Removal). Final Executed Tool Path (Pj): The actual sequence of tools (t1, t2, . . . , tk) that was executed by the planner (either pre-existing subroutine or path found via search during \"slow planning\" phase) to complete sj. Context Features: rich set of features describing the state of the image and relevant objects at the time of executing sj. These are gathered from various sources: Outputs from perception tools like YOLO [37] (e.g., object_size and SAM [19] (e.g., mask_properties, color_details). 18 Higher-level semantic features inferred by an LLM query at the start of the task for relevant objects (e.g., background_content_type, overlapping_critical_elements, object_clarity). (Further details on trace composition are in Appendix K). Aggregated Path Cost (Cpath): The total execution cost (e.g., sum of tool runtimes) for the path Pj. Aggregated Path Quality (Qpath): The overall quality of the outcome from Pj, typically an average of VLM scores for the constituent tools. Failure Information: Details of any intermediate tool failures or VLM quality check failures encountered during the execution of Pj, including the specific context features present at the moment of failure. These traces are stored in buffer B. F.2 Step 2: Periodic Refinement Trigger The learning process is not continuous but triggered periodically to manage computational load. After predefined number of task inferences, (e.g., = 20), the system initiates refinement cycle. The most recent traces, Trecent = {τk}t k=tK+1, are retrieved from the buffer to serve as the input for the learning phase. F.3 Step 3: Inductive Reasoning by LLM The core of the learning process involves an LLM (OpenAI o1) performing inductive reasoning. The LLM is prompted with: The set of recent traces, Trecent, which provide examples of successful and failed tool path executions under various contexts. The current Subroutine Rule Table, R. The LLMs task (guided by the prompt in Appendix P) is to analyze these inputs to identify patterns. It looks for correlations between context features, specific tool sequences (potential subroutines), and their observed execution outcomes (cost, quality, success/failure). Based on these identified patterns, the LLM proposes set of potential changes, proposals = {1, 2, . . . }, to the rule set R. Each proposed change can be one of the following: Adding new subroutine Pj along with its inferred activation rule Cj for specific subtask sj. Modifying the tool sequence Pj of an existing subroutine. Modifying the activation conditions Cj of an existing subroutine. F.4 Step 4: Verification and Selection of New/Modified Subroutines Each proposed change proposals undergoes rigorous verification process before being accepted into the active Subroutine Rule Table R. This ensures that only genuinely beneficial and robust rules are adopted. The verification for single proposed change (related to subtask s) proceeds as follows: Subtask-Specific Test Datasets: To evaluate , specialized test dataset Ds is used. This dataset, constructed from the CoSTA benchmark images [9], contains diverse tasks where all constituent subtask instances are exclusively of type s. (Details in Appendix J). Baseline Performance Establishment: baseline performance pair (Cbase, Qbase) is determined by executing baseline system on randomly sampled test set Ds . If this is the first refinement cycle (i.e., = K), the baseline system is CoSTA. For subsequent cycles, the baseline is FaSTA operating with its current, pre-change rule set R. Evaluation of Proposed Change: The proposed change is provisionally applied to the current rule set to create candidate rule set R. FaSTA is then executed with on the same sampled test set to obtain new performance metrics (Cnew, Qnew). Performance Metrics Calculation: The percentage changes in cost and quality are computed: C% = Q% = Cnew Cbase Cbase Qnew Qbase Qbase 100 Acceptance Criterion (Net Benefit): Net Benefit score B() is calculated to quantify the overall impact of the change: B() = C% Q% change is considered beneficial and is provisionally accepted if B() < 0. This criterion prioritizes changes that yield greater percentage improvement in cost than any percentage degradation in quality, or improve quality with no cost increase, etc. Retry Mechanism for Refinement: If the initial proposed change does not meet the Net Benefit criterion (B() 0), it is not immediately discarded. Instead, feedback detailing the failure (e.g., specific test cases where it underperformed, the nature of Cnew vs. Cbase and Qnew vs. Qbase) is provided to the LLM. The LLM is then prompted to refine its initial proposal, yielding modified change . This refinement-evaluation cycle (using new random test sample Ds for re-evaluation) can be repeated up to Nretries times (e.g., Nretries = 2). Final Decision: The proposed change (either the original or refined ) is permanently accepted and integrated into the main Subroutine Rule Table only if it satisfies the B() < 0 criterion within the allowed number of retries. If, after all retries, the criterion is still not met, the proposed change is discarded for this refinement cycle. This comprehensive verification loop ensures that the Subroutine Rule Table evolves with high-quality, empirically validated rules."
        },
        {
            "title": "G Efficacy of Online Subroutine Learning",
            "content": "To demonstrate the progressive effectiveness of our online subroutine induction and refinement process (Section 4.2), we analyzed how the performance of FaSTAs \"Fast Plan\" improved over time. This involved tracking the success rate of the high-level Fast Plan when applied to the main benchmark dataset at various stages of the learning process. The learning itself was driven by execution traces from tasks distinct from this benchmark dataset. Specifically, the data fed to the LLM for inductive reasoning (at = 40, 80, 120, . . . cumulative external task intervals) was generated from continuously expanding set of diverse, newly created prompts or random samples from broader image collections. This separation of \"training/learning\" tasks from the \"monitoring\" benchmark tasks was crucial to ensure that the observed improvements in Fast Plan success were due to genuine generalization of learned subroutines and not overfitting to the benchmark data itself. Figure 1 illustrates this learning efficacy. It shows the percentage of applicable subtasks within the held-out benchmark portion for which FaSTA successfully utilized learned subroutine (i.e., the Fast Plan step was not None and did not require fallback to the Slow Path due to VLM failure). This success rate is plotted against the cumulative number of non-benchmark \"training\" task executions that had been processed to refine the Subroutine Rule Table up to that point. The analysis focuses on subtasks where multiple tool paths are typically possible, making them prime candidates for subroutine mining. The bar chart displays the Fast Plan success rate at discrete intervals (e.g., after the Rule Table was updated based on 40, 80, 120, 160, and 200 external task evaluations), with an overlaid curve highlighting the improvement trend. The final Subroutine Rule Table used for the main benchmark evaluations reported in Section 5 is the one achieved after substantial number of such online learning and refinement iterations. The increasing trend observed in Figure 1 demonstrates that as FaSTA processes more diverse external tasks and iteratively refines its Subroutine Rule Table, its ability to successfully apply efficient, learned \"fast plans\" to unseen benchmark tasks improves. This, in turn, reduces the frequency of needing to resort to the more computationally intensive \"slow path\" search for subtask types amenable to subroutine learning, contributing to the overall efficiency reported in our main results. 20 Table 7: Complete Learned Subroutine Rule Table (R). Contains all mined subroutines, activation rules, and performance metrics used in the Fast Plan generation. Subtask Subroutine Name Activation Rules Avg. Cost (s) Avg. Quality - object_size: Not Too Small - overlapping_critical_elements: None (eg. Some text written on object to be recolored and this text is critical for some future or past subtask) 10.39 0. - color_transition = not extreme luminance change (e.g., not White Black) 12.92 0.95 Object Recoloration Grounding DINO [24] SAM [19] SD Inpaint [32] Object Recoloration SD Search & Recolor [32] Object Recoloration YOLO [37] SAM [19] SD Inpaint [32] Object Replacement Grounding DINO [24] SAM [19] SD Inpaint [32] Object Replacement SD Search&Replace [32] Object Replacement YOLO [37] SAM [19] SD Inpaint [32] Object Removal Grounding DINO [24] SAM [19] SD Erase [32] Object Removal YOLO [37] SAM [19] SD Erase [32] Object Removal Grounding DINO [24] SAM [19] SD Inpaint [32] Object Removal YOLO [37] SAM [19] SD Inpaint [32] SR1 SR2 SR3 SR4 SR SR6 SR7 SR8 - yolo_class_support: Object supported as yolo class - object_size: Not Too Small - overlapping_critical_elements: None (eg. Some text written on object to be recolored and this text is critical for some future or past subtask) - object_size = Not too small - size_difference(original, target objects) = Not too big (eg. hen to car, etc) - shape_difference(original, target objects) = Not too small (i.e., not confusingly similar, eg. bench and chair) - instance_count(object_to_replace) = 1, 2 - object_clarity = High (e.g., common, opaque, substantial, fully visible) - shape_difference(original, Large target) = Not Very - yolo_class_support: Object supported as yolo class - object_size = Not too small - size_difference(original, target objects) = Not too big (eg. hen to car, etc) - shape_difference(original, target objects) = Not too small (i.e., not confusingly similar, eg. bench and chair) - object_size = Not too big - background_content_type = Simple_Texture OR Homogenous_Area OR Repeating_Pattern (e.g., wall, sky, grass, water, simple ground) - ing/Inpainting (vs. Drawing/Semantic_Completion) background_reconstruction_need Fill- = - yolo_class_support: Object supported as yolo class - object_size = Not too big - background_content_type = Simple_Texture OR Homogenous_Area OR Repeating_Pattern (e.g., wall, sky, grass, water, simple ground) - ing/Inpainting (vs. Drawing/Semantic_Completion) background_reconstruction_need Fill- = SR9 SR - object_size = Not small - background_content_type = Complex_Scene OR Occludes_Specific_Objects - background_reconstruction_need = Drawing/Semantic_Completion (vs. Filling/Inpainting) - yolo_class_support: Object supported as yolo class - object_size = Not small - background_content_type = Complex_Scene OR Occludes_Specific_Objects - background_reconstruction_need = Drawing/Semantic_Completion (vs. Filling/Inpainting) Text Removal CRAFT [1] EasyOCR [20]+DeepFont [39] LLM SD Erase [32] SR11 Text Removal CRAFT [1] EasyOCR [20]+DeepFont [39] LLM DALL-E [29] SR12 Text Removal CRAFT [1] EasyOCR [20]+DeepFont [39] LLM Painting SR13 Text Replacement CRAFT [1] EasyOCR [20]+DeepFont [39] LLM SD Erase [32] Text Writing SR14 Text Replacement CRAFT [1] EasyOCR [20]+DeepFont [39] LLM DALL-E [29] Text Writing SR15 Text Replacement CRAFT [1] EasyOCR [20]+ DeepFont [39] LLM Painting Text Writing SR16 - background_content_behind_text = Plain_Color OR Simple_Gradient OR Simple_Texture (Not Complex_Image or Specific_Objects) - ing/Inpainting (vs. Drawing/Semantic_Completion) background_reconstruction_need Fill- = - background_artifact_tolerance = High (e.g., clouds, noisy textures, abstract patterns where minor flaws are acceptable) - surrounding_context_similarity(to_text) = Low (e.g., nearby areas do not contain other text or fine line patterns) background_content_behind_text - = Uniform_Solid_Color (Strictly no texture, gradient, or objects) - background_reconstruction_need = None (Simple solid color fill is sufficient) - background_content_behind_text = Plain_Color OR Simple_Gradient OR Simple_Texture (Not Complex_Image or Specific_Objects) - ing/Inpainting (vs. Drawing/Semantic_Completion) background_reconstruction_need Fill- = - background_artifact_tolerance = High (e.g., clouds, noisy textures, abstract patterns where minor flaws are acceptable) - surrounding_context_similarity(to_text) = Low (e.g., nearby areas do not contain other text or fine line patterns) background_content_behind_text - = Uniform_Solid_Color (Strictly no texture, gradient, or objects) - background_reconstruction_need = None (Simple solid color fill is sufficient)"
        },
        {
            "title": "H Complete Subroutine Rule Table",
            "content": "The complete Subroutine Rule Table (R) used by FaSTA is detailed below. This table stores the learned subroutines (Pj), their symbolic activation rules (Cj) based on context features, the associated subtask (sj), and the empirically measured average execution cost and quality observed during execution of evaluation tasks (Section 4.2). It is important to note that the inductive reasoning process for mining subroutines focuses primarily on subtasks where multiple viable tool sequences or configurations exist (e.g., object replacement, object recoloration, text removal), offering potential for optimization via learned rules. Subtasks typically solved by single, fixed tool path (e.g., depth estimation using MiDaS, basic text detection using CRAFT [1]) are generally not 21 10.36 0.88 10.41 0. 12.12 0.97 10.38 0.91 11.97 0. 11.95 0.98 10.39 0.95 10.37 0. 17.81 0.93 17.95 0.96 6.69 0. 17.85 0.92 18.02 0.94 6.77 0. subjected to this mining process and thus may not appear with complex rules or multiple subroutine options in this table. It should also be noted while most information used in traces used for inductive reasoning is obtained as inputs from outputs of intermediate tools along the path (eg. Object Size from YOLO [37], etc.), some information like the background_content_type, etc. is obtained by prompting the LLM separately on the image. This table is dynamically updated during the online refinement process."
        },
        {
            "title": "I Human Evaluation Methodology for Accuracy",
            "content": "To ensure robust and reliable assessment of model performance, particularly for complex, multi-step, and multimodal editing tasks where automated metrics like CLIP [28] similarity can be insufficient [9], we employ human evaluation to measure accuracy. Automated metrics often fail to capture nuanced errors, semantic inconsistencies, or critical local changes within complex edits [9, Sec 5.2, Appx J]. Our structured human evaluation process provides more accurate measure of task success. I.1 Subtask-Level Accuracy Scoring Human evaluators manually assess the output of each individual subtask si within larger task . Each subtask is assigned correctness score, denoted as A(si), based on the following scale: A(si) = 1, if the subtask is completed fully and correctly. A(si) = 0, if the subtask execution failed entirely or produced an unusable result. A(si) = x, where {0.1, 0.3, 0.5, 0.7, 0.8, 0.9}, if the subtask is partially correct [9, Eq. 4]. The specific score for partial correctness is determined using predefined rules tailored to different types of editing operations, ensuring consistency in evaluation. These rules, adapted from [9], are outlined in Table 8. Table 8: Predefined Rules (adapted from [9, Table 8]) for Assigning Partial Correctness Scores in Human Evaluation. Task Type Evaluation Criteria Assigned Score Image-Only Tasks Text+Image Tasks Minor artifacts, barely noticeable distortions Some visible artifacts, but main content is unaffected Noticeable distortions, but retains basic correctness Significant artifacts or blending issues Major distortions or loss of key content Output is almost unusable, but some attempt is visible Text is correctly placed but slightly misaligned Font or color inconsistencies, but legible Noticeable alignment or formatting issues Some missing or incorrect words but mostly readable Major formatting errors or loss of intended meaning Text placement is incorrect, missing, or unreadable 0.9 0.8 0.7 0.5 0.3 0.1 0.9 0.8 0.7 0.5 0.3 0.1 I.2 Task-Level Accuracy Calculation The accuracy for complete task , denoted as A(T ), is calculated as the arithmetic mean of the correctness scores of all its constituent subtasks ST [9, Eq. 5]: A(T ) = 1 ST (cid:88) siST A(si) This approach ensures that the task-level accuracy reflects the performance across all required steps. I.3 Overall System Accuracy To evaluate the overall performance of the system across the entire benchmark dataset, the overall accuracy Aoverall is computed by averaging the task-level accuracies A(Tj) for all evaluated tasks 22 Tj [9, Eq. 6]: Aoverall = 1 T (cid:88) j=1 A(Tj) where represents the total number of tasks evaluated in the dataset. Subroutine Verification: Datasets and Evaluation Protocol This section provides further details on the datasets and evaluation procedure used for verifying proposed subroutine changes () during the online refinement process described in Section F.4. J.1 Subtask-Specific Test Datasets (Ds ) To rigorously evaluate proposed change (which typically relates to specific subtask type s, e.g., Object Replacement), we created specialized test datasets, one for each subtask type supported by the system (Ds ). Figure 9: Distribution of total manipulations (subtask occurrences) across the specialized test datasets (Ds ) used for subroutine verification. Image Source: These datasets reuse the base images from the original CoSTA benchmark dataset [9]. Image-based subtask datasets (e.g., DObject Replacement, DObject Recoloration) utilize all 121 images. Text-related subtask datasets (e.g., DText Removal) utilize the subset of 40 images from the original benchmark that contain relevant text elements. Prompt Generation: For each base image and each target subtask type s, we generated new prompts focused exclusively on performing operations of that type. For example, using an image containing cat, the prompt for the DObject Replacement dataset might be replace the cat with dog, while the prompt for the same image in the DObject Recoloration dataset could be recolor the cat to pink. This ensures that when testing change related to Object Replacement subroutines, the evaluation focuses solely on the performance of that subtask type. Varying Complexity: Within each subtask-specific dataset Ds , the generated tasks feature varying complexity. For instance, in DObject Removal, some tasks might involve removing only one object, while others might require removing six or seven different objects from the same image. This ensures that subroutines are tested across different levels of difficulty for their specific function. 23 Dataset Statistics: Figure 9 shows the distribution of total manipulations (subtask occurrences) for each subtask-specific dataset. While image-based datasets share the same 121 base images, the total number of manipulations can differ based on the number of relevant objects/regions suitable for that subtask type within each image and the varying complexity levels introduced in the prompts. Figure 10: Illustration of subtask-specific dataset generation. single base image from the CoSTA benchmark is used with different prompts, each targeting distinct subtask type, to create evaluation instances for different datasets (e.g., DObject Replacement, DObject Recoloration). J.2 Evaluation Protocol for Subroutine Changes When evaluating proposed change for subroutine related to subtask s, we follow the procedure outlined in Section F.4: Test Set Sampling (T , ): random subset of tasks is sampled from the corresponding subtaskspecific dataset Ds . We sample 25 tasks for image-based subtasks and 20 tasks for text-based subtasks for each evaluation task (both baseline and candidate evaluation, including retries). Quality Evaluation (Qbase, Qnew): The quality score used for calculating the Net Benefit B() relies on automated VLM checks. For each task in the sampled test set (or ), we execute the respective system (baseline CoSTA/FaSTA or FaSTA with the candidate rule change R). During execution, the VLM quality check is applied after relevant tool steps, using the same methodology as used in CoSTA. The quality score for single task is computed as the average of the VLM quality scores obtained for all its constituent subtasks (which are all of type in these specialized datasets). The final quality metric (Qbase or Qnew) used in the Net Benefit calculation is the average of these task-level quality scores across all tasks sampled in (or ). Cost Evaluation (Cbase, Cnew): The cost metric is the average total execution time (in seconds) across all tasks sampled in the test set (or ). 24 This detailed dataset construction and evaluation protocol allows for focused and rigorous assessment of the impact of proposed subroutine changes on performance for the specific subtask type they target."
        },
        {
            "title": "K Trace Data for Inductive Reasoning",
            "content": "The online subroutine induction and refinement process (Section 4.2) relies on analyzing execution traces to identify patterns and propose subroutine rules. This appendix details the composition of these traces and provides an example. K.1 Trace Composition and Data Gathering For each task processed by FaSTA, detailed trace τ is logged. This trace is crucial for the LLM to perform inductive reasoning during the periodic refinement phase. The key information captured in trace for each subtask within completed task includes: Subtask (sj): The specific subtask being performed (e.g., Object Recoloration, Text Removal). Chosen Tool Path (Pj): The sequence of tools that was actually executed to complete the subtask (this could be selected subroutine or path found via low-level search). Context Features: set of relevant features extracted from the image context and the state of the objects being manipulated. This information can be gathered upfront for the relevant objects in the image or also during the execution of different tools along the path depending on the specific detail. Object-Specific Features (List not exhaustive): object_size: Derived from bounding boxes provided by object detection tools like YOLO [37] (or Grounding DINO [24] if YOLO [37]class is not supported for primary object). mask_properties: Such as mask area or mask ratio, obtained from segmentation tools like SAM [19]. color_details: The color of the original object, also derived from the mask output by SAM. Text-Specific Features: text_box_size: Obtained from text detection tools like CRAFT [1]. Relational/Global Features (Queried from LLM): For each primary object involved in the task, we also query an LLM at the beginning of the task to infer higher-level contextual attributes based on the image and prompt. This is done once for all objects. Examples include: background_content_type: Describes the area surrounding or behind an object (e.g., \"Simple_Texture\", \"Homogenous_Area\", \"Complex_Scene\"). overlapping_critical_elements: boolean indicating if an object overlaps with other elements (text, other objects) that are targets in subsequent subtasks. object_clarity: (e.g., \"High\", \"Medium\", \"Low\" - indicating how clearly visible and unambiguous the object is). Path Cost (Cpath): The aggregated execution cost (e.g., time) for the chosen tool path Pj. Path Quality (Qpath): The aggregated quality score for Pj, an average of VLM scores for the tools in the path. Failures: Information about any tool failures or VLM quality check failures encountered during the execution of Pj, along with the specific context features active at the time of failure. This comprehensive trace data, particularly the context features gathered from tools like YOLO, SAM, CRAFT [1] [1], and initial LLM queries, allows the inductive reasoning LLM (Section 4.2, Step 3) to correlate observed conditions with the success or failure of different tool paths, thereby proposing or refining subroutines and their activation rules. K.2 Example Trace for an Object Recoloration Subtask Consider an input image and prompt as shown in Figure 11. The task is Recolor the cup to blue. The subtask chain might simply be Object Recoloration (ball -> blue ball) with an initial path 25 Figure 11: Visual example for the object recoloration trace detailed in Appendix K.2. Left: Input image. Right: Conceptual representation of the initial failed toolpath trace and the subsequent successful toolpath trace with key context features noted. (Grounding DINO [24] -> SAM [19]-> SD Inpaint [32]) where SD Inpaint failed quality check so path was retraced and final path (SD Search&Recolor [32]) which passed all quality checks. The logged traces for this subtask are shown on right side in Figure 11. All details extracted from various tools used along the path, such as YOLO [37]and SAM, are included along with the status of the path. Some extra details which are not possible to be extracted from these tools are extracted with the help of an LLM which is called at the start of the task for all related objects (in this case only the ball). In case of the paths where tools like the YOLO [37]or SAM [19]are not included like in case of SD Search&Recolor, we use the LLM to extract an approximation of size, color, etc. information as well along with the other extra details for which this LLM was already needed. This structured trace provides rich data for the LLM to analyze during the subroutine induction and refinement phase. Subroutine Reuse Rate Figure 12 illustrates the reuse rate for each of the learned subroutines. The reuse rate is critical metric that quantifies the utility and applicability of each mined subroutine. Specifically, for each subroutine, this rate is calculated as the percentage of applicable subtasks where that particular subroutine was selected for execution and was also executed successfully. This means the calculation considers only those instances where subtask was of type that the subroutine could address. For example, if subroutine is designed for Object Recoloration, its reuse rate is determined by how often it was chosen when an Object Recoloration subtask was encountered, irrespective of the total number of other subtask types (e.g., Object Removal, Text Replacement) processed. higher reuse rate indicates that subroutine is frequently chosen when it is relevant, underscoring its effectiveness and the successful learning of its activation rules. The figure displays these rates for all sixteen subroutines (SR1 through SR16), providing insight into their individual contributions to the agents efficiency. Figure 12: Reuse rate (%) for all learned subroutines. The rate for each subroutine is calculated based on the percentage of applicable subtasks where it was selected for execution."
        },
        {
            "title": "M Algorithms",
            "content": "M.1 FaSTA Online Subroutine Induction Algorithm 1: High-Level Overview of Online Subroutine Learning. See detailed algorithm in Appendix M.1. 1 Log execution traces (data, paths, outcomes, context) continuously 2 After = 20 task executions, using recent traces: 3 LLM analyzes traces to propose/refine subroutines & activation rules Validate proposals on test data for net cost-quality benefit if validation fails then LLM refines proposal with feedback (max Nretries = 2 attempts) else if validation succeeds then Update Subroutine Rule Table with beneficial change 4 5 6 7 Repeat indefinitely to adapt and improve 27 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 Algorithm 2: FaSTA Online Subroutine Induction (Modified ICRL). Teal steps are specific additions/modifications for FaSTA. Red italic steps indicate standard ICRL practices replaced by FaSTAs approach. Input: LLM πLLM , Planner FaSTA, Initial Rule Set R0, Trace Buffer B, Update Freq. K, Max Retries Nretries, Subtask Test Datasets {Ds}sS Output: Continuously Updated Rule Set 1 R0; 2 ; 3 0; 4 while True do 5 Receive new task input (xt, ut); Execute FaSTA with current rules to get trace τt = (subtask, Pfinal, ContextFeatures, Cpath, Qpath, Failures); Add τt to Buffer {τt}; + 1; if > 0 and (mod K) == 0 then Trecent Get last traces from B; proposals πLLM (\"Propose changes to rules based on recent traces Trecent\"); foreach proposed_change in proposals do GetSubtaskType(); accepted False; current_delta ; for retry 0 to Nretries do Ttest SampleRandomSubset(Ds ); if == then (Cbase, Qbase) Evaluate(CoSTA, Ttest); else (Cbase, Qbase) Evaluate(FaSTA (R), Ttest); Rcandidate ApplyChange(R, current_delta); (Cnew, Qnew) Evaluate(FaSTA (Rcandidate), Ttest); C% (Cnew Cbase)/Cbase 100; Q% (Qnew Qbase)/Qbase 100; B(eval) C% Q%; if B(eval) < 0 then Rcandidate; accepted True; Break; else if retry < Nretries then feedback AnalyzeFailure(current_delta, Cnew, Qnew, Cbase, Qbase, Ttest); current_delta πLLM (\"Refine change based on feedback: feedback\"); // Standard ICRL might sample past episodes here to build context for next action (e.g., Algo 1/2 in [25]). FaSTA replaces this with explicit rule update.; 28 M.2 FaSTA Adaptive Fast-Slow Execution Algorithm 3: High-Level Overview of Adaptive Fast-Slow Planning. Detailed Algorithm can be found in Appendix M.2. // Input: Subtask chain, Subroutine Rule Table 1 LLM generates \"Fast Plan\" (sequence of subroutines or None) for the subtask chain 2 foreach step in the Fast Plan do 3 if Fast Plan step is valid subroutine then Attempt to execute the subroutine if subroutine fails VLM quality check then trigger Slow Path for this subtask else"
        },
        {
            "title": "Trigger Slow Path for this subtask",
            "content": "if Slow Path was triggered then Perform localized search on the subtasks low-level subgraph"
        },
        {
            "title": "Proceed to next step upon successful completion of current subtask",
            "content": "Algorithm 4: FaSTA Adaptive Fast-Slow Execution Input: Input image x0, Prompt u, Subtask Chain Gsc, Subroutine Rule Table R, LLM πLLM , VLM, Quality Threshold Qthresh, Cost params α, BT, MDT, TDG Output: Final Edited Image xf inal 1 Generate Fast Plan Msubseq πLLM (x0, u, Gsc, R); 2 Current Image xcurr x0; 3 Path Trace τpath []; 4 foreach subtask si in sequence from Gsc do 5 plan_step Msubseq(si); subtask_success False; if plan_step is Subroutine /* Attempt Fast Plan temp_image xcurr; fast_path_ok True; subroutine_trace []; foreach tool tk in si then si do Execute tk on temp_image to get xint, cost ck; Append (tk, ck) to subroutine_trace; qk VLMQualityCheck(xint, si, tk) ; if qk < Qthresh then fast_path_ok False; Break; temp_image xint; if fast_path_ok then xcurr temp_image; Append subroutine_trace to τpath; subtask_success True; else /* plan_step is None if not subtask_success then /* Execute Slow Path (Local A) Glow(si) ConstructSubgraph(si, MDT, TDG) ; slow_trace LocalAStarSearch(Glow(si), xcurr, si, α, BT, VLM, Qthresh); xcurr xout; Append slow_trace to τpath; subtask_success True; */ // Per [9] */ */ // Per [9] 4 5 7 8 9 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 Return xcurr, τpath; 29 Figure 13: Qualitative comparison of FaSTA against CoSTA [9] and Gemini 2.0 Flash Preview for complex multi-turn editing tasks (inputs on top). FaSTA produces high-quality outputs visually identical to CoSTA and superior to Gemini. Notably, FaSTA achieves this CoSTA-level quality at significantly reduced execution cost, demonstrating its enhanced efficiency."
        },
        {
            "title": "N LLM Prompt for Generating Subtask Chain",
            "content": "You are an advanced reasoning model responsible for decomposing given image editing task into structured subtask chain. Your task is to generate well-formed subtask chain that logically organizes all necessary steps to fulfill the given user prompt. Below are key guidelines and expectations: N.1 Understanding the Subtask chain subtask chain is structured representation of how the given image editing task should be broken down into smaller, logically ordered subtasks. Each node in the chain represents subtask which is involved in the prompt, and edges represent the ordering like which subtask needs to be completed before or after which. Each node of the chain represents the subtasks required to complete the task. The chain ensures that all necessary operations are logically ordered, meaning subtask that depends on another must appear after its dependency. N.2 Steps to Generate the Subtask chain Step 1: Identify all relevant subtasks needed to fulfill the given prompt. Step 2: Ensure that each subtask is logically ordered, meaning operations dependent on another should be placed later in the path. Step 3: Each subtask should be uniquely labeled based on the object it applies to and of the format (Obj1 Obj2) where obj1 is to be replaced with obj2 and in case of recoloring (obj new color) while with removal just include (obj) which is to be removed. Example: If two objects require replacement, the subtasks should be labeled distinctly, such as Object Replacement (Obj1 -> Obj2). Step 4: There also might be multiple possible subtasks for particular requirement like if part of task is to replace the cat with pink dog then the two possible ways are Object Replacement (cat-> pink dog) and another is Object Replacement (cat->dog) -> Object Recoloration (dog->pink) N.3 Logical Constraints & Dependencies When constructing the chain, keep in mind that you take care of the order as well like if task involves replacing an object with something and then doing some operation on the new object then this operation should always be after the object replacement for this object since we cannot do the operation on the new object till it is actually created and in the image. Input Format N.4 The LLM will receive: 1. An image. 2. text prompt describing the editing task. 3. predefined list of subtasks the model supports (provided below). N.5 Supported Subtasks Here is the complete list of subtasks available for constructing the subtask chain: Object Detection, Object Segmentation, Object Addition, Object Removal, Background Removal, Landmark Detection, Object Replacement, Image Upscaling, Image Captioning, Changing Scenery, Object Recoloration, Outpainting, Depth Estimation, Image Deblurring, Text Extraction, Text Replacement, Text Removal, Text Addition, Text Redaction, Question Answering based on text, Keyword Highlighting, Sentiment Analysis, Caption Consistency Check, Text Detection You must strictly use only these subtasks when constructing the chain. N.6 Expected Output Format The model should output the subtask chain in structured JSON format, where each node contains: 31 Subtask Name (with object label if applicable) Parent Node (Parent node of that subtask) Execution Order (logical flow of tasks) N.7 Example Inputs & Expected Outputs N.7.1 Example 1 Input Prompt: Detect the pedestrians, remove the car and replacement the cat with rabbit and recolor the dog to pink. Expected Subtask chain: { \"task\": \"Detect the pedestrians, remove the car and replacement the cat with rabbit and recolor the dog to pink\", \"subtask_chain\": [ \"subtask\": \"Object Detection (Pedestrian)(1)\", \"parent\": [] \"subtask\": \"Object Removal (Car)(2)\", \"parent\": [\"Object Detection (Pedestrian)(1)\"] \"subtask\": \"Object Replacement (Cat -> Rabbit)(3)\", \"parent\": [\"Object Removal (Car)(2)\"] \"subtask\": \"Object Recoloration (Dog -> Pink Dog)(4)\", \"parent\": [\"Object Replacement (Cat -> Rabbit)(3)\"] { }, { }, { }, { } ] } N.7.2 Example 2 Input Prompt: Detect the text in the image. Update the closed signage to open while detecting the trash can and pedestrian crossing for better scene understanding. Also, remove the people for clarity. Expected Subtask chain: { \"task\": \"Detect the text in the image. Update the closed signage to open while detecting the trash can and pedestrian crossing for better scene understanding. Also, remove the people for clarity.\", \"subtask_chain\": [ { }, { }, \"subtask\": \"Text Replacement (CLOSED -> OPEN)(1)\", \"parent\": [] \"subtask\": \"Object Detection (Pedestrian Crossing)(2)\", \"parent\": [\"Text Replacement (CLOSED -> OPEN)(1)\"] \"subtask\": \"Object Detection (Trash Can)(3)\", \"parent\": [\"Object Detection (Pedestrian Crossing)(2)\"] \"subtask\": \"Object Removal (People)(4)\", \"parent\": [\"Object Detection (Trash Can)(3)\"] \"subtask\": \"Text Detection ()(5)\", \"parent\": [\"Object Removal (People)(4)\"] { }, { }, { } ] } You can observe in the second example since there was subtask related to text replacement, it made sense to detec the text at last after all changes to text had been made. You should always be mindful that ordering is logical and if there is subtask whose output or input might change based on some other subtasks operation then it is always after this subtask on whose operation it depends. egrecolor the car to pink and replace the truck with car so in this one the recoloration of car always depends on the replecement of truck with car so the recoloration should always be done after replacement so you should think logically and it is not necessary that the sequence of subtasks in subtask chain is same as they are mentioned in the input prompt as was in this case the recoloration was mentioned before replacement in input prompt but logically replacement will come first. N.8 Your Task Now, using the given input image and prompt, generate well-structured subtask chain that adheres to the principles outlined above. Ensure logical ordering and clear dependencies. Label subtasks by object name where needed. Structure the output as JSON-formatted subtask chain. Input Details Image: input image Prompt: [prompt] Supported Subtasks: (See the list above) Now, generate the correct subtask chain. Before you generate the chain you need to make sure that for every path possible in the subtask chain all the subtasks in that chain are covered and none are skipped. Also if prompt involves something related to object replacement then just have that you dont need to think about its prerequisites like detecting it or anything bcz it already covers it."
        },
        {
            "title": "O LLM Prompt for Subroutine Selection",
            "content": "So we have this image: <image> and also have the following input prompt: <prompt> So we got the following subtask chain: <subtask chain> Note that in the subtask chain within particular node there is bracket which tell us about the object from and target and if there is only from then target is not mentioned like in removal only from object is needed no target is required while for replacement/recoloration target is also required. Now we have the following subroutines list for each subtask and each of the subroutines have some observations related to them which specify under which conditions they are to be used or not. So you need to read those subroutines and their observations then check the corresponding object for that subtask within the image like if its Object Removal (Cat) then check the cat in image and then from the subroutines list check that if for that particular subtask there is any subroutine in which the observation conditions are satisfied and if so give the list of those subroutines for that subtask and you need to do this for all subtasks in the subtask chain. Subroutine list and the details: {Subroutine Rule Table} Example: Suppose we have an image which has lots of objects along with very large car which has background with lots of objects and also brown wooden board with some text written on it. Now we have prompt that remove the car and recolor the wooden board to pink and detect the text and get the following subtask chain: Object Removal (Car) -> Object Recoloration (Wooden Board -> Pink Wooden Board) -> Text Detection () Now we see the subroutine list and find that for removal since the object is too big sub7 and sub8 are not possible. Now in sub9 and sub10 we see that the car class is supported by yolo so eventually we choose sub10 for this subtask. For recoloration we see that it has object (text) which is imp and is involved in subsequent subtask so sub1 and sub3 arent possible and we see that the color of board is light brown so light brown and pink dont have too much difference so we choose sub2. For text detection there is not subroutine available so we leave it like that. So output will be: Object Removal (Car) : [sub10] Object Recoloration (Wooden Board -> Pink Wooden Board) : [sub2] Text Detection () : [None] Now lets say the wooden board was black in color and had to be recolored to white. In this case the sub1 and sub3 are not possible because of the text as before but now sub2 is also not possible because the color difference is too much. So we do not choose any subroutine for this subtask and output is as follows: Object Removal (Car) : [sub10] Object Recoloration (Wooden Board -> Pink Wooden Board) : [None] Text Detection () : [None] Now lets change the details further. Lets say that the wooden board does not have any text written on it and has to be recolored from pink to yellow and the text detection subtask wasnt present so in this case for recoloration all subroutines are possible except sub3 bcz wooden board isnt class supported by yolo. New output: Object Removal (Car) : [sub10] Object Recoloration (Wooden Board -> Pink Wooden Board) : [sub1, sub2] Now lets change it bit assume that all conditions are as original but the car is small and behind the car only some walls, grass, etc are present some basic stuff and not lot of objects like occluded people, cats, etc so in this case we will choose sub8 and sub10 for it as it is not too plain that sub10 cannot be used and it is not way too complex that sub8 cannot be used. New output: Object Removal (Car) : [sub8, sub10] Object Recoloration (Wooden Board -> Pink Wooden Board) : [sub2] Text Detection () : [None] Now you need to do the same things for the current case where the input prompt is : <prompt> Subtask chain: <subtask chain> Also multiple options are possible if they satisfy all the conditions it is not necessary that only one is chosen and it is also possible that no subroutine fulfills all conditions so in that case choose None so that we can do search and find the correct output. Also keep in mind that only look at the details relevant say you need to check subroutine for some object which is to be removed and for some activation condition you need to see if the background is busy or plain, etc so you only see the background relevant like near that object and not for the entire image. So you need to extract all relevant details related to all relevant objects from the image given to you then check the subroutine list if anyone matches and give the output."
        },
        {
            "title": "P LLM Prompt for Inductive Reasoning on Subroutines",
            "content": "Goal: Analyze the provided experimental run data for specific task (e.g., Object Recoloration) to infer initial, potentially qualitative, activation rules (preference conditions) for each distinct execution path employed. <All Logged Data (The Traces)> So we run different models and tools for different or same image editing tasks and store the observations including what path was finally used and what were the conditions of objects, etc. and this data is provided to you. Now we wish to infer some subroutines or commonly used paths and their activation rules under which they are commonly activated. Can you find some commonly used subroutines or paths and infer some rules for these paths using the status of these cases and other factors and give the rules for both paths and they need not be too specific but bit vague is fine like if you observe that some particular path always fails in case object size is less then you can give the rule that this path should be used when object is not too small and not give any specific values so activation rule will include like object_size = not too small, etc like this based on all factors like object size, color transitions, etc and also it is possible that for some path it failed bcz of some specific condition like its not necessary all conditions led to failure so you need to check which is the condition which always leads to failures or which always leads to success and that will constitute rule if some condition leads to both failures and success with same value then it means that this is not the contributing factorand theres something else thats causing the failure or success and keep in mind that output rules should be of activation format like in what cases this should be used and not negative ones so if there is some path which always fails when object size is big then your activation rule will have object_size = small and not some deactivation rules which has object_size = big. You should also include some explanatory examples in the rule which can help some person or LLM understand them better when referring to these rules. eg. if there is rule where you want to say that this path will only succeed when the difference between size of objects is not too big then you can have rule like : size_difference(original, target objects) = Not too big (eg. hen to car, etc) where you include some example. You should focus on activation rules which are like in what case this particular path will always succeed and some activation rules should also include kind of deactivation rule with not like in case you observe that some path always fails when there is some condition where can be like object is too small or color difference is huge then you should infer an activation rule that is negate of this like the rule can be object is not too small or color difference is not huge so that these activation rules can act as kind of deactivation rules as well and prevent the path from getting activated in cases where we know for sure itll fail. An Example: Experimental Data: Subtask: s1, Object Size: 0.7px, Original Object Color: Yellow, Target Object Color: Black Path used: P1 Status: Fail Subtask: s1, Object Size: 0.2px, Original Object Color: Yellow, Target Object Color: Green Path used: P1 Status: Success Subtask: s1, Object Size: 5px, Original Object Color: White, Target Object Color: Black Path used: P1 Status: Fail 36 Subtask: s1, Object Size: 5px, Original Object Color: White, Target Object Color: Yellow Path used: P1 Status: Success Subtask: s1, Object Size: 0.7px, Original Object Color: Black, Target Object Color: White Path used: P1 Status: Fail Subtask: s1, Object Size: 3px, Original Object Color: Black, Target Object Color: Yellow Path used: P2 Status: Success Subtask: s1, Object Size: 0.9px, Original Object Color: Black, Target Object Color: Blue Path used: P2 Status: Fail Subtask: s1, Object Size: 0.2px, Original Object Color: White, Target Object Color: Yellow Path used: P2 Status: Fail Subtask: s1, Object Size: 0.6px, Original Object Color: White, Target Object Color: Blue Path used: P3 Status: Success So we see that paths P1 and P2 are very commonly used so these will be our subroutines or commonly used paths and now our goal is to infer some rules under which these subroutines or paths are commonly activated. So by observing the data you see that in P2 it always fails when object size is small while the color transitions doesnt matter so for P2 you can infer an activation rule which is object_size: not too small and while for P1 you observe that the object size doesnt really matter bcz it is able to succeed in both small and big object sizes and also fail in both cases but you observe that when the color transition is huge like white to black or black to white it always fails while when color transition is not extreme like white to yellow or yellow to green it is able to succeed even under same size conditions so you can infer rule that it depends on color transition and give rule with example for better understanding like: color_transition: not too extreme (eg. not white <-> black, etc.) The real experimental data will include much more info and it is your job to infer what data is useful and find patterns in it and give corresponding rules. Also you should not mix the observations from different paths or subtasks and treat all paths and subtasks independently so while infering rules for some path P1 for some subtask s1 then only look at the experimental data of that path P1 and subtask s1 and infer rules and patterns from that bcz observations of P2 doesnt affect P1 and neither do observations for P1 but related to s2 affect P1 for s1. So you should know that same path can be used for different subtasks and can have different activation rules for different subtasks so while inferring these rules you should see that you compare the object conditions for the same subtask and same path and then reach final conclusion like example you have some path p1 which is used in subtasks s1 and s2 37 and based on observations there are multiple failure cases for p1 where object size is small and subtask is s1 while there are some success cases for same p1 where object size is big and subtask is s2 so if you combine them you wont be infer any rule bcz nothings epcific but you need to treat both the p1s independently one is p1 with s1 and another is p1 with s2 and so for p1 with s1 you can infer rule that it oonly works when object size is not small. The output format for each path for which you can infer some rule/s will be following: Path: {path1} Subtask: {subtask} Activation Rules: * Rule a1 with some explanatory example if needed * Rule a2 with some explanatory example if needed . * Rule aN with some explanatory example if needed Path: {path2} Subtask: {subtask} Activation Rules: * Rule b1 with some explanatory example if needed * Rule b2 with some explanatory example if needed . * Rule bN with some explanatory example if needed"
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}