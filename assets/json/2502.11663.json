{
    "paper_title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
    "authors": [
        "Jingcheng Ni",
        "Yuxin Guo",
        "Yichen Liu",
        "Rui Chen",
        "Lewei Lu",
        "Zehuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 3 6 6 1 1 . 2 0 5 2 : r MaskGWM: Generalizable Driving World Model with Video Mask Reconstruction Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu SenseTime Research Project page: https://github.com/SenseTime-FVG/OpenDWM"
        },
        {
            "title": "Abstract",
            "content": "World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and genera- (3) we extend mask construction tive diffusion process. task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model. 1. Introduction As an pivotal application of artificial intelligence, autonomous driving technologies, which require comprehending the surrounding environment and executing correct actions, have achieved significant advancements following the emergence of various learning models [20, 24] with scalability. However, the challenge of limited generalization to complex and varied scenarios remains unresolved for state-of-the-art methods [25]. For example, perception may encounter performance drops [42] in cases like weather changes, scene variations and motion blur. promising solution for this problem is the use of world models, which directly predict environment changes under different actions. These models facilitate unraveling the complexities of data distributions and craft intricate regular patterns like human perception system [23]. Recently, advanced methods [12, 19, 3941, 44, 47] developed world models through diffusion-based generation task, capitalizing on the rapid development of advanced image generation systems [1, 7, 33]. Despite generating highfidelity results, these approaches still struggle with longhorizon prediction and zero-shot generalization. To address this, GenAD [44] attempts to conduct training on largescale OpenDV-2K [44] dataset with carefully-designed temporal modules, while VISTA [12] further introduces explicit re-weighted generation loss on structural and moving areas. However, two problems still exist in building generalizable world model for autonomous driving. First, the combination of large-scale training dataset with more scalable transformer architectures is still under exploration. Second, one fundamental question remains unanswered: Is diffusion-based generation sufficient to build generalizable world model? Since diffusion loss targets at iterative de-noising, the learning of visual semantics may not be straightforward. For example, MaskDiT [48] has shown diffusion models are complementary to well-known self-supervised methods [14], benefiting both convergence speed and generation quality. Based on these insights into the diffusion pipeline and data for our driving world model, we design new model, dubbed as MaskGWM, aiming at improving the fidelity, generalizability and long-time series prediction of the existing methods. Additionally, our model can also generate multi-view cases, by incorporating multi-view module. We adopt Diffusion Transformer (DiT) as our back1 Model Setup Framework Multi-view Method Data scale Drive-WM [40] DiVE [21] GenAD [44] Vista [12] MaskGWM(Ours) 5h 5h 1740h 1740h 1740h Unet DiT Unet Unet DiT Traget Diff Diff Diff Diff Diff+MR (a) Real-world multi-view driving world models. (b) Different context for mask reconstruction. Figure 1. (a). MaskGWM improve fidelity and generalization from web-scale dataset, scalable DiT architecture and Mask Reconstruction (MR) target. (b) proposed MR apply two branch structure for spatial context (scene objects) and temporal context (object motions) Figure 2. Our model facilitates zero-shot generation, consistent long-horizon prediction and multi-view video generation. bone, which is more scalable and could take the information from variety of datasets. Moreover, we introduce the mask reconstruction as complementary task for generation. Several impactful works [14, 43] have demonstrated that masked autoencoder is powerful self-supervised method for representation learning from large-scale data and it is also extended to some diffusion methods [11, 48, 50] as an additional supervision to improve the models performance. Additionally, the features obtained by self-supervised learning is more contextually meaningful [5], which can be used as an auxiliary supervision to further improve the generation quality [19] However, integrating existing mask reconstruction for image generation into driving world models is not straightforward. There are still two questions to answer: (1) How can we enhance the synergy between diffusion model and mask reconstruction? Though the Mask reconstruction improves the contextual reasoning ability, this task contradicts with diffusion steps which include high noise ratio obscuring the feature details. (2) What kind of mask strategy should we use for video data? Different from image generation, the prediction of driving future requires an understanding of not only the objects within the scene but also their dynamic movements. dress the aforementioned issues: (1) We make use of the mask tokens to improve the synergy between mask reconstruction and diffusion models. Specifically, we propose diffusion-related mask tokens (Sec.3.2) to initialized the invisible patches after DiT encoder. This special mask token can balance the learning of global and local features. (2) We design novel two-branch mask reconstruction strategy. For spatial modeling, we use mask shared across all frames and reconstruct invisible tokens via spatial transformer, this mask strategy is similar to some video mask modeling methods [36, 38]. For temporal modeling, we introduce frame-specific mask and recover masked tokens via temporal transformer. Unlike the spatial branch, we directly link the unaligned tokens on temporal dimension after masking, which can be seen as shift augmentation restricted by new-proposed row-wise policy. We find this temporal branch achieves both masked patches prediction in the temporal context and reduction in training costs. In summary, our main contributions are: We propose MaskGWM, generalizable DiT-based driving world model capable of forecasting long-term futures across web-scale scenes. We introduce mask reconstruction as complementary Therefore, we develop several special designs to adtask for diffusion-based world model. 2 Comprehensive experiments on nuScene, OpenDV-2K and Waymo datasets demonstrate the superior video quality and robust generalization capabilities across extended time spans and different viewpoints. 2. Related Works 2.1. World Models World models aim to infer the dynamic environment and ego state from past observations for accurate future predictions and planning. Most studies achieve world understanding by enabling the model to generate realistic videos that align with physical principles. In autonomous driving area, world models primarily focus on generating controllable real-world driving scenarios. GAIA-1 [19] employs an autoregressive transformer to predict tokens based on past state and then leverages diffusion decoder to generate high-quality videos. In contrast, DriveDreamer [39] directly use diffusion model to represent complex environments and generate driving videos from multi-modal inputs. Drive-WM [40] extends to consistent and controllable multi-view video generation, exploring its application in end-to-end planning. GenAD [44] enhances generalization capability of world models by training on large-scale datasets and Vista [12] achieves further improvements by introducing attention on structure and dynamic area. 2.2. Diffusion Models with Self-supervised Learning Recently, diffusion-based methods [13, 15, 18, 32, 46] have become the mainstream of image and video generation. One important advancement in this field is Diffusion Transformer(DiT) [30]. Due to its better scalability and lower computational cost, DiT has been successfully applied in various diffusion models, achieving state-of-the-art results [7, 8, 21, 49]. On the other hand, masking strategies from selfsupervised learning have been effectively applied to enhance generative models. With the development of DiT, research has focused on migrating this self-supervised approach to diffusion-based models. Initial works like MDT [11] modify DiT blocks to an asymmetric masking diffusion transformer architecture, where the encoder handles unmasked tokens only and side-interpolater is introduced to recover the latnet to the original shape. Following MDT [11], MaskDiT [48] simply utilizes learnable token to fill in the masked places. SD-DiT [50], noticing the training-inference discrepancy and fuzzy relations between mask strategy and diffusion process, introduce novel masking DiT with self-supervised discrimination. Despite their succuss, none of them applied the masking diffusion to the video generation models. Our work make this attempt on driving world model by applying different design of spatial context and temporal context, which focus on scene objects and nuance motion separately. 3. Method Fig. 3 illustrates an overview of the proposed pipeline. MaskGWM builds upon Stable Diffusion 3 (SD3) [7], which is well-studied DiT-based Text-to-Image (T2I) generation model, and introduce additional spatial and temporal blocks to extract the cross-view and cross-frame information. Moreover, we deploy mask reconstruction module during training to improve the performance of our model. In this section, we first briefly review our DiT-based driving world model in Sec. 3.1. Then, Sec. 3.2 details the pipeline for mask reconstruction and introduces novel diffusion-related mask tokens designed to enhance the synergy between diffusion generation and mask reconstruction. Following this, Sec. 3.3 describes the extension of mask reconstruction to the temporal dimension. Finally, in Sec. 3.4, we describe the details of our cross-view module. 3.1. Preliminaries Diffusion Models. mutli-view video sampled from video dataset pdata can be represented as x0 pdata, where x0 RT KCHW is sequence of frames with view K, height and width . We first transform x0 into video tokens z0 = P(Θ(x0)) RT KC ˆH ˆW via latent encoder Θ and patch encoding P. MaskGWM applies Rectified Flow [7, 26, 27] to model the generation process. Specifically, given standard normal distribution ϵ (0, I), Rectified Flow defines the intermediate noisy state as zτ = (1 τ )z0 + τ ϵ, where τ [0, 1] is the diffusion timestep. The training target of Rectified Flow adopts the v-prediction, defined as: = Ez0,ϵN (0,I),τ (cid:2)Gθ(zτ , τ, c, M) (z0 ϵ) 2 (cid:3) , (1) where is the condition, is binary mask for mask reconstruction and Gθ is the DiT model. Temporal Modeling. To facilitate temporal context learning, we attach temporal transformer block after each 2D spatial transformer block, following common practice in video generation models [2, 29]. During the forward process, to standardize the inputs to the different self-attention layers in the transformer blocks, we reshape the video latent representation to (TK)( ˆH ˆW)C for spatial self-attention and to (K ˆH ˆW)TC for temporal self-attention. Additionally, we introduce reference frames according to video DiT models [9, 29]. During diffusion process, diffusion timestep τ of the reference frames is always set as 0, while the following frames to be predicted are embedded with regular timestep. Unified Action Conditioning. Following VISTA, we provide nuanced control over low-level actions including angle, speed, trajectory and goal point, combining with high-level 3 Figure 3. Overview of the MaskGWM. We propose mask reconstruction containing token mask and token reconstruction as complementary task for training dring world model. Token Mask: we randomly sample tokens by temporal-shared Mspatial and temporal-unshared Mtime, specialized for spatial and temporal modeling. Token Reconstruction: we fill invisible tokens by diffusion-related mask tokens (Sec.3.2) and recover features by two-branch transformer. Moreover, we introduce row-wise mask strategy (Sec.3.3) for temporal branch. ρ = 1 is used for simplicity in encoder. command capabilities. We construct action embedding by concatenating the Fourier embeddings [35] of all actions. Subsequently, these action embeddings are projected and added to the key and value features of the cross-attention layers in temporal transformer blocks. 3.2. Diffusion-related Mask Reconstruction Motivated by previous works [48, 50], Masked Image Modeling (MIM) [14, 43] with mask reconstruction object has been adopted to diffusion-based generation model and achieve improvement on training efficiency and local contextual perception. However, these methods fail to consider the influence of diffusion process, which incorporate noise schedule and complex training target. Therefore, we propose this novel pipeline integrated with mask reconstruction and then introduce our diffusion-related mask tokens for compatibility with diffusion process, which can reduce the effect of noise during diffusion process. Mask Reconstruction. During the training phase, the DiT backbone is asymmetrically divided into an encoder and decoder for mask reconstruction and includes two additional processing steps. In the encoding stage, MaskGWM produces token mask by random sampling 4 binary mask RT K1HW , given the video latent zτ at timestep τ . Similar to [11, 50], is only use to partition zτ into visible patch tokens zv τ = zτ and invisible patch tokens ziv τ = zτ (1 M). In the decoding stage, an extra token reconstruction module is introduced to handle dropped invisible patches. Mask tokens mτ , representing invisible patches, are infilled at the positions of the dropped tokens. Then, transformer block is utilized to provide contextual awareness from visible patches. In details, Gθ(zτ , M) = D(F (E(zv τ ) + mτ (1 M))), (2) τ + ziv τ )). In practice, invisible patches ziv where τ, in Eq.(1) are ignored for simplicity. Note that mask reconstruction is skipped for inference in which all tokens are visible, equivalent to Gθ(zτ ) = D(E(zτ )) = D(E(zv τ are directly dropped during the encoding of training to enable memory and speed benefits. Diffusion-related Mask Tokens. SD-DiT [50] describes fuzzy relationship between generation process and mask reconstruction. Concretely, mask reconstruction focuses on context reasoning while generation diffusion process aims to model the translations between real and fake distributions. From the viewpoint of diffusion model, the mask Figure 4. The comparison of different mask types and attention operations for temporal transformer block with mask reconstruction task. Attention mask is only applied when = Mtime reconstruction for represent learning can be regard as z0prediction task, whereas rectified flow employs v-prediction (pred z0 ϵ). Therefore, simply combining these two distinct objectives may not lead to good performance of the diffusion model, as also demonstrated by MaskDiT [48]. Existing works either instantiate mask tokens as learnable parameters [11, 14] or directly take noisy tokens zτ as input [50]. As discussed above, these mask tokens cannot balance these two targets due to the absence of explicit information for acquiring ϵ. We bridge this gap by introducing fm(ϵ) into the mask token, where fm is small network for encoding noise ϵ. Given that ϵ is explicitly provided, it is easier to recover the original mask reconstruction target for representation learning within diffusion pipeline. Other than explicitly alignment for two prediction tasks, we further take τ into consideration. Overall, we define our mask token with learnable parameters as: mτ = (1 τ )fm(ϵ) + τ p. (3) Based on our experiments in Section 4.4, we designed this mask token accordingly. When τ is large, the generation is performed at high-noise level, and fine-grained image details are unknown. We hypothesize that the learnable parameters can estimate an average distribution under specific conditions (e.g., text) and assist in guiding the prediction direction when appearance details are lacking [45]. Conversely, on low-noise level, mask reconstruction encourages the model to be attentive to local details of visible patches. Therefore, the model can leverage the visible information to recover the patches filled with noise (by fm(ϵ)). 3.3. Mask Reconstruction Strategy Temporal and Spatial Mask. Previous methods [36, 38] extend MIM to video domain by sharing the random mask across time, where the mask = Mspatial = [M1, M2..., MT ] satisfy Mi = Mj when = j. Despite the effectiveness on understanding tasks in the aforementioned methods, this masking strategy may not be suitable for temporal modeling on driving video prediction. To incorporate temporal learning, we introduce temporal unshared mask Mtime satisfy Mi = Mj when = j. Then, we specialize Mspatial for spatial modeling and consider MIM with Mtime for temporal modeling. Then, we make tasks synergy by devising two-branch transformer block: = (cid:40) Fs Ft if = Mspatial if = Mtime , (4) where {Mspatial, Mtime}, Fs is spatial transformer block and Ft is temporal transformer block. Row-wise Approximation. The most straightforward strategy for masking video data is to directly apply random mask Mtime on each frame like [36, 38], as shown in the middle column of Fig. 4. However, this design cause tokens on masked positions should be masked rather than directly dropped, since temporal self-attention require all entries have the same sequence length and apply an attention mask to control sparsity. As result, this masked temporal selfattention requires 3D attention mask to skip masked tokens, which leads to additional computational cost and precludes the direct application of optimization operators like FlashAttention [6]. To address the aforementioned issues, we employ shifted temporal self-attention for Mtime. As illustrated in the right column of Fig. 4, we randomly mask the same number of tokens for each row. Similar to the spatial branch, all invisible tokens are dropped, and the visible tokens are directly connected. Consequently, this operation can be regarded as shift in temporal self-attention, which adheres to the core idea of Masked Reconstruction (MR): masked tokens are invisible and are predicted from context during the decoding stage. Additionally, rearranging the visible tokens row by row ensures the relevance of information in the temporal attention block during the encoding 5 stage. In experiments, we find this design improves not only the training speed but also the generation metrics, especially on larger mask ratio r. Since this row-wise shifting allows nearby tokens to fill in the blanks of masked tokens, We analysis this phenomenon by token filling makes all tokens on temporal axis are retained and minor shift can facilitate the temporal block in context reasoning. To formulate this row-wise temporal mask, we define the mask ratio as r. To generate the mask for ˆH ˆW image latent at frame t, we randomly generate ˆH one-dimensional mask, each having ˆW zero values, and concatenate them to formulate the final mask for this image, denoted by (cid:99)Mtime. We use (cid:99)Mtime to replace Mtime in our training. 3.4. Mask Reconstruction for Cross-View Our method can be extended to generate multi-view driving videos by introducing view transformer blocks. We propose cross-view row-wise self-attention mechanism, which concatenates video features horizontally and computes attention across multi-view features on the same row. Specifically, given feature tensor of shape ˆH ˆW , we apply masking, resulting in tensor of shape KC ˆH[(1 r) ˆW ]. We then reshape it into size (T ˆH)[K(1 r) ˆW ]C. The key insights of the proposed cross-view row-wise attention are twofold: First, since the vertical context can be modeled by spatial transformer blocks, row-wise feature exchange provides sufficient receptive field to extract multi-view information. Second, the proposed row-wise masking for reconstruction can also be utilized as data augmentation, since Kr ˆW tokens of each row are randomly dropped. Because the proposed masked modeling task focuses on spatio-temporal modeling, we do not apply mask reconstruction along the view dimension. 4. Experiments 4.1. Setup Datasets. We conduct comprehensive experiments on single-view dataset OpenDV-2K [44] and two multi-views datasets nuScenes [3] and Waymo [34]. We follow the official splits to divide the training and validation sets. Evaluation. The quality of the generated images and videos are assessed using the Frechet Inception Distance (FID) [16] for images and the Frechet Video Distance (FVD) [37] for videos. For fair comparison with previous works, we apply different evaluation settings for singleview model and multi-view model. For single view model, we align the evaluation setting of VISTA [12]. For multiview model, we adopt the setting of Drive-WM [40]. Please refer to Appendix.7.3 for more details. To assess generalization ability, we evaluate zero-shot performance on Waymo validation set, using 600 videos for FVD and 15K frames for FID. Method DriveDreamer [39] MagicDrive [10] DiVE [21] DriveDreamer-2 [47] Drive-WM [40] MaskGWM-mview DriveGAN [22] GenAD [44] Vista [12] MaskGWM-long MaskGWM-long Multiview Futurelayout FID FVD 14.9 19.1 - 11.2 15.8 8.9 73.4 15.4 6. 5.6 4.0 340.8 218.1 94.6 55.7 122.7 65.4 502.3 184.0 89.4 92.5 59.4 Table 1. Performance comparison with state-of-the-art methods on nuScene Dataset. The varying shades of gray indicate our multiview metric following Drive-WM and single-view metric following Vista for more fair comparison. Future layout refers to the availability of layout information for future time steps. denotes training without action. Method FVD FID VISTA [12] 176.56 MaskGWM-long 118.83 9.76 9.55 Table 2. Zero-shot metrics on 600 Waymo validation samples. denotes inference by official checkpoint. 4.2. Training Scheme Our model is initialized with SD3 [7] medium checkpoint with 2B parameters. There are three stages for our training: Stage 1 for pre-training on large-scale OpenDV-2K dataset, Stage 2 for single-view model MaskGWM-long and Stage 3 for multi-view model MaskGWM-mview. For all stages, we resize the original images to 512288, masking ratio is set to 0.25. Stage 1. Following VISTA [12], we first pre-train our model on OpenDV-2K dataset. As our model starts from image backbone, we first train our model using single frame videos with batch size 768 for 18K iterations and then we train our temporal blocks with 16/20/24 frame videos and batch size 64. In particular, temporal blocks is initialized with zero and the training of temporal blocks takes 24K iterations in this step. Afterwards, we insert the zero-initialized reconstruction blocks and train with masking strategy for extra 20K iterations. The reason for varying frame length during training is that the frame numbers for single-view and multi-view models are different. Stage 2 for MaskGWM-long. We devised MaskGWMlong to align with VISTA, where the frame length is set to 25 and cross-view blocks are skipped. We also follow 6 Figure 5. Long-horizon prediction results of MaskGWM. Our model is capable of forecasting long video sequences with stability, devoid of collapse or blurring issues. ting yields significantly improved results, with FID of 4.0 and FVD of 59.4. We present these results for reference. For multi-view cases, similar improvement can also be observed, with 8.9 FID and 65.4 FVD. It is worth noting that our model directly predict multi-view future without the requirement of future layouts, unlike those methods [21] deviating from video prediction task. Furthermore, our method represents pioneering effort in extending generalizable single-view model, trained on OpenDV-2K, to the domain of multi-view models. Generalization ability. In Tab.2, we assess the generalization capability of our approach on the Waymo dataset, which is excluded from our training datasets. We conducted inference using the official checkpoint of VISTA [12]. The results indicate that our method attains superior FVD while maintaining comparable FID, thereby demonstrating the generalization ability of our method. Long-horizon prediction. Fig.6 illustrates the comparison of long-time prediction with VISTA. Due to the computational cost of generating long videos, both FID and FVD metrics are calculated on 300 videos randomly sampled from the OpenDV-2K validation set. The slope of FVD curve is significantly lower than VISTA, showing the less degradation. Qualitative results are illustrated in Fig.5 and appendix. Figure 6. Comparison of Long-horizon FVD metric on OpenDV2K validation set. Our method demonstrates superior performance in terms of both value and growth rate. the stage 2 of VISTAs collaborative training, in which data were sampled equally from nuScene and OpenDV-2K, and the action module is zero-initialized and trained. Stage 3 for MaskGWM-mview. Based on the welltrained MaskGWM-long, we enable multi-view ability of MaskGWM-mview by adding the cross-view blocks and train it on nuScene dataset for 6K steps and frame length is reduced to 8. 4.3. Comparison 4.4. Ablation Studies Generation Quality. Tab.1 presents the quantitative comparison. In single-view generation, we achieve remarkable FID of 5.6 and an FVD of 92.5, surpassing the previous state-of-the-art approaches [12, 44] that are also trained on web-scale dataset. It is worth noting that there is discrepancy between the training and evaluation phases in Vista, which integrates action during training but excludes it during evaluation. To address this, we also experiment with an aligned setting that drops action information for both training and evaluation. As indicated in Table 1, this aligned setWe conduct comprehensive ablation study to verfiy the performance of every component in MaskGWM. Following the setting of GenAD for effective experiments, most ablation studies are conducted on stage 1 after mask reconstruction is inserted and the metrics are reported on the validation set of OpenDV-2K dataset with 3000 video clips for FVD and 18000 frames for FID. For the ablation of cross-view transformer blocks, we use multi-view nuScene metrics. Effect of mask tokens. As indicated in Tab.3a, we make comparisons across different designs for mask tokens 7 τ range contra. FVD FID Mtime row shift att. Mspatial FVD FID mτ fm(ϵ) fm(ϵ) zτ [0, 1] [0.5, 1] [0, 1] [0, 0.5] [0, 1] ours [0, 1] 126.71 120.35 116.85 109.87 113.26 105.52 7.35 7.12 6.40 5.92 6.32 5.69 0 0.25 0.25 0.25 0. 0.25 136.55 142.68 143.07 121.38 116.73 10.28 10.75 10.39 7.36 5.92 105.52 5.69 (a) Different design of mτ . (b) Ablations of two-branch mask reconstruction. Table 3. Ablations of the components in our mask reconstruction. contra stands for contrastive loss, row denotes whether Mtime satisfies above row-wise generation strategy (Mtime = ˆMtime) and att. refers to self-attention row&shift att FVD Time two-branch layers FVD FID att dim ratio FVD FID 0.1 0.25 0.1 0.25 133.24 142.68 123.70 121. 0.368d 0.352d 0.357d 0.329d 1 1 2 2 121.37 105.52 127.91 107.34 5.85 5.69 6.03 5. KW KW 0 0.25 0.25 KHW 0.25 65.9 65.4 71.5 64.7 9.2 8.9 8.7 8.5 (a) Different mask ratio r. (b) Different design of . (c) Ablation of cross-view block. Table 4. Ablations of the impact of mask ratio r, different view transformer block designs and the effect of mask reconstruction on convergence. share stands for applying one shared model for Fs and Ft. Time indicate the training time for 10k steps. mτ . To analysis the influence of diffusion timestep τ , we test only applying mask reconstruction on certain timestep range. We find learnable parameters shown better results on high noisy level and proposed noise embedding fm(ϵ) perform better on low noisy level. This demonstrates the merit of noise embedding, which can recover the original mask reconstruction target for represent learning by explicitly giving ϵ, achieving better result on generation steps with local details. In addition, we also try the mask tokens used in SD-DiT [50], combining with extra contrastive [4] loss. The experiment shows our proposed diffusion-related mask tokens achieve best performance via combining the advantage of and fm(ϵ). Effect of mask reconstruction. In Tab.3b and Tab.4a, we explore the effectiveness of proposed mask reconstruction in spatial-temporal domain. According to Tab.4a, we find that our row-wise time mask with shift attention mechanism assists lot the models convergence, while larger mask ratio can also make positive effect on the results. Thus, this setting is adopted in the ablation study on the mask strategy M. As shown in Tab.3b, combining Mspatial and Mtime can significantly improve the generation quality of the whole videos and each single images. We hypothesis this is because the temporal modeling is more sensitive to the dropout ratio than spatial counterpart (Please see more details on Appendix.6.1). When shifted temporal self-attention is applied with row-wise mask, all tokens on temporal axis are retained and experience only minor spatial shifts. Whereas invisible tokens are skipped without shift temporal self-attention, which results in discrepancy between the number of tokens used in training and inference. Moreover, the training speed is improved since invisible tokens dropped. From Tab.3b, we can see our two-branch mask reconstruction achieve best results, showing the effectiveness of introducing mask reconstruction on temporal context. Effect of cross-view module. As shown in Tab.4c, we make comparisons across different types of cross-view modeling. We find that introducing mask reconstruction on stage-3 training also yields favorable results. This indicates that randomly masking certain tokens along the view-row dimension is not detrimental and can even enhance the final results. This is different from temporal counterpart that requires shift-attention to avoiding tokens dropping. Furthermore, the experiment shows that proposed attention on dimensions KW outperforms view-attention on used in previous methods [39, 40]. For view attention on KHW, despite the minor improvement, the computation complexity explodes significantly. Consequently, this design is not adopted in our experiments. Effect of two-branch token reconstruction. We validate the effectiveness of two-branch transformer reconstruction structure for token reconstruction on Tab.4b. We remove two-branch structure by applying sequential spatialtemporal transformer blocks, which is shared by Fs and Ft. We find the two-branch structure produces better results, especially on FVD. Unshared design forces the model to reconstruct masked features by corresponding context, leading to better spatial and temporal modeling for different conditions. 5. Conclusion We introduce MaskGWM, the first DiT-based driving world model trained on web-scale datasets with masking strategy during training. By introducing novel video dual-branch mask reconstruction, our model excels in both numeric metrics on fidelity and generation ability. Additionally, our mask policy also accelerate the training process and read memory consumption. Our extensive experiments showcase MaskGWM achieves the state-of-the-art performance on generation quality on nuScene, zero-shot ability on Waymo and longtime prediction ability on OpenDV-2K. These results furtehr indicates that our method can serve as greate training programs to enable long driving video prediction."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [3] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multiIn Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. 6 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 8 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. 2 [6] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 5 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 3, [8] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: Diffusion transformer for image editing. arXiv:2411.03286, 2024. 3 arXiv preprint [9] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 3 [10] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. 6 [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2316423173, 2023. 2, 3, 4, 5 [12] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 1, 2, 3, 6, 7 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 1, 2, 4, 5 [15] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 3 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [19] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 1, 2, 3 [20] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1785317862, 2023. 1 9 [21] Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, et al. Dive: Dit-based video generation with enhanced control. arXiv preprint arXiv:2409.01595, 2024. 2, 3, 6, 7 [22] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards controllable high-quality In Proceedings of the IEEE/CVF Conneural simulation. ference on Computer Vision and Pattern Recognition, pages 58205829, 2021. 6 [23] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. [24] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In European conference on computer vision, pages 118. Springer, 2022. 1 [25] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose Alvarez. Is ego status all you need for openIn Proceedings of loop end-to-end autonomous driving? the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1486414873, 2024. 1 [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [27] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [28] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 2 [29] Open-Sora. Open-sora. URL: https://github.com/ hpcaitech/Open-Sora, 2024. 3 [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3 [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1 [34] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. [35] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. 4 [36] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 2, 5 [37] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [38] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1454914560, 2023. 2, 5 [39] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddriven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. 1, 3, 6, 8 [40] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for auIn Proceedings of the IEEE/CVF Contonomous driving. ference on Computer Vision and Pattern Recognition, pages 1474914759, 2024. 2, 3, 6, 8 [41] Wei Wu, Xi Guo, Weixuan Tang, Tingxuan Huang, Chiyu Wang, Dongyue Chen, and Chenjing Ding. Drivescape: Towards high-resolution controllable multi-view driving video generation. arXiv preprint arXiv:2409.05463, 2024. 1 [42] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Benchmarking and improving birds eye view perception robustness in autonomous driving. arXiv preprint arXiv:2405.17426, 2024. [43] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96539663, 2022. 2, 4 [44] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, et al. Generalized predictive model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1466214672, 2024. 1, 2, 3, 6, 7 [45] Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric Chang, Hanwang Zhang, et al. Exploring diffusion timearXiv steps for unsupervised representation learning. preprint arXiv:2401.11430, 2024. 5 [46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 3 10 [47] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. arXiv preprint arXiv:2403.06845, 2024. 1, [48] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 1, 2, 3, 4, 5 [49] Jun Zheng, Fuwei Zhao, Youjiang Xu, Xin Dong, and Xiaodan Liang. Viton-dit: Learning in-the-wild video try-on from human dance videos via diffusion transformers. arXiv preprint arXiv:2405.18326, 2024. 3 [50] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. Sd-dit: Unleashing the power of self-supervised discrimination in diffusion In Proceedings of the IEEE/CVF Conference transformer. on Computer Vision and Pattern Recognition, pages 8435 8445, 2024. 2, 3, 4, 5, 8 11 MaskGWM: Generalizable Driving World Model with Video Mask Reconstruction"
        },
        {
            "title": "Supplementary Material",
            "content": "6. More Ablation Experiments 6.3. Additional Results of Mask Reconstruction on 6.1. Additional Results of Mask Ratio Table 5 illustrates the impact of the mask ratio on mask reconstruction across various branches and temporal attention strategies. Our findings reveal several key insights: (1) The temporal branch equipped with masked temporal selfattention is more sensitive to mask ratio and necessitates substantially lower mask ratio compared to the spatial branch. (2) The influence of the mask ratio on the proposed shifted temporal self-attention is more consistent with that observed on the spatial branch. As depicted in Fig.4, the main difference in the DiT Encoder with the spatial branch is the positional shift, which can be effectively handled by positional encoding. Consequently, this allows for the attainment of an well-performed mask ratio (e.g. 0.25) in both spatial MR and temporal MR. = Mspatial = Mtime = ˆMtime 0 0.1 0.25 0.4 136.5 133.2 142.6 179. 136.5 125.8 116.7 155.9 136.5 123.7 121.3 149.8 Table 5. FVD comparisons on mask ratio. 6.2. Additional Results of Mask Reconstruction on"
        },
        {
            "title": "NuScene Dataset",
            "content": "As described in GenAD [44], the training and validation sets of OpenDV-2K are sourced from different YouTube the videos with significant scene changes. Therefore, models performance on this dataset can be used for the evaluation of its generalization ability. We also conduct ablation studies in the in-domain setting by evaluating metrics on the nuScenes validation dataset. As shown in Table 6, the proposed mask reconstruction method achieves significant improvements on both metrics. row&shift att. 0 FVD FID 107.2 0.25 92.5 7.5 5.6 Table 6. Ablations on nuScene dataset. 1 Long-Horizon Prediction To further analyse the influence of MR on auto-regressive generation, we extend the video duration to approximately 12 seconds and documented the metrics in Fig.7. The results indicate that MR is also effective in enhancing performance in long-sequence prediction. Although our baseline without MR still outperforms Vista, the quality of generation begins to deteriorate notably from about 8 seconds, and the FID score increases to 37.7 at the 10 second, making it also incapable to predict the distant future. Consequently, we conclude that this baselines improvements cannot translate to significant advancements in long-sequence. In contrast, when MR is integrated into our method, the fundamental enhancements in single-step generation lead to significantly alleviate quality degradation over time. As result, MaskGWM is capable of generating 10 Hz videos with discernible scene elements for long time, and even 60 second examples, which far exceeds both Vista and our nonMR baseline. Therefore, we regard MR as pivotal design that enables the model to make generalized predictions over extended durations. Note that this evaluation is conducted on 300 videos of OpenDV-2K validation set only, due to their longer video sequence. Thus, the single-step (2.5 seconds) FID and FVD are higher than results in Tab.6, which is computed on 1800 videos. 7. Implementation Details 7.1. Concrete DiT Structure We adopt the framework of SD3 and start our model with 2B parameters. We make several modifications to the original spatial transformer block to facilitate temporal and cross-view context modeling. First, Due to the limited availability of high-quality text data in our training datasets, e.g. only scene-level descriptions on nuScene, we skip the update of text feature by new-initialized temporal and cross-view transformer blocks. Then, for temporal transformer block, we make another modification to accommodate condition frames. To streamline the explanation, we represent the transformation within transformer block as out = out are the input and out features, respectively, and fb is the transformer block. Given the frame-level binary indicator mc with value 0 on condition frames, the diffusion time-step τ , and time-step aware embeddings for scale fscale and shift fshif t, we inin), where in + fb(z in and Figure 7. Comparison of Long-horizon FVD metric on OpenDV-2K validation set. MR plays crucial role in enhancing the capability to predict long video sequences, especially on FID. troduce condition frames by: 7.4. Details of Comparisons with Vista out = z in + fb(fscale(mcτ )z in + fshif t(mcτ )) (5) Here mcτ is employed to reset the time-step for conditional frames to zero and time-step aware embeddings is applied for linear transform. We append one temporal transformer block and one view transformer block after each spatial transformer block following the common practice of previous works [12, 40]. 7.2. Detailed Training Parameters We employ the Adam optimizer [28] for model training, using learning rate of 5e-5. Throughout all training stages, we initiate the process with 1K warm-up steps and then maintain constant learning rate. For condition frames, we randomly sample from zero to three frames following VISTA. We train Stage 1 for total 62K steps, Stage 2 for total 20K steps and Stage 3 for 6K steps. We select the training step based on numerical metrics from videos that are randomly sampled from the training set. Our training are conducted on 32 A800 GPUs with around 3 days on Stage 1. 7.3. Detailed Sampling Parameters Our sampling strategy does not incorporate any special designs. We generate the video by sampling 30 steps and utilize classifier-free guidance scale [17] of 4.0. Following Vista, we generate 25-frame videos containing one reference frame on full nuScene validation set with 5369 samples for our single-view model. All generated videos and corresponding frames are used for computing FVD and FID respectively. For our multi-view model, we generate 150 6view videos for each nuScene scene, resulting in 900 singleview videos. Then, 10K frames are randomly sampled from these 900 videos for computing FID. This is align with the evaluation setting of DriveWM [40]. 2 For comparisons with Vista, we use the official sample script and checkpoint. For zero-infer on Waymo dataset, we infer both models without action and the number of condition frame is set to 1. For long-horizon rollout on OpenDV2K dataset, we infer both models without action and the number of condition frame is set to 3 for better temporal continuity across auto-regressive steps. We find numeric improvement is similar for one condition frame but the qualitative continuity is reduced by one-frame auto-regression. For auto-regressive steps larger than 1, we randomly select 25 frames from the generated video sequences to calculate the FVD and FID metrics. 8. Qualitative Results 8.1. Long-horizon rollouts (what is rollout) Longer prediction We provide more qualitative and longer visualizations with 42-seconds videos in Fig.8. We find MaskGWM can predict stable and consistent driving future, combined with unseen scene with initial scope. Qualitative comparisons In Fig.11, we make qualitative comparisons with Vista, which is previous state-of-the-art method on generalizable driving world model. Our method can both make stable prediction and generate dynamic objects according to the future, e.g. unseen cars in initial visual scope. Diverse scenes In Fig.9, we present the extended generation results across various scenes, demonstrating the robust generalization capability of our approach. Action control In Figure 12, we illustrate the controllability of our method on the OpenDV-2K dataset, adhering to the action module in Vista. Multi-view generation In Figure 10, we show the multitailed local context. Our results show that diffusion models may excel in generating high-fidelity results but learn context reasoning slowly, which can be improved through the MR task. More generally, the effectiveness of MR shows that relying solely on diffusion may not be the optimal approach for driving world models. similar inspiration can also be found in GAIA-1, where the prediction ability is decomposed into an auto-regressive model and diffusion model. Exploring training targets for world models can be promising direction. 9.4. Limitations. Although better generalization ability and quality are achieved, there still exist some limitations that call for future works. (1) Controllability. Since we focused our main improvements on generalization ability and long-duration prediction, the action module follows the design of Vista. We have found several challenging cases in control, such as unreasonable commands. Similar to Vista, our method relies on resampling the nuScenes dataset to learn control ability. As result, finding better feedback strategies and larger datasets for action learning is promising direction. (2) Prediction of Uncertain Future. This phenomenon mainly arises when encountering complex traffic scenarios, especially when predicting the movement of each vehicle is difficult. (3) Generation of Non-Front View Images. Since multi-view capability is introduced only at the last training stage with single nuScenes dataset, the images of nonfront views lack exposure before this stage. Incorporating non-front view data at an earlier stage or adding more multiview datasets (e.g., Waymo) may help address this problem. view generation ability coming from lifting our single-view model by extra view transformer blocks. 9. Discussions 9.1. Differences to Vista Although both our method and Vista [12] aim to construct generalizable world model using the large-scale OpenDV2K dataset, we highlight several key distinctions here. First, our findings suggest that relying solely on the Diffusion loss may not be optimal for building world model. We introduce complementary MR task, which has demonstrated robust generalization capabilities in representation learning tasks. Second, our model enables multi-view video generation through an additional training stage. This also illustrates that multi-view generation can benefit from welltrained single-view model trained on dataset encompassing significantly longer durationsover 1,700 hours in the OpenDV-2K dataset. Third, our model achieves longer prediction durations than Vista. As indicated by the slope of the metric changes in Fig. 7, our method maintains stable video prediction results, up to 15 seconds by autoregressive generation, whereas the generation quality of Vista degrades notably at this point. Moreover, we have found that our model can sustain stable generation over longer time periods across diverse scenes. Regarding quantitative evaluation, our model exhibits superior generalization capabilities, as evidenced by results on both the OpenDV-2K and Waymo datasets. On the standard nuScene benchmark, our approach also yields better results, with 19% improvement in FID and 3% decrease in FVD. 9.2. Usage of Stable Diffusion 3 Our baseline, built upon the SD3 [7] model, yields superior results compared to GenAD (trained on SDXL [31]) and performance slightly lower than Vista (initialized with SVD [1]). Since both GenAD and our baseline are derived from image generation models, the improved performance of our baseline demonstrates the effectiveness of SD3. The superiority of SVD is attributed to its wellinitialized temporal blocks, which have undergone multistage pre-training on extensive video datasets. Therefore, enhancing the data efficiency of SD3as in our MR policyand incorporating more video data present promising avenues to bridge this performance gap. 9.3. Future impact of MR. In our method, MR acts as complementary task to the diffusion loss, incorporating better video prediction abilities. Within the scope of representation learning, MR conducts context reasoning in self-supervised way and can be generalized to various tasks. This aligns with our design: recovering the original MR at low noise levels using de3 Figure 8. Generalization ability of MaskGWM with longer time. 4 Figure 9. Generalization ability of MaskGWM in more scenarios. 5 Figure 10. Generalization ability of multi-view videos. Figure 11. Qualitative comparison with Vista. Figure 12. Action control ability of MaskGWM."
        }
    ],
    "affiliations": [
        "SenseTime Research"
    ]
}