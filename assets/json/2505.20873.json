{
    "paper_title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models",
    "authors": [
        "Chaeyoung Jung",
        "Youngjoon Jang",
        "Jongmin Choi",
        "Joon Son Chung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 7 8 0 2 . 5 0 5 2 : r Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models Chaeyoung Jung Youngjoon Jang Jongmin Choi Joon Son Chung Korea Advanced Institute of Science and Technology (KAIST)"
        },
        {
            "title": "Abstract",
            "content": "The goal of this work is to enhance balanced multimodal understanding in audiovisual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have demonstrated superior performance in various text-centric tasks such as problem solving, translation, and summarization [1, 3, 26, 40, 42, 43, 52]. Building on these successes in text processing, LLMs have evolved to handle additional modalities, including images [2, 4, 9, 15, 22, 25, 48, 51, 55], videos [24, 29], and audio [14, 33, 38], giving rise to multimodal LLMs (MLLMs). These models process diverse modalities through separate encoders and integrate their outputs within language model decoder, achieving remarkable performance across wide range of tasks. Among these, audio-visual LLMs (AV-LLMs) are particularly notable for their ability to jointly integrate visual and auditory information, enabling more sophisticated reasoning and closer alignment with human multimodal understanding [6, 35, 50]. To effectively leverage pretrained LLM decoders in AV-LLMs, various fusion strategies have been proposed to integrate audio and visual information. One common approach [5, 6, 7, 11, 28, 32, 47, 49, 50, 53] is token-wise fusion, where audio and visual features are extracted by separate encoders and then concatenated along the sequence dimension before being fed into the decoder as continuous input sequence. Several studies [7, 47] additionally introduce adapter modules that facilitate interaction between audio and visual features before they are passed into the LLM. Another approach [12, 34, 35] is channel-wise fusion, in which modality-specific features are concatenated along the channel dimension to form unified representation. In most current AV-LLM architectures, Equal contribution. Preprint. the decoder receives both audio and visual inputs simultaneously, which raises potential concern: if the model finds one modality easier to interpretperhaps due to better alignment with its pretraining objectivesit may over-rely on that modality, resulting in modality-specific hallucinations [19]. To investigate this possibility, we begin by analyzing the attention weight distributions over audio-visual inputs using 100 samples from the AVHBench dataset [37]. Our analysis focuses on the final decoder layer of VideoLLaMA2 [6], examining the attention weights of the last token to quantify the relative attention allocated to each modality. As shown in Figure 1, the vanilla decoding setuprepresenting the models default inference behaviorexhibits clear bias toward video inputs, with attention disproportionately concentrated on visual features over audio. This observation aligns with findings from recent studies [10, 19, 31, 41], which report that LLM-based decoders in multimodal settings often exhibit modality bias. These studies highlight that such imbalances can lead to hallucinations or flawed reasoning, emphasizing the need to mitigate modality bias for more robust and balanced multimodal understanding. Figure 1: Attention weight analysis in VideoLLaMA2 [6] on AVHBench dataset [37]. We analyze 100 samples and examine the attention weights from the last decoder layer, focusing on the last token in the question. The attention is disproportionately assigned to video inputs over audio, indicating modality bias. Our proposed FMD method mitigates this gap by promoting more balanced contributions from both modalities. To address this issue, we propose Fork-Merge Decoding (FMD), which is simple yet effective strategy that enhances multimodal understanding without altering the AV-LLM architecture or requiring additional training. The core idea of FMD is to divide the decoding phase into two stages: fork phase and merge phase, designed to improve both unimodal and multimodal understanding. In the fork phase, the original multimodal input is split into two unimodal branches by zeroing out either the visual or auditory modality while retaining the text question. Each branch is independently processed through the early layers of the pretrained AV-LLM, yielding modality-specific hidden representations. In the merge phase, these representations are combined and passed through the remaining decoder layers to enable joint reasoning. This separation enables the model to first attend to unimodal cues in isolation before integrating them for complementary multimodal understanding. As shown in Figure 1, FMD effectively reduces modality bias and balances audio-visual contributions. Furthermore, since recent AV-LLMs commonly adopt either token-wise or channel-wise concatenation for audio-visual fusion, we propose generalized decoding strategy that is compatible with both. This unified approach improves performance across variety of models, regardless of their fusion mechanism. Recent AV-LLM architectures and their fusion methods are summarized in Supp. A. Furthermore, we evaluate the effectiveness of FMD by applying it to two recent AV-LLMs: VideoLLaMA2, which performs token-wise fusion, and video-SALMONN [35], which adopts channel-wise fusion. With FMD, both models achieve improved decoding performance across three widely used audio-visual benchmarks: AVQA [45], MUSIC-AVQA [21], and AVHBench [37]. Notably, FMD enhances performance not only in tasks that emphasize single modality but also in tasks that require balanced reasoning across both modalities. These results demonstrate that FMD effectively supports multimodal understanding by fully leveraging the information from each modality during inference."
        },
        {
            "title": "2 Preliminaries",
            "content": "Input processing. In many existing AV-LLMs [5, 6, 7, 28, 32, 35, 47, 49, 50, 53], audio and video inputs are first processed independently through their respective encoders before being jointly fed into the LLM decoder. Visual inputs are typically handled by vision encoder that processes each sampled frame to generate frame-level embeddings capturing spatial information. Audio inputs are transformed by an audio encoder into semantic representations that encode acoustic and prosodic features of the signal. Textual inputs, such as instructions or questions, are tokenized and embedded using the tokenizer and embedding layer associated with the base language model. For token-wise fusion-based AV-LLMs, video inputs are first transformed into fixed set of embeddings, denoted as xv = {x1, x2, . . . , xM }. Similarly, audio inputs are encoded into 2 Figure 2: Overview of the Fork-Merge Decoding (FMD) pipeline. The AV-LLM takes video frames, an audio waveform, and question prompt as input. In the fork phase, FMD masks either the video or audio modality while preserving the question prompt, enabling independent reasoning over each modality. After Lfork decoder layers in ϕ, the merge phase combines the resulting hidden states, hv fork and ha fork, using an importance weight α derived from attention distributions over unmasked modality tokens; the complementary weight (1 α) is applied to masked counterparts. This decoding scheme enables the pretrained AV-LLM to integrate both audio and visual cues effectively, allowing it to resolve questions that may be ambiguous or misleading when relying on single modality. embeddings, represented as xa = {xM +1, . . . , xM +N }. These visual and audio embeddings are concatenated along the token dimension to form an initial multimodal sequence. The question text is tokenized and embedded into text embeddings using the language models text embedding layer, yielding xl = {xM +N +1, . . . , xM +N +L}. The final input sequence is constructed by concatenating the visual, audio, and text embeddings: = xv xa xl, where denotes tokenwise concatenation. This produces sequence of length + + L. For simplicity, we omit instruction tokens, which are typically prepended to the input sequence. In contrast, channel-wise fusion-based AV-LLMs first project both visual and audio features into embeddings of fixed length , and then concatenate them along the channel (feature) dimension to form joint audio-visual embeddings: xav denote the visual and audio embeddings, respectively. These audio-visual embeddings are subsequently combined with the text embeddings through token-wise concatenation. The resulting input sequence is defined as = {xav +L}, which is then fed into the decoder for further processing. ], where xv +1, . . . , xl 1 , . . . , xav = [xv and xa , xl ; xa Decoding. Both token-wise and channel-wise fusion-based AV-LLMs generate outputs from the input sequence using an autoregressive decoding strategy, where each token is predicted by attending to previously generated tokens under causal mask. At timestep t, the model generates the next token yt conditioned on the input sequence and the previously generated tokens y<t: yt p(ytx, y<t) exp (logit(ytx, y<t)) . (1) At each decoding step, the model computes hidden states conditioned on the causal context y<t. These hidden states are then passed through linear projection layer, commonly known as the vocabulary head, to produce logits over the vocabulary. softmax function is applied to the logits to produce probability distribution over candidates for the next token."
        },
        {
            "title": "3 Enhancing Audio-Visual understanding with Fork-Merge Decoding",
            "content": "This section introduces Fork-Merge Decoding (FMD), general decoding framework applicable to two representative fusion architectures in AV-LLMs: token-wise fusion (Section 3.1) and channel-wise fusion (Section 3.2). An overview of the process is illustrated in Figure 2. 3.1 Decoding with Token-wise Fusion in AV-LLMs Masking strategy for input sequences. In token-wise fusion models like VideoLLaMA2 [6], the input sequence is composed of = xv xa xl, where xv, xa, and xl denote visual, audio, and language embeddings, respectively. To enable modality specific processing for audio and video while preserving the textual question, we construct two masked variants of the input: xv[masked] = xv xa xl, xa[masked] = xv xa xl, (2) 3 where xv and xa denote the modality-masked embeddings for vision and audio, respectively. These are obtained by zeroing out the corresponding video frames or audio waveforms at the input level. This preserves original embedding shapes and positions while removing content information. Fork processing. To encourage independent understanding over each modality, each masked sequence is separately processed through the first Lfork transformer layers of the decoder ϕ: fork = ϕLfork(xv[masked]), ha hv fork = ϕLfork(xa[masked]), (3) fork and ha where hv fork denote the intermediate hidden states obtained from the vision-masked and audio-masked inputs, respectively. This design ensures that the model does not observe both audio and visual inputs simultaneously in the early stages, allowing it to focus on how each modality individually relates to the textual prompt. By reasoning over each modality in isolation, the model is less likely to become biased toward the more dominant or easier-to-interpret modality. Merge processing. After the Lfork layers, the hidden states hv fork are fused and passed through the remaining transformer layers. Each hidden state has sequence length of + + L, corresponding to the visual, audio, and text tokens. The fusion is performed by summing the corresponding embeddings at each modality position, and is formally defined as follows: fork and ha hmerge[V] = (1 α) hv hmerge[A] = α hv 2 (hv hmerge[L] = 1 fork[V] + α ha fork[A] + (1 α) ha fork[L] + ha fork[L]) , fork[V], fork[A], (4) where = [1:M ], = [M +1:M +N ], and = [M +N +1:M +N +L] denote the index ranges corresponding to the visual, audio, and language embeddings, respectively. The final merged representation hmerge is then constructed by concatenating the modality-specific segments along the token dimension as hmerge = hmerge[V] hmerge[A] hmerge[L]. Here, α serves as modality-specific weighting factor that controls the relative contribution of each unmasked modality in the fusion. We refer to this approach as attention-guided fusion, which is described in detail in the following section. Attention-guided fusion. To determine the fusion weight α, we leverage the attention matrix Afinal RT from the final transformer layer. Specifically, we focus on the attention vector of the T,: RT , which plays critical role in next-token prediction. Using masked last token, alast = Afinal inputs xv[masked] and xa[masked], we compute the attention-based contribution of each unmasked modality by comparing the attention mass it receives relative to the masked version as follows: αa = (cid:80) iA av[masked] [i] i{VA} av[masked] last last (cid:80) , αv = [i] (cid:80) iV aa[masked] [i] i{VA} aa[masked] last last (cid:80) , [i] (5) where av[masked] represent the attention weight distributions of the final token when the last visual or audio modality is masked, respectively. Finally, we compute the average attention mass assigned to the unmasked modality in each of the video-masked and audio-masked branches: and aa[masked] last α = 1 2 (αa + αv). (6) The aggregated weight α is used to interpolate between hv fork in Eq. (4). In practice, instead of computing α for each input, we estimate representative values by sampling 100 samples from the AVHBench [37] dataset. These fixed weights are then used consistently across all experiments. fork and ha This attention-guided fusion ensures that structurally aligned hidden states are preserved, while allowing the more informative modality to be emphasized. It enables flexible and interpretable merging scheme without disrupting the architectural integrity of pretrained AV-LLMs. Decoding. The merged hidden state is then forwarded through the remaining transformer layers: hfinal = ϕ>Lfork(hmerge), (7) producing the final prediction logits. By delaying modality fusion to deeper layerswhere individual representations become semantically richer through the fork phaseour method enhances multimodal understanding while mitigating issues caused by modality imbalance. Table 1: Comparison of audio-visual understanding performance. We evaluate FMD on two AV-LLMs: VideoLLaMA2 [6] (token-wise fusion) and video-SALMONN [35] (channel-wise fusion). Vanilla denotes each models original decoding strategy. Experiments are conducted on AVQA [45], MUSIC-AVQA [21], and AVHBench [37]. FMD consistently improves performance across all benchmarks, especially on VA tasks that rely more heavily on the audio modality. Model Decoding AVQA MUSIC-QA AVHBench AV VA AV matching AV captioning VideoLLaMA2 [6] video-SALMONN [35] Vanilla FMD Vanilla FMD 82.460.02 82.740.05 36.360.04 36.890.19 81.300.09 81.570.02 44.480.02 44.780.08 80.02 80.34 68.69 71. 77.03 77.70 62.39 65.31 57.75 58.89 49.46 51.49 2.840.01 2.940.02 1.830.01 1.890.01 3.2 Decoding with Channel-wise Fusion in AV-LLMs Masking strategy for input sequences. In channel-wise fusion models such as videoSALMONN [35], the input sequence is structured as = {xav , xl +L}, where xav denotes audio-visual embeddings and xl corresponds to language (prompt) embeddings. Here, and indicate the number of audio-visual and language sequence elements, respectively. To allow for modality-specific processing, we construct two masked variants of the input: +1, . . . , xl 1 , . . . , xav xv[masked] = xav xl, xa[masked] = xav xl, (8) where xav and xav denote the video-masked and audio-masked embeddings, respectively, obtained by zeroing out the corresponding inputs before they are passed into the decoder layers. fork and ha Fork-merge decoding. hv fork are obtained by passing the masked inputs xv[masked] and xa[masked] through the Lf ork layers (refer to Eq. (3)). To compute the merged hidden state hmerge, we perform element-wise addition over the audio-visual embedding representations from both branches, based on the assumption that they capture complementary information. Since each branch processes modality-masked inputs (i.e., audio-masked for visual features and vice versa), their combination is expected to yield more complete representation. Additionally, mean pooling is applied over the question prompt embedding positions to maintain consistency: hmerge[i] = (cid:26)hv fork[i] + ha 2 (hv fork[i], if U, if > U. fork[i] + ha We do not apply attention-guided fusion in the channel-wise setting, as the hidden states do not disentangle audio and visual embeddings. Finally, decoding is then continued by forwarding the merged hidden state hmerge through the remaining decoder layers to produce the final output logits (see Eq. (7)). Notably, FMD provides unified processing framework that seamlessly integrates input masking, modality-specific processing via separate decoder branches, and merged decodingmaking it broadly applicable to wide range of AV-LLM architectures, regardless of their fusion strategies. fork[i]) , (9)"
        },
        {
            "title": "4 Experiments",
            "content": "We conduct extensive experiments to evaluate the effectiveness of the proposed Fork-Merge Decoding (FMD) method. This section begins with description of the experimental setup, including baselines, datasets, and implementation details (Section 4.1). We then present both quantitative results (Section 4.2) and qualitative examples (Section 4.3) to demonstrate the effectiveness of our approach. Finally, we provide deeper analysis of FMD in (Section 4.4). 4.1 Experimental Setup Baselines. We evaluate our approach using two representative AV-LLMs: VideoLLaMA2 [6] and video-SALMONN [35], which adopt token-wise and channel-wise fusion strategies, respectively. These models are chosen to demonstrate the generality of our proposed FMD method across different fusion paradigms. We initialize the models using their official pretrained weights. Datasets and evaluation protocol. The AVQA [45] dataset contains 57,000 YouTube videos for evaluating real-world audio-visual understanding. MUSIC-AVQA [21] offers 45,867 QA pairs 5 from 9,288 music performance videos, focusing on fine-grained audio-visual reasoning such as identifying sound sources and temporally aligning auditory and visual cues. AVHBench [37] is the first benchmark specifically designed to assess audio-visual hallucinations in AV-LLMs. It comprises four subtasks: audio-driven video hallucination (AV), video-driven audio hallucination (VA), audio-visual matching (AV matching), and audio-visual captioning (AV captioning). For the transformer layer analysis in Section 4.4, we select 200 samples from each task (AV, VA, and AV matching), with the remaining data used for evaluation. For the AVHBench dataset, the three binary (yes/no) tasks except AV captioning, we report classification accuracy. For AVQA, MUSIC-AVQA and AV Captioning, which involve open-ended responses, we follow the GPT-assisted evaluation protocol from the official VideoLLaMA2 implementation2, reporting both the average score and standard deviation across multiple runs. Implementation details. The number of layers used for fork phase is set to 5 for VideoLLaMA2 (out of 28 layers) and 8 for video-SALMONN (out of 40 layers). For attention-guided fusion, we set the weighting parameter to α = 0.8, which is rounded value obtained from averaging fusion weights over 100 randomly sampled examples from AVHBench, as described in Section 3.1. 4.2 Quantitative Analysis Comparison with vanilla decoding. To verify the effectiveness of our proposed FMD method, we evaluate it using VideoLLaMA2 and video-SALMONN on three datasets: AVQA, MUSIC-AVQA, and AVHBench. As shown in Table 1, FMD consistently outperforms the baseline across all tasks. This suggests that FMD enables the model to aggregate more balanced multimodal information. Especially, this is achieved by performing unimodal analysis within the fork layers and cross-modal understanding within the merge layers, allowing for more effective integration of visual and auditory inputs. Moreover, even though video-SALMONN is not trained on AVQA or MUSIC-AVQA, FMD still enhances performance, demonstrating the robustness and generalizability of our approach. Table 2: Comparison of decoding methods on AVHBench [37] dataset using VideoLLaMA2 [6]. We compare the vanilla decoding, DoLa [8] VCD [20], SID [16], and two FMD variants (Gaussian noise injection and zero-out masking). Among them, FMD with zero-out masking achieves the highest overall accuracy, underscoring its effectiveness in audio-visual reasoning. Comparison with other decoding methods. We compare our FMD method with other testtime decoding strategies that operate at the logit level. The evaluation is conducted using the VideoLLaMA2 model on the AVHBench dataset. Specifically, the comparison includes the following: DoLa [8], VCD [20], SID [16], and FMD variants with Gaussian noise injection and zeroout masking. DoLa contrasts intermediate-layer outputs with final predictions to factually correct outputs. VCD reduces language bias by injecting Gaussian noise into the visual input and subtracting the resulting logits from the original ones. SID adopts similar approach with VCD but preserves the least informative visual tokens based on attention maps, then contrasts their logits with the original outputs. For DoLa, we extract the intermediate layers output from the same layer specified in the original paper. For both VCD and SID, originally designed for vision-language models (VLMs), we adapt their methods to jointly handle audio and video modalities in AV-LLMs. DoLa [8] VCD [20] SID [16] FMD w/ noise FMD w/ zero-out LLM VLM VLM AV-LLM AV-LLM 63.44 69.67 72.82 78.23 77.70 48.33 52.52 53.52 57.10 58.89 69.34 75.96 78.53 79.06 80.34 Designed for AV Matching AVHBench Decoding Vanilla VA AV 77.03 80.02 57.75 - As shown in Table 2, applying decoding strategies developed for LLMs or VLMs to AV-LLMs leads to degraded performance. This highlights the need for decoding methods that are specifically tailored to the unique characteristics of AV-LLMs, which differ substantially from those of unimodal or bimodal models. Additionally, injecting Gaussian noise into the inputs within FMD (denoted as FMD with noise) results in lower performance on AV and AV matching tasks compared to vanilla decoding, although it does lead to improved performance on the VA task. We attribute this to the inability of Gaussian noise to equally isolate modality-specific information, as further discussed in Supp. B. In contrast, zero-out masking within FMD yields consistent performance improvements across all tasks, demonstrating that the proposed FMD design is well-suited for audio-visual understanding. 2https://github.com/DAMO-NLP-SG/VideoLLaMA2/tree/audio_visual 6 (a) An example of audio-driven video hallucination (AV). (b) An example of video-driven audio hallucination (VA). (c) An example of audio-visual matching (AV matching). (d) An example of audio-visual captioning (AV captioning). Figure 3: Qualitative results using VideoLLaMA2 [6] on each task of the AVHBench [37] dataset. FMD generates more accurate answers by effectively leveraging both audio and visual modalities, whereas vanilla decoding is more prone to confusion due to over-reliance on single modality. 4.3 Qualitative analysis Audio-driven video hallucination (Figure 3a). The model is asked question based on video content, where woman appears visually but is not the one speaking; instead, separate female voice narrates the scene. Vanilla decoding wrongly assumes the visible woman is speaking, hallucinating audio-visual alignment. In contrast, FMD detects the mismatch and avoids hallucination. Video-driven audio hallucination (Figure 3b). The video depicts moving fire truck, but the audio contains only background chatter, with no siren or other relevant sounds. When asked whether the fire truck is making sound, vanilla decoding incorrectly assumes the presence of sound based on visual priors. In contrast, FMD correctly identifies that the fire truck does not produce any sound. Audio-visual matching (Figure 3c). This example presents synthetically mismatched audiovideo pair, where unrelated audio is overlaid on the video. Vanilla decoding fails to detect the inconsistency and incorrectly predicts match. Unlike vanilla decoding, FMD better balances modality contributions and correctly identifies the mismatch between audio and visual inputs. Audio-visual captioning (Figure 3d). This example requires more complex understanding and interpretation of both video and audio, rather than simple yes or no answer. Vanilla decoding relies on the video and merely infers that the saw is cutting piece of wood. In contrast, when FMD is applied, it shows an understanding of the saws shape, the red line, and the cutting sound, indicating deeper comprehension of the scene. This demonstrates the effectiveness of FMD in various audio-visual tasks. More qualitative results can be found in the Supp. C. 4.4 Further analysis Layer selection strategy for merge point. To determine the optimal layer for merging hidden states, we conduct two types of analysis. VideoLLaMA2 is used as the model, and 200 examples per task are randomly sampled from the AVHBench dataset across three tasks: AV, VA , and AV matching. To ensure fair evaluation, all sampled examples are excluded from the test set, 7 Figure 4: Layer-wise attention weight comparison on VideoLLaMA2 [6] using 600 samples from the AVHBench [37] dataset. We analyze the attention weights from the final token in the last decoder layer, focusing on the distribution across video and audio segments. Deeper merging within the network results in reduced attention to visual tokens and heightened attention to audio tokens. Figure 5: Layer-wise ablation results on VideoLLaMA2 [6] using 200 samples from the AVHBench [37] dataset for each task. To identify the optimal merge layer, we evaluate performance across three tasks, each focused on certain modality: AV for video-targeted understanding, VA for audio-targeted understanding, and AV Matching for joint audio-visual understanding. First, we examine the attention weight distribution across layers to choose the merge point, as shown in Figure 4. We observe that the deeper the merge occurs in the network, the lower the attention paid to visual tokens and the higher the attention paid to audio tokens. However, attention alone is insufficient to determine the optimal merge layer. Our goal is to enable effective decoding across diverse tasks by facilitating interaction between visual and audio information. Based on this, we further analyze task performance across different merge layers in Figure 5, We observe that as the merge layer becomes deeper, the performance on AV and AV matching tasks decreases, while performance on the VA task improves. This implies that overly low visual dependence can hinder the interpretation of visual information. From these results, we choose to merge at the 5th layer for VideoLLaMA2, which consistently improves performance across tasks while maintaining compatibility with the models original behavior. Similarly, for the video-SALMONN model, we merge at the 8th layer, which is approximately one-fifth into the full depth of the model. Ablation on audio-visual fusion strategy. To demonstrate the effectiveness of our attention-guided fusion, we compare results with three different audio-visual fusion strategies in Table 3. We evaluate on the AVHBench dataset using the VideoLLaMA2 model. Table 3: Ablation on fusion strategy. We ablate the fusion strategy in Eq. (4) with three methods: Exclusion, Average, and Attention-guided. Attention-guided fusion balances masked and unmasked inputs, yielding better performance. Methods In Eq. (4), the aggregation of masked and unmasked features is controlled by the fusion weight α. Exclusion corresponds to setting α = 1, where the masked modality is entirely excluded. This leads to performance drop compared to vanilla decoding. We attribute this to the causal nature of autoregressive models: fully ignoring one modality can disrupt the flow of information from previous tokens to the next prediction. Average fusion, where α = 0.5, also results in degraded performancelikely due to equal weighting between informative signals and noisy features. In contrast, when α is automatically determined via Eq. (5) and Eq. (6) by analyzing attention weights from the final decoder layer, it yields consistent performance gains across all tasks. This highlights the effectiveness of our attention-guided fusion strategy. Exclusion Average Attention-guided AV VA AV Matching 77.79 55.36 77.70 57.10 56.26 58.89 80.02 74.36 80.34 AVHBench [37] Vanilla 80. 77.03 57.75 8 Decoding speed comparison. To assess the efficiency of our FMD method, we compare its decoding speedmeasured in tokens generated per secondwith other representative test-time decoding methods: VCD and DoLa. For this evaluation, we use VideoLLaMA2 and sample 100 examples from the AVHBench dataset. Table 4: Decoding speed comparison. FMD improves efficiency by using only the initial Lfork layers in the fork phase. Decoding Inference Speed (tokens/s) As shown in Table 4, VCD exhibits the slowest decoding speed due to the requirement of an additional full model forward pass. DoLa is relatively faster, as it performs one full forward pass along with another partial forward pass up to an intermediate layer. FMD, on the other hand, requires two forward passes only up to the fork layers, after which the outputs are merged and passed through the remaining transformer layers. This makes FMD slightly faster than DoLa, while still achieving higher performanceas demonstrated in Table 2. These results suggest that FMD is designed with inference-time efficiency, and is also more effective than prior decoding methods originally developed for LLMs or VLMs. VCD [20] DoLa [8] FMD (Ours) 0.69 0.54 0.52 Vanilla 0."
        },
        {
            "title": "5 Related Works",
            "content": "Audio-visual large language models. Building upon the success of LLMs, there has been surge of interest in extending their capabilities to incorporate audio and visual modalities, with text serving as the central modality. ChatBridge [53] presents text-centric modality bridging framework trained on limited paired data, whereas models such as PandaGPT [34], ImageBindLLM [12], and OneLLM [11] leverage unified encoders to accommodate various modalities. Other approaches [5, 27, 28, 32, 49, 50] employ modality-specific encoders to better capture distinct feature spaces. To enhance spatial-temporal modeling across modalities, CAT [47] introduces clue aggregator for cross-modal reasoning, VideoLLaMA2 [6] utilizes spatial-temporal convolutional connector for video synchronization, and video-SALMONN [35] proposes multi-resolution causal Q-Former for audio-visual fusion. Meerkat [7] further refines multimodal interactions by aligning audio and visual signals at multiple levels through dedicated interaction modules prior to decoding. Inference-time reasoning enhancement with LLMs. Recent efforts have explored inference-time strategies to enhance the reasoning capabilities of LLMs without additional training. Chain-ofThought (CoT) guides LLMs to produce intermediate reasoning steps, and has been extended to VLMs through structured textual representations [13, 30, 54] or modular reasoning pipelines [36, 44, 46]. These approaches improve interpretability and robustness through more explicit reasoning process. In parallel, Contrastive Decoding (CD) improves inference-time decoding by comparing token-level logits between weaker and stronger model [23]. DoLA [8] develops this idea by contrasting early and late layer outputs within single model to refine predictions. Recently, VCD [20] extends CD to VLMs by injecting Gaussian noise into image inputs and contrasting the resulting biased predictions with the original outputs. Other CD-based approaches [18, 41] enhance decoding robustness by utilizing self-descriptions or distorting instructions. SID [16] further advances this line of work by preserving the least informative visual tokens and contrasting their influence on predictions. However, most of these inference-time reasoning methods have been developed for VLMs, while AV-LLMs remain relatively underexplored. This gap underscores the need for inference strategies specifically designed to address the unique challenges of audio-visual inputs."
        },
        {
            "title": "6 Conclusion",
            "content": "We analyze modality bias in current AV-LLMs, where jointly processing audio and visual inputs can hinder balanced reasoning. To address this, we propose Fork-Merge Decoding (FMD)a simple, training-free, efficient, and model-agnostic inference strategy that separates modality specific understanding in the early decoder layers (the fork phase) and merges their representations in later layers (the merge phase). FMD consistently improves performance on tasks requiring integrated multimodal understanding, as demonstrated across three audio-visual benchmarks using two representative AVLLMs: VideoLLaMA2 and video-SALMONN. Our approach is broadly applicable to AV-LLMs, offering plug-and-play solution that enables deeper unimodal and multimodal understanding during inference. We hope this work encourages further research into interpretable reasoning strategies tailored to the unique challenges of MLLMs operating over complex, multi-sensory modalities."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv, 2023. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Proc. NeurIPS, 2022. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proc. NeurIPS, 2020. [4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv, 2023. [5] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. In Proc. NeurIPS, 2023. [6] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv, 2024. [7] Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, and Dinesh Manocha. Meerkat: Audio-visual large language model for grounding in space and time. In Proc. ECCV, 2024. [8] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In Proc. ICLR, 2024. [9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Instructblip: Towards general-purpose Wang, Boyang Li, Pascale Fung, and Steven Hoi. vision-language models with instruction tuning. In Proc. NeurIPS, 2023. [10] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proc. CVPR, 2024. [11] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proc. CVPR, 2024. [12] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv, 2023. [13] Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, and William Yang Wang. Lets think frame by frame with vip: video infilling and prediction dataset for evaluating video chain-of-thought. arXiv, 2023. [14] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech, music, sound, and talking head. In Proc. AAAI, 2024. [15] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. In Proc. NeurIPS, 2023. [16] Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, and Peilin Zhao. Self-introspective decoding: Alleviating hallucinations for large vision-language models. In Proc. ICLR, 2025. 10 [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv, 2024. [18] Junho Kim, Hyunjun Kim, Yeonju Kim, and Yong Man Ro. Code: Contrasting self-generated description to combat hallucination in large multi-modal models. In Proc. NeurIPS, 2024. [19] Sicong Leng, Yun Xing, Zesen Cheng, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, and Lidong Bing. The curse of multi-modalities: Evaluating hallucinations of large multimodal models across language, visual, and audio. arXiv, 2024. [20] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proc. CVPR, 2024. [21] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. In Proc. CVPR, 2022. [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proc. ICML, 2023. [23] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proc. ACL, 2023. [24] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proc. EMNLP, 2024. [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proc. NeurIPS, 2023. [26] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology, page 100017, 2023. [27] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proc. CVPR, 2024. [28] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv, 2023. [29] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proc. ACL, 2024. [30] Minheng Ni, Yutao Fan, Lei Zhang, and Wangmeng Zuo. Visual-o1: Understanding ambiguous instructions via multi-modal multi-turn chain-of-thoughts reasoning. arXiv preprint arXiv:2410.03321, 2024. [31] Taichi Nishimura, Shota Nakada, and Masayoshi Kondo. On the audio hallucinations in large audio-video language models. arXiv preprint, 2024. [32] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. X-instructblip: framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning. arXiv, 2023. [33] Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: large language model that can speak and listen. arXiv preprint, 2023. [34] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv, 2023. [35] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. In Proc. ICML, 2024. [36] Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, and Wentao Zhang. Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification. arXiv, 2025. [37] Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, and Tae-Hyun Oh. Avhbench: cross-modal hallucination benchmark for audio-visual large language models. In Proc. ICLR, 2025. [38] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. In Proc. ICLR, 2024. [39] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv, 2024. [40] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv, 2022. [41] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding. In Proc. ACL, 2024. [42] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In Proc. ICLR, 2022. [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Proc. NeurIPS, 2022. [44] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv, 2024. [45] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: dataset for audio-visual question answering on videos. In Proc. ACM MM, 2022. [46] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv, 2023. [47] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios. In Proc. ECCV, 2024. [48] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proc. CVPR, 2024. [49] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv, 2024. [50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In Proc. EMNLP, 2023. [51] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. In Proc. ICLR, 2024. 12 [52] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv, 2023. [53] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chatbridge: Bridging modalities with large language model as language catalyst. arXiv, 2023. [54] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv, 2023. [55] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv, 2023. 13 Supplementary Material: Fork-Merge Decoding This supplementary material extends the main paper by including the following sections. To facilitate reproducibility, we provide the source code accompanied by README file. Architectural Analysis of Recent AV-LLMs Further Analysis B.1 Analysis of attention weights beyond the merge point . . . . . . . . . . . . . . . . B.2 Layer-wise performances on various dataset sizes . . . . . . . . . . . . . . . . . . B.3 Comparison between gaussian noise addition and zero-out masking . . . . . . . . More Qualitative Results C.1 Positive cases . C.2 Negative cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Computational Resource Limitations and Future Works Social Impact 15 16 16 16 17 17 17 18 18 Table A.1: Comparison of representative AV-LLMs in terms of open-source availability, checkpoint release, decoder backbone, and fusion strategy. Notably, several models provide full access to code and checkpoints, while others (e.g., GPT-4o and Gemini 1.5 Pro) remain closed. Fusion types are categorized as either token-wise or channel-wise. Model Open-source Checkpoints Decoder AV fusion GPT-4o [17] Gemini 1.5 Pro [39] Meerkat [7] video-SALMONN [35] VideoLLaMA-2 [6] CAT [47] AnyGPT [49] Unified-IO 2 [27] OneLLM [11] X-instructBLIP [32] ImageBind-LLM [12] Macaw-LLM [28] VideoLLaMA [50] VAST [5] PandaGPT [34] ChatBridge [53] (Unknown) (Unknown) (Unknown) (Unknown) LLaMA2-7B-Chat Token-wise Vicuna-7B-v1.5 Vicuna-13B-v1.5 LLaMA2 Channel-wise Qwne2-7B-Instruct Token-wise LLaMA2-7B Token-wise LLaMA2-7B Token-wise UIO-2-1.1B UIO-2-3.2B UIO-2-6.8B Token-wise LLaMA2-7B Token-wise Vicuna-7B-v1.1 Vicuna-13B-v1.1 Token-wise LLaMA-7B Channel-wise LLaMA-7B Token-wise Vicuna-7B Vicuna-13B Token-wise Vicuna-13B Token-wise Vicuna-13B-v Channel-wise Vicuna-13B Token-wise Architectural Analysis of Recent AV-LLMs We investigate recently proposed AV-LLMs by categorizing them based on open-source availability, the presence of pretrained checkpoints, decoder architecture, and the method used for audio-visual integration (AV fusion). As shown in Table A.1, all models except for the commercial AV-LLMs, GPT-4o [17] and Gemini 1.5 Pro [39], are open-sourced. While variety of decoder architectures are employed across models, most approaches rely on either token-wise or channel-wise concatenation to fuse audio and visual tokens as input. This suggests that the proposed FMD method has the potential to be applied in generalizable manner. (a) Performance on 500 samples from the AVHBench dataset. (b) Performance on 1000 samples from the AVHBench dataset. Figure A.2: Layer-wise accuracies of VideoLLaMA2 [6] on each task in the AVHBench [37] dataset. The consistent trends observed across 500 and 1000 samples further validate the 200-sample analysis presented in Figure 5."
        },
        {
            "title": "B Further Analysis",
            "content": "B.1 Analysis of attention weights beyond the merge point in the final Through the main paper Figure 1, we demonstrate that layer of VideoLLaMA2 [6], the attention weights are higher for the video input compared to the audio input. To analyze the weight distribution trend across the entire modelnot just the final layerwe visualize the average attention weights after the merge point at Lfork in Figure A.1. Specifically, we examine the last token in the sequence and compute the aggregated attention over video and audio segments, averaged across all attention heads, to estimate each modalitys contribution to the prediction. The proposed FMD consistently promotes more balanced attention distribution between modalities. Figure A.1: Attention weight analysis in VideoLLaMA2 [6] on the AVHBench dataset [37]. We analyze 100 samples and examine the attention weights in decoder layers after Lfork, focusing on the final token. FMD narrows the gap between audio and video attention weights, encouraging more balanced contributions from both modalities. B.2 Layer-wise performances on various dataset sizes In the main paper (see Section 4.4 and Figure 5), we describe our method for selecting Lfork. Assuming deployment scenario with limited labeled data, we use 200 samples per taskAV, VA, and AV matchingto determine the optimal Lfork. To examine whether this trend holds with more data, we repeat the analysis using 500 and 1000 samples per task (see Figure A.2a and Figure A.2b). We observe consistent trends across different sample sizes, demonstrating that Lfork can be reliably determined even with small number of samples. B.3 Comparison between gaussian noise addition and zero-out masking We analyze two input perturbation strategies that aim to suppress modality-specific information by replacing the original input: Gaussian noise injection, as proposed in VCD [20], and zero-out 16 Figure A.3: Cosine similarity comparison on VideoLLaMA2 [6] using 100 samples from the AVHBench [37] dataset. We compute the cosine similarity between the final-layer hidden states of the intact input and those of audioor video-perturbed inputs. The results indicate that video signals are not effectively isolated by additive Gaussian noise, whereas zero-masking reliably suppresses both modalities. masking, as employed in our method. Specifically, we apply each perturbation method to the video and audio inputs, respectively, and compute the cosine similarity between the final-layer hidden states of the perturbed inputs and those of the original inputs. As shown in Figure A.3, injecting Gaussian noise into the video input yields hidden states that remain highly similar to those of the original input, indicating that this approach fails to effectively isolate the visual signal. In contrast, when applied to audio inputs, Gaussian noise successfully disrupts the signal, leading to substantial differences in the hidden representations. This observation supports the performance improvement observed in the VA direction under the FMD with noise setting, as reported in Table 2. In comparison, zero-out masking completely suppresses both video and audio signals, resulting in hidden states that are clearly separated from those of the original input. This demonstrates that zero-out masking more effectively blocks modality-specific information from being encoded and consistently outperforms vanilla decoding across all tasks, as shown in Table 2."
        },
        {
            "title": "C More Qualitative Results",
            "content": "In addition to the qualitative analysis presented in Section 4.3, we include further examples that illustrate both successful outcomes and failure cases, providing more comprehensive understanding of the models behavior. C.1 Positive cases In addition to Figure 3, we further analyze various cases, including audio-driven video hallucinations, video-driven audio hallucinations, and audio-visual matching, as illustrated in Figure A.4, Figure A.5, and Figure A.6. We also examine complex audio-visual description scenarios in Figure A.7. In these cases, our proposed FMD effectively leverages both audio and visual modalities to accurately interpret the contexts. Notably, even when the audio and video are artificially constructed, FMD is able to detect inconsistencies and generate correct responses. Moreover, as shown in Figure A.7, FMD outperforms vanilla decoding by producing more precise and detailed descriptions of both modalities. C.2 Negative cases Although our FMD significantly enhances multimodal understanding without requiring additional training, it occasionally produces inaccurate phrases alongside otherwise detailed and accurate descriptions. For example, in the top case of Figure A.8, FMD successfully captures both visual and audio content, whereas vanilla decoding describes only the visual scene. However, the phrase talking to them is contextually inappropriate, as them refers to the ducks, while the audio indicates the man is actually speaking toward the cameraman. In the bottom example, the mention of camera flashes is inaccurate, since only stage lighting is present. While such instances reflect occasional hallucinations, FMD generally produces rich and informative responses by effectively modeling cross-modal relationships. Moreover, at broader scale, it tends to reduce hallucinations across datasets, thereby improving overall performance, as demonstrated in Table 1. Further investigation into minimizing fine-grained hallucinations remains an important direction for future work."
        },
        {
            "title": "D Computational Resource",
            "content": "All experiments are conducted on system equipped with an AMD EPYC 7513 32-Core CPU and single NVIDIA RTX A6000 GPU. To ensure fair measurement of inference speed across all experiments, we terminate all non-experimental processes during inference time."
        },
        {
            "title": "E Limitations and Future Works",
            "content": "Our proposed FMD is simple yet effective inference-time method that requires no additional training. However, its performance inherently depends on the quality and capacity of the underlying model. As future direction, we aim to develop models that explicitly incorporate unimodal interactions during training. We believe that such an approach will enable the model to more effectively capture and integrate modality-specific information, thereby enhancing its performance on multimodal reasoning tasks."
        },
        {
            "title": "F Social Impact",
            "content": "The rapid advancement of LLMs has significantly influenced various sectors, including technology, culture, and education, by making information more accessible and enabling efficient communication. In parallel, MLLMs have progressed through the integration of visual modalities into LLM decoders. Recently, AV-LLMs have emerged, extending these capabilities to both visual and auditory content. By directly understanding and reasoning over audio-visual content, AV-LLMs offer practical benefits in everyday settings where information is naturally multimodal, such as videos, lectures, conversations, and real-world environments. This opens new possibilities for applications like multimodal search, video question answering, and assistive technologies that go beyond text-based interfaces. Much like how LLMs have made text-based knowledge more accessible, AV-LLMs can help users navigate and interact with rich multimedia content more effectively. Despite these advantages, the inference-time behavior of AV-LLMs remains underexplored. Our proposed Fork-Merge Decoding (FMD) provides training-free framework to analyze and isolate modality-specific reasoning, offering insights into how AV-LLMs process complex audio-visual information and guiding future improvements in model design and interpretability. 18 Figure A.4: Qualitative results for audio-driven video hallucination tasks using VideoLLaMA2 [6]. Compared to vanilla decoding, FMD generates more accurate responses by effectively leveraging both audio and visual modalities. Figure A.5: Qualitative results for video-driven audio hallucination tasks using VideoLLaMA2 [6]. FMD produces more accurate responses in most cases, including instances where visual signals lead to confusion, as shown in the third example. 19 Figure A.6: Qualitative results for audio-visual matching tasks using VideoLLaMA2 [6]. FMD generates correct responses even when the synthesized audio and video inputs are semantically unrelated. Figure A.7: Qualitative results for audio-visual description tasks using VideoLLaMA2 [6]. FMD effectively describes both audio and visual content, capturing fine-grained detailssuch as person touches the edge of wooden clock in the top example. 20 Figure A.8: Failure case analysis for audio-visual description tasks using VideoLLaMA2 [6]. While FMD occasionally generates inaccurate details, it still captures both audio and visual content more effectivelyand with finer granularitycompared to vanilla decoding."
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science and Technology (KAIST)"
    ]
}