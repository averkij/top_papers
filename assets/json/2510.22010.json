{
    "paper_title": "FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing",
    "authors": [
        "Or Ronai",
        "Vladimir Kulikov",
        "Tomer Michaeli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at test time for controlled generation tasks. Examples range from image editing to restoration, compression and personalization. However, due to the iterative nature of the sampling process in those models, it is computationally impractical to use gradient-based optimization to directly control the image generated at the end of the process. As a result, existing methods typically resort to manipulating each timestep separately. Here we introduce FlowOpt - a zero-order (gradient-free) optimization framework that treats the entire flow process as a black box, enabling optimization through the whole sampling path without backpropagation through the model. Our method is both highly efficient and allows users to monitor the intermediate optimization results and perform early stopping if desired. We prove a sufficient condition on FlowOpt's step-size, under which convergence to the global optimum is guaranteed. We further show how to empirically estimate this upper bound so as to choose an appropriate step-size. We demonstrate how FlowOpt can be used for image editing, showcasing two options: (i) inversion (determining the initial noise that generates a given image), and (ii) directly steering the edited image to be similar to the source image while conforming to a target text prompt. In both cases, FlowOpt achieves state-of-the-art results while using roughly the same number of neural function evaluations (NFEs) as existing methods. Code and examples are available on the project's webpage."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 1 0 2 2 . 0 1 5 2 : r FLOWOPT: FAST OPTIMIZATION THROUGH WHOLE FLOW PROCESSES FOR TRAINING-FREE EDITING Or Ronai, Vladimir Kulikov, Tomer Michaeli Technion - Israel Institute of Technology {or.ronai@campus, vladimir.k@campus, tomer.m@ee}.technion.ac.il"
        },
        {
            "title": "ABSTRACT",
            "content": "The remarkable success of diffusion and flow-matching models has ignited surge of works on adapting them at test time for controlled generation tasks. Examples range from image editing to restoration, compression and personalization. However, due to the iterative nature of the sampling process in those models, it is computationally impractical to use gradient-based optimization to directly control the image generated at the end of the process. As result, existing methods typically resort to manipulating each timestep separately. Here we introduce FlowOpt zero-order (gradient-free) optimization framework that treats the entire flow process as black box, enabling optimization through the whole sampling path without backpropagation through the model. Our method is both highly efficient and allows users to monitor the intermediate optimization results and perform early stopping if desired. We prove sufficient condition on FlowOpts step-size, under which convergence to the global optimum is guaranteed. We further show how to empirically estimate this upper bound so as to choose an appropriate step-size. We demonstrate how FlowOpt can be used for image editing, showcasing two options: (i) inversion (determining the initial noise that generates given image), and (ii) directly steering the edited image to be similar to the source image while conforming to target text prompt. In both cases, FlowOpt achieves state-of-the-art results while using roughly the same number of neural function evaluations (NFEs) as existing methods. Code and examples are available on the projects webpage."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion and flow matching models have emerged as powerful generative frameworks, achieving state-of-the-art (SotA) results on image, video, and audio generation (Ho et al., 2020; Song et al., 2021a; Rombach et al., 2022; Lipman et al., 2023; Liu et al., 2023; Albergo & Vanden-Eijnden, 2023). However, as opposed to their generative adversarial network (GAN) predecessors, flow models generate samples through an iterative process that often involves dozens of neural function evaluations (NFEs). This makes it challenging to adapt them at inference time for solving controlled generation tasks. Indeed, while GANs naturally lend themselves to gradient-based optimization for directly minimizing losses on the generators output (Menon et al., 2020), in flow models this approach is computationally impractical. As result, methods that use pre-trained flow models for controlled generation typically intervene in each step of the sampling process separately, without employing any direct supervision on the final result. This strategy is used e.g., for image restoration, image editing (using inversion techniques), and image compression (Kawar et al., 2022; Tumanyan et al., 2023; Pan et al., 2023; Qi et al., 2023; Huberman-Spiegelglas et al., 2024; Hong et al., 2024; Cohen et al., 2024; Garibi et al., 2024; Manor & Michaeli, 2024; Elata et al., 2024; Wang et al., 2025; Martin et al., 2025; Deng et al., 2025; Ohayon et al., 2025; Samuel et al., 2025). Recently, Ben-Hamu et al. (2024) demonstrated the great potential of employing optimization through the whole flow process in the context of solving inverse problems with pre-trained flow models. Unlike other methods, this approach directly controls the generated image, and thus avoids accumulation of approximation errors that can build up throughout the flow path. However, performing gradient-based optimization is not scalable to reasonably sized models and image dimensions. In fact, even with small flow-matching model, small images (128 128), and memory-saving techniques like gradient checkpointing, this approach takes approximately 15 minutes to run on single input. 1 Figure 1: FlowOpt. We propose zero-order (gradient-free) framework for optimization through an unrolled flow sampling process. FlowOpt can efficiently optimize losses on the target image, even when working with large models and high resolution images. We leverage our framework for text-based image editing, demonstrating state-of-the-art results on both FLUX (first and third rows) and Stable Diffusion 3 (second row). Fine details are visible upon zooming in. In this work, we introduce FlowOpt zero-order (gradient-free) optimization framework for directly minimizing loss functions on the target image without backpropagating through the model. Specifically, unrolling the sampling process, flow model can be viewed as chain of neural networks, which we refer to as denoisers. Our approach treats this entire chain of denoisers as black box, and enables optimization with respect to arbitrary loss functions. Here we specifically focus on image-editing objectives. The avoidance of backpropagation enables working with large flow models and treating large images. Furthermore, it allows using small number of flow timesteps, which is in contrast with inversion-based techniques that often require many timesteps to avoid error accumulation. Taken together, these features enable FlowOpt to achieve SotA results at number of NFEs comparable to existing methods. Additionally, FlowOpt allows monitoring the intermediate optimization results. Thus, at the same budget of NFEs as existing methods, FlowOpt in fact provides multiple candidate edited images (one per optimization step) from which the user can choose. Zero-order optimization has been previously used in several computer vision contexts (Tao et al., 2017; Milanfar, 2018; Chen et al., 2019; Tu et al., 2019). FlowOpt is generalization of the method of Tao et al. (2017), with the difference that the update in each optimization step is multiplied by step-size η (the method of Tao et al. (2017) corresponds to FlowOpt with η = 1). As we show, this modification is of dramatic importance. Specifically, we prove sufficient condition on η under which convergence to the global minimum is guaranteed, and show that for popular flow models this bound is orders of magnitude smaller than 1. We demonstrate that FlowOpt indeed converges when η is chosen smaller than the bound, and fails to converge when it significantly exceeds the bound. We demonstrate the effectiveness of FlowOpt for both image reconstruction (inversion) and direct image editing  (Fig. 1)  , using the FLUX-1.dev (Black Forest Labs, 2024) and Stable Diffusion 3 (SD3) (Esser et al., 2024) text-to-image (T2I) models. We show that FlowOpt provides an efficient solution to these tasks, delivering SotA performance at running times comparable to existing methods."
        },
        {
            "title": "2 RELATED WORK",
            "content": "T2I diffusion and flow-based models (Saharia et al., 2022; Ramesh et al., 2022) generate images by steering diffusion or flow process according to text prompt provided by the user. Latent 2 diffusion/flow variants (Rombach et al., 2022; Vahdat et al., 2021; Dao et al., 2023) follow the same principle but operate in lower-dimensional latent space, improving computational efficiency while preserving visual fidelity. Many methods utilize these T2I foundation models for downstream tasks like image editing in zero-shot manner. common approach for performing image editing with pre-trained diffusion/flow models is to start with an inversion stage (Song et al., 2021a) (often referred to as DDIM or ODE inversion), whose goal is to extract the initial noise that would generate the input image if used in regular sampling process. Once this initial noise is obtained, it is used for sampling new image, but using text prompt that describes the desired edit. However, inversion methods introduce approximation errors that accumulate across the flow timesteps, and lead to significant reconstruction inaccuracies (Mokady et al., 2023; Huberman-Spiegelglas et al., 2024). One line of work focuses on improving the precision of ODE-inversion. Wang et al. (2025) employ high-order Taylor expansion to more accurately approximate the nonlinear components of the flow. Deng et al. (2025) propose solver that reuses intermediate velocity vector approximations. Yet, despite improving numerical accuracy, such methods still operate on each timestep separately and do not promote direct alignment with the given image during the inversion. Therefore, they still suffer from accumulation of errors that can degrade overall performance. different approach is to optimize each denoising timestep independently (Mokady et al., 2023; Pan et al., 2023; Hong et al., 2024; Garibi et al., 2024; Miyake et al., 2025; Samuel et al., 2025). For instance, Mokady et al. (2023) optimize the unconditional null prompt embedding used in classifier-free guidance (CFG) (Ho & Salimans, 2021) during the reverse process, aligning latent variables obtained through DDIM inversion. While effective, this approach requires storing all latent variables and optimized embeddings in memory, which becomes prohibitive for large number of timesteps. Furthermore, repeated backward passes through each timestep render such methods impractical for interactive editing with large-scale models. Hong et al. (2024) propose gradient-based inversion scheme applied independently at each timestep, however their method is computationally expensive and time-intensive, particularly for modern large-scale T2I models. Pan et al. (2023) and Garibi et al. (2024) mitigate this by introducing fixed-point iteration strategies that iteratively refine approximations of predicted states along the diffusion trajectory. However, all these methods rely on optimizing each timestep independently, ignoring the input image in each optimization step. This leads to accumulation of local approximation errors that degrade overall performance. There exist several optimization-based methods that may superficially seem similar to FlowOpt, as they neglect the Jacobian of the denoiser and thus avoid backpropagation through the model. These include Score Distillation Sampling (SDS) (Poole et al., 2023), Delta Denoising Score (DDS) (Hertz et al., 2023), Posterior Distillation Sampling (PDS) (Koo et al., 2024), and inverse Rectified Flow Distillation Sampling (iRFDS) (Yang et al., 2025). However, these methods still optimize each timestep separately by randomly sampling timestep in each optimization step and performing an update based on that timestep alone. This is in contrast with FlowOpt, which performs optimization through the whole chain of denoisers simultaneously. Finally, Ben-Hamu et al. (2024) proposed D-Flow, method that like FlowOpt, optimizes across the entire generative process. However, their framework relies on gradient-based optimization and requires repeated backpropagation through the entire chain of denoisers. This makes the method computationally intensive and impractical for high-resolution, real-world applications precisely the setting we aim to address with FlowOpt."
        },
        {
            "title": "3 PRELIMINARIES AND NOTATION",
            "content": "Probability flow ODE (Song et al., 2021b) and flow-matching models (Lipman et al., 2023; Liu et al., 2023; Albergo & Vanden-Eijnden, 2023) generate images by numerically solving an ODE over time parameter t. Focusing for simplicity on the flow-matching formalism, the ODE takes the form dzt = vt(zt, c) dt, (1) This ODE is designed such that when initialized at = 1 with sample from some source distribution π1, and run backwards in time until = 0, it yields (usually taken to be an isotropic Gaussian), z1 sample from desired target distribution (e.g. the distribution of natural images), z0 π0. The ) is time dependent vector field that optionally accepts condition (e.g., text function vt( [0, 1]. , 3 Figure 2: whole flow process as black box. We encapsulate the flow process as black box function , which receives an initial noise z1 and text conditioning c, and outputs clean sample z0. Each internal step within the black box is given by ψt(zt, c) = zt + vt(zt, c)t, where vt is the text-conditioned velocity predicting network. prompt) in its second argument. In practice, this velocity field is implemented by neural network, which we refer to as denoiser, and the ODE is discretized and solved numerically as zt+t = zt + vt(zt, c) t, (2) where is the (negative) discretization step. Unrolling Eq. (2), the sample z0 generated at the end of the flow process can be written as function of the initial noise z1, namely z0 = (z1, c). This function is given by (z1, c) = z1 + (cid:88) vti(zti, c) t, (3) where ti = 1 + (see Fig. 2). For notational simplicity, we henceforth omit the condition ) to denote the mapping whenever it is clear from the context. Furthermore, we sometimes use ( from some intermediate timestep < 1 to timestep = 0. Our method treats the function ( ) as black box in the sense that it can be evaluated but its Jacobian cannot be computed. Commonly, the flow process is defined in the latent space of an encoder is obtained by passing the generated sample z0 through the corresponding decoder ), so that the final image ). ( ("
        },
        {
            "title": "4 METHOD",
            "content": "Given source image y, text prompt csrc describing it, and target text prompt ctar describing desired edit, our goal is to generate an edited image yedit that conforms to ctar while being as similar as possible to y. Like previous approaches, we rely on pre-trained flow model. However, in contrast to existing methods we propose to achieve this by directly optimizing over the vector zt at some timestep (usually taken to be 1), such that the image z0 at the end of the flow process is close to y. Formalizing this mathematically, we are interested in is some dissimilarity measure. Let us focus on the L2 loss (see App. for other losses). In this case, = arg minzt (f (zt, c), y), where = arg min zt 1 2 (zt, c) 2. (4) This optimization problem can be used in two distinct ways. (i) Inversion: setting = csrc in Eq. (4) leads to that reconstructs the input image with the source prompt. (ii) Direct editing: setting = ctar in Eq. (4) leads to that directly approximates the input image with the target prompt. In both cases, once is obtained, it can be used to generate the edited image by performing sampling with the target prompt, yedit = (z , ctar). Using gradient descent to solve Eq. (4) would lead to the iterations z(i+1) where η is the step size and (z(i) . However, as mentioned above, backpropagation through whole flow processes is computationally impractical. ) is the Jacobian of ( ) with respect to z(i) z(i) η J(z(i) (z(i) ) (5) , ) (cid:16) (cid:17) 4 Figure 3: Image inversion with FlowOpt. Intermediate samples z(i) , c) attained during our zero-order optimization through chain of 10 denoising steps (FLUX) for the task of reconstruction (inversion), i.e., with = csrc. Notice the missing details in the early steps, such as the bicycle and the horizon. As the iterations progress, the reconstruction converges to the ground truth image. 0 = (z(i) Figure 4: Direct image editing with FlowOpt. Intermediate samples z(i) , c) attained during our zero-order optimization through chain of 15 denoising steps (FLUX) for direct image editing, i.e., with = ctar. Notice the misalignment in the dogs body structure in the first iterations. 0 = (z(i) Therefore, as an alternative, here we propose to simply ignore the Jacobian. This leads to the zero-order (gradient-free) iterations z(i+1) z(i) η (cid:16) (z(i) ) (cid:17) . (6) Figure 3 demonstrates the progression of those iterates when used for inversion (with the source prompt). Figure 4 demonstrates the progression of the iterates when used for direct editing (with the target prompt). Algorithm 1 summarizes the proposed method. Before providing theoretical convergence guarantee, two comments are in place. First, when η = 1, Eq. (6) degenerates to the method of Tao et al. (2017). However, as we show below, η is of crucial importance, as the maximal step size allowing convergence is much smaller than 1 for modern flow-matching models. Second, it is insightful to note that for flow-matching models, Eq. (6) is equivalent to using gradient descent with step-size η while applying the stop-grad operator on the output of the velocity prediction network. Similarly, for probability flow ODE models (Song et al., 2021b), (a.k.a. DDIM (Song et al., 2021a)), Eq. (6) is equivalent to using gradient descent with step size αT η while applying stop-grad on the noise prediction network (following the notation of Song et al. (2021a)). The derivations of those observations are provided in App. G. The iterations of Eq. (6) can be written as z(i+1) ), where g(u) y). By ) is contractive mapping1 then there exists unique point the Banach fixed-point theorem, if g( satisfying ), and thus (z ) = y. Furthermore, in this case the iterations converge to this unique solution. This fact can be used to obtain sufficient condition on the step size η under which the iterations are guaranteed to converge to the global minimum (see proof in App. F). = g(z = g(z(i) η(f (u) Theorem 1. Assume that inf u1=u2 u1u2,f (u1)f (u2) u1u2f (u1)f (u2) > 0. If the step size η satisfies 0 < η < 2 inf u1,u2 u1 u2, (u1) (u1) (u2) (u2) 2 (7) then there is unique t satisfying (z ) = and the iterations of Eq. (6) converge to this . 1g() is contractive mapping if it satisfies g(u1) g(u2) γu1 u2 for some γ < 1 and all u1, u2. Algorithm 1: Flow Zero-Order Optimization (FlowOpt) Require: step size η, number of iterations , condition c, input image Initialization: z(0) for Rd 1 do , c) 0, . . . , 0 = (z(i) z(i) z(i+1) z(i) 0 = (z(N ) z(N ) , c) z(i) Return 0 } i=0 { η(z(i) 0 y) Table 1: Step sizes guaranteeing convergence. Column 2 shows the estimated sufficient condition of Eq. (7) and column 3 reports the step size we chose for each model (see App. for details)."
        },
        {
            "title": "Model",
            "content": "Sufficient condition (Eq. (7)) Our chosen step size FLUX SD3 η < 2.70 η < 1.67 103 102 η = 2.5 η = 1. 103 102 The bound in Eq. (7) depends only on the flow ). It can thus be computed once for model ( each model in order to choose the step size. In App. we approximate this upper bound for the FLUX and SD3 models by drawing many pairs of samples u1, u2. As we show, the right-hand is side of Eq. (7) is smallest when small. Table 1 shows the bounds estimated for the two models, and the step sizes we chose for our experiments. u2 u1 As can be seen, the bounds in Tab. 1 are significantly smaller than 1, suggesting that the method of Tao et al. (2017) is inapplicable in our setting. Indeed, Fig. 5 shows the reconstruction error along the iterations for several choices of η when used for inversion with SD3 (results for FLUX are presented in App. F). When setting η = 102, which is below the bound of 102, the iterations converge. However, 1.67 102 or when using larger step sizes, like 4 102, the iterations fail to converge. The 5 setting of this experiment is as in Sec. 5.1. Figure 5: Convergence analysis. The plot shows RMSE in pixel space vs. number of iterations for the task of inversion, averaged over dataset. The step size we use (red) satisfies the sufficient condition of Eq. (7) and thus leads to convergence. Step sizes that are 4 larger (yellow and black) do not satisfy the condition and do not lead to convergence. The dashed orange line is the minimal RMSE achievable in this setting. It corresponds to passing images through the encoder and decoder. and"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We compare FlowOpt against competing methods on two tasks: image reconstruction (inversion) and text-based image editing. We show results with FLUX-1.dev in the main text and with SD3 in App. D. We use the step sizes reported in Tab. 1 and initialize our algorithm with the UniInv (Jiao et al., 2025) inversion method (see App. for details). All images are of dimension 1024 1024. 5.1 IMAGE RECONSTRUCTION (INVERSION) For inversion, we use = csrc in Eq. (4), setting it to text prompt describing the source image. We set the number of flow steps in FLUX (number of denoisers) to = 10 and evaluate the reconstruction error for various numbers of NFEs by varying the number of FlowOpt iterations . Specifically, we have NFE = (N + 2), as NFEs are used for the initialization, NFEs for the optimization process, and NFEs for the final sampling process. 6 Figure 6: Reconstruction accuracy vs. NFEs for inversion. The plots depict pixel-space RMSE, LPIPS, SSIM, and PSNR as function of the number of NFEs for several inversion methods. The dashed bound corresponds to passing the images through the encoder and decoder. FlowOpt achieves favorable reconstruction quality under 240 NFEs, which is the regime of practical interest. We randomly choose 100 real images from the DIV2K dataset (Agustsson & Timofte, 2017), and resize and center-crop them to dimension 1024 1024. For the source prompts, we caption each image with BLIP (Li et al., 2022) and then manually refine the prompt. We compare FlowOpt to several inversion methods: naive ODE Inversion, RF-Solver (Wang et al., 2025), FireFlow (Deng et al., 2025), UniInv (Jiao et al., 2025), and ReNoise (Garibi et al., 2024). We use the official implementations of all methods except for ODE Inversion and ReNoise (that lacks an implementation for flow models), which we implemented by ourselves. To ensure fair comparison, we set the number of timesteps for each method such that the total NFE count is the same for all methods. Specifically, for FireFlow and UniInv, which use single forward pass per timestep, we set = NFE 2 . For RF-Solver, which uses two forward passes per timestep for inversion and two for sampling, we set = NFE 4 . For ReNoise, we used = 50 and set the number of ReNoise steps so as to achieve the desired NFE count. We note that we evaluated ReNoise with various hyperparameter settings and chose the one that achieved the best results. Figure 6 shows the reconstruction accuracy achieved by all methods as function of the NFEs. The figure reports pixel-space RMSE, PSNR, SSIM (Wang et al., 2004), and LPIPS (Zhang et al., 2018). As can be seen, FlowOpt achieves the best reconstruction results over wide range of NFE counts. In App. we show that the same trend is obtained with empty text prompts, both with the CFG parameter of FLUX set to 0 and with it set to 1 (these options differ as FLUX is distilled model). 5.2 IMAGE EDITING Accurate inversion does not necessarily lead to good editing results. Indeed, even for synthetic images, for which the initial noise map is known, plain editing-by-inversion leads to unsatisfactory results (Kulikov et al., 2024; Huberman-Spiegelglas et al., 2024) (see App. for further discussion). Accordingly, for the task of editing we employ our direct optimization approach, where the target text prompt = ctar is used in Eq. (4). In this case, we do not necessarily want large number of iterations, to avoid getting too close to the original image. We therefore use . } We set the number of flow steps to = 15 and perform the optimization on the latent vector at (corresponding to in Eq. (4)). The total number of NFEs is given by timestep nmax 14, 13, 12 2, 3, 4, 5 { { } 7 Figure 7: Editing quantitative comparisons. Semantic preservation of different editing methods evaluated using CLIP-Image, DINOv3 and DreamSim as functions of text adherence, measured by CLIP-Text. Connected markers represent different set of hyperparameters (see App. B). Our method achieves the most favorable balance between semantic preservation and text adherence. Figure 8: FlowOpt editing results. Our method successfully preserves the objects semantics and structure, as well as the background details, all the while loyally adhering to the target text prompt. Fine details are visible upon zooming in. NFE = nmax(N + 2). We use the default CFG of 3.5. All visual results in the paper were obtained with nmax = 13, except for Fig. 1, whose hyperparameters are provided in App. H. We evaluate all methods on the dataset of Kulikov et al. (2024), which we enriched with additional images and editing prompts. In total, our dataset consists of 90 real images of dimensions 1024 1024 from the DIV2K dataset and from royalty free online sources (Pexels, 2025; PxHere, 2025). Each image was captioned by LLaVA-1.5 (Liu et al., 2024) and manually refined. For each image, we manually created target editing prompts. Overall, this led to about 400 text-image pairs. We compare our method against all aforementioned methods, in addition to FlowEdit (Kulikov et al., 2024) and RF-Inversion (Rout et al., 2025). These two methods were excluded from the inversion experiments of Sec. 5.1 as they do not use inversion in the regular sense (FlowEdit is inversion-free and RF-Inversion explicitly incorporates the source image into the denoising process). For ODE Inversion, we apply the same number of NFEs as our method. For other methods, we use the hyperparameters reported in the papers or in the official implementations. We performed 8 Figure 9: Qualitative comparisons. FlowOpt is the only method to consistently adhere both to target text prompt, and to the original image. Fine details are visible upon zooming in. For instance, the back legs of the zebra in the first row, the posture of the bear in the second row, the statues limbs in the third row, and the structure of the scene in the last row. hyperparameter search for all methods that provided more than single set of hyperparameters. Additional details and the final hyperparameters chosen for each method are provided in App. B. Figures 1, 8 and S1 showcase the diverse editing capabilities of our method, including object replacement, style changes, and texture editing. FlowOpt achieves high quality, text adherent edits that also remain loyal to the source image semantics. Figure 9 presents qualitative comparisons between FlowOpt and other methods. As can be observed, our edits maintain superior alignment with the source images structure while simultaneously adhering to the target text. For example, when turning the horse into zebra (first row), FlowOpt successfully preserves the leg positions. Similarly, when replacing the sitting man (third row) with golden sculpture of Buddha, FlowOpt preserves the original limb orientations and scene background. For additional comparisons, see App. B. Figure 7 presents numerical evaluation of the results obtained for various hyperparameters. We use cosine similarity on CLIP image and text embeddings (Radford et al., 2021) to measure adherence to the original image and to the target text prompt, respectively. For image adherence, we also use cosine similarity between DINOv3 embeddings (Caron et al., 2021; Siméoni et al., 2025), as well as DreamSim (Fu et al., 2023). As can be seen, our method achieves the best tradeoff between text adherence and structure preservation."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "We presented zero-order (gradient-free) framework that allows efficient optimization over the initial noise in flow process while minimizing loss over the sample generated at the end of the process. We demonstrated the effectiveness of our approach for performing image editing using pre-trained flow models. In particular, extensive comparisons showed that our FlowOpt method achieves SotA performance on both image reconstruction and editing. We note that, similarly to other training-free editing methods, our approach still encounters difficulties in certain settings, like modifying large regions of the image (see App. J). However, taking broader perspective, we believe that our zeroorder framework opens the door for exploiting pre-trained flow-models in diverse applications (e.g., restoration, compression, and personalization) and for diverse modalities (e.g., image, video, and audio). We leave those extensions for future work."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work builds upon pre-trained generative models, and thus inherits the broader ethical considerations associated with their use. Such models may reflect or amplify societal biases present in the training data, and their outputs could be misinterpreted or misused in sensitive applications. In addition, our approach involves large-scale flow matching models, which carry the potential risk of being repurposed for harmful or malicious purposes. We emphasize that our contributions are intended solely for advancing research in generative modeling."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research was supported by the Israel Science Foundation (grant no. 2318/22) and by the Ollendorff Minerva Center, ECE faculty, Technion. The authors thank Matan Kleiner for his insightful suggestions throughout this work."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 28 Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 126135, 2017. 7 Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. 1, 3 Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: differentiating through flows for controlled generation. In Proceedings of the 41st International Conference on Machine Learning, pp. 34623483, 2024. 1, 3 Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 2 Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David Cox. Zoadamm: Zeroth-order adaptive momentum method for black-box optimization. Advances in neural information processing systems, 32, 2019. 2 Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. Slicedit: zero-shot video editing with text-to-image diffusion models using spatiotemporal slices. In Proceedings of the 41st International Conference on Machine Learning, pp. 91099137, 2024. 1 Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv preprint arXiv:2307.08698, 2023. 3 Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. Fireflow: Fast inversion of rectified flow for image semantic editing. In Forty-second International Conference on Machine Learning, 2025. 1, 3, 7 Noam Elata, Tomer Michaeli, and Michael Elad. Zero-shot image compression with diffusion-based posterior sampling, 2024. 1 Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 10 Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: learning new dimensions of human visual similarity using synthetic data. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 5074250768, 2023. 9 Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising. In European Conference on Computer Vision, pp. 395413. Springer, 2024. 1, 3, 7 Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23282337, 2023. 3 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 3 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 Seongmin Hong, Kyeonghyun Lee, Suh Yoon Jeon, Hyewon Bae, and Se Young Chun. On exact inversion of dpm-solvers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 70697078, 2024. 1, Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1246912478, 2024. 1, 3, 7 Guanlong Jiao, Biqing Huang, Kuan-Chieh Wang, and Renjie Liao. Uniedit-flow: Unleashing inversion and editing in the era of flow models. arXiv preprint arXiv:2504.13109, 2025. 6, 7 Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. In European Conference on Computer Vision, pp. 428447. Springer, 2024. 24 Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in neural information processing systems, 35:2359323606, 2022. 1 Juil Koo, Chanho Park, and Minhyuk Sung. Posterior distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1335213361, 2024. Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models. arXiv preprint arXiv:2412.08629, 2024. 7, 8 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. 7 Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 1, 3 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024. 8 Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 1, 3 Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using ddpm inversion. In Proceedings of the 41st International Conference on Machine Learning, pp. 34603 34629, 2024. 11 Ségolène Tiffany Martin, Anne Gagneux, Paul Hagemann, and Gabriele Steidl. Pnp-flow: Plugand-play image restoration with flow matching. In The Thirteenth International Conference on Learning Representations, 2025. 1 Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The contextual loss for image transformation with non-aligned data. In Proceedings of the European conference on computer vision (ECCV), pp. 768783, 2018. 24 Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Selfsupervised photo upsampling via latent space exploration of generative models. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 24372445, 2020. 1 Peyman Milanfar. Rendition:: Reclaiming what black box takes away. SIAM Journal on Imaging Sciences, 11(4):27222756, 2018. Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 20632072. IEEE, 2025. 3 Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60386047, 2023. 3 Guy Ohayon, Hila Manor, Tomer Michaeli, and Michael Elad. Compressed image generation with denoising diffusion codebook models. In Forty-second International Conference on Machine Learning, 2025. 1 Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1591215921, 2023. 1, 3 Pexels. Pexels - free stock photos & videos you can use everywhere. https://www.pexels. com/, 2025. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. 3 PxHere. PxHere - free images & free stock photos. https://pxhere.com/, 2025. 8 Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1593215942, 2023. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. 9 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 1, 3 Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. In The Thirteenth International Conference on Learning Representations, 2025. 8 Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 12 Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, and Rami Ben-Ari. Lightning-fast image inversion and editing for text-to-image diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 1, Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 9 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. 1, 3, 5, 30 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b. 3, 5 Xin Tao, Chao Zhou, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Zero-order reverse filtering. In Proceedings of the IEEE International Conference on Computer Vision, pp. 222230, 2017. 2, 5, 6 Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 742749, 2019. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19211930, 2023. 1 Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in neural information processing systems, 34:1128711302, 2021. 3 Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. In Forty-second International Conference on Machine Learning, 2025. 1, 3, 7 Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 Xiaofeng Yang, Chen Cheng, Xulei Yang, Fayao Liu, and Guosheng Lin. Text-to-image rectified flow as plug-and-play priors. In The Thirteenth International Conference on Learning Representations, 2025. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018."
        },
        {
            "title": "A ADDITIONAL RESULTS",
            "content": "Additional results obtained by our method for image editing with FLUX are presented in Fig. S1. Figure S1: Additional FlowOpt results (FLUX)."
        },
        {
            "title": "B COMPARISONS",
            "content": "B.1 IMAGE RECONSTRUCTION (INVERSION) Figure S2 displays the results for the unconditional case for FLUX, evaluated by pixel-space RMSE, PSNR, SSIM and LPIPS as function of the NFEs. The left column in this figure is obtained by using CFG = 1, and the right column is obtained by using CFG = 0. The details of this experiments are the same as in Sec. 5. Figure S2: Reconstruction quantitative comparisons (FLUX). Pixel-space RMSE (first row), PSNR (second row), SSIM (third row), and LPIPS (last row) as functions of the number of NFEs for several inversion methods, for unconditional sampling with CFG = 1 (left) and with CFG = 0 (right). The dashed orange horizontal line is the average of forwarding the images through the encoder and decoder of the model. 15 B."
        },
        {
            "title": "IMAGE EDITING",
            "content": "B.2.1 ADDITIONAL QUALITATIVE COMPARISONS Figure S3 presents additional comparisons on image editing. We can see that FlowOpt achieves consistently the best results both in terms of source image adherence and in terms of text adherence. For example, in the third row our method is the only one that managed to preserve the background and the structure of the rocks in the foreground. Similarly, in the fifth row, our method is the only one that preserved the posture of the dogs. Figure S3: Additional qualitative comparisons (FLUX). Fine details are visible upon zooming in. B.2.2 DETAILS OF THE EXPERIMENT SETTINGS Figure 9 compares between all methods in terms of text adherence (CLIP Text) and image adherence measures (CLIP Image, DINOv3 and DreamSim). Figure S4 provides more detailed comparisons between all methods. FLUX hyperparameters. Table S1 lists the settings with which FlowEdit, ODE Inversion, and FlowOpt were run in Fig. S4. The hyperparameters for all figures in the main text (except for Fig. 1) are marked in bold in this table. 16 Figure S4: Editing quantitative comparisons (FLUX). Text adherence is measured by CLIP-Text (x-axis) for all figures. Image adherence (y-axis) is measured by CLIP-Image (left), DINOv3 (center), and DreamSim (right). Connected markers represent different hyperparameters. Table S1: FLUX hyperparameters. FlowEdit ODE Inversion FlowOpt 28 50 nmax 27, 26, 24, 22, 20 45, 40, 35, 30 14, 13, 12 CFG @ source CFG @ target iterations 1.5 1 1 5.5 3.5 3.5 - - 2, 3, 4, 5 For UniEdit, the evaluated hyperparmeters are presented in Tab. S2, with the chosen value for their α parameter marked in bold. In our notation, α = nmax/T . Table S2: FLUX UniEdit hyperparameters. 15 α delay rate 15 , 3 3 , 11 5 , 2 2 ω guidance scale 5 For RF-Solver and FireFlow, the hyperparameters that were evaluated are presented in Tab. S3, following their official implementation. Table S3: RF-Solver and FireFlow hyperparameters. RF-Solver FireFlow 15 30 CFG Injection step 2 2, 3 2, 3, 4, 5 1, 2, 3, 4, 5 For RF-Inversion, the paper provides set of hyperparameters for each kind of editing. We evaluate the sets of hyperparameters provided in their supplementary material. These are reported in Tab. S4. We choose the set that achieved the best results. Table S4: RF-Inversion hyperparameters. 28 starting time τ stopping time η strength 6, 7, 8 0.9, 1."
        },
        {
            "title": "C INITIALIZATION",
            "content": "We proved that if the step size is chosen appropriately, then FlowOpt necessarily converges to the unique global minimum of our optimization problem. However, for any finite number of iterations, 17 the initialization does have an impact on the result. This is illustrated in Figs. S5 and S6, where the red and yellow curves correspond to initialization with the UniInv and ODE Inversion methods, respectively. As the results obtained with the UniInv initialization are better than with ODE inversion, we chose the former for all experiments in the paper. Figure S5: Reconstruction quantitative comparisons (FLUX). Pixel-space RMSE (first row), PSNR (second row), SSIM (third row), and LPIPS (last row) as functions of the number of NFEs, for unconditional sampling with CFG = 1 (left), unconditional sampling with CFG = 0 (center), and text-conditional sampling (right). The red and yellow curves correspond to FlowOpt initialized with the UniInv and ODE Inversion methods, respectively. The dashed orange horizontal line is the average of forwarding the images through the encoder and decoder of the model. 18 STABLE DIFFUSION 3 (SD3) In this appendix, we repeat all the experiments of Sec. 5, but with SD3 instead of FLUX. In this case, we choose the step size η = 102 in the update rule of Eq. (6). D.1 IMAGE RECONSTRUCTION (INVERSION) Implementation details. We use the implementation details provided for FLUX. We set the number of denoisers to = 10, and evaluate the reconstruction error for various NFE values. Dataset. We use the same dataset used for evaluating FLUX randomly chosen same 100 real images of dimension 1024 1024 from the DIV2K dataset, automatically captioned by BLIP and manually refined. 2 . We also evaluated ReNoise, with both = Competing methods. As with the experiments on FLUX, we compare our method to ODE Inversion, RF-Solver, FireFlow and UniInv. For methods, like RF-Solver, which use two forward passes per timestep, we set = NFE 4 . For methods that use single forward pass per timestep, we set = NFE , and set the number of ReNoise steps so as to achieve the desired NFE count. We evaluated various hyperparameters for ReNoise and report the results with those that worked best. It should be noted that, for the fixed point iterations of ReNoise, one would expect that the final iteration would provide the best results. However, we observe that this does not necessarily happen in practice. We also note that as there was no official implementation for any method for SD3, we implemented all of them by ourselves. 10, 28 { } Quantitative evaluation. The reconstruction results of FlowOpt, as well as competing the methods are provided in Fig. S6 both for the unconditional and the conditional case. We can see that our method achieves the best reconstruction results for various NFE values, both for unconditional and for conditional sampling. We can also see that the initialization affects the results achieved by our method, with UniInv leading to better results than naive ODE Inversion and outperforming the competing methods. 19 Figure S6: Reconstruction quantitative comparisons (SD3). Pixel-space RMSE (first row), PSNR (second row), SSIM (third row), and LPIPS (last row) as functions of the number of NFEs for several inversion methods, for unconditional (left) and text-conditional (right) sampling. The red and yellow curves correspond to our FlowOpt initialized with the UniInv and ODE Inversion methods, respectively. The dashed orange horizontal line is the average of forwarding the images through the encoder and decoder of the model. 20 D."
        },
        {
            "title": "IMAGE EDITING",
            "content": "Implementation details. We set the number of denoisers to = 15, and evaluate our method for various nmax values. Specifically, we use nmax . We set the CFG to the default value, i.e., CFG = 3.5. We evaluate our method for various number of iterations, . For all } figures we present the results obtained with nmax = 12. 2, 3, 4, 5 13, 12 { { } Dataset. We use the same dataset used for evaluating FLUX about 400 text-image pairs. The 1024, which were captioned by LLaVA-1.5, dataset consists of 90 real images of dimensions 1024 and manually refined. The target prompts for editing the images were handcrafted. Competing methods. We compare our method against ODE Inversion, UniEdit and FlowEdit. As there was no official implementation for UniEdit for SD3, we implemented it by ourselves. For ODE Inversion, we apply the same number of NFEs used for our method. For all methods, we performed hyperparameters search. Additional details regarding the hyperparameters are provided below. Quantitative evaluation. We evaluate the results of all methods using the same measures reported for FLUX in Sec. 5. The results are presented in Fig. S7. We see that our method achieves results comparable to FlowEdit, and achieves better results than other competing methods. Figure S7: Editing quantitative comparisons (SD3). Text adherence is measured by CLIP-Text (x-axis) for all figures. Image adherence (y-axis) is measured by CLIP-Image (left), DINOv3 (center), and DreamSim (right). Connected markers represent different hyperparameters. Qualitative evaluation. Figure S8 shows comparisons between FlowOpt method and the competing methods. More details about the hyperparameters used to construct this figure are provided in Sec. D.2.1. We can see that our method achieves at least comparable results to other methods, for both object editing and style editing. For example, FlowOpt is the only method that preserves the structure of the scene and the running kid (second row), and successfully turns him into sculpture. Moreover, our method is the only one that preserves the cat and the crown structure (fifth row), and successfully edits its color. Additional results of our method are provided in Fig. S9. 21 Figure S8: Qualitative comparisons (SD3). Fine details are visible upon zooming in. 22 Figure S9: Additional FlowOpt results (SD3). Fine details are visible upon zooming in. 23 D.2.1 SD3 HYPERPARAMETERS. The hyperparameters presented in Fig. S8 for FlowEdit, ODE Inversion, and FlowOpt are listed in Tab. S5, where the chosen hyperparameters for the displayed figures are marked in bold. Table S5: SD3 hyperparameters."
        },
        {
            "title": "FlowEdit\nODE Inversion\nFlowOpt",
            "content": "T 50 50 15 nmax 45, 40, 33, 30, 27 45, 40, 35, 30 13, 12 CFG @ source CFG @ target iterations 3.5 1 1 13.5 3.5 3. - - 2, 3, 4, 5 For UniEdit, the evaluated hyperparmeters are presented in Tab. S6, with the chosen value for their α parameter marked in bold. In our notation, α = nmax/T . Table S6: SD3 UniEdit hyperparameters. 15 α delay rate 15 , 3 3 , 11 5 , 2 5 ω guidance scale"
        },
        {
            "title": "E EDITING WITH OTHER LOSS FUNCTIONS",
            "content": "As noted in Sec. 4, we can generalize the MSE loss defined in Eq. (4) to other loss functions (f (zt), y). In this case, the update rule in Eq. (6) becomes (S1) z(i+1) z(i) η (cid:16) (z(i) ), (cid:17) . Note that Eq. (S1) uses the gradient of the loss, but not the Jacobian of . That is, it does not require backpropagating through the flow process, though it does require backpropagating through decoder in cases where the loss is defined in pixel-space. However, while seemingly attractive, we have not seen significant advantages for using losses other than the L2 loss, except of infrequent cases, as presented in Fig. S12 (for FLUX). Moreover, we observed that other losses typically achieved satisfying results for larger number of iterations (N ). Specifically, other losses typically require 5 iterations that commonly suffice for the L2 loss. Therefore, the L2 loss has the advantage of achieving satisfying results while being computationally. Figures S10, S11 present results with different loss functions, for both SD3 and FLUX. These include the contextual loss (CX) (Mechrez et al., 2018) in pixel space and the ELatentLPIPS (Kang et al., 2024) in latent space, in addition to our default latent-space L2 loss. For all loss functions, we used the hyperparameters reported in Sec. 5 for FLUX, and in App. for SD3, except for and η. We can see that the CX and ELatentLpips losses achieve similar results to the ones obtained with the L2 loss. 30 iterations, which is significantly more than the 15 3 24 Figure S10: Qualitative comparisons using other loss functions (SD3). The results obtained for the update rule in Eq. (S1), for ELatentLPIPS loss (left), contextual (CX) loss (center), and our proposed approach MSE loss (right). The results obtained by all losses are similar. Figure S11: Qualitative comparisons using other loss functions (FLUX). The results obtained for the update rule in Eq. (S1), for ELatentLPIPS loss (left), contextual (CX) loss (center), and our proposed approach MSE loss (right). The results obtained by all losses are similar. 26 Figure S12: Qualitative comparisons using other loss functions (FLUX). The results obtained for the update rule in Eq. (S1), for ELatentLPIPS loss (left), contextual (CX) loss (center), and our proposed approach MSE loss (right). Infrequent cases where the results obtained with loss functions other than MSE are better than results obtained with the MSE loss (allowing weaker structure preservation in favor of stronger adherence to the target text prompt). The number of iterations is typically larger for other loss functions."
        },
        {
            "title": "F CONTRACTION MAPPING",
            "content": "F.1 PROOF OF THEOREM 1 Let g(u) such that η(f (u) y). By definition, g(u) is contraction mapping if there exists γ g(u2) for all u1, u2. Substituting g, the inequality reads g(u1) γ u2 (u η (f (u1) Squaring both sides, we get y)) (u η (f (u2) y)) γ u2 . Rearranging terms, we get u1 η (f (u1) (u2)) 2 γ u1 u2 2. [0, 1) (S2) (S3) (S4) u1 u2 2 + η2 Defining κ = 1 γ2 (u1) (u2) 2η u1 u2, (u1) (0, 1], we get quadratic inequality in η, 2 (u2) γ2 u1 u2 2. (S5) (u1) (u2) 2η2 For each given pair of u1, u2, the set of ηs that satisfy the inequality is η where 2 u1 u2, (u1) (u2) η + κ u1 u2 2 0. (S6) [η1(u1, u2), η2(u1, u2)], η1,2(u1, u2) = u1 u1 = u2, (u1) (u1) (u2) u2, (u1) (u1) (u2) (u2) 2 (u2) 2 (u1) (cid:18) u2, (u1) (u2) (cid:33)2 (u2) 2 (u2) u2, (u1) (u1) u1 (cid:18) κ u1 u2 (u2) (cid:19) u2 (u2) u1 (u1) (cid:19)2 . (S7) (cid:32) (cid:118) (cid:117) (cid:117) (cid:116) u1 (cid:115) 1 κ 1 Therefore, if we choose (cid:20) then the iterations are guaranteed to converge. To choose η2, we note that sup u1,u2 η1(u1, u2), inf u1,u2 η (η1, η2) (cid:21) η2(u1, u2) , (S8) inf u1,u2 η2(u1, u2) u1 inf u1,u2 u1 = inf u1,u2 u1 inf u1,u η2, u2, (u1) (u1) (u2) (u2) 2 u2, (u1) (u1) (u2) u2, (u1) (u1) (u2) (u2) 2 (u2) 2 inf u1,u2 1 + (cid:115) 1 κ 1 + (cid:115) 1 κ sup u1,u2 (cid:18) 1 + (cid:114) (cid:19) κ β2 1 (cid:18) (cid:18) (u1) u1 (u2) u2, (u1) u2 (u2) (u1) u1 (u2) u2, (u1) u1 u2 (u2) (cid:19)2 (cid:19)2 (S9) u1u2,f (u1)f (u2) where we denoted β = inf u1=u2 u1u2f (u1)f (u2) and used the assumption of the theorem that β > 0. Note that the first inequality here follows from the fact that both multiplicands are nonnegative, as inf u1=u2 (u1)f (u2)2 > 0 from the assumption of Eq. (7) in the theorem. In similar manner, we can choose η1 by noting that u1u2,f (u1)f (u2) sup u1,u2 η1(u1, u2) sup u1,u2 u1 u2, (u1) (u1) (u2) (u2) 2 (cid:18) 1 (cid:114) 1 κ β2 (cid:19) Now, since κ > 0 can be chosen arbitrarily small, we take the upper bound to be η1. (S10) lim κ0 η2 = 2 inf u1,u2 u1 and the lower bound to be u2, (u1) (u1) (u2) (u2) 2 lim κ0 η1 = 0. , (S11) (S12) This is allowed since for any η such that Eq. (S8) is satisfied with that particular κ. This completes the proof of the theorem. (limκ0 η1, limκ0 η2), there exists fixed κ > 0 small enough F.2 STEP SIZE UPPER BOUND To verify our choice of η, we drew many pairs of samples u1, u2, as we detail next. Specifically, we generated nonidentical 2000 text prompts using ChatGPT4 (Achiam et al., 2023), drew two different random white Gaussian noises u1, ε for each text prompt, and defined u2 as for various α values, so that both u1 and u2 are distributed u2 = αu1 + 1 αε, (0, I) (an isotropic Gaussian). (S13) By substituting u2 into Eq. (S11) we obtain, for each α value (cid:68) min u1,ε 2 (1 α) u1 (cid:13) (cid:13)f (u1) αε, (u1) (αu1 + 1 (αu1 + 1 αε)(cid:13) 2 (cid:13) (cid:69) αε) , (S14) Figure S13 presents the minimum in Eq. (S14) obtained over the 2000 different realizations, as function of α, for both FLUX and SD3. The global minimum, marked by blue star, is our approximation for the upper bound of Eq. (7). We can see that the minimum is obtained when is small (α close to 1). Our choice for η, which is presented as dashed red line, is below u2 u1 this upper bound. Figure S14 is the same as Fig. 5, but for FLUX instead of SD3, and the comparisons are to step sizes that are 4 larger than our choice, namely η 102, 2.5 and 10 102 1.0 . { } We note that this experiment was evaluated for = 10 both for SD3 and FLUX, and for latent 1024. For different number of variables denoisers, or images of other resolutions, the experiment should be redone. corresponding to images with dimensions of 1024 zt } { Figure S13: Step size upper bounds. The orange line is the minimum obtained over 2000 noise realizations in Eq. (S14), achieved for various α values. The approximation for the upper bound (Eq. (7)) is the starred blue point, and the dashed red line is our step size (η) choice (which is below the upper bound), for FLUX (left) and SD3 (right). Figure S14: Convergence analysis (FLUX). The plot shows RMSE in pixel space vs. number of iterations for the task of inversion, averaged over dataset. The step size we use (red) satisfies the sufficient condition of Eq. (7) and thus leads to convergence. Step sizes that are 4 larger (yellow and black) do not satisfy the condition and do not lead to convergence. The dashed orange line is the minimal RMSE achievable in this setting. It corresponds to passing images through the encoder and decoder. and"
        },
        {
            "title": "Each denoising step by the flow formulation is given by",
            "content": "zt+t = zt + vt(zt)t, where for notational convenience we omit the condition c. However, each denoising step by the DDIM formulation is given by zt1 = αt1 (cid:18) zt 1 αtϵt θ(zt) αt (cid:19) + (cid:112) αt1ϵt θ(zt), (S15) (S16) where αt are the diffusion coefficients as defined by Song et al. (2021a), and ϵt θ(zt) is the predicted noise for the current observation zt, replacing the learned vector field vt(zt) of the flow formulation. Rearranging Eq. (S16), we get zt1 = αt1 αt (cid:18) (cid:112)1 zt+ αt αt (cid:19) ϵt θ(zt). (S17) 1 αt As we relate to the entire process as black box, and stop-grad operator is applied on the output of each of the noise-predicting networks, the terms ϵt θ(zt) vanish under differentiation. Stacking all timesteps one after the other, the formulation remains the same as flows, but with multiplicative coefficient that corresponds to the product of the coefficients multiplying zt in each of the timesteps, δ (cid:89) t= αt1 αt = (cid:114) α0 αT = 1 αT . (S18) Therefore, for example, the update rule for the L2 loss in Eq. (4) for any condition c, is given by z(i+1) z(i) (cid:16) ηδ (z(i) , c) (cid:17) . (S19) HYPERPARAMETERS USED FOR FIGURE 1 The results presented in Fig. 1 were achieved by the hyperparameters provided in Tab. S7. Table S7: Figure 1 hyperparameters. Forest Penguins Owl Owls Corgi Cardboard Lego Paved pathway Glass sculpture in Anime style Deer Wolf Colorful toy bricks Crochet in Pixar style Cow Lizard Corgi Model nmax iterations FLUX FLUX FLUX SD3 SD3 SD3 FLUX FLUX FLUX 11 13 13 12 12 12 12 12 5 8 3 4 5 4 6 5"
        },
        {
            "title": "I EDITING BY INVERSION",
            "content": "In this section we show that even if we have good inversion method, we do not necessarily get good editing performance with the naive editing-by-inversion paradigm. Our definition for good inversion is that we have noise map zt, such that if we would forward it through the chain of denoisers, (zt, csrc), we would get almost the same original image z0. Figure S15 demonstrates this using the inversion map obtained by Eq. (6) with = csrc, where in the final iteration we perform sampling with = ctar. As can be seen, while increasing the number of iterations leads to better ivnersion (top row), it does not monotonically improve the editing measures (bottom plots). Here, we used = 15 and nmax = 13 with the same dataset as that we used for the image editing experiment of Sec. 5. Figure S15: Editing by inversion (FLUX). Reconstruction metrics (top) pixel-space PSNR, SSIM and LPIPS (left to right), and editing metrics (last two rows) CLIP-Text, CLIP-Image, DINOv3 and DreamSim (left to right, top to bottom), as function of the number of iterations (N ) by using = csrc in Eq. (4), and in the final sampling step using = ctar. The dashed orange horizontal line is the average of forwarding the images through the encoder and decoder of the model. Better Inversion (first row) doesnt imply better editing no improvement trend is observed in the editing metrics, even as reconstruction quality improves."
        },
        {
            "title": "J LIMITATIONS",
            "content": "Our method, similarly to other structure preserving editing methods, has limited performance when large geometrical changes to the image are required. For instance, for pose editing, larger number of optimization steps is required to deviate from the original geometry, and this eventually comes at the cost of diverging from the subjects identity. This can be observed in Fig. S16 where we attempt to make the dog jump. As can be seen, while the dog begins to perform the jumping action in the later optimization steps, its identity begins to drift away from the original. Moreover, as seen in iteration = 5, the edited dog has 5 legs. This is because our optimization tries to keep the source and target images close in terms of MSE, pushing towards strict alignment, while also trying to adhere to the text, resulting in deviations from the natural image domain. Choosing more semantic loss function for optimization might remedy these issues, and we leave this for future work. Figure S16: Pose limitation. FlowOpt often fails in preserving the identity of the edited object for pose editing. Another shortcoming of our method is the ability to edit existing text content in images, such as signs, as demonstrated in Fig. S17. We can see that in the first iterations of our optimization process, the method successfully edits the image. Although after several additional iterations, the edit reverts to the original text, failing to adhere to the requested prompt. However, as discussed in Sec. 4, the versatility of our algorithm allows the user to choose between all intermediate editing iterations, mediating this downside. An additional text editing example is presented in Fig. S18, where similar behavior is observed. Figure S17: Text limitation. FlowOpt often fails in text editing as the iterations progress it would make the result overly similar to the original image. Figure S18: Text limitation. FlowOpt often fails in text editing, as it struggles to preserve the structure of the original image and adhere to the target text prompt simultaneously."
        }
    ],
    "affiliations": [
        "Technion - Israel Institute of Technology"
    ]
}