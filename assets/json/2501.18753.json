{
    "paper_title": "INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation",
    "authors": [
        "Jian Hu",
        "Zixu Cheng",
        "Shaogang Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce \\textbf{I}nstance-specific \\textbf{N}egative Mining for \\textbf{T}ask-Generic Promptable Segmentation (\\textbf{INT}). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability."
        },
        {
            "title": "Start",
            "content": "INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation Jian Hu, Zixu Cheng, Shaogang Gong Queen Mary University of London {jian.hu,zixu.cheng,shaogang.gong}@qmul.ac.uk 5 2 0 2 0 3 ] . [ 1 3 5 7 8 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability."
        },
        {
            "title": "1 Introduction\nTask-generic promptable image segmentation leverages a\ntask-generic prompt to derive instance-specific prompts for\nsegmenting diverse images given a single task-query. Unlike\ntraditional segmentation methods that require per-instance\ndistintive and exhaustive prompting (labels) of different in-\nstances in a dataset, this approach to image segmentation\nonly requires a single task-generic prompt applicable to all\ntest samples in a target domain therefore more desirable in\npractice, e.g. “the polyp” is a task-generic prompt for all im-\nages in a polyp segmentation task. However, this approach is\nalso more challenging due to the lack of an instance-specific\nprompt (label) in segmenting each image.",
            "content": "Figure 1: (a) Motivation of INT. When task-related objects in the input to the VLM are occluded, the unique features of these objects are also obscured, leading to significant changes in the corresponding VLM output. In contrast, the features of other objects, which are not fully occluded, show only minor changes in the VLM output. We leverage this observation to assess the correctness of the generated instance-specific prompts without the need for ground truth. By progressive negative mining, we iteratively correct difficult-toidentify erroneous prompts. (b) Evaluation of INT. CLIP semantic similarities are compared between the instance-specific prompts INT generated and the ground truth. INTs contrastive negative mining mechanism effectively corrects erroneous samples, ensuring that the generated instance-specific prompts are instance-wise optimised. task-generic prompt, being both coarse and potentially ambiguous, can lead to poor segmentation when directly applied. Existing solutions [Hu et al., 2024a; Liu et al., 2023c] proposed to use VLMs to mine information from images, deriving instance-specific prompts from generic task prompt to guide SAM in segmenting task-related objects. However, they often struggle with complex images when task-relevant objects are hard to distinguish visually. This leads to inaccurate segmentation and poor model performance. ProMaC [Hu et al., 2024b] tackled this problem by exploring hallucinations as priori knowledge. It masks the objects of interest (foreground) to induce hallucinations, thereby extracting per-instance background information relevant to segmenting It helps to reason and optimise iteratively the foreground. instance-specific prompts. Although these methods have had some success, the lack of ground-truth labels on samples makes it hard to validate if the deduced prompts are accurate. As result, once an incorrect prompt is generated, error propogation is difficult to avoid and may increase over time. Despite the lack of manual annotation, the output changes In Fig. of the VLMs show how they link candidate categories to the task. As shown in Fig. 1(a), frog is hidden among branches that blend with the tree trunk, making it tough to spot. Using the VLM directly, even though the frog triggers high activation, the tree trunk incorrectly scores higher for the snake class, causing misclassification of the branch as snake. If we cover the likely area of the camouflaged animal, the identical feature identifying the frog is hidden, but the snake features remain visible on the tree trunk and bark. This leads to significant decrease in the frog score, while the snake score is barely affected. This drastic alteration in the VLM response when key features are occluded provides valuable insight: we can assess the accuracy of prompts by observing changes in VLM responses when parts of an image are hidden. The element with greater variation is more likely to be the correct instance-specific prompt. Moreover, the difference between correct category and easily mistaken classes in VLM output disparity is often small, leading to incorrect object identification when relying on single output. 1(b), under the premise of including the groundtruth mask, we randomly drew 2 masks and calculated the VLM changes before and after covering these masks, multiplying them by category to implement hard false class negative mining. Compared to using just one mask for covering, the instance-specific prompt inferred in this way has higher CLIP semantic similarity to the groundtruth class. However, previous methods lacked mechanisms to utilize such insights, making it difficult to correct wrong predictions without annotations, leading to error propagation during iterations. Inspired by the observation in Fig. 1(b), we leverage changes in VLM outputs as metric to progressively eliminate the influence of irrelevant categories, effectively addressing this issue. Specfically, in this work, we propose INT, which aims to gradually eliminate the impact of incorrect categories through progressive negative mining, utilizing changes in the VLMs output. It consists of instance-specific prompt generation and semantic mask generation. For prompt generation, the model splits the input image into different patches and processes them in parallel. In these patches, objects of interest might be fully or partially visible. This variation induces the model to use its pre-learned knowledge to predict the existance of objects with their names and locations within patch. The names and locations of objects predicted in each patch are then sent to an image inpainting module, which erases the predicted objects and fills the space with the surrounding background. The outputs of the VLMs are compared before and after the image inpainting, and the predictions with the largest output difference are selected as the instance-specific prompts for this iteration. The normalized weights of these differences are multiplied with the VLMs output in the next iteration. This approach helps iteratively improve the segmentation of hard-to-distinguish categories and allows for correction of initially misclassified samples as the iterations continue. For mask generation, GroundingDINO [Liu et al., 2023c] is deployed to locate task-related objects in the image. The detected bounding boxes and prompts are then processed by SAM and refined using semantic similarity with CLIP. Masks above the similarity threshold are combined to produce the segmentation for this iteration, and the corresponding soft mask is applied to the original image to enhance segmentation in the next iteration. Out contributions are as follows: (1). We introduce INT, training-free test-time adaptation approach that uses progressive negative mining to identify more accurate instance-specific prompts in the absence of annotations, enabling task-generic promptable segmentations to be more accurate. (2). Progressive negative mining identifies hard-to-distinguish error categories by cumulatively multiplying the changes in VLM outputs before and after masking across multiple iterations by category, thereby ensuring the accuracy of the generated instance-specific prompts. (3). Experiments on six datasets demonstrate the effectiveness of our method."
        },
        {
            "title": "2 Related Works",
            "content": "Vision Language Models (VLMs) are adept at handling tasks in both vision and vision-language modalities. They encompass visual understanding models [Krizhevsky et al., 2012; Radford et al., 2021; Liu et al., 2023a], visual generation models [Ramesh et al., 2021; Hu et al., 2020a], and general-purpose Interfaces [Alayrac et al., 2022]. Visual understanding models develop robust visual representations that serve as the backbone for various computer vision downstream tasks. Thanks to extensive image-text datasets, visual generation models are equipped to tackle range of visual tasks, such as segmentation and object detection in images or videos [Kirillov et al., 2023; OpenAI, 2024b], based on multimodal inputs. Although current models have succeeded in addressing many downstream visual tasks, they struggle with specific challenges such as medical image segmentation due to the scarcity of relevant data, making it difficult to achieve high performance in these demanding areas. Our INT introduces negative mining, employing VLMs to iteratively refine the correct prompts to enhance segmentation performance in these challenging tasks. Promptable Segmentation involves segmenting objects with user-provided inputs, such as points, boxes, or scribbles. Models like SAM [Kirillov et al., 2023], AV-SAM [Mo and Tian, 2023], GroundingSAM [Liu et al., 2023c], and SEEM [Zou et al., 2023b] extend this to multimodal inputs, including video and audio. However, these approaches often depend on manual prompts, which are prone to ambiguity and subjectivity, and are typically effective only for specific tasks. GenSAM [Hu et al., 2024a] introduces manual-free setting, using task-generic prompt for instance-specific segmentation across images without additional user inputs. It leverages VLMs to infer object names as prompts for SAM, but its lack of spatial information can result in inaccurate predictions in complex scenes. ProMaC [Hu et al., 2024b] introduced hallucination as type of prior knowledge to leverage taskrelevant information as much as possible to aid in the generation of more accurate instance-specific prompts. However, since there are no ground truths, how to effectively evaluate the quality of instance-specific prompts and exclude the effects of erroneous prompts becomes an unresolved problem. Prompt Engineering is developing field that focuses on creating and refining prompts to improve the effectiveness of large language models (LLMs) for variety of tasks, encompassing both language and vision modifications. In the language domain, zero-shot prompting [Wei et al., 2021] is employed to leverage LLMs generalization capabilities for new tasks, although it can lead to inaccurate outcomes. Recent advancements include chain-of-thought prompting [Wei et al., 2022] and graph prompting [Liu et al., 2023d], which enhance complex reasoning skills. Other strategies such as generated knowledge prompting [Liu et al., 2021] and self-consistency [Wang et al., 2022] have also been implemented to boost prediction accuracy. For vision-related tasks, prompt tuning is the primary method, using vision-driven [Jia et al., 2022; Zhou et al., 2023], language-driven [Zhou et al., 2022; Ma et al., 2023], and vision-language driven approaches [Zang et al., 2022; Xing et al., 2022] to create vision prompts that enhance model performance. Despite some successes in multimodal prompt engineering works [Zhang et al., 2023; Hu et al., 2024a; Hu et al., 2024b] in generating instance-specific prompts, these methods struggle to accurately identify correct prompts without annotations. Our INT approach effectively addresses this challenge by leveraging progressive negative mining."
        },
        {
            "title": "3 Methodology\nWe present INT, a training-free cycle-generation method that\nsegments multiple unknown classes of objects with only a sin-\ngle task-generic prompt. This innovative approach leverages\nnegative mining to iteratively reduce the impact of poten-\ntial erroneous predictions derived from task-generic prompts\nin an unlabelled setting. Specifically, for an image X ∈\nRH×W ×3 from a test set, INT utilizes a task-generic prompt\nPg to generate a final segmentation mask M ∈ RH×W . This\neliminates the need for separate supervision for each image,\nstreamlining the process across datasets in the same task cate-\ngory. The prompt generator identifies multiple candidates for\ninstance-specific prompts, which are evaluated by comparing\nthe VLM outputs before and after image removal. The differ-\nences in the outputs are normalized and iteratively weighted,\ninfluencing the selection of instance-specific prompts in sub-\nsequent iterations. In each iteration, the candidate with the\nlargest difference is chosen as the instance-specific prompt.\nThis selected prompt guides the segmentation process, which\nfurther refines the generation of improved instance-specific\nprompts in the next iteration.",
            "content": "Instance-Specific Prompt Generation"
        },
        {
            "title": "3.1\nPrompt Generation Using VLMs. For more accurate seg-\nmentation, the prompt utilizes VLMs to transform a generic\nprompt Pg into instance-specific prompts for each image.\nSpecifically, given an image X and a query P , the VLM with\nparameters θ generates a response that captures task-relevant\ninformation. The image X provides essential visual context,\naiding the model in formulating a relevant response y, which\nis derived auto-regressively from a probability distribution\nconditioned on P , X, and the previous tokens:",
            "content": "yt pθ(yt X, P, y<t) exp(logitθ(yt X, P, y<t)) (1) where yt denotes the token at time and y<t represents the sequence of tokens generated up to time t1. Despite the advanced capabilities of VLMs, task-relevant objects may blend into their background due to factors like texture, color, or position, making the instance-specific prompts they generate prone to inaccuracies. Inaccurate instance-specific prompts can directly lead to incorrect segmentation. However, without any labels, assessing whether these prompts meet the task requirements becomes challenging. However, VLM predictions are often driven by the unique features of the objects they detect. When such features are obscured, the models predictions for the corresponding category significantly drop (see Fig. 2). Therefore, if task-related objects can be accurately located and effectively removed, comparing the VLM outputs before and after removal provides reliable method for evaluating the quality of the instance-specific prompts. Hallucination-driven Candidates Generation. To generate accurate instance-specific prompts, it is essential to identify as many candidates as possible with reasonable level of confidence to ensure that the correct prompt is not overlooked. Inspired by ProMaC [Hu et al., 2024b], we divide the input image into patches of varying scales by cutting it horizontally, vertically, both, or leaving it uncut. Each patch is then processed individually by the VLM to generate preliminary instance-specific prompts. The visibility of task-relevant objects differs across patches, encouraging the VLM to leverage its prior knowledge to infer potential objects and their locations. This process helps identify candidate bounding boxes and object names by linking the visual data in each patch with the task context. The VLM processes each patch as follows: fore, Ak Bk = VLM(X k, k, PB), Ak fore and their backgrounds Ak back = VLM(X k, k, PA), (2) where is the caption for the k-th image patch k, and Pg is the task-generic prompt. For bounding box predictions, the prompt PB guides the VLM: This image pertains to the Pg detection task, output the bounding box of the Pg. This instructs the VLM to predict the bounding boxes Bk for objects related to the task within the patch. For object naming, the prompt PA states: Output the name of the Pg and its environment in one word. This directs the VLM to predict the names of the task-related objects Ak back from each patch. Object names Ak fore and bounding boxes Bk, collected from different patches, are compiled into candidate lists Ai and Bi, where denotes the iteration. Prompts Selection with Negative Mining. After generating the candidate lists, we evaluate which candidates are most likely correct. When prediction is accurate, removing the corresponding object causes significant change in the VLMs output for that category. Building on the previous section, where we ensured the ground truth was included, we now mask candidate-indicated areas and measure the change in the VLMs output scores. To achieve this, inspired by contrastive decoding, we compare the VLM softmax output of the original unprocessed image patch Xk with that of the masked patch for each patch k. The category with the largest difference in output is selected as the final prediction for the corresponding patch. This approach ensures that the most significant and task-relevant object is accurately identified and used as the instance-specific prompt as follows: D(yk ) = max(softmax(logitθ(yt Xk, P, y<t)) k, P, y<t))), softmax(logitθ(yt (3) Figure 2: INT consists of two main components: instance-specific prompt generation and semantic mask generation. Initially, the former uses VLMs to generate candidate instance-specific prompts. prompt selection module then selects the prompt with the highest VLM output contrast, refined through progressive negative mining. This selected prompt is passed to the semantic mask generation module, which employs GroundingDINO to ensure that all task-relevant samples in the image are collected as comprehensively as possible. Simultaneously, SAM and CLIP work together to ensure that the generated masks are semantically aligned with the task. here, is processed by the image inpainting module, which uses the predicted mask mk from the last iteration in Sec. 3.2 as the inpainting mask IMk to guide the modification of the k. To ensure that patch remains free of task-related ob- , we employ negative prompt Pn: Afore jects Afore is not Pg. For seamless Integration with the existing background Aback , high quality, dei tailed, and well-Integrated with the original image. is formalized as: , positive prompt Pp is used: Aback i k = Fin(Xk, IMk , Pp, Pn), (4) )}K where Fin represents the inpainting module, implemented using Stable Diffusion. Using this method, we construct set D(y) = {D(yk k=1, where represents the number of patch. The accuracy of the predicted category is directly related to the magnitude of change in unique information about the correct category before and after masking. By evaluating these changes, we identify the most reasonable foreground Au to serve as the instance-specific prompt for the i-th iteration. Specifically, we compare the variations in D(yk ) across patches and select the candidate with the largest change in output as the final prediction: Au = argmaxkD(yk ), = 1, 2, . . . , K, (5) This approach leverages the variations in softmax outputs to refine the instance-specific prompt Au , ensuring they are closely aligned with task-relevant information and enhancing the overall prediction accuracy. Progressive Negative Mining While this method effectively identifies correct categories when task-related objects are easily distinguishable, it struggles with more ambiguous samIn such cases, incorrect categories may occasionally ples. exhibit large score differences before and after masking during certain iterations but show little to no change in others. In contrast, the correct category consistently demonstrates stable and significant differences across all iterations. Building on this observation, we design progressive negative mining technique to iteratively refine Au . The idea is to accumulate and reinforce consistent patterns by iteratively multiplying scores from each iteration, reducing the influence of unstable changes caused by incorrect categories. To normalize the differences from each iteration for comparability, we use: Dnorm(yk ) = D(yk ) k=1 D(yk ) (cid:80)K , (6) where Dnorm(yk ) represents the normalized difference for category yk at iteration i, and is the total number of patches. We then iteratively update the differences for the next iteration by applying cumulative multiplication: D(yk i+1) = D(yk i+1) Dnorm(yk ), (7) where D(yk i+1) is the updated difference for category yk in iteration + 1, incorporating the influence of the current iterations normalized differences. By iteratively applying negative mining, we amplify consistent patterns in the score differences associated with the correct category while suppressing the sporadic and unstable changes caused by incorrect categories, effectively enhancing the models ability to accurately refine Au over successive iterations."
        },
        {
            "title": "3.2 Semantic Mask Generation\nAfter obtaining the instance-specific prompt Au\ni , we aim to\nproduce a mask that accurately delineates task-related ob-\njects without overlooking any targets. To achieve this, we",
            "content": "first process Au through Grounding DINO [Liu et al., 2023c] to gather all potential bounding boxes across various patches: Bk = GroundingDINO(X k, Au ) Subsequently, both Bk and Au mk = SAM(Spatial CLIP(Au are input into SAM: , , ), Bk ), (8) (9) where Spatial CLIP [Hu et al., 2024a] maps the text prompt Au to regions within the image Xi as the visual prompts. These visual prompts, along with the corresponding bounding box Bk are fed into SAM during the i-th iteration to generate the mask mk . These processed masks, along with the generated instance-specific text prompts Au , are subsequently input into CLIP to evaluate semantic similarity. s(mk ) = CLIP(mk Xi, Au ), (10) s(mk where the operation retains only those parts of Xi ) quantifies the covered by the predicted mask. similarity between the masked image and Au Similari . ity scores from various patches are represented as Si = [s(m1 ), . . . , s(mk )]. After normalizing these elements within Si, normalized s(mk ) closer to 1 indicates higher semantic alignment of mk with the instance-specific . The weighted sum of the normalized s(mk text prompt Au ) and mk is then computed as follows: ), s(m2 (cid:88) Mi = (s(mk ) mk ), (11) k=1 where Mi is the resultant mask from the i-th iteration of X. This mask, generated using SAMs capabilities, ensures highly detailed mask production. Concurrently, this mask semantic alignment process guarantees that the output mask is consistent with the tasks semantic requirements, overcoming the limitations of SAMs mask prediction. The mask is then applied to the original image as weighting factor to generate the next iteration image Xi for segmentation. This helps to exclude irrelevant regions and reduce Interference during segmentation: Xi+1 = (Xi Mi) + (1 w) Xi, (12) where is hyperparameter set to 0.3. The mask generated in the last iteration serves to guide the prompt generator in the subsequent iteration, focusing on potential task-related regions, mitigating the impact of irrelevant hallucinations, and yielding more precise instance-specific prompts. These prompts, in return, aid the mask generator in producing improved masks. Through iterative cycles of prompt and mask generation, both elements enhance significantly. Ultimately, masks from different iterations are averaged, and the mask closest to this mean is selected as the final output: (cid:12) (cid:19) (cid:12) (cid:12) (cid:12) (M1, . . . , MI) iresult = arg min (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) Mi (13) (cid:80) . Here, represents the number of adaptation epochs, and Mi is the definitive mask for image X."
        },
        {
            "title": "4.2 Results and Analysis\nResults on COD Task. The COD tasks are designed to\ndetect animals camouflaged within complex environments.\nWe tested INT on three benchmark datasets: CHAMELEON\n[Skurowski et al., 2018], CAMO [Le et al., 2019], and\nCOD10K [Fan et al., 2021a]. The CHAMELEON dataset in-\ncludes 76 images collected from the Internet specifically for\ntesting purposes. The CAMO dataset contains 1,250 images,\ndivided into 1,000 training images and 250 testing images.",
            "content": "Table 1: Results on Camouflaged Object Detection (COD) under different settings. Best are in bold. Methods Venue CHAMELEON [2018] Fβ Eϕ Sα CAMO [2019] Fβ Eϕ Sα COD10K [2021a] Fβ Eϕ Sα Camouflaged Object Detection WSSA[Zhang et al., 2020] SCWS[Yu et al., 2021] TEL[Zhang et al., 2020] SCOD[He et al., 2023c] SAM-S[Kirillov et al., 2023] WS-SAM[He et al., 2023b] WSSA[Zhang et al., 2020] SCWS[Yu et al., 2021] TEL[Zhang et al., 2020] SCOD[He et al., 2023c] SAM[Kirillov et al., 2023] SAM-P[Kirillov et al., 2023] WS-SAM[He et al., 2023b] Scribble Supervision Setting 0.067 0.692 0.860 0.782 0.118 0.615 0.786 0.696 0.071 0.536 0.770 0.684 CVPR20 0.053 0.758 0.881 0.792 0.102 0.658 0.795 0.713 0.055 0.602 0.805 0.710 AAAI21 0.073 0.708 0.827 0.785 0.104 0.681 0.797 0.717 0.057 0.633 0.826 0.724 CVPR22 0.046 0.791 0.897 0.818 0.092 0.709 0.815 0.735 0.049 0.637 0.832 0.733 AAAI23 0.076 0.729 0.820 0.650 0.105 0.682 0.774 0.731 0.046 0.695 0.828 0.772 ICCV23 NeurlPS23 0.046 0.777 0.897 0.824 0.092 0.742 0.818 0.759 0.038 0.719 0.878 0.803 point Supervision Setting 0.105 0.660 0.712 0.711 0.148 0.607 0.652 0.649 0.087 0.509 0.733 0.642 CVPR20 0.097 0.684 0.739 0.714 0.142 0.624 0.672 0.687 0.082 0.593 0.777 0.738 AAAI21 0.094 0.712 0.751 0.746 0.133 0.662 0.674 0.645 0.063 0.623 0.803 0.727 CVPR22 0.092 0.688 0.746 0.725 0.137 0.629 0.688 0.663 0.060 0.607 0.802 0.711 AAAI23 0.207 0.595 0.647 0.635 0.160 0.597 0.639 0.643 0.093 0.673 0.737 0.730 ICCV23 0.101 0.696 0.745 0.697 0.123 0.649 0.693 0.677 0.069 0.694 0.796 0.765 ICCV23 NeurlPS23 0.056 0.767 0.868 0.805 0.102 0.703 0.757 0.718 0.039 0.698 0.856 0.790 Task-Generic Prompt Setting CLIP Surgey+SAM GPT4V+SAM [OpenAI, 2024a; Kirillov et al., 2023] 0.147 0.606 0.741 0.689 0.189 0.520 0.692 0.612 0.173 0.488 0.698 0.629 0.180 0.557 0.710 0.637 0.206 0.466 0.666 0.573 0.187 0.448 0.672 0.601 LLaVA1.5+SAM [Liu et al., 2023b; Kirillov et al., 2023] NeurlPS23 0.168 0.561 0.718 0.666 0.314 0.401 0.585 0.501 0.170 0.530 0.728 0.662 0.124 0.654 0.748 0.716 0.104 0.628 0.745 0.709 0.171 0.556 0.705 0.652 CVPR23 NeurIPS23 0.094 0.011 0.307 0.454 0.192 0.023 0.315 0.404 0.143 0.001 0.280 0.425 0.122 0.662 0.776 0.744 0.157 0.656 0.753 0.707 0.085 0.670 0.813 0.764 ICCV23 0.073 0.696 0.806 0.774 0.106 0.669 0.798 0.729 0.058 0.695 0.843 0.783 AAAI24 NeurIPS24 0.044 0.790 0.899 0.833 0.090 0.725 0.846 0.767 0.042 0.716 0.876 0.805 0.039 0.801 0.906 0.842 0.086 0.734 0.853 0.772 0.037 0.722 0.883 0.808 X-Decoder [Zou et al., 2023a] SEEM [Zou et al., 2023b] GroundingSAM [Kirillov et al., 2023; Liu et al., 2023c] GenSAM [Hu et al., 2024a] ProMaC[Hu et al., 2024b] INT Arxiv23 Arxiv23 Ours Table 2: Results for Medical Image Segmentation (MIS) under task-generic prompt setting. Methods Venue GPT4V+SAM [OpenAI, 2024a; Kirillov et al., 2023] Arxiv23 LLaVA1.5+SAM [Liu et al., 2023b; Kirillov et al., 2023] NeruIPS23 CVPR23 NeruIPS23 ICCV23 AAAI24 NeurIPS24 Ours X-Decoder [Zou et al., 2023a] SEEM [Zou et al., 2023b] GroundingSAM [Kirillov et al., 2023; Liu et al., 2023c] GenSAM [Hu et al., 2024a] ProMaC [Hu et al., 2024b] INT CVC-ColonDB [2015] Fβ Eϕ 0.578 0.051 0.246 0.242 0.491 0.194 0.355 0.357 0.462 0.095 0.327 0.331 0.570 0.085 0.280 0.284 0.711 0.071 0.195 0.206 0.244 0.059 0.494 0.379 0.176 0.243 0.583 0.530 0.537 0.172 0.250 0.589 Polyp Image Segmentation Sα Kvasir [2020] Eϕ Fβ 0.614 0.128 0.236 0.479 0.293 0.400 0.449 0.202 0.371 0.520 0.215 0.339 0.387 0.353 0.521 0.172 0.210 0.619 0.166 0.394 0.726 0.161 0.401 Sα 0.253 0.403 0.384 0.367 0.468 0.487 0.573 0.732 0.5739 Skin Lesion Segmentation ISIC [2019] Eϕ Fβ 0.514 0.387 0.366 0.369 0.473 0.497 0.338 0.315 0.127 0.362 0.250 0.002 0.301 0.348 0.247 0.171 0.699 0.744 0.160 0.728 0.766 0.771 0.152 0.733 Sα 0.334 0.477 0.407 0.280 0.533 0.678 0.703 0.708 (a) Number of iteration I. 0.079 1 0.053 2 0.050 3 0.045 4 0.039 5 0.038 Fβ Eϕ 0.861 0.692 0.897 0.748 0.902 0.752 0.903 0.792 0.906 0.801 0.906 0.800 Sα 0.794 0.816 0.823 0.829 0.842 0.844 Table 3: Ablation study on CHAMELEON dataset. (b) Image preprocess strategy. (c) module ablation study. Scale Original Havel Quarters Original+Havel Original +Havel+Quarters 0.071 0.059 0.062 0.043 0.039 Fβ Eϕ 0.758 0.541 0.780 0.582 0.685 0.482 0.891 0.793 0.906 0.801 Sα 0.668 0.692 0.601 0.835 0.842 Methods Variants CHAMELEON [2018] HCG PSNM PNM SMG Fβ Eϕ Sα 0.071 0.541 0.758 0.668 0.083 0.703 0.811 0.746 0.060 0.732 0.836 0.768 0.053 0.772 0.895 0.818 0.039 0.801 0.906 0. The COD10K dataset comprises 3,040 training samples and 2,026 testing samples in total. As indicated in Table 1, we compared INT against other methods that apply different supervision levels. Generally, methods with scribble supervision outperformed those with point supervision. Notably, our INT, utilizing only single generic task prompt, outshines all point-supervised methods and scribble-supervised methods on all three datasets in terms of all the metrics. This highlights INTs effectiveness. Moreover, INT consistently surpasses SAM, SAM-P, SAM-S, and CLIP Surgery+SAM, demonstrating that the enhancements provided by INT are more than leveraging the segmentation capability of SAM. Results on MIS Tasks. The MIS task involves identifying pathological tissues in medical images. We utilized datasets such as CVC-ColonDB [Tajbakhsh et al., 2015] and Kvasir [Jha et al., 2020] for polyp image segmentation, and ISIC [Codella et al., 2019] for skin lesion segmentation. Comparisons from Table 2 show that we conducted experiments in the task-generic promptable segmentation setting. Since most VLMs have not been specifically trained on medical images, directly applying VLM to medical tasks results in significantly lower performance compared to natural image tasks. In contrast, our proposed INT, through extensive candidate sample inference and negative mining, effectively explores task-relevant information and progressively filters out incorrect samples. This ensures the accuracy of the generated instance-specific prompts, greatly enhancing INTs segmentation performance on the MIS task. Parameter Analysis. Tab. 3(a) investigates how various model metrics evolve over prolonged number of iterations. all the metrics stabilize after just 5 iterations. Although the results continue to change, the variations remain slight. This indicates that our method converges rapidly and remains staIt also underscores the importance of using iterationble. based termination criteria to establish early stopping conditions. Tab. 3(b) examines the effects of different image proFigure 3: Visualization of various segmentation methods among various segmentation tasks. cessing strategies. Original refers to the unmodified image, Halve splits the image horizontally or vertically into two parts, and Quarters divides it into four smaller patches. Testing results indicate that combining Original, Halve and Quarters achieves the best balance between global and local information, avoiding excessive fragmentation. Module Analysis. As shown in Tab. 3(c), we perform an ablation study on the COD and MIS tasks to assess the effects of different modules. HCG refers to hallucination-driven candidate generation, PSNM stands for prompt selection with negative mining, PNM is progressive negative mining, and SMG refers to the semantic mask generator. The first row shows that replacing HCG with just single original image leads to reduced performance, underscoring the importance of using hallucinations to extract task-relevant information. In the second row, replacing prompt selection with negative mining using the VLM inference result yields worse performance than the full model, highlighting the significance of proper prompt selection. Removing progressive negative mining results in significant drop in performance, indicating that the instance-specific prompts derived from the initial iteration may contain errors, and our approach effectively corrects these mistakes. The comparison between the last two rows emphasizes the importance of aligning the mask with task semantics. The consistently positive results across tasks confirm the robustness and effectiveness of our approach. Visualization. Fig. 3 visually compares our method, INT, with other approaches across two tasks. GenSAM performs well with clear objects but struggles in complex backgrounds. ProMaC produces solid segmentation results across various tasks, but it sometimes misidentifies instance-specific prompts in complex scenes, leading to segmentation results that are unrelated to the task (e.g., the first column in Fig. 3). In contrast, our INT introduces negative mining strategy that not only explores potential candidates to extract taskrelevant information from the image for better segmentation but also corrects misidentified instance-specific prompts from early iterations. This approach effectively improves performance. Additionally, our method can segment multiple taskrelated samples within the same image, something that previous methods could not achieve. It demonstrated the effectiveness of our approach."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced Instance-specific Negative Mining for Promptable Segmentation (INT) for task-generic promptable image segmentation. INT leverages the difference in VLM outputs before and after masking as metric for progressive (iterative) negative mining. By employing progressive negative mining, INT predicts more accurate instance-specific prompts from single coarse task-generic prompt. This allows for effective segmentation of different targets within the same task across various images, even in the absence of annotations. Experiments conducted on six diverse datasets demonstrate the effectiveness of our INT."
        },
        {
            "title": "References",
            "content": "[Alayrac et al., 2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [Codella et al., 2019] Noel Codella, Veronica Rotemberg, Philipp Tschandl, Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019. [Fan et al., 2017] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji. Structure-measure: new way to evaluate foreground maps. In Proceedings of the IEEE international conference on computer vision, pages 45484557, 2017. [Fan et al., 2021a] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling Shao. Concealed object detection. IEEE transactions on pattern analysis and machine intelligence, 44(10):60246042, 2021. [Fan et al., 2021b] Deng-Ping Fan, Ge-Peng Ji, Xuebin Qin, and Ming-Ming Cheng. Cognitive vision inspired object segmentation metric and loss function. Scientia Sinica Informationis, 6(6), 2021. [He et al., 2023a] Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Camouflaged object detection with feature decomposition and edge reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2204622055, 2023. [He et al., 2023b] Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Weakly-supervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping. arXiv preprint arXiv:2305.11003, 2023. [He et al., 2023c] Ruozhen He, Qihua Dong, Jiaying Lin, and Rynson WH Lau. Weakly-supervised camouflaged In Proceedobject detection with scribble annotations. ings of the AAAI Conference on Artificial Intelligence, volume 37, pages 781789, 2023. [Hu et al., 2019] Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, and Zhongliang Jing. Multi-weight partial domain adaptation. In BMVC, page 5, 2019. [Hu et al., 2020a] Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, Junchi Yan, Zhongliang Jing, and Henry Leung. Discriminative partial domain adversarial network. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16, pages 632648. Springer, 2020. [Hu et al., 2020b] Jian Hu, Hongya Tuo, Chao Wang, Haowen Zhong, Han Pan, and Zhongliang Jing. Unsupervised satellite image classification based on partial transfer learning. Aerospace Systems, 3:2128, 2020. [Hu et al., 2022] Jian Hu, Haowen Zhong, Fei Yang, Shaogang Gong, Guile Wu, and Junchi Yan. Learning unbiased transferability for domain adaptation by uncertainty modeling. In European Conference on Computer Vision, pages 223241. Springer, 2022. [Hu et al., 2024a] Jian Hu, Jiayi Lin, Shaogang Gong, and Weitong Cai. Relax image-specific prompt requirement in sam: single generic prompt for segmenting camouflaged objects. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1251112518, 2024. [Hu et al., 2024b] Jian Hu, Jiayi Lin, Junchi Yan, and Shaogang Gong. Leveraging hallucinations to reduce manual prompt dependency in promptable segmentation. arXiv preprint arXiv:2408.15205, 2024. [Jha et al., 2020] Debesh Jha, Pia Smedsrud, Michael Riegler, Pal Halvorsen, Thomas de Lange, Dag Johansen, and Havard Johansen. Kvasir-seg: segmented polyp dataset. In MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 58, 2020, Proceedings, Part II 26, pages 451462. Springer, 2020. [Ji et al., 2023] Wei Ji, Jingjing Li, Qi Bi, Wenbo Li, and Li Cheng. Segment anything is not always perfect: An investigation of sam on different real-world applications. arXiv preprint arXiv:2304.05750, 2023. [Jia et al., 2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, In European and Ser-Nam Lim. Visual prompt tuning. Conference on Computer Vision, pages 709727. Springer, 2022. [Kirillov et al., 2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander arXiv Berg, Wan-Yen Lo, et al. preprint arXiv:2304.02643, 2023. Segment anything. [Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [Le et al., 2019] Trung-Nghia Le, Tam Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro SugAnabranch network for camouflaged object imoto. segmentation. Computer vision and image understanding, 184:4556, 2019. [Liu et al., 2021] Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. arXiv preprint arXiv:2110.08387, 2021. [Liu et al., 2023a] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. [Liu et al., 2023b] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [Liu et al., 2023c] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [Liu et al., 2023d] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In Proceedings of the ACM Web Conference 2023, pages 417428, 2023. [Ma et al., 2023] Chengcheng Ma, Yang Liu, Jiankang Deng, Lingxi Xie, Weiming Dong, and Changsheng Xu. Understanding and mitigating overfitting in prompt tuning IEEE Transactions on Cirfor vision-language models. cuits and Systems for Video Technology, 2023. [Margolin et al., 2014] Ran Margolin, Lihi Zelnik-Manor, and Ayellet Tal. How to evaluate foreground maps? In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 248255, 2014. [Mo and Tian, 2023] Shentong Mo and Yapeng Tian. Avsam: Segment anything model meets audio-visual localization and segmentation. arXiv:2305.01836, 2023. [OpenAI, 2024a] OpenAI. Gpt-4v: Enhancing gpt-4 for visual processing. 2024. Accessed: 2024-05-20. [OpenAI, 2024b] OpenAI. Hello gpt-4o. 2024. Accessed: 2024-05-20. [Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [Ramesh et al., 2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image In International Conference on Machine generation. Learning, pages 88218831. PMLR, 2021. [Skurowski et al., 2018] Przemysław Skurowski, Hassan Abdulameer, Błaszczyk, Tomasz Depta, Adam Kornacki, and Kozieł. Animal camouflage analysis: Chameleon database. Unpublished manuscript, 2(6):7, 2018. [Tajbakhsh et al., 2015] Nima Tajbakhsh, Suryakanth Gurudu, and Jianming Liang. Automated polyp detection in colonoscopy videos using shape and context information. IEEE transactions on medical imaging, 35(2):630 644, 2015. [Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [Wei et al., 2021] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned lanarXiv preprint guage models are zero-shot arXiv:2109.01652, 2021. learners. [Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [Xing et al., 2022] Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guoqiang Liang, and Yanning Zhang. Class-aware visual prompt tuning for vision-language pretrained model. arXiv preprint arXiv:2208.08340, 2022. [Yu et al., 2021] Siyue Yu, Bingfeng Zhang, Jimin Xiao, and Eng Gee Lim. Structure-consistent weakly supervised salient object detection with local saliency coherence. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 32343242, 2021. [Zang et al., 2022] Yuhang Zang, Wei Li, Kaiyang Zhou, Unified viarXiv preprint Chen Huang, and Chen Change Loy. sion and language prompt arXiv:2210.07225, 2022. learning. [Zhang et al., 2020] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1254612555, 2020. [Zhang et al., 2021] Shizhao Zhang, Hongya Tuo, Jian Hu, and Zhongliang Jing. Domain adaptive yolo for one-stage cross-domain detection. In Asian conference on machine learning, pages 785797. PMLR, 2021. [Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt International Journal of for vision-language models. Computer Vision, 130(9):23372348, 2022. [Zhou et al., 2022] Kaiyang Zhou, [Zhou et al., 2023] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1117511185, 2023. [Zou et al., 2023a] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1511615127, 2023. [Zou et al., 2023b] Xueyan Zou, Jianwei Yang, Hao Zhang, Jianfeng Gao, and Yong Jae Segment everything everywhere all at once. Feng Li, Linjie Li, Lee. arXiv:2304.06718, 2023."
        }
    ],
    "affiliations": [
        "Queen Mary University of London"
    ]
}