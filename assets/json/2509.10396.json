{
    "paper_title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models",
    "authors": [
        "Siyan Zhao",
        "Mengchen Liu",
        "Jing Huang",
        "Miao Liu",
        "Chenyu Wang",
        "Bo Liu",
        "Yuandong Tian",
        "Guan Pang",
        "Sean Bell",
        "Aditya Grover",
        "Feiyu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs."
        },
        {
            "title": "Start",
            "content": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models Siyan Zhao1,2,,, Mengchen Liu1, Jing Huang1, Miao Liu3, Chenyu Wang1,4, Bo Liu1, Yuandong Tian1, Guan Pang1, Sean Bell1, Aditya Grover2, Feiyu Chen1, 1Meta Superintelligence Labs, 2UCLA, 3Tsinghua University, College of AI, 4MIT Work done at Meta, Core Contribution Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer distinctive opportunitytheir inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarksGSM8K, Math500, and AMCachieving new state-of-the-art results for full-attention masked dLLMs. Date: September 15, 2025 Correspondence: Siyan Zhao at siyanz@cs.ucla.edu, Feiyu Chen at feiyuc@meta.com 5 2 0 2 2 1 ] . [ 1 6 9 3 0 1 . 9 0 5 2 : r Figure 1 (a) Unlike autoregressive LLMs, diffusion LLMs can be conditioned on future reasoning hints during generation through inpainting via bidirectional attention, enabling guided exploration toward correct solutions. (b) Applying inpainting-guided exploration in policy optimization outperforms standard Group-relative Policy Optimization (GRPO) sampling and reduces all-wrong groups occurrences. (c) Our full training recipe combining Length-Aligned supervised fine-tuning on concise reasoning traces with IGPO achieves SoTA performance among full-attention masked dLLMs across three mathematical benchmarks."
        },
        {
            "title": "1 Introduction",
            "content": "Recent research has shown that masked diffusion large language models (dLLMs) (Austin et al., 2021; Lou et al., 2024; Shi et al., 2024) such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025) can achieve performance competitive with autoregressive models of similar size. Moreover, their capabilities and performance can be further enhanced via RL post-training (Zhao et al., 2025) and ability to flexibly include multimodal data (Li et al., 2025; Yang et al., 2025; You et al., 2025). Unlike autoregressive models, which decode in left-to-right manner, dLLMs iteratively unmask tokens in parallel. This formulation brings potential for faster inference as shown in closed models like Mercury (Inception Labs et al., 2025) and Gemini Diffusion (DeepMind, 2025), along with flexible inductive bias for operations such as inpainting, the ability to fill in missing content within existing text. In this work, we explore how inpainting can be leveraged to inform post-training algorithms for dLLMs. Recent work on post-training alignment of dLLMs has adopted training approaches similar to autoregressive LLMs, applying Reinforcement Learning with Verifiable Reward (RLVR) methods and demonstrating promising results across reasoning tasks (Zhao et al., 2025; Yang et al., 2025; Gong et al., 2025b). However, fundamental exploration challenge persists: for challenging tasks, policies struggle to discover correct solutions and binary rewards provide minimal learning signal when most generated solutions are incorrect. This leads to substantial sample waste and poor training efficiency, exacerbating the computational costs of online RL. The bidirectional generative structure of diffusion models provides unique mechanism to address this exploration challenge. Since dLLMs are trained through stochastic masking patterns, they possess inherent capability for accepting externally provided partial hints through inpainting operations. We leverage this ability to introduce IGPO (Inpainting Guided Policy Optimization), novel RL framework that strategically guides exploration for dLLMs by injecting reasoning hints when answering difficult problems. Specifically, when the policy is unlikely to generate correct solutions, partial reasoning traces are injected into the generation region, and the dLLM is tasked with completing the remaining reasoning sequence and output final answer. The final answers are verified against ground truth, and only successful completions are used for downstream policy optimization to mitigate the non-positive learning signal problem. We demonstrate IGPOs effectiveness by applying it to group-based policy optimization methods such as Group-relative Policy Optimization (GRPO) (Shao et al., 2024). These methods are particularly vulnerable to exploration failures: when all sampled responses within group produce uniformly incorrect (or correct) rewards, advantage estimation relies on group-relative reward normalization, causing the advantage signal to collapse to zero and resulting in zero gradients. This phenomenon occurs with alarming frequency in challenging reasoning domains, making group-based RL methods an ideal testbed for our approach. By reducing the prevalence of all-wrong groups, IGPO restores non-degenerate gradient signals, accelerates convergence and enables more effective RL. More broadly, IGPO can be viewed as form of guided exploration that interpolates between supervised and reinforcement learning paradigms. The injected tokens function as conditioning context that steers the policys action distribution toward high-reward regions. Unlike pure SFT, which suffers from distribution shift between data and policy rollouts (Zhang et al., 2025), IGPO maintains on-policy generation for the non-injected tokens, ensuring that gradient updates remain closer to the current policys actual sampling distribution. Finally, we augment IGPO with additional reinforcement learning techniques that improve learning stability and performance, including entropy-based gradient filtering for inpainted tokens, and conduct comprehensive experiments across mathematical reasoning benchmarks. Through careful ablation analysis, we systematically evaluate each component of our approach to understand the mechanisms underlying inpainting-guided policy optimization. Our work makes the following key novel contributions: We propose IGPO, the first work to utilize the unique inpainting capabilities of diffusion LLMs for reinforcement learning. By strategically injecting partial reasoning traces during exploration, IGPO alleviates the inefficiency of sparse verifiable rewards and effectively mitigates the zero-advantage dilemma in group-based policy optimization methods such as GRPO. By substantially reducing the proportion of all-wrong groups (by 60% as shown in Fig 1 (b)) and preserving output diversity, our approach yields non-degenerate gradients. 2 We propose Length-Aligned supervised fine-tuning strategy for full-attention based dLLMs using synthetically rewritten, concise reasoning traces. This design better aligns SFT data length with RL sampling and evaluation length, avoids the limitations of verbose traces, and provides stronger initialization for downstream RL. Our full training recipe achieves substantial improvements on mathematical reasoning benchmarks, including +4.9% on GSM8K, +8.4% on Math500, and +9.9% on AMC relative to the LLaDA-Instruct, achieving state-ofthe-art performance among full-attention based dLLMs on these mathematical benchmarks. We conduct comprehensive ablation study that disentangles the mechanisms of IGPO. We show that partial inpainting consistently outperforms full ground-truth inpainting by staying closer to the policy distribution in online RL, and propose an entropy-based gradient filtering mechanism that stabilizes training dynamics."
        },
        {
            "title": "2.1 Masked Diffusion Large Language Models",
            "content": "Masked diffusion LLMs (Austin et al., 2021; Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024; Lou et al., 2024) employ forward diffusion process that progressively corrupts token sequences x0 through introduction of mask tokens. This corruption process is parameterized by time [0, 1]. At any given timestep t, the resulting sequence xt contains partial masking, where each token maintains probability αt of remaining unmasked. The noise schedule αt exhibits strict monotonic decrease with respect to t. Complete masking occurs at = 1, where all tokens in x1 become masked. The training procedure for masked dLLMs follows forward process through definition of αt and bidirectional unmasking predictor fθ with learnable parameters. During each training step, we stochastically sample timestep [0, 1) and apply token masking according to the designated forward process. Given these corrupted sequences, the training objective seeks to recover the original tokens. The standard optimization criterion employs the negative evidence lower bound (NELBO), which provides an upper bound for the negative log-likelihood (NLL) of the training data. For masked dLLMs, this NELBO reduces to weighted NLL formulation, with weighting coefficients derived from transformations of αt (Sahoo et al., 2024, Equation (10)). For example, LLaDA (Nie et al., 2025) specifies the forward process through αt = 1 t, yielding the following NELBO formulation: EtU [0,1), x0pdata, xtqt0(xtx0) 1 xt (cid:88) k=1 1[xk = mask] log fθ(xk 0 xt) , (1) where xt denotes the sequence length of x, and xk represents the k-th token position. The loss computation is restricted to tokens masked at timestep t. During prompt conditional generation, the model starts with sequence where prompt tokens remain unmasked and continuation tokens are initially masked, then progressively unmasks the continuation tokens through ancestral sampling from the reverse process pθ(xs xt) for timesteps > s, where the model fθ provides the denoising predictions for masked positions. The reverse process maintains the property that unmasked tokens are carried over unchanged throughout all denoising steps."
        },
        {
            "title": "2.2 Policy Optimization for Masked Diffusion Large Language Models",
            "content": "Policy-gradient methods have gained widespread adoption for post-training LLMs (Ouyang et al., 2022; Bai et al., 2022; Li et al., 2023; Ahmadian et al., 2024). Online RLparticularly Group Relative Policy Optimization (GRPO)has proved effective for improving language models (Shao et al., 2024; Guo et al., 2025; Team et al., 2025). GRPO (Shao et al., 2024) offers computationally efficient alternative to PPO (Schulman et al., 2017) by using group-based statistics for advantage estimation, avoiding separate value-function training. The GRPO objective integrates clipping for stability and reverse KL regularization: LGRPO(θ) = qD o1,...,oGπθold (q) 1 (cid:88) i=1 1 oi oi (cid:88) k= min (cid:0)ρk Ai, clip (cid:0)ρk , 1 ε, 1 + ε(cid:1) Ai (cid:1) βDKL [πθ(q)πref(q)] , (2) 3 where ρk = πθ(ok πθold (ok q,o<k ) q,o<k is the likelihood ratio. ) For query q, GRPO samples responses {o1, . . . , oG} from the behavior policy πθold and assigns single sequence-level advantage per response. Following Liu et al. (2025b), we use the unnormalized group-relative advantage Ai = r(oi)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 r(oj), (3) where is the reward function. This scalar Ai is shared by all tokens in oi when forming the tokenwise objective. Challenges in Applying GRPO to Diffusion LLMs Applying GRPO to dLLMs is nontrivial. The objective in Equation (2) requires (i) token-level probabilities for importance ratios and (ii) sequence-level probabilities for KL regularization. Autoregressive models provide per-token conditionals via sequential factorization, enabling one-pass sequence scoring by the chain rule: log πAR(o q) = (cid:80)o k=1 log πAR(ok q, o<k). Accordingly, the reverse-KL decomposes as DKL (cid:2)πθ( q) (cid:13) (cid:13) πref ( q)(cid:3) = Eoπθ(q) (cid:88) k=1 log πθ(ok q, o<k) πref (ok q, o<k) . (4) In contrast, dLLMs do not admit sequential factorization of π(o q). dLLMs generation invokes the unmasking predictor fθ across denoising steps, making πθ composition of mappings. Exact tokenwise probabilities would require marginalization over denoising trajectories and maintaining (and differentiating through) full denoising trajectories, which is prohibitive. Mean-Field Approximation for Efficient Optimization To address this, recent work develops efficient approximations for policy optimization in masked diffusion LLMs. DiffuGRPO (Zhao et al., 2025) employs mean-field approximation that yields single-pass estimates of both token-level and sequence-level terms, replacing explicit multi-step unrolling with single-sample Monte Carlo estimate. While this introduces bias relative to the exact diffusion policy, it provides practical framework for GRPO-style optimization on dLLMs. In our method, we adopt the mean-field estimators of Zhao et al. (2025) to compute the token-level importance ratios ρk and the reverse-KL term with one forward pass per policy."
        },
        {
            "title": "3.1 IGPO: Inpainting Guided Policy Optimization",
            "content": "In the GRPO framework, when sampling responses {o1, o2, . . . , oG} for given Zero-Advantage Dilemma. prompt q, the advantage computation in Equation (3) relies on reward variance across the group. However, when all responses receive identical rewardseither all correct or all incorrect the advantages become zero: Ak j=1 r(oj) = 0. This zero-advantage scenario renders the policy gradient component degenerate. Specifically, the clipped surrogate objective collapses to zero regardless of whether the update lies = 0. The policy gradient therefore becomes: in the clipped or unclipped region, since both terms contain Ak = r(oi) 1 (cid:80)G θL(θ) = 1 (cid:88) i=1 1 oi oi (cid:88) k= Ai ρk θ log πθ(ok q) = 0, (5) As result, no meaningful policy update can be extracted from the reward signal, wasting compute sampling these responses. In this work, we specifically focus on mitigating the all-wrong case. 4 Figure 2 Overview of IGPO: When all sampled responses yield identical incorrect rewards (zero-advantage scenario), we perform hint-guided inpainting by generating additional responses using ground truth reasoning chunks as injected hints. Ground truth traces are segmented into variable-length chunks, and selected chunks are injected as fixed hints during generation while the model generates the remaining tokens. We then replace fraction of the original incorrect responses with correct responses generated through inpainting, creating reward variance that enables non-zero advantages for effective policy gradient updates. In full-attention masked dLLM generation, the model input at denoisMasked dLLM Generation and Inpainting. ing step 0 is the concatenation [q; zmask], where represents the prompt and zmask = [mask, mask, . . . , mask] denotes fully masked completion sequence of predetermined length L. The generation process progressively unmasks these positions through iterative denoising until producing the final output. Hint injection modifies this formulation by fixing selected positions of zmask to ground-truth tokens. Formally, given ground-truth reasoning trace = [y y] and binary mask {0, 1}L indicating which positions to inject as fixed hints, we construct the hint-injected initialization: 2, . . . , 1, zhint[i] = (cid:40) if m[i] = 1 and y, y[i] mask otherwise. (6) The masked dLLM then performs bidirectional denoising on [q; zhint] through the inpainting process, leveraging both the prompt and injected hint tokens to generate coherent responses. The injected hint tokens remain fixed throughout the iterative denoising steps. Constructing Hint Patterns for Inpainting. To construct meaningful hint patterns for the inpainting process, we segment the ground truth reasoning trace into variable-length contiguous chunks = {c1, c2, . . . , cN }, where each chunk length cj is sampled from U[smin, smax]. We explicitly exclude the final answer tokens from chunking to prevent reward hacking behaviors where the model ignores reasoning and collapses. For given hint injection ratio η [0, 1], we randomly select η chunks and set their corresponding positions in the binary mask to 1 for hint injection. Elastic Inpainting-Triggered Sampling. With the above inpainting setup, we design IGPO (as in Algorithm 1) to be elastic: hint injection is only triggered when all sampled responses in group yield incorrect rewards (the zero-advantage case), and when activated, both the hint injection ratio and chunk sizes are randomized to provide diverse training signals. When detecting that all sampled responses {o1, . . . , oG} for query yield identical correctness rewards r(oi) = 0, we generate an additional set of responses {o1, . . . , oG} through the inpainting process. Each response oi is generated via inpainting with distinct hint injection ratio ηi U[ηlow, ηhigh] to ensure diverse hint densities. Following inpainting generation, we evaluate the correctness of {oi} and only use the correct ones for replacement. Specifically, we replace = min({oi : r(oi) = 1}, λG) of the original incorrect responses with correct responses generated through inpainting, where λ (0, 1) controls the replacement fraction to maintain non-zero advantage variance. 5 Algorithm 1 IGPO: Inpainting-Guided Policy Optimization for Masked dLLMs Require: Reference model πref, prompt distribution D, ground-truth reasoning traces {y}, number of completions per prompt G, number of inner updates µ, hint injection ratio range [ηlow, ηhigh], replacement fraction λ, entropy filter threshold τ , positive boost coefficient α, chunk size range [smin, smax] 1: Initialize πθ πref 2: while not converged do 3: πold πθ Sample prompt and responses oi πold( q), [G] and compute rewards ri if all ri = 0 (zero-advantage case) then Segment ground-truth reasoning into chunks {c1, . . . , cN } with cj U[smin, smax] for = 1, . . . , do Sample hint injection ratio η U[ηlow, ηhigh] and select ηN chunks from {c1, . . . , cN } randomly Inject selected chunk tokens as fixed hints at corresponding positions Generate oi via inpainting: iteratively denoise the remaining masked positions while keeping hint tokens fixed Evaluate rewards r(oi) and replace up to λG incorrect responses with correct responses generated through inpainting Compute advantages Ak for = 1, . . . , µ do on the updated response set using Eq. 3 Estimate log-probabilities under πθ, πold, πref For hint token positions, update only top-τ percentile highest-entropy positions Update πθ via LIGPO(θ) (Eq. 7) 15: 16: 17: return πθ 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: The complete IGPO objective modifies the GRPO formulation by incorporating the augmented sampling procedure: LIGPO(θ) = qD {o1,...,oGK ,o1,...,oK }IGPO-Sample(πθ,q,y) (cid:34)(cid:32) 1 (cid:88) i=1 1 Li Li(cid:88) k= min (cid:0)ρk Ak , clip (cid:0)ρk , 1 ε, 1 + ε(cid:1) Ak (cid:33) (cid:1) βDKL [πθ(q)πref(q)] , (cid:35) (7) where IGPO-Sample(πθ, q, y) denotes the augmented sampling procedure that applies inpainting-based augmentation when zero-advantage scenarios are detected, producing the augmented RL sampling group {o1, . . . , oGK, o1, . . . , oK} containing (G K) original responses and verified correct inpainted responses {oi} after replacement, where Li denotes the length of the i-th response (whether oi or oi). Crucially, only inpainted responses that pass correctness verification are included in the augmented group, satisfying r(oi) = 1. Advantages Ak are computed normally according to Equation (3). We built IGPO with DiffuGRPO (Zhao et al., 2025)s log probability estimation methods, where all completion tokens are masked during estimation and we remove the random masking applied to prompt tokens as done in DiffuGRPO. Since we use small number of policy iterations (i.e. 4), this alleviates the need for random prompt masking to reduce overfitting. Inspired by Zheng et al. (2025), we compute sequence-level importance-ratio through mean-field approximation for stability purposes. Entropy-based Gradient Filtering for Hint Tokens. When applying IGPO to zero-advantage scenarios, the responses generated through inpainting contain ground truth reasoning chunks that originate from different distribution than the current policy πθ. This creates an off-policy learning scenario where gradient updates from ground truth tokens can conflict with the models current beliefs, particularly at positions where the model has high confidence (low entropy). To mitigate potential training instability from this distribution mismatch, we implement an entropy-based filtering approach that restricts learning to hint token positions where the model exhibits sufficient uncertainty, as inspired by Huang et al. (2025). Specifically, for each hint token position (i.e., positions with injected ground-truth tokens) we compute the entropy. We then apply gradient updates only to the top τ percentile of hint token positions with highest entropy values. This selective learning strategy serves two purposes: high-entropy positions represent genuine decision boundaries where the model is naturally uncertain and thus more receptive to external guidance, and they correspond to flatter probability distributions that yield more stable gradient updates when incorporating ground truth 6 information. This approach controls the policy shift by focusing learning on positions where the model is already open to change, rather than forcing updates against strong existing beliefs at low-entropy positions."
        },
        {
            "title": "3.2 Length-Aligned SFT via Concise Reasoning Trace Rewriting",
            "content": "We seek stronger RL initialization via SFT that matches the models generation length across SFT, RL sampling, and evaluation time. Full-attention masked dLLMs like LLaDA do not utilize KV cache optimization by default (Wu et al., 2025), requiring complete sequence attention computation at each denoising step. This computational constraint dominates online RL training time. Consequently, we are constrained to limit RL generation lengths to 256 tokens for computational feasibility and faster convergence with reduced exploration space. Also, most evaluation setups for LLaDA in recent works (Zhao et al., 2025; Zhu et al., 2025; Nie et al., 2025) use response sequences with fixed lengths up to 1024 tokens. However, popular reasoning SFT corpora (e.g., OpenR1) contain verbose reasoning traces that frequently exceed 10k tokens. SFT on these corpora introduces distribution mismatch across SFT training, RL and evaluation sampling. Moreover, these reasoning datasets contain many repeated reflective behaviors unsuitable for limited context generation. Addressing Generation Length Mismatch Through Concise Reasoning Trace Rewriting. To address this generation length mismatch, we propose systematic rewriting of verbose reasoning traces into concise, structured versions that preserve essential logical flow while conforming to the computational constraints of full-attention masked dLLMs. We employ LLaMA-4-Maverick (Meta, 2025) (with designed prompts in Section D) to eliminate redundant reflections, condense multi-sentence elaborations into precise, mathematically rigorous statements, and maintain essential reasoning steps. Our Length-Aligned SFT trains LLaDA exclusively on these rewritten traces, providing better initialization for RL learning by eliminating the need for implicit length compression during RL and allowing the model to focus on reasoning quality improvements within fixed computational bounds. Empirical evaluation demonstrates superior performance compared to training on original verbose traces. Additionally, we find that masked dLLMs benefit from longer training epochs (e.g. 200) compared to AR LLMs, as has also been noted in recent works (Ni and the team, 2025; Prabhudesai et al., 2025)."
        },
        {
            "title": "4 Experiments\nTo investigate how the inpainting capabilities of masked dLLMs can address the exploration challenges in RL,\nwe conduct comprehensive experiments to answer the following main research questions:",
            "content": "(1) How effectively does our complete training approach (Length-aligned SFT with rewritten reasoning traces followed by reinforcement learning with IGPO) improve the mathematical reasoning performance of LLaDA and reduce all-wrong groups occurrences? (4.3) (2) How does partial hint injection in IGPO bridge on-policy generation with ground truth guidance, and how does this improve learning compared to full supervision? (4.4) (3) How do other key design choicesincluding entropy filtering thresholds and reasoning trace rewritingaffect RL training dynamics and learning stability? (4.4)"
        },
        {
            "title": "4.1 Complete Training Recipe",
            "content": "Our complete learning framework consists of two-stage pipeline: Stage 1: Supervised Fine-Tuning with Rewritten Traces. We begin with Length-Aligned SFT on the LLaDA8B-Instruct model using the OpenR1-Math-220K datasets default split (94k math problems), but with all reasoning traces rewritten (See Section for length distribution before and after revision). This ensures consistency between training distribution and downstream RL/evaluation phases by aligning trace lengths. Stage 2: Reinforcement Learning with IGPO. Following Length-aligned SFT, we apply IGPO to further enhance reasoning capabilities through strategic inpainting-guided policy optimization. We utilize the reasoning traces from the MetaMathQA dataset for the elastic inpainting process, creating effective guidance signals that fit within our computational constraints. Detailed hyperparameters are provided in Section A."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We conduct experiments using LLaDA-8B-Instruct as the base model with sampling temperature of 1.2 for RL online generation. For reinforcement learning, we train on the MetaMathQA dataset, specifically using the Answer Augmentation\" split and combining questions from both GSM8K and MATH500. After deduplicating identical questions, we obtain 12,794 unique training examples. For supervised fine-tuning, we utilize the OpenR1-Math-220K dataset with rewritten reasoning traces as described in Section 3.2. We evaluate our approach on three mathematics benchmarks: GSM8K, MATH500 and AMC. Experiments are conducted on 88 80GB H100 GPUs. For UniGRPO (Yang et al., 2025) baseline, we reproduce based on their Algorithm 1. We provide detailed experiment hyperparameter setups in Section A."
        },
        {
            "title": "4.3 Main Results",
            "content": "Figure 3 RL training curves of IGPO versus normal GRPO sampling. (a) Starting from LLaDA-8B-Instruct. (b) Starting from the length-aligned SFT checkpoint. IGPO exhibits superior and more stable training performance under both initialization checkpoints. Results are averaged over 3 random seeds across three mathematical reasoning benchmarks (GSM8K, MATH500, and AMC), with standard errors shown as shaded regions. Table 1 Performance across multiple mathematics tasks. GSM8K and MATH500 are evaluated with pass@1 at temperature of 0.0, and AMC with avg@16 at temperature 0.1. Underlined scores indicate the best within each initialization group. Parenthesized deltas typeset via (+) denote absolute percentage-point improvements relative to the LLaDA-8B-Instruct baseline. Model GSM8K (pass@1) MATH500 (pass@1) AMC (avg@16) Average LLaMA3-8B (AI@Meta, 2024) Qwen2.5-7B (Team, 2024) 79.6 85.4 30.0 49.8 Similar-sized autoregressive LLMs Prior masked dLLM baselines Dream-7B (Ye et al., 2025) d1-LLaDA (Zhao et al., 2025) wd1 (Tang et al., 2025) LLaDA-1.5 (Zhu et al., 2025) LLaDA-Instruct (Nie et al., 2025) 77.2 82.1 82.3 83.3 81.5 (+0) 39.6 40.2 39.0 42.6 39.0 (+0) 13.6 14.5 (+0) 46.5 45.0 (+0) RL from LLaDA-Instruct LLaDA-Instruct + UniGRPO (Yang et al., 2025) LLaDA-Instruct + DiffuGRPO (Zhao et al., 2025) LLaDA-Instruct + IGPO (ours) 82.2 (+0.7) 81.9 (+0.4) 83.6 (+2.1) 39.2 (+0.2) 40.2 (+1.2) 42.8 (+3.8) 15.0 (+0.5) 17.5 (+3.0) 18.1 (+3.8) 45.5 (+0.5) 46.5 (+1.5) 48.2 (+3.2) Length-aligned SFT on LLaDA-Instruct and RL on SFT checkpoint LLaDA-Instruct + Length-aligned SFT (ours) LLaDA-Instruct + Length-aligned SFT + IGPO (ours) 83.6 (+2.1) 86.4 (+4.9) 45.2 (+6.2) 47.4 (+8.4) 22.3 (+7.8) 24.4 (+9.9) 50.4 (+5.4) 52.7 (+7.7) 8 As shown in Table 1, our training recipe demonstrates consistent improvements across all mathematical reasoning benchmarks. With Length-Aligned SFT on rewritten traces, LLaDA improves by 2.1% on GSM8K and 6.2% on MATH500 compared to the base LLaDA-8B-Instruct model. When applying IGPO on top of the SFT model, we observe further improvements of 2.8% on GSM8K to 86.4% and 2.2% on MATH500 to 47.4%. The complete two-stage pipeline yields cumulative improvements of 4.9% on GSM8K and 8.4% on MATH500 relative to the LLaDA-Instruct baseline. On the challenging AMC benchmark, our approach achieves 24.4% (avg@16), representing 9.9% improvement over the baseline. As shown in Figure 3, IGPO exhibits more stable training dynamics compared to standard GRPO sampling when initializing from either the base LLaDA checkpoint or after SFT. IGPO effectively reduces the all-wrong group ratio by around 60%, as shown in Figure 1(b). Our final model (LLaDA + SFT + IGPO) outperforms all baseline approaches including the recent LLaDA-1.5 model across all evaluated benchmarks. Notably, even without SFT, applying IGPO directly on LLaDA achieves better performance than the previous LLaDA-1.5 and other RL methods for full-attention dLLMs, establishing new state-of-the-art recipe for mathematical reasoning in masked diffusion language models."
        },
        {
            "title": "4.4 Analysis and Ablation Studies",
            "content": "Self-generated inpainted traces provide better learning signal than ground truth traces. The results in Figure 4 show that partial hint injection achieves higher performance than full hint injection. When the hint injection ratio varies within the lower range, the model needs to generate self-rationalized inpainting traces (with an example shown in Section B), and only those that lead to correct solutions are added to the group for gradient updates. Through inpainting, the model attempts to coherently connect provided hint chunks with its own reasoning steps. The inpainted generation produces learning signal that bridges the gap between the models current capabilities and the target behavior. The self-generated portions reflect the models current reasoning patterns and are more \"on-policy\" while incorporating structural guidance from ground truth chunks, resulting in more effective policy optimization compared to pure supervised learning, reducing the distributional mismatch. This bridging of SFT and online RL through partial self-generation leads to more effective policy optimization. Figure 4 Impact of hint injection ratio on performance across 3 datasets, averaged over 3 seeds with standard error shown as shaded areas. We compare partial hint injection (η U[0.2, 0.6]) versus full hint injection (η = 1.0). Partial hint injection consistently outperforms full hint injection, demonstrating the benefits of self-generated reasoning. Both hint-guided inpainting variants outperform the baseline without any hint injection. Entropy clipping prevents training instability from off-policy tokens. As shown in Figure 5, we observe that learning from only the top 20% highest-entropy hint token positions (τ = 0.2) achieves the best performance and exhibits the most stable training dynamics. In contrast, learning from all hint token positions (τ = 1.0) or large fraction (τ = 0.8) leads to more unstable training with performance fluctuations compared to lower values like 0.2. This empirical finding supports our motivation that restricting gradient updates to high-entropy positions prevents the destabilizing effects of large gradients on high-entropy positions. The validates the necessity of entropy-based filtering when incorporating ground truth traces from hint-guided inpainting into policy gradient training. 9 Figure 5 Impact of entropy clipping threshold on hint tokens. Performance comparison across different entropy clipping thresholds τ applied to hint token positions in IGPO, where τ = 0.2 represents learning from only the top 20% highest-entropy hint token positions, while τ = 1.0 indicates learning from all hint token positions without filtering. This results is run on GSM8K with temperature of 0.1 and generation length of 256. Figure 6 SFT and RL training dynamics with rewritten versus original traces. We compare two settings: (1) models fine-tuned with SFT on our rewritten concise traces (maximum length 1024 tokens), and (2) models fine-tuned with SFT on the original OpenR1-Math traces truncated at LLaDAs 4096-token context limit. After SFT, both checkpoints are used as initialization for RL training (either standard RL or IGPO). The rewritten traces yield higher SFT performance and lead to superior final RL accuracy. Across both initializations, IGPO consistently outperforms standard RL, maintaining stable pass@5 performance while standard RL suffers from declining diversity. This results is run on GSM8K with temperature of 0.1 and generation length of 256. Effect of reasoning trace rewriting for SFT and subsequent RL training. The results in Figure 6 illustrate two key findings. First, SFT on rewritten reasoning traces produces substantially stronger checkpoints than SFT on the original traces. Our rewritten traces eliminate verbose reflection behaviors and compress reasoning into concise trajectories (up to 1024 tokens), which are better aligned with LLaDAs generation budget (256 tokens) and evaluation sequence length. This alignment improves SFT accuracy at step 0 relative to models trained on the longer 4096-token traces. Second, while RL training can partially compensate for weaker SFT checkpointsthe models trained on 4096-token traces recover accuracy rapidly in early RL stepsstarting from stronger rewritten SFT checkpoints leads to consistently higher final performance. Importantly, across both initialization settings, IGPO outperforms standard RL without inpainting: IGPO preserves output diversity and stabilizes pass@5 performance throughout training, whereas standard RL exhibits decline in pass@k metrics, indicative of reduced exploration and mode collapse."
        },
        {
            "title": "5.1 Diffusion Language Models",
            "content": "Diffusion language models was first explored through continuous approaches that map discrete text to continuous representations, including learned embeddings, sequence-to-sequence conditioning, and binary bit representations (Chen et al., 2022; Li et al., 2022; Gong et al., 2023). Recently, discrete diffusion language models have been scaled up significantly, with masked diffusion established as specific instance of discrete diffusion (Austin et al., 2021; Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024; Nie et al., 2024). Notable developments include DiffuLLaMA (Gong et al., 2025a) and Dream (Ye et al., 2025), both adapted from pretrained autoregressive LLMs. LLaDA (Nie et al., 2025) represents breakthrough as masked diffusion LLM trained from scratch using full-attention, achieving performance comparable to similarly-sized autoregressive models. These approaches are predominantly based on masked modeling. Unlike these full-attention dLLMs, Block Diffusion (Arriola et al., 2025) introduced hybrid approach that models sequences block-by-block while applying diffusion within each block, enabling flexible length generation and improved inference efficiency through kv-caching. Recent commercial models like Mercury (Inception Labs et al., 2025) and Gemini Diffusion (DeepMind, 2025) have demonstrated the practical viability of diffusion-based code generation, achieving performance comparable to leading autoregressive models while offering significantly faster inference. More recent works have introduced caching and parallel decoding algorithms (Wu et al., 2025; Liu et al., 2025a; Ma et al., 2025; Israel et al., 2025; Sahoo et al., 2025; Hu et al., 2025) that significantly improve inference efficiency for masked diffusion language models. In this work, we focus on full-attention masked dLLMs."
        },
        {
            "title": "5.2 Reinforcement Learning for Diffusion Language Models",
            "content": "Applying reinforcement learning to diffusion language models presents unique challenges compared to autoregressive models. The primary obstacle is the intractability of likelihood functions in diffusion models, which necessitates approximating sequence-level likelihoods for policy optimization. This requirement introduces computational overhead and potential bias, particularly when approximation errors occur in policy ratios used for importance sampling. d1 proposed diffu-GRPO (Zhao et al., 2025) which adopts an efficient approximation through mean-field approximation. MMaDA (Yang et al., 2025) and diffucoders coupled-GRPO (Gong et al., 2025b) further improve the masking strategy in log probabilities estimation to achieve better learning efficiency. LLaDA 1.5 (Zhu et al., 2025) tackles the variance issues in ELBO-based likelihood estimates through preference optimization. Recently, wd1 (Tang et al., 2025) addresses these challenges by reformulating policy optimization as weighted likelihood objective that eliminates the need for policy ratios. SDPO (Han et al., 2025) decomposes the diffusion trajectory alignment problem into stepwise subproblems that align the posterior at each diffusion step. Our inpainting method can also be applicable to some of the above online RL methods. closely related work in RL for AR LLMs is Prefix-RFT (Huang et al., 2025), which samples prefixes from demonstrations to guide online exploration, though this is limited to left-to-right sequential generation that does not leverage the bidirectional conditioning capabilities of diffusion LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work we introduce IGPO, novel reinforcement learning algorithm that utilizes the inpainting capabilities of masked diffusion language models. By strategically incorporating ground truth reasoning hints during the denoising process, IGPO steers the policys action distribution toward high-reward regions and mitigates the fundamental exploration challenge in RL. IGPO addresses the zero-advantage dilemma by creating reward variance that enables effective policy gradient updates when standard sampling procedures yield uniform outcomes. As part of our recipe, Length-Aligned SFT mitigates the length distribution gap between SFT training, RL, and evaluation sampling, and provides stronger initialization for downstream RL. Our comprehensive evaluation demonstrates that this methodology, combined with the stabilization mechanism of entropy-based gradient filtering, establishes new state-of-the-art performance among full-attention masked dLLMs across multiple mathematical reasoning benchmarks. This contribution presents novel paradigm for reinforcement learning in masked diffusion language models, demonstrating how architectural properties can be systematically leveraged to overcome critical optimization challenges."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. AI@Meta. Llama 3 model card. 2024. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Ting Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. ArXiv, abs/2208.04202, 2022. DeepMind. Gemini diffusion, 2025. https://deepmind.google/models/gemini-diffusion/. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. DiffuSeq: Sequence to sequence text generation with diffusion models. In International Conference on Learning Representations, ICLR, 2023. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models. In The Thirteenth International Conference on Learning Representations, 2025a. Shansan Gong, Huangjie Zheng Ruixiang Zhang, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. 2025b. https://arxiv.org/ abs/2506.20639. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jiaqi Han, Austin Wang, Minkai Xu, Wenda Chu, Meihua Dang, Yisong Yue, and Stefano Ermon. Discrete diffusion trajectory alignment via stepwise decomposition, 2025. https://arxiv.org/abs/2507.04832. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guided diffusion, 2025. https://arxiv. org/abs/2505.21467. Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, and Ivan Titov. Blending supervised and reinforcement fine-tuning with prefix sampling, 2025. https://arxiv.org/abs/2507.01679. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, and Volodymyr Kuleshov. Mercury: Ultra-fast language models based on diffusion. 2025. https://arxiv.org/abs/2506.17298. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding, 2025. https://arxiv.org/abs/2506.00413. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. ArXiv preprint, abs/2505.16839, 2025. https://arxiv.org/abs/2505.16839. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. ArXiv, abs/2205.14217, 2022. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. 12 Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models, 2025. https://arxiv.org/abs/2505.15781. AI Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, 2025. Jinjie Ni and the team. Diffusion language models are super data learners. https://jinjieni.notion.site/ Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac, 2025. Notion Blog. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. https://arxiv.org/abs/2502.09992. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, and Deepak Pathak. Diffusion beats autoregressive in data-constrained settings, 2025. https://arxiv.org/abs/2507.15857. Subham S. Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T. Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. http://papers.nips.cc/paper_files/paper/ 2024/hash/eb0b13cc515724ab8015bc978fdde0ad-Abstract-Conference.html. Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, and Arash Vahdat. Esoteric language models, 2025. https://arxiv.org/abs/2506.01928. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. Simplified and generalized masked diffusion for discrete data. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. http: //papers.nips.cc/paper_files/paper/2024/hash/bad233b9849f019aead5e5cc60cef70f-Abstract-Conference.html. Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. wd1: Weighted policy optimization for reasoning in diffusion language models, 2025. https://arxiv.org/abs/2507.08838. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Qwen Team. Qwen2.5: party of foundation models, September 2024. https://qwenlm.github.io/blog/qwen2.5/. 13 Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025. https://arxiv.org/abs/2505.22618. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. ArXiv preprint, abs/2505.15809, 2025. https://arxiv.org/abs/2505.15809. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. https://hkunlp.github.io/blog/2025/dream. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting, 2025. https://arxiv.org/abs/2508.11408. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. ArXiv preprint, abs/2505.19223, 2025. https://arxiv.org/abs/2505.19223."
        },
        {
            "title": "A Experiments Hyperparameters",
            "content": "Table 2 Training Hyperparameters Parameter Value SFT Training Parameters Per Device Train Batch Size Hardware Configuration Gradient Accumulation Steps Learning Rate LR Schedule LR Warmup Steps LR Min Value LR Decay Period Number of Epochs RL Sampling Parameters RL Online Sampling Generation Length Diffusion Steps Block Length Sampling Temperature Generations Per Group RL Training Parameters Per Device Train Batch Size Hardware Configuration Gradient Accumulation Steps Effective Batch Size KL Beta β Policy Iterations µ Learning Rate LR Schedule LR Warmup Steps LR Decay Period Training Steps Clip Ratio Epsilon ε IGPO Specific Parameters Chunk Size cj U[smin, smax] Inpainting Ratio ηi U[ηlow, ηhigh] replacement fraction λ Entropy-based Gradient Filtering for Inpainted Tokens τ 4 88 H100 GPUs 8 5 106 Warmup-stable-decay 200 1 106 Final 10% of steps 256 128 32 1.2 8 8 88 H100 GPUs 1 512 0.01 4 5 107 Linear decay to 0 50 10 epochs 1440 0.2 U[5, 10] U[0.2, 0.6] 0.5 0."
        },
        {
            "title": "B Inpainting Generation Qualitative Examples",
            "content": "Blue text: Question Prompt Green text: Injected Hints Black text: Inpainted Generation Problem: Circle has radius 6 cm. How many square centimeters are in the area of the largest possible inscribed triangle having one side as diameter of circle C? Inpainting Input At Denosing Step 0: <start_header_id>user<end_header_id> Respond in the following format: <reasoning> ... </reasoning> <answer> boxed{<Your answer>} </answer> Circle $C$ has radius 6 cm. How many square centimeters are in the area of the largest possible inscribed triangle having one side as diameter of circle $C$?<eot_id><start_header_id>assistant<end_header_id> <mdm_mask><mdm_mask>...<mdm_mask> Since the diameter is twice the radius, the base of the triangle will have length $2 times 6 = 12$ cm. To <mdm_mask>...<mdm_mask> , we want the height to be as large <mdm_mask>...<mdm_mask> of the triangle is the distance from the midpoint of the base to the circumference of the circle. This is equal to the radius of the circle, <mdm_mask>...<mdm_mask> <mdm_mask>...<mdm_mask> <mdm_mask>...<mdm_mask> 12 times <mdm_mask>...<mdm_mask> Inpainted Generation After Denoising: <reasoning> We know to find the largest possible inscribed triangle in circle $C$, the base of the triangle should be the diameter of the circle. Since the diameter is twice the radius, the base of the triangle will have length $2 times 6 = 12$ cm. To find the largest possible area of the triangle, we want the height to be as large as possible. The largest possible height of the triangle is the distance from the midpoint of the base to the circumference of the circle. This is equal to the radius of the circle, which is 6 cm. Therefore, the area of the largest possible inscribed triangle is given by the formula for the area of triangle, which is $$ frac{1}{2} times textbase times textheight = frac{1}{2} times 12 times 6 = 36$$ square centimeters. </reasoning> <answer> boxed{36} </answer> 16 Length-Aligned SFT: SFT trace revision length distribution comparison As illustrated in Figure 7, the original OpenR1-Math-220K dataset exhibits substantial token length diversity, with reasoning traces extending beyond 10,000 tokens while LLaDAs maximum context length is only 4096 tokens. Naively applying SFT on this dataset would result in many truncated sequences, and even for samples within the 4096-token limit, significant distribution mismatch persists across training phaseswe use 256 tokens for RL sampling and 512 tokens for evaluation. Our rewriting using LLaMA-4-Maverick successfully constrains all traces to under 1500 tokens, creating alignment between SFT training, RL sampling, and evaluation phases. Additionally, while reflective behavior has been found helpful for LLaDA in prior work (Zhao et al., 2025), the excessive repeated reflective patterns in the original dataset are unsuitable for its constrained generation space. The rewriting process eliminates this redundancy while preserving essential reasoning structure. Figure 7 Token Length Distribution of SFT Dataset Before and After Revision. Comparison of token length distributions for the OpenR1-Math-220K dataset (94k math problems). After revision using LLaMA-4-Maverick, token lengths are constrained to below 1500 tokens, eliminating the extreme range of the original dataset where traces could exceed 20,000 tokens. This addresses the generation length mismatch across SFT training, RL sampling (256 tokens), and evaluation (512 tokens) phases."
        },
        {
            "title": "D Prompt for SFT traces revision",
            "content": "Prompt for SFT traces revision sent to LLaMA 4 Maverick Please rewrite the original solution to make it more concise and easier to understand without changing the details. Please put the explanation in the solution between <reasoning> and </reasoning> and put the final numerical answer between <answer> and </answer> in boxed format. Please shorten or rewrite the rewritten solution to random length between 100 and 1000 words while keeping sufficient details of the reasoning steps. Please do not return anything other than the rewritten solution. Example: <reasoning> xxx </reasoning> <answer> boxed{14} </answer> Original solution: {generations}. Your response: 18 SFT trace before rewritten by LLaMA 4 Maverick Problem: Find all primes between 1 and 15. <think> Okay, so need to find all the prime numbers between 1 and 15. Hmm, lets start by remembering what prime number is. From what recall, prime number is natural number greater than 1 that has no positive divisors other than 1 and itself. So, numbers like 2, 3, 5, etc., are primes because you cant divide them evenly by any other number besides 1 and the number itself. On the other hand, numbers like 4, 6, 8, and so on are not primes because they can be divided by other numbers. For example, 4 can be divided by 2, so its not prime. Alright, so the task is to list all primes between 1 and 15. Let me list out all numbers from 1 to 15 first to check them one by one. The numbers are: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. Now, lets go through each number and see if its prime. Starting with 1. Wait, remember that by definition, prime numbers are greater than 1. So 1 is not considered prime. So we can skip 1. Next up is 2. Lets check if 2 is prime. The divisors of 2 are 1 and 2. There are no other divisors, so yes, 2 is prime number. Moving on to 3. Similarly, the divisors are 1 and 3. No other numbers divide into 3, so 3 is prime. Next is 4. Lets check divisors. 4 divided by 2 is 2, which is an integer, so 4 is not prime. It can be divided by 2, so its composite. Then 5. The divisors of 5 are 1 and 5. No other numbers divide into 5, so 5 is prime. 6. Checking divisors. 6 divided by 2 is 3, so its divisible by 2 and 3. Not prime. 7. Divisors are 1 and 7. No other divisors, so 7 is prime. 8. Divided by 2 gives 4, so 8 is not prime. 9. Divided by 3 gives 3, so 9 is not prime (since 3 times 3 is 9). 10. Divided by 2 is 5, so 10 is not prime. 11. Lets see, 11. Divided by 2? 11 divided by 2 is 5.5, not an integer. Divided by 3? 11 divided by 3 is approximately 3.666... Not an integer. Divided by 4? 2.75, nope. 5? 2.2, nope. Since 11 is prime number because none of these divisions result in integers, so 11 is prime. 12. Divided by 2 is 6, so composite. 13. Checking divisors. 13 divided by 2 is 6.5, 13 divided by 3 is about 4.333..., 4? 3.25, 5? 2.6, 6? 2.166... None of these are integers. So 13 is prime. 14. Divided by 2 is 7, so composite. 15. Divided by 3 is 5, so composite. Let me recap the numbers between 1 and 15 and their primality: 1: Not prime 2: Prime 3: Prime 4: Not 5: Prime 6: Not 7: Prime 8: Not 9: Not 10: Not 11: Prime 12: Not 13: Prime 14: Not 15: Not So compiling the primes from this list: 2, 3, 5, 7, 11, 13. Wait second, let me verify each one again just to be sure didnt make mistake. Starting with 2: Definitely prime. Its the only even prime. 3: Prime, yes. 5: Prime, yes. 19 7: Prime. Correct. 11: Lets check again. 11 divided by 2 is not whole. 3? 3*3 is 9, 3*4 is 12. So no. 5? 5*2 is 10, 5*3 is 15. So 11 is prime. 13: Similarly, 13 divided by 2 is 6.5, 3 is 4.333, 5 is 2.6. So, prime. Yes, those all check out. So primes between 1 and 15 are 2, 3, 5, 7, 11, and 13. But hold on, sometimes people might make mistake with 9. Wait, 9 is 3 times 3, so thats composite, which had down. 15 is 3 times 5, composite. So all the non-primes are accounted for. Is there any number might have missed? Lets see. Lets list all numbers again: 2, 3, 5, 7, 11, 13. Are there any others? Well, 1 is excluded. 4,6,8,9,10,12,14,15 are composite. So no, those six numbers are all primes between 1 and 15. Wait, but the question says \"between 1 and 15.\" Does that include 15? Hmm, the wording is \"between 1 and 15,\" which could be bit ambiguous. But usually, when someone says \"between 1 and 15,\" they mean including both endpoints. However, since 15 is composite, its excluded either way. So primes are 2,3,5,7,11,13. Yes, that seems correct. dont think made any mistakes here. Let me cross-verify with another method, like the Sieve of Eratosthenes. The Sieve is method where you list all numbers and cross out multiples. Starting with 2, cross out multiples: 4,6,8,10,12,14. Then 3, cross out multiples: 6,9,12,15. Then 5, cross out 10,15. 7, cross out 14. 11 and 13 are primes, and their multiples are beyond 15. So remaining numbers are 2,3,5,7,11,13. Which matches our previous list. Therefore, the primes between 1 and 15 are 2,3,5,7,11,13. **Final Answer** The primes between 1 and 15 are 2 , 3 , 5 , 7 , 11 , and 13 . </think> To find all the prime numbers between 1 and 15, we start by recalling that prime number is natural number greater than 1 that has no positive divisors other than 1 and itself. We then check each number from 1 to 15: - **1**: Not prime number (by definition). - **2**: Prime (divisors: 1, 2). - **3**: Prime (divisors: 1, 3). - **4**: Not prime (divisible by 2). - **5**: Prime (divisors: 1, 5). - **6**: Not prime (divisible by 2, 3). - **7**: Prime (divisors: 1, 7). - **8**: Not prime (divisible by 2). - **9**: Not prime (divisible by 3). - **10**: Not prime (divisible by 2, 5). - **11**: Prime (divisors: 1, 11). - **12**: Not prime (divisible by 2, 3, 4, 6). - **13**: Prime (divisors: 1, 13). - **14**: Not prime (divisible by 2, 7). - **15**: Not prime (divisible by 3, 5). Using the Sieve of Eratosthenes method to cross out non-prime numbers confirms the primes listed above. Therefore, the prime numbers between 1 and 15 are: 2 , 3 , 5 , 7 , 11 , 13 20 SFT trace after rewritten by LLaMA 4 Maverick Problem: Find all primes between 1 and 15. <reasoning> To find all the prime numbers between 1 and 15, we recall that prime number is natural number greater than 1 that has no positive divisors other than 1 and itself. We then check each number from 1 to 15 to determine if it is prime. - **1**: Not prime number (by definition). - **2**: Prime (divisors: 1, 2). - **3**: Prime (divisors: 1, 3). - **4**: Not prime (divisible by 2). - **5**: Prime (divisors: 1, 5). - **6**: Not prime (divisible by 2, 3). - **7**: Prime (divisors: 1, 7). - **8**: Not prime (divisible by 2). - **9**: Not prime (divisible by 3). - **10**: Not prime (divisible by 2, 5). - **11**: Prime (divisors: 1, 11). - **12**: Not prime (divisible by 2, 3, 4, 6). - **13**: Prime (divisors: 1, 13). - **14**: Not prime (divisible by 2, 7). - **15**: Not prime (divisible by 3, 5). The Sieve of Eratosthenes method confirms the primes listed above. </reasoning> <answer> 2, 3, 5, 7, 11, 13 </answer>"
        }
    ],
    "affiliations": [
        "MIT",
        "Meta Superintelligence Labs",
        "Tsinghua University, College of AI",
        "UCLA"
    ]
}