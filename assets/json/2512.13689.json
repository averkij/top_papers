{
    "paper_title": "LitePT: Lighter Yet Stronger Point Transformer",
    "authors": [
        "Yuanwen Yue",
        "Damien Robert",
        "Jianyuan Wang",
        "Sunghwan Hong",
        "Jan Dirk Wegner",
        "Christian Rupprecht",
        "Konrad Schindler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT."
        },
        {
            "title": "Start",
            "content": "LitePT: Lighter Yet Stronger Point Transformer Yuanwen Yue1,2 Damien Robert3 Jianyuan Wang2 Christian Rupprecht2 Konrad Schindler1 Sunghwan Hong1 Jan Dirk Wegner3 1ETH Zurich 2University of Oxford 3University of Zurich 5 2 0 2 5 1 ] . [ 1 9 8 6 3 1 . 2 1 5 2 : r Figure 1. LitePT is lightweight, high-performance 3D point cloud architecture. Left: LitePT-S has 3.6 fewer parameters, 2 faster runtime and 2 lower memory footprint than the state-of-the-art Point Transformer V3, and is even more memory-efficient than classical convolutional backbones. Moreover, it remains fast and memory-efficient even when scaled up to 86M parameters (LitePT-L). Right: Already the smallest variant, LitePT-S, matches or outperforms state-of-the-art point cloud backbones across range of benchmarks."
        },
        {
            "title": "Abstract",
            "content": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has 3.6 fewer parameters, runs 2 faster, and uses 2 less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT. 1. Introduction Visual understanding of 3D point clouds is central to wide range of applications, including robotics [5, 86, 88, 97], autonomous driving [21, 68], localisation [45], mapping [52, 75, 77], and environmental monitoring [33, 64]. variety of deep learning architectures and neural processing layers for unstructured point clouds have been proposed, yet the field still lacks detailed understanding of their relative strengths and weaknesses, and principled guidelines on how to most efficiently combine them into versatile, highperformance architectures. Lately, Transformer-based models have dominated 3D In particular, their most recent incarnation benchmarks. Point Transformer V3 (PTv3) [84] has been shown to outperform earlier sparse convolutional [12, 22] and attentionbased models [25, 83, 98], and is considered the state of the art. Importantly, PTv3 is in fact not pure Transformer architecture: 67% of its parameters are allocated to (residual) sparse convolution layers. These are interleaved with the Transformer-style attention+MLP blocks and, among others, serve as form of positional encoding. That design, with both convolution and attention operations at all hierarchy levels (resp., depths) of U-net-like encoder-decoder scheme [58], is common in modern 3D point cloud architectures, which naturally leads to the question: what are the respective roles of convolution and attention? Here, we analyse the contribution and interplay of these layers in more detail. We find clear division of labour along the feature hierarchy. Early, high-resolution stages are dominated by the encoding of local geometry. Convolution or attention perform similarly well for that purpose, as the locality of convolutions is the right inductive bias. However, attention is substantially more expensive for early layers with high spatial resolutions (i.e., large number of tokens). Later, at lower-resolution stages, semantics and global context emerge. To capture the associated longrange interactions, the highly expressive attention mechanism is more suitable and also more parameter-efficient. As mentioned, in PTv3 and related architectures, the SparseConv [22] layer was primarily included to encode positional information. It turns out that, for that particular purpose, convolution is possible solution, but not necessity. We find that ROPE-inspired [67] query-key positional encoding, which we call PointROPE, fulfills the role more effectively, while being more efficient and introducing no learnable parameters. Overall, our analysis points to clear design principle: apply convolution when the focus is on local geometry, and attention when reasoning about semantics and global layout. Building on these insights, we design LitePT, hybrid network architecture for 3D point cloud analysis that leverages the computational tools in the most efficient manner; i.e., sparse convolutions in the early stages and PointROPEenhanced attention in the later stages. By tailoring the information processing to the level of abstraction, LitePT requires 3.6 fewer parameters than PTv3. Our architecture cuts memory consumption by 60.3% during training and by 51.2% during inference, and reduces latency by 34.5% during training and by 58.8% during inference. Remarkably, LitePT also improves performance compared to PTv3 across range of benchmarks on 3D semantic segmentation, 3D instance segmentation, and 3D object detection. 2. Related Work In line with the purpose of LitePT, we review deep learningbased point cloud representations, with specific focus on Transformer architectures and hybrid approaches. Deep Point Cloud Understanding. To take advantage of mature image-based networks, early approaches used to project 3D point clouds into 2D image planes and then leverage standard 2D CNNs to extract features [4, 9, 36, 40, 66, 79]. These projection-based methods tend to work well only when several implicit assumptions are met, e.g., relatively uniform point density, sufficient coverage, opaque surfaces, etc. Voxel-based methods transform irregular point clouds to regular voxel grids and then apply 3D convolution operations [26, 32, 42, 47, 65]. However, voxel representations are both computationally expensive and memoryintensive, motivating follow-up works to develop efficient sparse convolution frameworks [10, 12, 22, 51, 70]. Instead of projecting or quantising irregular point clouds into regular grids in 2D or 3D, point-based methods design operators that work directly on raw point coordinates, better preserving geometric information. Point operators have progressed from early MLP-based designs [17, 46, 5355, 95] to point convolutions [1, 23, 31, 41, 71, 81, 89], graph-based networks [39, 78], and, more recently, attention-based mechanisms [7, 25, 56, 57, 73, 83, 84, 98]. Among modern point cloud backbones, Transformer-based architectures represent the state of the art. Point Cloud Transformers. Transformer-based architectures employ the attention mechanism as their core feature extractor. To mitigate the quadratic complexity of global self-attention, most approaches adopt some form of windowed attention, restricted to local spatial neighbourhood. Point cloud Transformers mainly differ in how these localised attention patches are constructed to best balance performance and efficiency. Common strategies include knearest neighbour search [83, 93, 98], window or voxel partitioning [20, 43, 50, 69, 76, 91, 92, 96], superpoints [56, 57], and 1D sorting with space-filling curves [8, 84]. Such local attention mechanisms are often integrated with shifted patch grouping [92] and hierarchical architectures in the spirit of U-Net [58], so as to aggregate global context. Existing works typically apply attention at all stages of the hierarchical network. We argue that attention in shallow stages, where the number of tokens is large and local patterns dominate, is computationally inefficient and unnecessary, as seen in Secs. 3.1 and 4.1. Positional Encoding in Point Cloud Transformers. Attention does not take into account spatial layout; therefore, positional encoding plays an important role in Transformers. PTv1 [98] and PTv2 [83] employ relative positional encoding (RPE), where an MLP encodes relative positions between points. Stratified Transformer [37] and Swin3D [92] use contextual relative positional encoding (cRPE), which maintains three learnable look-up tables for the (x, y, z) axes that are computationally rather inefficient. OctFormer [76] and PTv3 [84] employ conditional positional encoding (CPE) [13], which is implemented via convolutional layer preceding each attention module. CPE improves efficiency, but introduces substantial number of learnable parameters. Here, we adapt rotary positional embedding (RoPE) [67] to point cloud learning, parameterfree module that offers both efficiency and strong empirical performance. Hybrid Models. Convolution is by design capable of capturing local features, whereas Transformers excel at modelling long-range dependencies. In the vision domain, since the introduction of the Vision Transformer [18], numerous studies have explored the integration of convolutional operators with attention for efficient image analysis [24, 48, 74, 80, 90]. Similarly, in the 3D point cloud field, several works have investigated hybrid architectures that combine the strengths of convolution and attention. Stratified Transformer [37] reports that KPConv [71] block provides substantially stronger local features than attention. Superpoint Transformer [56] leverages lightweight PointNet [53] to encode geometrically-homogeneous superpoints. PointConvFormer [82] and KPConvX [72] augment convolution kernels with attention to improve feature modelling. Following 2D vision, similar hybrid design has been employed in ConDaFormer [19], which adds two sparse convolution blocks before and after each attention module to better capture local structure. We note that PTv3 [84] is also arguably hybrid model, as it utilizes sparse convolutions as positional encoding, which account for the majority of its trainable parameters. While prior hybrid models typically adopt U-Net structure, they do not vary the layer design along the hierarchy. Their hybrid designs contain convolution and attention, but assemble them into fixed block structure that repeats uniformly throughout the hierarchy. In the present work, we rethink hybrid design from multi-scale perspective and decouple convolution and attention, allowing for the selective use of each at different hierarchy levels to exploit their complementary advantages. 3. Methodology Figure 2. PTv3 block. The block is composed of convolutional conditional positional encoding module followed by an attention module. To motivate our network design, we begin with an empirical study that investigates the respective roles of convolution and attention in PTv3 [84]. We then introduce the components of LitePT: computational blocks that are reduced to the essentials and tailored to different processing stages (Sec. 3.2); and an alternative, learning-free positional encoding for the simplified blocks (Sec. 3.3). Finally, we describe the overall architecture in Sec. 3.4. 3.1. Revisiting PTv3: Convolution vs. Attention and an attention module Preliminaries. PTv3 [84] represents the current state-ofthe-art architecture for point cloud understanding. Similar to earlier point cloud backbones [12, 56, 83, 98], it adopts U-Net architecture [58] composed of multiple encoder and decoder stages with skip connections. Between consecutive encoding (or decoding) stages, pooling (or unpooling) operations are applied to downsample (or upsample) the point cloud and its associated features. Each encoder and decoder stage consists of several blocks. Fig. 2 depicts single block as used in PTv3, consisting of convolutional positional . Inspired encoding module by [13], PTv3 adopts conditional positional encoding, implemented by prepending sparse convolution layer, linear projection, and LayerNorm, with skip connection, before each attention module. The attention module follows standard pre-norm structure [87], where self-attention is applied between local groups of points obtained via serialisation sorting, followed by multilayer perceptron (MLP). Conditional positional encoding, and in particular its sparse convolution layer, has proved to be an important part of the overall architecture, but its precise role remains somewhat unclear. Does it indeed just serve to encode the spatial layout of the tokens that flow through the attention layer, or does it actually act as local feature extractor in the spirit of classical convolutional networks? In the following, we analyse the parameter efficiency and the computational cost of different components along the U-Net hierarchy, revealing striking differences between the stages. ScanNet [14] NuScenes [6] Model #Params mIoU PTv3 [84] PTv3 w/o Transformer PTv3 w/o SPConv 1 2 46.1M 32.4M 15.4M 77.5 73.4 70.7 mIoU 80.4 76.1 74.9 Table 1. Revisiting PTv3. We evaluate two PTv3 variants: in 1, the attention and MLP modules are removed, and in 2, only the sparse convolution layers are removed. Number of parameters. An often overlooked, yet important fact is that 67% of the total parameter budget in PTv3 is spent on the sparse convolution layers of the positional encoding, while the Transformer part (i.e., attention and MLP) only accounts for 30% of the learnable parameters. Furthermore, the parameter count of the sparse convolution layers increases substantially with depth and is largest near (a) Breakdown of trainable parameters (b) Breakdown of latencies Figure 3. Parameter count and latency. E0-E4 denote encoder stages from shallow to deep, and D3-D0 denote decoder stages from deep to shallow. The length of each bar reflects the relative parameter count or latency of the corresponding module. Top: In PTv3, the positional encoding implemented via convolution block accounts for the majority of its parameters, particularly in the later stages. In contrast, our Point-ROPE is parameter-free. Bottom: The PTv3 latency map reveals the significant cost of early-stage attention. LitePT restricts attention to late stages, where it is most effective and less costly. the bottleneck, due to the high feature dimension of the late encoder and early decoder stages. See Fig. 3a. Latency. Fig. 3b graphically depicts the computational latency of attention and convolution across different network stages. Attention, with its quadratic computational complexity, accounts for the majority of the computational Importantly, that cost decreases as one progresses cost. towards deeper stages near the bottleneck, because hierarchical downsampling quadratically reduces the number of point tokens. Convolution vs. attention. So far, we have clarified that convolution accounts for the majority of trainable parameters, whereas attention dominates the computational cost, and that both vary strongly along the U-Net hierarchy. To separate the contributions of the two modules, we design two reduced variants of the PTv3 block. In the first one, we remove the attention modules. Using exclusively this variant degenerates to classical sparse U-Net strucIn the second variant, we remove only the ture [12, 22]. sparse convolution layer to obtain pure Transformer. Table 1 contrasts the semantic segmentation performance of the two variants for ScanNet [14] and NuScenes [6]. It turns out that removing convolutions causes larger performance drop than removing the attention modules, suggesting that the positional encoding actually does much of the heavy lifting. We visualise the learnt embeddings at each encoding stage for the three variants using PCA  (Fig. 4)  and find that distinct division of labour emerges along the hierarchy, regardless of whether convolution, attention, or both are used. Early stages primarily encode local geometry, later stages capture high-level semantics. Discussion. The above analysis leads us to the following hypotheses: 1. It may not be necessary to use both convolution and attention at every stage. In the early stages, which prioritise local feature extraction, convolution is adequate. In deep stages, where the focus is on long-range context and semantic concepts, attention is key. 2. It would be sweet spot in terms of efficiency if one could indeed avoid attention at early stages, where it is most expensive, and convolution at late stages, where it inflates the parameter count. 3. Pure attention blocks will require an alternative positional encodingbut storing spatial layout is apparently not the main function of the convolution, so more parameter-efficient replacement should be possible. Figure 4. Representations learnt by the hierarchical U-Net encoder. The hierarchical U-Net encoder exhibits an operatoragnostic feature hierarchy: shallow stages consistently encode local geometric structure, while semantics emerge in deeper stages. 3.2. Tailored Blocks for Different Network Stages Driven by the insights from the study described above, we propose simple yet effective design that retains only the Figure 5. LitePT-S architecture. Our model comprises five stages, employing convolution blocks in the early stages and Point-ROPE augmented attention blocks in the later ones. LitePT-S uses lightweight decoder. Alternatively, adding convolution or attention blocks symmetrically in the decoder produces LitePT-S*. essential operations in each stage. Convolutions are allocated to earlier stages with high spatial resolution and low channel depth, and attention is reserved for deep stages with only few, but high-dimensional tokens. Formally, let the hierarchical encoder consist of stages, where the i-th stage transforms the feature representation fi1 into fi via function Bi(): fi = Bi(fi1), = 1, ..., (1) Depending on the stage index, each block Bi is instantiated as either pure convolution or pure attention: (cid:40) Bi = ConvBlocki, AttnBlocki, if Lc if > Lc (2) Early stages (i Lc) operate on point sets with high spatial resolution and density, where local geometric reasoning is critical. Employing convolution layers in these stages efficiently aggregates information over local receptive fields, with minimal parameter overhead. As one progresses to deeper stages (i > Lc), the number of point tokens is greatly reduced and semantic abstraction becomes more important, hence one switches to attention-based blocks. Optionally, one can also include hand-over stage with both ConvBlocki and AttnBlocki. See ablation studies in Sec. 4.1. More gradual transitions between the two mechanisms are, in principle, possible, but unnecessarily complicate the design. Our LitePT follows different philosophy than PTv3 and other hybrid point cloud Transformers: [19, 72, 82] all uniformly repeat the same computational block at all stages; as consequence, that unit must include both attention and convolution. In contrast, we prefer to simplify individual blocks as much as possible, which then requires different forms of simplification depending on the network stage. Empirically, we find that strategically distributing custom blocks along the hierarchy yields higher performance with significantly lower memory footprint and computational cost. 3.3. Point Rotary Positional Embedding Discarding the expensive convolution layer at deep hierarchy levels has an undesired side effect: one loses the positional encoding. Hence, more parameter-efficient replacement is needed. Rotary Positional Embedding (RoPE) [67] has proven to be remarkably effective in natural language processing. In RoPE, relative positional awareness is introduced into the attention mechanism through rotations of the feature space. Originally, the method is designed for 1D sequence data. It does not have direct generalisation to irregular point clouds in 3D point space. We adapt RoPE to 3D in straightforward manner to obtain Point Rotary Positional Embedding (Point-ROPE). Given point feature vector fi Rd at position pi = (xi, yi, zi), we divide the embedding dimension into three equal subspaces corresponding to the x, y, and axes: fi = [f ; ; ], , x , Rd/3 . (3) We then independently apply the standard 1D RoPE embedding to each subspace, using the respective point coordinate, and concatenate the axis-wise embeddings to form the final point representation: fi = = i i i RoPE1D(f RoPE1D(f RoPE1D(f , xi) , yi) , zi) . (4) For each point with coordinates (xi, yi, zi), we directly use its grid coordinates as input, which are already correctly scaled during the pooling operation. The embedding scheme preserves the directional separability of 3D points while jointly encoding the features positional phase rotation, effectively capturing relative geometry. Compared to the learned convolutional positional encoding of PTv3 [84], Point-ROPE is parameter-free, lightweight, and, by construction, rotation-friendly. As part of our open source code, we provide an optimised CUDA implementation. 3.4. Architecture Our model follows the conventional U-Net [58] structure, with five stages. We build three variants of the encoder, with varying number of channels in each stage and blocks per stage. Note that must be divisible by 6 in stages that include PointROPE. LitePT-S: = (36, 72, 144, 252, 504), = (2, 2, 2, 6, 2) LitePT-B: = (54, 108, 216, 432, 576), = (3, 3, 3, 12, 3) LitePT-L: = (72, 144, 288, 576, 864), = (3, 3, 3, 12, 3) We use LitePT-S as the main variant for the experiments, since it already delivers excellent performance across all benchmarks. Model scaling is examined in Tab. 5. Per default, we set Lc = 3, meaning that stages 1, 2, 3 use ConvBlocki, while stages 4, 5 use AttnBlocki. Each ConvBlocki consists of sparse convolution layer, linear layer and LayerNorm, and has residual connection. Each AttnBlocki consists of PointROPE embedding followed by attention, where the latter is computed locally within groups of points, found with the same serialisation sorting as in PTv3 [84]. For semantic segmentation, we simplify the decoder to only the linear projection layer and LayerNorm in each stage. For instance segmentation, we apply the stage-specific design also in the decoder and symmetrically assign ConvBlocki and AttnBlocki, in reverse order of the encoder. 4. Experiments Training Inference Method #Params Latency Memory Latency Memory MinkUNet [84] PTv2 [84] PTv3 [84] 39.2M 60ms 12.8M 188ms 46.1M 110ms LitePT-S (Ours) LitePT-S* (Ours) LitePT-B (Ours) LitePT-L (Ours) 12.7M 16.0M 45.1M 85.9M 72ms 81ms 93ms 97ms 1.9G 22.8G 5.8G 2.3G 3.3G 5.5G 8.4G 21ms 151ms 51ms 21ms 26ms 33ms 41ms 2.4G 22.9G 4.1G 2.0G 2.0G 2.4G 2.6G Table 2. Efficiency comparison. Results are reported as average over the full ScanNet dataset using single RTX 4090 GPU. Automatic Mixed Precision (AMP) is enabled for all models during training and disabled during inference. * denotes our variant with heavier decoder that includes attention or convolutional blocks. We begin with series of ablation studies to analyse different configurations of our hybrid design, the models scaling behaviour, and PointROPE (Sec. 4.1). We then present comparisons with state-of-the-art methods for 3D semantic segmentation (Sec. 4.2), 3D instance segmentation (Sec. 4.3) and 3D object detection (Sec. 4.4). 4.1. Ablation Studies and Analysis Figure 6. Performance-efficiency trade off. Left: Progressively dropping attention in more of the early stages. Right: Progressively dropping convolution in more of the late stages. Are both convolution and attention needed at every stage? To verify our first hypothesis from Sec. 3.1, we design two sets of experiments on NuScenes. We begin with baseline model that incorporates both convolution and PointROPE attention at all stages. In Experiment 1, we progressively remove attention, first from stage 0, then from stages 0 and 1, etc. In Experiment 2, we progressively remove convolution, first from stage 4, then from stages 4 and 3, etc. We then plot the mIoU of those configurations against latency (resp. parameter count). As shown in Fig. 6 (left), removing attention in early stages boosts efficiency with almost no drop in mIoU, whereas removing attention in later stages harms performance. On the other hand, Fig. 6 (right) shows that removing convolution in later stages greatly reduces the parameter count with negligible change in mIoU, whereas removing convolution in early stages only marginally improves efficiency but adversely affects performance. The analysis confirms that one needs not include both convolution and attention at every stage. Their contribution and their cost highly depend on the hierarchy level. Where is the sweet spot in terms of efficiency and performance? To determine the optimal transition point Lc between convolution and attention, we conduct an ablation study on NuScenes as shown in Tab. 3. Optionally, we include hand-over stage, denoted by X, that includes both convolution and attention. Setting Lc = 3, i.e., convolution in the first three stages and attention in the last two, achieves the best trade-off between parameter count, latency, and mIoU. We adopt Lc = 3 as our default setting for all experiments. Decoder design. The mixed design with blocks tailored to the layer depth is always used in the U-Net encoder. On the contrary, we propose two design variants for the UNet decoder. In LitePT-S*, the same mixed design is used in the decoder, in reverse order. In LitePT-S, we further strip down the architecture and keep only linear projection layer per stage (as needed to integrate skip connections), making the method even more efficient. We find empirically that the optimal choice is task-dependent, as shown in Lc 0 1 2 3 4 Setting #Params Latency mIoU A-A-A-A-A C-A-A-A-A C-C-A-A-A C-C-C-A-A C-C-C-C-A C-C-C-C-C C-X-A-A-A C-C-X-A-A C-C-C-X-A 11.8M 11.9M 12.0M 12.7M 18.8M 26.9M 12.2M 13.2M 23.4M 35.1ms 30.4ms 25.8ms 21.5ms 16.2ms 13.5ms 30.9ms 26.7ms 24.9ms 82.1 81.7 82.0 82.2 80.9 75.4 81.9 82.3 82. Table 3. Effect of Lc and hand-over stage. C: convolutional block; A: attention block; X: both convolution and attention are used at that stage. We compare model variants and report latency, memory usage, and validation mIoU on the NuScenes dataset. The grey-shaded row is our recommended setting. Tab. 4. For semantic segmentation, the simple decoder is the best choice. For instance segmentation, the variant with convolution and attention blocks has noticeable edge. We point out that even the slightly heavier LitePT-S* is still lot more efficient than other Point Transformers (see Tab. 2), and leave the choice of decoder to the user. Semantic Segmentation (mIoU) Instance Segmentation (mAP50) Decoder ScanNet [14] Structured3D [99] NuScenes [6] Waymo [68] ScanNet [14] LitePT-S LitePT-S* 76.5 76.8 83.7 83.0 82.2 81.8 73.1 72. 62.2 64.9 Table 4. Decoder design. We compare two decoder variants: in LitePT-S*, we apply our stage-tailored design symmetrically to the decoder stages, while in LitePT-S, we retain only linear projection layers in all decoder stages. Model scaling. Due to the parameter-free PointROPE encoding, our model has substantially fewer trainable weights. This offers the possibility to repurpose the saved capacity and scale up LitePT. We assess scaling behaviour on Structured3D, the largest dataset in our evaluation suite. As shown in Tab. 5, the model scales favourably: increasing the model size from LitePT-S to LitePT-L continuously improves performance, with only modest increase in testtime latency and memory usage. Notably, even LitePT-L, with parameter count twice that of PTv3, still runs faster than PTv3 and has lower memory footprint. points in mIoU. We additionally ablate the base frequency d, which controls how fast each embedding dimension rotates as the position increases (uniformly for the three axes). PointROPE is fairly robust to the choice of frequency. Setting = 100 yields the best score; we fix that value for all datasets to avoid excessive hyperparameter tuning. w/o PointROPE = = 100 = 1000 = 10000 mIoU 79.6 81. 82.2 81.8 81.3 Table 6. PointROPE. Dedicated positional encoding is needed dropping PointROPE leads to significant performance drop. PointROPE works similarly well with wide range of base frequencies, the grey-shaded column is our recommended setting. 4.2. Semantic Segmentation NuScenes [6] Waymo [68] Method #Param mIoU mAcc mIoU mAcc MinkUNet [12] SPVNAS [70] Cylinder3D [100] AF2S3Net [11] SphereFormer [38] PTv2 [83] PTv3 [84] LitePT-S (Ours) 39.2M - - - - 12.8M 46.1M 12.7M 73.3 77.4 76.1 62.2 78.4 80.2 80.4 82. - - - - - - 87.2 88.1 65.9 - - - 69.9 70.6 71.3 73.1 76.6 - - - - 80.2 80.5 83. Table 7. Outdoor semantic segmentation on NuScenes and Waymo validation set. Scores of prior work courtesy of [84, 85]. Limited Scenes (Pct.) Limited Annotations (Pts.) Method #Params Full 1% 5% 10% 20% 50 100 MinkUNet [12] PTv2 [83] PTv3 [84] 39.2M 72.2 12.8M 75.4 46.1M 77.5 26.0 24.8 25.8 LitePT-S (Ours) LitePT-S* (Ours) 27.3 12.7M 76.5 16.0M 76.8 27.2 47.8 48.1 48.9 50.6 51.6 56.7 59.8 61.0 63.1 63.0 62.9 66.3 67. 67.3 67.1 41.9 58.4 60.1 62.5 63.2 53.9 62.2 66.1 70.3 67.9 71.4 68.4 70.9 72.0 69.5 65.5 71.2 72.7 72.8 74.2 Table 8. Indoor semantic segmentation on ScanNet validation set. In mean IoU. Scores of prior work courtesy of [84]. Val Test Method #Params mIoU mAcc mIoU mAcc MinkUNet [12] PTv2 [83] PTv3 [84] LitePT-S (Ours) 39.2M 12.8M 46.1M 12.7M 76.4 79.0 82.4 83.6 84.3 86.8 90. 90.7 77.4 78.5 82.1 82.4 85.5 86.6 90.3 90.3 #Params Latency Memory mIoU Table 9. Indoor semantic segmentation on Structured3D. Method PTv3 [84] LitePT-S (Ours) LitePT-B (Ours) LitePT-L (Ours) 46.1M 12.7M 45.1M 85.9M 57ms 23ms 36ms 44ms 5.83G 2.56G 2.60G 3.58G 82.4 83.6 85.1 85.4 Table 5. Model scaling on Structured3D dataset. Our model scales efficiently, achieving consistent performance gains from small to large variants with modest increases in latency and memory. Even when scaled to twice the parameters of PTv3, LitePT-L remains more efficient. PointROPE. In Tab. 6 we ablate the effectiveness of the proposed PointROPE, on NuScenes. Removing PointROPE leads to significant performance drop of 2.6 percentage Setup. We perform semantic segmentation for four different datasets. NuScenes [6] and Waymo [68] are two outdoor datasets of first-person driving scenes, captured with vehicle-mounted LiDAR. ScanNet [14] and Structured3D [99] show indoor settings. The former was captured using an RGB-D camera. It is relatively small by todays standards, comprising 1,201 training scenes. Structured3D is synthetic dataset and the largest public collection of 3D scenes with semantic annotations, and contains 18,348 training scenes. We follow PTv3 and use test time augmentation (TTA). Results without TTA can be found in the appendix. Results. Tab. 7 reports semantic segmentation results on the NuScenes and Waymo validation sets. LitePT achieves marked improvements over competing architectures, in both cases +1.8 mIoU. We note that automotive LiDAR has different, more challenging properties compared with indoor datasets: the model must learn to handle massive differences in point density due to the large range, and highly anisotropic point distributions due to the scan line pattern and frequent specular reflections and ray drops. Table 8 shows IoU scores for the ScanNet validation set. Following the literature [30], we also report results with limited training, obtained either by restricting the number of available training scenes or by reducing the number of annotated points per scene. The performance of LitePT is comparable to PTv3, which has 4 more parametersin data-constrained settings, even slightly betterand clearly superior to PTv2, which has similar parameter count. On the more than 10 larger Structured3D dataset, LitePT consistently outperforms all competing methods, including the much larger state-of-the-art PTv3. 4.3. Instance Segmentation ScanNet [14] ScanNet200 [59] PointGroup [35] #Params mAP25 mAP50 mAP mAP25 mAP50 mAP MinkUNet [12] PTv2 [83] PTv3 [84] 39.2M 12.8M 46.2M LitePT-S* (Ours) 16.0M 72.8 76.3 77. 78.5 56.9 60.0 61.7 64.9 36.0 38.3 40.9 41.7 32.2 39.6 40. 40.3 24.5 31.9 33.2 33.1 15.8 21.4 23.1 22.2 Table 10. Indoor instance segmentation on ScanNet and ScanNet200 validation set. Scores of prior work courtesy of [84]. Setup. We evaluate our method for instance segmentation on ScanNet [14] and ScanNet200 [59]. Following the protocol of prior work, we employ PointGroup [35] as instance segmentation head on top of the decoder to achieve fair comparison. Results. Tab. 10 summarise the results. On ScanNet, LitePT again outperforms all prior backbones and sets new state of the art, with 64.9 mAP50, +3.2 percentage point improvement over PTv3. On ScanNet200, which includes long tail of rare categories, the results are comparable to PTv3 and significantly better than all previous methods. For example, our method achieves 1.2% higher mAP50 than PTv2, which has similar parameter count, but 11 larger memory footprint and 6 longer runtime. 4.4. Object Detection Setup. We evaluate 3D object detection on Waymo. For fair comparison with prior work [43, 84], we employ the same 3D object detection framework, CenterPointPillar [94]. Consistent with [20, 43, 84], we avoid spatial downsampling, thus turning LitePT into single-stage network with 8 blocks, to allow detection of small objects. ObVehicle L2 Pedestrian L2 Cyclist L2 Mean Method mAP APH mAP APH mAP APH mAPH PointPillars [40] CenterPoint [94] SST [20] SST-Center [20] VoxSet [27] PillarNet [61] FlatFormer [43] PTv3 [84] LitePT (Ours) 63.6 66.7 64.8 66.6 66.0 70.4 69.0 71.2 71. 63.1 66.2 64.4 66.2 65.6 69.9 68.6 70.8 71.2 62.8 68.3 71.7 72.4 72.5 71.6 71.5 76.3 76.1 50.3 62.6 63.0 65.0 65.4 64.9 65.3 70.4 70. 61.9 68.7 68.0 68.9 69.0 67.8 68.6 71.5 71.8 59.9 67.6 66.9 67.6 67.7 66.7 67.5 70.4 70.7 57.8 65.5 64.8 66.3 66.2 67.2 67.2 70.5 70. Table 11. Outdoor object detection on Waymo with single frames input. Scores of prior work courtesy of [84]. jects are divided into two difficulty levels, and we report level-2 metrics. Results. Tab. 11 reports scores based on single-scan LiDAR inputs. Also in this application, LitePT reaches the highest score overall and on two out of three object categories, and comfortably matches the performance of the closest competitor, PTv3. 5. Conclusion and Discussion We have introduced LitePT, lighter yet stronger point Transformer for various point cloud processing tasks. Our starting point was the question, which distinct roles and impacts different operators have along the processing hierarchy. Experiments confirm that (sparse) convolutions are adequate, and more efficient, at early hierarchy levels, whereas attention comes into its own at higher levels, where semantic abstraction and global context over comparatively small token set are key. In itself, these observations are not unexpected, but surprisingly, they have not been leveraged in contemporary point cloud architectures. LitePT embodies the simple principle convolutions for low-level geometry, attention for high-level relations and strategically places only the required operations at each hierarchy level, avoiding wasted computations. To achieve this, we equip our method with parameter-free PointROPE positional encoding to compensate for the loss of spatial layout information that occurs when discarding convolutional layers. We hope that LitePT will be useful as generic high-performance backbone for 3D point cloud processing, and that our analysis can serve as practical guidance for architecture design beyond our current version. In our architecture, attention is applied only in the later stages, where the reduced token count is small. It would therefore be affordable to compute self-attention globally across all tokens, rather than locally. In future work, it may be interesting to eliminate the local grouping operation, which could on the one hand strengthen long-range context modelling, and on the other hand further reduce the computation time at inference. Acknowledgments. The project is partially supported by the Circular Bio-based Europe Joint Undertaking and its members under grant agreement No 101157488. Part of the compute is supported by the Swiss AI Initiative under project a144 and a154 on Alps. We thank Xiaoyang Wu, Liyan Chen and Liyuan Zhu for their help with the comparison to PTv3."
        },
        {
            "title": "Appendix",
            "content": "In this Appendix, we provide detailed architecture of LitePT (Sec. A), detailed experimental settings (Sec. B), additional experiments (Sec. C), and visualization of LitePTs predictions for 3D semantic segmentation, 3D instance segmentation, and 3D object detection (Sec. D). A. Detailed Architecture Our full architecture is shown in Fig. 7. It follows U-Netstyle [58] encoder-decoder design with skip connections, and is organized into five stages. Adjacent encoder (or decoder) stages are connected via pooling (or unpooling) blocks. We apply our stage-tailored design on the encoder: the first three stages use convolution blocks, while the final two use attention blocks. For LitePT-S/B/L, each stage in the decoder contains only an unpooling block. For LitePTS*, we mirror the stage-tailored design in the decoder as well. Detailed architecture specifications can be found in Tab. 12. Below, we describe each block type in detail. Attention block. Each attention block consists of PointROPE attention module and feed-forward network (FFN) module. Following the pre-norm [87] convention, LayerNorm [2] is placed before both the attention and FFN modules. The FFN uses hidden dimension four times larger than the channel dimension of its stage. We observe that adding an extra LayerNorm before the attention block further stabilizes the training. In the Point-ROPE attention module  (Fig. 8)  , input point features are projected to query (Q), key (K), and value (V) representations. PointROPE is computed from point coordinates and applied to and K, leaving unchanged. The resulting rotated and are fed into standard scaled dot-product multi-head attention together with V, followed by linear projection to produce the final output embeddings. Our PointROPE implementation is compatible with FlashAttention [15, 16, 60], which we use in our model. We apply PointROPE to locallyaggregated groups of 1024 points, formed using the same serialization sorting strategy as [84]. Convolution block. The convolution block includes of single sparse convolution layer [12, 22] with kernel size of 3 3 3, followed by linear projection layer and LayerNorm [2] layer. residual connection [28] links the blocks input and output. Pooling and unpooling blocks. We adopt the grid pooling and unpooling operation from [83]. During pooling, points are divided into non-overlapping partitions. Point features are first projected by linear layer, then points within the same partition are max-pooled, followed by GELU [29] activation and BatchNorm layer [34]. The pooling stride is set to 2 at each stage, reducing the spatial resolution by factor of 2 each time. During unpooling, point features from the current decoder stage and the corresponding encoder stage are each passed through their own linear layer, GELU activation, and BatchNorm. The resulting features are then merged through skip connection via summation. B. Detailed Experimental Settings For indoor datasets, we use RGB and surface normals as input features. For outdoor datasets, where RGB and normal information are unavailable, we use xyz coordinates and intensity (plus elongation for object detection). Following common practice [12, 83, 84], we first downsample the point cloud on grid. For 3D segmentation tasks, we set the grid size to 0.02m for indoor scenes and 0.05m for outdoor scenes. For 3D object detection, we adopt grid sizes of 0.32m in the xy plane and 6m along the axis, consistent with [43, 84]. Detailed training configurations for semantic segmentation, instance segmentation and object detection are provided in Tab. 13 Tab. 14, and Tab. 15, respectively. C. Additional Experiments C.1. Further Ablation on PointROPE Spherical vs. Cartesian coordinates. In PointROPE, we divide each pointfeature embedding into three equal subspaces and then apply the standard 1D ROPE [67] embedding to each subspace using the respective Cartesian coordinates. Here, we investigate an alternative design that uses spherical coordinates. Specifically, we transform each point (xi, yi, zi) into spherical coordinates (ri, θi, ϕi), using the mean of all points as the origin. We then apply 1D ROPE using ri, θi and ϕi separately and concatenate the resulting embeddings. The motivation is that spherical coordinates decouple radial distance and angular structure, which could potentially make positional relationships easier to learn. However, as shown in Tab. 16, we empirically find that PointROPE in spherical coordinates is effective but offers no improvement over Cartesian coordinates, while adding additional computational overhead. Therefore, we retain our simpler per-axis Cartesian design. Subdivision of the input space. For each attention head (with head dimension 18), we split the embedding evenly across three axes (xi, yi, zi). Here we explore the impact of different subdivisions on each axis. In addition to equal split (6 : 6 : 6), we try emphasizing the axis (4 : 4 : 10) and emphasizing the xy axes (8 : 8 : 2). As shown in Tab. 16, uneven Figure 7. Detailed architectures. We illustrate the full pipelines of LitePT-S, LitePT-S*, Point Transformer V3 [84], and the building blocks of each architecture. 84] to ensure fair comparison. The testing pipeline applies chunking and test-time augmentations (TTA). Specifically, each augmented sample is partitioned into overlapping chunks, ensuring that every point is assigned to at least one chunk during grid sampling. The model is then run on each chunk individually, and the final label of each point is aggregated by voting across the predictions from all chunks it appears in. Although this multi-run and TTA protocol is common practice and is known to boost performance [62], it obscures the intrinsic merits of the underlying backbone. To communicate performance in simpler single-pass setting useful for downstream users, we additionally report results for PTv3 and LitePT-S without TTA or chunking in Tab. 17. Overall, removing chunking and TTA reduces performance by roughly 2% mIoU for both methods. D. Visualization Figure 8. PointROPE attention. We apply PointROPE to query and key before standard scaled dot-product attention. splits lead to suboptimal performance compared with equal weighting. This suggests that positional information along all three axes is similarly important, and manual reweighting is unnecessary. C.2. Chunking and Test-Time Augmentation In the main paper, we report semantic segmentation results following the same evaluation protocol as prior works [83, We visualize sample predictions of LitePT on three tasks: 3D semantic segmentation (Figs. 9 to 12), 3D instance segmentation  (Fig. 13)  , and 3D object detection  (Fig. 14)  . stem E1 E2 E3 E4 LitePT-S LitePT-S* LitePT-B LitePT-L = 36, = 555 = 36 = 36, = 555 = 36 2 = 36, = 555 = 54 3 = 333 = 333 = 333 = 36, = 555 = 72 3 = 333 pool stride 2 = 72 pool stride 2 = 144 2 pool stride 2 = 72 pool stride 2 = 144 2 pool stride 2 = 108 3 pool stride 2 = 216 3 pool stride 2 = 144 3 pool stride 2 = 3 = 333 = 333 = 333 = 333 = 33 = 333 = 333 = 333 pool stride 2 pool stride 2 pool stride pool stride 2 = 252, = = 100, = 4 = 1024 pool stride 2 = 504, = 28 = 100, = 4 = 6 = 252, = 14 = 100, = 4 = 1024 pool stride 2 = 504, = 28 = 100, = = 1024 2 = 432, = = 100, = 4 = 1024 pool stride 2 = 576, = 32 = 100, = 4 = 12 = 576, = = 100, = 4 = 1024 12 3 pool stride 2 = 864, = = 100, = 4 = 1024 3 unpool = 252 unpool = 252 = 252, = 14 = 100, = 4 = 1024 2 D2 D1 unpool = 144 unpool = D0 unpool = 72 #Params 12.7M unpool = 144 = = 333 2 unpool = 72 = 72 = 333 unpool = 72 = 72 = 333 16.0M 2 unpool = unpool = 576 unpool = 216 unpool = 288 unpool = 108 unpool = 144 unpool = unpool = 72 45.1M 85.9M Table 12. Detailed architecture specifications. C: channel dimension, K: kernel size in the convolution block, H: number of head, b: base frequency of PointROPE, F: MLP ratio in the FFN module, N: number of points in local group. NuScenes [6] Waymo [68] ScanNet [14] Structured3D [99] mIoU mAcc Input feature Grid size Head (framework) Loss Optimizer Weight decay Scheduler Learning rate Block lr rate Batch size Epochs Num GPUs XYZ+Intensity 0.05m Linear segmentor CrossEntropy+Lovasz [3] AdamW [44] 0.005 OneCycleLR [63] 0.002 0.0002 12 50 4 RGB+Normal 0.02m Linear segmentor CrossEntropy+Lovasz [3] AdamW [44] 0.05 OneCycleLR [63] 0.012 0.0012 48 200 0.006 0.0006 12 1200 (800) 4 Data augmentation Random rotate, random scale random flip, random jitter Random shift, random dropout random rotate, random scale random flip, random jitter elastic distortion, color auto contrast color jitter, sphere crop color normalization Table 13. Detailed training settings for semantic segmentation. Input feature Grid size Head (framework) Loss Optimizer Weight decay Scheduler Learning rate Block lr rate Batch size Epochs Num GPUs Data augmentation ScanNet [14] ScanNet200 [59] RGB+Normal 0.02m PointGroup [35] check PointGroup [35] AdamW [44] 0.05 OneCycleLR [63] 0.006 0.0006 12 800 4 Random shift, random dropout random rotate, random scale random flip, random jitter elastic distortion, color auto contrast color jitter sphere crop, color normalization Table 14. Detailed training settings for instance segmentation. Input feature Grid size Head (framework) Loss Optimizer Weight decay Scheduler Learning rate Block lr rate Batch size Epochs Num GPUs Data augmentation Object Detection Waymo [68] XYZ+Intensity+Elongation (0.32m, 0.32m, 6.0m) CenterPoint-Pillar [40] check CenterPoint-Pillar [40] Adam [49] 0.01 OneCycleLR [63] 0.006 0.006 64 40 16 Random flip, random rotate random scale Table 15. Detailed training settings for object detection."
        },
        {
            "title": "References",
            "content": "w/o PointROPE Spherical Cartesian : : = 6 : 6 : 6 : : = 4 : 4 : 10 : : = 8 : 8 : 2 79.6 80.7 82.2 82.2 80.3 80. 86.5 87.1 88.1 88.1 86.8 86.7 Table 16. Additional ablation on PointROPE on NuScenes. Method PTv3 [84] LitePT-S PTv3 [84] (w/o chunking and TTA) LitePT-S (w/o chunking and TTA) #Param mIoU mAcc 46.1M 12.7M 46.1M 12.7M 80.4 82.2 78.3 80.4 87.2 88.1 86.0 86.9 Table 17. Semantic segmentation on NuScenes without chunking and TTA. ACM Transactions on Graphics (TOG), 2018. [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer Normalization. arXiv preprint arXiv:1607.06450, 2016. 9 [3] Maxim Berman, Amal Rannen Triki, and Matthew Blaschko. The Lovasz-Softmax Loss: Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 12 [4] Alexandre Boulch, Joris Guerry, Bertrand Le Saux, and Nicolas Audebert. Snapnet: 3D Point Cloud Semantic Labeling with 2D Deep Segmentation Networks. Computers & Graphics, 2018. 2 [5] Finn Lukas Busch, Timon Homberger, Jesus OrtegaPeimbert, Quantao Yang, and Olov Andersson. One Map to Find them All: Real-time Open-vocabulary Mapping for Zero-shot Multi-object Navigation. In International Conference on Robotics and Automation (ICRA), 2025. 1 [6] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: MulIn IEEE/CVF timodal Dataset for Autonomous Driving. Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3, 4, 7, 12 [7] Liyan Chen, Gregory Meyer, Zaiwei Zhang, Eric Wolff, and Paul Vernaza. Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [8] Wanli Chen, Xinge Zhu, Guojin Chen, and Bei Yu. Efficient In European Point Cloud Analysis Using Hilbert Curve. Conference on Computer Vision (ECCV), 2022. 2 [9] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-View 3D Object Detection Network for Autonomous Driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [1] Matan Atzmon, Haggai Maron, and Yaron Lipman. Point Convolutional Neural Networks by Extension Operators. [10] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. LargeKernel3D: Scaling Up Kernels in 3D Sparse CNNs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [11] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and Bingbing Liu. (AF)2-S3Net: Attentive Feature Fusion with Adaptive Feature Selection for Sparse Semantic Segmentation Network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [12] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 3, 4, 7, 8, 9 [13] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional Positional Encodings for Vision Transformers. International Conference on Learning Representations (ICLR), 2023. 3 [14] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3, 4, 7, 8, 12 [15] Tri Dao. FlashAttention-2: Faster Attention with BetarXiv preprint ter Parallelism and Work Partitioning . arXiv:2307.08691, 2023. 9 [16] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Advances in Neural Information Processing Systems (NeurIPS), 2022. [17] Hao Deng, Kunlei Jing, Shengmei Cheng, Cheng Liu, Jiawei Ru, Jiang Bo, and Lin Wang. LinNet: Linear Network for Efficient Point Cloud Representation Learning. Advances in Neural Information Processing Systems (NeurIPS), 2024. 2 [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An Image is Worth 1616 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations (ICLR), 2021. 3 [19] Lunhao Duan, Shanshan Zhao, Nan Xue, Mingming Gong, Gui-Song Xia, and Dacheng Tao. ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding. Advances in Neural Information Processing Systems (NeurIPS), 2023. 3, 5 [20] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Embracing Single Stride 3D Object Detector with In IEEE/CVF Conference on ComSparse Transformer. puter Vision and Pattern Recognition (CVPR), 2022. 2, 8 [21] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2012. 1 [22] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2, 4, 9 [23] Fabian Groh, Patrick Wieschollek, and Hendrik PA Lensch. In Asian Conference on Computer ViFlex-Convolution. sion (ACCV), 2018. 2 [24] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. CMT: Convolutional Neural Networks Meet Vision Transformers . In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [25] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph Martin, and Shi-Min Hu. PCT: Point Cloud Transformer. Computational Visual Media, 2021. 2 [26] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. OccuSeg: Occupancy-aware 3D Instance Segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [27] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. Voxel Set Transformer: Set-to-Set Approach to 3D Object DeIn IEEE/CVF Conference on tection From Point Clouds. Computer Vision and Pattern Recognition (CVPR), 2022. 8 [28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 9 [29] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016. 9 [30] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining Xie. Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 8 [31] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. PointIn IEEE/CVF wise Convolutional Neural Networks. Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [32] Jing Huang and Suya You. Point Cloud Labeling Using 3D In International ConferConvolutional Neural Network. ence on Pattern Recognition (ICPR), 2016. 2 [33] Jakob Iglhaut, Carlos Cabo, Stefano Puliti, Livia Piermattei, James OConnor, and Jacqueline Rosette. Structure from Motion Photogrammetry in Forestry: Review. Current Forestry Reports, 2019. 1 [34] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal In International Conference on Machine Covariate Shift. Learning (ICML), 2015. 9 [35] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 8, 12 [36] Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, and Siddhartha Chaudhuri. 3D Shape Segmentation with Projective Convolutional Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [37] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified TransIn IEEE/CVF former for 3D Point Cloud Segmentation. Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3 [38] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia. Spherical Transformer for LiDAR-Based 3D Recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 7 [39] Loic Landrieu and Martin Simonovsky. Large-Scale Point Cloud Semantic Segmentation with Superpoint Graphs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [40] Alex Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, PointPillars: Fast Jiong Yang, and Oscar Beijbom. Encoders for Object Detection from Point Clouds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 8, 12 [41] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. PointCNN: Convolution On XTransformed Points. Advances in Neural Information Processing Systems (NeurIPS), 2018. [42] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. PointVoxel CNN for Efficient 3D Deep Learning. Advances in Neural Information Processing Systems (NeurIPS), 2019. 2 [43] Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, and Song Han. FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 8, 9 [44] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. International Conference on Learning Representations (ICLR), 2019. 12 [45] Kan Luo, Hongshan Yu, Xieyuanli Chen, Zhengeng Yang, Jingwen Wang, Panfei Cheng, and Ajmal Mian. 3D Point Cloud-based Place Recognition: Survey. Artificial Intelligence Review, 2024. 1 [46] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking Network Design and Local Geometry in Point Cloud: Simple Residual MLP Framework. International Conference on Learning Representations (ICLR), 2022. 2 [47] Daniel Maturana and Sebastian Scherer. VoxNet: 3D Convolutional Neural Network for Real-Time Object Recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015. 2 [48] Sachin Mehta and Mohammad Rastegari. MobileViT: Light-Weight, General-Purpose, and Mobile-Friendly ViInternational Conference on Learning sion Transformer. Representations (ICLR), 2022. [49] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. International Conference on Learning Representations (ICLR), 2015. 12 [50] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast Point Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [51] Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, and Jiaya Jia. OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic SegmentaIn IEEE/CVF Conference on Computer Vision and tion. Pattern Recognition (CVPR), 2024. 2 [52] Patrick Pfaff, Rudolph Triebel, Cyrill Stachniss, Pierre Lamon, Wolfram Burgard, and Roland Siegwart. Towards Mapping of Cities. In International Conference on Robotics and Automation (ICRA), 2007. 1 [53] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2, 3 [54] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. PointNet++: Deep Hierarchical Feature Learning on Point Sets in Metric Space. Advances in Neural Information Processing Systems (NeurIPS), 2017. [55] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard PointNeXt: Revisiting PointNet++ with ImGhanem. proved Training and Scaling Strategies. Advances in Neural Information Processing Systems (NeurIPS), 2022. 2 [56] Damien Robert, Hugo Raguet, and Loic Landrieu. Efficient 3D Semantic Segmentation with Superpoint Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, [57] Damien Robert, Hugo Raguet, and Loic Landrieu. Scalable 3D Panoptic Segmentation As Superpoint Graph Clustering. In International Conference on 3D Vision (3DV), 2024. 2 [58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. UNet: Convolutional Networks for Biomedical Image SegIn International Conference on Medical Immentation. age Computing and Computer-Assisted Intervention ( MICCAI), 2015. 2, 3, 6, 9 [59] David Rozenberszki, Or Litany, and Angela Dai. Language-Grounded Indoor 3D Semantic Segmentation in In European Conference on Computer Vision the Wild. (ECCV), 2022. 8, 12 [60] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Lowprecision. Advances in Neural Information Processing Systems (NeurIPS), 2024. 9 [61] Guangsheng Shi, Ruifeng Li, and Chao Ma. PillarNet: Real-Time and High-Performance Pillar-Based 3D Object In European Conference on Computer Vision Detection. (ECCV), 2022. [62] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556, 2014. 10 [63] Leslie Smith and Nicholay Topin. Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications, 2019. 12 [64] Hongli Song, Weiliang Wen, Sheng Wu, and Xinyu Guo. Comprehensive Review on 3D Point Cloud Segmentation in Plants. Artificial Intelligence in Agriculture, 2025. 1 [65] Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, and Thomas Funkhouser. Semantic Scene In IEEE/CVF Completion from Single Depth Image. Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [66] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-View Convolutional Neural NetIn International Conworks for 3D Shape Recognition. ference on Computer Vision (ICCV), 2015. 2 [67] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. Neurocomputing, 2024. 2, 3, 5, 9 [68] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, 7, 12 [69] Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and Dragomir Anguelov. SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds. In European Conference on Computer Vision (ECCV), 2022. [70] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution. In European Conference on Computer Vision (ECCV), 2020. 2, 7 [71] Hugues Thomas, Charles Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, and Leonidas Guibas. KPConv: Flexible and Deformable Convolution for Point Clouds. In International Conference on Computer Vision (ICCV), 2019. 2, 3 [72] Hugues Thomas, Yao-Hung Hubert Tsai, Timothy Barfoot, and Jian Zhang. KPConvX: Modernizing Kernel Point Convolution with Kernel Attention. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 5 [73] Tuan Anh Tran, Duy Minh Ho Nguyen, Hoai-Chau Tran, Michael Barz, Khoa D. Doan, Roger Wattenhofer, Vien Anh Ngo, Mathias Niepert, Daniel Sonntag, and Paul Swoboda. How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need? Advances in Neural Information Processing Systems (NeurIPS), 2025. 2 [74] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. MaxViT: In European Conference Multi-axis Vision Transformer. on Computer Vision (ECCV), 2022. 3 [75] Nina Varney, Vijayan Asari, and Quinn Graehling. DALES: Large-scale Aerial LiDAR Data Set for Semantic Segmentation. CVPR Workshops, 2020. 1 [76] Peng-Shuai Wang. OctFormer: Octree-based Transformers for 3D Point Clouds. ACM Transactions on Graphics (TOG), 2023. [77] Ruisheng Wang, Jiju Peethambaran, and Dong Chen. Lidar Point Clouds to 3-D Urban Models: Review. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018. 1 [78] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay Sarma, Michael Bronstein, and Justin Solomon. Dynamic Graph CNN for Learning on Point Clouds. ACM Transactions on Graphics (TOG), 2019. 2 [79] Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D Lidar Point Cloud. In International Conference on Robotics and Automation (ICRA), 2018. 2 [80] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT: Introducing Convolutions to Vision Transformers. In International Conference on Computer Vision (ICCV), 2021. 3 [81] Wenxuan Wu, Zhongang Qi, and Li Fuxin. PointConv: Deep Convolutional Networks on 3D Point Clouds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [82] Wenxuan Wu, Li Fuxin, and Qi Shan. PointConvFormer: In IEEE/CVF Revenge of the Point-based Convolution. Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3, [83] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point Transformer V2: Grouped Vector Attention and Partition-Based Pooling. Advances in Neural Information Processing Systems (NeurIPS), 2022. 2, 3, 7, 8, 9, 10 [84] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point Transformer V3: Simpler, Faster, In IEEE/CVF Conference on Computer Vision Stronger. and Pattern Recognition (CVPR), 2024. 2, 3, 6, 7, 8, 9, 10, 12 [85] Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, and Julian Straub. Sonata: SelfSupervised Learning of Reliable Point Representations. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 7 [86] Kai Wurm, Armin Hornung, Maren Bennewitz, Cyrill Stachniss, and Wolfram Burgard. OctoMap: Probabilistic, Flexible, and Compact 3D Map Representation for Robotic Systems. In International Conference on Robotics and Automation (ICRA), 2010. 1 [87] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On Layer Normalization in the Transformer Architecture. In International Conference on Machine Learning (ICML), 2020. 3, 9 [88] Shengdong Xu, Dominik Honegger, Marc Pollefeys, and Lionel Heng. Real-time 3D navigation for autonomous In IEEE/RSJ International Confervision-guided MAVs. ence on Intelligent Robots and Systems (IROS), 2015. 1 [89] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filter. In European Conference on Computer Vision (ECCV), 2018. [90] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias. Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [91] Yu-Qi Yang, Yu-Xiao Guo, and Yang Liu. Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding. Computational Visual Media, 2025. 2 [92] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3D: Pretrained Transformer Backbone for 3D Indoor Scene Understanding. Computational Visual Media, 2025. 2 [93] Zetong Yang, Li Jiang, Yanan Sun, Bernt Schiele, and Jiaya Jia. Unified Query-Based Paradigm for Point Cloud Understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [94] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-Based 3D Object Detection and Tracking. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 8 [95] Ziyin Zeng, Mingyue Dong, Jian Zhou, Huan Qiu, Zhen Dong, Man Luo, and Bijun Li. DeepLA-Net: Very Deep Local Aggregation Networks for Point Cloud Analysis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [96] Cheng Zhang, Haocheng Wan, Xinyi Shen, and Zizhao Wu. PatchFormer: An Efficient Point Transformer with Patch Attention. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [97] Ji Zhang and Sanjiv Singh. LOAM: Lidar odometry and mapping in real-time. In Robotics: Science and Systems, 2014. 1 [98] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point Transformer. In International Conference on Computer Vision (ICCV), 2021. 2, 3 [99] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3D: Large Photo-Realistic Dataset for Structured 3D Modeling. In European Conference on Computer Vision (ECCV), 2020. 7, 12 [100] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 7 barrier bicycle bus car construction vehicle motorcycle pedestrian traffic cone trailer truck driveable surface other flat surface sidewalk terrain manmade vegetation unlabelled . . . 4 9 9 4 4 4 9 d . . . 2 4 7 6 1 8 7 6 2 . . . 0 6 9 4 0 5 2 3 9 3 8 5 . . . 8 2 2 4 8 8 7 7 0 4 6 6 . . . 4 5 8 4 8 6 6 4 4 8 7 8 . . . 1 9 4 2 9 9 5 1 1 9 4 0 (a) Input (b) Prediction (c) Ground Truth Figure 9. NuScenes semantic segmentation. We present various scenes of the nuScenes dataset: the input point cloud colored by LiDAR intensity, the semantic segmentation from LitePT-S, and the corresponding ground truth. car truck bus other vehicle motorcyclist bicyclist pedestrian sign traffic light traffic pole construction cone bicycle motorcycle building vegetation tree trunk curb road lane marker other ground walkable sidewalk unlabelled . . . 4 4 8 3 9 9 3 3 4 9 2 2 7 7 0 3 . . . 2 7 4 7 8 9 8 7 7 6 5 5 6 5 9 . . . 5 0 4 6 6 2 8 1 2 8 8 4 1 4 0 9 . . . 5 1 7 3 1 5 6 7 3 0 1 1 . . . 0 5 5 7 8 2 8 8 1 1 1 2 5 2 8 1 . . . 4 2 2 8 5 0 7 0 2 2 9 3 3 3 8 1 (a) Input (b) Prediction (c) Ground Truth Figure 10. Waymo semantic segmentation. We present various scenes of the Waymo dataset: the input point cloud colored by LiDAR intensity, the semantic segmentation from LitePT-S, and the corresponding ground truth. wall floor cabinet bed chair sofa table door window bookshelf picture counter desk curtain refrigerator shower toilet sink bathtub other furniture unlabelled 0 0 0 3 0 0 c 0 9 6 1 0 c 2 0 8 7 3 0 c 2 0 6 0 4 0 c 1 5 4 6 0 c 0 0 1 5 6 0 c (a) Input (b) Prediction (c) Ground Truth Figure 11. ScanNet semantic segmentation. We present various scenes of the ScanNet dataset: the input point cloud, the semantic segmentation from LitePT-S, and the corresponding ground truth. wall floor cabinet bed chair sofa table door window picture desk shelves curtain dresser pillow mirror ceiling refrigerator television nightstand sink lamp other structure other furniture other properties 5 6 7 8 r 2 2 0 3 0 c 1 0 4 r 4 3 0 3 0 c 0 6 5 r 3 1 1 3 0 c 4 6 7 1 r 5 9 1 3 0 c 4 9 8 4 r 3 2 2 3 0 c 6 4 8 2 r 7 3 2 3 0 c (a) Input (b) Prediction (c) Ground Truth Figure 12. Structured3D semantic segmentation. We present various scenes of the Structured3D dataset: the input point cloud, the semantic segmentation from LitePT-S, and the corresponding ground truth. 1 0 1 1 0 0 c 0 0 4 6 1 0 c 2 0 1 9 5 0 c 0 0 1 2 6 0 c 1 0 5 4 6 0 c 2 0 1 5 6 0 c (a) Input (b) Prediction (c) Ground Truth Figure 13. ScanNet instance segmentation. We present various scenes of the ScanNet dataset: segmentation from LitePT-S*, and the corresponding ground truth. Colors for each instance are randomly assigned. the input point cloud, the instance vehicle pedestrian cyclist (a) Input (b) Prediction (c) Ground Truth . . . 6 1 4 5 0 6 7 5 6 9 3 9 7 7 0 . . . 8 4 6 3 7 9 3 6 8 6 8 8 1 2 6 6 . . . 2 7 4 7 8 9 8 7 7 6 5 5 6 5 9 8 . . . 8 8 3 8 2 4 3 0 3 8 8 6 3 3 3 1 . . . 4 8 7 7 1 4 0 6 7 9 9 6 5 3 3 1 . . . 6 8 5 0 2 4 0 6 7 0 0 0 0 3 4 1 Figure 14. Waymo object detection. We present various scenes of the Waymo dataset: the input point cloud, the object detections from LitePT, and the corresponding ground truth."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "University of Oxford",
        "University of Zurich"
    ]
}