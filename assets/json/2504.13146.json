{
    "paper_title": "Antidistillation Sampling",
    "authors": [
        "Yash Savani",
        "Asher Trockman",
        "Zhili Feng",
        "Avi Schwarzschild",
        "Alexander Robey",
        "Marc Finzi",
        "J. Zico Kolter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. \\emph{Antidistillation sampling} provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 6 4 1 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Antidistillation Sampling",
            "content": "Yash Savani Asher Trockman Zhili Feng Avi Schwarzschild Alexander Robey Marc Finzi J. Zico Kolter Carnegie Mellon University {ysavani, ashert}@cs.cmu.edu https://antidistillation.com"
        },
        {
            "title": "Abstract",
            "content": "Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying models next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the models practical utility."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) trained to produce extended reasoning traces demonstrate impressive performance across math, coding, and general reasoning benchmarks [e.g 1]. These generated traces, however, serve dual purposenot only do they enhance model capabilities, but they also facilitate model distillation, wherein secondary model learns to replicate the original models abilities by training on its generated traces [2, 3]. Notably, model distillation enables substantial capability gains in the secondary model at fraction of the computational cost required for training similarly performant model from scratch [3]. Despite the benefits of model distillation, the effectiveness and efficiency of this technique present several downsides for companies serving frontier reasoning models. First, returning extended reasoning traces constitutes forfeiture of valuable intellectual property, enabling competitors to cheaply replicate frontier capabilities. Second, the possibility of distillation incentivizes frontier model providers to hide token probabilities, summarize reasoning traces, or, more generally, restrict user-model interaction. And finally, safe model behaviors (e.g., resistance to jailbreaking attempts [4, 5]) are often not inherited by distilled models, enabling the generation of objectionable content [6]. To address these issues, we introduce antidistillation samplinga method designed to reduce the effectiveness of model distillation. The core idea underpinning antidistillation sampling is to adjust reasoning models sampling distribution so that generated traces (1) poison distillation attempts while (2) maintain high likelihood under the original, unadjusted distribution. This approach protects proprietary capabilities while preserving the original models utility for downstream applications. Equal contribution. Preprint. Under review. Figure 1: Reasoning traces generated via antidistillation sampling poison distillation attempts, while simultaneously preserving the teachers downstream performance. The top and bottom rows shows results for MATH [7] and GSM8K [8], respectively. The left columns shows teacher accuracies under different sampling schemes, and the right column shows the performance of students distilled on traces sampled via these different sampling schemes. Notably, for the same teacher performance on both datasets, antidistillation sampling significantly degrades the performance of the distilled models relative to temperature sampling. 1.1 Related Work Various frontier AI labs acknowledge and benefit from the effectiveness of model distillation. For instance, OpenAI offers model distillation as service within their API [9]. The recognition of model distillations potential, which motivates such pipelines, dates back at least to Schmidhuber [10]. More recently, Hinton et al. [2] demonstrated that distilled specialist models can achieve impressive performance in multiple domains. Since then, growing body of work aims to understand the transference of capabilities via distillation [1114]. In fact, there is speculation that some labs train commercial-facing LLMs in part via distillation, possibly by harvesting extended reasoning traces generated from models belonging to their direct competitors [15]. This practice constitutes strategic vulnerability for frontier model maintainers, given the demonstrated value of these reasoning traces, and underscores the importance of the algorithm we introduce in this work. The threat model we address in this paperwhere student model is trained on data generated by teacher modelintersects with several aspects of model privacy and security. For instance, model extraction attacks acquire weights via query-level access without training or distillation [16] and training data extraction attacks harvest training data directly from frontier models [17]. While antidistillation sampling may offer some protection against these attacks, such analysis remains beyond our current scope. More relevant is the literature on data poisoning, where maliciously crafted data is injected into models training set to induce specific unintended downstream effects (see, e.g., [18]). Rando and Tramèr [19] even show the effectiveness of adding backdoors to preference data, sabotaging models fine-tuned with RLHF. Our contribution bridges data poisoning and privacy techniques to protect the valuable knowledge encoded in frontier models. Finally, we position antidistillation sampling within the broader framework of controlled decoding for LLMs [20], where supplementary objectives steer the decoding process. Existing approaches in this domain include leveraging contrastive objectives to enhance generation quality [21], reformulating constrained decoding as an optimization problem [22], and 2 incorporating energy-based constraints [23]. While related, antidistillation sampling solves different problem: by implementing new, distillation-aware penalization term in the decoding objective, our approach poisons generated reasoning traces to undermine the performance of models fine-tuned on these outputs."
        },
        {
            "title": "2 Antidistillation Sampling",
            "content": "To introduce and derive antidistillation sampling, we first provide high-level sketch, followed by the desiderata outlining the desired qualities for poisoning distillation attempts. We then derive the antidistillation sampling method in 2.2. Algorithm 1 summarizes the key steps involved in implementing our approach. An overview of our approach. The core objective of antidistillation sampling is to adjust models next-token distribution to balance two competing goals: sampling tokens with high likelihood under the original, unadjusted distribution and sampling tokens that effectively poison distillation attempts. Throughout, we refer to the model from which reasoning traces are sampled as the teacher, and the model being distilled as the student. We begin by introducing notation describing sampling from the teachers unadjusted distribution. Next, we quantify how model distillation impacts the student models performance on given downstream task. This analysis yields key insightwe can incorporate this performance metric directly into the teachers sampling distribution. This takes the form of directional derivative capturing the change in the teachers sampling distribution along the update direction in the students weight space. However, due to the high cost needed to compute this directional derivative, the final portion of our derivation identifies an efficient finite-difference approximation for this term, which is inexpensive to compute and, as we demonstrate in 3, results in effective distillation poisoning. 2.1 Preliminaries To formalize our presentation of antidistillation, we first introduce notation. We consider an LLM to be mapping from sequence of input tokens x1:t = (x1, . . . , xt) to distribution over the next token, where each token xj is an element of vocabulary set = {1, . . . , V}. This distribution is parameterized by weights θ and can be expressed as follows. p(x1:t; θ) : [0, 1]V (1) We write p(x1:t; θ) to denote the distribution of all next-token probabilities, whereas p(xt+1x1:t; θ) refers to the scalar probability of given next token xt+1. Typically, tokens are generated according to scaled version of this distribution by sampling as follows:2 xt+1 1 exp (log p(x1:t; θ)/τ) . (2) Here, τ is the temperature and is normalization term, which is computed by summing the exponential term over all possible next tokens. Using temperature of τ = 0 corresponds to greedy sampling, in which xt+1 is deterministically chosen to be the token with the largest log probability under the current model parameter θ. Desiderata for antidistillation sampling. Model distillation involves student language modelparameterized by θS, with distribution over next tokens given by p( x1:t; θS) trained on data generated from teacher model parameterized by θT. These models do not need to share the same parameter space, and therefore the parameter vectors θS and θT need not be comparable; indeed, in many cases, student model may have substantially fewer parameters than the teacher. The aim of antidistillation sampling is to generate tokens from the teacher θT, which perform well according to metric used to evaluate teacher samples, while simultaneously having 2Variants of this sampling scheme include top-k sampling (i.e., limiting sampling to the tokens with the top-k largest probabilities), greedy sampling (i.e., sampling from the same objective while letting τ 0), and beam search, but we focus mainly on temperature-based sampling here. 3 the property that training on these tokens cannot improve performance on this same task. In more detail, we aim to adjust the teachers sampling procedure to achieve the following objectives simultaneously: I. Non-distillablity. Student models trained on tokens sampled via antidistillation sampling should have degraded performance on chosen downstream task relative to training on tokens sampled from the teachers nominal distribution. II. Nominal utility. Tokens sampled via antidistillation sampling should remain probable under the teachers unadjusted sampling scheme p( x1:t; θT). Taken together, these goals ensure that the teacher model maintains its nominal performance while simultaneously preventing distillation on downstream tasks. Proxy models. In general, we do not expect to know the distilled students model architecture in advance. Therefore, rather than assuming access to the true student model, we develop antidistillation sampling based on the notion of proxy student model, which, for simplicity, we refer to as the proxy model. The proxy model is parameterized by θP, and specifies sampling distribution p(x1:t; θP). key aspect we consider below is whether the process generalizes, i.e., whether traces via antidistillation sampling to prevent the proxy model from distilling the teacher also prevent the distinct student models from distilling. 2.2 Deriving Antidistillation Sampling To operationalize antidistillation sampling, we first assume access to differentiable, realvalued downstream loss ℓ, which measures the proxy models performance on given downstream task. Throughout, we take ℓ to be the negative log-likelihood for generating sequence of tokens on fixed, potentially large dataset. For instance, ℓ could represent the cross entropy loss of predicting each token across large reasoning benchmark. However, ℓ can be chosen very broadly to capture any student capability that the teacher model maintainer may want to influence via poisoning. key point is that ℓ can be very costly to compute, as it may require evaluating the proxy model over large and diverse set of data. Given the non-distillability criteria outlined above, the goal of antidistillation sampling is for the downstream loss ℓ(θP) to increase3 whenever the student is fine-tuned on sequences of tokens generated by the teacher. To capture this, first consider the change in θP that results from fine-tuning to minimize the negative log-likelihood of token xt+1 generated by the teacher. Specifically, we consider one step of optimization via gradient descent on θP: θ+ = θP ηθP ( log p(xt+1x1:t; θP)) = θP + ηθP log p(xt+1x1:t; θP) where η > 0 is the step size. The impact of this update can then be quantified by measuring the difference in the loss ℓ before and after this update. In particular, for each token xt+1 V, we define the following difference term (xt+1x1:t) = ℓ(θ+ (5) If (xt+1x1:t) is positive, the update in eq. (4) increases the loss; if (xt+1x1:t) is negative, the update decreases the loss. Thus, our goal is to adjust the teachers sampling distribution so that tokens sampled from the teacher both have (1) high likelihood under the teachers unadjusted distribution and (2) yield larger (i.e., more positive) values of . ) ℓ (θP) = ℓ(θP + ηθP log p(xt+1x1:t; θP)) ℓ(θP). (3) (4) To implement antidistillation sampling, we propose adding penalty, proportional to (xt+1x1:t), to the teachers unadjusted log probabilities log p(xt+1x1:t; θT). This results in the following adjusted sampling distribution xt+1 1 exp (log p( x1:t; θT)/τ + λ( x1:t; θP)) , (6) 3We assume without loss of generality that increases in ℓ(θP) are desirable from the perspective of the poisoner; the procedure is easily adaptable to problems wherein the goal is to decrease ℓ(θP). 4 Figure 2: Antidistillation sampling uses tunable parameter λ to control the trade-off between teacher accuracy and distillability. The baseline involves sampling from the teacher with increasing temperature τ to show that we can produce traces that are bad for distillation at some cost in teacher accuracy. One important feature of the blue temperature sampling curve is that to bring the student accuracy down below the undistilled accuracy, the teacher performance has to drop to 20%. On the other hand, with antidistillation sampling, the teacher model can still get 70% accuracy while producing traces that bring the students performs down below the undistilled accuracy. where is normalization term appropriately scaled (relative to eq. (2)) to accommodate the penalty, and λ > 0 is regularization coefficient that facilitates trade-off between sampling from the teachers distribution and sampling tokens that maximally increase students downstream loss. Unfortunately, directly implementing eq. (6) is impractical, as we would need to compute (xt+1x1:t) for each potential next token xt+1 V, requiring gradients to be computed as well as evaluations of the downstream loss ℓ, which, in turn, is assumed to involve lengthy computation to produce. An efficient implementation of antidistillation sampling. The core of our proposed approach is an efficient mechanism to approximate the sampling process above. As starting point, observe that (xt+1x1:t) can be scaled by factor of 1/η without changing the relative penalties for each xt+1 (i.e., we could fold this term into the λ regularization penalty). Then, by taking the limit of (xt+1x1:t)/η as η 0, we have that lim η0 1 η (xt+1x1:t) = lim η0 ℓ(θP + ηθP log p(xt+1x1:t; θP)) ℓ(θP) η = (cid:10)ℓ (θP) , θP log p(xt+1x1:t; θP)(cid:11) . (7) (8) That is, the limit is the inner product between the gradient θP log p(xt+1x1:t; θP) and the downstream loss gradient ℓ(θP). Notice that the expression in eq. (8) no longer involves the evaluation of the downstream loss for each token in V. Rather, ℓ(θP) can be computed and stored once, after which the only remaining task is to efficiently evaluate eq. (8) for each token xt+1 V. To do so, the key observation is that the directional derivative is symmetrical. Thus, we can rewrite eq. (8) as finite difference limit in the other term, i.e., in 5 Algorithm 1: Antidistillation sampling Input: Prompt x1:n, max tokens N, penalty multiplier λ, approximation parameter ϵ, temperature τ 1. (Initialization) Compute the gradient of the downstream loss ℓ(θP) 2. For each token index = n, + 1, . . . , 1: i. Compute the antidistillation penalty term (cid:98)( x1:t) log p( x1:t; θP + ϵg) log p( x1:t; θP) ϵ ii. Sample the next token xt+1 from the teachers adjusted distribution xt+1 1 exp (cid:18) 1 τ log p( x1:t; θT) + λ(cid:98)( x1:t) (cid:19) Output: Sampled sequence x1:N terms of finite different update to log p(xt+1x1:t; θP). This gives lim η0 1 η (xt+1x1:t) = (cid:10)ℓ (θP) , θP log p(xt+1x1:t; θP)(cid:11) = lim ϵ0 log p(xt+1x1:t; θP + ϵℓ(θP)) log p(xt+1x1:t; θP) ϵ (9) (10) Importantly, this difference involves only the computation of next-token probabilities under two different models: the original proxy model θP and an updated copy of the proxy model θP + ϵℓ(θP). These models can be saved once before any sampling, and then an approximation of the antidistillation sampling term can be computed for all next tokens simply via two forward passes in the proxy model. In other words, we define (cid:98)( x1:t) = log p( x1:t; θP + ϵℓ(θP)) log p( x1:t; θP) ϵ (11) for some appropriately chosen small value of ϵ,4 where (cid:98)(xt+1x1:t) approaches eq. (8) for all next tokens xt+1 in the limit as ϵ 0. Finally, we sample according to the teachers adjusted sampling distribution: xt+1 (cid:16) exp 1 log p( x1:t; θT)/τ + λ(cid:98)( x1:t) (cid:17) . (12) In Algorithm 1, we summarize the procedure outlined in this section. Concretely, given prompt x1:t, using antidistillation sampling to generate new token xt+1 involves: (1) (once, at initialization) computing the gradient of the downstream loss; and (2) (for each token to be generated) compute the finite-difference approximation of (x1:t) and sample the token from the teachers adjusted softmax distribution."
        },
        {
            "title": "3 Empirical Results",
            "content": "Architectures and benchmarks. To demonstrate the effectiveness of antidistillation sampling in practice, we simulate real-world distillation by instantiating separate teacher, proxy 4It is also possible to compute the inner product of this gradient exactly via an explicit Jacobianvector product (not to be confused with vector-Jacobian product used in backpropagation). However, even in modern automatic differentiation frameworks, Jacobian-vector products tend lack support for handful of important operations, such as SDPA. Meanwhile flash attention [24] only supports Float16 and BFloat16. Thus, we use finite differences for the improved convenience and efficiency, but we verify the correctness of the finite difference approximation against autograd in C.1. 6 student, and student models. Concretely, we use deepseek-ai/DeepSeek-R1-DistillQwen-7B [25] as the teacher model, Qwen/Qwen2.5-3B [26] as the proxy model, metallama/Llama-3.2-3B [27] as the student model. We evaluate the teacher and the student peformance on the GSM8K [8] and MATH [7] benchmarks, as strong performance on these benchmarks requires training on high-quality traces. Baselines. Throughout, we compare to straightforward sampling method: temperature sampling. In this scheme, tokens are sampled from the teachers distribution with some temperature τ; this approximates what an API endpoint might be doing in the nominal case. We present comparisons to additional baselines in A. Hyperparameters. Antidistillation sampling involves two key hyperparameters: ϵ, which controls the approximation power of the finite-difference computation, and λ, which controls the degree to which the sampling distribution mixes in the antidistillation penalty. Regarding the choice of ϵ, we empirically verify that the finite difference approximation in eq. (11) is close to the JVP in eq. (8) on smaller model in C.1. In practice, we find that ϵ = 104 works reasonably well for the BFloat16 models we use. With regard to λ, we sweep over several values to study the trade off involved in perturbing the sampling distribution. 3.1 Main Results In Figure 1, we provide evidence that antidistillation sampling effectively meets the desiderata laid out in 2.1. That is, for fixed teacher accuracy, we observe significantly worse performance for students distilled from traces sampled via antidistillation sampling relative to models distilled from traces generated via temperature scaling. When performing distillation, we use LoRA [28] with rank 128, α equal to 128, dropout probability set to 0, learning rate equal to 0.0005, weight decay coefficient of 0.1, and the maximum gradient norm set to 1.0. We train with cosine learning rate schedule with warm-up parameter of 0.1, batch size of 32, and we train for 4 epochs. In Figure 2, we vary λ to examine the degree of control antidistillation sampling gives model owners over the trade-off between teacher performance and distillability. We note that as we use distinct student and proxy student architectures, our results indicate that antidistillation sampling can generalize across architectures."
        },
        {
            "title": "4 Conclusion",
            "content": "The value of proprietary frontier LLMs necessitates that their owners do what they can to protect their assets. As evidenced by the fact that the frontier companies limit exposure to their models via black-box APIs, these companies are already considering the threat of model stealing. However, given the recent attention paid to the effectiveness of distillation, it is imperative that model maintainers who wish to protect the information stored in their models guard against distillation. This paper provides proof-of-concept that antidistillation samplingwhich adjusts models sampling distributionis effective in blocking such attacks. We are excited at the prospect of continuing to refine and scale this approach, particularly with view toward more secure future frontier models."
        },
        {
            "title": "References",
            "content": "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [2] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 7 [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [4] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. [5] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. [6] Matt Burgess and Lily Hay Newman. Deepseeks safety guardrails failed every test researchers threw at its ai chatbot. URL https://www.wired.com/story/ deepseeks-ai-jailbreak-prompt-injection-attacks/. [7] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] OpenAI. Model distillation in the api. October 2024. URL https://openai.com/ index/api-model-distillation/. Accessed: 2025-04-10. [10] Jürgen Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural computation, 4(2):234242, 1992. [11] Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into smaller language models. Findings of the Association for Computational Linguistics: ACL 2023, pages 70597073, 2023. [12] Jierui Li and Raymond Mooney. Distilling algorithmic reasoning from llms via explaining solution programs. arXiv preprint arXiv:2404.08148, 2024. [13] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. [14] Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. Adversarially robust distillation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 39964003, 2020. [15] New York Times. Openai says it has evidence chinas deepseek used its model to train competitor. The New York Times, January 2025. URL https://www.nytimes.com/2025/ 01/29/technology/openai-deepseek-data-harvest.html. Accessed: 2025-04-10. [16] Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, et al. Stealing part of production language model. arXiv preprint arXiv:2403.06634, 2024. [17] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, Feder Cooper, Daphne Ippolito, Christopher Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023. [18] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander adry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):15631580, 2022. 8 [19] Javier Rando and Florian Tramèr. Universal jailbreak backdoors from poisoned human feedback. In International Conference on Learning Representations, 2024. [20] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. [21] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022. [22] Haozhe Ji, Pei Ke, Hongning Wang, and Minlie Huang. Language model decoding as direct metrics optimization. arXiv preprint arXiv:2310.01041, 2023. [23] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energybased constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:95389551, 2022. [24] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. [25] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [26] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [28] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 9 Figure 3: Permutation sampling is strong baseline where we destroy the information in antidistillation sampling while preserving statistical properties via random permutation and sign flipping."
        },
        {
            "title": "A Additional baselines",
            "content": "We also consider baseline perturbation to the outputs to ensure that the computation involved in antidistillation sampling is worthwhile. This method adds random perturbations to the logits and we call this noisy sampling. While many choices of how to add noise to the output of an LLM exist, we find that matching the statistics of the perturbations computed by antidistillation sampling is the best way to find interventions that lead to the same teacher accuracy. Therefore, we randomly permute and flip the sign of the perturbations computed with antidistillation sampling to execute permutation sampling, specific type of noisy sampling; we show the results of perturbation sampling in Figure 3."
        },
        {
            "title": "B Example traces",
            "content": "More example traces can be found at https://antidistillation.com/traces. 11 12 13 Figure 4: Relative error (Error) between the finite difference and the JVP results."
        },
        {
            "title": "C Hyperparameters",
            "content": "C.1 Verifying finite difference approximation We empirically verify that the finite difference in eq. (11) behaves as expected by computing the relative error between the finite difference result and term produced from autograd. As shown in fig. 4, we see it well approximates the autograd computed result for appropriately chosen step size. Here we compute (cid:10)ℓ (θP) , θP log p(xt+1x1:t; θP)(cid:11) and stack the different values of xt+1 into dimensional vector (cid:98) and compare to the autograd vector . We compute relative error being sensitive only to the direction as Error2 = 1 (cid:32) (cid:68) (cid:69) , (cid:98) (cid:98) (cid:33)2 , which represents the Error = sin θ, the sine of the angle between the two vectors. Due to memory constraint, we run this numerical experiment using GPT2-Large [29]. Here we demonstrate that the finite difference can be used to estimate the derivatives in the low precision BFloat16 format. In particular, too small an ϵ leads to round-off error in the perturbation and too large ϵ leads to high truncation error in the Taylor expansion, with sweet spot in the middle. The actual choice of ϵ depends heavily on the model size (and numerical precision), so we recommend choosing this value on the exact model in question. In our actual experiment, we pick ϵ empirically to be 104."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University"
    ]
}