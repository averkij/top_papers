{
    "paper_title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models",
    "authors": [
        "Minki Kang",
        "Jongwon Jeong",
        "Jaewoong Cho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 8 1 7 4 0 . 4 0 5 2 : r Preprint. Under review. T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models Minki Kang1,2 1KRAFTON, 2KAIST {zzxc1133, jwjeong, jwcho}@krafton.com Jongwon Jeong1 Jaewoong Cho"
        },
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and factchecking. To address this limitation, we propose Tool-integrated selfverification (T1), which delegates memorization-heavy verification steps to external tools, such as code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledgeintensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have demonstrated strong emergent abilities through large-scale pretraining (Brown et al., 2020; Hurst et al., 2024; Reid et al., 2024), enabling them to tackle complex reasoning tasks such as mathematical problemsolving and competitive coding (Wei et al., 2022b; Lightman et al., 2024). While small language models (sLMs) offer advantages in deployment efficiency and cost (Liu et al., 2024; Lu et al., 2024), they struggle significantly with high-complexity tasks (Wei et al., 2022a). Test-time compute scaling has emerged as promising approach to enhance sLMs by dynamically allocating additional computation during inference (Wu et al., 2024). Prior works suggest that test-time scaling can surpass pretraining-based scaling (Snell et al., 2024), allowing 3B LM to outperform 405B LLM on mathematical benchmarks such as MATH and AIME (Liu et al., 2025). This success depends on reliable verification of generated solutions. To enable reliable verification, existing approaches have leveraged process reward models (PRMs) and critic models (Wang et al., 2024a; Zhang et al., 2024), but these typically require LLMs (7B+ parameters). Relying on large verifiers counteracts the efficiency benefits of sLMs. Therefore, it remains unclear whether self-verification, where sLMs verify their own generated solutions, can enable strong reasoning capabilities without relying on larger models. This raises key research question: Can small language models reliably perform self-verification for test-time scaling? In this work, we explore test-time scaling in sLMs, focusing on their self-verification ability. While verification is often easier than generation, prior work indicates that sLMs still Equal Contribution 1 Preprint. Under review. (a) Concept figure (b) Concept-proof results Figure 1: (a) Concept figure. Small language models (sLMs) often fail due to their limited capacity. However, when sLMs utilize external tools, their reliability significantly improves. (b) Concept-proof experimental results. We evaluate Llama 1B model on their ability to verify arithmetic calculations of 3-digit numbers. The performance of 1B model consistently drops as increases. However, enabling code generation and execution for verification largely mitigates the performance drop. See Appendix for details of concept-proof experiments. struggle to verify their own solutions (Song et al., 2024). One plausible method to mitigate this limitation is knowledge distillation, transferring verification capabilities from larger verifiers to sLMs (Hinton et al., 2015; Kim & Rush, 2016). However, even after distillation, we found that sLMs still exhibit weak verification performance, which we hypothesize is due to their limited memorization capacity (Kandpal et al., 2023). Our preliminary findings in Figure 1 indicate that sLM struggles with numerical verification, particularly as the complexity of calculations increases. However, employing external tools such as code interpreter significantly improves the accuracy. This suggests that the tool use can be an effective method for enhancing the reliability of self-verification in sLMs. Motivated by these findings, we introduce Tool-integrated Self-verification (T1), where sLMs offload specific verification taskssuch as numerical calculations or fack-checkingto external tools. By leveraging tool usage as an additional axis for scaling during inference, T1 enables sLMs to maintain strong verification accuracy without increasing model size. Our theoretical analysis shows that learning to use tool reduces the necessary memorization, effectively converting the verification task into one learnable by sLMs. We also demonstrate that T1 can enhance performance under test-time scaling with imperfect verifier. Our experiments demonstrate that T1 integrates seamlessly with both generative verifiers (Zhang et al., 2024) and process reward models (PRMs) (Wang et al., 2024a; Zeng et al., 2025), enabling external tools for more accurate self-verification. This leads to significant performance improvements on widely-used benchmarks, with notable gains on GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and MMLU-Pro (Wang et al., 2024b). Our contributions are as follows: We conduct systematic study on sLM self-verification under test-time scaling, identifying weak verification despite LLM distillation as key bottleneck, and propose addressing it with external tools. We propose Tool-integrated self-verification (T1), which leverages tools (e.g., code interpreters, retriever) to offload memorization-heay verification steps where tools excel. We provide theoretical analysis of how learning to use tool enhances sLMs under limited memorization capability, and show that T1 enables more effective test-time scaling. We show that T1 integrates well with both generative verifiers and PRMs, achieving strong results on math and multi-domain reasoning benchmarks like MATH and MMLU-Pro."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Test-time compute scaling Test-time compute scaling has emerged as promising approach for improving the reasoning capabilities of large language models (LLMs) (Wu et al., 2024). It can be broadly categorized 2 Preprint. Under review. into sequential and parallel methods (Snell et al., 2024). Sequential scaling iteratively refines solutions by leveraging post-training, enabling the model to perform self-reflection and verification (Muennighoff et al., 2025; DeepSeek-AI et al., 2025). Parallel scaling, in contrast, generates multiple candidate solutions simultaneously and selects the best one using verifier model (Cobbe et al., 2021; Lightman et al., 2024; Brown et al., 2024). common strategy is the best-of-N method, which produces parallel outputs and ranks them based on verification scores (Cobbe et al., 2021; Lightman et al., 2024). Increasing has been shown to enhance LLM on challenging benchmarks (Brown et al., 2024; Snell et al., 2024). In this work, we focus on the parallel scaling paradigm due to its simplicity and popularity. Prior research shows that even small models can achieve strong results when paired with large verifier in parallel scaling (Liu et al., 2025). We further investigate whether small language models (sLMs) can self-verify, enabling test-time scaling without large models. 2.2 Verifier in test-time compute scaling The verifier plays crucial role in parallel scaling. One approach, the Process Reward Model (PRM), scoring each reasoning step individually using regression head, enables fine-grained feedback (Lightman et al., 2024; Wang et al., 2024a; Zeng et al., 2025). An alternative approach leverages an LLM itself as Critic model, prompting it to evaluate reasoning steps (Zheng et al., 2023). Zheng et al. (2024) have shown that powerful LLMbased critic models can outperform PRM, particularly in mathematical reasoning tasks. Recent works (Zhang et al., 2024; Mahan et al., 2024) proposed the Generative Reward Model (GenRM), formulating verification as next-token prediction problem with chain-ofthought (Wei et al., 2022b) improving interpretability in step-wise verification. Despite these advances, ensuring consistent and high-quality step-wise verification remains an open problem for both PRM and GenRM. Additionally, prior works have not thoroughly explored the change in verification performance depending on the size of LMs. 2.3 Tool-integrated language model The integration of external tools has significantly enhanced LLM capabilities. Programaided language models (Gao et al., 2023) introduced delegating computations to interpreters via synthesized code. Subsequent works expanded this by using tools like search engines and calculators for fact retrieval and arithmetic (Schick et al., 2023; Qin et al., 2024). Recent methods further integrate tools into multi-step reasoning (Gou et al., 2024) and reward modeling (Li et al., 2024), using them as intermediaries to improve performance. Our work extends this line of research by focusing on how tool integration benefits small language models in self-verification. We formulate tool use as an additional dimension of test-time scaling, emphasizing its effectiveness in memorization-heavy verification tasks."
        },
        {
            "title": "3 Preliminaries",
            "content": "Test-time scaling Following Snell et al. (2024), we view test-time scaling as modifying the models proposal distribution. Given problem , we sample solution from the policy π(y x, Ip; θ) in the set Y, where Ip is the generator-specific instruction prompt, and the policy is parameterized by θ which refers to the pre-trained language models. Several algorithms can be used to scale test-time computation, including those that adjust the input level (e.g., self-reflection (Kumar et al., 2024)) and those that modify the output level (e.g., best-of-N (Cobbe et al., 2021), beam search (Yao et al., 2023)). Among them, the best-of-N algorithm is simple yet powerful approach. It samples multiple candidate solutions from the policy and selects the one with the highest score, as determined by the verifier, as the final prediction. Formally, the Best-of-N policy πN (cid:0)y x, Ip; θ(cid:1) is defined as: (1) s.t. yi π(y x, Ip; θ), r(x, y), arg max y{y1,...,yN } where r(x, y) : is verifier that assigns scalar score r. 3 Preprint. Under review. Figure 2: Tool-integrated self-verification for mathematical reasoning. (a) Generator: small language model (sLM) may produce incorrect solutions due to calculation errors. (b) Tool-based Verifier (ToolV): The sLM generates executable code based on its reasoning; the output of the code is used to verify the solutions correctness. (c) Reward Model (RM)-based Verifier: The reward model (GenRM / PRM) still evaluates the solution as before, but its verdict only contributes to the final decision if the solution passes the tool-assisted filter. Concrete examples are in Appendix F. Verifier The verifier can be modeled using the following models: (1) process reward model (PRM), which assigns the score of each step of reasoning (Wang et al., 2024a), (2) critique model, which generates rationale for the verifiation (Zheng et al., 2023). For both cases, the sequence of verification scores or tokens are sampled from π(z x, y, Iv; θ) where Iv is the verifier-specific instruction prompt. In general, we use the last score or token of the sequence as the final score for the solution (Snell et al., 2024; Zhang et al., 2024). For instance, in generative verifier (Zhang et al., 2024), the verification score can be obtained as follows: (2) r(x, y) = π(zT = Yes x, y, Iv, z1:T1; θ), z1:T1 π(z x, y, Iv; θ), s.t. where z1:T1 is chain-of-thought (Wei et al., 2022b) and last token zT {Yes, No}."
        },
        {
            "title": "4 Method",
            "content": "4.1 Tool-integrated self-verification Test-time scaling can improve base policy model that generates valid solutions from its proposal distribution, but the effectiveness of scaling at test-time heavily relies on the performance of the verifier. However, sLMs often struggle to reliably verify the correctness of their generated outputs (Song et al., 2024). Specifically, sLMs exhibit limitations in precisely validating numerical computation or detecting incorrect or outdated knowledge information, due to their limited parameter size and insufficient memorization capacity. To address these limitations, we propose Tool-integrated self-verification (T1) approach for parallel test-time scaling in sLMs. As shown in Figure 2, our verification approach involves two stages: 1) filtering stage with Tool-based Verifier (ToolV) and 2) scoring stage with reward model (RM)-based verifier, and formally can be expressed as: = arg max y{y1,...,yN } (x, y; , θ) r(x, y; θ), s.t. yi π(y x, Ip; θ), {1, . . . , N}, (3) where (x, y; , θ) {0, 1} indicate binary tool-based verification function (with 0 indicating filtered-out response), denotes the utilized tool (e.g., code interpreter, retriever), and r(x, y; θ) denotes the verifier score defined in Equation 1. Tool-based verifier stage In this stage, sLM utilizes the external tool , such as code interpreter or knowledge retriever, to verify generated outputs. Here, we assume the scenario in which the utilized tool is explicitly known. Specifically, sLM uses these tools to verify numerical accuracy and validate the knowledge in generated solutions. One specific example is that multiple generated responses are filtered based on tool-based verifiers, discarding those with incorrect calculations or inaccurate knowledge information. Specifically, the tool-based verification function, (x, y; , θ), consists of three parts: 1) generating the tool-calling query (i.e., c1), 2) the execution of the tool (i.e., ()), and 3) extraction of the verification (i.e., c2). Therefore, (x, y; , θ) can be represented as: (x, y; , θ) = c2 π (cid:16) , where c1 π(c x, y, Ic; θ), (4) (cid:17) (c1) , x, y, ; θ 4 Preprint. Under review. and and Ic are task-specific instruction prompts. Detailed formulation for mathematical reasoning and knowledge-intensive tasks are represented in Appendix B. RM-based verifier stage Following ToolV stage, the remaining generated responses are scored using reward model, the same model used for generation and filtering. This reward model assesses the overall logical consistency, coherence, and correctness of each response. The final output is chosen as the response with the highest reward score. 4.2 Verifier distillation To further enhance the performance of both verification stages, we employ knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016) from LLMs. Specifically, we fine-tune sLM using tool-based and RM-based verifications generated by larger teacher model θT. To efficiently manage multiple distinct tasks during the distillation process, we adopt multi-LoRA (Hu et al., 2022) approach, assigning separate LoRA adapters, θtool, and θreward, for each verifier. The distillation for ToolV is formulated as: Ltool(θtool) = xXtrain, yπ(x,Ip;θ), cπ(t,x,y,I;θT ) log π(c t, x, y, I; θ + θtool), (5) where Xtrain is training dataset, {c1, c2}, {Ic, }, and is empty ϕ or the output of (c1). Note that we generate tool-based verifications using the teacher model by applying corresponding instruction in zero-shot manner (Ouyang et al., 2022; Chung et al., 2024). Similarly, distillation for the RM-based verifier is expressed as: Lreward(θreward) = xXtrain, yπ(x,Ip;θ), zπ(x,y,Ir;θT ) log π(z x, y, Ir; θ + θreward), (6) where Ir is RM-based verifier-specific instruction. In Equation 6, responses are first sampled from the student models proposal distribution. Each sampled response is then verified by the teacher model, and finally, the student model is fine-tuned based on these verifications."
        },
        {
            "title": "5 Theoretical analysis",
            "content": "5.1 Memorization bound with & without tool Let us consider simple verification task in which verifier assesses whether the given equation + = is true or not. Let this tasks data distribution is and = ((a, b, c), r) q. Assume we have number of training samples such that = {((a, b, c), r) a, {0, . . . , 1}, {0, . . . , 2M 2}, = 1a+b=c}, where ((a, b, c), r) is independently sampled according to q. Then, let qX be the random variable representing the distribution of the training set . learning algorithm receives to produce θ such that θ = (X). Then, we call that is ε-close-to-optimal if errq,X (A) (cid:1) + ε, where errq,X (A) = PrXqX ,((a,b,c),r)q,ˆrπ(ca+b,c,I;θ=A(X))(ˆr = r) and errq,X AOPT is the optimal learning algorithm. Then, Lemma 5.1 shows how much information of should be memorized within θ to satisfy almost zero error. Lemma 5.1 (Memorization without Tool (Brown et al., 2021)). Any learning algorithm that is ε-close-to-optimal with sufficiently small ε > 0 also satisfies (X; θ P) = Ω(cid:0)M3(cid:1), where is the mutual information. (cid:0)AOPT Proof sketch. Theorem 1.1 in Brown et al. (2021) said that I(X; θ P) is proportional to at least dataset size. Since = 2 (M 1)3, we can get Ω(cid:0)M3(cid:1). Refer to subsection C.1 for the detailed proof. On the other hand, using an external tool that verifies whether + = allows the model to avoid memorizing the full table of sums. Specifically, define tool . Suppose θ is generated by learning algorithm that has access to , and that (a, b, c; θ, ) = 1 a+b=c holds. Then we obtain the following result: 5 Preprint. Under review. Theorem 5.2 (Memorization with Tool). Suppose θ is generated by learning algorithm that has access to , and that (a, b, c; θ, ) = 1 a+b=c holds. Then, any learning algorithm that is ε-close-to-optimal with sufficiently small ε > 0 also satisfies (X; θ P) = 0, where is the mutual information. Proof sketch. As the learning algorithm can access an external tool , then θ = A(X) such that (a, b, c; θ, ) = 1 a+b = . makes errq,X (A) = 0. Also, θ is independently determined regardless of X, resulting in I(X; θ P) = 0. See subsection C.2 for the detailed proof. (X; θ P) quantifies the amount of information about X, drawn from P, that must be memorized in θ learned by to achieve near-zero error. By comparing (X; θ P) from Lemma 5.1 and Theorem 5.2, we demonstrate that tool drastically reduces the required memorization of within θ, lowering (X; θ P) from Ω(cid:0)M3(cid:1) to 0. Consequently, this result implies that with the tool, small models become reliable for the verification task. 5.2 Effect of tool-based verifier on test-time scaling We employ the toy setting introduced in Beirami et al. (2024). Specifically, for given input x, the ground-truth label produced by this generator is set as 1, and the generator π produces binary outputs, i.e., = {0, 1}. Furthermore, we consider an imperfect verifier inducing noise. Then, we can show Theorem 5.3 that increasing q1 directly increases the probability of obtaining correct output from the best-of-N. Theorem 5.3 (Monotonicity of Imperfect Verifier). Let the generator output 0 or 1 with equal probability, i.e., π (0x) = π (1x) = 1 2 , and the verifier with noise level p, be defined as follows: (x, 0) = (cid:40) 0, w.p. p, 1, w.p. 1 p, (x, 1) = (cid:40) 1, w.p. q, 0, w.p. 1 q, with the condition that > 1 and > 1 q. Assume and with > p. Then, for any 2, be the noise level of two verifiers πN(1 x) (cid:12) (cid:12) (cid:12)p= > πN(1 x) (cid:12) (cid:12) (cid:12)p= . (7) Proof sketch. By the law of total probability, we get πN(1 x). Then, we can get the monotonicity of πN(1 x). Refer to subsection C.3 for the detailed proof. If tool-based verification function in subsection 4.1 effectively acts as filter for incorrect solutions, we can say that using increases p, thus improving the verifiers capability to choose the correct label as shown in Theorem 5.3."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Setup Datasets We use (1) MATH500 (Hendrycks et al., 2021; Lightman et al., 2024), dataset containing college-level math problems. (2) GSM8K (Cobbe et al., 2021), which consists of grade-school math problems. (3) MMLU-Pro (Wang et al., 2024b), containing multi-domain knowledge-intensive problems. We use the training set of each dataset for distillation. Evaluation setting Following previous works (Cobbe et al., 2021; Lightman et al., 2024; Snell et al., 2024; Liu et al., 2025), we evaluate weighted Best-of-N performance, where we aggregate the score of the solutions ending with the same final answer, to assess testtime compute scaling. We generate 64 solutions using fixed generator and measure the percentage of correctly solved problems after verifications. As verifier, we use both PRM (Wang et al., 2024a) and GenRM-CoT (Zhang et al., 2024) (we refer to it as GenRM). 6 Preprint. Under review. Figure 3: MATH500 with PRM. Weighted Best-of-N performance of three small language models, emphasizing the benefits of ToolV on college-level math problems. ToolV significantly enhances PRM, enabling small models to outperform or match much larger models. Qwen2.5-1.5B and Llama3.1-8B performances are reported as = 1 greedy decoding. Figure 4: MATH500 with GenRM. Weighted Best-of-N performance of three small language models, showcasing the effectiveness of ToolV with GenRM, where even generative verification cannot supplement the calculation error which can be easily filtered out by using tool. Baselines We compare ours, Tool-integrated self-verification, that utilizes both fine-tuned reward model and tool-based verifiers (ToolV), against the following baselines: (1) Majority Voting (Wang et al., 2023) (without using verifier), (2) Zero-shot GenRM (Zheng et al., 2023; Song et al., 2024) (without any fine-tuning), (3) Distilled PRM/GenRM (with fine-tuning). Models & training As our main focus is self-verification, we use the same base model for the generator and verifiers in each experiment. We experiment with the smallest instructiontuned models from widely used families: Qwen-2.5-0.5B-Instruct (Yang et al., 2024) and Llama-3.2-1B-Instruct (Dubey et al., 2024). In addition, we test SmolLM2-360M-Instruct (Allal et al., 2025) for evaluation in extremely small model. As the teacher model, we employ gpt-4o-mini-2024-07-18 (Hurst et al., 2024). The teacher model is prompted to generate outputs used to fine-tune student models (Hu et al., 2022). For PRMs teacher, we use Qwen2.5-Math-PRM-7B (Zhang et al., 2025). We include more implementation details in Appendix D. 6.2 Experimental results ToolV improves PRM in small LMs As shown in Figure 3, ToolV improves performance when combined with the distilled Process Reward Model (PRM) on the MATH500 benchmark. Our results show that adding ToolV provides substantial gains in testtime scaling, suggesting that distilled PRM alone is still prone to numerical errors. Notably, with ToolV, only using Llama 1B models outperforms the performance of the 8B modeldemonstrating that extra test-time computation can meaningfully boost smaller models, where distilled PRM alone cannot enable the 1B model to reach that performance until generating 64 solutions. Similarly, ToolV enables Qwen2.5 0.5B to match the performance of the 1.5B model by generating just 16 solutions, showing impressive effectiveness. ToolV improves GenRM in sLMs As shown in Figure 4, ToolV boosts test-time scaling for three small language models on MATH500 when combined with the distilled GenRM (Zhang 7 Preprint. Under review. Figure 5: GSM8K with GenRM. Weighted Best-of-N performance comparison across three small language models. The results show that ToolV also improves model performance on graduate-level arithmetic problems. However, the gains are smaller on this simpler task, where existing verifiers already perform reliably compared to more challenging tasks. Figure 6: MMLU-Pro with PRM. Weighted Best-of-N (N = 64) performance of Llama3.2-1B-Instruct on three knowledge-intensive domains, illustrating the effect of different document sources in ToolV + Distilled PRM (retrieved and gold documents). ToolV extends beyond math, improving PRM on multi-domain knowledge-intensive reasoning tasks. et al., 2024). While GenRM struggles alone, ToolV compensatesat the cost of code generation. Similar gains appear on GSM8K in Figure 5, especially for SmolLM2-360M-Instruct, the weakest model. This supports our analysis in subsection 5.1 that ToolV enables even small models to memorize key information. Zero-shot GenRM ablations confirm that without distillation, small models struggle to verify solutions (Song et al., 2024). ToolV can be applied to multi-domain knowledge-intensive tasks We also demonstrate that ToolV is effective in verifying solutions across range of knowledge-intensive reasoning tasks. We adapt ToolV to function as fact-checker, verifying claims in solutions without other components such as query transformation and reranker (Wei et al., 2024; Kang et al., 2023). We provide experimental results on the MMLU-Pro benchmark with minimal framework in this work (Wang et al., 2024b). As shown in Figure 6, ToolV outperforms the distilled PRM baseline derived from the VersaPRM (Zeng et al., 2025). For the tool, we retrieve three documents from Wikipedia using BM25. Due to variability in document quality, performance is somewhat unstable in some cases. To explore ToolVs upper bound, we also evaluate it using gold documents generated by GPT-4o. Results show that ToolV performance improves significantly with higher-quality documents, demonstrating its potential for multi-domain knowledge-intensive reasoning. 6.3 Analysis Effects of ToolV on category and difficulty In Figure 7, we analyze the = 64 weighted Best-of-N performance on MATH500 using Llama-3.2-1B-Instruct. On the left, category-wise results show ToolV brings clear gains, especially in Algebra, Number Theory, and Counting & Probability. Geometry sees drop, likely due to ToolV being less effective in that domain. On the right, performance by problem level shows consistent improvements with ToolV for Levels 24. However, results dip at Level 5, suggesting ToolV struggles with the most 8 Preprint. Under review. Figure 7: Analysis with problem types and levels. We perform analysis on the effect of tool-based verifier with problem types and levels in MATH500 dataset. The results are from Llama-3.2-1B-Instruct with PRM using weighted Best-of-N (N = 64). This analysis shows ToolV is most effective on mid-level problems and calculational domains. Figure 8: Effects of ToolV on sizes of GenRM. Weighted Best-of-N (N = 64) performance of GenRM based on different sizes of Llama 3 (Dubey et al., 2024) on MATH500. For ToolV, we use 1B and only scale up the GenRM. Figure 9: Correct solutions ratio among = 64 generations to show how the toolbased verifier works. challenging problems. Overall, ToolV works best on mid-level problems and math areas requiring accurate calculation, but improvements are needed for higher-difficulty cases. ToolV benefits larger verifiers Figure 8 shows how performance varies with distilled GenRM size, keeping ToolV fixed at 1B. As GenRM scales, the gap with and without ToolV narrows but remains. Notably, on MATH500, 1B GenRM + ToolV outperforms 8B GenRM, suggesting ToolV can be more effective than scaling the verifierespecially on harder tasks. Effects of ToolV as filter Figure 9 shows how ToolV acts as an effective filter for mathematical solutions. Using Llama-3.2-1B-Instruct with GenRM on MATH500 (N = 64 candidates per sample), we recalculated accuracy after applying ToolV to remove wrong outputs. The results support our analysis in subsection 5.2, showing ToolV reliably filters out incorrect solutions and significantly improves accuracy."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduced Tool-integrated Self-verification (T1), which delegates memorization-intensive tasks in self-verification to external tools for sLMs. Our method involves tool-based verification stage and reward-model-based scoring stage, both enhanced by knowledge distillation from large verifiers. Theoretical analysis confirmed that tool use substantially reduces the memorization burden on sLMs and improves test-time scaling accuracy. Empirical experiments demonstrated that T1 significantly improves the test-time scaling performance of sLM on mathematical reasoning and knowledge-intensive tasks. key conclusion of our work is that tool integration is essential for enhancing sLM performance, even under test-time scaling, by reducing the memorization burden. 9 Preprint. Under review. Limitations & Future Works While T1 shows strong improvements, some limitations remain. (1) ToolV acts only as rejection filter and cannot recover from false negativescorrect solutions mistakenly rejected by the verifier. As one possible implementation of T1, this limitation could be mitigated by integrating tool-based reasoning into the verification step, allowing the verifier to leverage correctness guarantees from tool outputs (Gou et al., 2024; Li et al., 2024), which we do not explore in this work. (2) Our work focuses on best-of-N (parallel) test-time scaling, which lacks information sharing between generations. However, tools can also benefit other test-time scaling strategies, such as step-level search (Yao et al., 2023) in sLMs or long reasoning chains in sequential test-time scaling as demonstrated by Li et al. (2025). Exploring these directions presents promising avenue for future work."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was fully supported by the KRAFTON AI Research Center."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martın Blazquez, Guilherme Penedo, Lewis Tunstall, Andres Marafioti, Hynek Kydlıˇcek, Agustın Piqueres Lajarın, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alex DAmour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. arXiv, 2401.01879, 2024. URL https://doi.org/10.48550/arXiv.2401.01879. Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv, 2407.21787, 2024. URL https://doi.org/10.48550/arXiv. 2407.21787. Gavin Brown, Mark Bun, Vitaly Feldman, Adam M. Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, New York, NY, USA. Association for Computing Machinery, 2021. URL https://doi.org/10.1145/ 3406325.3451131. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. J. Mach. Learn. Res., 25:70:170:53, 2024. URL https://jmlr.org/papers/v25/23-0870.html. 10 Preprint. Under review. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv, 2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. arXiv, 2407.21783, 2024. URL https://doi.org/10.48550/arXiv.2407.21783. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In Andreas Krause, Emma 11 Preprint. Under review. Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1076410799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=Ep0TtjVoap. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem In Joaquin Vanschoren and Sai-Kit Yeung solving with the MATH dataset. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in neural network. arXiv, 1503.02531, 2015. URL http://arxiv.org/abs/1503.02531. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 2529, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. arXiv, 2410.21276, 2024. URL https://doi.org/10.48550/arXiv.2410.21276. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1569615707. PMLR, 2023. URL https://proceedings.mlr.press/v202/kandpal23a.html. Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, and Sung Ju Hwang. Knowledgeaugmented reasoning distillation for small language models in knowledge-intensive In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, tasks. and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/ 2023/hash/97faedc90260eae5c400f92d5831c3d7-Abstract-Conference.html. Preprint. Under review. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 13171327. The Association for Computational Linguistics, 2016. URL https: //doi.org/10.18653/v1/d16-1139. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning. CoRR, abs/2409.12917, 2024. URL https://doi.org/10.48550/arXiv.2409.12917. Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. Start: Self-taught reasoner with tools, 2025. URL https://arxiv.org/abs/2503.04625. Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-augmented reward modeling. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=d94x0gWTUX. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. Pyserini: Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pp. 23562362, 2021. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling, 2025. URL https://arxiv.org/abs/2502.06703. Zechun Liu, Changsheng Zhao, Forrest N. Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, and Vikas Chandra. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=EIGbXbxcUQ. Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, and Mengwei Xu. Small language models: Survey, measurements, and insights. arXiv, 2409.15790, 2024. URL https://doi.org/10.48550/arXiv.2409.15790. Dakota Mahan, Duy Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv, 2410.12832, 2024. URL https://doi.org/10.48550/arXiv.2410.12832. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, 13 Preprint. Under review. USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/ paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=dHng2O0Jjr. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv, 2403.05530, 2024. URL https://doi.org/10.48550/arXiv.2403.05530. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ d842425e4bf79ba039352da0f658a906-Abstract-Conference.html. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv, 2408.03314, 2024. URL https://doi.org/10.48550/arXiv.2408.03314. Yuda Song, Hanlin Zhang, Carson Eisenach, Sham M. Kakade, Dean P. Foster, and Udaya Ghai. Mind the gap: Examining the self-improvement capabilities of large language models. arXiv, 2412.02674, 2024. URL https://doi.org/10.48550/arXiv.2412.02674. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 94269439. Association for Computational Linguistics, 2024a. URL https://doi.org/10.18653/v1/2024.acl-long. 510. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024b. URL https: //arxiv.org/abs/2406.01574. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori 14 Preprint. Under review. Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022a. URL https: //openreview.net/forum?id=yzkSU5zdwD. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022b. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper files/paper/2024/hash/ 937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https: //openreview.net/forum?id=j7DZWSc8qu. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv, 2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, Hyung Il Koo, Kannan Ramchandran, Dimitris Papailiopoulos, and Kangwook Lee. Versaprm: Multi-domain process reward model via synthetic reasoning data, 2025. URL https: //arxiv.org/abs/2502.06737. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh arXiv, Agarwal. Generative verifiers: Reward modeling as next-token prediction. 2408.15240, 2024. URL https://doi.org/10.48550/arXiv.2408.15240. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning, 2025. URL https://arxiv.org/abs/2501.07301. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16 (12):38483860, 2023. URL https://www.vldb.org/pvldb/vol16/p3848-huang.pdf. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv, 2412.06559, 2024. URL https://doi.org/10.48550/arXiv.2412. 06559. 15 Preprint. Under review. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets and Benchmarks.html. Judging llm-as-a-judge with mt-bench and chatbot arena. 16 Preprint. Under review. Concept-Proof Experiment Details We provide additional details about the proof-of-concept experiment shown in Figure 1 (b) of the main paper. The verification task focuses on arithmetic calculations involving randomly selected three-digit numbers, using both addition and subtraction, with ranging from 3 to 10. For each value of N, we generate 500 equations with correct answers, along with another 500 equations where the output is slightly incorrectwithin 5% margin of error. We then prompt Llama-3.2-1B-Instruct (Dubey et al., 2024) to verify these calculations. Specifically, we use Prompt A.1 to make the language model to generate step-by-step explanation in natural language. Prompt A.1: Check Calculation Evaluate the below calculation. Is this calculation correct? If correct, return True. Return False otherwise. # Calculation: {exp} = {ans} If the calculation is correct, return True. If not, return False. Think step-by-step, and MUST output True or False at the end of your verification. To use the tool, we prompt LM to generate code instead of verification in natural language using Prompt A.2. Prompt A.2: Check Calculation with Code Generate simple Python script that evaluates the correctness of given mathematical calculation. # Calculation: {exp} = {ans} The script should print The calculation is correct if the calculation is correct, otherwise print The calculation is incorrect. ### Constraints: - The output must be single Python code block without any function definition. - The script should evaluate the expression as boolean comparison. If the evaluated result of exp matches ans, print The calculation is correct, otherwise print The calculation is incorrect."
        },
        {
            "title": "B Details of Our Method",
            "content": "The choice of the component of the tool-based verification function depends on the task: 1. Mathematical reasoning task: sLM generates executable programming code c1, then the code interpreter executes the code and validates the correctness of computations (Schick et al., 2023; Gou et al., 2024). Since the code interpreter executes the code as well as outputs the verification score, we can regard the extraction of the verification score part as the identity function, i.e., c2 = (c). Therefore, (x, y; , θ) for numerical reasoning tasks can be represented as: (x, y; , θ) = c2 = (c1), where c1 π(c x, y, Ic; θ). (8) 2. Knowledge-intensive task: The tool acts as retriever that returns set of relevant knowledge passages based on the input and candidate response y. Subsequently, sLM 17 Preprint. Under review. verifies the consistency between the retrieved knowledge and the claims within y. Since the retriever utilizes and directly as query, we can regard the tool-calling query part as the identity function, i.e., c1 = (x, y). Therefore, (x, y; , θ) for knowledge-intensive tasks can be represented as: (x, y; , θ) = c2 π (cid:16) (c1) , x, y, ; θ (cid:17) , where c1 = (x, y), (9)"
        },
        {
            "title": "C Proof for Theoretical Analysis",
            "content": "C.1 Proof of Lemma 5.1 Proof. has cardinality = 2 (M 1)3 = Θ(M3). Also, Theorem 1.1 in Brown et al. (2021) says that any learning algorithms that is ε-close-to-optimal with sufficiently small ε > 0 also satisfies that the mutual information between data samples and the model learned by given the data distribution is proportional to at least the number of data samples multiplying dimension of data dimension. Since the number of data samples cardinality in subsection 5.1 is Θ(M3) and the data dimension is 1, we can state that (X; θ P) = Ω(cid:0)M3(cid:1). This proof says that if model directly memorizes which (a, b, c) pairs map to each = + with near-zero error, θ must encode on the order of M3 bits of information about X. C.2 Proof of Theorem 5.2 Proof. In this case, has access to the tool function as follows: (a, b, c; ) = 1 a+b = . Then, θ = A(X) such that (a, b, c; θ, ) = 1 a+b = . Regardless of X, can perfectly get label r, resulting in errq,X (A) = 0. In addition, since θ is determined regardless of through A, we get I(X; θ P) = H(θ P) H(θ X, P) = H(θ P) H(θ P) = 0, where is the entropy. Therefore, we can state that (X; θ P) = 0. C.3 Proof of Theorem 5.3 We first show Best-of-N accuracy first in Theorem C.1. Then we show the monotonicity with respect to the p. Theorem C.1 (Best-of-N Accuracy with Imperfect Verifier). Let the generator output 0 or 1 with equal probability, i.e., 1 2 and let the verifier with noise level p, be defined as follows: π (0x) = π (1x) = , rp,q (x, 0) = (cid:40) 0, w.p. p, 1, w.p. 1 p, rp,q (x, 1) = (cid:40) 1, w.p. q, 0, w.p. 1 q, with the condition that > 1 and > 1 p. Then, for 1, the probability that the best-of-N output is ground truth label, i.e. 1, is given by πN (1 x) = + 1 (cid:34) 1 (cid:18) 1 + 2 (cid:19)N(cid:35) + 1 1 + (cid:18) 1 + 2 (cid:19)N . (10) 18 Preprint. Under review. Proof. single sample from the generator π is labeled 1 with probability 1 probability 1 2 . Given the verifier with noise level and q, the joint probability are: 2 and 0 with (y = 1, = 1 x) = (y = 0, = 1 x) = (y = 1, = 0 x) = (y = 0, = 0 x) = 1 2 1 2 1 2 1 2 q, (1 p), (1 q), p. Then, the probability that single sample yields verifier score of 1 and 0 are (r = 1 x) = i{0,1} (y = i, = 1 x) = 1 2 (1 p) + 1 (q) , (r = 0 x) = 1 (r = 1 x) = 1 + 2 , respectively. Define the event (cid:110) = at least one Best-of-N sample is r(x, y) = 1 (cid:111) . Then, from Equation 11, the probability that all candidates yield = 0 is P(Ac) = P(r = 0)N = (cid:18) 1 + 2 (cid:19)N , P(A) = 1 P(Ac) = 1 (cid:18) 1 + (cid:19)N . and consequently, Consider two cases: (11) (12) (13) Case 1. At least one candidate yields = 1 (event occurs). In this case, the final output is chosen uniformly among the candidates with = 1. For any candidate with = 1, the probability that it originated from = 1 is given by P(cid:0)y = 1 = 1, x(cid:1) = P(y = 1, = 1 x) P(r = 1 x) = 1 2 1 2 (q + 1 p) = + 1 . Case 2. All candidates yield = 0 (event Ac occurs). In this case, the output is chosen uniformly among all candidates. For candidate with = 0, the probability that it is 1 is P(cid:0)y = 1 = 0, x(cid:1) = P(y = 1, = 0 x) P(r = 0 x) = 1 2 (1 q) 1 2 (p + 1 q) = 1 + 1 . By the law of total probability, the overall probability that the Best-of-N output is 1 is πN(1 x) = P(A) P(cid:0)y = 1 = 1, x(cid:1) + P(Ac) P(cid:0)y = 1 = 0, x(cid:1) = P(A) + 1 + P(Ac) 1 + 1 . 19 Preprint. Under review. From equation 12 and equation 13, we obtain (cid:32) (cid:19)N(cid:33) πN(1 x) = 1 (cid:18) 1 + 2 + 1 + (cid:18) 1 + 2 (cid:19)N 1 + 1 . (14) Using Lemma C.1 , Theorem 5.3 is proven as follows: Proof. Define the difference = q, so that 1 + 2 = 1 + , and note that + 1 = 1 and + 1 = 1 + . Then, from Lemma C.1 , Equation 14 is rewritten as (cid:32) () = 1 (cid:19)N(cid:33) (cid:18) 1 + 2 1 + (cid:18) 1 + 2 (cid:19)N 1 1 + . (15) Since is held fixed, an increase in corresponds to an increase in . Define so that A() = (cid:18) 1 + (cid:19)N , () = (cid:2)1 A()(cid:3) 1 + A() 1 1 + . Differentiating equation 15 with respect to yields () = A() 1 + [1 A()] (1 )2 + A() 1 1 + A() 1 (1 + )2 , (16) with A() = (cid:18) 1 + 2 2 (cid:19)N . Reformulating Equation 16 results in () = (cid:104) (1 )2 1 A() (cid:0)1 (cid:1) A() (cid:125) (cid:123)(cid:122) (cid:124) (a) (cid:105) + 1 (1 + )2 (cid:104)(cid:0)1 + (cid:1) A() A() (cid:124) (cid:125) (cid:123)(cid:122) (b) (cid:105) . (a) Define = 1+ 2 . Then, 1 A() (cid:0)1 (cid:1) A() = 1 xN (1 x) xN1. Set g(x) = 1 xN (1 x) xN1. Then, g(0) = 1, g(1) = 0, and g(x) 0 induces g(x) >= 0 for [0, 1]. Since q1 q2 [1, 1], [0, 1] is satisfied. Therefore, 1 A() (cid:0)1 (cid:1) A() > 0. (b) Since A() = 2 (cid:19)N1 (cid:18) 1+ and A() = (cid:19)N (cid:18) 1+ 2 , we have (1 + ) A() = (1 + ) (cid:19)N1 (cid:18) 1 + 2 2 = (cid:19)N (cid:18) 1 + = A(). Hence, (1 + ) A() A() = A() A() = (N 1) A(). For 2 and > 1, both 1 > 0 and A() > 0. Therefore, (cid:0)1 + (cid:1) A() A() > 0. 20 Preprint. Under review. Since (a) and (b) are non-negative, we conclude that () > 0 for all and 2. Thus, for 2, if > (i.e. 1 > 2), it follows that (1) > (2) , or equivalently, πN(1 x) (cid:12) (cid:12) (cid:12)p= > πN(1 x) (cid:12) (cid:12) (cid:12)p= ."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Model We use LLaMA-3.2-1B-Instruct (Dubey et al., 2024), Qwen-2.5-0.5B-Instruct (Yang et al., 2024), SmolLM2-360M-Instruct (Allal et al., 2025) as base models for our experiments. D.2 Training Hyperparameters & Setting As mentioned in subsection 4.2, we fine-tune small language models (sLMs) for each module using LoRA (Hu et al., 2022). However, for PRM, we fine-tune the full model including the classifier head following Wang et al. (2024a). Only for SmolLM2-360M-Instruct, we fine-tune the model on generation as it achieves under 10% accuracy on both GSM8K and MATH. We organize the hyperparameter details in Table 1. We use 4 A100 40GB GPUs with FSDP (Zhao et al., 2023) for training. Table 1: Hyperparameters used in fine-tuning sLM for each component. Hyperparameter Verifier 1 104 16 2048 64 Learning rate Batch size Max length LoRA rank LoRA α Optimizer Training epochs Scheduler PRM 1 105 16 2048 - - AdamW AdamW AdamW 3 Linear ToolV 1 104 16 2048 64 128 3 Linear 1 Linear Dataset for Distillation We perform distillation using the training split of each dataset. For MMLU-Pro, we adopt the train-test split provided by Zeng et al. (2025). Training dataset sizes are 7473 for GSM8K, 7500 for MATH, 1284 for MMLU-Pro. During distillation, we prompt the teacher modelgpt-4o-mini-2024-07-18 in our experimentsto generate sequences used as supervision for training. For the generative verifier, we follow the prompt design from Zhang et al. (2024). Specifically, we generate 8 completions per problem using temperature of 0.6, and treat these outputs as the training data. For code generation tasks, we apply the Prompt D.1. Using this prompt, we generate 4 completions per problem at temperature of 0.6, which are then used as training samples. For fact-checking in MMLU-Pro, we similarly generate 8 completions per problem with temperature of 0.6, using the teacher model to construct the training dataset. We use the Prompt D.2. In addition, we retrieve 3 documents for fact-checking from wikipedia abstracts using BM25 implemtented in Pyserini (Lin et al., 2021). 21 Preprint. Under review. Prompt D.1: Code Generation SYSTEM PROMPT: Write Python code block that verifies whether given solution is correct based on the provided question, following these guidelines: - The code should be single Python block, formatted as: python CODE - The code should only print True if the solution is verified as correct. Otherwise, it should only print False if the solution is incorrect. - Use only the following built-in modules where necessary: - math (for floating-point comparisons using math.isclose()) - sympy (for symbolic calculations, including π and fractions) - cmath (for complex number operations) - For floating-point comparisons, use math.isclose() instead of ==. - Use sympy.pi for π and sympy.Rational for fractions. - Simplify all fractions and square roots without converting them to decimal values. USER PROMPT: ### Input - Question: {question} - Solution: {solution} ### Output: Return python code only. Prompt D.2: Fact-checking generation SYSTEM PROMPT: You are domain expert. USER PROMPT: Check the factual correctness of each statement in the provided solution to the question, using only the information available in the given document. - Evaluate only the explicit factual claims made in the solution. Do not verify or evaluate the final conclusion or answer itself (e.g., The answer is ...). - If statement is factually incorrect based on the document, mark it as incorrect. - If statement cannot be verified using the document (i.e., the document does not confirm or deny it), treat it as not verifiable, and assume it is correct for the purpose of final verification. question{question}/question document{document}/document solution{solution}/solution At the end of the fact check, provide final summary in the following format: Verification: Are all statements correct? (Yes/No)? (where is either Yes or No). If any verifiably false statement is found, output: Verification: Are all statements correct? (Yes/No)? No If no false statements are found (i.e., all are either correct or unverifiable), output: Verification: Are all statements correct? (Yes/No)? Yes 22 Preprint. Under review. Table 2: Performance comparison between GenRM and ToolV + GenRM. Results are from experiments with Llama-3.2-1B-Instruct on MATH500 benchmark. Method Accuracy Precision Recall F1 Score GenRM GenRM + ToolV 80.91% 86.99% 0.6153 0.7666 0.7759 0.7427 0.6863 0.7545 Table 3: Performance of LLama-3.2-1B-Instruct on the MATH500 benchmark for Python code generation, using teacher model outputs as reference (gold). We set rejection as positive label for computing precision, recall, and f1 score. Model Size Accuracy Precision Recall F1 Score 1B 3B 8B 0.7687 0.7973 0.7906 0.8720 0.8949 0.9207 0.6946 0.7286 0. 0.7733 0.8033 0.7893 D.3 Evaluation Hyperparameters & Setting We generate = 64 solutions using temperature of 0.8. In the case of GenRM, we follow the chain-of-thought variant proposed by Zhang et al. (2024). As in their setup, we generate = 8 rationales and average the correctness scores across them, following the self-consistency method (Wang et al., 2023), using temperature of 0.6. For PRM, we apply the final score aggregation approach, consistent with previous studies (Wang et al., 2024a; Snell et al., 2024). When using ToolV for mathematical reasoning, we generate 4 code completions with temperature of 0.6 and consider the result correct if at least one of the generated codes passes. In knowledge-intensive reasoning with ToolV, we generate 4 rationales at the same temperature and consider the result correct only if all of them pass. In the case of MMLU-Pro, we retrieve three documents following the training setup. Gold documents are generated from each question using GPT-4o."
        },
        {
            "title": "E Additional experimental results",
            "content": "E.1 Accuracy of generative reward model in verification generation In Table 2, we report the accuracy of GenRM with and without ToolV. The results show that ToolV significantly improves accuracy, precision, and F1 score, indicating that it effectively removes false positive cases among the solutions. The confusion matrix in Figure 10 further illustrates this trend. However, it also reveals that ToolV occasionally removes true positives, primarily due to incorrectly generated Python code. E.2 Accuracy of tool-based verifier in code generation In Table 3, we present the accuracy of Python code generation, treating the teacher-generated code as the ground truth. The results show that precision is quite higheven for the 1B model, the distilled 1B ToolV is able to filter out more than 85% of incorrect solutions among the incorrect solutions that teacher model predicted. However, the recall is low, indicating that the generated code sometimes mistakenly filters out correct solutions. E.3 Data efficiency of RM and tool-based verifier in distillation In Figure 11 We conduct experiments to evaluate how much data is needed for each RMbased and tool-based verifier to achieve satisfactory performance. In each plot, we reduce the distillation data to 10% and 1% for one verifier, while keeping the other verifier fully distilled using 100% of the dataset. The results show that ToolV maintains competitive performance even with only 10% of the data, demonstrating its data efficiency during distillation. 23 Preprint. Under review. Figure 10: Confusion matrix of verification results from GenRM and GenRM + ToolV, where True denotes the correct solution. This result indicates ToolV improves the performance on removing false positive cases. Results are from experiments with Llama-3.2-1B-Instruct on MATH500 benchmark. Figure 11: Data-scale experiment. Performance comparison with varying distillation data sizes. In each plot, one verifier is distilled with 10% or 1% of data, while the other uses the full dataset. ToolV remains competitive even with only 10% of data, highlighting its data efficiency. Results are from experiments with Llama-3.2-1B-Instruct on MATH500. Figure 12: MMLU-Pro with PRM (Line Plot). Weighted Best-of-N performance of Llama3.2-1B-Instruct on three knowledge-intensive domains from MMLU-Pro. E.4 Additional plots for MMLU-Pro In Figure 12, we plot the best-of-N results for all values used in the experiments from Figure 6. Compared to math reasoning tasks, the plot is less clearly separated. However, ToolV + PRM generally outperforms the other baselines and clearly surpasses them even when using gold documents. 24 Preprint. Under review."
        },
        {
            "title": "F Case analysis",
            "content": "In this section, we present cases where ToolV either enables or fails the self-verification process. All examples are taken from level 4 problems in MATH500. In Example F.1, the solution is incorrectit computes 37 2 as 374, which is wrong. However, the verifier (GenRM-CoT) fails to detect this error and incorrectly marks the step as correct, ultimately concluding that the entire solution is valid. In this case, ToolV implements program that checks the correctness of the solution by solving the equation using the SymPy library. As expected, the result of the check is False, successfully identifying and removing the incorrect solution. Occasionally, ToolV may produce incorrect Python code, as discussed in subsection E.2. To illustrate such case, we present failure example in Example F.2. Here, the solution is actually correct, and GenRM correctly verifies it. However, ToolV generates code that leads to an incorrect conclusion. Although the code appears reasonable, the comparison sp.simplify(result) == sp.simplify(expected result) might return False, since symbolic expressions can differ in form even when they are mathematically equivalent. To properly compare equations, the code should instead use sp.Eq(...) which is more reliable for symbolic equality checks. 25 Preprint. Under review. Prompt F.1: Success case 1 of ToolV Problem: Half the value of 3x 9 is + 37. What is the value of x? 2 = + 37. Solution: ## Step 1: Translate the given information into an equation We are given that half the value of 3x 9 is + 37. This can be translated into the equation 3x9 ## Step 2: Multiply both sides of the equation by 2 to clear the fraction Multiplying both sides of the equation by 2 to clear the fraction gives us 3x 9 = 2x + 374. ## Step 3: Subtract 2x from both sides of the equation to isolate the variable term on one side Subtracting 2x from both sides of the equation gives us 9 = 374. ## Step 4: Add 9 to both sides of the equation to further isolate the variable term Adding 9 to both sides of the equation gives us = 383. The final answer is: 383 GenRM-CoT: Lets verify the provided solution step by step. ## Step 1: Translate the given information into an equation The solution states that half the value of 3x 9 is + 37. The translation into the equation 3x9 2 = + 37 is correct. **Verification:** This step is correct. ## Step 2: Multiply both sides of the equation by 2 to clear the fraction The solution claims that multiplying both sides by 2 gives 3x 9 = 2x + 374. This is accurate since multiplying both sides by 2 eliminates the fraction. **Verification:** This step is correct. ## Step 3: Subtract 2x from both sides of the equation to isolate the variable term on one side The solution states that subtracting 2x from both sides results in 9 = 374. This is also correct. **Verification:** This step is correct. ## Step 4: Add 9 to both sides of the equation to further isolate the variable term The solution states that adding 9 to both sides gives = 383. This is accurate as well, as adding 9 to 9 gives + 9 = 374 and thus = 374 9. **Verification:** This step is correct. The final answer is given as 383 . ### Final Verification All steps have been verified to be correct, and the final answer is consistent with the calculations. Verification: Is the answer correct (Yes/No)? Yes ToolV: import sympy as sp # Define the variable = sp.symbols(x) # Given equation equation = sp.Eq((3*x - 9) / 2, + 37) # Solve the equation solution = sp.solve(equation, x)[0] # Verify the solution is correct = sp.simplify(solution) == 383 print(is correct) Interpreter output: False Preprint. Under review. Prompt F.2: Failure case 1 of ToolV Problem: The operation & is defined for positive integers and as a&b = ab + ab What is the value of 9&2? Express your answer as common fraction in simplest radical form. . Solution: ## Step 1: Substitute the given values of and into the definition of &. We have = 9 and = 2, so we can substitute these values into the definition of & as follows: 9&2 = (cid:112)(9)(2) + 9 (cid:112)(9)(2) 2 ## Step 2: Simplify the expressions inside the square roots. We can simplify the expressions inside the square roots as follows: 18 + 9 18 2 ## Step 3: Further simplify the expressions by finding square roots of perfect squares inside the fraction. We can simplify the expressions by finding square roots of perfect squares inside the fraction as follows: 9&2 = 16 27 = 9&2 = 3 3 4 The final answer is: 3 3 4 GenRM-CoT: Lets verify the solution step by step. ## Step 1: Substituting values The solution correctly substitutes = 9 and = 2 into the expression for 9&2: 9&2 = (cid:112)(9)(2) + 9 (cid:112)(9)(2) . This step is correct. ## Step 2: Simplifying inside the square roots The next part of the solution simplifies the expressions inside the square roots: 9&2 = 18 + 9 18 2 = 27 16 . This simplification is also correct. ## Step 3: Finding square roots The solution proceeds to find the square roots: This simplification is correct as ## Final Answer The final answer given is: 27 = 3 3 and 16 = 4. 3 4 3 . 3 4 3 . This matches the expected answer provided in the verification section. In summary, each step in the solution was executed correctly and led to the correct final answer. Verification: Is the answer correct (Yes/No)? Yes Preprint. Under review. Prompt F.3: Failure case 1 of ToolV (cont.) ToolV: import sympy as sp # Define the operation def operation(a, b): return sp.sqrt((a * + a) / (a * - b)) # Given values = 9 = # Calculate the result result = operation(a, b) # Expected result expected result = sp.Rational(3, 4) * sp.sqrt(3) # Verify if the result matches the expected result is correct = sp.simplify(result) == sp.simplify(expected result) print(is correct) Interpreter output: False"
        }
    ],
    "affiliations": [
        "KAIST",
        "KRAFTON"
    ]
}