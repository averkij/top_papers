{
    "paper_title": "HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition",
    "authors": [
        "Gio Paik",
        "Yongbeom Kim",
        "Soungmin Lee",
        "Sangmin Ahn",
        "Chanwoo Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at https://github.com/ThetaOne-AI/HiKE"
        },
        {
            "title": "Start",
            "content": "HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition Gio Paik1,5, Yongbeom Kim2,5, Soungmin Lee3,5, Sangmin Ahn1,2,5,, Chanwoo Kim1,4,5, 1Theta One AI, 2Seoul National University, 3Georgia Institute of Technology, 4Williams College, 5ROKAF Reserve Forces Correspondence: giopaik0@gmail.com : Same Contribution 5 2 0 2 5 ] . [ 2 3 1 6 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical KoreanEnglish code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of highquality, natural CS data across various topics, but also provides meticulous loanword labels and hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable systematic evaluation of models ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at https: //github.com/ThetaOne-AI/HiKE."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Automatic Speech Recognition (ASR) (Radford et al., 2023; Puvvada et al., 2024; Saon et al., 2025) have pushed monolingual error rates below 5% on standard benchmarks (Panayotov et al., 2015; Conneau et al., 2022; Rousseau et al., 2012), enabling novel applications such as vibe coding, language education utilizing AI, and automated podcast summarization that are redefining human-computer interaction. However, it remains significantly underexplored question whether the performance of these ASR models generalizes to Code-Switching (CS) scenarios, where multiple languages are mixed within single utterance. Consequently, the field of CS-ASR remains 1 Figure 1: Code-Switching Examples by CS-Level underdeveloped (Agro et al., 2025), especially for language pairs involving low-resource and typologically distant languages such as Korean and English. This research gap significantly impairs the user experience for the large global population of multilingual individuals who use CS as natural, everyday part of communication, particularly in regions where English is not the primary language. To address this research gap, this paper introduces HiKE: Hierarchical Korean-English codeswitching benchmark consisting of 1, 121 highquality CS utterances covering various topics (e.g., Software Engineering, Language Education). To reflect the various forms of CS that occur in realworld scenarios, we labeled the utterances according to three hierarchical CS-levels (i.e., word-, phraseand sentence-level), as illustrated in Figure 1. Using HiKE, we conduct two-part analysis. First, we evaluate nine multilingual ASR models, spanning range of architectures and model sizes, to assess their Korean-English CS-ASR capabilities. Second, we compare the CS-ASR performance achieved by fine-tuning with two different data types: natural wordand phrase-level CS data and synthesized sentence-level CS data. The contributions of this paper are three-fold: First, to the best of our knowledge, HiKE is the first to release publicly available Korean-English CS speech recognition benchmark, which includes loanword labels and CS-level annotations. Second, leveraging our loanword and hierarchical CS-level annotations, we precisely measure how the performance of 9 multilingual ASR models varies depending on the type of CS. Third, our fine-tuning experiments demonstrate that models CS-ASR capabilities can be effectively enabled through finetuning, and that this is achievable not only with natural CS data but also with synthetic data created by concatenating monolingual utterances."
        },
        {
            "title": "2.1 Script Writing & Cloning",
            "content": "We built our dataset through human-LLM collaborative process to ensure both high quality and minimal human effort. We began by manually authoring 575 seed scripts across 8 topics  (Table 1)  . Each script was then used as one-shot example to prompt CLAUDE-3.5-SONNET (Anthropic, 2024) to generate new script mimicking the originals topic and CS structure. As final quality control step, all generated scripts were manually reviewed and corrected by the authors."
        },
        {
            "title": "2.2 Recording",
            "content": "We recruited 13 bilingual Korean-English speakers, each of whom recorded 50 to 100 scripts in quiet environment using web-based interface on their personal devices, such as laptops and mobile devices. This process yielded an initial batch of 1, 150 recordings. Following manual review by the authors, 29 samples that deviated from the script were discarded, resulting in final curated dataset of 1, 121 high-quality utterances totaling approximately 2.2 hours."
        },
        {
            "title": "2.3 CS-Level Labeling",
            "content": "In contrast to previous work (Shi et al., 2020; Lovenia et al., 2022; Li et al., 2022) that only classified CS by its location within the utterance (intervs. intra-sentential), we propose more granular, three-level classification to analyze how models handle intra-sentential CS between typologically distant languages. HiKE categorizes codeswitching into three distinct levels: sentence, word, and phrase. Sentence-level CS is the most predictable, as switches occur only at utterance boundaries. Word-level CS involves the substitution of single lexical units, primarily testing models bilingual lexicon. Phrase-level CS, in contrast, poses more complex grammatical challenge, as it can introduce irregular structures like altered word"
        },
        {
            "title": "Topic",
            "content": "CS-Level"
        },
        {
            "title": "Total Proportion",
            "content": "word phrase sentence academic business entertainment everyday conversation language education medical software development travel and culture 40 45 53 69 82 28"
        },
        {
            "title": "Total",
            "content": "457 110 112 29 76 75 48 113 44 607 7 12 8 13 1 0 6 10 57 157 169 90 158 158 76 162 1,121 14.0% 15.1% 8.0% 14.1% 14.1% 6.8% 14.5% 13.5% 100% Table 1: Number of Utterances by Topic & CS-Level order, difficulty that is significantly amplified for typologically distant pairs such as Korean and English."
        },
        {
            "title": "2.4 Loanword Labeling",
            "content": "Loanwords are words adopted from foreign language and adapted to the phonology and orthography of the new language. For example, the Korean loanword 버스 [b@s] and the English word bus [b2s] are pronounced almost identically and can be used interchangeably in CS context. This creates an evaluation challenge: If ground-truth label is strictly constrained to either Hangul or the Roman alphabet, model producing the alternateyet perfectly validtranscription would be unfairly penalized. This ambiguity introduces significant noise into the assessment of CS performance. To avoid this problem, we meticulously labeled all loanwords contained in our dataset. Subsequently, during evaluation, both the Korean and English versions of these loanwords were treated as valid answers. Through our loanword labeling process, we achieved more precise evaluation by decreasing measurement noise by an average of 5.1% in MER and 5.9% in PIER."
        },
        {
            "title": "2.5 Metrics",
            "content": "Given that CS-ASR involves mixing languages with different linguistic properties, many prior works (Shi et al., 2020; Zhou et al., 2025; Li et al., 2025) have adopted the Mixed Error Rate (MER), which evaluates character-based languages such as Mandarin and Korean at the character level and word-based languages such as English at the word level. In contrast to MER, which assesses the entire utterance, the Point of Interest Error Rate (PIER) (Ugan et al., 2025) was later proposed to specifically evaluate performance at the points where language transitions occur. In this paper, we evaluate the CS-ASR capabilities of models using 2 # Params Mixed Error Rate (MER) Point of Interest Error Rate(PIER) Monolingual Word Phrase Sentence Overall Word Phrase Sentence Overall KOR ENG SENSEVOICE-SMALL (An et al., 2024) WHISPER-TINY (Radford et al., 2023) WHISPER-BASE (Radford et al., 2023) WHISPER-SMALL (Radford et al., 2023) WHISPER-MEDIUM (Radford et al., 2023) WHISPER-LARGE (Radford et al., 2023) SEAMLESS-M4T-V2-LARGE (Barrault et al., 2023) GPT-4O-TRANSCRIBE (Hurst et al., 2024) AUDIO-FLAMINGO-3 (Goel et al., 2025) 234M 38M 74M 244M 769M 1.5B 2.3B N/A 8.3B 29.3 37.9 78.8 124.7 59.2 44.6 30.6 115.0 17.8 79.2 128.7 84.2 46.0 33.6 23.5 71.7 27.1 83. 26.8 39.1 28.3 20.6 17.9 12.3 65.1 27.5 80.8 33.9 103.8 97.8 50.1 37.3 25.8 89.0 23.3 81. 53.9 89.1 92.2 69.3 40.6 43.7 78.1 25.9 102.2 56.6 76.4 76.5 46.7 51.7 28.9 64.7 30.8 98. 38.8 68.5 42.4 31.6 31.1 18.9 58.2 54.6 81.2 81.2 55.2 46.1 34.4 69.8 33.5 116.0 28.9 100. 6.4 11.9 7.8 4.5 3.4 3.2 6.4 2.5 25.1 7.6 14.6 9.8 8.3 4.6 4.4 6.3 3.3 8. Table 2: Benchmark Results. For each model, the best and worst scores are bolded and underlined, respectively. Monolingual performance is measured on the FLEURS dataset, using CER for Korean and WER for English. Mixed Error Rate (MER) Point of Interest Error Rate (PIER)"
        },
        {
            "title": "ENG",
            "content": "WHISPER-MEDIUM 44.6 33.6 17.9 37.3 40. 51.7 31.1 46.1 3.4 4.6 (a) FT with Natural Intra-Sentential CS Data (b) FT with Synthetic Inter-sentential CS Data (a) + (b) FT with Both Data 9.9(-34.7) 21.0(-23.6) 22.6(-22.0) 10.3(-23.3) 27.6(-6.0) 22.7(-10.9) 8.0(-9.9) 7.5(-10.4) 6.0(-11.9) 10.0(-27.3) 23.9(-13.4) 21.8(-15.5) 19.8(-20.8) 33.0(-7.6) 27.9(-12.7) 19.1(-32.6) 32.9(-18.8) 36.9(-14.8) 22.7(-8.4) 16.0(-15.1) 13.5(-17.6) 19.5(-26.6) 32.1(-14.0) 32.0(-14.1) 6.0(+2.6) 3.1(+0.3) 3.9(+0.5) 5.2(+0.6) 5.7(+0.5) 4.9(+0.3) Table 3: Fine-Tuning (FT) Results. For each metric, the best scores are highlighted in bold. Monolingual performance is measured on the FLEURS dataset, using CER for Korean and WER for English. both MER and PIER. For our PIER evaluation, we tagged the words at the locations where codeswitching occurs. This allowed us to assess whether the ASR model can accurately switch languages precisely at these transition points."
        },
        {
            "title": "3.1 CS-ASR of Multilingual ASR Models",
            "content": "We evaluated 9 multilingual ASR models with diverse architectures (CTC, Transformer, and LLMbased), assessing their CS-ASR performance using MER and PIER metrics. As shown in Table 2, severe performance drop occurred on CS data across all models, with the MER increasing by factor of 3 to 14 compared to monolingual performance, revealing significant limitations for practical use. closer look at specific architectures reveals further nuances. For instance, the CTC-based SENSEVOICE-SMALL, despite showing stable performance across CS-levels and better overall MER than the similarly-sized WHISPER-SMALL, still exhibited high error rate at the actual codeswitching points. This suggests that while CTC architectures may be robust in overall transcription, they struggle specifically with the precise moment of language transition. Furthermore, our analysis revealed clear divergence in performance patterns between nonLLM and LLM-based models. Most non-LLM models performed best on sentence-level CS and worst on word-level CS, which we attribute to the higher complexity and unpredictable nature of intra-sentential switch points. In contrast, the LLM-based GPT-4O-TRANSCRIBE exhibited the opposite behavior, performing best on word-level CS. We hypothesize that this reflects the distribution of its training data, as large text corpora are rich in word-level CS but contain comparatively few instances of sentence-level CS. To analyze the effect of model scale on CS-ASR performance, we evaluated the Whisper family of models. The results show clear correlation between size and capability: CS-ASR performance, nearly absent in the smallest models (e.g., Tiny and Base), gradually emerges with increasing scale. Despite this trend, even the largest models error rate on CS data was over six times higher than on monolingual data, indicating that model scaling alone is an insufficient solution for achieving practical CS-ASR performance."
        },
        {
            "title": "3.2.1 Experimental Details\nTo investigate the effect of fine-tuning with CS-\nASR performance, we prepared two distinct types\nof training data. The first was a natural, intra-\nsentential (word- and phrase-level) CS dataset from\nAIHub (AI-Hub, S. Korea). The second was a syn-\nthetic sentence-level CS dataset, which we gener-\nated by concatenating monolingual Korean and En-\nglish utterances from the FLEURS (Conneau et al.,",
            "content": "3 2022) and Common Voice (Ardila et al., 2020) datasets. Using these, we fine-tuned WHISPERMEDIUM (Radford et al., 2023) under three conditions: using the intra-sentential CS set only, the synthetic inter-sentential set only, and combination of both."
        },
        {
            "title": "3.2.2 Results",
            "content": "The results in Table 3 reveal that fine-tuning is an effective method for enabling CS-ASR, and that this can be achieved not only with natural intrasentential CS data (a) but also with synthetic intersentential CS data (b) created by concatenating monolingual utterances. As shown in row (b) of Table 3, fine-tuning of synthetic CS data alone improved both the overall MER and PIER by more than 13%. Given the wide availability and relative ease of collecting monolingual data compared to authentic CS data, this finding suggests that data synthesis via utterance concatenation represents promising and cost-effective direction for training CS-ASR models, especially in resourceconstrained scenarios. However, fine-tuning on natural intra-sentential CS data (a) yielded greater performance improvements than fine-tuning on synthetic inter-sentential data (b, c) across almost all metrics; the only exceptions were the MER and PIER for sentence-level CS. Although this result indicates that it is currently preferable to use natural intra-sentential CS data for fine-tuning when available, we anticipate that these limitations can be overcome. With the development of more sophisticated data synthesis or fine-tuning techniques, it may become possible to train robust CS-ASR models using only synthetic data."
        },
        {
            "title": "3.3 Qualitative Results",
            "content": "In our analysis of the CS transcription results, we primarily observed three types of errors, as illustrated in Figure 2. (i) Phonetic Transcription refers to cases where, for words that are not loanwords, the model does not transcribe them in the correct language but instead writes them out phonetically using the script of the other language. This error was commonly observed across all models. (ii) Instruction Following Failure is an error that occurs in multi-task models (e.g., WHISPER) capable of handling tasks beyond transcription, such as translation and question answering. This was especially pronounced with AUDIO-FLAMINGO-3; while this error was rare in monolingual settings, its high frequency in CS environments made the model unreliFigure 2: CS-ASR Fail Cases able for transcription without separate verification step. Finally, (iii) Hallucination is an error common in seq2seq models, including those based on Transformers and LLMs. It refers to cases where the model incorrectly generates repetitive or excessive content that is not present in the audio. Qualitatively, while errors like phonetic transcription often occurred even with monolingual data, instruction following failures and hallucinations increased markedly in CS data. We attribute this trend to the fact that during training, ASR models are exposed to abundant monolingual data but extremely rare CS data, which hinders the generalization of their performance to CS scenarios."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we propose HiKE, the first public high-quality hierarchical benchmark for KoreanEnglish CS-ASR with hierarchical CS-level labels and loanword labels. Our evaluation of 9 multilingual models in HiKE shows that strong monolingual performance does not generalize to CS scenarios. Specifically, we found that models are less accurate on wordand phrase-level CS, which feature dense, irregular switch points and complex grammatical structures, in contrast to the more predictable sentence-level CS. Furthermore, our fine-tuning experiments demonstrate that models CS-ASR capabilities can be improved, even with synthetic inter-sentential CS data created by concatenating monolingual utterances. We believe that this work will serve as foundation for future research in CS-ASR, including developing models for diverse language pairs beyond Korean-English, generating high-quality synthetic CS data, and analyzing the generalization of CS-ASR capabilities."
        },
        {
            "title": "Limitations",
            "content": "Our study has two primary limitations. First, the scarcity of ASR models that support both Korean and English precluded broad comparative study of diverse architectures. This was especially true for LLM-based models, as only GPT-4OTRANSCRIBE yielded meaningful CS-ASR performance, which prevented reliable analysis of their common characteristics. Second, the scarcity of high-quality CS data restricted our fine-tuning experiments to small scale, precluding thorough investigation into methods for eliciting robust CS capabilities, such as by scaling up synthetic data. We believe these limitations point to several avenues for future research."
        },
        {
            "title": "References",
            "content": "Maha Tufail Agro, Atharva Kulkarni, Karima Kadaoui, Zeerak Talat, and Hanan Aldarmaki. 2025. Codeswitching in end-to-end automatic speech recognition: systematic literature review. arXiv preprint arXiv:2507.07741. AI-Hub, S. Korea. Korean-english mixed speech recognition dataset. https://www.aihub.or.kr/ aihubdata/data/view.do?dataSetSn=71260. Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, and 1 others. 2024. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051. Anthropic. 2024. Claude 3.5 sonnet model card addendum. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common Voice: MassivelyMultilingual Speech Corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC), pages 42184222. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 1244912460. Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, and 1 others. 2023. Seamless: Multilingual expressive and streaming speech translation. arXiv preprint arXiv:2312.05187. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022. FLEURS: FEWShot Learning Evaluation of Universal Representations of Speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang-gil Lee, Chao-Han Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, and 1 others. 2025. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models. arXiv preprint arXiv:2507.08128. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. In IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), volume 29, pages 34513460. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Chengfei Li, Shuhao Deng, Yaoping Wang, Guangjing Wang, Yaguang Gong, Changbin Chen, and Jinfeng Bai. 2022. TALCS: An open-source MandarinEnglish code-switching corpus and speech recognition baseline. In Interspeech 2022, pages 17411745. Yupei Li, Zifan Wei, Heng Yu, Huichi Zhou, and Björn Schuller. 2025. Dota-me-cs: Daily oriented text audio-mandarin english-code switching dataset. arXiv preprint arXiv:2501.12122. Holy Lovenia, Samuel Cahyawijaya, Genta Winata, Peng Xu, Yan Xu, Zihan Liu, Rita Frieske, Tiezheng Yu, Wenliang Dai, Elham J. Barezi, Qifeng Chen, Xiaojuan Ma, Bertram Shi, and Pascale Fung. 2022. ASCEND: spontaneous Chinese-English dataset for code-switching in multi-turn conversation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC), pages 72597268. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Krishna C. Puvvada, Piotr Zelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, and Boris Ginsburg. 2024. Less is More: Accurate Speech Recognition & Translation without Web-Scale Data. In Interspeech 2024, pages 39643968. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak In International Conference on MaSupervision. chine Learning (ICML), pages 2849228518. 5 Anthony Rousseau, Paul Deléglise, and Yannick Estève. 2012. TED-LIUM: an Automatic Speech Recognition dedicated corpus. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC), pages 125129. George Saon, Avihu Dekel, Alexander Brooks, Tohru Nagano, Abraham Daniels, Aharon Satt, Ashish Mittal, Brian Kingsbury, David Haws, Edmilson Morais, and 1 others. 2025. Granite-speech: open-source speech-aware llms with strong english asr capabilities. arXiv preprint arXiv:2505.08699. Xian Shi, Qiangze Feng, and Lei Xie. 2020. The asru 2019 mandarin-english code-switching speech recognition challenge: Open datasets, tracks, methods and results. arXiv preprint arXiv:2007.05916. Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard Bärmann, and Alex Waibel. 2025. PIER: Novel Metric for Evaluating What Matters in Code-Switching. In 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems (NeurIPS), volume 30. Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, and 1 others. Cs-dialogue: 104-hour dataset of 2025. spontaneous mandarin-english code-switching diarXiv preprint alogues for speech recognition. arXiv:2502.18913."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Automatic Speech Recognition Early deep learning-based ASR systems were often streaming-based, using architectures such as Connectionist Temporal Classification (CTC) to make predictions on short audio frames (Baevski et al., 2020; Hsu et al., 2021; An et al., 2024). subsequent paradigm shift occurred towards nonstreaming, Transformer-based encoder-decoder models (Vaswani et al., 2017; Radford et al., 2023; Barrault et al., 2023), which leverage full audio context to achieve superior accuracy and robustness. The most recent trend involves directly leveraging pretrained large language models (LLMs) (Goel et al., 2025; Hurst et al., 2024), offering high accuracy and task extensibility, but requiring immense computational resources. A.2 Code-Switching Speech Datasets Publicly available CS-ASR datasets are exceptionally rare due to the inherent challenges of data collection. Even for Mandarin-English, whose large speaker population has resulted in relative abundance of data (Shi et al., 2020; Zhou et al., 2025; Li et al., 2025), only handful of public benchmarks exist. The scarcity of datasets is particularly acute for typologically distant language pairs like Korean-English, presenting critical bottleneck for research into improving CS-ASR performance. While government-funded Korean dataset (AIHub, S. Korea) exists, its use is restricted to Korean nationals, making it inaccessible to the global research community. To the best of our knowledge, HiKE is the first globally accessible KoreanEnglish CS benchmark."
        },
        {
            "title": "B Experimental Setup",
            "content": "Unless otherwise specified, all experiment results were obtained from single run on NVIDIA RTX 6000 Ada GPU. We used pyTorch 2.8.0 and transformers 4.56.2 for our experiments. In finetuning experiments, all models were finetuned for approximately 1 epoch on single A100 GPU with batch size of 16, using cosine annealing scheduler with 10% warmup and peak learning rate of 1e 5."
        },
        {
            "title": "C Dataset Recording",
            "content": "As mentioned in Section 2.2, participants performed the data recordings using web-based interface like the one shown in Figure 3. The webpage 6 Figure 3: Screenshot of Recording Tool provided recording instructions and scripts in both Korean and English, and allowed participants to review their own recordings and perform re-takes if necessary."
        },
        {
            "title": "D License",
            "content": "Our experiments utilize the official code implementation of PIER (Ugan et al., 2025) (Apache 2.0 License), along with other standard machine learning libraries. Our own evaluation code, developed for the HiKE benchmark, will also be publicly released under the Apache 2.0 License."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Seoul National University",
        "Theta One AI",
        "Williams College"
    ]
}