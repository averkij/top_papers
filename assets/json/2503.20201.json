{
    "paper_title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
    "authors": [
        "Salaheddin Alzubi",
        "Creston Brooks",
        "Purva Chiniya",
        "Edoardo Contente",
        "Chiara von Gerlach",
        "Lucas Irwin",
        "Yihan Jiang",
        "Arda Kaz",
        "Windsor Nguyen",
        "Sewoong Oh",
        "Himanshu Tyagi",
        "Pramod Viswanath"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 0 2 0 2 . 3 0 5 2 : r Open Deep Search: Democratizing Search with Open-source Reasoning Agents Salaheddin Alzubi Creston Brooks Purva Chiniya Edoardo Contente Chiara von Gerlach Lucas Irwin Yihan Jiang Arda Kaz Windsor Nguyen Sewoong Oh Himanshu Tyagi Pramod Viswanath"
        },
        {
            "title": "Abstract",
            "content": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexitys Sonar Reasoning Pro and OpenAIs GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is general framework for seamlessly augmenting any LLMsfor example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMESwith search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES."
        },
        {
            "title": "Introduction",
            "content": "Search AIsSearch engine-augmented Large Language Models (LLMs)combine the Retrieval Augmented Generation (RAG) capabilities of LLMs (e.g., [9]) with real-time information retrieval from search engines. This integration addresses the challenge of LLMs static knowledge base enabling LLMs to provide up-to-date and contextually relevant responses. Recent works like [27] have shown that feeding Search Engine Result Page (SERP) APIs as context to an LLM outperforms previous methodologies, such as self-ask. Advances in search AI have been dominated by proprietary solutions such as Google search, Bing, chatGPT search, and Grok. In particular, Perplexity AI [21] has excelled in this market, even threatening mature leaders. However, such closed-source solutions limit transparency, innovation, and entrepreneurship. With the goal of growing community of developers in search AI, harnessing collective talent, fostering innovation, and encouraging entrepreneurship, we introduce Open Deep Search (ODS), an open-source AI search solution that achieves state-of-the-art performance in benchmark evaluations, matching or surpassing those achieved by the best closed-source alternatives. Sentient, University of Washington, Princeton University, UC Berkeley Llama3.1-70B w/o access to the web DeepSeek-R1 GPT-4o Perplexity [21] Perplexity Sonar Reasoning Pro [22] with access to the web GPT-4o Search Preview [17] ODS-v1+DeepSeek-R1 (ours) ODS-v2+DeepSeek-R1 (ours)"
        },
        {
            "title": "SimpleQA FRAMES",
            "content": "20.4 82.4 37.5 82.4 85.8 90.0 87.7 88.3 34.3 30.1 50.5 42.4 44.4 65.6 56.7 75.3 Table 1: The proposed open-source search framework of ODS, when used with the open-source reasoning LLM of DeepSeek-R1 [4], achieves performance exceeding that of closed-source stateof-the-art search AI solutions of Perplexity [21], Perplexity Sonar Reasoning Pro [22] on the two popular factuality evaluation Benchmarks of FRAMES [8] and SimpleQA [30]. Compared to GPT4o Search Preview, ODS-v2 has significantly better FRAMES accuracy but slightly worse SimpleQA accuracy. ODS-v1 uses ReAct-based agent (Section 2.2.1) and ODS-v2 uses CodeActbased agent (Section 2.2.2) . Perplexity AI has two search AI products with API accesses: the default Perplexity [21] and Perplexity Sonar Reasoning Pro tailored for complex reasoning tasks [22]. Table 1 shows their performance on two popular evaluation benchmarks of SimpleQA [30] and FRAMES [8]. Note that both versions of the proposed Open Deep Search (when used together with the open-source DeepSeek-R1 model) outperform the Perplexity AIs flagship search AI products. Another important baseline is OpenAIs GPT-4o Search Preview [17] tailored for search AI and released on 3/11/2025. The second version of Open Deep Search, dubbed ODS-v2+DeepSeek-R1, surpasses GPT-4o Search Preview on FRAMES and almost matches it on SimpleQA. To achieve this state-of-the-art performance, we make two innovations in Open Deep Search: Open Search Tool and Open Reasoning Agent. Open Search Tool. Perplexitys and OpenAIs search solutions are closed-source products. Opensource alternatives to Perplexity, such as OpenPerplex [19] and Perplexica [20] are open-source search tools whose output is summarized and fed into an LLM to answer the query of interest. However, these open-source search tools have several weaknesses that leave significant room for improvement. First, OpenPerplex and Perplexica primarily pass raw SERP results as context to the LLM. Additionally, they chunk and rerank snippets based on their relevance to the user query. In contrast, our approach employs more sophisticated search process, which we call Open Search Tool (explained in detail in Section 2.1). Specifically, we rephrase queries when necessary, extract context from the top snippets, and apply chunking and re-ranking to filter content above relevance threshold. This ensures the inclusion of all relevant search result context. Additionally, we implement custom website handling for major APIs such as Wikipedia, ArXiv, and PubMed. Open Reasoning Agent. As shown in Table 1, we provide two versions of Open Reasoning Agent: one based on the ReAct agent [33] and another based on the CodeAct agent [14]. The goals of Open Reasoning Agent are to interpret the query, assess the retrieved context, and use appropriate tools, including Open Search Tool we provide for web search, to answer the query. For details we refer to Section 2.2. Outline. In the next section, we give details of the search AI that we call Open Deep Search. In Section 3, we provide empirical results on the two evaluation benchmarks of SimpleQA and FRAMES, along with an ablation study of various components of ODS."
        },
        {
            "title": "2 Open Deep Search (ODS)",
            "content": "Open Deep Search (ODS) is plug-and-play framework, where the user can seamlessly plug in any base LLM of their choice, either open-source LLMs or through API accesses to closed-source 2 In our experiments, we use ODS with the Llama3.1-70B model or with DeepSeek-R1. LLMs. ODS consists of two parts, Open Search Tool and Open Reasoning Agent, both fo which use the base LLM. Open Search Tool is our open-source tool for searching and processing the information from the Internet explained in Section 2.1 and Open Reasoning Agent is our open-source agent that answers the queries with access to tools as explained in Section 2.2. We provide two solutions for the Open Reasoning Agent: one using the ReAct agent (which we call ODS-v1 for version one) and one using the CodeAct agent (which we call ODS-v2 for version 2). The open-source repository that contains both versions of ODS can be found at https://github. com/sentient-agi/OpenDeepSearch. We use the terminology ODS-v?+base-model to refer to specific instantiation for ODS. For example, ODS-v1+Llama3.1-70B refers to the version one of ODS that uses the ReAct agent and uses Llama3.1-70B as the base model, and ODS-v2+DeepSeekR1 refers to the version one of ODS that uses CodeAct agent and DeepSeek-R1 as the base model. Figure 1: user can choose to plug in any base LLM of their choice and harness the benefits of the open-source framework of Open Deep Search (ODS), which consists of two components: Open Search Tool and Open Reasoning Agent. query is first fed into Open Reasoning Agent which orchestrates the available set of tools to interpret and answer the query. The most important tool is the Open Search Tool that we design, which provides high quality context from multiple retrieved sources from the web. In our experiments we use Llama3.1-70B and DeepSeek-R1 as our base model."
        },
        {
            "title": "2.1 Open Search Tool",
            "content": "We provide Open Search Tool that improves upon recent advancements [27] in search engine augmented LLMs whilst maintaining the commitment to scalable open-source system. In particular, our approach revisits different components along the querying, retrieval, and augmentation pipeline of conventional methodologies. Open Search Tool takes query as input and produces context to be fed into the base LLM that consists of relevant passages from the web search. The quality of the Open Search Tool is critical in the success of ODS, as we show with examples in Section 3 and Appendix A."
        },
        {
            "title": "2.1.1 Query Rephrasing",
            "content": "The first step in our search pipeline involves ingesting the original user query and generating new rephrased queries that maintain the original context. The necessity of this step stems from the gap between the original querys semantic structure and the underlying context sought to provide satisfactory answer. For example, user may be interested in knowing how to make my Internet faster, however, Google search may not yield satisfactory results as the query itself is too broad while the implicit context may not be covered. The query rephraser bridges that gap between broad queries and implicit context and will generate other queries such as: How to make the Wi-Fi Signal stronger, How to increase bandwidth, and How to reduce latency. We have found this step to be crucial in improving the coverage and diversity of retrieved contexts, and consequently, the overall performance of our system."
        },
        {
            "title": "2.1.2 Retrieval",
            "content": "The second step in our search pipeline involves retrieving relevant contexts from search engine result page API (SERP)2. The retrieved results from the API call are then formatted, processed, and inserted into the context of an LLM. Our formatting procedure is inspired by the FreshPrompt [27] prompt format where we include meta-data returned each search result such as the title, URL, the description, and date authored (if available) from every snippet. Additionally, we prompt the LLM to prioritize reliable sources, such as government agencies, educational institutions, and established research institutions, over other sources when the search context contains conflicting information."
        },
        {
            "title": "2.1.3 Augmentation",
            "content": "As an additional step in our pipeline, we choose to augment the LLMs context by adding relevant passages from the top retrieved links by the SERP API. In particular, we scrape the associated webpages, embed passage chunks and retrieve the top relevant passages from each webpage based on the re-ranker score assigned relevant to the user query. This enables the contexts to provide in-depth answers related to queries that elicit deeper reasoning."
        },
        {
            "title": "2.2 Open Reasoning Agent",
            "content": "Open Reasoning Agent takes query from the user as an input and uses the base LLM and various tools to produce an answer. The Open Search Tool from the previous section is one of the critical tools for Open Reasoning Agent to use. We provide two solutions for the user to choose from: one that is based on Chain-of-thought and ReAct agent (whose resulting end-to-end system is referred to as ODS-v1) and one that is based on Chain-of-code and CodeAct agent (whose resulting end-to-end system is referred to as ODS-v2)."
        },
        {
            "title": "2.2.1 ODS-v1 with ReAct Agent",
            "content": "Our first Open Reasoning Agent is based on Chain-of-thought (CoT) reasoning [6, 31] and ReAct [33] agent. Chain-of-Thought (CoT) prompting elicits impressive reasoning capabilities in LLM agents by encouraging the model to stop and think prior to answering [28, 3]. Zero-shot CoT consists of simply appending the phrase Lets think step by step. to the end of the prompt fed to the model. [6] The results are even further improved by combining CoT with the few-shot prompting, an approach we adopt. Few-shot CoT consists of appending several CoT examples to the prompt as form of in-context learning [31]. Chain-of-Thought Self-Consistency (CoT-SC) further improves on the greedy decoding techniques utilized in simple CoT. Instead of naively taking single greedy reasoning path, CoT-SC samples multiple different pathways and compares them to then select the most consistent answer among all pathways. This has led to signficiant improvements upon naive CoT in multiple reasoning tasks including arithmetic and question-answering tasks. [29] ReAct Framework. ReAct (Reasoning and Action) is language model framework that synergistically combines reasoning steps with action execution to enhance task completion and decisionmaking capabilities [33]. ReAct combines reasoning and action through an iterative process that enables language models to perform complex tasks with enhanced reliability. The framework consists of three interleaved components: (atst) = (rt, ot1, ht1) where at is the current action, st the current state, rt the reasoning output, ot1 the previous observation, and ht1 the interaction history. The framework implements tool integration through standardized interface: Thought: [Reasoning trace] Action: Tool[parameters] Observation: [Result] 2In our case, we used: serper.dev 4 Few-shot learning in ReAct leverages small set of demonstrative examples to guide the models reasoning and action patterns. <Question> What is the capital of France? </Question> <Thought> need to determine the capital city of France. </Thought> <Action> search_internet </Action> <Action_Input> \"capital of France\" </Action_Input> <Observation> The capital of France is Paris. </Observation> <Final_Answer> Paris </Final_Answer> Figure 2: Illustration of the ReAct prompt structure utilized in ODS-v1. Dynamic Few-Shot Learning. ReAct implements few-shot learning through example-based prompting, where small set of demonstrations guides the models reasoning and action patterns. To optimize prompt efficiency, dynamic few-shot selection system utilizes vector similarity matching to retrieve the most relevant examples for each task, maintaining performance while reducing prompt complexity. We ran community campaign to design the 200 ReAct prompts used in our few-shot template. Participants were asked to apply their own intuitions of reasoning to the prompt designs, resulting in wide range of approaches. They were provided with template describing the structure of ReAct prompts (Thought/Action/Action Input/Observation) and some sample queries from test set. We made sure not to grant them access to the benchmarks themselves. The campaign substantially improved the performance of our ReAct agent due to the diverse range of thinking processes represented in the ReAct prompts. Examples from the resulting few-shot prompts are provided in Appendix B. Tool Integration. The framework is integrated with external tools, enabling more sophisticated problem-solving capabilities. We use three tools in ODS-v1: 1. Web Search: Open Search Tool from Section 2.1. 2. Mathematical Processing: integration with Wolfram Alpha API for handling arithmetic and complex mathematical computations. 3. Continue Thinking: continued reasoning to break down complex queries using the base LLM. ODS-v1. We integrate CoT consistency sampling, the ReAct agentic framework, and few-shot prompting in Open Reasoning Agent of ODS-v1. For any query, we first run Open Search Tool and the resulting context, along with the original query, is fed into the ReAct Agent. The ReAct agent uses prompts that consist of structured steps: < hought >, < Action >, < Action Input >, and < Observation >, beginning with < Question > and concluding with < inal Answer >. These steps iterate until response is returned, though the agent may not return an answer if insufficient information is available. The ReAct agent has three action options. Continue thinking (< Action >= continue think) extends the reasoning process for complex problem decomposition. Search (< Action >= search internet) leverages OpenPerplex for finding factual information like dates, names, and academic content. Calculate (< Action >= calculate) connects to the Wolfram Alpha API for handling numerical computations that foundation models typically struggle with. When the ReAct agent fails to provide an answeras determined by Llama-3-8B judge modelthe system defaults to Chain-of-Thought Self-Consistency. This backup process makes 5 calls to the LLM, clusters similar responses, and returns random response from the largest cluster, ultimately leading to the final answer. In the following example taken from the FRAMES benchmark, we can observe the reasoning advantage ODS-v1 has over state-of-the-art closed models such as Perplexitys Sonar Reasoning Pro. Both ODS and the Sonar Reasoning Pro model get confused between potential answers. However, ODScorrectly identifies 112 inches as the correct answer and runs an additional check using the Wolfram-Alpha tool to convert the answer to 2,845mm as required. Conversely, Sonar Reasoning Pro just responds with the incorrect answer of 2,858mm. question from FRAMES: How old would the 1975 winner of the Lenore Marshall Poetry Prize have been if they were still alive on the date when Rupi Kaur released her book titled, \"Milk and Honey\"? Correct Answer: 90 years Perplexity response: To determine the age of the 1975 winner of the Lenore Marshall Poetry Prize if they were still alive when Rupi Kaur released her book \"Milk and Honey\" in 2014, we need to consider the following: 1. **Year of the Prize**: The Lenore Marshall Poetry Prize was awarded in 1975. 2. **Release Year of \"Milk and Honey\"**: Rupi Kaur released \"Milk and Honey\" in 2014. 3. **Age Calculation**: To find the age, subtract the year of the prize from the year of the book release. If the winner was alive in 2014, we calculate their age as follows: [ text{Age in 2014} = 2014 - 1975 = 39 text{ years} ] However, this calculation only gives the difference in years between the two events. To find the actual age of the person, we need to know their birth year. Since the specific winners birth year is not provided, we can only state that if the 1975 winner was still alive in 2014, they would be 39 years older than they were in 1975. For example, if the winner was 40 years old in 1975, they would be (40 + 39 = 79) years old in 2014. Without the exact birth year or age at the time of the award, we cannot provide precise age [1][4] ODS-v1+Llama3.1-70B response: $Initial_Thought$ To answer this question, Ill need to follow these steps: 1. Identify the winner of the 1975 Lenore Marshall Poetry Prize. 2. Determine the birth year of the winner. 3. Calculate the age of the winner as of the publication date of Rupi Kaurs book \"Milk and Honey\" (November 4, 2014). Lets start by identifying the winner of the 1975 Lenore Marshall Poetry Prize. According to the provided context , the winner was Cid Corman., {Thought: Now that have the birth year of Cid Corman, can calculate his age as of the publication date of Rupi Kaurs book \"Milk and Honey\"(November 4, 2014)}, Do need further steps? No., {Action: calculate}, {Action_Input: 2014-1924 Thought: final_result: Cid Corman was born in 1924, so as of the publication date of Rupi Kaurs book \"Milk and Honey\" (November 4, 2014), he would have been 90 years old. Do need further steps? No}, {Action: Done} Figure 3: An example question and answer from FRAMES where ODS-v1 uses the Wolfram calculator tool to accurately calculate the age difference, resulting in the correct answer of 90. Conversely, Perplexity pursues the wrong reasoning path to report 79 years of age."
        },
        {
            "title": "2.2.2 ODS-v2 with CodeAct Agent",
            "content": "Chain of Code (CoC). While Chain-of-Thought (CoT) has proven effective in semantic reasoning, it often encounters challenges with tasks requiring precise numeric or symbolic computations. To address these limitations, Chain-of-Code [11] (CoC) leverages the code-writing capabilities of LLMs to generate and execute code, or pseudocode, to tackle both algorithmic and semantic problems. This approach not only broadens the scope of reasoning questions LMs can address but also enhances their accuracy in solving complex tasks. CoC outperforms traditional CoT methods across various benchmarks, highlighting the potential of integrating code generation and execution within LLMs to achieve more robust reasoning capabilities. CodeAct. Recent advancements [14] have shown that generating executable Python code for tool calling yields significant boost in performance compared to conventional JSON-based approaches. In particular, LLMs are inherently adept at compressing the action-space of tasks using code. Additionally, code, as mode of representation, is more naturally suited for action taking as it can be composed, modularized, and generalized much easier than JSON based approaches. In ODS-v2, we adapt our search tool to work with SmolAgentss [25] framework as it allows for customization and can be easily distributed. Our most basic reasoning agent powered by search uses [25] CodeAgent (A variation of CodeAct) with access to our search tool as shown in 4. More advanced iterations of ODS-v2 involve multiple tools and agents working in-concert to solve more complicated tasks that may, or may not, involve search. 6 Figure 4: CodeAct agent in ODS-v2 answering multi-hop question."
        },
        {
            "title": "3 Experiments",
            "content": "Baselines. We compare against the popular closed-source search AIs from Perplexity [21] (their default search AI, which we call Perplexity, and an advanced reasoning search AI, which is called Perplexity Sonar Reasoning Pro [22]) and the state-of-the-art search AI from OpenAI: GPT-4o Search Preview [17]. These are the state-of-the-art AI solutions with access to search engines. As separate baselines, we also compare against large language models: GPT-4o, Llama-3.1-70B, and DeepSeekR1. Although these models do not have access to the Internet, we demonstrate in Table 1 that the LLMs with reasoning capabilities are surprisingly good at our evaluation benchmarks of FRAMES [8] and SimpleQA [30]."
        },
        {
            "title": "3.1 Numerical analyses on two evaluation benchmarks: FRAMES and SimpleQA",
            "content": "We use two evaluation benchmarks: FRAMES [8] and SimpleQA [30]. Originally, SimpleQA is intended to test frontier-models on factuality without web-browsing, and FRAMES is intended to test factuality and retrieval of models on single to multi-hop queries where the ground truth Wikipedia articles are given. In our scenario, we use both of them to evaluate the accuracy of search AIs with access to the Internet."
        },
        {
            "title": "3.1.1 Numerical analysis on FRAMES",
            "content": "FRAMES (Factuality, Retrieval, And reasoning MEasurement Set) dataset introduced in [8] comprises 824 challenging multi-hop questions requiring integrating multiple sources from Wikipedia. The best single-query search approach reported in [8] achieves score of 47.4% with Gemini-Pro1.5-0514 (5/14/2024 release) when using 4 retrieved documents from single query that have the 7 Figure 5: The histogram of how many searches are run by the CodeAct-based ODS-v2 reveals that it chooses to use less number of web searches on SimpleQA, which is relatively easier, compared to FRAMES, which is much more complex. highest BM25 score [24] from Wikipedia data dump3. ODS-v1+DeepSeek-R1 achieves 56.7% under the same condition of using single web search per query in Table 2. This is significant improvement over the state-of-the-art reasoning LLMs (such as DeepSeek-R1 and GPT-4o) and search AIs (such as Perplexity and Perplexity Sonar Reasoning Pro) as shown in Table 1. The CodeActbased agent of ODS-v2+DeepSeek-R1 chooses to use more searches, thus achieving 75.3% on FRAMES using 3.39 searches per query on average. With our reasoning agent, ODS-v1+DeepSeek-R1 has the option to run more searches but chooses not to. On the other hand, when ODS-v1 uses Llama3.1-70B as base model, it uses more web searches to improve accuracy, as shown in the table and also illustrated with an example in Figure 6. ODS+base-model ODS-v1+Llama3.1-70B ODS-v1+DeepSeek-R1 ODS-v2+DeepSeek-R Accuracy (%) SimpleQA FRAMES 83.4 87.7 88.3 49.5 56.7 75.3 # of web searches per query SimpleQA 1.09 1.00 1.45 FRAMES 1.05 1.00 3.39 Table 2: How many times ODS searches the web, on average per query, for the two benchmarks. ODS adapts to the quality of the first search result and the model output to judiciously use the extra search only when deemed necessary. It has been reported in [8] that the accuracy on FRAMES can be improved with multiple web searches. For example, performance similar to ODS-v1+DeepSeek-R1 can be achieved by GeminiPro-1.5-0514 by running the search engine 15 times with rephrased prompts. However, such multistep multi-search approach is inefficient since it calls fixed number, for example 15, of searches regardless of the hardness of the example or the correctness of the model output. This is one of the motivations for our agentic framework, where our reasoning agent adaptively selects whether to search again or not based on the initial search quality and the model output. This is shown Table 2, where ODS-v1 uses more web searches when paired with weaker base model of Llama3.1. The example in Figure 6 shows how ODS-v1 uses the search judiciously when the initial search is insufficient. Similarly for ODS-v2, it chooses to use fewer web searches on SimpleQA, which is relatively easier task requiring single correct source of information, compared to FRAMES, which is much more complex task requiring multiple sources of information."
        },
        {
            "title": "3.1.2 Numerical analyses on SimpleQA",
            "content": "SimpleQA, introduced in [30], is benchmark evaluation dataset, adversarially collected against GPT-4, and the questions are designed so that there is only single, indisputable answer. It consists of 4326 short-form factuality questions, e.g., On which U.S. TV station did the Canadian reality series *To Serve and Protect* debut? or Who received the IEEE Frank Rosenblatt Award in 2010?. An LLM-as-a-judge with GPT-4o-mini is used to check for correctness of the response w.r.t. the ground truths, e.g., KVOS-TV or Michio Sugeno. It consists of questions from diverse 3https://www.tensorflow.org/datasets/catalog/wikipedia 8 question from FRAMES: What year was the band leader of the group who originally performed the song sampled in Kanye Wests song Power born? Correct Answer: 1946 Perplexity response: The song \"Power\" by Kanye West samples several tracks, including \"21st Century Schizoid Man\" by King Crimson. The lead vocalist and bassist of King Crimson during the time of that song was Greg Lake. Greg Lake, the band leader and original lead vocalist of King Crimson for the song \"21st Century Schizoid Man,\" was born on November 10, 1947[2][4]. ODS-v1+Llama3.1-70B response: $Initial_Thought$: To answer this question, Ill need to follow these steps: 1. Identify the song sampled in Kanye Wests song \"Power\". 2. Identify the band that originally performed the sampled song. 3. Identify the band leader of the original band. 4. Find the birth year of the band leader. Based on the context, can start by identifying the song sampled in Kanye Wests song \"Power\". According to the context, the song sampled is \"21st Century Schizoid Man\" by King Crimson. Next, can identify the band that originally performed the sampled song, which is King Crimson. Now, need to identify the band leader of King Crimson. According to the context, the founder of King Crimson is Emilio Castillo, but this is not correct. After re-reading the context, found that the founder of King Crimson is not explicitly mentioned, but it is mentioned that the band was born in October 1968. However, can infer that the band leader of King Crimson is likely Robert Fripp, who is well-known guitarist and composer of the band. But couldnt find any information about Robert Fripps birth year in the given context. Let me continue searching for the birth year of the band leader of King Crimson... Action: $search_internet$, $Action_Input$: Robert Fripp birth year Thought: Based on the search results, found that Robert Fripp, the band leader of King Crimson, was born on May 16, 1946., $Action$: Done Figure 6: An example question and answer from FRAMES where ODS-v1+Llama3.1-70B realizes that second search is necessary, searches again to find the birth year of the lead singer of King Crimson, and correctly answers the question with 1946. Perplexity [21] on the other hand could not figure out the leader of the band King Crimson. topics including Science & Technology (n=858), Politics (n=709), and Art (n=550), Geography (n=424), Sports (n=368), Music (n=341), TV shows (n=293), History (n=173), and Video games (n=135). It is reported in [30] that GPT-4o scores 38.2% accuracy, Claude-3.5-sonnet (2024-0620 release) gets 28.9%, and OpenAI o1-preview gets 42.7%. ODS-v1 and ODS-v2 improve upon these closed-source Perplexity solutions, achieving 87.7% and 88.3%, respectively  (Table 1)  , by leveraging the high quality search results provided by our Open Search Tool (Section 2.1) together with the Open Reasoning Agent framework (Section 2.2). It is important to note that the gain comes from both high quality search tool (as shown in an example in Figure 7) and the reasoning agent (as shown in an example in Figure 8). In Table 3, we survey SimpleQA accuracy results reported in other sources together with ODSv1+Llama3.1-70B, ODS-v1+DeepSeek-R1, and ODS-v2+DeepSeek-R1. With the help of the reasoning power of DeepSeek-R1, ODS-v1+DeepSeek-R1 improves over ODS-v1+Llama3.1-70B. There are more recent closed-source solutions such as GPT-4o Search Preview [17], Exa [1], Linkup [15], and Perplexity Deep Research [23] that claim to achieve higher SimpleQA scores. ODS provides an open-source solution that can significantly close the gap between the open search AI solutions and the closed ones."
        },
        {
            "title": "3.2 Ablation study of ODS",
            "content": "In Table 4, we run an ablation study for various parts of the ReAct-based Open Reasoning Agent for ODS-v1. Llama3.1-70B alone achieves 21.2% on subset of 500 randomly selected examples from SimpleQA (we use subsampled evaluation dataset for efficiency, just in this section) and 34.3% on FRAMES. With our Open Search Tool, which we abbreviated as Search in the table, SimpleQA increases dramatically to 82.4% but FRAMES suffers performance drop to 27.6%. Our Open Reasoning Agent has two parts: Chain-of-Thought ReAct with self-consistency decoding (CoT-ReAct) and Few-shot prompting (FewShot). Adding these components one-by-one, we get gradual improvements in both subsampled SimpleQA and FRAMES. This shows that each component of ODS-v1 contributed to the performance achieved. Finally, by replacing the base-model from Llama3.1-70B with more powerful reasoning model of DeepSeek-R1, we show that ODS-v1 leverages the reasoning capabilities of the LLM to achieve even better performance. 9 question from SimpleQA: What is the name of track 10 on disc 2 of the Sen no Kiseki IV - The End of Saga - original soundtrack? Correct Answer: Break In Perplexity Sonar Reasoning Pro response: Okay, lets tackle this query. The user is asking for the name of track 10 on disc 2 of the Sen no Kiseki IV - The End of Saga original soundtrack. First, need to check the search results provided. Let me go through each source to find the track listing for disc 2. Looking at result [1], which is from Apple Music, it lists the tracks but only shows the first few tracks of disc 1. The full track list is not visible here, so this might not help directly. Result [2] from Spotify mentions some track names like \"Beyond Ten Millions of Nights\" and \"Path to the...\" but does not list track numbers or disc divisions clearly. Not enough info here. Result [3] .....But in the provided data from result [7], do not see breakdown by disc. Wait, maybe result [7] includes all tracks as single list? Let me check again. Result [7] lists \"Number of Files: 69\" and \"Total Filesize: 365 MB (MP3), 3,866 MB (FLAC).\" But the actual track list is not shown here. The user needs to know which specific track is number 10 on disc 2. Without seeing the full track list split by discs, this might not help directly. Result [8] on SoundCloud has track titled which translates to \"Each Persons Resolution,\" but it is part of disc 3 according to the URL. So that is not relevant here. If we consider that each disc has around 23 tracks, then Disc 2 Track 10 would be overall Track 33 or so, but without knowing exact names, this approach will not work. Alternatively, maybe someone can infer based on other games soundtracks structure ODS-v1+DeepSeek-R1 response: Initial_Thought: <think> Okay, lets try to figure out the name of track 10 on disc 2 of the Sen no Kiseki IV original soundtrack. First, need to go through the provided context and look for any mentions of disc 2 track listings. Looking at the context snippets, there are several mentions of disc 2. One of them says: \"Disc 2 ; 2.10, Break In Break In, : HS ; 2.11, Trump Card for the Worst Situation , : HS ; 2.12, Bold Assailants...\" ....So putting this together, track 10 on disc 2 is likely \"Break In\". </think> The name of track 10 on disc 2 of Sen no Kiseki IV -THE END OF SAGAOriginal Soundtrack is \"Break In\". This is confirmed by context snippets explicitly listing track 2.10 as \"Break In Break In\". Action: Done final_result: The name of track 10 on disc 2 of the Sen no Kiseki IV - The End of Saga - original soundtrack is \" Break In\" Figure 7: An example question and answer from SimpleQA where the ODS-v1 leverages the high quality retrieved context from Open Search Tool to identify the correct answer, by cross-checking multiple sources. Perplexity Sonar Reasoning Pro [22] failed to retrieve the relevant information in the search. question from SimpleQA: The Jensen Interceptor (1950), produced from 1950 to 1957, had wheelbase measurement of what in millimeters? Correct Answer: 2,845 mm Perplexity Sonar Reasoning Pro response: Lets tackle this query about the Jensen Interceptor (1950) wheelbase in millimeters. The user is asking specifically for the wheelbase measurement of the model produced between 1950 and 1957. First, Ill look through the provided search results to find mentions of the wheelbase. Going through each source: - Result [1] from automobile-catalog.com states: \"Wheelbase:2858 mm / 112.5 in\". That seems directly relevant. - Result [3] from motor-car.net lists \"Wheelbase: 112 in (2,845 mm)\". The Jensen Interceptor produced from 1950 to 1957 had wheelbase of 2,858 mm (112.5 inches)[1][8]. This measurement is consistent across technical specifications from multiple sources detailing the first-generation Interceptors dimensions[1][3][8]. ODS-v1+DeepSeek-R1 response: Initial_Thought: <think> Okay, lets tackle this question about the Jensen Interceptor (1950) wheelbase in millimeters. The user is asking specifically about the model produced from 1950 to 1957. First, need to go through the provided context and find any mentions of the wheelbase measurement for that specific model and years. Looking at the context snippets, there are several mentions of wheelbase measurements. Let me list out the relevant ones: 1. \"Jensen Interceptor (1950) ; 4-speed manual, 112 in (2,845 mm), 183 in (4,648 mm).\" Here, 112 inches is converted to 2,845 mm. This seems to directly mention the 1950 models wheelbase. 2. Another snippet says: \"Wheelbase: 112 in (2,845 mm). Length, 183 in (4,648 mm). Width, 66 in (1,676 mm).\" ...}, {Thought: \"The context mentions both 112 inches (2844.8 mm) and 112.5 inches (2857.5 mm). The 1950 models original specs from the article state 112 inches (2845 mm), while later references to the cabriolet and other variants might use 112.5 inches. Since the user specified the 1950-1957 model, the primary value from the 1950 article is 112 inches. Let me convert this value.\"}, {Action: calculate}, {Action_Input: 112 inches to millimeters}, {Observation: {2845mm\"}}, {Thought: }, {Action: Done}, final_result: The wheelbase measurement of the Jensen Interceptor (1950) produced from 1950 to 1957 is 2,845 millimeters. Figure 8: An example question and answer from SimpleQA where the Open Reasoning Agent correctly identifies 112 inches as the correct answer and makes an additional check with the WolframAlpha API to convert the answer to 2,845mm. Conversely, Perplexity Sonar Reasoning Pro [22] gets confused between 112.5 inches and 112 inches as potential answers. We abbreviated the responses here and refer to Appendix for the complete responses."
        },
        {
            "title": "4 Conclusion",
            "content": "To bridge the increasingly widening gap between state-of-the-art proprietary search AI solutions such as Perplexity Sonar Reasoning Pro [22] and GPT-4o Search Preview [17] and their open-source"
        },
        {
            "title": "Search AI",
            "content": "SimpleQA (%) Closed-source o1-mini [18] GPT-4o-mini-2024-07-18 [18] o3-mini-low [18] o3-mini [18] o3-mini-high [18] Grok 3 Mini Beta [32] Claude 3 Opus [18] GPT-4-turbo-2024-04-09 [18] Claude 3.5 Sonnet [18] GPT-4o [32] GPT-4o-2024-11-20 [18] GPT-4o-2024-05-13 [18] GPT-4o-2024-08-06 [18] o1 [18] o1-preview [18] Grok 3 Beta [32] Gemini 2.0 Pro [32] Perplexity Sonar [22] Perplexity [21] Perplexity Sonar Reasoning Pro [22] GPT-4o Search Preview [17] Exa [1] Linkup [15] Perplexity Deep Research [23] Open-source Qwen 2.5 [4] Llama3.1-70B DeepSeek-V3 [32] DeepSeek-R1 [5] ODS-v1+Llama3.1-70B ODS-v1+DeepSeek-R1 ODS-v2+DeepSeek-R1 7.6 9.5 13.0 13.4 13.8 21.7 23.5 24.2 28.9 38.2 38.8 39.0 40.1 42.6 42.7 43.6 44.3 77.3 82.4 85.8 90.0 90.0 90.1 93.9 9.1 20.4 24.9 82.4 84.0 87.7 88.3 Table 3: ODS-v2 achieves the best accuracy on SimpleQA among open-source solutions. indicates evaluations run by us. Otherwise, the SimpleQA accuracy is taken from the cited source in each row."
        },
        {
            "title": "Search AI Solution",
            "content": "Llama3.1-70B OpenSearchTool + Llama3.1-70B OpenSearchTool + Llama3.1-70B + CoT-ReAct OpenSearchTool + Llama3.1-70B + CoT-ReAct + FewShot OpenSearchTool + DeepSeek-R1 + CoT-ReAct + FewShot 500 subset of SimpleQA (%) 21.2 82.4 87.2 87.8 90.4 FRAMES (%) 34.3 27.6 37.4 49.5 56.7 Table 4: Ablation study of Open Search Tool, Llama3.1-70B/DeepSeek-R1, and the two components of the ReAct-based Open Reasoning Agent in ODS-v1: Chain-of-Thought ReAct with selfconsistency decoding (CoT-ReAct) and Few-shot prompting (FewShot). The SimpleQA accuracy is only reported on the 500 randomly sampled examples, for efficiency. Each component is critical in achieving the state-of-the-art performance. alternatives [27, 19, 20], we introduce Open Deep Search (ODS). This open-source search AI can be seamlessly combined with any LLM of the users choice in plug-and-play style. This allows ODS to harness the latest advances in reasoning LLMs and achieve increasingly accurate performance. When using DeepSeek-R1, ODS can achieve 75.3% accuracy on FRAMES benchmark, surpassing GPT-4o Search Preview released on 3/11/2025 by 10%  (Table 1)  . On another benchmark of SimpleQA, ODS significantly narrows the gap between open-source and closed-source solutions. This significant milestone is achieved by the synergy between the two components of ODS: Open Search Tool and Open Reasoning Agent. We introduce Open Search Tool to provide high quality retrievals from the Internet, to be used as tool by the reasoning agentic framework within ODS. Open Reasoning Agent interprets the given task and completes it by calling available tools, ranging from the search tool (our proposed Open Search Tool), calculator (provided by Wolfram Alpha API), reflection (using the provided base LLM), and code interpreter (we use Python interpreter). Together with the latest powerful reasoning LLMs, these two open-source components that make up ODS ensure that we achieve the state-of-the-art performance on search. The gain of Open Search Tool over the proprietary counterparts is demonstrated in, for example, Figure 7 and other examples in Appendix A. The gain of Open Reasoning Agent is demonstrated in, for example, Figures 6 and 8, and other examples in Appendix A. We provide two versions of Open Reasoning Agent: one based on ReAct and the other based on CodeAct. We publicly release all open-source implementations and invite the open-source community to build upon our work and further innovate, starting from our state-of-the-art search AI solution."
        },
        {
            "title": "References",
            "content": "[1] Will Bryk. https://exa.ai/blog/api-evals, 2025. [2] Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. Mindsearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183, 2024. [3] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402, 2023. [4] DeepSeek. https://www.deepseek.com, 2025. [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [6] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems, 35:2219922213, 2022. [7] Miloˇs Koˇsprdic, Adela Ljajic, Bojana Baˇsaragin, Darija Medvecki, and Nikola Miloˇsevic. Verif. ai: Towards an open-source scientific generative question-answering system with referenced and verifiable answers. arXiv preprint arXiv:2402.18589, 2024. [8] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. 2024. [9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Douwe Kiela, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [10] Alice Li and Luanne Sinnamon. Generative ai search engines as arbiters of public knowledge: An audit of bias and authority. Proceedings of the Association for Information Science and Technology, 61(1):205217, 2024. [11] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language model-augmented code emulator. In International Conference on Machine Learning, pages 2825928277. PMLR, 2024. [12] Yongqi Li, Xinyu Lin, Wenjie Wang, Fuli Feng, Liang Pang, Wenjie Li, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. survey of generative search and recommendation in the era of large language models. arXiv preprint arXiv:2404.16924, 2024. [13] Zeren Luo, Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Jingyi Zheng, and Xinlei He. The rising threat to emerging ai-powered search engines. arXiv preprint arXiv:2502.04951, 2025. [14] Weijie Lv, Xuan Xia, and Sheng-Jun Huang. Codeact: Code adaptive compute-efficient tuning framework for code llms. arXiv e-prints, pages arXiv2408, 2024. [15] Philippe Mizrahi. simpleqa, 2025. https://www.linkup.so/blog/linkup-establishes-sota-performance-on- [16] Petru Neague, Quinten Stokkink, Naman Goel, and Johan Pouwelse. Semantica: Decentralized search using llm-guided semantic tree overlay. arXiv preprint arXiv:2502.10151, 2025. [17] OpenAI. https://platform.openai.com/docs/models/gpt-4o-search-preview, 2025. [18] OpenAI. https://github.com/openai/simple-evals, 2025. [19] OpenPerplex. https://openperplex.com, 2024. [20] Perplexica. https://github.com/ItzCrazyKns/Perplexica, 2024. [21] Perplexity AI, Inc. Perplexity AI, 2024. https://www.perplexity.ai. [22] PerplexityAI. https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api, 2025. [23] PerplexityAI. https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research, 2025. [24] Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995. 13 [25] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismaki. smolagents: smol library to build great agentic systems. https://github. com/huggingface/smolagents, 2025. [26] Pranav Narayanan Venkit, Philippe Laban, Yilun Zhou, Yixin Mao, and Chien-Sheng Wu. Search engines in an ai era: The false promise of factual and verifiable source-cited responses. arXiv preprint arXiv:2410.22349, 2024. [27] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. Freshllms: Refreshing large language models with search engine augmentation, 2023. [28] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [29] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [30] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. [31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [32] xAI. https://x.ai/blog/grok-3, 2025. [33] Shinnosuke Yao, Joshua Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yunzhu Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [34] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [35] Shoshana Zuboff. The age of surveillance capitalism: The fight for human future at the new frontier of power, edn. PublicAffairs, New York, 2019."
        },
        {
            "title": "A Example responses and comparisons from SimpleQA and FRAMES",
            "content": "A.1 FRAMES In this FRAMES example, ODS+DeepSeek-R1 correctly identifies 112 inches as the correct answer and makes an additional check with the Wolfram-Alpha API to convert the answer to 2,845mm. On the other hand, Perplexity Sonar Reasoning Pro is confused between 2,858mm and 2,845mm. question from FRAMES: The Jensen Interceptor (1950), produced from 1950 to 1957, had wheelbase measurement of what in millimeters? Correct Answer: 2,845 mm Perplexity Sonar Reasoning Pro response: <think> Okay, lets tackle this query about the Jensen Interceptor (1950) wheelbase in millimeters. The user is asking specifically for the wheelbase measurement of the model produced between 1950 and 1957. First, Ill look through the provided search results to find mentions of the wheelbase. Lets go through each source: - ** Result [1]** from automobile-catalog.com states: \"Wheelbase:2858 mm / 112.5 in\". That seems directly relevant. - **Result [3]** from motor-car.net lists \"Wheelbase: 112 in (2,845 mm)\". Hmm, slight discrepancy here-2858 vs 2845 mm. Need to check which is correct. - **Result [7]** (Wikipedia) mentions different Interceptor model (1966-1976) with wheelbase of 107 inches (2,718 mm), but thats for later model, so not relevant here. - **Result [8]** also from automobile-catalog.com for the 4-Litre Saloon in 1950 says \"Wheelbase: 2858 mm / 112.5 in\", matching Result [1]. - **Result [9]** and others dont provide wheelbase info for the 1950-1957 model. The conflicting numbers are between Results [1]/[8] (2858 mm) and Result [3] (2845 mm). Since Results [1] and [8] are both from Automobile Catalog and specify the same measurement, and Result [3] is from Motor Car History with slightly different number, need to assess reliability. Automobile Catalogs detailed specs in Result [1] seem more precise, including both metric and imperial conversions. The discrepancy might be due to rounding or different measurement standards, but the majority of sources point to 2858 mm. Therefore, the most accurate answer based on provided sources is 2858 mm. </think> The Jensen Interceptor produced from 1950 to 1957 had **wheelbase of 2,858 mm** (112.5 inches)[1][8]. This measurement is consistent across technical specifications from multiple sources detailing the first-generation Interceptors dimensions[1][3][8]. For context: - The cars overall length was 4,775 mm (188 inches)[1], making it sizable grand tourer for its era. - The wheelbase contributed to its stable ride characteristics and spacious interior[2]. minor discrepancy exists in one source listing the wheelbase as 2,845 mm[3], but the majority of technical documentation confirms 2,858 mm as the standard measurement[1][8]. ODS-v1+DeepSeek-R1 response: Initial_Thought: <think>nOkay, lets tackle this question about the Jensen Interceptor (1950) wheelbase in millimeters. The user is asking specifically about the model produced from 1950 to 1957. First, need to go through the provided context and find any mentions of the wheelbase measurement for that specific model and years.nnLooking at the context snippets, there are several mentions of wheelbase measurements. Let me list out the relevant ones:nn1. Jensen Interceptor (1950) ; 4-speed manual * 112 in (2,845 mm) * 183 in (4,648 mm).\" Here, 112 inches is converted to 2,845 mm. This seems to directly mention the 1950 models wheelbase.n n2. Another snippet says: \"Wheelbase: 112 in (2,845 mm). Length, 183 in (4,648 mm). Width, 66 in (1,676 mm).\" Again, this matches the 1950 models specifications.nn3. Theres mention of \"characteristic dimensions: outside length: 4775 mm / 188 in, width: 1676 mm / 66 in, wheelbase: 2858 mm / 112.5 in; reference ...\". Wait, here the wheelbase is 2858 mm, which is 112.5 inches. But the previous entries say 112 inches (2,845 mm). This discrepancy needs to be resolved.nnAnother context line: \"Wheelbase: 2858 mm / 112.5 in. Claimed EPA passenger volume: Calculated EPA ... 1950 Jensen Interceptor 2-Door Cabriolet Engine Horsepower/ Torque Curve ...\". So here, the 1950 models wheelbase is 2858 mm (112.5 inches). But earlier, it was 2,845 mm (112 inches) . Which one is correct?nnLooking further, another snippet says: \"Technical specifications for the 1950-1957 Jensen Interceptor 2 door cabriolet ... Wheelbase: 112.2 in. Front Track: 66 in. Rear Track: 66 in ... Nov 21, 2023\". Here, the wheelbase is 112.2 inches. Converting that to mm: 112.2 inches * 25.4 mm/inch = 2849.88 mm, approximately 2850 mm. But another entry says 2858 mm (112.5 inches). Theres inconsistency here.nnWait, another snippet: Engine, 4.0 Austin D-Series I6 * Transmission, 4-speed manual. Dimensions. Wheelbase, 112 in (2,845 mm). Length, 183 in (4,648 mm). Width, 66 in (1,676 mm).\" This clearly states 112 inches (2,845 mm). But another part says 112.5 inches (2858 mm). Which is correct?nnI need to check if the 1950-1957 model had different wheelbases in different years. The user is asking about the model produced from 1950 to 1957. If theres variation within those years, but the context mentions both 112 and 112.5 inches. However, the majority of the entries for the 1950 model specify 112 inches (2,845 mm). The 112.5 inches (2858 mm) might be different model year or different variant, perhaps later version. But the user is asking about the entire production run from 1950-1957. nnWait, another snippet: \"characteristic dimensions: outside length: 4775 mm / 188 in, width: 1676 mm / 66 in, wheelbase: 2858 mm / 112.5 in; reference ...\". This is from section that also mentions the 4) Horsepower/Torque Curve. But the model years here arent specified. However, the same context also mentions \"1950 Jensen Interceptor 2-Door Cabriolet Engine Horsepower/ Torque Curve ...\" which might link the 2858 mm to the 1950 model. nnThis is conflicting. Some sources say 2,845 mm (112 in), others 2,858 mm (112.5 in). How to resolve this? Maybe check which measurement is more commonly cited in the provided context. nnLooking through the context, the first mention is 112 in (2,845 mm). Then another mentions 112.2 in (approx 2850 mm). Another mentions 112.5 in (2858 mm). The technical specifications for the 1950-1957 cabriolet mention Wheelbase: 112.2 in. But another part says 112.5 in. nnAlternatively, perhaps theres confusion between different models. The Jensen Interceptor name was used again in the 1960s, but the user is asking about}, {Thought: \"The context mentions both 112 inches (2844.8 mm) and 112.5 inches (2857.5 mm). The 1950 models original specs from the article state 112 inches (2845 mm), while later references to the cabriolet and other variants might use 112.5 inches. Since the user specified the 1950-1957 model, the primary value from the 1950 article is 112 inches. Let me convert this value.\"}, {Action: calculate}, { Action_Input: 112 inches to millimeters}, {Observation: {2845mm}}, {Thought: }, {Action: Done}, final_result: The wheelbase measurement of the Jensen Interceptor (1950) produced from 1950 to 1957 is 2,845 millimeters. 15 In the below Frames example, ODS+Llama3.1-70B correctly identifies Dune Part Two as the film in question, while Perplexity Sonar Reasoning Pro [22] and ODS+DeepSeek-R1 is unable to determine the specific answer. question from FRAMES: Which film featuring solar eclipse in its opening scene is adapted from the same source material as David Lynch movie? Correct Answer: Dune Part Two Perplexity Sonar Reasoning Pro: <think> Okay, lets tackle this query. The user is asking which film featuring solar eclipse in its opening scene is adapted from the same source material as David Lynch movie. First, need to identify David Lynch movies that are adaptations. From the search results, see that David Lynch directed the 1984 adaptation of *Dune* [1][3][4][7][9]. The source material for that is Frank Herberts novel *Dune*. Now, looking for another film adapted from the same novel (*Dune*) that has solar eclipse in its opening scene. The search results mention Denis Villeneuves 2021 *Dune: Part One* and the upcoming *Dune: Part Two*. Let me check the trailers and scenes described. In search result [1], the trailer breakdown for *Dune: Part Two* mentions scene with blood orange hues, possibly due to an environmental effect like an eclipse. However, its not explicitly stated as the opening scene. Search result [3] talks about Villeneuves 2021 *Dune* but doesnt mention an eclipse in the opening. Wait, search result [7] is about how David Lynchs *Dune* redefined the saga. Maybe theres connection here. But the user is asking for another film with solar eclipse opening, same source as Lynchs. Looking at search result [9], the new *Dune* trailer uses Pink Floyds \"Eclipse,\" which might be nod to an eclipse scene. The 2021 *Dune* film by Villeneuve is part one, and part two might have such scene. However, the user specified \"opening scene.\" Wait, search result [1] describes the trailer opening with Paul and Chani on Arrakis....Given the information, the most accurate answer would be that the film in question is likely one of the adaptations of \"Dune,\" but the specific detail about the solar eclipse in the opening scene is not clearly confirmed in the provided sources. ODS-v1+Llama3.1-70B response: $Initial_Thought$ To answer this question, Ill follow these steps: 1. Identify the source material that David Lynch adapted into film. 2. Find film featuring solar eclipse in its opening scene that is adapted from the same source material. From the given context, can see that David Lynch adapted the novel \"Dune\" into film in 1984. Next, Ill look for film featuring solar eclipse in its opening scene that is also adapted from the novel \" Dune\", {Thought: }, {Action: search_internet}, {Action_Input: film featuring solar eclipse in opening scene adapted from Dune}, {Observation: Dune: Part Two opening scene features an ambush between Harkonnen soldiers and Fremen against the backdrop of visually striking solar eclipse ... Apr 9, 2024, \"Paul eclipses the Golden Path by being the wrong son born to the wrong mother at the wrong time, and thus, all is in tune, but the sun is eclipsed by the ... Sep 10, 2020\"}, {Thought: Based on the search results, found that the film featuring solar eclipse in its opening scene that is adapted from the same source material as David Lynch movie is \"Dune: Part Two\" (2024). The source material is the novel \"Dune\" by Frank Herbert, which was also adapted into film by David Lynch in 1984. Do need further steps? No.}, {Action: Done} ODS-v1+DeepSeeek-R1 response: final_result: The film \"Dune\" (1984), directed by David Lynch and adapted from Frank Herberts novel, shares its source material with Denis Villeneuves \"Dune\" adaptations. However, the film featuring solar eclipse in its opening scene from the same context is \"2001: Space Odyssey\" (1968), though it is not related to Lynchs work. Based on the provided context, the answer reconciling both criteria is **\"Dune\" (1984)**, as it is Lynch adaptation of Herberts novel, though no solar eclipse is mentioned in its opening., react_sequence: [{Initial_Thought: <think>nOkay, lets tackle this question step by step. The user is asking which film that features solar eclipse in its opening scene is adapted from the same source material as David Lynch movie. nnFirst, need to recall the context provided. The context mentions several David Lynch films, including his 1984 adaptation of \"Dune.\" It also talks about other films with solar eclipses, like \"2001: Space Odyssey\" which opens with an eclipse. The key here is to find connection between solar eclipse in films opening and David Lynch adaptation.nnDavid Lynch directed \"Dune\" (1984), which is based on Frank Herberts novel. The context also states that Denis Villeneuves \"Dune\" is another adaptation, split into two parts. However, the user is looking for film with solar eclipse in the opening that shares the same source material as Lynchs movie. nnLooking at the solar eclipse films mentioned, \"2001: Space Odyssey\" (1968) opens with an eclipse. But thats based on Arthur C. Clarkes work, not related to Lynchs \"Dune.\" Another mention is \"Barabbas\" (1961) which used real eclipse during filming, but thats biblical story. nnWait, the context also mentions \"Dune\" adaptations. Lynchs \"Dune\" is based on Herberts book. The other adaptation by Villeneuve is also from the same source. But does Villeneuves \"Dune\" have solar eclipse in the opening? The context doesnt specify that. However, \"2001: Space Odyssey\" is noted for its eclipse opening but isn related to \"Dune.\" nnWait, maybe the user is conflating two different elements: film with solar eclipse opening and Lynch adaptation. Since Lynchs \"Dune\" is the key here, and the other adaptation is Villeneuve s, but neither of them have solar eclipse in their openings. The context does mention \"2001: Space Odyssey\" as having an eclipse opening, but its not related to Lynchs source material. nnWait, perhaps the answer is that there isnt direct connection, but the user might be thinking of \"Dune\" adaptations. However , the context mentions that Villeneuves \"Dune\" is split into two parts, avoiding Lynchs issue. But the solar eclipse isnt in \"Dune.\" nnWait, another angle: the context mentions \"Bloody Birthday\" (1981) where lunar eclipse is part of the plot, but thats horror film. The 1907 film \"The Eclipse\" is about an eclipse but unrelated. nnWait, the users question is tricky. They want film that both has solar eclipse in the opening and is adapted from the same source as Lynch movie. Lynchs \"Dune\" is from Frank Herberts book. Are there other adaptations of \"Dune\" that have solar eclipse opening? The 2000 miniseries by John Harrison? The context doesnt mention an eclipse there. nnAlternatively, maybe the answer is \"2001: Space Odyssey\" but thats not based on the same source as Lynchs movie. Wait, no. The only common source material for Lynch movie is \"Dune.\" So unless another \"Dune\" adaptation has solar eclipse opening, which isnt mentioned in the context, perhaps the answer is that there isnt one. But the context does mention \"2001: Space Odyssey\" as having an eclipse opening, and its directed by Kubrick. But thats unrelated to Lynchs work....}, {Action: Done}, {Action_Input: } 16 In this example from FRAMES, ODS+Llama3.1-70B searches again once it finds that the company which launched the commercial was Levi & Strauss. This leads the agent to correctly identify Dockers as the first branch launched by Levi & Strauss after they changed the name of Meat Loaf in 1984. Meanwhile, Perplexity [21] is unable to determine the answer from single search. question from FRAMES: After Meat Loaf legally changed his name due to commercial, what was the first new brand launched by the company to whom the commercial belonged? Correct Answer: Dockers Perplexity response: The information provided does not link the name change of Meat Loaf (from Marvin Lee Aday to Michael Lee Aday) to specific commercial or company that launched new brand subsequent to this name change. Meat Loaf changed his first name from Marvin to Michael, but the reason given was due to jeans commercial, not because of any direct involvement with company that then launched new brand[5]. There is no additional information in the sources provided that connects this name change to the launch of new brand by any company. ODS-v1+Llama3.1-70B response: $Initial_Thought$: To answer this question, Ill follow these steps: 1. Identify the company that owned the commercial that led to Meat Loafs name change. 2. Research the companys product launches after the name change. From the context, found that the commercial was from Levi Strauss & Co. According to the text, Meat Loaf changed his name from Marvin to Michael in 1984 due to Levis commercial. Next, Ill search for information on new brands launched by Levi Strauss & Co. after 1984.\", Action: search_internet, Action_Input: Levi Strauss & Co. new brands launched after 1984, Observation: The Dockers brand launches sunglasses, bed linens, & bath items. The Dockers brand introduces innovations such as the Never Iron & Thermal Adapt. Thought: \"Based on the search results, found that Levi Strauss & Co. launched the Dockers brand in 1986, which was the first new brand launched by the company after Meat Loafs name change in 1984. Do need further steps? No .\", Action: Done In this example from FRAMES, ODS+Llama3.1-70B uses the Wolfram calculator tool to report the correct result of 2 years. On the other hand, Perplexity reports 2 years and 4 months in the wrong format since the question explicitly asked for the result in years. question from FRAMES: How many years elapsed between the release of the song \"I Think Im Go Go\" by the band Squeeze and the theatrical premier of E.T. the movie? Correct Answer: 2 years Perplexity response: The song \"I Think Im Go Go\" by Squeeze was released on February 4, 1980[1][4]. The theatrical premiere of the movie \"E.T. the Extra-Terrestrial\" was on June 10, 1982, in Los Angeles, California[2]. Therefore, the elapsed time between the release of the song and the movie premiere is approximately 2 years and 4 months. ODS-v1+Llama3.1-70B response: $Initial_Thought$To answer this question, Ill need to follow these steps:nn1. Identify the release year of the song \"I Think Im Go Go\" by Squeeze.n2. Identify the theatrical premiere year of the movie E.T.n3. Calculate the difference between the two years to find out how many years elapsed between the two events.n nLets start by looking for the release year of the song \"I Think Im Go Go\" by Squeeze..., {Thought: According to the search results, the song \"I Think Im Go Go\" by Squeeze was released in 1980. need to find the theatrical premiere year of the movie E.T. will calculate the difference between the two years to find out how many years elapsed between the two events. Do need further steps? No.}, Do need further steps? No ., {Action: calculate}, {Action_Input: 1982-1980}Thought: final_result: Cid Corman was born in 1924, so as of the publication date of Rupi Kaurs book \"Milk and Honey\" (November 4, 2014), he would have been 90 years old. Do need further steps? No, {Action: Done}{final_result: The song \"I Think Im Go Go\" by Squeeze was released in 1980, and the theatrical premiere of the movie E.T. was in 1982. Therefore, 2 years elapsed between the release of the song and the theatrical premiere of the movie. In this example from FRAMES, ODS+DeepSeek-R1 is encouraged to continue thinking and utilizes chain-of-thought to produce the correct answer by the Open Reasoning Agent. By reflecting on its own answer and feeding back its reflection to the language model, ODS is able to find the correct answer that 31 years passed since Starbucks introduced juice-blended Frappuccinos and when the company that is accredited for inventing the Frappuccinos is founded. Perplexity Sonar Reasoning Pro [22] fails to answer this question and concludes that there wasnt enough information provided in the context. In the next example question and answer from FRAMES, the ODS+DeepSeek-R1 is encouraged to continue thinking via the ReAct Thought/Action/Observation sequence and utilizes chain-ofthought to produce the correct answer. The name of the Japanese man who protested the U.S.s involvement in the Iraq War, and who has also been awarded multiple times the same award that Shrek won in 2002, beating Jimmy Neutron: Boy Genius, is Miyazaki Hayao. On the other 17 question from FRAMES: How many years before Starbucks introduced juice-blended Frappuccinos, was the company that is accredited for inventing the Frappuccino, founded? Correct Answer: 31 years Perplexity Sonar Reasoning Pro response: <think> Okay, lets tackle this query step by step. The user is asking how many years before Starbucks introduced juice-blended Frappuccinos, the company credited with inventing the Frappuccino was founded. First, need to identify the company that invented the Frappuccino. From the search results, its clear that George Howells The Coffee Connection created the original Frappuccino....So assuming CC existed since at least late 80s or early 90s but without exact date. Given this ambiguity and lack of explicit founding date cannot answer the question. ODS-v1+DeepSeek-R1 response: Initial_Thought: <think>nOkay, lets tackle this question step by step. The user is asking how many years before Starbucks introduced juice-blended Frappuccinos the company credited with inventing the Frappuccino was founded.nnFirst, need to identify the key pieces of information required. The two main dates here are: 1) the year Starbucks introduced juice-blended Frappuccinos, and 2) the founding year of the company that originally invented the Frappuccino.... {Thought: \"I need to find the year Starbucks introduced juice-blended Frappuccinos and the founding year of The Coffee Connection (the original inventor of Frappuccino). Now, need to determine when The Coffee Connection ( credited with inventing Frappuccino) was founded. Calculate the time difference between 1975 (The Coffee Connections founding) and 2006 (juice-blended Frappuccino introduction). The answer is derived using external knowledge for The Coffee Connections founding year, as the context lacks this detail.\"}, {Action: Done}, {Action_Input: }final_result: The company credited with inventing the Frappuccino, The Coffee Connection , was founded in 1975. Starbucks introduced juice-blended Frappuccinos in 2006, which is 31 years later. Therefore, the difference is 31 years hand, Perplexity Sonar Reasoning Pro fails to answer this question and concludes that there wasnt enough information provided in the context. 18 question from FRAMES: What is the name of the Japanese man who protested the U.S.s involvement in the Iraq War, and who has also been awarded multiple times the same award that \"Shrek\" won in 2002, beating \"Jimmy Neutron: Boy Genius\"? Correct Answer: Hayao Miyazaki Perplexity Sonar Reasoning Pro response: <think> Okay, lets tackle this query. The user is asking for the name of Japanese man who protested the U.S. involvement in the Iraq War and also won the same award that \"Shrek\" won in 2002, beating \"Jimmy Neutron: Boy Genius.\" First, need to figure out what award \"Shrek\" won in 2002. quick recall tells me that the first Shrek movie was released in 2001, and it won the first Academy Award for Best Animated Feature in 2002. The other nominee that year was \"Jimmy Neutron: Boy Genius.\" So the award in question is the Oscar for Best Animated Feature. Next, need to find Japanese man involved in protests against the Iraq War who has also won this Oscar. Looking through the provided search results, most of them discuss Japanese anti-war protests, particularly focusing on individuals like Obora Toshiyuki and others from the Tachikawa Tent Village group. However, none of these sources mention any connection to film awards. The user might be conflating two different people or theres mix-up in the details. The search results dont indicate any link between the anti-war protesters and the Academy Awards. Its possible that the user has confused protester with someone else in the film industry. Alternatively, there could be misunderstanding about the awards won by individuals involved in protests. Given that none of the search results mention an Oscar winner among the protesters, its likely that theres no such person as described. The protesters mentioned, like Obora Toshiyuki, were recognized for their activism but not for film achievements. Therefore, the answer should clarify that theres no known Japanese protester against the Iraq War who also won the mentioned Oscar. </ think> The query appears to contain contradictions or factual errors based on available evidence. Heres why: **Regarding Anti-Iraq War Protests in Japan:** - Japanese antiwar activists like **Obora Toshiyuki** and his colleagues from the **Tachikawa Tent Village** group were arrested in 2004 for distributing antiwar flyers opposing Japans Self-Defense Forces deployment to Iraq [1][5][8]. Amnesty International designated them as Japans first \"Prisoners of Conscience\"[3]. **Regarding Awards:** - *Shrek* (2001) won the **first Academy Award for Best Animated Feature** in 2002, defeating *Jimmy Neutron: Boy Genius*. No Japanese individual or film connected to Iraq War protests has won this award. **Key Discrepancies:** 1. No evidence links Obora Toshiyuki or other Japanese antiwar activists to animation awards. 2. The Academy Award category referenced ( Best Animated Feature) did not exist before 2001, and Japanese films/artists have rarely won it (*Spirited Away* won in 2003 but is unrelated to Iraq protests). This suggests either conflation of unrelated facts or fictional premise. For further clarification, additional context about the alleged award connection would be needed. ODS-v1+DeepSeek-R1 response: Initial_Thought: <think>nOkay, lets try to figure this out. The user is asking for the name of Japanese man who protested the U.S.s involvement in the Iraq War and also won the same award that \"Shrek\" won in 2002, which beat \"Jimmy Neutron: Boy Genius.\" nnFirst, need to recall that the award in question here is the Academy Award for Best Animated Feature. \"Shrek\" won the first Oscar in that category in 2002, and \"Jimmy Neutron: Boy Genius\" was one of the other nominees that year. So the award is the Oscar for Best Animated Feature.nnNext, the person were looking for is Japanese, involved in protests against the Iraq War, and has won this award multiple times. From the context provided, there are mentions of Japanese antiwar protesters, specifically three activists who were arrested for distributing antiwar flyers. However, those individuals are referred to as \"Prisoners of Conscience\" by Amnesty International, but their names arent given in the context snippets.nnLooking at the Oscar-related context, theres mention of Studio Ghibli and Hayao Miyazaki. For example, \"Spirited Away\" won the Oscar for Best Animated Feature in 2003, which is after the category was introduced in 2001. Miyazakis films have won multiple times, including \"Spirited Away\" and \"The Boy and the Heron\" in 2024. The context also states that Studio Ghibli has the most wins (two) and nominations (seven). nnNow, connecting the dots: the user is asking for someone who both protested the Iraq War and won the Best Animated Feature Oscar multiple times. Hayao Miyazaki is prominent Japanese filmmaker known for his anti-war stance. The context mentions that Miyazaki didnt attend the 2003 Oscars because of his opposition to the Iraq War. Additionally, his films have won the Best Animated Feature award twice (\" Spirited Away\" and \"The Boy and the Heron\"), which fits the \"multiple times\" part. nnHowever, the context also mentions that \"Shrek\" was the first winner in 2002, and Miyazakis first win was in 2003. The users question specifies that the person has won the same award that \"Shrek\" won, which Miyazaki did, albeit in subsequent years. The protest connection is there because Miyazaki opposed the Iraq War, as indicated in the context where he didnt attend the Oscars due to this opposition. nnTherefore, putting it all together, the answer should be Hayao Miyazaki. He is the Japanese filmmaker known for his anti-war views (including opposition to the Iraq War) and has won the Academy Award for Best Animated Feature multiple times, which is the same award \"Shrek\" won in 2002.n</think>nnThe Japanese individual in question is **Hayao Miyazaki**. n**Step-by-Step Explanation:** n1. **Award Context:** The Academy Award for Best Animated Feature was first awarded in 2002 (*Shrek* won, beating *Jimmy Neutron: Boy Genius*). n2. **Japanese Oscar Winners:** Studio Ghibli co-founder Hayao Miyazaki won this award twice: for *Spirited Away* (2003) and *The Boy and the Heron* (2024). n3. **Anti-War Stance:** Miyazaki publicly opposed the 2003 Iraq War. The context notes he boycotted the 2003 Oscars (where *Spirited Away* won) due to his anti-war principles. nnThus, Miyazaki matches both criteria: protesting the Iraq War and winning the same Oscar category as *Shrek*.}, {Thought: \"I need to identify the Japanese man who protested the U.S. involvement in the Iraq War and also won the Best Animated Feature Oscar multiple times. Verify Miyazakis Oscar wins. Confirm his anti-war stance. now know the final answer.\"}, {Action: Done}, {Action_Input: } final_result: Hayao Miyazaki, A.2 SimpleQA ODS+DeepSeek-R1 correctly follows reasoning path which distinguishes July 21, 2022 as the correct date that Kaitlin Armstrong pleaded not guilty to the murder charge of Moriah Wilson and was arraigned. The ODS agent cross checks the two conflicting dates of July 20 and July 21, 2022 and chooses July 21 correctly. Conversely, Perplexity Pro gets confused and gives the the wrong answer of July 20, 2022, lacking the additional check which allowed /osasp to answer the query. ODS+DeepSeek-R1 correctly follows reasoning path which identifies segment producer as the role Rosie Perez held in the TV show, In Living Color. ODS cross checks multiple sources to confirm the result. Conversely, Perplexity Sonar REasoning Pro talks about her directing segments and fails to organise its reasoning into the correct response. question from SimpleQA: What is the month, day, and year Kaitlin Armstrong pleaded not guilty to the murder charge of Moriah Wilson and was arraigned?} Correect answer: July 21, 2022 Perplexity Sonar Reasoning Pro response: <think> Okay, so need to find the exact date when Kaitlin Armstrong pleaded not guilty to the murder charge of Moriah Wilson and was arraigned. Let me go through the search results provided to pinpoint this information. Starting with the first search result [1], it mentions that on July 20, 2022, Armstrong had her first court appearance where she pleaded not guilty. The source is CBS Austin, which seems reliable. Another mention in [4] from CBS Austins timeline confirms that on July 20, 2022, Armstrong entered not guilty plea and requested speedy trial. Looking at [6], ESPNs article also states that after being apprehended in Costa Rica and returned to the U.S., Armstrong pleaded not guilty. The timeline in [4] lists July 20 under 2022 events, specifying her first court appearance where she pleaded not guilty. Additionally, [7] from CBS News mentions that Armstrong had pleaded not guilty to first-degree murder, but doesnt specify the date. However, combining all these sources, the consistent date across multiple references is July 20, 2022. should cross-check if any other dates are mentioned elsewhere. For example , [2] talks about the trial timeline starting in October 2023 but doesnt mention the arraignment date. [8] and [9] discuss later events like appeals and escape attempts but not the initial plea. Therefore, consolidating the information from [1], [4], and [6], the correct date is July 20, 2022. </think> Kaitlin Armstrong pleaded not guilty to the murder charge of Moriah Wilson and was arraigned on **July 20, 2022**[1][4][6]. This occurred during her first court appearance after being extradited to the U.S., where she also requested speedy trial[4]. The Travis County court processed her plea following her arrest in Costa Rica and subsequent return to Texas custody[6]. ODS-v1+DeepSeek-R1 response: {Initial_Thought: <think>nOkay, lets tackle this query step by step. The user is asking for the specific date ( month, day, and year) when Kaitlin Armstrong pleaded not guilty to the murder charge of Moriah Wilson and was arraigned.nnFirst, need to recall the context provided. The context includes multiple dates and events related to Kaitlin Armstrongs case. Let me go through each relevant piece of information.nnLooking at the context entries, several mention her plea. For example, one entry says: \"In July 2022, Armstrong pleaded not guilty to Wilsons murder. Her trial date was initially scheduled to begin on June 26, 2023. However, on ... Aug 27, 2024\". Another entry states: \"Armstrong was arraigned on July 21, 2022, pleading not guilty to the murder charge...., {Thought: need to resolve the conflicting dates. The context mentions both July 20 and July 21, 2022, for her plea. Let me check more entries. Another entry states: \"Kaitlin Armstrong was arraigned on July 21, 2022, pleading not guilty to the murder charge. She was held on $3.5 million bond...\" This explicitly links the arraignment to July 21.....Given the explicit mention of arraignment on July 21, 2022, in the context, thats likely the correct date. Therefore, the answer is July 21, 2022.}, {Action: Done}, {Action_Input: } final_result: Kaitlin Armstrong pleaded not guilty to the murder charge of Moriah Wilson and was arraigned on July 21, 2022. question from SimpleQA: Other than being choreographer for the TV series In Living Color, what other job did Rosie Perez do on the show? Correct answer: segment producer Perplexity Sonar Reasoning Pro response: <think> Okay, lets tackle this query about Rosie Perez and her roles on In Living Color.\" The user wants to know what other job she did besides being choreographer. First, need to go through the search results provided. Lets start by skimming through each source. From the search results, [1] mentions Rosie Perez as part of the Fly Girls collaboration. [2] states she was cast member and choreographer. ...However, other sources like [9] dont list her as part of the main cast but as crew member. The IMDb entry in [8] shows she directed some segments of the show. Therefore, besides being choreographer, Rosie Perez also directed certain segments of In Living Color,\" specifically the dance bumpers. Additionally, if being cast member refers to appearing in sketches, but thats less clear from the sources..... she **directed segments** of the show[8]. Specifically, Perez directed the Dance Bumpers\" segments during Seasons 2-4 (1991-1993)[8]. These short dance sequences transitioned between sketches and commercials, showcasing her creative vision beyond choreography....While not regular sketch performer herself[9], her behind-the-scenes influence extended into production decisions and cultural curation[7]. ODS-v1+DeepSeek-R1 response: {Initial_Thought: <think>nOkay, lets tackle this question. The user is asking what other job Rosie Perez did on the TV show \"In Living Color\" besides being choreographer. need to go through the provided context to find any mentions of her roles beyond choreography. Looking at the context snippets, several mention her as the choreographer for The Fly Girls. But theres line that says: \"Career ; In Living Color, and also worked as segment producer....Also, another line: \"Career ; In Living Color, and also worked as segment producer.\" So multiple mentions of her being segment producer... This is explicitly mentioned in the context snippets, which highlight her dual contributions to the show.}, {Thought: }, {Action: Done}, {Action_Input: } final_result: Rosie Perez also worked as segment producer on the TV series \"In Living Color\". Question and agentic response examples for few-shot chain-of-thought to be used in the ReAct-based Open Reasoning Agent of ODS-v The following 20 examples we used in few-shot chain-of-thought were derived from community campaign we ran to improve ODS with better prompts. Example 1 <Question> What historical event happened closest in time to the invention of the 20 telephone: the American Civil War or the establishment of the Eiffel Tower? </Question> <Thought> need to find the year the telephone was invented. </Thought> <Action>search_internet</Action><Action_Input>year of telephone invention</Action_Input> <Observation> The telephone was invented in 1876. </Observation> <Thought> need to find the year the American Civil War ended. </Thought> <Action>search_internet</Action><Action_Input>year American Civil War ended</Action_Input> <Observation> The American Civil War ended in 1865. </Observation> <Thought> need to find the year the Eiffel Tower was established. </Thought> <Action>search_internet</Action><Action_Input>year Eiffel Tower established</Action_Input> <Observation> The Eiffel Tower was completed in 1889. </Observation> <Thought> will calculate the time differences from 1876. </Thought> <Action>calculate</Action><Action_Input>1876 - 1865 and 1889 - 1876</Action_Input> <Observation> 11 years (Civil War) and 13 years (Eiffel Tower). </Observation> <Thought> The event closest in time to the invention of the telephone is the end of the American Civil War. </Thought> <Action>Done</Action> <Final_Answer> The historical event closest in time to the invention of the telephone is the end of the American Civil War (11 years apart). </Final_Answer> Example <Question> Which country has higher population density: Japan or India? </Question> <Thought> need to find the population and area of Japan. </Thought> <Action>search_internet</Action> <Action_Input>population and area of Japan</Action_Input> <Observation> Japan has population of 125 million and an area of 377,975 square kilometers. </Observation> <Thought> need to find the population and area of India. </Thought> <Action>search_internet</Action> <Action_Input>population and area of India</Action_Input> <Observation> India has population of 1.38 billion and an area of 3,287,263 square kilometers. </Observation> <Thought> need to calculate the population densities of Japan and India. </Thought> <Action>calculate</Action> <Action_Input>125 million / 377,975 and 1.38 billion / 3,287,263</Action_Input> <Observation> Japan: 330.7 people/km²; India: 419.6 people/km². </Observation> <Thought> India has higher population density than Japan. </Thought> <Action>Done</Action> <Final_Answer> India has higher population density (419.6 people/km²) than Japan (330.7 people/km²). </Final_Answer> Example 3 <Question> Which country has won more total Olympic gold medals: the United States or China? </Question> <Thought> need to find the total Olympic gold medals won by the United States. </Thought> <Action>search_internet</Action> <Action_Input>total Olympic gold medals won by the United States</Action_Input> <Observation> The United States has won 1,127 gold medals. </Observation> <Thought> need to find the total Olympic gold medals won by China. </Thought> <Action>search_internet</Action> <Action_Input>total Olympic gold medals won by China</Action_Input> <Observation> China has won 283 gold medals. </Observation> <Thought> will compare the two numbers. </Thought> <Action>calculate</Action> <Action_Input>1,127 - 283</Action_Input> <Observation> The United States has 844 more gold medals than China. </Observation> <Thought> The United States has won more gold medals. </Thought> 21 <Action>Done</Action> <Final_Answer> The United States has won more Olympic gold medals (1,127) than China (283). </Final_Answer> Example <Question> Who discovered the structure of DNA, and in which year was the discovery made? </Question> <Thought> need to find the scientists who discovered the structure of DNA. </Thought> <Action>search_internet</Action> <Action_Input>scientists who discovered DNA structure</Action_Input> <Observation> James Watson and Francis Crick discovered the structure of DNA. </Observation> <Thought> need to find the year of their discovery. </Thought> <Action>search_internet</Action> <Action_Input>year DNA structure discovered</Action_Input> <Observation> The structure of DNA was discovered in 1953. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> James Watson and Francis Crick discovered the structure of DNA in 1953. </Final_Answer> Example 5 <Question> How many meters taller is the Burj Khalifa compared to the Empire State Building? </Question> <Thought> need to find the height of the Burj Khalifa. </Thought> <Action>search_internet</Action> <Action_Input>height of Burj Khalifa</Action_Input> <Observation> The Burj Khalifa is 828 meters tall. </Observation> <Thought> need to find the height of the Empire State Building. </Thought> <Action>search_internet</Action> <Action_Input>height of Empire State Building</Action_Input> <Observation> The Empire State Building is 381 meters tall. </Observation> <Thought> will calculate the difference in height. </Thought> <Action>calculate</Action> <Action_Input>828 - 381</Action_Input> <Observation> The difference is 447 meters. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> The Burj Khalifa is 447 meters taller than the Empire State Building. </Final_Answer> Example 6 <Question> Which country launched the first satellite into space, and what was the name of the satellite? </Question> <Thought> need to find the country that launched the first satellite into space. </Thought> <Action>search_internet</Action> <Action_Input>first satellite launched into space</Action_Input> <Observation> The Soviet Union launched the first satellite. </Observation> <Thought> need to find the name of the satellite. </Thought> <Action>search_internet</Action> <Action_Input>name of first satellite in space</Action_Input> <Observation> The first satellite was Sputnik 1. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> The Soviet Union launched the first satellite into space, named Sputnik 1. </Final_Answer> Example 7 <Question> Which novel by George Orwell introduced the concept of Big Brother, and in what year was it published? </Question> <Thought> need to identify the novel by George Orwell that introduced Big Brother. </Thought> <Action>search_internet</Action> <Action_Input>novel by George Orwell Big Brother</Action_Input> <Observation> The novel is 1984. </Observation> <Thought> need to find the year it was published. </Thought> <Action>search_internet</Action> <Action_Input>year 1984 by George Orwell published</Action_Input> <Observation> 1984 was published in 1949. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> George Orwells novel 1984, which introduced the concept of Big Brother, was published in 1949. </Final_Answer> Example 8 <Question> Which country hosted the first FIFA World Cup, and in what year? </Question> <Thought> need to find the country that hosted the first FIFA World Cup. </Thought> <Action>search_internet</Action> <Action_Input>country hosted first FIFA World Cup</Action_Input> <Observation> Uruguay hosted the first FIFA World Cup. </Observation> <Thought> need to find the year of the first FIFA World Cup. </Thought> <Action>search_internet</Action> <Action_Input>year of first FIFA World Cup</Action_Input> <Observation> The first FIFA World Cup was held in 1930. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Uruguay hosted the first FIFA World Cup in 1930. </Final_Answer> Example <Question> Who invented the light bulb, and what company did he later establish? </Question> <Thought> need to find who invented the light bulb. </Thought> <Action>search_internet</Action> <Action_Input>inventor of the light bulb</Action_Input> <Observation> Thomas Edison invented the light bulb. </Observation> <Thought> need to find the company Thomas Edison established. </Thought> <Action>search_internet</Action> <Action_Input>company founded by Thomas Edison</Action_Input> <Observation> Thomas Edison founded General Electric. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Thomas Edison invented the light bulb and later established General Electric. </Final_Answer> Example 10 <Question> In which city was the Declaration of Independence signed, and in what building? </Question> <Thought> need to find the city where the Declaration of Independence was signed. </Thought> 23 <Action>search_internet</Action> <Action_Input>city where Declaration of Independence was signed</Action_Input> <Observation> The Declaration of Independence was signed in Philadelphia. </Observation> <Thought> need to find the building where it was signed. </Thought> <Action>search_internet</Action> <Action_Input>building where Declaration of Independence was signed</Action_Input> <Observation> It was signed in Independence Hall. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> The Declaration of Independence was signed in Philadelphia at Independence Hall. </Final_Answer> Example <Question> Who developed the theory of general relativity, and in what year was it published? </Question> <Thought> need to find who developed the theory of general relativity. </Thought> <Action>search_internet</Action> <Action_Input>developer of general relativity</Action_Input> <Observation> Albert Einstein developed the theory of general relativity. </Observation> <Thought> need to find the year it was published. </Thought> <Action>search_internet</Action> <Action_Input>year general relativity published</Action_Input> <Observation> The theory of general relativity was published in 1915. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Albert Einstein developed the theory of general relativity, which was published in 1915. </Final_Answer> Example 12 <Question> Which Shakespeare play features the phrase To be, or not to be, and who speaks this line? </Question> <Thought> need to find the Shakespeare play containing the phrase To be, or not to be. </Thought> <Action>search_internet</Action> <Action_Input>Shakespeare play To be, or not to be</Action_Input> <Observation> The play is Hamlet. </Observation> <Thought> need to find who speaks this line in Hamlet. </Thought> <Action>search_internet</Action> <Action_Input>character who says To be, or not to be in Hamlet</Action_Input> <Observation> The line is spoken by Hamlet. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> The phrase To be, or not to be is from Shakespeares Hamlet, and it is spoken by the character Hamlet. </Final_Answer> Example 13 <Question> What is the tallest mountain in Africa, and how high is it? </Question> <Thought> need to find the tallest mountain in Africa. </Thought> <Action>search_internet</Action> <Action_Input>tallest mountain in Africa</Action_Input> <Observation> Mount Kilimanjaro is the tallest mountain in Africa. </Observation> <Thought> need to find the height of Mount Kilimanjaro. </Thought> <Action>search_internet</Action> <Action_Input>height of Mount Kilimanjaro</Action_Input> <Observation> Mount Kilimanjaro is 5,895 meters tall. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Mount Kilimanjaro, the tallest mountain in Africa, is 5,895 meters high. </Final_Answer> Example 14 <Question> Who was the first President of the United States to serve two non-consecutive terms? </Question> <Thought> need to find the President of the United States who served two non-consecutive terms. </Thought> <Action>search_internet</Action> <Action_Input>President who served two non-consecutive terms</Action_Input> <Observation> Grover Cleveland was the first President to serve two non-consecutive terms. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Grover Cleveland was the first President of the United States to serve two non-consecutive terms. </Final_Answer> Example 15 <Question> What planet is the largest in our solar system, and what is its diameter? </Question> <Thought> need to find the largest planet in our solar system. </Thought> <Action>search_internet</Action> <Action_Input>largest planet in solar system</Action_Input> <Observation> Jupiter is the largest planet in the solar system. </Observation> <Thought> need to find the diameter of Jupiter. </Thought> <Action>search_internet</Action> <Action_Input>diameter of Jupiter</Action_Input> <Observation> Jupiters diameter is approximately 139,820 kilometers. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Jupiter is the largest planet in the solar system, with diameter of approximately 139,820 kilometers. </Final_Answer> Example <Question> What was the first airplane to fly, and in what year did it achieve this feat? </Question> <Thought> need to find the name of the first airplane to fly. </Thought> <Action>search_internet</Action> <Action_Input>first airplane to fly</Action_Input> <Observation> The first airplane to fly was the Wright Flyer. </Observation> <Thought> need to find the year the Wright Flyer achieved its first flight. </Thought> <Action>search_internet</Action> <Action_Input>year Wright Flyer first flight</Action_Input> <Observation> The Wright Flyer flew for the first time in 1903. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> The Wright Flyer was the first airplane to fly, achieving this feat in 1903. </Final_Answer> Example 17 <Question> Who painted the Mona Lisa, and where is it displayed? </Question> <Thought> need to find the artist who painted the Mona Lisa. </Thought> 25 <Action>search_internet</Action> <Action_Input>artist who painted Mona Lisa</Action_Input> <Observation> Leonardo da Vinci painted the Mona Lisa. </Observation> <Thought> need to find where the Mona Lisa is displayed. </Thought> <Action>search_internet</Action> <Action_Input>where is the Mona Lisa displayed</Action_Input> <Observation> The Mona Lisa is displayed in the Louvre Museum in Paris. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Leonardo da Vinci painted the Mona Lisa, which is displayed in the Louvre Museum in Paris. </Final_Answer> Example <Question> Who has won the most Grand Slam tennis titles, and how many have they won? </Question> <Thought> need to find the tennis player with the most Grand Slam titles. </Thought> <Action>search_internet</Action> <Action_Input>player with most Grand Slam tennis titles</Action_Input> <Observation> Novak Djokovic has won the most Grand Slam titles. </Observation> <Thought> need to find the number of Grand Slam titles he has won. </Thought> <Action>search_internet</Action> <Action_Input>number of Grand Slam titles Novak Djokovic</Action_Input> <Observation> Novak Djokovic has won 24 Grand Slam titles. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Novak Djokovic has won the most Grand Slam tennis titles, with 24 titles. </Final_Answer> Example 19 <Question> Who was the longest-reigning monarch in British history, and how many years did they reign? </Question> <Thought> need to find the longest-reigning monarch in British history. </Thought> <Action>search_internet</Action> <Action_Input>longest reigning monarch in British history</Action_Input> <Observation> Queen Elizabeth II was the longest-reigning monarch in British history. </Observation> <Thought> need to find how many years she reigned. </Thought> <Action>search_internet</Action> <Action_Input>length of reign Queen Elizabeth II</Action_Input> <Observation> Queen Elizabeth II reigned for 70 years. </Observation> <Thought> Do need further steps? No. </Thought> <Action>Done</Action> <Final_Answer> Queen Elizabeth II was the longest-reigning monarch in British history, with reign of 70 years. </Final_Answer> Example 20 <Question> Which Shakespeare play contains the line \"All the worlds stage,\" and how many years ago was it first performed if today is 2024? </Question> <Thought> need to identify the Shakespeare play with the line \"All the worlds stage.\" </Thought> <Action> search_internet </Action> <Action_Input> Shakespeare play All the worlds stage </Action_Input> <Observation> The line is from \"As You Like It.\" </Observation> <Thought> need the year \"As You Like It\" was first performed. </Thought> <Action> search_internet </Action> <Action_Input> year As You Like It first performed </Action_Input> <Observation> \"As You Like It\" was first performed in 1603. </Observation> <Thought> need to calculate how many years ago 1603 was from 2024. </Thought> <Action> calculate </Action> <Action_Input> 2024 - 1603 </Action_Input> <Observation> 421 years. </Observation> <Thought> Do need to use tool? No. </Thought> <Action> Done </Action> <Final_Answer> \"As You Like It\" contains the line \"All the worlds stage\" and was first performed 421 years ago in 1603. </Final_Answer>"
        },
        {
            "title": "C Related work",
            "content": "Recent advancements in NLP have increasingly combined retrieval with generation to boost factual accuracy in knowledge-intensive tasks. Frameworks like Retrieval-Augmented Generation (RAG) dynamically retrieve and update external knowledge sources (as opposed to knowledge contained in an LLMs static, pre-stored parameters) [9]. Prompting techniques such as Chain-of-Thought (CoT) [31] and ReAct [33] further bolsters factuality through reasoning steps and tool access. Closed-source, commercial search engines have arisen from the success of such retrieval techniques. Fresh off $500 million funding round in December, 2024, Perplexity has positioned itself as premiere LLM-enabled search engine [21]. Other closed-source frameworks perform notably on search benchmarks, such as Exa [1] and Linkup [15]. While the monetary incentive for closed-source search engines is clear, opaque systems pose downsides for users and prevent researchers from building on their advances. Generative search engines are rife with bias [10], prone to hallucination [26], amenable to surveillance [35], and susceptible to providing dangerous information [13]. Generative web-enabled search is burgeoning field, catalyzed by expanding context windows and better reasoning abilities [12]; with such growth comes responsibility to democratize access and mitigate risks with open-source research. Some open-source efforts have made commendable strides in the desiging LLM-assisted search architectures. [7] create an information retrieval system to access pre-indexed scientific articles with the assistance of fine-tuned LLM. [16] construct new, fully decentralized database using LLMderived embeddings (rather than accessing an existing web index) for scalable semantic search by routing queries through semantically similar neighbors. Recent work has also focused on actively querying the web, such as [2], which harnesses multiagent LLM framework to incrementally plan, coordinate, and integrate large volumes of live web information in real time. Other work has brought multimodal dimension to web-enabled search for real-time interaction with live websites [34]."
        }
    ],
    "affiliations": [
        "Princeton University",
        "Sentient",
        "UC Berkeley",
        "University of Washington"
    ]
}