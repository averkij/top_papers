{
    "paper_title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
    "authors": [
        "Thanh-Long V. Le",
        "Myeongho Jeon",
        "Kim Vu",
        "Viet Lai",
        "Eunho Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 8 8 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "NO PROMPT LEFT BEHIND: EXPLOITING ZEROVARIANCE PROMPTS IN LLM REINFORCEMENT LEARNING VIA ENTROPY-GUIDED ADVANTAGE SHAPING Thanh-Long V. Le1, Myeongho Jeon2, Kim Vu1, Viet Lai3, Eunho Yang1 1 KAIST 2 EPFL 3 Adobe Research"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same rewardso-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with ZeroVariance Prompts (RL-ZVP), novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR. Figure 1: Left: RL-ZVP uses an entropy-guided advantage formulation to extract learning signals from zero-variance prompts, while reverting to GRPO on other prompts. Right: RL-ZVP demonstrates significantly higher average accuracy than GRPO across six math reasoning benchmarks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent frontier models such as OpenAI o1 (OpenAI et al., 2024), Claude 3.7 Sonnet (Anthropic, 2025) DeepSeek-R1 (DeepSeek-AI et al., 2025), and Kimi k1.5 (Team et al., 2025) provide compelling proof of concept for leveraging long chain of thought (Wei et al., 2023; Kojima et al., 2023) to enhance the reasoning capabilities of Large Language Models (LLMs)."
        },
        {
            "title": "Preprint",
            "content": "Following these advances, Reinforcement Learning with Verifiable Rewards (RLVR) techniques such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024) and Dynamic Sampling Policy Optimization (DAPO) (Yu et al., 2025) have emerged as the de facto standard for strengthening LLM reasoning. In RLVR, the policy model is trained by sampling multiple responses (rollouts) to prompt and then reinforcing correct answers while penalizing incorrect ones. While this approach is effective when responses for the same prompt contains both right and wrong responses, it does not account for scenarios when group of rollouts are entirely correct or entirely incorrect (with identical rewards), referred to as zerovariance prompts (Zheng et al., 2025). In such situations, the normalized rewards (i.e., advantage values) collapse to zero, resulting in no training signal, rendering all rollouts for those prompts uninformative. While it may seem conceptually reasonable to ignore these prompts, doing so remains problematic because rollout generation itself is major bottleneck in RL training (Zhong et al., 2025; Noukhovitch et al., 2025; Sheng et al., 2024). Our preliminary experiments show that response sampling typically takes up about 50% of the total time per training step, which emphasizes the significant throughput overhead introduced by rollouts (Figure 2). Figure 2: Rollout generation overhead as percentage of total time of the training step. Recent methods acknowledge the inefficiency of zero-variance prompts and attempt to filter them out, either pre-rollout (Zheng et al., 2025; Qu et al., 2025) or post-rollout (Yu et al., 2025). However, to the best of our knowledge, no prior work has investigated whether zero-variance prompts themselves can serve as valuable source of learning signals for policy optimization. In this work, we argue that zero-variance prompts could, in fact, provide valuable feedback for policy learningfor example, encouraging the model to exploit all-correct paths while discouraging allwrong paths and promoting exploration of alternatives. Furthermore, inspired by recent advances in reward shaping (Deng et al., 2025; Lei et al., 2025; Devidze et al., 2022; Ng et al., 1999), we propose that token-level information should be explicitly considered in how such feedback is utilized. Building on these insights, we introduce RL-ZVP (Reinforcement Learning with Zero-Variance Prompts), novel algorithm that extract useful learning signals from zero-variance prompts to improve the effectiveness of RL training. Our approach rests on two key intuitions: (i) the policy model should still be rewarded for correct answers even if no incorrect answers are present in the same group (and penalized analogously), and (ii) the degree of reward or penalty should be determined by the characteristics of sampled tokens. Concretely, we formulate the advantage for zero-variance prompts around two properties: direction, which aligns with the correctness of the response, and magnitude, which is quantified by the entropy value of each token within the response (Figure 1). Experiments on six math reasoning benchmarks across two model scalesQwen3-1.7B-Base and Qwen-8B-Base (Yang et al., 2025b)show that RL-ZVP significantly outperforms GRPO, with an average improvement of 4.00 points in accuracy (Acc@8) and 4.28 points in pass rate (Pass@8). Notably, RL-ZVP achieves gains of up to 8.66 points in Acc@8 (on AIME25) and 7.77 points in Pass@8 (on OlympiadBench). RL-ZVP also consistently surpasses other baselines that filter out zero-variance prompts, even under unfavorable settings where these baselines are given 35 more rollouts. Moreover, across both scales, RL-ZVP demonstrates more stable and consistent learning process. These results establish zero-variance prompts as valuable resource for learning signals and open new perspective on how to utilize them, rather than discarding them, in RLVR."
        },
        {
            "title": "2 PRELIMINARY: GROUP RELATIVE POLICY OPTIMIZATION",
            "content": "Group Relative Policy Optimization (GRPO) (Shao et al., 2024) is variant of Proximal Policy Optimization (PPO) (Schulman et al., 2017) tailored for LLM post-training. Given questionanswer pair (q, a) from dataset D, group of responses (rollouts) {oi}G i=1 is sampled from the old policy πθold (typically the policy from the previous training iteration). Each response is compared against the ground-truth answer to produce final rewards {Ri}G i=1. Instead of relying on costly value model like PPO, GRPO normalizes these rewards to compute the advantage as: ˆAi,t = Ri mean({Ri}G std({Ri}G i=1) i=1) , (1)"
        },
        {
            "title": "Preprint",
            "content": "where denotes token position in oi. Note that the advantage ˆAi is computed at the response level and subsequently broadcast to all tokens in the response, i.e., ˆAi,1 = ˆAi,2 = = ˆAi,G = ˆAi. The policy objective1 for single prompt is then defined as: JGRPO (cid:16) θ q, a, {oi}G i=1 (cid:17) = 1 (cid:88) i=1 1 oi oi (cid:88) t= (cid:16) ri,t(θ) ˆAi,t, clip(cid:0)ri,t(θ), 1 ϵ, 1 + ϵ(cid:1) ˆAi,t min (cid:17) , (2) where ri,t(θ) = πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) . Here, ri,t(θ) represents the importance sampling ratio that adjusts the objective to account for the off-policy distribution of the training batch. The final GRPO objective for the whole dataset is: JGRPO(θ, D) = (q,a)D,{oi}G i=1πθold (q) (cid:2)JGRPO(θq, a, {oi}G i=1)(cid:3) . (3)"
        },
        {
            "title": "3 REINFORCEMENT LEARNING WITH ZERO-VARIANCE PROMPTS",
            "content": "In this section, we introduce our algorithm, Reinforcement Learning with Zero-Variance Prompts. We begin by motivating the approach through the advantage vanishing problem in zero-variance prompts (Section 3.1). We then present our entropy-guided advantage formulation (Section 3.2) and conclude with the full training objective and algorithm (Section 3.3). 3.1 MOTIVATION: ADVANTAGE VANISHING IN ZERO-VARIANCE PROMPTS During GRPO training, the policy model may encounter questions where all sampled responses in group are either entirely correct or entirely incorrect, resulting in identical rewards. Following Zheng et al. (2025), we call these cases zero-variance prompts. They typically arise in two situations: when the dataset is either (i) too difficult or (ii) too easy for the model. From training perspective, zero-variance prompts appear most frequently (i) at the beginning of training, when the model often fails to solve problems even once across multiple responses, and (ii) near convergence, when the model has become strong enough to solve many questions entirely. To understand the prevalence of zero-variance prompts, we examine two settings: Qwen3-1.7B-Base trained on MATH, and Qwen38B-Base trained on DAPO-Math-17k. We found that zero-variance prompts account for roughly 30% to 99% of each batch during training (Figure 3). This high proportion suggests that if these prompts could be leveraged effectively, they potentially represent an untapped source of useful learning signal for the policy model. Remark 1 (Advantage Vanishing). In Equation (1) and (2), if all responses within group receive identical rewards, their advantages degenerate to zero. In this case, the GRPO objective reduces to JGRPO (cid:1) = 0, effectively discarding all the potential learning signal. Figure 3: The percentage of zero-variance prompts. (cid:0)θ qzvp, a, {oi}G i=1 From Equation 2, taking the gradient of both sides2 gives: θJGRPO(q, a, {oi}G i=1) = 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 ri,t(θ) ˆAi,tθ log πθ (cid:0)oi,t q, oi,<t (cid:1). (4) From this expression, the gradient of the GRPO objective with respect to the policy parameters can be interpreted as weighted sum of the log-probability gradients for all tokens in response. Intuitively, GRPO raises the probabilities of tokens in correct responses while lowering those of tokens in incorrect ones. Under GRPO advantage formulation, however, if we ignore the importance-sampling ratio, each token in response receives the same weight, regardless of its role or informativeness. 1We omit the KL penalty here for clarity. 2We omit the clipping factor before gradient derivation for clarity."
        },
        {
            "title": "Preprint",
            "content": "Remark 2 (The Role of Advantage). The sign of the advantage ˆAi,t determines the direction of the gradient update for each token since the importance sampling ratio ri,t in Equation (4) is strictly positive, while its absolute value ˆAi,t governs the magnitude of the update in conjunction with ri,t. Recently, several methods have excluded zero-variance prompts from RL training (Yu et al., 2025; Zheng et al., 2025; Cheng et al., 2025), viewing them merely as way to reduce rollout cost and implicitly assuming they offer no value for policy optimization. We challenge this assumption and argue that, when handled appropriately, zero-variance prompts could provide valuable learning signals for RL training."
        },
        {
            "title": "3.2 ADVANTAGE FORMULATION",
            "content": "We propose Reinforcement Learning with Zero-Variance Prompts (RL-ZVP), novel algorithm designed to exploit zero-variance prompts. The key intuition is to address the limitation noted in Remark 1: the policy should be rewarded or penalized based solely on the correctness of its responses, even when no contrasting answers are present. RL-ZVP behaves identically to GRPO on non-zero-variance prompts, but adopts different strategy for zero-variance ones, thereby generalizing GRPO. We first classify zero-variance prompts with all correct responses as positive prompts and those with all incorrect responses as negative prompts. Based on Remark 2, we frame advantage shaping as developing appropriate scaling factors, with focus on two properties: direction and magnitude. Advantage direction. The sign of the advantage is straightforward: positive (+) for positive prompts and negative () for negative ones. Intuitively, when all sampled responses are correct, the model should increase their probabilities, reinforcing certainty and encouraging exploitation. Conversely, when all responses are incorrect, their probabilities should be reduced, discouraging repeated mistakes and promoting exploration of alternative sampling paths. Advantage magnitude. Inspired by recent works that incorporate token-level signals to augment verifiable rewards (Cui et al., 2025; Fang et al., 2025; Agarwal et al., 2025; Deng et al., 2025; Lei et al., 2025), we leverage token entropy in the formulation of the advantage magnitude for zero-variance prompts. The key idea is to scale each tokens gradient update in proportion to its entropy. For correct responses, high-entropy tokenstypically those crucial for reasoning, such as connectors or branching points(Wang et al., 2025)receive larger updates than semantically trivial tokens. This encourages reasoning behaviors such as reflection and verification (DeepSeek-AI et al., 2025), while avoiding over-exploitation of trivial text completion patterns. For incorrect responses, the probabilities of all tokens are reduced, but high-entropy tokens (that initiate reasoning paths and behaviors) are penalized less severely, preserving flexibility for the model to revisit and explore those reasoning paths in future iterations. Advantage formula. Combining these two design choices, our final token-level advantage formulation for zero-variance prompts, replacing the zero assigned in GRPO, is: ˆAZVP(oi,t) = αHi,t, (cid:16) α maxoi k=1(Hi,k) Hi,t if std({Ri}G i=1) = 0 Ri > 0, (cid:17) , if std({Ri}G i=1) = 0 Ri 0, (5) with Hi,t = (cid:88) j=1 πθ(vj q, oi,<t) log(cid:2)πθ(vj q, oi,<t) (cid:3), 3 is the entropy of each token oi,t in the where α denotes the scaling factor hyperparameter, and Hi,t response oi given the policy models vocabulary set = {v1, v2, }. The term maxoi k=1(Hi,k) reverts the effect of token entropy in negative prompts to align with our intuition: tokens with higher entropy incur smaller penalties, while low-entropy tokens are penalized more. 3In the implementation, we detach every occurrence of Hi,t from the computational graph so that it acts as scalar factor rather than differentiable parameter."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: Reinforcement Learning with Zero-Variance Prompts Input: initial policy model πθ; training dataset D; for iteration = 1, . . . , do Update the previous policy πθold πθ; Sample (w/o replacement) prompt batch D; foreach (q, a) do Sample outputs {oi}G Compute rewards {Ri}G for gradient step = 1, . . . , µ do i=1 πθold( q); i=1 by verifying each oi against a; Sample (w/o replacement) mini-batch Bmini B; foreach (q, a, {Ri}G if std({Ri}G else compute advantages { ˆAZVP(oi,t)} using equation 5; i=1) Bmini do i=1) > 0 then compute advantages { ˆAi,t} using equation 1; Update the policy model πθ by maximizing the RL-ZVP objective 6; Output: final policy model πθ 3.3 FINAL OBJECTIVE Let DZVP be the set of zero-variance prompts encountered during training (DZVP D). Incorporating our advantage shaping for zero-variance prompts, the RL-ZVP objective becomes: JRL-ZVP(θq, a, {oi}G i=1) = 1 (cid:80)G i=1 (cid:80)oi 1 oi t=1 min ri,t(θ) ˆAZVP(oi,t), (cid:17) clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAZVP(oi,t) (cid:16) JGRPO(θq, a, {oi}G i=1), , if DZVP, (6) otherwise. Remark 3 (Relationship to GRPO). RL-ZVP does not introduce any new loss term beyond the GRPO objective. It simply reformulates the special case where the prompt is zero-variance. In fact, the GRPO objective from Equation (2) can be equivalently expressed in branching form: JGRPO(θ q, a, {oi}G i=1) = (cid:40)0, if DZVP, JGRPO(θ q, a, {oi}G i=1), otherwise. (7) In this regard, our formulation serves as generalization of GRPOreducing exactly to GRPO when prompts are non-zero-variance, while resulting in non-trivial update for zero-variance prompts rather than collapsing to zero."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we first outline our experimental setup, including models, datasets, and implementation details (Section 4.1). We then present validation results and analysis, comparing RL-ZVP with GRPO and prompt filtering baselines (Section 4.2). Finally, we conduct ablation studies to assess the contribution of each sub-component in RL-ZVP (Section 4.3). 4.1 EXPERIMENTAL SETTINGS Models & Datasets. We conduct experiments on both Qwen3-1.7B-Base and Qwen3-8BBase (Yang et al., 2025a; Liu et al., 2025b) to assess performance across model scales. For Qwen31.7B-Base, we train on the MATH dataset (Hendrycks et al., 2021; Lightman et al., 2023), which contains 7.5k problems. For Qwen3-8B-Base, we adopt the more challenging DAPO-Math-17k dataset (Yu et al., 2025) to better exploit the capacity of larger models. Within the scope of this work, we refer to these two setups as small scale and large scale, respectively. We then evaluate the trained models on six standard math reasoning benchmarks: Minerva, AMC23, MATH (Hendrycks"
        },
        {
            "title": "Preprint",
            "content": "et al., 2021), AIME24, AIME25, and OlympiadBench (He et al., 2024). For each question, we generate eight responses and report both the accuracy (Acc@8) and the pass rate (Pass@8). To further evaluate RL-ZVP, we also report additional results with different training dataset in Appendix A. Baselines. Since RL-ZVP is the first method that attempts to explicitly leverage zero-variance prompts, we compare it against GRPO (Shao et al., 2024) and two representative methods that filter out zero-variance prompts: GRPO with Dynamic Sampling (GRPO-DS) (Yu et al., 2025) and GRESO (Zheng et al., 2025). GRPO-DS repeatedly samples questions, generates group of responses, discards zero-variance cases, and constructs batches containing only non-zero-variance prompts, ensuring that every prompt contributes to the gradient signal. In contrast, GRESO predicts and skips zero-variance prompts without responses sampling, using probabilistic filtering strategy based on historical training dynamics. For GRPO-DS and GRESO, we report results under two settings: (r) using the same number of rollouts as RL-ZVP for fair comparison, and (g) using the same number of gradient steps. The latter favors the prompt filtering baselines, as it allows roughly 35 more rollouts than RL-ZVP and GRPO (see Table 5 in Appendix B). Implementation Details. All experiments are implemented using the verl framework (Sheng et al., 2024), following the standard hyperparameter settings from recent works (Yu et al., 2025; Zheng et al., 2025). We train with batch size of 512 and mini-batch size of 32, resulting in 16 gradient steps per training iteration. Full implementation details are provided in Appendix C. 4.2 RESULTS & ANALYSIS Table 1: Detailed evaluation results on six math reasoning benchmarks. The best and second best performance across all settings are bold and underscored, respectively. Under fair setup of equal rollout budget, RL-ZVP achieves the best performance across all datasets, heavily outperforming GRPO. (*) Even in the unfavorable setting where Dynamic Sampling baselines use 3 5 more rollouts, RL-ZVP still outperforms them on the majority of benchmarks. Method Minerva AMC23 MATH500 AIME24 AIME25 OlympiadBench Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Qwen3-1.7B-Base trained with MATH (small scale) GRPO GRPO-DS-r GRPO-DS-g* GRESO-r GRESO-g* 29.09 29.96 29.96 27.99 30.19 RL-ZVP (Ours) 29.96 (w.r.t GRPO) +0.87 43.19 44.02 44.02 43.39 43. 44.76 +1.57 42.19 42.50 46.25 40.00 45.00 48.75 +6.56 72.00 73.84 73.84 70.25 73.86 77.19 +5.19 69.09 68.31 70.72 67.53 69. 70.98 +1.89 84.05 83.04 84.61 83.07 84.77 87.35 +3.3 8.75 7.50 7.50 7.08 9.58 12.50 +3.75 17.24 17.41 25.77 18.48 19. 23.99 +6.75 4.17 2.92 7.50 3.75 5.00 6.25 +2.08 Qwen3-8B-Base trained with DAPO-Math-17k (large scale) GRPO GRPO-DS-r GRPO-DS-g* GRESO-r GRESO-g* 44.99 29.96 44.76 30.74 45. RL-ZVP (Ours) 45.96 (w.r.t GRPO) +0.97 58.42 50.85 59.60 50.53 58.69 58.83 +0.41 73.75 47.19 76.25 49.38 78.13 78.75 +5.00 92.48 76.70 92.43 80.53 92. 94.18 +1.70 83.00 68.20 84.98 67.40 83.23 89.73 +6.73 92.40 87.26 92.42 87.79 92.24 96.13 +3.73 28.33 12.08 27.08 12.08 25. 33.33 +5.00 50.64 31.33 42.72 29.31 41.80 54.66 +4.02 20.00 9.17 19.58 10.83 17.92 24.58 +4.58 6.67 8.73 12.95 12.89 11. 13.23 +6.56 31.59 22.02 35.93 26.26 32.12 39.36 +7.77 33.20 32.67 35.68 31.30 34.68 35.11 +1.91 49.59 43.88 44.83 44.92 44. 58.20 +8.61 50.52 49.89 53.03 49.13 52.43 54.84 +4.32 66.20 58.16 60.42 59.66 59.79 72.28 +6.08 RL-ZVP achieves improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO. We present our main results in Table 1. Across six benchmarks, RL-ZVP consistently delivers significant gains over GRPO, ranging from 0.87 to 6.75 points on Qwen3-1.7B-Base and from 0.41 to 8.61 points on Qwen3-8B-Base. On small scale, the average improvements are 2.84 points in Acc@8 and 4.62 points in Pass@8, while on large scale, the average gains are 5.15 points in Acc@8 and 3.95 points in Pass@8. These results indicate that leveraging zero-variance prompts provides strong and informative learning signals that substantially enhance reasoning performance, with improvements generalizing across both model scales and datasets. RL-ZVP consistently outperforms prompt-filtering baselines, even under disadvantageous settings. Under the fair setting with the same rollout budget, RL-ZVP substantially surpasses both GRPO-DS-r and GRESO-r across all benchmarks, achieving average gains of 3.28 points in Acc@8 and 4.03 points in Pass@8 on the small scale, 19.20 points in Acc@8 and 13.56 points in Pass@8 on the large scale (See Figure 6 in Appendix B). Even in the less favorable setting with the same number"
        },
        {
            "title": "Preprint",
            "content": "of gradient stepswhere the baselines require 35 more rolloutsRL-ZVP still heavily outperforms GRPO-DS-g and GRESO-g on the majority of benchmarks, with average improvements (over their best performance) of 0.99 points in Acc@8 and 1.19 points in Pass@8 on the small scale, and 5.51 points in Acc@8 and 5.32 points in Pass@8 on the large scale. These results not only reaffirm the importance of learning signals from zero-variance prompts, but also provide new insight: leveraging such prompts brings greater benefits than discarding them, directly challenging the prevailing practice adopted in recent works. (a) Acc@8 (small scale) (b) Entropy (small scale) (c) Response Length (small scale) (d) Acc@8 (large scale) (e) Entropy (large scale) (f) Response Length (large scale) Figure 4: Validation accuracy and training dynamics at different experiment scales. Each row shows Acc@8, entropy, and response length during training for Qwen3-1.7B-Base (top) and Qwen3-8B-Base (bottom). RL-ZVP exhibits more consistent and stable trends than GRPO. RL-ZVP demonstrates more stable and consistent learning process. Figure 4a and 4d present the average validation accuracy across benchmarks of RL-ZVP and GRPO as training progresses. On 1.7B scale, RL-ZVP initially shows competitive results with GRPO, but after roughly 500 gradient steps, it achieves clear performance boost, creating significant performance gap. On 8B scale, the difference is even more pronounced: RL-ZVP exhibits smooth and steadily improving curve, while GRPO suffers sharp drop during the first 200 gradient steps, then recovers and but eventually underperforms RL-ZVP. This highlights RL-ZVPs superior effectiveness and stability, which avoids the performance collapses observed at the start of training with GRPO on large scale. We attribute this robustness to the quantity and quality of learning signals each method receives at different training stages. In small scale experiments, the number of zero-variance prompts increases as training progresses (Figure 3). The model improves and produces more positive prompts (allcorrect responses), which RL-ZVP can leverage as useful signals, while GRPO discards them. This leads to widening gap in later stages of training. For large-scale experiments, the situation is reversed at the start of training: the model initially solves fewer than 5% of problems per batch (Figure 3), leaving GRPO with sparse and noisy feedback. In contrast, RL-ZVP can still exploit the abundance of negative prompts (all-incorrect responses), obtaining denser and thereby resulting in greater training stability in the early stages. Although the proportion of zero-variance prompts decreases as training continues, it remains non-negligible, sustaining huge advantage for RL-ZVP over GRPO. Overall, across both scales, RL-ZVP consistently benefits from the additional signals provided by zero-variance prompts, ensuring stronger and more stable performance than GRPO. RL-ZVP encourages longer and more confident reasoning across model scale. As training progresses, RL-ZVP steadily reduces entropy while producing increasingly longer responses. Both trends align with significant gains in validation results (Figure 4). These patterns, along with the qualitative demonstration (Appendix D), suggest two complementary effects related to the policy model: (i) enhanced capabilities due to the emergence of long-form reasoning behaviorsthe socalled aha-moment (DeepSeek-AI et al., 2025; Jin et al., 2024)and (ii) greater confidence in exploratory decisions, as reflected in the low entropy (Cheng et al., 2025; Kadavath et al., 2022)."
        },
        {
            "title": "Preprint",
            "content": "On the small scale, models trained with RL-ZVP show significant increase in response length over training, indicating the ability to generate extensively long chains of reasoning that greatly improves its reasoning abilities (Jin et al., 2024; DeepSeek-AI et al., 2025). In contrast, GRPOtrained models exhibit minimal growth in response length, aligning with their bounded validation performance and potentially their inability to generate multi-step, well-structured solutions. On the large scale, entropy initially drops sharply but then stabilizes after roughly 300 gradient steps rather than collapsing. Combined with the consistent growth in response length and accuracy, this rules out entropy collapse and emphasizes RL-ZVPs ability to promote detailed and confident reasoning (Cui et al., 2025; Wang et al., 2024). RL-ZVP facilitates more sophisticated reasoning even with small policy models. In many cases, especially on 1.7B scale, GRPO-trained models produce simple and direct solutions, jumping straight into problem-solving with little structure. Even when they show glimpses of more complex behaviors such as reflection, these behaviors are shallow and disorganized. In contrast, models trained with RL-ZVP display much more sophisticated and systematic reasoning strategies: they restate and reflect on the key hypotheses, carefully interpret the clues, plan solutions step by step, organize their reasoning into major steps with detailed sub-solutions, and reasonably attempt alternative strategies when encountering dead ends. Illustrative examples are provided in Appendix D. For instance, during exploration, GRPO-trained models often rely on shallow trial-and-error heuristics (e.g., lets try another set of values,lets try different method), which resemble secondguessing and lack of direction. In contrast, models trained with RL-ZVP adopt more principled and comprehensive strategies (e.g., lets double check, assume symmetry to simplify the problem, try to factor directly,lets try to express the sides using the Law of Cosines), demonstrating focused and systematic exploration. Remarkably, such behaviors emerge even in models as small as 1.7B when trained with RL-ZVP, whereas GRPO fails to elicit them at this scale. On 8B scale, both GRPO and RL-ZVP produce systematic solutions with clear structure. However, GRPO outputs tend to be repetitive and rigid in tone, while RL-ZVP exhibits more flexible and exploratory style of reasoning, reflecting deeper ability to search for and evaluate alternative solution paths. 4.3 ABLATION STUDY To better understand the contribution of each design choice in RL-ZVP, we conduct ablation experiments on Qwen3-1.7B-Base trained with MATH unless stated otherwise. Impact of zero-variance type and entropy scaling. We examine the contribution of three subcomponents in our advantage formulation for RL-ZVP: negative prompts, positive prompts, and the entropy scaling factor. Table 2 reports the performance when each component is removed, comparing against full RL-ZVP and GRPO. The results are consistent across benchmarks: full RL-ZVP achieves the best performance in all evaluations. Ignoring either negative or positive prompts yields comparable performance, confirming that both play similar roles in providing learning signals for the policy models. Importantly, removing the entropy scaling factor (e.g., using only sample-level +1/-1 as the advantage) causes the most severe degradation. This emphasizes the importance of advantage magnitude design for zero-variance prompts learning signal: simply promoting correct answers and penalizing incorrect ones is insufficient. Rewards and penalties must also be weighted appropriately to be effective. Table 2: Ablation on the subcomponents of RL-ZVP. Full RL-ZVP achieves the best average performance, while the second-best results mostly occur when single component is removed. Method RL-ZVP (Ours) - w/o negative prompts - w/o positive prompts - w/o entropy scaling GRPO Minerva AMC23 Math500 Average Acc@8 29.96 Pass@8 Acc@8 48.75 44.76 Pass@8 Acc@8 70.98 77. Pass@8 Acc@8 49.90 87.35 Pass@8 69.77 29.92 29.50 28.95 29.09 43.87 43.88 43.02 43. 44.69 41.56 42.50 42.19 70.51 70.40 70.76 72.00 68.64 70.70 69.19 69. 84.40 85.81 84.37 84.05 47.75 47.25 46.88 46.79 66.26 66.70 66.05 66. Impact of α. At the beginning of training, the top 20% of tokens with the highest entropy often take values between 2 and 10, which can result in very large advantages if left unscaled. To avoid explod-"
        },
        {
            "title": "Preprint",
            "content": "ing updates and potential training instability, we introduce α as the scaling factor in the advantage computation. Table 3 shows the performance of RL-ZVP under different α values. We observe that RL-ZVP achieves its best results with α = 0.10 or α = 0.20, while performance drops sharply at both α = 0.05 and α = 0.30. This demonstrates that α plays critical role in balancing stability and effective learning: overly small values fail to sufficiently utilize learning signal from zero-variance prompts, while overly large values may cause unstable training with large gradient updates. Thus, α must be tuned carefully to fully realize the benefits of our entropy-guided advantage design. Table 3: Validation results under different α. Alpha α = 0.05 α = 0.10 α = 0.20 α = 0.30 Minerva AMC Math500 Average Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@ Pass@8 29.60 29.96 29.60 28.68 43.78 44.76 44.54 42.61 42.19 48.75 43.75 41.56 69.44 77.19 74.14 71.22 68.56 70.98 71.10 67. 82.96 87.35 86.47 83.46 46.78 49.90 48.15 45.81 65.39 69.77 68.38 65."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reinforcement Learning for LLM Reasoning. With the rise of test-time scaling (Muennighoff et al., 2025) and large reasoning models (Team et al., 2025; DeepSeek-AI et al., 2025; OpenAI et al., 2024), the post-training process has increasingly shifted toward Reinforcement Learning with Verifiable Rewards (RLVR), which enhances reasoning ability using rule-based feedback. However, these methods either discard zero-variance prompts implicitly (Shao et al., 2024; Liu et al., 2025a) or explicitly (Yu et al., 2025; Zheng et al., 2025; Qu et al., 2025). This leads to wasted computation and reduced data efficiency during online training as rollouts remains major bottleneck in RL training (Zhong et al., 2025; Noukhovitch et al., 2025). Our work addresses this gap by extracting useful signals from zero-variance prompts, thereby mitigating useless rollout costs. Online Prompt Selection. Zero-variance prompts have been identified as key weakness of GRPO in recent works Yu et al. (2025); Zheng et al. (2025); Qu et al. (2025). DAPO addresses this issue with post-rollout oversampling: repeatedly sampling until batch containing only non-zero-variance prompts is obtained (Yu et al., 2025). GRESO improves efficiency with pre-rollout probabilistic filter, leveraging training history to predict whether prompt is likely to be zero-variance (Zheng et al., 2025). While these approaches can be broadly viewed as online prompt selection methods, our work differs fundamentally: instead of filtering out zero-variance prompts, we propose extracting useful learning signals from them, thereby improving the effectiveness of RL training."
        },
        {
            "title": "6 CLOSING REMARKS",
            "content": "In this paper, we introduced RL-ZVP, novel algorithm that extracts useful learning signals from zero-variance prompts to enhance policy optimization. Within these prompts, RL-ZVP directly rewards correct responses and penalizes incorrect ones, without requiring contrastive answers within the rollout group. Furthermore, RL-ZVP adopts fine-grained optimization strategy, scaling updates by token entropy to elicit stronger reasoning behaviors in the policy model. Empirical results show that RL-ZVP delivers significant improvements, outperforming GRPO by up to 8.61 points in accuracy and 7.77 points in pass rate, while also substantially surpassing other prompt filtering baselines. To our best knowledge, this is the first work to demonstrate that zero-variance prompts can be exploited rather than discarded. We believe this perspective opens new direction for RL training, with possibilities for even larger gains through refined formulations in future work. Limitations and Future Directions. Due to computational constraints, our experiments are limited to models up to 8B parameters. Future work should investigate how RL-ZVP scales to larger models (e.g., 14B or 32B). Furthermore, we only validate RL-ZVP on verifiable tasks with binary rewards; extending it to settings with graded or ambiguous feedback (e.g., open-ended QA, text summarization, safety alignment) remains an open challenge. Finally, RL-ZVP is sensitive to the fixed scaling hyperparameter α, and more sophisticated strategies such as adaptive scheduling may provide more robust alternatives."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We thank Brano Kveton, Nikos Vlassis, Yoonjeon Kim, Doohyuk Jang, Gyouk Chu, Minh-Hai Nguyen, and Tri Le for their valuable feedback and discussions. We are also grateful to Trung Bui for supporting compute allocation. This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) [RS-2024-00337693] (Myeongho Jeon)."
        },
        {
            "title": "REFERENCES",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning, 2025. URL https://arxiv.org/abs/ 2505.15134. Anthropic. Claude 3.7 sonnet, 2025. URL https://www.anthropic.com/news/ claude-3-7-sonnet. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. URL https://arxiv.org/pdf/2506.14758. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. URL https://arxiv. org/pdf/2505.22617. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, and Ji-Rong Wen. Decomposing the entropyperformance exchange: The missing keys to unlocking effective reinforcement learning, 2025. URL https://arxiv.org/abs/2508.02260."
        },
        {
            "title": "Preprint",
            "content": "Rati Devidze, Parameswaran Kamalaruban, and Adish Singla. learning under sparse rewards. Exploration-guided reward shaping for reinforcement In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 58295842. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/266c0f191b04cbbbe529016d0edc847e-Paper-Conference.pdf. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling?, 2025. URL https://arxiv.org/abs/2410.23771. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. The impact of reasoning step length on large language models, 2024. URL https://arxiv.org/abs/2401.04925. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. URL https://arxiv.org/ abs/2207.05221. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205. 11916. Shiye Lei, Zhihao Cheng, Kai Jia, and Dacheng Tao. Revisiting llm reasoning via information bottleneck, 2025. URL https://arxiv.org/abs/2507.18391. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025a. URL https: //arxiv.org/abs/2503.20783. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Siran Yang, Jiamang Wang, Wenbo Su, and Bo Zheng. Part i: Tricks or traps? deep dive into rl for llm reasoning, 2025b. URL https://arxiv. org/abs/2508.08221. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393."
        },
        {
            "title": "Preprint",
            "content": "Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML 99, pp. 278287, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1558606122. Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron Courville. Asynchronous rlhf: Faster and more efficient off-policy rl for language models, 2025. URL https://arxiv.org/abs/2410.18252. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Yun Qu, Qi Wang, Yixiu Mao, Vincent Tao Hu, Bjorn Ommer, and Xiangyang Ji. Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models?, 2025. URL https://arxiv.org/abs/2507.04632. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347."
        },
        {
            "title": "Preprint",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling, 2024. URL https://arxiv.org/abs/2401.06080. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. URL https://arxiv.org/pdf/2506.01939. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025a. URL https://arxiv.org/abs/2505.09388. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025b. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025."
        },
        {
            "title": "Preprint",
            "content": "Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization, 2025. URL https: //arxiv.org/abs/2504.05812. Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts, 2025. URL https://arxiv.org/abs/2506.02177. Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, and Xin Jin. Optimizing rlhf training for large language models with stage fusion, 2025. URL https://arxiv.org/abs/2409.13221."
        },
        {
            "title": "A ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "To further validate the effectiveness of RL-ZVP on an alternative training dataset, we conduct experiments using random 20k-prompt subset of NuminaMath-CoT (LI et al., 2024; Zhang et al., 2025), comparing RL-ZVP with GRPO under the same setup as our main experiments. The detailed evaluation results are shown in Table 4, and the corresponding average validation accuracy and training dynamics are provided in Figure 5. RL-ZVP consistently outperforms GRPO in all benchmarks, surpassing it throughout the training process. Moreover, RL-ZVP exhibits distinct training dynamics, characterized by lower entropy and longer response lengths. These findings confirm that RL-ZVP is effective not only across evaluation benchmarks but also across different training datasets, further strengthening LLM reasoning capabilities. Table 4: Additional evaluation results on six math reasoning benchmarks when the model is trained with NuminaMath-CoT. Method Minerva AMC MATH500 AIME24 AIME25 OlympiadBench Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Acc@8 Pass@8 Qwen3-1.7B-Base trained with NuminaMath-CoT subset GRPO 30.01 44.88 43.75 72.20 67. 83.12 6.67 RL-ZVP (w.r.t GRPO) 30.56 +0.55 45.96 +1.08 44.69 +0. 70.81 -1.39 70.80 +3.17 86.78 +3.66 8.33 +1.66 18.59 18.32 -0. 4.58 13.64 30.45 47.68 5.42 +0.84 13.94 +0. 32.78 +2.33 51.95 +4.27 Qwen3-8B-Base trained with NuminaMath-CoT subset GRPO 42.42 54. 66.88 90.10 83.63 93.32 25.00 45. 20.83 38.49 54.01 68.29 RL-ZVP (w.r.t GRPO) 44.72 +2. 56.15 +1.74 73.13 +6.25 93.91 +3.81 87.23 +3.60 95.20 +1.88 28.75 +3. 51.54 +6.14 22.92 +2.09 40.61 +2.12 55.21 +1.20 69.67 +1.38 (a) Acc@8 (Qwen3-1.7B-Base) (b) Entropy (Qwen3-1.7B-Base) (c) Response Length (Qwen3-1.7B-Base) (d) Acc@8 (Qwen3-8B-Base) (e) Entropy (Qwen3-8B-Base) (f) Response Length (Qwen3-8B-Base) Figure 5: Validation accuracy and training dynamics at different experiment scales."
        },
        {
            "title": "B SUPPLEMENTARY FIGURES AND TABLES",
            "content": "(a) Accuracy (b) Pass rate Figure 6: Average accuracy (a) and pass rate (b) on six math reasoning benchmarks across model scale and training methods. RL-ZVP consistently delivers the strongest performance among all baselines. Table 5: Number of rollouts and gradient steps of RL-ZVP compared to the baselines. For GRPO-DS-r and GRESO-r on the large scale, the policy initially solves only 2% of problems per training batch. This leads to extremely poor data efficiency, requiring over-sampling of 50 to form full non-zero-variance batch for each gradient step. As result, these methods achieve far fewer gradient updates than GRPO and RL-ZVP, despite using nearly 1.5 more rollouts. This highlights the severe inefficiency of dynamic sampling on challenging training data. Method GRPO GRPO-DS-r GRPO-DS-g GRESO-r GRESO-g RL-ZVP #Rollouts 172,032 172,032 421,888 173,600 272,030 172,032 Small Scale Rollouts (GRPO) 1.00 1.00 2.45 1.01 1.58 1.00 Grad Steps #Rollouts Large Scale Rollouts (GRPO) Grad Steps 672 320 672 320 672 672 540,672 827,392 2,859,008 815,232 2,159,104 540,672 1.00 1.53 5.29 1.51 3.99 1.00 1,056 32 1,056 32 1,056 1,"
        },
        {
            "title": "C FULL IMPLEMENTATION DETAILS",
            "content": "We present the full experimental setup details here. RL-ZVP and all baselines are implemented on top of the verl framework, ensuring that every experiment is conducted within the same pipeline. Training is performed on 4RTX 3090 GPUs for Qwen3-1.7B-Base and 8A100 GPUs for Qwen38B-Base. For evaluation, we initially attempted to measure validation results after every training iteration (16 gradient steps), but this proved extremely inefficient given the size of the validation set (1534 problems with 8 responses each). Instead, on the small scale, we evaluate the model performance on Minerva, AMC23, and MATH after every iteration and report the best result, while for AIME24, AIME25, and OlympiadBench we use the final checkpoint. On the large scale, thanks to greater training efficiency on more powerful GPUs, we evaluate Minerva, AMC23, MATH, AIME24, and AIME25 after every iteration and report the best results, while for OlympiadBench we use only the final checkpoint. The detailed hyperparameter settings are provided in Table 6. We mostly follow the standard hyperparameter settings from recent works (Yu et al., 2025; Cheng et al., 2025; Zheng et al., 2025). We use the clip-higher scheme with ϵhigh = 0.28 and ϵlow = 0.2, apply token-level loss, and exclude the KL divergence term from the objective. In addition to the common settings shared across methods, we also adopt the unique hyperparameter configurations of each baseline. For GRESO (Zheng et al., 2025), we follow their probabilistic filtering setup exactly: αeasy = 1 6 , peasy = 0.5, phard = 0.5, = 0.01, and β = 1.25. For GRPO-DS, the only deviation from DAPO (Yu et al., 2025) is that we omit the soft overlong punishment to ensure fair comparison with other methods, which also do not include this form of length-based reward shaping. 12 , αhard = 1 Table 6: Full hyperparameter setting. Hyperparameters Qwen3-1.7B-Base + MATH Qwen3-8B-Base + DAPO-Math-17k Training batch size Mini batch size Samples per prompt Max response length Clip ratio Training temperature Training top Validation temperature Validation top Number of epochs Total gradient steps Optimizer Learning rate LR warmup steps LR scheduler 512 32 8 4096 (0.20, 0.28) 1.0 1.0 1.0 0.7 3 672 AdamW 3 106 4 cosine 512 32 16 8192 (0.20, 0.28) 1.0 1.0 1.0 0. 2 1056 AdamW 1 106 10 constant"
        },
        {
            "title": "D QUALITIVATE EXAMPLES",
            "content": "This appendix demonstrates some output examples generated by policy models trained with GRPO and RL-ZVP. Minerva: Question and Ground Truth Question: particular star has an absolute magnitude = 7. If this star is observed in galaxy that is at distance of 3Mpc, what will its apparent magnitude be? Ground Truth: 20.39 Minerva: Response from GRPO (Qwen3-1.7B-Base) To determine the apparent magnitude of star with an absolute magnitude of = 7 that is observed at distance of 3Mpc, we can use the distance modulus formula: = 5 log10(d) + 5 where: - is the apparent magnitude, - is the absolute magnitude, - is the distance to the star in parsecs. Given: - = 7 - = 3Mpc First, we need to convert the distance from Mpc to parsecs. Since 1Mpc = 3.2616 1018 parsecs, we have: = 3 3.2616 1018 parsecs Now, we can plug the values into the distance modulus formula: (7) = 5 log10(3 3.2616 1018) Simplify the expression inside the logarithm: 3 3.2616 1018 = 9.7848 1018 Now, calculate the logarithm: log10(9.7848 1018) = log10(9.7848) + log10(1018) 0.9908 + 18 = 18.9908 Next, multiply by 5: Now, add 5 to the result: 5 18.9908 = 94.954 + 7 = 94.954 = = 94.954 7 = 87.954 So, the apparent magnitude of the star is approximately 87.954. Therefore, the final answer is: 87."
        },
        {
            "title": "Preprint",
            "content": "Minerva: Response from RL-ZVP (Qwen3-1.7B-Base) Alright, lets solve the problem step by step. **Problem Statement:** We have star with an absolute magnitude = 7. This star is observed in galaxy that is at distance of 3Mpc. We need to find its apparent magnitude. **Understanding the Concepts:** 1. **Absolute Magnitude (M ):** This is the brightness of star if it were placed at standard distance of 10 parsecs (pc) from Earth. It gives us measure of the stars intrinsic brightness. 2. **Apparent Magnitude (m):** This is the brightness of star as seen from Earth. It depends on both the stars intrinsic brightness (absolute magnitude) and the distance to the star. 3. **Distance Modulus Formula:** The relationship between absolute magnitude (M ), apparent magnitude (m), and distance (d) in parsecs is given by: where is the distance to the star in parsecs. = 5 log10(d) **Given:** - Absolute magnitude = 7 - Distance = 3Mpc **Step 1: Convert the Distance to Parsecs** First, we need to convert the distance from megaparsecs (Mpc) to parsecs (pc) because the distance modulus formula uses parsecs. 1Mpc = 106 pc So, = 3Mpc = 3 106pc **Step 2: Plug the Values into the Distance Modulus Formula** Using the formula: = 5 log10(d) 5 Substitute = 7 and = 3 106: (7) = 5 log10(3 106) 5 + 7 = 5 log10(3) + 5 log10(106) 5 **Step 3: Simplify the Logarithms** (. . . ) **Step 5: Substitute and Solve for m** Now, substitute log10(3) into the equation: + 7 = 5 0.4771 + 25 + 7 = 2.3855 + + 7 = 27.3855 = 27.3855 7 = 20.3855 **Final Answer:** The apparent magnitude of the star is approximately 20.39 ."
        },
        {
            "title": "Preprint",
            "content": "AIME24: Question and Ground Truth Question: Let be the set of rectangular boxes with surface area 54 and volume 23. Let be the radius of the smallest sphere that can contain each of the rectangular boxes that are elements of B. The value of r2 can be written as , where and are relatively prime positive integers. Find + q. Ground Truth: 721 AIME24: Response from GRPO (Qwen3-1.7B-Base) To solve the problem, we need to find the radius of the smallest sphere that can contain each of the rectangular boxes with surface area 54 and volume 23. The radius of the smallest sphere that can contain rectangular box is the maximum distance from the center of the box to any of its vertices. Lets denote the dimensions of the rectangular box by a, b, and c. The volume of the box is given by: (. . . ) We need to find the roots of this polynomial. By trial and error or using numerical methods, we can find that one possible set of roots is = 1, = 1, and = 23. Lets check if these values satisfy the equations: abc = 1 1 23 = 23 ab + bc + ca = 1 1 + 1 23 + 23 1 = 1 + 23 + 23 = 47 = 27 This set of values does not work. Lets try another set of values. Suppose = 1, = 1, and = 23 is not the solution, lets try another approach. We can use the fact that the box is symmetric and assume = b. Lets assume = and solve for c: From the first equation: Substitute into the second equation: a2c = 23 2a2 + 2ac = 54 = 23 a2 Multiply through by a: 2a2 + 2a (cid:19) (cid:18) 23 a2 = 54 2a2 + 46 = 2a3 + 46 = 54a 2a3 54a + 46 = 0 By trial and error, we find that = 2 is root: (. . . ) The answer is 673 ."
        },
        {
            "title": "Preprint",
            "content": "AIME24: Response from RL-ZVP (Qwen3-1.7B-Base) Alright, have this problem about rectangular boxes and spheres. Lets break it down step by step. **Problem Statement:** We have set of rectangular boxes with: 1. Surface area = 54 2. Volume = 23 Let be the set of all such boxes. We need to find the radius of the smallest sphere that can contain each of these boxes. Finally, we need to express r2 as fraction where and are coprime positive integers, and then find + q. **Understanding the Problem:** First, lets understand whats given: - rectangular box has dimensions a, b, and c. - The surface area of rectangular box is given by: = 2(ab + bc + ca) = 54 - The volume of the box is: = abc = 23 - We need to find the smallest sphere that can contain any box in B. The radius of this sphere will be the maximum distance from the center of the sphere to any point on the box. So, is half the length of the space diagonal of the box. **Finding the Space Diagonal:** The space diagonal of rectangular box is given by: (cid:112) = a2 + b2 + c2 (. . . ) We need to maximize a2 + b2 + c2. Lets see if we can express a2 + b2 + c2 in terms of ab + bc + ca and abc. Recall the identity: (a + + c)2 = a2 + b2 + c2 + 2(ab + bc + ca) But we dont know + + directly. Alternatively, we can consider using the method of Lagrange multipliers or express two variables in terms of the third, but that might get complicated. Wait, perhaps we can assume symmetry to simplify the problem. Lets assume that two of the dimensions are equal, and the third is different. Lets set = b, and express in terms of a. (. . . ) We need to find r2: r2 = (cid:33)2 (cid:32) 3 73 8 = 9 73 = 657 64 Here, = 657 and = 64, which are coprime. **Final Answer:** 721 + = 657 + 64 ="
        },
        {
            "title": "Preprint",
            "content": "AIME25: Question and Ground Truth Question: Let A1A2 . . . A11 be an 11-sided non-convex simple polygon with the following properties: * The area of AiA1Ai+1 is 1 for each 2 10, * cos(AiA1Ai+1) = 12 13 for each 2 10, * The perimeter of A1A2 . . . A11 is 20. If A1A2 + A1A11 can be expressed as for positive integers m, n, p, with squarefree and no prime divides all of m, p, q, find + + + q. np Ground Truth: 19 AIME25: Response from GRPO (Qwen3-8B-Base) ### Step-by-Step Solution: 1. **Understanding the Given Information:** - We have an 11-sided non-convex simple polygon A1A2 . . . A11. - The area of triangle AiA1Ai+1 is 1 for each 2 10. - The cosine of angle AiA1Ai+1 is 12 13 for each 2 10. - The perimeter of the polygon is 20. - We need to find A1A2 + A1A11 expressed in the form np and compute + + + q. 2. **Analyzing the Triangles:** - For each triangle AiA1Ai+1, we know: - Area = 1 - cos(θi) = 12 13 , where θi = AiA1Ai+1. (. . . ) - However, this approach seems too complex. Lets try different method. 8. **Using Trigonometry and Perimeter:** - Lets denote = A1A2 and = A1A11. - The perimeter equation: + 10 (cid:88) i= A1Ai + + A11A1 = 20 (. . . ) - This seems too complicated. Lets try different approach. 10. **Final Calculation:** - Lets assume A1Ai = for = 2, 3, . . . , 10. (. . . ) - This approach is too complex. Lets try different method. 11. **Simpler Approach:** - Lets denote = A1A2 + A1A11. (. . . ) - Therefore: + + + = 80 + 26 + 260 + 19 = 385 ### Final Answer:"
        },
        {
            "title": "Preprint",
            "content": "AIME25: Response from RL-ZVP (Qwen3-8B-Base) Alright, have this problem about an 11-sided non-convex simple polygon with some specific properties. need to find the value of A1A2 + A1A11 expressed in certain form and then compute + + + q. Lets break it down step by step. **Given:** 1. A1A2 . . . A11 is an 11-sided non-convex simple polygon. 2. The area of triangle AiA1Ai+1 is 1 for each 2 10. 3. cos(AiA1Ai+1) = 12 4. The perimeter of the polygon is 20. 13 for each 2 10. **Goal:** Find A1A2 + A1A11 expressed as Alright, lets tackle this step by step. np and then compute + + + q. ### Step 1: Understanding the Triangle Areas and Angles First, we have triangles AiA1Ai+1 for = 2 to 10. The area of each of these triangles is 1, and we know the cosine of the angle at A1 in each triangle is 12 13 . (. . . ) But from the triangles we have relationships between xi and yi. Lets see how xi and yi relate to the sides of the polygon. Wait, perhaps its better to think in terms of vectors or use the Law of Cosines. Alternatively, perhaps we can express everything in terms of A1A2 and A1A11. ### Step 3: Expressing the Polygon in Terms of Vectors (. . . ) Wait, perhaps we can find xi + yi or another relationship. Alternatively, perhaps we can sum up all the triangles. Wait, lets think differently. ### Step 4: Summing the Areas (. . . ) But how do these sides relate to and y? Wait, perhaps we can express each side AiAi+1 in terms of and y. Lets see. (. . . ) Wait, perhaps we can express and in terms of and y. Alternatively, perhaps we can find + directly. Lets think differently. ### Step 8: Finding + (. . . ) Thus, **Final Answer:** 19 + + + = 9 + 5 + 1 + 4 ="
        },
        {
            "title": "Preprint",
            "content": "MATH500: Question and Ground Truth Question: If ω1997 = 1 and ω = 1, then evaluate 1 1 + ω + 1 1 + ω2 + + 1 1 + ω1997 . Ground Truth: 1997 2 MATH500: Response from GRPO (Qwen3-8B-Base) ### Step-by-Step Solution: 1. **Understanding the Problem:** - We are given that ω1997 = 1 and ω = 1. This means ω is primitive 1997th root of unity. - We need to evaluate the sum: = 1997 (cid:88) k=1 1 1 + ωk 2. **Simplify the Sum:** - Notice that ω1997 = 1 implies ωk for = 1, 2, . . . , 1996 are all distinct roots of unity, and ω1997 = 1 repeats the root 1. - However, since ω = 1, ω1997 = 1 is already included in the sum. 3. **Pairing Terms:** - Consider the sum: 1997 (cid:88) = k=1 - Notice that ω1997k = ωk (since ω1997 = 1). 1 - The term 1 1+ω1997k : 1+ωk can be paired with 1 1 + ωk 1 1 + ωk + 1 1 + ω1997k = 1 1 + ωk + 1 1 + ωk - Simplify the pair: 1 1 1 + ωk = 1 + ωk + - Therefore, each pair sums to 1. (1 + ωk) + (1 + ωk) (1 + ωk)(1 + ωk) = 2 + ωk + ωk 1 + ωk + ωk + 1 = 2 + ωk + ωk 2 + ωk + ωk = 1 4. **Counting the Pairs:** - Since there are 1997 terms in the sum, and each pair consists of 2 terms, there are 19971 2 = 998 complete pairs. - The remaining term is when = 999, but since 1997 is odd, there is no unpaired term. 5. **Calculating the Total Sum:** - The sum of 998 pairs, each summing to 1, is: = 998 1 = 998 ### Final Answer:"
        },
        {
            "title": "Preprint",
            "content": "MATH500: Response from RL-ZVP (Qwen3-8B-Base) Alright, have this problem to solve: **Problem:** If ω1997 = 1 and ω = 1, then evaluate 1 1 + ω + 1 + ω2 + + 1 1 + ω1997 . Alright, lets break this down. ### Understanding the Problem (. . . ) So, the total sum is: = 998 1 + 1 2 = 998 + 1 2 = 1996 + 1 2 = 1997 2 Wait, thats 1997 2 . But lets double-check this. ### Double-Checking the Calculation (. . . ) Lets multiply by (1 ω): (1 ω)S = 1996 (cid:88) k=1 1 ω 1 + ωk This seems complicated. Maybe theres better approach. ### Using the Identity for the Sum recall that for any primitive n-th root of unity ω, the sum (cid:80)n1 k=1 1 1+ωk = n1 2 . (. . . ) Final Answer: 1997 2 But since the problem likely expects an integer, perhaps missed something. Wait, lets think differently. ### Final Verification Lets consider the sum: = 1997 (cid:88) k= 1 1 + ωk (. . . ) Therefore, the correct answer is indeed 1997 2 . Final Answer: 1997"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "EPFL",
        "KAIST"
    ]
}