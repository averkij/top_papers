{
    "paper_title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
    "authors": [
        "Sumeet Ramesh Motwani",
        "Chandler Smith",
        "Rocktim Jyoti Das",
        "Markian Rybchuk",
        "Philip H. S. Torr",
        "Ivan Laptev",
        "Fabio Pizzati",
        "Ronald Clark",
        "Christian Schroeder de Witt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step toward \"Multi-agent LLM training\" (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model's specialized capabilities as part of a joint sequential system. We evaluate our approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches."
        },
        {
            "title": "Start",
            "content": "MALT: Improving Reasoning with Multi-Agent LLM Training Sumeet Ramesh Motwani 1 Chandler Smith 2 Rocktim Jyoti Das 3 Markian Rybchuk 4 Philip H. S. Torr 1 Ivan Laptev 3 Fabio Pizzati 3 Ronald Clark 1 Christian Schroeder de Witt 1 4 2 0 2 2 ] . [ 1 8 2 9 1 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Enabling effective collaboration among LLMs is crucial step toward developing autonomous systems capable of solving complex problems. Although LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present first step towards Multiagent LLM training (MALT) on reasoning problems. Our approach employs sequential multiagent setup with heterogeneous LLMs assigned specialized roles: generator, verifier, and refinement model iteratively solving problems. We propose trajectory-expansion-based synthetic data generation process and credit assignment strategy driven by joint outcome-based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each models specialized capabilities as part of joint sequential system. We evaluate our approach on MATH, GSM8k, and CSQA, where MALT using Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides concrete direction for research around multi-agent LLM training approaches. Preliminary work. 1University of Oxford 2Cooperative AI Foundation 3MBZUAI 4UC Berkeley. Correspondence to: Christian Schroeder de Witt <cs@robots.ox.ac.uk>, Sumeet Ramesh Motwani <sumeet.motwani@eng.ox.ac.uk>. Copyright 2024 by the author(s). 1 Figure 1. Overview of MALT. During joint-training we use base LLM (eg. Llama) to generate several reasoning trajectories. These trajectories are used to create 3 specialized agents, generator, verifier and refiner through SFT and DPO training. During inference, the 3 agents work together to answer given question. 1. Introduction In tasks requiring complex reasoning, such as mathematical problem solving, research assistance, coding, or creative writing, decomposing the system into multiple cooperating agents focusing on different aspects of the problem has been shown to improve performance (Li et al., 2023; Wang et al., 2024b). This mirrors human collaboration, where specialized roles lead to better solutions (Minsky, 1988). For instance, in software development, programmer may propose an initial solution by providing some code, which is then analyzed by tester for bugs or edge cases. Finally, supervisor integrates this feedback to improve the original codes quality. LLM agents can interact in similar way (Qian et al., 2024). Recent advancements in Large Language Models (LLMs) have demonstrated their potential to collaborate effectively in multi-agent systems across wide range of scenarios, ranging from virtual settings such as software development and interactions between personal assistants to market trading and autonomous driving (Li et al., 2023; Chan et al., 2024; Liang et al., 2024; Wu et al., 2024). However, these collaborative systems are composed of individually-trained agents that have not been jointly optimized for cooperative tasks. To the best of our knowledge, we are the first to address this critical gap with Multi-Agent LLM Training (MALT). Despite the growing interest in multi-agent LLM setups, however, most approaches currently rely on agent definition via zero-shot prompting pretrained LLM. While this approach is fast and effective, recent advances in chainof-thought training (OpenAI, 2024) suggest that the full potential of multi-agent LLM systems can only be realized through the joint post-training of individual agents. Therefore, in this paper we ask: can we train LLMs to act as specialized agents to improve task performance? Addressing this question is challenging due to several factors: the difficulty of backpropagating gradient signals through multiple LLMs that produce discrete outputs, the lack of role-specific training data for general reasoning tasks, and credit assignment based on only final binary rewards on outputs (Sutton & Barto, 2018). In this paper, we bridge these gaps by proposing strategy involving three models working sequentially on problems: generator, verifier, and refinement model. MALT employs sampling strategy that expands search tree based on the outputs of each model with an exponential branching factor, producing large amounts of useful synthetic data. Role-specific data, particularly when augmented with rationales, has been shown in previous works to significantly improve performance. However, this opens up credit assignment problem where internal branches may be correct or incorrect and need to be labelled to update models based on just final output signals. To address this, we propose value iteration based attribution strategy (Sutton & Barto, 2018). By analyzing only the outputs of the search tree, our method determines which model introduced an error, enabling credit assignment without requiring human-labeled data or separate value function to evaluate trajectories. This eliminates the need for human intervention in selecting trajectories, generating role-specific data, or designing value functions, instead enabling the automatic generation of reasoning traces from the search tree for post-training via supervised fine-tuning and preference optimization. MALT combines all these steps into an intuitive jointtraining procedure, providing performance improvements and acting as an early first step towards integrating search and learning for multi-agent LLM systems. Our contributions are threefold: We propose Multi-Agent LLM Training (MALT), an approach for cooperative training that leverages synthetic data, search, preference optimization, and inference-time compute strategies to jointly post-train generator, verifier, and refinement model on challenging problems. MALT leverages trajectory search tree process and discrete credit assignment strategy based on binary outcome rewards, which can automatically attribute correct and incorrect rationale traces to individual, specialized models within the multi-agent system. These rationales can then be utilized for supervised fine-tuning and preference optimization. We apply MALT to mathematical and common sense reasoning questions from MATH, CSQA, and GSM8k, achieving relative improvements of 14.14%, 9.40%, and 7.12% over single-model baseline, as well as outperforming other comprehensive baselines. The two methods that seem to scale arbitrarily [...] are search and learning. Richard Sutton, The Bitter Lesson 2. Related Work Our work intersects broad spectrum of research areas, including agent design, self-improvement, reasoning, and reinforcement learning. In the following sections, we provide concise overview of related works that have shaped these domains. 2.1. Fine-tuning Methods and Foundation Models Advancements in generative language models have led to the development of powerful foundation models that serve as the basis for various fine-tuning approaches. Supervised Fine-Tuning (SFT) adapts pre-trained models to perform more effectively on specific downstream tasks by updating model parameters using labeled data. In Full Fine-Tuning, all parameters of the pre-trained model are updated during training on the target task, allowing the model to adjust its entire representation to new data (Devlin et al., 2019). While this method can yield high performance, it is computationally intensive and may risk overfitting. To address these limitations, parameter-efficient fine-tuning (PeFT) methods were developed (Houlsby et al., 2019). These Adapter Modules introduce small, trainable layers within each transformer block while keeping the original model parameters fixed, while Low-Rank Adaptation (LoRA) inserts trainable low-rank matrices into the weight matrices of the pre-trained model (Hu et al., 2022). Both approaches significantly reduce the number of trainable parameters, leading to faster training times and lower memory consumption without substantial performance loss. Recently, Direct Preference Optimization (DPO) has emerged as an effective method for fine-tuning language models based on preference data (Rafailov et al., 2023). DPO directly optimizes the model to align with human preferences by fine-tuning on datasets containing comparisons of model outputs. This has significantly improved model alignment and performance across various domains, including reasoning and problem-solving tasks. 2 2.2. Synthetic Data Generation for Model Improvement The generation of synthetic data for fine-tuning has gained traction as strategy for improving the reasoning capabilities of LLMs, reducing the dependence on large humanannotated datasets. In Self-Taught Reasoner (STaR), models iteratively refine their reasoning through self-produced explanations, leading to notable gains (Zelikman et al., 2022). Similarly, it has been shown that LLMs can serve as prompt engineers, generating high-quality synthetic data that, when used for fine-tuning, enhance model capabilities (Singh et al., 2024). However, these methodologies are constrained by their exclusive focus on correct data, limiting the models ability to learn from their own errors. To address this, Setlur et al. (2024) leverage reinforcement learning on synthetic incorrect data, enabling models to learn from mistakes and iteratively enhance performance. Analogous strategies have been explored in the field of computer vision (Liu et al., 2024), where positive and negative examples generated by LLMs improve robustness against adversarial attacks. Further, Pang et al. (2024) propose Iterative Self-Learning, where models are trained on their own erroneous outputs, refining their predictions over successive iterations. Inspired by Setlur et al. (2024), we leverage combination of SFT and DPO to enable better performance on rolespecific tasks. Our our novel sampling algorithm uses combination of fine-tuning techniques to train multi-agent setup that consists of generators, verifiers and refinement models. These advancements underscore the potential of synthetic data generation techniques for LM improvement. Our work builds on these foundations, exploring synthetic data generation in multi-agent systems to improve individual agent performance and overall system efficacy. 2.3. Self-improving Models and Iterative Refinement Self-improvement mechanisms have become crucial aspect of advancing language model capabilities. Agent combines guided search, self-critique, and iterative finetuning to improve generalization in complex reasoning tasks (Putta et al., 2024). Saunders et al. (Saunders et al., 2022) further explored self-critiquing models that assist human evaluators by generating critiques of their own responses. By fostering an internal feedback mechanism, models can better align with human values and expectations, enhancing both safety and effectiveness. Recent work has shown that inference-time compute can be scaled sequentially by teaching model to self-correct mistakes over multiple turns (Qu et al., 2024). These self-improving methodologies underscore the potential of iterative refinement and self-feedback in advancing the capabilities of language models. By enabling models to learn from their successes and failures, they achieve greater autonomy and adaptability in complex reasoning tasks. concurrent work that focuses on multi-agent setting is presented in (Chen et al., 2024), which uses an MCTS-based generation strategy and performs preference optimization similar to (Putta et al., 2024; Zhang et al., 2024) in limited multi-agent debate or information-exchange settings with two agents. In contrast, we present more scalable, intuitive method extending beyond two-agent setting in sequential setup of heterogeneous agents, leveraging data generated via exponential tree based sampling. 2.4. Inference-time Search, Scaling, and Consistency Recent research has shifted focus towards optimizing inference-time computation to increase model performance without increasing model size or training data. Prior work has shown that using more computation at inference time can significantly improve task performance, allowing smaller models to outperform larger ones (Snell et al., 2024; Brown et al., 2024). The Self-Consistency technique (Wang et al., 2022), which enhances reasoning by sampling multiple reasoning paths and aggregating them to select the most consistent answer. Approaches that scale parallel inference-time compute like Best-of-N, majority voting, and revision have all proven valuable (Cobbe et al., 2021). Recent work has shown that inference-time compute can be scaled sequentially by teaching model to self-correct mistakes over multiple turns (Qu et al., 2024). These strategies highlight the efficacy of leveraging increasing computational resources at inference time, and lay the foundation for the feasible implementation of our novel algorithm. The convergence of fine-tuning methodologies, synthetic iterative self-improvement, enhanced data generation, inference-time computation, and multi-agent systems underscores significant advancements in language model capabilities. By building upon these foundational works, our approach aims to push the boundaries of collaborative reasoning and problem-solving in language models, contributing to the development of more sophisticated AI systems. 3. Preliminaries We consider reasoning tasks to be represented by problem description and an associated solution. For example, problem description may contain one or several natural language statements involving an unknown numerical variable ξ that is to be reasoned over, and unique correct solution for ξ. Given θ-parameterized policy πθ, the task is then to learn parameters θ given training dataset Dtrain = {(x1, y1), (x2, y2), . . . , (xN , yN )} , N, and test dataset Dtest under the generalization objective 3 θ = arg min (cid:88) θ (x,y)Dtest 1πθ(x)=y (1) Note that in the case of autoregressive large language models, the model parameters θ have commonly been pre-trained on next-token prediction tasks over large corpora Dpre-train of natural language documents minimizing the loss L(πθ) = E(x,y)Dpre-train (cid:88) t=1 log πθ(yty<t, x), (2) where the output is sampled as πθ(x) given some temperature τ . Supervised Finetuning (SFT) As proxy for optimising Equation 1, our training set of positive examples Dtrain can be used in SFT by minimizing the loss L(πθ) = E(x,y)Dtrain (cid:88) t= log πθ(yty<t, x). (3) Direct Preference Optimisation (DPO) In SFT, we can exclusively use positive data, such as the data from our training set Dtrain, in order to improve our test set performance. By using preference optimisation, we can also leverage negative examples y+, where means that y+ is preferred over y. While RLHF (Christiano et al., 2017) used to be the preferred preference optimisation strategy, it was recently shown that reinforcement learning can be effectively replaced by modified supervised learning objective (Rafailov et al., 2024, DPO): LDPO(πθ) = E(x,y+,y)Dtrain σ (cid:16) β log πθ(y+x) πref(y+x) β log πθ(yx) πref(yx) (cid:17) separate heterogeneous LLM agents offers various benefits analogously to those observed in decentralized multi-agent learning (Schroeder de Witt et al., 2020): first, decentralization factorizes large joint action spaces, which can be beneficial even if implicit. Secondly, it allows different agents to maintain different contexts, including differing role-specific instruction sets: if simulated by single centralized agent, context leaks between agents would be more likely to occur. Lastly, in contrast to chain-of-thought reasoning by single centralized agent, decentralized instances of cooperative LLM agents allow for parallelization of both computation and in-context memory, both of which can increase inference performance beyond chain-of-thought reasoning. In this section, we present our methodology for multi-agent setting consisting of sequential heterogeneous process where agents can be trained based on joint rewards. We present the complete algorithm described in this section as Algorithm ??. 4.1. Multi-Agent Inference Setting We formulate our multi-agent inference setting as collaborative reasoning framework designed to solve complex tasks. Let denote dataset of natural language questions, where each represents specific task instance. The objective is to generate prediction for given input q, where is the set of all possible answers. We assume the existence of ground truth function : A, where (q) = aGT serves as the ideal prediction for evaluating a. Our framework consists of three specialized Large Language Models (LLMs) acting as distinct agents, each defined as function: (4) 1. Generator (G : PG OG): Produces an initial response to the question. We assume πref is reference policy, typically obtained through SFT. Dtrain is dataset of triplets including both queries and positive/negative outputs. Finally, σ is the sigmoid function and β scaling parameter. 2. Verifier (V : OG PV OV ): Critiques the generated response for potential errors. 3. Refinement Model (R : OG OV PR A): Integrates feedback to improve the final prediction. 4. Method: Multi-Agent LLM Training (MALT) Systems of decentralized LLMs arise naturally in any circumstances in which each LLM agent has differing objectives and/or partial observability. For example, autonomous web agents commonly follow the goals of different principals and condition on different instructions and private contexts. However, even in fully observable and cooperative cases where systems of LLM agents could technically be simulated by single centralized LLM, decomposition into Here, PG, PV , PR are the sets of possible prompts for each model, and OG, OV are the sets of possible outputs for the generator and verifier, respectively. Formally, we define the interaction between these agents as: go = G(q, pg) where go OG, pg PG vo = (q, pv, go) where vo OV , pv PV = R(q, pr, go, vo) where A, pr PR (5) (6) (7) Let ΘG, ΘV , ΘR denote the parameter sets for the generator, verifier, and refinement model, respectively. The multi-agent 4 Figure 2. Method overview. Given an input question, we consider three-agent system composed by Generator for initial answer production, Verifier providing critique, and Refinement Model for integrating everything into final output. For given question q, we sample multiple interactions between the agents, where the multi-agent system produces either final or incorrect answers judged by ground truth reward. Our credit attribution strategy based on this joint-reward then generates individual preference pairs for each model based on their output trajectories to collect synthetic dataset (left). Finally, this synthetic dataset is used for SFT and DPO on individual models (right). system can then be expressed as: = RΘR (qi, go, vo, pr) = RΘR (qi, go, VΘV (qi, go, pv), pr) (8) This multi-agent set up is reminiscent of how LLMs are used in production, where they receive initial prompts containing questions, feedback, and are then asked to refine answers (Wang et al., 2024a). The verifier is specifically designed to identify faulty reasoning that may lead to incorrect answers, while the refinement model integrates this feedback to produce an improved final prediction. Our empirical results demonstrate that this multi-agent inference setting enhances performance compared to singlemodel approaches. The key insight, however, relies on leveraging this multi-agent inference setting for large amounts of synthetic data generation which scales exponentially with respect to sampling branching factor. Below, we discuss our data generation and multi-agent training algorithms. 4.2. Capturing Reasoning Trajectories Standard zeroor few-shot settings typically involve Generator producing an answer for given question q. While base Generator could just be fine-tuned on the original training data to obtain generator Gsft, our multi-agent setting allows further improvement. We define reasoning trace for question as rq = [rg, rv, rr], where rg, rv, and rr represent the reasoning steps (outputs) of the generator, verifier, and refinement model, respectively. An outcome reward model : {0, 1}, based on the ground truth in the training set, evaluates rr to mark the trajectory as either correct (1) or incorrect (0). Specifically, for predicted answer and ground truth aGT, we define: R(a, aGT) = (cid:40) if = aGT, 1, 0, otherwise. (9) To generate diverse reasoning trajectories, we employ the following sampling strategy with branching factor of for all models: 1. For each problem xi Dquery in the training data, we sample completions from the generator: {gi,j G(xi)}n j=1. (10) 2. For each generator output gi,j, we produce verifications: {vi,j,k (gi,j, xi)}n k=1. (11) 3. For each verifier output vi,j,k, we generate refinements: {ri,j,k,l R(gi,j, vi,j,k, xi)}n l=1. (12) This process results in n3 trajectories for each training example, totaling Dquery n3 trajectories. We use the outcome reward model to label the refinement outputs as correct () or incorrect (). To effectively utilize the reward signals from the refinement outputs, we adopt value iteration approach to propagate values backward through the reasoning chain. Specifically, we compute the expected value of each node (generator 5 Algorithm 1 Multi-Agent LLM Training and Synthetic Data Generation (MALT) Require: Initial Dataset D, Models G, , R, Branching factor GOAL: Trained models G, , j=1 {vj,k = (q, gj)}n Agj for each vj,k Agj 1: Initialize datasets SG, SV , SR as empty sets 2: for do 3: AG {gj = G(q)}n for each gj AG do 4: 5: 6: 7: 8: 9: 10: 11: Agj ,vj,k for each rj,k,l Agj ,vj,k Compute (rj,k,l) = R(rj,k,l, aGT) Add (q, gj, vj,k, rj,k,l, (rj,k,l)) to SR {rj,k,l = R(q, gj, vj,k)}n do do k= l=1 end for Compute (vj,k) = Binarize ˆvj,k = I[V (vj,k) > 0.5] Add (q, gj, vj,k, ˆvj,k) to SV (cid:80)n 1 l=1 (rj,k,l) 12: 13: 14: 15: 16: end for Compute (gj) = Binarize ˆgj = I[V (gj) > 0.5] Add (q, gj, ˆgj) to SG (cid:80)n 1 k=1 (vj,k) end for 17: 18: 19: 20: end for 21: Training the Models 22: SFT(G, SG) 23: VSFT SFT (V, {(q, gj, vj,k) (q, gj, vj,k, ˆvj,k) SV , ˆvj,k = 1}) 24: DPO(VSFT, SV ) 25: RSFT SFT (R, {(q, gj, vj,k, rj,k,l) (rj,k,l) = 1}) 26: DPO(RSFT, SR) 27: return G, , Generate outputs from Generate outputs from Generate outputs from Reward for output Value for output Value for output Fine-tune with supervised data Fine-tune on positive samples Train with DPO using preferences Fine-tune on positive samples Train with DPO using preferences and verifier outputs) based on the values of its descendant leaves (refinement outputs). This global pooling mechanism ensures that the values accurately reflect the overall performance of each node in contributing to correct final answers. Value Function Definitions Let (ri,j,k,l) denote the value of refinement output, defined as: (ri,j,k,l) = R(ri,j,k,l, aGT) {0, 1}. (13) The value of verifier output vi,j,k is then computed as the expected value of its associated refinements: (vi,j,k) = El[V (ri,j,k,l)] = 1 (cid:88) l= (ri,j,k,l). (14) Similarly, the value of generator output gi,j is computed as the expected value of its associated verifier outputs: This value iteration process allows us to propagate the correctness information from the leaves (refinement outputs) up to the root nodes (generator outputs), capturing the overall effectiveness of each output in the reasoning trajectories. Thresholding and Binarization To prepare the data for training (SFT and DPO), we binarize the values using threshold of 0.5, aligning with majority voting principles. Nodes with values greater than 0.5 are labeled as correct (), and those with values less than or equal to 0.5 are labeled as incorrect (). The choice and intuition behind this thresholding strategy is discussed in more detail in Appendix A.2, where we demonstrate that it ensures the policys expected value monotonically increases. Formally, for each node n, we define its label ˆn as: ˆn = (cid:40), , if (n) > 0.5, otherwise. (16) (gi,j) = Ek[V (vi,j,k)] = 1 (cid:88) k=1 (vi,j,k). (15) Preference Data Generation We detail how preference data for each model is generated: 6 Refinement Model Training Each refinement output ri,j,k,l has an associated value (ri,j,k,l) {0, 1}. We create preference pairs (r+, r) where r+ is correct refinement (V (r+) = 1) and is an incorrect refinement (V (r) = 0) for the same verifier input vi,j,k. Formally: DR = (cid:40) (cid:12) (cid:12) (r+, r) (cid:12) (cid:12) (cid:12) r+, {ri,j,k,l}n (r+) = 1, (r) = 0 l=1, (cid:41) . (17) Verifier and Generator Model Training For each verifier output vi,j,k, we compute its value (vi,j,k) and binarize it as follows: ˆvi,j,k = (cid:40), , if (vi,j,k) > 0.5, otherwise. (18) Preference pairs for the verifier model are created by comparing outputs under the same generator output gi,j: (cid:40) DV = (v+, v) v+, {vi,j,k}n ˆv+ = , ˆv = k=1, (cid:41) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (19) similar process applies to the generator model, where generator outputs gi,j are binarized based on their values (gi,j), and preference pairs DG are created by comparing outputs under the same query xi. The intuition and approach behind this training strategy, along with the results of our experiments, are detailed in Section 5. 5. Experiments In this section, we detail our experiments. We begin by outlining the experimental details, including description of the benchmarks used for evaluation, and the proposal of several comprehensive baselines for comparison. We then present our main experimental results, where we show the effectiveness of MALT on three standard LLM evaluation benchmarks. 5.1. Experimental Details Models Our experiments are based on the Llama-3.1-8BInstruct model (Grattafiori et al., 2024), chosen for the strong balance it provides between competitive initial baseline performance and size that fits within limited computational budget. The performance of Llama 3.1 8B across benchmarks provides good testbed, allowing us to extract positive and negative samples for each model that we can use for post-training and pushing performance further. Moreover, the baseline benchmark scores enable us to see clear results while remaining sufficiently performant and not saturating benchmarks. Open-source smaller-scale models allow for reproducability and accessibility while providing meaningful insights into multi-agent interactions and post-training. 7 Benchmarks We use three popular benchmarks for our First, we consider the GSM8K benchexperiments. mark (Cobbe et al., 2021), which consists of 7.47k training examples and 1.32k test questions, focused on linguistically diverse grade school math problems. Furthermore, to evaluate MALT on more difficult mathematical reasoning benchmark, we use MATH (Hendrycks et al., 2021), consisting of 7.5k training and 5k test problems. MATH has proven to be consistently difficult, especially for smaller language models, with Llama 3.1 8B performing around 49.50% test-accuracy based on our baselines. Lastly, to extend the scope beyond mathematical tasks, we evaluate on CommonsenseQA (Talmor et al., 2019) with 9.74k training examples and 1.22k dev-set questions. CommonsenseQA is multiple-choice question answering dataset around commonsense reasoning problems and has been used similarly by prior work in this line (Zelikman et al., 2022; Grattafiori et al., 2024; Zelikman et al., 2024; Wei et al., 2023). Baselines To establish comprehensive set of baselines, we evaluate against total of eight baselines in 2 primary settings. First, we employ inference-only baselines that use only pre-trained models with the same prompts we use for our trained setup to provide answers for given question. Within this setting, we propose (1) single-agent (SA) naive setting in which single model is used as generator to provide reply, (2) multi-agent (MA) setting, where the pre-trained baseline model operates in sequential way as generator, verifier, and refinement agent solving problem in multi-agent inference setting. Moreover, in order to provide an fair comparison to MALT, we implement simple majority voting based consistency mechanism as part of our baselines (discussed further in Section 5.3). Our second setting of baselines is with supervised fine-tuning on models in the four previous baselines mentioned. We refer to this as our STaR baseline with the generator and all three models trained on the synthetic rationales collected as part of MALT with next-token prediction loss. All eight of our baselines are based on the Llama 3.1 8B model. future version of our work will discuss detailed ablation studies in addition to our current baselines. Synthetic Data Generation For MALTs synthetic data generation pipeline, we generate the sequential trees for each question only in the training set with the process mentioned in Algorithm ?? or more concisely in Section 4.2. Using = 3 allows us to generate 27 trajectories per question and between approximately 2k to 6k trajectory pairs based on different models and benchmarks. We provide more details on these sizes in the Appendix. MALT samples from Llama 3.1 8B models with temperature of 0.3 to ensure some controlled variability in responses. Each model has its fixed prompt for role conditioning that is also used for baseline performance calculation. The question is Test Accuracy (%) over 4 seeds Benchmark Inference-only STaR Training MALT MALT SA SA+MV MA MA + MV SA SA+MV MA (w/o MV) GSM8K 84.25 2. 86.75 2.38 84.75 2.86 87.00 4.00 81.75 0.83 84.75 2.68 80.00 1. 83.50 2.18 90.25 1.92 CSQA 74.50 3.35 75.75 5.49 77.50 5. 79.00 4.69 76.25 4.32 78.75 4.26 75.50 2.69 77.50 1.12 81.50 2. MATH 49.50 2.06 52.50 2.50 51.75 3.56 53.50 2.87 52.25 1. 54.50 3.50 52.50 3.20 52.25 1.79 56.50 1.50 Table 1. Benchmark results. We compare MALT with baselines on three different benchmarks. For baselines, we include different setups such as single agent (SA) and multi-agent (MA), both with and without majority voting (MV). MALT outperforms all baselines. accessible to all three models, and outputs flow sequentially through the system until final natural language output is generated that is compared with the ground truth answers provided in the dataset. In Appendix A.1, we present an example of collected trajectories part of this dataset. We train models for all three benchmarks separately on the data generated with their individual training sets. Training Configurations We use LoRA adapter based fine-tuning (Hu et al., 2021) which freezes the pre-trained model to inject trainable rank decomposition matrices into each layer, reducing the computational load required for post-training models via both SFT and DPO. We delegate details about our training configuration to Appendix A.3. MALT Procedure MALT involves the following steps: Synthetic data is generated for each benchmark individually, and the credit assignment heuristic based on joint binary rewards (in this case, each benchmarks training set consists of ground truth answers that can be compared against for binary rewards at the leaf nodes of the search process) is used to produce role-specific training data for each model. Training then follows the configurations outlined above, starting with SFT, and then DPO on the SFT policy. In practice, training on long sequences of text can potentially be unstable or produce hallucinations (Park et al., 2024). To address this, MALT employs simple majority voting mechanism during inference with three productions to ensure self-consistency (Wang et al., 2023). Inference is computationally efficient since it does not require tree search process and follows the same sequential pattern as our baselines, which also include majority voting process for comparison  (Table 1)  . Further discussion is provided in Section 5.3. 5.2. Experimental Results Our experimental results along with all baselines over all three datasets are presented in Table 1. Our results are averaged over four runs on random subsets (due to computational constraints) across seeds of the large test-sets 8 provided for each benchmark, and we report the mean and standard deviation for all our results. Baseline single agent scores on MATH, CSQA, and GSM8K are 49.50%, 74.50%, and 84.25%, approximately in line with scores reported in Llama 3.1 8Bs release (Grattafiori et al., 2024). These scores go up to 52.50%, 75.75%, 86.75% and 53.50%, 79.00%, 87.00% with single model majority voting and multi-agent majority voting respectively. MALT in turn achieves an accuracy of 56.25%, 81.5%, and 90.25% on these benchmarks. Overall, MALT significantly outperforms all baselines, including all settings with supervised finetuned models. Over the base models performance as generator, MALT achieves relative improvements of 14.14%, 9.40%, and 7.12% on MATH, CSQA, and GSM8K. This early advance demonstrates the efficacy of our search and attribution based data generation, training, and inference pipeline in MALT on competitive problem-solving and reasoning benchmarks. Note that prompts are based on CoT and are fixed throughout both baselines and trained settings. MALT performs well with majority voting, but its performance without majority voting remains close to that of the multi-agent setup in an inference-only setting. qualitative examination of output trajectories revealed that while MALT was able to solve questions previously unsolvable, it experienced self-consistency issues due to hallucinations during the process. Majority voting, simple self-consistency mechanism for language models (Wang et al., 2022), has proven effective in such situations. For MALT, the performance improvement from majority voting is significantly higher than the improvements observed in other baselines. For instance, on MATH, majority voting with factor of 3 resulted in relative improvement of 8.14%, compared to improvements of 6.06%, 3.38%, and 4.31% in the inferenceonly single-agent and multi-agent settings and STaR baseline, respectively. Importantly, majority voting enhances system performance when the true answer is reachable state for the model, even in cases where hallucination occurs, making it particularly effective with MALT. 5.3. Discussion We have introduced Multi-Agent LLM Training (MALT), which demonstrates improvements in reasoning performance over all our benchmarks and represents an important first step in the training of multi-agent role specific LLM systems over joint reward propagated to individual models in discrete manner. MALT is simple yet intuitive approach that addresses significant gap in LLM post-training and multi-agent inference settings. Multi-agent training remains underexplored due to key challenges, including the lack of role-specific training data, the absence of setups that benefit from multi-agent interactions, and the inherent complexity of training such systems. MALT bridges this gap by leveraging advances in synthetic data generation to improve model performance, guiding scalable strategy for training across multiple models. MALT tackles the problem of attributing synthetic data to individual models based on outcome rewards and enhances them using stable training techniques such as SFT and DPO. core strength of our empirical results lies in how we approach multi-agent problem solving, joint-training critic whose outputs guide subsequent refinements. This setup also enables backtracking, addressing key limitation of single LLMs (Bachmann & Nagarajan, 2024). Synthetic data is crucial not only for improving the generator but also for enhancing the verifier and refinement models, which require targeted training due to limited role-specific data. SFT helps these models develop reasoning structures aligned with their roles, while DPO enables higher-quality verification and refinement strategies, such as recalculating steps to double-check answers. Setlur et al. (Setlur et al., 2024) show that using incorrect synthetic data helps models unlearn spurious correlations, boosting reasoning efficiency and generalization. Building on these insights, we design MALT to further enhance robustness and effectiveness. We now discuss key design choices. Simplicity with theoretical groundingparticularly the assurance of trained policy with monotonically increasing valuewas primary consideration. We opted for DPO over PPO-based methods due to our focus on straightforward offline data generation process, treating our approach as an independent learning problem in centralized setting (Lerer et al., 2019) with single iteration (the key difference being that our agent policies post-training differ). In this setting, DPO is more stable than PPO and requires less overhead. While PPO could use the value computed at each branch as reward for post-training nodes (a promising future direction), it introduces significant computational complexity. Our value iteration method, when binarized, resembles global majority-based pooling: for given node and branch, the binary reward of the leaf nodes in the subtree determines the usefulness of the branch, analogous to binarizing values propagated through the tree. In contrast, local pooling computes the binary value of branch based only on the majority outcomes of its direct children, propagating this process to the leaf nodes. In Appendix A.3, we explain why global pooling is more principled, although local pooling offers an intuitive, computationally efficient heuristic. We also leave the choice between MCTS and an expansive tree-based sampling strategy as an open problem. Given our limited tree depth, tree-based sampling proved efficient, supported synthetic data generation with an exponential branching factor, and produces explainable outputs. Our dataset is collected offline, and individual models are trained on this synthetic data. While this approach works empirically, handling any new, out-of-distribution data would require iterative rollout and post-training methods. 6. Conclusion and Future work Multi-Agent LLM Training (MALT) is an introductory advance towards building novel algorithm combining supervised fine-tuning and preference optimization to train systems of generators, critiques, and refinement models on joint multi-agent trajectories. MALT enables small LLMs to achieve reasoning performance comparable to much larger models, paving the way for post-training optimization of general and collaborative LLM agent systems. With our experiments on three benchmarks, we provided new insights into multi-agent LLM post-training and demonstrated higher performance than baselines. Safety Our approach can be used not just to enhance the reasoning capabilities of LLM systems, but also address crucial open problems in the safety of multi-agent systems. Importantly, MALT-trained systems of trusted small models could attain better task performance while retaining high degrees of trust, producing more powerful overseers within the AI control setting (Greenblatt et al., 2024). Another prominent application of our approach would be to train verifiers as safety critics within multi-agent setup. This could scale up the settings such as OpenAI CriticGPT (McAleese et al., 2024) to any number of models, resulting in more powerful safety critics and potentially allowing for the legibility of solutions to be improved. Future Directions Our findings, while still preliminary, showcase the potential of multi-agent LLM systems optimized with fine-tuning and collaborative inference techniques. There are several future directions from this line of work: Using PPO (Schulman et al., 2017b) and the exact value propogated backward for each trajectory to update model weights might produce strong results with additional computational overhead (Ivison et al., 2024). Moreover, we provide several levers around the number of models 9 (where the three model setup can be used iteratively), controlling the branching factor for data generation, examining the effect of majority voting on more samples, changing the attribution threshold, or treating the attribution threshold as an adaptive parameter when iteratively training and rolling out from the multi-agent system (see Appendix A.3). Moreover, prompt-tuning strategies and different roles can be considered or distillation techniques. We note that these are all specific and interesting directions. However, they lie beyond the scope of this paper, where our goal is to introduce new multi-agent post-training methodology and demonstrate strong empirical performance."
        },
        {
            "title": "Limitations and Ethics Statement",
            "content": "We note that even at low temperatures, model performance on benchmarks often exhibits high variance. To address this within our computational constraints, we conducted evaluations on subsets of test-sets (size 100) across four seeds. While CommonsenseQA is known to contain many biased or incorrectly labelled questions (Geva et al., 2019), we utilized it in manner consistent with prior work. Due to computational limitations, we used OpenPipe for training and have made all our training data and code publicly available. In future iteration of this work, we plan to release an ablation study on models post-trained using MALT."
        },
        {
            "title": "References",
            "content": "Bachmann, G. and Nagarajan, V. The pitfalls of next-token prediction, 2024. URL https://arxiv.org/abs/ 2403.06963. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J., and Liu, Z. Chateval: Towards better llm-based evaluators through multi-agent debate. In ICLR, 2024. Chen, W., Yuan, J., Qian, C., Yang, C., Liu, Z., and Sun, M. Optima: Optimizing effectiveness and efficiency for llm-based multi-agent system, 2024. URL https:// arxiv.org/abs/2410.08115. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/hash/ d5e2c0adad503c91f91df240d0cd4e49-Abstract. html. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. URL https://api.semanticscholar. org/CorpusID:52967399. Geva, M., Goldberg, Y., and Berant, J. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets, 2019. URL https://arxiv.org/abs/1908.07898. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., and Team, L. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Greenblatt, R., Shlegeris, B., Sachan, K., and Roger, F. AI Control: Improving Safety Despite Intentional Subversion, July 2024. URL http://arxiv.org/abs/ 2312.06942. arXiv:2312.06942 [cs]. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 27902799. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/v97/ houlsby19a.html. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. org/abs/2106.09685. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Ivison, H., Wang, Y., Liu, J., Wu, Z., Pyatkin, V., Lambert, N., Smith, N. A., Choi, Y., and Hajishirzi, H. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback, 2024. URL https://arxiv.org/abs/2406.09279. Lerer, A., Hu, H., Foerster, J., and Brown, N. Improving policies via search in cooperative partially observable games, 2019. URL https://arxiv.org/abs/ 1912.02318. Li, G., Hammoud, H., Itani, H., Khizbullin, D., and Ghanem, B. Camel: Communicative agents for mind exploration of large language model society. NeurIPS, 2023. Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S. Encouraging divergent thinking in large language models through multi-agent debate. In EMNLP, 2024. Liu, R., Khakzar, A., Gu, J., Chen, Q., Torr, P., and Pizzati, F. Latent guard: safety framework for text-to-image generation. In ECCV, 2024. McAleese, N., Pokorny, R. M., Uribe, J. F. C., Nitishinskaya, E., Trebacz, M., and Leike, J. LLM Critics Help Catch LLM Bugs, June 2024. URL http://arxiv.org/ abs/2407.00215. arXiv:2407.00215 [cs]. Minsky, M. Society of mind. Simon and Schuster, 1988. OpenAI. 2024. learning-to-reason-with-llms/. Learning to Reason with LLMs, July URL https://openai.com/index/ Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S., and Weston, J. Iterative reasoning preference optimization, 2024. Park, R., Rafailov, R., Ermon, S., and Finn, C. Disentangling length from quality in direct preference optimization, 2024. URL https://arxiv.org/abs/2403. 19159. Putta, P., Mills, E., Garg, N., Motwani, S., Finn, C., Garg, D., and Rafailov, R. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. URL https: //arxiv.org/abs/2408.07199. Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., Xu, J., Li, D., Liu, Z., and Sun, M. ChatDev: Communicative Agents for Software Development. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1517415186, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.810. URL https:// aclanthology.org/2024.acl-long.810. Qu, Y., Zhang, T., Garg, N., and Kumar, A. Recursive introspection: Teaching language model agents how to selfimprove, 2024. URL https://arxiv.org/abs/ 2407.18219. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=HPuSIXJaa9. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv.org/abs/ 2206.05802. Schroeder de Witt, C., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H. S., Sun, M., and Whiteson, S. Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?, November 2020. URL http://arxiv.org/abs/2011.09533. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. Trust region policy optimization, 2017a. URL https://arxiv.org/abs/1502.05477. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017b. URL https://arxiv.org/abs/1707. 06347. Setlur, A., Garg, S., Geng, X., Garg, N., Smith, V., and Kumar, A. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold, 2024. URL https://arxiv.org/abs/2406.14532. Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Garcia, X., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., Kumar, A., Alemi, A., Rizkowsky, A., Nova, A., Adlam, B., Bohnet, B., Elsayed, G., Sedghi, H., Mordatch, I., Simpson, I., Gur, I., Snoek, J., Pennington, J., Hron, J., Kenealy, K., Swersky, K., Mahajan, K., Culp, L., Xiao, L., Bileschi, M. L., Constant, N., Novak, R., Liu, R., Warkentin, T., Qian, Y., Bansal, Y., Dyer, E., Neyshabur, B., Sohl-Dickstein, J., and Fiedel, N. Beyond human data: Scaling self-training for problem-solving with language models, 2024. URL https://arxiv.org/abs/2312.06585. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. Sutton, R. S. and Barto, A. G. Reinforcement learning: an introduction, 2nd edn. adaptive computation and machine learning, 2018. 11 Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: question answering challenge targeting commonsense knowledge, 2019. URL https: //arxiv.org/abs/1811.00937. Wang, J., Ma, W., Sun, P., Zhang, M., and Nie, J.-Y. Understanding user experience in large language model interactions, 2024a. URL https://arxiv.org/abs/ 2401.08329. Wang, Q., Wang, Z., Su, Y., Tong, H., and Song, Y. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? In ACL, 2024b. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models, 2022. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203. 11171. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. In COLM, 2024. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning, 2022. Zelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. D. Quiet-star: Language models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/2403.09629. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., and Tang, J. Rest-mcts*: Llm self-training via process reward guided tree search, 2024. URL https://arxiv. org/abs/2406.03816. 12 A. Appendix A.1. Qualitative Examples In this section, we present three key qualitative examples: synthetic training example for SFT and DPO, qualitative performance comparison between an untrained and trained verifier, and complete MALT setup demonstrating backtracking and improvement reasoning cycle that generates correct answer. GSM8K Synthetic Dataset Sample for the Refinement Model (GT Answer = 25) System prompt You are an expert problem solver that refines math solutions based on feedback. Input Question: Marco loves trading cards. He has total of 500 cards on hand, and fourth of them are duplicates. Marco decides to trade one-fifth of these duplicates for completely new cards to his friend Josh. How many new cards did Marco get? Initial Answer: Id be happy to help you solve this problem step by step. Step 1: Identify the key information given in the question * Marco has total of 500 cards. * fourth of them are duplicates. Step 2: Determine what the question is asking us to find * The question is asking us to find how many new cards Marco got from trading one-fifth of his duplicates with Josh. Step 3: Plan our approach to solving the problem * First, we need to find the number of duplicates Marco has. Since fourth of his cards are duplicates, we can find the number of duplicates by multiplying 500 by 1/4. * Then, we need to find one-fifth of the number of duplicates to find out how many cards Marco traded with Josh. * Finally, well subtract the number of cards traded from the total number of duplicates to find the remaining duplicates. Step 4: Execute our plan * Number of duplicates = 500 1/4 = 125 * One-fifth of the number of duplicates = 125 1/5 = 25 * Marco traded 25 cards with Josh. Step 5: State our conclusion and double-check our answer * Marco got 25 new cards from trading with Josh. Final Answer: 25 Rejected Output Based on the initial answer and the verification feedback, refine the solution as follows: Step 1: Identify the key information given in the question * Marco has total of 500 cards. * fourth of them are duplicates. Step 2: Determine what the question is asking us to find * The question is asking us to find how many new cards Marco got from trading one-fifth of his duplicates with Josh. Step 3: Plan our approach to solving the problem * First, we need to find the number of duplicates Marco has. Since fourth of his cards are duplicates, we can find the number of duplicates by multiplying 500 by 1/4. * Then, we need to find one-fifth of the number of duplicates to find out how many cards Marco traded with Josh. * Finally, well subtract the number of cards traded from the total number of duplicates to find the remaining duplicates. Step 4: Execute our plan * Number of duplicates = 500 1/4 = 125 * One-fifth of the number of duplicates = 125 1/5 = 25 * Marco traded 25 cards with Josh. Since he traded one-fifth of his duplicates, we can find the total number of duplicates he has left by adding the number of cards traded back to the number of duplicates: 125 + 25 = 150 * To find the number of new cards Marco got, we subtract the number of duplicates he traded from the total number of duplicates: 150 - 25 = 125 Step 5: State our conclusion and double-check our answer * Marco got 125 new cards from trading with Josh. 13 Final Answer: Final Answer: 125 Accepted Output Refined Solution: Step 1: Identify the key information given in the question * Marco has total of 500 cards. * fourth of them are duplicates. * Marco trades one-fifth of his duplicates with Josh. Step 2: Determine what the question is asking us to find * The question is asking us to find how many new cards Marco got from trading one-fifth of his duplicates with Josh. Step 3: Plan our approach to solving the problem * First, we need to find the number of duplicates Marco has. Since fourth of his cards are duplicates, we can find the number of duplicates by multiplying 500 by 1/4. * Then, we need to find one-fifth of the number of duplicates to find out how many cards Marco traded with Josh. * Finally, well subtract the number of cards traded from the total number of duplicates to find the remaining duplicates. Step 4: Execute our plan * Number of duplicates = 500 1/4 = 125 * One-fifth of the number of duplicates = 125 1/5 = 25 * Marco traded 25 cards with Josh. * Remaining duplicates = 125 - 25 = 100 Step 5: State our conclusion and double-check our answer * Marco got 25 new cards from trading with Josh. Final Answer: 25 Qualitative Example of an Untrained VS Trained Verifier Question Billy buys 12-pack of soda from the store. If he has twice as many brothers as sisters, and he has 2 sisters, how many sodas can he give to each of his siblings if he wants to give out the entire 12-pack while giving each the same number? Initial Answer Id be happy to help you solve this problem step by step. **Step 1: Identify the key information given in the question.** * Billy buys 12-pack of soda. * He has twice as many brothers as sisters. * He has 2 sisters. **Step 2: Determine what the question is asking us to find.** The question is asking us to find out how many sodas Billy can give to each of his siblings if he wants to give out the entire 12-pack while giving each the same number. **Step 3: Plan our approach to solving the problem.** Lets let be the number of siblings (brothers and sisters). We know that Billy has 2 sisters, so the number of brothers is 2 times the number of sisters, which is 2x. The total number of siblings is the sum of the number of sisters and brothers, which is + 2x = 3x. We are given that Billy has 12-pack of soda, so we can set up the equation: 12 = + 2x **Step 4: Execute our plan, showing all work clearly.** To solve for x, we can combine like terms: 12 = 3x Divide both sides by 3: 4 = So, Billy has 4 siblings in total (2 sisters and 2 brothers). **Step 5: State our conclusion and double-check our answer.** Since Billy has 12 sodas and 4 siblings, he can give: 12 4 = 3 sodas to each of his siblings. 14 Final Answer: 3 Untrained Verifier As verifier, will critically evaluate the initial answer provided above. The initial answer states that Billy can give 3 sodas to each of his siblings. Lets verify the answer by performing the calculations. **Verification:** 1. We are given that Billy has 12 sodas and 4 siblings (2 sisters and 2 brothers). 2. To find out how many sodas each sibling can get, we can divide the total number of sodas (12) by the number of siblings (4): 12 4 = 3 The initial answer is correct! Billy can indeed give 3 sodas to each of his siblings. **Feedback:** The initial answer is mathematically correct. The calculations are accurate, and the solution is well-explained. The answer is also logically consistent with the given information. Well done! However, its always good idea to double-check the calculations to ensure accuracy. In this case, the verification step confirms that the initial answer is correct. Trained Verifier As verifier, will carefully evaluate the initial answer provided above. The initial answer states that Billy can give 3 sodas to each of his siblings. Lets double-check the calculations to ensure the answer is mathematically correct. The initial answer claims that Billy has 4 siblings (2 sisters and 2 brothers). However, the question states that Billy has twice as many brothers as sisters, and he has 2 sisters. This implies that the number of brothers is 2 times the number of sisters, which is 2 2 = 4 brothers. Therefore, Billy actually has 2 sisters and 4 brothers, making total of 6 siblings. The initial answer also claims that Billy can give 12 sodas to 4 siblings, resulting in 3 sodas per sibling. However, dividing 12 sodas among 6 siblings would actually give 2 sodas per sibling, not 3. **Conclusion:** The initial answer contains calculation error. The correct number of siblings is 6, not 4. Therefore, Billy can give 2 sodas to each of his siblings, not 3. As verifier, recommend that the initial answer be corrected to reflect the correct calculation. The correct answer is 2 sodas per sibling. Final Answer: 2 Complete Reasoning and Improvement Sequence Example with MALT on CommonsenseQA Question He looked at the field of pumps, all slowly churning oil out of the what? Choices A: manual B: street C: restaurant D: ground E: service station Initial Answer (Generator) Reasoning steps: 1. The context of the question is about pumps churning oil, which suggests location where oil is 15 being dispensed or extracted. 2. manual (A) does not typically relate to location where oil is churning. 3. street (B) is general location, but its not specific to oil churning. 4. restaurant (C) is not associated with oil churning. 5. The ground (D) could be possible location, but its not the most specific or common location for oil churning. 6. service station (E) is common location where oil is churning, as its where cars are serviced and oil is dispensed. Final Answer: Verifier The initial answer is E: service station. This answer seems to be the most logical choice given the context of the question. The phrase field of pumps suggests location where multiple pumps are present, and service station is common place where multiple pumps are used to dispense oil and other fuels. However, the question asks what the pumps are churning oil out of, not where the pumps are located. This changes the interpretation of the question. Considering the phrase churning oil out of, it implies that the pumps are extracting or pumping oil from source. In this case, the most likely source of oil would be the ground, as oil is often extracted from the ground. Therefore, the correct answer is D: ground. Refined Answer Reasoning steps: 1. The context of the question is about pumps churning oil, which suggests location where oil is being extracted or dispensed. 2. The phrase churning oil out of implies that the pumps are extracting or pumping oil from source. 3. The most common source of oil is the ground, as it is often extracted from the earth. 4. Therefore, the pumps are churning oil out of the ground. Final Answer: Ground Truth (not provided to any model) A.2. Theoretical Justification for MALT (Work in Progress) A.2.1. CREDIT ASSIGNMENT STRATEGY Here, we provide theoretical justification for why our framework, when updating the agent policies based on binarized pooled rewards with threshold at θ = 0.5, leads to policy improvements. We formalize the pooling operation, define the dynamic programming approach of value iteration in our context, and show that our policy updates lead to monotonic increases in expected reward. We consider tree-structured reasoning process where each node represents model in our system: Generator (G), Verifier (V ), or Refinement Model (R). The branches connecting these nodes represent the outputs (actions) generated by each model. For each model, multiple outputs (branches) are possible, representing the different actions or decisions the model can make. Each leaf node corresponds to the terminal outputs of the Refinement model and receives binary reward: (cid:40) R(r) = if is correct, 1, 0, otherwise. (20) We define the value function (n) for node (model) as the expected reward of its downstream refinements starting from that node: (n) = Eπ[R(r) n], where the expectation is over all possible branches under the current policy π, starting from node n. Starting from the leaves: (21) (r) = R(r) {0, 1}, (22) 16 At each node (model), the value is computed as: (n) ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 Q(m, ai), (23) where: is the number of outputs sampled at node n, is the subsequent model with outputs This pooling operation aggregates the expected rewards from downstream paths in dynamic-programming approach. We then apply binarization function ˆV (n) to label nodes as useful (1) or not useful (0) using threshold θ = 0.5: ˆV (n) = (cid:40) if (n) > 0.5, 1, 0, otherwise. (24) There is monotonic relationship between values and binarization. Note that we binarize our values because of computational constraints and DPOs requirement of pair of useful and not useful outputs for preference optimization. With the use of PPO, we could potentially use these values directly as rewards for optimal post-training and leave this for future work. For any two nodes and m, if (n) (m), then ˆV (n) ˆV (m), due to binarization being non-decreasing function. We update the policy pii by using the binarized pooled reward ˆV (s) of the next node as the reward signal. We now show that if we use the binarized pooled reward for policy gradient updates, we will achieve monotonic improvements in the expected reward. Let πt be the policy at iteration t, and Vπt(n) be the value of node under policy πt. Base Case: At iteration = 0, we have the initial value function Vπ0 (n) under the initial policy π0. Inductive Step: Assume that at iteration t, the value function satisfies: Vπt(n) Vπt1(n) for all nodes n. (25) Policy Update: We update the policy πt to πt+1 by using policy gradient methods with the binarized pooled reward ˆV (n) as the reward signal. Specifically, we adjust the policy to increase the probability of outputs leading to child nodes with ˆV (ci) = 1. Now in order to show that: Vπt+1(n) Vπt(n) for all nodes n. At each node n, the value under policy πt is: After the policy update, the value under policy πt+1 is: Vπt(n) = 1 (cid:88) i= Qπt(m, ai), Vπt+1(n) = 1 (cid:88) i=1 Qπt+1(m, ai). (26) (27) (28) Using policy gradient methods (Schulman et al., 2017a) that increase the probabilities of selecting outputs leading to child nodes with ˆV (ci) = 1, and because ˆV (ci) = 1 implies (ci) > 0.5, we can infer that: Vπt+1(ci) Vπt(ci) for all child nodes ci. (29) 17 We note that methods such as DPO or PPO are not inherently guaranteed to be monotonically increasing in terms of expected reward and discuss this further below in Appendix A.2.2. TRPO does offer such guarantees (Schulman et al., 2017a). At the very least, if the update is monotonic when considering the actual value, it is guaranteed to be monotonic when using the binarization based on the proof below. By the inductive hypothesis and the policy update, the values of child nodes do not decrease: Therefore, at node n: Vπt+1(ci) Vπt(ci). Vπt+1(n) ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 Vπt+1(ci)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 Vπt(ci) = Vπt(n). Thus, Vπt+1(n) Vπt(n). By induction, this holds for all nodes in the tree, including the root node. Therefore, we get: J(πt+1) = Vπt+1 (G) Vπt(G) = J(πt). (30) (31) (32) Adaptive Thresholding and Sample Efficiency: In iterative settings, the threshold θ should be an adaptive factor increasing from 0.5 to 1. In our setting, where all data is collected offline, 0.5 is balanced threshold to use for the following reasons: Lower Thresholds (θ < 0.5): This allows for greater sample-efficiency as more branches labeled as correct are used as part of training. However, it might introduce noise into the training process with samples that have low values being chosen as correct. Higher Thresholds (θ > 0.5): This would allow for focus on actions leading to higher-value nodes, reducing variance. However, having θ too high would reduce sample efficiency. Using θ = 0.5 provides balance suitable for single iteration based on an offline generated dataset. By formalizing our pooling and value iteration approach, we have shown that updating the policy based on the binarized pooled reward with thresholding at θ = 0.5 leads to monotonic improvement in the expected reward. This holds because increasing the probability of selecting actions (outputs) leading to next nodes with higher ˆV (s) does not decrease the expected reward at any node, and thus the overall expected reward improves or remains the same. A.2.2. POLICY OPTIMIZING THE DPO OBJECTIVE IS IDENTICAL TO OPTIMAL RL POLICY To support our claims in Appendix A.2.1, we leverage Theorem 1 from (Putta et al., 2024) and Theorem 6.1 from (Setlur et al., 2024), adjusted for our binarization setting: Theorem. Consider policy π that optimizes our objective over trajectories generated by reference policy πref. At each node (state) ht, preferences between actions during DPO are generated according to: p(aw al ht) σ (cid:16) ˆQ(ht, aw ) ˆQ(ht, al t) (cid:17) , (33) where: aw and al are two win/loss actions at node ht, ˆQ(ht, a) {0, 1} is the binarized value function, representing the expected reward of action at state ht, 18 Then, the policy that optimizes the Direct Preference Optimization (DPO) objective is identical to the optimal RL policy: π(a ht) πref(a ht) exp (cid:32) ˆQ(ht, a) β (cid:33) , (34) where β is the DPO hyperparameter. The proof for Theorem 1 in (Putta et al., 2024) shows that the policy π approximates the optimal RL policy. That is, we can approximate the optimal RL policy if we generate preferences under the optimal value function (or an approximation thereof, i.e. our binarized version as shown below). In our setting, since ˆQ(ht, a) {0, 1}, the exponential term simplifies to: exp (cid:17) (cid:16) 1 β when ˆQ(ht, a) = 1, 1 when ˆQ(ht, a) = 0. Therefore, the optimized policy becomes: π(a ht) πref(a ht) exp πref(a ht), (cid:19) , (cid:18) 1 β if ˆQ(ht, a) = 1, if ˆQ(ht, a) = 0. (35) This means that the policy π increases the probability of selecting actions with ˆQ(ht, a) = 1 by constant factor relative to the reference policy πref. By optimizing the DPO objective with these binarized preferences, we ensure that the policy increasingly favors actions leading to higher expected rewards, aligning with our credit assignment strategy described in Appendix A.2. This supports our claim of (approximate) monotonic improvement, as the policy updates move us closer to the optimal policy by consistently selecting actions associated with higher binarized values. A.3. Additional Information For SFT, we used LoRA with learning rate multiplier of 0.1 and batch size of 8 to avoid overfitting. For preference optimization, we used Direct Preference Optimization (DPO) with preference tuning learning rate multiplier to 0.1, training beta parameter of 0.2, and adapter weight configured to 0.2. We varied the number of epochs between 1 to 10 based on the size of the synthetic dataset for each model and leave deeper exploration of hyperparameter configurations that could require significant amount of compute to future work. SFT training was often until convergence. DPO training did not necessarily converge by the end of all iterations. We keep our prompts the same for every baseline and trained model on given benchmark. Our prompts use CoT and zero-shot prompting. We use temperature of 0.3 for Llama 3.1 8B since it was qualitatively good enough to prevent hallucinations and still led to diverse enough samples. MALT requires the presence of an initial training set containing question-answer pairs, which led to the use of MATH, CSQA, and GSM8K."
        }
    ],
    "affiliations": ["University of Oxford", "Cooperative AI Foundation", "MBZUAI", "UC Berkeley"]
}