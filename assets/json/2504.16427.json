{
    "paper_title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark",
    "authors": [
        "Hanlei Zhang",
        "Zhuohang Li",
        "Yeshuang Zhu",
        "Hua Xu",
        "Peiwu Wang",
        "Haige Zhu",
        "Jie Zhou",
        "Jinchao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 2 7 2 4 6 1 . 4 0 5 2 : r Can Large Language Models Help Multimodal Language Analysis? MMLA: Comprehensive Benchmark Hanlei Zhang1 Zhuohang Li1 Yeshuang Zhu2 Hua Xu1 Peiwu Wang1 Haige Zhu Jinchao Zhang2 Jie Zhou2 1Department of Computer Science and Technology, Tsinghua University 2Pattern Recognition Center, WeChat AI, Tencent Inc, China 3Kennesaw State University"
        },
        {
            "title": "Abstract",
            "content": "Multimodal language analysis is rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA."
        },
        {
            "title": "Introduction",
            "content": "Multimodal language analysis has emerged as prominent research area [11], utilizing various modalities to decode cognitive-level semantics in human utterances (e.g., emotion, intent). This analysis is crucial for understanding psychological and behavioral motivations, and it has broad applications in virtual assistants [58], recommender systems [9], and social behavior analysis [40]. This field has attracted significant attention, with early works focusing on annotating sentiment intensity from social media videos [68, 71] and conversations from various TV shows or movies [66, 37]. Additionally, researchers have provided emotion categories for TV shows [45] based on Ekmans six universal emotions [15]. Building on these resources, numerous methods have been developed to learn complementary information and alleviate the challenges posed by the heterogeneous nature of different modalities [53, 23, 26, 78]. In addition to sentiment and emotion, researchers have investigated other linguistic properties such as sarcasm [5, 80] and humor [21, 8], with multimodal fusion methods specifically designed for binary classification tasks [22, 46]. More recently, studies have focused on analyzing coarse-grained and fine-grained intents using new datasets and taxonomies [49, 73, 74], although this area is still in its early stages [52, 85]. *Equal contribution. Corresponding author. Figure 1: Overview of the MMLA benchmark. The left side shows examples from six evaluation dimensions and nine datasets. The right side displays three methods for evaluating both LLMs and MLLMs: (1) zero-shot inference (top right), which generates predictions from task-specific prompts; (2) supervised fine-tuning (middle right), which trains on each supervised task; and (3) instruction tuning (bottom right), which trains on multiple tasks simultaneously. Both (2) and (3) utilize LoRA to efficiently adapt foundation models. Despite these advances, existing methods predominantly rely on fusion techniques built on lightweight neural networks [48, 67, 6], which show limited performance on more complicated reasoning tasks [65]. The advent of MLLMs [33, 35, 84, 56] reveals the huge potential for emergent cross-modal reasoning capabilities through scalable model parameters [65]. However, existing MLLM benchmarks mainly focus on low-level perceptual semantics, such as scene and procedure understanding [32], instance location [38], and elementary cognitive-level tasks like video content analysis [17] and commonsense reasoning [65]. These benchmarks fail to address high-level semantics in conversations. Other benchmarks in this field include only few semantic dimensions, such as emotion and intent [63, 36], or are incapable of evaluating LLMs [34]. To address these challenges, we propose MMLA, the first comprehensive benchmark for multimodal language analysis, aimed at evaluating foundation models. Figure 1 provides an overview of MMLA. In this benchmark, we introduce six representative semantic dimensions for evaluation: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. These dimensions cover the most important cognitive-level semantic aspects of multimodal conversational interactions. We then collect nine publicly available multimodal language datasets, totaling over 61K multimodal utterances across more than 76 hours of video, with each utterance containing text, video, and audio modalities. These datasets span various sources, including both staged scenarios (e.g., TV series, films, TED talks) and real-world settings (e.g., spontaneous social media videos and motivational interviews). Next, we evaluate state-of-the-art (SOTA) LLMs and MLLMs on MMLA. In particular, five branches of MLLMs are employed to leverage both language and video modalities. Additionally, three branches of LLMs that process only text are used for comparison with MLLMs, to assess the effect of non-verbal modalities. The sizes of these models range from 0.5B to 72B parameters. We apply zero-shot inference, supervised fine-tuning, and instruction tuning as evaluation methods. Extensive experiments demonstrate that existing MLLMs show limited performance in understanding high-level cognitive semantics. Supervised fine-tuning can significantly enhance the multimodal capabilities and achieve new SOTA performance on most tasks. In particular, smaller-scale models show great potential with performance comparable to that of larger-scale models. Through instruction tuning, foundation models can successfully handle multiple tasks with unified model, achieving performance comparable to supervised fine-tuning. However, even after tuning, these models still exhibit significant limitation on these tasks, with average accuracy scores below 70%. 2 Our contributions are summarized as follows: (1) We propose MMLA, large-scale multimodal language analysis benchmark containing 61K multimodal utterances drawn from over 76 hours of video. MMLA spans six core dimensions that are crucial for understanding high-level cognitive semantics. (2) To the best of our knowledge, MMLA is the first to comprehensively assess the capabilities of foundation models in multimodal language analysis, by evaluating nine mainstream models across three strategies. (3) Extensive experiments reveal new insights into foundation models for multimodal language analysis. MMLA also pushes the limits of existing MLLMs, providing solid basis and promising new directions for further research. The code is publicly available, and the data are released under their respective licenses (see Appendix for details)."
        },
        {
            "title": "2 Related Works",
            "content": "Multimodal Language Datasets. With the boom in multimodal language analysis, many significant tasks have emerged alongside the development of benchmark datasets. For example, early research focused on multimodal sentiment analysis and emotion recognition, and there are numerous datasets designed to analyze multilingual opinion sentiment [44, 68, 66, 37]. Zadeh et al. [71] constructed the first large-scale dataset in this field and additionally annotated emotion labels following Ekmans taxonomies [15]. However, these datasets only involve individual opinions and lack conversational interactions among multiple speakers. Busso et al. [3] introduced dataset that records conversations between two speakers and annotates each utterance with emotion labels from nine categories in multimodal contexts. Nonetheless, dyadic sessions pose limitations when dealing with real-world multi-party scenarios. To address this, Poria et al. [45] provided sentiment and emotion labels for conversations taken from TV series involving multiple speakers. These abundant resources have led to extensive research on designing effective multimodal fusion methods, including tensor operation-based [69, 39, 70] and transformer-based approaches [53, 48, 23, 20, 26, 75]. Beyond the relatively shallow semantics of sentiment or emotion, researchers have begun to explore more diverse and complex intent semantics in utterances, resulting in substantial new resources. Early work in this field analyzed authors intents on social media platforms. For instance, Kruk et al. [30] proposed taxonomy of eight intents based on rhetorical classes, and Zhang et al. [72] introduced four intent classes related to metaphor. However, these intents differ from those found in conversational scenarios. Saha et al. [49] annotated 12 dialogue acts drawn from the Switchboard[19] tag set for two multimodal emotion recognition datasets [3, 45]. Nevertheless, these dialogue acts are coarse-grained communicative intents that are not directly applicable to real-world applications [73]. To address this issue, Zhang et al. [73] proposed the first hierarchical intent taxonomy specifically designed for multimodal contexts and introduced the first multimodal conversational intent recognition dataset. Zhang et al. [74] subsequently extended this dataset into larger-scale version that accommodates multi-party interactions and includes out-of-scope utterances, reflecting real-world conditions. In addition, some research has focused on individual speaking styles, such as humor [21] and sarcasm [5], which are driven by particular human intents, such as joking or mocking. Recently, Wu et al. [60] investigated more complex communication behaviors between clients and therapists through motivational interviewing in counseling scenarios. Benchmarks. There are also multimodal benchmarks related to this work. For example, MultiBench [34] constructs large-scale multimodal learning benchmark spanning various areas, such as healthcare and robotics. Nevertheless, it only covers dimensions related to affective computing and evaluates traditional multimodal machine learning methods without incorporating powerful MLLMs. To investigate the capability of MLLMs, numerous benchmarks have been proposed in recent years. However, most benchmarks focus on perceptual-level or elementary cognitive-level tasks such as visual recognition [32], optical character recognition [17], multimodal question answering [41], video content analysis [1, 16], scientific calculation [38], and visual reasoning [35]. While previous benchmarks cover diverse domains and tasks, none specifically target large-scale multimodal language analysis. MMLA is the first benchmark designed to advance this field in foundation models. Multimodal Large Language Models. Multimodal large language models have emerged as new paradigm in multimodal learning due to their superior scalability and cross-modal reasoning capabilities. For example, VideoLLaMA2 [7] introduces Spatio-Temporal Convolution (STC) connector that excels in capturing spatiotemporal dynamics for audio-visual tasks. LLaVA-OneVision [31] pioneers cross-modal transfer learning, excelling in zero-shot video understanding despite being trained only on image datasets. LLaVA-Video [79] introduces new video representation technique"
        },
        {
            "title": "Datasets",
            "content": "#C #U #Train #Val #Test"
        },
        {
            "title": "Speaking\nStyle",
            "content": "MIntRec MIntRec2."
        },
        {
            "title": "MELD\nIEMOCAP",
            "content": "MOSI CH-SIMS v2.0 UR-FUNNY-v2 MUStARD"
        },
        {
            "title": "Communication\nBehavior",
            "content": "Anno-MI (client) Anno-MI (threapist) 20 30 12 12 7 6 2 3 2 3 4 2,224 9,304 9,989 9,416 13,708 7,532 2,199 4,403 9,586 4,713 4,773 1,334 6,165 6,992 6,590 9,989 5,354 1,284 2,722 7,612 3,123 3,161 445 445 1,106 2,033 999 942 1,998 1,884 1,109 2,610 1,650 528 229 980 138 461 472 686 1,034 994 138 1,129 1,"
        },
        {
            "title": "Video\nHours",
            "content": "1.5 7.5 8.8 11.7 12.2 9.6 2.6 4.3 12.9 1.0 10.8 12."
        },
        {
            "title": "TV series\nImprovised scripts",
            "content": "Youtube TV series, films"
        },
        {
            "title": "TED\nTV series",
            "content": "YouTube & Vimeo #Video Length avg. / max. #Text Length avg. / max."
        },
        {
            "title": "Language",
            "content": "2.4 / 9.6 2.9 / 19.9 3.2 / 41.1 4.5 / 34.2 3.2 / 305.0 4.6 / 34.2 4.3 / 52.5 3.6 / 42.7 4.8 / 325.7 5.2 / 20.0 8.2 / 600.0 9.1 / 1316. 7.6 / 27.0 8.5 / 46.0 8.6 / 72.0 12.4 / 106.0 8.7 / 72.0 12.8 / 106.0 12.5 / 114.0 1.8 / 7.0 16.3 / 126.0 13.1 / 68.0 16.3 / 266.0 17.9 / 205."
        },
        {
            "title": "English",
            "content": "Table 1: Dataset statistics for each dimension in the MMLA benchmark. #C, #U, #Train, #Val, and #Test represent the number of label classes, utterances, training, validation, and testing samples, respectively. avg. and max. refer to the average and maximum lengths. that allows maximizing the sampling of video frames, and high-quality dataset is constructed to promote its instruction-following capability. Qwen2-VL [55] leads in vision-language understanding and generation, performing well in both zero-shot and few-shot settings. MiniCPM-V [64] innovates in model compression to enable efficient mobile deployment without compromising performance. Although current MLLMs perform well on various tasks, no benchmark evaluates their ability to handle complex multimodal language analysis."
        },
        {
            "title": "3.1 Evaluation Dimensions",
            "content": "To comprehensively evaluate the complexity and diversity of human interactions, we select six representative dimensions across various linguistic and interactional levels: intent, dialogue act, emotion, sentiment, speaking style, and communication behavior. These dimensions collectively encapsulate the core aspects of multimodal language analysis [50, 18]. In particular, intent captures the ultimate purpose or goal of human communication, such as requesting information or making decisions [47]. In contrast, dialogue act is more coarse-grained type of intent [49]. It typically focuses on the dynamic progression of communication, such as questioning or stating opinions [51]. Nonverbal signals (e.g., gaze shifts, gestures, and facial expressions) provide valuable clues to resolve ambiguities in both perspectives [73, 74]. Sentiment, emotion, and speaking style are three significant aspects often accompanying communicative interactions. Sentiment refers to the polarity (e.g., positive or negative) of subjective opinions [11], emotion conveys the speakers internal psychological state (e.g., happiness, anger) [14], and speaking style refers to individual expressive variations in communication (e.g., sarcasm, humor) [43]. Multimodal cues (e.g., facial expressions and gestures) play crucial role in inferring these communicative characteristics [62, 22, 42]. Communication behavior explores the interaction behaviors between individuals (e.g., sustain, change, and reflection), which facilitate the progression of conversations and exhibit social properties within groups [60]. Non-verbal signals (e.g., eye contact and gestures) can help uncover these behaviors and offer insights into modeling social cohesion [54]. Detailed information about the labels used for each dimension in each dataset can be found in Appendix B."
        },
        {
            "title": "3.2 Data Sources",
            "content": "We collect nine typical publicly available multimodal language datasets corresponding to the evaluation dimensions. Detailed statistics for these datasets are provided in Table 1. For the intent dimension, we use two pioneering multimodal intent datasets, MIntRec [73] and MIntRec2.0 [74], which cover up to 30 intent classes commonly occurring in daily life. For the emotion dimension, we utilize two widely used multimodal emotion recognition datasets, MELD [45] and IEMOCAP [3], both containing Ekmans six universal emotion categories as suggested in [26]. Additionally, MELD includes neutral class. For the dialogue act dimension, we use curated versions of the MELD and IEMOCAP datasets, with annotations provided by EmoTyDA [49]. These annotations consist of 12 commonly occurring classes selected from the SwitchBoard tag set [19]. For the sentiment dimension, 4 we use two popular multimodal sentiment analysis datasets: MOSI [68] and CH-SIMS v2.0 [37]. Both are annotated with sentiment intensity values in the range of [-3, 3]. Following [67], we convert these annotations into polarity-based two-class and three-class labels for evaluation. For the speaking style dimension, we focus on two properties that play significant roles in social interactions: humor and sarcasm. We use UR-FUNNY-v2 [21] and MUStARD [5] for binary classification tasks, respectively. For the communication behavior dimension, we employ the Anno-MI [60] dataset, which involves motivational interviewing (MI) in counseling dialogues. This dataset is divided into two subsets, each analyzing three or four typical behaviors exhibited by clients and therapists. Details of the annotation quality assurance for each dataset are provided in Appendix C. These datasets contain wide variety of characters, scenes, and background contexts in both English and Mandarin. They are sourced from popular TV series (e.g., Friends, The Big Bang Theory, Superstore, etc.), films, online video-sharing platforms (e.g., YouTube, Vimeo, Bilibili), idea-sharing platforms (e.g., TED), and scripted dyadic sessions. We perform necessary cleaning and corrections to ensure the quality of each multimodal sample, aligning transcriptions, raw videos, and audio data. The datasets in the benchmark consist of 61,016 high-quality multimodal samples, totaling 76.6 hours of video, with 12,093 samples reserved for testing."
        },
        {
            "title": "3.3 Method Overview",
            "content": "Zero-shot Inference. We leverage the generalization capabilities of foundation models for zero-shot inference. Specifically, for LLMs, the prompt template includes the transcribed utterances of speakers as text information, followed by task-specific query with candidate labels. The LLM generates response by predicting the next token in an autoregressive manner [2], which corresponds to the most appropriate label. For MLLMs, we extend this template by adding the special token <video> at the beginning of the instruction, with its number aligned to the number of videos. This ensures structured alignment across modalities, enhancing the models capacity to process multimodal input. Details of the prompt templates used for inference can be found in Appendix D. Supervised Fine-tuning (SFT). We further optimize foundation models to enhance their instructionfollowing capabilities using SFT techniques while employing the same instruction templates as used during inference. Fine-tuning is performed by minimizing the cross-entropy loss between the models autoregressively predicted token probabilities and the ground-truth tokens corresponding to the labels. Let the input sequence be = (x1, x2, . . . , xn) and the target sequence be = (y1, y2, . . . , ym). The cross-entropy loss LCE is defined as: LCE = (cid:88) t=1 log (ytx, y<t; θ), where (ytx, y<t; θ) is the probability of token yt given the input x, the previous tokens y<t, and the model parameters θ. To ensure training stability and reduce computational cost, we adopt the Low-Rank Adaptation (LoRA) [25] technique, which significantly reduces the number of parameters to be fine-tuned while preserving the models generalization capabilities. Instruction Tuning (IT). Since SFT addresses only the single-task scenario, we further explore the generalization ability of foundation models on multiple tasks. We first combine the training data from all datasets of each task for training, then we use the same template as the other two strategies, with the difference being that the task is not limited to one. The optimization objective follows SFT and uses the candidate labels of each task as supervised targets."
        },
        {
            "title": "4 Experiments",
            "content": "Evaluation Metrics. We employ six commonly used metrics: accuracy (ACC), weighted F1-score (WF1), weighted precision (WP), macro F1-score (F1), recall (R), and precision (P) for evaluation, as suggested in the literature [74, 45, 83, 68, 22]. In particular, we report the primary results of ACC in this paper, with additional results for the remaining metrics provided in the Appendices. Evaluation Baselines. We apply the three evaluation methods as described in Section 3.3 on advanced LLMs and MLLMs as baselines. We also compare the foundation models with SOTA multimodal machine learning (MML) methods. 5 LLMs. Three series of different parameter scales of unimodal foundation models are included: Llama-3 [13] (8B), InternLM-2.5 (7B) [4], and Qwen2 [61] (0.5B, 1.5B, and 7B). MLLMs. Five series of different parameter scales of multimodal foundation models are included: VideoLLaMA2 [7] (7B), Qwen2-VL [55] (7B and 72B), LLaVA-Video [79] (7B and 72B), LLaVAOneVision (LLaVA-OV) [31] (7B and 72B), and MiniCPM-V-2.6 [64] (8B). For language decoding, the first series use Mistral [28], and the last four series use Qwen2 with the same parameter scale as the MLLM. We follow the same vision encoders as those in the corresponding released open-source models. We also apply zero-shot inference on one closed-source MLLM, GPT-4o [27] as baseline. MML Methods. We collect open-source MML methods with SOTA performance for each dataset for detailed comparison. Specifically, for MIntRec: MIntOOD [76], MIntRec2.0: MulT [53], MELD and IEMOCAP: UniMSE [26], MELD-DA: TCL-MAP [83], IEMOCAPDA: MIntOOD [76], MOSI: MMML [59], CH-SIMS v2.0: ALMT [77], UR-FUNNY-v2 and MUStARD: SimMMDG [12]. The results are reported as they appear in the corresponding papers. Experimental Setup. We mostly follow the original data splits for training, validation, and testing for each dataset, as detailed in Table 1. Each sample consists of text and video data aligned at the utterance level for speakers. For SFT and IT methods, we utilize LLaMAFactory [82] for all LLMs and Qwen2-VL, SwiFT [81] for MiniCPM-V-2.6, LLaVA-NeXT 1 for LLaVA-OV and LLaVA-Video, and VideoLLaMA2 using its own public code 2, respectively. We employ FlashAttention-2 [10] to optimize the attention modules of transformers, reducing memory and time costs. Besides, we leverage the DeepSpeed library for distributed training (e.g., using ZeRO-3 for memory optimization) and parallel computation. The precision type is set to BF16, offering reduced computational costs compared to FP16 or FP32. The learning rates range from 2e-5 to 1e-3, and cosine learning rate scheduler with warmup ratios from 0.1 to 0.3 is applied. The training batch sizes are chosen from {4, 8, 16, 24}. The rank and α parameters of the LoRA module are set to {8, 16, 64, 128} and {16, 32, 128, 256}, respectively. All experiments are conducted on NVIDIA A100 GPUs. We monitor model accuracy on the validation set to select the best checkpoint for inference. Details of the used hyperparameters and the full experimental results are shown in Appendix E."
        },
        {
            "title": "5.1 Main Results",
            "content": "To clearly illustrate the performance differences between foundation models on the MMLA benchmark, we present the ranking statistics of the average accuracy (ACC) across all combined testing sets. Specifically, the zero-shot inference performance is shown in Figure 2, while the performance after SFT and IT is shown in Figure 3. We find some interesting and new insights as below. Comparable Performance between LLMs and MLLMs in Zero-shot Inference. As shown in Figure 2, the closed-source GPT-4o achieves the best performance, and the three opensource 72B MLLMs occupy the remaining positions in the top four. This is unsurprising, as these models contain more parameters and therefore exhibit stronger generalization and reasoning capabilities, consistent with the scaling laws [29]. However, we note that the much smaller-scale LLM InternLM2.57B achieves comparable performance, within approximately 2% difference. Furthermore, among models with similar scale (7B or 8B parameters), most MLLMs exhibit lower performance than LLMs. For example, InternLM2.5 and Qwen2 outperform most MLLMs (e.g., LLaVA-Video, VideoLLaMA2) by 58%. These results indicate that existing MLLMs have significant limitations in leveraging non-verbal information to capture complex high-level semantics without supervision from domain-specific data. 1https://github.com/LLaVA-VL/LLaVA-NeXT 2https://github.com/DAMO-NLP-SG/VideoLLaMA2 6 Figure 2: Rank of foundation models after zero-shot inference. Small MLLMs Rival Large Ones After SFT and IT. Although MLLMs exhibit substantial performance gaps in zero-shot inference, parameter size matters far less once theyre trained with SFT or IT. For example, as shown in Figure 3, 7B MLLMs trained with SFT achieve 67.4768.30% ACC, while their 72B counterparts reach 68.4469.18%, performance gap of only 12%. Specially, the 8B MiniCPM-V-2.6 after SFT attains second place with 68.88%, only 0.3% lower in ACC than the top model, and surpasses several much larger MLLMs. 7B, 8B, and 72B MLLMs trained with IT also achieve ACC scores within 2% of each other (i.e., 67.2568.87%). These results show that small-scale well-trained MLLMs can capture the cognitive semantics underlying human language, suggesting lightweight foundation models are feasible and significantly reduce costs. Comparable Performance of MLLMs Between SFT and IT. Although SFT offers the advantage of task-specific fine-tuning to boost individual task performance, we observe that MLLMs trained via IT can achieve comparable results or even surpass those of SFT when evaluated on multiple tasks. For example, in Figure 3, the 72B LLaVA-Video ranks third and outperforms its SFT counterpart by 0.43%. Qwen2-VL shows only slight performance drop after IT (0.54% for 72B and 0.26% for 7B). In contrast, MiniCPM-V-2.6 suffers more pronounced decline of 1.63% compared with its SFT counterpart, and some models (e.g., LLaVA-Video and LLaVA-OV) are omitted because they exhibit severe hallucinations, producing irrelevant outputs following IT. However, the strong performance of certain MLLMs highlights the potential of training unified model to excel across diverse tasks without the overhead of maintaining multiple models, thereby demonstrating the robust generalization capabilities of MLLMs on this task. MLLMs Still Face Challenges on the MMLA Benchmark. From Figures 2 and 3, we observe that the best MLLM in zero-shot inference (GPT-4o) achieves only 52.6% ACC, and the best model after training with supervised data (72B Qwen2-VL) reaches just 69.18% ACC, still exhibiting huge limitations. These findings underscore the difficulty and importance of the MMLA benchmark, pushing the boundaries of existing MLLMs and laying solid foundation for future related research. Figure 3: Rank of foundation models after SFT and IT."
        },
        {
            "title": "5.2 Fine-grained Performance on Different Dimensions",
            "content": "To further investigate fine-grained performance across different dimensions, we present the results of three methods for each MLLM and LLM on every dataset, as shown in Figures 4 and 5. Foundation Models Struggle with Zero-Shot Inference. As shown in Figure 4, zero-shot performance is substantially limited, with ACC scores below 60% on many challenging semantic dimensions (e.g., Intent, Emotion, Dialogue Act, and Communication Behavior). This shortcoming arises because these dimensions typically involve numerous categories with nuanced differences. In contrast, performance on the Sentiment and Speaking Style dimensions is generally higher because these tasks are simpler, requiring only two or three classes to be distinguished. GPT-4o achieves the best results in several dimensions, such as Intent, Dialogue Act, and Sentiment, highlighting its strong ability to leverage multiple modalities for reasoning. However, it still struggles with tasks like sarcasm detection, emotion recognition, and communication behavior recognition, likely due to interference from scene context, background, and characters. Finally, while LLMs show performance comparable to or better than MLLMs of the same parameter scale, their scores remain below 60% in most cases, underscoring the significant limitations of current foundation models on our benchmark. Foundation Models Significantly Improve After SFT. As shown in Figure 4, foundation models exhibit notable performance boost after SFT. For example, ACC scores increase by 2040% on Intent, 1040% on Dialogue Act, 420% on Speaking Style, and 550% on Communication Behavior. Specifically, MiniCPM-V-2.6 achieves improvements of over 30% across most dimensions. These results demonstrate that training with supervised instruction data effectively helps MLLMs and LLMs distinguish complex semantic categories. Moreover, although both MLLMs and LLMs benefit from SFT, MLLMs consistently outperform LLMs (see Figure 3), despite showing similar 7 Figure 4: Fine-grained zero-shot inference and SFT performance (ACC). Within each bar, the lightcolored lower segment corresponds to zero-shot inference performance, while the darker upper segment represents the additional gains from SFT. The performance of SOTA MML methods (if available) and GPT-4o are indicated with purple and green dashed lines, respectively. zero-shot performance. This suggests that SFT not only aligns modalities better to activate multimodal reasoning, but also that incorporating non-verbal information reduces hallucinations more effectively than using text alone. Finally, MLLMs after SFT set new state-of-the-art results on most datasets except IEMOCAP and MUStARD, highlighting their great potential in multimodal language analysis. Foundation Models Master Multiple Tasks After IT. As shown in Figure 5, MLLMs after IT can simultaneously match or surpass previous SOTA methods on most datasets. In particular, 72B Qwen2-VL is the first to exceed human performance on MIntRec [73] (86.3% vs. 85.5%), marking remarkable progress toward human-level semantic comprehension. 72B LLaVA-Video improves over the SOTA method by 6.3% and approaches human performance on MIntRec2.0 [74]. Similarly, most MLLMs exhibit superior results on sentiment analysis (Ch-sims-v2), humor detection (UR-FUNNY-v2), and emotion recognition (MELD). We also observe that the small-scale MLLM (i.e., 8B MiniCPM-V-2.6) outperforms SOTA on seven datasets across five dimensions and achieves the best score on Ch-sims-v2. Moreover, small-scale MLLMs outperform LLMs on nearly every dataset and task, underscoring that IT enhances multimodal reasoning and demonstrating the potential of training unified MLLM to tackle multiple complex multimodal language tasks."
        },
        {
            "title": "5.3 Scalability of Foundation Models on MMLA",
            "content": "To examine the scalability of foundation models [29], we analyze the effect of parameter scale using Qwen2 and Qwen2-VL, presenting both zero-shot inference and SFT results in Figure 6. Scaling Performance of Zero-Shot Inference. In zero-shot inference, scaling Qwen2 from 0.5B to 1.5B parameters achieves significant improvements across all dimensions except Communication Behavior. When scaling from 1.5B to 7B, performance gains accelerate on Intent and Communication Behavior, slow down on Emotion and Dialogue Act, and even slightly decrease on Sentiment and Speaking Style. This phenomenon indicates that larger gains occur with smaller scale changes. When moving from Qwen2 to Qwen2-VL, performance is comparable or better in all dimensions except for 8 Figure 5: Fine-grained performance (ACC) of instruction-tuned MLLMs and LLMs on each dataset across six dimensions. The performance of SOTA MML methods and humans are indicated with dashed lines, if available. Communication Behavior, which shows dramatic drop. However, scaling Qwen2-VL from 7B to 72B yields substantial improvements, further validating the scalability of MLLMs. Scaling Performance of SFT. After SFT, scaling Qwen2 from 0.5B to 7B yields modest improvements of 35% on the Intent, Sentiment, Speaking Style, and Emotion dimensions, with limited gains of less than 2% on Communication Behavior and Dialogue Act. Besides, scaling Qwen2-VL from 7B to 72B achieves substantial improvements of over 5% on Speaking Style and Intent dimensions, while yielding under 2% gains in Sentiment, Communication Behavior, and Dialogue Act. These results suggest that simply enlarging model parameters provides little benefit for analyzing complex multimodal language semantics when using supervised instructions as prior knowledge. They also highlight the significant challenge posed by this benchmark and underscore the need to design appropriate architectures and curate highquality data for learning high-level cognitive semantics."
        },
        {
            "title": "6 Conclusions",
            "content": "Figure 6: Scalability of Qwen2 and Qwen2-VL on the MMLA benchmark. This paper proposes MMLA, the first large-scale benchmark for evaluating foundation models on multimodal language analysis. It covers six core semantic dimensions across more than 61,000 utterances from nine diverse datasets spanning text, audio, and video modalities. We evaluate five branches of MLLMs and three branches of LLMs, ranging from 0.5B to 72B parameters, using three methods to provide comprehensive analysis. This benchmark yields several new insights. First, existing MLLMs exhibit poor capabilities and offer no advantage over LLMs in zero-shot inference. Second, supervised fine-tuning (SFT) effectively activates MLLMs, enabling them to leverage non-verbal modalities to understand cognitive-level semantics, and achieves substantial improvements over LLMs. Third, instruction tuning (IT) can further fine-tune unified model to achieve comparable or better performance on all SFT tasks. Interestingly, we find that smaller MLLMs, after both SFT and IT, demonstrate enormous potential, achieving performance comparable to much larger models while significantly reducing computational costs. Finally, existing MLLMs still face significant challenges, with an average accuracy below 70%, underscoring the importance and difficulty of the proposed benchmark and pushing the limits of research in multimodal language analysis. MMLA establishes rigorous foundation for advancing multimodal language understanding and cognitive-level humanAI interaction."
        },
        {
            "title": "References",
            "content": "[1] Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny. Infinibench: comprehensive benchmark for large multimodal models in very long video understanding. arXiv preprint arXiv:2406.19875, 2024. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pages 18771901, 2020. [3] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42:335359, 2008. [4] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [5] Santiago Castro, Devamanyu Hazarika, Verónica Pérez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. Towards multimodal sarcasm detection (an _obviously_ perfect paper). In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 46194629, 2019. [6] Yifan Chen, Kuntao Li, Weixing Mai, Qiaofeng Wu, Yun Xue, and Fenghuan Li. D2R: Dual-branch dynamic routing network for multimodal sentiment detection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 35363547, 2024. [7] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [8] Lukas Christ, Shahin Amiriparian, Alexander Kathan, Niklas Müller, Andreas König, and Björn W. Schuller. Towards multimodal prediction of spontaneous humor: novel dataset and first results. IEEE Transactions on Affective Computing, 2024. [9] Chen Cui, Wenjie Wang, Xuemeng Song, Minlie Huang, Xin-Shun Xu, and Liqiang Nie. User attentionguided multimodal dialog systems. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pages 445454, 2019. [10] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In Proceedings of the 12th International Conference on Learning Representations, 2024. [11] Ringki Das and Thoudam Doren Singh. Multimodal sentiment analysis: survey of methods, trends, and challenges. ACM Computing Surveys, 55(13s):270307, 2023. [12] Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, and Olga Fink. Simmmdg: simple and effective framework for multi-modal domain generalization. In Proceedings of the Advances in Neural Information Processing Systems, pages 7867478695, 2023. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Paul Ekman. An argument for basic emotions. Cognition & emotion, 6(3-4):169200, 1992. [15] Paul Ekman, Wallace Freisen, and Sonia Ancoli. Facial signs of emotional experience. Journal of personality and social psychology, 39(6):1125, 1980. [16] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. [17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [18] Ankita Gandhi, Kinjal Adhvaryu, Soujanya Poria, Erik Cambria, and Amir Hussain. Multimodal sentiment analysis: systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions. Information Fusion, 91:424444, 2023. 10 [19] John Godfrey, Edward Holliman, and Jane McDaniel. Switchboard: Telephone speech corpus for research and development. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 517520, 1992. [20] Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 91809192, 2021. [21] Md Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed Ehsan Hoque. Ur-funny: multimodal language dataset for understanding humor. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 20462056, 2019. [22] Md Kamrul Hasan, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan Hoque. Humor knowledge enriched transformer for understanding multimodal humor. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1297212980, 2021. [23] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariant and-specific representations for multimodal sentiment analysis. In Proceedings of the 28th ACM international conference on multimedia, pages 11221131, 2020. [24] Dou Hu, Xiaolong Hou, Lingwei Wei, Lianxin Jiang, and Yang Mo. Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 70377041, 2022. [25] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Proceedings of the 10th International Conference on Learning Representations, 2022. [26] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, and Yongbin Li. UniMSE: Towards unified multimodal sentiment analysis and emotion recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 78377851, 2022. [27] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [28] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [29] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [30] Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, and Ajay Divakaran. Integrating text and image: Determining multimodal document intent in Instagram posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 46224632, 2019. [31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [32] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: In Proceedings of the IEEE/CVF Conference on Benchmarking multimodal large language models. Computer Vision and Pattern Recognition, pages 1329913308, 2024. [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742, 2023. [34] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning. In Proceedings of the Advances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proceedings of the Advances in Neural Information Processing Systems, pages 3489234916, 2023. 11 [36] Rui Liu, Haolin Zuo, Zheng Lian, Xiaofen Xing, Björn Schuller, and Haizhou Li. Emotion and intent joint understanding in multimodal conversation: benchmarking dataset. arXiv preprint arXiv:2407.02751, 2024. [37] Yihe Liu, Ziqi Yuan, Huisheng Mao, Zhiyun Liang, Wanqiuyue Yang, Yuanzhe Qiu, Tie Cheng, Xiaoteng Li, Hua Xu, and Kai Gao. Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module. In Proceedings of the 2022 International Conference on Multimodal Interaction, pages 247258, 2022. [38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision, pages 216233, 2025. [39] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 2247 2256, 2018. [40] Trisha Mittal, Sanjoy Chowdhury, Pooja Guhan, Snikitha Chelluri, and Dinesh Manocha. Towards determining perceived audience intent for multimodal social media posts using the theory of reasoned action. Scientific Reports, 14:10606, 2024. [41] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. [42] Badri N. Patro, Mayank Lunayach, Deepankar Srivastava, Sarvesh, Hunar Singh, and Vinay P. Namboodiri. Multimodal humor dataset: Predicting laughter tracks for sitcoms. In Proceedings of the 2021 IEEE Winter Conference on Applications of Computer Vision, pages 576585, 2021. [43] James Pennebaker, Matthias Mehl, and Kate Niederhoffer. Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology, 54(1):547577, 2003. [44] Verónica Pérez-Rosas, Rada Mihalcea, and Louis-Philippe Morency. Utterance-level multimodal sentiment analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973982, 2013. [45] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. MELD: multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 527536, 2019. [46] Shraman Pramanick, Aniket Roy, and Vishal M. Patel Johns. Multimodal learning using optimal transport for sarcasm and humor detection. In 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, pages 546556, 2022. [47] Libo Qin, Tianbao Xie, Wanxiang Che, and Ting Liu. survey on spoken language understanding: Recent advances and new frontiers. In Proceedings of the 30th International Joint Conference on Artificial Intelligence, pages 45774584, 2021. [48] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, LouisPhilippe Morency, and Ehsan Hoque. Integrating multimodal information in large pretrained transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 23592369, 2020. [49] Tulika Saha, Aditya Patra, Sriparna Saha, and Pushpak Bhattacharyya. Towards emotion-aided multi-modal dialogue act classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 43614372, 2020. [50] Tobias Schröder, Terrence Stewart, and Paul Thagard. Intention, emotion, and action: neural theory based on semantic pointers. Cognitive Science, 38(5):851880, 2014. [51] Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339373, 2000. [52] Kaili Sun, Zhiwen Xie, Mang Ye, and Huyin Zhang. Contextual augmented global contrast for multimodal intent recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2696326973, June 2024. 12 [53] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 65586569, 2019. [54] Alessandro Vinciarelli, Maja Pantic, and Hervé Bourlard. Social signal processing: Survey of an emerging domain. Image and vision computing, 27(12):17431759, 2009. [55] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [56] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. In Advances in Neural Information Processing Systems, volume 37, pages 121475121499, 2024. [57] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can shift: Dynamically adjusting word representations using nonverbal behaviors. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 72167223, 2019. [58] Te-Lin Wu, Satwik Kottur, Andrea Madotto, Mahmoud Azab, Pedro Rodriguez, Babak Damavandi, Nanyun Peng, and Seungwhan Moon. SIMMC-VR: task-oriented multimodal dialog dataset with situated and immersive VR streams. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 62736291, 2023. [59] Zehui Wu, Ziwei Gong, Jaywon Koo, and Julia Hirschberg. Multimodal multi-loss fusion network for sentiment analysis. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35883602, 2024. [60] Zixiu Wu, Simone Balloccu, Vivek Kumar, Rim Helaoui, Ehud Reiter, Diego Reforgiato Recupero, and Daniele Riboni. Anno-mi: dataset of expert-annotated counselling dialogues. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 61776181, 2022. [61] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [62] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang. Disentangled representation learning for multimodal emotion recognition. In Proceedings of the 30th ACM International Conference on Multimedia, pages 16421651, 2022. [63] Qu Yang, Mang Ye, and Bo Du. Emollm: Multimodal emotional understanding meets large language models. arXiv preprint arXiv:2406.16442, 2024. [64] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [65] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. [66] Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, and Kaicheng Yang. CH-SIMS: Chinese multimodal sentiment analysis dataset with fine-grained annotation of modality. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 37183727, 2020. [67] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1079010797, 2021. [68] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos. arXiv preprint arXiv:1606.06259, 2016. [69] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 11031114, 2017. [70] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe In Proceedings of the AAAI Morency. Memory fusion network for multi-view sequential learning. Conference on Artificial Intelligence, pages 56345641, 2018. [71] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 22362246, 2018. [72] Dongyu Zhang, Minghao Zhang, Heting Zhang, Liang Yang, and Hongfei Lin. Multimet: multimodal dataset for metaphor understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 32143225, 2021. [73] Hanlei Zhang, Hua Xu, Xin Wang, Qianrui Zhou, Shaojie Zhao, and Jiayan Teng. Mintrec: new dataset for multimodal intent recognition. In Proceedings of the 30th ACM International Conference on Multimedia, pages 16881697, 2022. [74] Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, Wenrui Li, Yanting Chen, et al. Mintrec2.0: large-scale benchmark dataset for multimodal intent recognition and out-of-scope detection in conversations. In Proceedings of the the 12th International Conference on Learning Representations, 2024. [75] Hanlei Zhang, Hua Xu, Fei Long, Xin Wang, and Kai Gao. Unsupervised multimodal clustering for semantics discovery in multimodal utterances. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 1835, 2024. [76] Hanlei Zhang, Qianrui Zhou, Hua Xu, Jianhua Su, Roberto Evans, and Kai Gao. Multimodal classification and out-of-distribution detection for multimodal intent understanding. arXiv preprint arXiv:2412.12453, 2024. [77] Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, and Tianshu Yu. Learning languagearXiv preprint guided adaptive hyper-modality representation for multimodal sentiment analysis. arXiv:2310.05804, 2023. [78] Xinxin Zhang, Jun Sun, Simin Hong, and Taihao Li. Amanda: Adaptively modality-balanced domain adaptation for multimodal emotion recognition. In Findings of the Association for Computational Linguistics: ACL, pages 1444814458, 2024. [79] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [80] Wenye Zhao, Qingbao Huang, Dongsheng Xu, and Peizhi Zhao. Multi-modal sarcasm generation: dataset and solution. In Findings of the Association for Computational Linguistics: ACL, pages 56015613, 2023. [81] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2973329735, 2025. [82] Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYanhan YeYanhan, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics : System Demonstrations, pages 400410, 2024. [83] Qianrui Zhou, Hua Xu, Hao Li, Hanlei Zhang, Xiaohan Zhang, Yifan Wang, and Kai Gao. Token-level contrastive learning with modality-aware prompting for multimodal intent recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1711417122, 2024. [84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. In Proceedings of the the 12th International Conference on Learning Representations, 2024. [85] Zhihong Zhu, Xuxin Cheng, Zhaorun Chen, Yuyan Chen, Yunyan Zhang, Xian Wu, Yefeng Zheng, and Bowen Xing. Inmu-net: advancing multi-modal intent detection via information bottleneck and multisensory processing. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 515524, 2024."
        },
        {
            "title": "A License",
            "content": "This benchmark uses nine datasets, each of which is employed strictly in accordance with its official license and exclusively for academic research purposes. We fully respect the datasets copyright policies, license requirements, and ethical standards. For those datasets whose licenses explicitly permit redistribution, we release the original video data (e.g., MIntRec3, MIntRec2.04, MELD5, UR-FUNNY-v26, MUStARD7, MELD-DA and IEMOCAP-DA8, CH-SIMS v2.09). For datasets that restrict video redistribution, users should obtain the videos directly from their official repositories (e.g., MOSI10, IEMOCAP11, Anno-MI12). In compliance with all relevant licenses, we also provide the original textual data unchanged, together with the specific dataset splits used in our experiments. This approach ensures reproducibility and academic transparency while strictly adhering to copyright obligations and protecting the privacy of individuals featured in the videos."
        },
        {
            "title": "B Used Labels for Each Dataset",
            "content": "Intent. The MIntRec dataset uses 20 predefined intent categories derived from two coarse-grained classes (i.e., achieve goals and express emotions and attitudes), as described in [73]. These categories are: complain, praise, apologize, thank, criticize, agree, taunt, flaunt, joke, oppose, comfort, care, inform, advise, arrange, introduce, leave, prevent, greet, and ask for help. MIntRec2.0 adds 10 more labels (i.e., doubt, acknowledge, refuse, warn, emphasize, ask for opinions, confirm, explain, invite, and plan) to the original 20. We use the in-scope portion of this dataset for intent recognition. Dialogue Act. The MELD-DA and IEMOCAP-DA datasets select the 12 most frequent dialogue-act tags in everyday conversation, based on the 42 acts defined in [51]. The chosen tags are: greeting, question, answer, statement-opinion, statement-non-opinion, apology, command, agreement, disagreement, acknowledge, backchannel, and others. Emotion. The IEMOCAP dataset adopts Ekmans six universal emotions (as in prior work [57, 24]): angry, happy, sad, neutral, frustrated, and excited. The MELD dataset uses seven emotion classes [45]: neutral, surprise, fear, sadness, joy, anger, and disgust. Sentiment. For the MOSI and CH-SIMS v2.0 datasets, sentiment intensity scores range from 3 to 3 and are mapped to twoor three-way polarity classes (e.g., positive, neutral, negative), as recommended in [68, 37]. Speaking Style. The UR-FUNNY-v2 and MUStARD datasets both perform binary classification tasks: humor detection (humorous vs. serious) and sarcasm detection (sarcastic vs. sincere), respectively. Communication Behavior. The Anno-MI dataset is split into two parts for counseling dialogue analysis. The first part contains four therapist communication skills: question, input, reflection, and other. The second part contains three client talk types: change, neutral, and sustain."
        },
        {
            "title": "C Assurance of Annotation Quality",
            "content": "We employ rigorous procedures to select datasets with high-quality annotations. Quality is ensured through the following strategies and statistical measures for each dataset: MIntRec [73] and MIntRec2.0 [74]. Intent labels are assigned by majority voting (three of five and two of three annotators, respectively). Fleisss kappa values of 0.88 for MIntRec and 0.69 for MIntRec2.0 indicate excellent and substantial agreement, respectively. MELD [45] and IEMOCAP [3]. Emotion labels are determined by three-annotator majority voting, yielding Fleisss kappa values of 0.43 (MELD) and 0.40 (IEMOCAP), reflecting acceptable reliability for emotion annotation. MELD-DA and IEMOCAP-DA [49]. Dialogue-act labels are annotated by three experts, achieving over 80% inter-annotator agreement. MUStARD [5]. Three annotators achieved Cohens kappa of 0.588 for sarcasm detection. 3https://github.com/thuiar/MIntRec 4https://github.com/thuiar/MIntRec2.0 5https://github.com/declare-lab/MELD 6https://github.com/ROC-HCI/UR-FUNNY 7https://github.com/soujanyaporia/MUStARD 8https://github.com/sahatulika15/EMOTyDA 9https://github.com/thuiar/ch-sims-v2 10https://github.com/matsuolab/CMU-MultimodalSDK 11https://sail.usc.edu/iemocap 12https://github.com/uccollab/AnnoMI 15 UR-FUNNY-v2 [21]. The original UR-FUNNY was annotated based on direct laughter markers in punchlines; noisy and overlapping instances were removed to form the second version, which we use in our benchmark. MOSI [68]. Five master workers (approval rate > 95%) annotated sentiment intensity, with Krippendorffs alpha of 0.77. Anno-MI [60]. Ten therapists from the International Organization of Authoritative Motivational-Interviewing Trainers annotated communication behavior labels, with Fleisss kappa values of 0.74 (therapist) and 0.47 (client), indicating substantial and moderate agreement, respectively. CH-SIMS v2.0 [37]. This version corrects potential errors in the original CH-SIMS [66]. Seven well-trained annotators rated sentiment intensity: the highest and lowest scores were removed, and the average of the remaining five was mapped to discrete sentiment labels. We use this latest release, which also addresses potential misalignment issues."
        },
        {
            "title": "D Used Prompts",
            "content": "Zero-shot inference, supervised fine-tuning (SFT), and instruction tuning (IT) employ the following template: You are presented with video in which the speaker says: <context>. Based on the textual, visual, and audio content, what is the <dimension> of this speaker? The candidate labels for <dimension> are: <the list of labels>. Respond in the following format: <dimension>: label. Only one label should be provided. Here, <context> denotes the the speakers utterance, while <dimension> refers to one of the six evaluation dimensions described in Section 3.1. The placeholder <the list of labels> corresponds to the specific label set for each dimension (cf. Appendix B). In SFT, the ground-truth dimension and label are provided for supervised training. In IT, neither the queried dimension nor its label set is fixed to single dataset, unlike in SFT."
        },
        {
            "title": "E Detailed Experimental Results",
            "content": "Due to space constraints, the main paper presents only subset of the results. Here, we provide the complete results for zero-shot inference, supervised fine-tuning, and instruction-tuning across six evaluation metrics (ACC, WF1, WP, F1, P, R) in Table 2, Table 3, and Table 4. For both LLMs and MLLMs, the best and second-best results are highlighted in bold and underline, respectively. The results align with the discussions and conclusions in the main paper. For zero-shot inference, multimodal models with 72B parameters achieve the best overall performance across all datasets. However, when comparing LLMs and MLLMs of the same scale, LLMs often exhibit competitive or even superior performance. After supervised fine-tuning, MLLMs show significant improvements and surpass LLMs on almost all datasets across all six metrics, underscoring the importance of incorporating non-verbal modalities for cognitively demanding tasks. After instruction-tuning, both the 7B and 72B MLLMs achieve excellent performance on all tasks, with results comparable to or better than those from supervised fine-tuning, indicating the potential of smallscale MLLMs to solve multiple tasks simultaneously. Moreover, under this evaluation protocol, MLLMs also outperform LLMs, further confirming the benefit of leveraging non-verbal modalities. We also evaluate one powerful closed-source MLLM, GPT-4o. Specifically, we use OpenAIs GPT-4o API for zero-shot inference with the same prompts as in Appendix D. During inference, we find that GPT-4o can be overly cautious with certain videos. For example, it sometimes fails to select label from the candidate list and instead outputs responses like: Im unable to determine the dimension based on the given information. To address this, we iteratively modify the prompts based on such outputs until GPT-4o consistently chooses label from the list. The final results, shown in Table 2, demonstrate that GPT-4o achieves the best or second-best performance on most metrics across datasets, highlighting its effectiveness on this challenging task. Details of the hyperparameters used for supervised fine-tuning (SFT) and instruction-tuning (IT) of all LLMs and MLLMs are provided in Table 5, Table 6, Table 7, and Table 8. 16 Table 2: Full experimental results on the MMLA benchmark using zero-shot inference."
        },
        {
            "title": "Models",
            "content": "Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B GPT-4o Datasets Models Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B GPT-4o Datasets Models Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-ov-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B GPT-4o Datasets Models Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-ov-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B GPT-4o L L s s M L L M M M"
        },
        {
            "title": "MIntRec",
            "content": "MIntRec2."
        },
        {
            "title": "MELD",
            "content": "ACC WF1 WP F1 ACC WF1 WP R ACC WF1 WP F1 18.88 15.08 26.81 23.60 18.60 26.71 33.26 24.79 29.97 37.98 33.62 42.04 56.18 56.06 62.22 49.44 48.57 55.22 50.56 50.53 58.96 56.63 51.99 62.51 44.94 39.71 51.18 56.40 56.22 62.26 55.06 53.75 61.36 52.36 51.09 61.91 41.35 43.72 60.00 61.35 60.89 65.44 62.70 62.42 68.83 62.92 62.46 67.96 63.37 63.68 69.69 9.03 10.03 16.06 15.08 16.06 25.55 17.29 21.16 21.48 30.99 35.60 36.84 50.16 53.18 54.36 41.16 43.98 47.12 44.57 46.70 49.88 43.92 47.92 61.03 33.01 38.22 45.75 51.00 52.43 56.43 45.63 49.25 54.33 43.95 47.07 54.02 37.20 35.27 54.39 55.47 60.14 57.55 55.78 59.27 60.93 55.88 59.17 59.89 58.37 61.46 62.13 6.79 4.08 11.33 13.63 9.93 23.98 20.31 16.59 34.48 28.58 26.34 42.81 36.25 36.08 43.31 28.92 28.50 43.77 34.58 34.96 43.26 37.28 34.15 43.54 27.84 24.32 39.82 35.27 34.59 44.32 34.33 32.93 37.79 32.02 31.47 43.18 21.10 24.62 41.75 39.50 39.91 51.32 40.88 40.28 48.25 43.78 43.12 48.60 42.32 43.60 52. 3.15 5.49 9.48 9.56 12.98 22.73 15.36 18.79 28.24 23.27 26.76 34.87 32.37 35.73 37.11 24.58 27.96 35.30 31.17 35.29 35.28 31.33 35.90 40.50 20.61 21.22 36.53 31.24 33.91 38.54 29.74 33.56 32.84 27.47 28.91 39.11 21.51 19.08 36.26 36.25 39.84 42.64 36.18 41.36 41.49 38.80 42.63 42.97 37.49 42.38 42.01 21.03 12.89 52.20 22.34 18.60 47.81 43.56 45.35 51.16 38.24 38.56 59.46 55.67 57.96 63.46 38.20 41.47 59.17 39.04 40.39 61.66 56.36 56.30 59.67 55.06 53.59 57.71 60.31 56.55 59.70 42.38 41.14 59.14 34.18 30.66 60.44 31.03 36.70 62.26 63.18 60.06 63.23 40.23 37.37 62.94 41.30 39.68 63.44 51.17 52.80 61.45 11.79 19.00 37.51 18.30 23.33 25.34 30.97 33.85 32.05 30.60 34.59 34.35 40.16 40.87 41.76 31.95 34.20 34.59 34.32 37.46 37.80 37.19 37.45 41.18 32.94 32.92 39.10 35.91 32.62 47.34 32.02 36.64 41.97 26.15 32.63 42.68 28.64 27.00 38.88 41.17 37.72 50.21 33.69 42.73 35.99 34.59 40.73 41.25 21.32 23.99 21. IEMOCAP MELD-DA IEMOCAP-DA ACC WF1 WP F1 P ACC WF1 WP F1 ACC WF1 WP F1 24.91 12.40 41.94 24.17 23.21 41.64 33.79 29.16 49.46 35.02 35.47 42.31 39.27 37.35 48.95 37.73 39.01 43.55 35.82 36.93 44.32 38.84 35.17 50.73 38.29 33.38 50.01 32.24 24.16 58.41 41.18 36.63 51.15 39.09 35.20 48.97 31.94 33.12 52.09 38.29 34.03 55.34 43.53 41.81 48.96 43.46 41.59 49.79 45.81 43.38 55.47 9.53 16.82 30.62 19.17 21.45 35.61 24.17 26.56 41.62 29.34 28.98 35.67 31.11 31.54 41.75 32.77 31.41 36.67 31.29 29.65 38.00 29.87 32.29 41. 31.72 35.24 50.93 23.65 27.61 59.96 30.20 35.50 42.69 33.19 40.93 47.03 25.99 23.91 46.01 33.33 34.00 55.48 35.32 38.44 41.19 34.67 35.95 43.05 40.26 40.77 58.40 4.75 2.06 8.00 11.41 8.74 13.69 31.43 27.45 37.87 33.08 33.65 45.47 35.84 35.77 46.81 42.09 40.19 44.32 39.34 38.96 47.48 46.30 41.39 46.93 40.59 38.65 39.14 34.78 32.42 47.01 29.33 24.30 46.13 31.33 26.16 34.38 27.93 24.93 42.23 38.59 34.58 53.49 50.25 47.22 50.85 48.95 47.10 55.59 49.00 46.50 55.90 4.31 8.87 7.28 9.86 13.01 12.93 21.87 27.10 25.37 25.61 34.62 28.06 29.72 35.78 33.05 30.59 36.56 32.96 29.07 39.52 29.23 26.61 30.31 37.00 26.21 33.05 24.45 27.83 33.30 33.21 26.86 38.42 35.81 26.83 32.78 29.06 25.01 33.40 29.05 34.40 40.22 41.47 38.62 39.79 45.22 37.64 41.78 47.71 36.70 46.24 40.77 2.65 2.92 19.23 18.21 13.09 16.81 38.32 32.71 35.87 33.92 34.88 44.73 41.08 40.13 50.25 43.58 42.09 52.59 40.34 40.59 53.26 44.59 40.68 52. 34.50 32.14 39.03 38.75 32.82 38.68 37.00 30.57 40.89 38.75 30.89 41.71 33.17 31.05 38.79 44.32 36.16 55.51 52.87 50.11 54.29 52.71 50.64 55.37 53.56 49.99 54.47 4.49 13.73 9.53 10.74 18.45 12.60 21.93 26.21 22.16 21.81 31.54 23.71 28.92 33.65 32.85 30.30 34.17 35.45 27.33 34.76 30.47 25.65 29.36 38.94 21.94 33.61 22.82 27.74 31.57 32.99 25.47 34.41 30.04 26.44 29.89 37.88 23.13 28.11 24.25 34.17 35.85 45.28 36.84 38.05 42.04 35.60 38.68 38.01 33.80 40.01 34.59 MOSI CH-SIMS v2.0 UR-FUNNY-v ACC WF1 WP F1 ACC WF1 WP R ACC WF1 WP F1 61.22 67.27 74.76 70.99 72.57 78.10 80.17 81.50 84.15 81.20 82.52 86.63 81.92 85.52 89.49 81.63 82.63 86.04 84.26 84.72 86.44 84.55 86.28 88.26 79.74 79.44 81.83 86.59 86.54 87.03 84.11 84.22 85.20 87.17 87.49 87.86 79.74 82.72 86.37 88.05 88.24 88.53 86.44 86.50 86.59 88.48 88.60 88.78 87.32 87.22 88.36 44.83 40.79 49.84 48.36 47.23 52.15 54.35 53.52 56.06 54.98 54.03 57.81 57.01 54.60 59.66 55.07 54.33 57.42 56.46 56.10 57.67 57.51 56.34 58.86 79.47 79.88 81.74 86.53 86.52 87.07 56.16 56.13 56.76 58.32 58.10 58.58 55.14 53.12 57.61 58.83 58.71 59.00 57.66 57.62 57.73 59.07 58.97 59.20 87.20 87.22 88.43 27.47 25.21 68.65 65.28 61.12 58.60 65.18 66.94 69.64 66.73 65.28 64.29 62.48 66.30 74.48 60.35 64.85 70.21 67.41 68.18 69.20 62.48 65.34 71.11 51.90 53.52 69.98 73.91 69.30 65.27 57.25 61.09 68.24 67.58 66.94 68.37 60.91 62.76 64.98 72.45 68.26 66.37 66.60 67.81 70.24 65.34 67.89 71.98 67.80 71.30 79. 25.24 38.77 54.10 34.55 36.63 33.36 43.02 43.75 43.45 39.44 39.79 39.78 43.55 46.37 46.20 41.62 39.39 44.38 57.49 57.75 57.56 42.53 44.26 44.11 30.67 30.30 39.06 52.30 55.87 49.20 34.65 32.76 38.30 38.01 38.70 38.49 35.64 34.70 36.78 38.65 41.41 37.27 38.45 38.00 39.58 38.57 37.36 40.62 47.40 51.04 49.76 45.27 47.11 49.27 55.43 55.08 55.48 62.37 61.23 63.93 66.80 66.79 66.80 57.95 50.23 69.23 63.98 66.15 68.50 67.10 67.74 68.82 61.67 58.30 66.50 64.29 61.91 68.43 57.65 50.96 65.42 59.46 54.68 65.38 56.04 46.41 68.82 57.55 55.67 64.98 68.01 67.03 70.06 65.19 62.73 69.99 63.68 60.13 70.39 73.11 73.11 73.12 31.42 30.21 32.84 55.02 55.31 55.48 40.75 41.42 42.66 66.78 66.78 66.80 33.30 38.26 46.29 44.08 42.63 45.66 45.17 44.78 45.85 58.13 61.27 66.62 61.77 63.94 68.55 50.70 57.13 65.57 54.47 59.00 65.51 46.09 55.44 69.03 36.99 38.11 43.39 66.95 67.77 70.15 62.59 64.83 70.13 59.96 63.27 70.55 73.11 73.11 73. MUStARD Anno-MI (client) Anno-MI (therapist) ACC WF1 WP F1 P ACC WF1 WP F1 ACC WF1 WP F1 50.00 33.50 25.18 51.45 37.61 62.87 52.90 52.90 52.90 57.25 52.16 62.61 57.25 56.45 58.63 52.90 54.19 62.94 54.35 54.72 60.77 61.59 61.35 61.90 60.87 60.86 60.88 56.52 55.93 56.89 51.45 47.69 52.03 57.97 51.02 68.45 60.87 59.71 69.01 63.04 62.33 64.11 53.62 45.24 59.35 57.97 53.68 62.66 55.07 44.60 70.80 22.33 33.33 16.79 37.61 51.45 62.87 52.90 52.90 52.90 52.16 57.25 62.61 37.63 38.16 39.09 36.13 35.27 41.96 36.48 36.23 40.51 61.35 61.59 61. 60.86 60.87 60.88 55.93 56.52 56.89 47.69 51.45 52.03 51.02 57.97 68.45 39.81 40.58 46.01 62.33 63.04 64.11 45.24 53.62 59.35 53.68 57.97 62.66 44.60 55.07 70.80 57.04 49.22 49.63 28.34 30.10 45.86 36.67 40.45 52.43 43.93 46.77 52.03 49.87 49.25 54.96 47.65 49.98 52.92 32.42 34.66 52.83 58.99 51.52 51.63 20.29 16.67 51.14 31.38 30.97 54.69 38.92 42.33 49.41 16.58 10.84 46.19 27.64 28.56 55.50 53.90 47.65 52.17 48.58 49.00 50.99 44.33 45.17 50.79 28.90 26.75 54.96 25.14 27.58 31.72 26.30 34.79 34.64 25.72 28.48 30.84 28.33 26.70 32.97 30.44 32.99 34.63 30.49 29.64 31.91 24.10 25.99 30.30 35.99 38.53 41.16 18.43 35.57 41.69 25.15 38.30 43.39 34.90 36.00 37.78 15.24 32.56 35.46 18.85 27.65 33.13 34.81 37.06 43.62 30.02 30.62 31.21 27.10 29.59 30.21 29.63 42.76 43.46 25.70 25.55 27.59 22.19 22.89 30.10 40.18 37.14 47.12 43.60 43.35 49.07 54.21 52.63 66.77 35.53 28.09 48.90 57.98 58.53 67.16 38.86 33.63 36. 35.71 35.37 40.14 49.69 46.81 55.70 47.06 37.98 32.12 43.90 35.63 59.17 48.25 41.87 57.49 49.43 46.40 62.68 61.11 61.33 65.21 61.81 60.93 62.11 55.05 53.62 61.56 18.76 19.27 19.71 16.97 16.70 21.86 37.73 41.17 45.99 34.43 34.98 38.05 42.22 43.93 51.52 25.00 31.07 40.06 47.06 47.51 51.74 35.64 41.57 36.28 37.00 38.85 40.91 40.26 43.27 44.76 34.18 42.12 28.99 31.81 40.47 43.79 37.06 42.25 45.65 37.64 42.58 46.81 48.24 48.59 50.86 47.56 47.94 49.34 43.56 46.24 47.08 17 Table 3: Full experimental results on the MMLA benchmark using supervised fine-tuning."
        },
        {
            "title": "Models",
            "content": "Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Datasets Models Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Datasets Models Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Datasets Models Qwen2-0.5B Llama-3.2-1B Qwen2-1.5B Llama-3.2-3B Qwen2-7B Llama-3-8B Llama-3.1-8B Internlm-2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B L L s s M L L M M M"
        },
        {
            "title": "MIntRec",
            "content": "MIntRec2."
        },
        {
            "title": "MELD",
            "content": "ACC WF1 WP F1 ACC WF1 WP F1 ACC WF1 WP F1 70.11 70.01 71.37 73.26 73.45 73.93 74.61 74.71 75.88 77.30 77.17 77.97 74.61 75.10 77.87 77.30 77.26 77.99 74.61 74.67 76.73 77.30 76.41 76.57 79.33 78.82 78.98 80.45 80.28 80.48 80.00 79.71 80.85 80.00 80.02 80.97 79.10 78.88 79.78 81.35 81.00 81.78 82.47 82.41 83.34 84.04 84.00 84.94 66.97 66.90 68.97 71.19 71.30 71.50 72.85 72.30 74.96 74.40 73.04 77.02 69.90 70.26 71.89 74.40 73.82 75.95 71.99 73.13 73.04 72.32 72.28 74.01 76.29 76.45 77.07 78.55 78.32 79.22 77.89 77.87 79.87 78.05 78.03 79.39 77.44 79.70 76.29 75.30 73.97 78.59 79.19 79.33 80.69 81.70 81.62 83.04 55.83 55.04 55.74 59.37 59.06 60.63 58.39 57.78 59.06 60.45 59.74 60.27 61.49 60.91 62.70 62.91 62.58 63.70 62.86 62.40 63.28 61.83 61.12 62.06 65.03 64.64 65.10 64.19 63.51 64.86 63.11 62.93 63.66 63.60 63.04 63.98 63.99 63.28 66.15 67.39 66.98 68.34 65.81 65.82 66.23 66.01 66.15 66. 48.08 47.03 51.22 53.49 53.17 56.33 52.74 51.82 56.25 55.28 56.52 56.07 53.72 54.29 56.61 57.03 57.38 58.91 57.12 57.52 58.97 57.14 56.87 60.00 58.29 57.08 61.09 59.68 58.64 63.51 57.19 56.49 59.28 56.44 55.36 59.83 57.43 58.05 62.21 63.65 63.12 67.04 62.38 61.70 63.69 61.01 60.36 62.51 63.95 62.73 62.15 61.57 60.84 60.83 64.71 63.74 63.57 64.41 63.40 63.11 65.71 65.11 65.12 64.41 63.84 63.64 65.02 64.16 63.95 66.25 65.18 65.11 63.79 63.17 62.80 67.09 65.41 65.75 63.49 62.44 62.44 62.72 62.90 63.85 68.05 66.26 66.92 68.81 66.68 68.01 60.65 61.43 64.27 61.00 61.79 65.06 45.04 43.43 47.30 44.91 43.99 47.90 47.16 45.96 49.89 47.59 46.17 50.20 50.86 49.28 54.90 48.45 47.50 50.29 49.40 47.84 52.27 48.78 46.66 55.56 43.27 42.21 44.81 51.45 48.06 57.26 40.67 38.90 46.05 43.90 43.92 46.17 48.83 46.85 58.22 53.43 49.27 61.81 48.88 50.79 49.22 49.94 50.86 51. IEMOCAP MELD-DA IEMOCAP-DA ACC WF1 WP F1 ACC WF1 WP F1 ACC WF1 WP F1 46.61 45.90 47.25 49.08 48.46 49.38 49.88 49.46 51.22 50.00 49.24 51.04 52.10 50.96 52.20 51.42 50.32 51.84 52.47 52.04 53.45 38.41 34.18 51.24 56.84 56.93 59.16 55.12 55.08 56.27 58.94 59.01 60.17 58.82 58.88 60.01 56.47 56.64 57.94 56.97 56.50 59.84 60.17 60.10 62.21 61.53 61.36 63.89 43.61 43.44 46.54 45.99 46.46 47.64 47.61 48.31 49.21 47.28 48.28 48.89 48.31 49.84 49.85 47.92 49.18 49.48 50.44 51.00 52.08 28.64 31.25 42. 47.63 47.75 49.62 45.99 45.74 47.40 49.64 48.76 51.36 49.41 49.18 50.47 56.19 57.39 56.35 54.76 55.65 58.50 58.87 58.26 61.68 60.07 59.52 63.45 57.31 56.11 56.26 58.36 57.25 57.21 57.21 56.62 56.66 58.71 57.36 56.90 57.71 56.98 57.17 57.21 56.74 57.29 56.61 55.30 55.59 58.81 58.00 58.45 61.01 60.11 59.86 60.91 60.15 61.26 61.01 60.09 60.98 59.81 59.16 59.36 63.01 61.59 62.23 58.86 55.39 59.85 61.11 60.59 60.47 61.26 60.73 61.55 48.98 47.79 53.58 50.09 48.81 53.16 49.56 47.74 54.17 49.70 47.80 53.59 49.25 46.87 53.22 48.63 47.05 51.35 47.78 45.51 51.87 50.22 50.22 52.28 47.67 46.23 50.71 49.70 49.13 57.26 46.78 43.80 54.29 48.63 47.51 52.21 54.06 51.00 61.48 47.99 48.98 55.37 52.71 51.58 54.80 53.37 52.10 56.75 68.21 68.00 68.84 69.11 68.90 69.39 67.14 66.42 67.78 71.39 70.95 71.31 72.08 72.00 72.94 73.41 73.14 73.74 74.31 74.06 74.60 72.08 71.80 72. 76.43 76.28 76.58 68.10 67.09 68.72 72.29 71.92 73.77 72.82 72.36 73.25 74.95 74.90 75.64 73.14 72.38 74.88 75.53 75.31 76.09 73.62 73.36 74.50 65.01 61.75 75.87 63.55 62.00 66.01 58.91 60.83 58.58 67.71 68.66 67.62 65.16 65.36 65.69 71.05 69.17 74.16 72.09 70.46 74.33 68.57 68.65 69.44 68.39 66.62 70.65 59.85 56.91 66.75 68.19 64.93 73.72 68.83 66.86 72.13 71.68 71.88 73.70 72.22 70.81 75.80 72.25 69.93 76.95 71.86 69.74 76.10 MOSI CH-SIMS v2.0 UR-FUNNY-v ACC WF1 WP F1 ACC WF1 WP F1 ACC WF1 WP F1 82.94 82.90 83.44 82.80 82.79 82.95 85.42 85.42 85.43 86.44 86.44 86.46 87.32 87.32 87.32 86.73 86.74 86.74 87.61 87.61 87.65 84.99 84.95 85.42 87.03 87.02 87.17 88.34 88.34 88.34 88.48 88.48 88.49 88.92 88.92 88.92 84.26 84.10 85.87 87.03 86.98 87.63 87.76 87.76 87.76 88.92 88.92 88.92 82.91 83.01 83.40 82.79 82.83 82.92 85.42 85.41 85.44 86.44 86.43 86.46 87.32 87.31 87.32 86.73 86.74 86.74 87.61 87.63 87.64 84.96 85.05 85.37 87.02 87.06 87.15 88.34 88.33 88.34 88.48 88.49 88.48 88.92 88.92 88.92 84.12 84.38 85.78 86.99 87.10 87.58 87.75 87.76 87.76 88.92 88.92 88.92 68.96 68.61 68.29 64.70 65.00 65.70 69.34 69.49 69.77 66.25 66.09 65.94 73.11 71.71 71.36 70.41 68.67 67.83 68.47 69.17 69.98 70.21 70.78 71.70 74.98 74.82 75.12 74.49 69.71 66.19 70.50 71.18 72.00 72.83 72.82 72.82 76.24 76.68 77.47 75.56 70.95 68.27 72.74 74.54 77.28 72.35 74.17 77. 57.73 57.63 57.90 54.54 54.57 54.88 59.36 59.39 59.44 55.19 55.18 55.22 61.41 60.62 64.51 56.22 56.31 57.73 58.90 59.21 58.85 60.44 60.73 60.61 64.46 64.24 65.04 52.69 55.99 50.27 60.88 61.21 60.83 63.20 63.17 63.23 67.11 67.44 67.17 53.60 57.52 51.22 64.91 66.48 65.27 65.03 66.91 65.28 65.59 65.54 65.63 69.62 69.60 69.72 68.81 68.81 68.85 71.43 71.43 71.46 71.33 71.33 71.35 72.23 72.20 72.44 72.64 72.60 72.83 54.43 43.86 65.01 74.75 74.75 74.77 74.04 73.99 74.17 73.64 73.60 73.73 74.65 74.64 74.66 75.45 75.26 76.50 76.36 75.78 79.54 76.26 76.21 76.56 77.16 77.10 77.59 65.51 65.54 65.64 69.61 69.65 69.69 68.81 68.83 68.84 71.43 71.44 71.44 71.33 71.34 71.34 72.21 72.29 72.41 72.61 72.69 72.80 43.51 53.82 65.18 74.75 74.76 74.76 73.97 73.98 74.19 73.58 73.59 73.75 74.62 74.62 74.67 75.28 75.58 76.43 75.83 76.58 79.40 76.23 76.32 76.52 77.12 77.24 77. MUStARD Anno-MI (client) Anno-MI (therapist) ACC WF1 WP F1 ACC WF1 WP F1 ACC WF1 WP F1 60.87 60.87 60.84 65.94 65.94 65.53 63.04 63.04 63.03 63.04 62.60 63.69 63.04 63.04 63.05 64.49 64.48 64.52 60.14 59.67 60.65 68.12 68.11 68.13 71.01 71.01 71.03 67.39 66.28 70.02 70.29 70.02 71.04 63.04 62.89 63.27 68.84 68.84 68.84 74.64 74.60 74.77 71.74 71.62 72.12 69.57 69.57 69.40 60.84 60.87 60.91 65.53 65.94 66.73 63.03 63.04 63.07 62.6 63.04 63.69 63.04 63.04 63.05 64.48 64.49 64.52 59.67 60.14 60.65 68.11 68.12 68. 71.01 71.01 71.03 66.28 67.39 70.02 70.02 70.29 71.04 62.89 63.04 63.27 68.84 68.84 68.84 74.60 74.64 74.77 71.62 71.74 72.12 69.40 69.57 69.40 62.98 60.96 61.87 61.29 55.87 58.20 62.89 58.90 61.09 64.75 60.16 64.96 64.57 60.88 63.18 65.28 60.70 64.70 39.15 41.84 53.19 62.71 59.50 61.19 64.54 61.97 62.87 65.98 63.03 65.44 65.69 63.65 64.60 65.34 64.21 64.63 64.66 60.70 64.41 65.53 59.57 70.30 67.46 65.90 66.55 67.73 65.99 67.40 52.69 51.37 56.95 42.10 42.98 54.74 48.33 46.96 58.27 50.28 48.38 65.06 50.56 48.87 61.41 49.47 48.09 64.33 37.18 40.87 41.38 49.83 48.39 56.72 53.17 51.27 59.64 54.54 52.38 62.88 56.35 54.27 61.86 57.66 56.63 60.19 51.87 49.69 63.91 48.68 47.21 73.38 59.10 57.12 63.52 59.71 58.14 64.43 73.33 73.33 74.51 74.47 74.50 75.17 74.30 74.44 75.28 73.33 73.71 75.93 75.53 75.80 77.49 76.05 76.41 77.98 75.61 76.03 77.85 73.42 73.78 76. 76.25 76.25 76.34 75.73 75.78 75.97 75.68 75.80 76.76 76.82 77.15 78.80 75.96 76.17 78.05 77.60 77.90 79.56 76.65 76.70 77.38 77.79 77.95 78.74 71.34 71.47 72.47 72.77 73.04 73.13 72.91 73.50 72.96 72.48 73.46 73.04 74.67 75.39 75.16 75.48 76.56 75.50 74.83 75.68 75.15 72.50 73.03 73.56 75.12 74.95 75.37 74.50 74.72 74.42 74.92 75.93 74.69 76.11 77.39 76.11 74.99 75.57 75.87 61.55 62.34 61.67 75.66 76.60 75.39 77.03 78.05 76.71 18 Table 4: Full experimental results on the MMLA benchmark using instruction tuning."
        },
        {
            "title": "MIntRec",
            "content": "MIntRec2.0 ACC WF1 WP F1 ACC WF1 WP F1 s"
        },
        {
            "title": "M\nL\nL",
            "content": "Qwen2-7B Llama-3-8B Internlm-2.5-7B Qwen2-VL-7B M MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 77.53 77.78 79.08 78.20 78.14 79.94 75.96 76.06 78.46 82.92 82.79 83.78 80.67 80.56 82.19 86.29 86.09 86.75 80.22 79.94 80.63 74.66 75.42 75.57 76.62 76.46 79.43 73.11 71.81 76. 80.44 81.08 81.53 77.75 78.80 79.70 85.17 84.64 86.76 77.77 76.97 80.29 61.49 61.07 62.56 61.44 61.33 63.12 60.06 59.35 62.27 64.19 63.31 64.39 63.31 61.78 65.75 66.99 66.63 67.45 64.98 64.72 65.40 57.48 56.55 61.02 57.05 56.28 60.05 54.13 52.69 58.83 59.96 59.06 63.31 55.88 55.64 65.71 62.96 62.05 65.24 60.86 59.97 63."
        },
        {
            "title": "IEMOCAP",
            "content": "ACC WF1 WP F1 ACC WF1 WP F1 s"
        },
        {
            "title": "M\nL\nL",
            "content": "Qwen2-7B Llama-3-8B Internlm-2.5-7B Qwen2-VL-7B M MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 64.64 64.09 65.15 66.02 64.79 65.19 62.84 62.93 63.71 67.55 66.15 66.26 65.86 65.62 68.69 67.59 65.08 66.67 65.36 65.24 65.49 48.83 48.05 52.81 49.86 48.77 53.82 49.76 49.90 51. 51.78 48.60 57.86 48.80 50.39 56.36 50.71 45.59 61.56 51.93 51.18 53.78 48.09 47.00 52.49 52.34 51.28 56.88 50.80 50.21 54.10 49.88 49.10 51.13 53.58 51.91 61.66 54.07 53.33 57.92 59.93 59.86 60.16 45.53 47.99 49.46 51.04 53.07 54.89 49.56 50.75 52.31 46.98 45.82 51.21 52.24 55.18 58.79 50.95 52.81 56.19 58.63 58.40 59."
        },
        {
            "title": "Models",
            "content": "MELD-DA IEMOCAP-DA ACC WF1 WP F1 ACC WF1 WP F1 s"
        },
        {
            "title": "M\nL\nL",
            "content": "Qwen2-7B Llama-3-8B Internlm-2.5-7B Qwen2-VL-7B M MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 58.71 57.46 57.54 58.91 57.44 57.26 57.51 55.80 54.94 60.71 59.00 61.08 61.71 59.00 59.71 62.36 60.80 61.72 61.46 60.55 60.24 48.81 48.48 52.23 47.53 45.89 52.29 45.16 44.79 47. 51.98 51.43 56.90 48.10 45.12 56.61 51.20 51.31 53.38 50.34 49.48 52.07 65.50 64.91 66.69 69.06 68.36 69.38 67.57 66.91 67.60 62.47 61.04 64.42 68.15 66.15 70.01 72.66 72.02 73.52 76.06 75.96 76.22 60.28 63.67 63.60 62.65 64.99 64.40 59.69 61.16 61.67 57.80 59.33 60.50 62.29 61.48 66.88 66.68 67.44 67.34 73.17 73.94 73."
        },
        {
            "title": "MOSI",
            "content": "CH-SIMS v2.0 ACC WF1 WP F1 ACC WF1 WP F1 s"
        },
        {
            "title": "M\nL\nL",
            "content": "Qwen2-7B Llama-3-8B Internlm-2.5-7B Qwen2-VL-7B M MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 87.17 87.17 87.20 87.32 87.31 87.35 86.01 86.00 86.02 79.45 83.78 89.74 86.15 86.08 87.06 81.63 81.31 84.25 86.88 86.88 86.88 87.17 87.18 87.18 87.31 87.30 87.36 86.00 85.99 86. 55.82 52.89 59.86 86.09 86.24 86.99 81.34 81.79 84.14 86.88 86.88 86.88 70.50 69.59 68.88 69.54 70.04 70.62 69.63 70.06 70.69 75.85 73.05 73.07 79.15 74.99 79.88 72.65 68.26 67.52 70.69 71.64 73.20 58.09 57.97 58.69 60.50 60.86 60.32 59.45 59.57 59.58 59.54 59.82 65.70 58.95 61.00 81.29 51.56 55.53 50.45 61.49 62.12 61."
        },
        {
            "title": "Models",
            "content": "UR-FUNNY-v"
        },
        {
            "title": "MUStARD",
            "content": "ACC WF1 WP F1 ACC WF1 WP F1 s"
        },
        {
            "title": "M\nL\nL",
            "content": "Qwen2-7B Llama-3-8B Internlm-2.5-7B Qwen2-VL-7B M MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 70.22 70.22 70.22 72.13 72.12 72.15 69.11 68.99 69.57 73.34 73.28 73.46 74.14 74.15 74.15 76.96 76.35 80.48 74.25 74.14 74.54 70.21 70.21 70.22 72.10 72.10 72.16 69.01 69.21 69. 73.26 73.28 73.48 74.14 74.15 74.14 76.40 77.19 80.33 74.11 74.16 74.57 63.04 61.34 65.84 57.97 55.25 60.53 65.94 64.59 68.82 65.22 65.21 65.23 60.87 60.84 60.91 70.29 68.49 76.31 71.74 71.62 72.12 61.34 63.04 65.84 55.25 57.97 60.53 64.59 65.94 68.82 65.21 65.22 65.23 60.84 60.87 60.91 68.49 70.29 76.31 71.62 71.74 72."
        },
        {
            "title": "Models",
            "content": "Anno-MI (client) Anno-MI (therapist) ACC WF1 WP F1 ACC WF1 WP F1 s"
        },
        {
            "title": "M\nL\nL",
            "content": "Qwen2-7B Llama-3-8B Internlm-2.5-7B Qwen2-VL-7B M MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 64.04 61.00 62.87 66.16 61.34 68.87 62.18 54.40 64.20 64.54 60.34 62.21 64.48 57.16 71.31 64.89 57.99 72.12 66.93 65.06 66.28 52.00 50.43 58.88 52.60 50.38 69.72 40.80 41.92 67. 48.41 47.60 58.61 42.60 43.75 78.95 46.01 45.35 76.80 58.04 56.09 63.26 72.28 71.61 74.19 75.26 75.40 76.26 71.23 71.42 73.42 76.44 76.65 77.58 75.18 75.27 77.83 76.93 76.82 77.73 77.61 77.64 78.05 69.37 69.89 72.78 74.21 75.00 74.06 69.54 69.25 71.73 75.14 75.26 75.68 74.00 74.96 75.30 75.46 75.88 76.11 76.65 77.29 76.39 Table 5: Primary hyperparameters for SFT and IT on the MIntRec, MIntRec2.0, and MELD datasets."
        },
        {
            "title": "Epochs Rank",
            "content": "2 2 2 1 8 4 6 1 4 1 1 1 1 1 8 1 2 1 2 2 2 1 16 6 6 1 4 1 1 1 1 1 8 1 2 1 2 2 2 1 16 4 6 1 1 4 1 1 1 1 1 8 1 2 8 8 8 10 60 10 15 8 20 5 5 8 8 8 5 5 3 3 8 8 8 10 30 5 10 8 20 5 5 8 8 8 5 5 3 3 8 8 8 10 30 8 10 10 8 10 5 5 8 8 8 5 5 3 8 8 8 128 8 128 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 128 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 64 128 16 16 8 8 8 16 16 16 64 16 64 R M 0 . 2 t M"
        },
        {
            "title": "D\nL\nE\nM",
            "content": "Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B VideoChat2-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 0.0001 0.0001 0.0001 0.00002 0.00011 0.0002 0.0001 0.0001 0.0001 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.00012 0.0002 0.0001 0.0001 0.0001 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.00011 0.0001 0.0001 0.00001 0.0001 0.00011 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0."
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT 0.1 0.1 0.1 0.5 0.1 0.3 0.1 0.05 0.1 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.3 0.3 0.1 0.05 0.1 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.1 0.2 0.1 0 0.05 0.4 0.3 0. 0.1 0.1 0.1 0.3 0.05 0.3 0.3 20 α 16 16 16 256 16 256 256 32 16 16 16 32 32 32 128 32 128 16 16 16 16 256 16 256 256 32 16 16 32 32 32 128 32 128 16 16 16 16 256 16 128 256 32 32 16 16 16 32 32 32 128 32 128 16 Table 6: Primary hyperparameters for SFT and IT on the IEMOCAP, MELD-DA, and IEMOCAP-DA datasets."
        },
        {
            "title": "Epochs Rank",
            "content": "2 2 2 1 2 6 6 1 2 1 1 1 1 1 8 1 2 1 2 2 2 1 8 4 6 1 4 1 1 1 1 1 8 1 2 1 2 2 2 1 12 4 4 1 8 1 1 1 1 1 8 1 2 8 8 8 10 15 8 10 8 7 5 5 8 8 8 5 5 3 3 8 8 8 10 50 8 10 8 10 5 5 8 8 8 5 5 3 3 8 8 8 10 50 8 10 8 5 5 5 8 8 8 5 5 3 8 8 8 128 8 128 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 64 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 64 128 16 8 8 8 16 16 16 64 16 64 Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 0.0001 0.0001 0.0001 0.00002 0.0001 0.0002 0.0001 0.0001 0.0001 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.0003 0.0001 0.0001 0.0001 0.00011 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.0003 0.0001 0.0001 0.0001 0.0003 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0."
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT O -"
        },
        {
            "title": "A\nD\nD\nL\nE\nM",
            "content": "A - O 0.1 0.1 0.1 0.5 0.2 0.2 0.1 0.05 0.2 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.1 0.2 0.1 0.05 0.5 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.1 0.2 0.1 0.05 0.1 0.3 0. 0.1 0.1 0.1 0.3 0.05 0.3 0.3 21 α 16 16 16 256 16 256 256 32 16 16 16 32 32 32 128 32 128 16 16 16 16 256 16 128 256 32 16 16 32 32 32 128 32 128 16 16 16 16 256 16 128 256 32 16 16 16 32 32 32 128 32 128 16 Table 7: Primary hyperparameters for SFT and IT on the MOSI, CH-SIMS v2.0, and UR-FUNNY-v2 datasets."
        },
        {
            "title": "Epochs Rank",
            "content": "2 2 2 1 8 6 6 1 2 1 1 1 1 1 8 1 2 1 2 2 2 1 12 6 6 1 4 1 1 1 1 1 8 1 2 1 2 2 2 1 12 4 6 1 4 1 1 1 1 1 8 1 2 8 8 8 10 20 5 20 8 10 5 5 8 8 8 5 5 3 3 8 8 8 10 30 5 15 8 10 5 5 8 8 8 5 5 3 3 8 8 8 10 10 8 10 8 3 5 5 8 8 8 5 5 3 8 8 8 128 8 128 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 128 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 64 128 16 8 8 8 16 16 16 64 16 64 Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 0.0001 0.0001 0.0001 0.00002 0.0001 0.0002 0.0001 0.0001 0.0001 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.00011 0.0002 0.0001 0.0001 0.0001 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.001 0.0001 0.0001 0.0001 0.0003 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0."
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT M . 0 2 I - 2 - U - 0.1 0.1 0.1 0.5 0.1 0.3 0.1 0.05 0.2 0.3 0. 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.1 0.3 0.1 0.05 0.2 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.2 0.2 0.1 0.05 0.1 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 α 16 16 16 256 16 256 256 32 16 16 16 32 32 32 128 32 128 16 16 16 16 256 16 256 256 32 16 16 16 32 32 32 128 32 128 16 16 16 16 256 16 128 256 32 16 16 32 32 32 128 32 128 16 Table 8: Primary hyperparameters for SFT and IT on the MUStARD, Anno-MI (client), and Anno-MI (therapist) datasets."
        },
        {
            "title": "Epochs Rank",
            "content": "1 1 1 1 2 2 6 1 2 1 1 1 1 1 8 1 2 1 2 2 2 1 8 4 6 1 4 1 1 1 1 1 8 1 2 1 2 2 2 1 12 4 4 1 8 1 1 1 1 1 8 1 2 8 8 8 10 10 10 15 8 10 3 20 8 8 8 5 5 3 3 8 8 8 10 50 8 10 8 10 5 5 8 8 8 5 5 3 3 8 8 8 10 50 8 10 8 5 5 5 8 8 8 5 5 3 16 16 16 128 8 128 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 64 128 16 8 8 8 16 16 16 64 16 64 8 8 8 8 128 8 64 128 16 8 8 8 16 16 16 64 16 64 Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B Llama3-8B Qwen2-7B InternLM2.5-7B VideoLLaMA2-7B Qwen2-VL-7B LLaVA-Video-7B LLaVA-OV-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B LLaVA-OV-72B Llama3-8B Qwen2-7B InternLM2.5-7B Qwen2-VL-7B MiniCPM-V-2.6-8B Qwen2-VL-72B LLaVA-Video-72B 0.00005 0.00005 0.00005 0.00002 0.001 0.0002 0.0001 0.0001 0.0004 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.0003 0.0001 0.0001 0.0001 0.00011 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.00002 0.0003 0.0001 0.0001 0.0001 0.0003 0.0001 0.0001 0.00005 0.00005 0.00005 0.0001 0.0001 0.0001 0."
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "SFT",
            "content": "IT"
        },
        {
            "title": "D\nR\nA\nt\nS\nU\nM",
            "content": ") i ( - A ) p h ( - A 0.1 0.1 0.1 0.5 0.2 0.4 0.1 0.05 0.3 0.3 0.2 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.1 0.2 0.1 0.05 0.5 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0.3 0.1 0.1 0.1 0.5 0.1 0.2 0.1 0.05 0.1 0.3 0.3 0.1 0.1 0.1 0.3 0.05 0.3 0. 23 α 32 32 32 256 16 256 256 32 16 16 16 32 32 32 128 32 128 16 16 16 16 256 16 128 256 32 16 16 16 32 32 32 128 32 128 16 16 16 256 16 128 256 32 16 16 16 32 32 32 128 32"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University",
        "Kennesaw State University",
        "Pattern Recognition Center, WeChat AI, Tencent Inc, China"
    ]
}