{
    "paper_title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information",
    "authors": [
        "Chih-Kai Yang",
        "Neo Ho",
        "Yen-Ting Piao",
        "Hung-yi Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research."
        },
        {
            "title": "Start",
            "content": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information Chih-Kai Yang1, Neo Ho1, Yen-Ting Piao1, Hung-yi Lee1 1National Taiwan University, Taiwan chihkaiyang1124@gmail.com, hungyilee@ntu.edu.tw 5 2 0 2 9 1 ] . e [ 1 7 3 2 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audioprocessing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, benchmark assessing LALMs multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting fundamental challenge in multimodal reasoning. Our findings expose critical limitation in LALMs, offering insights and resources for future research. Index Terms: Large audio-language model, multi-hop reasoning, benchmark 1. Introduction Large language models (LLMs) [1, 2] have revolutionized AI research, extending beyond natural language processing to domain-specific applications like computer vision [3] and speech processing [4, 5]. This shift drives the rise of large multimodal models, e.g., large vision-language models (LVLMs) [6,7] and large audio-language models (LALMs) [713], equipping LLMs with cross-modal understanding. Research has also explored LLMs fundamental reasoning abilities [1416], with multi-hop reasoning, the skill of recalling and connecting multiple facts, recognized as key to complex reasoning [1720]. Particularly, latent multi-hop reasoning [19, 20], which involves multi-hop reasoning over internal parameterized knowledge stored in LLMs, is crucial for efficiently utilizing learned knowledge without heavily relying on external retrieval. Multi-hop reasoning datasets also provide insights into LLMs internal mechanism [21], underscoring the importance of studying this ability. Despite growing interest in reasoning, multi-hop reasoning of large audio-language models (LALMs) based on speech and audio remains unexplored. Existing benchmarks assess general speech and audio-processing performance [22, 23], conversational abilities [24, 25], and fairness/bias [26, 27], while studies on LALMs fundamental abilities [2830] remain limited and do not specifically address multi-hop reasoning. To address this gap, we introduce SAKURA (Speech and Audio-based Question-answering Benchmark for Multi-hop *Equal Contribution Reasoning of Large Audio-Language Models), novel benchmark for systematic evaluation of LALMs multi-hop reasoning using speech and audio information. SAKURA has four tracks based on speech/audio attributes including speaker gender, spoken language, speaker emotion, and animal sound. Each track includes single-hop and multi-hop sub-tracks, both comprising 500 multiple-choice questions. Single-hop questions require direct perception of an attribute (e.g., What is the animal in the sound?), whereas multi-hop questions demand further reasoning based on the extracted information (e.g., Which of the listed features aligns best with the physical appearance of the animal in the sound?). This design compels models to recall and integrate the extracted auditory cues when tackling multi-hop questions. Fig. 1 provides examples of single-hop and multi-hop reasoning paths. In total, SAKURA offers 4000 human-verified multiple-choice questions, providing not only structured framework for evaluating LALMs speech/audiobased multi-hop reasoning but also valuable resource for future research on their reasoning mechanisms. Through evaluating various LALMs [2, 713] on SAKURA, we find that LALMs struggle to recognize certain attributes and fail in multi-hop reasoning even when recognizing accurately. Moreover, while LALMs excel in reasoning based on text, they fail to integrate latent speech/audio representations into the reasoning process, contradicting expectations for end-to-end LALMs that should internally unify speech/audio understanding with LLM reasoning capabilities. Overall, our contributions are: (1) introducing SAKURA, the first benchmark for systematically evaluating LALMs multi-hop reasoning, and (2) uncovering LALMs difficulties in integrating speech/audio for multi-hop reasoning. Relevant resources such as dataset, evaluation code, etc., can be found at https://github.com/b08202033/SAKURA. 2. Dataset construction 2.1. Overview SAKURA evaluates LALMs multi-hop reasoning with speech/audio information across four tracks, covering fundamental attributes: speaker gender (Gender), spoken language (Language), speaker emotion (Emotion), and animal sounds (Animal), chosen for their significance in speech/audio processing. Each track includes single-hop and multi-hop sub-tracks. Here, we define the single-hop reasoning of LALMs as the direct perception and extraction of speech/audio attributes, forming the foundation for speech/audio-based multi-hop reasoning. In single-hop sub-tracks, models must extract an attribute directly from speech/audio input (e.g., identifying the animal sound). In multi-hop sub-tracks, models must use the extracted information for further reasoning (e.g., determining physical Figure 1: Overview of dataset construction and experiments. Question-answer pairs were generated by GPT-4o and manually verified. text-based reasoning, where text-based replaces auditory inputs with captions, LALMs were evaluated on speech/audio-based vs. assessing their ability to integrate auditory information. Icon source: https://www.flaticon.com trait that matches the identified animal). Each sub-track contains 500 multiple-choice questions, each consisting of (1) speech/audio input, (2) textual question with fixed number of choices, and (3) golden answer. The multiple-choice format was adopted to facilitate standardized assessment and comparison. Additionally, speech/audio inputs are shared across subtracks within each track, enabling controlled evaluation of reasoning relative to perception accuracy. Fig. 1 provides data example, and Table 1 shows key dataset statistics, showcasing its diversity without strong biases toward any particular ranges. Table 1: Statistics of SAKURA. Avg, Std, Min, and Max represent the average, standard deviation, minimum, and maximum of the measured quantities. Speech/audio duration (s)"
        },
        {
            "title": "Avg",
            "content": "4."
        },
        {
            "title": "Std Min Max",
            "content": "2.23 0.22 20.78 Instruction length (words) 31.32 12.57 8 66 2.2. Speech/audio sources We describe the speech/audio data sources for each track: Gender: we randomly selected 500 samples from the testing split of the English subset of Common Voice 17.0 [31], which contains over 20k hours of validated human speech in 124 languages. We balance the genders of the speakers to mitigate potential biases. Language: we drew 500 samples from the testing splits of eight languages1 in Common Voice 17.0, while balancing both the language distribution and the speaker genders. Emotion: we randomly chose 500 samples from CREMAD [32] and MELD [33,34], both widely used for speech emotion recognition. Samples were evenly distributed across five emotions2 while ensuring gender balance among speakers. Animal: we collected 500 audio samples from ESC-50 [35] and the dataset curated by asmaz et al. [36]. ESC-50 contains 5-second environmental sounds categorized into 50 categories, from which we selected samples from nine animal categories3. We included additional samples from asmaz et al., selecting only those matching the same nine categories. 2.3. Textual question-answer pair generation Fig. 1 illustrates our question-answer data generation pipeline. We used GPT-4o4 to generate question-answer pairs. In the single-hop sub-tracks, questions directly queried target attributes. To enhance diversity, we used GPT-4o to paraphrase manually crafted questions, such as What is the animal in the sound?, producing multiple variations with differing phrasings and lengths. These paraphrased questions were then paired with speech/audio data, with the associated attribute label as the golden answer and distractors selected from alternative labels. The generation process for the multi-hop sub-tracks was more complex. For each track, we used GPT-4o to generate several tested topics and three sample questions per topic to clarify their meaning. topic is concept that multi-hop questions can target based on the tracks attribute labels (e.g., physical traits in the Animal track). To ensure quality and diversity, we manually filtered the topics using these criteria: 1. Relevance and Groundedness: Topics must be relevant to the tracks attribute and supported by the corresponding information. For example, topics like determining the age of the speaker in the Gender track or determining the religious beliefs of the speaker in the Language track were excluded. 2. Objectivity: Topics must be objective and unbiased. For example, the association of animals and human personality was excluded due to cultural ambiguity. 3. Uniqueness: Topics should be distinct. For example, only one of feeding habits and food sources could be included. For each topic, GPT-4o generated diverse questions along with golden answer candidates and distractors for all attribute labels, forming choice pool for pairing with speech/audio data. For example, for question on physical traits with cat as the attribute label, GPT-4o might generate claws as golden answer candidate and tentacles and hooves as distractors. 1English/German/Spanish/French/Italian/Chinese/Japanese/Korean 2Happy / Disgust / Sad / Fear / Angry 3Dog / Cat / Pig / Cow / Frog / Hen / Rooster / Sheep / Crow 4gpt-4o-2024-11-20, https://openai.com Table 2: Accuracies (%) and 95% confidence intervals of the baselines on SAKURA. Single and Multi denote the single-hop and multi-hop sub-tracks. The best and the second-best performances among open-source and proprietary LALMs are marked in bold and underlined. Model sizes (except proprietary ones) are provided, with cascaded models showing the sum of involved modules. Size (B) Gender Language Emotion Animal Average Single Multi Single Multi Single Multi Single Multi Single Multi Open-source LALMs 52.4 4.4 19.6 3.5 16.8 3.3 11.4 2.8 28.6 4.0 19.6 3.5 65.6 4.2 21.8 3.6 40.9 4.3 18.1 3.4 7 LTU-AS 76.4 3.7 39.8 4.3 5.6 2.0 19.4 3.5 5.6 2.0 24.2 3.8 85.2 3.1 51.4 4.4 43.2 4.3 33.7 4.1 7 GAMA-IT 7.5 59.8 4.3 48.6 4.4 21.8 3.6 29.6 4.0 19.8 3.5 28.2 3.9 68.6 4.1 34.6 4.2 42.5 4.3 35.3 4.2 SALMONN 8.3 88.4 2.8 85.2 3.1 94.2 2.0 75.4 3.8 34.8 4.2 36.4 4.2 34.4 4.2 31.2 4.1 63.0 4.2 57.1 4.3 DeSTA2 8.4 49.6 4.4 43.8 4.3 87.6 2.9 40.6 4.3 63.2 4.2 37.0 4.2 92.2 2.4 66.0 4.2 73.2 3.9 46.9 4.4 Qwen-Audio-Chat Qwen2-Audio-Instruct 8.4 88.0 2.8 47.2 4.4 83.8 3.2 48.0 4.4 64.2 4.2 39.8 4.3 88.8 2.8 61.4 4.3 81.2 3.4 49.1 4.4 Proprietary LALMs GPT-4o Audio Gemini-1.5-flash Gemini-1.5-pro - - - - 95.2 1.9 83.6 3.2 38.2 4.3 23.8 3.7 80.6 3.5 55.4 4.4 71.3 4.0 54.3 4.4 77.0 3.7 24.2 3.8 98.2 1.2 79.8 3.5 24.6 3.8 19.4 3.5 27.2 3.9 16.2 3.2 56.8 4.3 34.9 4.2 74.0 3.8 43.4 4.3 97.2 1.4 90.6 2.6 39.2 4.3 24.0 3.7 42.0 4.3 28.6 4.0 63.1 4.2 46.6 4.4 - ASR+LLM ASR+AAC+LLM 9.5 24.2 3.8 32.2 4.1 93.6 2.1 82.4 3.3 21.4 3.6 30.6 4.0 30.8 4.0 27.6 3.9 42.5 4.3 43.2 4.3 17.9 85.0 3.1 79.6 3.5 93.4 2.2 88.8 2.8 60.0 4.3 51.4 4.4 78.0 3.6 78.4 3.6 79.1 3.6 74.5 3.8 Chance level - 50.0 50.0 25.0 25.0 25. 25.0 25.0 25.0 31.3 31.3 Random Baseline Cascaded Systems To ensure data quality, human annotators rigorously verified the correctness of golden answer candidates and the inaccuracy of distractors by cross-referencing reliable sources (e.g., Wikipedia) based on the questions and attribute labels. Each golden answer candidate and distractor was reviewed by at least three annotators and accepted only if all annotators agreed, ensuring the quality. Once validated, questions were paired with corresponding speech/audio data, and the verified choice pools were used to sample the appropriate golden answer and distractors based on the attribute label of the paired speech/audio input. Before finalizing the instructions, the choice order, including the golden answer and distractors, was shuffled to ensure the golden answer appeared equally frequent across positions (e.g., (a), (b), etc.), mitigating potential positional biases in LALMs (i.e., favoring certain option, such as (a), irrespective of content). The instruction was constructed by concatenating the question with the shuffled choices, with option prefixes (e.g., (a), (b)) inserted. The paired speech/audio inputs, instructions, and golden answers were included in SAKURA. 3. Experimental setups 3.1. Evaluation metrics Since SAKURA comprises multiple-choice questions, accuracy is natural metric. However, because LALMs sometimes generate descriptive responses instead of explicitly selecting choice, we adopt an LLM-as-a-judge [37] approach following prior works [22, 23]. Specifically, we employ GPT-4o5 as the evaluator. During evaluation, the evaluator is provided with well-defined criteria, the original instruction, the golden answer, and the response from the evaluated model. It then determines whether the response aligns with the golden answer, following these rules: 1. Each question in SAKURA has exactly one correct answer. If model fails to select one and only one choice from the given choices, either by selecting multiple choices or failing to explicitly choose any, it should be marked as incorrect. 2. The evaluator must assess the alignment between the golden 5gpt-4o-2024-11-20, https://openai.com answer and the models response, providing an explanation to enhance the evaluation transparency. 3. The final judgment should be summarized as binary correct/incorrect label to facilitate post-processing. We incorporate in-context examples to enhance evaluation quality. To verify the evaluators reliability, we conducted human verification on its judgments on 200 randomly selected samples, finding 99.5% agreement with human annotations. This confirms the robustness of our LLM-based evaluation. Final accuracy is computed based on the judgments. 3.2. Baselines We incorporated models from three categories: open-source LALMs, proprietary LALMs, and cascaded systems. For all baselines, we applied greedy decoding without system prompts. Open-source LALMs involved models of comparable sizes, including LTU-AS6 [8], GAMA-IT7 [9], SALMONN8 [10], Qwen-Audio-Chat9 [12], Qwen2-Audio-Instruct10 [13], and DeSTA211 [11]. Proprietary LALMs included GPT-4o Audio12 [2], Gemini-1.5-flash [7], and Gemini-1.5-pro13 [7]. Cascaded systems comprised three components: speech recognition (ASR) module, an audio captioning (AAC) module, and text-based LLM, forming two variants: (1) ASR+LLM, where the LLM answered questions based only on ASR transcriptions, and (2) ASR+AAC+LLM, where audio captions enriched the input to the LLM. Specifically, we used Whisperlarge-v3 [38] for ASR, Qwen2-Audio-Instruct for AAC, and LLaMA-3.1-8B-Instruct [1] as the LLM. Experiments on models except for proprietary LALMs took 68 V100 GPU hours, while those on proprietary LALMs took 15.5 hours. The full LLM-based evaluation spanned 62 hours. 6https://github.com/YuanGongND/ltu 7https://github.com/Sreyan88/GAMA 8https://github.com/bytedance/SALMONN 9https://github.com/QwenLM/Qwen-Audio 10https://github.com/QwenLM/Qwen2-Audio 11https://github.com/kehanlu/DeSTA2 12gpt-4o-audio-preview-2024-12-17, https://openai.com 13gemini-1.5-flash-002 and gemini-1.5-pro-002, https://deepmind.google/technologies/gemini/ Table 3: Accuracies (%) and 95% confidence intervals of open-source and proprietary LALMs on multi-hop sub-tracks, considering only instances where the corresponding single-hop questions were correctly answered. Results under speech/audio-based (S/A) and text-based (Text) reasoning settings are reported. The better performances among the two settings are marked in bold."
        },
        {
            "title": "Average",
            "content": "S/A"
        },
        {
            "title": "Text",
            "content": "S/A"
        },
        {
            "title": "Text",
            "content": "S/A"
        },
        {
            "title": "Text",
            "content": "S/A"
        },
        {
            "title": "Text",
            "content": "S/A"
        },
        {
            "title": "Text",
            "content": "19.5 3.5 23.3 3.7 20.2 3.5 42.9 4.3 28.7 4.0 32.2 4.1 22.9 3.7 18.3 3.4 22.8 3.7 32.8 4.1 LTU-AS 44.5 4.4 69.6 4.0 28.6 4.0 64.3 4.2 35.7 4.2 64.3 4.2 53.5 4.4 74.2 3.8 40.6 4.3 68.1 4.1 GAMA-IT 51.8 4.4 76.6 3.7 48.6 4.4 83.5 3.3 38.4 4.3 56.6 4.3 34.1 4.2 58.0 4.3 43.2 4.3 68.7 4.1 SALMONN 90.0 2.6 98.6 1.0 78.6 3.6 93.6 2.1 51.1 4.4 94.2 2.0 43.6 4.3 86.6 3.0 65.8 4.2 93.3 2.2 DeSTA2 40.3 4.3 55.6 4.4 40.2 4.3 73.3 3.9 44.9 4.4 69.0 4.1 68.3 4.1 80.0 3.5 48.4 4.4 69.5 4.0 Qwen-Audio-Chat Qwen2-Audio-Instruct 49.3 4.4 72.5 3.9 50.4 4.4 74.7 3.8 45.5 4.4 74.8 3.8 62.2 4.3 75.7 3.8 51.8 4.4 74.4 3.8 GPT-4o Audio Gemini-1.5-flash Gemini-1.5-pro - 86.5 3.0 91.6 2.4 38.2 4.3 73.3 3.9 63.0 4.2 68.5 4.1 62.6 4.2 77.8 3.6 27.5 3.9 78.7 3.6 80.9 3.4 95.5 1.8 37.4 4.2 85.4 3.1 20.6 3.5 71.3 4.0 41.6 4.3 82.7 3.3 48.1 4.4 91.3 2.5 91.6 2.4 97.7 1.3 40.3 4.3 79.1 3.6 34.8 4.2 69.5 4.0 53.7 4.4 84.4 3.2 - 4. Results 4.1. Main results on SAKURA Table 2 shows the baseline performances on SAKURA, with chance level included for reference. GPT-4o Audios results on the Gender track are omitted as it refuses to answer genderrelated questions due to post-training constraints, consistent with prior findings [39]. In the single-hop sub-tracks, which assess perception (information extraction) ability, Qwen2-Audio-Instruct achieves the highest average accuracy. However, no model consistently outperforms across all tracks, with each exhibiting distinct blind spots, indicating weaknesses in processing certain speech/audio attributes. For instance, DeSTA2 and Gemini-1.5-flash struggle on the Emotion and Animal tracks despite high accuracy elsewhere, while GAMA-IT and SALMONN perform worse than chance on the Language and Emotion tracks. Notably, despite being trained on emotion-related tasks, most LALMs struggle with the Emotion track, likely due to the inherent subtlety of emotional cues that require more nuanced perception. Our findings highlight the need for improving LALMs fundamental perception capabilities. In the multi-hop sub-tracks, accuracy drops drastically compared to single-hop ones, even when models exhibit good perception. For example, although Qwen-Audio-Chat and Qwen2-Audio-Instruct excel at recognizing emotion and animal sounds, and DeSTA2 and Gemini-1.5-flash identify the languages well, they all face substantial declines in multi-hop reasoning. This sharp contrast suggests that while LALMs may extract correct information, they struggle to reason based on that information, exposing fundamental limitation in their speech/audio-based multi-hop reasoning ability. Comparing model groups, the best LALMs in both singlehop and multi-hop sub-tracks are generally open-source, suggesting that proprietary models do not always have competitive edge. Their advantage appears only in the Language track, likely due to larger and more diverse pre-training datasets, but it diminishes in other tracks. Additionally, ASR+AAC+LLM achieves higher average accuracy than most LALMs on both sub-tracks, showing that current LALMs still fall short of cascaded approaches, as observed in prior works [5, 29]. 4.2. Fail to reason, or fail to reason with speech and audio? To further investigate why LALMs struggle with multi-hop reasoning, we compared their performances under two conditions: (1) speech/audio-based reasoning, where models process raw speech/audio inputs, and (2) text-based reasoning, where these inputs are replaced by captions summarizing the relevant attribute information (e.g., The animal making the sound is cat.) alongside the original instructions (see Fig. 1). This comparison examines how LALMs reason using encoded auditory representations compared to an approximate textual equivalent summarizing relevant information. For each model, we filtered the multi-hop sub-tracks to include only instances where it correctly answered the corresponding single-hop questions. This ensured that failures in multi-hop reasoning stemmed from reasoning limitations rather than misperception of attributes. As shown in Table 3, most LALMs struggle with speech/audio-based reasoning despite correctly recognizing attributes, yet perform significantly better when similar information is provided as text. For example, DeSTA2 exceeds 90% accuracy in the text-based setting, demonstrating strong reasoning ability when leveraging explicitly provided textual information alongside stored knowledge. Since the only difference between these conditions is the modality used to present attribute information, this discrepancy suggests that while LALMs possess basic reasoning abilities, their reasoning remains predominantly text-driven, revealing lack of true multimodal integration. Even when extracting accurate information, LALMs fail to incorporate latent speech/audio representations into the reasoning process, contradicting expectations for end-to-end models to unify speech/audio understanding with reasoning. Our findings highlight the urgent need for improved multimodal reasoning capabilities. 5. Conclusion, limitations, and future work We introduce SAKURA, the first benchmark for systematically evaluating LALMs multi-hop reasoning with speech and audio. Our findings show that LALMs struggle to recognize certain speech and audio attributes, exhibiting perception blind spots. Even when accurately extracting relevant information, they fall short in speech/audio-based multi-hop reasoning despite good text-based reasoning skills, highlighting the need for better integration of multimodal information into their reasoning process. SAKURA has certain limitations that we aim to address in future work. As an initial study, it covers only four speech/audio attributes and does not specifically account for acoustic variations (e.g., background noise, speaking rate) that may affect model performance. Expanding both the range of attributes and the diversity of acoustic conditions is key future direction for assessing model robustness. Additionally, while SAKURA currently employs multiple-choice evaluation for text-generating LALMs, future iterations will investigate speech-generating models [4042] and open-ended tasks for deeper insights. 6. References [1] A. Dubey et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [2] O. (2024) et al., arXiv:2410.21276, 2024. Gpt-4o system card, arXiv preprint [3] D. SurÄ±s, S. Menon, and C. Vondrick, Vipergpt: Visual inference via python execution for reasoning, Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023. [4] R. Huang et al., Audiogpt: Understanding and generating speech, music, sound, and talking head, in Proceedings of the AAAI Conference on Artificial Intelligence, 2024, pp. 23 80223 804. [5] C.-Y. Kuan et al., Speech-copilot: Leveraging large language models for speech processing via task decomposition, modularization, and program generation, in 2024 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2024, pp. 10601067. [6] H. Liu et al., Visual instruction tuning, in Advances in Neural Information Processing Systems, 2023, pp. 34 89234 916. [7] G. Team et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [8] Y. Gong et al., Joint audio and speech understanding, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023. [9] S. Ghosh et al., Gama: large audio-language model with advanced audio understanding and complex reasoning abilities, arXiv preprint arXiv:2406.11768, 2024. [10] C. Tang et al., SALMONN: Towards generic hearing abilities for large language models, in The Twelfth International Conference on Learning Representations, 2024. [11] K.-H. Lu et al., Developing instruction-following speech language model without speech instruction-tuning data, arXiv preprint arXiv:2409.20007, 2024. [12] Y. Chu et al., Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models, arXiv preprint arXiv:2311.07919, 2023. [13] Y. Chu, J. Xu, Q. Yang, H. Wei, X. Wei et al., Qwen2-audio technical report, arXiv preprint arXiv:2407.10759, 2024. [14] B. Jiang et al., peek into token bias: Large language models are not yet genuine reasoners, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Nov. 2024, pp. 47224756. [15] J. Huang et al., Large language models cannot self-correct reasoning yet, in The Twelfth International Conference on Learning Representations, 2024. [16] X. Chen et al., Premise order matters in reasoning with large language models, in Forty-first International Conference on Machine Learning, 2024. [17] O. Press et al., Measuring and narrowing the compositionality gap in language models, in Findings of the Association for Computational Linguistics: EMNLP 2023, Dec. 2023, pp. 56875711. [18] Y. Shalev et al., Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning, arXiv preprint arXiv:2406.13858, 2024. [19] S. Yang et al., Do large language models latently perform multihop reasoning? in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Aug. 2024, pp. 10 21010 229. [20] E. Biran et al., Hopping too late: Exploring the limitations of large language models on multi-hop queries, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Nov. 2024, pp. 14 11314 130. [21] T. Ju et al., Investigating multi-hop factual shortcuts in knowledge editing of large language models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Aug. 2024, pp. 89879001. [22] C.-y. Huang et al., Dynamic-SUPERB phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks, in The Thirteenth International Conference on Learning Representations, 2025. [23] Q. Yang et al., AIR-bench: Benchmarking large audio-language models via generative comprehension, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Aug. 2024, pp. 19791998. [24] G.-T. Lin, C.-H. Chiang, and H.-y. Lee, Advancing large language models to capture varied speaking styles and respond properly in spoken conversations, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Aug. 2024, pp. 66266642. [25] J. AO et al., Sd-eval: benchmark dataset for spoken dialogue understanding beyond words, in Advances in Neural Information Processing Systems, vol. 37, 2024, pp. 56 89856 918. [26] Y.-C. Lin et al., Listen and speak fairly: study on semantic gender bias in speech integrated large language models, in 2024 IEEE Spoken Language Technology Workshop (SLT), 2024, pp. 439446. [27] Y.-C. Lin, W.-C. Chen, and H.-Y. Lee, Spoken stereoset: on evaluating social bias toward speaker in speech large language models, in 2024 IEEE Spoken Language Technology Workshop (SLT), 2024, pp. 871878. [28] S. Ghosh et al., Compa: Addressing the gap in compositional reasoning in audio-language models, in The Twelfth International Conference on Learning Representations, 2024. [29] S. Sakshi et al., MMAU: massive multi-task audio understanding and reasoning benchmark, in The Thirteenth International Conference on Learning Representations, 2025. [30] C.-Y. Kuan et al., Understanding sounds, missing the questions: The challenge of object hallucination in large audio-language models, in Interspeech 2024, 2024, pp. 41444148. [31] R. Ardila et al., Common voice: massively-multilingual speech corpus, in Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020, pp. 4211 4215. [32] H. Cao et al., Crema-d: Crowd-sourced emotional multimodal actors dataset, IEEE transactions on affective computing, vol. 5, no. 4, pp. 377390, 2014. [33] S. Poria et al., MELD: multimodal multi-party dataset for emotion recognition in conversations, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Jul. 2019, pp. 527536. [34] S. M. Zahiri et al., Emotion detection on tv show transcripts with sequence-based convolutional neural networks, in Workshops at the thirty-second aaai conference on artificial intelligence, 2018. [35] K. J. Piczak, Esc: Dataset for environmental sound classification, in Proceedings of the 23rd ACM international conference on Multimedia, 2015, pp. 10151018. [36] E. asmaz and F. B. Tek, Animal sound classification using convolutional neural network, in 2018 3rd International Conference on Computer Science and Engineering (UBMK). IEEE, 2018, pp. 625629. [37] J. Gu et al., survey on llm-as-a-judge, arXiv preprint arXiv:2411.15594, 2024. [38] A. Radford et al., Robust speech recognition via large-scale weak supervision, in International Conference on Machine Learning. PMLR, 2023, pp. 28 49228 518. [39] Y.-X. Lin, C.-K. Yang, W.-C. Chen, C.-A. Li, C.-y. Huang, X. Chen, and H.-y. Lee, preliminary exploration with gpt-4o voice mode, arXiv preprint arXiv:2502.09940, 2025. [40] Q. Fang et al., Llama-omni: Seamless speech interaction with large language models, arXiv preprint arXiv:2409.06666, 2024. [41] A. Defossez et al., Moshi: speech-text foundation model for real-time dialogue, arXiv preprint arXiv:2410.00037, 2024. [42] C.-K. Yang et al., Building taiwanese mandarin spoken language model: first attempt, arXiv preprint arXiv:2411.07111, 2024."
        }
    ],
    "affiliations": [
        "National Taiwan University, Taiwan"
    ]
}