{
    "paper_title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
    "authors": [
        "Yulei Qin",
        "Xiaoyu Tan",
        "Zhengbao He",
        "Gang Li",
        "Haojia Lin",
        "Zongyi Li",
        "Zihan Xu",
        "Yuchen Shi",
        "Siqi Cai",
        "Renting Rui",
        "Shaofei Cai",
        "Yuzheng Cai",
        "Xuan Zhang",
        "Sheng Ye",
        "Ke Li",
        "Xing Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 0 6 2 2 . 9 0 5 2 : r Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning Youtu-Agent Team SPEAR Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agents own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR , curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within well-balanced range of entropy across stages. Specifically, our approach incorporates curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Regularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence. We also combine bag-of-tricks, which are effective industrial optimizations of agentic RL, into strong baseline, Dr.BoT, to demonstrate the practical effectiveness of our proposed SPEAR. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR. Codes and checkpoints will be available soon. Date: September 22, 2025 Correspondence: {yuleiqin, arthurtan}@tencent.com"
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement Learning (RL) [1, 2, 3] has driven the development of reasoning capabilities of Large Language Models (LLMs). Built upon the reason-and-act (ReAct) paradigm [4], LLMs have powered various agentic applications such as simulated robot navigation [5, 6], mobile assistant [7, 8], web navigator [9, 10], deep searcher [11, 12, 13], and GUI master [14, 15]. fundamental challenge in applying RL to LLM agents, especially for long-horizon tasks with sparse rewards, is managing the balance between exploration and exploitation. The LLM agent needs to exploit both its pretrained knowledge and feedback from past interactions to identify and refine strategies that maximize ultimate reward. At the same time, it must explore novel behaviors, using different tools to discover more effective solutions, which involves extending text-based reasoning to tool-augmented reasoning. The interweaving between exploration and exploitation determines the emerging agents competence upon convergence. *Full author list in contributions. 1 SPEAR Figure 1. The core concept of our proposed SPEAR for training long-horizon LLM agents via group-based RL. Compared with the vanilla GRPO-like algorithms, we introduce the curriculum-based self-imitation learning with intrinsic reward shaping. Given the same data input, group of trajectories are generated with multi-turn tool interactions and then engaged for episode-level reward computation and advantage estimation. Then, we propose filtering valuable good trajectories to update the replay buffer, where the stored past experiences guide the agent to explore effectively on sparsely rewarded tasks via self-imitation. The total training batch contains both on-policy and off-policy data from the replay buffer. With self-guided progressive exploration, SPEAR boosts performance of various LLMs and baselines on ALFWorld, WebShop, and AIME24/25 in plug-and-play manner. Existing studies often quantify the exploration potential through entropy [16, 17, 18], where the decline of policy entropy indicates the over-confidence of the current policy with insufficient exploration. In this case, series of regularization techniques [19, 20, 21] have been proposed to maximize entropy, particularly in the context of agent training [22, 23, 24, 25, 26, 27, 28, 29]. However, when it comes to LLM-driven agents, entropy-based control is fragile: the accumulation of low-probability tokens from the environment feedback induces severe distribution shifting, often leading to mode collapse [18, 30]. Unlike conventional agents, these models may experience sustained entropy growth due to policy over-uncertainty during multi-turn interactions with external tools. As result, the policy struggles to exploit tool-use skills effectively, and training instability becomes frequent [31, 32, 33]. Recent approaches attempt to mitigate this issue by relying either on cold-start supervised fine-tuning (SFT) [13, 14, 34, 35] or on hybrid schemes that combine RL with SFT [36]. Although these methods improve stability, they compromise RLs ability to discover strategies beyond those present in the SFT corpus. This limitation highlights the need for adaptive training frameworks that can dynamically schedule LLM-driven agents to decide when to explore and when to exploit. In this paper, we are trying to answer the following core research question: During RL of agentic LLMs, can we schedule smooth transition between exploration and exploitation guided by the policys own experiences without going to extremes of either entropy collapsing or runaway divergence? We hypothesize that policy entropy should serve as progressive guide for balancing exploration and exploitation throughout training. Specifically, the agent should maintain its entropy within dynamic but controlled range that evolves over time: 1) At the early stages of training, increasing entropy is beneficial for supporting broad skill-level exploration and rapidly developing tool-use capabilities. The agent is expected to encounter unfamiliar observations, engage in trial-and-error interactions, and discover diverse reasoning trajectories. 2) As training advances, however, shift toward converging entropy is required. This enables the agent to consolidate problem-solving heuristics and emphasize action-level exploration. In this phase, the agent exploits reward signals to choose comparatively more effective actions and adapts to changing trajectory distributions, thereby stabilizing its evolutionary path. To address this, we propose the Self-imitation with Progressive Exploration for Agentic Reinforcement , curriculum-based RL recipe for improving the exploration-exploitation balance with learning (SPEAR) self-imitation and intrinsic reward. As shown in Figure 1, the core principle of philosophy follows the vanilla 2 SPEAR Figure 2. Overview of SPEAR in terms of data flow. During each episode, the agent interacts with the environment to generate set of trajectories. These trajectories are processed along two complementary paths. First, they are used for intrinsic reward shaping, advantage estimation, and on-policy updates, following mechanism similar to the vanilla GRPO. Second, they are selectively filtered and stored in replay buffer, enabling off-policy updates through the proposed self-imitation scheme with advantage recalibration and regularization. This dual integration allows the agent to maximize the utility of rewarding past experiences, thereby expanding the exploration space effectively, while simultaneously mitigating persistent over-uncertainty in decision-making under shifting distributions of external feedback. As result, SPEAR achieves stable balance between exploration and exploitation through self-guided policy adaptation. 3 SPEAR Self-Imitation Learning (SIL) [37, 38] where an independent replay buffer is prepared to store the state-action pairs (i.e., trajectories) only when their returns in the past episodes exceed the baselines (i.e., those with positive advantages). Such replay buffer is exploited to encourage actions with good returns and improve hard exploration based on these successful trajectories under the sparse-reward, long-horizon agent tasks. Specifically, we introduce three modifications to SIL tailored to the dynamics of policy entropy in agentic tasks. First, we incorporate curriculum to integrate both skill-level and action-level exploration by adjusting reward shaping and self-imitation across stages. For skill-level exploration, intrinsic rewards encourage frequent tool usage, pushing the agent to interact with uncertain environments. At early stages, exploration remains unconstrained to avoid overfitting immature experiences in the replay buffer. For action-level exploration, we progressively amplify self-imitation of promising trajectories, allowing the agent to iteratively refine behaviors while preventing unbounded entropy growth. To ensure outcome reward remains dominant, we gradually reduce the weight of intrinsic rewards. Second, we tackle the off-policy nature of the update with experiences in the buffer and avoid advantage recomputation by advantage recalibration. Unlike GRPO-like methods [39, 40] that estimate baselines via intra-group averages or the vanilla SIL algorithm [37, 38] that require re-estimating the advantage, we compensate for performance drift in the replay buffer under the assumption of iterative policy improvement to avoid computation. Third, we regularize policy updates to stabilize entropy and mitigate reward hacking. Specifically, we introduce covariance-based clipping [17] at the trajectory level, where high-probability actions strongly correlated with advantage gains are excluded from gradient contributions. This prevents over-confidence in narrow success patterns while avoiding excessive uncertainty. Finally, inspired by existing industrial bag-of-tricks, in LLM-based agentic RL, we present strong baseline, Dr.BoT. It is yet another GRPO variant combining stabilityand efficiency-oriented techniques. Empirical results demonstrate its effectiveness across diverse LLM architectures and agentic tasks. We show that our proposed SPEAR brings considerable performance gains to multiple baselines, improving over GRPO/GiGPO [41]/Dr.BoT respectively up to 16.1%/5.1%/8.6% on ALFWorld [5] and 20.7%/11.8%/13.9% on WebShop [42]. It boosts our Dr.BoT respectively up to 3.8% on AIME24 and 6.1% on AIME25 [43]. These gains come with around 10% 25% computation overhead in theoretical complexity, but end up with quite similar and comparable runtime per iteration in practice. Such compatibility and scalability make our SPEAR plug-and-play algorithm for training versatile LLM agents. In summary, our contributions are: We propose SPEAR, generalization of the self-imitation learning method for training agentic LLMs. We bring in curriculum scheduling for progressive transition from skill-based to action-based exploration. It bypasses the costly imitation of expert demonstrations and allows the agent to explore for long-term, distal rewards under the guidance of ones own experiences. We target the management of policy entropy to prevent either premature collapsing or persistent instability. We propose an integration of regularization techniques and intrinsic reward shaping to balance exploration-exploitation via the lens of entropy. We propose strong GRPO-variant baseline, Dr.BoT, which combines established agentic RL training techniques validated in industrial practice, confirming its effectiveness and superiority over existing baselines. Extensive experiments across LLMs and tasks demonstrate that our SPEAR consistently enhances the performance of various baselines, showcasing its plug-and-play nature with high compatibility and scalability."
        },
        {
            "title": "2 Related Work",
            "content": "4 SPEAR"
        },
        {
            "title": "2.1 Reinforcement Learning Algorithms for LLMs",
            "content": "Reinforcement learning (RL) was first introduced into LLM training to improve instruction following and alignment [44]. With the advent of large-scale reasoning models (e.g., OpenAIs o1 [45]), RL has been adopted more broadly in post-training pipelines. Proximal Policy Optimization (PPO) [20], as the canonical online RL method, leverages an actorcritic architecture together with the clipped surrogate objective and KullbackLeibler (KL) divergence penalty to constrain policy update while achieving strong performance. Group Relative Policy Optimization (GRPO) [2, 39] simplifies this setup by replacing the critic with group-wise baseline: for each prompt, multiple rollouts are sampled to form group, and their rewards are normalized using the group mean and standard deviation. This critic-free design reduces memory usage and engineering complexity while retaining PPO-like trust-region behavior. Building on GRPO, many variants target the training stability and sample efficiency. DAPO [46] uses dynamic sampling and \"clip higher\" to encourage exploration and stabilize training. Dr.GRPO [40] addresses length bias and the difficulty bias by normalization, yielding better token efficiency. GSPO [47] proposes the sequence-level importance ratio to improve training efficiency notably for Mixture-of-Experts (MoE) [48] LLMs. Existing methods have greatly advanced RL for LLMs. However, naively combining them can lead to conflicts or tight couplings among methods. To this end, we harmonize the strengths of DAPO, Dr.GRPO, and other agent RL studies from both research and industrial practice to establish strong baseline, Dr.BoT, as detailed in Section 4.3."
        },
        {
            "title": "2.2 Optimization of LLM Agents",
            "content": "The success of GRPO for RL has expedited the training of LLM-based agents. growing line of researches investigate how to endow models with better tool-use capabilities so that they can complete complex tasks [34, 49, 18]. In parallel, recent works [11, 13, 50] connect LLMs to the open web to strengthen information seeking capability with smarter search and browsing. RAGEN [33] improves the stability of multi-turn RL training through instance filtering and gradient shaping. GiGPO [41] augments group-level advantages with additional step-level advantage estimates by identifying shared states across different trajectories, capturing both global and local preference against the sparse reward problem. ARPO [30] monitors entropy dynamics during rollouts and branches trajectories adaptively, promoting exploration at steps with high uncertainty after tool usage. Such branching internalizes advantage differences in stepwise tool-calls like GiGPO. In this work, we address the explorationexploitation dilemma under multi-turn tool-use settings. We introduce curriculumregulated RL regime that gradually shifts skill-based exploration towards actionbased exploration. We integrate self-imitation and intrinsic reward to consolidate successful behaviors, which will be discussed in Section 4.1. It is noted that our SPEAR can work with existing algorithms in plug-and-play manner, exhibiting high level of compatibility and generalization for policy entropy control."
        },
        {
            "title": "2.3 Exploration in Reinforcement Learning",
            "content": "In sparse or delayed environments, it is pivotal to motivate the agents to explore unknown states for sample efficiency and generalization. Curiosity-driven method [51] grant intrinsic rewards for prediction error or novelty to actively seek unfamiliar states without extrinsic feedback. Similarly, VIME [52] rewards actions that maximize the information gains about the agents belief of the environments dynamics, improving exploration in continuous control task. Count-based algorithms encourage exploration by awarding higher intrinsic rewards for visiting rare or previously unseen states. [53] introduces pseudo-counts derived from density model to assign count-based bonuses in high-dimensional problems. [54] shows that simple 5 SPEAR hashing of discrete states for counting enables strong exploration in continuous control and visual games. Skill acquisition methods learn diverse repertoire of skills or options through intrinsic objectives, so that these behaviors can later aid exploration or downstream tasks. VIC [55] discovers set of distinct options by maximizing the mutual information and can effectively learn skills that reliably reach different regions of the state space. DIAYN [56] learns useful skills without any extrinsic reward by maximizing an information-theoretic objective under maximum-entropy policy. Entropy-regularization methods explore high-entropy actions to prevent premature convergence. SAC [21] establishes framework where the agent explicitly maximizes the expected reward and entropy. [17] proposes to control entropy by restricting the update of high-covariance tokens to prevent entropy collapsing of LLMs. However, in the era of agentic LLMs, the naive application of entropy maximization could lead to divergence as the multi-turn interactions already result in the increased uncertainty on unfamiliar observations. Under such circumstance, we propose the curriculum-guided self-imitation to leverage the agents own experiences for balancing exploration and exploitation, where the intrinsic reward and self-imitation respectively enhance skill-based and action-based exploration. It avoids handcrafted heuristic techniques in previous studies and instead fully relies on the agent itself to reinforce successful patterns for efficiency of valid exploration."
        },
        {
            "title": "2.4 Experience Replay in Reinforcement Learning",
            "content": "Self-Imitation Learning (SIL) [37] aims to take advantage of the past successful experiences [57, 58, 59, 60] to drive its future learning. The core insight is that learning from ones own high-return trajectories could indirectly encourage exploration in environments with sparse rewards. Building upon SIL, [61] formulates policy optimization as divergence minimization problem, which can be further reduced into policy-gradient algorithm with shaped rewards learned from experience replays. Self-imitation advantage learning (SAIL) [38] extends SIL to off-policy, action-value-based RL methods. [62] proves that SILs return-based update provides biasvariance trade-off that can speed up learning by propagating rewards over longer horizons. Self-imitation learning from demonstrations (SILfD) [63] extends SIL to leverage external demonstrations and the agents own experiences for the improved performance. For training LLMs, Generalized SIL (GSIL) [64] proposes an offline alignment framework that uses self-imitation on demonstration data. By defining surrogate loss with density ration estimation, GSIL allows the model to learn from its own generated solutions alongside reference answers. The success of SIL in both game environments and modern LLM applications shows that agents can effectively learn from themselves by leveraging their own successful experiences as guideposts for future behavior. While SIL helps on long-horizon problems, its direct application on agentic RL induces entropy collapsing. To mitigate this, we introduce an intrinsic reward to sustain skill-based exploration and harmonize its contribution with self-imitation via curriculum. We additionally suppress over-confidence by clipping high covariance tokens out of SIL as regularization."
        },
        {
            "title": "3.1 Problem Definition",
            "content": "Given textual task description p(X) where p(X) represents the distribution over task descriptions, LLM agent parameterized by θ interacts with the environment via multiple turns until it completes the task or exceeds the maximum number of allowed turns. Such processes can be described in the following Markov Decision Process (MDP) (S, A, P, R): 6 SPEAR State Space S: At time t, the state st observed by the agent include the observations from the environment, previous interaction history, and the task description. Action Space A: In the context of LLM agent, the textual action at at time can either be discrete operations (e.g., task-specific tool calling) or continuous sequence (e.g., reasoning traces up to the maximum of tokens). Transition Probability Function P: The transition from the st to st+1 given the action at is generally environment-dependent. It can be either deterministic (e.g., code interpretation) or stochastic (e.g., gaming and web environment). Reward Function R: The reward Rt of the current state st and the action at can be dense (e.g., process reward), sparse, or even only at the end of the trajectory (e.g., outcome reward). full episode can be denoted as τ = {(s1, a1, R1), (s2, a2, R2), ..., (sT, aT, RT)} with multi-turn (T times) interaction. We aim to optimize the agentic LLM as policy model πθ by encouraging those trajectories with higher rewards. Due to the large vocabulary size V, the distribution modeling of the output by policy πθ(atst, x) is complex and multiple alternative actions are potential. In practice, most of the feedback from the environment may not indicate success or failure, making process supervision quite challenging. In this case, we shape agentic LLMs via RL solely with the outcome reward."
        },
        {
            "title": "3.2 Design of Action Space for Agentic LLMs",
            "content": "In this study, we investigate the optimization of agentic LLMs that are capable of taking actions or using external tools to solve tasks. Building on the experimental settings proposed in recent work on agentic RL [65, 41, 34, 30], we define three distinct types of action spaces tailored to different task scenarios and use them to evaluate the effectiveness of SPEAR: TextWorld Embodied Tool: The embodied actions follows ALFWorld [5] where language-driven agent interacts with the TextWorld [66]. It allows the agent to take one of the following high-level actions: goto {recep}, take {obj} from {recep}, put {obj} in/on {recep}, open {recep}, close {recep}, toggle {obj}{recep}, clean {obj} with {recep}, heat {obj} with {recep}, and cool {obj} with {recep}, where {obj} and {recep} denote objects and receptacles, respectively. Web Browsing Tool: The definition of web browsing follows WebShop [42] where only two actions are allowed: search[query] and choose[button] where query and button respectively stand for searching query and clickable elements such as back to search, prev/next page, {product title}, {option}, {desc/overview}, previous, and buy. Code Interpreter Tool: The code interpreter executes the code generated by the language model and return both the stdout and stderr. If the code runs correctly, the stdout contains the output. On the other hand, the compiler error messages are provided for the next-round correction. We follow [34] to deploy SandBox [67] service that receives execution requests from the interpreter tool. In addition, we add reminder in the stdout for empty output when the LLM forgets to print computation results: Empty stdout! You might forget to print the answer. For non-empty stderr, we also add an instruction as hint: Errors occurred! Check your code."
        },
        {
            "title": "3.3 Policy Optimization",
            "content": "Without loss of generality, we adopt the GRPO [39, 2] algorithm framework for optimizing πθ. It stems from the PPO [20] but goes one step further by replacing the model-based advantage estimation (i.e., 7 SPEAR Generalized Advantage Estimation [68]) with the intra-group relative advantage ˆA over the averaged baseline. Proximal Policy Optimization (PPO) It maximizes the objective [69, 20]: (πθ) = xp(X),aπθ (x,s) R(x, s, a) βDKL[πθ(x, s)πref(x, s)] , (1) (cid:34) (cid:35) where R(x, s, a) = t=1 rt(x, st, at) is the return [70] for the trajectory and πref is the reference policy model. The KL divergence proposed [71] to prevent the policy πθ from deviating greatly from the reference πref (β > 0). In consideration of the simplicity, we follow TULU 3 [1] to adopt RL with the verifiable reward where the rule-based verifiers are designed to provide the outcome reward signal instead of the reward model rθ. In addition, we follow [40] to drop the KL term by setting β = 0, which not only emphasizes agent performance but also saves memory and computation during training. Group Relative Policy Optimization (GRPO) Specifically, the policy model πθold from the previous iteration generates group of individual trajectories {τi}G i=1. GRPO updates the policy πθ by maximizing the objective below. JGRPO(πθ) = xp(X),{τi}G i= (x) 1 i=1 2), ..., (si πθold 2, ai"
        },
        {
            "title": "J i",
            "content": "GRPO, 1, ai 1, Ri 1), (si 2, Ri τi = {(si (cid:34) T, Ri T)}, T, ai (cid:35) min t(θ) ˆAi ri t, clip[ri t(θ), 1 ϵ, 1 + ϵ] ˆAi βDi KL(πθπref),"
        },
        {
            "title": "J i",
            "content": "GRPO = 1 t=1 ri = πθ(ai tx, si t) tx, si (ai t) πθold , ˆAi = Ri std({Ri}G i=1) , = mean({Ri}G i=1), Di KL(πθπref) = πref(ai πθ(ai tx, si t)) tx, si t) log πref(ai πθ(ai tx, si t)) tx, si t)) 1. (2) (3) (4) (5)"
        },
        {
            "title": "4 Training Agentic LLMs with SPEAR",
            "content": "While the self-imitation learning (SIL) technique has been proposed as powerful tool to train the gaming and embodied agents [37, 72, 38], its extension to the LLM-driven agents faces critical challenges of entropy collapse. Figure 3 illustrates such challenges in which the overfitting of the few available successful experiences causes irreversible stagnation of agent exploration. In addition, our preliminary experiments demonstrate that the inclusion of the auxiliary tool-call reward is double-edged sword (as shown in Figure 5), where the competition between the reward terms causes the oscillations to converge. To address these challenges, we introduce SPEAR for progressive exploration with self-imitation. The approach begins with skill-level exploration, where the agent is guided by tool-call intrinsic reward to broadly investigate tool usage. Successful experiences with tool calls are then used for self-imitation, enabling action-level exploration that refines specific tool-call behaviors. This curriculum design gradually regulates policy entropy, ensuring both exploration and stability in policy learning. In addition, we extend the standard self-imitation framework with two modifications: advantage recalibration to improve off-policy estimation, and entropy-regularization to prevent policy collapse. Further details on intrinsic reward shaping, along 8 SPEAR with the proposed strong baseline Dr.BoT, are presented later in this section. For clarity, we also provide complete summary of the training algorithm in Appendix A.1."
        },
        {
            "title": "4.1 Self-Imitation Learning",
            "content": "Prioritized Experience Replay in Self-Imitation To cope with the sparse reward problem inherent from the long-horizon tasks, we propose to leverage the SIL [37] under our RL framework. It encourages the LLM agent to imitate its past preferred experiences, driving the policy model towards taking actions whose immediate or ultimate returns are unexpectedly good. To this end, we maintain replay buffer that stores previous trajectories, their respective rewards and advantages = {(τj, Rj, ˆAj)}, = 1, 2, ..., ND where ND denotes the buffer size. Under the context of GRPO training, the observed returns are only available at the end of an episode, and therefore, the original discounted returns with the trainable value model involved are simply replaced by the rule-verified rewards. To exploit only good trajectories in the replay buffer, we propose to keep those with positive in-group advantages in the off-policy GRPO loss: GRPO(πθ) = SIL {τj}ND j=1 {πθold (x), xp(X)} ND j=1 GRPO 1( ˆAj > 0), (6) where the indicator function 1() equals to 1 when the condition satisfied and 0 otherwise. It is noted that different from the vanilla GRPO, the past trajectories might not only come from the last policy πθold but also the policy of few steps earlier. Therefore, we heuristically use {πθold } to refer to multiple previous policy models that generate the history trajectories. Advantage Recalibration for Off-Policy Estimation The underlying challenge of applying vanilla SIL into LLM-based agentic RL comes from the off-policy return in advantage estimation [37]. That is to say, the observed return of trajectory from the past policy becomes increasingly different from the current one, under the assumption that the agentic LLM keeps improving during iterative policy update [38, 72]. Under this assumption, vanilla SIL computes the advantage with pointwise max with the per-state empirical return as baseline, which can be seen as proxy for the upper-envelope projection of the value function onto empirical returns. GRPO removes the learned value baseline by estimating the state-dependent baseline performance through its reliance on intra-group reward averaging, but this still depends on the target policy and requires extra computation resources for sampling. Therefore, we propose to recalibrate the off-policy advantage by dynamically adjusting the baseline performance of the agent so that the relative gains of past good experiences over baselines become more associated with the current policy improvement. Specifically, we maintain First-In-First-Out (FIFO) buffer of intra-group baselines for the latest NDR trajectories DR = { Rj}DR j=1 where NDR denotes the size of the baseline buffer. As training progresses, due to the high variance nature of agentic RL, we utilize the 50-th percentile P50(DR) as conservative but robust estimation of the policy baseline with either upward or downward trends. To bypass the inaccurate estimation of intra-group standard deviation, we follow [40] to simply remove such term in advantage computation: = Ri P50(DR). It is noted that such recalibrated advantage enjoys three benefits: 1) the baseline performance correlates with the policy changes either towards better or worse; 2) the outdated experiences in the replay buffer can be further filtered out with both ˆAj > 0 and Aj > 0; 3) the difficulty bias of intra-group normalization on policy update can be mitigated. In this case, the updated off-policy self-imitation objective is: Ai (7) GRPO(πθ) = SIL {τj}ND j=1 {πθold (x), xp(X)} ND j=1 GRPO 1( ˆAj > 0 & Aj > 0), (8) 9 i GRPO = (cid:34) 1 t=1 (min(ri t(θ) Ai t, clip(ri t(θ), 1 ϵ, 1 + ϵ) Ai t) βDi KL(πθπref) SPEAR (cid:35) . (9) Regularization with Curriculum Schedule and Covariance-based Clipping The trade-off between exploration and exploitation has always been the core challenge in RL [73], where the policy entropy is often used as proxy to monitor if the policy has failed to explore new possible ways in return for suboptimal reward predictability. The policy entropy quantifies the confidence inherent in the actions triggered by the LLM. Under the context of agent tasks, we measure the average sequence level token-sum entropy of the entire trajectory τ for the policy model. Given the training data batch DB, the token-sum entropy is defined as: H(πθ, DB) = EDB,πθ [log πθ(τx)] = 1 DB xDB,xp(X) (st,at)τ Eatπθ [log πθ(atx, st)] (10) Through the lens of policy entropy, we observe an entropy collapsing pattern under the tool-integrated reasoning settings (i.e., ReTool [34]). As shown in Figure 3, the vanilla GRPO training without SIL first experiences rise in entropy which indicates the pattern transition from text-based reasoning to CI-based reasoning. Due to the feedback from the CI (e.g., code execution stderr and stdout), the policy at first exhibits uncertainty about its multi-turn actions. During RL, the policy is rewarded for certain tool-using strategy, and it becomes more certain on exploiting CI for double-check and reflection. Therefore, the entropy decreases at the later stage in exchange for more predictable, steady returns. However, the vanilla application of SIL as complementary loss causes inevitable, monotonic entropy dropping to near zero. Such entropy collapsing rules out the possibility of policy improvement as demonstrated by the stagnant performance on AIME 2025. We believe there exist two underlying reasons: The mechanical imitation of the experiences at the early stage restricts the exploration of novel reasoning paradigms. The early successful trajectories in the replay buffer are often of low diversity, where only the suboptimal solutions on simple, easy problems are stored with positive advantage. The overfitting of such trajectories reduces exploration and leads to overconfidence on the existing reasoning patterns. The changes of the output logits that are highly associated with the advantage gains greatly decrease the entropy in return for rewarded behavior [17, 74]. The covariance between the log probability and the advantage of action tokens is indicative for entropy control. The over-optimization on learning tokens with high covariance improves exploitation but sacrifices exploration for further improvement. We resort to regularization based on both curriculum and covariance for entropy control. For the former, we apply cosine warm-up schedule on the SIL term under the assumption that initially the transition of distribution towards exploration of diverse actions outweighs the imitation of fixed, limited solution patterns towards outcome success (see Figure 4). For the later, we remove tokens with high covariances [17] out of loss contribution, preventing aggressive changes of log probability for advantage acquisition. JTotal(πθ) = JGRPO(πθ) + γ SIL-R GRPO(πθ), γ = (cid:40) 1 2 (1 cos(π titer 1, Twarm-up )), titer Twarm-up, titer > Twarm-up, GRPO(πθ) = SIL-R {τj}ND j=1 {πθold (x), xp(X)} ND j=1 GRPO 1( ˆAj > 0 & Aj > 0) Mj, 10 (11) (12) (13) SPEAR (a) Entropy (seq-mean-token-sum-norm). (b) Accuracy on AIME 2025. Figure 3. Effect of our self-imitation on action-level strategy exploration (Qwen2.5-32B with code interpreter [34, 46]). The vanilla experience replay technique [37] that enforces early overfitting of the few available trajectories in the buffer causes entropy collapsing and exploration shrinkage. At the beginning, the LLM agent struggles at tool-calling skills and fails to cultivate the transition of distribution towards frequent tool utilization and tool-integrated reasoning. The naive replay limits the transformation of reasoning paradigm. In contrast, our SPEAR introduces both curriculumand covariancebased regularization into self-imitation. Its curriculum schedule with an increasing emphasis on the replay data allows easy acquisition of skills (e.g., tool-calling) at first, and stimulates strategic action plans later. The covariance clipping removes over-confident tokens, whose log probabilities are highly associated with their advantage gains, out of optimization. Our self-imitation gives promises to exploring novel strategies and achieves steady growth on AIME 2025. Mj = 0, 1, / clip, clip, Ii clip = Ind Uniform(tωlb Cov(log πθ(ai tx, si t), Ai t) ωub, Ni clip), Cov(log πθ(ai tx, si t), Ai t) = (log πθ(ai tx, si t) 1 j=1 log πθ(a tx, t)) ( Ai 1 j=1 Aj t), (14) (15) (16) where titer and Twarm-up respectively denote the training iteration step and the total warm-up steps. The lower bound and upper bound for determining the range of high-covariance tokens are respectively represented as ωlb and ωub. The operation Uniform(t, Nclip) refers to the uniform sampling of tokens with high covariance until budget of Nclip tokens. The indices of the selected tokens for loss masking are represented as Ind. It is noted that such masking introduces randomness which benefits the convergence of RL. The detailed settings of ωlb, ωub, and Nclip are subject to both the LLM and the task. We empirically set the rounded integers of the mean covariance in the range of top 20% and top 0.02% respectively for ωlb and ωub, and set Ni clip = λNi with Ni being the total number of learnable tokens of τi and λ denoting the clipping ratio. SPEAR Figure 4. Visualization of γ with Twarm-up = 200. The weight of SIL loss gradually increases from 0 to 1 in the first Twarm-up steps. 4."
        },
        {
            "title": "Intrinsic Reward Shaping",
            "content": "During RL, we propose to guide the agentic LLM with composite reward that not only considers the accuracy of the final outcome but also the beneficial behaviors that are promising to achieve the goal. Without losing generality, we adopt sparse outcome accuracy reward, dense, continuous tool-call reward, and simple format reward. Outcome Reward binary signal is assigned at the end of episode according to the pre-defined verification rules. (cid:40) Ri outcome = τi succeeds, 1, 1, otherwise. (17) Tool-call Reward To incentivize multi-turn interactions, an action-based reward that is proportional to the number of tool call turns is added. To avoid reward hacking where the LLM repeats meaningless tool calling, the action reward is confined smaller than the outcome reward. Ri tool-call = min(1, 0.1 ntool-call), ntool-call 0, (18) where ntool-call denotes the number of valid tool call turns in the trajectory τi. Format Reward negligible reward is assigned to the trajectory if the models output contains valid wrapping format given the task descriptions (e.g., <think>...</think><action>...</action>). Ri format = (cid:40) 0.1, 0, is wrapped correctly, (si t, ai t, Ri t) τi if ai otherwise. (19) SPEAR (a) Number of tool-call turns. (b) Accuracy on AIME 2025. Figure 5. Effect of our intrinsic reward on skill-level strategy exploration (Qwen2.5-32B with code interpreter [34, 46]). The baseline does not consider tool-calling as rewarded behavior and its number of interaction with the environment drops quickly due to the negative feedback (e.g., bad codes that ignore import of modules, mention undefined variables, pass unexpected indentation, and forget print of intermediate results). In this case, the LLM gives up coding and degrades to text-based reasoning. The vanilla tool-call reward, despite being effective in learning tool-call skills at first, causes competition with the accuracy reward later. Due to the limited context length, the excessive tool-call turns prevents submission of the final answer and thereafter the accuracy declines immediately. We propose to apply the curriculum schedule as an intrinsic reward design where its strength decays over step to allow the agent to merely focus on the accuracy upon mastering tool-call skills. It prevents reward hacking for unnecessarily long interactions. Dominance of Outcome Reward with Curriculum Schedule Although many previous studies [75, 76, 77, 78, 49, 79, 80, 81, 82, 83, 84] have experimented with various design of auxiliary rewards for improving the usage (e.g., execution success rate) of tools, our preliminary experiments show that the addition of tool-call reward is double-edged sword. As shown in Figure 5, the agentic LLM trained without the tool-call reward fails to develop tool-integrated reasoning for solving mathematic problems under the DAPO settings [46, 34]. The reason behind such degradation to text-based reasoning paradigm is that the role of tool usage in reward acquisition is not consistent at the beginning. The success rate of tool call execution is around 0.75 initially for Qwen2.5-32B-Instruct where most of the errors are caused by coding. Since the LLM has to fill in the written code as the argument of the interpreter function, there exist three common error patterns: 1) missing import of third-party modules such as numpy or scipy; 2) reference to variables that are defined in the history interaction context but remain undefined in the current code; 3) unexpected indentation error in the while or for loops; and 4) forgetting to print all the intermediate results that end up with empty stdout. Without the tool-call reward, the LLM agent has to deal with these errors that are newly introduced by tool calling but cause detriments to the final accuracy. Therefore, it quickly gives up tool-call and turns to pure text-based reasoning. On the other hand, the enforcement of tool-call reward stimulates an increasing number of interaction turns during training, leading to over-long responses (e.g., longer than 16K tokens) that fail to parse final answers. Such excessive tool-calls are unnecessary and detrimental to improving the outcome reward where an oscillation is observed at the later stage for both the number of tool-call turns and the performance on AIME 2025. To achieve balance between reward terms, we alleviate the competition for dominance against the outcome reward by applying the scheduled decay on the tool-call reward: Ri = Ri outcome + µ Ri tool-call + Ri format, µ = (cid:40) 1 2 (cos(π titer Tdecay 0, ) + 1), titer Tdecay, titer > Tdecay, 13 (20) (21) where the Tdecay denotes the number of decaying steps. Figure 6 illustrates the total intrinsic reward where the dominance of the outcome reward is guaranteed. SPEAR Figure 6. Visualization of the composite intrinsic reward of outcome reward, tool-call reward, and format reward (Tdecay = 200). The tool-call reward gradually decays from 1 to 0 in the first 200 training steps."
        },
        {
            "title": "4.3 Dr.BoT as A Strong Baseline",
            "content": "To provide strong baseline for the agentic performance after RL, we refer to the existing studies [85, 86, 87] for the enhancement of diverse exploration, stable convergence, and effective training. Our baseline, termed as Dr.BoT, consists of the following bag-of-tricks modifications to the vanilla GRPO [39]. Removal of KL Divergence We follow [46, 40] to simply remove the KL divergence by setting β = 0. This allows the distribution of the LLM to diverge from the initial policy π0 for adaptation to tool-integrated reasoning under the agent tasks. Clip-Higher We follow [46] to raise the upper clip bound ϵub = 0.28 and keep the lower bound ϵlb = 0.2 as default. The decoupled lower and higher clipping range leaves more space for the increase of low-probability tokens. It relaxes the exploration of the policy which benefits premature entropy collapsing. Removal of Intra-group Normalization We follow [40] to drop the advantage normalization term where the standard deviations lead to difficulty bias in optimization. It has two benefits: 1) The samples with smaller intra-group standard deviations contribute more to the policy update and the removal of normalization allows balancing between samples of various difficulty; 2) The estimation of standard deviations are inaccurate for the off-policy advantage recalibration of replay samples. It is challenging to measure the sampling diversity of specific group. Removal of Length Normalization We follow [40] to drop the length normalization terms. We choose the token-level sum and sequence-level normalization as the aggregation approach for both loss computation and the entropy monitoring. SPEAR Filtering of Over-long and Void-turn Samples We follow [83, 46] to mask out the loss for rollout samples that exceed the predefined maximum response length. The improper reward shaping for overlong samples introduces noise into training, which causes instability of training. Besides, it prevents from test-time scaling when the context length of evaluation is longer than that of training. In addition, we mask out all the trajectories with void turns [18], where the LLM fails to call any tools in the response. Such void turns are often accompanied with the occurrence of repetitive reasoning contents, wrong chat-template formatting, and nonsensical tokens. The filtering of these void-turn samples prevents mode-collapsing where their distribution deviate severely from the initial policy. Filtering of Low-variance Groups We follow [33] to only keep groups with high intra-group variance for each batch of training samples. The bottom 25% samples with small intra-group reward standard deviations are removed to keep the policy update informative. High intra-group variance indicates diverse agent behaviors and the contrast between different actions is beneficial to exploitation."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we present the empirical evaluations of our SPEAR across variety of LLM agent tasks."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Environments In accordance with our design of agent action space (see Section 3.2), we train the LLM agents on three challenging benchmarks: ALFWorld [5], WebShop [42], and DAPO-Math-17K [46]. ALFWorld is an interactive environment created to evaluate how well LLM agents can handle multi-step decisionmaking tasks. In each scenario, the agent is given textual goal and must achieve it by engaging in multiple rounds of interaction with the environment. The platform offers 4,639 task examples spanning six typical household activity categories: Pick & Place (Pick), Examine in Light (Look), Clean & Place (Clean), Heat & Place (Heat), Cool & Place (Cool), and Pick Two & Place (Pick2). WebShop, on the other hand, is sophisticated web-based platform aimed at assessing LLM agents in authentic online shopping situations. Agents are required to interact with simulated HTML shopping site to search for products, browse items, and purchase an appropriate product. WebShop supports broad and varied action space, featuring more than 1.1 million products and 12K user instructions. DAPO-Math-17K is rigorously engineered, competitiongrade benchmark designed to stress-test large-scale RL on LLM agents. The agent must develop multi-step mathematical reasoning, perform strategic tool-calling for code verification, and reflect on feedback from the sandbox before submitting its final answer. It contains 17K manually-curated prompts sourced from olympiad-level problems, each transformed so that every ground-truth label is an integereliminating symbolic-parsing noise and yielding clean, deterministic reward signal. Baselines With respect to ALFWorld and WebShop, we follow [41] to report range of competitive baselines such as prompting-based method (i.e., direct I/O) for the proprietary models GPT-4o [88] and Gemini [89], framework-based method such ReAct [4] and Reflexion [90], RL methods including PPO [20], RLOO [91, 92], GRPO [39, 2], GiGPO [41], and our proposed strong baseline Dr.BoT. For the DAPO-Math-17K, we follow [34] to report the performance of domain-specific expert (e.g., Qwen2.5-Math [93]), existing reasoning models (e.g., Sky-T1 [94], o1 [45], DeepSeek-distilled Qwen 32B [2], QwQ [95], and s1 [96]), and the tool-integrated RL counterparts (e.g., ReTool [34], SimpleTIR [18], ZeroTIR [31], and AFM [97]). 15 SPEAR Training Details For ALFWorld and WebShop, we follow [41] to use Qwen2.5-1.5B-Instruct and Qwen2.57B-Instruct [93] as our base models. For DAPO-MATH-17K, we follow [34] to use Qwen2.5-32B-Instruct [93] for fair comparison. In addition, we use the latest Qwen3-32B-Instruct [98] for generalization studies. All the training settings and the details on hyper-parameters can be found in the appendix Section A.2. We use VeRL [99] as our codebase for RL."
        },
        {
            "title": "5.2 Performance",
            "content": "Table 1. Performance on ALFWorld and WebShop (%). For ALFWorld, we report the average success rate for each subtask as well as the overall results. For WebShop, we report the average score and the success rate (SR). PT, FW, and RL stand for prompting, framework, and reinforcement learning, respectively. Type Method Pick Look Clean Heat Cool Pick2 All Score SR ALFWorld WebShop PT PT I/O I/O 75. 60.8 31.2 GPT-4o 56.7 Gemini-2.5-Pro 21.6 49. 48.0 31.8 23.7 92.8 63.3 62. 69.0 26.6 58.7 60.3 42.5 35. I/O PT FW ReAct FW Reflexion RL RL RL RL RL RL RL RL RL RL RL PPO RLOO GRPO + SPEAR (ours) Dr.BoT (GRPO) + SPEAR (ours) GiGPO w/std GiGPO w/o std + SPEAR (ours) Dr.BoT (GiGPO) + SPEAR (ours) I/O PT FW ReAct FW Reflexion RL RL RL RL RL RL RL RL RL RL RL PPO RLOO GRPO + SPEAR (ours) Dr.BoT (GRPO) + SPEAR (ours) GiGPO w/std GiGPO w/o std + SPEAR (ours) Dr.BoT (GiGPO) + SPEAR (ours) Qwen2.5-1.5B-Instruct 4.2 9.7 7.7 6.2 19.4 13.6 46.4 60.6 66.4 62.8 59.7 78.2 88.3 87.4 72.8 81.8 88.3 95.1 79.8 94.4 71.7 91.3 88.8 94.0 85.4 93.8 87.6 99.0 3.3 15.7 21.7 57.1 71.0 84.5 96.4 81.0 94.1 94.8 91.8 89.1 93.7 96.1 Qwen2.5-7B-Instruct 2.8 6.9 19.3 18.2 13.2 34.3 36.3 30.9 44.9 80.3 89.5 92.5 71.9 81.3 87.3 72.5 74.7 89.3 83.1 78.0 97.2 90.4 92.8 93.8 89.2 88.5 97.1 89.3 83.7 98.8 86.5 90.2 95.9 92.6 92.8 98.0 91.8 92.8 96.9 89.9 96.4 95.6 5.5 20.5 22.2 40.5 52.8 53.7 80.9 75.8 72.2 67.5 76.5 79.2 91.4 86.5 21.6 35.4 41.6 64.0 78.2 66.1 62.4 95.8 97.9 82.7 88.6 82.4 99.9 85.1 5.9 17.4 35.3 64.8 88.3 85.3 93.9 92.2 91.2 94.4 96.0 95.2 98.6 96. 33.4 48.5 62.0 92.3 87.6 90.8 93.7 99.9 98.8 97.7 91.8 99.9 98.3 99.9 0.0 2.0 3.7 47.4 56.9 53.5 79.1 61.9 74.4 76.4 79.5 95.5 78.4 91.6 3.2 17.6 23.8 68.8 48.9 64.7 75.5 80.6 87.2 79.2 85.2 86.6 88.3 95.1 4.1 12.8 21.8 54.4 69.7 72.8 88.9(+16.1%) 79.1 87.7(+8.6%) 86.7 86.1 91.2(+5.1%) 90.6 93.2(+2.6%) 14.8 31.2 42.7 80.4 75.5 77.6 85.2(+7.6%) 92.4 93.8(+1.4%) 90.8 90.2 94.1(+3.9%) 94.0 94.7(+0.7%) 23.1 40.1 55.8 73.8 73.9 75.8 90.0 78.7 88.4 83.1 83.5 90.7 84.1 90. 26.4 46.2 58.1 81.4 80.3 79.3 92.4 90.4 91.4 84.4 86.2 92.7 90.7 92.5 5.2 11.3 21.9 51.5 52.1 56.8 77.5(+20.7%) 62.9 76.8(+13.9%) 65.0 67.4 79.3(+11.8%) 68.8 81.1(+12.2%) 7.8 19.5 28.8 68.7 65.7 66.1 84.6(+18.5%) 80.5 84.8(+4.3%) 72.8 75.2 83.8(+8.6%) 81.8 85.7(+3.9%) Table 1 demonstrates the effectiveness of our proposed SPEAR on both ALFWorld and WebShop. We show that the SPEAR is compatible with various GRPO-like baselines including the vanilla GRPO [39], the 16 SPEAR Table 2. Performance (mean@30) on AIME 2024 and AIME 2025 (%). : Results of the official ReTool implementation under the VeRL framework are cited where the GRPO already utilizes multiple DAPO tricks for faster convergence and better performance. : Official results reported by Qwen [98]. PT and RL stand for prompting and reinforcement learning, respectively. Type Method Model Tool Context Train Test AIME24 AIME PT PT PT PT PT PT PT PT PT PT RL RL RL RL RL RL RL RL RL RL PT PT PT RL RL RL RL I/O I/O I/O I/O I/O I/O I/O I/O Qwen2.5-Math-72B Qwen2.5-Math-72B CI Sky-T1 o1-preview DeepSeek-R1-Qwen-32B QwQ-32B-Preview QwQ-32B s1-32B Qwen2.5-32B-Instruct I/O Qwen2.5-32B-Instruct I/O PPO Qwen2.5-32B-Instruct GRPO Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct ReTool Qwen2.5-32B-Instruct SimpleTIR Qwen2.5-32B-Instruct ZeroTIR Qwen2.5-32B-Instruct AFM Dr.BoT (GRPO) Qwen2.5-32B-Instruct + SPEAR (ours) Qwen2.5-32B-Instruct Dr.BoT (GRPO) Qwen2.5-32B-Instruct + SPEAR (ours) Qwen2.5-32B-Instruct Qwen3-32B-Instruct I/O I/O Qwen3-32B-Instruct Qwen3-32B-Instruct I/O Qwen3-32B-Instruct Dr.BoT (GRPO) + SPEAR (ours) Qwen3-32B-Instruct Dr.BoT (GRPO) Qwen3-32B-Instruct + SPEAR (ours) Qwen3-32B-Instruct CI CI CI CI CI CI CI CI CI CI CI CI CI CI CI CI 16K 16K 16K 12K 8K 32K 16K 16K 32K 32K 16K 16K 32K 32K 30.0 4K 40.0 4K 32K 43.3 128K 44.6 47.0 32K 50.0 32K 79.5 38K 56.7 32K 16K 16K 16K 16K 16K 12K 8K 32K 16K 16K 32K 32K 16K 38K 16K 16K 16K 32K 32K 13.4 29.6 67.0 59.9 56.7 66.7 64.7 66.3(+1.6%) 67.2 71.0(+3.8%) 68.5 81.4 31.1 81.3 81.8(+0.5%) 82.5 85.6(+3.1%) 37.9 33.5 69.5 12.9 23.1 55.0 60.0 49.3 49.2 33.3 59.8 54.0 60.1(+6.1%) 55.1 61.0(+5.9%) 53.5 72.9 24.4 74.1 78.8(+4.7%) 77.3 80.5(+3.2%) 17 SPEAR competitive GiGPO [41], and our proposed strong baseline Dr.BoT. SPEAR brings consistent performance gains over baselines across 1.5B and 7B models up to 20%. We believe such generalization benefits from the collection of successful trajectories during training, which acts as walkthrough guide to the agent. Especially for these two tasks where the success rate is fairly low at the beginning, the agent has to figure out the underlying logic of interaction with the environment (e.g., the simulated world and the offline website), and summarizes from the trajectories for its own action paradigm tailored specific to the tasks. In this case, the replayed successful trajectories expedite the accumulation of promising reasoning-action experiences, and thereafter reduce blind trials and errors under the given budget of interaction turns. Furthermore, our proposed Dr.BoT boosts the performance of either GRPO and GiGPO up to 15%, showcasing that the mixture of tricks indeed benefits the trade-off between exploration and exploitation, and stabilizes agentic training on long-horizon tasks. Table 2 reports the performance under the settings of CI-integrated reasoning for solving math problems. As shown, the proposed Dr.BoT indeed outperforms series of recent RL methods as strong baseline. It is noted that the strict comparability among existing RL methods cannot be guaranteed due to the following reasons: 1) the dataset used for training, 2) the implementation framework, and 3) the hyper-parameter settings. Especially for the context length, we observe that the maximum number of tokens allowed during inference exists great effect on the reported results. For example, the results of pure-text reasoning on Qwen3 are reported under the context of 38K and the reduced context length impedes answer parsing from the overlong responses. Given the CI tool, the agentic LLM learns to exploit the feedback from the sandbox for double-check and self-reflection. It achieves comparable performance on Qwen3 but with much smaller token budget, highlighting the efficiency of reasoning with code. When the context is relaxed to 32K during inference, an improvement is observed on both Qwen2.5 and Qwen3, confirming the generalization of our SPEAR with more interactions turns and reasoning tokens. Due to the long reasoning nature of Qwen3, its performance with and without our SPEAR on the AIME24 remain almost the same under 16K budget where over 10% trajectories are overlong. With 32K tokens, our superiority is revealed on both AIME24 and AIME25."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Table 3. Ablation study on ALFWorld and WebShop. For ALFWorld, we report the average success rate (%) for each subtask as well as the overall result. For WebShop, we report both the average score and the average success rate (SR) (%). SI and IR stand for Self-Imitation and Intrinsic Reward, respectively. Type Method Pick Look Clean Heat Cool Pick2 All Score SR ALFWorld WebShop RL RL RL RL RL RL RL RL RL RL RL RL GRPO + SI + SI + IR (SPEAR) GiGPO w/o std + SI + SI + IR (SPEAR) GRPO + SI + SI + IR (SPEAR) GiGPO w/o std + SI + SI + IR (SPEAR) 85.3 86.8 93.9 96.0 93.2 95. 90.8 93.2 93.7 91.8 96.1 99.9 Qwen2.5-1.5B-Instruct 84.5 87.4 96.4 91.8 96.3 89.1 78.2 87.7 87.4 91.3 87.4 94.0 59.7 71.1 88.3 71.7 92.7 88.8 Qwen2.5-7B-Instruct 89.3 96.3 97.2 95.9 98.4 98.0 74.7 87.4 78.0 90.2 95.3 92.8 72.5 92.7 83.1 86.5 94.5 92.6 53.7 61.0 80.9 76.5 82.5 79.2 66.1 82.5 62.4 88.6 81.9 82.4 53.5 56.6 79.1 79.5 87.5 95. 64.7 87.5 75.5 85.2 83.9 86.6 72.8 77.3(+4.5%) 88.9(+16.1%) 86.1 90.6(+4.5%) 91.2(+5.1%) 77.6 90.6(+13.0%) 85.2(+7.6%) 90.2 93.6(+3.4%) 94.1(+3.9%) 75.8 85.1 90.0 83.5 89.4 90.7 79.3 90.4 92.4 86.2 94.6 92.7 56.8 74.2(+17.4%) 77.5(+20.7%) 67.4 79.0(+11.6%) 79.3(+11.8%) 66.1 83.4(+17.3%) 84.6(+18.5%) 75.2 87.5(+12.3%) 83.8(+8.6%) 18 SPEAR Table 4. Ablation study (mean@30) on AIME 2024 and AIME 2025 (%). : Results of the official ReTool implementation under the VeRL framework are cited where the GRPO already utilizes multiple DAPO tricks for faster convergence and better performance. SI and IR stand for Self-Imitation and Intrinsic Reward, respectively. Type Method Model Tool Context Train Test AIME24 AIME25 RL RL RL RL RL RL RL RL RL RL RL RL Qwen2.5-32B-Instruct CI Dr.BoT (GRPO) Qwen2.5-32B-Instruct CI + SI + SI + IR (SPEAR) Qwen2.5-32B-Instruct CI Qwen2.5-32B-Instruct CI Dr.BoT (GRPO) Qwen2.5-32B-Instruct CI + SI + SI + IR (SPEAR) Qwen2.5-32B-Instruct CI Qwen3-32B-Instruct Dr.BoT (GRPO) Qwen3-32B-Instruct + SI + SI + IR (SPEAR) Qwen3-32B-Instruct Qwen3-32B-Instruct Dr.BoT (GRPO) Qwen3-32B-Instruct + SI + SI + IR (SPEAR) Qwen3-32B-Instruct CI CI CI CI CI CI 16K 16K 16K 32K 32K 32K 16K 16K 16K 32K 32K 32K 16K 64.7 16K 63.8(-0.9%) 16K 66.3(+1.6%) 32K 67.2 32K 66.0(-1.2%) 32K 71.0(+3.8%) 16K 81.3 16K 81.2(-0.1%) 16K 81.8(+0.5%) 32K 82.5 32K 81.8(-0.7%) 32K 85.6(+3.1%) 54.0 56.9(+2.9%) 60.1(+6.1%) 55.1 60.5(+5.4%) 61.0(+5.9%) 74.1 75.8(+1.70%) 78.8(+4.70%) 77.3 78.2(+0.9%) 80.5(+3.2%) To investigate the effectiveness of our SPEAR, we conduct ablation studies on various tasks and baselines. Tables 3 and 4 respectively report the results on ALFWorld and WebShop, and DAPO-MATH-17K. Self-Imitation The SIL improves over baselines consistently on the ALFWorld and WebShop across model scales. Since either 1.5B or 7B models perform poorly at the early stage (i.e., success rate < 15%), past experiences are quite beneficial to LLM agents for exploring promising strategies. The re-use of the precious trajectories facilitates convergence and prevents mechanical trials especially for small LLMs. For DAPOMATH-17K, the performance on AIME24 dropped bit by self-imitation but that on AIME25 still gets improved. Such fluctuation is related to the phenomenon we observed in Figure 5 where the imitation of samples with multiple tool calls leads to rapid increase of interaction turns and thereafter causes training instability. The competition between different reward terms affects the robust selection of correct, good experiences, ultimately degrading the effectiveness of self-imitation. Intrinsic Reward For the ALFWorld and WebShop tasks, we observe that the rewarding of interaction turns benefit 1.5B models consistently. For 7B models, two outliers are found where the self-imitation alone brings the most performance gains (up to 13%). Such exception might be related to both the task definition and the RL algorithm in use. One should experiment with different combinations in practice to determine the optimal settings. For DAPO-MATH-17K, we find that the intrinsic reward is indispensable for both Qwen2.5 and Qwen3 models because the reward to encourage more interactions with the sandbox is critical to the transform from text-based reasoning to tool-integrated reasoning. Both the two models have been pretrained on massive mathematic corpus and tend to solve maths problems with textual reasoning. The intrinsic reward promotes frequent tool calling and such rich observation signals from the sandbox motivate the agent to correct coding errors, check the validity of the answer, and reflect on alternative solutions."
        },
        {
            "title": "5.4 Discussions on Hyper-parameters",
            "content": "We investigate the following key hyper-parameters (see Figure 7) of Dr.BoT (GRPO) with SPEAR on WebShop (Qwen2.5-7B-Instruct) while keeping the value of others fixed (see Table 8). 19 SPEAR (a) Replay Buffer Size ND. (b) Baseline Buffer Size NDR . (c) Covariance-based Clipping Ratio λ. (d) Warm-up Steps Twarm-up. (e) Decay Steps Tdecay. Figure 7. Effect of hyper-parameters of Dr.BoT (GRPO) with SPEAR on WebShop (Qwen2.5-7B-Instruct). 20 SPEAR Replay Buffer Size ND As the buffer size increases, the performance first improves due to the improved diversity and impact of the collected trajectories in the buffer. However, when the buffer continues to expand, trajectories in the buffer might come from earlier training batches and thereafter causes more severe degree of off-policy. The self-imitation of excessively outdated experiences becomes detrimental to the update of current policy. In addition, the large replay buffer takes more iterations to refill and thereafter the policy update frequency from self-imitation is lower than that of smaller buffer, further diminishing its intervention in agent exploration. = 0, the original advantages are used without recalibration and Baseline Buffer Size NDR When NDR filtering (see Equation 8). It shows that the direct imitation of these experiences can be suboptimal where certain trajectories are outdated for the current policy. By timely adjusting the advantages and removing inappropriate experiences ( Aj 0), we reduce the inaccurate estimation for off-policy update. We also find that NDR should not be set too large as such 50-th percentile reward deviates from the latest ones, decreasing the effectiveness of recalibration. Covariance-based Clipping Ratio λ The clipping ratio can be viewed as the degree of regularization for policy entropy, where larger ratio causes more tokens to be ignored during policy update. In this case, the contribution of self-imitation gets weakened. modest range of clipping ratio (e.g., 0.0002 0.02) not only suffices the entropy management but also allows proper exploitation of the collected experiences. Warm-up Step Twarm-up smaller warm-up step implies earlier self-imitation of the premature, suboptimal experiences during RL. Especially when the distribution of the task and environment differs greatly from the pre-trained knowledge, the overfitting of the initial trajectories hinders exploration of low-probable solutions and leads to action-level local optima. Intuitively, Twarm-up can be first set the same as the total number of training steps and then adjusted according to the task and the model for the improved performance. Decay Step Tdecay smaller decay step reduces the stimulation from the intrinsic reward for acquisition of tool-use skills. If the LLM already excels at interacting with the environment (e.g., use of tools and comprehension of observations), Tdecay can be set close to 0. large Tdecay is not encouraged as the interference with the outcome reward causes inconsistent policy optimization for convergence."
        },
        {
            "title": "5.5 Qualitative Analysis",
            "content": "Skill Development We follow [34] to analyze the coding capabilities of the agent before and after RL by classifying the purpose of the code snippets. Specifically, we employ Hunyuan-Large [100] to interpret reasoning contexts before each tool-calling and judge the intention of the codes passed into the code interpreter on DAPO-MATH-17K dataset. The code purposes with their frequency over twice are kept and illustrated in Figure 8. We find that the after RL, the agent becomes proficient in writing codes for solution finding, problem-solving, and solution verification, which is quite advanced compared with the basic calculation and computation before RL. Our case study shows that after training, the agent learns to master matrix computation with numpy for higher efficiency, confirming the improvement of coding skills. Action Refinement As shown in Figure 9, the agent initially aims at finding the target product that satisfies all the constraints simply by searching. However, such continuous choice of the action search is trapped by 21 SPEAR (a) Before RL training. (b) After RL training. (c) The evolution of efficient coding from the purpose of computation to verification. Figure 8. Development of the agents coding skills. 22 SPEAR Figure 9. The advancement of strategy from the search query perfectionism to goal-oriented active progression. SPEAR the unqualified retrieval results. The attributes of product such as color and size should be determined only at the product page. After RL, the agent jumps out of the perfectionism for the search queries and tries to break the task step by step. It learns to choose the action wisely for persistent focus on the task."
        },
        {
            "title": "5.6 Training Cost and Complexity",
            "content": "Table 5. Comparison on the complexity of the vanilla GRPO and the proposed SPEAR. PG, FW, BP, RB, and Adv respectively stand for the policy gradient loss computation, forward, back-propagation, replay buffer, and advantage. Out of simplicity, we use the O(M) to denote the forward FLOPs which is positively associated with the model size and the input length. O(P) denotes the BP operations proportional to the number of LLM parameters. We use nSIL to refer to the equivalent number of off-policy update (by SIL) per on-policy update. After filtering by ˆAj > 0 & Aj > 0 (Equation 8), the number of samples in SIL is represented as K, ND. Training Stage Computation of GRPO (vanilla) Additional Computation by SPEAR Description On-policy Rollout RB Update On-policy PG On-policy BP RB Filtering Adv Recalibration 2GTO(M) GTO(M) O(P) Replay PG Replay BP In Total 3GTO(M) + O(P) O(GT) O(ND) O(ND)+O(NDR nSILKTO(M) + nSILO(KT) nSILO(P) nSIL(KTO(M) + O(P)) Dominance by FW & BP FW & sampling w/ πθold . Copy operation (negligible). FW w/ πθ (w/o KL πθref). BP w/ πθ. Look-up operation (negligible). Additive operation (negligible). FW w/ πθ, token-wise clip& min (negligible). BP w/ πθ. ) (a) ALFWorld. (b) WebShop. (c) DAPO-MATH-17K. Figure 10. The averaged policy training time (s) per step with and without the proposed SPEAR. We compare the computational complexity of our SPEAR with the vanilla GRPO algorithm in Table 5. Most of the computation comes from the forward and back-propagation of the filtered samples in the replay buffer. The memory operations such as the update and filtering of the buffer are light-weight and can be simply ignored. Given the current experimental settings (see Table 8), we observe that nSIL 0.5 for ALFWorld and WebShop, and nSIL 0.33 for DAPO-MATH-17K. In this case, our SPEAR additionally introduces around 10% 25% computation overhead with G. Such computation complexity is acceptable in practice as the time of each training iteration is dominated by that of on-policy rollout generation. Figure 10 shows the runtime per iteration step with and without the proposed SPEAR across different tasks and model scales. the total optimization procedure (including the rollout generation, advantage computation, log-probability inference, reward computation, and the actor update) is quite similar on 24 SPEAR average for ALFWorld, WebShop, and their SPEAR counterparts. For ALFWorld and WebShop, the 1.5B models exhibit larger variance than 7B models in training time. We believe such variance is associated with findings of the previous study [101] that the size of LLMs matters to the exploration diversity. Smaller LLMs are less diverse in exploring strategies due to their shallower reasoning nature, and are therefore prone to suboptimal policies with relatively increased stochasticity in training dynamics. For DAPO-MATH-17K, an increase around 5% and 26% is observed respectively on Qwen2.5 and Qwen3 models. Since the time per step is dominated by the rollout generation and actor update, we believe such increase in time is caused by the longer reasoning traces, more tool call interactions, and the additional iterations from the replay buffer. Such encouraged exploration by SPEAR is exaggerated on the reasoning model Qwen3 and leads to longer training time."
        },
        {
            "title": "5.7 Generalization on Vision-Language Agents",
            "content": "Table 6. Success rate (%) of the visual agent for playing Sokoban."
        },
        {
            "title": "Sokoban",
            "content": "Qwen2.5-VL-3B-Instruct I/O GRPO + SPEAR (ours) Dr.BoT (GRPO) + SPEAR (ours) GiGPO w/ std GiGPO w/o std + SPEAR (ours) Dr.BoT (GiGPO) + SPEAR (ours) 11.7 67.1 86.7(+19.6%) 76.0 85.4(+9.4%) 76.9 81.0 87.7(+6.7%) 81.3 87.9(+6.6%)"
        },
        {
            "title": "PT\nRL\nRL\nRL\nRL\nRL\nRL\nRL\nRL\nRL",
            "content": "(a) Before (step 15). (b) After (step 125). Figure 11. During training, the agentic LLM learns to push the box towards the target for reward. To test whether the proposed SPEAR is still complimentary to existing GRPO-like algorithms on training visual agents, we follow [41] to conduct experiments on the popular visual game Sokoban [102]. In this setting, the Qwen2.5-VL-3B-Instruct [103] is adopted as the agentic LLM to solve the puzzle game where the player must push the boxes along the grid towards target positions without hitting the walls. It challenges the agent on spatial comprehension and long-term planning capabilities. The grid size is of 6 6 and the visual agent receives both the visual (RGB arrays) and textual inputs as states. As shown in Table 6, the proposed method generally improves the performance on Sokoban with either GRPO, GiGPO, and the proposed Dr.BoT baselines. At first, the visual agent is unaware of the winning logic behind the game and wanders around for \"aimlessly\" exploration (see Figure 11(a)). After optimization, it not only comprehends the spatial relationship to control the box but also learns to stop moving when the task is completed."
        },
        {
            "title": "6 Conclusions and Limitations",
            "content": "In this paper, we target the pivotal challenge of balancing exploration and exploitation in RL training of LLM agents. Our proposed solution, SPEAR , extends the vanilla SIL by incorporating replay buffer of highreturn trajectories with advantage recalibration, scheduled entropy control, and intrinsic rewards. These components work in curriculum manner to prevent policy collapse and excessive uncertainty, progressively guiding the policy through smooth transition between exploration and exploitation. In addition, we propose strong baseline Dr.BoT tailored for agentic RL training with existing bag-of-tricks verified from numerical 25 SPEAR industrial practices. Empirical results across various tasks and models showcase SPEARs superiority over existing methods, with performance boosts and acceptable computational overhead. The effectiveness of our SPEAR underscores the value of learning from past experiences while managing policy entropy, offering robust framework for training LLMs with strong reasoning and tool integration skills. There exist two potential limitations: The vague definition of good experiences under highly complex, stochastic environments with unreliable tools. In such cases, the observations from the tool (e.g., online searching) or the environments (e.g., simulation of robotics, traffics, and financial trading) can be noisy and severely degrade the feasibility of the task. The sparse outcome reward cannot distinguish between good and bad experiences and therefore the relative advantages might be simply attributed to randomness instead of the agents behavior (e.g., reasoning capacity or action rationality). We suggest possible solution that more fine-grained, stepwise supervision should be enforced. For example, for each tool response or the environment feedback, an LLM-based judge determines whether the contents are meaningful for the final solution. In addition, step-wise process reward that evaluates the logical consistency [84] of the agents thought and action might be helpful in selecting good experiences (although it may also lead to negative outcomes). The rigidity of entropy control which relies on prior-based scheduling and covariance-based clipping. In the present study, we design regularization techniques based on our prior knowledge about policy entropy as proxy to monitor exploration and exploitation situations. Such scheduling and clipping designs might not be optimal for all kinds of agentic tasks. We believe more adaptive solution lies in the policys self-confidence on decisions under the observations of each task. For example, one might use the token-level dynamic reweighting for the SIL loss [104] which avoids over-concentration on certain low-probability reference tokens in the replay buffer. Similarly, the covariance-based clipping can also introduce the token-wise output probability into the sampling process to replace the vanilla bounded random sampling. We leave the exploration mentioned above as promising direction for improvement in the future."
        },
        {
            "title": "Contributions",
            "content": "SPEAR Authors Yulei Qin1* Xiaoyu Tan1* Zhengbao He1,2* Gang Li1 Haojia Lin1 Zongyi Li1 Zihan Xu1 Yuchen Shi1 Sheng Ye1,5 Ke Li1 Xing Sun1 Shaofei Cai1,3 Yuzheng Cai1,4 Xuan Zhang1,4 Siqi Cai1 Renting Rui1,2 Affiliations 4Fudan University 5Xiamen University 1Tencent Youtu Lab 2Shanghai Jiao Tong University 3Peking University Equal Contributions Yulei Qin Xiaoyu Tan Zhengbao He Acknowledgments We greatly thank the VeRL [99] and the VeRL-agent [41] communities for their implementation of various RL training and inference frameworks for multi-turn agent development. 27 SPEAR"
        },
        {
            "title": "References",
            "content": "[1] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, and Xing Sun. Incentivizing reasoning for advanced instruction-following of large language models. arXiv preprint arXiv:2506.01413, 2025. [4] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [5] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. [6] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37:100428100534, 2024. [7] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024. [8] Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying Peng, and Weinan Zhang. Mobileuse: gui agent with hierarchical reflection for autonomous mobile operation. arXiv preprint arXiv:2507.16853, 2025. [9] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023. [10] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [11] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [12] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025. [13] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via informationseeking formalization. arXiv preprint arXiv:2507.15061, 2025. [14] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 28 SPEAR [15] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [16] Ronald Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241268, 1991. [17] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. [18] Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning, 2025. URL https:// arxiv.org/abs/2509.02479. [19] Brian Ziebart, Andrew Maas, Andrew Bagnell, Anind Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 14331438. Chicago, IL, USA, 2008. [20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [21] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pages 18611870. Pmlr, 2018. [22] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning, pages 13521361. PMLR, 2017. [23] Rui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement learning. In International Conference on Machine Learning, pages 75537562. PMLR, 2019. [24] Bo Xin, Haixu Yu, You Qin, Qing Tang, and Zhangqing Zhu. Exploration entropy for reinforcement learning. Mathematical Problems in Engineering, 2020(1):2672537, 2020. [25] Chuheng Zhang, Yuanying Cai, Longbo Huang, and Jian Li. Exploration by maximizing rényi entropy for reward-free rl framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1085910867, 2021. [26] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient exploration. In International conference on machine learning, pages 94439454. PMLR, 2021. [27] Negar Mehr, Mingyu Wang, Maulik Bhatt, and Mac Schwager. Maximum-entropy multi-agent dynamic games: Forward and inverse solutions. IEEE transactions on robotics, 39(3):18011815, 2023. [28] Woojun Kim and Youngchul Sung. An adaptive entropy-regularization framework for multi-agent reinforcement learning. In International Conference on Machine Learning, pages 1682916852. PMLR, 2023. [29] Jianye Hao, Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Zhaopeng Meng, Peng Liu, and Zhen Wang. Exploration in deep reinforcement learning: From single-agent to multiagent domain. IEEE Transactions on Neural Networks and Learning Systems, 35(7):87628782, 2023. [30] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. [31] Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. 29 SPEAR [32] Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao.notion. site/off-policy-rl. [33] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. [34] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. [35] Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, and Xing Sun. Unleashing the power of data tsunami: comprehensive survey on data assessment and selection for instruction tuning of language models. Transactions on Machine Learning Research, 2025. [36] Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. arXiv preprint arXiv:2508.11408, 2025. [37] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International conference on machine learning, pages 38783887. PMLR, 2018. [38] Johan Ferret, Olivier Pietquin, and Matthieu Geist. Self-imitation advantage learning. arXiv preprint arXiv:2012.11989, 2020. [39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [40] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [41] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. [42] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. [43] AIME. Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/AIME_ Problems_and_Solutions, 2025. [44] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [45] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [46] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [47] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 30 SPEAR [48] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. [49] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. [50] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976, 2025. [51] Deepak Pathak, Pulkit Agrawal, Alexei Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 27782787. PMLR, 2017. [52] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. Advances in neural information processing systems, 29, 2016. [53] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016. [54] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: study of count-based exploration for deep reinforcement learning. Advances in neural information processing systems, 30, 2017. [55] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016. [56] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without reward function. arXiv preprint arXiv:1802.06070, 2018. [57] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [58] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018. [59] Yangchen Pan, Jincheng Mei, Amir-massoud Farahmand, Martha White, Hengshuai Yao, Mohsen Rohani, and Jun Luo. Understanding and mitigating the limitations of prioritized experience replay. In Uncertainty in Artificial Intelligence, pages 15611571. PMLR, 2022. [60] Baturay Saglam, Furkan Mutlu, Dogan Cicek, and Suleyman Kozat. Actor prioritized experience replay. Journal of Artificial Intelligence Research, 78:639672, 2023. [61] Tanmay Gangwani, Qiang Liu, and Jian Peng. Learning self-imitating diverse policies. arXiv preprint arXiv:1805.10309, 2018. [62] Yunhao Tang. Self-imitation learning via generalized lower bound q-learning. Advances in neural information processing systems, 33:1396413975, 2020. [63] Georgiy Pshikhachev, Dmitry Ivanov, Vladimir Egorov, and Aleksei Shpilman. Self-imitation learning from demonstrations. arXiv preprint arXiv:2203.10905, 2022. [64] Teng Xiao, Mingxiao Li, Yige Yuan, Huaisheng Zhu, Chao Cui, and Vasant Honavar. How to leverage demonstration data in alignment for large language model? self-imitation learning perspective. arXiv preprint arXiv:2410.10093, 2024. [65] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025. 31 SPEAR [66] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: learning environment for text-based games. In Workshop on Computer Games, pages 4175. Springer, 2018. [67] Bytedance-Seed-Foundation-Code-Team, :, Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bo Li, Bowen Li, Linyi Li, Boyi Liu, Jiaheng Liu, Kaibo Liu, Qi Liu, Shukai Liu, Siyao Liu, Tianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Z. Y. Peng, Kai Shen, Jiahao Su, Jing Su, Tao Sun, Yifan Sun, Yunzhe Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite Wang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xia Xiao, Yongsheng Xiao, Chenguang Xi, Shulin Xin, Jingjing Xu, Shikun Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jianbo Yuan, Jun Zhang, Yufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, and Ming Zhu. Fullstack bench: Evaluating llms as full stack coders, 2025. URL https://arxiv.org/abs/2412.00535. [68] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [69] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017. [70] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [71] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [72] Sha Luo, Hamidreza Kasaei, and Lambert Schomaker. Self-imitation learning by planning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 48234829. IEEE, 2021. [73] Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1): 944, 1988. [74] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. [75] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [76] Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-augmented reward modeling. arXiv preprint arXiv:2310.01045, 2023. [77] Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, and Sean Hendryx. Agentrlvr: Training software engineering agents via guidance and environment rewards. arXiv preprint arXiv:2506.11425, 2025. [78] Yu Xia, Jingru Fan, Weize Chen, Siyu Yan, Xin Cong, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Agentrm: Enhancing agent generalization with reward modeling. arXiv preprint arXiv:2502.18407, 2025. [79] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. [80] Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. Autotir: Autonomous tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836, 2025. [81] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. 32 SPEAR [82] Heng Lin and Zhongwen Xu. Understanding tool-integrated reasoning. arXiv preprint arXiv:2508.19201, 2025. [83] Richard Zhuang*, Trung Vu*, Alex Dimakis, and Maheswaran Sathiamoorthy. Improving multi-turn tool use with reinforcement learning. https://www.bespokelabs.ai/blog/improving-multi-turn-tooluse-with-reinforcement-learning, 2025. Accessed: 2025-04-17. [84] Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844, 2025. [85] Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, et al. Part i: Tricks or traps? deep dive into rl for llm reasoning. arXiv preprint arXiv:2508.08221, 2025. [86] Zetian Sun, Dongfang Li, Zhuoen Chen, Yuhuai Qin, and Baotian Hu. Stabilizing long-term multi-turn reinforcement learning with gated rewards. arXiv preprint arXiv:2508.10548, 2025. [87] Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Towards effective code-integrated reasoning. arXiv preprint arXiv:2505.24480, 2025. [88] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [89] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [90] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [91] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! 2019. [92] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [93] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [94] NovaSky Team. Sky-t1: Train your own o1 preview model within $450, 2025. [95] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. [96] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [97] Weizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen, Weichen Sun, Qiexiang Wang, et al. Chain-of-agents: End-to-end agent foundation models via multi-agent distillation and agentic rl. arXiv preprint arXiv:2508.13167, 2025. [98] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 33 SPEAR [99] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [100] Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024. [101] Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. [102] Max-Philipp B. Schrader. gym-sokoban. https://github.com/mpSchrader/gym-sokoban, 2018. [103] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [104] Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629, 2025. [105] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. SPEAR"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Pseudo Code Algorithm 1 summarizes the full training procedure of the proposed SPEAR. It is noted that our SPEAR is compatible with various baselines such as GRPO [39] and GiGPO [41], enjoying high-level of generalization. Specifically, the algorithm is featured by: Maintenance of replay buffer and baseline buffer that respectively stores the trajectories for good experience replay and estimates the current policys average performance; Recalibration of the previous advantages for off-policy update; Regularization against the pre-mature entropy collapsing; Shaping of the composite intrisic rewards for dominance of the outcome reward. Compared with the vanilla GRPO-like training, the proposed method only introduced: Additional policy update iterations positively associated with the number of ND in terms of computational complexity; replay buffer of the size ND and baseline performance buffer of the size NDR in terms of space complexity. Since we re-utilize previous trajectories without completely re-computing the rollout generation, logprobability estimation, and the advantages, such operations are light-weight and incur minimal computation overhead. In the present study, we empirically set ND = 2048 without meticulous hyper-parameter tuning. For both ALFWorld, WebShop, and Sokoban, the number of trajectories per data batch is the product of train_batch_sizen_samples_per_prompt=256 and there exist around 4K turn-level training samples under the VeRL-agent [41] framework. For the DAPO-MATH-17K, the number of trajectories per data batch is 2048 and there exist exactly 2048 trajectory-level training samples under the VeRL [99] framework. In this case, our replay buffer reaches its full capacity around every two or three training steps on average for all experiments. For each policy update by self-imitation, the number of iterations is comparable to that of the vanilla policy update by GRPO under the present settings. The detailed analyses on the training cost and complexity can also be found in Section 5.6. 35 Algorithm 1 Training Agentic LLMs with SPEAR Require: Initial policy πθold , data distribution p(X), clipping bounds ϵlb, ϵub, KL penalty β (β = 0), replay buffer with buffer size ND, intra-group baseline buffer DR with buffer size NDR , the warm-up factor γ with the number of warm-up steps Twarm-up, covariance clipping bounds ωlb, ωub, the covariance-based clipping ratio λ (λ = 0.02), the decay factor µ with the number of decay stepsTdecay, the group size G, the maximum allowed interaction turns T. SPEAR Ensure: Updated policy πθ 1: Initialze = and DR = 2: for each training step titer do 3: Update the old policy model: θold θ 4: 5: 6: 7: # Repeat batch sampling and rollout generation for trajectories Sample data batch with each unique sample p(X) # Sample trajectories {τi}G for = 1 to do i=1 for each Initialize environment states si 1 # Sample at most actions for = 1 to do Sample action ai Execute actions, receive rewards Ri πθ(x, si t) end for Organize the trajectory τi = {(si t, observe the new states si t+1 1, ai 1, Ri 1), (si 2, ai 2, Ri 2), ..., (si T, ai T, Ri T)} end for # Apply intrinsic reward shaping for advantage estimation Compute the vanilla objective JGRPO(πθ) via Equation 2 with the decay-scheduled Ri via Equation 20 # Maintain the replay buffer and the baseline buffer 17: 18: 19: DR DR { R}, = mean({Ri}G 20: while DR > NDR do 21: 22: 23: end while if < ND then Pop the oldest baseline DR DR { R0} i=1) # Add trajectories into the buffer only when their advantages are positive {τi ˆAi > 0} # Apply on-Policy update with the vanilla GRPO Update policy by maximizing objective JGRPO(πθ) else # Recalibrate the advantage Compute the newly estimated advantage Aj for all τj via Equation 7 Only keep τj with positive Aj as {τj Aj > 0, τj D} # Apply regularization on self-imitation learning Compute the self-imitation objective SIL-R Equation 14 Apply the warm-up schedule for the total objective JTotal(πθ) via Equation 11 # Apply both the on-policy and the off-policy update for self-imitation Update policy by maximizing objective JTotal(πθ) Reset the replay buffer GRPO(πθ) via Equation 13 with covariance-based clipping via 34: 35: 36: 37: end if 38: 39: end for 40: return πθ 8: 9: 10: 11: 12: 13: 14: 15: 16: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 36 SPEAR A."
        },
        {
            "title": "Implementation Details",
            "content": "The implementation of the present study is based on VeRL [99] and its extension VeRL-Agent [41]. We use the vLLM [105] as the inference engine during online rollout generation. Table 7. Descriptions of the hyper-parameters for training and inference. Config Explanation The batch size for training The batch size for validation The mini batch size for actor update iterations The maximum number of tokens on each GPU for training The micro batch size on each GPU for training The maximum number of tokens on each GPU for log-probability train_batch_size val_data_size ppo_mini_batch_size ppo_max_token_len_per_gpu ppo_micro_batch_size_per_gpu log_prob_max_token_len_per_gpu log_prob_micro_batch_size_per_gpu The micro batch size on each GPU for log-probability use_dynamic_bsz ulysses_sequence_parallel_size tensor_model_parallel_size temperature top_p n_samples_per_prompt actor_learning_rate max_epochs num_steps Twarm-up Tdecay use_kl_in_reward kl_coef use_kl_loss β max_prompt_length max_response_length multi_turn_max_turns ϵlb ϵub ND NDR ωlb ωub λ rollout_filter_type rollout_filter_ratio loss_agg_mode norm_adv_by_std_in_grpo training strategy Whether to use dynamic batch size for load balance The sequence parallel size for training efficiency The tensor parallel size of model deployment for rollout generation The temperature for decoding in LLM generation The top-p for decoding in LLM generation The number of generated samples per prompt The learning rate of the actor The maximum number of epochs The number of steps The number of steps The number of steps Whether to use the KL term in reward The coefficient for the KL divergence term Whether to use the KL loss The coefficient of the KL loss (i.e., kl_loss_coef) The maximum length of input prompt The maximum length of output generation The maximum number of tool-call turns The lower bound of the policy ratio clipping (i.e., clip_ratio_low) The upper bound of the policy ratio clipping (i.e., clip_ratio_high) The replay buffer size for self-imitation learning The baseline buffer size for storing the intra-group average performance The lower bound of the value for dual-clip PPO/GRPO (i.e., clip_ratio_c) The lower bound of the covariance-based clipping The upper bound of the covariance-based clipping The ratio of the covariance-based clipping The type of filtering based on intra-group variance The ratio of filtered group The aggregation technique for loss Whether to drop the advantage normalization The strategy of training (e.g., FSDP, megatron) A.2.1 Hyper-parameters We present the details of the hyper-parameter settings in the present study. Table 7 provides the definitions of the hyper-parameters used in the present study. We follow [99] to keep most of the default empirical settings unchanged for comparability. For the covariance-based clipping, we follow [17] to set the clipping bounds ωlb, ωub respectively as the mean value of the top 0.02% and top 2% covariance. It is noted that the token-level covariance differs from task to task. Therefore, we perform statistics analysis on the covariance between action probability and the advantage with the initial model at the first training step to determine the clipping bounds. 37 SPEAR Table 8. Hyper-parameters settings of ALFWorld, WebShop, DAPO-MATH-17K, and Sokoban. Config ALFWorld WebShop DAPO-MATH-17K Sokoban train_batch_size val_data_size ppo_mini_batch_size ppo_max_token_len_per_gpu ppo_micro_batch_size_per_gpu log_prob_max_token_len_per_gpu log_prob_micro_batch_size_per_gpu use_dynamic_bsz ulysses_sequence_parallel_size tensor_model_parallel_size temperature top_p n_samples_per_prompt actor_learning_rate max_epochs num_steps Twarm-up Tdecay use_kl_in_reward kl_coef use_kl_loss β max_prompt_length max_response_length multi_turn_max_turns ϵlb ϵub ND NDR ωlb ωub λ rollout_filter_type rollout_filter_ratio loss_agg_mode norm_adv_by_std_in_grpo training strategy 128 64 18432 73728 True 8 4 1.0 0.6 16 1 300 32 64 8 8 False 2 0.4 1 8 200 100 32 32 1024 8 8 False 2 0.4 1 8 200 100 256 4 4 False 2 0.4 1 8 350 200 1e-6 200 False 0 False 2048 512 50 4096 1024 15 2048 16384/30000 8 1024 1024 15 0.2 0.28 2048 10240 10 2 2 60 1 40 2 60 0.02 std. 0.75 seq-mean-token-sum-norm False FSDP 38 SPEAR All the settings of their values can be found in Table 8. Without loss of generalizability, we do not perform meticulous fine-tuning of the hyper-parameters. One would expect better performance with grid search for the optimal hyper-parameters. A.2.2 Computing Resources All experiments are performed on workstations with 380 CPU cores, 2.2TB memory, and 8 GPUs. For both 1.5B/7B LLMs and 3B VLMs, the training is performed on four workstations with 32 GPUs in total. For the 32B models, the training is performed on sixteen workstations with 128 GPUs in total. For ALFWorld, Webshop, and Sokoban, it takes less than 60 hours for optimization of 1.5B and 7B models. While for the DAPO-MATH-17K, it takes around week for training the 32B models."
        }
    ],
    "affiliations": [
        "Tencent"
    ]
}