{
    "paper_title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
    "authors": [
        "Zhenghuang Wu",
        "Kang Chen",
        "Zeyu Zhang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D."
        },
        {
            "title": "Start",
            "content": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Zhenghuang Wu Kang Chen Zeyu Zhang Hao Tang School of Computer Science, Peking University Equal contribution. Project lead. Corresponding authors: bjdxtanghao@gmail.com. 6 2 0 2 2 1 ] . [ 1 9 6 7 1 1 . 2 0 6 2 : r Figure 1: Visual results of Light4D for training-free 4D video relighting. Our framework robustly handles extreme viewpoint changes and diverse lighting conditions while maintaining strict geometric-illumination consistency. Abstract Recent advances in diffusion-based generative models have established new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive 1 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting performance in temporal consistency and lighting robustly handling camera rotations fidelity, from 90 to 90. Code: https://github. com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/ Light4D. 1. Introduction is fundamenThe synthesis of dynamic 4D content tal for next-generation immersive applications, including cinematic virtual production (Bahmani et al., 2024), AR/VR (Pang et al., 2025), and interactive simulations (Wen et al., 2025). Since 4D geometric synthesis alone is insufficient for photorealism, achieving simultaneous control over both camera trajectory and illumination becomes pivotal requirement for high-fidelity generation. However, current research focuses predominantly on either video relighting or 4D geometric generation, leaving their intersection largely unexplored. Existing video relighting approaches (Zhang et al., 2025; Lin et al., 2025; Bharadwaj et al., 2025) operate primarily in the 2D domain. Although these methods improve the temporal consistency of videos with limited motion, they fundamentally struggle to maintain spatiotemporal consistency under complex camera trajectories. In contrast, camera-controlled 4D generation methods (Chen et al., 2025; Mi et al., 2025; Zhou et al., 2025a) excel in synthesizing coherent dynamic geometry but typically overlook controllable illumination. In these frameworks, lighting effects are embedded in the texture (Zhang et al., 2021b), making the illumination uneditable and preventing adaptation to novel environments. To address these limitations, recent research has explored 4D relighting via end-to-end supervised training. Frameworks such as Light-X (Liu et al., 2025) aim to learn joint camera and illumination control directly. However, these methods face two critical bottlenecks. First, they are fundamentally constrained by the severe data scarcity. Training these models necessitates massive paired datasets featuring both multi-view and multi-illumination consistency, which are prohibitively expensive to acquire in real-world environments. Second, these approaches often struggle to generalize to extreme viewpoints. Due to their reliance on limited synthetic data, the generated illumination tends to appear rigid and flat, resembling 2D texture mapping rather than volumetric light transport (Chaturvedi et al., 2025), which severely compromises photorealism. In this work, we propose Light4D, training-free framework tailored for 4D video relighting under extreme viewpoint changes. As shown in Figure 1, our method achieves high-fidelity results under extreme viewpoint changes. Unlike supervised methods that rely on expensive retraining, our method leverages the generative power of pre-trained expert models, synergizing the geometric prior of EX-4D (Hu et al., 2025) with the illumination prior of IC-Light (Zhang et al., 2025). critical challenge in this integration arises from the inherent conflict between geometric reconstruction and illumination synthesis within the generative manifold. To resolve this, we introduce Disentangled Flow Guidance (DFG), time-aware strategy that harmonizes lighting injection with geometric preservation. Specifically, this mechanism establishes robust geometric foundation during the initial denoising phase, subsequently utilizing lightingfused latent state as rectified target to steer the simultaneous synthesis of geometry and illumination. Furthermore, to ensure cross-frame coherence, we develop Temporal Consistent Attention (TCA) within the IC-Light architecture and employ deterministic regularization to suppress stochastic fluctuations and ensure temporal consistency. Our contributions are summarized as follows: We propose Light4D, the first training-free framework capable of achieving joint control over extreme camera trajectories (90 90) and illumination. By leveraging pre-trained generative priors, our approach eliminates the need for large-scale paired datasets. We introduce Disentangled Flow Guidance strategy and Temporal Consistent Attention (TCA). These designs jointly resolve the inherent conflict between geometric reconstruction and illumination synthesis, ensuring strict temporal coherence and preventing structural degradation. Extensive experiments demonstrate that our method achieves competitive performance in terms of temporal consistency and lighting fidelity compared to existing video relighting baselines, particularly under extreme viewpoint changes. 2. Related Work Learning-based Illumination Editing. Early learningbased relighting methods typically rely on paired supervision, where encoder-decoder CNNs map an input image to relit output. Extending relighting from images to videos often introduces temporal flicker, motivating explicit crossframe consistency constraints (Zhang et al., 2021a; Chandran et al., 2022). In recent years, diffusion-based formulations have become dominant direction (Ponglertnapakorn et al., 2023; Chaturvedi et al., 2025; Wang et al., 2025; Fang et al., 2025; Magar et al., 2025). Their performance further benefits from scaling choices such as larger training corpora, stronger backbones, and physically grounded objectives and constraints (Kocsis et al., 2024; Kim et al., 2024; 2 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 2: Overview of the Light4D framework. Our training-free approach employs time-aware paradigm in latent flow-matching process. Using multi-phase adaptive schedule λ(t), we prioritize 3D geometric completion via the EX-4D backbone before injecting illumination cues through IC-Light. Zhang et al., 2025). Beyond general-purpose relighting, specialized settings have also been studied, including portrait performance relighting with conditional video diffusion and hybrid datasets (Mei et al., 2025), as well as approaches that jointly model intrinsic decomposition and relighting synthesis to better capture complex illumination effects (He et al., 2025). Among recent methods, IC-Light (Zhang et al., 2025) demonstrates strong results through scalable diffusion training coupled with light-transport consistency. Building on this line, RelightVid (Ponglertnapakorn et al., 2025) extends relighting to videos via temporal attention, whereas Light-A-Video (Zhou et al., 2025b) provides training-free alternative by injecting cross-frame attention during inference. 4D Video Generation. Camera-controllable 4D video generation aims to synthesize temporally coherent dynamic scenes from novel viewpoints. Existing approaches can be roughly organized by how they enforce 3D/4D structures during generation. One line of work injects explicit geometric priors or relies on intermediate reconstructions to stabilize view changes. For instance, EX-4D uses watertight mesh reasoning to support extreme viewpoint variations (Hu et al., 2025), and Light-X introduces disentangled geometry cues to enable joint camera and illumination control (Liu et al., 2025). Another line emphasizes feed-forward generative modeling over dynamic 3D/4D representations for novel-view rendering, including point-cloud-based formulations in 4DNeX (Chen et al., 2025) and diffusion-based 4D occupancy generation in DynamicCity (Bian et al., 2024). In parallel, camera-conditioned diffusion and pseudo-4D guidance have improved controllability for view synthesis and camera-aware video generation (Fan et al., 2025; Bian et al., 2025). 4D Relighting. Relighting in dynamic scenes with viewpoint changes remains relatively underexplored. Most existing systems are training-based and rely on explicit scene representations. Relighting4D (Chen & Liu, 2022) reconstructs spacetime neural fields and uses physically-based rendering to decompose scenes into components such as normals, occlusion, and diffuse effects. Relightable Neural Actor (Luvizon et al., 2024) targets human performance capture by leveraging multi-view inputs together with intrinsic and material decomposition for pose-conditioned relighting. Light-X (Liu et al., 2025) provides unified pipeline for joint camera and illumination control by training on synthetic paired data, producing videos with coupled viewpoint and lighting variations. BEAM (Hong et al., 2025) represents dynamic scenes using 4D Gaussians and physically-based rendering to recover material properties for high-quality relightable video. Recent monocular relightable Gaussian methods (Choi et al., 2025; Jiang et al., 2025; Schmidt et al., 2025) aim to reduce the need for multiview capture. 3. The Proposed Method 3.1. Overview We propose Light4D, novel training-free framework designed to synthesize target 4D video = {I }F =1 from monocular source video Vs = {I =1, as illustrated in }F 3 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting lighting prompt L: ˆxlight 0 = Mlight (D(ˆzgeo ), L, ϵshared) , (1) where ϵshared represents canonical noise prior. We then construct hybrid flow target ztarget by fusing the geometric structure and illumination cues in the latent space, governed by time-dependent fusion weight λ(t): ztarget = (cid:16) (1 λ(t)) D(ˆzgeo 0 ) + λ(t) ˆxlight 0 (cid:17) , (2) where denotes the VAE encoder. This hybrid target ztarget serves as the target, defining rectified flow trajectory toward the illumination-enhanced manifold. Based on this target, we perform discrete ODE update step to compute the state zt for the next timestep t. Specifically, we employ first-order Euler solver: zt = zt + (σt σt) zt ztarget σt + δ , (3) where σt and σt denote the noise levels, and δ is numerical stabilizer. Crucially, to mitigate interference between geometry generation and illumination refinement, λ(t) follows multi-phase adaptive schedule. Unlike standard linear annealing, this schedule disentangles the trajectory into four distinct phases, parameterized by the peak intensity λmax and the terminal weight λend: λ(t) = 0, λmax (cid:113) τgt τgτr , λmax, λmaxλend τs + λend, (τg, 1] [τr, τg] [τs, τr) [0, τs) (4) where τg, τr, τs are the time thresholds for the geometric, ramp-up, and stable phases, respectively. Initially, during the geometric isolation phase (t > τg), we enforce λ(t) = 0 to strictly prohibit illumination cues from interfering with the completion of invisible regions and occlusion relationships. Subsequently, root-squared ramp [τr, τg] is employed to rapidly anchor lighting conditions without inducing feature shock, followed by stable plateau [τs, τr) at λmax that allows the model to reconcile appearance cues with the established 3D structure. Finally, for < τs, the weight linearly decays to λend, ensuring soft landing that prevents the relighting prior from dominating fine-grained details during convergence. 3.3. Temporal Consistent Attention While Disentangled Flow Guidance (Section 3.2) effectively steers the geometric trajectory, the reliance on frameindependent relighting prior inevitably introduces stochastic 4 Figure 3: Design of Temporal Consistent Attention (TCA). TCA enforces temporal coherence through dualpath mechanism that interpolates between standard selfattention (Path A) for frame-specific structure and consistent path (Path B) that regularizes appearance context via Gaussian-weighted sliding window aggregation. Figure 2. Our primary objective is to rerender the dynamic scene under target camera trajectory = {Pf }F =1 and user-specified illumination conditions L. Specifically, the camera trajectory spans wide viewpoint range (90 to 90) relative to the original coordinate system, while the illumination is modulated by text prompts corresponding to distinct lighting directions (e.g., Left, Right, Top, Bottom). The generated video must faithfully preserve the 3D geometry and motion dynamics of the source Vs, while accurately adhering to the novel viewpoint and lighting constraints L. The overall inference pipeline achieving these goals is summarized in Algorithm 1. 3.2. Disentangled Flow Guidance critical challenge in 4D relighting arises from the inherent conflict between geometric reconstruction and illumination synthesis within the generative manifold. Standard flow matching models typically prioritize early denoising stages to establish coherent 3D structures. Consequently, the premature injection of high-frequency illumination cues can disrupt this delicate structural formation, resulting in geometric collapse. To mitigate these issues, we employ time-aware strategy that progressively integrates illumination cues into the latent space. Formally, let zt denote the latent state at timestep [0, 1]. We formulate the generative process as guiding pretrained geometric flow vgeo instantiated by EX-4D, using an illumination-aware correction derived from single-view relighting prior Mlight based on IC-Light. 0 At each timestep t, we first estimate the clean geometric state ˆzgeo derived from the current flow velocity. Since the relighting prior Mlight operates in the image domain, we project this latent estimate into the pixel space via the VAE decoder D. Then, we utilize the relighting prior Mlight to predict the relighted image ˆxlight conditioned on the 0 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting fluctuations in the feature space, leading to both geometric instability and temporal flickering. To resolve this, we introduce Temporal Consistent Attention (TCA). As illustrated in Figure 3, this dual-path mechanism modifies standard self-attention to enforce strict temporal coherence while preserving high-fidelity illumination details. We redesign its internal self-attention mechanism to assign distinct temporal behaviors. Specifically, the Query features (Q), which encode frame-specific geometric structure, are preserved in their transient state to faithfully capture dynamic changes. In contrast, the Key (K) and Value (V) pairs, which provide the appearance context, are regularized to exhibit strong temporal stability. We employ Gaussian-weighted sliding window approach to construct smoothed context {Kf , Vf } by aggregating features within local temporal neighborhood (f ) = {j j r}, weighted by their temporal proximity: (cid:88) Kf = w(f j) Kj, Vf = jN (f ) (cid:88) jN (f ) w(f j) Vj, (5) where denotes the temporal radius, represents the current frame index, and w(d) = exp(d2/2σ2) serves as Gaussian weighting kernel that prioritizes contributions from temporally adjacent frames. To balance single-frame sharpness with temporal stability, TCA implements dual-path residual injection strategy. We simultaneously compute the original stochastic features Horig using standard self-attention to retain high-frequency details, and the temporally consistent features Hcons by attending the frame-specific query Qf to the temporally smoothed context {Kf , Vf }: Horig = Attention(Qf , Kf , Vf ), Hcons = Attention(Qf , Kf , Vf ). (6) Subsequently, the final hidden state is derived via linear interpolation controlled by mixing coefficient γ [0, 1]: Hout = (1 γ)Horig + γHcons. (7) This residual formulation effectively mitigates the propagation of temporal variance into the rectification target while preserving the structural distinctiveness of individual frames. Deterministic Coherence. Before fusing the IC-Light prediction into the hybrid target, we apply deterministic stabilization procedure to suppress temporal stochasticity, including Canonical Noise Initialization (CNI), Global Moment Matching (GMM), and Frequency-Decoupled Illuminance Regularization (FDI). More details are provided in Section B. 5 4. Experiment 4.1. Experimental Setup Baselines. Since Light4D is the first training-free framework for 4D video relighting, we compare it with both stateof-the-art training-based methods and cascaded trainingfree baselines. For training-based approaches, we select Light-X (Liu et al., 2025), which is trained to jointly control camera motion and illumination. For training-free comparisons, we construct two sequential pipelines: EX-4D (Hu et al., 2025) Light-A-Video (LAV) (Zhou et al., 2025b), and LAV (Zhou et al., 2025b) EX-4D (Hu et al., 2025). We also include naive baseline EX-4D (Hu et al., 2025) + IC-Light (Zhang et al., 2025) to expose lower bound on temporal stability when relighting is applied independently to each frame. Benchmark. To evaluate 4D relighting under extreme viewpoint changes, we generated an evaluation benchmark of 100 high-quality videos produced by recent video models (Sora (Brooks et al., 2024), WanVideo (Wan et al., 2025), and Kling (Team et al., 2025)), spanning humans, animals, objects, and landscapes. Each video is evaluated in three movement ranges of the camera (30, 90, 180). To investigate generalization to real captured videos, we additionally apply our method to real-world driving sequences from the OpenScene dataset (OpenScene Dataset Contributors, 2023) and use these results to qualitatively analyze lighting behavior on real videos. Evaluation Metrics. We evaluate relit videos against source videos under three camera movement ranges (30, 90, 180) using two metric groups. Relighting-related metrics include: (i) CLIP-Frame (Radford et al., 2021), the inter-frame similarity of CLIP embeddings computed on consecutive frames to quantify temporal coherence; (ii) Motion Flow L1, the mean distance ℓ1 between RAFT (Teed & Deng, 2020) optical flows estimated from relit and source videos to measure motion preservation; (iii) High Frequency Preservation Ratio (HFPR) (Zhu et al., 2012; 2013), the ratio of Laplacian-filtered high-frequency energy between the relit and source videos to assess detail retention; and (iv) Aesthetic Score (Schuhmann et al., 2022) for perceptual appeal. Video quality metrics measure reconstruction fidelity: (i) Frame PSNR, and (iiiii) warp-aligned SSIM and LPIPS, where we estimate flow using RAFT and warp adjacent frames to common reference before computing the metrics, thereby reducing motion-induced misalignment and better isolating appearance and structural consistency over time (Wang et al., 2004; Zhang et al., 2018). Implementation Details. Our framework is based on EX4D (Hu et al., 2025) as the generative backbone of 4D and IC-Light (Zhang et al., 2025) as the illumination prior. Our default setting generates 49-frame videos at resolution of Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 4: Qualitative relighting results. Comparison of baselines and our method under two prompts: Sunlight (left) and Pink neon light (right). Our method yields more stable illumination changes over time and reduces temporal flicker. 384 384 with = 25 denoising steps, and all experiments are run on NVIDIA H20 GPU. To mitigate geometryillumination interference, we adopt time-aware fusion strategy that delays illumination injection to later denoising stages, prioritizing 4D structure formation before enforcing relighting cues. We further improve temporal stability via lightweight temporal consistency module and postprocessing smoothing. The complete hyperparameters and trajectory alignment details are provided in the Appendix C. 4.2. Main Results Relighting Quality Results. We visualize relighting outputs under two lighting conditions, Sunlight (left) and Pink neon light (right), as shown in Figure 4. Quantitatively  (Table 1)  , our method achieves the strongest overall performance at 30, 90, and 180 in CLIP-Frame, HFPR, and Aesthetic Score, suggesting improved temporal coherence and perceptual quality while preserving fine details. These gains are consistent with the visual comparisons in Figure 4, where our relighting exhibits smoother light transitions and fewer flickering artifacts. The naive EX-4D+ICLight baseline (Hu et al., 2025; Zhang et al., 2025) can yield sharp individual frames, but frame-wise relighting introduces severe temporal inconsistency, resulting in higher Motion Flow L1 and visibly unstable illumination. The cascade pipelines exhibit different trade-offs: both EX-4DLAV and LAVEX-4D (Hu et al., 2025; Zhou et al., 2025b) improve temporal behavior compared to frame-wise relighting; however, yet their lighting cues are not consistently aligned with the evolving 4D geometry under large rotations. This often leads to less natural appearance and reduced motion or geometry stability as the viewpoint change increases. LightX (Liu et al., 2025) performs well at moderate rotations, but degrades in larger viewpoints settings. In particular, visualizations reveal more static, baked-in illumination pattern that does not adapt faithfully when newly visible geometry emerges. 4D Video Quality Results. Relighting 4D content while preserving geometry and fine-grained content under extreme viewpoints is particularly challenging. Figure 5 shows qualitative results under the Sunlight prompt, and Table 2 reports frame-wise reconstruction metrics between relit and source videos. Our method consistently leads in Frame PSNR and SSIM across all viewpoint ranges and achieves the lowest LPIPS at 90 and 180, indicating strong content preservation without sacrificing perceptual detail. This indicates that Light4D preserves the underlying 4D content while applying substantial illumination edits, even as viewpoint changes increase. This indicates that Light4D preserves the underlying 4D content while applying substantial illumination edits, even as viewpoint changes increase. 6 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Table 1: Relighting-related metrics across viewpoint changes. We report CLIP-Frame, Motion Flow L1, HFPR, and Aesthetic Score under camera motion ranges of 30, 90, and 180. These metrics jointly reflect the trade-off between lighting consistency, 4D geometric and motion stability, and detail fidelity under extreme viewpoints. Best results are bolded and second-best are underlined. METHOD Training-based LIGHT-X CLIP-FRAME 90 180 30 MOTION FLOW L1 90 30 180 HFPR 90 180 30 AESTHETIC 90 180 30 0.956 0.900 0.901 0.814 1.832 4. 0.945 0.949 0.931 0.235 0.229 0. Training-free EX-4D + IC-LIGHT 0.923 0.961 EX-4D + LAV 0.959 LAV + EX-4D 0.885 0.925 0.918 0.885 0.923 0.922 15.725 1.287 0.982 21.652 3.200 2.923 24.114 6.095 4. 0.842 0.951 0.949 0.838 0.959 0.932 0.817 0.946 0.944 0.228 0.209 0.199 0.223 0.189 0.174 0.219 0.168 0. LIGHT4D (OURS) 0.975 0.930 0.930 0.791 1. 3.524 0.974 0.963 0.966 0.243 0. 0.231 Table 2: Video quality metrics across viewpoint changes. We measure frame-wise reconstruction similarity between the relighting videos and the corresponding source videos under viewpoint changes of 30, 90, and 180. We report per-frame PSNR, per-frame SSIM, and per-frame LPIPS to quantify content and detail preservation. Best results are bolded and second-best are underlined. METHOD Training-based LIGHT-X Training-free EX-4D + IC-LIGHT EX-4D + LAV LAV + EX-4D LIGHT4D (OURS) FRAME PSNR 90 30 180 30 FRAME SSIM 90 180 FRAME LPIPS 90 180 30 12.899 13.544 13. 0.738 0.733 0.752 0.349 0.358 0. 9.147 11.957 12.348 14.056 9.412 11.843 12.007 13.728 9.035 11.449 12.023 13. 0.493 0.696 0.636 0.761 0.489 0.659 0.564 0.759 0.473 0.652 0.554 0. 0.754 0.572 0.535 0.360 0.760 0.625 0.560 0.341 0.748 0.624 0.552 0. Table 3: Ablation Study: Relighting Metrics at 30. Best results are bolded; second best are underlined. METHOD CLIP MOTION HFPR AES W/O CLA W/O DGA W/O FDI W/O CNI W/O GMM W/O ALL 0.972 0.965 0.970 0.971 0.972 0.793 FULL MODEL 0.975 0.903 1.259 0.885 0.875 0.860 0. 0.791 0.882 0.754 0.910 0.925 0.940 0.893 0.974 0.229 0.213 0.228 0.229 0.230 0. 0.243 In contrast, cascaded baselines (Hu et al., 2025; Zhou et al., 2025b; Zhang et al., 2025) are more sensitive to large viewpoint changes: introducing relighting cues either before or after 4D generation can amplify small geometric inconsistencies into visible distortions when rotations reach 90 and beyond. Light-X (Liu et al., 2025) maintains reasonable fidelity, yet shows marginally lower PSNR and SSIM scores in this extreme-view evaluation. In summary, our method offers more balanced compromise under extreme camera motion, simultaneously maintaining illumination coherence, stabilizing 4D geometry, and preserving video details. 7 4.3. Ablation Studies We validate the effectiveness of our design by ablating key components and reporting results from 30 viewpoint. Specifically, we isolate the impact of each proposed module to demonstrate its necessity in our pipeline. Table 3 summarizes relighting metrics, while the full set of metrics  (Table 4)  and more ablation details are provided in Appendix A. Impact of Disentangled Guidance (DGA). Removing DGA (w/o DGA) markedly degrades relighting quality: Motion Flow L1 increases substantially and HFPR drops, indicating weaker motion preservation and loss of fine details. This supports the role of DGA in injecting illumination cues without destabilizing the underlying 4D geometry. Effect of Consistent Light Attention (CLA). Disabling CLA (w/o CLA) reduces temporal coherence in relighting, reflected by higher Motion Flow L1 compared to the full model. It also leads to more noticeable frame-to-frame lighting jitter in visually smooth regions. This suggests that CLA is important for stabilizing illumination across consecutive frames. Effect of Regularization. We further ablate the Canonical Noise Initialization (CNI), Global Moment MatchLight4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 5: 4D video quality under extreme viewpoints. Qualitative comparison of baselines and our method using the prompt Sunlight. Under extreme viewpoint changes, our method better balances relighting coherence, 4D geometric stability, and detail fidelity. ing (GMM), and Frequency-Decoupled Illuminance (FDI). Overall, removing these regularizers harms relighting quality in complementary ways. Specifically, removing all smoothing increases temporal instability, whereas the full model preserves sharp details with higher HFPR and achieves better perceptual quality with higher Aesthetic Score. Across our regularization modules, we observe the largest degradation when removing FDI, followed by CNI, and then GMM. While their effects are partially complementary and can vary slightly across metrics, the overall trend is consistent across temporal coherence and detailpreservation measures. nature of the IC-Light prior. Consequently, maintaining rigorous global lighting consistency remains challenging when adapting 2D image priors to the temporal domain, particularly during extreme viewpoint traversals (90 90). Future work will leverage the frameworks modularity to integrate more advanced video-native illumination models and stronger 4D generative backbones to further improve synthesis quality. We also aim to investigate specialized mechanisms to mitigate photometric inconsistencies induced by large viewpoint variations, while extending the framework to handle complex light transport effects, such as cast shadows and inter-reflections. 5. Limitation and Future Work 6. Conclusion As training-free framework, Light4Ds performance is bounded by the capabilities of its foundation models, specifically the geometric fidelity of EX-4D and the single-frame In this paper, we present Light4D, training-free framework for 4D video relighting under extreme viewpoint changes. By identifying and resolving the conflict between geomet8 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting ric reconstruction and illumination synthesis, we propose Disentangled Flow Guidance to harmonize these objectives with time-aware schedule. Furthermore, we introduce Temporal Consistent Attention and Deterministic Coherence Regularization to ensure the generated content remains geometrically consistent and free of visible flicker. Extensive experiments show that our method achieves competitive performance in both geometric consistency and lighting fidelity compared to baselines, offering scalable solution for controllable 4D content creation. Finally, the modular design of our framework makes it extensible, enabling integration of future advances in generative models."
        },
        {
            "title": "References",
            "content": "Bahmani, S., Skorokhodov, I., Rong, V., Wetzstein, G., Guibas, L., Wonka, P., Tulyakov, S., Park, J. J., Tagliasacchi, A., and Lindell, D. B. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 79968006, 2024. Bharadwaj, S., Feng, H., Becherini, G., Fernandez Abrevaya, V., and Black, M. J. Genlit: Reformulating singleimage relighting as video generation. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pp. 112, 2025. Bian, H., Kong, L., Xie, H., Pan, L., Qiao, Y., and Liu, Z. Dynamiccity: Large-scale 4d occupancy generation from dynamic scenes. arXiv preprint arXiv:2410.18084, 2024. Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., and Li, H. Gs-dit: Advancing video generation with dynamic 3d gaussian fields through efficient dense 3d point tracking. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2171721727, 2025. Chen, Z. and Liu, Z. Relighting4d: Neural relightable human from videos, 2022. URL https://arxiv.org/ abs/2207.07104. Chen, Z., Liu, T., Zhuo, L., Ren, J., Tao, Z., Zhu, H., Hong, F., Pan, L., and Liu, Z. 4dnex: Feed-forward arXiv preprint 4d generative modeling made easy. arXiv:2508.13154, 2025. Choi, S., Choi, M., Jang, M., Kim, J., Cai, J., Cheng, W.-H., and Lee, S. Relightable and dynamic gaussian avatar reconstruction from monocular video. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 74057414, 2025. Fan, X., Girish, S., Ramanujan, V., Wang, C., Mirzaei, A., Sushko, P., Siarohin, A., Tulyakov, S., and Krishna, R. Omniview: An all-seeing diffusion model for 3d and 4d view synthesis. arXiv preprint arXiv:2512.10940, 2025. Fang, Y., Sun, Z., Zhang, S., Wu, T., Xu, Y., Zhang, P., Wang, J., Wetzstein, G., and Lin, D. Relightvid: Temporal-consistent diffusion model for video relighting. arXiv preprint arXiv:2501.16330, 2025. He, K., Liang, R., Munkberg, J., Hasselgren, J., Vijaykumar, N., Keller, A., Fidler, S., Gilitschenski, I., Gojcic, Z., and Wang, Z. Unirelight: Learning joint decomposition and synthesis for video relighting, 2025. URL https: //arxiv.org/abs/2506.15673. Hong, Y., Wu, Y., Shen, Z., Guo, C., Jiang, Y., Zhang, Y., Yu, J., and Xu, L. Beam: Bridging physically-based rendering and gaussian modeling for relightable volumetric video. arXiv preprint arXiv:2502.08297, 2025. Hu, T., Peng, H., Liu, X., and Ma, Y. Ex-4d: Extreme viewpoint 4d video synthesis via depth watertight mesh. arXiv preprint arXiv:2506.05554, 2025. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Chandran, S., Hold-Geoffroy, Y., Sunkavalli, K., Shu, Z., and Jayasuriya, S. Temporally consistent relighting for portrait videos. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022. Chaturvedi, S., Ren, M., Hold-Geoffroy, Y., Liu, J., Dorsey, J., and Shu, Z. Synthlight: Portrait relighting with diffusion model by learning to re-render synthetic faces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 369379, 2025. Jiang, Z., Wang, S., and Tang, S. Dnf-avatar: Distilling neural fields for real-time animatable avatar relighting. arXiv preprint arXiv:2504.10486, 2025. Kim, H., Jang, M., Yoon, W., Lee, J., Na, D., and Woo, S. Switchlight: Co-design of physics-driven architecture and pre-training framework for human portrait relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2509625106, 2024. Kocsis, P., Philip, J., Sunkavalli, K., Nie ssner, M., and Hold-Geoffroy, Y. Lightit: Illumination modeling and control for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 93599369, 2024. Lin, Y., Chen, Y.-W., Tsai, Y.-H., Clark, R., and Yang, M.-H. Illumicraft: Unified geometry and illumination 9 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting diffusion for controllable video generation. arXiv preprint arXiv:2506.03150, 2025. Liu, T., Chen, Z., Huang, Z., Xu, S., Zhang, S., Ye, C., Li, B., Cao, Z., Li, W., Zhao, H., et al. Light-x: Generative 4d video rendering with camera and illumination control. arXiv preprint arXiv:2512.05115, 2025. Luvizon, D., Golyanik, V., Kortylewski, A., Habermann, M., and Theobalt, C. Relightable neural actor with intrinsic decomposition and pose control, 2024. URL https: //arxiv.org/abs/2312.11587. Magar, N., Hertz, A., Tabellion, E., Pritch, Y., Rav-Acha, A., Shamir, A., and Hoshen, Y. Lightlab: Controlling light sources in images with diffusion models. arXiv preprint arXiv:2505.09608, 2025. Mei, Y., He, M., Ma, L., Philip, J., Xian, W., George, D. M., Yu, X., Dedic, G., Tasel, A. L., Yu, N., Patel, V. M., and Debevec, P. Lux post facto: Learning portrait performance relighting with conditional video diffusion and hybrid dataset, 2025. URL https: //arxiv.org/abs/2503.14485. Mi, Z., Wang, Y., and Xu, D. One4d: Unified 4d generation and reconstruction via decoupled lora control. arXiv preprint arXiv:2511.18922, 2025. OpenScene Dataset Contributors. OpenScene: The Largest Up-to-Date 3D Occupancy Prediction Benchmark in Autonomous Driving, August 2023. URL https:// github.com/OpenDriveLab/OpenScene. Pang, H. E., Liu, S., Cai, Z., Yang, L., Zhang, T., and Liu, Z. Disco4d: Disentangled 4d human generation and animation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2633126344, 2025. Ponglertnapakorn, P., Tritrong, N., and Suwajanakorn, S. Difareli: Diffusion face relighting, 2023. Ponglertnapakorn, P., Tritrong, N., and Suwajanakorn, S. Difareli++: Diffusion face relighting with consistent cast IEEE transactions on pattern analysis and shadows. machine intelligence, 2025. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. Schmidt, J., Giebenhain, S., and Niessner, M. Becominglit: Relightable gaussian avatars with hybrid neural shading. arXiv preprint arXiv:2506.06271, 2025. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C. W., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S. R., Crowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev, J. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and Benchmarks, 2022. Team, K., Chen, J., Ci, Y., Du, X., Feng, Z., Gai, K., Guo, S., Han, F., He, J., He, K., Hu, X., Hu, X., Jiang, B., Kong, F., Li, H., Li, J., Li, Q., Li, S., Li, X., Li, Y., Liang, J., Liao, B., Liao, Y., Lin, W., Liu, Q., Liu, X., Liu, Y., Liu, Y., Lu, S., Mao, H., Mao, Y., Ouyang, H., Qin, W., Shi, W., Shi, X., Su, L., Sun, H., Sun, P., Wan, P., Wang, C., Wang, C., Wang, M., Wang, Q., Wang, R., Wang, X., Wang, X., Wang, Z., Wei, M., Wen, T., Wu, G., Wu, X., Wu, Z., Xie, D., Xiong, Y., Xu, Y., Yang, S., Yang, Z., Ye, W., Yuan, Z., Zhang, S., Zhang, S., Zhang, Y., Zhang, Y., Zhao, W., Zhou, R., Zhou, Y., Zhu, G., and Zhu, Y. Kling-omni technical report, 2025. URL https://arxiv.org/abs/2512.16776. Teed, Z. and Deng, J. RAFT: Recurrent all-pairs field transforms for optical flow. In European Conference on Computer Vision (ECCV), 2020. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, J., Liu, J., Sun, X., Singh, K. K., Shu, Z., Zhang, H., Yang, J., Zhao, N., Wang, T. Y., Chen, S. S., et al. Comprehensive relighting: Generalizable and consistent monocular human relighting and harmonization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 380390, 2025. Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: From error visibility IEEE Transactions on Image to structural similarity. Processing, 13(4):600612, 2004. Wen, K., Huang, Y., Chen, R., Zheng, H., Lin, Y., Pan, P., Li, C., Cong, W., Zhang, J., Lu, J., et al. Dynamicverse: physically-aware multimodal framework for 4d world modeling. arXiv preprint arXiv:2512.03000, 2025. Zhang, L., Zhang, Q., Wu, M., Yu, J., and Xu, L. Neural video portrait relighting in real-time via consistency modeling. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 802812, 2021a. Zhang, L., Rao, A., and Agrawala, M. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. 10 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Zhang, X., Srinivasan, P. P., Deng, B., Debevec, P., Freeman, W. T., and Barron, J. T. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 40(6):118, 2021b. Zhou, H., Yu, W., Guan, J., Cheng, X., Tian, Y., and Yuan, L. Holotime: Taming video diffusion models for panoramic 4d scene generation. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 97639772, 2025a. Zhou, Y., Bu, J., Ling, P., Zhang, P., Wu, T., Huang, Q., Li, J., Dong, X., Zang, Y., Cao, Y., et al. Light-a-video: Training-free video relighting via progressive light fusion. arXiv preprint arXiv:2502.08590, 2025b. Zhu, K., Li, S., and Saupe, D. An objective method of measuring texture preservation for camcorder performance In Proceedings of SPIE-IS&T Electronic evaluation. Imaging: Image Quality and System Performance IX, volume 8293, pp. 829304, 2012. doi: 10.1117/12.907265. Zhu, K., Hirakawa, K., Asari, V. K., and Saupe, D. noreference video quality assessment based on Laplacian pyramids. In 2013 IEEE International Conference on Image Processing (ICIP), pp. 4953. IEEE, 2013. 11 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting A. Additional Ablation Studies A.1. Ablation on Geometric Isolation Phase (τg). To validate the necessity of the Geometric Isolation Phase described in Section 3.2, we conduct an ablation study on the timing threshold τg. This parameter controls how long the illumination fusion weight λ(t) is forced to zero, allowing the EX-4D backbone to establish structure without interference from the lighting prior. quantitative comparison of geometric isolation thresholds τg is provided in Figure 6. We use all evaluation metrics, including video quality and relighting metrics. The results indicate clear trade-off: premature lighting injection can disrupt the formation of coherent 4D geometry, often leading to reduced temporal consistency and visible flickering, as evidenced by higher motion errors and lower detail preservation. Conversely, excessively delayed injection reduces the number of denoising steps available for illumination integration, potentially resulting in incomplete or unnatural lighting effects. Overall, our results suggest that setting τg = 0.7 offers favorable balance, anchoring the 4D structure while leaving sufficient flexibility for the relighting prior. B. Deterministic Coherence and Regularization To eliminate temporal artifacts, we apply deterministic regularization to the raw predictions ˆxlight from the IC-Light prior before integration. Instead of directly injecting the stochastic output, we first rectify the signal to improve temporal stability. This processed appearance is then fused with the geometric projection to construct the hybrid flow target. 0 Canonical Noise Initialization. The stochastic nature of diffusion sampling introduces aleatoric uncertainty, leading to high-frequency texture jitter across views. We mitigate this by conditioning the relighting model on canonical noise prior. Instead of independent sampling, we broadcast fixed noise map ϵshared across the temporal sequence, ensuring topologically consistent generation path for identical semantic regions. ϵf = ϵshared, [1, ] (8) where denotes the number of frames in the video. Global Moment Matching. To address global exposure fluctuations caused by frame-independent processing, we enforce statistical alignment across the video sequence. We construct canonical reference by computing the temporal average of the entire prediction sequence. Specifically, we rectify the intensity distribution of each raw prediction ˆxlight to match the mean µref and standard deviation σref of this temporal average reference. This ensures consistent brightness levels throughout the trajectory, yielding the aligned intermediate frame xf : xf = µref + σref σf (ˆxlight µf ) (9) where µf and σf denote the intensity mean and standard deviation of the current frame , respectively. Frequency-Decoupled Illuminance Regularization. Finally, to resolve luminance flickering without compromising geometric sharpness, we employ spectral decomposition strategy. Based on the assumption that valid illumination changes are spectrally concentrated in the low-frequency domain while texture resides in high frequencies, we decompose the aligned signal xf . We apply temporal smoothing operator exclusively to the base illumination layer, while preserving the high-frequency details: (10) (cid:123)(cid:122) Smoothed Illumination where denotes the convolution operation. The resulting xlight matching objective defined in Eq. (2). xlight = (xf Gσ) (cid:125) (cid:124) + (xf xf Gσ) (cid:125) (cid:123)(cid:122) Preserved Texture (cid:124) serves as the final robust appearance target for the flow C. More Implementation Details C.1. Baselines. We evaluate Light4D against both training-based and training-free baselines under identical extreme viewpoint changes (30, 90, and 180). To ensure fair comparison, we align the camera trajectories used for video generation across methods. 12 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Table 4: Ablation Study: Summary of relighting and video quality metrics at 30 viewpoint change. Best results are bolded, and second-best results are underlined. METHOD W/O CLA W/O DGA W/O FDI W/O CNI W/O GMM W/O ALL FULL MODEL CLIP MOTION HFPR 0.972 0.965 0.970 0.971 0.972 0.793 0.975 0.903 1.259 0.885 0.875 0.860 0. 0.791 0.882 0.754 0.910 0.925 0.940 0.893 0.974 AES 0.229 0. 0.228 0.229 0.230 0.231 0.243 PSNR SSIM LPIPS 13.698 10. 13.351 13.487 13.650 13.207 14.056 0.739 0.513 0.680 0.783 0.711 0.657 0.761 0.420 0. 0.502 0.470 0.461 0.518 0.360 Figure 6: Quantitative ablation on the geometric isolation threshold τg. (a) HFPR scores, (b) Temporal CLIP scores, and (c) Motion Flow L1 errors are shown. Specifically, for Light-X (Liu et al., 2025), we adopt its original camera extrinsic sequence with an EX-4D-style trajectory parameterization that matches both the viewpoint range and the motion pattern used in our setup, so that all methods are evaluated under the same camera motion. For Light-A-Video (LAV) (Zhou et al., 2025b), we use the officially released implementation of Wan2.1 (Wan et al., 2025). C.2. Time-aware fusion schedule. We use piecewise schedule for the fusion weight: it is set to 0 for the first 60% of denoising steps (Steps 014) to prioritize geometric completion, and then linearly increases to maximum value of 0.5 during the final 40% (Steps 1524) for progressive illumination injection. C.3. Temporal consistency and smoothing. The Consistent Light Attention (CLA) module uses balance factor of γ = 0.7. The final output is processed with adaptive temporal smoothing using window size of 9 and Gaussian kernel with σ = 25 to suppress high-frequency flicker while preserving texture details. C.4. User Study and Preference We select three test sequences with viewpoint-change ranges of 30, 90, and 180. For each viewpoint setting, we generate relit videos using three baselines (Hu et al., 2025; Zhou et al., 2025b; Zhang et al., 2025; Liu et al., 2025) and our method, all conditioned on the same source 4D video and the same relighting prompt. We distribute the survey to 30 participants and collect their ratings for each relit video on 15 Likert scale along four dimensions: (i) Prompt Match (1 = poor, 5 = excellent), (ii) Lighting Consistency (1 = flickery, inconsistent, 5 = very stable), (iii) Geometric Consistency (1 = unstable, 5 = very stable), and (iv) Relighting Realism (1 = fake or overlay-like, 5 = realistic lighting). Figure 7 shows an example 13 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Table 5: User Study. Ratings are on 1 to 5 Likert scale. PM: Prompt Match; LC: Lighting Consistency; GC: Geometric Consistency; RR: Relighting Realism. Best results are bolded; second best are underlined. Method PM LC GC RR PM LC GC RR PM LC GC RR 30 90 180 Training-based Light-X Training-free EX-4D + IC-Light EX-4D + LAV Light4D (Ours) 4.2 4.0 4.1 1. 4.0 3.7 3.9 1.4 3.6 2. 3.5 1.2 4.0 3.8 4.5 1.8 3.6 4.4 4.2 3.2 4.3 2.2 3.1 4. 3.8 3.6 4.4 1.5 3.3 4.2 4.0 2.9 4.2 2.0 2.8 4.2 3.5 3.3 4.2 1.3 2.8 3. 3.7 2.5 4.0 1.8 2.4 3.9 Algorithm 1 Inference of Light4D 1: Input: source video Vs, target camera C, lighting prompts L, EX-4D model Vgeo, IC-Light prior Mlight, VAE (E, D), noise levels {σk}, time schedule {tk} 2: Output: relit 4D video 3: Sample noise ϵshared 4: zinit(Vs, C) 5: for = 1, 2, . . . , do 6: 7: 8: 9: 10: tk ˆzgeo 0 Vgeo(z, σt) if λ(t) > 0 then ) 0 xgeo D(ˆzgeo ˆxlight 0 Mlight(xgeo, L, ϵshared; TCA) GMM(ˆxlight xlight FDI(x) xf use (1 λ(t))xgeo + λ(t)xlight ztarget E(xf use) ) ztarget ˆzgeo 0 end if + (σt σt) zztarget σt+δ else 11: 12: 13: 14: 15: 16: 17: 18: 19: end for 20: D(z) 21: return // Canonical Noise Initialization (Equation (8)) // Initialize latent // Map discrete step to continuous time // Estimate geometry state // Decode to RGB space // Inject light (Equation (1)) // Global Moment Matching (Equation (9)) // Freq-Decoupled Regularization (Equation (10)) // Hybrid target (Equation (2)) // Geometric Isolation Phase // Euler solver (Equation (3)) survey interface used in our study, where participants are provided with the source 4D video, the relighting prompt, and the five anonymized relighting results for side-by-side comparison. Results are presented in Table 5. D. Limitation and Future Work As training-free framework, Light4Ds performance is bounded by the capabilities of its foundation modelsspecifically, the geometric fidelity of EX-4D and the single-frame nature of the IC-Light prior. Consequently, maintaining rigorous global lighting consistency remains challenging when adapting 2D image priors to the temporal domain, particularly during extreme viewpoint traversals (90 90). Future work will leverage the frameworks modularity to integrate more advanced video-native illumination models and stronger 4D generative backbones to further improve synthesis quality. We also aim to investigate specialized mechanisms to mitigate photometric inconsistencies induced by extreme viewpoint changes. E. More Experimental Results In this appendix, we provide additional qualitative results that highlight the effectiveness of our method under challenging Light4D: Training-Free Extreme Viewpoint 4D Video Relighting viewpoints, diverse lighting prompts, and complex real-world environments. E.1. Visualizations under Normal Viewpoints. We show qualitative comparisons across variety of object categories under standard viewing conditions. As shown in Figures 8 and 9, these results highlight high-frequency detail preservation and natural lightmaterial interactions. Our method also produces smooth temporal transitions as the light source moves. With our deterministic coherence and regularization mechanisms, we suppress the temporal flicker commonly observed in diffusion-based baselines, yielding stable 4D sequences. E.2. Visualizations under Extreme Viewpoints. To evaluate geometric robustness, we visualize relighting results under large camera viewpoint shifts. These settings are challenging because the overlap between reference and target views is small, which can cause geometric distortion or texture drift in prior methods. As shown in Figure 10, our method preserves the underlying 3D structure without noticeable ghosting artifacts. By decoupling the geometric isolation phase from illumination modulation, our approach keeps relit shadows and highlights spatially coherent even under large viewpoint changes. E.3. Visualizations in Autonomous Driving Scenarios. To demonstrate practical utility on real videos, we apply Light4D to real-world autonomous driving sequences from OpenScene (OpenScene Dataset Contributors, 2023). These scenes include dynamic foreground objects and large outdoor environments. As shown in Figure 11, our method handles outdoor lighting changes and produces realistic shadows on moving vehicles, highlighting its potential for high-fidelity relighting and data augmentation in driving scenarios. 15 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 7: Example of User Study Survey Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 8: Extended Qualitative Results Under Normal Viewpoints. Our method generates realistic light-material interactions and smooth temporal transitions over diverse object categories. 17 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 9: Another Extended Qualitative Results Under Normal Viewpoints. Our method generates realistic light-material interactions and smooth temporal transitions over diverse object categories. Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 10: Extended qualitative results under extreme viewpoints. Our method shows strong geometric robustness and maintains stable 3D structure even under large camera viewpoint shifts. 19 Light4D: Training-Free Extreme Viewpoint 4D Video Relighting Figure 11: Extended qualitative results in autonomous driving scenarios. Our method generalizes well to complex outdoor environments and produces realistic shadows for dynamic vehicles."
        }
    ],
    "affiliations": [
        "School of Computer Science, Peking University"
    ]
}