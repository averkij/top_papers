{
    "paper_title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
    "authors": [
        "Yiming Ren",
        "Junjie Wang",
        "Yuxin Meng",
        "Yihang Shi",
        "Zhiqiang Lin",
        "Ruihang Chu",
        "Yiran Xu",
        "Ziming Li",
        "Yunfei Zhao",
        "Zihan Wang",
        "Yu Qiao",
        "Ruiming Tang",
        "Minghao Liu",
        "Yujiu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support."
        },
        {
            "title": "Start",
            "content": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature Yuxin Meng1* Junjie Wang1,3* Yiming Ren1,2* Yihang Shi1* Zhiqiang Lin1 Zihan Wang3,6 Ruihang Chu1 Yu Qiao2 Yiran Xu1 Ruiming Tang4 Ziming Li4 Minghao Liu3 Yunfei Zhao3,5 Yujiu Yang1 1Tsinghua University 4KuaiShou Inc. 2Shanghai AI Laboratory 32077AI 6Harvard University 5Stanford University rym24@mails.tsinghua.edu.cn, wangjunjie@sz.tsinghua.edu.cn, qiaoyu@pjlab.org.cn, yang.yujiu@sz.tsinghua.edu.cn https://github.com/IIGROUP/sin-bench 6 2 0 2 5 1 ] . [ 1 8 0 1 0 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic Needle-In-A-Haystack tests often reward answer matching without requiring causal, evidence-linked reasoning trace in the document. We propose the Fish-inthe-Ocean (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SINVerify), grounded QA (SIN-QA), and evidenceanchored synthesis (SIN-Summary). We further introduce No Evidence, No Score, scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.566), while GPT5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing gap between correctness and traceable support."
        },
        {
            "title": "Introduction",
            "content": "The deep analysis of long-form, multimodal scientific literature represents hallmark of human cognition (Xia et al., 2024; Chen et al., 2024; Bai et al., 2025a). Accordingly, this task serves as vital benchmark for the assessment of reasoning capabilities at the expert level, such as the proficiency of doctoral researchers. Advancements in Multimodal Large Language Models (MLLMs) (Bai *Equal contribution. Corresponding Author. Under Review. Figure 1: Comparison of long-context multimodal evaluation paradigms. (a) Current NIAH approaches embed artificial needles into irrelevant noise, focusing on surface-level retrieval. (b) The proposed FITO paradigm evaluates deep comprehension within the native document ecosystem (ocean). It requires the model to aggregate interconnected knowledge units (fish) across sections to form an evidence chain for reasoning. et al., 2025b; OpenAI, 2025; DeepMind, 2025; xAI, 2025), particularly in long-context processing and multimodal alignment, render the attainment of this expert reading ability achievable. However, when models process these lengthy and symboldense documents, existing evaluation systems fail to distinguish whether the system genuinely comprehends the complex document logic or merely relies on parametric knowledge to infer the answer. Despite the rapid progress in long-context evaluation, significant misalignment persists between existing benchmarks and authentic scientific cognition (Fu et al., 2024). This discrepancy manifests primarily in task construction and evaluation metrics. First, for task design, as shown in Fig. 1 (a), the prevailing Needle In Haystack (NIAH) paradigm favors the insertion of artificially constructed and semantically isolated segments (Wang et al., 2024b). While this synthetic configuration effectively tests retrieval boundaries, it fails to simulate the nativeness and long-range dependencies inherent in genuine literature. For instance, the interpretation of figures frequently demands retrospective reference to experimental setups within the methodology sections. Second, concerning evaluation metrics, an exclusive reliance on answer accuracy obscures the opacity of the reasoning process (Mathew et al., 2021; Xia et al., 2024). This approach allows models to exploit parametric knowledge as shortcut, which causes hallucinations when the system confronts scientific inquiries. To bridge this gap, we advocate fundamental shift in evaluation focus from mere correctness of answers to the construction of cross-modal evidence chains. Accordingly, as shown in Fig. 1 (b), we introduce the Fish-in-the-Ocean (FITO) evaluation paradigm. To demonstrate that the model avoids mere speculation, the system must explicate the process of evidence retrieval (Dasigi et al., 2021). Distinct from the artificially implanted needle (artificial facts within noise) characteristic of NIAH, critical information within ocean (scientific literature) resembles native fish (interconnected knowledge). Consequently, the resolution of scientific inquiries essentially requires the capture and correlation of these fish dispersed across components such as the abstract, figures, and appendices. The core competency of model lies in the identification of high-value evidence from the vast information space and the aggregation of these findings into self-consistent logical chain. To operationalize the Fish-in-the-Ocean (FITO) paradigm, we build SIN-Data, unified Scientific INterleaved data infrastructure tailored for longform scientific documents. We aggregate papers from sources such as arXiv and PMC, and parse each document into unified representation that preserves the native alternation of prose and visual evidence (e.g., figures) in its original reading order. Following the arXiv taxonomy, SIN-Data covers more than 12 primary disciplines and over 80 subfields, yielding broad substrate of domain knowledge. Building on SIN-Data, we develop scalable pipeline to construct SIN-Bench. Each instance is defined by query, answer(s), and an evidence chain grounded to native anchors. As hierarchical evaluation suite aligned with real scientific cognition, SIN-Bench instantiates progressive discoveryverificationsynthesis workflow via four tasks: (1) SIN-Find (evidence discovery), (2) SIN-Verify (hypothesis verification), (3) SIN-QA (grounded reasoning), and (4) SINSummary (evidence-anchored synthesis). To avoid isolated, answer-only evaluation, we adopt the principle of No Evidence, No Score principle: Highconfidence reasoning is credited only when it is grounded in verifiable evidence. Beyond answer accuracy, we assess evidence quality along three dimensions: Logic, Matching, and Relevance. Empirically, evaluating eight widely-used MLLMs reveals that evidence grounding is the primary bottleneck in long-form scientific reading. For example, Gemini-3-pro achieves the best average overall score (0.566), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) yet lags in evidence-aligned overall scores on SIN-Find and SIN-QA, indicating that correct answers do not necessarily imply traceable support. Furthermore, structured evidence output remains challenging for several open-weight models (often yielding invalid scores). In summary, the contributions are as follows: We introduce the Fish-in-the-Ocean paradigm for evaluating evidence-based long-context multimodal scientific reasoning. We build SIN-Data and SIN-Bench, four-task suite spanning discovery, verification, QA, and evidence-anchored synthesis. We propose No Evidence, No Score and multi-dimensional evidence metrics to diagnose grounding quality beyond answer accuracy."
        },
        {
            "title": "2 Related Work",
            "content": "MLLM Long-Context Understanding Tasks. Recent progress in long-context understanding for MLLMs primarily focuses on exploring retrieval boundaries. Early paradigms, such as MM-NIAH (Wang et al., 2024b) and MMLongCite (Zhou et al., 2025), adopt Needle-InA-Haystack setup that inserts synthetic text or image needles into long contexts to test recall limits. These methods effectively quantify context capacity, but they rely on semantically isolated synthetic content and do not reflect the complex logical dependencies in real cognition. To improve realism, later benchmarks such as MMLongBenchDoc (Ma et al., 2024) and LongDocURL (Deng et al., 2025) move to real web pages and multipage reports. However, these benchmarks mainly emphasize single-point information extraction or page layout perception, and they often overlook long-range cross-modal reasoning that spans the document. To address this gap, we propose the Fish-In-The-Ocean paradigm. Unlike isolated retrieval tasks, this paradigm requires the construction of coherent cross-modal evidence chain. This work further introduces SIN-Bench, which simulates complete scientific research workflow and evaluates deep reasoning in highly interconnected scientific literature. Evaluating Scientific Reasoning in MLLMs. Scientific literature contains dense multimodal content and therefore serves as an effective domain for evaluating expert-level reasoning. Several efforts focus on converting PDF and LaTex documents into machine-readable formats and provide foundation for subsequent research (Wang et al., 2024a; Zhong et al., 2019; Xia et al., 2024). Building on this foundation, recent studies shift toward higher-level cognition. M-DocSum (Yan et al., 2025) introduces reference-grounded interleaved text-and-image summarization, and MMIE (Xia et al., 2025) designs information extraction tasks for knowledge-intensive domains. However, many benchmarks rely on prediction-oriented metrics, such as ROUGE or simple question answering accuracy, and they do not assess the transparency of the reasoning process. This black-box evaluation setting allows answers that come from parametric memorization rather than document-level understanding. Inspired by recent works on deep research (Jiang et al., 2024; Yang et al., 2025b), we introduce an evidence-centered metric suite that includes match, validity, and logic, and it enforces No Evidence, No Score principle to enable fine-grained diagnosis of reasoning behavior."
        },
        {
            "title": "3 The Fish-in-the-Ocean Paradigm",
            "content": "Contrary to the Needle-in-a-Haystack (Wang et al., 2024b) paradigm, which relies on inserting synthetic noise, the Fish-in-the-Ocean (FITO) paradigm evaluates the comprehension of inherently complex, long-form multimodal documents. In this analogy, long-context multimodal documents serve as the canonical ocean, characterized by three essential properties that challenge expertlevel reasoning: (i) Nativeness: Information is inherently present and semantically entangled, rather than artificially injected; (ii) Interconnectivity: Evidence spans sections/modalities (e.g., text, figures, tables); (iii) Long-range Dependency: Logical chains span disjoint sections, such as linking the method to the result and the conclusion. To evaluate this process, we move beyond simple QA likelihood maximization, (AD, Q), where A, D, and denote the answer, document, and query, respectively. Instead, FITO models the joint probability of the answer and its supporting evidence-chain (E): (A, ED, Q) = (ED, Q) (AE, D, Q). (1) This formulation shifts the evaluation objective from result-oriented prediction to process-oriented reasoning. It implies that valid system must explicitly instantiate the latent variable and validate its sufficiency before deriving A."
        },
        {
            "title": "4 SIN-Bench: Evidence-based Evaluation",
            "content": "To operationalize the FITO paradigm, we introduce SIN-Bench. We select scientific literature as the evaluation substrate, as its inherent information density and rigorous logical for assessing expertlevel cognition. Specifically, we establish semiautomated framework that transforms raw scientific corpora into structured reasoning challenges. As shown in Fig. 2, this framework encompasses the construction of unified Scientific INterleaved (SIN) representation (Sec. 4.1), an evidence-based hierarchical task design (Sec. 4.2), an evidencedriven metric system (Sec. 4.3), and scalable benchmark construction pipeline (Sec. 4.4). 4.1 SIN-Data Infrastructure To facilitate rigorous evaluation of long-context multimodal reasoning, we establish the SIN-Data infrastructure, pipeline designed to unify original documents into semantics-aware Scientific INterleaved (SIN) format. Our primary objective is to transcend the limitations of raw formats (e.g., PDF layout or raw LaTeX) by reconstructing linear data stream that preserves the logical and, rather than spatial, coupling between text and visual evidence. As shown in Fig. 2 (a), inspired by recent work (Wang et al., 2024a), our pipeline processes 50k raw source packages from arXiv1 and PubMed Central (PMC)2 through three progressive stages: Stage 1: Element Parsing. We decouple content extraction from presentation layers. For arXiv, the source LaTeX is compiled into responsive HTML via Engrafo, followed by text parsing. Images are recovered from the DOM tree and re-anchored to 1https://arxiv.org/ 2https://www.ncbi.nlm.nih.gov/pmc/ Figure 2: The framework of SIN-Bench. (a) SIN-Data Infrastructure: We parse raw source packages into unified Scientific Interleaved format using semantic-first strategy. (c) Construction Pipeline: Based on the data, we employ an iterative Multi-MLLM synthesis loop with cross-validation and human auditing to generate high-quality samples. (b) Task & Metrics: The benchmark features four hierarchical tasks evaluated under the No Evidence, No Score protocol. We assess evidence chains across Matching, Relevance, and Logic dimensions. their textual context via visual matching. For PMC, we parse JATS XML files into structured JSON with the s2orc-doc2json parser, which preserves tables and citation links while stripping decorative artifacts. Stage 2: Semantic-First Formatting. Structured data is unified into Interleaved Markdown. Unlike layout-dependent PDF extraction, we propose Citation-Driven Injection Strategy to preserve the logical chain of evidence. Each visual component is assigned unique ID xk and inserted near the paragraph of its first citation (e.g., before Figure 3 shows that...). This ensures visual evidence aligns with the reasoning logic. We concurrently compute quality signals, such as the count of textimage interleaved segments and token density, for subsequent processing. Stage 3: Quality Filtering. To ensure high multimodal density, we rigorously prune samples with sparse visual context or broken citation links by quality signals. Moreover, we reference the official arXiv taxonomy to categorize and annotate samples, ensuring the retention of representative distribution across diverse domains. This process yields final set of 4, 000 highquality documents (D). These samples cover diverse range of 12 top-level disciplines and over 80 subfields. Details are in Sec. A.1. 4.2 Evidence-based Hierarchical Tasks To operationalize the evidence-chain-driven objective of the FITO paradigm into actionable benchmark instances, SIN-Bench abstracts the professional reading workflow into four hierarchical tasks: Discovery Verification Question Answering (QA) Synthesis. Each instance employs single document as the substrate, organizing annotations around query Q, an answer A, and an evidence chain = [e1, . . . , eN ], structured interleaved anchors, where odd-indexed elements denote visual anchors and even-indexed elements represent text spans. This unified interface ensures semantic alignment across all four tasks, facilitating consistent diagnosis. SIN-Find (Evidence Discovery). This task assesses the models ability to identify reasoning paths within long-context documents. Given document and query Q, the model must construct the evidence chain E: = ffind(D, Q). (2) Unlike keyword-matching retrieval, is designed to require complex reasoning (e.g., factor induction or method comparison). Consequently, the model must retrieve semantically sufficient crosssectional or cross-modal information and organize it into logically ordered chain E, rather than merely extracting disjointed snippets. SIN-Verify (Hypothesis Verification). This task targets the auditing capability, determining whether the provided evidence suffices to support specific conclusion. The formulation is binary classification problem: = fverify(D, Q, A, E), {0, 1} (3) where the model judges if constitutes consistent and sufficient support for A. To enforce strict discrimination, we introduce negative samples through systematic perturbations (e.g., omitting premises or mismatching conditions). This simulates the critical review process in authentic research environments. SIN-QA (Grounded Reasoning). This task demands the joint generation of the answer and its provenance. The model is required to output both the answer and the evidence chain E: (A, E) = fqa(D, Q). (4) Here, and the explanatory text within are generated de novo rather than extracted. This configuration constrains the models reasoning to verifiable evidence, effectively distinguishing evidencedriven comprehension from hallucinations based on parametric priors. SIN-Summary (Evidence-Anchored Synthesis). This task evaluates holistic understanding and longrange integration over the full document. Given D, the model generates an interleaved, evidenceanchored summary consisting of multiple claims, each paired with its evidence: ˆS = (cid:8)aj, Ej (cid:9)J j=1 = fsum(D). (5) Under our interleaved response format, the overall answer and evidence are overlapping: the synthesis can be equivalently viewed as = {aj}J j=1, = [ j=1 Ej, (6) where each statement aj is accompanied by anchors that make it verifiable. By requiring both cross-sectional integration and explicit evidence for each claim, this task operationalizes the longrange dependencies and multimodal connectivity. SIN-Bench adheres to No Evidence, No Score philosophy. As shown in Fig. 2 (b), we structure the evaluation framework into two layers. For tasks that require generating evidence chains (SIN-Find, SIN-QA, and SIN-Summary), we employ shared protocol to assess the quality of the evidence. Building on this foundation, SIN-QA incorporates an additional assessment of the answer, while SINVerify relies on accuracy. Interleaved Evidence Chain. We denote the ground-truth evidence chain as = [e 1, . . . , ] and the predicted chain as ˆE = [ˆe1, . . . , ˆeM ]. Consistent with the unified interface, elements at odd indices represent visual anchors, and elements at even indices correspond to text spans. We define the grouping components as: = N/2, ) = (e 2i1, 2i), (v , ˆK = M/2, (ˆvi, ˆti) = (ˆe2i1, ˆe2i). (7) During evaluation, we treat the adjacent pair (vi, ti) as the minimal unit of evidence and compute the quality metrics at this granularity. For the SIN-Find, SIN-QA, and SIN-Summary tasks, we assess the quality of the evidence chain via the MRL (Matching, Relevance, and Logic) metrics. We employ LLM as the evaluator to robustly capture semantic equivalence in long-form evidence, overcoming the rigidity of traditional ngram metrics. Specifically, involves semantic adjudication by the LLM (e.g., Qwen3 (Yang et al., 2025a); see Sec. D.3 for selection), whereas and rely on deterministic analytical formulations. (M) Matching. Let = {v i=1 denote the set of ground-truth visual anchors. We define the hit set as the subset of indices in the prediction where the visual anchor matches ground-truth anchor: = {i [1, ˆK] ˆvi }. For each index H, we identify the corresponding ground-truth index j(i) such that j(i) = ˆvi. We then employ the LLM to evaluate the semantic consistency between the predicted text ˆti and the reference text j(i), assigning score si {0, 1, 2, 3}. We normalize this score as si = si/3. The matching metric is defined as: }K Match = ( 1 iH 0, si, > 0, = 0. (8) 4.3 Metrics: No Evidence, No Score To mitigate evaluation bias where models derive correct answers through incorrect reasoning paths (the right for the wrong reasons phenomenon), (R) Relevance (F1). We formulate the correctness of an evidence unit as binary classification problem. The indicator function for correct prediction is Ii = 1[ˆvi si τ ], where τ represents the semantic threshold (set to 2/3 in our implementation). Letting the number of True Positives be = ˆK i= Ii, we calculate: Prec = ˆK , Rec = , F1 = 2 Prec Rec Prec + Rec . (9) (L) Logic (KendallTau similarity). For the set of matched anchors, let ˆπ and π represent their relative permutations in the prediction and the ground truth, respectively, and let = denote the total number of matches. The KendallTau correlation coefficient (Kendall, 1948) is derived as: τ = 1 2 inv(ˆπ, π) K(K 1)/2 [1, 1], KT-sim = τ + 1 2 [0, 1]. (10) SIN-QA: Answer Accuracy. For the predicted answer ˆA, LLM assigns semantic correctness score {0, 1, 2, 3}. We normalize this value to derive the answer accuracy metric, AnsAcc = g/3. SIN-Verify: Accuracy. This task functions as binary classification problem. We evaluate performance using standard accuracy over samples: ScoreVERIFY = Acc = 1 i = 1T 1[ˆyi = yi]. (11) Furthermore, we compute the overall score for each task by averaging its metrics. 4.4 Benchmark Construction Pipeline Building upon the interleaved samples provided by SIN-Data, we design an iterative human-model collaborative pipeline. It achieves scalability through an iterative synthesis process while ensuring factual correctness, reasoning consistency, and evidence verifiability via cross-model filtering and humanin-the-loop auditing. As shown in Fig. 2 (c), the pipeline consists of the following key steps: Seed Examples. We manually create small set of high-quality seed examples for each task. (1) Multi-MLLM Synthesis. Given single document D, we employ multiple MLLMs for collaborative synthesis. We designate SIN-QA and SIN-Summary as the core generation pivot. The model jointly produces the query Q, the answer A, and the draft evidence chain within single context, thereby minimizing semantic drift in SINQA. Subsequently, we derive other tasks within the same synthesis round. We reformulate SIN-Find by reversing SIN-QA to focus on locating the evidence chain that supports the conclusion. For SIN-Verify, we construct negative samples by systematically perturbing (e.g., through random shuffling) or by creating hard samples that feature insufficient evidence but superficially plausible conclusions. For SIN-Summary, models generate comprehensive summary with Cite-as-you-write prompting. This step yields pool of candidate samples. (2) Cross-validation. To mitigate potential synthesis noise, we implement cross-validation across three powerful models from model set. They independently evaluate the candidate samples on scale of 1 to 5 across three dimensions: rationality of the question, correctness of the answer, uniqueness and consistency of the supporting evidence. We strictly retain sample only if it receives majority vote and scores 4 in all three dimensions. (3) Human-in-the-Loop Audit. This process focuses on verifying the precise location of evidence anchors and the rigor of the supporting relationships. Human experts also correct residual factual inaccuracies and formatting deviations to produce high-confidence golden samples. Iterative Loop. These verified golden samples feed back into the system as updated seed examples for subsequent cycles until the dataset reaches the target scale and quality stability. Utilizing 4, 000 high-quality scientific documents, this iterative process yields approximately 3, 200 raw candidate samples. After multipe loops, we collect final SIN-Bench with 490 samples (Find: 159, QA: 158, Summary: 89, Verify: 84). Further details appear in Sec. A.2."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup We examine 8 widely-used MLLMs, comprising 5 proprietary and 3 open-weight models (Details in Sec. D.1). Specifically, 5 proprietary models includes: Gemini-3-pro-preview (DeepMind, 2025), Gemini-2.5-pro (Team, 2025), GPT-5 (OpenAI, 2025), Grok 4 (xAI, 2025), and Claude-sonnet4.5 (Anthropic, 2025). Considering diversity of architectures in open-weight models, including dense and MoE, we selsect 3 widely-used models: Qwen3-VL series with Thinking mode (2B, 8B, and 30B-A3B MoE version) (Bai et al., 2025b). Furthermore, we present detailed discussion regarding all employed models and the rationale for the selection in Sec. D.2. 5.2 Main Results Overall Performance. As reported in Table 1, Gemini-3-Pro-Preview attains the highest avg. overModel SIN-Find SIN-Verify SIN-QA SIN-Summary Avg. M Acc AnsAcc R M Overall Proprietary Gemini-3-pro Claude-sonnet-4.5 GPT-5 Gemini-2.5-pro Grok-4 0.524 0.512 0.534 0.536 0. 0.381 0.490 0.333 0.300 0.339 0.294 0.378 0.268 0.203 0.266 0.399 0.460 0.378 0.346 0.378 0.697 0.697 0.667 0.697 0.682 0.697 0.697 0.667 0.697 0.682 0.726 0.708 0.767 0.684 0. 0.569 0.550 0.578 0.522 0.522 0.521 0.361 0.406 0.240 0.371 0.454 0.333 0.336 0.197 0.289 0.567 0.488 0.522 0.405 0.464 0.784 0.765 0.808 0.762 0.699 0.546 0.493 0.538 0.547 0. 0.471 0.399 0.483 0.472 0.275 0.600 0.552 0.610 0.593 0.458 Open-weight Qwen3-VL-8B 0.500 Qwen3-VL-30B-A3B 0.522 Qwen3-VL-2B 0.485 0.293 0.307 0.160 0.245 0.226 0. 0.346 0.352 0.251 0.667 0.667 0.546 0.667 0.667 0.546 0.476 0.441 0.337 0.514 0.528 0.510 0.341 0.321 0. 0.234 0.214 0.164 0.357 0.340 0.252 0.683 0.676 0.589 0.359 0.345 0.228 0.276 0.275 0.165 0.439 0.432 0. 0.566 0.549 0.544 0.510 0.495 0.452 0.448 0.344 Table 1: Main results on SIN-Bench across diverse MLLMs. We report matching (M), relevance (R), logic (L), answer accuracy (AnsAcc), and verification accuracy (Acc). denotes the task-specific overall score, and Avg. Overall represents the arithmetic mean of overall scores across all tasks. The best performance is bolded. Setting Gemini-3-pro GPT-5 Qwen3-VL-8B Easy Negatives Hard Negatives 1.000 0.250 1.000 0.208 1.000 0.044 Table 2: Accuracy on SIN-Verify under different negative-sampling settings. Hard negatives, which use near-miss evidence, lead to near-chance performance. (overall: 0.460), whereas Gemini-2.5-pro excels in logic preservation (0.536). This pattern indicates trade-off between precise anchor identification and robust evidence-order preservation. In SIN-Verify, accuracy remains tightly clustered under the standard setting (Acc=0.6670.697 in Table 1), but Table 2 shows sharp drop under hard negatives (for example, GPT-5: 1.000 0.208; Qwen3-VL-8B: 1.000 0.044), which indicates limited logical discrimination for near-miss evidence. Detailed experiments appear in Sec. E.2. In SIN-QA, Gemini-3-pro achieves the strongest overall score (0.567), whereas GPT-5 delivers the best AnsAcc (0.767), which indicates gap between answer correctness and evidence quality. This pattern suggests that joint decoding of answers and evidence increases semantic consistency, but it does not guarantee an entailment-support chain. In SIN-Summary, GPT-5 leads overall score (0.610) and achieves the best Logic and Relevance score. This result suggests that the Logic metric benefits from learning the high-level flow of scientific writing (e.g., Abstract Method Result). 5.3 Discussion Importance of Interleaved Input. Fig. 5 shows that preserving the native interleaved structure of scientific papers yields substantial gains over separated layout (imagestext), improving SINQA by +0.102 and SIN-Summary by +0.129 for Gemini-3-pro. Moreover, modality ablations indicate Interleaved > Text-only (captions) > Figure 3: Task-level overall performance heatmap across models in SIN-Bench. Darker cells imply higher scores. all score (0.566), demonstrating robust capability for evidence-driven reasoning. Although GPT-5 leads in AnsAcc for the SIN-QA task, the overall performance on SIN-Find and SIN-QA (overall: 0.378/0.522) remains inferior to that of Gemini3-pro (0.399/0.567). This discrepancy indicates that Gemini-3-Pro possesses superior capability in grounding reasoning within the multimodal context. In contrast, the performance of GPT-5 suggests reliance on parametric knowledge to infer answers rather than strict adherence to the process of evidence identification. More analysis are in Sec. E. Moreover, Qwen3-VL-8B surpasses the larger Qwen3-VL-30B-A3B variant across nearly all metrics. This finding suggests that, in the context of scientific reasoning, the density of reasoning-oriented fine-tuning proves more critical than raw parameter count or MoE architectures. Additionally, multiple open-source models fail to comply with evidenceformatting constraints. This inability leads to invalid scores and underscores the significant challenge associated with generating structured outputs within long multimodal contexts. Task-Level Analysis. As shown in Fig. 3, we report task-level heatmaps across models. In SIN-Find, Claude-sonnet-4.5 shows superior precision in the identification of scientific anchors Figure 4: Qualitative examples of reasoning failures by Gemini-3-pro in the SIN-Find and SIN-QA tasks. Figure 5: Impact of interleaved input and modality encodings on SIN-QA and SIN-Summary (Gemini-3-pro). Figure 6: The score density distribution across varying input token lengths for SIN-QA and SIN-Summary. Image-only (rendered pages), suggesting that captions retain coarse semantics but lose fine-grained empirical evidence, while raw visuals become most useful when locally grounded by adjacent text. The Evidence-chain Effect. We evaluate whether requiring an explicit evidence chain affects answer correctness by comparing Gemini-3pro on SIN-QA with and without evidence-chain generation. In our result, enforcing evidence output improves performance from 0.694 to 0.726. This suggests that evidence-chain generation serves as lightweight multimodal chain-of-thought, promoting evidence grounding before answering and reducing unsupported guesses. Impact of Text Length on Performance Stability. Fig. 6 examines reasoning stability for input contexts exceeding 19k text tokens (Further analysis in Sec. E.3). Both Gemini-3-pro and GPT-5 demonstrate strong adaptability, although GPT-5 exhibits marked tail-end degradation in SIN-QA. Conversely, Qwen3-VL-2B displays instability. For example, it suffers from precipitous performance collapse, evidenced by heavy concentration of scores near zero in SIN-Summary. Error Analysis. As shown in Fig. 4, our qualitative review identifies two failure modes: (1) Information Deficiency, where models omit critical prerequisites in logical chains; (2) Spurious Reasoning, characterized by irrelevant shotgun citations that negatively impact precision. Additionally, we observed that for the same context, when performing SIN-QA, the model tends to provide more coherent reasoning process and reduces the omission of key information points. This gap suggests the target answer in SIN-QA acts as semantic anchor, guiding the model to track long-range dependencies effectively. Further analysis is in Sec. F."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce the Fish-in-the-Ocean paradigm and establish SIN-Bench, comprehensive benchmark tailored for long-context, interleaved scientific reasoning. By mandating the construction of explicit evidence chains via the No Evidence, No Score mechanism, our framework exposes critical grounding gap in state-of-the-art MLLMs: models like GPT-5 often rely on parametric priors to hallucinate correct answers without genuine contextual comprehension, whereas Gemini-3-pro demonstrates superior adherence to provided evidence. Furthermore, our analysis identifies significant vulnerabilities in current models, particularly when confronting adversarial hard negatives or adhering to structured output constraints in open-weight architectures. Ultimately, this study advocates for fundamental shift in multimodal evaluationmoving beyond mere response accuracy to rigorously assess the traceability and logic of the reasoning process. Anthropic. 2025. Claude sonnet 4.5 system card. System card, Anthropic."
        },
        {
            "title": "Limitations",
            "content": "The breadth of the evaluation faces constraints due to the input requirements of current multimodal architectures. While the study incorporates leading generalist models such as GPT-5 and Gemini-3-Pro, the scarcity of models supporting long-context interleaved inputs prevents the inclusion of experts in specific domains. To mitigate this, we open-source all data construction and evaluation codes. This standardized interface assists the community in the integration of emerging MLLMs, thereby expanding future evaluation coverage. Furthermore, strict filtering strategies designed to minimize parsing noise result in trade-off regarding data utilization, as documents with minor imperfections are inadvertently excluded. Fortunately, the pipeline features highly configurable filtering rules. This flexibility allows users to adjust thresholds and voting mechanisms, enabling an optimal balance between purity and scale to retrieve valuable documents."
        },
        {
            "title": "Ethical considerations",
            "content": "We adhere strictly to data compliance principles. All documents in SIN-Data originate from openaccess repositories, specifically arXiv and PMC, and we comply with relevant licensing agreements such as CC-BY 4.0. However, the risk of misuse warrants attention, as entities such as paper mills (Kendall and da Silva, 2024) may exploit this technology to generate fabricated papers. While the No Evidence, No Score principle serves as verification mechanism, we acknowledge the potential for reverse exploitation to create realistic fabrications with plausible evidence chains. Consequently, we urge the community to shift the research focus from generation to the utilization of evidence chains for the detection of academic fraud, and we encourage the disclosure of technical details to promote transparency."
        },
        {
            "title": "References",
            "content": "AI at Meta. 2025. The Llama 4 herd: The beginning of new era of natively multimodal https://ai.meta.com/blog/ AI llama-4-multimodal-intelligence/. Published April 5, 2025. Accessed 2025-12-30. innovation. Lei Bai, Zhongrui Cai, Yuhang Cao, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, and 81 others. 2025a. Intern-s1: scientific multimodal foundation model. CoRR, abs/2508.15763. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025b. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631. Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. 2024. Nougat: Neural optical understanding for academic documents. In ICLR. OpenReview.net. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, and 16 others. 2024. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Sci. China Inf. Sci., 67(12). Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. dataset of information-seeking questions and answers anchored In NAACL-HLT, pages 4599 in research papers. 4610. Association for Computational Linguistics. Google DeepMind. 2025. Gemini 3 pro model card. Technical report, Google. Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, ZhongZhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, and Cheng-Lin Liu. 2025. Longdocurl: comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating. In ACL (1), pages 11351159. Association for Computational Linguistics. Chaoyou Fu, Yifan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, Caifeng Shan, and Ran He. 2024. Mme-survey: comprehensive survey on evaluation of multimodal llms. CoRR, abs/2411.15296. Goeric Huybrechts, Srikanth Ronanki, Sai Muralidhar Jayanthi, Jack FitzGerald, and Srinivasan Veeravanallur. 2025. Document haystack: long context multimodal image/document understanding vision LLM benchmark. CoRR, abs/2507.15882. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu xAI. 2025. Grok-4 model card. Model card, xAI. Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, and Huaxiu Yao. 2025. MMIE: massive multimodal interleaved comprehension benchmark for large vision-language models. In ICLR. OpenReview.net. Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie Wu, Hancheng Ye, Shiyang Feng, Bin Wang, Chao Xu, Conghui He, Pinlong Cai, Min Dou, Botian Shi, Sheng Zhou, Yongwei Wang, and 4 others. 2024. Docgenome: An open large-scale scientific document benchmark for training and testing multi-modal large language models. CoRR, abs/2406.11633. Haolong Yan, Kaijun Tan, Yeqing Shen, Xin Huang, Zheng Ge, Xiangyu Zhang, Si Li, and Daxin Jiang. 2025. M-docsum: Do lvlms genuinely comprehend interleaved image-text in document summarization? CoRR, abs/2503.21839. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 40 others. 2025a. Qwen3 technical report. CoRR, abs/2505.09388. Zhaorui Yang, Bo Pan, Han Wang, Yiyao Wang, Xingyu Liu, Minfeng Zhu, Bo Zhang, and Wei Chen. 2025b. Multimodal deepresearcher: Generating text-chart interleaved reports from scratch with agentic framework. CoRR, abs/2506.02454. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS. Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. 2019. Publaynet: Largest dataset ever for document layout analysis. In ICDAR, pages 10151022. IEEE. Keyan Zhou, Zecheng Tang, Lingfeng Ming, Guanghao Zhou, Qiguang Chen, Dan Qiao, Zheming Yang, Libo Qin, Minghui Qiu, Juntao Li, and Min Zhang. 2025. Mmlongcite: benchmark for evaluating fidelity of long-context vision-language models. CoRR, abs/2510.13276. Song, Peng Gao, Yu Liu, Chunyuan Li, and Hongsheng Li. 2024. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. CoRR, abs/2409.12959. Graham Kendall and Jaime A. Teixeira da Silva. 2024. Risks of abuse of large language models, like chatgpt, in scientific publishing: Authorship, predatory publishing, and paper mills. Learn. Publ., 37(1):5562. Maurice George Kendall. 1948. Rank correlation methods. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023. OBELICS: an open web-scale filtered dataset of interleaved image-text documents. In NeurIPS. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In EMNLP, pages 25112522. Association for Computational Linguistics. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, YuGang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun. 2024. MMLONGBENCH-DOC: benchmarking long-context document understanding with visualizations. In NeurIPS. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2021. Docvqa: dataset for VQA on document images. In WACV, pages 21992208. IEEE. OpenAI. 2025. Gpt-5 system card. Technical report, OpenAI. Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. 2024. Scifibench: Benchmarking large multimodal models for scientific figure interpretation. In NeurIPS. Gemini Team. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261. Junjie Wang, Yin Zhang, Yatai Ji, Yuxiang Zhang, Chunyang Jiang, Yubo Wang, Kang Zhu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Bei Chen, Qunshu Lin, Minghao Liu, Ge Zhang, and Wenhu Chen. 2024a. PIN: knowledge-intensive dataset for paired and interleaved multimodal documents. CoRR, abs/2406.13923. Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, and Wenhai Wang. 2024b. Needle in multimodal haystack. In NeurIPS."
        },
        {
            "title": "Appendix",
            "content": "A.1.1 Stage 1: Element Parsing"
        },
        {
            "title": "A Details of Benchmark Creation",
            "content": "A.1 SIN-Data Construction Pipeline Design Philosophy. We strategically target the Scientific INterleaved (SIN) format to address the scarcity of high-fidelity reasoning data in current multimodal training and evaluation. While largescale datasets like OBELICS (Laurençon et al., 2023) provide vast interleaved sequences, they primarily originate from general web sources where the semantic alignment between text and images is often loose or incidental. In contrast, scientific literature exhibits rigorous symbiotic dependency where the text provides analytical narratives and figures offer empirical evidence. This domain offers unique advantage in content density because scientific writing is inherently knowledge-intensive with high signal-to-noise ratio. It demands precise terminology and logic rather than generic descriptions. Furthermore, the interleaved structure is critical for fostering long-context reasoning capabilities. In the scientific context, the interpretation of single figure often necessitates synthesizing information scattered across pages of dense prose. We preserve this non-linear chain of evidence by logically embedding visuals within the narrative. This design compels models to perform multi-hop reasoning across extended contexts, capability often absent in standard caption-centric datasets. To harness this potential, the SIN-Data pipeline is engineered to bridge the structural heterogeneity between disciplines, specifically between the LaTeX-based source packages predominant in Physics and Computer Science and the XMLstructured archives typical of Biomedicine. Our pipeline unifies these disparate sources into standardized format while offering granular controllability. By computing rich quality signals during the homogenization process, such as token length and visual density, we transform the dataset from raw collection into structured resource. This enables precise stratification and analysis of model performance across varying degrees of multimodal complexity. We draw inspiration from the PIN dataset (Wang et al., 2024a) to re-engineer the parsing workflow and include the quality signals for constructing our SIN-Data dataset. As shown in Fig. 2 (a), our workflow operates in three progressive stages: The initial phase focuses on decoupling content from presentation to generate intermediate structured files. arXiv Stream: We compile LaTeX source packages into responsive HTML using Engrafo3, followed by NOUGAT (Blecher et al., 2024) parsing to extract text. visual matching algorithm recovers image references from the DOM tree, re-anchoring them to their textual contexts. PMC Stream: We leverage customized s2orc-doc2json4 parser to process JATS XML files. This module robustly extracts core academic elementsdiscarding stylistic markup while preserving reference links and tablesto output structured JSON files. A.1.2 Stage 2: Semantic-First Formatting & Signal Extraction We transform the intermediate files into Unified Interleaved Markdown format. core innovation is our citation-driven injection strategy, which inserts unique image placeholder xk immediately preceding the paragraph where the image is first cited, preserving the logical chain of evidence rather than spatial layout. Moreover, during this linearization process, we compute suite of granular quality signals for each document. Volume & Granularity: We record total_tokens (calculated via the Llama-4 tokenizer (AI at Meta, 2025)) and avg_segment_length (the average number of tokens per text block) to measure context length and textual density. Visual Density: We calculate image_count and image_ratio to quantify the proportion of visual information. Interconnectivity: We track interleave_segments, representing the frequency of text-image intersections, which serves as proxy for the complexity of the multimodal reasoning chain. Layout & Resolution: We also extract formatspecific features, such as column layout (single vs. double) and average image resolution, to assess visual quality. These signals are embedded as metadata, serving as the quantitative basis for the subsequent filtering. 3https://github.com/arxiv-vanity/engrafo 4https://github.com/allenai/s2orc-doc2json A.1.3 Stage 3: Quality Filtering & Taxonomy Alignment By utilizing the quality signals computed in Stage 2, we curate the dataset based on the principle of High-Density Multimodal Interconnectivity. We filter out samples with sparse visual contexts, broken reference chains, or extreme lengths (retaining 32k1M tokens). To ensure broad domain coverage, we reference the arXiv taxonomy to categorize and annotate all samples with Qwen3-VL-2B (Bai et al., 2025b) (Details are in Sec. B.4). We employ stratified sampling strategy across these categories to prevent domain collapse, ensuring the final dataset retains representative distribution across diverse scientific fields. From an initial collection of 50, 000 source packages, our rigorous filtering pipeline yielded curated set of 4, 000 high-quality documents. This final corpus spans 10+ top-level disciplines and over 80+ subfields, providing diverse and structurally controllable testbed for scientific reasoning. A.2 SIN-Bench Construction Pipeline Building upon the interleaved document representations from SIN-Data, we establish an iterative humanmodel collaborative pipeline for the construction of SIN-Bench. This framework adheres to the principle of minimizing human intervention while strictly maintaining the validity of samples. As shown in Fig. 2 (c), to achieve scalability and reliability, we employ structured synthesis alongside cross-model consistency checks and Human-in-theLoop protocols. These measures ensure factual correctness, logical self-consistency, and the verifiability of evidence. As illustrated, the workflow comprises four sequential stages and self-improving loop: Seed Example Collection (1) Multi-Model Synthesis (2) Cross-Verification (3) Humanin-the-Loop Audit Golden-to-Seed Iteration. Input Preparation: Pruning and Alignment. Given the extensive length typical of scientific literature, we perform strategic pruning to generate input segments compatible with the context limits of models, while strictly preserving the logical coupling of the SIN format. We prioritize the retention of core sectionssuch as the Abstract, Introduction, Methods, Experiments, and Conclusionand preserve text spans adjacent to the first citation of figures or tables, alongside their corresponding visual anchors xk. This process guarantees that key claims and their associated evidence anchors remain locatable and verifiable, even under the compression of the input. Seed Examples. We manually curate small set of high-quality seed examples for each of the four tasks to standardize the output format and the interface of the evidence chain. (1) Multi-MLLM Collaborative Synthesis. Given single document D, we leverage diverse array of MLLMs to collaboratively synthesize candidate samples, which includes Gemini-3-pro, GPT-5, Qwen3-VL, Grok-4, and Claude-4.5. We designate SIN-QA and SIN-Summary as the core generation pivot. By requiring models to co-generate the content (answers or summaries) and the evidence chain within unified context window, we mitigate the semantic drift and lack of self-consistency often inherent in traditional pipelines. Subsequently, we perform synchronous task derivation within the same synthesis session to structurally guarantee cross-task consistency: SIN-QA: The model cgenerates the answer and the evidence chain for specific reasoning query Q. This process ensures strict semantic alignment between the generated rationale and the source substrate. SIN-Find: We reformulate the valid outputs from the core tasks into evidence localization challenges. By fixing the query or specific claim from the summary, we require the reconstruction of the supporting evidence chain. SIN-Verify: We construct discriminative instances by applying negative sampling to the valid chains. We introduce Insufficient Evidence (pairing valid claims with unrelated evidence) and Perturbed Evidence (disrupting the order or alignment of valid chains) to test the capability of the model to audit logical gaps. SIN-Summary: Adopting cite-as-youwrite strategy, the model produces holistic summary where each key statement explicitly links to verifiable evidence anchors in D, thereby operationalizing grounded longcontext synthesis. This phase produces pool of candidate samples covering all four tasks. It maintains structural coupling across samples, which facilitates unified quality control in subsequent steps. Cross-Validation Auditor Prompt: SIN-Summary Role: You are rigorous academic reviewer auditing the evidence quality of summary generated from research paper. Your judgment must be evidence-centric and strictly grounded in the provided excerpt. Inputs (you will receive): 1) Markdown excerpt from research paper (may contain <figure> placeholders corresponding to images/figures). 2) list of summary evidence items. Each item contains (i) an image anchor marker (e.g., x1, x2, . . . ) and (ii) text snippet describing key points. Task Description: Evaluate whether the provided evidence items faithfully support the claimed summary points, with explicit verification of anchor-to-content alignment (i.e., whether each xk correctly maps to relevant visual content and its surrounding text context in the excerpt). Rubric (Criterionized, 15): Evidence Quality (integer score only) - 5: Faithful and complete support; anchors are correctly matched; coverage includes key aspects (method, results, conclusions); organization is coherent. - 4: Mostly faithful with minor omissions or slightly weak anchor justification; no critical mismatches. - 3: Partial support; noticeable gaps in coverage or unclear anchor relevance; some evidence is loosely connected. - 2: Weak support; multiple items are irrelevant, mismatched, or insufficiently grounded in the excerpt. - 1: Unsupported or hallucinated; anchors do not match the described content; evidence is largely invalid. Evaluation Checklist (Evidence-First): - Do the text snippets accurately reflect key points present in the excerpt? - Are cited image anchors (x1, x2, . . . ) relevant to their corresponding text snippets? - Do the evidence items cover important aspects (methodology, results, conclusions) rather than superficial details? - Is the evidence list well-organized and logically ordered (e.g., method experiments conclusions)? - Does the judgment rely only on the provided excerpt (no external knowledge)? Output Format (STRICTLY FOLLOW): Evidence Quality Score: <INTEGER 1-5> Reason: <ONE PARAGRAPH JUSTIFICATION> Important Constraints: - The score must be on its own line starting with Evidence Quality Score:. - The score must be an integer from 1 to 5 (no decimals). - Do not output any additional fields, bullet lists, or extra lines beyond the two required lines. - Do not use Chinese characters or brackets like { }. - If an item cites an anchor that is not supported by the excerpt, you must penalize the score accordingly. Example Output: Evidence Quality Score: 4 Reason: The evidence items are mostly grounded in the excerpt: item 1 correctly cites x1 to describe the proposed architecture and matches the surrounding text, and item 2 references x2 for the main experimental comparison with accurate reported gains. However, item 3s anchor-to-claim linkage is under-specified because the cited x3 is only loosely related to the stated ablation conclusion, reducing overall faithfulness despite generally coherent organization. Figure 7: Cross-validation prompt used to audit SIN-Summary evidence chains. It operationalizes an evidence-first rubric with discrete 15 tiers and enforces strict, machine-parsable two-line output schema, enabling scalable filtering of mismatched or weakly grounded imagetext evidence. (2) Cross-Validation. To systematically filter out plausible yet unverifiable instances, we employ three-model jury system, with members dynamically sampled from Gemini-3-pro, GPT-5, Claude4.5, and Qwen3-VL-Plus. Each model independently audits candidate samples and assigns integer scores ranging from 1 to 5 across three dimensions: Question Validity (assessing clarity, non-triviality, and document support), Answer Correctness (ensuring factual consistency without critical hallucinations or omissions), and Evidence Consistency (verifying relevance, sufficiency, and logical ordering). To ensure high quality, we enforce strict admission standard: we retain sample only if it secures approval from the majority of the jury and achieves minimum score of 4 across every evaluation dimension. The core objective of cross-validation is not to perform redundant generation but to transform subjective review into scalable filtering mechanism using adjudicative, parsable, and evidence-centric prompts. We uniformly adopt the following design principles across all validation prompts: Rigor of Support: The evidence chain must be sufficient and necessary to support the conclusion. For negative samples in SIN-Verify, the reviewer confirms that the attribute of insufficient or inconsistent holds true. Evidence-First: We explicitly require the review to center on evidence anchors and support relations, minimizing bias towards linguistic fluency or superficial plausibility. Criterionized Rubric: We decompose the evaluation into fixed dimensions and discrete tiers (15) to reduce calibration drift between different auditor models. Strict I/O Schema: We enforce fixed output fields and integer scores to facilitate automatic parsing and threshold-based filtering, avoiding the noise associated with free-form responses. Anchored Validation: We explicitly verify the mapping between visual identifiers (e.g., xk) and text segments to detect image-text mismatches and irrelevant citations. Anti-Hallucination: We require the judgment to rely strictly on support from the provided document segments and anchors, preventing completion based on external parametric knowledge. As an illustrative example in Fig. 7, the validation prompt for SIN-Summary requires the auditor to focus on Evidence Quality, checking whether summary items accurately cover key points of the paper, if citation anchors remain relevant, and if the organization follows logical order. (3) Human-in-the-Loop Audit. Despite passing cross-validation, candidate samples may retain finegrained grounding biases, such as anchors that are locatable but lack rigorous logical support, or the omission of key premises. To address this, we assemble review team consisting of 24 graduate students and doctoral candidates. Each sample undergoes independent verification by 23 reviewers, while senior researchers adjudicate any disagreements. This manual review strictly adheres to the following criteria: Anchored Locatability: Every xk and text span must be precisely locatable within the SIN document, ensuring unambiguous citation. Factual Consistency: The answer or summary must not introduce information external to the document. Numerical values, comparative conclusions, and conditional settings must align strictly with the source text. Structural Compliance: The output must conform to the task format and the specifications of the interleaved evidence chain, ensuring that the order of evidence matches the logical dependencies of the reasoning process. Discriminability: Questions must avoid being overly trivial or undecidable. The evidence chain must possess diagnostic value, effectively distinguishing between evidence-driven reasoning and lucky guesses. This stage produces high-confidence golden samples and provides reliable signals for the update of seed examples. (4) Bootstrapping Iteration (Golden Seed). The golden samples flow back into the system to serve as updated seed examples for the next cycle of synthesis and derivation. This forms closed feedback loop that continues until the benchmark meets the targets for scale and quality stability. Scale and Composition. Building upon corpus of 4, 000 high-quality scientific documents, the multi-round synthesis pipeline yields approximately 3, 200 raw candidate samples. Following the three-model cross-validation and Human-inthe-Loop review, SIN-Bench comprises final set of 490 golden samples: 159 for SIN-Find, 158 for SIN-QA, 89 for SIN-Summary, and 84 for SINVerify. Furthermore, the volume of the currently released Golden Samples faces constraints due to the cost of human auditing. The number of highquality samples is currently limited. However, the benchmark possesses inherent scalability due to the proposed semi-automated collaborative pipeline. Given sufficient computational and human resources, the pipeline encounters no bottleneck regarding the quantity of samples. Task Avg. Length Max Length Min Length Scope Value SIN-Find SIN-QA SIN-Verify 108.89 122.77 243.49 186 203 322 60 78 160 Table 3: Token-length statistics of input contexts across task types. On the Principle of No Evidence, No Score No Evidence, No Score is an evidence-centered evaluation principle: model should not obtain high task score solely from answer-only guessing when its output cannot be grounded to verifiable document anchors. Accordingly, for evidencerequired tasks, evidence quality is treated as firstclass component of the final score rather than an optional explanation. Crucially, this principle targets verifiability rather than citation surface forms. In practice, we do not penalize minor formatting variations (e.g., Fig. 2 vs. Figure 2 vs. x2) as long as the anchor is recoverable and can be matched to the papers indexed visual markers. Only when the prediction provides no recoverable anchors (e.g., missing or invalid identifiers) does it receive zero evidence credit. We also clarify how this principle remains logically self-consistent with separate answer scoring. We always compute AnsAcc as standalone assessment of the predicted answer, but AnsAcc alone is not sufficient to yield high overall task score without evidence. When no anchor is matched, evidence metrics collapse to zero by definition (e.g., Matching = 0, F1 = 0; and we set KT-sim = 0 when it is undefined due to insufficient matches). This explicitly bounds the contribution of answeronly correctness and ensures that competitive performance requires grounded evidence, which is the intended behavior of No Evidence, No Score. Tokens (Text / Image / Total) Image Ratio Text Ratio Avg. #Images per instance 15k / 3k / 18k 15% 85% 6. Table 4: Overall modality composition of benchmark inputs per sample. Task Bold Avg. Italic Avg. Title Avg. SIN-Find SIN-QA SIN-Summary SIN-Verify Overall 70.52 70.83 37.46 90.35 68.01 122.46 118.17 155.74 155.65 132.81 14.64 15.03 14.91 15.94 15. Table 5: Markup statistics (bold/italic/title tokens) across tasks, reflecting structural formatting cues in the inputs. B.1 General Statistics We report descriptive statistics of the benchmark inputs, focusing on (i) instance length, (ii) textimage composition, and (iii) structural markup density. Table 3 summarizes token-length statistics by task type. Overall, SIN-Verify instances are substantially longer than SIn-Find and SIN-QA (roughly 2 times on average), indicating higher contextual and reasoning load, while SIN-Find and SIN-QA exhibit comparable length ranges. Table 4 characterizes the overall modality composition. On average, image-related tokens account for 15% of the total tokens, with 6.6 images per instance, suggesting that tasks require non-trivial visual grounding while remaining predominantly text-driven. Finally, Table 5 reports structural markup statistics (bold/italic/title tokens), which reflect the density of formatting cues and sectioning signals in the inputs. Such cues affect how models parse document structure and may influence both difficulty and computational cost across task types. B.2 Samples from SIN-Bench Finally, our existing metrics already cover the practical corner cases that commonly arise in longform scientific grounding. Over-generation of irrelevant anchors is reflected by reduced precision in Relevance; under-coverage of required evidence is reflected by reduced recall; and plausible but cherry-picked or mis-ordered evidence is captured by degradation in Logic. These behaviors are handled within the metric suite defined in the main text, without introducing additional indicators. As shown in Fig. 8 and Fig. 9, we visualize one golden instance per task in SIN-Bench. Since scientific documents are long, rendering full instances would be space-inefficient and would dilute the task-relevant signal. We therefore show only cropped screenshots that cover the document, the question, the annotated evidence region(s), and the gold output (answer, verification decision, or summary items). The benchmark itself operates on the full interleaved source files (Markdown with image Figure 8: Golden examples from SIN-Bench: SIN-Find and SIN-QA. We visualize one curated instance per task with explicit markers for Document, Question, Answer, and the gold Evidence chain. Figure 9: Golden examples from SIN-Bench: SIN-Verify and SIN-Summary. We visualize one curated instance per task with explicit markers for Document, Question, Answer, and the gold Evidence chain. assets), which are released with stable identifiers for exact reconstruction. B.3 License The benchmark dataset and source code are available under the MIT License. The full text of the license is provided at https://opensource.org/ licenses/MIT. B.4 Discipline Taxonomy List To ensure comprehensive evaluation of multimodal reasoning across scientific disciplines, we constructed three-level hierarchical taxonomy referencing the arXiv subject classification. Our pipeline integrates large model capabilities with expert human verification. We first utilized Qwen3-VL-2B to perform preliminary classification on the collected samples. Subsequently, domain experts reviewed the results to correct misclassifications and refine subfield definitions. The resulting taxonomy comprises 12 Level-1 major disciplines, branching into 35 Level-2 domains and 84 Level-3 subfields. The hierarchical structure is organized as follows: Astronomy & Astrophysics: Extragalactic Astronomy & Cosmology: Active Galactic Nuclei & Quasars; Galaxy Formation & Evolution. Galaxy Evolution & Structure: Structure & Evolution of Galaxies. tion. Software Engineering & Programming: AI for Software Engineering; Program Synthesis & Verification. Systems & Infrastructure: Edge & Cloud Computing & Serverless. Earth & Environmental Sciences: Climate & Atmospheric Science: Atmospheric Chemistry & Air Quality; Climate Modeling & Prediction. Geophysics & Remote Sensing: Geospatial Intelligence & GIS; Satellite & Aerial Imaging. Oceanography & Hydrology: Water Resources Planetary & Space Science: Planetary Geology & Hydrology. & Exploration. Planetary Systems & Formation: Planetary Systems Protoplanetary Disks. Biology & Life Sciences: Neuroscience: Neuroimaging. Computer Science: Algorithms & Theory: Combinatorial Algorithms; Graph & Combinatorial Algorithms; Optimization; Quantum Algorithms & Computation Theory. Planetary & Space Science: Planetary Geology & Exploration; Space Instrumentation & Missions; Space Weather & Near Earth Environment. Economics & Finance: Macroeconomics & International Trade: Sovereign Debt & Default Risk. Engineering: Mechanical & Robotics Engineering: Robotics & Automation. Communication & Networking: Information Mathematics: Theory & Coding; Channel Modeling. Pure Mathematics: Analysis & Functional Computer Vision: 2D 3D Reconstruction & Modeling; Image Processing & Restoration; Video Understanding & Compression; Vision. Electrical & Computer Engineering: Embedded & Hardware Systems. Analysis. Medicine & Health Sciences: Neurology & Psychiatry: Computational Psychiatry. Philosophy & Ethics: Human-Computer Interaction: Virtual AugAI & Technology Ethics: Philosophy of Quanmented Reality & Haptics. Machine Learning & AI: Active Learning; Clustering & Unsupervised Learning; Deep Learning; Deep Reinforcement Learning; Differential Privacy & Anonymization; Explainable & Fair AI; Federated & Continual Learning; Generative Models; Graph Neural Networks; Multimodal Learning & Fusion; Nonlinear Time Series; Object Detection; Particle Filters & Data Assimilation; Privacy; Quantum Algorithms & Computation Theory; Second Order Methods; Submodular Optimization. Natural Language Processing: Computational Linguistics; Information Retrieval & QA Systems. Robotics: Autonomous & Mobile Robotics; Learning for Robotic Control. Security & Privacy: Adversarial Machine Learning; Blockchain & Distributed Ledger Security; Differential Privacy & Anonymizatum Physics. Physics: Astrophysics & Cosmology: Astronomical Instrumentation; Atmospheric Chemistry & Air Quality; Cosmology & Galaxy Formation; Exoplanets & Planetary Science; Gravitational Lensing & Cosmology; Gravitational Lensing & Microlensing; Neutron Stars & Pulsars; Planetary Geology & Exploration; Satellite & Aerial Imaging; Space Instrumentation & Missions; Supernova Remnants & Cosmic Rays; Supernovae & Stellar Evolution. Condensed Matter & Materials: Computational Materials Physics; Condensed Matter Physics; Materials Science & Characterization; Spin Glass & Magnetic Materials; Superconductivity & Magnetism. Optics, Fluids & Dynamics: Laser Physics & Photonics; Photonic Crystals & Band Gap Physics; Plasma & Fusion Physics; Quantum Optics & Atomic Physics . Optics & Photonics: Nonlinear Optics & Optical Processing; Optical Imaging & Sensing. Quantum & Particle Physics: Gravitational Waves & Cosmology; High; Neutrino Astronomy & Astrophysics; Quantum Computing & Information; Quantum Optics & Atomic Physics. Social Sciences: Communication, Media & Linguistics: Computational Social Media Analysis. Economics & Finance: Econometrics & Applied Microeconomics. Education & Learning Sciences: Educational Technology & AI in Education. Sociology & Social Policy: Social Networks & Computational Social Science. Statistics & Data Science: Data Science Applications: Health & Social Data Science. Statistical Theory & Methods: Nonparametric & Bayesian Statistics. This that SIN-Bench covers wide spectrum of knowledge. structured distribution ensures"
        },
        {
            "title": "Benchmarks",
            "content": "Table 6 positions SIN-Bench against representative long-document and scientific multimodal benchmarks. Prior benchmarks have substantially advanced long-context stress testing and documentcentric task coverage, including long-PDF understanding suites (e.g., MMLongBench-Doc) and needle-style retrieval tests that insert sparse targets into lengthy contexts (e.g., Document Haystack). Scientific-domain resources further expand coverage via structured document tasks (DocGenome (test)) or figure-focused evaluations (SciFIBench). Despite these advances, most existing benchmarks still rely primarily on answer-level grading and do not explicitly require models to produce auditable cross-modal evidence chains, which makes it difficult to disentangle document-grounded reasoning from plausible but ungrounded generation. In contrast, SIN-Bench targets evidence-based scientific comprehension over native scientific documents in an interleaved image and format, and organizes four progressive text tasks (Find/Verify/QA/Summary) that mirror discoveryverificationsynthesis workflow. Beyond answer correctness, we explicitly score evidence quality (e.g., Matching/Relevance/Logic), enabling fine-grained diagnosis of where failures arise (missing prerequisites vs. spurious citations) rather than collapsing them into an answer-only signal. While SIN-Bench contains fewer evaluated documents than some large-scale resources, it is distilled from an initial pool of 50k collected papers through multi-stage cleaning and filtering pipeline designed to preserve high-quality scientific documents at each step. Importantly, this pipeline is scalable: human involvement is lightweight and predominantly verification-based (binary/ordinal judgments), avoiding the need for extensive manual authoring of questions and reference answers while still producing reliable, document-grounded evaluation instances."
        },
        {
            "title": "D Details of Experimental Setting",
            "content": "D.1 Baselines We report the version identifiers for all proprietary models evaluated to support reproducibility. We run all evaluations once with the temperature set to 0, including open-weight models. The evaluated proprietary models and their version identifiers are as follows: Gemini-3-pro: gemini-3-pro-preview-11-2025 Gemini-2.5-pro: gemini-2.5-pro-thinking Claude-sonnet-4.5: claude-sonnet-4-5-20250929-thinking GPT-5: gpt-5 Grok-4: grok-4-0709 D.2 Models Used in the Benchmark and Experiments We evaluate mix of proprietary and open-source models in SIN-Bench. In the analysis experiments, compute budget and iteration cost constrain the number of controlled ablations, and the experimental design therefore prioritizes representative strong model as the primary testbed. Gemini-3-pro serves as this primary model since it achieves the best overall performance on SIN-Bench when averaging across tasks, which makes it representative reference point for diagnostic analyses. In the modality study in Fig. 5 (Sec. 5.3), we isolate the impact of raw visual inputs by constructing Benchmark Doc type Scale Avg. length Domain coverage Main tasks Ev. Ev.-score MMLongBench-Doc (Ma et al., 2024) Scientific PDFs (multi-domain) 130135 docs (benchmark) 21k tokens 7 domains (e.g., acad./legal/tech) DocGenome (test) (Xia et al., 2024) Scientific papers (structured doc) 8 classes +153 subclasses Document Haystack (Huybrechts et al., 2025) Long PDFs (general) 400 docs 5200 pp. General SciFIBench (Roberts et al., 2024) Scientific figures (charts) 8 figure types (CS-centric) SIN-Bench (ours) Scientific docs (interleaved) 231 docs / 490 inst. 18k tokens 10+ L1 domains; 30+ L2; 80+ L3 Long-doc understanding; numerical reasoning; cross-element search 7 doc tasks (e.g., cls./grounding/QA/ layout detection, . . . ) Needle-in-a-haystack retrieval (long-doc) Figure interpretation 4 tasks: Find/Verify/ QA/Summary (discoverysynthesis) No No No No No No No No Yes Yes Table 6: Comparison with representative long-document and scientific multimodal benchmarks. Ev. indicates whether the benchmark requires an explicit evidence chain in the model output, and Ev.-score indicates whether evidence quality is explicitly evaluated (beyond answer-only scoring). pure text setting. This setting replaces each image slot in the original interleaved sequence with natural-language description of the corresponding figure. The descriptions are generated by Qwen3-VL-8B-Instruct, which preserves the document structure while removing direct visual signals. For evaluation components that require semantic judgment, we adopt an LLM-based evaluator instead of n-gram overlap metrics. This design reduces sensitivity to paraphrase and surface-form variation, which n-gram metrics often penalize despite semantic equivalence. It also fits settings with diverse valid answer phrasings, where single reference string can under-specify correctness and cause systematic false negatives. In addition, an LLM judge supports rubric-guided, criteria-based grading that is directly aligned with task definitions. Prior work shows that rubricor form-based LLM evaluation improves agreement with human judgments and provides scalable assessment for open-ended outputs (Liu et al., 2023; Zheng et al., 2023). Concretely, Matching and answer correctness (AnsAcc) use Qwen3-8B as the automatic judge. Furthermore, in Sec. D.3, we benchmark the judge against human expert judgments and observe close agreement, which supports the reliability of the LLM-based evaluator for scalable evaluation. D.3 Judges in Metrics To ensure the reliability of our automated evaluation in matching and question accuracy metrics, we employ Qwen3-8B as the judge. We validated this choice through rigorous consistency study involving all instances for each task. For these samples, we applied dual-scoring protocol to compare the automated judge against human experts. Task Metric Pearson (r) Spearman (ρ) SIN-Find Matching (M) SIN-QA Matching (M) Answer Acc. SIN-Summary Matching (M) - Average 0.824 0.812 0.895 0.768 0.825 0.795 0.788 0. 0.741 0.797 Table 7: Correlation between human expert ratings and automated scores (evaluated by Qwen3-8B) across different tasks and metrics. All results are statistically significant with < 0.001. Specifically, Qwen3-8B (Yang et al., 2025a) independently evaluated the model predictions, while panel of 23 Masters and Ph.D. students with relevant domain expertise performed manual evaluation. The human ratings were aggregated via majority voting to establish the evaluation gold standard. We then computed the correlation between the automated scores and this human consensus. As shown in Table 7, the automated judge exhibits strong alignment with the gold standard (Avg Pearson = 0.825), confirming that Qwen3-8B serves as valid and consistent surrogate for human evaluation."
        },
        {
            "title": "Analysis",
            "content": "E.1 Discipline-level Performance Analysis Fig. 10 summarizes discipline-level performance under our Level-1 taxonomy. We observe the crossdiscipline variation, indicating that SIN-Bench captures meaningful shifts in evidence-grounded scientific comprehension. Aggregated across models, Economics & Finance and Medicine & Health Sciences exhibit relatively strong overall performance (avg. 0.66 and 0.65), whereas MatheFigure 10: Domain-wise performance on SIN-Bench. Radar plot of average overall scores (across all tasks) for GPT-5, Gemini-3-pro, and Qwen3-VL-8B across 12 primary disciplines. Higher is better, highlighting substantial domain-dependent variability in evidencegrounded scientific comprehension. matics is consistently the most demanding domain (avg. 0.31). This contrast suggests that rigorous symbolic manipulation and quantitative grounding remain key bottlenecks for current MLLMs. Model rankings are also discipline-dependent. Gemini-3-pro shows clear advantages in visually intensive or layout-heavy disciplines such as Astronomy & Astrophysics and Medicine & Health Sciences (e.g., +0.01 and +0.05 over GPT-5), consistent with stronger cross-modal anchoring when evidence is primarily conveyed through figures, tables, and chart-like graphics. In contrast, GPT-5 performs better in more formal/quantitative disciplines, most notably Mathematics (+0.10 over Gemini-3-pro) and Statistics & Data Science. Qwen3-VL-8B trails the frontier models in most disciplines, yet remains comparatively competitive in Social Sciences and converges with the others in Philosophy & Ethics, where the decision signal may rely more on general discourse understanding and less on fine-grained multimodal evidence. E.2 An Analysis of Near-Miss Evidence in SIN-Verify To probe verification difficulty beyond trivial mismatch, we construct two negative sets during crossvalidation (Sec. A.2, step (2)). Easy negatives pair question with evidence that is clearly irrelevant, making rejection straightforward. Hard negatives are near-miss cases: the question quality is high, and the answer is correct, but the evidence chain fails cross-validation agreement, i.e., it is ambiguFigure 11: Accuracy on SIN-Verify with two negative sets. Easy negatives are clearly irrelevant evidence; hard negatives are cross-validation near-misses with correct answers but insufficient/ambiguous evidence support. ous or insufficient to conclusively support the answer. We sample 24 instances for each setting. As shown in Fig. 11, MLLMs nearly saturate on easy negatives, indicating strong coarse mismatch detection. In contrast, accuracy drops sharply on hard negatives, indicating that near-miss evidence makes verification substantially more demanding than relevance detection. Models must assess evidence sufficiency via fine-grained cross-modal grounding (e.g., numbers, conditions, and figuretext alignment) while avoiding answer-driven rationalization. Gemini-2.5-pro performs best on hard negatives (0.417), consistent with stronger calibration toward insufficient evidence decisions. Gemini-3-pro (0.250) and GPT-5 (0.208) remain substantially below the ceiling, implying that general reasoning strength does not directly translate to stricter support judgments under near-miss evidence. The remaining models fall to near-zero, aligning with verification behavior dominated by surface relevance cues. E. Impact of Document Length on Performance Stability. Scientific papers are long-form and multimodal by construction. To characterize how performance varies with context length under interleaved inputs, we analyze the joint distribution between prompt length and overall score using two token accounting schemes: text-only tokens  (Fig. 12)  and text+image tokens  (Fig. 13)  . In this subsection, we report results for SIN-Find, SIN-QA, and SIN-Summary. In Sec. 5.3, we further focus the interpretation on SIN-QA and SIN-Summary, since these two tasks are high-level and require holistic document understanding plus evidence-backed generation. Figure 12: Overall score frequency vs. text-token length for SIN-Find, SIN-QA and SIN-Summary. Normalized 2D frequency of samples binned by text prompt length and overall scores. Setup. For each evaluated sample, we record the true input token count used in the API call and the samples overall score. In Fig. 12, the x-axis counts text tokens only. We adopt this view as the primary fairness axis because English tokenization is broadly similar across models, while image tokenization can be strongly modeland pipelinedependent. Therefore, we consider text-only token length more representative and fair basis for crossmodel comparison. In Fig. 13, the x-axis counts text+image tokens, where image tokens depend on visual encoding policies (e.g., native-resolution v.s. resized inputs). For Qwen3-VL-2B, token statistics follow the official configuration. Each panel visualizes the normalized frequency over (prompt length, overall score); the inset reports the mean overall score. Text-only length  (Fig. 12)  : robustness under long textual contexts. Across all three tasks, Gemini-3pro and GPT-5 place substantial probability mass in the mid-to-high score region across the full text-length range (roughly 1k19k tokens). GPT-5 shows more pronounced score drop than Gemini3-pro as prompts become longer. However, neither model exhibits systematic collapse of probability mass into the low-score band at the longest contexts. This pattern suggests that, for strong models, increased text length alone is unlikely to be the primary bottleneck; residual errors more plausibly stem from selective evidence identification and finegrained grounding in dense scientific narratives. In contrast, Qwen3-VL-2B exhibits consistently low-scoring regime. For SIN-Find and SINQA, the score distribution concentrates near the lower end, with limited migration toward higher scores even at shorter text lengths. similar skew is observed for SIN-Summary, where performance remains dominated by low-score mass throughout. Overall, these trends indicate that the limiting factor for Qwen3-VL-2B is not merely long-context handling, but broader gap in robust evidence retrieval, cross-modal alignment, and high-level synthesis under interleaved scientific inputs. Text+image length  (Fig. 13)  : length expansion and clustering induced by visual tokenization. When image tokens are included, the prompt-length Figure 13: Overall score frequency vs. total (text and image) token length. Same as Fig. 12 but with prompt length computed from text plus image tokens from the API request. range expands substantially (up to 87k tokens). This expansion largely reflects figure count and model-specific visual encoding. Relative to Fig. 12, the distributions also become more clustered along the x-axis. This is consistent with total length discretizing into regimes induced by typical figure counts/resolutions and the underlying visual tokenizer. Therefore, trends along the total-length axis should be interpreted cautiously, as it conflates document visual content with preprocessing and encoding choices. figures are redundant or directly supportive, GPT5 can exploit the extra multimodal evidence and retain high scores. However, when longer inputs introduce visually similar distractors, dense multifigure evidence chains, or resolution-sensitive cues (e.g., small text, numbers, axis labels), errors become more likely, yielding low-score mode. In other words, longer multimodal contexts do not uniformly degrade performance; instead, they increase instance-level variance by mixing helpful and adversarial visual additions. Gemini-3-pro and GPT-5 attain similar mean overall scores, but their distributions differ qualitatively as context grows. Gemini-3-pro remains comparatively smooth and spread across mid-tohigh scores, with consistent trend across lengths. By contrast, GPT-5 exhibits clearer bifurcation: as total length increases, probability mass splits, with one mode staying in high-score regions while another shifts toward lower scores. possible explanation is that total length mainly increases through additional figures, which amplifies variance in visual grounding difficulty. When the added For Qwen3-VL-2B, the score distribution is still dominated by low-score regions across tasks. Nevertheless, in SIN-Find and SIN-QA, we observe non-trivial mass at moderate scores under longer total lengths. One likely reason is that these longer prompts are driven more by additional images than by substantially longer text, and the tasks can sometimes be solved via salient visual cues or nearsurface matching once the correct figure is present. In contrast, SIN-Summary remains almost entirely concentrated in low-score regions even at large total lengths, suggesting that additional visual tokens Figure 14: Error cases on SIN-QA. plifying the role of standard deviation in variability analysis. The resulting answer appears quantitative but violates key methodological constraints, leading to method-inconsistent conclusions. alone do not alleviate the core difficulty of globally organizing and synthesizing evidence into coherent, document-level summary."
        },
        {
            "title": "F Case Study",
            "content": "As shown in Fig. 14, we further analyze SIN-QA, since it requires both final answer and an explicit evidence chain, making it suitable for diagnosing grounding failures beyond aggregate scores. In addition to the two failure modes summarized in Sec. 5.3 (Information Deficiency and Spurious Reasoning), our case studies reveal two specific instantiations. In Case (a), the model injects unsupported concepts absent from the reference (e.g., expanding tidal-force explanation into an earthquake narrative). While some statements remain superficially related, the added content shifts the explanatory frame and induces semantic drift, exemplifying Spurious Reasoning: fluent and confident outputs contaminated by ungrounded expansions that reduce precision. In contrast, Case (b) reflects procedural form of Information Deficiency: the model does not adhere to the references intended statistical interpretation, by conflating or oversim-"
        }
    ],
    "affiliations": [
        "2077AI",
        "Harvard University",
        "KuaiShou Inc.",
        "Shanghai AI Laboratory",
        "Stanford University",
        "Tsinghua University"
    ]
}