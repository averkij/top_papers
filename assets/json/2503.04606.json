{
    "paper_title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "authors": [
        "Aoxiong Yin",
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Xinyu Zhou",
        "Juncheng Li",
        "Siliang Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/."
        },
        {
            "title": "Start",
            "content": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Aoxiong Yin * 1 Kai Shen * 2 Yichong Leng 2 Xu Tan 2 Xinyu Zhou 2 Juncheng Li 1 Siliang Tang"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, hybrid framework that synergizes the strengths of both paradigms through coarse-tofine generation. Our architecture introduces three key innovations: (1) semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving 14,000 compression ratio; (2) language model that generates semantic tokens with high-level semantic relationships; (3) streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, 5B model, achieves score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art opensource models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https: //landiff.github.io/. 5 2 0 2 6 ] . [ 1 6 0 6 4 0 . 3 0 5 2 : r 1. Introduction Text-to-video (T2V) (Blattmann et al., 2023; Kondratyuk et al., 2024; Wang et al., 2024a; Yang et al., 2024b) has made significant progress in recent years, becoming an important research direction in the fields of computer vision *Equal contribution 1College of Computer Science and Technology, Zhejiang University, Hangzhou, China 2Moonshot AI, Beijing, China. Correspondence to: Xu Tan <tanxu@moonshot.cn>, Juncheng Li <junchengli@zju.edu.cn>. Figure 1. The rate-distortion curve illustrates how visual distortion decreases as the number of transmitted bits increases. With just small number of bits representing high-level semantic features, we can already achieve relatively low visual distortion. Building on this information-theoretic insight, LanDiff combines the strengths of both paradigms: LLMs efficiently generate compact semantic features in the first stage, followed by diffusion models that add perceptual details in the second stage, before final decoding to pixels via VAE. Data from Ho et al. (2020), illustration is conceptual. and artificial intelligence. Recent works in T2V models have primarily revolved around two predominant paradigms: autoregressive large language model (LLM)-based (Kondratyuk et al., 2024; Wang et al., 2024a) frameworks and diffusion-based architectures (Blattmann et al., 2023; Yang et al., 2024b). However, each paradigm has their own advantages and limitations, as shown in Table 1. Table 1. The comparison between LLM based and diffusion based T2V systems. The advantages and disadvantages are marked by and Methods , respectively. Representations LLM Diffusion Discrete Tokens Low Reconstruction Quality Highlight Semantic Information Continuous Vectors High Reconstruction Quality Lack Semantic Information Modeling Autoregressvie No Refinement Causal Modeling Non-Autoregressvie Progressive Refinement Non-causal Modeling From representation perspective, LLM-based methods leverage discrete tokenization to explicitly encode high-level The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Table 2. The comparison between LanDiff and previous large-scale T2V systems. The compression rates of LLM-based and diffusionbased models are illustrated using VideoPoet (Kondratyuk et al., 2024) and CogVideoX (Yang et al., 2024b) as examples, respectively. LLM Diffusion LanDiff Models Highlight Semantic Information? Progressive Refinement? Causal Modeling? High Visual Quality? Compression Ratio Long video Generation? 256 1024 14000 semantics through vector quantization, effectively prioritizing conceptual abstraction and narrative coherence. However, this discretization inherently sacrifices low-level visual fidelity due to information compression, resulting in low reconstruction quality. In contrast, diffusion-based approaches employ continuous latent representations to preserve much more perceptual details, enabling superior reconstruction quality at the cost of diluted semantic interpretability, as hierarchical features remain entangled in the latent space. From generative modeling perspective, LLM-based systems adopt autoregressive modeling to enforce causal dependencies between video frames, ensuring strong temporal coherence. However, the autoregressive generation inherently risks error propagation across time steps, where inaccuracies in early predictions amplify during decoding. In contrast, diffusion-based methods employ non-autoregressive generation, refining outputs in parallel through iterative denoising steps. Although this design mitigates error accumulation and enhances generation flexibility, the absence of explicit causal constraints often leads to temporal inconsistencies or semantic hallucinations. In this work, we propose hybrid architecture that synergizes the strengths of both Language models and Diffusion models, named LanDiff, through coarse-to-fine generation paradigm. As shown in Figure 1, inspired by the human creation of video which will generate the high-level storyline first and then add low-level visual details based on the storyline to form the video, we design two-stage video generation process with the number of bits gradually increasing and carefully design the autoregressive model and the diffusion model to be responsible for different stages of T2V generation, so as to play to their strengths and avoid their weaknesses. In detail, 1) at the low-bit position semantic feature, the low-bit information ensures that the token sequence is not too long, and the high-level information makes it easier for the model to capture the overall semantic entity motion of the video, so as to play to the strengths of the autoregressive model and avoid its weaknesses. Thus we use LLM to generate coarse-grained video in the first stage; 2 2) at the high-bit position perceptual feature, since we have already obtained the coarse-grained with rich semantic and time-serial information, we only need to focus on how to add details to the coarse-grained video. Thus we apply diffusion model in the second stage. Finally, VAE decoder transforms the generated perceptual feature into the final RGB video output. By unifying these complementary mechanisms, we demonstrate that hybrid architectures can overcome the inherent limitations of isolated approaches, enabling coherent, semantically faithful, and visually compelling video generation from textual descriptions, as shown in Table 2. With this design, the ideal semantic feature should contain high-level semantic information and motion information and only require few bits to represent. We achieve this goal by performing extreme compression on video representations rich in high-level semantics. For video representation, we select the Theia model (Shang et al., 2024) as our visual representation backbone, which has been distilled from multiple visual understanding and self-supervised representation models, ensuring the encoded features contain rich semantic information. To achieve extreme compression and reduce the number of bits, we design an efficient tokenizer to compress 3D visual features into 1D discrete representations. The tokenizer is based on the Transformer (Vaswani et al., 2017) structure, uses query embedding to aggregate visual features, and has higher compression rate than CNN-based structures (Yu et al., 2024b). To further compress the video by fully utilizing the temporal redundancy of the video, inspired by the MP4 video encoding algorithm (Le Gall, 1991), we divide the video into keyframes and non-keyframes, and set more numbers of tokens for keyframes. The detailed design is shown in subsection 2.1. For the diffusion model, we use generated semantic tokens as conditions and generate the target video by gradually removing the noises. To better support the long-video generation, we train chunk-wise streaming diffusion model that only uses limited number of historical frames as conditions, thereby greatly reducing the computational cost of training and inference. Thanks to these designs, our LanDiff has made significant progress in spatial relationship compliance, action coherence, visual quality, etc. Specifically, 1) for short video generation, our 5B model achieved score of 85.43 on the VBench T2V benchmark, surpassing the state-of-theart open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo; 2) for long video generation, after testing by the VBench T2V benchmark, our model also achieved state-of-theart performance, surpassing other open-source models in this field. Our video examples can be viewed at https: //landiff.github.io/. The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 2. The architecture of LanDiff. Given text inputs, we first extract text embeddings and employ an LLM to generate semantic tokens in the first stage. Subsequently, we utilize diffusion model to synthesize perceptual features conditioned on these semantic tokens, followed by VAE decoder that transforms these features into the final video frames. 2. Method In this work, we propose novel text-to-video generation framework that synergistically integrates the strengths of autoregressive modeling and diffusion processes while circumventing their respective limitations. The framework mainly consists of the following components: 1) an efficient tokenizer that transforms 3D visual features into compact 1D discrete representations while preserving and enhancing their semantic information; 2) language model that performs temporal sequence modeling to generate semantic tokens representing video blueprints from textual descriptions; 3) streaming diffusion model that progressively refines coarse semantic videos by adding fine-grained details to produce high-quality VAE features; and 4) VAE decoder that reconstructs the final video frames from the refined VAE features. 2.1. Video Semantic Tokenizer In this section, we introduce novel video semantic tokenizer that efficiently compresses the video into semantic information. Firstly, we will introduce the video semantic representation used in tokenization. Then, considering the high redundancy of video in both spatial and temporal dimensions, we introduce two compression strategies: 1) querybased causal tokenization that efficiently reduces spatial redundancy while preserving essential semantic information through vector quantization; 2) inspired by MP4 (Le Gall, 1991), we implement video frame grouping to minimize temporal redundancy by treating grouped frames as unit, where the first frame (I-frame) is fully encoded while subsequent frames (P-frames) only capture the temporal changes by referencing content from previous frames. Video Semantic Representation. Generally, video representations can be divided into two categories: 1) some methods (Wang et al., 2024a; Yu et al., 2024a) directly utilize an autoencoder to learn the video representations. 2) Some works (Koh et al., 2023; Jin et al., 2023; Sun et al., 2024; Jin et al., 2024) use pre-trained visual self-supervised learning features (SSL) as video representations. Compared with the first directly learned autoencoder latents which contain lots of visual details, the second SSL features maintain much more semantic features, which enable the LLM to focus more on the high-level semantic information of the video. Based on these thoughts, we choose the pretrained SSL features as the video representations. For better usage of different SSL models, we choose Theia model (Shang et al., 2024), which is unified visual feature extractor distilled from multiple visual task models, including CLIP (Radford et al., 2021), (Kirillov et al., 2023), DINOv2 (Oquab et al., 2024), ViT (Dosovitskiy et al., 2021), and Depth-anything (Yang et al., 2024a). Tokenizer Design. In this part, we introduce the design of our tokenizer, which leverages the query tokens to compress the video semantic features and uses quantization to discrete the video semantic representation while minimizing the reconstruction loss. In detail, firstly, we extract the semantic features using the Theia model and flatten it to obtain R(T HW )D. Inspired by TiTok (Yu et al., 2024b), we use randomly initialized tokens as query tokens RN and concatenate 3 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 3. Proposed architecture of the video semantic tokenizer. We use query tokens to compress the semantic sequence length. Furthermore, we group the frames into groups (3 frames in group in this figure). In group, the first frame is the IFrame and the rest frames are PFrames. We use different query token numbers for them. The attention mask design is shown in the right. them with the semantic features . Then we use transformer encoder to encode them and only take the encoded features of the query tokens. ZQ = Enc([F ; Q]), (1) where [; ] represents the concatenation operation and ZQ RN D. We then apply vector quantization on ZQ by train VQ-VAE model and obtain the quantized feature ˆZQ. In the decoding stage, the quantized feature ˆZQ is used as condition, and then sequence of mask tokens RT HW are added in front of the ˆZQ to form the inputs of decoder. Then we only take the features of the mask tokens as follows: ˆF = Dec([M ; ˆZQ]), (2) where ˆF R(T HW )D represents the reconstructed feature, and represents the mask tokens. Inspired by Wang et al. (2024c); Huang et al. (2023), we minimize the reconstruction loss of the video semantic feature during training the VQ-VAE. For the VQ-VAE, we follow the method of Yu et al. (2022); Wang et al. (2024c). We update the codebook using exponential moving average (EMA). We also optimize the codebook with the video semantic feature reconstruction loss. The loss function is shown as follows: = λrec ˆF 2 + λcommitsg( ˆZQ) ZQ2, where sg() is the stop-gradient operation, 2 is the L2 loss. (3) Video Frame Grouping. For video, straightforward observation is the redundancy in time series (i.e., the difference between adjacent video frames is minimal). Intuitively, we can achieve better compression rate by modeling the difference between adjacent video frames instead of tokenizing all frames equally. Inspired by the popular video compression method MP4 (Le Gall, 1991), given frames of video, we first split them into N/T groups (i.e., frames as group. For clarification, as shown in Figure 3, we use = 3 as an example.). Then: 1) we will model different groups independently; 2) for each group, we will fully encode the first frame (Intra-coded Frame, IFrame), while for the remaining [1, 1] frames (the Predictive-coded Frame, PFrame), we encode them by referencing their previous frames. To achieve this, within group, the first frame (IFrame) will only see itself and have large number of query tokens (e.g., 3 query tokens in Figure 3) to achieve better reconstruction quality. For the rest frame (PFrame) [1, 1], it will see the previous frames (i.e., frames [0, 1]) and have small number of query tokens (e.g., 1 query token which is 1/3 of the IFrame) to force the model to learn the difference. Technically, as shown in the attention mask on the right side of Figure 3, we use frame-level causal masks for the feature sequence during encoding. The query tokens are also divided according to the frame, and each token can only attend to the features of the corresponding frame and the previous frames. During decoding, the mask token corresponding to each frame can see the previous 4 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation features, the corresponding query tokens, and the previous query tokens. 2.2. Language Model for Semantic Token Generation As shown in Figure 2, after training an efficient tokenizer, we use language model to generate semantic tokens autoregressively based on text. Specifically, we first use pre-trained text encoder T5-XXL (Raffel et al., 2020) to extract text features X. We use the video tokenizer encoder trained in subsection 2.1 to convert the video into 1D discrete token sequence . To enhance the controllability of the generated results, we introduce additional control conditions CC. It includes: 1) frames condition for requiring the model to generate videos with specified number of frames; 2) motion score condition, which is value between 0 and 1, used to control the degree of motion in the generated video. The discrete tokens are converted into embedding vectors during generation and added with positional encoding, and then concatenated with the control conditions as input. The structure of the language model follows the typical LLaMA model (Touvron et al., 2023) network structure. We train the model from scratch, using cross-entropy loss as the loss function. LLM = E[ log p(YiX, CC, Y<i)] (4) 2.3. Diffusion Model for Perceptual Feature Generation The video detokenizer in LanDiff is responsible for converting semantic tokens into VAE latent vectors. As shown in Figure 2 we use conditional diffusion model to complete this task. Especially to support long-video generation, we also design the streaming inference strategy. In this section, we will first introduce the architecture for single semantic token chunk. Then we will introduce the chunk-wise streaming strategy. Architecture. Our diffusion model employs an architecture similar to MMDiT(Esser et al., 2024; Yang et al., 2024b). Specifically, 1) we use the video tokenizer decoder trained in subsection 2.1 to decode the semantic tokens into semantic features ˆF . Then we use the semantic features ˆF as condition to guide the diffusion model to generate videos; 2) we inject the control signals into the model in similar way to ControlNet (Zhang et al., 2023b), as shown in Figure 4. In detail, during training, the parameters of the main model are not updated, the control module copies the parameters of the first half layers of the main model, and adds them to the output of the main model after linear layer initialized with zeros; 3) to make the semantic features match the target VAE features in the space dimension, we additionally add an upsampling module. During training, given the input video chunk Figure 4. Proposed diffusion model structure. We use ControlNet-style control module to guide the model to generate perceptual feature based on semantic features. R(T HW )D, where is the frame length of the VAE latent and is the VAE latent feature dimension, the diffusion algorithm progressively adds noise to the video and produces noisy video t, where represents the number of times noise is added. The diffusion algorithm uses time step t, and semantic features ˆF R(T HW )D as conditions, and then uses network ϵθ to predict the noise added to the noisy video Vt through: = EV,t,cs,ϵN (0,1) (cid:104) ϵ ϵθ(V t, t, ˆF ))2 2 (cid:105) (5) Chunk-wise Streaming Strategy. To support long video generation scenario whose semantic token sequence is very long and difficult to generate as whole, we propose the chunk-wise streaming strategy. During training, given video latent chunk R(T HW )D, to maintain the appearance continuity in the video, we use the first half of (Vl) as the prompt and generate the second half of Vr. In detail, we do not add noise to the first half chunk and give time condition to 0.999. We also randomly mask the first half chunk with ratio of 20%. The loss function for training at this time is: = EVr,t,cs,ϵN (0,1) (cid:104) ϵ ϵθ(Vl, , t, cs))2 2 (cid:105) . (6) During inference, the first L/2 VAE latents will be generated without the prompt. For the following VAE latents, we will accumulate L/2 tokens to form chunk and use the previous chunk as prompt. 5 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Table 3. Performance comparison of Text-to-video (T2V) generation between our LanDiff and other state-of-the-art models on VBench benchmark. We selected 8 out of the 16 evaluation dimensions from VBench, along with Total Score, Quality Score, and Semantic Score, for presentation. The complete results with all 16 evaluation dimensions can be found in the appendix Table 8. The best and second-best scores are highlighted in bold and underline, respectively. indicates the scores we reproduced, while indicates the scores from the original papers, and other scores are from the VBench benchmark. Model Name Type Model Size Total Score Quality Score Semantic Score background consistency dynamic degree motion smoothness multiple objects object class scene spatial relationship subject consistency Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion LLM Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion InstructVideo Latte-1 OpenSoraPlan V1.1 Show-1 OpenSora V1.2 LTX-Video Mochi-1 AnimateDiff-V2 VideoCrafter-2.0 CogVideoX-2B Emu3 Vchitect-2.0-2B CogVideoX-5B DiT RepVideo Vchitect-2.0[E] HunyuanVideo Pika-1.0 Kling Jimeng Gen-3 Hailuo Sora ARLON ARLON LanDiff Open Sourced Models 1.3B 76.61 81.56 0.7B 77.29 79.72 2.7B 78.00 80.91 6.3B 78.93 80.42 1.1B 79.76 81.35 1.9B 80.00 82.30 10B 80.13 82.64 1.3B 80.27 82.90 1.7B 80.44 82.20 2B 80.91 82.18 8B 80.96 N/A 2B 81.57 82.51 5B 81.61 82.75 7B 81.85 82.70 2B 81.94 82.70 2B 82.24 83.54 13B 83.24 85.09 56.81 67.58 66.38 72.98 73.39 70.79 70.08 69.75 73.42 75.83 N/A 77.79 77.04 78.42 78.91 77.06 75.82 96.97 95.40 96.73 98.02 97.61 97.20 97.28 97.68 98.22 96.63 97.69 96.53 96.52 97.65 96.56 96.66 97.76 Close Sourced Models N/A 80.69 82.92 N/A 81.85 83.39 N/A 81.97 83.29 N/A 82.32 84.11 N/A 83.41 84.85 N/A 84.28 85.51 N/A 5B 82.31 83.58 LLM+Diffusion 1.5B N/A LLM+Diffusion LLM+Diffusion 5B 85.43 86.13 71.77 75.68 76.69 75.17 77.65 79.35 N/A 77.27 82.61 97.36 97.60 98.39 96.62 97.05 96.35 97.10 98.65 98. 69.72 68.89 47.72 44.44 42.39 54.35 61.85 40.83 42.50 59.86 79.27 58.33 70.97 51.02 57.78 63.89 70.83 47.50 46.94 38.43 60.14 64.91 79.91 52.77 72.22 92.71 96.62 94.63 98.28 98.24 98.50 98.96 99.02 97.76 97.73 97.73 98.93 97.76 96.92 98.94 98.13 98.98 98.99 99.50 99.40 98.09 99.23 99.22 98.74 98.92 97.56 97. 21.57 34.53 40.35 45.47 51.83 45.43 50.47 36.88 40.66 62.63 44.64 69.35 62.11 69.63 71.18 68.84 68.55 43.08 68.05 69.08 53.64 76.04 70.85 N/A 74.49 73.26 22.21 86.53 36.26 76.30 27.17 93.07 47.03 82.22 42.44 83.45 51.07 86.51 36.99 90.90 50.19 92.55 55.29 83.37 51.14 86.17 37.11 87.81 57.51 85.23 53.20 93.40 57.10 87.83 52.96 86.61 56.57 86.10 53.88 88.72 49.83 87.24 50.86 89.62 44.94 87.81 54.57 87.83 50.68 93.93 56.95 89.80 54.43 90.60 52.07 86.69 94.94 53. 43.49 41.53 53.11 53.50 68.56 65.43 69.24 34.60 35.86 69.90 68.73 54.64 66.35 60.89 74.74 57.55 68.68 61.03 73.03 77.45 65.09 75.50 74.29 N/A 62.53 73.74 95.30 88.88 95.73 95.53 96.75 96.56 96.99 95.30 96.85 96.78 95.32 96.42 96.23 97.05 96.25 96.83 97.37 96.94 98.33 97.25 97.10 97.51 96.23 93.41 95.59 96. 3. Experiments and Results 3.1. Experimental Settings Datasets. For the video tokenizer and language model, we use an internal dataset with 200M video-text pairs for training. All videos with duration of less than 6s are filtered, and the videos are kept in the aspect ratio and then scaled to around 480x720 resolution for center cropping. Consistent with CogVideoX (Yang et al., 2024b), we set the fps of all videos to 8. For the diffusion model, we select dataset containing 3M high-quality video-text pairs for training. To evaluate the performance of the text-tovideo generation model, we use the prompts provided by the widely used VBench (Huang et al., 2024) T2V benchmark to generate videos. Implementation Details. We use interpolation of positional encoding to enable the encoder of the Theia (Shang et al., 2024) model to handle 480x720 resolution videos. For the video tokenizer, we set the group size mentioned in subsection 2.1 to 13. We set the number of tokens corresponding to the IFrame to 330, and the number of tokens corresponding to the PFrame to 74. On average, for 480x720 resolution video of one second, our tokenizer generates about 200 tokens. In contrast, common tokenizers such as Magvit2 (Yu et al., 2024a) generate about 10,000 tokens per second for videos of this resolution, and our sequence length is about 1/50 of Magvit2. The dimension of the tokenizers codebook is set to 16, and the vocabulary size is set to 2048. For the language model, we use the LLaMA (Touvron et al., 2023) structure, with 2B model parameters, and use 1D rope (Su et al., 2024) positional encoding. We set the text, motion score, and frames conditions in subsection 2.2 to null with probabilities of 10%, 50%, and 50%, respectively. We apply classifier-free guidance (Ho & Salimans, 2022) for better generation quality, and the guidance scale is set to 6.5. We do not use top-k and top-p sampling. We use the 2B version of the CogVideoX1 (Yang et al., 2024b) model as the base model for our video detokenizer. We copy the first 15 layers of the base model as the proposed trainable control module in subsection 2.3. We use structure similar to the VQ-GAN (Esser et al., 2021) decoder as the upsampling module and change the upsampling method to pixelshuffle (Shi et al., 2016). The total number of parameters of the video detokenizer is 3B, and the number of parameters of the trainable control module is 1B. During inference, we follow the same sampling strategy as Yang et al. (2024b). We list the structural settings and training details of each module in the appendix. 1https://huggingface.co/THUDM/ CogVideoX-2b 6 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Table 4. Long video generation results of LanDiff and other models on VBench. The best and second-best scores are highlighted in bold and underline, respectively. Models FreeNoise StreamingT2V OpenSora-V1.2 ARLON LanDiff Total Score 62.60 62.92 64.24 65.09 68.34 Subject Consist Background Consist Motion Smooth Dynamic Degree Aesthetic Quality Imaging Quality Overall Consist 96.59 87.31 96.30 97. 95.41 97.48 94.64 97.39 97.56 97.88 98.36 93.83 98.94 98.50 97.38 17.44 85.64 44.79 50. 72.86 47.39 44.57 56.68 56.85 60.96 63.88 53.64 51.64 53.85 63.00 25.78 23.65 26.36 26. 27.29 Figure 5. Qualitative comparison of long video generation results between LanDiff and other state-of-the-art models (FreeNoise, StreamingT2V and OpenSora-V1.2). Evaluation Metrics. To evaluate the text-to-video generation task, we use the metrics proposed in the VBench and VBench-Long (Huang et al., 2024) benchmarks. Evaluation Baselines. We compare LanDiff with baselines: 1) Sora (Sor, 2024). 2) Jimeng (Jim, 2024). 3) Hailuo (Hai, 2024). 4) OpenSoraPlan V1.1 (Lin et al., 2024a). 5) Kling (Luo et al., 2023). 6) InstructVideo (Yuan et al., 2024). 7) Gen-3 (Gen, 2024). 9) Latte-1 (Ma et al., 2024) 9) HiGen (Qing et al., 2024). 10) AnimateDiff-V2 (Guo et al., 2024). 11) Show-1 (Zhang et al., 2023a). 12) Pika (Pik, 2024). 13) VideoCrafter-2.0 (Chen et al., 2024). 14) OpenSora V1.2 (Zheng et al., 2024). 15) LTX-Video (HaCohen et al., 2024). 16) Mochi-1 (Team, 2024). 17) CogVideoX (Yang et al., 2024b). 18) Vchitect-2.0 (Fan et al., 2025). 19) RepVideo (Si et al., 2025). 20) HunyuanVideo (Kong et al., 2024). 21) ARLON (Li et al., 2024). 22) Emu3 (Wang et al., 2024a). 3.2. Experimental Results Text to Video Generation. As shown in Table 3, we conducted quantitative comparison between LanDiff and other 7 Figure 6. Comparison of qualitative results for text-to-video generation. The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation state-of-the-art open-source models on the VBench benchmark. In the table, we selected 9 dimensions related to temporal modeling and semantics for presentation, and we show the scores of all 16 dimensions in the appendix. Our 5B LanDiff model outperforms the state-of-the-art opensource 13B Hunyuan Video model and achieves score of 85.42. Our model achieves the highest semantic score and the second-highest quality score, indicating that our model can follow the text well and generate high-quality videos. Compared with the same-sized CogVideoX-5B, our model achieves better results in almost all dimensions. To eliminate the interference of the training data volume, we train 7B Dit-based diffusion model with the same training data recipe as LanDiff. We mark it as Dit in the table. This indicates that with the same data volume, by reasonably dividing the task load, we can combine the advantages of autoregressive models and diffusion models to achieve better performance than diffusion models alone. Figure 6 shows the qualitative comparison between the videos generated by LanDiff and CogVideoX-5B. In the first example, it can be seen that in the video generated by CogVideoX-5B, one fish disappears after the fish meet, while in the video generated by our model, the fish can still remain intact after meeting. This demonstrates that our model can understand the concept of fish as an entity and maintain consistency over time. Additionally, our model more faithfully adheres to the background elements described in the prompt with rocks and shadows in the background, rendering these environmental details with greater accuracy and consistency. In the second example, our model accurately captures the temporal dynamics described in the prompt, successfully rendering melting ice sculpture of dog. LanDiff properly depicts the progressive melting process, with the ice dog gradually losing its form and eventually transforming into puddle of water. In contrast, CogVideoX-5B generates static representation of the ice dog sculpture that remains largely unchanged throughout the sequence, failing to capture the crucial temporal narrative of melting described in the prompt. This highlights our models superior capability in understanding and visualizing complex temporal transformations and physical processes. Long Video Generation. We compare our model with other open-source text-to-long video generation models: 1) FreeNoise (Qiu et al., 2024). 2) StreamingT2V (Henschel et al., 2024). 3) OpenSora-V1.2 (Zheng et al., 2024). 4) ARLON (Li et al., 2024). As shown in Table 4, our LanDiff achieves state-of-the-art performance in long video generation tasks with the highest Total Score of 68.34, outperforming other open-source models across almost all dimensions. While StreamingT2V exhibits the highest Dynamic Degree (85.64), this comes at clear cost to consistency metrics, as evidenced by its significantly lower Subject Consistency (87.31) and Background Consistency (94.64). An optimal video generation model should maximize dynamic content while maintaining temporal coherence. Compared with models that excel in consistency metrics but show limited dynamism (such as ARLON with 50.42 Dynamic Degree), our approach demonstrates superior dynamism (72.86) while preserving strong consistency scores and achieving the best results in Aesthetic Quality (60.96) and Overall Consistency (27.29). As illustrated in Figure 5, LanDiff effectively generates complex motion dynamics when prompted with car accelerating to gain speed, successfully depicting the progressive increase in vehicle velocity while preserving visual consistency across frames. In contrast, FreeNoise, StreamingT2V, and OpenSora-V1.2 all struggle to properly render the acceleration motion, exhibiting either static vehicles, inconsistent car appearances, or unrealistic motion patterns that fail to convey the sense of increasing speed. Figure 7. Visualization results of video reconstruction using video tokenizer. Figure 8. Training loss comparison of the video tokenizer. The plot illustrates the reconstruction loss trajectories for IFrame and PFrame components over training iterations. Despite the different token allocation strategies (330 tokens for IFrame vs. 74 tokens for PFrame), both frame types achieve comparable reconstruction quality. Video Tokenization. We present visualization results of our models video reconstruction in Figure 7. We extract semantic tokens from the reference video and subsequently convert these tokens back into video using the video detokenizer. 8 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation The results demonstrate that through our careful design, our video tokenizer can accurately reconstruct videos with accurate semantics and actions using only approximately 1/50th of the sequence length required by Magvit2 (Yu et al., 2024a). While some minor discrepancies in clothing details exist between the reconstructed and reference videos, these differences remain within acceptable limits for practical video generation applications. Figure 8 shows the reconstruction loss trajectories for IFrame and PFrame components during training. Despite allocating significantly different token quantities (330 vs. 74), both frame types converge to comparable reconstruction quality. This validates our video frame grouping strategy that prioritizes key frames while minimizing tokens for intermediate frames. The results confirm our approach successfully balances reconstruction fidelity with computational efficiency. Table 5. The ablation study of video tokenizer and classifier free guidance (cfg) on Vbench benchmark. Models LanDiff -video tokenizer -cfg Total Score 85. 82.31 81.06 Quality Score Semantic Score 86.13 83.58 83.04 82. 77.27 73.13 3.3. Ablation Study In this section, we conducted ablation experiments on the video tokenizer and classifier-free guidance. To fairly evaluate the effectiveness of our video tokenizer, we implemented an ARLON-like method as comparison baseline. Specifically, for the control group without our proposed video tokenizer, we followed ARLONs architectural approach by training large language model to predict quantized VAE features, then using CogVideoX-5B as the base diffusion model conditioned on these LLM-predicted VAE features to generate videos. This baseline model was trained with exactly the same dataset and training recipe as LanDiff, ensuring strict experimental control variables and effectively eliminating interference factors such as model parameter count and training data scale. As shown in Table 5, the experimental results clearly demonstrate that our proposed video tokenizer significantly improves both the quality score and semantic score of generated videos, validating the importance of this component in our framework. In the case of not using classifier-free guidance for text, our model has decreased in quality and semantic scores. 9 4. Related Work 4.1. Video Tokenization Video tokenization plays crucial role in video understanding and generation tasks. Since video can be represented as sequence of continuous frames, some works directly use image tokenizers to process videos frame by frame. For example, Wang et al. (2024a) directly use SBER-MoVQGAN as the video tokenizer. However, this method ignores the temporal redundancy in videos, resulting in low compression rate. To reduce temporal redundancy, some works (Yan et al., 2021; Yu et al., 2024a) try to extend image tokenizers based on 2D convolution to 3D convolution, which can process both temporal and spatial information simultaneously. These methods encode videos in the original RGB space and perform video reconstruction tasks, which are more about perceptual compression. In addition, some works (Ge et al., 2023; Jin et al., 2023) try to train video tokenizers on features extracted from pre-trained visual encoders. These tokenizers can achieve good performance in understanding and generation tasks while maintaining high compression rate. The video tokenizer we propose belongs to this category. Unlike previous feature-based tokenizers that achieved limited compression rates and primarily focused on image processing, our video tokenizer delivers significantly higher compression while handling both images and videos in unified framework. 4.2. LLM based Video Generation LLM-based video generation methods are usually based on the Transformer (Vaswani et al., 2017) structure, learning the mapping from text to video through next token prediction. TATS (Ge et al., 2022) uses VQ-GAN as the tokenizer and predicts video tokens using GPT-like model structure. VideoGPT (Yan et al., 2021) uses 3D convolution to extract features and quantize them, and predicts the quantized video discrete tokens using GPT-like model. Recently, VideoPoet (Yu et al., 2024a) uses MagViT2 (Yu et al., 2024a) as the tokenizer and unifies multiple modalities as input to large language model (LLM) to conditionally generate video tokens. In addition, Emu3 (Wang et al., 2024a) uses SBER-MoVQGAN as the tokenizer to perform video understanding and generation by predicting the next token. These works all use LLMs to directly generate perceptual features that contain rich visual details with high bit rates. Recently, ARLON (Li et al., 2024) attempts to discretize VAE features into small number of tokens to reduce the bit rate required for LLMs prediction. In this way, the tokens retain low-frequency visual information such as blurry contours rather than high-level semantic information. In contrast, our method employs tokens containing high-level semantic information as prediction targets for LLMs, which enables us to fully leverage LLMs advantages in causal The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation modeling to precisely generate high-quality videos. 4.3. Diffusion based Video Generation Diffusion-based methods have achieved great success in image generation, and recently many people (Chen et al., 2024; Ho et al., 2022a;b; Singer et al., 2023; Zhou et al., 2023) have tried to apply them to video generation tasks. VDM (Ho et al., 2022b) extends the 3D U-Net structure for video generation. Wang et al. (2024b) propose to generate highquality and aesthetically pleasing videos in cascaded manner. Benefiting from the success of the text-to-image (T2I) field, some works such as Animatediff (Guo et al., 2024), SVD (Blattmann et al., 2023), and PixelDance (Zeng et al., 2023) try to use pre-trained T2I models as initialization, and then add modules for temporal modeling to capture motion information for video generation. Ma et al. (2024) explore the generation capabilities of multiple different structures of latent diffusion transformer. After the release of SORA, series of video generation methods based on the DiT (Peebles & Xie, 2023) model have been proposed, including OpenSora (Zheng et al., 2024), OpenSoraPlan (Lin et al., 2024a), Cogvideox (Yang et al., 2024b), Hunyuan Video (Kong et al., 2024), Mira (Ju et al., 2024) and STIV (Lin et al., 2024b) etc. These methods can only generate short videos of few seconds. Recently, StreamingT2V (Henschel et al., 2024) generates long videos by block-wise generation on pre-trained short video generation model, and then uniformly performs mixed augmentation. In addition, some works improve the consistency of long video generation by leveraging noise rescheduling techniques (Qiu et al., 2024; Lu et al., 2024). Our method employs diffusion models as renderers for semantic features, enabling us to leverage their superior visual generation quality while circumventing their limitations in causal modeling. 5. Conclusion In this paper, we propose new text-to-video generation model, LanDiff. It combines the advantages of autoregressive models and diffusion models, including: 1) an efficient 1D video tokenizer to extract videos into semantic tokens; 2) language model to generate semantic tokens based on text; 3) video detokenizer to convert semantic tokens into videos. LanDiff outperforms the state-of-the-art opensource models on the VBench benchmark, surpassing other open-source models in quality and semantic scores. At the same time, LanDiff also achieves state-of-the-art performance in long video generation tasks. 10 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation"
        },
        {
            "title": "References",
            "content": "Gen-3. https://runwayml.com/research/ introducing-gen-3-alpha, 2024. Hailuo. 2024. https://platform.minimaxi.com/, Jimeng. https://jimeng.jianying.com/, 2024. Pika 1.0. https://www.pika.art/, 2024. Openai sora. https://openai.com/index/sora, 2024. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., Jampani, V., and Rombach, R. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets, November 2023. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., and Shan, Y. VideoCrafter2: Overcoming data limitations for high-quality video diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 73107320. IEEE, 2024. doi: 10.1109/CVPR52733.2024. 00698. Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High Fidelity Neural Audio Compression, October 2022. Dong, J., Feng, B., Guessous, D., Liang, Y., and He, H. Flex Attention: Programming Model for Generating Optimized Attention Kernels, December 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image In 9th International Conference recognition at scale. on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. Esser, P., Rombach, R., and Ommer, B. Taming Transformers for High-Resolution Image Synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1287312883, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https:// openreview.net/forum?id=FPnUhsQJ5B. Fan, W., Si, C., Song, J., Yang, Z., He, Y., Zhuo, L., Huang, Z., Dong, Z., He, J., Pan, D., Wang, Y., Jiang, Y., Wang, Y., Gao, P., Chen, X., Li, H., Lin, D., Qiao, Y., and Liu, Z. Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models, January 2025. Ge, S., Hayes, T., Yang, H., Yin, X., Pang, G., Jacobs, D., Huang, J.-B., and Parikh, D. Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer, September 2022. Ge, Y., Ge, Y., Zeng, Z., Wang, X., and Shan, Y. Planting SEED of Vision in Large Language Model, August 2023. Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., and Dai, B. AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., Panet, P., Weissbuch, S., Kulikov, V., Bitterman, Y., Melumian, Z., and Bibi, O. LTX-Video: Realtime Video Latent Diffusion, December 2024. Henschel, R., Khachatryan, L., Hayrapetyan, D., Poghosyan, H., Tadevosyan, V., Wang, Z., Navasardyan, S., and Shi, H. StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text, March 2024. Ho, J. and Salimans, T. Classifier-Free Diffusion Guidance, July 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, URL https://proceedings. virtual, 2020. neurips.cc/paper/2020/hash/ 4c5bcfec8584af0d967f1ab10179ca4b-Abstract. html. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A. A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., and Salimans, T. Imagen video: High definition video generation with diffusion models. CoRR, abs/2210.02303, 2022a. doi: 10.48550/ARXIV.2210.02303. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video Diffusion Models, June 2022b. Huang, Z., Meng, C., and Ko, T. Repcodec: speech representation codec for speech tokenization. arXiv preprint arXiv:2309.00169, 2023. 11 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., Wang, Y., Chen, X., Wang, L., Lin, D., Qiao, Y., and Liu, Z. VBench: Comprehensive benchmark suite for video generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 2180721818. IEEE, 2024. doi: 10.1109/CVPR52733.2024.02060. Jin, Y., Xu, K., Xu, K., Chen, L., Liao, C., Tan, J., Huang, Q., Chen, B., Lei, C., Liu, A., Song, C., Lei, X., Zhang, D., Ou, W., Gai, K., and Mu, Y. Unified LanguageVision Pretraining in LLM with Dynamic Discrete Visual Tokenization, September 2023. Jin, Y., Sun, Z., Xu, K., Xu, K., Chen, L., Jiang, H., Huang, Q., Song, C., Liu, Y., Zhang, D., Song, Y., Gai, K., and Mu, Y. Video-LaVIT: Unified Video-Language Pretraining with Decoupled Visual-Motional Tokenization, February 2024. Ju, X., Gao, Y., Zhang, Z., Yuan, Z., Wang, X., Zeng, A., Xiong, Y., Xu, Q., and Shan, Y. MiraData: LargeScale Video Dataset with Long Durations and Structured Captions, July 2024. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollar, P., and Girshick, R. Segment Anything, April 2023. Koh, J. Y., Fried, D., and Salakhutdinov, R. Generating images with multimodal language models. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Schindler, G., Hornung, R., Birodkar, V., Yan, J., Chiu, M.-C., Somandepalli, K., Akbari, H., Alon, Y., Cheng, Y., Dillon, J. V., Gupta, A., Hahn, M., Hauth, A., Hendon, D., Martinez, A., Minnen, D., Sirotenko, M., Sohn, K., Yang, X., Adam, H., Yang, M.-H., Essa, I., Wang, H., Ross, D. A., Seybold, B., and Jiang, L. VideoPoet: large language model for zero-shot video generation. In Forty-First International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Yuan, J., Long, Y., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. HunyuanVideo: Systematic Framework For Large Video Generative Models, December 2024. Le Gall, D. MPEG: video compression standard for multimedia applications. Communications of the ACM, 34(4):4658, 1991. Li, Z., Hu, S., Liu, S., Zhou, L., Choi, J., Meng, L., Guo, X., Li, J., Ling, H., and Wei, F. ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation, October 2024. Lin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S., He, X., Ye, Y., Yuan, S., Chen, L., Jia, T., Zhang, J., Tang, Z., Pang, Y., She, B., Yan, C., Hu, Z., Dong, X., Chen, L., Pan, Z., Zhou, X., Dong, S., Tian, Y., and Yuan, L. Open-Sora Plan: Open-Source Large Video Generation Model, November 2024a. Lin, Z., Liu, W., Chen, C., Lu, J., Hu, W., Fu, T.-J., Allardice, J., Lai, Z., Song, L., Zhang, B., Chen, C., Fei, Y., Jiang, Y., Li, L., Sun, Y., Chang, K.-W., and Yang, Y. STIV: Scalable Text and Image Conditioned Video Generation, December 2024b. Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Lu, Y., Liang, Y., Zhu, L., and Yang, Y. FreeLong: TrainingFree Long Video Generation with SpectralBlend Temporal Attention, July 2024. Luo, Z., Chen, D., Zhang, Y., Huang, Y., Wang, L., Shen, Y., Zhao, D., Zhou, J., and Tan, T. Videofusion: Decomposed diffusion models for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.-F., Chen, C., and Qiao, Y. Latte: Latent Diffusion Transformer for Video Generation, January 2024. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. 2024, 2024. Peebles, W. and Xie, S. Scalable Diffusion Models with Transformers, March 2023. 12 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Qing, Z., Zhang, S., Wang, J., Wang, X., Wei, Y., Zhang, Y., Gao, C., and Sang, N. Hierarchical spatio-temporal decoupling for text-to-video generation. In CVPR, 2024. Qiu, H., Xia, M., Zhang, Y., He, Y., Wang, X., Shan, Y., and Liu, Z. FreeNoise: Tuning-free longer video diffusion via noise rescheduling. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, pp. 87488763. PMLR, July 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21: 140:1140:67, 2020. Shang, J., Schmeckpeper, K., May, B. B., Minniti, M. V., Kelestemur, T., Watkins, D., and Herlant, L. Theia: Distilling Diverse Vision Foundation Models for Robot Learning, October 2024. Shi, W., Caballero, J., Huszar, F., Totz, J., Aitken, A. P., Bishop, R., Rueckert, D., and Wang, Z. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 18741883. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.207. Si, C., Fan, W., Lv, Z., Huang, Z., Qiao, Y., and Liu, Z. RepVideo: Rethinking Cross-Layer Representation for Video Generation, January 2025. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Su, J., Ahmed, M. H. M., Lu, Y., Pan, S., Bo, W., and Liu, Y. RoFormer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Wang, Y., Rao, Y., Liu, J., Huang, T., and Wang, X. Generative multimodal models are in-context learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1439814409. IEEE, 2024. doi: 10.1109/CVPR52733. 2024.01365. Team, G. Mochi 1. https://github.com/ genmoai/models, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. LLaMA: Open and Efficient Foundation Language Models, February 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention In Advances in Neural Information is All you Need. Processing Systems, volume 30. Curran Associates, Inc., 2017. Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., Zhao, Y., Ao, Y., Min, X., Li, T., Wu, B., Zhao, B., Zhang, B., Wang, L., Liu, G., He, Z., Yang, X., Liu, J., Lin, Y., Huang, T., and Wang, Z. Emu3: Next-Token Prediction is All You Need, September 2024a. Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: Highquality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pp. 120, 2024b. Wang, Y., Zhan, H., Liu, L., Zeng, R., Guo, H., Zheng, J., Zhang, Q., Zhang, X., Zhang, S., and Wu, Z. Maskgct: Zero-shot text-to-speech with masked generative codec transformer. arXiv preprint arXiv:2409.00750, 2024c. Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. VideoGPT: Video Generation using VQ-VAE and Transformers, September 2021. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., and Zhao, H. Depth anything: Unleashing the power of large-scale unlabeled data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1037110381. IEEE, 2024a. doi: 10.1109/CVPR52733.2024.00987. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., Yin, D., Gu, X., Zhang, Y., Wang, W., Cheng, Y., Liu, T., Xu, B., Dong, Y., and Tang, J. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer, August 2024b. Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation modeling with improved VQGAN. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., Gong, B., Yang, M.-H., Essa, I., Ross, D. A., and Jiang, L. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. Yu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. An Image is Worth 32 Tokens for Reconstruction and Generation, June 2024b. Yuan, H., Zhang, S., Wang, X., Wei, Y., Feng, T., Pan, Y., Zhang, Y., Liu, Z., Albanie, S., and Ni, D. InstructVideo: Instructing video diffusion models with human feedback. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 64636474. IEEE, 2024. doi: 10.1109/CVPR52733.2024.00618. Zeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y., and Li, H. Make Pixels Dance: High-Dynamic Video Generation, November 2023. Zhang, D. J., Wu, J. Z., Liu, J.-W., Zhao, R., Ran, L., Gu, Y., Gao, D., and Shou, M. Z. Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation, October 2023a. Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 38133824. IEEE, 2023b. doi: 10.1109/ICCV51070.2023.00355. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all, March 2024. URL https: //github.com/hpcaitech/Open-Sora. Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., and Feng, J. MagicVideo: Efficient Video Generation With Latent Diffusion Models, May 2023. 14 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation A. Implementation Details Table 6. The detailed model configurations of Video Tokenizer."
        },
        {
            "title": "Value",
            "content": "#Parameters"
        },
        {
            "title": "Transformer Layer\nHidden Size\nAttention Heads\nMask Tokens",
            "content": "12 768 12 330 74 12 768"
        },
        {
            "title": "Codebook size\nCodebook Dimension\nSimilarity Metric",
            "content": "2048 16 Cosine Total 85M 85M 25.4K 170M A.1. Details of Video Tokenizer The video tokenizer model follows similar structure to TiTok (Yu et al., 2024b). However, to flexibly encode videos with different numbers of frames, we replace absolute position encoding with 3D rope position encoding (Su et al., 2024). The model configuration of the tokenizer and the parameters of each part are shown in Table 6. The model is Transformer structure, with 12 layers in both the encoder and decoder, hidden layer size of 768, and 12 heads. To improve the computational efficiency of the attention mechanism in the tokenizer, we employ flex attention(Dong et al., 2024). In addition, inspired by EnCodec (Defossez et al., 2022), to avoid discontinuities when encoding videos, we set 20% overlap between groups. During training, the batch size is 96, we use the AdamW (Loshchilov, 2017) optimizer, the learning rate is constant at 1e 4, and the learning rate decay factor is 0. During training, we use Model Exponential Moving Average (EMA) to smooth the model parameters, and the decay rate of EMA is 0.8. The weights of the reconstruction loss and the commitment loss are both 1. A.2. Details of LLM We use model structure similar to LLaMA (Touvron et al., 2023) as the LLM. The model has 24 layers, hidden layer size of 2048, 16 heads, and an MLP hidden layer size of 11008. The batch size for model training is 4096, we use the AdamW optimizer, the learning rate is 1e 3, the learning rate decay factor is 0.1, and we use warm-up strategy for the first 1000 steps of training. We use cosine learning rate decay strategy. A.3. Details of Diffusion Model The batch size for training is 128, we use the AdamW optimizer, the learning rate is 1e 4, and the learning rate decay factor is 1e 4. To speed up training, we first train directly on the original features extracted from Theia. Then we use the quantized reconstructed features for training. A.4. More Analysis on VBench Benchmark As shown in Table 8, we conduct comprehensive comparison with state-of-the-art text-to-video generation models on VBench benchmark. The benchmark evaluates models across multiple dimensions including quality, semantics, aesthetics, and temporal consistency. Our LanDiff achieves superior performance in most metrics, particularly excelling in overall quality (86.13) and semantic accuracy (82.61). Among open-sourced models, there is clear trend of performance improvement with model size, from Latte-1 (0.7B) to HunyuanVideo (13B). However, our hybrid LLM+Diffusion approach (5B) demonstrates that architectural innovation 15 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 9. Radar chart visualization of performance comparison across different dimensions on VBench. The plot compares LanDiff against five competitive baselines: Sora, Hailuo, HunyuanVideo, Kling, and CogVideoX-5B. 16 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Table 7. The detailed model configurations of LanDiff."
        },
        {
            "title": "Value",
            "content": "#Parameters"
        },
        {
            "title": "LLM",
            "content": "Diffusion Backbone Module (Frozen) Diffusion Control Module Transformer Layer Hidden Size Attention Heads MLP Dimension Activation RoPE θ Text Drop Rate Micro Conditioner Hidden Size"
        },
        {
            "title": "Transformer Layer\nAttention Heads\nHidden Size\nTime Embedding Size",
            "content": "Total 24 2048 16 11008 GELU 10000 0.1 512 30 8 1920 256 15 8 1920 256 2B 2B 1B 5B Table 8. Performance comparison of Text-to-video (T2V) generation between our LanDiff and other state-of-the-art models on VBench benchmark. The remaining 8 evaluation dimensions of VBench that are not shown in the main text. The best and second-best scores are highlighted in bold and underline, respectively. indicates the scores we reproduced, while indicates the scores from the original papers, and other scores are from the VBench benchmark. Model Name Type Model Size Total Score Quality Score Semantic Score aesthetic quality appearance style color human action imaging quality overall consistency temporal flickering temporal style Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion LLM Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion InstructVideo Latte-1 OpenSoraPlan V1.1 Show-1 OpenSora V1.2 LTX-Video Mochi-1 AnimateDiff-V2 VideoCrafter-2.0 CogVideoX-2B Emu3 Vchitect-2.0-2B CogVideoX-5B DiT RepVideo Vchitect-2.0[E] HunyuanVideo Pika-1.0 Kling Jimeng Gen-3 Hailuo Sora ARLON ARLON LanDiff Open Sourced Models 1.3B 76.61 81.56 0.7B 77.29 79.72 2.7B 78.00 80.91 6.3B 78.93 80.42 1.1B 79.76 81.35 1.9B 80.00 82.30 10B 80.13 82.64 1.3B 80.27 82.90 1.7B 80.44 82.20 2B 80.91 82.18 8B 80.96 N/A 2B 81.57 82.51 5B 81.61 82.75 7B 81.85 82.70 2B 81.94 82.70 2B 82.24 83.54 13B 83.24 85. 56.81 67.58 66.38 72.98 73.39 70.79 70.08 69.75 73.42 75.83 N/A 77.79 77.04 78.42 78.91 77.06 75.82 52.55 61.59 56.85 57.35 56.85 59.81 56.94 67.16 63.13 60.82 59.64 61.47 61.98 60.00 62.40 60.41 60.36 Close Sourced Models N/A 80.69 82.92 N/A 81.85 83.39 N/A 81.97 83.29 N/A 82.32 84.11 N/A 83.41 84.85 N/A 84.28 85.51 N/A 5B 82.31 83.58 LLM+Diffusion 1.5B N/A LLM+Diffusion LLM+Diffusion 5B 85.43 86.13 71.77 75.68 76.69 75.17 77.65 79.35 N/A 77.27 82.61 62.04 61.21 68.80 63.34 63.03 63.46 61.01 60.58 64.78 20.16 23.74 22.90 23.06 23.95 21.47 20.33 22.42 25.13 24.80 20.92 24.93 24.91 24.95 25.12 23.73 19. 22.26 19.62 22.27 24.31 20.06 24.76 N/A 25.26 25.60 77.14 85.20 85.31 90.00 89.19 86.80 86.35 95.60 90.08 91.20 81.45 92.80 79.73 94.60 87.47 92.60 92.92 95.00 79.41 98.00 N/A 77.71 86.87 97.00 82.81 99.40 78.62 98.20 82.51 98.00 87.04 97.20 91.60 94.40 90.57 86.20 89.90 93.40 89.05 90.10 80.90 96.40 90.36 92.40 80.11 98.20 N/A N/A 85.86 92.20 68.01 61.92 62.28 58.66 63.34 60.28 60.64 70.10 67.22 61.68 N/A 65.60 62.90 63.80 63.16 65.35 67.56 61.87 65.62 67.09 66.82 67.17 68.28 60.98 62. 91.09 97.20 65.69 19.91 27.33 26.52 27.46 26.85 25.19 25.15 27.04 28.23 26.66 N/A 28.01 27.59 27.85 26.96 27.57 26.44 25.94 26.42 27.10 26.69 27.10 26.26 27.27 26.14 27.43 98.19 98.89 99.03 99.12 99.53 99.34 99.40 98.75 98.41 98.89 N/A 98.45 98.66 99.13 99.16 98.57 99. 99.74 99.30 99.03 98.61 99.10 98.87 99.37 99.39 99.43 21.26 24.76 23.87 25.28 24.54 22.62 23.65 26.03 25.84 24.36 N/A 25.56 25.38 26.10 25.31 25.01 23.89 24.22 24.17 24.70 24.71 25.63 25.01 25.33 24.07 25.26 can be more impactful than simply scaling up model parameters. Notably, LanDiff outperforms much larger models like HunyuanVideo (13B) and Mochi-1 (10B) across most metrics. As visualized in Figure 9, we compare LanDiff with five representative models across different evaluation dimensions. 17 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation The radar chart reveals that LanDiff (shown in red) demonstrates well-balanced performance across all metrics, with notably strong results in quality score and semantic accuracy. While Sora (shown in blue) achieves competitive scores in imaging quality and scene, and HunyuanVideo excels in certain visual aspects, LanDiff maintains consistently superior performance across the entire spectrum of metrics. The comprehensive comparison with these strong baselines, including both commercial (Sora, Hailuo, Kling) and open-source models (HunyuanVideo, CogVideoX-5B), further validates the effectiveness of our hybrid LLM+Diffusion approach. A.5. More Examples Figure 10. Examples of text to videos generation of LanDiff and CogVideoX-5B. Figure 11. Examples of text to videos generation of LanDiff and CogVideoX-5B. 18 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 12. Examples of text to videos generation of LanDiff and CogVideoX-5B. Figure 13. Examples of text to videos generation of LanDiff and CogVideoX-5B. The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 14. Examples of text to videos generation of LanDiff and CogVideoX-5B. Figure 15. Examples of text to videos generation of LanDiff and CogVideoX-5B. 20 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 16. Examples of text to long video generation. Figure 17. Visualization results of video reconstruction using video tokenizer. 21 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation Figure 18. Visualization results of video reconstruction using video tokenizer."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
        "Moonshot AI, Beijing, China"
    ]
}