{
    "paper_title": "Generative Visual Code Mobile World Models",
    "authors": [
        "Woosung Koh",
        "Sungjun Han",
        "Segyu Lee",
        "Se-Young Yun",
        "Jamin Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance."
        },
        {
            "title": "Start",
            "content": "Woosung Koh * 1 2 Sungjun Han * 1 Segyu Lee 1 2 Se-young Yun 2 Jamin Shin"
        },
        {
            "title": "Abstract",
            "content": "Mobile Graphical User Interface (GUI) World Models (WMs) offer promising path for improving mobile GUI agent performance at trainand inference-time. However, current approaches face critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose novel paradigm: visual world modeling via renderable code generation, where single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with data generation framework that automatically synthesizes code-based training data. In extensive evaluation across 4 in-distribution and 2 out-of-distribution benchmarks, gWorld sets new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25 larger. Further analyses show that (1) scaling training data yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance. Project Page Code gWorld (8B, 32B) MWMBENCH 6 2 0 2 2 ] . [ 1 6 7 5 1 0 . 2 0 6 2 : r 1. Introduction Improving policy performance on mobile Graphical User Interface (GUI) tasks has become rapidly expanding research *Equal contribution 1Trillion Labs 2KAIST AI. Correspondence to: Se-young Yun <yunseyoung@kaist.ac.kr>, Jamin Shin <jay@trillionlabs.co>. Preprint. February 3, 2026. 1 Figure 1. Average Instruction Accuracy (IAcc.) across all six benchmarks. gWorld 8B and 32B achieve new pareto frontier in terms of model size (log10 scaled). The existing pareto frontier was defined by Qwen3 VL 8B, 32B, and GLM 4.6V 106B. Notably, extremely large models (e.g., Llama 4 402B) do not reach this pareto frontier, while text-image-to-image models (e.g., Emu3.5 34B) struggle with mobile GUI dynamics. area (Wang et al., 2024; Ye et al., 2025; Zhang et al., 2025; Li et al., 2025b; Nguyen et al., 2025; Liu et al., 2025; Niu et al., 2025), driven by the ubiquitous nature of mobile computing, with an estimated 8.9 billion mobile subscriptions worldwide (Ericsson, 2025). An emerging line of literature demonstrates that leveraging generative mobile World Model (WM) to predict future states can significantly enhance policy performance during both training (Fang et al., 2025; Wang et al., 2025) and inference (Chae et al., 2025; Gao et al., 2025; Gu et al., 2025; Li et al., 2025c; Cao et al., 2026). While these approaches yield substantial gains, they predict the next state in text; an abstraction over the pixel-space GUI state. This abstraction discards critical GUI information, including fine-grained spatial layout and visual attributes (e.g., iconography, typography, and color) (Chae et al., 2025; Luo et al., 2025; Cao et al., 2026). Moreover, text-only world representations limit Vision-Language Model (VLM)-based policies, which have been shown to outperform languageonly models on mobile GUI tasks (Hong et al., 2024; Lu et al., 2024; Gou et al., 2025). In response, VIMO (Luo et al., 2025) introduced the first visual mobile GUI WM and showed that visual world modeling yields larger policy Figure 2. Mobile GUI world modeling via renderable code. Given an image state St and action At, the model predicts the next state St+1. Our model, gWorld, generates renderable web code to ensure pixel-perfect text and structurally accurate layouts. In contrast, image-gen baselines (e.g., Qwen-Image-Edit 20B) struggle with the discrete nature of GUIs, frequently producing illegible text and distorted layouts. See Appendix Fig. 13, 14, 15 for additional qualitative examples. improvements than text-based alternatives. However, we observe three notable disadvantages of VIMO. First, VIMO relies on complex multi-stage pipeline rather than single self-contained model, resulting in significant computational overhead and latency (Luo et al., 2025; Cao et al., 2026). Concretely, their framework uses (1) an external Optical Character Recognition (OCR) model for text detection, (2) box-based text masking, (3) an external frontier VLM (GPT-4o) to filter masked regions, (4) customtrained diffusion model to generate the next-state image, and (5) two additional GPT-4o calls to fill in next-state text. Second, their formulation converts coordinate-based actions into natural-language instructions via GPT-4o, effectively outsourcing visual grounding to closed-weight model. Lastly, VIMO does not release the weights of its custom-trained diffusion model, making the system difficult to reproduce and deploy. In response, we present gWorld (8B, Contribution. 32B), which to our knowledge, are the first open-weight, single self-contained world models specialized for visual mobile GUI world modeling that operates via renderable code generation. We start by analyzing the limitations of using image-generation model for mobile GUI World Models ( 2.2), with further detailed analysis in 4.3. To alleviate this, we show for the first time that code-based representation can be leveraged for mobile GUI World Models ( 2.3). As there are no code-based GUI world modeling training datasets, we present our data generation framework ( 2.4). Specifically, we repurpose offline mobile-agent trajectories into (St, At)-conditioned next-state pairs, automatically converts St+1 from pixels to renderable web code, and adds free look-ahead reasoning traces, producing large-scale SFT data for training code-generating GUI WMs. Furthermore, due to the lack of comprehensive visual mobile GUI world modeling benchmarks, we present MWMBENCH, curating and open-sourcing four inand two out-of-distribution (OOD) benchmarks to evaluate mobile GUI WMs ( 3, Tab. 1). We empirically demonstrate that our data generation framework and model is effective: Our models outperform 8 frontier open-weight imageand code-generation models up to 50.25 larger, across 6 inand out-of-distribution benchmarks ( 4.2, Fig. 1, Tab. 2). We demonstrate that scaling our dataset size (37K, 77K, 129K, 240K) leads to predictable gains, closely following power law ( 4.4, Fig. 5). Ablation studies demonstrate that each component of our method contributes meaningfully ( 4.5, Tab. 3, Fig. 6). 2 Figure 3. Schematic diagram of our data generation pipeline. We construct VLM world modeling data via three steps: (1) Repurposing offline policy trajectories into transition triplets; (2) Cross-modal relabeling of the ground-truth next state from pixels (Simage t+1 ) to renderable web code (Scode t+1 ); and (3) Synthesizing reasoning traces (Rt) using look-ahead access to the target state. The final training objective is to predict both the reasoning trace and the code-based next state: (St, At) (Rt, Scode as St without the superscript in the diagram. t+1). For visual succintness we denote Simage Finally, we experimentally demonstrate that improved WM performance translates to policy model performance gains ( 4.6, Tab. 4). 1.1. Related Work Code-based World Models. Dainese et al. (2024); Copet et al. (2025) study code-based WMs primarily to improve code-generation tasks. Alternatively, code-based WMs have been applied to different domains like game playing and fictional worlds (Feng et al., 2025; Lehrach et al., 2025). However, none study code-based world models for mobile GUI world modeling. Image to Web Code. Prior work investigates converting web page images into web code to automate front-end development (Yun et al., 2024; Gui et al., 2025; Wan et al., 2025; Jiang et al., 2025; Si et al., 2025; Leviathan et al., 2025). In contrast, we show that modern VLMs can be trained to reconstruct complex mobile GUIs (e.g., settings screens and camera UIs) via web code generation alone. Mobile UI Simulator. We discuss UISim (Xiang et al., 2025), which employs multi-stage mobile GUI generation pipeline similar to VIMO, combining VLM with diffusion-based image model. However, the authors provide minimal implementation details necessary for replication, and the model weights remain closed source. Critically, their evaluation is limited to visual fidelity within single in-distribution setting, lacking assessment of world modeling dynamics; i.e., action-contingent transitions. 2. gWorld: Generative Visual Code World"
        },
        {
            "title": "Modeling",
            "content": "2.1. Problem Setting In Markov Decision Process (MDP; (Bellman, 1957)), WM corresponds to the transition distribution : 3 (S), where and denote the state and action spaces and (S) is the probability simplex over next states. Our goal is to train generative WM parameterized by θ, i.e., pθ(St+1 St, At)  (Fig. 2)  . For supervised fine-tuning (SFT; Wei et al. (2022a)), we denote the input as and the target label as . 2.2. Motivation: Limitation of Generating Pixel-based Next State. As shown in Fig. 2 and 13, while frontier open-weight models reconstruct layouts resembling the input (St), they often fail to generate plausible next states (St+1) respecting transition dynamics. Specifically, they frequently produce illegible text (Luo et al., 2025) and struggle with states requiring novel layouts. We empirically corroborate these qualitative observations in 4.3. 2.3. Next World States as Renderable Code To overcome the limitations of direct pixel generation, we emulate mobile GUI world modeling with structured web code. More specifically, we post-train VLMs to generate next states in web code. VLMs are well suited for modeling GUI transitions due to their linguistic priors and broad world knowledge. First, VLMs can generate precise, legible text, which remains major bottleneck for image-gen models (see Fig. 2, 13). Futhermore, they can synthesize semantically coherent linguistic content aligned with the application context. For example, when predicting the next state of an email app, GUI world model should render an interface populated with contextually plausible, realistic email content (see Fig. 2, 13, 14, 15). Finally, the prevalence of structured web code in VLM pre-training provides strong inductive bias, making VLMs natural foundation for generative visual code GUI world models. Benchmark Action Space World Modality OOD MobileWorldBench VIMOs Benchmark 2 (AW, KA) MWMBENCH (ours) Table 1. Comparison with existing mobile world modeling benchmarks. Prior benchmarks simplify the problem by converting actions to text and testing only in-distribution. In contrast, MWMBENCH allows evaluations on the native visual action space (preserving original coordinates) and is the first to assess zero-shot generalization on held-out out-of-distribution sets. Datasets: Android in the Wild (AitW), GUIOdyssey (GUIO), AndroidControl (AC), Android Multi-annotation Expo (AMEX), AndroidWorld (AW), and KApps (KA). 2 (AitW, AC) 2 (AitW, AC) 4 (AitW, GUIO, AC, AMEX) Converted to Text Converted to Text Original Coordinates and Text Text Visual Visual In-Distribution Dataset Composition 2.4. World Model Training Data Generation look-ahead is provided in Appendix A. t+1 ))}T 1 , At), : (Rt, Scode Our framework generates VLM SFT (Liu et al., 2023) data of the form {(X : (Simage t=1 . Here Rt is text-based reasoning trace (Wei et al., 2022b), included because reasoning supervision is well-established way to improve VLM performance (Rose et al., 2023; Chen et al., 2024c; Zhang et al., 2024; Chen et al., 2024b). (1) First, we repurpose abundant offline policy trajectory data as WM training data. (2) Second, we convert the next-state supervision (St+1) from pixels to renderable web code. (3) Finally, we synthesize reasoning traces (Rt) using free lookahead to the ground-truth next state. We provide visual diagram in Fig. 3, and pseudocode in Appendix A. (1) Repurposing Policy Trajectory as World Modeling Data. We first repurpose existing large-scale mobile agent policy trajectory data into world modeling data. Given an episode trajectory {(X : Simage t=1, we synthesize transition examples {(X : Simage t+1 )}T 1 t=1 , matching the WM objective pθ(St+1St, At). This transformation reduces the number of examples per episode from to 1. , At, : Simage , : At)}T (2) Synthetic Cross-modal State Re-labeling. As our VLM outputs text, we re-label the next-state target from pixels to renderable web code. Leveraging frontier model π with strong image-to-web-code capabilities, we obtain π(Simage Scode , img-to-code); the prompt img-to-code is provided in Appendix A. (3) Reasoning Data with Free Look-ahead. As we have access to the ground truth next state (St+1) for St, we generate reasoning traces Rt with look-ahead access to St+1. The look-ahead grounds Rt in the true next-state transition, ensuring alignment between the reasoning trace and Scode t+1 . For the WM, the introduction of Rt decomposes the complex world modeling problem into two simpler sub-problems: first predicting state changes in natural language, then converting this description to web code. Concretely, we use frontier model π to synthe- , At, Simage size Rt π(Simage t+1 , look-ahead); the prompt 4 2.5. World Model Training. We generate dataset of 260K samples using our method, derived from existing policy trajectories in Android in the Wild (AitW; Rawles et al. (2023)), GUIOddyssey (GUIO; Lu et al. (2025)), AndroidControl (AC; Li et al. (2024)), and Android Multi-annotation Expo (AMEX; Chai et al. (2025)). The frontier model (π) used for data generation experiments is Gemini 3 Flash. The base models for training are Qwen3 VL 8B and 32B (Bai et al., 2025), selected as they represent the frontier of open-weight VLMs. We validate our post-training data on frontier open-weight models as we can examine whether our proposed dataset is novel and useful, given good base model. Hyperparameters and further details are available in Appendix C. 3. MWMBENCH: Comprehensive Mobile GUI"
        },
        {
            "title": "World Modeling Benchmark",
            "content": "We introduce Mobile World Model Bench (MWMBENCH), comprehensive benchmark for evaluating world modeling in mobile GUI environments. MWMBENCH, consisting of (St, At, St+1) tuples from 6 data sources, enables systematic measurement of next-state prediction quality ˆSt+1. It represents real-world mobile GUI usage, spanning diverse applications, tasks, and interaction patterns in different languages. Furthermore, MWMBENCH addresses three critical limitations of existing mobile GUI WM benchmarks that ensures close alignment with real-world deployment scenarios (see Tab. 1). Details are in Appendix B. Visual World Modeling. First, unlike MobileWorldBench (Li et al., 2025c) which can only evaluate text-based WMs, we evaluate world models in the native visual modality so that rich GUI details and semantics are preserved. Real-world Action Space. Second, unlike MobileWorldBench and VIMOs benchmark, we keep actions in coordinate space rather than converting them to text, avoiding dependence on an external frontier model (Luo et al., 2025). Image-gen Code-gen Model: Parameter Size: Qwen-I-E 20B Emu3.5 34B Llama 4 109B-A17B 402B-A17B 8B Qwen3 VL 32B 235B-A22B GLM-4.6V 106B gWorld 8B 32B IAcc. (%, ) Render Fail (%, ) Similarity (%, ) IAcc. (%, ) Render Fail (%, ) Similarity (%, ) IAcc. (%, ) Render Fail (%, ) Similarity (%, ) IAcc. (%, ) Render Fail (%, ) Similarity (%, ) IAcc. (%, ) Render Fail (%, ) Similarity (%, ) IAcc. (%, ) Render Fail (%, ) Similarity (%, ) IAcc. (%, ) Render Fail (%, ) Similarity (%, ) MWMBENCH-AITW 47.6 4.4 57.9 47.2 9.4 58.9 21.5 33.8 49.9 MWMBENCH-GUIODYSSEY 53.1 1.2 62. 55.8 7.8 64.0 28.2 51.4 48.3 MWMBENCH-ANDROIDCONTROL 50.7 1.0 61.4 58.6 8.6 63.1 31.1 42.8 53. MWMBENCH-AMEX 49.0 0.6 66.9 58.3 12.6 68.1 33.7 31.6 59.2 46.8 11.6 59.0 52.0 16.0 62. 53.2 13.4 64.1 56.9 3.8 70.0 23.4 68.7 25.8 68.8 27.7 68.6 21.7 71. MWMBENCH-ANDROIDWORLD (out-of-distribution) 29.1 74.2 26.8 71.2 25.8 70.5 51.0 2.9 61.7 54.3 14.4 61. 30.8 42.3 49.9 53.4 13.1 61.2 MWMBENCH-KAPPS (out-of-distribution) 48.4 1.8 57.3 50.0 2.0 61.2 Average 59.9 2.2 58.6 30.1 38.8 50.5 55.7 9.2 62.4 29.2 40.1 51.8 52.5 8.1 62.8 52.5 11.0 63. 15.4 60.1 13.0 63.8 11.7 63.8 10.9 64.4 13.8 67.9 15.7 71. 13.4 65.2 36.1 40.0 62.9 54.7 27.2 69.7 51.9 34.2 68.8 51.2 30.0 71.7 51.1 30.0 65. 64.2 15.4 67.3 51.5 29.5 67.6 60.9 2.4 64.7 68.8 0.8 66.3 68.2 3.8 72.5 77.2 1.2 73. 74.2 1.4 71.4 78.4 2.6 72.8 69.5 1.2 73.2 82.6 0.8 74.3 74.1 1.9 72.2 75.0 2.3 69. 57.4 4.4 63.7 67.4 0.8 66.1 67.4 2.5 69.6 74.9 1.4 70.3 71.7 0.6 67.3 81.5 0.8 73. 82.9 0.8 74.2 86.1 0.4 75.4 79.9 0.4 71.6 75.7 0.6 66.2 79.6 0.6 71.4 Table 2. Main mobile world modeling results. We compare gWorld against frontier image-generation and VLM baselines across in-distribution and OOD benchmarks. The best scores are bolded and the second best are underlined. gWorld 8B, 32B establishes new pareto frontier, consistently outperforming significantly larger models (e.g., Llama 4 402B-A17B, Qwen3-VL 235B-A22B). Notably, our code-based approach virtually eliminates structural errors (<1% Render Fail), driving +45.7% and +27.1% gain in average Instruction Accuracy (IAcc.) over the base models Qwen3 VL 8B, 32B, respectively. This makes the evaluated world models directly compatible with real-world mobile execution, where actions are issued in coordinate space (Rawles et al., 2025). Inand Out-of-distribution Evaluations. Lastly, we support four in-distribution (ID) and two out-of-distribution (OOD) evaluation for comprehensive assessment. For ID evaluation, we randomly sample 500 world modeling instances ( 2.4 (1)) from trajectories in Android in the Wild (AitW; Rawles et al. (2023)), GUIOddyssey (GUIO; Lu et al. (2025)), AndroidControl (AC; Li et al. (2024)), and Android Multi-annotation Expo (AMEX; Chai et al. (2025)) to form held-out test sets. We designate these as ID because they are the training sets of prior works and ours. To evaluate OOD generalization, we curate two new benchmarks: ANDROIDWORLD (AW) and KAPPS (KA). For ANDROIDWORLD, we automatically collect offline trajectories from AndroidWorld (Rawles et al., 2025) and convert them into world modeling tasks. For KAPPS, we manually curate extensive ground truth policy trajectories in Korean, reflecting Korea-centric mobile usage, and convert them into world modeling tasks. We designate these as OOD as no corresponding training datasets are publicly available. Details of these assets are provided in Appendix B, Tab. 5. 4. Empirical Study 4.1. World Modeling Experiment Set-up Evaluation Metric. Following Luo et al. (2025), we use (1) Instruction Accuracy (IAcc.) and (2) similarity score 5 against ground truth. (1) IAcc. This is our primary metric: VLM-as-a-Judge that outputs binary pass/fail verdict on whether the generated next state is consistent with the current stateaction pair. IAcc. π(St, At, ˆSt+1; IAcc.), where ˆSt+1 is generated by our model and IAcc. is the evaluation prompt (Appendix C). IAcc. measures action-conditioned next-state correctness, directly reflecting world-modeling performance. IAcc. has been extensively studied to correlate highly with humans in Luo et al. (2025). To mitigate judge-model (family) bias (Chen et al., 2024a; Panickssery et al., 2024; Li et al., 2025a), we compute IAcc. as the mean of the binary verdicts from three frontier VLM judges: GPT-5 Mini, Claude 4.5 Haiku, and Gemini 3 Flash. Per-judge IAcc. scores are reported in Appendix Tab. 9, and we observe high inter-judge agreement (see Appendix Fig. 11, 12). To reduce computational overhead, we use rule-based filter that identifies un-renderable web code which classifies these faulty instances as automatic failures prior to evaluation. (2) Similarity. Embedding similarity reports the cosine similarity between the respective embeddings of the generated next-state image ˆSt+1 and the ground-truth next-state image St+1. This metric captures perceptual similarity but does not verify action-conditioned semantic correctness (i.e., whether the transition matches the instruction). We report the average value of DINO v1 (Caron et al., 2021) and v2 (Oquab et al., 2024) vision encoders embeddings. The granular results for each encoder is organized in Appendix Tab. 9. Baseline Models. To the best of our knowledge, this work is the first to propose unified visual world model for mobile GUIs; consequently, there are no specialized baselines directly comparable to our approach. We therefore benchmark against widely adopted frontier open-weight models (Maslej et al., 2025). We select frontier open-weight models released in the past year, including text-image-to-image models Qwen-Image-Edit 20B (Wu et al., 2025) and Emu3.5 34B (Cui et al., 2025), as well as VLMs including Llama 4 (109B, 402B) (Meta, 2025), Qwen3 VL (8B, 32B, 235B) (Bai et al., 2025), and GLM-4.6V 106B (Team et al., 2025). 4.2. World Modeling Results Strong In/Out-of-Distribution Results against Existing Models. gWorld 32B and 8B achieve the best and second-best performance (IAcc.), respectively, across all six benchmarks (see Tab. 2). Notably, gWorld demonits performance on OOD strates robust generalization; benchmarks does not significantly degrade compared to in-distribution settings. The next best-performing baselines are GLM-4.6V 106B and Llama 4 402B-A17B. This Figure 4. Correlation between input-output similarity and model performance. Top: Pearson correlation ρ between Sim(St, St+1) and Sim( ˆSt+1, St+1). Image generation models show strong positive correlations (ρ > 0.7), suggesting output quality largely depends on how similar St and St+1 already are. Bottom: Sim( ˆSt+1, St+1) Sim(St, St+1) vs. Sim(St, St+1), with the gray line indicating the score ceiling. Emu3.5 34B clusters near zero, implying Sim( ˆSt+1, St+1) Sim(St, St+1); i.e., outputs nearly identical to inputs, St ˆSt+1. In contrast, gWorld 32B shows wide vertical spread, indicating active state transformation with many samples achieving large positive gains toward the ceiling. Same analysis with Qwen-Image-Edit 20B is available in Appendix Fig. 21 with equivalent results. is particularly notable given that these models exceed the parameter count of gWorld 8B by factors of 13.25 and 50.25, respectively. Moreover, gWorld 32B and 8B rank first in terms of Similarity, with the exception of two benchmarks with Emu3.5 34B being the highest. 4.3. Further Analysis: Limitation of Image-gen Models While image-generation baselines attain high visual similarity scores, they fail to capture mobile dynamics, achieving only 10.9 to 29.1% IAcc. (Tab. 2). We attribute this discrepancy to the high visual redundancy in mobile GUIs, where transitions often involve minimal changes (e.g., typing). Consequently, image-generation models can maximize similarity metrics by learning trivial identity mapping copying St with minor editsrather than modeling the semantic state transition to St+1. Fig. 4 confirms that image-generation models performance relies on input-output similarity rather than action understanding. Image-generation models exhibit strong Pearson correlations between the ground-truth transition similarity Sim(St, St+1) and their output similarity Sim( ˆSt+1, St+1) (Emu3.5 34B: ρ = 0.92, Qwen-Image-Edit 20B: 6 Metric Alternative Ours Renderable Code (%, ) IAcc. (%, ) - Gemini 3 Pro IAcc. (%, ) - Claude 4.5 Opus Table 3. Ablation on Scode t+1 train data quality. Our method outperforms the naïve alternative, achieving perfect code renderability and higher IAcc. across strong VLM-as-a-Judges. +3 +5.40 +1.90 97 94.60 84.80 100 100 86.70 Figure 6. Ablation on Rt train data quality. Our method consistently outperforms the naïve alternative in terms of IAcc. Both models are trained on 37K samples on top the base model Qwen3 VL 8B. 4.5. Further Analysis: Ablation Ablation Analysis on Generating Scode t+1 . For our first ablative study, we examine whether the synthetic crossmodel policy trajectory repurposed training (see Step 1 and 2 in Fig. 3) is superior to naïve alternative of generating next state code (Scode t+1 ) with the same frontier model t+1 π(Simage Scode , At, WM), where WM is the identical prompt used for all WM evaluations (see Appendix C). We randomly sample 100 Scode t+1 instances (25 from each dataset) and evaluate them using our established metrics. Given the managable sample size, we employ the most capable (expensive) frontier models, Gemini 3 Pro and Claude 4.5 Opus, for our IAcc. evaluations. As presented in Tab. 3, our approach for generating Scode t+1 outperforms the naïve alternative. Our approach generates renderable code 100% of the time, with perfect 100% IAcc. score when using Gemini 3 Pro as the judge. π(Simage Ablation Analysis on Generating Rt. We evaluate the efficacy of our reasoning generation (Step 3 in Fig. 3) against baseline generated by the same frontier model without look-ahead: , At, WM). We train two gWorld variants using Qwen3 VL 8B on 37K samples using the SFT dataset {(X : Simage t=1, Rt {Rt, }. Crucially, we use identical validated Scode t+1 targets for both models to strictly isolate the performance impact of the reasoning trace (Rt vs. ). As shown in Fig. 6, while both strategies improve world modeling, our method outperforms the alternative across all five benchmarks, con- , At, : Rt, Scode t+1 )}T Figure 5. Data scaling laws for mobile world modeling at 8B. We fit power-law curves (y = axb) to the test performance across five distinct benchmarks as function of training dataset size. The high coefficients of determination (R2 0.94 for most splits) indicate predictable and non-saturating relationship between data scale and performance. This suggests that our data generation pipeline has not yet reached its upper bound and will continue to improve with larger-scale repurposed trajectories. ρ = 0.74), whereas gWorld shows much weaker correlation (ρ 0.4). Furthermore, plotting the similarity gain (Fig. 4, bottom) reveals that Emu3.5 34B consistently yields near-zero values regardless of difficulty, implying the output ˆSt+1 remains nearly identical to St. In contrast, gWorld 32B displays substantial variance, indicating it actively predicts structural changes based on the action rather than defaulting to copying strategy. 4.4. Further Analysis: Scaling Data To assess the scalability of our approach, we examine whether increasing the dataset size yields consistent performance improvements. We test data set sizes of 37K, 77K, 129K, and 240K and plot our training curves in Fig. 5; with further granular plots in Appendix Fig. 9 and 10. We observe monotonic performance gains as the dataset size increases, providing strong evidence of data quality and effective scaling. Consistent with empirical scaling laws demonstrated in Li et al. (2024), Kaplan et al. (2020), Hoffmann et al. (2022), we observe that our dataset scaling follows power law with an average R2 of 0.948 (see Fig. 5). This analysis enables performance projection as we continue to generate data via our method. We compute the maximum number of trainable transitions attainable in Appendix C.1 based on the four existing offline trajectory data sets we use (AitW, GUIO, AC, and AMEX), and arrive at maximum data set size of 3.7 million. Based on the power law trajectory in Fig. 5, we project significant performance gains by utilizing the remaining available trajectories. GUIO AC AMEX KApps Avg. downstream policy. Method Backbone Policy: M3A + Value wo. WM + Qwen3 VL 8B + Qwen3 VL 32B + gWorld 8B vs. Qwen3 VL 8B Gemini 2.5 Flash 51.5 65.5 68.9 43.1 53.9 58.5 45.8 54.5 48.3 53.6 70.5 70.9 55.4 72.9 79.6 +9.6 +18.4 +31.3 63.2 54.4 41.4 57.4 71.8 +30.4 62.3 52.5 47.5 63.1 69.9 +22.4 42.8 58.6 42.8 60.2 72.4 +29.6 59.5 69.9 47.9 63.9 75.2 +27. GPT-5 Mini 49.7 57.7 51.8 66.7 74.1 +22.3 Backbone Policy: M3A + Value wo. WM + Qwen3 VL 8B + Qwen3 VL 32B + gWorld 8B 44.2 55.8 45.4 59.0 67.2 +21.8 Table 4. Step-wise accuracy (%) comparison across world models. Rows highlighted in gray leverage our proposed gWorld models. The rows indicate the absolute performance gain over the corresponding Qwen3 VL baseline. The best scores are bolded and the second best are underlined. 25.0 37.0 39.2 45.2 47.0 +7.8 vs. Qwen3 VL 8B firming the superiority of our look-ahead strategy. 4.6. Potential World Model-enhanced Policy Model Performance Gains Finally, we demonstrate the downstream efficacy of our WM by applying it to enhance mobile GUI agent policies. We provide granular details in Appendix C.2. , . . . , AK Experiment Set-up. Inspired by Luo et al. (2025), we evaluate the potential performance gains achieved by integrating WM into the policy via breadth-wise rollout and value estimation. Specifically, we present the M3A agent (Rawles et al., 2025) with = 3 action candidates {A1 }, which include the ground truth action. For the M3A + WM setting, we (1) roll out these candidate actions using the WM to predict next states {S1 t+1, . . . , SK t+1}, (2) compute the value of these transitions {V (St, Ak k=1 by prompting the policy backbone, and (3) select the action with the highest estimated value. For the M3A + Value wo. WM baseline, the value function estimates utility directly from the current state and action, i.e., {V (St, Ak k=1, without future state prediction. The value and policy models are set the same models to ensure that the method is self-contained. t+1)}K )}K , Sk Results. As shown in Tab. 4, across two arbitrarily set backbone policies (Gemini 2.5 Flash, GPT-5 Mini) with = 3, incorporating gWorld 8B yields the most significant performance gains over the M3A baseline. Across gWorld 8B, Qwen3 VL 32B, and Qwen3 VL 8B, we observe that world modeling performance is positively correlated with downstream policy gains. On average, 1.0 percentage point increase in world modeling performance translates to 0.49 percentage point improvement in 8 5. Additional Discussion Evaluation Metric Comparison with VIMO. While it was not possible to compare directly with VIMO on our experimental settings as they do not open-weight their diffusion model as of the time of writing, we compare Similarity v1 scores (in Appendix Tab. 9) on MWMBENCHAITW and MWMBENCH-ANDROIDCONTROL as they are directly comparable with the experiments conducted in Luo et al. (2025). gWorld 8B and 32B achieves an average of 81%, 81.9%, while VIMO achieves 74% suggesting that the gWorld is superior in terms of generating next states closer to the ground truth. Rendering Web Code is Virtually Overhead Free. We empirically find that the wall-clock time of rendering the generated web code is virtually cost-free. When we start the experiment we incur 1 second one-off wall-clock cost of launching the browser, but afterwards it takes approximated 0.3 seconds per render and capture. This minimal wallclock time can also easily be parallelized using more process threads. Fast Parallel Inference. Using vLLM (Kwon et al., 2023) on 4xH200 GPUs, gWorld 32B and 8B achieve throughputs of 5,000 and 20,000 tokens/sec, respectively. This corresponds to generation latencies of approximately 1s (32B) and 0.25s (8B) per state, enabling significant parallel rollout. Potential Application: World Model for Synthetic Data Scaling and Scalable RL. Mobile GUI agent training faces three key challenges that WMs can address. First, irreversible or consequential actions (e.g., financial transactions) are too risky to execute during training. Second, deep application states require many sequential actions to access, making data collection expensive. An accurate WM enables agents to simulate critical actions without real execution and to expand deep-state coverage by recursively generating trajectories from already-collected states. Third, online RL for GUI agents faces fundamental scalability bottleneck due to device-policy coupling. Each rollout requires persistent Android emulator, creating 1:1 coupling where GPUs sit idle during action execution in emulators (>2s latency). WMs eliminate this device-bound bottleneck, enabling massively parallel, compute-bound rollout generation. We look forward to future works expanding on the wide-reaching applications of mobile GUI world models. Limitation and Future Direction. We discuss the limitation and future direction of this work in Appendix E."
        },
        {
            "title": "Acknowledgements",
            "content": "This research was fully funded by Trillion Labs. We acknowledge the following members of Trillion Labs who participated in collecting the data for MWMBENCH-KAPPS: Hongjoon Ahn, Hyungguk Kim, Juyoung Suk, Kyuseok Kim, Suyeoung An, and Wonsuk Yang. Special thanks to Hongjoon Ahn and Juyoung Suk for their valuable discussions. We would also like to thank Haein Lee for their assistance in designing Figures 2, 3, 13, 14, and 15."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning by enabling more capable and efficient mobile agents and world models. The primary societal benefits include enhancing digital accessibility for users with impairments and democratizing AI research through high-performance open-weight (as of publication) models that require less compute than prior methods. While improved GUI automation carries dual-use risks (e.g., automated fraud), we believe open research is crucial for developing robust safety measures."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Bellman, R. markovian decision process. Journal of mathematics and mechanics, pp. 679684, 1957. Cao, Y., Zhong, Y., Zeng, Z., Zheng, L., Huang, J., Qiu, H., Shi, P., Mao, W., and Guanglu, W. Mobiledreamer: Generative sketch world model for gui agent. arXiv preprint arXiv:2601.04035, 2026. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 96509660, October 2021. Chae, H., Kim, N., iunn Ong, K. T., Gwak, M., Song, G., Kim, J., Kim, S., Lee, D., and Yeo, J. Web agents with world models: Learning and leveraging environIn The Thirteenth ment dynamics in web navigation. International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=moWiYJuSGF. Chai, Y., Huang, S., Niu, Y., Xiao, H., Liu, L., Wang, G., Zhang, D., Ren, S., and Li, H. AMEX: Android multiannotation expo dataset for mobile GUI agents. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 21382156, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 110. URL https://aclanthology.org/2025. findings-acl.110/. Chen, D., Chen, R., Zhang, S., Wang, Y., Liu, Y., Zhou, H., Zhang, Q., Wan, Y., Zhou, P., and Sun, L. MLLMas-a-judge: Assessing multimodal LLM-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024a. URL https: //openreview.net/forum?id=dbFEFHAD79. Chen, Y., Sikka, K., Cogswell, M., Ji, H., and Divakaran, A. Measuring and improving chain-of-thought reasoning in vision-language models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 192210, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long. 11. URL https://aclanthology.org/2024. naacl-long.11/. Chen, Z., Zhou, Q., Shen, Y., Hong, Y., Sun, Z., Gutfreund, D., and Gan, C. Visual chain-of-thought prompting for knowledge-based visual reasoning. Proceedings of the AAAI Conference on Artificial Intelligence, 38 (2):12541262, Mar. 2024c. doi: 10.1609/aaai.v38i2. 27888. URL https://ojs.aaai.org/index. php/AAAI/article/view/27888. Copet, J., Carbonneaux, Q., Cohen, G., Gehring, J., Kahn, J., Kossen, J., Kreuk, F., McMilin, E., Meyer, M., Wei, Y., et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., Wang, Y., Wang, C., Zhang, F., Zhao, Y., Pan, T., Li, X., Hao, Z., Ma, W., Chen, Z., Ao, Y., Huang, T., Wang, Z., and Wang, X. Emu3.5: Native multimodal models are world learners, 2025. URL https://arxiv.org/abs/2510. 26583. 9 Dainese, N., Merler, M., Alakuijala, M., and Marttinen, P. Generating code world models with large language models guided by monte carlo tree search. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C. (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 6042960474. Curran Associates, Inc., 2024. doi: 10.52202/079017-1933. https://proceedings.neurips. URL cc/paper_files/paper/2024/file/ 6f479ea488e0908ac8b1b37b27fd134c-Paper-Conference. pdf. Ericsson. Ericsson mobility report: November 2025. Technical report, Ericsson, November 2025. Fang, T., Zhang, H., Zhang, Z., Ma, K., Yu, W., Mi, H., and Yu, D. WebEvolver: Enhancing web agent self-improvement with co-evolving world model. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 89598975, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 9798-89176-332-6. doi: 10.18653/v1/2025.emnlp-main. 454. URL https://aclanthology.org/2025. emnlp-main.454/. Feng, J., Zhang, Y., Zhang, C., Lu, Y., Liu, S., and Wang, M. Web world models, 2025. URL https://arxiv. org/abs/2512.23676. Gao, Y., Ye, J., Wang, J., and Sang, J. Websynthesis: Worldmodel-guided mcts for efficient webui-trajectory synthesis. arXiv preprint arXiv:2507.04370, 2025. Gou, B., Wang, R., Zheng, B., Xie, Y., Chang, C., Shu, Y., Sun, H., and Su, Y. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=kxnoqaisCT. Gu, Y., Zhang, K., Ning, Y., Zheng, B., Gou, B., Xue, T., Chang, C., Srivastava, S., Xie, Y., Qi, P., Sun, H., and Su, Y. Is your LLM secretly world model of the internet? model-based planning for web agents. Transactions on Machine Learning Research, 2025. ISSN 28358856. URL https://openreview.net/forum? id=c6l7yA0HSq. Gui, Y., Li, Z., Zhang, Z., Wang, G., Lv, T., Jiang, G., Liu, Y., Chen, D., Wan, Y., Zhang, H., Jiang, W., Shi, X., and Jin, H. Latcoder: Converting webpage In Proceeddesign to code with layout-as-thought. ings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, KDD 25, pp. 721732, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714542. doi: 10. 1145/3711896.3737016. URL https://doi.org/ 10.1145/3711896.3737016. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022. URL https://arxiv.org/ abs/2203.15556. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Jiang, Y., Zheng, Y., Wan, Y., Han, J., Wang, Q., Lyu, M. R., and Yue, X. Screencoder: Advancing visual-tocode generation for front-end automation via modular multimodal agents. arXiv preprint arXiv:2507.22827, 2025. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Lehrach, W., Hennes, D., Lazaro-Gredilla, M., Lou, X., Wendelken, C., Li, Z., Dedieu, A., Grau-Moya, J., Lanctot, M., Iscen, A., et al. Code world models for general game playing. arXiv preprint arXiv:2510.04542, 2025. Leviathan, Y., Valevski, D., Kalman, M., Lumen, D., Segalis, E., Molad, E., Pasternak, S., Natchu, V., Nygaard, V., Venkatachary, S. C., Manyika, J., and Matias, Y. Generative ui: Llms are effective ui generators. Google Research, 2025. URL https://generativeui. github.io/static/pdfs/paper.pdf. Li, D., Sun, R., Huang, Y., Zhong, M., Jiang, B., Han, J., Zhang, X., Wang, W., and Liu, H. Preference leakage: contamination problem in llm-as-a-judge. arXiv preprint arXiv:2502.01534, 2025a. Li, N., Qu, X., Zhou, J., Wang, J., Wen, M., Du, K., Lou, X., Peng, Q., Wang, J., and Zhang, W. Mobileuse: hierarchical reflection-driven GUI agent for autonomous mobile operation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. URL https://openreview.net/forum? id=KR6tnkb6h4. Li, S., Kallidromitis, K., Gokul, A., Kato, Y., Kozuka, K., and Grover, A. Mobileworldbench: Towards semantic world modeling for mobile agents. arXiv preprint arXiv:2512.14014, 2025c. Li, W., Bishop, W. E., Li, A., Rawles, C., Campbell-Ajala, F., Tyamagundlu, D., and Riva, O. On the effects of In The Thirty-eight data scale on UI control agents. Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=yUEBXN3cvX. Liu, G., Zhao, P., Liang, Y., Liu, L., Guo, Y., Xiao, H., Lin, W., Chai, Y., Han, Y., Ren, S., Wang, H., Liang, X., Wang, W., Wu, T., Lu, Z., Chen, S., LiLinghao, Wang, H., Xiong, G., Liu, Y., and Li, H. LLMpowered GUI agents in phone automation: Surveying progress and prospects. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https: //openreview.net/forum?id=yWQqoi1G1K. Etchemendy, J., et al. Artificial intelligence index report 2025. arXiv preprint arXiv:2504.07139, 2025. Meta. Llama 4: Advancing multimodal intelligence with mixture-of-experts. https://ai.meta.com/ blog/llama-4-multimodal-intelligence/, 2025. Accessed: 2026-01-20. Nguyen, D., Chen, J., Wang, Y., Wu, G., Park, N., Hu, Z., Lyu, H., Wu, J., Aponte, R., Xia, Y., Li, X., Shi, J., Chen, H., Lai, V. D., Xie, Z., Kim, S., Zhang, R., Yu, T., Tanjim, M., Ahmed, N. K., Mathur, P., Yoon, S., Yao, L., Kveton, B., Kil, J., Nguyen, T. H., Bui, T., Zhou, T., Rossi, R. A., and Dernoncourt, F. GUI agents: survey. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 2252222538, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 1158. URL https://aclanthology.org/2025. findings-acl.1158/. Niu, R., Ji, J., Chang, Y., and Wang, Q. Screenexplorer: Training vision-language model for diverse exploration in open gui world. arXiv preprint arXiv:2505.19095, 2025. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3489234916. Curran Associates, Inc., 2023. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/ 6dcf277ea32ce3288914faf369fe6de0-Paper-Conference. pdf. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https:// openreview.net/forum?id=a68SUt6zFt. Featured Certification. Lu, Q., Shao, W., Liu, Z., Du, L., Meng, F., Li, B., Chen, B., Huang, S., Zhang, K., and Luo, P. Guiodyssey: comprehensive dataset for cross-app gui navigation on In Proceedings of the IEEE/CVF Inmobile devices. ternational Conference on Computer Vision (ICCV), pp. 2240422414, October 2025. Lu, Y., Yang, J., Shen, Y., and Awadallah, A. Omniparser for pure vision based gui agent, 2024. URL https: //arxiv.org/abs/2408.00203. Luo, D., Tang, B., Li, K., Papoudakis, G., Song, J., Gong, S., Hao, J., Wang, J., and Shao, K. Vimo: generative visual gui world model for app agents. arXiv preprint arXiv:2504.13936, 2025. Maslej, N., Fattorini, L., Perrault, R., Gil, Y., Parli, V., Kariuki, N., Capstick, E., Reuel, A., Brynjolfsson, E., Panickssery, A., Bowman, S. R., and Feng, S. LLM evaluators recognize and favor their own generations. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=4NJBV6Wp0h. Rawles, C., Li, A., Rodriguez, D., Riva, O., and Lillicrap, T. P. Androidinthewild: large-scale dataset for android In Thirty-seventh Conference on Neudevice control. ral Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview. net/forum?id=j4b3l5kOil. Rawles, C., Clinckemaillie, S., Chang, Y., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W. E., Li, W., CampbellAjala, F., Toyama, D. K., Berry, R. J., Tyamagundlu, D., Lillicrap, T. P., and Riva, O. Androidworld: dynamic benchmarking environment for autonomous agents. In 11 The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=il5yUQsrjC. Rose, D., Himakunthala, V., Ouyang, A., He, R., Mei, A., Lu, Y., Saxon, M., Sonar, C., Mirza, D., and Wang, W. Y. Visual chain of thought: bridging logical gaps with multimodal infillings. arXiv preprint arXiv:2305.02317, 2023. Si, C., Zhang, Y., Li, R., Yang, Z., Liu, R., and Yang, D. Design2Code: Benchmarking multimodal code generation for automated front-end engineering. In Chiruzzo, L., Ritter, A., and Wang, L. (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39563974, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long. 199. URL https://aclanthology.org/2025. naacl-long.199/. Team, V., Hong, W., Yu, W., Gu, X., Wang, G., Gan, G., Tang, H., Cheng, J., Qi, J., Ji, J., Pan, L., Duan, S., Wang, W., Wang, Y., Cheng, Y., He, Z., Su, Z., Yang, Z., Pan, Z., Zeng, A., Wang, B., Chen, B., Shi, B., Pang, C., Zhang, C., Yin, D., Yang, F., Chen, G., Xu, J., Zhu, J., Chen, J., Chen, J., Chen, J., Lin, J., Wang, J., Chen, J., Lei, L., Gong, L., Pan, L., Liu, M., Xu, M., Zhang, M., Zheng, Q., Yang, S., Zhong, S., Huang, S., Zhao, S., Xue, S., Tu, S., Meng, S., Zhang, T., Luo, T., Hao, T., Tong, T., Li, W., Jia, W., Liu, X., Zhang, X., Lyu, X., Fan, X., Huang, X., Wang, Y., Xue, Y., Wang, Y., Wang, Y., An, Y., Du, Y., Shi, Y., Huang, Y., Niu, Y., Wang, Y., Yue, Y., Li, Y., Zhang, Y., Wang, Y., Wang, Y., Zhang, Y., Xue, Z., Hou, Z., Du, Z., Wang, Z., Zhang, P., Liu, D., Xu, B., Li, J., Huang, M., Dong, Y., and Tang, J. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2507.01006. Wan, Y., Wang, C., Dong, Y., Wang, W., Li, S., Huo, Y., and Lyu, M. Divide-and-conquer: Generating ui code from screenshots. Proc. ACM Softw. Eng., 2(FSE), June 2025. doi: 10.1145/3729364. URL https://doi.org/10. 1145/3729364. Wang, J., Xu, H., Jia, H., Zhang, X., Yan, M., Shen, W., Zhang, J., Huang, F., and Sang, J. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=O0nBMRlkc8. Wang, Y., Yin, D., Cui, Y., Zheng, R., Li, Z., Lin, Z., Wu, D., Wu, X., Ye, C., Zhou, Y., et al. Llms as scalable, generalpurpose simulators for evolving digital agent training. arXiv preprint arXiv:2510.14969, 2025. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a. URL https: //openreview.net/forum?id=gEZrGCozdqR. Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022b. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference. pdf. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Xiang, J., Zhu, Y., Shu, L., Wang, M., Yu, L., Barcik, G., Lyon, J. D., Sunkara, S., and Chen, J. UISim: An interactive image-based UI simulator for dynamic mobile environments. In NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning, 2025. URL https://openreview.net/ forum?id=ubbYuG64m4. Ye, J., Zhang, X., Xu, H., Liu, H., Wang, J., Zhu, Z., Zheng, Z., Gao, F., Cao, J., Lu, Z., et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Yun, S., Lin, H., Thushara, R., Bhat, M. Q., Wang, Y., Jiang, Z., Deng, M., Wang, J., Tao, T., Li, J., Li, H., Nakov, P., Baldwin, T., Liu, Z., Xing, E. P., Liang, X., and Shen, Z. Web2code: large-scale webpage-to-code dataset and evaluation framework for multimodal LLMs. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum? id=hFVpqkRRH1. Zhang, Z., Zhang, A., Li, M., hai zhao, Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning ISSN 2835-8856. URL https:// Research, 2024. openreview.net/forum?id=y1pPWFVfvR. Zhang, Z., Lu, Y., Fu, Y., Huo, Y., Yang, S., Wu, Y., Si, H., Cong, X., Chen, H., Lin, Y., Xie, J., Zhou, W., Xu, W., Zhang, Y., Su, Z., Zhai, Z., Liu, X., Mei, Y., Xu, J., Tian, H., Wang, C., Chen, C., Yao, Y., Liu, Z., and Sun, M. AgentCPM-GUI: Building mobile-use agents with reinforcement fine-tuning. In Habernal, I., Schulam, P., and Tiedemann, J. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 155180, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-334-0. doi: 10.18653/v1/2025. emnlp-demos.12. URL https://aclanthology. org/2025.emnlp-demos.12/. 13 A. Further Details on our Method The pseudocode of our method is available in Alg. 1. Prompt img-to-code and look-ahead is available below. Algorithm 1: gWorld: World Model SFT Data Generation Input : Offline policy trajectories Dπ = {τ }, where τ = {(Simage and look-ahead , At)}T t=1; frontier model π; prompts img-to-code Output :World-model SFT dataset DWM = {(X : (Simage /* Initialize output dataset DWM []; for each episode τ Dπ do , At)}T Extract {(Simage /* (1) Repurpose policy trajectory: for each timestep {1, . . . , 1} do t=1 from τ ; , At), : (Rt, Scode t+1 ))} iterate transitions St St+1 t+1 , img-to-code); /* (2) Synthetic cross-modal re-labeling of next-state St+1 t+1 π(Simage Scode /* (3) Reasoning generation with free look-ahead to St+1 Rt π(Simage /* Construct SFT pair (Simage , At); (Rt, Scode t+1 ); DWM.append((X, )); t+1 , look-ahead); , At, Simage return DWM 14 */ */ */ */ */ Prompt img-to-code: Mobile GUI Image to Code You are an expert mobile UI developer. Given screenshot of mobile interface, you must first analyze it and then generate clean, responsive HTML code. Your task has TWO steps: 1. REASONING: Analyze the screenshot and plan the HTML structure. 2. HTML GENERATION: Create the HTML code based on your analysis. Focus on these two critical criteria: 1. Each buttons function should be \"inferable\" / \"differentiable\" - users must be able to understand what each button does. 2. Each text content should be well-represented in the HTML output - all visible text must be accurately captured. In your REASONING, address: The overall structure and layout of the screen (header, main content, footer, etc.) Important UI elements and their hierarchy (buttons, text, images, icons, etc.) Which parts of the screen are most important for functionality How to ensure buttons are clearly differentiated and their functions are inferable How to accurately represent all text content Color scheme and visual styling that supports clarity Any interactive elements and their purposes Requirements for HTML: 1. Generate complete, valid HTML5 code. 2. Choose between using inline CSS and utility classes from Bootstrap, Tailwind CSS, or MUI for styling. 3. Use mobile-first design principles matching screenshot dimensions. 4. For images, use inline SVG placeholders with explicit width and height. 5. Make it visually as close to the provided screenshot. 6. Each buttons function must be \"inferable\" / \"differentiable\". 7. All text content from the screenshot must be well-represented. Return ONLY JSON object with this exact structure: { \"reasoning\": \"Your detailed analysis and planning here\", \"html\": \"Your complete HTML code here\" } 15 Split #Trans #Apps Lang Example Apps ANDROIDWORLD 686 18 KAPPS 495 14 EN Productivity: Joplin, Markor, Tasks, Simple Calendar Media: Retro Music, Simple Gallery Navigation: OpenTracks KO Food & Shopping: Baemin, Coupang, Coupang Eats Mobility: Naver Map, Uber Communication: KakaoTalk, Discord, Gmail Table 5. Composition of MWMBENCHs out-of-distribution splits. AndroidWorld primarily consists of open-source productivity apps, while KApps features popular Korean apps with Korean-language interfaces. Both splits cover apps and domains that are underrepresented in our training data. Prompt look-ahead: Look-Ahead Reasoning Synthesis You are GUI Agent. Action: {action} You are given current screenshot state (first image), action and the next state (second image). Action is also visually annotated in the first image 1. Clicks: Red circle with crosshair + yellow center dot 2. Scrolls: Blue line with green start point + red end point or based on direction Generate reasoning on what this next state would look like as if you were only given the current screenshot. Focus only on the changes that can be predicted from the current screenshots. In the reasoning, do not mention the visual annotation of the action or the existence of the ground truth next state. Only generate the reasoning, nothing else. B. Extended Details on MWMBench The composition of our OOD splits is summarized in Tab. 5, and the detailed distribution of transitions across apps and domains is shown in Fig. 7. B.1. MWMBENCH-ANDROIDWORLD MWMBENCH-ANDROIDWORLD was collected by running M3A (Rawles et al., 2025) with Qwen3 VL 235B-A22B as the base policy model on AndroidWorld. The resulting benchmark contains 686 transitions across 88 episodes spanning 18 distinct applications. Deduplication. The M3A agent often undergoes extensive trial-and-error during task execution, repeatedly visiting similar states or performing redundant actions before reaching the goal. This results in trajectories with numerous nearly identical transitions. If used directly for evaluation, such redundancy would cause the benchmark to disproportionately weight certain transitions, skewing the assessment of world model performance. To ensure balanced evaluation across diverse situations, we apply two-stage deduplication procedure to remove redundant transitions. In the first stage, we automatically identify candidate duplicates based on visual similarity. We group transitions by their (app, action) pairs and compute pairwise similarity within each group by comparing both the pre-action screenshot St and post-action screenshot St+1. Transitions with both similarities exceeding 0.997 are clustered via connected components, producing candidate duplicate groups. In the second stage, human annotators manually review each candidate cluster to verify whether the transitions are truly redundant. Only transitions confirmed as duplicates through this manual inspection are removed, with one representative retained per cluster. 16 (a) MWMBENCH-ANDROIDWORLD: 686 transitions across 18 apps (b) MWMBENCH-KAPPS: 495 transitions across 14 apps Figure 7. Distribution of transitions in MWMBenchs OOD splits. Left: per-app transition counts (colors indicate domain). Right: domain-wise distribution. AndroidWorld is dominated by productivity apps (55.4%), while KApps shows more balanced distribution across food & shopping, communication, and productivity domains. This process reduces the dataset from 1,094 to 686 transitions (37% reduction), removing redundant transitions such as repeated app launches and common navigation patterns while preserving task-specific unique transitions. B.2. MWMBENCH-KAPPS MWMBench-KApps was collected manually in-house by technical staff members selected for their proficiency in Korean and English. The statistics for the top 10 most popular apps in Korea were used to determine which apps to collect data from. Both virtual and physical devices were used for collection, as some target apps were not supported on emulators. The collection interface was built using fixed overlay that first records actions, then saves screenshots and accessibility trees. After the state is saved, interface converts the action into Android Virtual Device (AVD) command for execution. For accessibility tree communication, gRPC and HTTP were used for virtual and physical devices, respectively. However, for this work, we only utilize the screenshots and actions. Figure 8 shows the collection interface, and Table 6 shows the action space used for collection. After collection, each episode was manually filtered for action quality, excluding episodes that did not properly progress toward the goal at every step. Out-of-Distribution Setting. MWMBENCH-KAPPS serves as an OOD evaluation set for assessing multilingual generalization capabilities. While our training data (AITW, AndroidControl, AMEX, and GUI Odyssey) and most existing 17 Table 6. Action space used for Kapps data collection. Action Parameters [x, y] [start_x, start_y, velocity, end_x, end_y] click swipe system_button {recent, home, back} set_text long_press wait complete impossible launch_app [x, y], text [x, y] duration comment (optional) comment (optional) package_name world model research are predominantly English-centric, MWMBENCH-KAPPS is entirely Korean-based: all task goals are written in Korean, and 94.5% of transitions contain Korean text in their UI screenshots. Additionally, KApps features popular Korean applications (e.g., Baemin, Coupang, KakaoTalk, Naver Map) that are absent from our English-focused training data. This benchmark thus provides unique testbed for evaluating whether world models can generalize beyond their monolingual training distribution to unseen languages and regional applications. C. Further Details on Experiments Hardware. Experiments were conducted on cluster of up to four H200 nodes. Each node comprises eight NVIDIA H200 GPUs, featuring intra-node communication via NVLink and inter-node connectivity through InfiniBand. Training Settings. We used the same hyperparameters for training both Qwen3 VL 8B and Qwen3 VL 32B. See Tab. 7 for the complete set of hyperparameters. We utilized the training code for finetuning made available by the Qwen team: https://github.com/QwenLM/Qwen3-VL/tree/main/qwen-vl-finetune. We only train the LLM and MLP projector layer. We freeze the vision encoder as unfreezing did not lead to meaningful improvement in performance. Table 7. Training Hyperparameters Hyperparameter Value Batch Size Learning Rate MLP Learning Rate LR Scheduler Warm up Ratio Weight Decay Training Epochs Max Image Pixels Min Image Pixels 64 2e-7 2e-7 Cosine 0.01 0.01 5 4,233,600 3,136 Trainable Components Vision Encoder MLP Projector LLM Frozen Tuned Tuned Evaluation and Inference Settings. We use greedy decoding (temperature = 0) and set the max model length to 16384 such that there is no premature cut-off. World Model Evaluation Prompt. We use the same WM prompt for all models. This prompt was first curated based on zero-shot performance on Qwen3 VL 235B-A22B. We apply model-specific chat templates to ensure consistent formatting across different architectures. Figure 8. Data collection interface software built for KApps. Prompt WM: World Model Evaluation You are an expert mobile UI World Model that can accurately predict the next state given an action. Given screenshot of mobile interface and an action, you must generate clean, responsive HTML code that represents the state of the interface AFTER the action is performed. First generate reasoning about what the next state should look like based on the action. Afterwards, generate the HTML code representing the next state that logically follows the action. You will render this HTML in mobile viewport to see how similar it looks and acts like the mobile screenshot. Requirements: 1. Provide reasoning about what the next state should look like based on the action 2. Generate complete, valid HTML5 code 3. Choose between using inline CSS and utility classes from Bootstrap, Tailwind CSS, or MUI for styling, depending on which option generates the closest code to the screenshot. 4. Use mobile-first design principles matching screenshot dimensions. 5. For images, use inline SVG placeholders with explicit width and height attributes that match the approximate dimensions from the screenshot. Matching the approximate color is also good. 6. Use modern web standards and best practices 7. Return ONLY the HTML code, no explanations or markdown formatting 8. The generated HTML should render properly in mobile viewport. 9. Generated HTML should look like the screen that logically follows the current screen and the action. Action: {action} Output format: Next State Reasoning: <your reasoning about what the next state should look like> HTML: <valid_html_code > Generate the next state reasoning and the next state in html: Metric: Instruction Accuracy. generated by our model of interest, and the prompt IAcc. follows Luo et al. (2025). Instruction Accuracy is obtained by: IAcc. π(St, At, ˆSt+1, IAcc.) where ˆSt+1 is Prompt IAcc.: VLM-as-a-Judge Evaluation of Generated Next State You are an expert in evaluating the performance of mobile emulator. The mobile emulator is designed to navigate the UI change based on human instruction. Inputs: Current UI Screenshot: The present state of the cellphones user interface. Next UI Screenshot: The mobile emulator generated UI indicating the next state of the cellphones user interface based on human instruction. Human instruction: The action applied on the current UI screenshot. Your goal is to determine whether the mobile emulator successfully predicts the next UI image with current information and layout based on the current UI and the user action. Consider these aspects: - Does the generated UI show plausible result of applying the action? - Is the layout and structure consistent with what would happen after the action? - Are interactive elements (buttons, inputs, etc.) in expected states? - Does the content reflect the expected changes from the action? IMPORTANT Format your response into JSON map as shown below: { \"Thoughts\": \"<your thoughts and reasoning process>\", \"Status\": \"success\" or \"failure\" } C.1. Further Details on Scaling Analysis While we only generate up to 240K samples for training, Tab. 8 reports maximum of 3.7 million samples available for training. Dataset Episodes Policy Transitions Available World Model Transitions GUIOdyssey Android Control AitW AMEX 8,334 14,501 707,186 3,046 119,559 73,968 4,232,911 35,661 Total 733,067 4,462,099 Table 8. Maximum existing transitions available for training gWorld 111,225 59,467 3,525,725 32,615 3,729,032 C.2. Further Details on World Model-enhanced Policy Experiments We implement an oracle variant of M3A policy agent (Rawles et al., 2025) so that we can clearly observe potential gains via world modeling. We provide the agent with the ground truth action, current screenshot St, goal G, and history of natural language actions Ht. Given the ground truth action AGT , it first generates 1 alternatives (see alt). The agent then selects the best action from all candidates to progress toward the goal (see select). Accuracy measures how often the agent selects the ground truth. We formalize this procedure in Algorithm 2. This setup isolates the policys selection ability from its generation ability in single-step evaluation. We adopt this setting as most policies we tested failed to show meaningful improvements at higher i.e., often failed to generate at least one correct action among candidates. We note 20 that enabling the policy for effective test-time scaling remains an open challenge and is beyond the scope of this work. Here, we focus on quantifying the improvement gains from using world model as value function. The next baseline augments M3A with value function without the world model. After generating 1 alternatives, each action including the ground truth is passed to the backbone policy model in parallel to judge its validity and assign confidence score (see value-no-wm). The valid action with the highest confidence is selected. We outline this in Algorithm 3. Finally, M3A augmented with WM which generates the next state for each of the actions. Each next state is provided along with its corresponding action to the backbone policy model in parallel to judge validity and assign confidence score (see value-wm). The highest-scored valid action is selected. The full procedure is given in Algorithm 4. Prompt alt: M3A Alternative Action Generation You are an AI agent that can operate an Android phone. Given goal and the current screenshot, suggest alternative actions that could be taken. Goal: {goal} Previous actions: {history} The following action has already been suggested (DO NOT repeat this action): {gt_action} Available action types: TAP: Tap on location. Format: {{\"action_type\": \"TAP\", \"x\": <x>, \"y\": <y>}} SCROLL: Scroll in direction. Format: {{\"action_type\": \"SCROLL\", \"direction\": \"<updownleftright>\"}} TYPE: Type text. Format: {{\"action_type\": \"TYPE\", \"text\": \"<text>\"}} BACK: Press back button. Format: {{\"action_type\": \"BACK\"}} HOME: Press home button. Format: {{\"action_type\": \"HOME\"}} ENTER: Press enter key. Format: {{\"action_type\": \"ENTER\"}} LONG_PRESS: Long press on location. Format: {{\"action_type\": \"LONG_PRESS\", \"x\": <x>, \"y\": <y>}} Coordinates are in range [0, 1000] where (0,0) is top-left and (1000,1000) is bottom-right. Suggest {num_alternatives} DIFFERENT alternative actions that could also make progress toward the goal. Critical requirements: 1. Each alternative MUST be completely different action from the one already suggested above. 2. Do NOT repeat or slightly modify the already-suggested action (e.g., if the suggested action is TAP at (500, 300), do NOT suggest TAP at (500, 301) or nearby coordinates). 3. For TAP actions, choose DIFFERENT UI elements to tap, not the same element with slightly different coordinates. 4. Each alternative should represent meaningfully different approach to achieving the goal. For each action, explain the reasoning behind it. You must output exactly {num_alternatives} actions numbered 1 to {num_alternatives}: {{1: {{Reason: tion_type\":...}}}}}} ..., Action: {{\"action_type\":...}}}}, ..., {num_alternatives}: {{Reason: ..., Action: {{\"acPrompt select: Action Selection from Action Candidates You are an AI agent that can operate an Android phone. Given goal and the current screenshot, select the best action from the candidates below. Goal: {goal} Previous actions: {history} Candidate actions: {candidates} Available action types: TAP: Tap on location. Format: {{\"action_type\": \"TAP\", \"x\": <x>, \"y\": <y>}} SCROLL: Scroll in direction. Format: {{\"action_type\": \"SCROLL\", \"direction\": \"<updownleftright>\"}} TYPE: Type text. Format: {{\"action_type\": \"TYPE\", \"text\": \"<text>\"}} BACK: Press back button. Format: {{\"action_type\": \"BACK\"}} HOME: Press home button. Format: {{\"action_type\": \"HOME\"}} ENTER: Press enter key. Format: {{\"action_type\": \"ENTER\"}} LONG_PRESS: Long press on location. Format: {{\"action_type\": \"LONG_PRESS\", \"x\": <x>, \"y\": <y>}} Coordinates are in range [0, 1000] where (0,0) is top-left and (1000,1000) is bottom-right. Analyze each candidate action carefully based on the screenshot and goal. Select the candidate most likely to help achieve the goal. Output format: Reason: <your analysis of why this candidate is best> Best: <candidate number> 22 Prompt value-no-wm: Value Estimation without World Model You are an AI agent evaluating whether an action will help achieve goal on an Android phone. Goal: {goal} Previous actions: {history} The action being evaluated: {action} Reason for this action: {reason} You are given the CURRENT screenshot showing the UI state before the action. Available action types: TAP: Tap on location. Format: {{\"action_type\": \"TAP\", \"x\": <x>, \"y\": <y>}} SCROLL: Scroll in direction. Format: {{\"action_type\": \"SCROLL\", \"direction\": \"<updownleftright>\"}} TYPE: Type text. Format: {{\"action_type\": \"TYPE\", \"text\": \"<text>\"}} BACK: Press back button. Format: {{\"action_type\": \"BACK\"}} HOME: Press home button. Format: {{\"action_type\": \"HOME\"}} ENTER: Press enter key. Format: {{\"action_type\": \"ENTER\"}} LONG_PRESS: Long press on location. Format: {{\"action_type\": \"LONG_PRESS\", \"x\": <x>, \"y\": <y>}} OPEN_APP: Open an app. Format: {{\"action_type\": \"OPEN_APP\", \"app_name\": \"<name>\"}} Coordinates are in range [0, 1000] where (0,0) is top-left and (1000,1000) is bottom-right. Your task is to judge whether the action is reasonable step toward achieving the goal based on the current UI state. Evaluate based on these criteria: 1. Does the action target the correct UI element or area visible on screen? 2. Is the action type appropriate for the current context? 3. How directly does this action advance the goal vs. being roundabout step? Respond in JSON format: {{\"Reason\": \"Your explanation\", \"Judgement\": \"valid\" or \"invalid\", \"Confidence\": <score>}} IMPORTANT: Use the FULL range of confidence scores to differentiate action quality: 0.91.0: Clearly the optimal action, directly advances the goal 0.70.8: Good action, makes progress but may not be the most efficient path 0.50.6: Acceptable action, loosely related to goal but indirect 0.30.4: Weak action, unlikely to help but not harmful 0.10.2: Poor action, probably wrong target or type Avoid defaulting to 1.0 or 0.9 unless the action is clearly optimal. Be critical and discriminating. 23 Prompt value-wm: Value Estimation With World Model You are an AI agent evaluating whether predicted action will help achieve goal on an Android phone. Goal: {goal} Previous actions: {history} The action being evaluated: {action} Reason for this action: {reason} You will be given two screenshots: 1. BEFORE screenshot: The current UI state before the action 2. AFTER screenshot: The predicted UI state after performing the action Available action types: TAP: Tap on location. Format: {{\"action_type\": \"TAP\", \"x\": <x>, \"y\": <y>}} SCROLL: Scroll in direction. Format: {{\"action_type\": \"SCROLL\", \"direction\": \"<updownleftright>\"}} TYPE: Type text. Format: {{\"action_type\": \"TYPE\", \"text\": \"<text>\"}} BACK: Press back button. Format: {{\"action_type\": \"BACK\"}} HOME: Press home button. Format: {{\"action_type\": \"HOME\"}} ENTER: Press enter key. Format: {{\"action_type\": \"ENTER\"}} LONG_PRESS: Long press on location. Format: {{\"action_type\": \"LONG_PRESS\", \"x\": <x>, \"y\": <y>}} OPEN_APP: Open an app. Format: {{\"action_type\": \"OPEN_APP\", \"app_name\": \"<name>\"}} Coordinates are in range [0, 1000] where (0,0) is top-left and (1000,1000) is bottom-right. Your task is to judge whether the action is reasonable step toward achieving the goal. Evaluate based on this criterion: Does the predicted \"after\" screenshot show expected progress toward the goal? Respond in JSON format: {{\"Reason\": \"Your explanation\", \"Judgement\": \"valid\" or \"invalid\", \"Confidence\": <score>}} IMPORTANT: Use the FULL range of confidence scores to differentiate action quality: 0.91.0: Clearly the optimal action, directly advances the goal 0.70.8: Good action, makes progress but may not be the most efficient path 0.50.6: Acceptable action, loosely related to goal but indirect 0.30.4: Weak action, unlikely to help but not harmful 0.10.2: Poor action, probably wrong target or type Avoid defaulting to 1.0 or 0.9 unless the action is clearly optimal. Be critical and discriminating. D. Extended World Modeling Experiment Results Tab. 9 organizes results for each of the different VLM-as-a-Judges, and each of the vision encoders. Training curves and data scaling analysis is presented in Fig. 9, 10. High inter-judge rank correlation is visualized in Fig. 11 and 12. Additional qualitative results are available in Fig. 13, 14, 15, 16, 17, 18, 19, 20. Fig. 21 is Fig. 4 with Qwen-Image-Edit 20B. E. Limitations and Future Work While gWorld establishes new paradigm for visual world modeling, we identify several limitations inherent to the current approach that pave the way for future research. First, regarding data scale, we currently utilize only 260K training samples out of potential pool of 3.7 million transitions (approximately 7% of available data). Given the predictable power-law scaling demonstrated in Figure 5, future work can Algorithm 2: M3A: Oracle Policy Evaluation Input : Test samples Dtest = {(St, AGT is the goal, and Ht is the action history; policy model π; number of candidates K; prompts alt, select , G, Ht)} where St is the current screenshot, AGT is the ground truth action, t Output :Accuracy (rate of selecting ground truth) correct 0; for each sample (St, AGT , G, Ht) Dtest do }; /* (1) Build candidate set with GT as first candidate {1 : AGT /* (2) Generate 1 alternative actions Aalt π(St, G, Ht, AGT for 2 to do , alt); C[i] Aalt[i 1]; π(St, G, Ht, C, select); /* (3) Policy selects best action from candidates Aselected /* (4) Check if selected action is ground truth = AGT if Aselected correct correct + 1; then */ */ */ */ return correct/Dtest Figure 9. Data scaling analysis. We report average Instruction Accuracy (IAcc.) across the four in-distribution test splits as we scale the repurposed training data from 37K to 240K examples. The results demonstrate strong positive scaling. significantly enhance performance by scaling the training data to utilize the full set of available offline trajectories. Second, there are fundamental limitations to rendering photo-realistic content via web code. For instance, video player displaying complex natural imagery is difficult to reconstruct with high fidelity under the current paradigm. While we posit that this does not significantly impact the semantic utility of mobile GUI world modeling or downstream policy performance, future work may explore hybrid techniques to address these specific visual failure cases. Finally, we aim to extend gWorld beyond the single-frame Markov assumption by incorporating explicit working memory. While our current model demonstrates robust recursive prediction, many GUI environments exhibit long-range temporal dependencies that require context from previous interactions (e.g., maintaining state for shopping basket across multiple pages). Transitioning from single-observation state to memory-augmented framework will be essential for capturing these long-term dependencies, ultimately developing gWorld into realistic simulator capable of supporting long-horizon online reinforcement learning. 25 Figure 10. Data scaling analysis. We report average Instruction Accuracy (IAcc.) in MWMBENCH-ANDROIDWORLD. as we scale the repurposed training data from 37K to 240K examples. The results demonstrate strong positive scaling. Figure 11. Inter-judge agreement on world model instruction-following. Each scatter plot compares Instruction-following Accuracy (IAcc., %) scored by two different VLM-as-a-Judge models (Gemini 3 Flash, GPT-5 Mini, Claude Haiku 4.5) across MWMBENCH datasets (AitW, GUIOdyssey, AndroidControl, AMEX, AndroidWorld, KApps). Each point corresponds to (WM model, dataset) result, colors denote datasets, and the dashed line shows the linear regression fit. Spearmans ρ and Kendalls τ show strong rank and score consistency across judges. Figure 12. Consistency of model rankings across VLM-as-a-Judge choices. Bump chart shows the relative rank (1 = best) of each world model under different judges (Gemini 3 Flash, GPT-5 Mini, Claude Haiku 4.5), where ranks are determined by average IAcc. over MWMBENCH datasets. Rankings remain largely stable across judges, with gWorld consistently achieving top performance compared to other code-generation and image-generation baselines. 26 Figure 13. Additional qualitative example 1. Figure 14. Additional qualitative example 2. 27 Figure 15. Additional qualitative example 3. Figure 16. Additional qualitative example 4. The red marker on the input is for visualization only and was not provided to the model. Action: click at normalized coordinates (802, 394). 28 Figure 17. Additional qualitative example 5. The red marker on the input is for visualization only and was not provided to the model. Action: click at normalized coordinates (913, 143). Figure 18. Additional qualitative example 6 (Android World). The red marker on the input is for visualization only and was not provided to the model. Action: click at normalized coordinates (756, 685). 29 Figure 19. Additional qualitative example 7 (Android World). The red marker on the input is for visualization only and was not provided to the model. Action: click at normalized coordinates (819, 549). Figure 20. Additional qualitative example 8. The red marker on the input is for visualization only and was not provided to the model. Action: TAP at normalized coordinates (857, 421). 30 Algorithm 3: M3A + Value Function (No World Model) Input : Test samples Dtest = {(St, AGT , G, Ht)} where St is the current screenshot, AGT is the ground truth action, is the goal, and Ht is the action history; policy model π; value function ; number of candidates K; prompts alt, value-no-wm t Output :Accuracy (rate of selecting ground truth) correct 0; for each sample (St, AGT , G, Ht) Dtest do }; /* (1) Build candidate set with GT as first candidate {1 : AGT /* (2) Generate 1 alternative actions Aalt π(St, G, Ht, AGT for 2 to do , alt); C[i] Aalt[i 1]; /* (3) Score each candidate using value function (current state only) for each {1, . . . , K} in parallel do (vi, ci) (St, C[i], G, Ht, value-no-wm); /* (4) Select valid action with highest confidence Aselected = AGT if Aselected correct correct + 1; arg maxi:vi=valid ci; then return correct/Dtest */ */ */ */ Input Algorithm 4: M3A + World Model Evaluation : Test samples Dtest = {(St, AGT is the goal, and Ht is the action history; policy model π; world model W; value function ; number of candidates K; prompts alt, value-wm , G, Ht)} where St is the current screenshot, AGT is the ground truth action, Output :Accuracy (rate of selecting ground truth) correct 0; for each sample (St, AGT , G, Ht) Dtest do }; /* (1) Build candidate set with GT as first candidate {1 : AGT /* (2) Generate 1 alternative actions Aalt π(St, G, Ht, AGT for 2 to do , alt); C[i] Aalt[i 1]; /* (3) Predict next state for each candidate using world model for each {1, . . . , K} in parallel do Scode,(i) t+1 W(St, At, C[i]); /* (4) Score each (action, predicted next state) pair for each {1, . . . , K} in parallel do (vi, ci) (St, C[i], Scode,(i) , G, Ht, value-wm); t+1 /* (5) Select valid action with highest confidence Aselected = AGT if Aselected correct correct + 1; arg maxi:vi=valid ci; then return correct/Dtest 31 */ */ */ */ */ Model: Parameter Size: Qwen-I-E 20B Emu3.5 34B Llama 4 109B-A17B 402B-A17B Qwen3 VL 8B 32B 235B-A22B GLM-4.6V 106B gWorld 8B 32B Image-gen Code-gen IAcc. - Gemini 3 Flash (%, ) IAcc. - GPT-5 Mini (%, ) IAcc. - Claude Haiku 4.5 (%, ) Render Fail (%, ) Similarity v1 (%, ) Similarity v2 (%, ) IAcc. - Gemini 3 Flash (%, ) IAcc. - GPT-5 Mini (%, ) IAcc. - Claude Haiku 4.5 (%, ) Render Fail (%, ) Similarity v1 (%, ) Similarity v2 (%, ) IAcc. - Gemini 3 Flash (%, ) IAcc. - GPT-5 Mini (%, ) IAcc. - Claude Haiku 4.5 (%, ) Render Fail (%, ) Similarity v1 (%, ) Similarity v2 (%, ) IAcc. - Gemini 3 Flash (%, ) IAcc. - GPT-5 Mini (%, ) IAcc. - Claude Haiku 4.5 (%, ) Render Fail (%, ) Similarity v1 (%, ) Similarity v2 (%, ) IAcc. - Gemini 3 Flash (%, ) IAcc. - GPT-5 Mini (%, ) IAcc. - Claude Haiku 4.5 (%, ) Render Fail (%, ) Similarity v1 (%, ) Similarity v2 (%, ) IAcc. - Gemini 3 Flash (%, ) IAcc. - GPT-5 Mini (%, ) IAcc. - Claude Haiku 4.5 (%, ) Render Fail (%, ) Similarity v1 (%, ) Similarity v2 (%, ) 10.4 17.8 18.0 70.8 49.4 7.4 17.9 13.8 74.2 53.4 4.2 19.9 11.0 76.4 51.2 3.2 18.2 11.2 77.0 51.8 8.0 15.9 17.6 81.1 54. 8.7 23.0 15.4 83.0 58.9 MWMBench-AitW 33.6 54.2 55.1 4.4 71.4 44.4 MWMBench-GUIOdyssey 37.5 63.2 58.6 1.2 77.6 47.0 36.3 54.0 51.4 9.4 72.9 44. 40.0 67.5 60.0 7.8 79.1 48.9 12.8 31.0 26.4 79.9 57.4 15.5 30.5 31.5 80.7 56.9 MWMBench-AndroidControl 17.4 32.8 32.9 81.2 55.9 12.0 27.4 25.8 84.2 58. 33.4 56.0 62.7 1.0 75.6 47.2 MWMBench-AMEX 33.2 54.9 58.8 0.6 81.8 52.0 45.6 65.8 64.3 8.6 78.6 47.6 51.2 64.7 59.1 12.6 82.7 53.4 15.2 22.8 26.4 33.8 61.5 38. 22.2 31.8 30.6 51.4 59.4 37.2 26.0 35.2 32.1 42.8 63.1 43.6 26.6 35.6 39.0 31.6 70.8 47.6 MWMBench-AndroidWorld (out-of-distribution) 25.8 35.9 25.5 87.3 61.0 32.4 63.9 56.7 2.9 76.2 47. 41.3 63.6 58.0 14.4 76.5 47.0 26.8 34.3 31.2 42.3 60.9 38.8 MWMBench-Korean (out-of-distribution) 10.1 36.6 33.8 83.5 58.9 32.5 59.0 53.7 1.8 71.8 42.7 Table 9. Granular results. 45.7 72.9 61.2 2.2 73.7 43. 24.6 34.9 30.7 38.8 60.9 40.0 35.4 52.9 52.0 11.6 71.1 46.8 45.0 60.7 50.2 16.0 75.0 50.3 45.4 61.0 53.3 13.4 75.3 52.9 51.6 58.8 60.4 3.8 82.0 57.9 44.8 63.1 52.2 13.1 74.2 48. 45.1 61.8 50.5 8.1 74.2 51.4 32.4 39.8 36.1 40.0 75.3 50.5 50.0 61.2 53.0 27.2 81.6 57.7 51.4 55.9 48.3 34.2 80.3 57.3 48.7 54.0 50.8 30.0 83.6 59.8 47.7 59.6 46.1 30.0 77.7 52. 59.2 74.3 59.0 15.4 78.0 56.6 50.1 68.0 64.6 2.4 76.6 52.7 65.6 76.8 62.2 3.8 83.7 61.3 74.6 80.8 67.1 1.4 82.8 60.0 68.9 72.8 66.8 1.2 84.7 61.6 70.7 81.9 69.7 1.9 84.1 60. 52.1 67.7 52.3 4.4 74.7 52.7 68.2 76.0 62.2 0.8 77.6 54.9 79.0 85.0 67.6 1.2 83.9 62.7 81.2 84.6 69.3 2.6 83.2 62.4 84.3 86.8 76.8 0.8 84.8 63.8 72.8 83.1 69.1 2.3 81.2 57. 65.5 75.4 61.4 0.8 76.1 56.0 69.6 81.2 64.2 0.6 78.6 55.9 87.2 87.4 69.8 0.8 84.7 62.6 86.8 85.4 76.5 0.8 84.8 63.5 90.6 87.2 80.4 0.4 85.6 65.1 79.9 85.9 73.9 0.4 83.1 60. 77.2 83.6 66.3 0.6 76.3 56.0 32 Figure 21. Same analysis as Figure 4 (bottom) with Qwen-Image-Edit 20B (Qwen-I-E). Qwen-Image-Edit 20B exhibits Sim( ˆSt+1, St+1) Sim(St, St+1), indicating that its outputs are nearly identical to the inputs (St ˆSt+1) regardless of the required action."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "Trillion Labs"
    ]
}