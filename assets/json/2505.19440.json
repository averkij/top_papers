{
    "paper_title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models",
    "authors": [
        "Shashata Sawmya",
        "Micah Adler",
        "Nir Shavit"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, we identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 4 4 9 1 . 5 0 5 2 : r The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models Shashata Sawmya1 Micah Adler1 Nir Shavit1,2 1Massachusetts Institute of Technology 2Red Hat, Inc. {shashata, micah, shanir}@mit.edu"
        },
        {
            "title": "Abstract",
            "content": "This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, we identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) and multimodal Vision-Language Models (VLMs) have become the standard computational tools across numerous applications, ranging from natural language understanding and generation to complex multimodal reasoning tasks. Their extensive deployment in both research and industry highlights their versatility and efficacy in handling broad spectrum of computational problems. Despite this widespread usage, the internal mechanisms by which these models achieve their impressive performance remain largely opaque, resulting in their characterization as complex, \"black-box\" systems [1]. Mechanistic interpretability has emerged as promising research area aimed at dissecting the internal functioning of neural networks [2]. By systematically identifying and describing the internal structures, this field seeks to uncover specific computational componentssuch as feature-level and circuit-level elementsthat correspond to interpretable, human-understandable concepts. Among the methodological tools available for mechanistic interpretability, sparse autoencoders (SAEs) have proven particularly useful. SAEs impose explicit sparsity constraints on learned representations, thereby facilitating the extraction and interpretation of semantically meaningful features embedded within neural activations. Previous studies have primarily quantified emergent behaviour by tracking aggregate metricstask accuracy, generalization scores, or other benchmarks collected as models grow in size and training duration [35]. While informative, such evaluations reveal little about how the underlying computations themselves change over training or scaling . To address this gap we adopt an interpretability-oriented methodology that probes the formation of semantically coherent features inside LLMs. Our analysis targets three complementary axes: time (checkpoints along the training trajectory), space (positions in the transformer stack), and scale (parameter count). The prior hypothesis is that model is comparatively un-knowledgeable at its first gradient updates, within its earliest blocks, and when small in size; conversely, the density and specificity of category-aligned features should increase as optimization proceeds, depth grows, Preprint. Under review. Figure 1: Axes of our emergent-knowledge probe. We track how interpretable, categorical features surface in language model over time (training checkpoints), space (depth across transformer blocks), and scale (parameter count), progressing from sparse or absent concepts (grey bulbs) to rich representations (yellow bulbs) and parameters multiply [610]. Using sparse autoencoders we map when and where such features emerge, thereby charting the progressive structuring of internal representations in LLMs. The primary contributions of this study are two-fold. First, we demonstrate the application of sparse autoencoders as mechanistic interpretability tools for uncovering emergent interpretable features within the residual streams of LLMs across three different axes. Second, we conduct an extensive and fine-grained analysis of feature emergence and evolution in those dimensions across nine broad topical domains that span both the sciences and the arts. By probing into the feature dynamics associated with each topic, our analysis identifies the timing, location, and scale at which various interpretable features arise and mature within LLMs. The remainder of this paper is organized as follows. Section 2 and 3 presents the sparse autoencoder and autointerpretability methodology, experimental datasets, and model configurations. Section 4 examines the emergence of interpretable features along the training trajectory. Section 5 analyzes these features across the transformer stack, while Section 6 investigates their evolution under parameter scaling. Section 7 situates our work within the existing literature, and Section 8 offers concluding remarks."
        },
        {
            "title": "2 Background and Methods",
            "content": "2.1 Model and Data Datasets. All experiments operate on the public MMLU test set (14 042 multiple-choice questions drawn from 57 academic subjects) [11] and its harder extension MMLU-PRO (12 032 questions covering 14 broad categories) [12]. Together the two benchmarks probe models multitask generalknowledge competence over disciplines that span the sciences and the arts, e.g. Physics, Chemistry, Economics, Philosophy, History, and Ethics. For every item the question stem and all candidate answers are concatenated into single stringomitting the correct letterand passed through language model once. The final-token hidden state is retained as the sample embedding used throughout this study. Pythia checkpoints for temporal, spatial, and scale analyses. We adopt the PYTHIA suite of autoregressive transformers as fully open substrate for mechanistic analysis [13]. For the temporal investigation we track the 12-Billion-parameter model across 25 publicly released training checkpoints: {0, 1, 2, 4, 8, 16, 256, 512, 1000, 5000, 10000, 20000, . . . , 140000, 143000}, where the first ten steps give fine-grained coverage of early training and the remainder are spaced every 10 updates up to near-convergence. For the scale study we analyse all ten model sizes in the suite 14M, 31M, 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B parameterseach at its final checkpoint. For the spatial study we focus on the 12 model and extract embeddings from all 36 transformer blocks, allowing feature emergence to be mapped layer by layer. 2.2 Sparse Autoencoders and AutoInterpretability 2.2.1 Architecture and Objective Let Rd be residualstream activation drawn from transformer block. sparse autoencoder maps to an overcomplete latent space of width > in order to partition the original representation into finer-grained, potentially disentangled features. Formally, the encoder Rmd produces = x, (1) after which hard top-k operator, = topk(z, k), retains the largest-magnitude coordinates and zeros the rest. The decoder Rdm reconstructs ˆx = z, LSAE = ˆx2 2. (2) We adopt OpenAIs implementation of the top-k SAE [14], which applies the masking step on every forward pass without an explicit sparsity penalty. The strict cap of active units accelerates dictionary formation: each optimisation step forces exactly basis vectors to participate, so the overcomplete matrix is populated with meaningful directions more quickly than in L1or KL-regularised variants, where sparsity emerges only through gradual weight adjustment. In the standard top-k sparse auto-encoder only the strongest latents update, so unused dictionary columns die. The OpenAI implementation adds two small tricks: multi-k, which reruns the same forward pass with extra seeds of multi-k units receive gradient, and aux-k, where each silent neuron keeps miss counter that, once past threshold, promotes up to kaux long-inactive units into the mask for one update before resetting. Both fixes periodically wake dormant features, cut dead-unit rates, and expand semantic coverage without changing the hidden width or inference cost. 2.2.2 AutoInterp Pipeline We adopt the external AUTOINTERP framework to generate and vet natural-language label for every latent neuron in the SAE [15]; our own contribution is limited to downstream use of the verified labels. Label generation. For neuron AUTOINTERP constructs two equal-sized example pools , S+ D, S+ = j = nlabel, where S+ contains data-points on which the neuron fires (posttop-k activation > 0) and contains the same on which it is silent. These text snippets are supplied to teacher LLM with the instruction: Describe the concept present in the first set but absent in the second. The LLMs response is stored as the provisional label ℓj. Verification via classifier metrics. fresh, disjoint pair = + , , + = nverify, is drawn using the same activation criterion. AUTOINTERP asks the teacher LLM whether ℓj applies to each element of + . Predictions are compared with ground truth (activation vs. no activation) to compute accuracy, precision, recall, and F1-score. Neuron is deemed interpretable if its F1-score exceeds preset threshold τF1; otherwise it is excluded from further analysis. This automatic labelverify loop scales linearly with the number of neurons while requiring only two small sample budgets, nlabel for hypothesis formation and nverify for metric-based validation. 2.3 Design Choices Modified AutoInterp. We follow the AUTOINTERP framework (Section 2.2.2) with two pragmatic deviations. First, the label prompt is constructed from the top-activation set only; we use nlabel = 10 examples and omit the non-activating counterparts. Second, the verification prompt retains the original balanced split, drawing nverify = 5 activating and 5 non-activating samples to compute classifier metrics (accuracy, precision, recall, F1) for each candidate label. Figure 2: Hyperparameter sweep for sparseautoencoder interpretability. Left: mean F1-score as the activation budget varies with width fixed at = 256; right: mean F1-score as the latent width varies with = 1. The optimal setting for our data is = 1, = 512, which maximises mean F1. Selecting the activation budget k. With the hidden dimension fixed at = 256 we tested {1, 2, 4, 8}. The mean F1-score across all labelled neurons decreased monotonically with larger k, and = 1 achieved the highest score (Figure 2, left). We therefore set = 1 for the remainder of the study. Selecting the latent width h. Holding = 1 constant, we varied the latent dimensionality {32, 64, 128, 256, 512, 1024}. The mean F1-score peaked at = 512 (Figure 2, right). Dead-latent incidence was non-negligible; the number of neurons that ever activated during training was {32, 54, 120, 170, 211, 122} for the six values, respectively. Balancing interpretability (via F1) against parameter count, we chose = 512. Why d. The residual-stream dimensionality of the Pythia-12B model is = 5120. We intentionally select for two reasons. First, the combined benchmark contains only 57 + 14 nominal subject labels, several of which overlap (e.g. high_school_physics vs. college_physics); the effective concept inventory is therefore far smaller than d. Second, this study focuses on the emergence of coarse interpretable features rather than full disentanglement of polysemantic directions, making an overcomplete basis of size = 512 sufficient for our analysis goals."
        },
        {
            "title": "3 EyeSee: A Framework for Probing Categorical Concepts",
            "content": "Modern benchmarks such as MMLU and MMLU-PRO are organised around textbook disciplines (e.g. Physics, History). If an LLM truly internalises these domains, one would expect dedicated latent directions to emerge that activate whenever the input concerns given subject. Our goal is therefore to distil the full set of SAE neurons down to those that reliably represent such high-level categories and to track their behaviour across time, space, and scale. High-fidelity neuron pool. Running AUTOINTERP on the selected SAE (h = 512, = 1) yields verification F1-score for every latent label. We keep only neurons whose score exceeds 0.9, forming trusted set Nhi. Query-driven concept matching. We begin with the free-text neuron labels associated with the trusted high-fidelity pool Nhi and encode each label once with the sentence-embedding model all-mpnet-base-v2, thereby building lightweight vector database. Whenever query subject (e.g. Physics, History) is posed, we encode that query with the same model and retrieve the neuron-label vectors that lie closest to it in the embedding space. Cosine similarity acts as the nearest-neighbour"
        },
        {
            "title": "AutoInterp Label",
            "content": "Cos. Sim. F1 38 295 269 194 125 38 78 195 12 195 400 12 367 55 218 4 58 446 510 57 305 350 285 446 367 456 55 Natural Phenomena and Processes Genetic Variation Source Human-related Processes Paper-related Chemistry Iron and Silver Chemistry Natural Phenomena and Processes Newtons Laws Applications Mathematical Problem Solving Real-world Mathematical Applications Mathematical Problem Solving Logical and Mathematical Concepts Real-world Mathematical Applications Business and Economic Dynamics Human Behavior and Decision-Making Economic Growth Factors Social Dynamics and Influence Comparative Analysis Ethical and Cultural Positions Legal Decision-Making Criteria Warranty Types in Law Decision-Making in Institutions Ethical and Philosophical Concepts Socratic Philosophy Concepts Ethical and Cultural Positions Business and Economic Dynamics Distribution Channels Human Behavior and Decision-Making 0.377 0.366 0.347 0.491 0.406 0.385 0.513 0.425 0.401 0.707 0.590 0.549 0.600 0.485 0.480 0.387 0.322 0.320 0.454 0.377 0.345 0.606 0.477 0.462 0.548 0.419 0.314 0.91 1.00 0.91 1.00 1.00 0.91 1.00 1.00 1.00 1.00 0.91 1.00 1.00 0.91 1.00 1.00 0.91 1.00 1.00 1.00 1.00 0.91 1.00 1.00 1.00 1.00 0. Table 1: Top three high-fidelity concept neurons per subject, selected by cosine similarity 0.3 between the subject name and the AutoInterp label in MPNet embedding space. The additional F1 column reports the verification fidelity for each neuron. Complete ranked lists are provided in Appendix A.1. criterion that links query subjects to the neurons whose labels express the most semantically aligned concepts. The precise formulation is given below. Denote by φ the all-mpnet-base-v2 embedding function. For each neuron Nhi we compute vj = φ(ℓj). Given subject with embedding uq = φ(q), the cosine similarity is sj,q = vj uq2 vj2 . (3) Fixing threshold τ = 0.3, the candidate concept set for is Nq = { Nhi : sj,q τ }. Neurons in Nq are ranked by sj,q. Table 1 reports the three highest-similarity neurons per subject for illustration; complete ranked lists appear in Appendix A.1. These subject-aligned neurons serve as probes when analysing feature emergence along the temporal, spatial, and scaling dimensions in subsequent sections."
        },
        {
            "title": "4 Temporal Emergence of Categorical Knowledge in PYTHIA-12B",
            "content": "Using the SAE trained at the last training checkpoint, we ask the following question, What concepts in the form of feature neurons are activated by LM embeddings curated from other gradient steps? 5 Figure 3: Activation patterns of categorical concepts in 12B-parameter language model across training checkpoints. The left panel illustrates the global activation trajectory, while panels on the right display domain-specific emergence patterns, highlighting distinct activation timings for various knowledge concepts. Figure 3 tracks the percentage of concepts that become active as the model traverses its training trajectory (horizontal axis is log-scaled steps). We begin with the global curve (far-left panel) and then drill down into ten representative knowledge areas (right grid). Global pattern. For the first 103 optimization steps fewer than 3% of concepts are active. first increase of +19.4 percentage points (pp) appears at 5 000 steps. Two larger increments follow: 1. 10 000 20 000 steps: +17.5 pp, 2. 30 000 40 000 steps: +55.9 pp, following 31.8 pp dip in the 20 000 30 000 interval. This dip may reflect either (i) re-organisation of feature representations or (ii) temporary reduction in gradient-driven optimization efficacy before training resumes. After 40 000 steps the curve continues to rise and exceeds 99% by the final checkpoint at 143 000 steps. Domain-specific activation patterns. Inspection of Figure 3 reveals two broad temporal patterns. Early-onset domains (Physics, Mathematics, Economics, Law, Philosophy). These subjects register non-zero activations from the very first optimisation steps and rise gradually, then surge after 3 104 steps to exceed 60 90%. Their early presence hints at the high frequency of numerical, symbolic, and formal language in the training text corpus. Late-onset domains (History, Biology, Chemistry, Business). Activations remain at 0% until roughly 104 steps, after which they climb steeplyoften in single burstto reach near-saturation between 3 104 and 6 104 steps. These topics appear to depend on higher-level contextual structures that only stabilise once lower-level patterns have been learned. By the final checkpoint, all nine subjects approach full activation, indicating progressivenot instantaneousaccumulation of domain knowledge during pre-training."
        },
        {
            "title": "5 Analysis of Representation Space Across Model Layers",
            "content": "Figure 4 illustrates the internal representational dynamics of the model using global cosine similarity between layers embeddings (left panel) and sparse-autoencoder (SAE) feature-activation probes trained on selected layers (right panel). The cosine similarity heatmap identifies three distinct representational blocks across the models depth: the input-like block (Layers 13), closely aligned with the token-embedding space; the processing core (Layers 435), characterized by internal coherence yet significantly different from both input and output spaces; and the output block (Layers 36), realigning representations toward the prediction task. Figure 4: Cosine similarity (left) reveals three macro blocks (embedding, processing core, output), while SAE probes (right) show that feature directions are highly local in depthwith striking echo between the first and last layersindicating that the network temporarily hides early lexical axes during computation before restoring them for final prediction. Complementing this global perspective, SAE probes trained on individual layers offer localized insights by highlighting specific representational directions active at different depths. Notably, earlylayer SAEs (Layers 0 and 2) exhibit high initial activations that sharply decline across intermediate layers before partially re-emerging at later layers, particularly at the final output stage. This reactivation underscores semantic linkage, suggesting that early semantic and lexical features, initially presumed to be transient, actually reappear strategically at later stages. Mid-layer SAEs (Layers 10 and 15) reveal strongly localized activations around their training layers, emphasizing transient, depth-specific representational roles. In contrast, upper-layer SAE (Layer 30) captures broader, sustained activations at higher layers, indicating stable high-level representations crucial for model predictions. The observed re-emergence of early-layer features in later layers challenges the initial hypothesis of spatially \"un-knowledgeable\" representations, demonstrating complex semantic continuity between early and late stages. It can happen for number of reasons such as early and late layers capturing token specific details, whereas the processing core is distilling and For brevity and clarity, detailed per-concept analyses like section 4 is reported in the Appendix."
        },
        {
            "title": "6 Feature Emergence at Scale",
            "content": "6.1 Crossscale alignment To embed all checkpoints in common feature space we apply an orthogonal Procrustes transformation [16, 17]. For each model with hidden-state width dm and activation matrix Xm RN dm 7 we solve = arg min O(dm,5120) (cid:13) (cid:13)XmW X12B (cid:13) (cid:13)F , (4) where X12B RN 5120 is the reference matrix from the 12-Billion-parameter checkpoint. Zeropadding each Xm to 5120 dimensions would introduce empty coordinates and bias similarity metrics; solving (4) keeps the full rank of every smaller model while rotating it into the reference basis. The optimization is applied to activations of = 26,074 evaluation sequences for checkpoints at 14 M, 31 M, 70 M, 160 M, 410 M, 1 B, 1.4 B, 2.8 B, and 6.9 parameters, whose hidden widths are {128, 256, 512, 768, 1024, 2048, 2560, 4096}; the 12 checkpoint supplies the 5120-dimensional reference space. After alignment each projected matrix XmW shares this basis, allowing direct comparison of feature activations.We assess the fidelity of every projection with two geometrypreservation scores: (i) linear CKA, which compares dot-product structure, and (ii) pairwise cosine matrix correlation, which preserves local angular relationships. Both metrics are reported for all scale points in the appendix. High values in both panels indicate that the Procrustes rotation maintains the global and local geometry of the original activations. 6.2 Scalewise Concept Activation Figure 5 plots the percentage of concepts activated as function of model size (log-scaled parameter count, left) together with perdomain traces (right). Domain definitions follow the EYESEE taxonomy introduced in Section 3. Global pattern. Models below 200 parameters activate fewer than 5% of the labelled concepts. single transitionbetween the 160 and 410 checkpointsraises the activation rate by +92.9 pp to 95%. Beyond this point activation saturates, peaking at 2.8 parameters and remaining above 98% for all larger scales considered. Figure 5: Concept-activation saturation with model scale. Left: percentage of all concepts or features which is activated for each Pythia checkpoint from 14 to 12 parameters (log scale). single inflection between the 160 and 410 models raises activation from < 5% to 95%, after which the curve plateaus. Right: per-domain activation profiles show similar critical points for most areas, while Business concepts rise more gradually. Domain-level thresholds. Most subject areas share the same critical jump at 410 M: STEM: Biology, Chemistry, Physics, and Mathematics all move from 10% at 160 to 90% activation at 410 M; minor gains follow up to 1 B, after which they saturate. Social sciences: Economics and Law rise similarly (+72 pp and +91.7 pp respectively) at"
        },
        {
            "title": "410 M, reaching full activation by 1 B.",
            "content": "History: shifts directly from 0 Philosophy: increases by +82.8 pp at 410 and stabilises thereafter. Business: diverges from the pattern: minimal activation already appears at 1431 M, oscillates at intermediate scales, and only reaches stable activation ( 90%) once the model exceeds 1 parameters. Interpretation. The uniform 410 threshold suggests capacity requirement for storing the categorical concepts defined in EYESEE. Smaller models allocate parameters to high-frequency surface statistics but cannot sustain the richer feature subspaces captured by our activation metric. Business concepts appear earlier, possibly due to the higher lexical frequency of business-related terms in the pre-training corpus, but still require larger scales for consistent coverage."
        },
        {
            "title": "7 Related Work",
            "content": "Emergence has mostly been studied along single axis. Time. Checkpoint-level analyses follow when circuits appear or phase-shift (e.g., induction heads [18]; feature-coherence phases [10]). Depth. Layer probes reveal lexicalsyntacticsemantic hierarchy [7] and show concept complexity rising toward upper layers [8]; recent alignment methods match features across neighbouring layers [6]. Scale. Parameter-sweep studies document capability jumps at size thresholds [4, 3] and find that larger models contain more monosemantic features [19]. Our contribution. We jointly track the same sparse-auto-encoder features across time, depth and scale in single model family, revealing cross-axis interactions invisible to single-axis work."
        },
        {
            "title": "8 Conclusion",
            "content": "In this study, we investigated the emergence and evolution of interpretable categorical features within large language models (LLMs) across the complementary axes of time (training checkpoints), space (positions in the transformer stack), and scale (parameter count). Using sparse autoencoders as mechanistic interpretability tools, we demonstrated their effectiveness in identifying semantically meaningful features within model activations. Our fine-grained analysis across domains revealed structured and progressive activation pattern, where different knowledge areas emerge and stabilize at distinct points during training, varying both temporally and by model scale. Additionally, we observed that features identified in early layers of the model often re-emerge at later stages, challenging the hypothesis of spatially uniform \"unknowledgeable\" representations. Despite the detailed observations provided, this work primarily remains descriptive. One limitation is that we did not conduct finer-grained analysis to elucidate the underlying reasons for these emergent patterns, such as the specific contributions of training data distributions or internal network circuitry to the observed feature activations. Additionally, our choice of sparse autoencoders and specific concept matching criteria impose methodological constraints, potentially missing other important feature dynamics. These aspects represent key avenues for future research. On broader, philosophical note, our findings affirm the common hypothesis that knowledge indeed emerges at particular points in time and scale within LLMs. However, the dynamics across the spatial dimensionwhere semantic features appear transiently, disappear, and then re-emergehighlight more nuanced reality. This indicates that while knowledge acquisition aligns well with intuitive expectations temporally and at scale, its spatial organization within neural architectures may not conform to straightforward hypotheses, underscoring the complexity inherent in interpreting neural representation spaces."
        },
        {
            "title": "References",
            "content": "[1] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [2] Chris Olah, Nick Cammarata, Shan Carter, Gabriel Voss, Ludwig Schubert, Adam Rabinowitz, and Jared Kaplan. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill. 00024. [3] Jason Wei, Yi Tay, Rishi Bommasani, and et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [4] Jared Kaplan, Sam McCandlish, Tom Henighan, and et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [5] Tom B. Brown, Benjamin Mann, Nick Ryder, and et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020. [6] Nikita Balagansky, Ian Maksimov, and Daniil Gavrilov. Mechanistic permutability: Match features across layers. arXiv preprint arXiv:2410.07656, 2025. doi: 10.48550/arXiv.2410.07656. [7] Ganesh Jawahar, Benoit Sagot, and Djamé Seddah. layer-wise analysis of transformer representations. In Proceedings of ACL 2019, 2019. [8] Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, and Yongfeng Zhang. Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers? arXiv preprint arXiv:2404.07066, 2024. doi: 10.48550/arXiv.2404.07066. [9] Glenn Conmy, Qiyuan Sun, Liang Feng, and et al. Autointerp: Automated mechanistic interpretability for language models. arXiv preprint arXiv:2407.06543, 2024. [10] Yang Xu, Yi Wang, and Hao Wang. Tracking the feature dynamics in llm training: mechanistic study. arXiv preprint arXiv:2412.17626, 2024. doi: 10.48550/arXiv.2412.17626. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, and et al. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2021. [12] Jian Sun, Siyu Wang, Xiaoyang Zhu, and et al. Mmlu-pro: more challenging benchmark for language model evaluation. arXiv preprint arXiv:2310.15420, 2023. [13] Stella Biderman, Sidney Black, Jonas Mueller, Jason Phang, Yada Pruksachatkun, and et al. Pythia: suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023. [14] Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. [15] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. [16] Peter H. Schönemann. generalized solution of the orthogonal procrustes problem. Psychometrika, 31:110, 1966. [17] Sam Smith and Nicholas B. Turk-Browne. Linking the deep neural network model and brain using alignment methods. Nature Neuroscience, 2019. [18] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context Learning and Induction Heads. arXiv preprint arXiv:2209.11895, 2022. doi: 10.48550/arXiv.2209.11895. [19] Samuel Templeton, Oliver Tworkowski, Neel Nanda, and et al. Scaling monosemanticity: Extracting interpretable features from claude. arXiv preprint arXiv:2402.01232, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Complete ranked list of different EyeSee concepts Table 2: Complete catalogue of subject-aligned neurons obtained from the = 512, = 1 SAE. Cosine similarity is computed between the MPNet embedding of the AutoInterp label and the subject query; F1 is the verification fidelity. Data source: AutoInterp output. Subject"
        },
        {
            "title": "Chemistry",
            "content": "Physics Mathematics Economics Neuron ID AutoInterp Label Cos. Sim. F1 38 295 269 511 386 47 404 194 125 38 483 126 78 195 12 403 484 38 341 400 421 183 291 135 282 324 483 473 392 195 400 12 130 183 484 324 58 392 38 234 421 377 276 367 Natural Phenomena and Processes Genetic Variation Source Human-related Processes Sexual Dimorphism Selection Life and Development Concepts Genetic and Sensory Differences Fatty Acid Transport Paper-related Chemistry Iron and Silver Chemistry Natural Phenomena and Processes Water-related Thermodynamics Free Radicals and Psychoanalysis Newtons Laws Applications Mathematical Problem Solving Real-world Mathematical Applications Light-related Phenomena Numerical Problem Solving Natural Phenomena and Processes Airflow and Heat Transfer Logical and Mathematical Concepts Human Perception and Interaction Expressing Quantities Mathematically Mechanical Design Calculations Passive Transport Heat Transfer and Efficiency Numerical Computation Problems Water-related Thermodynamics Electron and Electromagnetic Concepts Performance and Analysis Mathematical Problem Solving Logical and Mathematical Concepts Real-world Mathematical Applications Simple Arithmetic Problems Expressing Quantities Mathematically Numerical Problem Solving Numerical Computation Problems Comparative Analysis Performance and Analysis Natural Phenomena and Processes Conditional Reasoning Human Perception and Interaction Complex Procedural Knowledge Logical Reasoning in Statements Business and Economic Dynamics Human Behavior and Decision-Making 0.377 0.366 0.347 0.342 0.339 0.331 0. 0.491 0.406 0.385 0.335 0.302 0.513 0.425 0.401 0.384 0.379 0.368 0.363 0.346 0.336 0.323 0.318 0.317 0.312 0.312 0.309 0.306 0.302 0.707 0.590 0.549 0.528 0.492 0.450 0.377 0.324 0.319 0.316 0.311 0.306 0.304 0.301 0.600 0.485 0.91 1.00 0.91 1.00 1.00 1.00 1.00 1.00 1.00 0.91 1.00 1. 1.00 1.00 1.00 1.00 0.91 0.91 1.00 0.91 1.00 0.91 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.91 1.00 1.00 0.91 0.91 1.00 0.91 1.00 0.91 0.91 1.00 1.00 0.91 1.00 0.91 Continued on next page 11 Subject Neuron ID AutoInterp Label Cos. Sim. F1 Table 2 continued from previous page 218 4 305 23 269 457 195 492 294 264 456 446 488 58 400 354 386 350 370 245 61 421 4 58 446 510 57 305 50 446 4 105 440 23 187 400 425 350 285 446 187 23 50 488 496 400 4 55 305 69 245 457 Economic Growth Factors Social Dynamics and Influence Decision-Making in Institutions Environmental Ethics Human-related Processes Mills Utilitarian Philosophy Mathematical Problem Solving Interest Group Influence Long-term Consequences Cost and Tax Analysis Distribution Channels Ethical and Cultural Positions Psychological Concepts and Ethics Comparative Analysis Logical and Mathematical Concepts Contrast and Comparison Life and Development Concepts Ethical and Philosophical Concepts Family-related Decision Making Critique of Consequentialism Decision-Making Scenarios Human Perception and Interaction Comparative Analysis Questions Social Dynamics and Influence Comparative Analysis Ethical and Cultural Positions Legal Decision-Making Criteria Warranty Types in Law Decision-Making in Institutions Ethical and Moral Concepts Ethical and Cultural Positions Social Dynamics and Influence Land and Property Rights Warrantless Searches and Privacy Environmental Ethics Virtue Ethics and Morality Logical and Mathematical Concepts Congressional Powers and Limitations Ethical and Philosophical Concepts Socratic Philosophy Concepts Ethical and Cultural Positions Virtue Ethics and Morality Environmental Ethics Ethical and Moral Concepts Psychological Concepts and Ethics Kantian Ethics Principles Logical and Mathematical Concepts Social Dynamics and Influence Human Behavior and Decision-Making Decision-Making in Institutions Ethical Debates on Euthanasia Critique of Consequentialism Mills Utilitarian Philosophy 0.480 0.476 0.418 0.405 0.371 0.369 0.368 0.357 0.349 0.348 0.346 0.344 0.342 0.340 0.322 0.319 0.318 0.315 0.312 0.309 0.306 0.303 0.302 0.387 0.322 0.320 0.454 0.377 0.345 0.341 0.337 0.324 0.324 0.323 0.321 0.314 0.311 0.304 0.606 0.477 0.462 0.453 0.453 0.452 0.428 0.397 0.391 0.387 0.387 0.384 0.379 0.375 0.361 1.00 1.00 1.00 1.00 0.91 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.91 0.91 0.91 1.00 0.91 1.00 1.00 0.91 1.00 1. 1.00 0.91 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.91 1.00 0.91 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.91 1.00 0.91 1.00 1.00 1.00 1.00 Continued on next page 12 History Law Philosophy Subject Neuron ID AutoInterp Label Cos. Sim. F1 Table 2 continued from previous page 386 18 269 360 421 58 306 195 276 298 157 354 98 104 367 456 55 269 492 4 Life and Development Concepts Moral Complexity in Abortion Human-related Processes Evidence-Based Reasoning Human Perception and Interaction Comparative Analysis Self-related Psychological Concepts Mathematical Problem Solving Logical Reasoning in Statements Comparative Analysis Questions Ethical Dilemmas in Abortion Contrast and Comparison Carl Jung Concepts Ethical Dilemmas in Therapy Business and Economic Dynamics Distribution Channels Human Behavior and Decision-Making Human-related Processes Interest Group Influence Social Dynamics and Influence 0.359 0.357 0.342 0.340 0.333 0.327 0.326 0.323 0.322 0.321 0.319 0.313 0.305 0.303 0.548 0.419 0.314 0.312 0.310 0.308 1.00 1.00 0.91 0.91 1.00 0.91 1.00 1.00 0.91 1.00 1.00 0.91 1.00 1.00 1.00 1.00 0.91 0.91 1.00 1."
        },
        {
            "title": "Business",
            "content": "13 A.2 EYESEE analysis for space Across depth the subject-aligned neurons exhibit the same disappear-and-return motif seen in our global SAE study: most topics fire strongly in the first three blocks, fall almost completely silent throughout the mid-stack, and then re-emerge when the Layer-36 SAE is probed. However, the precise silence window and reactivation point vary by domaine.g., History and Business neurons vanish after Block 3 but resurface sharply at the final block, whereas Philosophy keeps faint 34 % signal until about Block 11 before dropping off and later returning. These differences suggest that each subjects semantic cues are distilled and re-inserted on slightly different schedules, yet the overarching early-hide-late-recall pattern remains consistent. Figure 6: Percentage of high-fidelity neurons (F1 0.9) that fire for each subject in every transformer block of the 12-B Pythia model. Values are averaged over the combined MMLU + MMLU-PRO prompt set. 14 A.3 Performance of Procrustes Rotation Alignment Figure 7 quantifies how closely each checkpoint matches the 12-B reference after Procrustes rotation. The left panel shows that linear CKA stays above 0.90 even for the smallest 14-M model and increases monotonically to 0.99 at 6.9 B, indicating strong preservation of the global dot-product structure across scale. The right panel plots the pairwise cosine matrix correlation, which starts lower (0.34 for 14 M) because local neighbourhood geometry differs more in tiny models, but rises sharply from the 410-M checkpoint onward and exceeds 0.95 for all models with 2.8 parameters. Together, the two metrics confirm that the orthogonal alignment retains both coarse and fine-grained geometry, with local agreement improving consistently as model size grows. Figure 7: Alignment quality across model scale. Left: linear CKA between projected activations XmW and the reference X12B. Right: pairwise cosine matrix correlation for the same pairs."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Red Hat, Inc."
    ]
}