{
    "paper_title": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement",
    "authors": [
        "Gabriele Sarti",
        "Vilém Zouhar",
        "Malvina Nissim",
        "Arianna Bisazza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices."
        },
        {
            "title": "Start",
            "content": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement Gabriele Sarti1 Vilém Zouhar2 Malvina Nissim1 Arianna Bisazza1 1CLCG, University of Groningen 2ETH Zurich {g.sarti, a.bisazza}@rug.nl 5 2 0 M 9 2 ] . [ 1 3 8 1 3 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices."
        },
        {
            "title": "Introduction",
            "content": "Word-level error spans are widely used in machine translation (MT) evaluation to obtain robust and fine-grained estimates of translation quality (Lommel et al., 2014; Freitag et al., 2021a,b; Kocmi et al., 2024b). Due to the cost of manual annotation, word-level quality estimation (WQE) was proposed for assisting in annotating error spans over MT outputs (Zouhar et al., 2025). Modern WQE approaches generally rely on costly inference with large language models (LLMs) or ad-hoc training with large amounts of human-annotated texts (Fernandes et al., 2023; Kocmi and Federmann, 2023; Guerreiro et al., 2024), making them impractical for less resourced settings (Zouhar et al., 2024). To improve the efficiency of MT quality assessment, several works explored the use of signals derived from the internals of neural MT systems Figure 1: Example of GermanEnglish translation with two sets of human word-level error span annotations and two examples of continuous and binary WQE metrics. (Fomicheva et al., 2020b, 2021; Leiter et al., 2024), for identifying problems in MT outputs, such as hallucinations (Guerreiro et al., 2023a,b; Dale et al., 2023a,b; Himmi et al., 2024). However, previous works focus on sentence-level metrics for overall translation quality, and do not evaluate performance on multiple label sets due to high annotation costs (Fomicheva et al., 2022; Zerva et al., 2024).1 In this work, we conduct more comprehensive evaluation spanning 10 unsupervised metrics derived from models inner representations and predictive distributions to identify translation errors at the word level. We test three open-source multilingual MT models and LLMs of different sizes across 12 translation directions, including typologically diverse languages and challenging textual domains. Importantly, we focus on texts with multiple human annotations to measure the impact of individual annotator preferences on metric performance, setting human-level baseline for the WQE task. We address the following research questions: i) How accurate are unsupervised WQE metrics in detecting MT errors compared to trained metrics and human annotators? ii) Are popular supervised WQE metrics well-calibrated? iii) Are the relative performances of WQE metrics affected by the variability in human error annotations? We conclude Materials: gsarti/labl/examples/unsup_wqe. 1Other relevant works are discussed in Appendix A"
        },
        {
            "title": "DivEMT",
            "content": "WMT24 QE4PE"
        },
        {
            "title": "Languages",
            "content": "ENAR,IT, NL,TR,UK,VI ENJA,ZH, HI,CS,RU CSUK ENIT,NL"
        },
        {
            "title": "Errors type",
            "content": "Post-edit Annotation Post-edit"
        },
        {
            "title": "Label sets",
            "content": "1"
        },
        {
            "title": "Domains",
            "content": "Wiki Multiple"
        },
        {
            "title": "MT Model",
            "content": "mBART-50 Aya23 # Segments 2580 5124 Social, Biomed NLLB 3888 Table 1: Summary of tested datasets. Error spans are obtained from explicit error annotations or post-edited spans. Additional details are available in Appendix B. with recommendations for improving the evaluation and usage of future WQE systems."
        },
        {
            "title": "2 Data",
            "content": "We use datasets containing error annotations or post-edits on the outputs of open-source models to extract unsupervised WQE metrics on real model outputs, avoiding possible confounders. We select the following datasets, summarized in Table 1: DivEMT (Sarti et al., 2022) contains single set of post-edits over translations produced by mBART50 (Tang et al., 2021) for subset of Wiki texts from the FLORES dataset (Goyal et al., 2022) spanning six typologically diverse target languages (ENAR,IT,NL,TR,UK,VI). We use it for crosslingual comparisons over fixed set of examples. WMT24 (Kocmi et al., 2024a) contains error spans on the outputs of the Aya23-35B LLM (Aryabumi et al., 2024) produced for the WMT24 General Translation Shared Task spanning multiple domains across six directions (ENJA,ZH,HI,CS,RU and CSUK). It was selected to extend our evaluation to state-of-the-art LLM, given the popularity of such systems in MT (Kocmi et al., 2023). QE4PE (Sarti et al., 2025) contains multiple human professional post-edits over translations produced by the NLLB 3.3B model (Costa-jussà et al., 2024) for ENIT and ENNL on challenging textual domains (social posts and biomedical abstracts). This dataset is used to conduct our evaluation across multiple annotation sets."
        },
        {
            "title": "3 Evaluated Metrics",
            "content": "The following metrics were evaluated using the Inseq library (Sarti et al., 2023, 2024b). Predictive Distribution Metrics. We use the Surprisal of the predicted token t, as negative log-probablity log p(t t<i), and the Entropy of the output distribution PN over vocabulary , (cid:80)V i=1 p(tit<i) log2 p(tit<i), as simple metrics to quantify pointwise and full prediction uncertainty (Fomicheva et al., 2020b). For surprisal, we also compute its expectation (MCDAVG) and variance (MCDVAR) with = 10 steps of Monte Carlo Dropout (MCD, Gal and Ghahramani, 2016) to obtain robust estimate and measure of epistemic uncertainty in predictions, respectively.2 Vocabulary Projections. We use the LogitLens (LL, nostalgebraist, 2020) to extract probability distributions P0, . . . , PN 1 over from intermediate activations at every layer l0, . . . , lN 1 of the decoder. We use the surprisal for the final prediction at every layer (LL-Surprisal) to assess the presence of layers with high sensitivity to wrong predictions. Then, we compute the KL divergence between every layer distribution and the final distribution PN , e.g. KL(PN 1PN ), to highlight trends in the shift in predictive probability produced by the application of remaining layers (LL KL-Div). Finally, we adapt the approach of Baldock et al. (2021) and use the number of the first layer for which the final prediction corresponds to the top logit as metric of model confidence, s.t. arg max Pl = and arg max Pi = i < (LL Pred. Depth). Context mixing. We use the entropy of the distribution of attention weights3 over previous context as simple measure of information locality during inference (Ferrando et al., 2022; Mohebbi et al., 2023). Following Fomicheva et al. (2020a), we experiment with using the mean and the maximum entropy across all attention heads of all layers as separate metrics (Attn. EntropyVAR/MAX). Finally, we evaluate the Between Layer OOD method by Jelenic et al. (2024), employing gradients to estimate layer transformation smoothness for OOD detection (BLOOD). Supervised baselines. We also test the state-ofthe-art supervised WQE model XCOMET (Guerreiro et al., 2024) in its XL (3.5B) and XXL (10.7B) 2Epistemic uncertainty reflects models lack of knowledge rather than data ambiguity. MCD is tested only on encoderdecoder models since Aya layers do not include dropout. 3For encoder-decoder model, self-attention and crossattention weights are concatenated and renormalized."
        },
        {
            "title": "Method",
            "content": "Random I P U Surprisal Out. Entropy Surprisal MCD AVG Surprisal MCD VAR LL Surprisal BEST LL KL-Div BEST LL Pred. Depth Att. Entropy AVG Att. Entropy MAX BLOOD BEST . XCOMET-XL U XCOMET-XL CONF XCOMET-XXL XCOMET-XXL CONF . Hum. Editors MIN Hum. Editors AVG Hum. Editors MAX DivEMT WMT24 QE4PE AP F1 AP F1 AP F1 .34 .43 .46 .43 .47 .42 .43 .39 .37 .34 .34 .42 .54 .43 .56 - - - .50 .53 .51 .53 .54 .53 .51 .51 .50 .50 .50 .45 .55 .41 .55 - - - .05 .08 .10 - - .09 .07 .06 .05 .05 - .09 .15 .09 .16 - - - .09 .13 .16 - - .15 .12 .12 .09 .09 - .19 .23 .20 .24 - - - .17 .23 .23 .24 .26 .23 .20 .20 .18 .16 .17 .23 .32 .22 .33 .24 .28 .32 .27 .32 .31 .33 .34 .32 .29 .29 .28 .28 . .34 .37 .31 .37 .34 .41 .47 Table 2: Average Precision (AP) and Optimal F1 (F1) for metrics across tested datasets. Results are averaged across all languages and annotators, with best unsupervised and overall best results highlighted. sizes, using them as binary metrics. Contrary to the continuous metrics from the previous section, binary labels from XCOMET cannot be easily calibrated to match subjective annotation propensity. Hence, we propose to adapt the XCOMET metric to use the sum of probability for all error types as token-level continuous confidence metric, s(t) = p(MINOR) + p(MAJOR) + p(CRITICAL), which we dub XCOMETCONF. Human Editors. For QE4PE, we report the min/mean/max agreement between each annotators edited spans and those of the other five editors as less subjective human-level quality measure."
        },
        {
            "title": "4 Experiments",
            "content": "How Accurate are Unsupervised WQE Metrics? Table 2 reports the average metrics performance across all translation directions across tested datasets.4 We report Average Precision (AP) as general measure of metric quality across the full score range, and we estimate calibrated metric performance as the best F1 score (F1) across all thresholds for binarizing continuous metric scores into pos./neg. labels matching human annotation.5 Our results show that, despite high variability in 4Full breakdown available in the Appendix (Tables 5 to 8). 5Random baseline AP values match the proportion of tokens marked as errors, which can vary greatly. Figure 2: Precision-Recall tradeoff for binary and confidence-weighted XCOMET variants and the Surprisal MCDVAR metric for DivEMT ENIT. error span prevalence across different models, languages and annotators, metric rankings remain generally consistent, suggesting that variability in does not affect the general ability of best metrics to discern underlying phenomena resulting in translation errors. Among unsupervised metrics, we find those based on the output distribution to be most effective at identifying error spans, in line with previous segment-level QE results (Fomicheva et al., 2020b). Notably, the Surprisal MCDVAR shows strong performances in line with the default XCOMET models. For the multi-label QE4PE dataset, we find that the best supervised metrics score on par with the average human annotator consensus (Hum. EditorsAVG), while unsupervised metrics generally obtain lower performances. Confidence Weighting Enables XCOMET Calibration. From Table 2 results, default XCOMET metrics underperform compared to the best unsupervised techniques, surprising result given their ad-hoc tuning. On the contrary, our XCOMETCONF method consistently reaches better results across all tested sets. Figure 2 shows the precision-recall tradeoff for these metrics on the ENIT subset of the DIVEMT dataset.6 In their default form commonly used for evaluation via the unbabel-comet library, XCOMET metrics consistently outperform Surprisal MCDVAR in terms of precision (51-60%, compared to 34% optimal precision for MCDVAR), but identify only 32-26% of tokens annotated as errors, resulting in lower AP. The low recall of these metrics might be problematic in WQE applications where omitting an error might result in oversights from human post-editors trusting the comprehensiveness of WQE predictions. On the 6Results for all datasets in the Appendix (Figures 4 to 7). contrary, the confidence-weighted XCOMETCONF show strong performances across the whole recall range, resulting in consistent improvements in both F1 and AP Table 2. Concretely, these results confirm that default XCOMET performance does not reflect the full capacity of the metric, and operating with granular confidence scores can be beneficial when calibration is possible. Metrics Performance for Multiple Annotations. While our evaluation so far employed human error span annotations as binary labels, we set out to assess how more granular labeling schemes impact metrics performance. Given sets of binary labels (up to 6 per language for QE4PE), we assign score {1, . . . , L} to every MT token using the number of annotators that marked it as an error, resulting in edit counts reflecting human agreement rate.7 Figure 3 presents the correlation of various metrics when the number of annotators available is increased, with median values and confidence bounds are obtained from edit counts across all combinations of label sets.8 The increasing trend for correlations across all reported metrics indicates that these methods reflect well the aleatoric uncertainty in error span labels, i.e. the disagreement between various annotators. In particular, the Surprisal MCDVAR metric sees steeper correlation increase than other well-performing metrics, surpassing default XCOMET supervised approaches for higher correlation bins. This suggests the epistemic uncertainty derived from noisy model predictions might be promising way to anticipate the aleatoric uncertainty across human annotators for WQE. We observe that 95% confidence intervals for high-scoring metrics are largely overlapping when single set of labels is used, indicating that rankings of metric performance are subject to change depending on subjective choices of the annotator. While this poses problem when attempting robust evaluation of WQE metrics, we remark that including multiple annotations largely mitigates this issue. As result, we recommend to explicitly account for human label variation by including multiple error annotations in future WQE evaluations to ensure generalizable findings."
        },
        {
            "title": "5 Conclusion",
            "content": "We conducted comprehensive evaluation of supervised and unsupervised WQE metrics across multi7An example is available in the Appendix  (Table 3)  . 8x=1 corresponds to binary labels from previous sections. Figure 3: Spearman correlation between WQE metric scores and human edit counts across multiple annotation sets for QE4PE ENIT (left) and ENNL (right). ple languages and annotation sets. Our results show that i) While unsupervised metrics generally lag behind state-of-the-art supervised systems, some uncertainty quantification methods based on the predictive distribution show promising correlation with human label variation; ii) Popular supervised WQE metrics have generally low levels of recall, and can benefit from confidence weighting to when calibration is possible; and iii) Individual annotator preferences are key confounders in WQE evaluations and can be mitigated by making use of multiple annotation sets. We offer the following practical recommendations for evaluating WQE systems: Use agreement between multiple human annotations to control the effect of subjective preferences and rank WQE metrics robustly. Employ an in-distribution calibration set of error spans before testing to ensure fair metric comparisons, and favor evaluations accounting for precision-recall tradeoffs to ensure their usability across various confidence levels. Previous work showed the effectiveness of visualization reflecting prediction confidence (Vasconcelos et al., 2025), such as highlights for various error severity levels (Sarti et al., 2025). Consider using continuous WQE metrics in real-world applications such as WQE-augmented post-editing to convey fine-grained confidence variations."
        },
        {
            "title": "Limitations",
            "content": "Our findings are accompanied by number of limitations. Firstly, our choice of tested datasets was limited by the availability of annotated outputs generated by open-source MT models. While several other datasets matching these criteria exist (Fomicheva et al., 2022; Yang et al., 2023; Dale et al., 2023b), we restricted our assessment to sufficient subset to ensure diversity across languages and tested models to support our findings. To facilitate comparison with other datasets, our evaluation for WMT24 treats available error spans as binary labels and does not directly account for error severity in human-annotated spans. Our choice of unsupervised metrics was largely driven by previous work on uncertainty quantification in MT, and ease of implementation for popular methods in mechanistic interpretability literature (Ferrando et al., 2024). However, our choices in the latter category were limited since most methods are nowadays developed and tested specifically for decoder-only transformer models. Finally, despite their strong performance, we found unsupervised methods based on MCD to require substantial computational resources, and as such we could not evaluate them on Aya23 35B. While our main focus was to establish baseline performances across various popular methods, future work should leverage the latest insights from more advanced techniques requiring, for example, the tuning of vocabulary projections (Belrose et al., 2023; Yom Din et al., 2024) or the identification of confidence neurons modulating predictive entropy (Stolfo et al., 2024)."
        },
        {
            "title": "References",
            "content": "Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. Aya 23: Open weight releases to further multilingual progress. Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. 2023. Uncertainty in natural language generation: From theory to applications. Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. 2021. Deep learning through the lens of example difficulty. In Advances in Neural Information Processing Systems, volume 34, pages 10876 10889. Curran Associates, Inc. Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan OGara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, and Yarin Gal. 2025. Open problems in machine unlearning for ai safety. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions from transformers with the tuned lens. Frederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh Kanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and André Martins. 2023. Findings of the WMT 2023 shared task on quality estimation. In Proceedings of the Eighth Conference on Machine Translation, pages 629653. Association for Computational Linguistics. Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. 2023. close look into the calibration of pre-trained language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13431367. Association for Computational Linguistics. Benjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, and Aleksander adry. 2024. Contextcite: In AdAttributing model generation to context. vances in Neural Information Processing Systems, volume 37, pages 9576495807. Curran Associates, Inc. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang, and NLLB Team. 2024. Scaling neural machine translation to 200 languages. Nature, 630(8018):841846. David Dale, Elena Voita, Loic Barrault, and Marta R. Costa-jussà. 2023a. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity Even better. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3650. Association for Computational Linguistics. David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta Costa-jussà. 2023b. HalOmi: manually annotated benchmark for multilingual hallucination and omission detection in machine translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 638653. Association for Computational Linguistics. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. mathematical framework for transformer circuits. Transformer Circuits Thread. Https://transformercircuits.pub/2021/framework/index.html. Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, and Maxim Panov. 2024. Fact-checking the output of large language models via token-level uncertainty In Findings of the Association for quantification. Computational Linguistics: ACL 2024, pages 9367 9385. Association for Computational Linguistics. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. 2023. LM-polygraph: Uncertainty estimation for language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 446461. Association for Computational Linguistics. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation, pages 10661083. Association for Computational Linguistics. Javier Ferrando, Gerard I. Gállego, and Marta R. Costajussà. 2022. Measuring the mixing of contextual information in the transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 86988714. Association for Computational Linguistics. Javier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, and Marta R. Costa-jussà. 2023. Explaining how transformers use context to build predictions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 54865513. Association for Computational Linguistics. Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. 2024. primer on the inner workings of transformer-based language models. Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estiIn Proceedings of mation: Overview and results. the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 165178. Association for Computational Linguistics. Marina Fomicheva, Shuo Sun, Erick Fonseca, Chrysoula Zerva, Frédéric Blain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia, and André F. T. Martins. 2022. MLQE-PE: multilingual quality estimation and post-editing dataset. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 49634974. European Language Resources Association. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Vishrav Chaudhary, Mark Fishel, Francisco Guzmán, and Lucia Specia. 2020a. BERGAMOT-LATTE submissions for the WMT20 In Proceedings of quality estimation shared task. the Fifth Conference on Machine Translation, pages 10101017. Association for Computational Linguistics. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020b. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:14601474. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondˇrej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733774. Association for Computational Linguistics. Yarin Gal and Zoubin Ghahramani. 2016. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 10501059, New York, New York, USA. PMLR. Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, and Barbara Plank. 2023. What comes next? evaluating uncertainty in neural text generators against human production variability. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1434914371. Association for Computational Linguistics. Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 2933. Association for Computational Linguistics. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Nuno M. Guerreiro, Pierre Colombo, Pablo Piantanida, and André Martins. 2023a. Optimal transport for unsupervised hallucination detection in neural machine translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1376613784. Association for Computational Linguistics. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Nuno M. Guerreiro, Elena Voita, and André Martins. 2023b. Looking for needle in haystack: comprehensive study of hallucinations in neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 10591075. Association for Computational Linguistics. Dan Hendrycks and Kevin Gimpel. 2017. baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations (ICLR 2017). Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, and Nuno Guerreiro. 2024. Enhanced hallucination detection in neural machine translation through simple detector aggregation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1857318583. Association for Computational Linguistics. Fran Jelenic, Josip Jukic, Martin Tutek, Mate Puljiz, and Jan Snajder. 2024. Out-of-distribution detection by leveraging between-layer transformation smoothness. In The Twelfth International Conference on Learning Representations. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438. Armen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or epistemic? does it matter? Structural Safety, 31(2):105112. Risk Acceptance and Risk Communication. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, Mariya Shmatova, Steinthór Steingrímsson, and Vilém Zouhar. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, pages 142. Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193203. European Association for Machine Translation. Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popovic, Mrinmaya Sachan, and Mariya Shmatova. 2024b. Error span annotation: balanced approach for human evaluation of machine translation. In Proceedings of the Ninth Conference on Machine Translation, pages 14401453. Association for Computational Linguistics. Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2024. Towards explainable evaluation metrics for machine translation. J. Mach. Learn. Res., 25(1). Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78717880. Association for Computational Linguistics. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pretraining for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726742. Arle Lommel, Aljoscha Burchardt, Maja Popovic, Kim Harris, Eleftherios Avramidis, and Hans Uszkoreit. 2014. Using new analytic measure for the annotation and analysis of MT errors on real data. In Proceedings of the 17th Annual Conference of the European Association for Machine Translation, pages 165172. European Association for Machine Translation. Hosein Mohebbi, Willem Zuidema, Grzegorz Chrupała, and Afra Alishahi. 2023. Quantifying context mixing in transformers. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 33783400. Association for Computational Linguistics. Marius Mosbach, Vagrant Gautam, Tomás Vergara Browne, Dietrich Klakow, and Mor Geva. 2024. From insights to actions: The impact of interpretability and analysis research on NLP. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 30783105. Association for Computational Linguistics. nostalgebraist. 2020. Interpreting GPT: the logit lens. AI Alignment Forum. Barbara Plank. 2022. The problem of human label variation: On ground truth in data, modeling and evaluation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1067110682. Association for Computational Linguistics. Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Linguistically debatable or just plain wrong? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 507511. Association for Computational Linguistics. Jirui Qi, Gabriele Sarti, Raquel Fernández, and Arianna Bisazza. 2024. Model internals-based answer attribution for trustworthy retrieval-augmented generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 60376053. Association for Computational Linguistics. Shauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. 2023. Conformal nucleus sampling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2734. Association for Computational Linguistics. Ricardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon Lavie, and André Martins. 2023. The inside story: Towards better understanding of machine translation neural evaluation metrics. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 10891105. Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702. Association for Computational Linguistics. Raphael Rubino, Atsushi Fujita, and Benjamin Marie. 2021. Error identification for machine translation with metric embedding and attention. In Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 146156. Association for Computational Linguistics. Gabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas, and Antonio Toral. 2022. DivEMT: Neural machine translation post-editing effort across typologically diverse languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 77957816. Association for Computational Linguistics. Gabriele Sarti, Grzegorz Chrupała, Malvina Nissim, and Arianna Bisazza. 2024a. Quantifying the plausibility of context reliance in neural machine translation. In The Twelfth International Conference on Learning Representations. Gabriele Sarti, Nils Feldhus, Jirui Qi, Malvina Nissim, and Arianna Bisazza. 2024b. Democratizing advanced attribution analyses of generative language In xAI-2024 Latemodels with the inseq toolkit. breaking Work, Demos and Doctoral Consortium Joint Proceedings, pages 289296, Valletta, Malta. CEUR.org. Gabriele Sarti, Nils Feldhus, Ludwig Sickert, and Oskar van der Wal. 2023. Inseq: An interpretability toolkit for sequence generation models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 421435. Association for Computational Linguistics. Gabriele Sarti, Vilém Zouhar, Grzegorz Chrupała, Ana Guerberof-Arenas, Malvina Nissim, and Arianna Bisazza. 2025. QE4PE: Word-level quality estimation for human post-editing. Rion Snow, Brendan OConnor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254263. Association for Computational Linguistics. Lucia Specia, Frédéric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, and André F. T. Martins. 2021. Findings of the WMT 2021 shared task on quality estimation. In Proceedings of the Sixth Conference on Machine Translation, pages 684725. Association for Computational Linguistics. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958. Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, and Neel Nanda. 2024. Confidence regulation neurons in language models. In Advances in Neural Information Processing Systems, volume 37, pages 125019 125049. Curran Associates, Inc. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 34503466. Association for Computational Linguistics. Dennis Ulmer, Jes Frellsen, and Christian Hardmeier. 2022. Exploring predictive uncertainty and calibration in NLP: study on the impact of method & data scarcity. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 27072735. Association for Computational Linguistics. Alexandra Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. 2021. Learning from disagreement: survey. Journal of Artificial Intelligence Research, 72:13851470. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589415939. Association for Computational Linguistics. Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman Vaughan. 2025. Generation probabilities are not enough: Uncertainty highlighting in ai code completions. ACM Trans. Comput.-Hum. Interact., 32(1). Leon Weber-Genzel, Siyao Peng, Marie-Catherine De Marneffe, and Barbara Plank. 2024. VariErr NLI: Separating annotation error from human label variation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22562269. Association for Computational Linguistics. Zhen Yang, Fandong Meng, Yuanmeng Yan, and Jie Zhou. 2023. Rethinking the word-level quality estimation for machine translation from human judgeIn Findings of the Association for Compument. tational Linguistics: ACL 2023, pages 20122025. Association for Computational Linguistics. Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. 2024. Jump to conclusions: Shortcutting transformers with linear transformations. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 96159625. ELRA and ICCL. Chrysoula Zerva, Frederic Blain, José G. C. De Souza, Diptesh Kanojia, Sourabh Deoghare, Nuno M. Guerreiro, Giuseppe Attanasio, Ricardo Rei, Constantin Orasan, Matteo Negri, Marco Turchi, Rajen Chatterjee, Pushpak Bhattacharyya, Markus Freitag, and André Martins. 2024. Findings of the quality estimation shared task at WMT 2024: Are LLMs closing the gap in QE? In Proceedings of the Ninth Conference on Machine Translation, pages 82109. Association for Computational Linguistics. Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orasan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 6999. Association for Computational Linguistics. Chrysoula Zerva and André F. T. Martins. 2024. Conformalizing machine translation evaluation. Transactions of the Association for Computational Linguistics, 12:14601478. Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter Liu. 2023. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations. Vilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, and Brian Thompson. 2024. Fine-tuned machine translation metrics struggle in unseen domains. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 488500. Association for Computational Linguistics. Vilém Zouhar, Tom Kocmi, and Mrinmaya Sachan. 2025. AI-assisted human evaluation of machine translation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 49364950, Albuquerque, New Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A Additional Background",
            "content": "In this section, we provide additional background information regarding the topics of our work. Unsupervised Quality Estimation for Machine Translation. The use of unsupervised signals from MT models for the task of MT quality estimation was introduced by Fomicheva et al. (2020b). Their evaluation showed that high-performing unsupervised methods could rival state-of-the-art supervised QE models in predicting translation quality at the sentence level. Since then, several evaluation campaigns assessed the quality of QE methods (Specia et al., 2021; Zerva et al., 2022; Blain et al., 2023; Zerva et al., 2024), including shared task dedicated to explainable QE metrics (Fomicheva et al., 2021). However, such evaluations have typically focused on segment-level evaluation quality, with word-level error spans being generally obtained by attributing the predictions of supervised segment-level metrics (Rubino et al., 2021; Rei et al., 2023). By contrast, recent work on LLMs evaluates various metrics to detect errors from the generator model, without additional systems involved, both at the sentence (Fadeeva et al., 2023) and at the token level (Fadeeva et al., 2024). Our work follows the latter approach by testing unsupervised metrics extracted from an MT model during generation, akin to out-of-distribution detection in signal processing research (Hendrycks and Gimpel, 2017). Actionable Insights from Interpretability. Advances in interpretability research have elucidated multiple mechanisms underlying decision-making, knowledge representation, and biases in LMs (Ferrando et al., 2024). However, better understanding of models inner workings often did not translate to tangible gains in model design and other practical applications, which remain rarely explored (Mosbach et al., 2024). Some examples in this direction include using targeted machine unlearning methods for safety-critical scenarios (Barez et al., 2025), or the use of attribution for trustworthy context citations in LM generations (Cohen-Wang et al., 2024; Sarti et al., 2024a; Qi et al., 2024). In this work, signals extracted from model internals are employed to detect errors in models generated outputs. Uncertainty Estimation for Language Models The estimation of uncertainty in language models has garnered increasing attention (Baan et al., 2023), particularly in the context of generation tasks for which the set of plausible responses is large (Giulianelli et al., 2023). Predictive uncertainty is typically decomposed into its aleatoric and epistemic components, representing respectively the irreducible variability in the modeled phenomena, and the improvable confidence in model predictions (Kiureghian and Ditlevsen, 2009). Popular methods for uncertainty estimation involve the calibration of predictive probabilities to reflect aleatoric uncertainty (Jiang et al., 2020; Ulmer et al., 2022; Zhao et al., 2023; Chen et al., 2023), and conformal sets prediction (Zerva and Martins, 2024; Ravfogel et al., 2023). In this work, we exploit uncertainty signals from the predictive distribution of MT models and its internal processing for efficiently predicting the resulting generation quality at fine-grained, token-level scale. Human Label Variation. Human label variation is type of uncertainty that arises from the inherent variability in human judgments (Plank et al., 2014; Plank, 2022), which can be hard to disentangle from actual annotation mistakes (Snow et al., 2008; Weber-Genzel et al., 2024). The usage of multiple references was recently recommended to ensure sound evaluation of generative LMs reflecting human-plausible levels of variability (Giulianelli et al., 2023), contrary to common practices employing single set of gold labels. In our analysis on QE4PE data containing multiple edits, we adopt perspectivist approach9 to ensure robust assessment of WQE metrics by accounting for annotators disagreement (Uma et al., 2021)."
        },
        {
            "title": "B Details about Models and Datasets",
            "content": "B.1 MT Models mBART-50 1-to-many. The original multilingual BART (mBART-25) model by Liu et al. (2020) is an encoder-decoder Transformer model pretrained on monolingual documents in 25 languages with the BART denoising objective for sequenceto-sequence learning (Lewis et al., 2020). Tang et al. (2021) extended mBART-25 by including 25 additional languages during pre-training and performing multilingual translation fine-tuning across 50 languages. In this work, we employ the oneto-many version of the model specialized in outof-English translation that was employed by Sarti 9pdai.info et al. (2022) to produce part of the translations postedited by DivEMT annotators.10 The model is standard Transformer with 12 layers of encoder and 12 layers of decoder, with model dimension of 1024 and 16 attention heads (680M parameters). NLLB 3.3B (No Language Left Behind) is collection of multilingual MT models covering up to 202 languages, including low-resource directions (Costa-jussà et al., 2024). The largest NLLB model available is mixture-of-experts model with 54.4B parameters, which comes with high computational cost. In this work we employ the largest available dense variant of the model (3.3B parameters), which was used by Sarti et al. (2025) for collecting the QE4PE post-editing dataset.11 The model is an encoder-decoder Transformer with 24 layers for each module, model dimension of 2048 and 16 attention heads per layer. Aya23 35B is large language model introduced by Aryabumi et al. (2024) to improve the multilingual capabilities of the original Aya model (Üstün et al., 2024) on selected set of 23 languages. The model was included in the WMT24 evaluation of Kocmi et al. (2024a), resulting in the best translation performances among tested open-source models. The model is decoder-only Transformer model with 40 layers, model dimension of 8196 and 64 attention heads per layer. B.2 Datasets DivEMT was created by (Sarti et al., 2022) to evaluate the impact of language typology on MT quality, and how that would influence the productivity of human post-editors working with those systems. The dataset includes out-of-English machine translations for Wiki data produced by Google Translate and mBART-50 1-to-many, with edits made by professional translators in six languages. In this work, we evaluate unsupervised metrics on the mBART-50 1-to-many model, converting the human post-edits. WMT24 employed in this study is taken from the General Machine Translation Shared Task at It contains WMT 2024 (Kocmi et al., 2024a). evaluation of several machine translation systems across English{Czech, Hindi, Japanese, Chinese, Russian} (634 segments) and CzechUkrainian (1954 segments). The human evaluation was done 10facebook/mbart-large-50-one-to-many-mmt 11facebook/nllb-200-3.3B with the Error Span Annotation protocol (ESA, Kocmi et al., 2024b), which has human annotators highlighting erroneous spans in the translation and marking them as either MINOR or MAJOR errors. This dataset covers the news, social, and speech (with automatic speech recognition) domains. We adopt the official prompting setup from the WMT24 campaign, using the Aya23 model alongside the provided prompt and 3 in-context translation examples per language to ensure uniformity with previous results.12 QE4PE The QE4PE dataset was created by Sarti et al. (2025) for measuring the effect of word-level error highlights when included in real-world human post-editing workflows. The QE4PE data provides granular behavioral metrics to evaluate the speed and quality of post-editing of 12 annotators for ENIT and ENNL across four error span highlighting modalities, including the unsupervised Surprisal MCDVAR method and the supervised XCOMET-XXL we also test in this study. Provided that the presence of error span highlights was found to influence the editing choices of human editors, we limit our evaluation to the six human annotators per language that post-edited sentences without any highlights (3 for the Oracle Post-edit task to produce initial human-based highlights, and 3 for the No Highlight modality in the main task). This prevents us from biasing our evaluation of WQE metrics in favor of the metrics that influenced editing choices. We use the post-edited versions to synthetically create error spans, which can be used as binary labels to evaluate WQE metrics."
        },
        {
            "title": "C Details about Tested Metrics",
            "content": "Monte Carlo Dropout (MCD) is technique introduced by Gal and Ghahramani (2016) for estimating model uncertainty at inference time. MCD uses the dropout mechanism in neural networks (Srivastava et al., 2014), commonly employed for regularization during training, at inference time to produce set of noisy predictions from unique model, approximating Bayesian inference. For given input x, forward passes are performed through the network. In each pass , different random dropout mask Θt is applied, resulting in slightly different output probabilities p(x Θt). The set of predictions {p(x Θ1), . . . , p(x ΘT )} can be seen as sam12wmt-conference/wmt-collect-translations Source EN MT IT (NLLB) So why is it that people jump through extra hoops to install Google Maps? Quindi perché le persone devono fare un salto in più per installare Google Maps? salto Annotator Quindi perché le persone devono fare un passaggio in più per installare Google Maps? Annotator t2 Quindi perché le persone devono fare un salto in più fanno salti mortali per installare Google Maps? devono fare un salto in più Annotator t3 Quindi perché le persone Annotator t4 Quindi Allora perché le persone effettuano dei passaggi ulteriori superflui per installare Google Maps? devono fare salto fanno un passaggio in più per installare Google Maps? Annotator t5 allora mi chiedo: perché gli utenti iPhone si affannano tanto per installare Google Maps? Quindi perché le persone devono fare un salto in più Annotator Quindi perché le persone devono fare un salto in più fanno di tutto Edit Counts  (Fig. 3)  2 Quindi 1 perché le persone 5 devono fare per installare Google Maps? 4 un 6 salto 4 in più per installare Google Maps? XCOMET-XL Quindi perché le persone devono fare un salto in più per installare Google Maps? minor minor minor major XCOMET-XXL XCOMET-XL CONF XCOMET-XXL CONF Surprisal MCD VAR Quindi perché le persone .36 perché .83 perché .01 perché .41 Quindi .51 Quindi .05 Quindi .51 le .20 le .04 le .50 persone .20 persone .00 persone devono fare un salto in più per installare Google Maps? .69 devono .42 devono .41 devono .73 fare .84 fare .09 fare .51 un .90 un .04 un .81 salto .95 salto .59 sal .74 in .86 in .76 più .78 più .39 per .03 per .47 install .00 install .53 are .01 are .00 to .12 in .00 più .00 per .00 installare .26 Google .00 Google .00 Google .36 Maps 00 Maps .00 Maps .24 ? .00 ? .00 ? Table 3: Annotated example from the ENIT portion of the QE4PE dataset. Top: Annotator edits with highlighted final text and replaced text on top, with count-based aggregation showing inter-annotator agreement. Bottom: Word-level annotations for best-performing metrics discussed in the study. Source EN MT IT (Aya23) So the challenges in this are already showing themselves. Im likely going to have VERY difficult time getting medical clearance due to the FAAs stance on certain medications. Takže problémy tím se již projevují. Pravdˇepodobnˇe budu mít ˇRESN ˇE obtížný ˇcas dostat lékaˇrské potvrzení kvuli postoji FAA nˇekterým lékum. Annotator problémy tím se již projevují. Pravdˇepodobnˇe budu mít Takže kvuli postoji FAA nˇekterým lékum. minor major ˇRESN ˇE obtížný ˇcas dostat lékaˇrské potvrzení XCOMET-XL Takže problémy tím se již projevují. Pravdˇepodobnˇe budu mít minor ˇRESN ˇE obtížný minor ˇcas dostat lékaˇrské XCOMET-XXL XCOMET-XL CONF XCOMET-XXL CONF Out. Entropy minor potvrzení kvuli postoji FAA nˇekterým lékum minor Takže problémy tím se již projevují . Pravdˇepodobnˇe budu mít kvuli postoji FAA nˇekterým lékum. 0.19 již 0.28 problémy 0.31 projevují 0.23 Takže 0.28 tím 0.17 se 0.26 0.17 . 0.23 Pravdˇepodobnˇe major ˇRESN ˇE obtížný ˇcas dostat lékaˇrské potvrzení 0.40 budu 0.48 mít 0.79 ˇRESN ˇE 0.65 obtížný 0.76 ˇcas 0.64 dostat 0.25 Takže 0.24 dostat 0.88 Takže 1.40 dostat 0.50 lékaˇrské 0.51 potvrzení 0.19 kvuli 0.34 postoji 0.27 FAA 0.20 0.20 nˇekterým 0.21 lékum 0.17 . 0.24 problémy 0.26 0.31 tím 0.29 se 0.23 již 0.26 projevují 0.01 . 0.01 Pravdˇepodobnˇe 0.03 budu 0.37 ˇRESN ˇE 0.30 obtížný 0.32 ˇcas 0.10 lékaˇrské 0.13 potvrzení 0.01 kvuli 0.00 postoji 0.00 FAA 0.00 0.00 nˇekterým 0.00 lékum 0.00 . 1.93 problémy 1.88 0.84 tím 1.66 se 1.13 již 0.89 projevují 0.11 . 0.44 Pravdˇepodobnˇe 0.22 budu 0.09 mít 2.09 ˇRESN ˇE 3.70 obtížný 0.09 ˇcas 1.02 lékaˇrské 0.64 potvrzení 0.69 kvuli 0.24 postoji 0.80 FAA 1.01 0.55 nˇekterým 0.18 lékum 0.11 . Table 4: Annotated example from the ENCS portion of the WMT24 dataset. Top: Annotator edits with highlighted Error Span Annotation of minor and major errors. Bottom: Word-level annotations for best-performing metrics discussed in the study. ples from an approximate posterior distribution. In this work, we employ the mean of the negative log probabilities as robust estimate of surprisal: Surprisal MCDavg = ˆyMCD ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 log p(xΘt) Moreover, we estimate predictive uncertainty by calculating the variance of predictive probabilities under the same setup: Surprisal MCDvar ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:0)log p(xΘt)ˆyMCD (cid:1) Vocabulary Projections. The Logit Lens (nostalgebraist, 2020) is an interpretability technique used to understand the internal workings of Transformer models, particularly how their predictions evolve layer by layer. Activations hl produced by the model layer are projected to vocabulary space using the model unembedding matrix, WU , normally used to produce output logits. For the NLLB and mBART-50 models we also apply final layer normalization before the projection, following the model architecture, while for the Aya model we scale logits by 0.0625 (the default logit_scale defined in the model configuration). Following the residual stream view of the Transformer model (Elhage et al., 2021), the resulting logits provide view into the model predictive confidence at that specific depth of processing. Context mixing. Several works studied the mixing of contextual information across language model layers to attribute model predictions to specific input properties (Ferrando et al., 2022; Mohebbi et al., 2023; Ferrando et al., 2023 inter alia). In this work we employ simple estimates of context relevance using attention weights produced during the Transformer attention operation. More specifically, for every attention head at every layer of the decoder module, we extract score for every token in the preceding context, employing crossattention weights to account for source-side context in encoder-decoder models. XCOMET is suite of MT evaluation metrics introduced by Guerreiro et al. (2024) extending the popular COMET metric (Rei et al., 2020) to combine combines sentence-level and word-level error span prediction for improved explainability of results. XCOMET metrics come in 3B (XL) and 11B (XXL) size and support both reference-based and reference-less usage, hence enabling usage for quality estimation purposes. Concretely, XCOMET models are Transformer encoders fine-tuned from pre-trained XLMR encoders (Goyal et al., 2021) using mix of sentence-level Direct Assessment scores and word-level MQM error spans. In this work, we focus on their word-level error span prediction capabilities in quality estimation setup, where the model classifies every input token according to MQM severity levels {OK, MINOR, MAJOR, CRITICAL} with learned linear layer.13 Token-level Evaluation. Error spans used as labels in our evaluation are defined at the character level, while metric scores depend on the tokenization employed by either the MT model (for unsupervised metrics) or XCOMET (for supervised metrics). To allow for comparison, we label tokens as being part of an error span if at least one character contained in it was marked as an error or edited by an annotator. Tables 3 and 4 provide examples of various segmentations for the same MT output. Constraining generation Evaluating metrics at the word level can be challenging due to the need for perfect uniformity between model generations and annotated spans. For this reason we extract unsupervised metrics during generation while forcedecoding the annotated outputs from the MT model to ensure perfect adherence with annotated error spans. In general, such an approach could introduce problematic confounder in the evaluation, since observed results could be the product of constraining model towards an unnatural generation, rather than reflecting underlying phenomena. However, in this study, we carefully ensure that the generation setup matches exactly the one of previous works where the annotated translations were produced, using the same MT model and the same inputs.14 Hence, the constraining process is simple insurance of conformity in light of potential discrepancies introduced by different decoding strategies, and does not affect the soundness of our method. 13The default XCOMET metric was used with the unbabel-comet library (v2.2.6). 14Generation parameters are not relevant in this setting, provided that they only alter the selection of the following output token, which we do via force-decoding."
        },
        {
            "title": "Method",
            "content": "Random Baseline Surprisal Out. Entropy Surprisal MCD AVG Surprisal MCD VAR LL Surprisal BEST LL KL-Div BEST LL Pred. Depth Attn. Entropy AVG Attn. Entropy MAX BLOOD BEST XCOMET-XL XCOMET-XL CONF XCOMET-XXL XCOMET-XXL CONF Human Editors MIN Human Editors AVG Human Editors MAX QE4PEt1 QE4PEt2 QE4PEt3 QE4PEt4 QE4PEt5 QE4PEt6 QE4PEavg F1 AP F1 AP F1 AP F1 AP F1 AP F1 AP F1 AP . .11 .12 .12 .13 .11 .09 .09 .11 .09 .08 .11 .20 .13 .19 .17 .20 .24 .14 .20 .18 .20 .21 .19 .16 .16 .16 .14 .14 .24 .25 .27 . .33 .38 .43 .15 .21 .22 .22 .26 .21 .19 .18 .17 .15 .16 .22 .30 .22 .31 .26 .29 .31 . .31 .30 .32 .33 .32 .28 .28 .27 .26 .26 .35 .36 .32 .36 .38 .43 .47 .06 .11 .10 .11 .12 .11 .08 .07 .12 .10 .06 .10 .14 .10 . .10 .14 .20 .12 .17 .16 .17 .20 .16 .14 .13 .17 .18 .12 .20 .21 .24 .24 .21 .30 .41 . .16 .17 .16 .19 .16 .13 .14 .11 .09 .11 .16 .25 .17 .26 .16 .22 .24 .19 .25 .24 .26 .27 .25 .21 .21 .19 .19 .19 .30 .31 .31 . .26 .39 .43 .22 .30 .30 .30 .31 .29 .25 .25 .23 .20 .23 .27 .37 .28 .37 .25 .32 .37 . .40 .39 .41 .40 .40 .37 .37 .36 .36 .36 .35 .40 .32 .41 .36 .38 .50 .18 .25 .26 .26 .29 .26 .22 .21 .19 .16 .18 .23 .31 .23 . .23 .30 .33 .30 .35 .34 .36 .36 .35 .31 .31 .31 .30 .30 .34 .36 .31 .39 .30 .40 .50 . .19 .19 .19 .22 .19 .16 .16 .15 .13 .14 .18 .26 .19 .27 .19 .25 .28 .23 .28 .27 .29 .30 .28 .25 .24 .24 .24 .23 .30 .32 .30 . .31 .39 .46 Table 5: WQE metrics performance for predicting error spans from the six edit sets over NLLB 3.3B translations in the ENIT QE4PE dataset (Sarti et al., 2025). Best unsupervised and overall best metric results are highlighted."
        },
        {
            "title": "Random Baseline",
            "content": "Surprisal Out. Entropy Surprisal MCD AVG Surprisal MCD VAR LL Surprisal BEST LL KL-Div BEST LL Pred. Depth Attn. Entropy AVG Attn. Entropy MAX BLOOD BEST XCOMET-XL XCOMET-XL CONF XCOMET-XXL XCOMET-XXL CONF"
        },
        {
            "title": "Human Editors MIN\nHuman Editors AVG\nHuman Editors MAX",
            "content": "QE4PEt1 QE4PEt2 QE4PEt3 QE4PEt4 QE4PEt5 QE4PEt6 QE4PEavg F1 AP F1 AP F1 AP F1 AP F1 AP F1 AP F1 AP .07 .12 .11 .12 .13 .12 .09 .09 .09 .09 .07 .13 .24 .13 .24 .16 .17 .19 . .19 .18 .19 .21 .19 .15 .16 .15 .15 .13 .27 .31 .28 .30 .29 .33 .36 .34 .41 .41 .42 .45 .42 .39 .37 .37 .35 .35 .39 .47 .39 . .43 .44 .46 .51 .51 .51 .52 .53 .53 .52 .52 .51 .51 .51 .39 .53 .29 .53 .51 .51 .51 . .30 .31 .31 .36 .30 .28 .26 .22 .22 .22 .31 .43 .30 .43 .34 .34 .36 .36 .39 .37 .40 .41 .40 .37 .37 .36 .36 .36 .44 .45 .35 . .45 .45 .51 .19 .29 .29 .30 .34 .29 .25 .24 .20 .18 .19 .28 .40 .26 .40 .33 .33 .37 . .37 .36 .40 .40 .38 .34 .33 .32 .32 .32 .32 .43 .35 .42 .47 .47 .53 .13 .21 .20 .21 24 .21 .17 .17 .13 .12 .14 .20 .29 .19 . .26 .26 .32 .24 .30 .27 .30 .32 .30 .26 .25 .24 .24 .24 .35 .36 .31 .35 .42 .42 .51 . .31 .31 .31 .36 .31 .29 .27 .23 .21 .23 .31 .43 .30 .43 .36 .36 .40 .36 .41 .39 .42 .42 .41 .38 .38 .37 .37 .36 .44 .46 .35 . .46 .46 .53 .20 .27 .27 .28 .31 .27 .25 .23 .21 .19 .20 .27 .38 .26 .38 .32 .32 .35 . .36 .35 .37 .38 .37 .34 .33 .32 .32 .32 .38 .42 .32 .42 .43 .43 .49 Table 6: WQE metrics performance for predicting error spans from the six edit sets over NLLB 3.3B translations in the ENNL QE4PE dataset (Sarti et al., 2025). Best unsupervised and overall best metric results are highlighted."
        },
        {
            "title": "Method",
            "content": "Random Baseline Surprisal Out. Entropy Surprisal MCD AVG Surprisal MCD VAR LL Surprisal BEST LL KL-Div BEST LL Pred. Depth Attn. Entropy AVG Attn. Entropy MAX BLOOD BEST XCOMET-XL XCOMET-XL CONF XCOMET-XXL XCOMET-XXL CONF Italian AP F1 AP F1 AP F1 AP F1 AP Turkish Vietnamese Ukrainian Average AP F1 AP F"
        },
        {
            "title": "Dutch",
            "content": "F1 .25 .34 .37 .34 .39 .33 .34 .30 .28 .25 .26 .34 .46 .34 .48 .40 .45 .43 .45 .46 .44 .42 .42 .41 .41 . .39 .47 .36 .49 .28 .36 .39 .37 .41 .36 .37 .32 .30 .26 .28 .37 .49 .35 .50 .43 .46 .45 .47 .47 .45 .45 .44 .43 .43 . .44 .50 .35 .50 .33 .42 .45 .43 .47 .41 .41 .39 .35 .34 .35 .41 .51 .43 .55 .49 .51 .50 .52 .53 .51 .51 .50 .49 .49 . .47 .53 .47 .54 .34 .43 .49 .44 .49 .44 .44 .40 .37 .34 .35 .44 .58 .45 .58 .50 .54 .52 .54 .55 .54 .52 .52 .51 .50 . .50 .56 .48 .56 .35 .46 .48 .46 .48 .44 .44 .39 .40 .35 .36 .42 .53 .43 .56 .52 .55 .54 .55 .55 .55 .52 .53 .52 .52 . .44 .55 .42 .57 .48 .55 .58 .56 .61 .55 .56 .54 .50 .47 .49 .56 .68 .57 .70 .65 .65 .65 .65 .67 .66 .65 .66 .65 .65 . .44 .67 .41 .67 .34 .43 .46 .43 .48 .42 .43 .39 .37 .34 .35 .42 .54 .43 .56 .50 .53 .51 .53 .54 .53 .51 .51 .50 .50 . .45 .55 .42 .55 Table 7: WQE metrics performance for predicting error spans from multiple edit sets (one per language) over mBART-50 translations across the six topologically diverse target languages of DIVEMT (Sarti et al., 2022)."
        },
        {
            "title": "Method",
            "content": "En Ja Average AP F1 AP F1 AP F1 AP F1 AP F1 AP F1 AP F1 En Zh En Hi Cs Uk En Cs En Ru"
        },
        {
            "title": "Random Baseline",
            "content": "Surprisal Out. Entropy LL Surprisal BEST LL KL-Div BEST LL Pred. Depth Attn. Entropy AVG Attn. Entropy MAX XCOMET-XL XCOMET-XL CONF XCOMET-XXL XCOMET-XXL CONF .02 .03 .03 .03 .02 .02 .02 .01 .04 .08 .04 .07 . .07 .08 .07 .05 .05 .03 .03 .09 .14 .11 .15 .03 .05 .06 .05 .04 .04 .03 .03 .05 .10 .06 .09 . .09 .11 .09 .07 .08 .07 .07 .11 .16 .13 .19 .03 .05 .06 .05 .04 .04 .03 .03 .06 .10 .05 .09 . .09 .10 .09 .08 .09 .07 .07 .12 .19 .11 .17 .05 .14 .20 .14 .10 .09 .03 .03 .13 .18 .13 .19 . .20 .27 .20 .17 .18 .09 .09 .28 .30 .28 .29 .06 .10 .12 .10 .09 .08 .05 .05 .11 .19 .11 .22 . .16 .18 .16 .15 .14 .11 .11 .24 .29 .24 .30 .08 .13 .14 .13 .12 .11 .07 .08 .16 .24 .16 .28 . .19 .20 .19 .19 .18 .16 .16 .32 .32 .33 .33 .05 .08 .10 .08 .07 .06 .04 .04 .09 .15 .09 .16 . .13 .16 .13 .12 .12 .09 .09 .19 .23 .20 .24 Table 8: WQE metrics performance for predicting error spans from the ESA annotations (one set per language) over Aya23-35B outputs for the WMT24 dataset (Kocmi et al., 2024a). Figure 4: Precision-recall curves for XCOMET metrics and Surprisal MCDVAR for all annotators of QE4PE ENIT. Figure 5: Precision-recall curves for XCOMET metrics and Surprisal MCDVAR for all annotators of QE4PE ENNL. Figure 6: Precision-recall curves for XCOMET metrics and Surprisal MCDVAR on all DIVEMT languages. Figure 7: Precision-recall curves for XCOMET metrics and Out. Entropy on all WMT24 languages."
        }
    ],
    "affiliations": [
        "CLCG, University of Groningen",
        "ETH Zurich"
    ]
}