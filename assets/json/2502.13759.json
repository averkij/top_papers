{
    "paper_title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
    "authors": [
        "Zirui Song",
        "Jingpu Yang",
        "Yuan Huang",
        "Jonathan Tonglet",
        "Zeyu Zhang",
        "Tao Cheng",
        "Meng Fang",
        "Iryna Gurevych",
        "Xiuying Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Geolocation, the task of identifying an image's location, requires complex reasoning and is crucial for navigation, monitoring, and cultural preservation. However, current methods often produce coarse, imprecise, and non-interpretable localization. A major challenge lies in the quality and scale of existing geolocation datasets. These datasets are typically small-scale and automatically constructed, leading to noisy data and inconsistent task difficulty, with images that either reveal answers too easily or lack sufficient clues for reliable inference. To address these challenges, we introduce a comprehensive geolocation framework with three key components: GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe, with each location annotated thousands to tens of thousands of times by human users. The dataset offers diverse difficulty levels for detailed analysis and highlights key gaps in current models. Building on this dataset, we propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning framework designed to enhance the reasoning capabilities of Large Vision Models (LVMs) in geolocation tasks. GeoCoT improves performance by integrating contextual and spatial cues through a multi-step process that mimics human geolocation reasoning. Finally, using the GeoEval metric, we demonstrate that GeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing interpretability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 5 7 3 1 . 2 0 5 2 : r Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework"
        },
        {
            "title": "Zirui Song\nMBZUAI\nUnited Arab Emirates",
            "content": "Jingpu Yang, Yuan Huang Northeastern University China"
        },
        {
            "title": "Iryna Gurevych\nMBZUAI\nUnited Arab Emirates",
            "content": "Xiuying Chen MBZUAI United Arab Emirates Abstract Geolocation, the task of identifying an images location, requires complex reasoning and is crucial for navigation, monitoring, and cultural preservation. However, current methods often produce coarse, imprecise, and non-interpretable localization. major challenge lies in the quality and scale of existing geolocation datasets. These datasets are typically small-scale and automatically constructed, leading to noisy data and inconsistent task difficulty, with images that either reveal answers too easily or lack sufficient clues for reliable inference. To address these challenges, we introduce comprehensive geolocation framework with three key components: GeoComp, large-scale dataset; GeoCoT , novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), large-scale dataset collected from geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe, with each location annotated thousands to tens of thousands of times by human users. The dataset offers diverse difficulty levels for detailed analysis and highlights key gaps in current models. Building on this dataset, we propose Geographical Chain-of-Thought (GeoCoT), novel multi-step reasoning framework designed to enhance the reasoning capabilities of Large Vision Models (LVMs) in geolocation tasks. GeoCoT improves performance by integrating contextual and spatial cues through multi-step process that mimics human geolocation reasoning. Finally, using the GeoEval metric, we demonstrate that GeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing interpretability. Access dataset here. Corresponding Author. CCS Concepts Computing methodologies Natural language generation; Computer vision tasks. Keywords Geolocation, Reasoning, Chain of thought, Dataset ACM Reference Format: Zirui Song, Jingpu Yang, Yuan Huang, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, and Xiuying Chen. 2018. Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework. In . ACM, New York, NY, USA, 11 pages. https: //doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction",
            "content": "Dataset Im2GPS3K [51] YFCC4K [51] YFCC26K [47] MP-16 [23] Google-WS-15k [8] GMCP [60] StreetCLIP [15] OSV-5M [1] GeoComp Size 2997 4536 26k 4.7M 15k 105K 1M 5.1M 3.3M Geographic Coverage Local Local Local Local Global Local Unknown Global Global Source Web-Scraped Web-Scraped Web-Scraped Web-Scraped Map Service Map Service Map Service Map Service Map Service Open Access Annotation Human Table 1: Comparison of Existing Geolocation Datasets and GeoComp. GeoComp is the first to include real, rich player performance data. Local refers to cityor region-specific data, while Global spans multiple continents. Darker green shades indicate broader geographic coverage. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, Washington, DC, USA 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Geolocation, the task of determining an images geographical location, is crucial for applications like crime tracking, navigation, fact-checking, and cultural exploration [6, 7]. It involves interpreting contextual clues within an image, such as architectural styles, road signs, natural landscapes, and cultural markers. Inferring location from such diverse indicators demands advanced reasoning, making geolocation challenging task for both artificial models and human experts [20]. Conference17, July 2017, Washington, DC, USA Trovato et al. hand, some tasks are highly challenging, where only few users spend considerable time before providing accurate answers. Unlike previous approaches that address this task with coarse level of granularity, we conduct comprehensive evaluation of recent advanced LVMs on GeoComp, where the models are required to reason and predict the exact city of given location. Our findings reveal that this task poses significant challenge for existing LVM models. To address this, we introduce Geographical Chain of Thought (GeoCoT) approach, which automatically guides the reasoning process through multi-step analysis of geographical cues, such as landmarks, environmental features, and spatial relationships. For the evaluation of the reasoning process, we propose set of articulated evaluation metrics, named as GeoEval including comparison with ground truth reasoning data and intrinsic evaluation. The results demonstrate that our GeoCoT paradigm significantly improves geolocation accuracy. It not only helps break down complex tasks into manageable reasoning steps but also enhances the interpretability of the inference process. Our work makes key contributions to geolocation. First, we present GeoComp, large-scale, human-annotated geolocation dataset with over 3.9 million tasks and 25 million human player annotations, featuring diverse geographic regions, languages, and environmental contexts. These annotations identify high-difficulty geolocation cases and establish benchmarks to guide future advancements. Second, we introduce the Geographical Chain of Thought (GeoCoT) framework, multi-step reasoning approach that improves geolocation accuracy by leveraging geographical cues like landmarks, environmental features, and spatial relationships. Finally, through comprehensive evaluations involving human assessments and LLM inferences, we show that GeoCoT improves predictive performance by up to 25% while enhancing interpretability."
        },
        {
            "title": "2.2 Geolocation Dataset\nExisting geolocation datasets primarily originate from web-scraped\nor street-view images that have not been human-validated, raising",
            "content": "Figure 1: The gaming logic of our platform: Two players independently guess the location based on the same image and their own hints, with scores determined by the distance between their predictions and the ground truth location. Significant effort has been devoted to solving the geolocation task, but often at coarse level of granularity. For example, methods like Im2GPS3K [50] and PlaNet [53] frame the task by dividing the globe into grid cells and training deep neural networks to predict the correct cell for given image. Subsequent studies improve precision by retrieving the most visually similar image from dataset and using its coordinates as the predicted location [38, 76]. The reason for this coarse granularity in many approaches is potentially due to the lack of high-quality datasets. For example, Im2GPS3K contains up to 35% non-localizable images [1], while the YFC100M dataset includes irrelevant data such as indoor photos and food images, which provide little to no locational information [47]. Additionally, many datasets are limited in size, with Georeasoner [24] featuring only 3K images, thereby restricting the robustness and generalizability of geolocation models. comparison of these datasets is shown in Table 1. To address the above obstacles, in this work, we leverage the contributions of hundreds of thousands of geolocation game enthusiasts who provide valuable annotation labels while playing the game. Specifically, we launched free, public-benefit-oriented online geoguessing website in June 2022 (tuxun.fun), as shown in Figure 1. Users are required to guess the latitude and longitude based on these images, which are sourced from Google Maps, Baidu Maps, and Gaode Maps. The platform offers various game formats, including team contests and solo matches. As of December 17, 2024, this platform has 740,468 users, 3,954,397 locations as unique geolocation tasks, and 25,355,174 human response records. We name the collected dataset GeoComp. This rich and valuable dataset of real human responses enables us to evaluate task difficulty and filter out unreasonable cases. For instance, some tasks are too easy, such as when the name of shopping mall in city is clearly visible in the image, enabling most users to answer correctly. On the other Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework Conference17, July 2017, Washington, DC, USA Figure 2: Spatial distribution of 3,238,919 geo-tagged locations in GeoComp: (a) The global map shows spatial heterogeneity, with dense clusters in more urbanized regions like Europe and Asia, and sparse coverage in areas like Africa and Siberia. (b) The pie chart highlights the proportional geo-tagged locations distribution, led by North America and Asia. (c) Unlike previous datasets like OSV-5M, where single country (e.g., America) dominates 25% of the data, our dataset is balanced at country level. concerns about their quality for effectively evaluating geolocation capabilities. For instance, datasets derived from web scraping, such as YFCC100M [48] and Im2GPS3K [50], include significant proportion of images depicting food, art, pets, and personal photographs. These types of images are often weakly localizable or entirely non-localizable [47]. Street-view datasets also face limitations, such as restricted geographic coverage [1]; for example, [37]s work includes data from only three cities in the United States. Furthermore, dataset collection processes often introduce biases. For instance, some commonly used platforms are inaccessible in certain countries, resulting in uneven geographic representation. Additionally, the difficulty of individual geolocation tasks varies widely within these datasets, but this aspect has not been comprehensively evaluated. For example, images taken at prominent landmarks are relatively easy to geolocate, while others offer no clear hints and are highly challenging [1]. These limitations hinder the ability of current datasets to provide reliable benchmark for geolocation task."
        },
        {
            "title": "2.3 Large Vision Language Models\nLLMs have exhibited extraordinary emergent abilities by scaling\nup data and model sizes, notably including instruction following\n[10, 26], in-context learning [4], and Chain of Thought (CoT) [21].\nBuilding on these emergent capabilities, significant research ef-\nforts have focused on enhancing cross-modality understanding and\nreasoning capabilities. Numerous studies have been conducted on\nvarious aspects of LVMs, encompassing structural design [5, 28, 30],\ndata construction [22, 67], training strategies [33, 35, 72], evalua-\ntions [3], and the development of lightweight LVMs [75]. Addition-\nally, the robust capabilities of LVMs have been applied to other",
            "content": "fields, such as medical image understanding [61, 63, 73] and document parsing [32, 59]. Furthermore, the development of multimodal agents has advanced real-world applications, including embodied agents [19, 40] and GUI agents [27, 45, 58]. However, the reasoning capabilities of LVMs in geolocation tasks remain underexplored. One of the primary reasons for this limitation is the lack of high-reasoning-value geographic data."
        },
        {
            "title": "3.1 Geolocation Competition\nInspired by geoguessr.com, we developed a free geolocation game\nplatform that tracks participants‚Äô competition histories. Unlike most",
            "content": "Conference17, July 2017, Washington, DC, USA Trovato et al. Figure 3: Performance of game players of different levels in mainstream countries. geolocation websites, including Geoguessr, which rely solely on samples from Google Street View, our platform integrates Baidu Maps and Gaode Maps to address coverage gaps in regions like mainland China, ensuring broader global accessibility. The platform offers various engaging competition modes to enhance user experience, such as team contests and solo matches. Each competition consists of multiple questions, and teams are assigned vitality score. Users mark their predicted location on the map, and the evaluation is based on the ground truths surface distance from the predicted location. Larger errors result in greater deductions from the teams vitality score. At the end of the match, the team with the higher vitality score wins. We also provide diverse game modes, including street views, natural landscapes, and iconic landmarks. Users can choose specific opponents or engage in random matches. To prevent cheating, external search engines are banned, and each round is time-limited. To ensure predictions are human-generated rather than machine-generated, users must register with phone number, enabling tracking of individual activities. Using this platform, we collected GeoComp, comprehensive dataset covering 1,000 days of user competition."
        },
        {
            "title": "3.3 Human Player Performance\nOur dataset not only includes image and location information but\nalso rich human player performance data on the task. This label\ninformation serves not only as a valuable metric for evaluating\nthe difficulty of different tasks but also as a benchmark for under-\nstanding human decision-making in geolocation challenges. In this\nsubsection, we analyze the performance score across players and\ncountries, providing insights into how human players perform on a\nglobal scale and how their accuracy varies across different regions\nand task types. We use GeoGuessr‚Äôs scoring formula to evaluate a\nuser‚Äôs performance on a single task:\n(cid:17)",
            "content": "(cid:106) (cid:107) (cid:16) ùëÜ = exp ùëë ùë†ùëë 5000 . Here, ùëÜ is the users score (0 to 5000), ùëë is the geographic distance between the predicted and actual locations (in kilometers), and ùë†ùëë is the maximum distance within the area divided by 10. The score decreases exponentially as ùëë increases. For example, ùë†ùëë is 1805 km globally (based on Earths maximum distance of 18,050 km) and 615 km for China, reflecting smaller scales. perfect prediction (ùëë = 0) yields ùëÜ = 5000. players performance score is defined as the average score across all their tasks. Similarly, countrys performance score is the average score across all tasks performed within that country. Player Performance Across Levels. The performance of game players across different levels, as illustrated in Figure 3, highlights significant gaps between beginners and experts in mainstream countries. Expert players, defined as the top 15% of performers, consistently achieve much higher performance metrics compared to beginners, defined as the bottom 15%, with noticeable gaps in countries like Canada, China, and India. For example, in Canada, experts perform nearly 10 times better than beginners, underscoring the steep learning curve involved in mastering the game. This performance gap presents challenges for new players, as it emphasizes the level of skill, strategy, and game knowledge required to compete effectively at higher levels. Player Performance Across Countries. The player performance across countries, as shown in Figure 3, demonstrates significant variations influenced by three key factors: climate, geographic size, and language. Players tend to perform well in countries such Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework Conference17, July 2017, Washington, DC, USA Figure 4: Comparison of previous geolocation tasks and our proposed paradigm: while previous works focused on coarse-grained predictions limited by dataset quality, our generation and reasoning-based method enables fine-grained city-level predictions. as Germany, France, and Japan. These nations are characterized by unique languages and relatively small geographic sizes. The presence of distinctive languages on urban street signs provides clear linguistic clues, enabling players to quickly identify the country. Additionally, the compactness of these countries allows for more precise guesses, resulting in higher scores. In contrast, despite Chinas unique language, its vast size and diverse climates make pinpointing specific locations challenging, leading to lower scores. Similarly, large countries like the USA, China, and Canada face additional challenges due to their shared temperate climates and extensive territories, where players often confuse them due to similar vegetation and climate, reducing accuracy. Player Performance Across Tasks. From Figure 3, we can also observe significant variations in player performance across different tasks. In certain countries, player performance is relatively low, while in others, it is notably higher. This highlights the diversity in task difficulty within our dataset, offering valuable insights for assessing and categorizing task complexity."
        },
        {
            "title": "4.1 Rethinking Geolocation Task\nThe geolocation task has traditionally relied on classification-based [9,\n41, 54] and retrieval-based methods [62, 76], as shown in Figure 4.\nThere are also some works that combine these two paradigms to-\ngether such as [16]. While these approaches have advanced the\nfield, they face significant limitations in precision and scalability,\nprompting a rethinking of the task. Classification-based methods di-\nvide the Earth into grid cells, predicting an image‚Äôs cell and using its\ncenter as the location. While simplifying the problem, this approach\nlimits granularity, and increasing cell numbers adds computational\ncomplexity, hindering global use. Retrieval-based methods rely on\ngeographically tagged databases but face scalability issues and poor\nperformance in underrepresented regions, limiting effectiveness.",
            "content": "Inspired by how humans gradually narrow down locations from broad to fine-grained observations [34], we propose new geolocation paradigm: predicting geographic locations through stepby-step reasoning process. Unlike traditional approaches limited by grid-based classification and exhaustive databases, our model generates natural language reasoning, guiding it to the final predicted city. To implement this paradigm, we introduce GeoCoT (Geographic Chain-of-Thought), framework designed for both interpretability and accuracy."
        },
        {
            "title": "4.2 GeoCoT Deisgn\nSpecifically, our design of GeoCoT is inspired by the way humans\napproach geolocation tasks, progressing from broad to fine-grained\nanalysis. When tasked with identifying a location from an image,\nhumans naturally begin with macro-level observations, such as\ngeographic features or climate, and progressively narrow their focus\nto country-specific, city-specific, and ultimately micro-level details.\nGeoCoT mimics this hierarchical reasoning to achieve accurate and\ninterpretable geolocation predictions.",
            "content": "Concretely, GeoCoT operates in five sequential stages: 1. Continental or Climate Zone Identification. The process begins with identifying broad regions using natural features like mountains, vegetation, or soil, narrowing the scope to continent or climate zone. 2. Country-Level Localization. Cultural markers, language on signs, and architectural styles are analyzed to refine predictions to the country level. 3. City-Level Refinement Using Infrastructure. Street elements, such as driving direction, bollards, and license plate colors, are used to locate specific cities or regions. 4. LandmarkBased Verification. Features like fire hydrants, guideposts, and street signs help validate and further refine the predicted location. 5. Fine-Grained Micro-Level Validation. Finally, subtle details such as sidewalk patterns and clothing styles confirm precise localization at city or neighborhood level. Detailed prompts can be found in Appendix A. The answers generated at each of these stages are combined to produce the final predicted city, ensuring comprehensive and step-by-step reasoning process guided by LVMs. By emulating human reasoning, GeoCoT surpasses traditional methods by avoiding coarse classification grids and exhaustive image databases, providing scalable, interpretable, and accurate solution Conference17, July 2017, Washington, DC, USA Trovato et al. Table 2: Comparison of Precision, Recall and F1 scores in country-level and city-level geolocation. The scores are represented as follows: best , second , and third . Bold values indicate that our model achieves the best performance. Model City Accuracy Recall F1 Country Accuracy Recall LLaVA-1.6 LLama-3.2-Vision Qwen-VL GeoCLIP GeoReasoner GPT-4o GPT-4o(CoT) GeoCoT 0.002 0.081 0.016 0.018 0.018 0.092 0.094 0.118 0.001 0.037 0.013 0.007 0.014 0.048 0.052 0. 0.002 0.030 0.014 0.008 0.012 0.044 0.042 0.086 0.041 0.630 0.069 0.550 0.092 0.615 0.623 0.640 0.019 0.199 0.042 0.197 0.053 0.188 0.194 0.260 F1 0.028 0.217 0.070 0.204 0.085 0.208 0.212 0.291 Continent Accuracy Recall 0.175 0.866 0.130 0.872 0.208 0.807 0.819 0.862 0.067 0.643 0.115 0.746 0.161 0.468 0.430 0.638 F1 0.056 0.639 0.077 0.731 0.144 0.487 0.449 0.646 for geolocation tasks. It is important to note that GeoCoT does not require any concrete knowledge about the specific features of locations. Instead, it offers reasoning tutorials designed to help LVMs identify geographic clues by leveraging their existing knowledge."
        },
        {
            "title": "5 Experiments\nIn this section, we first introduce our experimental settings, then\nevaluate GeoCoT in terms of its general geolocalization ability,\nfollowed by a detailed evaluation of its reasoning process.",
            "content": "coordinates using retrieval-based approach to enhance geolocation. GeoReasoner [24] combines geospatial reasoning with visuallanguage alignment for state-of-the-art geolocation performance. Note that we report all baseline results on traditional benchmarks and partially on our dataset, as some models are not open-source, and others operate under different settings from our task. ClosedSource VLMs: GPT-4o [39] excels in vision reasoning tasks with its advanced multi-modal capabilities. Furthermore, GPT-4o with CoT [52] leverages chain-of-thought reasoning to improve zeroshot geolocation performance in complex scenarios."
        },
        {
            "title": "5.3 Overall Performance Evaluation of GeoCoT\nIn this subsection, we evaluate the city location prediction perfor-\nmance of our model in comparison with the latest LVM models. We\nevaluate geolocation performance from two aspects: first, location\nprediction compared with the ground truth at various levels; and\nsecond, the direct calculation of the Earth‚Äôs surface distance. We\npresent the location prediction performance in Table 2, evaluated\nacross three levels: city, country, and continent. Performance is mea-\nsured using accuracy, which calculates the proportion of correct\npredictions out of all predictions; recall, which determines the pro-\nportion of true positive predictions out of all actual positive cases;\nand the F1 score, which balances precision and recall to provide\ntheir harmonic mean.",
            "content": "The results reveal several key observations. First, open-source LVMs such as LLaMA-3.2-Vision achieve competitive performance, performing on par with GPT-4o and GPT-4o (CoT), demonstrating their effectiveness in location prediction tasks. Second, performance varies across different levels of granularity. While GPT-4o (CoT) ranks second at the city level, it underperforms at the country level, highlighting the importance of multi-level evaluation to fully assess models geolocation reasoning ability. Finally, our model, GeoCoT, consistently achieves top performance across all nine metrics and three levels, demonstrating its robustness and adaptability in geolocation tasks. Additionally, GeoCLIP surpasses GPT-4o at the continent level, which can be attributed to its pretraining on imageGPS pairs, making it particularly well-suited for coarse-grained geolocation tasks. Coarse-grained continent-level predictions typically require less detailed local knowledge and instead rely on broader geographic cues, such as climate, landscapes, and cultural Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework Conference17, July 2017, Washington, DC, USA markers. However, GeoCLIP performs poorly at finer granularities like country and city levels, suggesting that it lacks strong capability for geographic reasoning beyond direct visual features. Table 3: Accuracy of different models on geolocation tasks at various scales. Model LLaVA-1.6 Llama-3.2-Vision Qwen-VL GeoCLIP GeoReasoner GPT-4o GPT-4o(CoT) GeoCoT Street Country City 1km 25km 750km 0.082 0.020 0.006 0.638 0.104 0.018 0.090 0.014 0.004 0.625 0.077 0.035 0.128 0.020 0.010 0.678 0.147 0.045 0.701 0.151 0.047 0.711 0.157 0.073 Next, in Table 3, we present the accuracy of each model by measuring the geographic distance between the predicted city and the ground truth. The metrics represent the proportion of predictions within three distance thresholds: Street (1 km), City (25 km), and Country (750 km). Higher values indicate better performance, with stricter thresholds assessing fine-grained localization and larger thresholds evaluating coarse-level accuracy. The results show that GPT-4o and Llama-3.2-vision outperform the dedicated large-scale model GeoCLIP for geolocation, even under finer-grained evaluation settings. For example, at the street-level threshold, GPT4o achieves 0.045 compared to GeoCLIPs 0.035, and at the citylevel threshold, GPT-4o scores 0.147, nearly double GeoCLIPs 0.077. Moreover, our proposed GeoCoT paradigm demonstrates even greater improvements. At the street level, GeoCoT achieves 0.073, significantly outperforming both GeoCLIP (0.035) and GPT4o (0.045). Similarly, at the city level, GeoCoT achieves 0.157, and at the country level (750 km), it achieves 0.711, the highest among all models. These results highlight GeoCoTs strong performance and the potential of its reasoning framework for geolocation tasks."
        },
        {
            "title": "Reasoning",
            "content": "Beyond evaluating overall task performance, we focus on analyzing the reasoning process of GeoCoT, which emulates human-like reasoning approach. To establish reference for this evaluation, three gaming enthusiasts collaboratively constructed reasoning processes for the same 500 cases based on geo-tagged locations, which we designated as the ground truth (GT). These GT annotations serve as benchmark within our evaluation framework, GeoEval. The evaluation process utilizes GPT-based assessment through GPTScore [13] and prompt-based scoring across three custom-defined aspects. The first dimension is the completeness of feature extraction (CE), which evaluates whether all key clues provided in the GT are comprehensively covered and accurately described in the reasoning process. Comprehensive feature extraction ensures that reasoning outcomes are based on sufficient factual evidence, thereby enhancing their reliability and accuracy. The second dimension is the accuracy of feature extraction (AE), which measures whether the identified and described attributes or characteristics of the key information in the GT are correct. Misidentified features can lead to reasoning outcomes that deviate from the facts, reducing the credibility of the results. The third dimension is the accuracy of reasoning and cue correspondence (AC), which assesses whether the reasoning process derives reasonable conclusions based on the extracted cues and maintains consistency with the reasoning logic presented in the GT. Incorrect correspondence between cues and conclusions can result in outcomes that deviate from reality. The final dimension is the logical coherence of reasoning (LC), which evaluates the consistency, logical flow, and adherence to common sense within the reasoning chain. Logical errors compromise the reliability of the reasoning process and hinder the models ability to arrive at accurate conclusions. Table 4: Evaluation of GeoCoTs reasoning process using ground truth-based metrics within the GeoEval framework. Model Similarity GPTScore GeoEval CE AE AC LC LLaVA-1.6 Llama-3.2-Vision Qwen-VL GeoReasoner GPT-4o GPT-4o(CoT) GeoCoT 0.478 0.566 0.371 0.424 0.613 0.663 0.728 1.262 2.203 1.231 1.421 2.320 2.462 2.690 1.271 2.386 1.255 1.533 2.891 3.136 3.538 1.446 2.558 1.453 1.719 2.809 3.156 3.696 1.490 2.721 1.484 2.038 3.143 3.540 3.945 The experimental results in Table 4 highlight the significant advantages of GeoCoT compared to baseline models across all evaluation metrics. GeoCoT achieves the highest GPTScore of 0.728, outperforming GPT-4o (CoT) (0.663) and -1.6 (0.478), demonstrating its superior alignment with human-constructed reasoning processes. In terms of feature extraction, GeoCoT achieves CE score of 2.690 and an AE score of 3.538, significantly surpassing GPT-4o (CoT) and the dedicated GeoReasoner model. Furthermore, GeoCoTs performance in reasoning accuracy and logical coherence is unmatched, with AC and LC scores of 3.696 and 3.945, compared to GPT-4o (CoT), which scores 3.156 and 3.540, and GeoReasoner, which lags behind at 1.719 and 2.038. These results clearly demonstrate that GeoCoT not only captures key information more comprehensively but also maintains more accurate and logically coherent reasoning process compared to both reasoning-based models and traditional baselines like GeoReasoner."
        },
        {
            "title": "5.5 Intrinsic Evaluation of GeoCoT Reasoning\nThe ground truth-based evaluation compares GeoCoT‚Äôs reason-\ning to human-crafted processes, assessing its alignment with es-\ntablished reasoning patterns. Moreover, to gain a more holistic\nunderstanding, we also evaluate the reasoning process from an\nintrinsic perspective, examining its logical consistency, coherence,\nand robustness without relying on external references. This intrin-\nsic evaluation is conducted through human evaluation, as it requires\nmultimodal assessment and certain hallucinations are difficult for\nVLMs to identify effectively.",
            "content": "Conference17, July 2017, Washington, DC, USA Trovato et al. Figure 5: Qualitative comparison of LLaVA, GPT4o, and GeoReasoner. Clues are shown in blue, correct predictions in green, incorrect in red, and vague/uncertain guesses in orange. Following previous work on assessing hallucinations in terms of objects, attributes, and relationships [25, 46], we evaluate the quality of synthetic data across three key dimensions: (1) Object Hallucination(OH): assesses whether the synthetic data includes objects that do not exist in the image. Object Hallucination evaluates the extent to which synthetic data introduces fictional elements. (2) Fact Hallucination(FH) measures the accuracy of factual information within the synthetic data. Fact Hallucination occurs when the synthetic data contains facts, figures, or other information that is incorrect or not supported by the original data. (3) Attribution Hallucination(AH) evaluates whether the synthetic data incorrectly attributes properties, characteristics, or relations to entities or objects. To evaluate these dimensions, we invited 2 human annotators with professional backgrounds in geographic reasoning and data validation to assess GPT-4o, GeoReasoner, and our proposed GeoCoT model. These three baselines provide textual reasoning processes across 1,500 evaluated cases. The results, shown in Table 5, indicate the number of errors in each dimension, demonstrating that GeoCoT significantly reduces hallucination errors compared to the other models."
        },
        {
            "title": "5.6 Case Study\nWe present two examples in Figure 5 to analyze the performance\nof LLaVA, GPT4o, and GeoReasoner, highlighting the effectiveness\nof our GeoCoT approach. In the first example, struggles to provide\na specific prediction, reflecting its reliance on general architectural\ncues and its tendency to consider broad regions such as the United\nKingdom or France. GPT4o, despite identifying key features of the",
            "content": "Table 5: Hallucination Evaluation on Reasoning Data. Model OH FH Count Count Count AH GeoReasoner GPT-4o GeoCoT 237 43 5 151 4 1 203 35 18 Table 6: Performance comparison of GeoCoT and state-ofthe-art geolocation models on traditional benchmarks. Model LLaVA-1.6 Llama-3.2-Vision Qwen-VL GeoCLIP GeoReasoner PlaNet CPlaNet Translocator GeoDecoder GPT-4o GPT-4o(ZS-CoT) GeoCoT Street 1km 0.04 0.09 0.04 0.17 0.05 0.08 0.17 0.20 0.22 0.13 0.16 0.22 Im2GPS City 25km 0.18 0.37 0.21 0.41 0.19 0.25 0.37 0.48 0.50 0.47 0.49 0.55 Country 750km 0.39 0.65 0.37 0.77 0.33 0.54 0.62 0.76 0.80 0.74 0.77 0.83 Street 1km 0.03 0.07 0.04 0.13 0.04 0.09 0.10 0.12 0.13 0.14 0.14 0.15 Im2GPS3K City 25km 0.14 0.27 0.15 0.32 0.15 0.25 0.27 0.31 0.34 0.40 0.45 0.46 Country 750km 0.32 0.52 0.26 0.67 0.26 0.48 0.49 0.59 0.61 0.66 0.69 0. European landscape, incorrectly associates them with Germany, indicating limitations in handling specific regional markers. In Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework Conference17, July 2017, Washington, DC, USA contrast, GeoCoT accurately pinpoints the location in France by effectively integrating textual clues, architectural elements, and environmental context. In the second example, GeoCoT correctly identifies the location as San Francisco, USA, by analyzing U.S. traffic standards, license plates, and local signage, demonstrating strong contextual reasoning. LLaVA-1.6 makes broad prediction, covering the U.S., Australia, and the U.K., showing uncertainty from general cues. GPT-4o misidentifies the scene as Seattle, relying on architectural similarities but missing key details."
        },
        {
            "title": "5.7 Generalizability Evaluation\nEven though our dataset is more comprehensive and human-annotated,\nwe are also interested in evaluating how our model performs on tra-\nditional geolocation datasets to provide a more thorough compari-\nson. Hence, we select two existing benchmark datasets, Im2GPS [18]\nand Im2GPS3K [51], due to their popularity and widespread use\nin geolocation tasks as standard benchmarks for evaluating model\nperformance. Similarly, we use the center point coordinates of the\ncity text address in GeoCoT‚Äôs output and measure the distance\nbetween the output and the ground truth locations.",
            "content": "We present the performance results in Table 6. We observe that state-of-the-art geolocation models, such as GeoCLIP, achieve performance on traditional geolocation tasks that surpass GPT-4o and are almost comparable to our model, GeoCoT. However, this result is inconsistent with the performance shown in Table 2, where GeoCLIP significantly underperforms GPT-4o on fine-grained city and country-level geolocation tasks. This indicates that these baseline models are overfitting on specific datasets and highlights the importance of comprehensive evaluation across different datasets. In contrast, our model consistently outperforms traditional methods across different granularity levels and datasets."
        },
        {
            "title": "6 Conclusion\nIn this work, we present the largest geolocation dataset to date, col-\nlected from a geolocation game platform with 740K users over two\nyears. The dataset comprises 25M entries of metadata, including\n3M geo-tagged locations spanning most of the globe, each anno-\ntated thousands to tens of thousands of times by human users. This\ndataset enables diverse difficulty-level analysis and highlights the\nlimitations of current LVMs. We also introduce a generation-based\nreasoning solution for the geolocation task, where the LVM gener-\nates reasoning chains by leveraging clues from images and produces\nthe final predicted location. Using our GeoEval set of metrics, we\ndemonstrate that our GeoCoT framework significantly outperforms\nstate-of-the-art general and task-specific baselines on this dataset.\nIn future work, we plan to enhance model interpretability and\nrobustness, explore multi-modal integration of text and visuals, and\nexpand the dataset to better cover underrepresented regions for\nimproved global coverage and fairness.",
            "content": "Data Ethics The creation and release of our dataset adhere to stringent ethical standards to ensure the privacy and well-being of all contributors. We have conducted rigorous anonymization of the dataset to protect user privacy. All personally identifiable information, such as usernames, email addresses, and IP addresses, has been permanently removed. Only non-identifiable behavioral data, such as prediction outcomes and timestamps, are retained. The dataset originates from user participation on our open-source geolocation game platform. Users were informed during the registration process that their activity data might be used for research purposes. This ensures transparency in data collection and maintains user trust. We have explicitly designed the dataset for research purposes, with the sole intention of advancing geolocation and related artificial intelligence technologies. Importantly, our dataset does not include the images directly but instead provides links to images hosted on platforms such as Google Maps or Baidu Maps, which can be accessed through their official APIs. We are committed to ensuring the responsible use of this dataset. Researchers accessing the data must agree to data usage agreement that prohibits unethical or illegal use. Detail of GeoCoT We present the detailed prompt of our GeoCoT process below: Question1: Are there prominent natural features, such as specific types of vegetation, landforms (e.g., mountains, hills, plains), or soil characteristics, that provide clues about the geographicalregion? Question2: Are there any culturally, historically, or architecturally significant landmarks, buildings, or structures, or are there any inscriptions or signs in specific language or script that could help determine the country or region? Question3: Are there distinctive road-related features, such as traffic direction (e.g., left-hand or righthand driving), specific types of bollards, unique utility pole designs, or license platecolors and styles, which countries are known to have these characteristics? Question4: Are there observable urban or rural markers (e.g., street signs, fire hydrants guideposts) , or other infrastructure elements, that can provide more specific information about the country or city? Question5: Are there identifiable patterns in sidewalks (e.g., tile shapes, colors, or arrangements), clothing styles worn by people, or other culturally specific details that can help narrow down the city or area? Lets think step by step. Based on the question provided, locate the location of the picture as accurately as possible. Identify the continent, country, and city, and summarize it into paragraph. For example: the presence of tropical rainforests, palm trees, and red soil indicates tropical climate... Signs in Thai, right-side traffic, and traditional Thai architecture further suggest it is in Thailand... Combining these clues, this image was likely taken in city in Bangkok, Thailand, Asia. Here, cyan highlights potential clues within the image to help the model infer geographic locations. Green defines the geographic scope inferred from the clues, such as region, country, or city. Orange provides detailed descriptions of the cyan clues, enhancing the models understanding. Red specifies the expected output format, including city, country, and continent. Human Annotation Example Below we show an example of human annotated ground truth to demonstrate the annotation process, criteria, and the reasoning behind the annotations, where clues are shown in blue, correct predictions in green. Conference17, July 2017, Washington, DC, USA Trovato et al. The image shows rural residential area with dense trees and expansive green lawns. The terrain is flat, and the soil is reddish-brown, which matches the temperate climate of central Europe, particularly rural areas of France. The architectural style of the house is distinctive: red-tiled sloped roof, yellow walls, and solar panels, reflecting the regions focus on renewable energy, common feature in French countryside homes. The red mailbox at the gate is hallmark of rural French residences. The design of the fences and modern gates aligns with typical styles in the French countryside. The house design and surrounding natural environment suggest rural European region. Based on the architectural style, natural landscape, and street elements, the image was most likely taken in Aumont, France, Europe. References [1] Guillaume Astruc, Nicolas Dufour, Ioannis Siglidis, Constantin Aronssohn, Nacim Bouia, Stephanie Fu, Romain Loiseau, Van Nguyen Nguyen, Charles Raude, Elliot Vincent, et al. 2024. OpenStreetView-5M: The Many Roads to Global Visual Geolocation. In Proc. of CVPR. 2196721977. [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv:2309.16609 (2023). [3] Shivangi Bithel and Srikanta Bedathur. 2023. Evaluating Cross-modal generative models using retrieval task. In Proc. of SIGIR. 19601965. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Proc. of NeurIPS (2020), 18771901. [5] Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Xing Luo, Chenyu Yi, and Alex Kot. 2023. Benchlmm: Benchmarking cross-style visual capability of large multimodal models. arXiv preprint arXiv:2312.02896 (2023). [6] Athanasios Chalvatzaras, Ioannis Pratikakis, and Angelos Amanatiadis. 2022. Survey on Map-Based Localization Techniques for Autonomous Vehicles. IEEE Transactions on Intelligent Vehicles (2022), 15741596. [7] Wenqing Cheng, Ruxue Wen, Haojun Huang, Wang Miao, and Chen Wang. 2022. OPTDP: Towards optimal personalized trajectory differential privacy for trajectory data publishing. Neurocomputing (2022), 201211. [8] Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vicente Vivanco Cepeda, and Mubarak Shah. 2023. Where we are and what were looking at: Query based worldwide image geo-localization using hierarchies and scenes. In Proc. of CVPR. 2318223190. [9] Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vicente Vivanco Cepeda, and Mubarak Shah. 2023. Where We Are and What Were Looking At: Query Based Worldwide Image Geo-Localization Using Hierarchies and Scenes. In CVPR. 2318223190. [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Proc. of NeurIPS (2024). [11] Ishita Dasgupta, Andrew Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051 (2022). [12] Zhiyu Fang, Shuai-Long Lei, Xiaobin Zhu, Chun Yang, Shi-Xue Zhang, Xu-Cheng Yin, and Jingyan Qin. 2024. Transformer-based reasoning for learning evolutionary chain of events on temporal knowledge graph. In Proc. of SIGIR. 7079. [13] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166 (2023). [14] Jiaxin Ge, Hongyin Luo, Siyuan Qian, Yulu Gan, Jie Fu, and Shanghang Zhang. 2023. Chain of Thought Prompt Tuning in Vision Language Models. (2023). [15] Lukas Haas, Silas Alberti, and Michal Skreta. 2023. Learning generalized zero-shot learners for open-domain image geolocalization. arXiv preprint arXiv:2302.00275 (2023). [16] Lukas Haas, Michal Skreta, Silas Alberti, and Chelsea Finn. 2024. Pigeon: Predicting image geolocations. In Proc. of CVPR. 1289312902. [17] Xiao Han, Xiangyu Zhao, Liang Zhang, and Wanyu Wang. 2023. Mitigating action hysteresis in traffic signal control with traffic predictive reinforcement learning. In Proc. of KDD. 673684. [18] James Hays and Alexei Efros. 2008. IM2GPS: estimating geographic information from single image. In CVPR. 18. [19] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 2023. An embodied generalist agent in 3d world. arXiv:2311.12871 (2023). [20] Sohail Ahmed Khan, Laurence Dierickx, Jan-Gunnar Furuly, Henrik Brattli Vold, Rano Tahseen, Carl-Gustav Linden, and Duc-Tien Dang-Nguyen. 2024. Debunking war information disorder: case study in assessing the use of multimedia verification tools. Journal of the Association for Information Science and Technology (2024). [21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Proc. of NeurIPS (2022), 2219922213. [22] LAION. 2023. Gpt-4v dataset. [23] Martha Larson, Mohammad Soleymani, Guillaume Gravier, Bogdan Ionescu, and Gareth JF Jones. 2017. The benchmarking initiative for multimedia evaluation: MediaEval 2016. IEEE MultiMedia (2017), 9396. [24] Ling Li, Yu Ye, Bingchuan Jiang, and Wei Zeng. 2024. GeoReasoner: Geolocalization with Reasoning in Street Views using Large Vision-Language Model. In Proc. of ICML. [25] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023). [26] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2023. LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models. arXiv preprint arXiv:2311.17043 (2023). [27] Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. 2024. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824 (2024). [28] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, et al. 2024. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935 (2024). [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved Baselines with Visual Instruction Tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Proc. of NeurIPS (2024). [31] Liu Liu and Hongdong Li. 2019. Lending orientation to neural networks for cross-view geo-localization. In Proc. of CVPR. 56245633. [32] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. 2024. TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document. arXiv:2403.04473 (2024). [33] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. 2024. Deepseek-vl: towards realworld vision-language understanding. arXiv preprint arXiv:2403.05525 (2024). [34] Grace Luo, Giscard Biamby, Trevor Darrell, Daniel Fried, and Anna Rohrbach. 2022. G3: Geolocation via Guidebook Grounding. In Proc. of EMNLP Findings. 58415853. [35] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. 2024. MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training. arXiv:2403.09611 (2024). [36] AI Meta. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Meta AI Blog. Retrieved December (2024), 2024. [37] Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, et al. 2019. The StreetLearn environment and dataset. arXiv preprint arXiv:1903.01292 (2019). [38] Eric M√ºller-Budack, Kader Pustu-Iren, and Ralph Ewerth. 2018. Geolocation Estimation of Photos using Hierarchical Model and Scene Classification. In Proc. of ECCV. 563579. [39] OpenAI. 2024. GPT-4o System Card. (2024). [40] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. 2024. Grounding Multimodal Large Language Models to the World. In Proc. of ICLR. [41] Shraman Pramanick, Ewa Nowara, Joshua Gleason, Carlos Castillo, and Rama Chellappa. 2022. Where in the World is this Image? Transformer-based Geo-localization in the Wild. In Proc. of ECCV. 196215. [42] Shraman Pramanick, Ewa Nowara, Joshua Gleason, Carlos Castillo, and Rama Chellappa. 2022. Where in the world is this image? transformer-based geo-localization in the wild. In Proc. of ECCV. 196215. [43] Paul Hongsuck Seo, Tobias Weyand, Jack Sim, and Bohyung Han. 2018. CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps. In Proc. of ECCV. 536551. [44] Paul Hongsuck Seo, Tobias Weyand, Jack Sim, and Bohyung Han. 2018. Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps. In Proc. of ECCV. 536551. [45] Zirui Song, Yaohang Li, Meng Fang, Zhenhao Chen, Zecheng Shi, Yuan Huang, and Ling Chen. 2024. Mmac-copilot: Multi-modal agent collaboration operating system copilot. arXiv preprint arXiv:2404.18074 (2024). [46] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv:2309.14525 (2023). Geolocation with Real Human Gameplay Data: Large-Scale Dataset and Human-Like Reasoning Framework Conference17, July 2017, Washington, DC, USA [47] Jonas Theiner, Eric M√ºller-Budack, and Ralph Ewerth. 2022. Interpretable se- [76] Sijie Zhu, Mubarak Shah, and Chen Chen. 2022. TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization. In CVPR. 11621171. [77] Yuanshao Zhu, Yongchao Ye, Ying Wu, Xiangyu Zhao, and James Yu. 2023. Synmob: Creating high-fidelity synthetic gps trajectory dataset for urban mobility analysis. Proc. of NeurIPS (2023), 2296122977. [78] Yuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, and James Yu. 2023. Difftraj: Generating gps trajectory with diffusion probabilistic model. Proc. of NeurIPS (2023), 6516865188. [79] Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian Zhang, Xuetao Wei, and Yuxuan Liang. 2024. Controltraj: Controllable trajectory generation with topology-constrained diffusion model. In Proc. of KDD. 46764687. mantic photo geolocation. In WACV. [48] Jonas Theiner, Eric M√ºller-Budack, and Ralph Ewerth. 2022. Interpretable Semantic Photo Geolocation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 750760. [49] Vicente Vivanco Cepeda, Gaurav Kumar Nayak, and Mubarak Shah. 2024. Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization. Proc. of NeurIPS (2024). [50] Nam Vo, Nathan Jacobs, and James Hays. 2017. Revisiting im2gps in the deep learning era. In Proc. of ICCV. 26212630. [51] Nam Vo, Nathan Jacobs, and James Hays. 2017. Revisiting IMG2GPS in the deep learning era. In ICCV. [52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Proc. of NeurIPS (2022), 2482424837. [53] Tobias Weyand, Ilya Kostrikov, and James Philbin. 2016. PlaNet - Photo Geolocation with Convolutional Neural Networks. 3755. [54] Tobias Weyand, Ilya Kostrikov, and James Philbin. 2016. PlaNet - Photo Geolocation with Convolutional Neural Networks. In Proc. of ECCV. 3755. [55] Tobias Weyand, Ilya Kostrikov, and James Philbin. 2016. Planet-photo geolocation with convolutional neural networks. In Proc. of ECCV. 3755. [56] Scott Workman, Richard Souvenir, and Nathan Jacobs. 2015. Wide-area image geolocalization with aerial reference imagery. In Proc. of ICCV. 39613969. [57] Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oƒüuz, James C. Gee, and Yixin Nie. 2023. The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task. ArXiv (2023). [58] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. arXiv:2312.13771 (2023). [59] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. 2023. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv:2307.02499 (2023). [60] Amir Roshan Zamir and Mubarak Shah. 2014. Image geo-localization based on multiplenearest neighbor feature matching usinggeneralized graphs. IEEE transactions on pattern analysis and machine intelligence (2014), 15461558. [61] Peng-Fei Zhang, Zi Huang, and Guangdong Bai. 2024. Universal adversarial perturbations for vision-language pre-trained models. In Proc. of SIGIR. 862871. [62] Xiaohan Zhang, Xingyu Li, Waqas Sultani, Yi Zhou, and Safwan Wshah. 2023. Cross-View Geo-Localization via Learning Disentangled Geometric Layout Correspondence. In Proc. of AAAI. 34803488. [63] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering. arXiv:2305.10415 (2023). [64] Zijian Zhang, Ze Huang, Zhiwei Hu, Xiangyu Zhao, Wanyu Wang, Zitao Liu, Junbo Zhang, Joe Qin, and Hongwei Zhao. 2023. MLPST: MLP is All You Need for Spatio-Temporal Prediction. In Proc. of CIKM. 33813390. [65] Zijian Zhang, Xiangyu Zhao, Qidong Liu, Chunxu Zhang, Qian Ma, Wanyu Wang, Hongwei Zhao, Yiqi Wang, and Zitao Liu. 2023. Promptst: Prompt-enhanced spatio-temporal multi-attribute prediction. In Proc. of CIKM. 31953205. [66] Zijian Zhang, Xiangyu Zhao, Hao Miao, Chunxu Zhang, Hongwei Zhao, and Junbo Zhang. 2023. Autostl: Automated spatio-temporal multi-task learning. In Proc. of AAAI. 49024910. [67] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. 2024. Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference. arXiv preprint arXiv:2403.14520 (2024). [68] Xiangyu Zhao, Wenqi Fan, Hui Liu, and Jiliang Tang. 2022. Multi-type urban crime prediction. In Proc. of AAAI. 43884396. [69] Xiangyu Zhao and Jiliang Tang. 2017. Modeling temporal-spatial correlations for crime prediction. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. 497506. [70] Xiangyu Zhao, Tong Xu, Yanjie Fu, Enhong Chen, and Hao Guo. 2017. Incorporating spatio-temporal smoothness for air quality inference. In Proc. of ICDM. 11771182. [71] Xiangyu Zhao, Tong Xu, Qi Liu, and Hao Guo. 2016. Exploring the choice under conflict for social event participation. In Proc. of DASFAA. 396411. [72] Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, and Jing Liu. 2023. Mamo: Fine-grained vision-language representations learning with masked multimodal modeling. In Proc. of SIGIR. 15281538. [73] Baichuan Zhou and Lei Huang. 2024. TinyLLaVA. [74] Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, and Gengchen Mai. 2024. Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation. In Proc. of SIGIR. 27492754. [75] Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang. 2024. Comprehensive Overhaul of Multimodal Assistant with Small Language Models. arXiv preprint arXiv:2403.06199 (2024)."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "Northeastern University"
    ]
}