{
    "paper_title": "Self-Improving VLM Judges Without Human Annotations",
    "authors": [
        "Inna Wanyin Lin",
        "Yushi Hu",
        "Shuyue Stella Li",
        "Scott Geng",
        "Pang Wei Koh",
        "Luke Zettlemoyer",
        "Tim Althoff",
        "Marjan Ghazvininejad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 5 4 1 5 0 . 2 1 5 2 : r Self-Improving VLM Judges Without Human Annotations Inna Wanyin Lin1,2,, Yushi Hu1, Stella Li1,2, Scott Geng2, Pang Wei Koh2, Luke Zettlemoyer1,2, Tim Althoff2, Marjan Ghazvininejad 1FAIR at Meta, 2University of Washington Work done at Meta Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present framework to self-train VLM judge model without any human preference annotations, using only selfsynthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instructionresponse pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves Llama-3.211B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for future self-judge that evolves alongside rapidly improving VLM capabilities. Correspondence: Inna Lin ilin@cs.washington.edu, Marjan Ghazvininejad ghazvini@meta.com Date: October 2,"
        },
        {
            "title": "1 Introduction",
            "content": "VLM reward models are critical for evaluating output quality and enabling alignment with human preferences Yu et al. (2025); Jing and Du (2024); Sun et al. (2023a); Chen et al. (2024). Existing approaches for training such model evaluators have primarily followed two directions: scaling up human preference collection and distilling from large closed-source models such as GPT and Claude (which also indirectly rely on significant human annotation) Li et al. (2024c); Xiong et al. (2025); Zhang et al. (2023); Zhou et al. (2024a). However, extensive human preference annotation is both costly and labor-intensive, and becomes obsolete as models advance and new tasks arise. In this paper, we show that it is possible to self train VLM judge given only the VLMs own generations. Our method requires no human preference annotations, thereby significantly reducing the cost of reward model training. The key idea is that it is possible to use simple, general purpose heuristics to find or construct VLM outputs with varied quality levels, which provide enough signal to train judge. Our approach follows three-step iterative process: Synthetic preference pair generation. We create synthetic preference pairs tailored to different question types. For questions with closed-ended answers (e.g. multiple choice or numerical problems) we generate many candidate responses and pair the majority answer with random alternative. For open-ended questions (e.g. like captioning) we generate responses and deliberately inject meaningful errors into one version, such as changing object attributes or spatial relationships. We create substantial differences rather than minor edits, exposing the model to realistic evaluation scenarios. Judge training data generation. We use the previous-iteration judge model to evaluate the newly synthesized preference pairs and gather the judges answer and reasoning traces. Since we know the preferred pair by construction, we retain only judgments that align 1 Figure 1 Self-improving VLM judge: iterative synthetic preference data generation and judge model fine-tuning pipeline. I. Synthetic preference pair generation. We create synthetic preference pairs tailored to different question types. For open solution space questions (long answers, captions), we generate an original response and deliberately inject meaningful errors to create less preferred version. For closed solution space questions (numerical, multiple choice, short phrases), we generate multiple candidates and pair the majority answer with random alternative. II. Iterative judge training data generation. We use the previous-iteration judge model to evaluate the newly synthesized preference pairs and gather the judges reasoning traces. We retain only judgments that align with our synthetic preferences. III. Judge model training. We fine-tune the previous-iteration judge model on these filtered reasoning traces. We iterate this three-step process several times. More details in 3. with our synthetic preferences. Judge model training. We fine-tune the previous-iteration judge model on these filtered reasoning traces and answers. We iterate this three-step process several times. We iteratively train small Llama-3.2-11B model Grattafiori et al. (2024) using our framework, and evaluate the model with VL-RewardBench Li et al. (2025b) and Multimodal RewardBench Yasunaga et al. (2025). Our method improves the model from 0.38 to 0.51 overall accuracy, often outperforming much larger models like Llama-3.2-90B and Claude 3.5 Sonnet Anthropic (2024) on VL-RewardBench. In Section 6, we systematically analyze the impact of synthetic data design and iterative refinement, providing insights into the conditions under which self-improvement is most effective for VLM judge training. Specifically, our synthetic data construction provides way to create preference dataset from any visual queries, even without any reference answers. This makes our framework applicable to new visual tasks where ground-truth annotations are unavailable or scarce, such as evaluating responses to novel image collections or emerging visual domains. Overall, our contributions include: method to create diverse synthetic preference data for VLMs without human preference annotations, using majority voting consensus for closed-ended tasks and controlled error injection for open-ended tasks. An iterative self-improvement framework that trains VLM judges by filtering and learning from self-generated reasoning traces, enabling iterative refinement of evaluation capabilities. Empirical demonstration that our approach enables compact 11B model to gain substantial improvements on several reward benchmarks, even surpass much larger models (90B) and proprietary systems (Claude 3.5 Sonnet) on VL-RewardBench. Analysis showing scaling trend over increasing iterations, and operates effectively without access to ground-truth answers, making it applicable to emerging visual domains where annotations are scarce or unavailable."
        },
        {
            "title": "2.1 LLM and VLM as a judge",
            "content": "LLM-as-a-judge can provide scalable reward signals for RL fine-tuning, reducing dependence on expensive human annotations while enabling more efficient preference learning. In text-only domains, large language models (LLMs) serve 2 as automatic evaluators for tasks including summarization, dialogue, and reasoning, often achieving high consistency with human annotators.Recent work has explored various training approaches for LLM-as-a-judge systems Zheng et al. (2023); Lee et al. (2023); Yuan et al. (2024); Cui et al. (2023); Lambert et al. (2024); Whitehouse et al. (2025); Saha et al. (2025). Similarly, vision-language models (VLMs) have been deployed as multimodal judges for captioning, visual question answering (VQA), and reasoning tasks. Training approaches for model evaluators have primarily followed two directions: scaling human preference collection and distilling knowledge from large closed-source models such as GPT and Claude (which themselves rely heavily on human annotation) Xiong et al. (2025); Wang et al. (2025); Li et al. (2025b); Yasunaga et al. (2025). Some works focus on building specialized judges for single dimensions like hallucination detection using synthetic data, while general-purpose judges typically require substantial amounts of preference data Jing and Du (2024); Sun et al. (2023a)."
        },
        {
            "title": "2.2 Synthetic Data for Model Self-Improvement",
            "content": "Synthetic data has emerged as central driver of self-improvement in both LLMs and VLMs, enabling models to generate, evaluate, and refine their own outputs. Instruction-tuning pipelines bootstrap new tasks from model-generated data to expand task coverage without human labels, while iterative refinement frameworks enable models to critique and improve their own responses Wang et al. (2022); Li et al. (2024b); Madaan et al. (2023); Shinn et al. (2023). Selfrewarding and self-play approaches further demonstrate that models can generate training signals to surpass supervised baselines Alemohammad et al. (2024). However, uncontrolled synthetic data generation can lead to degenerative feedback loops and preference leakage, underscoring the importance of careful curation Alemohammad et al. (2023); Li et al. (2025a); Liu et al. (2024b). The multimodal domain presents greater opportunities and unique challenges. While scarce and expensive multimodal annotations create stronger incentives for synthetic data generation, VLMs face distinctive issues including hallucination Sun et al. (2023b); Zhou et al. (2024a,c); Leng et al. (2023) and text-image misalignment Zhou et al. (2024b). Most such work relies on larger models to create synthetic data. Our work extends text-domain self-improvement methods to address multimodal-specific challenges of maintaining visual grounding and factual accuracy without human annotations or larger models."
        },
        {
            "title": "3 Method",
            "content": "Our self-improving multimodal judge framework consists of three key stages: synthetic preference pair generation (Section 3.1), constructing judge training data with filtering (Section 3.2), and iterative training (Section 3.3). The key idea in this process is to generate diverse data that best help the model learn how to judge."
        },
        {
            "title": "3.1 Synthetic Data Generation",
            "content": "The core challenge in training multimodal judges is obtaining high-quality preference pairs that capture the nuanced ways VLM outputs can fail. Human annotation is expensive and doesnt scale to new domains, while relying on stronger models for data generation creates dependency on external better models. We instead ask: can model generate its own training signal by creating and recognizing flawed outputs? While self-improvement methods have been explored in text domains, extending these approaches to multimodal settings introduces new challenges: the model must identify failures that are fundamentally visualsuch as incorrectly counting objects, misinterpreting spatial relationships, or hallucinating image content. Our approach is entirely self-containedthe same model generates and perturbs its own responses. This ensures the model learns directly from its internal knowledge rather than distilling from an external teacher. Critically, we design our synthetic data generation to create meaningful contrasts rather than superficial differences. We then know the preference direction by construction, eliminating the need to obtain ground truth labels. We broadly categorize evaluation tasks into two types based on the open-endedness of the answers. Datasets requiring captions or longer responses have open-ended answers that lack objectively verifiable solutions, while datasets with multiple choice, numerical answers, or short phrases have closed-ended answers with deterministic correct responses. 3 Formally, let = {(Ii, Qi)}n the corresponding question. We denote the open-ended subset as Dnv and the closed-ended subset as Dv. i=1 denote collection of image-question pairs, where Ii represents the i-th image and Qi"
        },
        {
            "title": "3.1.1 Detail Alteration for Open-Ended Task",
            "content": "Data in this category are typically captions or long answers to reasoning questions. Our approach generates pairs of responses where one contains subtle inaccuracies, creating natural preference ranking even when the absolute correctness cannot be verified. For each image-question pair, we produce two sets of responses using our base VLM. We designate one generation as the original response and use the other to create altered versions by prompting the model to introduce specific inaccuracies, such as swapping numbers or object attributes, while preserving the original style, structure, and length. By introducing these targeted errors, we create challenging cases where the altered responses remain contextually plausible and internally coherentmentioning objects or details that could reasonably appear in such scenes but are not actually present in the specific image. This tests known weakness in VLMs: the tendency to conflate what is visually plausible with what is visually present (Liu et al., 2024a; Zhou et al., 2024a). he judge must ground evaluations in fine-grained visual details rather than textual plausibility or language priors. list of prompts used to alter the generations can be found in Appendix C. For caption tasks specifically, we aim for diverse synthetic responses to increase variety in styles and lengths. This way we try to avoid that the base model always generate captions in its own default style. We randomly vary prompts with different length specifications (short or long) and style examples. The list of prompts we used to generate in two styles for captions can be found in Appendix D. The underlying assumption is that while we cannot verify absolute accuracy, systematically introducing errors creates reliable preference ranking where the original is preferred over the degraded version. Formally, for each datapoint (I, Q) Dnv, we generate two sets of responses using our base VLM Mbase. From the two generation sets, we designate one as the original response = Mbase(I, Q) and use the other to create altered versions. We then prompt Mbase to introduce inaccuracies into these generations, producing degraded responses . This generates preference pairs (I, Q, T, ) where is preferred over . Image Original Answer (T) Altered Answer (T) [City skyline] The image shows bustling downtown area with modern skyscrapers reaching heights of approximately 4050 stories. The glass facades reflect the afternoon sunlight. The image shows quiet suburban area with colonial buildings reaching heights of approximately 1015 stories. The brick facades absorb the evening moonlight. Table 1 Example of detail alteration for synthetic data generation"
        },
        {
            "title": "3.1.2 Majority Voting for Closed-Ended Tasks",
            "content": "In closed-ended answers, simply altering details in responses is insufficienterrors in logical reasoning cannot be effectively simulated by changing descriptive information. Moreover, for short answers without reasoning traces, directly changing the answer (e.g., as simple example, from The answer is to The answer is D) does not provide meaningful learning signal, as the resulting pairs can be trivially easy when the wrong answer is obviously incorrect. Therefore, we leverage the closed-ended nature of these answers by using majority voting to identify likely correct responses, which we then pair with randomly sampled alternatives as likely incorrect responses. For closed-ended answers (I, Q) Dv, we generate responses from the base model. We select as the response chosen by the majority of the generations, and randomly sample different answer = as the less preferred response, creating preference pairs (I, Q, , ). To ensure the quality of the majority answer, we only include examples where at least 5 responses are identical, filtering out cases with greater diversity."
        },
        {
            "title": "3.1.3 Correct Answer Filtering",
            "content": "As an alternative approach to majority voting, we also explore using ground truth labels from the original datasets to construct preference pairs. Note that these labels represent the correct answer to the question rather than human preferences over response quality. This provides an interesting point of comparison: while majority voting derives 4 supervision from the models own consistency, gold label filtering relies on external ground truth that may not always align with what the model can reliably distinguish. For each (I, Q) with gold label G, we generate responses and construct preference pairs (I, Q, +, ) where + matches the gold standard and does not."
        },
        {
            "title": "3.2 Training Data Generation",
            "content": "We sample the models own reasoning traces where judgments align with our synthetic preference assumptions, inspired by self-training literature Wang et al. (2024); Hu et al. (2024). For each preference pair (I, Q, +, ), we query the current judge (k) judge at iteration to generate judgments: {J1, J2, . . . , JN } = { (k) judge(I, Q, +, ) }N j=1 Each judgment Jj contains reasoning trace Rj and binary decision Dj {0, 1} indicating whether + is preferred. We construct our training set (k+1) by selecting only samples where the model correctly identifies the preference: (k+1) = {(I, Q, +, , Rj, 1) : Dj = 1} To mitigate positional biasthe tendency for models to favor the first response positionwe conduct this process with both orderings: (T +, ) (where the preferred response appears first) and (T , +) (swapped order), retaining only pairs where the model makes the correct judgment in both configurations. Correct judgments in both orderings indicate that the models reasoning signal is stronger than its positional bias, reducing cases where the model arrives at the right answer for the wrong reasons. Intuitively, we only listen to the models reasoning when it gets the answer right, bootstrapping from its own reasoning without amplifying mistakes. This focuses learning on reasoning patterns that consistently lead to correct judgments."
        },
        {
            "title": "3.3 Training",
            "content": "We use supervised fine-tuning to train the judge using the curated dataset (k+1). The training objective maximizes the likelihood of generating correct reasoning traces and judgments: = (cid:88) log (R, I, Q, +, ) (I,Q,T +,T ,R,D)T (k+1) The input format combines the image, question, and candidate responses, while the output includes both the reasoning trace and final judgment."
        },
        {
            "title": "4.1 Dataset",
            "content": "We construct our synthetic training set using the LLaVA-OneVision Li et al. (2024a), which covers diverse multimodal tasks including reasoning, math and coding, and captioning. To focus on multimodal evaluation, we restrict ourselves to the single-image subsets. To avoid dominance by any single sub-dataset, we impose cap of 10k on the number of examples drawn from each sub-dataset, resulting in 100k prompts. This ensures balanced coverage across categories while maintaining manageable data size."
        },
        {
            "title": "4.2 Evaluation",
            "content": "We evaluate our VLM judge and baselines on two comprehensive benchmarks: Multimodal RewardBench Yasunaga et al. (2025) and VL-RewardBench Li et al. (2025b). Both benchmarks measure judge performance across multiple dimensions including general instructions, hallucinations, correctness, reasoning, and VQA. Multimodal RewardBench comprises 5,211 preference pairs covering both long-form and short-answer formats, with labels derived from ground 5 Model Ave. Gen. Hallu. Reason. Ave. Gen. Know. Reason. Safe. VQA VLRB MMRB Larger Models Claude-3.5 GPT-4o Llama-90B 0.536 0.624 0.539 0.434 0.491 0.426 0.550 0.676 0.573 0.623 0.705 0. 0.721 0.713 0.618 0.652 0.658 0.642 Our Method (Based on Llama-3.2-11B-Vision-Instruct) 0.297 Base Iteration 1 0.398 0.453 Iteration 2 0.425 Iteration 3 Iteration 4 0.503 0.383 0.452 0.521 0.488 0.538 0.304 0.362 0.529 0.426 0.514 0.549 0.596 0.580 0.612 0. 0.499 0.519 0.521 0.538 0.539 0.590 0.625 0.591 0.599 0.591 0.739 0.720 0.612 0.506 0.532 0.506 0.540 0.556 0.669 0.649 0.547 0.517 0.500 0.513 0.543 0. 0.687 0.668 0.519 0.856 0.872 0.771 0.317 0.289 0.304 0.307 0.329 0.565 0.651 0.693 0.701 0.689 Table 2 Performance comparison on VLRB and MMRB benchmarks. Our iteratively trained judge model based on Llama-3.2-11B achieves competitive performance with significantly larger models across multiple evaluation dimensions. truth correctness or high-quality human expert annotations. VL-RewardBench spans general multimodal instructions, hallucination detection, and complex reasoning tasks, combining human-verified and AI-annotated preference labels to ensure comprehensive coverage of real-world scenarios."
        },
        {
            "title": "4.3 Implementation details",
            "content": "We fine-tune the Llama-3.2-11B-Vision-Instruct model for 5 epochs using learning rate of 1e-5 and batch size of 2 per GPU across 8 GPUs (effective batch size of 16). We employ FSDP for distributed training and enable fast kernels for computational efficiency. We use cross-entropy loss computed over the generated reasoning traces from each iteration cycle. We iterate the training process until overall performance plateaus, operationally defined as less than 1% relative improvement over three consecutive iterations on either of the benchmarks. In our experiments, we observe diminishing returns after iteration 4, though individual dimensions may peak at different iterations due to varying learning dynamics across task types. More details on the hyperparameters in data generation, sampling, and training can be found in Appendix E."
        },
        {
            "title": "5.1 Iterative Training Improvements",
            "content": "Table 2 demonstrates the effectiveness of iterative training on our Llama3-11B based judge model. On VLRB, our model improves from 0.383 (base) to 0.538 (iteration 4), representing 40.5% relative gain. On MMRB, we observe improvement from 0.499 (base) to 0.539 (iteration 4), 7.5% relative gain. The progression across iterations demonstrates relatively consistent improvement, though the improvement trajectory can vary across different dimensions. Compared with larger models, despite using only 11B parameters, our model achieves competitive performance on several dimensions: on general instruction following, our model (0.503) substantially outperforms the 90B parameter Llama-3.2-90B (0.426), Claude-3.5-Sonnet (0.434) and GPT-4o (0.491). Across VLRB hallucination and MMRB VQA, our model (0.514, 0.689) approaches larger models despite the base models significantly weaker starting performance (0.304, 0.565)."
        },
        {
            "title": "5.2 Performance Gains Across Dimensions",
            "content": "Our iterative training exhibits dimension-specific learning dynamics that reveal the differential effectiveness of our synthetic data generation strategies. On VLRB, general instruction following demonstrates the largest relative gain of 69% (0.297 0.503), indicating that our preference pair construction successfully captures the supervision signal necessary for open-ended multimodal instruction assessment. Hallucination detection on VLRB achieves 40.9% relative improvement (0.304 0.514), validating our detail alteration methodology for training factual consistency discrimination. Reasoning evaluation shows 8.6% relative improvement (0.549 0.596 at iteration 4), though exhibits 6 non-monotonic optimization behavior with peak performance at iteration 3 (0.612), suggesting potential signs of overfitting in later iterations. On MMRB, VQA tasks yield substantial gains (18.0% relative improvement, 0.565 0.689), confirming effective transfer from our majority voting-based preference construction for closed-ended answers. In contrast, several dimensions show relatively marginal improvement: reasoning capability increases from 0.517 to 0.531 (2.7% relative gain), general evaluation (averaging correctness and preference) stayed flat from 0.590 0.591, and safety evaluation demonstrates limited responsiveness (0.317 0.329). These marginal gains indicate potential distributional mismatch between our synthetic data and the supervision required for these specific capabilities."
        },
        {
            "title": "6 Analysis and Discussion",
            "content": "Our results show that self-improvement provides promising path for building multimodal judges without relying on costly human annotations or stronger teacher models. In this section, we reflect on the strengths and limitations of our approach."
        },
        {
            "title": "6.1 Scaling number of iterations",
            "content": "Figure 2 Judge model performance across training iterations. The left lanel shows average VLRB scores and the right panel shows average MMRB scores. After 4 iterations, our 11B judge model is comparable with Claude-3.5 and Llama-90B on VLRB. Our iterative training scheme enables the model to generate more training data pairs at each iteration as the judge models performance improves. Figure 3 shows that the percentage of retained training data increases from 19% at baseline to 43% by iteration 4, with the largest gains in early iterations. Beyond increasing the number of training examples, we hypothesize that the quality of reasoning also improves across iterations. Blind manual evaluation of 20 randomly sampled examples showed later iterations produced superior reasoning in 55% of cases versus 10% for early iterations, confirming genuine quality improvement beyond metrics. An demonstrative example of the reasoning progression for the same pairs across training iterations are shown in Appendix A. Iterative training yields substantial improvements, with the largest gains in the first iteration (10% average improvement across VLRB and MMRB) and continued progress in later iterations (3% average per iteration). However, improvement rates differ across dimensions. For instance, VLRB General and Hallucination show rapid, monotonic gains, while MMRB General Correctness exhibits fluctuations. These patterns reveal that different task domains respond to iterative refinement at different rates: some capabilities converge quickly and steadily with each training cycle, while others exhibit slower or more volatile learning dynamics. Figure 3 Increasing % of data sampled from each training iteration Figure 4 Performance comparing using majority voting and correct answer to filter synthetic pairs before sampling. For VLRB reasoning and MMRB VQA using majority voting to filter the synthetic pairs yields better performance after few iterations. When reducing data size to the same with correct answer filtering, majority performance also reduced."
        },
        {
            "title": "6.2 Analysis on reasoning / VQA: filtering with majority votes v.s.correct answers",
            "content": "In our preference pair synthesis, for reasoning / VQA tasks that have short answer, we sample 16 times, and pick the majority answer as the preferred answer, and randomly sample an answer that is different as the less preferred answer. For ablation analysis, we also experiment with using the oracle correct answers (coming from datasets) to select the preferred answer and the less preferred answer. Note that in the majority-voting setup, the majority answer may not be correct, especially in areas when the model struggles. In figure 4, we show the comparison on dimensions where closed-ended type questions are more relevant (i.e. multiple choices, numerical answers, short answers, not captions or long answers). On VLRB-Reasoning and MMRB-VQA, synthetic pairs constructed via majority voting yield 8.6% and 9.5% average improvement over correctness-filtered pairs. While this result may appear counterintuitive, the following underlying mechanism explains our observations. First, we find that the judge model consistently samples greater number of data points per iteration when using majority voting compared to correctness-filtered approaches.This increased sampling capacity provides richer training signal during each optimization cycle. As Figure 4 shows, constraining the data size of the majority set to match the correct set in each iteration reduces the performance of the original majority set. However, there still exists gap between the performance trained with the majority set and with the correct set, which leads to our second analysis. We hypothesize that even when the judge model selects the pair containing the correct answer, this may not necessarily ensure that the judges reasoning process is sound. For instance, in the example shown in Appendix 4, the judges reasoning prioritizes stylistic factors over factual accuracy, failing to identify that Response contains transcription error. Such patterns suggest that the correctness of the preferred answer does not guarantee the validity of the judgment reasoning. While using ground truth as filtering strategy improves judge performance in this setup, majority voting filters samples more efficiently and achieves even greater performance gains. Importantly, majority voting eliminates the dependency on ground truth labels, enabling the approach to scale to any image dataset by generating questions without requiring pre-existing answers."
        },
        {
            "title": "6.3 Analysis on other task dimensions",
            "content": "In 6.2, we shows the effectiveness of our method on reasoning and VQA tasks. In this section, we analyze the performance on other task dimensions. Specifically, we focus on the categories that exhibit the most and the least significant improvements. VLRB General & Hallucination. Our self-improving pipeline demonstrates particularly strong performance on VLRB general and hallucination categories. On VLRB general dimensions, our iteratively trained Llama-3.2-11B-based judge model outperforms both Llama-3.2-90B and Claude-3.5-Sonnet, while also achieving significant improvements on the hallucination dimension. These results are notable given that VLRB general draws from WildVision Lu et al. (2024) and VLFeedback Li et al. (2024c), which are datasets that capture authentic human preferences and diverse instruction-following scenarios from real-world VLM interactions. Our models strong performance on this category 8 Figure 5 The dimensions that showed the most significant improvements (VLRB General, Hallucination) and the least significant improvements (MMRB Safety, General). More details in 6.3. indicates that our synthetic data generation pipeline effectively captures learning signals reflected by these real-world datasets. MMRB General. The performance on MMRB General remains relatively flat compared to improvements observed in other dimensions. We hypothesize that this stems from the image diversity present in MMRBs benchmark composition. The general categories draw from two distinct datasets: NoCaps Agrawal et al. (2019) and VisitBench Bitton et al. (2023). After 4 training iterations, we observe consistent performance gains on VisitBench (correctness 0.54 0.64), whereas NoCaps performance has either declined or plateaued (correctness 0.58 0.52). This divergence suggests potential domain mismatch: NoCaps sources images from OpenImages Kuznetsova et al. (2020), which exhibits greater visual and object diversity for captioning tasks. The broader distribution of visual concepts in OpenImages likely challenges our models ability to generalize beyond the training distribution. This finding underscores that both task diversity and image diversity are critical factors in synthetic data generation. While our approach successfully expands task diversity through the LLaVA-OneVision dataset Li et al. (2024a), the results indicate that incorporating more diverse image sources could further enhance model robustness and generalization to visual domains such as those present in NoCaps. MMRB Safety. The improvement on safety benchmarks is not as consistent as observed in other evaluation metrics. This limitation likely stems from the fact that our training data generation process did not explicitly prompt the model to produce biased or toxic content for creating synthetic pairs. The inconsistent safety performance emphasizes the critical importance of task alignment in synthetic data generationsafety improvements require training data that specifically addresses potential risks and harmful outputs. Since safety evaluation typically demands specialized guardrails and redteaming approaches that extend beyond the scope of our current methodology, we leave comprehensive safety-focused synthetic data generation as an important direction for future work."
        },
        {
            "title": "7 Conclusion",
            "content": "We present self-improving framework for training vision-language model judges that eliminates the need for human preference annotations through strategic synthetic data generation and iterative refinement. Our approach demonstrates that compact 11B parameter model can achieve competitive performance with significantly larger proprietary systems across multiple evaluation dimensions, particularly excelling in general instruction following, hallucination detection, 9 and VQA tasks. Our analysis reveals clear pathways for further improvement through targeted data augmentation in specialized domains such as safety evaluation, while our core methodology remains applicable to emerging visual tasks where ground-truth annotations are scarce or unavailable."
        },
        {
            "title": "Limitations",
            "content": "Safety Evaluation Limitations. The modest improvements observed on safety benchmarks reflect deliberate methodological boundary: our framework does not generate synthetic training data containing biased, toxic, or otherwise harmful content. Robust safety evaluation requires specialized infrastructure including red-teaming protocols, carefully designed adversarial examples, and nuanced understanding of subtle policy violationscomponents that fall outside the scope of our general-purpose self-improvement approach. Developing effective safety evaluation capabilities within self-improving frameworks remains an important open challenge, one that will require principled approaches for safely generating and learning from examples of harmful content without amplifying risks. Task-Specific Performance Variation. Our synthetic data generation framework exhibits differential effectiveness across evaluation dimensions, reflecting the inherent complexity and diversity of multimodal judgment tasks. While substantial improvements emerge in general instruction following, hallucination detection, and visual question answering, other capabilities such as complex reasoning and nuanced preference judgments show more limited gains. This performance variance suggests potential benefits from mixture-of-experts architectures, where routing mechanism could direct different judgment queries to specialized expert judges trained on domain-specific synthetic data. Exploring such multimodel approaches represents promising direction for future work that could build upon our iterative selftraining framework, extending the principle of self-generated supervision signals to specialized judge models tailored for different evaluation dimensions"
        },
        {
            "title": "References",
            "content": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. Sina Alemohammad, Ziqi Huang, Tuomas Kynkäänniemi, Jascha Sohl-Dickstein, Alexander A. Alemi, Thibaut Durand, and Richard G. Baraniuk. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850, 2023. https://arxiv. org/abs/2307.01850. Sina Alemohammad, Ahmed Imtiaz Humayun, Shruti Agarwal, John Collomosse, and Richard Baraniuk. Self-improving diffusion models with synthetic data. arXiv preprint arXiv:2408.16333, 2024. Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/claude-3-family, March 2024. Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schmidt. Visit-bench: benchmark for vision-language instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. arXiv preprint arXiv:2310.01377, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95909601, 2024. Liqiang Jing and Xinya Du. Fgaif: Aligning large vision-language models with fine-grained ai feedback. arXiv preprint arXiv:2404.05046, 2024. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Harrison Lee, Sheng Shen, Mike Wu, David Ramirez, Serena Yeung, James Zou, and Dawn Song. Rlaif: Scaling reinforcement learning from ai feedback. arXiv preprint arXiv:2309.00267, 2023. https://arxiv.org/abs/2309.00267. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding, 2023. https://arxiv.org/abs/2311.16922. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024a. https://arxiv.org/abs/2408.03326. Di Li, Yining Xu, Zhenyu Zhao, Furu Wei, and Zhouhan Lin. Preference leakage: contamination problem in llm-as-a-judge. arXiv preprint arXiv:2502.01534, 2025a. https://arxiv.org/abs/2502.01534. Hao Li, Wei Liang, Yutong Chen, Shujian Zhang, Rui Wang, Ying Luo, and Jie Zhou. Generalized instruction tuning for language models. arXiv preprint arXiv:2405.15972, 2024b. https://arxiv.org/abs/2405.15972. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. Vlfeedback: large-scale ai feedback dataset for large vision-language models alignment. arXiv preprint arXiv:2410.09421, 2024c. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. Vl-rewardbench: challenging benchmark for vision-language generative reward models, 2025b. https://arxiv.org/abs/2411.17451. 11 Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning, 2024a. https://arxiv.org/abs/2306.14565. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai. Best practices and lessons learned on synthetic data, 2024b. https://arxiv.org/abs/2404.07503. Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. Advances in Neural Information Processing Systems, 37:4822448255, 2024. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. https://arxiv.org/abs/2303.17651. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. Learning to plan & reason for evaluation with thinking-llm-as-a-judge, 2025. https://arxiv.org/abs/2501.18099. Noah Shinn, Federico Cassano, Yonatan Bisk Labash, Kurt Shuster, Jason Weston, and Jing Liu. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. https://arxiv. org/abs/2303.11366. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf. 2023a. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023b. https://arxiv.org/abs/2309.14525. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators, 2024. https://arxiv.org/abs/2408.02666. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv:2212.10560, 2022. https://arxiv.org/abs/2212. 10560. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning, 2025. https://arxiv.org/abs/2505.10320. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1361813628, June 2025. Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. Multimodal rewardbench: Holistic evaluation of reward models for vision language models, 2025. https://arxiv.org/abs/2502.14191. Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, et al. Rlaif-v: Open-source ai feedback leads to super gpt-4v trustworthiness. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1998519995, 2025. Weizhe Yuan, Richard Yuanzhe Pang, et al. Self-rewarding language models. In Proceedings of the 41st International Conference on Machine Learning (ICML), Proceedings of Machine Learning Research, 2024. https://arxiv.org/abs/2401.10020. Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as generalist evaluator for vision-language tasks. arXiv preprint arXiv:2311.01361, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS 2023, 2023. https://arxiv.org/abs/2306.05685. Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning, 2024a. https://arxiv.org/abs/2402.11411. Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning, 2024b. https://arxiv.org/abs/2402.11411. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models, 2024c. https://arxiv.org/abs/2310.00754."
        },
        {
            "title": "A Training example from different iterations",
            "content": "13 Component Content Response Response Iteration 1 Reasoning Iteration 2 Reasoning Iteration 3 Reasoning The image shows blue double-decker bus driving down street in town or city. The bus is in the center of the image and has large front window and sign on the top that reads Brighton Open Top in yellow letters. The bus is driving away from the camera, and there are people standing on the sidewalk next to it. In the background, there are several buildings lining the street, including row of shops and few houses. The sky is overcast, and the overall atmosphere of the image is one of quiet, everyday scene in small town or city. The image shows red double-decker bus driving down street in town, surrounded by tall buildings and parked cars. The bus has red color and features large front window, with the words \"Brighton Open Top\" written in yellow on the front. The bus appears to be in motion, with the front wheels turned to the left as it navigates the street. In the background, the buildings are tall and white, with many windows and balconies. Some of the buildings have signs or advertisements on them, but they are not clearly visible. The sky above is blue and cloudy, adding to the overall atmosphere of the scene. Overall, the image captures typical urban scene, with the red bus being the main focus of attention. The surrounding buildings and parked cars add depth and context to the image, while the cloudy sky provides sense of atmosphere and mood. Both responses aim to describe the image, but they differ in their level of detail, accuracy, and relevance to the users question. Assistant As response provides clear and concise description, accurately identifies the bus as blue, and notes people on the sidewalk. Assistant Bs response provides more detailed description including the buss color (red) and parked cars, and describes the surrounding buildings height and signs. While both provide accurate information, Assistant Bs response is more detailed and engaging, making it better choice. The two responses have similarities but notable differences. Assistant As response is more detailed and descriptive, highlighting the red double-decker bus, tall buildings, and cloudy sky with simple, accessible language. Assistant Bs response is more concise and lacks detail, only mentioning the blue double-decker bus and surrounding buildings without additional context. The two responses differ significantly in accuracy, relevance, and depth. Assistant As response is more accurate and relevant, correctly identifying the bus as red and describing features like the large front window and \"Brighton Open Top\" text. It provides detailed description of surrounding buildings and parked cars, adding context. Assistant Bs response is less accurate, incorrectly identifying the bus as blue and failing to mention the red color. It lacks detailed description of surroundings, making it less informative. Table 3 Comparison of assistant responses and judge reasoning across training iterations for the double-decker bus image example. Response describes blue bus while Response describes red bus. The judges reasoning improves from iteration 1 (focusing on detail and engagement) to iteration 3 (explicitly identifying factual accuracy errors)."
        },
        {
            "title": "B Judge Prompt",
            "content": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the users instructions and answers the users question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: [[A]] if assistant is better, [[B]] if assistant is better. 14 [User Question] {q} [The Start of Assistant As Answer] {r1} [The End of Assistant As Answer] [The Start of Assistant Bs Answer] {r2} [The End of Assistant Bs Answer] YOU MUST STRICTLY FOLLOW THE FORMAT BELOW. Start with **EVALUATION** and make sure to add double brackets around the final answer. **EVALUATION**: Provide detailed comparison of both responses, analyzing their strengths and weaknesses based on the above factors. Be specific about why one response better serves the users needs. **FINAL ANSWER**: [[A]] if assistant is better, [[B]] if assistant is better."
        },
        {
            "title": "C Detail Alternation Prompts",
            "content": "INSTRUCTION=\"Below is text (description, reasoning, explanation, etc.). Your task is to generate version that incorporates alternative details while keeping original style and adhering to the following guidelines: 1. Detail Modifications: Replace exactly TWO specific details (e.g., visual elements, facts, examples, reasoning steps, methods, premises) with alternative details, or change the perspective on TWO features. 2. Length, Structure and Style: Keep the overall length, structure, and style identical to the original. 3. Core Identity & Function: Ensure the main subjects identity and function remain unchanged. 4. Tone & Detail Level: Maintain the original tone and level of detail. 5. Style Mimicry: Mirror the exact writing style, vocabulary choices, sentence patterns, and linguistic quirks of the original text.; Below is text (description, reasoning, explanation, etc.). Your task is to generate reinterpreted version that incorporates ONE alternative detail while adhering to the following guidelines: 1. Detail Modifications: Replace exactly ONE specific detail (e.g., visual element, fact, example, reasoning step, method, premise) with alternative details, or change the perspective on ONE feature. 2. Length, Structure and Style: Keep the overall length, structure, and style identical to the original. 3. Core Identity & Function: Ensure the main subjects identity and function remain unchanged. 4. Tone & Detail Level: Maintain the original tone and level of detail. 5. Style Mimicry: Mirror the exact writing style, vocabulary choices, sentence patterns, and linguistic quirks of the original text.; Below is text (description, reasoning, explanation, etc.). Your task is to generate reinterpreted version that incorporates TWO alternative details while adhering to the following guidelines: 1. Detail Modifications: Replace exactly TWO specific details (e.g., visual elements, facts, examples, reasoning steps, methods, premises) with alternative details, or change the perspective on TWO features. 2. Length, Structure and Style: Keep the overall length, structure, and style identical to the original. 3. Core Identity & Function: Ensure the main subjects identity and function remain unchanged. 4. Tone & Detail Level: Maintain the original tone and level of detail. 15 5. Style Mimicry: Mirror the exact writing style, vocabulary choices, sentence patterns, and linguistic quirks of the original text.; Below is text (description, reasoning, explanation, etc.). Your task is to generate reinterpreted version that incorporates 1-2 alternative details while adhering to the following guidelines: 1. Detail Modifications: Replace exactly 1-2 specific details (e.g., visual elements, facts, examples, reasoning steps, methods, premises) with alternative details, or change the perspective on 1-2 features. 2. Length, Structure and Style: Keep the overall length, structure, and style identical to the original. 3. Core Identity & Function: Ensure the main subjects identity and function remain unchanged. 4. Tone & Detail Level: Maintain the original tone and level of detail. 5. You MUST make the required modifications at the beginning of the text. 5. Style Mimicry: Mirror the exact writing style, vocabulary choices, sentence patterns, and linguistic quirks of the original text.; Below is text (description, reasoning, explanation, etc.). Your task is to generate reinterpreted version that incorporates 1-2 alternative details while adhering to the following guidelines: 1. Detail Modifications: Replace exactly 1-2 specific details (e.g., visual elements, facts, examples, reasoning steps, methods, premises) with alternative details, or change the perspective on 1-2 features. 2. Length, Structure and Style: Keep the overall length, structure, and style identical to the original. 4. Tone & Detail Level: Maintain the original tone and level of detail. 5. You MUST make the required modifications close to the END of the text. 5. Style Mimicry: Mirror the exact writing style, vocabulary choices, sentence patterns, and linguistic quirks of the original text.; Below is text (description, reasoning, explanation, etc.). Your task is to generate reinterpreted version that incorporates 1-2 alternative details while adhering to the following guidelines: 1. Detail Modifications: Replace exactly 1-2 specific details (e.g., visual elements, facts, examples, reasoning steps, methods, premises) with alternative details, or change the perspective on 1-2 features. 2. Length, Structure and Style: Keep the overall length, structure, and style identical to the original. 3. Core Identity & Function: Ensure the main subjects identity and function remain unchanged. 4. Tone & Detail Level: Maintain the original tone and level of detail. 5. You MUST make the required modifications close to the MIDDLE of the text, instead of at the beginning or the end. 5. Style Mimicry: Mirror the exact writing style, vocabulary choices, sentence patterns, and linguistic quirks of the original text.\""
        },
        {
            "title": "D Generation Meta Prompt for Captions",
            "content": "{ \"llava_image_description_prompts\": { \"concise\": [ \"Describe the image concisely.\", \"Provide brief description of the given image.\", \"Offer succinct explanation of the picture presented.\", \"Summarize the visual content of the image.\", \"Give short and clear explanation of the subsequent image.\", \"Share concise interpretation of the image provided.\", \"Present compact description of the photos key features.\", \"Relay brief, clear account of the picture shown.\", \"Render clear and concise summary of the photo.\", \"Write terse but informative summary of the picture.\", 16 \"Create compact narrative representing the image presented.\" ], \"detailed\": [ \"Describe the following image in detail\", \"Provide detailed description of the given image\", \"Give an elaborate explanation of the image you see\", \"Share comprehensive rundown of the presented image\", \"Offer thorough analysis of the image\", \"Explain the various aspects of the image before you\", \"Clarify the contents of the displayed image with great detail\", \"Characterize the image using well-detailed description\", \"Break down the elements of the image in detailed manner\", \"Walk through the important details of the image\", \"Portray the image with rich, descriptive narrative\", \"Narrate the contents of the image with precision\", \"Analyze the image in comprehensive and detailed manner\", \"Illustrate the image through descriptive explanation\", \"Examine the image closely and share its details\", \"Write an exhaustive depiction of the given image\" ] } } { \"caption_styles\": { \"concise\":{ \"chatgpt\": \"The image shows golden retriever sitting in sunny park with green grass and trees in the background. The dog appears happy and is looking directly at the camera with its tongue out. There are some fallen leaves scattered around, suggesting it might be autumn. The lighting is natural and warm, creating pleasant, cheerful atmosphere.\", \"claude\": \"A golden retriever sits attentively in park setting, its mouth open in what appears to be content pant. The warm lighting and scattered autumn leaves create peaceful scene, with the dog positioned centrally against backdrop of grass and mature trees. The composition suggests casual outdoor moment captured during walk or play session.\", \"qwen\": \"Golden retriever dog sitting on grass in park environment. Autumn season indicated by fallen leaves on ground. Natural daylight illumination. Dog exhibits typical friendly expression with open mouth. Background contains trees and vegetation. Image quality appears high resolution with good color saturation.\", \"llama\": \"This image depicts golden retriever in an outdoor setting. The dog is positioned on grass with trees visible in the background. The scene appears to be during daytime with natural lighting. The dogs posture suggests it is alert and engaged. The presence of fallen leaves indicates the photograph was likely taken during autumn months.\", \"gemini\": \"A beautiful golden retriever enjoys moment in the park! The friendly pup is sitting on the grass, surrounded by the warm colors of fall. With its tongue happily hanging out, this good boy seems to be having wonderful time outdoors. The natural lighting and peaceful setting make for 17 heartwarming scene that captures the joy of spending time with our four-legged friends.\" } } }"
        },
        {
            "title": "E Hyperparameters",
            "content": "In data generation, we use temperature of 0.6, top_k=50, top_p=0.9. In data sampling for majority voting (Section 3), we set = 16 to generate 16 candidate responses per closed-ended question. We experimented with different majority thresholds and found that requiring at least 5 identical responses (out of 16) provides the best balance between data quality and quantity. Lower thresholds (e.g., 3) produce more training data but include less reliable majorities, while higher thresholds (e.g., 8) overly restrict the dataset. We selected the threshold of 5 after empirically comparing thresholds of 3 and 5: both achieved similar final performance, but the threshold of 5 required less training data, suggesting better sample efficiency and filtering quality. This threshold effectively filters out cases with greater response diversity where the models uncertainty is higher, retaining only examples where the model shows reasonable consensus. In training, we use the default training setup in llama-cookbook which has learning rate of 1e-5, we train for 5 epochs. The data generation and model training were conducted on H100 GPUs, with approximately 400 GPUs hours. This represents cost-efficient alternative to human annotation ( $500K-1M for equivalent data) or GPT-4 distillation ( $5K-10K in API costs)."
        },
        {
            "title": "F Correctness Filter Negative Example",
            "content": "Component Content Response The image contains series of numbers, specifically 20000700000010. Response Evaluation The image contains the text 2000070000010 in purple, pixelated font. The text is centered at the top of the image and spans the entire width. It appears to be numerical code or identifier, but without more context, its difficult to determine its meaning or significance. Both responses attempt to decipher the content of the image, but they differ in their approach and level of detail. Assistant As response is concise and straightforward, simply stating the numbers visible in the image. However, it lacks context and fails to provide any additional information about the meaning or significance of these numbers. Assistant Bs response provides more detailed observations, including information about the font style, color, and layout of the text. It also acknowledges the limitations of interpretation without additional context. FINAL ANSWER: [[B]] Table 4 Example of judge getting the correct answer but providing incorrect reasoning. Even though the final answer is correct, the reasoning failed to point out that the number in Response has an extra 0 and it is wrong answer. While our filtering mechanism successfully removes many incorrect judgments, we acknowledge it is not perfect. This section illustrates case where correctnessbased filtering can retain problematic examples: the judge selects the correct answer (Response B) but fails to identify the specific transcription error in Response (an extra digit \"20000700000010\" vs. \"2000070000010\"). Instead, the reasoning focuses on superficial featuresResponse Bs additional descriptive details and contextual observationsrather than the factual discrepancy that should determine the judgment. This example suggests why filtering based on ground-truth correctness alone may be insufficienteven when the final judgment is correct, the reasoning may prioritize stylistic features over factual accuracy. This observation partially motivates our majority voting approach (Section 6.2), which filters training data based on the models own consistency rather than relying solely on external correctness labels. We hypothesize that by requiring the judge to correctly identify preferences across multiple synthetically generated pairs with diverse error types, majority voting may implicitly filter for more robust reasoning patterns rather than spurious correlations with superficial features. judgment that succeeds consistently across varied synthetic contrasts could provide stronger evidence of genuine understanding than single correct answer that might result from incidental heuristics. We include this example not as definitive evidence of fundamental flaw, but as an illustrative case that helps explain our design rationale. While we cannot conclusively verify that majority voting eliminates all imperfect reasoning, the empirical advantages shown in Section 6.2 suggest that consistency-based filtering may provide more robust supervision than correctness checking alone for learning generalizable judgment criteria."
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "University of Washington"
    ]
}