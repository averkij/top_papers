{
    "paper_title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models",
    "authors": [
        "Hongbo Liu",
        "Jingwen He",
        "Yi Jin",
        "Dian Zheng",
        "Yuhao Dong",
        "Fan Zhang",
        "Ziqi Huang",
        "Yinan He",
        "Yangguang Li",
        "Weichao Chen",
        "Yu Qiao",
        "Wanli Ouyang",
        "Shengjie Zhao",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new state-of-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 6 5 3 1 2 . 6 0 5 2 : r ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models Hongbo Liu1,3,* Jingwen He2,3,* Yi Jin1 Dian Zheng3 Yuhao Dong4 Fan Zhang3 Ziqi Huang4 Yinan He3 Yangguang Li2 Weichao Chen1 Yu Qiao3 Wanli Ouyang2 Shengjie Zhao1 Ziwei Liu4 1Tongji University, 2The Chinese University of Hong Kong, 3Shanghai Artificial Intelligence Laboratory, 4S-Lab, Nanyang Technological University, *Equal contribution. Corresponding authors. Project page: https://vchitect.github.io/ShotBench-project/"
        },
        {
            "title": "Abstract",
            "content": "Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new stateof-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation."
        },
        {
            "title": "Introduction",
            "content": "Cinematography, the art of crafting visual narratives through meticulously designed shots [4, 15], forms the bedrock of high-quality filmmaking. Each shot, from framing and lens choice to lighting and camera movement, is deliberately composed to convey narrative meaning, emotional tone, and aesthetic impact. For text-to-image/video generation [2, 9, 21, 22, 38, 57] to achieve similar cinematic quality, it requires mechanism capable of understanding these cinematographic principles. Vision-Language Models (VLMs) [3, 24, 31, 33, 46, 59, 63] are the primary candidates for developing such understanding. Thus, the core challenge is whether current VLMs can genuinely grasp the nuanced language of cinematography and its artistic intent, moving beyond literal scene interpretation. This deep cinematographic comprehension remains significantly underexplored. Existing VLM benchmarks, while diverse [6, 18, 29, 61], typically lack the necessary focus for robust cinematographic evaluation, gap exacerbated by scarcity of specialized models, datasets with rich cinematic annotations, and consequently, rigorous benchmarks for this specific type of understanding. Preprint. Under review. Figure 1: Overview of ShotBench. The benchmark covers eight core dimensions of cinematography: shot size, framing, camera angle, lens size, lighting type, lighting condition, composition, and camera movement. To bridge this critical gap, we introduce ShotBench, comprehensive benchmark specifically designed to assess VLMs understanding of cinematic language. ShotBench comprises over 3.5k expert-annotated multiple-choice QA examples, meticulously curated from both images and video clips across over 200 films, predominantly those that have received Oscar nominations for Best Cinematography1. It rigorously spans eight fundamental cinematography dimensions: shot size, shot framing, camera angle, lens size, lighting type, lighting condition, composition, and camera movement. Our rigorous annotation pipeline, combining trained annotators with expert oversight, ensures high-quality evaluation set grounded in professional cinematic knowledge. We conduct an extensive evaluation of 24 leading open-source and proprietary VLMs on ShotBench. Our results reveal that even the strongest VLM (GPT-4o [36]) in our evaluation averages below 60% accuracy, clearly indicating considerable gap between current VLM capabilities and genuine cinematographic comprehension. In-depth analysis further highlights specific weaknesses: advanced models such as GPT-4o [36] and Qwen2.5-VL [3], despite grasping core cinematic concepts, often struggle to map subtle visual details to precise professional terminology (e.g., distinguishing medium shot from medium close-up). They also demonstrate constrained spatial reasoning, especially regarding camera position and angle. Strikingly, the camera movement dimension proved exceptionally challenging, with over half of the models failing to surpass 40% accuracy. To further advance cinematography understanding in VLMs, we construct ShotQA, the first largescale multimodal dataset for cinematic language understanding, consisting of approximately 70k high-quality QA pairs derived from movie images and video clips. Leveraging ShotQA, we develop ShotVL, an optimized VLM based on Qwen2.5-VL-3B [3], trained with supervised fine-tuning and Group Relative Policy Optimization (GRPO) [44] to enhance its alignment of visual features with cinematography knowledge and strengthen its reasoning capabilities. Experimental results demonstrate that ShotVL achieves consistent and substantial improvements across all ShotBench dimensions (with gain of 19.0%) compared to the original Qwen2.5-VL-3B [3], establishing new state-of-the-art performance and decisively surpassing both the best-performing open-source (Qwen2.5-VL-72B-Instruct [3]) and proprietary (GPT-4o [36]) models. Our contributions are summarized as follows: We introduce ShotBench, comprehensive benchmark for evaluating VLMs understanding of cinematic language. It comprises over 3.5k expert-annotated QA pairs derived from images and video clips of over 200 critically acclaimed films (predominantly Oscar-nominated), covering 1https://en.wikipedia.org/wiki/Academy_Award_for_Best_Cinematography 2 eight distinct cinematography dimensions. This provides rigorous new standard for assessing fine-grained visual comprehension in film. We conducted an extensive evaluation of 24 leading VLMs, including prominent open-source and proprietary models, on ShotBench. Our results reveal critical performance gap: even the most capable model, GPT-4o, achieves less than 60% average accuracy. This systematically quantifies the current limitations of VLMs in genuine cinematographic comprehension. To address the identified limitations and facilitate future research, we constructed ShotQA, the first large-scale multimodal dataset for cinematography understanding, containing approximately 70k high-quality QA pairs. Leveraging ShotQA, we developed ShotVL, novel VLM trained using Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). ShotVL significantly surpasses all tested open-source and proprietary models, establishing new state-ofthe-art on ShotBench. We will open-source our codes, models, and dataset, hoping our work will contribute significantly to future progress in image/video understanding and generation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Benchmarking Vision-Language Models Vision-Language Models (VLMs) [3, 24, 31, 33, 46, 59, 63] are large-scale models designed to integrate visual perception with natural language understanding. In recent years, VLMs have demonstrated strong capabilities across perception, reasoning, and wide range of multi-disciplinary applications [1, 13, 25, 28, 29, 32, 47, 54, 55]. Recently, researchers have proposed variety of benchmarks to assess VLMs capability. For example, MMBench [29] evaluates VLMs across 20 distinct ability dimensions, and MMVU [61] focuses on video understanding across four core academic disciplines. Other benchmarks target specific cognitive or reasoning capacities: LogicVista [52] assesses visual logical reasoning in multi-choice format, and SPACE [39] systematically compare spatial reasoning abilities between VLMs and animals. Additional efforts, EgoSchema [35], and VSI-Bench [53], evaluate egocentric video understanding. Moreover, some works introduce tasks with specific domains, such as scientific and mathematical figure interpretation [42, 51], knowledge acquisition [19], and visual coding [58]. However, despite this breadth, existing benchmarks do not specifically evaluate VLMs grasp of nuanced cinematic language. This crucial aspect of visual storytelling, encompassing artistic choices in shot design, remains underexplored. ShotBench is introduced to address this critical gap, fostering development in areas like AI-assisted video generation. 2.2 Cinematography Understanding Early work on automatic film analysis includes many sub-tasks such as shot type classification [40], scene segmentation [41, 45, 56], and cut type recognition [37]. To clearly situate ShotBench and highlight its unique contribution in terms of dimensional coverage, we present comparison with existing cinematography-related benchmarks in Table 1. Dimensions Shot size Shot framing Camera angle Lens size Lighting type Lighting condition Composition Camera movement Table 1: Cinematography Understanding Benchmark Comparison MovieShots [40] MovieNet [20] CineScale2 [43] CameraBench [27] CineTechBench [50] ShotBench As Table 1 illustrates, earlier works like MovieShots [40] and MovieNet [20] provided foundational taxonomies but were limited in scope, primarily addressing only shot size and camera movement. While valuable, these benchmarks do not capture the full complexity of cinematographic language. More recently, CameraBench [27] focused on more granular understanding of camera motion 3 primitives, covering camera angle and camera movement, but still omitted other critical visual elements. CineTechBench [50] proposed broader set of dimensions to evaluate the understanding and generation capabilities of MLLMs and video generation models. However, as evident from Table 1, even CineTechBench does not encompass all eight core dimensions evaluated by our work. In stark contrast, ShotBench is meticulously designed to be comprehensive, covering all eight fundamental cinematography dimensions listed in Table 1. This breadth is crucial for holistic assessment of VLMs ability to grasp nuanced cinematographic techniques."
        },
        {
            "title": "Understanding",
            "content": "Here, we introduce ShotBench, dedicated benchmark designed to evaluate VLMs understanding of cinematography language in comprehensive and structured manner. ShotBench covers eight core dimensions2 commonly used in cinematic analysis: shot size, shot framing, camera angle, lens size, lighting type, lighting conditions, composition, and camera movement. These dimensions reflect key principles of visual storytelling in film production and serve as the foundation for evaluating model comprehension. Each sample in ShotBench is paired with multiple-choice question targeting specific cinematography aspect, requiring the model to not only perceive the scene holistically, but also extract fine-grained visual cues to reason about the underlying cinematic techniques. An overview of the benchmark framework is illustrated in Figure 1. 3.1 Data Construction Process Figure 2: An overview of the ShotBench construction pipeline. To construct ShotBench, we design systematic data collection and processing pipeline, as illustrated in Figure 2. The process consists of four key stages: Data Curation & Pre-processing, Annotator Training, QA Annotation, and Verification. Data Curation & Pre-processing. We collect the dataset primarily from films that won or were nominated for the Academy Award for Best Cinematography, ensuring high-quality and professionally crafted shot. Data are sourced from public websites and include high-resolution images and video clips. To ensure quality and safety, we apply the LAION aesthetic predictor [23] for filtering lowquality samples, NSFW detection [48] to remove inappropriate content, and FFmpeg [16] to crop black bars. For video processing, we use TransNetV2 [45] to segment footage into individual shots. The full list of collected movies is provided in Appendix C. Annotator Training. To ensure high-quality annotations, we first curated comprehensive reference materials from publicly available cinematography tutorials covering all eight dimensions in ShotBench. Annotators were required to study these materials before labeling. We then conducted multi-round pilot annotations, supported by expert audits and daily discussions to resolve ambiguities. All issues and resolutions were documented to guide the final annotation phase. QA Annotation. Based on ShotBenchs predefined dimensions, we automatically generated question prompts using templated formats (e.g., \"What is the shot size of this movie shot?\"). For image data, we extracted candidate labels from Shotdeck 3, professional cinematography reference platform, where metadata had been curated by experienced photographers. Annotators verified these labels against ShotBench guidelines and corrected any discrepancies. All label modifications were reviewed by experts. For videos, annotators identified all valid camera movement intervals by marking start and end timestamps. 2See general cinematography principles outlined at: https://en.wikipedia.org/wiki/Cinematography 3https://shotdeck.com 4 Table 2: Evaluation results for 24 VLMs. Abbreviations adopted: SS for Shot Size; SF for Shot Framing; CA for Camera Angle; LS for Lens Size; LT for Lighting Type; LC for Lighting Conditions; SC for Shot Composition; CM for Camera Movement. Bold indicates the best result, and underline indicates the second best in current VLMs. Importantly, our model ShotVL surpasses all the evaluated ones in all evaluation dimensions. Models SS SF CA LS LT LC SC CM Avg Open-Sourced VLMs Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] LLaVA-NeXT-Video-7B [60] LLaVA-Video-7B-Qwen2 [60] LLaVA-Onevision-Qwen2-7B-Ov-Chat [24] InternVL2.5-8B [5] InternVL3-2B [63] InternVL3-8B [63] InternVL3-14B [63] Internlm-xcomposer2d5-7B [12] Ovis2-8B [34] VILA1.5-3B [26] VILA1.5-8B [26] VILA1.5-13B [26] Instructblip-vicuna-7B [7] Instructblip-vicuna-13B [7] InternVL2.5-38B [5] InternVL3-38B [63] Qwen2.5-VL-32B-Instruct [3] Qwen2.5-VL-72B-Instruct [3] InternVL3-78B [63] 54.6 69.1 35.9 56.9 58.4 56.3 56.3 62.1 59.6 51.1 35.9 33.4 40.6 36.7 27.0 26.8 67.8 68.0 62.3 75.1 69.7 56.6 73.5 37.1 65.4 71.0 70.3 56.0 65.8 82.2 71.0 37.1 44.9 44.5 54.6 27.9 29.2 85.4 84.0 76.6 82.9 80. 43.1 53.2 32.5 45.1 52.3 50.8 44.4 46.8 55.4 39.8 32.5 32.1 39.1 40.7 34.5 27.9 55.4 51.9 51.0 56.7 54.5 36.6 47.0 27.8 36.0 38.7 41.1 34.6 42.9 40.7 32.7 27.8 28.6 29.7 34.8 29.4 28.0 41.7 43.6 48.3 46.8 44.0 59.3 60.5 50.9 63.5 59.5 60.2 56.8 58.0 61.7 59.3 50.9 50.6 48.9 52.8 44.4 39.0 61.7 64.4 61.7 59.0 65.5 45.1 47.4 31.7 45.4 44.9 45.1 44.6 44.3 44.6 35.7 31.7 35.7 32.9 35.4 29.7 24.0 48.9 46.9 44.0 49.4 47.4 41.5 49.9 28.0 37.4 50.9 50.1 43.0 46.8 51.1 35.7 28.0 28.4 34.4 34.2 27.1 27.1 52.4 54.7 52.2 54.1 51.8 31.9 30.2 31.3 35.3 39.7 33.6 38.1 44.2 38.2 38.8 35.3 21.5 36.9 31.3 25.0 22.0 44.0 44.6 43.8 48.9 44. 46.1 53.8 34.4 48.1 51.9 50.9 46.7 51.4 54.2 45.5 34.9 34.4 38.4 40.1 30.6 28.0 57.2 57.3 55.0 59.1 57.2 Gemini-2.0-flash [10] Gemini-2.5-flash-preview-04-17 [11] GPT-4o [36] Proprietary VLMs 48.9 57.7 69.3 75.5 82.9 83.1 Ours 44.6 51.4 58.2 31.9 43.8 48.9 62.2 65.2 63.2 48.9 45.7 48.0 52.4 45.9 55.2 47.4 43.5 48. 51.5 54.5 59.3 ShotVL (3B) 77.9 85.6 68.8 59. 65.7 53.1 57.4 51.7 65.1 Verification. All questionanswer pairs were reviewed through multiple expert audits, with batches revised iteratively until reaching satisfactory quality. Through this rigorous pipeline, we further sampled from the validated data to construct the final benchmark, consisting of 3,049 images and 464 video clips, resulting in 3,572 high-quality question-answer pairs across all eight ShotBench dimensions. 3.2 Evaluation Setup. To provide comprehensive assessment of the challenges posed by ShotBench and establish reference baselines for future research, we evaluate diverse set of state-of-the-art multimodal foundation models that support video or multi-image inputs. Specifically, we evaluate total of 24 foundation models, including both open-source and proprietary models: Qwen2.5-VL [3], LLaVAVideo [60], LLaVA-OneVision [24], InternVL-2.5 & 3 [5, 63], InternLM-XComposer-2.0 [12], Ovis2 [34], VILA1.5 [26], InstructBLIP [7], and Gemini-2.0 & 2.5 [10, 11]. To ensure fairness and reproducibility, we adopt the VLMEvalKit [14] framework for standardized evaluation. We report accuracy as the primary metric to quantify model performance on ShotBench. Additional implementation details and evaluation prompts are provided in the Appendix B. Results and Findings. The evaluation results, reported in Table 2, yield several key findings: (1) Approximately half of the evaluated models attain an overall accuracy below 50%. Even the leading model, GPT-4o, fails to reach 60% accuracy, underscoring the significant gap between current VLMs and true understanding of cinematography. (2) The overall performance differences between opensource and proprietary models are marginal. Notably, Qwen2.5-VL-72B-Instruct (59.1%) achieves almost the same performance as GPT-4o (59.3%) (3) The camera movement dimension represents particular area of weakness across current models, with achieved accuracy often approximating random selection (around 25%). (4) Within each series, larger models generally achieve higher accuracy (as shown in Figure 3), suggesting potential scaling effect with respect to model size in cinematography language understanding. 5 Figure 3: Overall performance comparison of InternVL3, Qwen2.5-VL, and VILA-1.5 model families, highlighting variations by model size. The results consistently show that larger models within each series generally yield superior performance outcomes. Figure 4: Performance evaluation of six Vision Language Models (VLMs) on cinematographic understanding, visualized across several dimensions. Stronger models perform well uniformly, without specific dimensional weaknesses. To better understand the limitations of current VLMs in cinematic language understanding, we conduct extensive quantitative and qualitative analyses on the prediction results of representative models. Our analysis reveals significant challenges for current models across three core aspects: (1) fine-grained visual-terminology alignment, (2) spatial perception of camera position and orientation, and (3) visual reasoning in cinematography. Fine-Grained VisualTerminology Alignment. Through extensive case studies, we find that current VLMs frequently fail to precisely align visual cues with specific cinematic terms, particularly when the task requires expert-level distinctions. Such shortcomings are especially evident in dimensions like shot size and lens size, where categories are defined by fine-grained framing or focal length conventions. For example, Medium Wide Shot (MWS) typically frames the subject from the knees up, while Medium Shot (MS) frames from the waist up. Regarding lens size, Ultra Wide offer broader field of view and often introduce edge distortion, whereas Long Lens compress spatial depth, making the foreground and background elements appear closer. We draw the confusion matrices based on results of GPT-4o, shown in Figure 5. It reveals that most misclassifications occur between visually adjacent categories. For instance, MS is frequently confused with MCU (36.2%) or MWS (10.1%), and Medium lens is often misclassified as Wide or Long lens. These findings suggest that current VLMs lack fine-grained alignment needed to reliably distinguish between visually similar but semantically distinct categories. plausible explanation is that the training data used for these models may lack sufficient annotation granularity or consistency in cinematography labeling, limiting their ability to internalize professional-level distinctions. Figure 5: Confusion matrices of GPT-4o predictions on shot size (left) and lens size (right). 6 Figure 6: Examples of failure cases where VLMs struggle with fine-grained visualterminology alignment, spatial perception, and visual reasoning. Spatial Perception of Camera Position and Orientation. It evaluates both static and dynamic camera attributes. For static scenarios, models are tested on fixed-angle concepts such as low angle and high angle. In dynamic cases, the camera movement dimension probes models ability to recognize changes in position (e.g., pull out), angle (e.g., tilt up), and focal length (e.g., zoom in). Performance on static camera angles reveals significant challenges: even the best-performing model, GPT-4o, achieves only 58.2% accuracy, highlighting its difficulty in perceiving and reasoning about the cameras spatial orientation. This deficiency is even more pronounced for dynamic camera movements, where over half of the evaluated models fall below 40% accuracymarkedly lower than their performance across other ShotBench dimensions. For instance, most frontier models struggle to distinguish between camera position changes (push in/out) and focal length adjustments (zoom in/out), task reliant on perceiving parallax (Figure 6, first example). Similarly, many models confuse camera rotation (e.g., tilt up/down) with vertical camera displacement (e.g., boom up/down). This distinction is crucial for determining whether the camera is pivoting on its axis or undergoing physical movement (Figure 6, second example). Collectively, these findings underscore significant limitations in the models capacity to accurately perceive dynamic visual information, interpret crucial spatial cues like parallax, and consequently, infer the true nature of camera operations. Visual Reasoning in Cinematography. We observe that understanding some dimensions might need VLM to reason like cinematography expert. For example, recognizing short side composition (Figure 6, first row, fourth column) requires the model to infer the subjects gaze direction relative to their frame positiona subtle yet important cue. We hypothesize that reasoning processes can help VLMs attend to critical visual details relevant to cinematic semanticssuch as spatial reasoning for determining camera angle or lens size, identifying camera movement from the motion of elements within the frame, and even discerning the directors intent in guiding the viewers attention through compositional choices. We provide quantitative and qualitative analyses in Section 5.3 and Appendix A.2. Our findings suggest that encouraging VLMs to engage in structured reasoning provides noticeable improvements in their ability to understand cinematic language."
        },
        {
            "title": "Targeted Training",
            "content": "To address the nuanced challenge of enabling Visual Language Models (VLMs) to perceive and reason about cinematic elements, we introduce ShotQA, novel large-scale dataset, and ShotVL, VLM specifically designed for cinematography understanding. ShotVL employs strategic two-stage training pipeline: initial large-scale Supervised Fine-tuning (SFT) for broad knowledge acquisition, followed by Group Relative Policy Optimization (GRPO) [44] for fine-grained reasoning refinement on curated subset. 7 ShotQA: Dedicated Dataset for Cinematography Comprehension. ShotQA stands as the first large-scale dataset meticulously designed to benchmark and enhance VLMs grasp of cinematographic techniques. It comprises 58,140 images and 1,200 video clips, collectively yielding approximately 70,000 multi-choice question-answer (QA) pairs. These resources are sourced from 243 diverse films to ensure broad coverage of cinematic styles. All samples are formatted as multi-choice QA pairs, facilitating structured evaluation and targeted training. Each entry is enriched with metadata, including film title and source clip timestamp, allowing for contextual understanding. Table 9 details the sample distribution, revealing noteworthy balance across most cinematic dimensions, which typically range from approximately 6,800 to 9,600 samples each. The scale and specificity of ShotQA provide critical resource for advancing research in this domain. Stage 1: Large-scale Supervised Fine-tuning for Foundational Alignment. In the foundational first stage, ShotVL undergoes SFT using approximately 70k QA pairs sampled from the ShotQA dataset. We utilize Qwen-2.5-VL-3B-Instruct [3] as the base model. The model processes an image or video alongside question and multiple-choice options, and is trained to directly predict the correct answer via cross-entropy loss. This SFT phase is crucial for establishing strong alignment between visual features and specific cinematic terminology, equipping the model with broad understanding of cinematographic concepts. Stage 2: Reinforcement Learning with GRPO for Enhanced Reasoning. Building upon the SFTinitialized model, the second stage employs GRPO to further elevate ShotVLs reasoning capabilities and prediction accuracy. Given multimodal input (an image/video and textual query), GRPO generates distinct responses {o1, . . . , oG} from the current policy πθold . These are evaluated using rule-based binary reward function, inspired by prior work [17, 30, 49]: r(o, x) = (cid:26)1, if is correct (matches the ground truth), 0, otherwise. (1) Following DeepSeek-R1 [17], our reward incorporates two components: (1) format reward to ensure outputs adhere to structured pattern (<think>...</think> and <answer>...</answer> tags), and (2) an accuracy reward comparing the extracted answer from the <answer> block with the ground truth. The advantage Ai for the i-th response is calculated by normalizing its reward within the group: ri mean({r1, . . . , rG}) std({r1, . . . , rG}) + δ (where δ is small constant for numerical stability, e.g., 1e 8). Ai = Finally, GRPO optimizes the policy πθ by maximizing the objective: (2) LGRPO = 1 (cid:88) i=1 min (cid:18) πθ(oix) πθold (oix) Ai, clip (cid:18) πθ(oix) πθold(oix) (cid:19) (cid:19) , 1 ϵ, 1 + ϵ Ai (3) Here, ϵ is hyperparameter controlling the policy update step size, and the clipping mechanism stabilizes training. For this RL phase, we utilize focused subset of approximately 8k high-quality multiple-choice QA instances from ShotQA to refine the models ability to select the correct option with higher confidence and precision."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Details Our implementation is based on ms-swift [62]. We initialize Qwen2.5-VL-3B-Instruct [3] as our base model. We use around 60k samples for SFT and approximately 8k samples for GRPO. We use Flash Attention-2 [8] as the models attention implementation and bfloat16 precision for both training and inference to reduce memory consumption. In supervised finetuning stage, the global batch size is set to 4, and the model is trained for 1 epoch with learning rate of 1 105. In reinforcement learning stage, we set the group size to 12 and the global batch size to 24. The clipping parameter ϵ is set to 0.2. The model is trained for 10 epochs with learning rate of 1 106. Detailed hyper-parameters are provided in the Appendix B. 8 5.2 Main Results For comparison, we include results from the strongest open-source model (Qwen2.5-VL-72B-Instruct) and the leading proprietary model (GPT-4o) from Table 2, alongside the baseline Qwen2.5-VL-3BInstruct, as reported in Table 3. Compared to the baseline Qwen2.5-VL-3B-Instruct, our model achieves substantial improvements across all dimensions, with an average gain of 19 points, demonstrating the effectiveness of our dataset and training methodology. Furthermore, despite having only 3B parameters, our model surpasses both GPT-4o and the strongest open-source model, Qwen2.5-VL72B-Instruct, setting new state of the art in cinematography language understanding while offering significantly lower deployment and usage costs. We present further experiments and analysis in the following section, with visualizations of representative model outputs included in the Appendix A.2. , Table 3: Quantitative comparison of GPT-4o [36], Qwen2.5-VL-72B-Instruct [3], Qwen2.5-VL-3BInstruct [3], and ShotVL on ShotBench. Bold indicates the best result, and underline indicates the second best in each group. Models Qwen2.5-VL-72B-Instruct [3] GPT-4o [36] Qwen2.5-VL-3B-Instruct [3] ShotVL (3B) SS 75.1 69.3 54. 77.9 SF 82.9 83.1 56.6 85.6 CA 56.7 58.2 43. 68.8 LS 46.8 48.9 36.6 59.3 LT 59.0 63.2 59. 65.7 LC 49.4 48.0 45.1 53.1 SC CM Avg 54.1 55.2 41.5 57.4 48.9 48.3 31.9 51.7 59.1 59.3 46.1 65. 5.3 Ablation Study In this section, we investigate the effectiveness of ShotVLs two-stage training strategy. In particular, we compare five training strategies: SFT, CoT-SFT, GRPO, SFTGRPO, and CoT-SFTGRPO. For fast exploration, we sample approximately 4k images for the SFT stage and around 1k for GRPO. Besides, we reduce the batch size for SFT to 2, and set the group size and batch size for GRPO to 6. The number of training epochs for GRPO is also reduced to 5, while all other settings remain unchanged. To generate reasoning process for CoT-SFT, we first construct JSON-formatted knowledge base containing definitions and identification methods for all cinematic terms covered in ShotBench. For each training sample, we retrieve the relevant entries corresponding to the question and candidate choices from the knowledge base, and prompt Gemini-2.0-flash to produce reasoning process grounded in cinematic knowledge. More details of the knowledge base are provided in the Appendix B. , Table 4: Performance comparison of different training strategies. Bold indicates the best result, and underline indicates the second best in each group. Method SFT CoT GRPO Choices SS 54.6 68.2 52.6 69.3 66.8 69.1 SF 56.6 78.6 64.3 75.5 78.2 79.3 CA 43.1 53.6 47.3 52.1 52.1 56.7 LS 36.6 47.2 36.4 46.0 46.4 51.1 LT 59.3 63.2 54.8 63.0 60.0 60. LC 45.1 44.9 38.0 47.4 44.9 45.4 SC 41.5 53.0 42.2 48.2 51.4 53.2 CM Avg 45.3 25.8 54.3 25.8 45.4 27.4 53.5 26.2 30.4 53.8 55.5 28.6 We report the performance of each training method in Table 4. It is observed that all training strategies yield notable improvements over the baseline, demonstrating the high quality and effectiveness of our constructed dataset. Comparing SFT with CoT-SFT, we find that the latter yields very small gains. This may be due to the low quality of reasoning chains generated by Gemini-2.0-flash, which fail to provide effective supervision and may introduce noise. This further highlights the advantage of GRPO, which focuses solely on outcome reward supervision. Another observation is that reasoning-augmented training consistently improves performance in the camera movement dimension (ranging from +0.4% to +4.6%), despite the ablation experiments being conducted solely on static images and containing no camera movement related questions. This may indicate that reasoning chain generation may implicitly enhance VLMs capability to recognize dynamic motion. From Figure 7, GRPO consistently improves performance across most dimensions under all training settings. Among all configurations, the SFTGRPO setup achieves the best overall Figure 7: Comparison of different training strategies across different cinematography dimensions. performance, confirming its effectiveness for enhancing cinematography understanding. More case studies are provided in Appendix A.2."
        },
        {
            "title": "6 Conclusion",
            "content": "This work addresses the gap in Vision-Language Models (VLMs) comprehension of nuanced cinematic language by introducing ShotBench, comprehensive benchmark. Our evaluation revealed substantial limitations in existing VLMs. To advance the field, we constructed the large-scale ShotQA dataset and developed ShotVL, model that significantly outperforms all prior open-source and proprietary models on ShotBench, establishing new state-of-the-art. We open-source our models, data, and code to foster future progress in this crucial domain."
        },
        {
            "title": "References",
            "content": "[1] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Blain Brown. Cinematography: theory and practice: image making for cinematographers and directors. Routledge, 2016. [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [6] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Instructblip: Towards general-purpose Wang, Boyang Li, Pascale Fung, and Steven Hoi. vision-language models with instruction tuning, 2023. [8] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [9] Google Deepmind. Veo2. Accessed: 2025-05-02. 10 [10] Google DeepMind. agentic google-gemini-ai-update-december-2024/, 2024. era. Introducing gemini 2.0: the https://blog.google/technology/google-deepmind/ our new ai model for [11] Google DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/, 2025. [12] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. [13] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 90629072, 2025. [14] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [15] Columbia faculty. Shot, scene, and sequence. Accessed: 2025-05-02. [16] FFmpeg Developers. complete, cross-platform solution to record, convert and stream audio and video. https://ffmpeg.org/, 2006. Accessed: 2025-05-02. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [19] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos, 2025. [20] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 709727. Springer, 2020. [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [22] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [23] LAION-AI. aesthetic-predictor. https://github.com/LAION-AI/aesthetic-predictor, 2022. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [25] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 11 [26] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. [27] Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, et al. Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376, 2025. [28] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. arXiv preprint arXiv:2408.00754, 2024. [29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [30] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [31] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. [32] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. [33] Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. arXiv preprint arXiv:2502.04328, 2025. [34] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. [35] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding, 2023. [36] OpenAI. Hello gpt-4o, 2024. [37] Alejandro Pardo, Fabian Caba Heilbron, Juan León Alcázar, Ali Thabet, and Bernard Ghanem. Moviecuts: new dataset and benchmark for cut type recognition. In European Conference on Computer Vision, pages 668685. Springer, 2022. [38] Quynh Phung, Long Mai, Fabian David Caba Heilbron, Feng Liu, Jia-Bin Huang, and Cusuh Ham. Cineverse: Consistent keyframe synthesis for cinematic scene composition. arXiv preprint arXiv:2504.19894, 2025. [39] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? arXiv preprint arXiv:2410.06468, 2024. [40] Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and Dahua In Lin. unified framework for shot type classification based on subject centric lens. Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pages 1734. Springer, 2020. [41] Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, and Dahua Lin. local-to-global approach to multi-modal movie scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1014610155, 2020. [42] Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. Scifibench: Benchmarking large multimodal models for scientific figure interpretation. arXiv preprint arXiv:2405.08807, 2024. [43] Mattia Savardi, András Bálint Kovács, Alberto Signoroni, and Sergio Benini. Cinescale2: dataset of cinematic camera features in movies. Data in Brief, 51:109627, 2023. 12 [44] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [45] Tomáš Souˇcek and Jakub Lokoˇc. Transnet v2: An effective deep network architecture for fast shot transition detection. arXiv preprint arXiv:2008.04838, 2020. [46] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [47] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [48] Tost-AI. nsfw-detector. nsfw-image-detection-large, 2024. https://huggingface.co/TostAI/ [49] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [50] Xinran Wang, Songyu Xu, Xiangxuan Shan, Yuxuan Zhang, Muxi Diao, Xueyan Duan, Yanhua Huang, Kongming Liang, and Zhanyu Ma. Cinetechbench: benchmark for cinematographic technique understanding and generation, 2025. [51] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. [52] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. [53] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. [54] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. In European Conference on Computer Vision, pages 2038. Springer, 2024. [55] Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, et al. Egolife: Towards egocentric life assistant. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2888528900, 2025. [56] Yang Yang, Yurui Huang, Weili Guo, Baohua Xu, and Dingyin Xia. Towards global video scene segmentation with context-aware transformer. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 32063213, 2023. [57] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [58] Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, and Jacky Keung. Humaneval-v: Evaluating visual understanding and reasoning abilities of large multimodal models through coding tasks. arXiv preprint arXiv:2410.12381, 2024. [59] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 13 [60] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. [61] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, et al. Mmvu: Measuring expert-level multi-discipline video understanding. arXiv preprint arXiv:2501.12380, 2025. [62] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. [63] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Discussions",
            "content": "A.1 Limitations (1) Both ShotBench and ShotQA are constructed from real-world movie data. However, cinematic shots are not always with standard and clearly defined terminology. Besides, the data distribution is imbalanced across some dimensions (e.g., camera movements like dolly zoom is rare in real data). Moreover, high-quality video annotation is labor-intensive. To improve scalability, future work may explore synthetic data for more robust performance. (2) We primarily validate the effectiveness of our dataset and training approach using Qwen2.5-VL3B-Instruct, which has limited capability due to relatively fewer parameters. Further studies may focus on larger base model to further improve performance. A.2 ShotVL: Reasoning Like Cinematographer ShotVL Beats GPT-4o. Our experimental results show that our model outperforms previous SOTA GPT-4o, we visualize some cases and compare the outputs between ShotVL and GPT-4o in Figure 8. Reasoning process improves performance. We visualize the outputs between ShotVL and the pure SFT variant in Figure 9. A.3 Current VLMs Cinematography Understanding Needs Further Enhancement We further visualize failure cases from the strongest open-source model, Qwen2.5-VL-72B-Instruct, as well as the strongest proprietary model, GPT-4o, as shown in Figure 10. A.4 More Visualizations More output cases with thinking process from ShotVL are provided in Figure 11."
        },
        {
            "title": "B Implementation Details for Evaluation and Experiments",
            "content": "Details of Evaluation. During evaluation, we first attempt to extract each models final answer using template-based matching. If no valid match is found, we follow the previous works [29, 61] to use GPT-4o as an automatic answer extractor. For open-source models, we densely sample video frames at 12 FPS with maximum resolution of 360640 pixels. For image-based samples, we follow the default input configurations of each model. We apply greedy decoding during inference for reproducibility. For proprietary models, we evaluate them via their official APIs, setting the temperature to 0 to produce deterministic outputs. We use GPT-4o (2024-08-06) to extract final answers, based on the prompt in Figure 12. Details of Experiments. Our training implementation is based on ms-swift framework [62]. All hyper-parameters we use for the main experiments are reported in Table 5 and Table 6. The training 14 Figure 8: Comparison between GPT-4o and ShotVL. Table 5: Hyper-parameters for SFT. Table 6: Hyper-parameters for GRPO. Parameter Value Parameter Value model attn_impl train_type torch_dtype num_train_epochs per_device_train_batch_size per_device_eval_batch_size learning_rate gradient_accumulation_steps eval_steps save_steps save_total_limit logging_steps max_length system warmup_ratio dataloader_num_workers Qwen2.5-VL-3B-Instruct flash_attn full bfloat16 1 1 1 1e-5 16 100 100 3 5 3072 You are helpful assistant. 0.05 model rlhf_type use_vllm vllm_device vllm_gpu_memory_utilization train_type torch_dtype max_length max_completion_length num_train_epochs per_device_train_batch_size per_device_eval_batch_size learning_rate gradient_accumulation_steps save_strategy eval_strategy eval_steps save_steps save_total_limit logging_steps warmup_ratio dataloader_num_workers num_generations temperature repetition_penalty deepspeed num_iterations num_infer_workers async_generate beta max_grad_norm Qwen2.5-VL-3B After SFT grpo true auto 0.6 full bfloat16 2048 1024 10 4 4 1e-6 4 steps steps 500 500 3 1 0.01 12 12 1.0 1.1 zero3 1 2 false 0.001 0.5 15 Figure 9: Comparison between GPT-4o and ShotVL. Figure 10: Visualization of failed cases. Figure 11: Output visualization of ShotVL. Figure 12: Prompt format used for answer extraction from GPT-4o. 17 process of SFT is performed on 4 Nvidia A100 GPUs and the GRPO process is performed on 8 Nvidia A100 GPUs. In our ablation study, we construct JSON formatted knowledge base on cinematography and use it to prompt Gemini-2.0-flash to generate reasoning process. We visualize some examples in Figure 13. Figure 13: Examples of constructed knowledge base on cinematic language."
        },
        {
            "title": "C Dataset Statistics",
            "content": "ShotBench Below is the list of titles used in constructing the benchmark. # Title Year IMDb ID Manchester by the Sea The Kids Are All Right Little Women Flamin Hot Quiet Place Belfast Green Book Phantom Thread Bridge of Spies 20th Century Women Passengers The Fabelmans Moonlight Licorice Pizza BARDO, False Chronicle of Handful of Truths 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Women Talking Foxcatcher 17 Christopher Robin 18 The Great Gatsby 19 Blade Runner 2049 20 Marriage Story 21 Tinker Tailor Soldier Spy 22 Nebraska 23 Black Swan 24 Youth 25 The Batman 26 Mad Max: Fury Road 27 2016 2010 2019 2023 2018 2021 2018 2017 2015 2016 2016 2022 2016 2021 2022 2022 2014 2018 2013 2017 2019 2011 2013 2010 2015 2022 2015 tt4034228 tt0842926 tt3281548 tt8105234 tt6644200 tt12789558 tt6966692 tt5776858 tt3682448 tt4385888 tt1355644 tt14208870 tt4975722 tt11271038 tt14176542 tt13669038 tt1100089 tt4575576 tt1343092 tt1856101 tt7653254 tt1340800 tt1821549 tt0947798 tt3312830 tt1877830 tt1392190 18 # Title Year IMDb ID Minari Sicario Knives Out Ma Raineys Black Bottom Amour Lincoln Judas and the Black Messiah Life of Pi Jojo Rabbit Inside Llewyn Davis The Banshees of Inisherin Barbie The Favourite 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Whiplash 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 Straight Outta Compton The Revenant Top Gun: Maverick Nomadland Carol Ad Astra RRR Midnight in Paris Separation The Hobbit: The Desolation of Smaug The Worst Person in the World The Power of the Dog Alice in Wonderland TÁR Can You Ever Forgive Me? Ted Hugo Cold War May December Her Unbroken Ida Ex Machina Beyond the Lights The Kings Speech American Sniper The Imitation Game Before Midnight Promising Young Woman Baby Driver Indiana Jones and the Dial of Destiny Captain Phillips Glass Onion: Knives Out Mystery The Disaster Artist Never Look Away Hail, Caesar! Star Trek Into Darkness Nightmare Alley All Quiet on the Western Front Fences Harriet Zero Dark Thirty No Time to Die Get Out Moneyball Skyfall Living The Lobster The Big Sick 2020 2015 2019 2020 2012 2012 2021 2012 2019 2013 2022 2023 2018 2014 2015 2015 2022 2020 2015 2019 2022 2011 2011 2013 2021 2021 2010 2022 2018 2012 2011 2018 2023 2013 2014 2013 2014 2014 2010 2014 2014 2013 2020 2017 2023 2013 2022 2017 2018 2016 2013 2021 2022 2016 2019 2012 2021 2017 2011 2012 2022 2015 2017 tt10633456 tt3397884 tt8946378 tt10514222 tt1602620 tt0443272 tt9784798 tt0454876 tt2584384 tt2042568 tt11813216 tt1517268 tt5083738 tt2582802 tt1398426 tt1663202 tt1745960 tt9770150 tt2402927 tt2935510 tt8178634 tt1605783 tt1832382 tt1170358 tt10370710 tt10293406 tt1014759 tt14444726 tt4595882 tt1637725 tt0970179 tt6543652 tt13651794 tt1798709 tt1809398 tt2718492 tt0470752 tt3125324 tt1504320 tt2179136 tt2084970 tt2209418 tt9620292 tt3890160 tt1462764 tt1535109 tt24734444 tt3521126 tt5311542 tt0475290 tt1408101 tt7740496 tt1016150 tt2671706 tt4648786 tt1790885 tt2382320 tt5052448 tt1210166 tt1074638 tt9051908 tt3464902 tt5462602 # Title Year IMDb ID Silver Linings Playbook Logan The Hobbit: The Battle of the Five Armies The Post The Descendants Triangle of Sadness The Trial of the Chicago 7 If Beale Street Could Talk The Ballad of Buster Scruggs Spectre Napoleon El Conde Lion Arrival Parasite The Lost Daughter Gravity The White Tiger 91 92 93 94 95 96 97 98 99 100 Mank 101 102 Maestro 103 Silence 104 Drive My Car 105 106 107 108 Moonrise Kingdom 109 Room 110 111 Real Steel 112 113 Roma 114 115 116 Django Unchained 117 The Lighthouse 118 Star Is Born 119 120 Babylon 121 Once Upon Time in Hollywood 122 123 King Richard 124 125 126 127 Winters Bone 128 129 Beasts of the Southern Wild 130 Da 5 Bloods 131 The Irishman 132 Darkest Hour 133 The Father 134 BlacKkKlansman 12 Years Slave 135 136 Adaptation. 137 Bridesmaids 138 Vice 139 140 141 142 Boyhood First Man 143 Parallel Mothers 144 The Martian 145 The Social Network 146 147 First Reformed 148 Deepwater Horizon 149 150 Dallas Buyers Club 151 Hacksaw Ridge 152 Dunkirk 153 Lady Bird Joker The Danish Girl The Wolf of Wall Street The Shape of Water La La Land Selma The Girl with the Dragon Tattoo The Hobbit: An Unexpected Journey Into the Woods 20 2015 2023 2023 2016 2016 2019 2021 2013 2021 2020 2020 2023 2016 2021 2012 2017 2014 2012 2015 2022 2011 2017 2018 2018 2018 2012 2019 2018 2011 2022 2016 2017 2021 2017 2019 2015 2010 2016 2012 2020 2019 2017 2020 2018 2013 2002 2011 2018 2011 2012 2014 2014 2018 2021 2015 2010 2017 2016 2013 2013 2016 2017 2014 tt2379713 tt13287846 tt21113540 tt3741834 tt2543164 tt6751668 tt9100054 tt1454468 tt6571548 tt10618286 tt1070874 tt5535276 tt0490215 tt14039582 tt1045658 tt3315342 tt2310332 tt1748122 tt3170832 tt7322224 tt0433035 tt6294822 tt6155172 tt7125860 tt6412452 tt1853728 tt7984734 tt1517451 tt1033575 tt10640346 tt4010884 tt5580390 tt9620288 tt4925292 tt7286456 tt0810819 tt1399683 tt3783958 tt2125435 tt9777644 tt1302006 tt4555426 tt10272386 tt7349662 tt2024544 tt0268126 tt1478338 tt6266538 tt1568346 tt0903624 tt2180411 tt1065073 tt1213641 tt12618926 tt3659388 tt1285016 tt6053438 tt1860357 tt0993846 tt0790636 tt2119532 tt5013056 tt1020072 # Title Year IMDb ID Fifty Shades of Grey The Theory of Everything The Two Popes Elvis Everything Everywhere All at Once True Grit Jackie The Adventures of Tintin The Greatest Showman Empire of Light Interstellar Extremely Loud & Incredibly Close 1917 127 Hours Spotlight 154 155 Nightcrawler 156 157 158 159 Killers of the Flower Moon 160 One Night in Miami... 161 Ford Ferrari 162 Anatomy of Fall The Midnight Sky 163 164 The Tree of Life 165 Brooklyn 166 American Hustle 167 The Lone Ranger 168 Mudbound 169 Oppenheimer 170 Another Round 171 172 Argo 173 174 175 Hell or High Water 176 The Hateful Eight 177 Mollys Game 178 News of the World 179 180 181 182 183 184 185 186 187 Rocketman 188 Marshall 189 Drive 190 191 192 Captain America: The Winter Soldier The Big Short 193 Tenet 194 Sound of Metal 195 The Holdovers 196 197 Les Misérables 198 Call Me by Your Name 199 Hidden Figures 200 201 202 Dont Look Up 203 204 Mr. Turner 205 Prisoners 206 Casino Royale 207 Polytechnique 208 Gladiator 209 47 Ronin 210 Mindhunters 211 212 Mission: Impossible 213 Wednesday 214 No Country for Old Men 215 216 Rebel Moon The Grand Budapest Hotel Past Lives Inherent Vice Tangled The Last Duel Poor Things The Curious Case of Benjamin Button 21 2014 2014 2022 2010 2016 2023 2020 2019 2023 2020 2011 2015 2013 2013 2017 2023 2020 2015 2012 2019 2022 2016 2015 2017 2020 2011 2017 2022 2014 2011 2019 2010 2015 2019 2017 2011 2014 2010 2014 2015 2020 2019 2023 2012 2017 2016 2014 2023 2018 2023 2014 2013 2006 2009 2000 2013 2004 2008 2011 2007 2007 2021 2023 tt2980516 tt2872718 tt6710474 tt1403865 tt1619029 tt5537002 tt10612922 tt1950186 tt17009710 tt10539608 tt0478304 tt2381111 tt1800241 tt1210819 tt2396589 tt15398776 tt10288566 tt2322441 tt1024648 tt8404614 tt3704428 tt2582782 tt3460252 tt4209788 tt6878306 tt0983193 tt1485796 tt14402146 tt0816692 tt0477302 tt8579674 tt1542344 tt1895587 tt2066051 tt5301662 tt0780504 tt1791528 tt0398286 tt1843866 tt1596363 tt6723592 tt5363618 tt14849194 tt1707386 tt5726616 tt4846340 tt2278388 tt13238346 tt6134232 tt14230458 tt2473794 tt1392214 tt0381061 tt1194238 tt0172495 tt1335975 tt0297284 tt0421715 tt1229238 tt1024676 tt0477348 tt4244994 tt14998742 # Title Year IMDb ID Thor: Love and Thunder Forrest Gump Loki season 2 217 218 One Piece Star Wars 219 The Witcher 220 The Lord of the Rings: The Rings of Power 221 222 There Will Be Blood 223 Bullet Train 224 Quantum of Solace 225 Dune: Part Two 226 227 228 Cruella 229 Fight Club 230 The Fall Guy James Bond 231 232 Mission: Impossible Scott Pilgrim vs. the World 233 John Wick 234 The Suicide Squad 235 The Killer 236 Superman 237 238 Inception 239 World of Warcraft 240 The Raid 241 Barry Lyndon 242 Captain America: Civil War 243 Marvels Jessica Jones 2022 2018 1977 2017 2022 2007 2022 2008 2024 1994 2023 2021 1999 2024 2015 2011 2010 2014 2021 2023 1978 2010 2014 1954 1975 2016 2015 tt10648342 tt10109772 tt0076759 tt7351402 tt7631058 tt0469494 tt12593682 tt0830515 tt15239678 tt0109830 tt18271346 tt3228774 tt0137523 tt1684562 tt4896340 tt1229238 tt0446029 tt2911666 tt6334354 tt1136617 tt0078346 tt1375666 tt4191810 tt0047388 tt0072684 tt3498820 tt2357547 An overview of cinematic terms used in ShotBench and the distribution of QA pairs are visualized in Figure 14a and Figure 14b. Details of ShotBench are provided in Table 8. (a) Overview of cinematic terms used in ShotBench. (b) Distribution of questions. Figure 14: Statistics of ShotBench across different dimensions. ShotQA The data distribution of ShotQA is reported in Table 9. Reference Materials on Cinematography Used in ShotBench Construction. During the construction of ShotBench, we trained annotators through professional teaching websites and teaching videos publicly available on the Internet. We provide some representive materials in Table 10. 22 Table 8: Distribution of cinematic terms used in ShotBench % 13.3 13.1 13.1 12.6 12.1 11.8 11.6 15.4 14.8 14.5 14.2 14.2 13.6 13.5 20.7 20.1 19.8 19.7 19.7 25.5 25.2 24.7 24.7 12.7 11.9 10.5 10.0 9.4 9.4 9.3 8.8 8.6 7.7 1.2 0.8 10.8 10.7 10.3 10.2 10.1 10.1 10.0 10.0 9.8 8. 17.4 17.1 16.7 16.4 16.3 16.2 10.4 9.1 8.5 7.7 7.4 7.1 6.8 6.5 6.4 6.3 5.9 4.7 4.5 3.9 3.7 0.6 Dimension Term Shot Size Shot Framing Camera Angle Lens Size Lighting Type Lighting Condition Composition Camera Movement Wide Close Up Extreme Wide Medium Close Up Medium Wide Medium Extreme Close Up Single Insert 2 shot Group shot Establishing shot Over the shoulder 3 shot Aerial Overhead Low angle High angle Dutch angle Long Lens Wide Ultra Wide&Fisheye Medium Daylight Artificial light Mixed light Firelight Overcast Practical light Sunny Moonlight Fluorescent HMI Tungsten LED Side light Backlight High contrast Silhouette Edge light Underlight Top light Hard light Soft light Low contrast Center Balanced Symmetrical Right heavy Left heavy Short side Push in Pull out Boom up Pan left Pan right Tilt down Tilt up Boom down Zoom in Static Move to the right Move to the left Zoom out Arc Camera roll Dolly zoom 23 Table 9: Sample distribution in the ShotQA. Dimension #Samples Camera Angle (CA) Shot Composition (SC) Lens Size (LS) Lighting Condition (LC) Lighting Type (LT) Shot Framing (SF) Shot Size (SS) Camera Movement (CM) 9,405 9,597 8,324 8,778 6,811 8,298 8,579 1,200 Dimension Shot Size Shot Framing Camera Angle Lens Size Lighting Shot Composition Camera Movement Table 10: Representative reference materials used to train annotators. Website Video https://www.studiobinder.com/blog/ types-of-camera-shots-sizes-in-film/ https://www.studiobinder.com/blog/ types-of-camera-shot-frames-in-film/ https://www.studiobinder.com/blog/ types-of-camera-shot-angles-in-film/ https://www.studiobinder.com/blog/ focal-length-camera-lenses-explained/ https://www.studiobinder.com/blog/ film-lighting/ https://www.studiobinder.com/blog/ rules-of-shot-composition-in-film/ https://www.studiobinder.com/blog/ different-types-of-camera-movements-in-film/ https://www.youtube.com/watch?v= AyML8xuKfoc https://www.youtube.com/watch?v= qQNiqzuXjoM https://www.youtube.com/watch?v= wLfZL9PZI9k https://www.youtube.com/watch?v= uSsIqR3DuK8 https://www.youtube.com/watch?v=r2nD_ knsNrc https://www.youtube.com/watch?v= hUmZldt0DTg&t=10s https://www.youtube.com/watch?v= IiyBo-qLDeM"
        },
        {
            "title": "E Societal Impact Statement",
            "content": "This work presents ShotBench, benchmark for evaluating vision-language models (VLMs) on cinematc language understanding, and ShotQA, large-scale dataset designed for training such capabilities. Additionally, we propose ShotVL, reasoning-enhanced VLM trained via SFT and GRPO. Positive Societal Impact. By improving VLMs understanding of professional cinematc conventions, our work can contribute to the development of AI systems that assist in film production. Specifically, cinematography-aware models may support AI-assisted filmmaking tasks such as shot planning, automated style matching, and film-level image/video generation. These capabilities could help democratize access to professional filmmaking workflows, reduce production costs, and empower creators with limited resources. In addition, our benchmark and dataset may foster research into multimodal reasoning, benefiting broader applications in video understanding and generation. Negative Societal Impact. As with other generative or vision-language technologies, there are potential negative applications. For example: Disinformation and deepfakes: Enhanced understanding of cinematic language could be exploited to make AI-generated fake content more visually convincing or emotionally manipulative. Creative job displacement: The use of cinematography-aware models in automated filmmaking pipelines may marginalize certain creative roles (e.g., assistant editors, junior cinematographers). Bias propagation: If the training data or annotations reflect specific cultural aesthetics or norms (e.g., Western cinematc styles), the resulting models may encode biased visual preferences or overlook underrepresented filmmaking traditions."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "Tongji University"
    ]
}