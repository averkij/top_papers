{
    "paper_title": "Training-free Regional Prompting for Diffusion Transformers",
    "authors": [
        "Anthony Chen",
        "Jianjin Xu",
        "Wenzhao Zheng",
        "Gaole Dai",
        "Yida Wang",
        "Renrui Zhang",
        "Haofan Wang",
        "Shanghang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 9 3 2 0 . 1 1 4 2 : r Training-free Regional Prompting for Diffusion Transformers Anthony Chen1,2, Jianjin Xu3, Wenzhao Zheng4 Gaole Dai1, Yida Wang5, Renrui Zhang6, Haofan Wang2, Shanghang Zhang1 1Peking University, 2InstantX Team, 3Carnegie Mellon University 4UC Berkeley, 5Li Auto Inc., 6CUHK antonchen@pku.edu.cn Figure 1: Overview of our method. Given user-defined or LLM-generated regional prompt-mask pairs, we can effectively achieve fine-grained compositional text-to-image generation."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1. In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX. Corresponding Author"
        },
        {
            "title": "Introduction",
            "content": "Text-to-image models have been evolving over the past few years and have made great progress thanks to the emergence of diffusion models [1, 2, 3, 4, 5, 6, 7]. These models come with superior visual quality, capability of generating diverse styles and promising prompt adherence given short or descriptive prompts. However, despite these remarkable advances, they still face challenges in accurately processing prompts with complex spatial layouts [8]. On the one hand, it is very difficult to describe the exact spatial layout through natural language, especially when the number of objects increases or precise position control is required. For example, we rely on external conditional control (such as ControlNet [9]) to generate precise poses instead of describing the movements of the hand. On the other hand, although the ability of prompt adherence has been improved with the advancement of the model [4, 3, 10], when dealing with complex elements and relationships in long texts, the model still has drift problems such as confusing concepts and missing elements. Therefore, explicit spatial control is still necessary for compositional generation by now. To tackle these challenges, people have made different attempts [4, 3, 11, 10, 12]. With the base model, research shows that the prompt following ability of text-to-image generation depends largely on the representation ability of the text encoder; that is, larger and stronger text model can significantly enhance the prompt following ability. For example, Stable Diffusion 3/3.5 [3] and FLUX.1 [4] additionally introduce T5-XXL [13] as one of the text encoders besides of the coarse-align CLIP [14] encoder, allowing the model to have the ability to render visual text. The Playground-3.0 [12] model further replaces the text encoder with large language model (Lllma3 [15]) and uses the representation of the intermediate layer instead of the global pooling representation to achieve stronger text understanding capabilities. Clearly, these advances in base models improve overall semantic understanding but still fall short of precise compositional generation to meet user needs. In addition to improving the base model, some recent studies [16, 8, 17, 18, 19] have proposed to handle compositional control by providing spatial conditions (layout/box) and training control module as plugin on top of the base model, or to manipulate the attention score map using region masks in training-free manner. For example, InstanceDiffusion [16] adds precise instance-level control via learnable UniFusion blocks to handle the additional per-instance conditioning. RPG [8] employs the Multi-modal Large Language Model (MLLM) [20] as global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions, and proposes complementary regional diffusion to enable region-wise compositional generation. DenseDiffusion [18] and Omost [19] develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. In this report, we are mainly inspired by Omost, but work on one of the recent diffusion transformer based models, FLUX.1-dev, which differs from previous base models mainly in its design of MMDiT structure where the prompt representation updates dynamically. We investigate training-free attention manipulation for this structure, so that it is not tied to specific model and can be easily applied to models with similar designs. The code will be released and we hope the community can enjoy."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Prompt Following in Diffusion Models Generative models, especially diffusion-based models [1, 2, 3, 4], have achieved remarkable progress in both image quality and semantic understanding. Latent diffusion model (LDM), originally known as Stable Diffusion 1.4/1.5 [1], adopts CLIP ViT-L [14] as text encoder for text representation, and shows promising text-guided image generation at that moment. SDXL [2], as successor, uses OpenCLIP ViT-bigG [21] in combination with CLIP ViT-L [14], and shows that the scale and the larger text encoder help to improve the text rendering capabilities. In recent diffusion transformer based models, Pixart [5, 6], Stable Diffusion 3/3.5 [3] and FLUX.1 [4] further use T5-XXL [13] as the text encoder to handle longer and more complex prompts, and demonstrate remarkable visual text rendering capability in the absence of any explicit text prior. Hold on, scaling up text encoders seems to have immediate gains and has become consensus, so natural thought is, what if large language model is used? After all, intuitively it should have better text understanding capabilities. Therefore, there are also some works have turned their attention to large language models (LLMs). Kolors [11] follows the structure of SDXL [2], but adopts pre-trained General Language Model(GLM) [22] as 2 Figure 2: Main results. Simplified regional prompts are colored according to the layout mask. In practice, we input more detailed regional prompt about each region. text encoder, and exhibits an advanced comprehension of both English and Chinese, as well as its superior text rendering capabilities. Lumina-T2X [10] incorporate variety of diverse text encoders with varying sizes, including CLIP [14], LLama [15, 23], SPHINX [24], and Phone encoders [25], to optimize text conditioning. But so far, all these works have only used the last hidden states of the text model or the global pooling representation. Playground V3 [12] takes very different way, and novelly proposes to fully integrates Large Language Models (LLMs) with novel visual structure, based on the statement that the knowledge within the LLM spans across all its layers, rather than being encapsulated by the output of any single layer. This design allows to take the hidden embedding outputs from each corresponding layer from LLM as conditioning at each respective layer of visual model, and achieves unparalleled prompt-following and prompt-coherence abilities. In nutshell, scaling up text encoders, cascading multiple text encoders, and leveraging multi-scale hidden representations all improve prompt following in diffusion models. 2.2 Compositional text-to-image Generation Compositional generation introduces spatial conditioning to guide the image generation process with precise layout. Although prompt adherence has been greatly improved, precise layout control that meets real-world demand is still far from enough. There have been many works [26, 27, 18, 17, 16, 28, 29, 8, 19] that enable region-wise compositional generation. These approaches can be roughly divided into two categories based on whether they are training-based or not. GLIGEN [17] users Fourier embedding to encode bounding box coordinates and fuses it with corresponding text feature via learnable projection. new gated self-attention layer is inserted to take in the new conditional 3 Figure 3: Illustration of our Region-Aware Attention Manipulation module. The unified self-attention in FLUX can be broken down into four parts: cross-attention from image to text, cross-attention from text to image, and self-attention between image. After calculating the attention manipulation mask, we merge them to get the overall attention mask that is later fed into the attention calculation process. localization information. InstanceDiffusion [16] adds precise instance-level control via learnable UniFusion blocks to handle the additional per-instance conditioning. MS-Diffusion [28] trains grounding resampler to adeptly assimilate visual information and facilitates precise interactions between the image condition and the diffusion latent within the multi-subject attention layers. These works usually require training external control modules to process regional masks or bounding box. For training-free methods, Mixture of Diffusers [26] and MultiDiffusion [27] conduct denoising steps on each region using the corresponding description, and the overall noise prediction is obtained by merging each individual noise prediction. Similarly, RPG [8] parallelly denoises each subregion and applies resize-and-concatenate post-processing step to achieve high-quality compositional generation. Furthermore, DenseDiffusion [18] and Omost [19] develop attention modulation within cross-attention layers that guides objects to appear in specific regions according to layout guidance."
        },
        {
            "title": "3 Method",
            "content": "Task Formulation. Our main objective is to enhance the compositional generation capabilities of current text-to-image models, enabling them to capture textual and spatial conditions in trainingfree manner. To be more specific, we focus on the state-of-the-art text-to-image generation model, FLUX.1, and formally define our condition as set of tuples and global description. Each tuple (ci, mi) describes sub-region within the image, where ci is description for region, and mi is corresponding binary mask. The global description cbase captures the overall semantic content. Given input spatial conditions, we modulate attention maps so that the layout of objects specified by ci can be generated within the corresponding area mi. In practical use cases, users can automatically generate these conditions leveraging the reasoning capacity of large language models (LLM). Region-Aware Attention Manipulation. We construct an attention mask = {mij} RLL, where = Limage + (cid:80)N i=1 Li, Limage is the length of image (e.g., for flattened 2D downsampled feature map), and Li is the length of the i-th regional prompt token. 4 Let RLimageD represent the full image feature, where is the feature dimension. We have regional prompts {c1, c2, ..., cN } corresponding to regions {m1, m2, ..., mN } in the image. The unified mask attention operation in MMDiT can be expressed as: Attention(Q, K, V, ) = Softmax (cid:18) QK dk (cid:19) V, (1) where Q, K, and are concatenated features from the image and all texts, dk is the dimension of the key vectors, and denotes element-wise multiplication. To simplify illustration, as in Figure 3. we can consider the unified attention operation as four parts: cross-attention from image to text, cross-attention from text to image, self-attention between image and self-attention between text. Thus, we denote our mask construction process as, = (cid:20)Mi2i Mi2t Mt2i Mt2t (cid:21) (2) For cross-attention from image to text, similar to previous work [19], we apply regional masks to ensure only the image tokens within each region attend to their corresponding text, which helps maintain region-specific visual-textual associations: Mi2t = [R1 11L1, R2 11L2, ..., RT 11LT ] (3) where Ri {0, 1}Limage is the binary mask for the i-th region, Li is the number of tokens in the i-th text prompt, and denotes the outer product. For cross-attention from text to image, we control each query text to only attend to its corresponding region in the image, ensuring focused text-to-image interactions: Mt2i = i2t For self-attention between texts, to prevent prompt leakage and maintain the independence of each text prompt, each prompt can only attend to itself: Mt2t = diag(1L1, 1L2, ..., 1LT ) (4) For self-attention within the image, we allow attention only within each region: Mi2i = (cid:80)T RT . This approach maintains the integrity of region-specific information. i=1 Ri Our region-aware attention module ensures that each region-text pair is properly considered in the attention mechanism while maintaining the integrity of the full image feature and preventing unwanted interactions between unrelated regions and prompts. We obtain our regional latent as zregion t1 = ψ(Attention(Qregion t1 , region , region t1 , region) (5) where ψ denotes the post-process in each transformer blocks. To further improve the overall coherence of image compositions and ensure harmonious transition in the boundaries of different regions, we also update base latent as t1 = ψ(Attention(Qbase zbase t1 , base t1 , base t1 , base)) (6) we leverage the generated latent from base prompt cbase by combining it with the aforementioned regional latent following [8]. zt1 = β zbase t1 + (1 β) zregion t1 (7) The parameter β serves as balancing coefficient, optimizing the trade-off between aesthetic fidelity to human visual preferences and semantic alignment with the intricate textual prompt guiding the image generation process. This calibration allows for adjusting the models output to achieve an optimal synthesis of visual appeal and prompt adherence. 5 Figure 4: Results with LoRAs and ControlNet. Colored prompts and masks are provided for the regional control for each example. The control image (pose & depth-map) for controlnet is attached within the left image. Zoom in to see in detail. 6 Figure 5: Ablation results with base ratio β, control steps and control blocks B. Besides, there are another two factors that matter: which denoising steps and which DiT blocks B, to inject regional control. Empirically, we only inject regional control in the first few denoising steps, as we find that the earlier steps in the diffusion process decide the overall layout of the image. This also helps minimize additional computation cost. For the choice of DiT blocks to inject control, we use all layers by default, but we find that the single blocks directly affect the strength of control, thus, if severe visual boundaries are observed, choosing less single layers would help."
        },
        {
            "title": "4 Experiments",
            "content": "Implementation Details. We use FLUX.1-dev as our DiT backbone and GPT-4o as our regional prompt generator. The choice of the three factors (base ratio β, injection steps , and injection blocks B) varies for each sample in the following figures, we manually select balanced factors for the best visual results. The experiments are conducted on single NVIDIA A800-SXM4-80GB GPU. 7 Main results. Our main results in Figure 2 demonstrate our performance across various regional mask settings, highlighting its adaptability and precision in handling diverse visual prompts. As the number of regional masks increases, the model maintains strong alignment with each specified region, accurately translating the characteristics and distinctions required by the prompt into visually cohesive sections. Generalization Ability. As shown in Figure 4, our method can be combined with other plug-and-play modules like LoRAs [30] and ControlNet [9]. Provided with regional prompts, FLUX generates images that have richer details. The LoRAs and ControlNet weights are taken from public repositories by Shakerlab2. Figure 6: Inference speed and gpu memory consumption comparison with standard FLUX.1-dev, FLUX equipped with RPG-based regional control, and our method. Ablation Analysis. Figure 5 ablates the three factors (base ratio β, injection steps , and injection blocks B). We observe that while increasing regional alignment, each factor introduces trade-offs affecting image quality. Specifically, lower base ratio, additional injection steps, and more injection blocks enhance alignment with regional prompts, creating images that better adhere to specified regions and their attributes. However, this intensified alignment often comes at the cost of severe visual boundaries between regions, disrupting the overall cohesion and aesthetic appeal. Consequently, achieving optimal image quality hinges on balanced choice of these factors, where fine-tuning each parameter avoids harsh regional divisions while maintaining high degree of prompt fidelity. Memory Consumption and Inference Speed. We compare our method with standard FLUX.1-dev and RPG with FLUX.1-dev. Note that this comparison is done on single NVIDIA A800-SXM4-80GB GPU. We implement RPG by generating hidden states and later resize-and-concatenat them all together, before joining base hidden statues in weighted sum manner. As shown in Figure 6, under same region masks input, we run much faster than RPG-based regional control method, when mask number reaches 16, our inference time is 9 times faster than RPG. Additionally, our GPU memory consumption is also less than RPG. Limitations. key limitation of our approach lies in the difficulty of tuning the factors as the number of regional masks increases. While the model is designed to handle multiple regions effectively, achieving perfectly balanced image becomes increasingly challenging with higher number of masks. With more regions to manage, the tuning of factors such as the base ratio, injection steps, and injection blocks requires delicate adjustment to maintain both semantic alignment with the prompt and visual cohesion across regions. This complexity often leads to trade-offs where enhancing prompt fidelity in one region may introduce unintended visual boundaries or affect the seamless integration of other regions. Consequently, as the number of masks grows, it becomes more difficult to calibrate these factors precisely to produce cohesive and visually satisfying image."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose training-free regional prompting method for FLUX.1, enabling fine-grained compositional generation for transformer-based models with swift and responsive image generation. Our approach enhances the prompt-following capability of FLUX.1, allowing it to handle complex, multiregional prompts with improved semantic alignment and precise regional differentiation, all without the need for model retraining or additional data. This efficiency not only reduces the time required for image production but also streamlines users workflows, allowing them to work more effectively. While an increased number of regional masks can make fine-tuning factors challenging for seamless outputs, this method represents significant advancement in flexible, precise, and high-speed image generation within transformer-based frameworks. 2https://huggingface.co/Shakker-Labs"
        },
        {
            "title": "References",
            "content": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [2] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [3] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning. [4] Black Forest Labs. black-forest-labs/flux github page, 2024. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. [7] Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Yu Qiao, Peng Gao, and Hongsheng Li. Pixwizard: Versatile image-to-image visual assistant with open-language instructions. arXiv preprint arXiv:2409.15278, 2024. [8] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-toimage diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. [9] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [10] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [11] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. [12] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [14] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [16] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62326242, 2024. [17] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. [18] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77017711, 2023. [19] Omost Team. Omost github page, 2024. [20] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 9 [21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below. [22] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. [23] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In ICLR 2024, 2024. [24] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. [25] Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, Tao Qin, and Tie-Yan Liu. Token-level ensemble distillation for grapheme-to-phoneme conversion. arXiv preprint arXiv:1904.03446, 2019. [26] Álvaro Barbero Jiménez. Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412, 2023. [27] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. [28] Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. [29] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Compositional text-to-image synthesis with attention map control of diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 55445552, 2024. [30] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021."
        }
    ],
    "affiliations": [
        "Peking University",
        "InstantX Team",
        "Carnegie Mellon University",
        "UC Berkeley",
        "Li Auto Inc.",
        "CUHK"
    ]
}