{
    "paper_title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
    "authors": [
        "Youngsik Yun",
        "Dongjun Gu",
        "Youngjung Uh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR."
        },
        {
            "title": "Start",
            "content": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization Youngsik Yun Yonsei University bbangsik@yonsei.ac.kr Dongjun Gu UNIST djku1020@unist.ac.kr Youngjung Uh Yonsei University yj.uh@yonsei.ac.kr 5 2 0 2 2 ] . [ 1 8 1 9 7 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from machine learning perspective, framing novel view synthesis as generalization problem to unseen viewpointsan underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves wide range of baselines. Code will be available at https: //bbangsik13.github.io/FASR. 1. Introduction Reconstructing 3D scenes from multi-view 2D images has been long-standing problem of interest. 3D Gaussian Splatting (3DGS) [23] achieves photo-realistic fidelity in novel view synthesis with real-time rendering. However, it requires densely captured input views, which are costly to obtain. In sparse-view settings, they often overfit to training views, resulting in poor generalization to novel viewpoints with unresolved details and floating artifacts. To improve Figure 1. Overview. Our proposed optimization algorithm improves generalization. Given eight training views rendered from the lego scene in Blender synthetic dataset [41], our method maintains low Average Error [44] across interpolated novel views, whereas 3DGS exhibits overfitting. Plots are means and standard deviations over ten runs. generalization, previous approaches have adopted various strategies, such as integrating geometric priors from depth and flow estimators [8, 66], and leveraging dense correspondence prediction models [20]. Although these methods show effectiveness, 3DGS optimization has been underexplored as machine learning problem where generalization to novel viewpoints is important. In this paper, we propose an optimization algorithm for 3DGS variants to improve quality in novel viewpoints, i.e., generalization  (Fig. 1)  . Our motivation is to optimize the model toward the flat minima in the loss landscape, which are widely known to promote better generalization. As illustrated in Fig. 2, sharp minima1 exhibit large gap between train and test loss, whereas flat minima show much 1We omit local for brevity. smaller gap, indicating better generalization. For example, floaters are invisible in training views but critically emerge in novel viewpoints; i.e., the Gaussians overfit to the training views. This large generalization gap implies sharp minimum, i.e., loss dramatically changes even with slight perturbations on the model parameters2. Although Sharpness-Aware Minimization (SAM) [12] theoretically and empirically demonstrates that pursuing flatter minima leads to improved generalization of classification networks, achieving flat minima does not always guarantee better generalization in reconstruction tasks because the importance of sharpness for accurate reconstruction varies across regions. In particular, model parameters representing high-frequency details (e.g., edges) inherently induce sharp loss landscape; even small changes in wellfitted Gaussian cause drastic change in the loss. In contrast, for parameters representing low-frequency regions, the loss changes more gradually, favoring flat minima. In this sense, finding flat minimum for all Gaussians is problematic; sharpness in high-frequency areas is desirable for accurate reconstruction, flatter minima in low-frequency regions are preferable for better generalization. To this end, we propose Frequency-Adaptive Sharpness Regularization (FASR), an optimization algorithm that penalizes local sharpness, where both the neighborhood radius used for its computation and the regularization weight are set in inverse proportion to the local frequency, encouraging flatter loss landscape while retaining the sharpness required for fine details. We show that the enhancement of our method is complementary to prior methods, thereby providing additional performance gains across them. Our contributions are significant as follows: To the best of our knowledge, we present the first fundamental investigation into the relationship between loss landscape and generalization in novel view synthesis. Consequently, we propose an optimization algorithm for 3DGS by reformulating SAM in frequency-adaptive manner, overcoming the limitation of SAM, which oversmooths high-frequency details. Our method is versatile in improving wide range of baseline methods on various datasets. 2. Related work 2.1. Sparse-view reconstruction Reconstructing scenes from highly sparse inputs remains fundamental challenge, as limited supervision often leads 2Camera perturbations can be interpreted as Gaussian extrinsic parameter perturbations, e.g., parallel movement of all Gaussians is equivalent to moving the camera. 3Figure is reproduced from Keskar et al. [24]. As empirically shown in Izmailov et al. [18], the test loss landscape tends to shift relative to the train loss landscape, while Liu et al. [37] showed that the loss landscape differs with the imbalance level of the dataset. Figure 2. Conceptual 1D Loss Landscape of Flat and Sharp Minima3. Flat minimum better generalize then sharp minimum. models to overfit training views, resulting in insufficient generalization in unseen views. Early efforts sought to extend neural radiance fields (NeRFs) [41] by incorporating additional regularizations or auxiliary cues. For example, these studies [9, 44, 54, 60] introduce geometryor depthbased constraints and frequency regularization to alleviate the underconstrained nature of sparse-view optimization. Although these approaches demonstrate effectiveness, they inherit the slow training and rendering processes of NeRF. Recent studies have shifted to use 3D Gaussian Splatting (3DGS) [23], aiming for real-time rendering efficiency. These studies [8, 20, 31, 58, 66, 68] similary leverage external priors such as depth [6, 46], correspondence [30], or flow [49], while another focuses on ensemble-like regularization [45, 63, 65]. Sharing key insight with our method, Sparfels [21] aims to minimize worst-case loss for robustness; however, it does so by freezing the Gaussian means and approximating the objective with an upper bound, which simplifies to color variance regularization along each ray. Although this proxy effectively improves details, it may overlook color-matched floaters. In contrast, our method directly optimizes the worst-case loss with respect to all Gaussian parameters, interpreting it as sharpness regularization. Importantly, our optimization algorithm is complementary: rather than altering the representation or adding priors, we fundamentally guide 3DGS to converge on general solution in under-constrained sparse supervision. 2.2. Flatness and generalization The relationship between the flatness of the loss landscape and model generalization has been extensively investigated in prior research. Keskar et al. [24] shows that converging to sharp minima leads to poor generalization, while Jiang et al. [22] identifies that sharpness is the most correlated indicator of generalization. Subsequent work [7, 18] demonstrated that averaging parameters along training trajectories can lead to flatter minima with better generalization. Beyond averaging strategies, Sharpness-Aware Minimization (SAM) [12] explicitly estimates and reduces sharpness, inspiring subsequent work [2, 34, 42, 51] to analyze and improve upon its formulation. Moreover, some works have analyzed the accuracy of estimated sharpness as measure of generalization. They show that sharpness changes with parameter rescalings [10], and the appropriate sharpness estimation differs across different training setups and tasks [3], which hinders the correlation between sharpness and generalization. Consequently, recent work introduces normalization and invariant formulations to appropriately estimate sharpness, enabling more reliable link between sharpness and generalization [16, 19, 26, 28, 53]. On the other hand, some works report counterexamples where sharper models generalize well, indicating that flatter minima are not always the optimal strategy for generalization [4, 10, 56]. Building on this perspective, we revisit the loss landscape sharpness in the context of the reconstruction task. We hypothesize that the optimal sharpness is not uniform but varies with the local frequency of the signal. Furthermore, the radius for estimating the local sharpness should vary with the local frequency. Therefore, instead of pursuing universally flat minimum, we introduce frequencyadaptive optimization strategy that appropriately regularizes sharpness for the reconstruction task, allowing sharp minima where they benefit high-frequency details. 2.3. Reconstructing with perturbation Random perturbation has been introduced into radiance field training for two primary purposes. Unlike our approach, which aims to improve generalization, some studies employ perturbation for uncertainty quantification rather than relying on deterministic approach. Stochastic or Bayesian formulations of NeRF have explicitly modeled radiance or density distributions [29, 48]. Subsequent work perturbs trained models to estimate epistemic uncertainty [15]. Similar ideas have been extended to 3DGS, where Gaussian parameters are sampled from learned distributions to render both images and calibrated uncertainty maps [1]. Some works [13, 25, 36] achieve robustness by injecting random noise into Gaussian parameters or query locations, which can be interpreted as randomly choosing parameters in neighborhood radius. Nevertheless, this random perturbation serves as an inefficient proxy for finding the worst-case loss and may also introduce unexpected artifacts (Sec. provides more details). In contrast, our method adversarially perturbs parameters along the gradient direction to calculate the worst-case loss within that radius. 3.1. Preliminary: Sharpness-Aware Minimization Sharpness-Aware Minimization (SAM) [12] is an optimization method that improves generalization by encouraging solutions to lie in flat regions of the loss landscape. Along with minimizing the empirical loss at parameter point w, SAM estimates the loss sharpness within neighborhood of radius ρ. The optimization then jointly minimizes both the empirical loss and this estimated sharpness: LSAM (cid:122) max ϵ2ρ loss sharpness (cid:125)(cid:124) L(w + ϵ) L(w) + (cid:123) empirical loss (cid:122) (cid:125)(cid:124) (cid:123) L(w) L(w + ϵ) , (1) = max ϵ2ρ (cid:124) (cid:123)(cid:122) worst-case loss (cid:125) which is equivalent to the worst-case loss. In practice, it is approximated via first-order Taylor expansion for the loss function around the current parameters, perturbing the parameters in the gradient direction with magnitude ρ: ˆϵ(w) arg max ϵ2ρ L(w + ϵ) ρ wL(w) wL(w)2 . (2) Then, the model parameters are updated via Stochastic Gradient Descent (SGD) [43] step, with the gradient computed at the estimated local maximum + ˆϵ(w): λlr wL(w)w+ˆϵ(w), where λlr is learning rate. Supporting the intuition of Fig. 2, this method improves generalization by tightening the generalization bound based on sharpness derived from the PAC-Bayesian framework [11, 39, 47]: Theorem 1. For any ρ > 0, with training set from data distribution D, LD (w) max ϵ2ρ LS (w + ϵ) + h(w2 2/ρ2), where : R+ R+ is strictly increasing function (under some technical conditions on LD (w)). In practice, ρ is hyperparameter. The sharpness can be interpreted as regularization term, as in Weighted SAM (WSAM) [61]: LWSAM L(w) + γ 1 γ [ max ϵ2ρ γ 1 γ L(w + ϵ) L(w)] (3) max ϵ2ρ L(w + ϵ). 3. Method = 1 2γ 1 γ L(w) + We first provide preliminary on SAM [12] (Sec. 3.1). Then, we explore applying SAM to 3DGS (Sec. 3.2). Finally, Sec. 3.3 presents our frequency-adaptive sharpness regularization. When the weight hyperparameter γ is set to 0, the sharpness term vanishes and the optimization minimizes the empirical loss only. When γ = 0.5, WSAM is equivalent to SAM (Eq. (1)), while for 0.5 < γ < 1, the sharpness term Algorithm 1 Applying SAM to 3DGS Input: Multi-view images v, where camera Output: Optimized = (µ, q, s, σ, YDC, YAC) 1: while not converged do 2: 3: Loss(Render(G, v), v) for all θ do ˆθ θ + ρθ 4: θL θL2 end for ˆG ( ˆθ θ G) ˆL Loss(Render( ˆG, v), v) Adam( ˆG 5: 6: 7: 8: 9: end while ˆL) // Get loss // Ascent step // Local maximum // Get loss // Descent step is emphasized. Accordingly, in the extended Theorem 1, maxϵ2ρ LS (w + ϵ) = LSAM , and the function is modified. becomes LWSAM See Foret et al. [12] and Yue et al. [61] for the full theorem statement and proof. 3.2. Applying SAM to 3DGS defined as Gi = Let Gi be the i-th Gaussian, (µi, qi, si, σi, YDC ), where µ, q, s, σ, and represent the mean, rotation, scale, opacity and spherical harmonics (SH) coefficients, respectively, with YDC and YAC corresponding to the DC and AC terms of SH. , YAC As in the 3DGS [23] pipeline, we render 3D Gaussians = (Gi)N i=1 from the camera view and compute the loss (Algorithm 1.2). The gradient of this loss guides the parameter to its local maximum (Algorithm 1.3-6). Similar to the SAM pipeline, we then evaluate the loss ˆL at the local maximum and use this gradient to update the original parameters via the Adam optimizer [27] (Algorithm 1.7-8). However, this direct application often leads to suboptimal performance as shown in Sec. 4.2.1. Unlike the classification task, in the reconstruction task, the curvature of the landscape is highly correlated with the image frequency. Loss in high-frequency regions requires sharp minima for accurate reconstruction, making it sensitive to perturbations, whereas low-frequency regions require flat minima and are less sensitive to them. Due to this differing sensitivity, fixed ρθ is problematic. Specifically, in highfrequency regions, it leads to inaccurate sharpness estimation by causing large first-order approximation errors (Eq. 2). Conversely, in low-frequency regions, the perturbation is too weak to be meaningful, limiting the improvement of SAM. Moreover, SAM penalizes estimated sharpness with the fixed regularization weight γ = 0.5. It over-penalizes the high-frequency regions, which must remain sharp to preserve details. Concurrently, it under-penalizes the lowfrequency regions, failing to sufficiently improve generalization. Figure 3. Overview of our proposed method. Key hypothesis. Based on this intuition, we hypothesize that for each Gaussian, the optimal neighborhood radius ρθ and regularization weight γ vary in correlation with image frequency, which effectively tightens the WSAM-extended version of the generalization bound in Theorem 1. 3.3. Frequency-Adaptive Sharpness Regularization To address the limitation of SAM, we introduce FrequencyAdaptive Sharpness Regularization (FASR). As illustrated in Fig. 3, we first estimate the local sharpness of each Gaussian attribute independently (Sec. 3.3.1). Then we adjust the perturbation magnitude and the regularization weight of each Gaussian attribute4 (Secs. 3.3.2 and 3.3.3) using the precomputed scale map Γv at view v, which is inversely proportional to the local frequency (detailed in Sec. E). 3.3.1. Separate sharpness per-Gaussian Unlike neural networks, 3DGS is an explicit model, which allows us to associate the local frequency of each pixel with its corresponding Gaussian. To apply frequency adaptivity, we begin by computing gradient on the per-Gaussian attribute θi Gi separately rather than the entire parameter set. Accordingly, Algorithm 1.4 becomes ˆθi θi + ρθ θiL θiL2 . (4) This step has the advantage of estimating the local sharpness of each Gaussian attribute independently. Specifically, it mitigates the first-order approximation error of sharpness by large gradient Gaussians. 3.3.2. Frequency-adaptive perturbation magnitude Going step further, we adapt the perturbation magnitude according to the local image frequency. Specifically, for each Gaussian, we query the value of the optimal scale map at the projected center coordinate (x, y) on the 2D camera plane, γi Γv(x, y). Since this value is obtained in the 2D image space, we multiply the rendered depth at the same location, di Dv(x, y), and divide by the focal length 4We apply this adaptivity to the mean µi, rotation qi, and scale si, which are geometric attributes of the Gaussian. Table 1. Quantitative comparison. Our method improves baselines across the board. Method PSNR LLFF (3 views) SSIM LPIPS MipNeRF-360 (12 views) SSIM PSNR LPIPS 3DGS [ACM ToG23] + Ours 19.810 .339 20.783 .300 .6790 .0078 .7197 .0032 .2145 .0065 .1965 .0034 18.903 .179 19.303 .185 .5499 .0036 .5622 .0051 .3734 .0042 .3552 . CoR-GS [ECCV24] + Ours 20.185 .142 20.862 .154 .7015 .0040 .7283 .0042 .2029 .0035 .1932 .0030 19.515 .243 19.805 .233 .5733 .0049 .5833 . .3741 .0066 .3681 .0069 DropGaussian [CVPR25] + Ours 20.461 .212 20.853 .227 .7070 .0045 .7295 .0045 .2064 .0047 .1969 .0040 19.514 .199 19.625 . .5722 .0042 .5750 .0053 .3657 .0036 .3627 .0051 NexusGS [CVPR25] + Ours 21.048 .049 21.348 .078 .7382 .0008 .7511 .0012 .1776 .0009 .1714 . 18.506 .098 18.736 .103 .5222 .0031 .5316 .0034 .3587 .0021 .3522 .0024 SE-GS [ICCV25] + Ours 20.725 .217 21.141 .223 .7203 .0049 .7403 . .1861 .0058 .1803 .0038 19.931 .288 20.135 .232 .5930 .0063 .5960 .0059 .3702 .0054 .3644 .0052 to adjust it for the Gaussian mean µi and scale si, so that Gaussians located farther from the camera are perturbed more strongly in 3D space. Then, the resulting value is multiplied by predefined neighborhood radius to determine the final perturbation. Therefore, instead of Eq. (4) Algorithm 1.4 becomes ˆθi θi + γi di ρθ θiL θiL2 . (5) As result, the perturbation magnitude is adaptive to the frequency of the corresponding Gaussian. This adaptation mitigates the first-order approximation error of sharpness at Gaussians in high-frequency regions and perturbs strongly in low-frequency regions. Consequently, estimation of sharpness is more appropriate, thus improving generalization. 3.3.3. Frequency-adaptive sharpness weighting Finally, we adapt the regularization weight according to the frequency of each Gaussian, where the frequency is defined the same as in Sec. 3.3.2. From Eq. (3), the final gradient is weighted combination of the gradient at the original point and the gradient at the perturbed point: ˆL ˆθi 1 2γi 1 γi θi + γi 1 γi ˆL. ˆθi (6) Here, γi 0.95γi/γmax, where γmax is the maximum candidate scale of the LoG kernel, and 0.95 is determined empirically. We then update the parameter at the original point using this weighted gradient (Algorithm 1.8). As result, the sharpness penalty is reduced in highfrequency regions, preserving fine details, while applying stronger penalty in low-frequency regions to facilitate better regularization. 4. Experiments Dataset. We evaluate our method on LLFF [40] and MipNeRF-360 [5], following previous works [45, 63, 66, 68], where the input resolution is 8 downsampled, and 3 and 12 input views are split for LLFF and MipNeRF-360, respectively. Implementation. We choose the publicly available 3DGS [23] and its follow-up works as baselines. Specifically, we choose the state-of-the-art NexusGS [66], which leverages foundation models, and CoR-GS [63], DropGaussian [45], and SE-GS [65], which do not. Metrics. We use PSNR, SSIM [55], and LPIPS [64] as evaluation metrics, where LPIPS is computed with VGG network [50]. Additionally, we use Average Error (AVGE) [44], which is the geometric mean of PSNR, SSIM, and LPIPS. Considering the randomness of 3DGS, we conduct ten runs on the LLFF dataset and five runs on the MipNeRF-360 dataset. 4.1. Reconstruction quality As shown in Tab. 1, our method achieves clear gains in all metrics, datasets, and baselines. Importantly, these improvements are achieved without any architectural changes or additional priors, but only with our optimization strategy. Fig. 4 show visual comparisons. Applying our method consistently improves the baselines by correcting geometric inaccuracies and reducing floating artifacts in novel viewpoints. These results reveal that our proposed optimization algorithm can be seamlessly integrated into current and future 3DGS-based frameworks, providing complementary enhancements. Figure 4. Qualitative comparison. Please zoom on the insets in red boxes to compare reconstruction quality. 4.2. Analysis 4.2.1. Ablation study Directly applying SAM [12] to 3DGS leads to degraded performance. This naive approach strongly perturbs Gaussians with large gradients, resulting in blurry reconstructions (Fig. 5, second column). Moreover, it increases the firstorder approximation error of sharpness estimation, hindering the optimization process of SAM (Tab. 2, fourth row). Separate sharpness per-Gaussian, handles each Gaussian separately. Thus, it mitigates this issue, leading to improvements in some metrics (Tab. 2, fifth row), but the results remain blurry (Fig. 5, third column). Frequency-adaptive sharpness weighting preserves sharpness in high-frequency details. Meanwhile, frequency-adaptive perturbation magnitude enables more faithful estimation by adaptively adjusting perturbation magnitude. Applying either component individually shows performance gains (Tab. 2, sixth and seventh rows). However, removing either component degrades performance (Fig. 5, fourth and fifth columns). Using both achieves better generalization, balancing between sharpness reduction and detail preservation (Fig. 5, sixth column; Tab. 2, eighth row). 4.2.2. Loss landscape visualization To analyze the convergence behavior of our method, we visualize the reconstruction loss landscape and the corresponding optimization trajectories. We first train 3DGS for 5k iterations. Subsequently, we continue training for an additional 5k iterations with the densification process disabled, under three distinct settings: 3DGS optimization, SAM, and our proposed method. For visualization, we project the high-dimensional parameter trajectories onto 2D plane using Principal Component Analysis with parameters from both trajectories. Figure 5. Ablation study. FAP and FAS denote frequency-adaptive perturbation magnitude and frequency-adaptive sharpness weighting, respectively. 3DGS, SAM, and w/o FAS & FAP produce inaccurate geometry (red box). All except Ours Full show blurry results (yellow box). Table 2. Ablation study. We report averaged results over ten runs. Standard deviations are omitted due to space constraints. SSG, FAP, and FAS denote separate sharpness per-Gaussian, frequency adaptive perturbation magnitude, and frequency adaptive sharpness weighting, respectively. Components LLFF (3 views) SAM SSG FAP FAS PSNR SSIM LPIPS 19. 20.198 20.390 20.570 20.560 20.783 .6790 .6958 .6977 .6980 . .7197 .2145 .2095 .2174 .2142 .2023 .1965 SAM converges to flatter minima than 3DGS. In Fig. 6a, the loss range between the local maximum and minimum is 2.37, and the measured sharpness λmax is 1.49 smaller. SAM also achieves test loss of 0.0129 lower than 3DGS, narrowing the generalization gap from 0.0985 to 0.0809. However, our method converges to less flat minima than SAM. As shown in Fig. 6b, the local loss range and measured sharpness of ours are 1.11 smaller and 1.01 smaller than 3DGS, respectively. Interestingly, our method achieves test loss of 0.0141 lower than 3DGS and further narrows the generalization gap to 0.0805, outperforming SAM in generalization. These results suggest that sharpness is not strictly correlated with generalization, supporting our hypothesis that sharpness of high-frequency pixel should be preserved. This aligns with our finding (Fig. 5, second column) that SAM tends to over-penalize high-frequency details, leading to blurry results. 4.2.3. Performance improvement by covisibility level The improvement from our method is more substantial in regions observed by fewer training views. Following CoMapGS [20], we compute covisibility maps using MASt3R [30]. As shown in Tab. 3, the improvement of our method over the baseline 3DGS increases progressively as the covisibility decreases. This behavior aligns with our Figure 6. Loss landscape visualization. We compare the convergence behaviors of 3DGS, SAM, and Ours. Because the visualization produces smoothed loss landscape, we provide zoomed-in view near the convergence points. We measure sharpness as the maximum eigenvalue λmax of the Hessian matrix [38, 57]. Table 3. Performance by covisibility level on LLFF dataset. Our method shows greater improvement with higher view sparsity. Covisibility level 3DGS AVGE + Ours Covisibility 3 Covisibility 2 Covisibility 1 .0483 .0093 .0757 .0156 .0948 .0256 .0417 .0080 .0644 .0137 .0806 . - .0066 .0043 - .0114 .0067 - .0142 .0074 hypothesis that our method enhances generalization, particularly in under-constrained regions. 4.3. Application 4.3.1. Reducing computation cost of FASR The main limitation of SAM and its follow-up work is that they compute the loss gradient twice at each step, theoretically doubling the training time. Fortunately, applying SAM on the last few training epochs improves performance similarly to the full application [67]. Based on this finding, we apply our method during the last 12.5% of the total iterations, denoted as Ours-L. As shown in Tab. 4, OursTable 4. Applying FASR at late training phase. For efficient training, we apply our method during the later iterations, denoted as Ours-L. We report metrics (and their changes relative to 3DGS) averaged over ten runs on an RTX A5000; standard deviations are omitted for brevity. Bold indicates the best performance, and underline indicates the second best. Method 3DGS LLFF (3 views) AVGE Time (sec) . 85.7 .0979 (-.0132) 3DGS + Ours 3DGS + Ours-L .1016 (-.0095) 237. (2.77) 98.2 (1.15) Table 5. Applying FASR to online dynamic 3D Gaussians. Our method is effective in dynamic scenarios under temporal sparsity, notably improving temporal consistency by reducing mTV. Figure 7. Qualitative comparison with FreeNeRF. Please zoom on the insets in red boxes to compare reconstruction quality. Table 6. Quantitative comparison with FreeNeRF. Our approach improves the NeRF baseline. Method LLFF (3 views) PSNR SSIM LPIPS FreeNeRF [CVPR23] + Ours 19.523 19. .6063 .6182 .3103 .2983 Method Neural 3D Video PSNR SSIM mTV 4.3.3. Applying our intuition to NeRF Yun et al. [SIGGRAPH25] + Ours 32.542 32.622 .9486 .9497 .1109 .0989 improves 3DGS by 0.0095 in AVGE, which are slightly (by 0.0037) less than full Ours. Moreover, compared to the baseline 3DGS, Ours increases the training time by 2.77, Ours-L increases it by only 1.15, making it more efficient. The performance degradation of Ours-L occurs because 3DGS adaptively adjusts its number of learnable parameters through densificationa key difference from neural networks. This makes early training iterations influential, leading different results from Zhou et al. [67]. 4.3.2. Improving generalization in temporal sparsity Additionally, we extend our method to dynamic scenes captured with multi-view cameras, the Neural 3D Video dataset [33]. Specifically, we conduct experiments in an online configuration [14, 17, 32, 59], where observations are spatially dense but temporally sparse, meaning that we can only access the current frame in sequentially processed video stream. We applied our method to Yun et al. [62] with 3DGStream [52] backbone. Following their protocol, we select the first frame 3D Gaussians with the highest PSNR for initialization and compute the masked total variation (mTV) to measure temporal consistency. Our method improves temporal consistency and visual quality compared to the baseline (Tab. 5). This shows that our approach enhances generalization not only in the spatial domain but also in the temporal domain. Furthermore, Yun et al. [62] claims that one cause of temporal jittering is the inevitable noise in training datasets. Since SAM shows robustness on noisy training data, our finding aligns with this explanation. Despite our method is not directly designed for implicit representations such as NeRF [41] because single parameter affects all pixels, we can alternatively apply our intuition to baselines that learn in coarse-to-fine procedure [36, 60]. Specifically, we set strong perturbation magnitude and regularization weight when the NeRF parameters learn lowfrequency components, and gradually decrease these values as the model learns high-frequency details. We applied this approach to FreeNeRF [60] and demonstrate that it improves both visual quality in novel view synthesis  (Fig. 7)  and quantitative results (Tab. 6). Although the improvement over the NeRF baseline is smaller than that in the 3DGS baselines due to discrepancy in the representation, these additional gains still support our intuition and versatility across different representations. 5. Conclusion In this work, we present the first fundamental investigation linking loss landscape and generalization to novel view synthesis, thereby improving quality of 3D Gaussian Splatting [23], especially in sparse view reconstruction. We propose Frequency-Adaptive Sharpness Regularization (FASR), an optimization algorithm that reformulates Sharpness-Aware Minimization (SAM) [12] in frequencyadaptive manner. This reformulation overcomes the limitation of SAM in reconstruction tasks, achieving generalization as well as fine detail reconstruction. FASR is easily applicable across diverse baselines and can be further extended to NeRF-based models and dynamic scenes in temporally sparse scenarios. We hope this work inspires the research community to explore the link between sharpness and generalization in reconstruction fields."
        },
        {
            "title": "References",
            "content": "[1] Luca Savant Aira, Diego Valsesia, and Enrico Magli. Modeling uncertainty for gaussian splatting. IEEE Transactions on Neural Networks and Learning Systems, 36(6):11657 11663, 2025. 3 [2] Maksym Andriushchenko and Nicolas Flammarion. Towards In Proceedunderstanding sharpness-aware minimization. ings of the 39th International Conference on Machine Learning, pages 639668. PMLR, 2022. 2 [3] Maksym Andriushchenko, Francesco Croce, Maximilian Muller, Matthias Hein, and Nicolas Flammarion. modern look at the relationship between sharpness and generalization. In Proceedings of the 40th International Conference on Machine Learning, pages 840902. PMLR, 2023. 3 [4] Maksym Andriushchenko, Francesco Croce, Maximilian Muller, Matthias Hein, and Nicolas Flammarion. modern look at the relationship between sharpness and generalization. In Proceedings of the 40th International Conference on Machine Learning, pages 840902. PMLR, 2023. 3 [5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of the anti-aliased neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54705479, 2022. 5 [6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth, 2023. [7] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. In Advances in Neural Information Processing Systems, pages 2240522418. Curran Associates, Inc., 2021. 2 [8] Dongrui Dai and Yuxiang Xing. Eap-gs: Efficient augmentation of pointcloud for 3d gaussian splatting in few-shot scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1649816507, 2025. 1, 2 [9] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster trainIn Proceedings of the IEEE/CVF Conference ing for free. on Computer Vision and Pattern Recognition (CVPR), pages 1288212891, 2022. 2 [10] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning, pages 10191028. PMLR, 2017. 3 [11] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI), 2017. 3 [12] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently In International Conference on improving generalization. Learning Representations, 2021. 2, 3, 4, 6, [13] Qiankun Gao, Jiarui Meng, Chengxiang Wen, Jie Chen, and Jian Zhang. Hicom: Hierarchical coherent motion for dyIn namic streamable scenes with 3d gaussian splatting. Advances in Neural Information Processing Systems, pages 8060980633. Curran Associates, Inc., 2024. 3 [14] Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, and Shalini De Mello. Queen: Quantized efficient encoding of dynamic gaussians for streaming In Advances in Neural Information free-viewpoint videos. Processing Systems, pages 4343543467. Curran Associates, Inc., 2024. 8 [15] Lily Goli, Cody Reading, Silvia Sellan, Alec Jacobson, and Andrea Tagliasacchi. Bayes rays: Uncertainty quanIn Proceedings of tification for neural radiance fields. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2006120070, 2024. 3 [16] Moritz Haas, Jin Xu, Volkan Cevher, and Leena Chennuru Vankadara. µp2: Effective sharpness aware minimizaIn Advances tion requires layerwise perturbation scaling. in Neural Information Processing Systems, pages 38888 38959. Curran Associates, Inc., 2024. 3 [17] Qiang Hu, Zihan Zheng, Houqiang Zhong, Sihua Fu, Li Song, Xiaoyun Zhang, Guangtao Zhai, and Yanfeng Wang. 4dgc: Rate-aware 4d gaussian compression for efficient In Proceedings of the streamable free-viewpoint video. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 875885, 2025. [18] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Proceedings of the 34th Annual Conference on Uncertainty in Artificial Intelligence (UAI), 2018. 2 [19] Cheongjae Jang, Sungyoon Lee, Frank Park, and Yung-Kyun Noh. reparametrization-invariant sharpness measure based on information geometry. Advances in neural information processing systems, 35:2789327905, 2022. 3 [20] Youngkyoon Jang and Eduardo Perez-Pellitero. Comapgs: Covisibility map-based gaussian splatting for sparse novel In Proceedings of the Computer Vision view synthesis. and Pattern Recognition Conference (CVPR), pages 26779 26788, 2025. 1, 2, 7 [21] Shubhendu Jena, Amine Ouasfi, Mae Younes, and Adnane Sparfels: Fast reconstruction from sparse Boukhayma. In Proceedings of the IEEE/CVF Inunposed imagery. ternational Conference on Computer Vision (ICCV), pages 2747627487, 2025. 2 [22] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2020. 2 [23] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4), 2023. 1, 2, 4, 5, [24] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On largebatch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2017. 2 [25] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Yang-Che Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3d gaussian splatIn Advances in Neural ting as markov chain monte carlo. Information Processing Systems, pages 8096580986. Curran Associates, Inc., 2024. 3 [26] Minyoung Kim, Da Li, Shell Hu, and Timothy Hospedales. Fisher SAM: Information geometry and sharpIn Proceedings of the 39th Inness aware minimisation. ternational Conference on Machine Learning, pages 11148 11161. PMLR, 2022. 3 [27] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. 4 [28] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In Proceedings of the 38th International Conference on Machine Learning, pages 59055914. PMLR, 2021. 3 [29] Sibaek Lee, Kyeongsu Kang, Seongbo Ha, and Hyeonwoo Yu. Bayesian nerf: Quantifying uncertainty with volume IEEE Robotics and Audensity for neural implicit fields. tomation Letters, 10(3):21442151, 2025. 3 [30] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r. In Computer Vision ECCV 2024, pages 7191, Cham, 2025. Springer Nature Switzerland. 2, 7 [31] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20775 20785, 2024. 2 [32] Lingzhi LI, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan. Streaming radiance fields for 3d video synthesis. In Advances in Neural Information Processing Systems, pages 1348513498. Curran Associates, Inc., 2022. 8 [33] Tianye Li, Mira Slavcheva, Michael Zollhofer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5521 5531, 2022. [34] Tao Li, Pan Zhou, Zhengbao He, Xinwen Cheng, and Xiaolin Huang. Friendly sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 56315640, 2024. 2 [35] Tony Lindeberg. Scale selection properties of generalized scale-space interest point detectors. Journal of Mathematical Imaging and Vision, 46(2):177210, 2013. 13 [36] Selena Ling, Merlin Nimier-David, Alec Jacobson, and Nicholas Sharp. Stochastic preconditioning for neural field optimization. ACM Trans. Graph., 44(4), 2025. 3, 8 [37] Yahao Liu, Qin Wang, Lixin Duan, and Wen Li. Balanced sharpness-aware minimization for imbalanced regression. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 62426251, 2025. 2 [38] Haocheng Luo, Tuan Truong, Tung Pham, Mehrtash Harandi, Dinh Phung, and Trung Le. Explicit eigenvalue regularization improves sharpness-aware minimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 7 [39] David A. McAllester. Some pac-bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, page 230234, New York, NY, USA, 1998. Association for Computing Machinery. 3 [40] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph., 38(4), 2019. 5 [41] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Computer Vision ECCV 2020, pages 405421, Cham, 2020. Springer International Publishing. 1, 2, 8 [42] Maximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that sharpnessaware minimization needs. In Advances in Neural Information Processing Systems, pages 6922869252. Curran Associates, Inc., 2023. [43] Y. Nesterov. method for solving the convex programming problem with convergence rate o(1/k2), 1983. 3 [44] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54805490, 2022. 1, 2, 5 [45] Hyunwoo Park, Gun Ryu, and Wonjun Kim. Dropgaussian: Structural regularization for sparse-view gaussian In Proceedings of the IEEE/CVF Conference on splatting. Computer Vision and Pattern Recognition (CVPR), pages 2160021609, 2025. 2, 5 [46] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1217912188, 2021. 2 [47] John Shawe-Taylor and Robert C. Williamson. pac analysis of bayesian estimator. In Proceedings of the Tenth Annual Conference on Computational Learning Theory, page 29, New York, NY, USA, 1997. Association for Computing Machinery. [48] Jianxiong Shen, Adria Ruiz, Antonio Agudo, and Francesc Moreno-Noguer. Stochastic neural radiance fields: Quantifying uncertainty in implicit 3d representations. In 2021 International Conference on 3D Vision (3DV), pages 972981, 2021. 3 [49] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15991610, 2023. 2, 13 of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 31853194, New York, NY, USA, 2023. Association for Computing Machinery. 3, 4 [62] Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, and Youngjung Uh. Compensating spatiotemporally inconsistent observations for online dynamic 3d gaussian splatting. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, New York, NY, USA, 2025. Association for Computing Machinery. 8, 13 [63] Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, and Xiao Bai. Cor-gs: Sparse-view 3d gaussian splatting via co-regularization. In Computer Vision ECCV 2024, pages 335352, Cham, 2025. Springer Nature Switzerland. 2, 5 [64] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 5 [65] Chen Zhao, Xuan Wang, Tong Zhang, Saqib Javed, and Mathieu Salzmann. Self-ensembling gaussian splatting for the few-shot novel view synthesis. IEEE/CVF International Conference on Computer Vision (ICCV), pages 49404950, 2025. 2, In Proceedings of [66] Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, and Yong Du. Nexusgs: Sparse view synthesis with epipolar depth priors in 3d gausIn Proceedings of the Computer Vision sian splatting. and Pattern Recognition Conference (CVPR), pages 26800 26809, 2025. 1, 2, 5 [67] Zhanpeng Zhou, Mingze Wang, Yuchen Mao, Bingrui Li, and Junchi Yan. Sharpness-aware minimization efficiently selects flatter minima late in training. In The Thirteenth International Conference on Learning Representations, 2025. 7, 8, 13 [68] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In Computer Vision ECCV 2024, pages 145163, Cham, 2025. Springer Nature Switzerland. 2, 5 [50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 5 [51] Hao Sun, Li Shen, Qihuang Zhong, Liang Ding, Shixiang Chen, Jingwei Sun, Jing Li, Guangzhong Sun, and Dacheng Tao. Adasam: Boosting sharpness-aware minimization with adaptive learning rate and momentum for training deep neural networks. Neural Networks, 169:506519, 2024. 2 [52] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic freeviewpoint videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2067520685, 2024. [53] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using pac-bayesian analysis. In International Conference on Machine Learning, pages 96369647. PMLR, 2020. 3 [54] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 90659076, 2023. 2 [55] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 5 [56] Kaiyue Wen, Zhiyuan Li, and Tengyu Ma. Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. Advances in Neural Information Processing Systems, 36:10241035, 2023. 3 [57] Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How sharpnessIn The aware minimization minimizes sharpness? Eleventh International Conference on Learning Representations, 2023. 7 [58] Yexing Xu, Longguang Wang, Minglin Chen, Sheng Ao, Li Li, and Yulan Guo. Dropoutgs: Dropping out gaussians for In Proceedings of the Combetter sparse-view rendering. puter Vision and Pattern Recognition Conference (CVPR), pages 701710, 2025. [59] Jinbo Yan, Rui Peng, Zhiyan Wang, Luyang Tang, Jiayu Yang, Jie Liang, Jiahao Wu, and Ronggang Wang. Instant gaussian stream: Fast and generalizable streaming of dyIn Pronamic scene reconstruction via gaussian splatting. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1652016531, 2025. 8 [60] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regIn Proceedings of the IEEE/CVF Conference ularization. on Computer Vision and Pattern Recognition (CVPR), pages 82548263, 2023. 2, 8 [61] Yun Yue, Jiadi Jiang, Zhiling Ye, Ning Gao, Yongchao Liu, and Ke Zhang. Sharpness-aware minimization revisited: Weighted sharpness as regularization term. In Proceedings Figure 8. Hyperparameter grid search, where each parameter is perturbed individually on the LLFF dataset. Plots are means and standard deviations over ten runs. We mark star at the best hyperparameter value. A. Hyperparameter search Selecting the optimal neighborhood of radius ρ, i.e., the perturbation magnitude, remains challenging problem in SAM-based methods. Although our method scales ρ for each Gaussian considering local image frequency, ρ itself remains core hyperparameter. Similar to SAM, we first grid search the optimal perturbation magnitude by perturbing each parameter individually, as shown in Fig. 8. However, the optimal ρ when perturbing all parameters simultaneously is typically smaller than the values found in this search because the parameters are not independent and mutually influence each other. Therefore, we select candidates including the individually found ρ and smaller values, and then perform grid search with simultaneous perturbation to determine the final hyperparameter. B. Contribution of each Gaussian arrtibute To analyze the contribution of each Gaussian attribute to the overall performance, we apply our method either exclusively to single attribute or to all attributes except one. Nevertheless, all attributes contribute positively to the performance, the Gaussian mean is the dominant contributor to the performance gain (Tab. 7). This finding suggests that instead of perturbing all attributes simultaneously, applying our method exclusively to the Gaussian mean could be simplified approach, significantly reducing hyperparameter search complexity while likely retaining substantial portion of the performance improvement. C. Comparison to random perturbation To support our claims in Sec. 2.3, we demonstrate the impact of applying random perturbations to Gaussian parameters during training. As shown in Fig. 9, random perturbaTable 7. Ablation study on the contribution of each Gaussian attributes. The Gaussian mean is the dominant contributor to the performance improvement. Method PSNR LLFF (3 views) SSIM LPIPS 3DGS + Ours 20.783 .300 .7197 . .1965 .0034 w/o mean w/o rotation w/o scale w/o opacity w/o SHDC w/o SHAC w/ mean w/ rotation w/ scale w/ opacity w/ SHDC w/ SHAC 20.053 .228 20.656 .241 20.641 .235 20.628 .218 20.594 .242 20.599 .205 20.561 .268 19.854 .168 20.036 .222 19.888 .188 19.990 .202 19.914 .186 .6903 .0051 .7175 .0046 .7171 .0049 .7175 .0044 .7152 .0052 .7169 . .7147 .0046 .6828 .0029 .6871 .0048 .6803 .0045 .6867 .0048 .6827 .0042 .2102 .0040 .1976 .0040 .1987 .0035 .1977 .0033 .1972 .0036 .1973 .0034 .1975 .0037 .2110 .0024 .2093 .0037 .2124 .0033 .2115 .0034 .2114 .0033 3DGS 19.810 .339 .6790 . .2145 .0065 tions often introduce unexpected artifacts and yield smaller performance gains than ours (Tab. 8). Furthermore, to demonstrate the importance of finding local maximum by adversarial perturbation, we compare our method with variant that randomly samples parameters within the neighborhood radius. As shown in Tab. 8, the improvement of random perturbation is smaller than ours, indicating that adversarially perturbing in the gradient direction is essential for effective sharpness regularization. Figure 9. Qualitative comparison of random perturbation and our method. Random perturbation often introduce unexpected artifacts. Method 3DGS w/ RP PSNR LLFF (3 views) SSIM LPIPS 19.810 .339 20.174 .207 .6790 .0078 .6987 .0040 .2145 .0065 .2009 . w/o AP 3DGS + Ours 20.263 .289 20.783 .300 .6990 .0031 .7197 .0032 .2056 .0027 .1965 .0034 Table 8. Quantitative comparison of random perturbation and adversarial perturbation. RP and AP denote random perturbation on Gaussian parameters and adversarial perturbation, respectively D. Analysis of late-phase application We provide detailed study on the application timing of our method. In Sec. 4.3.1, we follow Zhou et al. [67], applying our method only during the final 12.5% of the total training iterations. Extending this, we further analyze the impact of the starting point by varying the application duration in 10% increments of the total iterations. As shown in Fig. 10, fully applying our method yields the best performance, and the performance degrades as the application phase is later in the training process. E. LoG for local image frequency estimation. We calculate the local frequency at each pixel of the input image employing multi-scale analysis inspired by blob detection using the Laplacian of Gaussian (LoG) [35]. For the grayscale image, we calculate the absolute LoG response at each candidate scale. We capture the optimal scale at each pixel by finding the first significant increase in its response curve that exceeds the threshold. The optimal scale is the smallest one that precedes such rise; otherwise, we assign the maximum candidate scale. This process results in an optimal scale map Γv RHW at view v, with and denoting the height and width of the image, respectively. The local frequency is inversely proportional to this selected Figure 10. Ablation study on the application phase. We demonstrate the performance improvements obtained by varying the application duration in 10% increments of the total training iterations. scale. F. Implementation detail For all methods except 3DGS, we use the official repository. Since the official 3DGS does not target sparseview reconstruction, we refactor it by referring to CoRGS and DropGaussian. NexusGS does not provide optical flow for MipNeRF-360; we compute the flow using FlowFormer++ [49]. When the baseline utilizes regularization, we backpropagate it after computing the gradient of FASR. When applying our method in the deformation process, we compute the FASR gradient on the deformed Gaussians and propagate it back to the deformation network. We update the residual map of Yun et al. [62] during the ascent step and the Gaussians during the descent step. For each dataset, we use the same values of ρθ and γ for all baselines except NexusGS, where we set them to 0.1 the values used for other baselines due to its use of denser per-pixel Gaussians. G. More results We report the quantitative results of each scene in Tab. 9. Our method method outperforms the baseline in most cases. Table 9. Per-scene quantitative results on LLFF and MipNeRF-360 dataset. We report AVGE (lower is better) of each scene. Method 3DGS + Ours CoR-GS + Ours LLFF (3 views) fern flower fortress horns leaves orchids room trex .0950 .01790 .0765 .00080 .1147 .00180 .1066 . .0885 .00390 .0781 .00530 .1276 .00240 .1044 .00280 .1362 .00170 .1170 .00130 .1769 .00190 .1605 .00190 .0753 .00160 .0686 .00190 .0749 .00210 .0712 . .0806 .00080 .0724 .00090 .1088 .00290 .1014 .00220 .0754 .00160 .0683 .00190 .1205 .00120 .1075 .00200 .1427 .00310 .1326 .00240 .1735 .00200 .1603 . .0723 .00130 .0695 .00100 .0699 .00440 .0614 .00340 DropGaussian + Ours .0791 .00120 .0717 .00100 .1056 .00210 .1048 .00170 .0796 .00420 .0784 . .1186 .00270 .1101 .00340 .1284 .00190 .1176 .00140 .1650 .00160 .1536 .00170 .0715 .00150 .0701 .00200 .0687 .00190 .0657 .00260 NexusGS + Ours SE-GS + Ours Method 3DGS + Ours CoR-GS + Ours .0893 .00030 .0848 .00050 .0981 .00020 .0989 . .0498 .00030 .0475 .00090 .0909 .00050 .0897 .00070 .1072 .00030 .0993 .00060 .1383 .00060 .1354 .00040 .0755 .00060 .0696 .00030 .0810 .00050 .0741 . .0720 .00040 .0675 .00060 .1052 .00180 .0992 .00160 .0662 .00130 .0661 .00820 .1104 .00180 .1028 .00160 .1392 .00230 .1310 .00160 .1707 .01440 .1546 . .0635 .00220 .0629 .00120 .0596 .00180 .0564 .00080 bicycle bonsai counter MipNeRF-360 (12 views) garden kitchen room stump .1763 .00400 .1714 .00120 .1408 .00400 .1285 .00240 .1464 .00250 .1350 . .1302 .00190 .1240 .00130 .1052 .00160 .1017 .00160 .1078 .00110 .1021 .00460 .2347 .00280 .2253 .00640 .1734 .00340 .1733 .00170 .1262 .00250 .1131 . .1310 .00120 .1229 .00060 .1297 .00370 .1287 .00290 .1054 .00360 .1044 .00320 .0931 .00230 .0993 .00440 .2214 .00770 .2073 .00950 DropGaussian + Ours .1646 .00110 .1613 .00310 .1301 .00210 .1286 .00070 .1356 .00140 .1309 .00100 .1243 .00110 .1218 .00200 .0979 .00270 .1027 .00210 .1026 .00500 .1003 . .2187 .00410 .2171 .01180 NexusGS + Ours SE-GS + Ours .1818 .00100 .1739 .00100 .1308 .00190 .1278 .00060 .1563 .00130 .1531 . .1249 .00030 .1243 .00040 .0941 .00070 .0921 .00090 .1506 .00380 .1421 .00450 .2352 .00240 .2299 .00210 .1624 .00350 .1599 .00690 .1179 .00300 .1105 . .1251 .00370 .1217 .00170 .1207 .00160 .1179 .00250 .1107 .00370 .1092 .00350 .0875 .00380 .0973 .00090 .2014 .00830 .1941 ."
        }
    ],
    "affiliations": [
        "UNIST",
        "Yonsei University"
    ]
}