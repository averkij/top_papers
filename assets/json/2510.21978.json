{
    "paper_title": "Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models",
    "authors": [
        "Hoang Phan",
        "Xianjun Yang",
        "Kevin Yao",
        "Jingyu Zhang",
        "Shengjie Bi",
        "Xiaocheng Tang",
        "Madian Khabsa",
        "Lijuan Liu",
        "Deren Lei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 7 9 1 2 . 0 1 5 2 : r Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models Hoang Phan1,2,, Xianjun Yang1, Kevin Yao1, Jingyu Zhang1,3,, Shengjie Bi1, Xiaocheng Tang1, Madian Khabsa1, Lijuan Liu1, Deren Lei1 1Meta Superintelligence Labs, 2New York University, 3Johns Hopkins University Work done at Meta Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAPa replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards. Date: October 28, 2025 Correspondence: Hoang Phan hvp2011@nyu.edu, Deren Lei deren@meta.com."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable generalpurpose capabilities (Achiam et al., 2023; Yang et al., 2023), yet strengthening their proficiency in complex reasoning remains key frontier of research. Reinforcement Learning with Verifiable Rewards (RLVR) (Shao et al., 2024), an extension of Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022), has emerged as powerful paradigm for this purpose. By providing explicit reward signals such as exact-match correctness, format adherence or brevity in final answers, RLVR has been applied to instruction following, STEM problem solving, code generation and logical reasoning models (Lightman et al., 2023b; Peng et al., 2025), resulting in large performance gains on benchmark scores, leading to headlines that language models can learn to reason (Guo & DeepSeek-AI, 2025). Despite strong headline gains, RLVR exhibits recurring failure modes, prompting questions about whether current pipelines genuinely expand reasoning abilities (Shojaee et al., 2025). For example, exploration and diversity collapse occur when on-policy finetuning overly narrows the policy distributionraising Pass@1 but reducing Pass@k and solution-path diversity (Yue et al., 2025; Dang et al., 2025). Likewise, outcomeonly rewards introduce sparse credit assignment and instability, and not every task is naturally cast as reinforcement-learning problem (e.g., translation, summarization, or captioning). In addition, strict answer formats and format-sensitive graders may conflate genuine reasoning improvements with mere format compliance, even introducing evaluation artifacts (Petrov et al., 2025). Recent studies report that many RL-trained models even underperform the base model in standardized evaluation, where formatting-reward 1 Figure 1 General capabilities comparison of base VLMs (blue) and their reasoning-tuned variants (green/purple/red) on six representative, non-reasoning benchmarks (higher is better): A-OKVQA (knowledge-based VQA), AesBench (Huang et al., 2024b) (image aesthetics) , VStar (Wu & Xie, 2023) (spatio-temporal reasoning), VisOnly (Kamoi et al., 2025) (vision-only recognition aggregate) , OCRBench (Liu et al., 2024b) (text recognition), and R-Bench-Dis (Li et al., 2025b) (distribution-shift robustness). Across both Qwen2.5-VL families, reasoning-finetuned models generally underperform their base models on perception and robustness tasks, whereas MiMo-VL-7B-RL remains close to its SFT baseline. only baseline degrades the original performance more severely (Prabhudesai et al., 2025). The format reward model might be underoptimized or optimizing the format reward causes forgetting of math capabilities (Chandak et al., 2025). Another critical yet under-explored issue in RLVR is that optimizing for narrow set of targeted, verifiable rewards can lead to regression in general capabilities acquired during pretraining. Although models become proficient in following formatting requirements and solving reasoning tasks, they simultaneously exhibit increased hallucinations (Jaech et al., 2024; Yao et al., 2025b) and are more vulnerable to jailbreak attacks (Lou et al., 2025; Yao et al., 2025a). These results suggest that reasoning-oriented post-training can improve reasoning but at the cost of trading off non-target competencies (e.g., perception, safety, factual grounding), especially when prolonging the RL training without explicitly applying regularization (Liu et al., 2025a). To examine the forgetting issue of reasoning-focused finetuning, we begin by probing the general abilities of open-source reasoning models beyond their target reasoning tasks. As shown in Fig. 1, models finetuned for chain-of-thought or RL reasoning frequently lag behind their base counterparts on perception and robustness. For example, we observe the consistent drop on VisOnlyQA across both Qwen2.5-VL families, while MiMo-VL-7B-RL performs competitively against its base model on those non-reasoning tasks. We hypothesize this is due to their special finetuning strategy, where they employ mixed on-policy reinforcement learning that tries to maximize the model capacity on multiple axes beyond math and reasoning, according to the MiMo-VL-7B technical report (Team et al., 2025). However, the detailed framework and the sampling or reweighting are not disclosed. These patterns support our central claim: optimizing for reasoning rewards can erode non-reasoning capabilities, motivating continual learning method to preserve general skills during reasoning-oriented post-training. Our initial experiments with Qwen2.5-VL-7B show that training solely on reasoning rewards degrades performance on general capabilities, for example, by 7% on the perception task, according to Figure 2. To address this, we propose to gather general-capability 2 Figure 2 Performance comparison between the model finetuned solely on math data and with RECAP. Our model not only preserves the base model performance but also improves it (2%) while the reasoning model quickly falls behind the base model after 100 iterations. Figure 3 Overview of our RECAP. Along with the target reasoning task, we sample data from general domains in order to maintain that knowledge during finetuning. Initially, the objectives of interest are reweighted uniformly to optimize the main model. After few iterations, we record the convergence behavior of individual objectives. Based on this, we adjust the focus to prevent the dominance of any objectives and put less weight on saturated ones. data and integrate it into RLVR via an online weighting mechanism. However, due to cross-domain heterogeneity, it is nontrivial to decide how much weight to assign to each loss term or reward. We then measure how different reward signals evolve during RLVR training in Section 4 and find that some rewards converge up to three times faster than others, which should therefore not be emphasized later in training when the model is already able to obtain the corresponding skills. Motivated by the above observations, we propose Replay-Enhanced CApability Preservation (RECAP) - principled replay-based training strategy that mixes general data back into the RL objective, then dynamically reweights objectives based on their convergence rate and instability. As shown in Figure 3, RECAP computes the relative priority between objectives of interest by inspecting their convergence behaviors to reweight them in the final loss. Experiments demonstrate that our method not only preserves general capabilities but also improves reasoning performance by allowing flexible trade-offs among reward types. In summary, our contributions are depicted as follows: We systematically re-evaluate open-source reasoning models and show that reasoning-focused finetuning consistently regresses general capabilities. This motivates replaying general-capability data during RLVR to preserve pretraining knowledge. We further show that objectives exhibit distinct convergence behaviors, making commonly used, manually tuned reweighting schemes suboptimal in such scenarios. We introduce RECAP, plug-in scheduler that replays general-capability data during RLVR and dynamically reweights both general main task objectives. RECAP naturally down-weights saturated format signals and refocuses capacity on harder, high-variance objectives. The method is end-to-end, magnitude-agnostic, requires no auxiliary models, and drops into RLVR pipelines without overhead. In our experiments, RECAP preserves or improves general capabilities while matching or exceeding the reasoning performance of reasoning-only finetuning. It consistently outperforms strong continualfinetuning baselines and is competitive with specialized open models while utilizing smaller compute. Empirically, we also find that replaying general data yields shorter, more concise rationale while not compromising reasoning ability."
        },
        {
            "title": "2 Related work",
            "content": "Foundation models and post-training. Large transformer models pretrained on broad corpora serve as generalpurpose backbones with strong abilities and wide transfer across domains (Brown et al., 2020; Touvron et al., 2023). Post-training adapts these backbones to downstream tasks via (i) supervised finetuning, from early ULMFiT (Howard & Ruder, 2018) to instruction tuning in FLAN (Wei et al., 2021) or Flan-T5 (Chung et al., 2024); (ii) reinforcement learning from human or AI feedback, typically combining preference modeling with policy optimization; and (iii) direct preference optimization objectives that bypass explicit reward models. For reasoning, reinforcement learning with verifiable rewards has become common recipe: verifiers or rule-based checkers score final solutions in math and related domains, often within PPO-style pipelines (Guo & DeepSeek-AI, 2025; Liu et al., 2025b). Process-based neural reward models provide supervision for 3 intermediate progress (Setlur et al., 2025) rather than only the final output. However, PRMs can induce reward hacking: agents learn to exploit the appearance of correct process rather than achieving the intended outcome (Wang et al., 2025b; Shao et al., 2024). Catastrophic forgetting in continual and post-training. Catastrophic forgetting describes performance regressions on previously acquired skills when adapting to new data (McCloskey & Cohen, 1989; French, 1999). Early work in this vein introduced regularization-based mitigations such as Elastic Weight Consolidation (Kirkpatrick et al., 2017), Synaptic Intelligence (Zenke et al., 2017), and Memory-Aware Synapses (Aljundi et al., 2018) that prevent excessive change on important parameters. Functional approaches like Learning without Forgetting (Li & Hoiem, 2016) constrain outputs via distillation (Hinton et al., 2015), and replay via small episodic memories (Rebuffi et al., 2017; Lopez-Paz & Ranzato, 2017; Rolnick et al., 2019; Buzzega et al., 2020) are consistently strong baselines across settings. This problem is not unique to older neural networks, recent studies show that large language models also forget under sequential post-training. For instance, continual instruction tuning degrades domain knowledge, reasoning, and reading-comprehension performance unless replay or modularization is introduced (Luo et al., 2023; He et al., 2023; Wang et al., 2024; Huang et al., 2024a; Schen et al., 2024). Mechanistic analyses suggest that finetuning can shift implicit task inference, making models less likely to deploy pretraining-acquired skills outside the adapted distribution (Kotha et al., 2024). In RLHF post-training, KL penalty toward the reference policy is commonly used to stabilize updates and curb over-optimization (Ouyang et al., 2022). However, recent reasoning-focused RL pipelines sometimes reduce or remove KL to encourage exploration (Hu et al., 2025a; Hao et al., 2025), potentially exacerbating forgetting. Mitigations in post-training practice. Practical pipelines increasingly combine RL with regularization terms or replay knowledge to regularize the policy, preventing it from shifting too far from the original base model. Along with the standard KL-regularization approaches, InstructGPT Ouyang et al. (2022) interleaves the pretraining gradients with RLHF updates to reduce drift relative to KL-only regularization (Zheng et al., 2023). Concurrent works (Zhang et al., 2025; Fu et al., 2025) integrate verified rollouts to stabilize learning or penalize the discrepancy on augmented training data (Wang et al., 2025d). Yet, those methods do not guarantee the performance preservation on non-target domains. Other approaches tackle this by incorporating mixed, verifiable reward suites (Team et al., 2025) or introducing reflection or re-attention mechanisms under RL objectives (Chu et al., 2025)."
        },
        {
            "title": "3 Background",
            "content": "This section first provides the essential background on standard post-training for large language models, covering (i) supervised finetuning (SFT), (ii) RL-based alignment from preferences, (iii) reinforcement learning with verifiable rewards, and (iv) GRPO for long-form reasoning. Our approach builds on recent progress in reasoning-centric LLMsexemplified by DeepSeek-R1 (Guo & DeepSeek-AI, 2025) and other contemporary models (Ji et al., 2025; Yu et al., 2025; Chen et al., 2025a; Wang et al., 2025a). We adopt GRPO as our primary RL algorithm because it effectively reduces memory and compute overhead relative to standard PPO. Problem setup. Let an LLM with parameters θ induce conditional policy πθ( x) over responses to prompt x. Post-training adapts pretrained reference πref to target behaviors while constraining drift from the base model. Supervised finetuning (SFT). SFT optimizes the negative log-likelihood on instructionresponse pairs = {(x(i), y(i))}N : i=1 LSFT(θ) = (cid:88) i=1 log πθ (cid:0)y(i) x(i)(cid:1). SFT has been central to transferring general-purpose LMs to downstream instruction following and broad zero-shot generalization (e.g., ULMFiT (Howard & Ruder, 2018); instruction-tuned FLAN (Wei et al., 2021)), and it typically provides the initialization for subsequent preferenceor reward-based alignment. RL-based post-training. Reinforcement learning from human feedback (RLHF) fits reward model rϕ(x, y) from pairwise human preferences (Ziegler et al., 2019; Rafailov et al., 2023; Lambert, 2025), commonly using BradleyTerry likelihood (Bradley & Terry, 1952), and then maximizes reward while regularizing toward πref (often with KL penalty) via policy optimization such as PPO (Schulman et al., 2017): (cid:104) Exµ, yπθ(x) (cid:105) rϕ(x, y) (cid:104) β Exµ"
        },
        {
            "title": "DKL",
            "content": "(cid:0)πθ( x) πref( x)(cid:1)(cid:105) . max θ This pipeline improves helpfulness/harmlessness while retaining base-model competence; see early LMpreference work and InstructGPT for canonical formulations, and PPO for the underlying stable policy-gradient updates (Stiennon et al., 2020). Preference modeling itself is often grounded in the classical BradleyTerry model for pairwise comparisons (Rafailov et al., 2023). Reinforcement learning with verifiable rewards (RLVR). In settings with programmatic or automatic verifiers (e.g., exact-match answers, execution-based checks, or constraint checkers), RLVR replaces learned human-preference rewards with verifiable signals r(x, y) [0, 1]. This reduces labeler noise and can better target reasoning fidelity by rewarding demonstrably correct steps or outcomes, while retaining the same KL-regularized RL form (Lightman et al., 2023b). GRPO for long-form reasoning. Group Relative Policy Optimization (GRPO) is PPO-style algorithm tailored for LLM reasoning that forgoes learned value/critic and instead computes advantages from group-normalized sequence rewards. For each prompt x, sample group of rollouts = {oi}G from frozen rollout policy πθold . Let Ri be the verifiable sequence-level reward (e.g., exact-match correctness), and define the group-normalized advantage (cid:98)Ai = (cid:0)Ri mean(R)(cid:1)/std(R). With token-wise importance ratio i=1 GRPO maximizes the clipped surrogate plus KL regularization: ri,t(θ) = πθ(oi,t x, oi,<t) πθold (oi,t x, oi,<t) , JGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 min (cid:110) ri,t(θ) (cid:98)Ai, clip(cid:0)ri,t(θ), 1 ϵ, 1 + ϵ(cid:1) (cid:111) (cid:98)Ai β DKL (cid:0)πθ πref (cid:1) . By normalizing across group of responses per prompt, GRPO stabilizes updates without critic, which is preferable for long chain-of-thought rewards that are sparse and verifier-based. Empirically, GRPO has been shown to boost mathematical-reasoning performance in open models (Shao et al., 2024). General ability degradation. Unlike traditional continual learning studies, modern post-training pipelines must jointly consider gains in reasoning and retention of inherent general abilities (e.g., perception, grounding, instruction following, safety). Let = {G1, . . . , GM } denote suite of general-ability tasks and = {R1, . . . , RL} suite of reasoning datasets. We denote the performance on the i-th general task as SG(i) and on the j-th reasoning benchmark as SR(j). Earlier efforts (Ziegler et al., 2019; Nakano et al., 2021; Korbak et al., 2022) in mitigating this drift adopt KL penalties to let the policy model move toward higher rewards while penalizing large shifts from the pretrained model. However, this regularization term is imposed on the target domain data and does not guarantee retention of arbitrary non-target skills."
        },
        {
            "title": "4 Our Method: Replay-Enhanced CApability Preservation (RECAP)",
            "content": "We address forgetting in RLVR by (i) replaying general-capability data alongside reasoning data, and (ii) dynamically reweighting objectives online using the local estimation of progress and instability for individual objectives. Below, we present how RECAP governs loss coefficients and shifts the optimization away from saturated objectives toward underperforming or volatile oneswithout changing the underlying RL algorithm. 5 Setting. In the context of supervised learning, let = {Dn}N n,k(θ) the mini-batch loss of objective {1, . . . , K} on domain at iteration for parameters θ. Note that as there are some tasks using more than one reward or objective. The model parameter θ is thus optimized by minimizing the average loss across objectives: be domains and ℓ(t) n=1 Our framework acts on {L(t) supervised learning loss term. }K k=1 L(t) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) n=1 ℓ(t) n,k(θ). regardless of whether each Lk arises from an RL reward surrogate or Per-objective rate and the stability of convergence. Due to the unstable nature of RL training, we can not rely solely on the per-step objective/reward value to compute the reweighting coefficients. Instead, for each objective k, we measure the convergence rate over sliding window of length 2 by computing the current window average and the previous window average by: µ(t) ="
        },
        {
            "title": "1\nW",
            "content": "t (cid:88) L(s) , µ(t) = tW (cid:88) L(s) k"
        },
        {
            "title": "1\nW",
            "content": "s=tW +1 (cid:124) (cid:125) (cid:123)(cid:122) estimated current loss value (cid:124) s=t2W +1 (cid:123)(cid:122) estimated old loss value (cid:125) and the instability (coefficient of variation) in the same window: (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nW − 1",
            "content": "σ(t) = t+W (cid:88) (cid:0)L(s) µ(t) (cid:1) . s=tW +1 Based on these measures, we form two signals (i) the convergence rate c(t) > 1 indicates recent improvement (loss dropping relative to the previous window), while c(t) loss is improved, while (ii) the inverse signal-to-noise ratio i(t) Intuitively, c(t) signals saturation. The term i(t) Relative priority between domains. We convert these signals into normalized coefficients via temperaturecontrolled softmax. With the temperature > 0, we define the priority of t-objectives as s(t) and compute the coefficients λ for reweighting objectives. is larger when the objective is noisy/unstable. = σ(t) /(µ(t) = µ(t) + µ(t) captures how fast the /µ(t) ) captures loss instability. = c(t) s(t) + i(t) , λ(t) = exp(cid:0)s(t) i=1 exp(cid:0)s(t) (cid:80)K /T (cid:1) /T (cid:1) . (1) The prefactor preserves average scale so that 1 approaches uniform mixing. We set = 5 by default in our experiments. Overall training objective. At step t, we minimize the following weighted objective: λ(t) (cid:80) = 1. Lower sharpens priorities while higher L(t)(θ) ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 L(t) λ(t) . Optimizing θ with θL(t) steers learning towards objectives that are both slow-to-converge (high ck) and fluctuated (high ik), while leaving well-learned, stable objectives with lower weight. The scheme reduces to the standard equal weighting as (i.e., uniformly sample from all domains without loss reweighting). 6 Figure 4 Different rewards have different behaviors of convergence. While the format reward is easy to optimize and obtains the highest rate of convergence, it quickly saturates and thus yields low convergence rate (c 1) and instability (i 0) after 50 steps. By contrast, the reasoning accuracy fluctuates the most, thereby steering the optimization toward the corresponding objective. IoU and ntp indicate the IoU reward and the next-token-prediction accuracy during training. The result is obtained on the first setting in our experiment section below. Figure 4 illustrates the insight behind our proposed method, where we finetune the model on reasoning dataset (tracked by accuracy) while replaying perception data (IoU) and an SFT dataset for generality. Earlier in training, the format signal is easy to optimize and saturates quickly, so its falls toward 1 and toward 0, reducing its priority. After 100 steps, many signals plateau (convergence rate 1) while they still differ in the level of stability. Among them, the reasoning reward remains fluctuating the most ( 0.3, yielding higher and thus higher λ. At this point, the model has learned to answer according to the predefined template; thus the corresponding signal-to-noise ratio for formatting reward is 0. This motivates combining both progress (c) and instability (i) Equation 1 as they complement each other. Finetuning the trade-off between those two terms offers finer-grained control and potentially improves the performance. However, we simply take their unweighted sum = + for simplicity, which performs consistently well in our experiments."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we demonstrate how our proposed method can be used to replay general capabilities data during RLVR under two complementary experimental settings. First, we evaluate under the RLVR-Only Setting (Liang et al., 2025), which serves as our most comparable baseline where they mix diverse datasets in RLVR. Second, since MoDoMoDo enforces binary (01) rewards across all domains and thus cannot directly handle non-RL tasks, we further extend our evaluation to Hybrid Setting that integrates RL-style and supervised (non-RL) objectives, bringing it closer to the standard practices used in large-scale LLM training."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "To evaluate our proposed approach, we conduct our experiments using the two base models: Qwen2.5-VL-3B and Qwen2.5-VL-7B on two setups in different scales: 1. RLVR-Only Setting: This smaller setup follows the experimental configuration of Liang et al. (2025), focusing on domain-specific RLVR, which could serve as the upper-bound baseline of static data mixture approaches. The Qwen2.5-VL-3B model is trained until data from particular domain is exhausted. We train on 8 GPUs using data parallelism, with per-device batch size of 2 and 4 rollouts per prompt. We evaluate this setup on six reasoning benchmarks: SAT (Ray et al., 2024a), ScienceQA (Lu et al., 2022a), MathVista (Lu et al., 2023), ChartQA (Masry et al., 2022), InfoVQA (Mathew et al., 2022), and MMMU (Yue et al., 2023). 2. Hybrid Setting: To bridge RL and supervised training paradigms, we finetune Qwen2.5-VL-7B under larger mixed objective regime that combines RLVR with SFT-style training. Specifically, we use ThinkLite-VL-70k (Wang et al., 2025c) while jointly replaying perception-oriented datasets such as RefCOCO (Kazemzadeh et al., 2014) and LLaVA-OneVision OCR (Li et al., 2024). Each model is trained for 500 steps on 8 GPUs using data parallelism, with per-device batch size 1, 2 gradient accumulation 7 Table 1 Benchmark results in RLVR-only setting. We report the accuracy score over six benchmarks, in which the MoDoMoDo baseline is trained to maximize performance. Please note that in this benchmark only, we use the rule-based evaluator on the MathVista dataset instead of \"gpt-3.5-turbo\" to align with Liang et al. (2025). Model SAT ScienceQA MathVista (mini) ChartQA InfoVQA MMMU Open-source reasoning baselines VLAA-Thinker-3B MM-R1-MGT-PerceReason Ocean_R1_3B_Instruct Qwen2.5VL-3b-RLCS vision-grpo-qwen-2.5-vl-3b Qwen2.5-VL-3B-Instruct-GRPO-deepmath Qwen2.5-VL-3B and our variants Base model Uniform MoDoMoDo RECAP 49.38 50.83 59.49 24.12 50.57 34. 43.98 44.55 49.95 55.19 14.63 34.21 68.72 21.32 4.17 45.27 6.20 64.85 65.74 71.59 30.4 33.4 38.7 17.2 32.4 32. 23.6 32.4 32.2 33.2 45.84 44.88 54.00 3.32 67.80 70.24 43.88 69.68 70.40 70.40 30.81 61.42 38.02 10.86 58.29 49. 32.02 58.30 59.88 32.22 40.22 40.89 27.11 37.22 39.11 38.67 39.44 39.11 60.78 42.44 steps (effective batch size 16), and 4 rollouts per prompt (64 rollouts per optimizer step). For this larger hybrid setting, we evaluate our models on broad suite of widely used VLM benchmarks spanning general multimodal understanding, visual reasoning, math-in-vision, OCR, and accessibility: LISA (Lai et al., 2024), MMMU-Pro (Yue et al., 2024), AI2D (Kembhavi et al., 2016), MathVista (Lu et al., 2023), MathVision (Liu et al., 2024c), MathVerse (Zhang et al., 2024b), MMBench (Liu et al., 2024a), VizWiz (Gurari et al., 2018), and OCRBench v2 (Fu et al., 2024). For the smaller setup, we also follow (Liang et al., 2025) and evaluate the model on SAT (Ray et al., 2024a), ScienceQA (Lu et al., 2022a), MathVista (Lu et al., 2023), ChartQA (Masry et al., 2022), InfoVQA (Mathew et al., 2022), and MMMU (Yue et al., 2023). To isolate the effect of replay and dynamic reweighting and also for the ease of convenient implementation, we uniformly sample across data sources by default and reweight only the objectives of interest. Unless otherwise noted, we disable the reference-KL penalty to disentangle the effectiveness of regularization approaches (Li & Hoiem, 2017) and our replay mechanism. We also include the comparison against this approach in our list of established baselines for continual learning below: Reasoning only: We train solely on the target reasoning task with fixed reward weights (no replay). This is the most straightforward approach in continual learning. PropMix: General data is included during finetuning, with data across domains sampled in proportion to their source size. Losses are not reweighted. Uniform: Data are sampled uniformly across domains. Losses are not reweighted. Coreset: Replay size-limited subset of general data (half the reasoning-data volume in our setup) to align with standard coreset-style replay methods (Rebuffi et al., 2017; Chaudhry et al., 2019a). LwF: Data are sampled uniformly, and we set the KL regularization coefficient β to 0.01. We refer to this method as LwF, as it shares similar approach to Learning without Forgetting (Li & Hoiem, 2017). For context, we also include some representative open-source vision language models specializing in reasoning derived from the corresponding base models in each experiment. Please note that we list them here for easier benchmarking and we are not aiming to outperform them, as those models often undergo many complicated training pipelines. Models are evaluated with LMMS-Eval (Zhang et al., 2024a)."
        },
        {
            "title": "5.2 Experimental Results",
            "content": "According to Table 1, RL training on the reasoning domain helps improve the performance of the base model consistently on both reasoning and perception benchmarks. Especially on SCIENCEQA, RL lifts the performance of Qwen2.5-VL-3B from 6 to 60. On this benchmark, our proposed method even outperforms 8 comparative open-source reasoning models. We consider MoDoMoDo as the upper-bound approach of static data mixture approaches due to (i) MoDoMoDo has access to the target tasks performance during finetuning, which requires rerunning the experiments if new target tasks are introduced (ii) they train multiple proxy models of the same size as the baseline models to learn the test performance as function of mixing ratio, which is computationally expensive, especially in the context of reinforcement learning. Even after selecting an optimal mixture, the method still depends on hand-tuned reward weights (e.g., doubling accuracy and IoU relative to formatting rewards). Those trade-off coefficients are also set differently in prior work without clarification, which limits the generality. Table 2 Benchmark results in large hybrid setting. We report accuracy scores (higher is better) on nine perception and reasoning benchmarks. Rows above the break are open-source reasoning models with different backbones; the lower block compares variants finetuned from the same Qwen2.5-VL-7B base model. Bold = best; underline = second best within the Qwen2.5-VL-7B family. Model LISA MMMU-PRO AI2D MathVista MathVision MathVerse MMBench VizWiz OCRBenchv Open-source reasoning baselines 63.14 VLAA-Thinker-7B Vision-R1-7B 47.30 OpenVLThinker-7B 42.73 Qwen2.5-VL-7B and our variants Base model LwF PropMix Uniform Coreset Reasoning-only 65.13 65.08 66.80 65.18 64.82 57.58 RECAP 67.24 26.30 26.76 21. 25.55 29.59 31.39 31.91 31.91 33.87 34.15 75.45 0.00 59.94 67.62 73.93 75.32 76.43 79.92 74.97 78.21 63.90 61.80 59. 61.70 63.90 63.40 65.60 66.90 65.50 66.70 11.18 18.75 5.59 9.54 18.42 21.05 22.13 23.36 24.87 25.11 29.87 23.32 19. 26.29 33.98 34.75 36.07 37.58 40.74 40.83 75.95 69.46 71.53 71.82 73.11 73.54 75.34 78.09 77.84 78.52 47.57 53.12 52. 50.82 53.12 57.05 54.05 63.76 62.45 61.97 40.23 24.63 28.30 39.49 39.56 37.60 38.06 35.49 38.55 39.72 Table 2 reports the performance of models finetuned from Qwen2.5-VL-7B across different benchmarks in the hybrid setting, showcasing more general scenario than RLVR-only training. Overall, RECAP achieves the best or runner-up performance across different datasets, except for VizWiz where we still improve the base model performance by more than 10%. Compared with the base model, naively finetuning on the reasoning domain causes significant forgetting, especially on tasks not requiring extensive thinking. For example, finetuning on ThinkLite-VL-70k reduces the segmentation ability from 65.13 to 57.58 on LISA. Meanwhile, replaying the old data helps preserve the performance for all baselines on this dataset ( 64). Compared with uniform sampling, LwF achieves similar scene understanding performance while obtaining lower scores on the reasoning benchmark (e.g., 29.59 vs 31.91 on MMMU-PRO). Similar behaviors are observed in (Wang et al., 2025d; Hu et al., 2025a), where they remove this term for more plasticity. Among all baselines, our proposed method obtains the highest segmentation score, boosting the performance of the base model by more than 2%. This improvement highlights the impact of loss reweighting over the uniform baseline."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "We conduct an ablation study on the large hybrid setting by comparing the accuracy and formatting reward of our proposed method with the uniform baseline to isolate the effect of our reweighting. The uniform baseline employs identical hyperparameters, including data sampling and model training pipeline, yet only differs from ours in the loss reweighting mechanism (λk = 1/K). In Figure 5, we present the curves for formatting and accuracy reward during training. In the early phase, the baseline climbs format fasterconsistent with format being low-variance, easy-to-optimize signalyet crossover soon appears and our method surpasses it as training progresses (step 40). In contrast, for accuracy, our method opens growing lead over time (right). The behavior aligns with our scheduler: once the format objective shows fast convergence and low instability, its weight is down-regulated and capacity is reallocated to harder, higher-variance objectives (e.g., accuracy), avoiding over-optimization of formatting while improving task correctness. We also empirically found that using the same format reward for different domains is suboptimal. We start by examining the approach from Liang et al. (2025) by employing the same thinking reward on every domain, including scene understanding tasks. In Figure 6, we plot the response length on the segmentation task 9 Figure 5 Evolution of format and accuracy rewards on the reasoning domain during training: Training curves for the format reward over full training (left), an early-training zoom (middle), and the accuracy reward (right). While the uniform baseline is better in maximizing the format reward, it falls behind our proposed method later in terms of accuracy, as we prioritize correct solutions over formatting once the model can follow the predefined template. Curves are smoothed with an exponential moving average for readability. during training and find that the Qwen2.5-VL model rapidly trims its chain-of-thought and answers the question directly, later during training. This behavior suggests that explicit reasoning is unnecessary for such perception tasks and that encouraging long rationales can even be detrimental. We also include qualitative examples in the appendix to show how the model gradually suppresses its reasoning trace during training. Motivated by this observation, in our broader setting, we keep answer-format rewards for perception domains (no thinking) and reserve thinking rewards for tasks that truly benefit from step-by-step reasoning. Figure 6 Thinking length on segmentation task during finetuning. When uniform thinking reward is applied to all domains, the model quickly learns that long chains of thought are unnecessary for segmentation. The average response length drops from several hundred characters at the start of training to tens (often near zero) later on. Figure 7 Thinking length on reasoning task during finetuning. We compare model trained reasoning-only (blue) against our replay + dynamic reweighting method (orange). Highlighted curves show the running average thinking length per example, where our method generates only 27.3 words per question, compared with baseline ( 67.3). Figure 7 tracks the length of the generated thinking segment on the reasoning task throughout training. When trained only on reasoning data, the model maintains long chains of thought with high variability. In contrast, mixing general-capability replay with dynamic objective reweighting progressively reduces thinking length and stabilizes variance, converging to concise rationales (60% reduction, 67 27 words on average) while preserving accuracy. This shorter reasoning directly improves inference efficiencyfewer generated tokens reduce latency and compute costwithout sacrificing problem-solving quality."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we investigate the forgetting issues of recent reasoning vision language models and find that those models exhibit clear forgetting of general knowledge obtained during pretraining. Motivated by this, we 10 propose fix by replaying general data during finetuning and plug-in method to reweight objectives without the additional cost of training external models. On reasoning benchmarks, our proposed method not only preserves the general knowledge but also improves the target reasoning performance by properly reweighting the rewards of those tasks."
        },
        {
            "title": "Limitations",
            "content": "Our framework RECAP is generic and extends naturally beyond RLVR and SFT to preferenceand alignmentbased objectives (Rafailov et al., 2023; Garg et al., 2025; Hong et al., 2024; Ethayarajh, 2024), process reward models (Lightman et al., 2023a; Setlur et al., 2024). However, due to constraints on the training datasets available for this work, our empirical evaluation focuses on RLVR and standard SFT settings. We expect RECAP to yield similar gains over uniform or manually tuned baselines with any heterogeneous objective sets, but we leave comprehensive evaluation across non-RL objectives to future work. In practice, applying our scheduler to non-RL losses does not require an expensive coefficients search or per-objective normalization due to its magnitude-agnostic nature."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Riham Mansour and Misha Bilenko for their steadfast leadership and support. We would also like to thank the Meta legal and policy team for their review and approval of this research."
        },
        {
            "title": "References",
            "content": "HME100K: dataset for handwritten mathematical expressions. https://github.com/Phymond/HME100K, 2021. Accessed October 2025. Josh Achiam, Steven Adler, Sandhini Agarwal, and et al. GPT-4 technical report. arXiv:2303.08774, 2023. URL https://arxiv.org/abs/2303.08774. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, pp. 139154, 2018. doi: 10.1007/978-3-030-01219-9_9. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: strong, simple baseline. In NeurIPS, volume 33, pp. 1592015930, 2020. Nikhil Chandak, Shashwat Goel, and Ameya Prabhu. Incorrect baseline evaluations call into question recent llmrl claims. https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL -Claims-2012f1fbf0ee8094ab8ded1953c15a37?pvs=4, 2025. Notion Blog. Arslan Chaudhry, MarcAurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with A-GEM. In ICLR, 2019a. URL https://openreview.net/forum?id=Hkf2_sC5FX. Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Philip H. S. Torr, and MarcAurelio Ranzato. On tiny episodic memories in continual learning. arXiv:1902.10486, 2019b. Jiaze Chen et al. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. 2025a. doi: 10.48550/arXiv.2504.13914. URL https://arxiv.org/abs/2504.13914. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, et al. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410, 2025b. 11 Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, and Weiping Li. Qwen look again: Guiding vision-language reasoning models to re-attention visual information. arXiv:2505.23558, 2025. Introduces BRPO with reflection and visual re-attention to reduce hallucinations. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Xingyu Dang, Christina Baek, Zico Kolter, and Aditi Raghunathan. Assessing diversity collapse in reasoning. In Scaling Self-Improving Foundation Models without Human Supervision, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. K. Ethayarajh. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Robert M. French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4):128135, 1999. doi: 10.1016/S1364-6613(99)01294-2. Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and Xiang Bai. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. URL https://arxiv.org/abs/2501.00321. Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. Shivank Garg, Ayush Singh, Shweta Singh, and Paras Chopra. Ipo: Your language model is secretly preference classifier. arXiv preprint arXiv:2502.16182, 2025. Daya Guo and DeepSeek-AI. Deepseek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 2025. doi: 10.1038/s41586-025-09422-z. URL https://www.nature.com/articles/s41586-025-09422-z. Yihang Guo, Tianyuan Yu, Liang Bai, Yanming Guo, Yirun Ruan, William Li, and Weishi Zheng. Revisit the imbalance optimization in multi-task learning: An experimental analysis. arXiv preprint arXiv:2509.23915, 2025. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. URL https://openaccess.thecvf.com/content_cvpr_ 2018/papers/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.pdf. Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, and Jiawei Chen. Rethinking entropy interventions in rlvr: An entropy change perspective. arXiv preprint arXiv:2510.10150, 2025. Junjie He, Jierui Lin, Zihan Liu, et al. Continual instruction tuning for large multimodal models. arXiv:2311.16206, 2023. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv:1503.02531, 2015. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2024. Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025a. 12 Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, and Junnan Li. Beyondaha!: Toward systematic meta-abilities alignment in large reasoning models. arXiv preprint arXiv:2505.10554, 2025b. Jiarui Huang et al. Mitigating catastrophic forgetting in large language models via self-synthesized rehearsal. In ACL, pp. 13551370, 2024a. Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu, Pengfei Chen, Yuzhe Yang, Leida Li, and Weisi Lin. Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception. arXiv preprint arXiv:2401.08276, 2024b. URL https://arxiv.org/abs/2401.08276. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li. Am-thinking-v1: Advancing the frontier of reasoning at 32b scale. arXiv preprint arXiv:2505.08311, 2025. Ryo Kamoi, Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, and Rui Zhang. Visonlyqa: Large vision language models still struggle with visual perception of geometric information. In Conference on Language Modeling (COLM), 2025. URL https://arxiv.org/abs/2412.00947. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787798, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1086. URL https://aclanthology. org/D14-1086. Aniruddha Kembhavi, Michael Salvato, Bardia Fang, Alaaeldin El-Nouby, Ludovic Schmidt, et al. diagram is worth dozen images: Developing diagram taxonomy for diagram understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL https://arxiv.org/abs/1603.07396. Dataset page: https://prior.allenai.org/projects/diagram-understanding. James Kirkpatrick, Razvan Pascanu, and Neil et al. Rabinowitz. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):35213526, 2017. doi: 10.1073/pnas.1611835114. Tomasz Korbak, Ethan Perez, and Christopher Buckley. Rl with kl penalties is better viewed as bayesian inference. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 10831091, 2022. Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit task inference. ICLR 2024 (poster), arXiv:2312.02311, 2024. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. 2023. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. URL https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_LISA_Reasoning_ Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf. Nathan Lambert. Reinforcement learning from human feedback. arXiv preprint arXiv:2504.12501, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Bo Li, Kaichen Zhang, and Andrés Marafioti. Multimodal open r1. https://github.com/EvolvingLMMs-Lab/ open-r1-multimodal, 2025a. Accessed: 2025-02-08. Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiongkuo Min, Xiaohong Liu, Weisi Lin, et al. R-bench: Are your large multimodal model robust to real-world corruptions? IEEE Journal of Selected Topics in Signal Processing, 2025b. Zhizhong Li and Derek Hoiem. Learning without forgetting. In ECCV, pp. 614629, 2016. doi: 10.1007/ 978-3-319-46493-0_37. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):29352947, 2017. 13 Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, and Jiacheng Zhu. Modomodo: Multi-domain data mixtures for multimodal llm reinforcement learning, 2025. URL https://arxiv.org/abs/2505.24871. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step: Improving mathematical reasoning with process supervision. In OpenAI Technical Report, 2023a. Introduces PRM800K and shows advantages of process-supervised verifiers. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023b. Haotian Liu et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision (ECCV), 2024a. URL https://arxiv.org/abs/2307.06281. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024b. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Zifan Liu et al. Measuring multimodal mathematical reasoning with the math-vision dataset. In Advances in Neural Information Processing Systems (NeurIPS), 2024c. URL https://neurips.cc/virtual/2024/poster/97697. David Lopez-Paz and MarcAurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, pp. 64676476, 2017. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, and Kaiyu Huang. Think in safety: Unveiling and mitigating safety alignment collapse in multimodal large reasoning model. arXiv preprint arXiv:2505.06538, 2025. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022a. URL https://arxiv.org/abs/2209.09513. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. URL https://arxiv.org/abs/2310.02255. Yuchen Luo, Zhenhailong Yang, Rui Zhang, Xu Sun, Boyang He, and Yang Liu. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv:2308.08747, 2023. U.-V. Marti and H. Bunke. The IAM-database: an english sentence database for offline handwriting recognition. International Journal on Document Analysis and Recognition, 5(1):3946, 2002. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of ACL, 2022. URL https://aclanthology. org/2022.findings-acl.177.pdf. Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. Infographicvqa. In WACV, 2022. URL https://arxiv.org/abs/2104.12756. Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation, volume 24, pp. 109165. Academic Press, 1989. doi: 10.1016/ S0079-7421(08)60536-8. 14 Paulius Micikevicius, Sharan Narang, Jonah Alben, Greg Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations (ICLR), 2018. Ankush Mishra, Karteek Alahari, and C. V. Jawahar. Scene text recognition using higher order language priors. In British Machine Vision Conference (BMVC), 2012. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Long Ouyang, Jeff Wu, Xu Jiang, and et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL https://arxiv.org/abs/2203.02155. Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. Verif: Verification engineering for reinforcement learning in instruction following. arXiv preprint arXiv:2506.09942, 2025. Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024a. URL https://arxiv.org/abs/2412.07755. Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. Sat: Spatial aptitude training for multimodal language models. 2024b. Sylvestre Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. iCaRL: Incremental classifier and representation learning. In CVPR, pp. 55335542, 2017. doi: 10.1109/CVPR.2017.587. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. Experience replay for continual learning. In NeurIPS, pp. 350360, 2019. Zixuan Schen et al. Coin: benchmark of continual instruction tuning for large language models. In NeurIPS Datasets and Benchmarks Track, 2024. URL https://neurips.cc/virtual/2024/poster/97786. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=A6Y7AqlzLW. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity. arXiv preprint arXiv:2506.06941, 2025. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In European Conference on Computer Vision (ECCV), 2020. 15 A. Singh et al. TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. arXiv preprint arXiv:2105.05486, 2021. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025a. Shibo Wang and Przemyslaw Kanwar. Google Cloud Blog, bfloat16-the-secret-to-high-performance-on-cloud-tpus. 2019. URL Bfloat16: The secret to high performance on cloud tpus. https://cloud.google.com/blog/products/ai-machine-learning/ Teng Wang, Zhangyi Jiang, Zhenqi He, Shenyang Tong, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, and Hailei Gong. Towards hierarchical multi-step reward models for enhanced reasoning in large language models. arXiv preprint arXiv:2503.13551, 2025b. Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025c. Yizhe Wang et al. Inscl: data-efficient continual learning paradigm for instruction-tuned language models. In NAACL, pp. 584598, 2024. Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025d. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. URL https://vstar-seal.github.io/. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, and Yingchun Wang. mousetrap: Fooling large reasoning models for jailbreak with chain of iterative chaos. arXiv preprint arXiv:2502.15806, 2025a. Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are reasoning models more prone to hallucination? arXiv preprint arXiv:2505.23646, 2025b. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. 16 Xin Yue et al. Mmmu: massive multi-discipline multimodal understanding benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. URL https://arxiv.org/abs/2311.16502. Xin Yue et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. URL https://arxiv.org/abs/2409.02813. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https://openreview.net/forum?id=upehLVgq1b. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. Proceedings of the National Academy of Sciences, 114(13):35213526, 2017. doi: 10.1073/pnas.1611835114. Supplementary/alternate venue listings exist. Hongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, and Guorui Zhou. Rlep: Reinforcement learning with experience replay for llm reasoning. arXiv:2507.07451, 2025. Experience replay improves stability and final accuracy on AIME/AMC. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a. URL https://arxiv.org/abs/2407.12772. Rui Zhang et al. Mathverse: Does your multi-modal llm truly see the diagrams? arXiv preprint arXiv:2403.14624, 2024b. URL https://arxiv.org/abs/2403.14624. Ruxin Zheng, Yifei Li, Xiang Li, Chong Chen, et al. Secrets of rlhf in large language models part i: Ppo. arXiv:2307.04964, 2023. Analyzes PPO variants and shows PPO-ptx mitigates knowledge decline. Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, and et al. Fine-tuning language models from human preferences. arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593."
        },
        {
            "title": "A Experimental setup",
            "content": "In this section, we provide the detailed statistics of the training datasets used in our experiments, along with the implementation details. A.1 Data statistics and implementation details Table 3 reports the full statistics of the training corpora used in our experiments. For LLaVA-OneVision-OCR, we extract OCR-focused subsets from the official LLaVA-OneVision release (Li et al., 2024):IIIT5K (Mishra et al., 2012), HME100K (hme, 2021), IAM (Marti & Bunke, 2002), TextCaps (Sidorov et al., 2020), and TextOCR (Singh et al., 2021) alongside release-provided synthetic/curated subsets (rendered_text, k12_printing, chrome_writing). Those images are resized so that the longer side is 512 px while preserving aspect ratio to mitigate out-of-memory errors without altering task semantics. Table 3 Data statistics of each data source. We present the original volume of data (# samples). Answer Type Dataset 2D Bounding Box RefCOCO (Kazemzadeh et al., 2014) LLaVA-OneVision-OCR (Li et al., 2024) Natural Language Next Token Prediction ThinkLite-VL-70k (Wang et al., 2025c) Math Reasoning & Natural Image/Chart Understanding Natural Language Acc, Thinking Format IoU, Thinking Format LISA-train (Lai et al., 2023) 2D Bounding Box Multiple Choice GeoQAV (Li et al., 2025a) Acc, Thinking Format Natural Language Acc, Thinking Format SAT-train (Ray et al., 2024b) Acc, Thinking Format Multiple Choice ScienceQA-train (Lu et al., 2022b) Domain Referring Expression Comprehension Scene Text-Centric Visual Question Answering Referring Expression Math Visual Question Answering Spatial Visual Question Answering Science Visual Question Answering Rewards/Objectives IoU, Answer Format # samples 321327 66468 69997 1326 1969 15000 6218 We optimize with GRPO and SFT losses using AdamW (Loshchilov & Hutter, 2017) (β1=0.9, β2=0.999, ε=108). The learning rate follows linear schedule: 10% warmup to ηmax=1106, then linear decay to 0. All runs use bfloat16 precision (Wang & Kanwar, 2019; Micikevicius et al., 2018) and FlashAttention kernels (Dao et al., 2022) for memoryand throughput-efficient attention. We enable thinking mode on reasoning 17 tasks by enforcing structured traces (i.e., wrapping thoughts in <think>...</think>). which has been shown to improve reasoning and transparency (Hu et al., 2025b; Xie et al., 2025; Chen et al., 2025b). Our evaluation protocol closely follows LMMS-Eval (Zhang et al., 2024a) and VLMEvalKit (?)."
        },
        {
            "title": "Evaluation prompt",
            "content": "Non-Thinking: {Question} Output the in <answer> </answer> tags. Thinking: {Question} Output the thinking process in <think> </think> and final answer (option) in <answer> </answer> tags. 18 A.2 Prompts used in our experiments The prompt used for training our model is shown in Figure 8 Figure 8 Example conversation for finetuning We ask the model to produce its reasoning chain within <think> . . . </think> and the final answer in <answer>. . . </answer> tags. 19 A.3 Qualitative examples Responses from models trained with reasoning data only 9 and when including general data. Figure 9 Generated answer from reasoning-only finetuned model and our model While the reasoning-only finetuned model generates long reasoning chains for simple visual questions, our model produces more concise answers, especially on non-math tasks. 20 A.4 Evolution of rollouts during training with thinking reward Below, we show the generated responses from the model at different periods during training, where we can observe the finetuned model gradually shorten its thought on image understanding tasks to provide final answers directly. This motivates our usage of non-thinking rewards on those tasks that do not require reasoning instead of employing the same format reward for every task (Liang et al., 2025). Example 1: rollout # 0 Prompt: Please provide the bounding box coordinate of the region this sentence describes: flicking tail. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. The output answer format should be as follows: <answer>[x1, y1, x2, y2]</answer> Answer: To determine the bounding box coordinates for the zebra that is flicking its tail, we need to follow these steps: 1. Identify the zebra in the image that is flicking its tail. 2. Observe the position and direction of the zebras tail. 3. Define bounding box that encapsulates the entire zebra, keeping in mind that the tail should be included in the box. Upon inspection: - The zebra on the right is flicking its tail, as evident from the blurred tail motion and the position of the tail. - The bounding box should encompass the entire zebra, including its tail. <answer>[291, 41, 432, 315]</answer> Example 2: rollout # 500 Prompt: Please provide the bounding box coordinate of the region this sentence describes: second from right. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. The output answer format should be as follows: <answer>[x1, y1, x2, y2]</answer> Answer: <think>To determine the bounding box for the second from right, first identify the rightmost object, which is the one on the far right, identified by its position and bounding box. The second from right is the one to the left of the rightmost object. By visual inspection, the bounding box for the second from right should start after the farthest object and extend leftward.</think> <answer>[237, 72, 374, 328]</answer> Example 3: rollout # Prompt: Please provide the bounding box coordinate of the region this sentence describes: boat blocked by cow stomach. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. The output answer format should be as follows: <answer>[x1, y1, x2, y2]</answer> Answer: <think> The sentence refers to the boat that is partially covered by the cows stomach. This boat seems to be the one with the green and white colors.</think> <answer>[327,21,631,280]</answer> 21 Example 4: rollout # 5000 Prompt: Please provide the bounding box coordinate of the region this sentence describes: guy on left with backpack on. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. The output answer format should be as follows: <answer>[x1, y1, x2, y2]</answer> Answer: <think>[114,53,236,378]</think> <answer>[114,53,236,378]</answer> A.5 Reward values at the end of training Similar to what we observe in the main paper, Figure 10 showcases the final performance of our model and the uniform baseline. Results show near-parity on thinking formatting (< think >< /think > < answer > < /answer >) and direct answer reward < answer > < /answer >) but consistent improvements on reasoning score, IoU and mean token accuracy (+2.01, +1.11, and +1.40 points respectively). This aligns with our design goal: once format signals saturate, down-weight them and shift capacity to harder, higher-variance objectives, improving accuracy while maintaining output format. Figure 10 Final performance across metrics. We compare uniform baseline with our dynamic reweighting. The gains on correctness-oriented metrics indicate that reallocating weight away from saturated format rewards toward harder objectives yields better solutions without sacrificing adherence to templates. A.6 Reward dynamics during training Given the multi-objective nature of the problem, one might apply existing methods in multi-task learning literature (Guo et al., 2025) for reweighting different objectives and rewards. In practice, this is difficult for two reasons. First, computing per-objective gradients is prohibitively expensive at LLM scale, especially under reinforcement learning. Second, on-policy RL signals are high-variance and non-stationary (Henderson et al., 2018), making per-iteration statistics unreliable indicators of task progress. As shown in Figure 11, all rewards fluctuate substantially within their [0, 1] range, with the standard deviation of the total reward peaks near 0.9 around step 20. Thus, we propose method that utilizes sliding window, which can provide more robust proxy to understand convergence behaviors. 22 Figure 11 Reward dynamics and variability during RLVR training. Per-step rewards (light traces) and sliding-window means (dark curves) for six metrics: Next-Token Prediction, Answer-Format, Thinking-Format, IoU, Accuracy, and the Total-Reward Standard Deviation (lower-right). Asynchronous convergence and high variances motivate short-horizon statistics for dynamic objective reweighting rather than per-iteration magnitudes."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "Meta Superintelligence Labs",
        "New York University"
    ]
}