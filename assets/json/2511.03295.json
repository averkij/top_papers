{
    "paper_title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics",
    "authors": [
        "Mauro Cettolo",
        "Marco Gaido",
        "Matteo Negri",
        "Sara Papi",
        "Luisa Bentivogli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 5 9 2 3 0 . 1 1 5 2 : r How to Evaluate Speech Translation with Source-Aware Neural MT Metrics Mauro Cettolo, Marco Gaido, Matteo Negri, Sara Papi, Luisa Bentivogli Fondazione Bruno Kessler cettolo@fbk.eu mgaido@fbk.eu negri@fbk.eu spapi@fbk.eu bentivo@fbk.eu Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation. 1. Introduction Translating between natural languages is complex task even for humans, due to intrinsic linguistic challenges such as ambiguity and polysemy, dependence on context, structural differences across languages (e.g., Subject-Verb-Object vs. Verb-ObjectSubject orders), the presence of idiomatic and figurative expressions, and the influence of pragmatics (Jurafsky and Martin 2025). Despite these intrinsic difficulties, human Corresponding author translation can be assumed to be error-free, especially for professionals, while the same cannot be said for translation performed automatically by machines, neither from text to text (machine translation, MT) nor from speech to text (ST). Therefore, for automatic translation, an additional challenge arises, that of evaluating its quality. The evaluation of the quality of translations generated by machines can be performed either manually or automatically. Manual evaluation involves the annotation of systems outputs by human professionals. While it is considered the most reliable option, it is rarely employed due to its large cost and the consequent infeasibility of performing it at scale (Freitag et al. 2021). For this reason, research advancements mostly rely on automatic metrics (Marie, Fujita, and Rubino 2021), the focus of this work. Traditionally, automatic MT metrics rely on comparing system output against one or more human reference translations assumed to represent the correct rendering of the source sentence to translate. This is the case of BLEU (Papineni et al. 2002), the most widespread MT metric in the scientific community over the last two decades (Mathur, Baldwin, and Cohn 2020), which computes n-gram overlaps between the system output (hypothesis) and the reference(s). However, following evidence of the mismatch between rankings produced by BLEU and by human evaluations (Bojar et al. 2018; Barrault et al. 2019), the community has undertaken efforts to build more reliable metrics. While it has not yet been possible to define perfect holistic metric, as demonstrated by the fact that specific shared tasks on evaluation metrics are still annually organized,1 recent years have seen the rise of neural metrics that do not rely solely on the similarity between hypotheses and references, but also take the source text into account. The first and most widespread metric of this type is COMET (Rei et al. 2020), which demonstrated its effectiveness from its first participation in the MT Metrics shared task (Mathur et al. 2020) within the WMT 2020 conference. Since then, other source-aware metrics have been proposed showing steady advancements (Freitag et al. 2023, 2024). In the related field of ST, the community has traditionally relied on the same metrics, evaluation procedures, and insights coming from MT. In particular, reference-based evaluation remains the same across the two fields, as the only difference is the modality of the source, which, in ST, is audio rather than text. However, the input dissimilarity means that the more recent and reliable neural source-aware MT metrics cannot be employed in ST because they require textual source, not audio. As solution, two approaches are possible: (i) designing novel multimodal metrics capable of directly exploiting the audio, or (ii) relying on textual proxy for the source speech. In this paper, we focus on the second approach, which has recently been adopted also in the annual evaluation campaign organized by the International Conference on Spoken Language Translation (IWSLT) (Abdulmumin et al. 2025). In the last few years, IWSLT evaluation has indeed shifted from source-independent string-based metrics like BLEU to the source-aware neural metrics like COMET as the official ranking criterion. In this shift, however, the organizers have not explicitly disclosed which source was used to feed COMET, nor have they provided any evidence supporting the reliability of their choice, leaving room for dedicated research to support the choice of best practices. To this aim, given the proven superiority of source-aware metrics, it is important to understand whether automatic source generation is viable option when manual transcripts are 1 Since 2008, the conference on Machine Translation (WMT) has organized shared task on MT automatic evaluation metrics. Over the years, it has become the reference on the topic and annually attracts large part of the scientific community working on it. The link to the 2025 edition is: https://www2.statmt.org/wmt25/mteval-subtask.html Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST not available, typical real-world condition which remains underexplored. To fill this gap, the first research question we address in this paper is (RQ1): Can we automatically derive the source text corresponding to the audio without compromising the reliability of the source-aware metrics? To answer RQ1, we study two alternative ways to generate synthetic textual source: the automatic transcription of the audio with an automatic speech recognition (ASR) system and the back-translation (BT) of the reference translation with an MT system. This leads to our second research question (RQ2): Which of the two is the best method to automatically generate synthetic textual sources for ST evaluation? If neither method proves consistently superior, what factors should guide the choice between them in given setting? We answer RQ2 in two different scenarios. First, we study the simpler case, in which the reference translation is paired with the corresponding segment of source audio. This is typical in controlled evaluation settings based on segment-level assessments. Then, we move to the more challenging setting, in which the ST benchmark is made of long audio recordings with corresponding document-level reference translations, without sentence-level audio-textual alignments. In this second scenario, the ASR-based solution requires cross-lingual re-segmentation to pair the generated transcripts with the reference translations. This problem constitutes our last research question (RQ3): When the synthetic source text is not aligned with the reference text, can we re-align it without affecting the quality of the resulting evaluation? To answer RQ3, we propose novel two-stage, cross-lingual algorithm, XLR-Segmenter,2 which we validate not only against the back-translation alternative but also against the easier scenario in which the audio segmentation is given. Our investigation and evaluation of the proposed solutions is carried out in rich experimental setup that leverages ST benchmarks in which human-labelled transcripts and sentence-level alignments are available. To ensure the soundness and robustness of our findings, we consider: (cid:114) (cid:114) (cid:114) Two ST benchmarks covering 79 language pairs from different language families, for total of over 120,000 sentences; Two representative source-aware evaluation metrics (COMET and MetricX), selected based on recent studies showing their high correlation with human judgments; Six ST systems, evenly divided between cascaded and direct architectures, spanning wide performance range. Our empirical results highlight that both synthetic sources serve as effective textual proxies for input audio, enabling the application of source-aware metrics in ST (RQ1). At the same time, we also observe the superior reliability of automatic transcription over back translation, provided that its word error rate remains below 20% (RQ2). Finally, the proposed cross-lingual re-segmentation algorithm proves to support reliable evaluation, yielding only negligible degradation in the absence of audio-text alignments (RQ3). The paper starts by providing an overview of the recent literature that defines the scope of our investigation (Section 2). It then describes the solutions we propose 2 XLR-Segmenter, licensed under Apache Version 2.0, is available on GitHub (https://github.com/hlt-mt/source-resegmenter) and on PyPi (pip install source_resegmenter). 3 (Section 3) and provides the experimental setup (Section 4). The experiments (Section 5) are divided into four blocks. The first two blocks (Sections 5.1 and 5.2), validate and compare the synthetic sources under controlled conditions, where source audio and reference translations are manually aligned, to observe their performance without external influence. The third block (Section 5.3) assesses the source re-segmentation algorithm under controlled conditions as well, using reference transcripts instead of automatically generated ones. Finally, in the fourth block (Section 5.4), experiments are conducted in the most realistic scenario possible, to validate all the proposed solutions \"in the wild\". Each block is organized by first providing the experimental outline, then presenting the results, and finally listing key observations and takeaways. thorough discussion (Section 6) and the examination of the limitations of the work (Section 7) conclude the paper. 2. Related Works 2.1 Metrics for Machine Translation The automatic evaluation of MT outputs has been central topic of research since the early days of statistical approaches (Brown et al. 1990). The first evaluation strategies relied on surface-based comparisons between system outputs and human references, with metrics such as BLEU (Papineni et al. 2002), TER (Snover et al. 2006), and chrF (Popovic 2015). While computationally efficient, early n-gramor characterbased metrics often failed to capture semantic adequacy or tolerate syntactic variation. To address these shortcomings, improved solutions such as METEOR (Banerjee and Lavie 2005) incorporated synonym matching and paraphrasing, moving beyond strict n-gram overlap. More recently, the advent of neural metrics has further transformed MT evaluation by modeling semantic similarity directly through contextual embeddings and learned representations. Leveraging pretrained language models, approaches such as BERTScore (Zhang et al. 2020), BLEURT (Sellam, Das, and Parikh 2020), COMET (Rei et al. 2020), and MetricX (Juraska et al. 2023) have demonstrated substantially stronger correlations with human judgments. This dominance of learned metrics over traditional surface-level scores has been repeatedly confirmed in recent WMT General MT Shared Tasks (Freitag et al. 2022, 2023). Since 2008, WMT has hosted dedicated Metrics Shared Task (Callison-Burch et al. 2008), providing systematic comparisons of evaluation metrics across systems, languages, and evaluation conditions (Koehn et al. 2022, 2023; Haddow et al. 2024). Despite the current dominance of COMET in system ranking, meta-evaluation studies (Mathur, Baldwin, and Cohn 2020; Freitag et al. 2021; Moghe et al. 2025) have highlighted the limitations of relying on single metric, including problems in reproducing and comparing scores across works (Zouhar et al. 2024), echoing earlier critiques of BLEU (Callison-Burch, Osborne, and Koehn 2006; Post 2018). Overall, this body of work underscores both the progress achieved in MT evaluation and the persistent challenges in designing metrics that are reliable, robust, and broadly generalizable. 2.2 Metrics for Speech Translation In contrast to MT, evaluation in ST has received comparatively limited attention. While IWSLT has provided the primary benchmark for ST since its first edition in 2004 (Akiba et al. 2004), its focus has largely been on system comparison rather than the development of evaluation metrics, which are substantially inherited as-is from MT. ST, how4 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST ever, introduces two distinctive challenges for automatic evaluation: i) segmentation mismatches between speech and text, which complicate direct comparison (Papi et al. 2021; Fukuda, Sudoh, and Nakamura 2022), and ii) the possible absence of gold source transcripts (Cheng, Lee, and Wang 2021; Fang and Feng 2023), which undermines the applicability of source-aware MT metrics. Although few quality estimation metrics, both sourceand reference-free, could be employed (e.g. GEMBA-MQM (Kocmi and Federmann 2023)), they typically rely on large language models and/or API-based services. Such dependence not only entails substantial computational and monetary cost but also raises concerns regarding accessibility, replicability, and long-term stability, as closed-source models can evolve over time and their internal behavior remains opaque. Therefore, they continue to be relatively underused (Larionov and Eger 2025). Among ST-specific metrics, BLASER 2.0 (Dale and Costa-jussà 2024) is, to our knowledge, the only metric capable of source-based evaluation directly from speech. It operates by projecting source audio, reference text, and hypothesis text into shared multilingual embedding space to assess translation quality. However, Han, Duh, and Carpuat (2024) report that source-aware metrics, such as XCOMET and MetricX, often achieve stronger correlation with human judgments than the BLASER 2.0 end-to-end metric. This suggests that, despite the conceptual advantage of accessing the speech source directly, the current performance of end-to-end speech-based metrics is not yet sufficient to surpass metrics operating on high-quality transcripts. Within IWSLT evaluation campaigns, recent editions have examined the robustness of standard MT metrics for ST under segmentation variations and proposed resegmentation protocols for human evaluation (Sperber et al. 2024). In similar vein, (Post and Hoang 2025) analyzed the effect of automatic alignment tools on MT metrics, reporting only minor impacts of segmentation strategies on COMET-based system rankings. However, in contrast to the ongoing efforts in MT, no comparable systematic study or shared task initiative has been conducted in ST, leaving the problem of source-reference misalignments underexplored in the ST domain. Our work addresses these gaps. Building on advances in MT evaluation, we analyze source-aware metrics in the context of ST, where the speech modality challenges the availability of gold source text and its alignment with reference translations. By systematically assessing their reliability and applicability, we aim to shed light on which evaluation and re-segmentation strategies are most appropriate for ST, in scenarios where the availability of transcripts, or their alignment, cannot be assumed. 3. Methodology In this section, we address the two challenges that must be overcome to reliably deploy source-aware metrics in the ST setting: the possible absence of source text (Section 3.1) and the potential segmentation mismatch between the source and the reference translation (Section 3.2). 3.1 Synthetic Source Generation Our primary goal is to validate the effectiveness of source-aware MT metrics also for ST. Often, however, ST benchmarks do not provide reference transcripts. To address this limitation, synthetic source text can be created either by automatically transcribing the input audio (ASR) or by back-translating the reference translation into the source language (BT). Both methods present distinct advantages and drawbacks, which vary depending on the specific aspect under consideration, as discussed below: 5 Coverage and Quality. Although there are many ASR models available, even more than MT ones (e.g., on Huggingface3), their coverage in terms of languages is lower. For example, the only multilingual ASR models supporting more than few dozen high-resource languages are Whisper (Radford et al. 2023), SeamlessM4T (Seamless Communication et al. 2023), XLS-R (Babu et al. 2022), and OWSM (Peng et al. 2023). Their performance, however, is not always excellent across different languages and conditions. On the contrary, there are several multilingual MT models that cover hundreds of languages and thousands of high-performance models specialized in one or few language pairs. From the first group, we can mention: Madlad-400 (Kudugunta et al. 2023), NLLB (Costa-jussà et al. 2022), mBART-50 (Tang et al. 2020), M2M100 (Fan et al. 2021), DeltaLM (Ma et al. 2021), and SeamlessM4T (Seamless Communication et al. 2023). From the second one: the Helsinki-NLP/opus-mt family (Tiedemann et al. 2023), which altogether covers hundreds of languages and thousands of language pairs, and IndicTrans2 (Gala et al. 2023), which supports 22 Indic languages. While the comparison between ASR and MT performance must be interpreted with caution, existing studies (see Appendix A) consistently show that MT systems typically achieve higher overall quality than ASR models. Neutrality with respect to third-party system evaluation. When evaluating ST systems, using ASR-derived transcripts may introduce bias if the evaluated ST system shares components or training data with the ASR model used to generate the synthetic source. The BT approach avoids this specific dependency, or at least only residual bias might still occur when the same MT model is used, in opposite directions, for both back-translation and in the evaluated system. This issue is discussed in Section 5.1. Similarity to the missing reference transcript. ASR outputs are generally closer to the gold transcripts, particularly in terms of lexicon, syntax, and adherence to the spoken content. BT outputs, instead, may differ more substantially, as they are influenced by the lexical and syntactic choices made in the reference translation, potentially introducing artifacts not present in the original audio. Empirical evidence for this claim comes from multiple results showing ASRs to be more reliable than BTs (see Section 5.1). Alignment with the segmentation of the reference translations. For evaluation purposes, the synthetic source needs to be aligned, segment by segment, to the reference translation. The BT ensures alignment by construction. On the contrary, ASR-based synthetic sources have to be (automatically) re-segmented, thus introducing additional processing costs (see next item) and errors that this step typically commits, as discussed in Section 5.4. Cost. The ASR approach requires speech recognition system and often resegmenter, which may introduce significant additional computational costs. The BT approach, instead, requires only machine translation system, typically less demanding in terms of computational resources. Appendix reports on specific experiments performed to compare the two approaches from this perspective. In light of the above, the choice between ASR and BT for generating synthetic source text depends on the specific constraints of the evaluation scenario, including computational budget, the availability of effective ASR/BT models, the need for system neutrality, the desired fidelity to the original spoken input, and the ability to effectively perform the automatic re-segmentation, if needed. All these aspects are explored through the experimental results presented in Section 5 and Appendices B, D, and E, while the final discussion in Section 6 is organized around them. 3 https://huggingface.co/models 6 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Figure 1 Scheme of XL-Segmenter and XLR-Segmenter. 3.2 Source Re-Segmentation In realistic settings, automatic translations of input speech are generated from automatically segmented audio. In such scenario, segment-level evaluation requires aligning the hypothesis against the gold reference text. To re-segment the translation hypothesis so that its segments match those of the reference translation, the most commonly adopted method is the one proposed in (Matusov et al. 2005).4 For the sake of simplicity, we name it L-Segmenter, where the highlights its intra-lingual nature: both the generated hypothesis to be re-segmented and the reference text are in the same language (the target one, in this case). The method5 re-segments the hypothesis so that its Levenshtein distance from the reference is minimized. The optimal segmentation is searched through dynamic programming. Although it is widely adopted by the scientific community, the L-Segmenter algorithm is affected by an intrinsic limitation regarding unaligned boundary words (Post and Hoang 2025). The upper table of Figure D.1 in Appendix shows an example of L-Segmenter output, highlighting this problem. The application of source-aware ST metrics extends the segmentation problem to include the source text, which, similar to the translation hypothesis, has to be aligned with the gold reference translation. However, when the source text is not available and needs to be generated with an ASR model, cross-lingual approach is required to align synthetic transcripts in the source language with target language translations. possible solution to this problem, out of the reach of the monolingual L-Segmenter, is shown in Figure 1. The target text is first translated into the source language, segment by segment, by means of an MT model (back-translation). The source text produced by the ASR model is then aligned by means of L-Segmenter to the back-translation of the target, thus indirectly achieving the cross-lingual alignment. In the figure, we name this segmentation procedure as XL-Segmenter, where emphasizes its cross-lingual nature. However, as mentioned above, L-Segmenter may misplace unaligned words occurring at segment boundaries. Alignment fails because L-Segmenter performs it solely 4 https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz 5 comparison between mwerSegmenter, the original implementation of this method, and mweralign, its new re-implementation by (Post and Hoang 2025), is provided in Appendix C. 7 Algorithm 1 Pseudo-code of the algorithm for the refinement of segment boundaries Input: = (. . . , si, . . .), = (. . . , ti, . . .) Output: = (. . . , si, . . .) <si, ti>: pseudo // sentences source sentences with refined boundaries 1: procedure BOUNDARIESREFINEMENT(s, t) 2: 3: SimAlign(si + si+1, ti + ti+1) for [1, ..., len(s) 1] do + is the concat operator, are word alignments FindOptimalSourceSplit(a) find in si + si+1 minimizing cross-alignments (si, si+1) AdjustSource(j, si, si+1) move words from/to si and si+1 according to si+1 si+1 4: 5: 6: end for 7: return 8: 9: end procedure on string matching, ignoring any grammatical considerations (e.g., lexicon, syntax, semantics). Therefore, to refine the boundaries of segments generated by XL-Segmenter, our further extension looks at word alignments built on word embeddings rather than on words surface form, as the Levenshtein distance does. Embeddings have the advantage of capturing semantic relationships between different words with similar meaning (such as synonyms), thus yielding to more robust and precise alignments. Algorithm 1 provides high-level description of the procedure. Given pair of source/target sentences <si,ti>, each element is concatenated with its immediate successor (si+1 and ti+1, respectively); the tokens of the resulting concatenations are then aligned using SimAlign (Jalili Sabet et al. 2020) (step 3). At this point, while keeping the original bipartition of ti + ti+1 fixed, the number of cross-alignments6 is computed for all possible bipartitions of si + si+1; the bipartition with the minimal number of cross-alignments is selected as the new segmentation of si + si+1 (step 4). If the optimal segmentation of si + si+1 differs from the original one, words are moved accordingly from one segment to the other (steps 5 and 6). The strength of the proposed algorithm lies in its use of SimAlign, word alignment tool that leverages both static and contextualized embeddings and is particularly effective at aligning words even when they appear far apart in the sequence. In our implementation, embeddings are provided by mBERT (Devlin et al. 2018), which supports 104 languages. The two tables at the bottom of Figure D.1 in Appendix illustrate how the typical boundary issues observed in segments produced by XL-Segmenter are solved by our refinement algorithm. We name XLR-Segmenter the whole segmentation procedure depicted in Figure1, where the indicates the boundary Refinement stage, and its effectiveness is shown in Sections 5.3 and 5.4. 4. Experimental Setting In this section, we describe each building block of our experiments, i.e., data, metrics, systems, and models, providing the motivations supporting each choice. 6 cross-alignment is such if the tokens it connects fall in opposite partitions of the respective strings (the right one of the source and the left one of the target, or vice versa). 8 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Table 1 On the left: statistics for each section of the MuST-C test set: number of segments, total duration of the English audio, number of (detokenized) source and target words. () For the Chinese side, characters are counted instead of words. On the right: statistics for each section of the Europarl-ST test set: total duration (h:mm) of the audio in the source language (top half) and number of (parallel) segments (bottom half) MuST-C Europarl-ST trg #seg h:mm src trg ar cs de es fa fr it nl pt ro ru tr vi zh 2019 4:05 42.2k 36.4k 2035 3:15 36.1k 29.7k 2641 4:09 46.4k 44.0k 2502 4:10 46.3k 42.7k 2113 3:37 40.0k 56.7k 2632 4:09 46.4k 49.7k 2574 4:10 46.3k 42.2k 2615 4:09 46.3k 42.6k 2502 4:10 46.2k 43.2k 2556 4:10 46.3k 44.2k 2513 4:10 46.1k 36.5k 2408 4:10 45.9k 33.1k 2361 4:09 45.2k 60.4k 1824 4:00 39.7k 74.9k total 33295 56:31 619.1k 636.3k src de en es fr trg it nl pl pt ro de en 2:53 es 3:09 5:05 fr 2:52 4:40 2:55 it 2:43 5:9 2:38 2:39 nl 2:30 4:01 2:24 2:21 2:14 pl 3:08 5:27 3:06 3:04 3:02 3:01 pt 3:34 6:28 3:32 3:34 3:31 3:26 3:22 ro 3:37 5:38 3:33 3:24 3:32 3:33 3:25 3:32 6:03 3:16 3:10 2:57 2:58 3:08 3:11 2:48 2:55 2:47 2:43 2:51 2:51 2:53 2:31 3:05 3:08 3:05 3:01 3:05 2:36 2:52 3:01 2:55 2:51 2:31 2:28 2:25 2:35 2:13 2:18 2:13 2:02 3:06 2:29 3:05 de en 1253 es 1114 1816 fr 1093 1804 1098 it 922 1686 885 893 nl 1063 1747 1014 1012 890 pl 1284 2231 1254 1259 1180 1225 pt 1271 2286 1256 1273 1205 1228 1196 ro 1231 1963 1204 1157 1168 1210 1164 1200 2631 1421 1401 1217 1305 1376 1387 1233 1267 1214 1130 1235 1238 1262 1095 1082 1079 1094 1059 1089 910 1046 1150 1113 1100 949 837 820 871 742 967 942 877 1252 993 1108 4.1 Data We experiment with the test sets of two multilingual ST corpora: MuST-C v1.2 (Di Gangi et al. 2019) and Europarl-ST v1.1 (Iranzo-Sánchez et al. 2020). MuST-C (Multilingual Speech Translation Corpus) is large-scale corpus comprising English audio segments aligned with manual transcripts and their corresponding translations into 14 target languages. The corpus is derived from TED Talks,7 ensuring coverage of diverse topics and speaking styles, and speaker gender and nationality. Each language-specific section of MuST-C contains hundreds of hours of speech and is segmented at the sentence level, which facilitates supervised training for ASR, MT, and ST models. Our experiments are carried out with all the 14 languages covered by the corpus, namely: Arabic, Czech, Dutch, Farsi, French, German, Italian, Portuguese, Romanian, Russian, Spanish, Turkish, Vietnamese, and Chinese. Table 1 (left) provides statistics of the MuST-C test set, named tst-COMMON. Europarl-ST is multilingual speech translation corpus built to support research in multilingual and domain-specific settings. It is derived from the Europarl cor7 www.ted.com 9 pus (Koehn 2005) of European Parliament proceedings, which features textual translations of formal, structured speech across range of political and legislative topics. The source audio was extracted from publicly available recordings of parliamentary sessions, and the corpus was constructed through careful alignment and filtering to ensure high-quality data for both ASR and ST. Europarl-ST consists of audio recordings in nine European languages (German, English, Spanish, French, Italian, Dutch, Polish, Portuguese, and Romanian), each accompanied by manual transcripts and sentencealigned translations into the other eight languages, thus covering 72 translation directions. Table 1 (right) provides statistics of the Europarl-ST test set. Since both corpora provide reference transcripts aligned to reference translations, they enable systematic investigation into how replacing these gold-standard sources with synthetic counterparts, whether ASR-based or BT-based, impacts the reliability of source-aware evaluation metrics. This setup thus allows us not only to assess the potential degradation introduced by synthetic sources but also to anticipate their viability as practical substitutes in real-world scenarios where manual transcripts are unavailable. Moreover, the large total number of segments (33,295 for MuST-C and 88,227 for Europarl-ST), combined with the rich variety of languages, provides solid empirical basis, ensuring that the results reported in the following sections are grounded on broad and statistically reliable pool of observations. 4.2 Metrics For our study, we selected two metrics that represent the two best families according to the WMT24 Metrics Shared Task (Freitag et al. 2024). In both the official and Error Span Annotation rankings (Kocmi et al. 2024), metrics that incorporate the source text alongside the translation hypothesis and reference consistently achieve the highest performance. We therefore selected COMET-22 (ranked third and widely adopted in the research community) and MetricX-24-Hybrid (ranked first), which are described below: COMET (Rei et al. 2020) is learned metric based on multilingual pre-trained language models. It operates by encoding the source sentence, MT hypothesis, and human reference using contextualized embeddings (typically from XLM-R (Conneau et al. 2020)), and then scoring the translation quality through regression model trained on human judgments. Unlike traditional surface-level metrics, COMET captures deeper semantic and syntactic correspondences, yielding stronger correlations with human evaluations across diverse language pairs and domains. Among the several models available, in our experiments we employed the default one, that is wmt22-comet-da (Rei et al. 2022),8 which covers about 100 languages. The MetricX family consists of state-of-the-art neural models (Juraska et al. 2023) that leverage large-scale, multilingual transformer architectures to produce quality scores. They are trained on extensive human-labeled data and optimized for robustness across diverse domains, language pairs, and quality ranges. The Hybrid variants (Juraska et al. 2024) combine both reference-based and reference-free evaluation signals, incorporating features from the source, hypothesis, and optionally the reference translation. In our experiments, we employed the MetricX-24-Hybrid-XL9 model, which supports 101 languages. 8 https://huggingface.co/Unbabel/wmt22-comet-da 9 https://huggingface.co/google/metricx-24-hybrid-xl-v2p6 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST In some of the experiments discussed below, we also evaluate the quality of ASR and MT, as well as the semantic similarity between source and target texts. For those purposes, we rely on standard metrics, respectively, WER, BLEU, and LASER: WER quantifies the accuracy of ASR systems as the Levenshtein distance, i.e., the minimum number of word-level substitutions, insertions, and deletions needed to transform the system output into the reference transcript, normalized by the total number of words in the reference. It is computed via jiWER.10 Unless otherwise specified, before computation, hypotheses and references are lowercased and punctuation is removed. BLEU measures the degree of overlap between systems output and one or more reference translations by comparing matching n-grams. More precisely, it computes the geometric mean of (modified) n-gram11 precisions between translation hypothesis and reference(s), combined with brevity penalty to account for length differences and discourage overly short hypotheses. It is computed via sacreBLEU (Post 2018).12 LASER (Artetxe and Schwenk 2019) is not an evaluation metric, but rather sentence embedding model that produces multilingual vector representations of sentences. We used laserembeddings 1.1.2,13 the pip-packaged, production-ready port of LASER, to compute the cosine similarity for any sentence pair, even in different languages. We use it to evaluate the quality of XL-Segmenter and XLR-Segmenter by measuring the similarity between an automatically segmented source text and the manually segmented reference translation. From now on, we refer to the LASER-based similarity score simply as LASER. 4.3 ASR and BT Models For synthetic source generation, we used three ASR models to transcribe the audio source (Whisper, OWSM, and SeamlessM4T), and two MT models for back-translating the reference translations into the source language (MADLAD and NLLB), which are described below: Whisper (Radford et al. 2023) is family of open-source models for speech-related tasks trained on 680,000 hours of labeled audio data collected from the web. Built upon Transformer-based encoder-decoder architecture, Whisper is capable of performing ASR (supporting nearly 100 languages), ST (from those languages into English), timestamp estimation, and language identification. We employed the 1.55B parameters v3-large multilingual model14 (ASRwh) to perform the speech transcription. OWSM (Peng et al. 2023) is family of multilingual, encoder-decoder speech foundation models, designed for several speech-related tasks, ASR and ST included. For ASR, we used OWSM v3.1 medium (ASRos) (Peng et al. 2024),15 model with 1.02B parameters in total, trained on 180k hours of public speech data and supporting the transcription for 151 languages. SeamlessM4T (Seamless Communication et al. 2023) is foundation all-in-one Massively Multilingual and Multimodal Machine Translation model supporting several 10 https://github.com/jitsi/jiwer 11 Typically, BLEU considers 1-grams, 2-grams, 3-grams, and 4-grams. 12 signature: nrefs:1case:mixedeff: notok:13asmooth:expversion:2.0.0 13 https://pypi.org/project/laserembeddings/ 14 https://github.com/openai/whisper 15 https://huggingface.co/espnet/owsm_v3.1_ebf 11 speech-related tasks, ASR and ST included, in nearly 100 languages. For ASR, we used SeamlessM4T v2 large (ASRsm),16 transformer model with 2.3B parameters. MADLAD is family of large-scale, pre-trained models for MT. These models are designed to support translation across over 400 languages, many of which are lowresource and underrepresented in existing MT systems. The training corpus for MADLAD models comprises approximately 18 billion sentence pairs, mined and filtered from wide variety of multilingual web and public sources to ensure broad linguistic and domain diversity. In our experiments, we used MADLAD-400-3B-MT (BTmd) (Kudugunta et al. 2023), 3B-parameters MT model capable of performing bidirectional translation between all supported languages. The model is based on Transformer architecture and is trained with multilingual objective to enable zero-shot and few-shot generalization across diverse language pairs. NLLB (Costa-jussà et al. 2022) is family of multilingual MT models, aimed at enabling translation across wide spectrum of languages, including low-resource and underrepresented ones. NLLB-200-3.3B (BTnl) is 3.3-billion-parameter Transformerbased model that supports direct translation between any pair of the 200 covered languages. The average quality of the three ASR models and of the two MT models for the BT task is provided in Table 2. Table 2 ASR (left) and MT (for the BT task, right) performance on the MuST-C and Europarl-ST test sets model MuST-C EP-ST %WER %WER ASRwh 6.97 ASRos 9.37 ASRsm 18.53 10.40 20.71 9.84 MuST-C Europarl-ST model BLEU COMET MetricX BLEU COMET MetricX (0-25) (0-25) (0-100) (0-100) (0-1) (0-1) BTmd BTnl 38.74 39. 0.8562 0.8510 3.398 3.326 31.40 25.63 0.8810 0.8627 2.439 2.891 Starting from ASR, although performance is overall excellent, we observe cases where results are particularly low, specifically those of SeamlessM4T on MuST-C and of OWSM on Europarl-ST. In the first case, SeamlessM4Ts ASR results on MuST-C tend to be systematically less accurate compared to Whisper and OWSM.17 Concerning OWSM, it is quite strong in transcribing English (WERos=12.70% vs. WERwh=11.33 and WERsm=11.24) but rather weak in other languages, particularly Portuguese (WER=39.35%), Romanian (WER=27.69%) and Polish (WER=27.35%). This explains its high average WER on Europarl-ST. Regarding BT, the values of COMET and MetricX are all very good, ensuring an extremely high quality of the synthetic sources generated by MADLAD and NLLB. However, while on MuST-C the two models are substantially equivalent, MADLAD turns out to be significantly better than NLLB on Europarl-ST. 16 https://huggingface.co/facebook/seamless-m4t-v2-large 17 While this result is surprising, it is beyond the scope of this work to identify the causes of this behavior, which we leave to future works. 12 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST 4.4 ST Models To ensure the broad validity of our findings, we complemented the variety of data and domains (Section 4.1) with heterogeneous set of ST systems, differing in both architecture and performance. We opted for three direct systems among those capable of covering most of the translation directions present in our two benchmarks, and three cascaded systems, where the ASR instance of the direct ST models was coupled with MADLAD, the best performing state-of-the-art MT model among the two involved in our investigation.18 Specifically, the six ST systems are: (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) (cid:114) whisperST: We employed the same Whisper model used to perform ASR to also generate the direct translation from non-English speech into English text. whsp+mdld: Cascade of the Whisper model, used as ASR for transcribing the speech of the original audio recording, and MADLAD for translating the transcripts into the target language. owsmST: We employed the same OWSM model used to perform ASR to also generate the direct translations. owsm+mdld: Cascade of the OWSM model for ASR and MADLAD for MT. seamlessST: We employed the same SeamlessM4T model used to perform ASR to also generate the direct translations. smls+mdld: Cascade of the SeamlessM4T model for ASR and MADLAD for MT. Table 3 Overview of data used: number of language pairs and of segments each system worked on system whisperST whsp+mdld owsmST owsm+mdld seamlessST smls+mdld total MuST-C EuroparlST src-tgt #seg src-tgt #seg - 14 14 14 14 70 - 33,295 33,295 33,295 33,295 33,295 8 72 64 72 72 72 16,164 88,227 79,294 88,227 88,227 88,227 166,475 448,366 Table 3 shows the main statistics regarding the sections of the benchmarks actually covered by each of the six ST systems. Since whisperST supports ST only into English, and MuST-C includes only English speech, the system cannot be used in experiments on that corpus. Europarl-ST, instead, can be employed only for the eight to-English directions. owsmST does not cover the translation into Polish; therefore, the eight toPolish tasks are excluded from experiments. Table 4 provides the overall performance of the six ST systems on the two benchmarks. To better contextualize these values, we compare the MuST-C scores with those reported by (Tsiamas et al. 2024), which spans 35 recent systems. The best BLEU, aver18 Direct ST systems map input speech directly into the target language text in single end-to-end model, without generating intermediate transcripts. Cascade ST systems operate in two stages: first, transcribing the source speech into text with an ASR model and then translating that text into the target language with an MT model. See (Bentivogli et al. 2021) for comparison of the two architectures. Table 4 ST performance of each system on the MuST-C and the Europarl-ST test sets. For the cascade architectures, the quality of the automatic transcription is also provided, in terms of WER. For each metric, values are arithmetically averaged on all language pairs covered by each system (see Table 3) MuST-C Europarl-ST system WER BLEU COMET MetricX WER BLEU COMET MetricX (0-25) % (0-25) % (0-100) (0-100) (0-1) (0-1) - whisperST whsp+mdld 6.97 owsmST owsm+mdld 9.37 seamlessST smls+mdld 18.53 - 25.79 16.18 25.49 22.34 24.13 - 0.8167 0.6504 0.8041 0.7833 0.7974 - 3.963 10.798 4.543 5.103 4.553 10.40 20. 9.84 28.89 29.79 3.87 26.23 22.04 30.26 0.8091 0.8524 0.5292 0.8028 0.7977 0.8542 5.636 3.608 15.005 5.583 5.449 3.469 aged on 8 language pairs, is 33.2. Our whsp+mdld system scores 31.8 on the same set of language pairs, and it would rank fifth. In general, cascaded systems perform better than direct ones. The only competitive direct system is whisperST, while owsmSTs average performance is particularly low, especially on Europarl-ST. In fact, while owsmST translates the English speech section of Europarl-ST similarly to how it does in MuSTC (the average BLEU on the seven Europarl-ST from-English pairs is 15.50, compared to the average of 16.18 on MuST-C), it performs poorly on all other language pairs, especially those not involving English. All in all, our choice of ST models yields broad diversification not only in terms of architecture but also in terms of performance, including cases of particularly low quality of transcripts and translations. 4.5 Metric Evaluation Criterion As our goal is to verify the effectiveness of using synthetic sources to compute sourceaware metrics, we need to evaluate whether the scores obtained through synthetic sources are reliable or not. To this aim, we consider the source-aware metrics with manual transcripts as our gold standard, relying on their demonstrated high correlation with human judgment, and we compute the Pearson correlation (Pearson 1895) of the scores obtained with synthetic sources with those obtained with manual transcripts. We choose Pearson correlation over alternative methods such as Kendall and Spearman because we care not only about the returned ranking but also about the magnitude of the differences in scores. The correlation scores are computed independently for the two benchmarks (see Section 4.1), for each metric (see Section 4.2), for each method used to generate synthetic sources (see Section 4.3) and for each ST system (see Section 4.4). For instance, if we use the Whisper ASR to generate the synthetic source, MuST-C as benchmark, COMET as metric, and SeamlessM4T as an ST model, we generate the outputs with SeamlessM4T for all the 33,295 segments of MuST-C, then we compute the segment-level COMET scores both using the manual (gold) transcript and the synthetic one generated by Whisper, and finally compute the correlation between the pairs of segment-level scores. The resulting correlation score tells us whether the synthetic source reflects the behavior obtained with the manual source (if the score is close to 1) or it changes to be unrelated 14 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST to the correct score (if the score is close to 0). Following (Cohen 1988), we consider correlations > 0.80 as very strong. We do not report in the main body of this work the correlations between system rankings and scores on the whole test sets, as we noticed that these correlations are always very high (> 0.99) for all methods and metrics, as we show in Appendix E. From here on, for simplicity, we name standard the metric that uses the manual transcript, and either BT, ASR or simply synthetic the one with, respectively, the BT source, the ASR source or any of them. 5. Experiments In this section, we first investigate the impact of using synthetic sources in the simplified scenario in which the alignment between the reference translation and the source audio is provided (Section 5.1). This preliminary investigation has two-fold objective: to assess whether synthetic sources compromise the reliability of source-aware metrics, and to compare the ASR and BT options under optimal conditions for ASR (without possible realignment errors). We complement this investigation with an analysis of the factors driving the superiority of one source generation method over the other (Section 5.2). Then, we move to the scenario in which the reference translation is not aligned to the source audio. In Section 5.3, we evaluate the effectiveness of our re-segmentation method on the manual transcripts, thus preventing errors made by ASR systems from influencing the results of the synthetic source text. Lastly, we compare the BT and ASR approaches in realistic scenario, where the ASR source is realigned to the reference translation using our proposed method (Section 5.4). 5.1 Comparison of Synthetic Sources with Known Audio-Reference Alignments Outline of experiments. In this first set of experiments, we exploit the manual audio segmentation, which ensures the alignment of the ASR transcripts with the reference translations. This controlled setup allows us to perform accurate comparisons that avoid the influence of the automatic re-segmentation of the transcripts. Results. The upper part of Tables 5 and 6 provides, for both corpora (MuST-C and Europarl-ST) and both source-aware metrics (COMET and MetricX), the correlations scores of the five ST systems capable of translating from English (see Table 3). To define lower bound for the correlation values, we include the correlations with the scores obtained by randomly shuffling the segments of reference transcripts (shuff), that is, by completely inhibiting any relationships between the source and the two translations (hypothesis and reference). The lower part of the tables indicates the extent to which the correlation gap between random and manual sources is recovered by exploiting synthetic sources.In other words, by setting the lower bound of the correlation obtained with the shuffled source to 0, and the upper bound of the correlation obtained with the manual source to 100, the correlation value obtained with the synthetic sources is positioned proportionally. Key observations and takeaways. Both synthetic sources are effective. Overall, all correlation values are very high. On MuST-C, the correlations for COMET are always above 0.99, while for MetricX, there is only one case where the correlation is slightly below 0.93. On Europarl-ST, the minimum values are 0.9784 for COMET and 0.8882 for MetricX. In any case, values above 0.80 15 Table 5 Pearson correlations between the COMET/MetricX scores of each system on MuST-C, computed (i) by using as the source the reference transcript (i.e., in the standard way), and (ii) by using synthetic sources (either automatic transcripts or back-translations of the reference target sentences). Each correlation coefficient is calculated on all segments of all language pairs indicate very strong correlation (Cohen 1988) between the standard and synthetic metrics, meaning that any proposed synthetic source is an effective substitute for the manual transcript. ASR is better than BT. In general, with ASR sources, the correlations are higher than with BT, with few exceptions when the sources are generated by the worst ASRs (from Table 4, SeamlessM4T for MuST-C and OWSM for Europarl-ST). Therefore, ASR sources appear to be more effective than BT ones as long as their quality is good enough, an aspect we explore in Section 5.2. The correlation gap is almost fully recovered. Looking at the percentage recovery instead of absolute correlations, values are always very high for MetricX (ranging from minimum of 67.63 to maximum of 97.07), and high for COMET, with the exception of ASRos on Europarl-ST, due to the poor performance of the recognizer. These numbers 16 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Table 6 Pearson correlations between the COMET/MetricX scores of each system on Europarl-ST, computed (i) by using the reference transcript as the source (i.e. in the standard way), and (ii) by using synthetic sources (either automatic transcripts or back-translations of the reference target sentences). Each correlation coefficient is calculated on all segments of all language pairs confirm the general effectiveness of synthetic sources as substitute of manual sources, although conditioned to transcription quality. Random sources have different impact. COMETs correlations with the randomly shuffled source are surprisingly high (from 0.9794 to 0.9897). We hypothesize that this metric was trained to use the source but gives it definitely smaller weight than the reference translation in evaluating the quality of the translation hypothesis. For MetricX, the source contribution to the final score is significantly higher, as using source uncorrelated with the targets causes the correlations to plunge even to 0.4185. This suggests that COMET may attenuate any differences observed on MetricX in experiments involving the source text, which should be taken into account when interpreting the results. Biased conditions. The final observation concerns specific cases where the ASR source is linked to the ST system under evaluation, that is, when the ASR source is produced by system directly involved in the generation of the ST hypotheses, such as the ASR component of an ST cascade (e.g., whsp+mdld) or the ST system itself (e.g., owsmST). In 17 these biased conditions (see the scores marked with dagger), we observe correlation values that tend to be lower. Our hypothesis is that, in these cases, the metric scores are artificially inflated, regardless of whether the transcript and translation are actually correct. In particular, for cascaded systems, since the output translation is generated using the same transcript as input of the MT component, the metric with ASR source actually assesses the MT component rather than evaluating the whole pipeline. The effect is particularly evident for MetricX, which, as already noted, relies more heavily on the source compared to COMET. In light of this observation, biased cases will be excluded from the aggregate results we present from now on. This first set of experiments indicates that the proposed synthetic replacements of the original sources are extremely effective, yielding synthetic versions of the two metrics that correlate very strongly with their standard versions. The observation that COMET consistently correlates well, even by replacing the original source with randomly shuffled version of it, leads us to continue the investigation only on MetricX, which, by contrast, proved to be more sensitive to the contribution of the source. 5.2 Insights into Synthetic Source Comparison Outline of experiments. In this set of experiments, we examine whether the previously observed superiority of ASR-based sources over the BT-based ones holds systematically or only under specific conditions. Specifically, we investigate how the effectiveness of synthetic sources is influenced by three key factors: (i) their quality; (ii) the languages involved in the evaluation; and (iii) the architecture of the ST system being evaluated. Results. Concerning the possible dependency on the quality, Figure 2 plots the quality of the ASR source (measured by WER) against the quality of the BT (measured by MetricX): for each possible ASR vs. BT pair, if the winner (higher correlation with the standard metric) is ASR, the (blue) point is placed on the left chart; if it is BT, the (red) cross mark is placed on the right chart. Figure 3 shows the same distributions from another perspective, where the positioning and the height of the histogram bars (proportional to the counts) allow for more immediate visualization of the dependence (if any) of the effectiveness of the ASR and BT sources on their quality. It is evident that the effectiveness of ASR sources strictly depends on their quality (ASR is the winner only if the WER is low), while there is no such regularity for BT. Since both figures suggest the WER=20% threshold as particularly discriminatory, Table 7 collects the counts broken down by that value. It turns out that ASR is preferable in 78.6% of the cases, but the percentage increases up to 87.4% when WER20%. Above that threshold, BT wins 85.5% of times. In other words, we can state that in the presence of an ASR with WER not exceeding 20%, the probability that it is better substitute of the ideal source than any BT is over 87%, otherwise the choice of BT is winning at almost 86%. From the language perspective, Table 8 shows breakdown of the total counts of Table 7 by language pair. In most cases, ASR wins with the only exception of Polish as the source language, where BT slightly prevails over ASR. Overall, the results show no dependence on the specific language pair, confirming the previous finding that ASR sources are generally better substitutes of manual sources regardless of the languages involved in the translation process. In relation to the ST system architecture, Table 9 shows breakdown of the total counts of Table 7 by system. First, ASR sources are consistently the best option across all the systems. Comparing the two types of ST architectures (cascaded and direct), their prevalence is slightly more marked in direct 18 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Figure 2 The plane of these scatter charts is defined by the WER and MetricX scores of the ASR and BT sources, respectively. For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the two charts show where the cases in which it was preferable to use the ASR (on the left) or the BT (on the right) as source for the computation of MetricX are placed in that plane. Biased ASR MetricXs, i.e., those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded. The total number of points is 1672, 1315 on the left (ASR wins, 78.6%), 357 on the right (BT wins, 21.4%). random 1% change was applied to all values to avoid the overlapping of points and make all of them visible. (575 vs. 97, i.e., 86% vs. 14%) than in cascaded systems (740 vs. 260, i.e., 74% vs. 26%). Despite this, the greater effectiveness of ASR sources compared to BT sources remains confirmed regardless of the architecture of the ST system being evaluated. Key observations and takeaways. The quality of ASR impacts the effectiveness of synthetic metrics. Our results confirm that, in general, ASR sources work better than BT sources if the WER remains below 20%, otherwise BT is the best choice. Our findings are consistent across languages. There is no evidence that the specific languages involved in the speech translation process need to be taken into account when choosing between ASR or BT sources. Systems architecture does not impact. Similarly, the architecture of the ST system being evaluated does not appear to impact the choice either. These experiments show that ASR sources are preferable to BT sources, provided their WER is sufficiently good (20%). They also highlight that the choice between ASR and BT is not affected by the languages involved in the ST process nor by the architecture of the ST systems being evaluated. 5.3 Source Re-segmentation of Manual Transcripts Outline of experiments. In the previous experiments, we assumed that the alignment between the reference translation and the source audio is known. As discussed in Section 1 (RQ3), this represents simplistic assumption that might not hold in realworld conditions, but allows performing the comparisons, avoiding the influence of the automatic re-segmentation of the transcripts. Similarly, to evaluate the effectiveness of our approach for re-segmenting the synthetic sources (see Section 3.2), in this section we assess its ability to re-segment the manual transcripts. This controlled setup enables us to precisely measure how well our re-segmenters (Figure 1) can recover the original 19 Figure 3 For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the histograms illustrate the distribution of cases in which the standard MetricX shows higher correlation with MetricX using either the ASR or the BT as source input. Biased ASR MetricXs, i.e., those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded. The left chart reflects this distribution as function of transcription quality (WER), while the right chart does so with respect to (back-)translation quality (MetricX). Table 7 For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the total number of wins per synthetic source type (ASR whisper, owsm, seamless or BT - madlad, nllb) is given here. Biased ASR MetricXs, i.e. those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded WERASR 20% WERASR >20% total corpus BT wins ASR wins ASR wins BT wins counts % counts % counts % counts % counts % counts % 20.0 21.6 21.4 0.0 80.0 224 94.0 1091 78.4 85.5 1315 78. BT wins ASR wins 100.0 6.0 14.5 21.4 10.7 12.6 56 130 186 0 171 171 56 301 18 11 29 MuST-C 78.6 Europarl-ST 1080 89.3 1286 87.4 total 206 segmentation, preventing errors made by ASR systems from influencing the results. Specifically, we first artificially modify the gold, reference-aligned segmentation of the manual transcripts by randomly splitting the transcripts into segments containing 5 to 100 detokenized words. Then, we apply XL-Segmenter and XLR-Segmenter to the randomly segmented sources19 and evaluate how their outputs deviate with respect to the original manual segmentation of the dataset. To this aim, we compute LASER scores between the source text in the new segments and the corresponding segments of the reference translations.20 Since LASER scores are based on the cosine similarity between 19 Since our re-segmentation procedure requires generating BT of the original translations, we perform this step using both MT models considered in our study, MADLAD and NLLB (Section 4.3). 20 We compare our outputs with the references instead of the original source text, as the benchmarks used in our experiments contain some misalignments between the source and target text (Ouyang, Ye, and Li 2022), most likely due to the fact that these alignments are automatically produced. While (Ouyang, Ye, and Li 2022) showed that these misalignments do not compromise the reliability of the evaluation of ST quality, they might have an impact when it comes to assessing the correct alignment between texts. To avoid this affecting our evaluation, we assess the alignment of the source text against the reference target text. 20 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Table 8 Counts of ASR/BT wins per language pair trg de en es fr ar cs de en es fa fr it nl pl pt ro ru tr vi zh 24/0 20/ 20/0 20/0 20/0 10/0 20/0 20/0 20/0 16/4 25/15 32/8 20/0 19/21 29/11 26/14 6/4 30/10 24/16 20/0 18/2 14/6 20/0 20/0 22/2 20/0 20/0 20/0 10/0 20/0 20/0 19/1 17/7 19/ 19/1 18/2 10/0 18/2 20/0 src it 20/0 23/1 20/0 20/0 20/0 10/0 20/0 20/ nl pl pt ro total 14/6 16/8 16/ 14/6 17/3 9/1 17/3 14/6 8/12 4/20 7/13 14/6 11/9 14/6 14/6 10/10 14/6 10/0 11/9 13/7 4/20 14/ 20/0 16/4 14/6 10/10 130/50 126/66 16/8 142/38 14/6 20/0 134/46 147/33 142/38 71/9 150/30 145/35 20/0 18/2 14/6 20/0 16/4 14/6 14/6 6/4 14/6 14/6 total 154/0 319/111 152/2 140/14 153/1 117/37 78/86 110/44 92/62 1315/357 Table 9 Counts of ASR/BT wins per system whisperST 19/ direct owsmST 295/17 575/97 cascade seamlessST whsp+mdld owsm+mdld smls+mdld 261/67 212/116 297/47 740/ 231/97 1315/357 the two sentence embeddings, the closer these scores are to 1, the better the automatic re-segmentation works. Results. Table 10 shows the LASER scores computed between the manually segmented reference translations and differently segmented reference transcripts. The LASER scores for the manual segmentation of the two benchmarks evaluate the correspondence between the reference translations and the manual transcripts, without these scores being affected by segmentation differences. They therefore represent the upper bound for the scores obtainable when the transcripts are automatically segmented.21 The minimal differences observed between the manual and automatic segmentation scores 21 It is possible that these upper bounds are exceeded in cases where the reference segmentations of manual transcripts and reference translations are not perfectly aligned, anomalies that we have indeed observed in our benchmarks. demonstrate the ability of our segmenters to reconstruct the manual segmentations, to the extent discussed below. Key observations and takeaways. XL-Segmenter approaches manual segmentation. The XL-Segmenter works well, as the observed relative degradation compared to manual segmentation is limited to 1-3%. This result represents baseline for the cross-lingual re-segmentation problem, as XLSegmenter is direct extension of L-Segmenter towards its application in cross-lingual settings. XLR-Segmenter closes the gap with manual segmentation. The proposed boundary refinement stage allows XLR-Segmenter to completely close the gap with manual segmentation, showing consistent improvement over the XL-Segmenter baseline. Our automatic re-segmentation is effective and robust to variable BT quality. Using higher quality BT, the one guaranteed by MADLAD compared to NLLB (see Table 2), yields only slightly better segmentation, thus demonstrating the robustness of our re-segmentation procedure in relation to the quality of the BT step it relies on. These experiments indicate that XL-Segmenter, the direct extension of L-Segmenter for its use across different languages, is extremely effective, and that our boundary refinement stage allows XLR-Segmenter to substantially eliminate all remaining alignment errors. Table 10 LASER scores computed on the manually segmented reference translations and the reference transcripts segmented: manually, re-segmented by either XL-Segmenter or XLR-Segmenter with respect to the BT by either MADLAD or NLLB BT for reseg mdld nllb segmentation manual XL-Segmenter XLR-Segmenter XL-Segmenter XLR-Segmenter MuST-C Europarl-ST 0.8550 0.8440 0.8562 0.8409 0. 0.9008 0.8822 0.9013 0.8741 0.9003 5.4 In-the-Wild Evaluation Outline of experiments. Our final set of experiments focuses on realistic working conditions, where audio is automatically segmented and transcripts are generated by ASR systems. We first evaluate the quality of the source segmentation similarly to what we did under controlled conditions (Section 5.3), with the difference that an automatic segmentation of the audio is used instead of the manual one. The audio is segmented by means of SHAS (Tsiamas et al. 2022), using the specific model for those languages for which it is available (English, Spanish, French, Italian, and Portuguese), and the multilingual model for the other languages. Each segment is then transcribed with the three ASR systems listed in Section 4.3. Finally, automatic transcripts are resegmented by either XL-Segmenter or XLR-Segmenter, exploiting the BT of the reference translations provided by the two BT models also listed in Section 4.3. The evaluation of re-segmented automatic transcripts is done both in terms of WER against the manual transcripts and with LASER against the reference translations. Then, following the same methodology adopted under controlled conditions (Section 5.1), we compare BT sources 22 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST with those obtained on automatic transcripts of automatically segmented audio. This setup enables comprehensive assessment of the advantages and limitations of both families of synthetic sources when applied in the wild. In-the-wild Re-Segmentation: Results. Table 11 shows the quality of ASR sources in terms of LASER and WER. For comparison purposes, the scores obtained by automatically transcribing the manual audio segments are also provided. The WER is computed in both \"case-insensitive, no-punctuation\" mode (the standard default) and \"case-insensitive, with-punctuation\" mode, to highlight the ability of our refinement stage to also correct the placement of punctuation marks. Table 11 Evaluation of automatically re-segmented automatic transcripts. LASER scores are calculated with respect to the reference translation, and WER scores with respect to the reference transcript. WER is computed ignoring casing and considering (wp) or not (np) punctuation marks; in wp mode, text is tokenized, i.e., words are separated from punctuation. The LASER score for the manually segmented reference transcript is 0.8550 for MuST-C and 0.9008 for Europarl-ST ASR by BT for reseg by madlad whisper owsm seamless nllb madlad nllb madlad nllb - M A R wp np A - p E wp np manual seg shas+XL-Segmenter shas+XLR-Segmenter manual seg shas+XL-Segmenter shas+XLR-Segmenter manual seg shas+XL-Segmenter shas+XLR-Segmenter manual seg shas+XL-Segmenter shas+XLR-Segmenter manual seg shas+XL-Segmenter shas+XLR-Segmenter manual seg shas+XL-Segmenter shas+XLR-Segmenter 0.8256 0.8176 0.8060 0.8216 0.8320 0.8185 0.8310 0.8150 0. 0.8124 0.8238 0.7286 0.7327 0.7255 0.7317 12.21 13.66 21.32 17. 6.97 15.13 11.55 21.95 17.90 15.90 11.88 23.12 19.54 9. 17.56 14.02 23.70 19.86 18.25 14.37 0.8849 0.8604 22. 48.18 47.77 48.82 48.18 18.53 43.90 43.44 44.60 43.88 0. 0.8642 0.8833 0.8563 0.8824 0.8361 0.8530 0.8276 0.8517 0.8648 0.8816 0.8569 0. 14.61 24.52 13.43 23.96 18.62 26.41 18.88 34.58 30. 37.16 30.41 23.75 18.92 10.40 20.71 9.84 18.43 13. 21.00 13.74 29.38 25.01 32.10 25.35 18.51 13.84 26.22 19.18 21.09 14. In-the-wild Re-Segmentation: Key observations and takeaways. The boundaries refiner handles real-world conditions. The refinement stage proves to be extremely effective also in realistic conditions, allowing XLR-Segmenter to consistently outperform the XL-Segmenter, independently from the ASR system (Whisper, Owsm, Seamless), the corpus (MuST-C, Europarl-ST), and the metric (LASER, WERwp, WERnp). The observed WER improvement, up to more than 7 absolute points (from WERnp=21.00 to 13.74 for Whisper/NLLB on Europarl-ST), confirms the ability of the refinement stage to correctly reposition words misplaced by XL-Segmenter. The semantics of automatic transcripts is fully preserved. In terms of LASER, the resegmentation by XLR-Segmenter of automatic transcripts enables approaching the manual segmentation on Europarl-ST (e.g., on Whisper/MADLAD: 0.8833 vs. 0.8849) 23 and even improves it on MuST-C (e.g., on Whisper/MADLAD: 0.8320 vs. 0.8256). This means that, despite the presence of transcription errors, our XLR-Segmenter can adjust the source text segmentation inherited by the SHAS-based audio segmentation in such way as to recover the semantic correspondence between the source and target segments. few words are still misplaced. In terms of WER, the re-segmentation by XLR-Segmenter of automatic transcripts generated on the SHAS audio segmentation closely approximates the manual segmentation, albeit less closely than when they are evaluated by LASER. The degradation ranges from 3.08 WERnp of ASRwhmd on Europarl-ST (from 10.40 to 13.48), to 6.20 WERwp of ASRosnl on MuST-C (from 13.66 to 19.86). The gap is ascribable to the cases in which XLR-Segmenter does not correctly reposition words (36%) of the automatic transcript. However, as the LASER results indicate, these errors do not alter the semantic content of the segments. The refiner correctly repositions punctuation marks. The refinement stage also adjusts punctuation: although the gain is small, the absolute WER improvement is consistently larger when punctuation is taken into account than when it is ignored. For example, in the case of Whisper with the re-segmentation based on MADLADs BT, the refinement lowers the WERnp from 15.13 to 11.55 on MuST-C (WERMuSTC = 3.58) and from 18.43 to 13.48 on Europarl-ST (WEREPST = 4.95). The WERwp is instead brought from 21.32 to np 17.60 (WERMuSTC = 5.34), respectively. Since the difference between WERXnp and WERXwp stems solely from punctuation (as the word sequences are identical), the observed improvement indicates that the refinement stage successfully enhanced punctuation placement. = 3.72) and from 23.96 to 18.62 (WEREPST wp np wp The semantics of manual transcripts is almost fully preserved. The fully automated process is able to generate segmented ASR texts (with whisper), aligned to the reference translations, whose LASER scores are 0.8320 for MuST-C and 0.8833 for Europarl-ST. These are only 2.0-2.5% worse than the upper bound scores obtained by computing LASER against manually segmented transcripts (0.8550 for MuST-C and 0.9008 for Europarl-ST, see Table 10). This further demonstrates the effectiveness of our source re-segmentation solution. These experiments show that, even under realistic conditions where ASR sources are generated fully automatically, our proposed alignment solution remains highly effective, yielding only minimal degradation (2.0-2.5%) compared to the upper bound scores computed on manually segmented sources. In-the-wild Synthetic Sources: Results. Tables 12 and 13 provide, for the two corpora, the correlations achieved by the various ASR sources generated in realistic conditions, together with the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those produced by our two-stage re-segmentation algorithm, whose quality is shown in rows shas+XLR-Segmenter of Table 11. Due to the observed lower sensitivity of COMET to the source content (see Section 5.1), here we focus on MetricX, omitting COMET results (which are reported in Appendix for completeness). It is worth noting that realistic conditions do not impact the BT sources, as they are generated by back-translating segment by segment the reference translations. For ease of reference, MetricXBTmd and MetricXBTnl scores that are reported in Tables 12 and 13 correspond to those originally reported in Tables 5 and 6. Figure 4 and Figure 5 are equivalent to Figures 2 and 3, but related to realistic conditions under investigation in this last set of experiments. In-the-wild Synthetic Sources: Key observations and takeaways. 24 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Table 12 Pearson correlations between the MetricX scores of each system on MuST-C, computed (i) by using as the source the reference transcript (i.e., in the standard way), and (ii) by using as the source the automatically re-segmented automatic transcripts generated from automatic audio segmentation. Each correlation coefficient is calculated on all segments of all language pairs ASR sources remain effective. In general, compared with those obtained in controlled conditions, the correlations of ASR-derived sources marginally decrease, but remain consistently above 0.92. The few exceptions concern the ASR sources derived from high-WER transcripts, namely, those generated by SeamlessM4T on MuST-C and by OWSM on Europarl-ST. Even in these cases, however, values are always above 0.80 (with minimum value of 0.8140), which still reflects strong correlation. These results indicate that, even under realistic conditions, ASR sources remain effective substitutes for manual transcripts. ASR confirms to be better proxy. In 9 cases out of 11 (for 5/5 ST systems on MuST-C and for 4/6 ST systems on Europarl-ST) there is at least one ASR source that is more effective than any BT source, indicating generic greater effectiveness of ASR sources compared to BT sources also in this challenging, realistic setting. The 20% threshold is still discriminative. Figures 4 and 5 remain overall similar to those obtained under controlled conditions, thus confirming our findings in Section 5.1 and, in particular, that (i) ASR sources are generally more effective than the BT ones, and (ii) 25 Table 13 Pearson correlations between the MetricX scores of each system on Europarl-ST, computed (i) by using as the source the reference transcript (i.e. in the standard way), and (ii) by using as the source the automatically re-segmented automatic transcripts generated from automatic audio segmentation. Each correlation coefficient is calculated on all segments of all language pairs Table 14 For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the total number of wins per synthetic source type (ASR - whisper, owsm, seamless resegmented by XLR-Segmenter wrt madlad or nllb - or BT - madlad, nllb) is given here. Biased ASR MetricXs, i.e., those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded WERASR 20% WERASR >20% total corpus BT wins ASR wins BT wins ASR wins counts % counts % counts % counts % counts % counts % 83.0 33.4 66.6 86.5 1731 60.1 1149 39.9 85.5 2104 61.2 1336 38.8 BT wins ASR wins 0.3 28.9 25.3 17.0 13.5 14.5 186 474 660 1 675 676 38 74 112 MuST-C 99.7 Europarl-ST 1657 71.1 1992 74.7 total 335 373 187 the 20% WER threshold discriminates between them. These findings are also supported by the results reported in Table 14, not too dissimilar from those in Table 7. It emerges that ASR is preferable in 61.2% of all cases, but also that this preference rises to 74.7% when the WER is at most 20%. Conversely, when the WER exceeds 20%, BT is the best choice in 85.5% of cases. 26 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Figure 4 The plane of these scatter charts is defined by the WER and MetricX scores of the ASR and BT sources, respectively. For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the two charts show where the cases in which it was preferable to use the ASR (on the left) or the BT (on the right) as the source for the computation of MetricX are placed in that plane. Biased ASR MetricXs, i.e. those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded. The total number of points is 3440, 2104 on the left (ASR wins, 61.2%), 1336 on the right (BT wins, 38.8%). random 1% change was applied to all values in order to avoid the overlapping of points and make all of them visible. Figure 5 For all possible comparisons between the MetricX correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, these histograms illustrate the distribution of cases in which the standard MetricX shows higher correlation with MetricX using either the ASR or the BT as source input. Biased ASR MetricXs, i.e. those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded. The left chart reflects this distribution as function of transcription quality (WER), while the right chart does so with respect to (back-)translation quality (MetricX). These final experiments demonstrate that, even under realistic conditions, synthetic MetricX maintains strong correlation with its standard version based on manual transcripts. Furthermore, they also confirm that, when transcripts are of sufficient quality (WER not exceeding 20%), ASR is the preferred synthetic source, whereas for higher WERs, BT provides better alternative. 27 6. Discussion and Conclusions We investigated the reliability of source-aware evaluation metrics for speech-to-text translation (ST) when the original source transcripts are unavailable and have to be replaced with synthetic alternatives. We systematically examined two types of textual proxies, automatic transcripts (ASR) and back translations (BT), and analyzed their impact on COMET and MetricX across different datasets, languages, and models. In addition, we also delved into the problem of re-segmenting and aligning these synthetic sources with the corresponding reference translations, critical step for computing reliable evaluation scores. Our findings demonstrate that both synthetic sources constitute effective substitutes for manual transcripts, both under controlled conditions (Sections 5.1 and 5.2) and in the wild (Section 5.4). This supports the use of source-aware metrics in ST evaluation (RQ1), practice whose methodological assumptions and effects have received little attention. Between ASR-based and BT-based sources, the former exhibit superior reliability compared to the latter, provided that the transcription quality is high enough. Extensive experiments in diverse conditions indicate that case-insensitive WER, computed without punctuation, should not exceed 20% (RQ2).22 Finally, we showed that the proposed cross-language re-segmentation algorithm (XLR-Segmenter) allows for reliable evaluation even when audio-text alignments are unavailable (Section 5.4), yielding only negligible degradation (RQ3). The experiments also revealed that our boundary refinement step effectively restores the semantic correspondence between automatically generated ASR segments and the reference translation segments. surprising outcome that emerged during the study is that COMET appears to make relatively limited use of the source text. Despite this, what was observed for MetricX also holds true for COMET, which further strengthens our findings. Beyond empirical performance, we extended our analysis to practical considerations that influence the adoption of synthetic sources in real-world ST evaluation, as discussed in Section 3.1. Regarding the language coverage, MT systems currently offer broader range of supported languages than ASR models, making BT more versatile solution for low-resource or less commonly studied languages (see also Appendix A). Concerning model quality, our experiments show that deploying strong MT models is generally easier and more consistent  (Table 2)  , while achieving comparable ASR quality remains more challenging  (Table 4)  . When considering the neutrality of synthetic sources with respect to the ST system under evaluation, as discussed in Section 5.1, we observed that using ASR outputs generated by models related to the evaluated ST system introduces bias, artificially inflating metric values and reducing correlation with reference-based metrics. In this regard, BT constitutes the safest and most neutral option. The similarity between the synthetic source and the original transcripts further influences evaluation reliability, and the results in Section 5.1  (Table 7)  indicate that ASR-based sources more closely approximate human transcripts, explaining their superior effectiveness when unbiased and well-aligned. Additionally, we discussed the alignment between synthetic sources and reference translations that, unlike BT, needs to be restored for ASR sources through re-segmentation, which entails both computational overhead and measurable drop in evaluation accuracy (Section 5.4). Therefore, BT sources should be preferred 22 We highlight that the application of the threshold requires estimating the error rate of the ASR system used to generate the synthetic source. This information can either be inferred from known performance in similar operational conditions, situation that rarely occurs, or measured on suitable test set, whose availability is far from guaranteed. 28 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST when the accuracy of the automatic alignment of ASR sources to the reference translations is critical. Lastly, in terms of overall cost of creating synthetic sources, we carried out experiments on subset of data (Appendix B). As expected, results show that the ASR generation is more demanding than BT generation. Overall, our study provides the first systematic investigation on the deployment of source-aware metrics for ST evaluation, offering practical recommendations for their selection based on specific operating conditions. The outcomes reveal that synthetic source-aware metrics provide reliable and effective means of evaluating ST systems, achieving strong correlation with standard metrics. By addressing both empirical and pragmatic aspects, we hope to facilitate more consistent evaluation practices in ST research. 7. Limitations While the findings presented in this work provide valuable insights, certain limitations should be acknowledged and are detailed below. First, as discussed above, our approach relies on using source texts as proxies for input audio, which cannot be applied to languages without standardized written form. These languages represent the majority of the thousands of languages spoken worldwide, and addressing this limitation would require the development of novel multimodal, source-aware metrics capable of directly leveraging the audio signal. However, this falls outside the scope of this work. second limitation concerns language coverage. All the languages included in the two datasets are highor medium-resource, and therefore, low-resource languages are not represented in our study. This choice reflects an intrinsic limitation of modern neural evaluation metrics, which, being data-driven, generally perform less reliably on lowresource languages. Our experimental setup was therefore designed consistently with the current capabilities of state-of-the-art metrics. further limitation relates to the nature of the benchmarks used. Both datasets consist of clean, single-speaker recordings without background noise, rather than speech recorded in natural conditions. While this setting may not capture the full complexity of real-world scenarios, these datasets remain the only benchmarks currently available for speech translation that cover such broad range of languages (i.e., 79 language pairs). Another consideration is that our analysis focuses on only two source-aware evaluation metrics, COMET and MetricX. Since MetricX is the only metric significantly affected by the source, some detailed results for COMET were reported in Appendix to enhance clarity and focus of the paper. Although this scope may appear narrow, these metrics were deliberately selected as widely used representatives of two distinct families of source-aware metrics, both consistently ranked among the top-performing ones in the WMT Metrics Shared Tasks. Furthermore, the fact that the findings obtained with MetricX remain valid even when considering COMET further supports the broader applicability and reliability of our conclusions. final limitation concerns the quality of the systems used to generate synthetic sources. The two MT models employed for back-translation are both high performing, whereas the three ASR systems used to produce source ASRs do not always reach the same level of accuracy. We acknowledge that including lower-performing MT models in the analysis would likely have resulted in reduction of the reliability of BT sources, similar to the degradation observed for ASR sources with high WERs. However, in typical research and experimental conditions, high-quality MT models are generally easier to obtain than equally reliable ASR systems, as discussed in Appendix A. Accordingly, our experimental setup was designed to be consistent with the capabilities of systems commonly available in current research environments. Appendix A: On the Quality of ASR and MT Models In Section 3.1, we discussed that MT models tend to exhibit higher quality than ASR models. In the following, we outline typical performance levels reported in the literature for both tasks, to contextualize our findings and clarify the expected reliability of the synthetic sources used in our experiments. For ASR, transcription quality is influenced by wide variety of factors, leading to outcomes that range from near-human accuracy to levels of limited practical utility. For clean, read speech (e.g., the test-clean split of LibriSpeech), current systems achieve WERs of 1-3% (Gulati et al. 2020; Baevski et al. 2020). On spontaneous speech (e.g., Switchboard, GigaSpeech), WER typically rises to 5-10% or higher (Zeineldeen et al. 2022; Ng et al. 2021), as also reflected on the Hugging Face Open ASR leaderboard (Srivastav et al. 2023). For instance, the Canary-1B model reports an average WER of 6.5%, less than one point above the best score of 5.6%,23 but its performance on spontaneous telephone conversations from the CallHome benchmark drops to 16.9% WER (Wang et al. 2024), showing the substantial degradation under less controlled conditions. More challenging operating conditions are proposed annually by the CHiME challenge. In the 2024 edition,24 two ASR tasks were organized: DASR (multi-channel distant ASR) and NOTSOFAR (single-device meeting transcription). Both tasks combine spontaneous speech with multiple (sometimes overlapping) speakers and variable microphone distances, resulting in WERs ranging from 20% to 50%.25 Transcribing nonnative speech or speech with strong regional accents is also similarly challenging: (Do et al. 2024) reports WERs above 30% on test set of 40 English accents, despite the model achieves 1-3% WER on clean, read speech. Finally, dysarthric speech represents another critical condition (Qian and Xiao 2023), with transcription accuracy highly varying with the severity of dysarthria and exceeding 60% WER with high levels of impairment (Almadhor et al. 2023). For MT, as for ASR, performance scores vary substantially depending on domain, language pair, and, most importantly, the amount of available training data. Notably, the OPUS-MT Dashboard26 shows that high-resource language pairs achieve consistently high performance not only for closely related languages, such as English-French or English-German for which BLEU scores often exceed 40 and COMET scores 85-90, but also for linguistically distant ones, such as English-Chinese or English-Japanese. In contrast, substantial drops in quality are observed mainly for very low-resource languages, such as Yoruba or Wolof, where the scarcity of training data imposes severe constraints on model performance. In conclusion, if ASR can be problematic due to the intrinsic difficulties of the task even in quite common conditions (meetings, non-native speakers), the main problem for MT does not lie in the task itself but in the availability of training data. This supports our choice to conduct the investigation including also rather poor ASR models but only good quality MT models. 23 Visited on 29 Aug 2025 24 https://www.chimechallenge.org/workshops/chime2024/ 25 https://huggingface.co/spaces/NOTSOFAR/CHiME8Challenge 26 https://opus.nlpl.eu/dashboard/, visited on 29 Aug 2025 30 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Appendix B: Cost of Generating Synthetic Sources To estimate the computational cost associated with generating the two types of synthetic sources, we executed each step of the respective pipelines independently on single Tesla A40 GPU with 48GB of memory. For each step, we measured the execution time of its core operations (excluding overheads such as model loading), and recorded the peak GPU memory usage. The evaluation was conducted on the esit Europarl-ST test set, which contains approximately 3 hours and 8 minutes of Spanish audio. Table B.1 Costs in terms of total execution time (excluding the model loading) and GPU memory usage of the various steps required for generating either ASR or BT sources. The size of models in terms of number of parameters is also shown size model 1 MT id step MADLAD batch size 1 32 1 32 1 1 32 3 L-Segmenter mwerSegmenter 1 4 Refinement Algorithm 1 (mBERT) 180M 1 SeamlessM4T Whisper 2 ASR NLLB 1.55B 2.3B 3.3B 3B - time 20m:30s 2m:48s 12m:47s 3m:02s 21m:26s 17m:45s 3m:43s 0.6s 17m:44s GPU memory 12.5GB 44.6GB 13.7GB 42.6GB 9.7GB 10.8GB 38.7GB - 1.1GB The resulting values, reported in Table B.1, allow to compare the resource demands of the ASRand BT-based pipelines: since the generation of BT sources requires only the MT step, while the generation of ASR sources requires all four steps, the BT pipeline results 2-7 times faster than the ASR pipeline, depending on the level of hardware parallelism exploitation within the GPU via larger batchsizes. One of the factors driving this significant difference is the high cost of the segment boundary refinement Algorithm 1 (Table B.1, id 4). It is important to note that the algorithm is not parallelizable by design, since shifting the right boundary of segment changes the left boundary of the next segment (steps 7-9), preventing their concurrent processing. Ultimately, given the same available hardware and parallelism exploitation, BT sources are significantly more costeffective to generate than ASR sources. Appendix C: mwerSegmenter vs. mweralign The first stage of our re-segmentation algorithm (Section 3.2) employs mwerSegmenter (Matusov et al. 2005) for the initial source/target alignment, code distributed only as executable binary without the source. Very recently, new implementation, named mweralign, was made available as Python module by (Post and Hoang 2025). To assess whether the adoption of the new code would substantially affect our results, we applied it to subset of experiments, specifically those concerning source resegmentation under controlled conditions (Section 5.3). Table C.1 corresponds to Table 10, except that the re-segmentation step is performed with mweralign instead of mwerSegmenter: the absolute difference of LASER scores ranges from 0 to 0.0025 (0.8822 vs. 0.8847), i.e., it is always lower than 0.3%. Such minimal difference is not expected to affect the overall validity of our findings. Table C.1 Same as Table 10 but using mweralign instead of mwerSegmenter BT for reseg mdld nllb segmentation manual XL-Segmentermweralign XLR-Segmentermweralign XL-Segmentermweralign XLR-Segmentermweralign MuST-C Europarl-ST 0.8550 0.8453 0.8560 0.8409 0. 0.9008 0.8847 0.9012 0.8758 0.9001 Appendix D: The Two-Stage Re-Segmentation Algorithm at Work Figure D.1 illustrates how the two-stage re-segmentation algorithm works on controlled example, where the objective is to realign the reference transcript, whose gold segmentation is disregarded, with the segments of the reference translation. The results of experiments conducted in this controlled mode are reported in Section 5.3. Figure D.1 Operation of the two-stage re-segmentation algorithm on real example under controlled conditions (re-segmentation of manual transcripts). Row 1 of the table contains the reference transcript, with the portion of text under focus highlighted in red violet; the original segmentation point is also shown but ignored (EoS), in order to evaluate the algorithms ability to correctly (re)identify it. Row 4 contains the segmented reference translation, while the back-translation of its segments is shown in Row 2. The contents of rows 1 and 2 represent the input to LSegmenter. It outputs the re-segmentation of the row 1 text as shown in row 3, derived from the \"WER labels\" assigned to Correct word matches, Substitutions, Insertions, and Deletions. Notably, near the EoS marker, whose placement derives from its position in the back-translation, the re-segmented source has two consecutive words, \"Are not\", Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Table E.1 COMET/MetricX correlations of systems scores averaged across language pairs in each test set synthetic source ASRwh ASRos ASRsm BTmd BTnl ASRwh-md ASRwh-nl ASRos-md ASRos-nl ASRsm-md ASRsm-nl COMET EP-ST MuST-C MetricX EP-ST MuST-C 0.999339 0.992541 0.999462 0.995429 0.994826 0.999869 0.999872 0.997725 0.997721 0.999957 0.999955 0.999225 0.987877 0.992222 0.999881 0.999865 0.999935 0.999935 0.999175 0.999188 0.999906 0.999909 0.999987 0.999851 0.999980 0.999951 0.999953 0.999997 0.999997 0.999943 0.999944 0.999997 0.999997 0.999865 0.998601 0.998012 0.999996 0.999987 0.999986 0.999983 0.999901 0.999901 0.999932 0.999936 that are not aligned to any word in the back-translation and are therefore labeled as Insertions. These words could, in principle, be assigned to the left segment, to the right segment, or split between them. In the example shown, both words are (incorrectly) assigned to the left segment. The boundary refinement stage is then applied, aligning words using the SimAlign algorithm. Working at the embedding level, SimAlign can identify correspondences not only between words in different languages but also in more robust manner than the Levenshtein distance minimization does. Rows 4 and 5 (partially) show the word alignments generated by SimAlign between the reference translation and the text obtained by concatenating the two segments of row 3 (Step 5 of Algorithm 1). Given this alignment, the number of cross-alignments (see the lower table in the figure) is minimized (#crossalignments=0) precisely when the moving EoS tag is placed to (correctly) reassign the two words \"Are not\" to the right segment. Appendix E: Systems Ranking Correlation As further confirmation of the reliability of our synthetic variants of COMET and MetricX, we verified the preservation of the ranking among the ST systems by calculating the correlations between the system scores on the entire test sets.For each synthetic variant (based on either ASRor BT-generated sources) of each source-aware metric, and for every test set and language pair, we computed the correlation between the scores of the six ST systems considered in our experiments as measured by the synthetic and by the corresponding standard version of the metric. The values shown in Table E.1 are averages of these correlations across all language pairs in the two datasets, and are reported both for the controlled setting, where the alignment between reference translations and source audio is known, and for the uncontrolled setting, where it is not. As we can see, the correlation is always higher than 0.99, with the only exception of OWSM with known audio-reference alignment on MuST-C. The generation of synthetic sources with SeamlessM4T with unknown alignment (ASRsm-md, ASRsm-nl) emerges as the only method that has correlations higher than 0.9999 on both metrics and datasets. However, correlations are very high for all methods. This enforces that not only systems 33 rankings are stable regardless of the source and condition, but also the differences between the scores obtained by different systems are not significantly impacted by the chosen method. Appendix F: COMET in the Wild (Realistic Conditions) Tables F.1 and F.2 show for COMET the scores that Tables 12 and 13 show for MetricX. They provide, for the two corpora, the correlations related to the various ASR sources generated in realistic conditions, and the corresponding gap recovery with respect to the shuffled sources. The ASR sources are those available at the end of the two-stage re-segmentation algorithm, whose quality is provided in rows labeled as shas+XLR-Segmenter of Table 11. Values have to be compared to those of CometBTmd and CometBTnl in Tables 5 and 6, which are shown in Tables F.1 and F.2 as well for ease of reading. Table F.1 Pearson correlations between the COMET scores of each system on MuST-C, computed (i) by using as the source the reference transcript (i.e., in the standard way), and (ii) by using as the source the automatically re-segmented automatic transcripts generated from automatic audio segmentation. Each correlation coefficient is calculated on all segments of all language pairs Plots in Figure F.1, histograms in Figure F.2, and Table F.3 are equivalent to Figures 4, 5 and Table14, respectively, related to COMET metric instead of MetricX. 34 Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Table F.2 Pearson correlations between the COMET scores of each system on Europarl-ST, computed in the standard way, i.e. by using as the source the reference transcript, and the COMET scores computed using as the source the automatically re-segmented automatic transcripts generated from automatic audio segmentation. Each correlation coefficient is calculated on all segments of all language pairs. Despite the weak dependence of COMET on the source, the empirical evidences observed for MetricX remain valid for COMET as well; among these, there is the 20% WER threshold that distinguishes whether the ASR source quality is sufficient or not to make it preferable to the BT source. Table F.3 For all possible comparisons between the COMET correlation with the ASR source and that with the BT source, computed on all language pairs of the two corpora and for all ST systems, the total number of wins per synthetic source type (ASR - whisper, owsm, seamless resegmented by XLR-Segmenter wrt madlad or nllb - or BT - madlad, nllb) is given here. Biased ASR COMETs, i.e., those of ST systems that are somehow involved in the generation of the ASR source of the metric, are excluded WERASR 20% WERASR >20% total corpus BT wins ASR wins ASR wins BT wins counts % counts % counts % counts % counts % counts % 36.4 63.6 75.0 98.7 1829 63.5 1051 36.5 91.8 2185 63.5 1255 36.5 BT wins ASR wins 10.7 21.9 20. 25.9 1.3 8.2 168 541 709 36 510 546 56 7 63 89.3 MuST-C Europarl-ST 1822 78.1 2122 79.5 total 204 356 35 Figure F.1 The plane of these two scatter charts is defined by the WER and MetricX scores of the ASR and BT sources, respectively. Considering all unbiased pairwise combinations between language pairs in each corpus and ST systems, they show where the cases in which it was preferable to use the ASR (on the left) or the BT (on the right) as source for the computation of COMET are placed in that plane. The total number of points is 3440, 2185 on the left (ASR wins, 63.5%), 1255 on the right (BT wins, 36.5%). random 1% change was applied to all values in order to avoid the overlapping of points and make all of them visible. Figure F.2 Considering all unbiased pairwise combinations of language pairs in each corpus and ST systems, these histograms illustrate the distribution of cases in which the standard COMET shows higher correlation with COMET using either the ASR or the BT as source input. The left plot reflects this distribution as function of transcription quality (WER), while the right plot does so with respect to (back-)translation quality (MetricX). Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST References Abdulmumin, Idris, Victor Agostinelli, Tanel Alumäe, Antonios Anastasopoulos, Luisa Bentivogli, Ondˇrej Bojar, Claudia Borg, Fethi Bougares, Roldano Cattoni, Mauro Cettolo, Lizhong Chen, William Chen, Raj Dabre, Yannick Estève, Marcello Federico, Mark Fishel, Marco Gaido, Dávid Javorský, Marek Kasztelnik, Fortuné Kponou, Mateusz Krubi nski, Tsz Kin Lam, Danni Liu, Evgeny Matusov, Chandresh Kumar Maurya, John P. McCrae, Salima Mdhaffar, Yasmin Moslem, Kenton Murray, Satoshi Nakamura, Matteo Negri, Jan Niehues, Atul Kr. Ojha, John E. Ortega, Sara Papi, Pavel Pecina, Peter Polák, Piotr Połec, Ashwin Sankar, Beatrice Savoldi, Nivedita Sethiya, Claytone Sikasote, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Brian Thompson, Marco Turchi, Alex Waibel, Patrick Wilken, Rodolfo Zevallos, Vilém Zouhar, and Maike Züfle. 2025. Findings of the IWSLT 2025 Evaluation Campaign. In Proc. of IWSLT, Vienna, Austria. Akiba, Yasuhiro, Marcello Federico, Noriko Kando, Hiromi Nakaiwa, Michael Paul, and Junichi Tsujii. 2004. Overview of the IWSLT Evaluation Campaign. In Proc. of IWSLT, Kyoto, Japan. Almadhor, Ahmad, Rizwana Irfan, Jiechao Gao, Nasir Saleem, Hafiz Tayyab Rauf, and Seifedine Kadry. 2023. E2E-DASR: End-to-End Deep Learning-based Dysarthric Automatic Speech Recognition. Expert Systems with Applications, 222:119797. Artetxe, Mikel and Holger Schwenk. 2019. Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond. Transactions of the Association for Computational Linguistics, 7:597610. Babu, Arun, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. 2022. XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale. In Proc. of Interspeech, Incheon, Korea. Baevski, Alexei, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: Framework for Self-Supervised Learning of Speech Representations. In Proc. of NeurIPS, Red Hook, US-NY. Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, US-MI. Barrault, Loïc, Ondˇrej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation (WMT19). In Proc. of WMT (Volume 2: Shared Task Papers, Day 1), Florence, Italy. Bentivogli, Luisa, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto Martinelli, Matteo Negri, and Marco Turchi. 2021. Cascade versus Direct Speech Translation: Do the Differences Still Make Difference? In Proc. of ACL-IJCNLP (Volume 1: Long Papers), Bangkok, Thailand. Bojar, Ondˇrej, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 Conference on Machine Translation (WMT18). In Proc. of WMT: Shared Task Papers, Brussels, Belgium. Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. Statistical Approach to Machine Translation. Computational Linguistics, 16(2):7985. Callison-Burch, Chris, Philipp Koehn, Christof Monz, Josh Schroeder, and Cameron Shaw Fordyce, editors. 2008. Proceedings of WMT. Association for Computational Linguistics, Columbus, US-OH. Callison-Burch, Chris, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the Role of Bleu in Machine Translation Research. In Proc. of EACL, Trento, Italy. Cheng, Yao-Fei, Hung-Shin Lee, and Hsin-Min Wang. 2021. AlloST: Low-Resource Speech Translation Without Source Transcription. In Proc. of Interspeech, Brno, Czech Republic. Cohen, J. 1988. Statistical Power Analysis for the Behavioral Sciences. Lawrence Erlbaum Associates. Conneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proc. of ACL, Online. Costa-jussà, Marta R., James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, 37 Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No Language Left Behind: Scaling Human-Centered Machine Translation. arXiv, 2207.04672. Dale, David and Marta R. Costa-jussà. 2024. BLASER 2.0: Metric for Evaluation and Quality Estimation of Massively Multilingual Speech and Text Translation. In Proc. of EMNLP: Findings, Miami, US-FL. Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv, 1810.04805. Di Gangi, Mattia A., Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. MuST-C: Multilingual Speech Translation Corpus. In Proc. of NAACL: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, US-MN. Do, Cong-Thanh, Shuhei Imai, Rama Doddipatla, and Thomas Hain. 2024. Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis. In Proc. of EUSIPCO, Lyon, France. Fan, Angela, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2021. Beyond English-Centric Multilingual Machine Translation. J. Mach. Learn. Res., 22(1). Fang, Qingkai and Yang Feng. 2023. Back Translation for Speech-to-text Translation Without Transcripts. In Proc. of ACL (Volume 1: Long Papers), Toronto, Canada. Freitag, Markus, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, Errors, and Context: Large-Scale Study of Human Evaluation for Machine Translation. Transactions of the Association for Computational Linguistics, 9:14601474. Freitag, Markus, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Frederic Blain, Tom Kocmi, Jiayi Wang, David Ifeoluwa Adelani, Marianna Buchicchio, Chrysoula Zerva, and Alon Lavie. 2024. Are LLMs Breaking MT Metrics? Results of the WMT24 Metrics Shared Task. In Proc. of WMT, Miami, US-FL. Freitag, Markus, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent. In Proc. of WMT, Singapore. Freitag, Markus, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 Metrics Shared Task: Stop Using BLEU Neural Metrics Are Better and More Robust. In Proc. of WMT, Abu Dhabi, United Arab Emirates. Fukuda, Ryo, Katsuhito Sudoh, and Satoshi Nakamura. 2022. Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation. In Proc. of Interspeech, Incheon, Korea. Gala, Jay, Pranjal Chitale, Raghavan, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar M, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023. IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages. Transactions on Machine Learning Research. Gulati, Anmol, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. ArXiv, 2005.08100. Haddow, Barry, Tom Kocmi, Philipp Koehn, and Christof Monz, editors. 2024. Proceedings of WMT. Association for Computational Linguistics, Miami, US-FL. Han, HyoJung, Kevin Duh, and Marine Carpuat. 2024. SpeechQE: Estimating the Quality of Direct Speech Translation. In Proc. of EMNLP, Miami, US-FL. Iranzo-Sánchez, J., J. A. Silvestre-Cerdà, J. Jorge, N. Roselló, A. Giménez, A. Sanchis, J. Civera, and A. Juan. 2020. Europarl-ST: Multilingual Corpus for Speech Translation of Parliamentary Debates. In Proc. of ICASSP. Jalili Sabet, Masoud, Philipp Dufter, François Yvon, and Hinrich Schütze. 2020. SimAlign: High Quality Word Alignments without Parallel Training Data using Static and Contextualized Embeddings. In Proc. of EMNLP: Findings, Online. Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Jurafsky, Daniel and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, with Language Models, 3rd edition. Online manuscript released on August 24, 2025: https://web.stanford.edu/jurafsky/slp3/. Juraska, Juraj, Daniel Deutsch, Mara Finkelstein, and Markus Freitag. 2024. MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task. In Proc. of WMT, Miami, US-FL. Juraska, Juraj, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023. MetricX-23: The Google Submission to the WMT 2023 Metrics Shared Task. In Proc. of WMT, Singapore. Kocmi, Tom and Christian Federmann. 2023. GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4. In Proc. of WMT, Singapore. Kocmi, Tom, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popovic, Mrinmaya Sachan, and Mariya Shmatova. 2024. Error Span Annotation: Balanced Approach for Human Evaluation of Machine Translation. In Proc. of WMT, Miami, US-FL. Koehn, Philipp. 2005. Europarl: Parallel Corpus for Statistical Machine Translation. In Proc. of MT Summit: Papers, Phuket, Thailand. Koehn, Philipp, Loïc Barrault, Ondˇrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri, editors. 2022. Proceedings of WMT. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates. Koehn, Philipp, Barry Haddow, Tom Kocmi, and Christof Monz, editors. 2023. Proceedings of WMT. Association for Computational Linguistics, Singapore. Kudugunta, Sneha, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. MADLAD-400: Multilingual And Document-Level Large Audited Dataset. In Proc. of NeurIPS, New Orleans, US-LA. Larionov, Daniil and Steffen Eger. 2025. BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched Prompting and Prompt Compression. arXiv, 2503.02756. Ma, Shuming, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, and Furu Wei. 2021. DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. arXiv, 2106.13736. Marie, Benjamin, Atsushi Fujita, and Raphaël Rubino. 2021. Scientific Credibility of Machine Translation Research: Meta-Evaluation of 769 Papers. In Proc. of ACL, Bangkok, Thailand. Mathur, Nitika, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics. In Proc. of ACL, Online. Mathur, Nitika, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondˇrej Bojar. 2020. Results of the WMT20 Metrics Shared Task. In Proc. of WMT, Online. Matusov, Evgeny, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005. Evaluating Machine Translation Output with Automatic Sentence Segmentation. In Proc. of IWSLT, Pittsburgh, US-PA. Moghe, Nikita, Arnisa Fazla, Chantal Amrhein, Tom Kocmi, Mark Steedman, Alexandra Birch, Rico Sennrich, and Liane Guillou. 2025. Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets. Computational Linguistics, 51(1):73137. Ng, Edwin G., Chung-Cheng Chiu, Yu Zhang, and William Chan. 2021. Pushing the Limits of Non-Autoregressive Speech Recognition. In Proc. of Interspeech, Brno, Czech Republic. Ouyang, Siqi, Rong Ye, and Lei Li. 2022. On the Impact of Noises in Crowd-Sourced Data for Speech Translation. In Proc. of IWSLT, Dublin, Ireland. Papi, Sara, Marco Gaido, Matteo Negri, and Marco Turchi. 2021. Dealing with training and test segmentation mismatch: FBK@IWSLT2021. In Proc. of IWSLT, Online. Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: Method for Automatic Evaluation of Machine Translation. In Proc. of ACL, Philadelphia, US-PA. Pearson, Karl. 1895. Note on Regression and Inheritance in the Case of Two Parents. Proc. of the Royal Society of London, 58:240242. 39 Peng, Yifan, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee weon Jung, and Shinji Watanabe. 2024. OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. In Proc. of Interspeech, Kos Island, Greece. Peng, Yifan, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee weon Jung, Soumi Maiti, and Shinji Watanabe. 2023. Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data. In Proc. of ASRU, Taipei, Taiwan. Popovic, Maja. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proc. of WMT, Lisbon, Portugal. Post, Matt. 2018. Call for Clarity in Reporting BLEU Scores. In Proc. of WMT: Research Papers, Belgium, Brussels. Post, Matt and Hieu Hoang. 2025. Effects of Automatic Alignment on Speech Translation Metrics. In Proc. of IWSLT, Vienna, Austria. Qian, Zhaopeng and Kejing Xiao. 2023. Survey of Automatic Speech Recognition for Dysarthric Speech. Electronics, 12(20). Radford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision. In Proc. of ICML, volume 202, Honolulu, US-HI. Rei, Ricardo, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task. In Proc. of WMT, Abu Dhabi, United Arab Emirates. Rei, Ricardo, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: Neural Framework for MT Evaluation. In Proc. of EMNLP, Online. Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler Wang. 2023. SeamlessM4T: Massively Multilingual & Multimodal Machine Translation. arXiv, 2308.11596. Sellam, Thibault, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning Robust Metrics for Text Generation. In Proc. of ACL, Online. Snover, Matthew, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. Study of Translation Edit Rate with Targeted Human Annotation. In Proc. of AMTA: Technical Papers, Cambridge, US-MA. Sperber, Matthias, Ondˇrej Bojar, Barry Haddow, Dávid Javorský, Xutai Ma, Matteo Negri, Jan Niehues, Peter Polák, Elizabeth Salesky, Katsuhito Sudoh, and Marco Turchi. 2024. Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations, Automatic Metrics, and Segmentation. In Proc. of LREC-COLING, Torino, Italia. Srivastav, Vaibhav, Somshubra Majumdar, Nithin Koluguri, Adel Moumen, Sanchit Gandhi, et al. 2023. Open Automatic Speech Recognition Leaderboard. https://huggingface.co/spaces/hf-audio/open_asr_leaderboard. Tang, Yuqing, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. Multilingual Translation with Extensible Multilingual Pretraining and Finetuning. arXiv, 2008.00401. Tiedemann, Jörg, Mikko Aulamo, Daria Bakshandaeva, Michele Boggia, Stig-Arne Grönroos, Tommi Nieminen, Alessandro Raganato Yves Scherrer, Raul Vazquez, and Sami Virpioja. 2023. Democratizing Neural Machine Translation with OPUS-MT. Language Resources and Evaluation, 58:713755. Cettolo, Gaido, Negri, Papi, Bentivogli Source-Aware MT Metrics for ST Tsiamas, Ioannis, Gerard Gállego, José Fonollosa, and Marta Costa-jussà. 2024. Pushing the Limits of Zero-shot End-to-End Speech Translation. In Proc. of ACL: Findings, Bangkok, Thailand. Tsiamas, Ioannis, Gerard I. Gállego, José A. R. Fonollosa, and Marta R. Costa-jussà. 2022. SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. In Proc. of Interspeech, Incheon, Korea. Wang, Weiqing, Kunal Dhawan, Taejin Park, Krishna C. Puvvada, Ivan Medennikov, Somshubra Majumdar, He Huang, Jagadeesh Balam, and Boris Ginsburg. 2024. Resource-Efficient Adaptation of Speech Foundation Models for Multi-Speaker ASR. In Proc. of SLT, Macao, China. Zeineldeen, Mohammad, Jingjing Xu, Christoph Lüscher, Wilfried Michel, Alexander Gerstenberger, Ralf Schlüter, and Hermann Ney. 2022. Conformer-Based Hybrid ASR System For Switchboard Dataset. In Proc. of ICASSP, Singapore. Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In Proc. of ICLR, Addis Ababa, Ethiopia. Zouhar, Vilém, Pinzhen Chen, Tsz Kin Lam, Nikita Moghe, and Barry Haddow. 2024. Pitfalls and Outlooks in Using COMET. In Proc. of WMT, Miami, US-FL."
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler"
    ]
}