{
    "paper_title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
    "authors": [
        "Wenhao Yan",
        "Sheng Ye",
        "Zhuoyi Yang",
        "Jiayan Teng",
        "ZhenHui Dong",
        "Kairui Wen",
        "Xiaotao Gu",
        "Yong-Jin Liu",
        "Jie Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 5 0 9 5 0 . 2 1 5 2 : r SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations Wenhao Yan1 Sheng Ye1 Zhuoyi Yang1,2 Jiayan Teng1,2 ZhenHui Dong1 Kairui Wen1 Xiaotao Gu2 Yong-Jin Liu1 Jie Tang1 1Tsinghua University 2Z.ai Figure 1. We propose SCAIL, studio-grade character animation framework. SCAIL enables high-fidelity character animation under diverse and challenging conditions, including large motion variations, stylized characters, and multi-character interactions."
        },
        {
            "title": "Abstract",
            "content": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from driving video to reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity anIn this work, we present SCAIL (Studio-grade imations. Character Animation via In-context Learning), framework designed to address these challenges from two key innovations. First, we propose novel 3D pose representation, providing more robust and flexible motion signal. Second, we introduce full-context pose injection mechanism within diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop curated data pipeline ensuring both diversity and *Equal contribution. Work done during internship at Z.ai. Project leader. Corresponding author: jietang@tsinghua.edu.cn, liuyongjin@tsinghua.edu.cn quality, and establish comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism. Code and model will be publicly available at our project page. 1. Introduction High-fidelity character animation has tremendous potential for film production. Conventional filmmaking pipelines rely on complicated workflowinvolving motion capture, rigging, renderingthat demands expensive hardware and significant expert labor. Recently, the emergence of video generation models [2, 13, 36, 47] introduces new paradigm: given reference image and driving video, such models can directly synthesize photorealistic animation that follows the target motion in driving video, dramatically lowering the production barrier for high-quality character animation. Similar to real-world production pipelines, current methods [4, 15, 30, 31, 38, 44, 49, 52] typically begin by extracting skeletal motion sequences from the driving video as form of motion capture, and then inject this information into video generation model to perform rig1 ging and rendering. However, in studio-grade production setting, these methods often struggle with challenging scenarios, such as complex motions (e.g., turning, rolling, flipping), multi-person interactions (e.g., dancing, hugging, fighting), and cross-domain animation where the reference and driving subjects differ significantly in appearance or body shape. As result, generated videos frequently exhibit artifacts including distorted appearance, implausible body poses, and incorrect occlusions. These limitations reveal that current skeletal pose representations and generation models fail to adequately capture 3D structure of driving motions, inter-character spatial and occlusion relationships, and temporal correlations of motion sequences. To address the above limitations, we present SCAIL, framework that revisits two core technical bottlenecks: (1) how to build pose representation that effectively bridges driving and generated videos with unambiguous and accurate motion encoding, and (2) how to inject pose control in way that enables the model to capture spatiotemporal motion structures. First, for motion representation, prior works [4, 15, 17, 31, 38, 49] typically rely on 2D skeletons (extracted from DWPose [46], ViTPose [43], etc.), which suffer from noisy predictions in complex motions and cannot encode occlusion. Some recent methods [37, 52] adopt SMPL-based [20] controls, which offer strong 3D human priors but have limited augmentation flexibility and cause identity leakage due to person-specific shape parameters. To address these issues, we propose novel 3D pose representation. Specifically, we connect estimated 3D human keypoints according to skeletal topology and represent bones as spatial cylinders. The resulting 3D skeleton is rasterized to obtain 2D motion guidance signals. Building on this representation, we further design pose augmentation and retargeting strategies, enabling seamless scaling across diverse characters and scenarios. Second, regarding pose injection, common approach in diffusion transformer (DiT) [23] model is to concatenate conditions channel-wise [4, 51]. However, this only provides local motion cues without capturing global motion dependencies. To overcome this, we propose full-context pose injection mechanism within DiT-based architecture. This design allows the model to attend to the entire pose sequence when generating each frame, enabling it to reason about motion context across time, and better capture high-level motion semantics. As result, our method supports robust motion transfer across diverse body types and visual domains. To achieve studio-level generation quality, we build data curation pipeline ensuring both diversity and fidelity. Our pipeline integrates multi-domain sources covering various character types and motion styles. During curation, automatic filtering removes low-quality clips based on human presence and clarity, and motion-based metrics are further computed to identify motion-rich samples. final manual stage further selects finetuning set of superior quality. We also observe that current evaluations lack comprehensive benchmark that adequately reflects production-level requirements. To address this gap, we propose StudioBench. It consists of two parts: the first evaluates motion adherence and structural integrity under complex singleand multi-person actions, while the second measures model generalization when the reference image and driving video differ in identity or domain. Our Studio-Bench covers challenging real-world cases and provides realistic and rigorous measure of studio-level generation capability. Our main contributions can be summarized as follows: (1) We propose scalable 3D pose representation that unifies the strengths of 2D skeleton and SMPL, serving as robust motion-driving signal. (2) We inject driving-pose controls via in-context reasoning to enable effective spatiotemporal motion modeling, yielding superior results in complex and multi-person scenarios. (3) We construct pipeline for curating high-quality, diverse training data, and establish comprehensive Studio-Bench for systematic evaluation. (4) Our SCAIL framework achieves state-of-the-art performance over existing baselines, and advances character image animation toward production-level readiness. 2. Related Work 2.1. Diffusion Models for Video Generation Diffusion models [11, 28] effectively overcome the training instability and mode collapse of generative adversarial networks (GANs) [8], and emerge as the dominant paradigm in visual content generation. The success of the Stable Diffusion (SD) [24, 25] in image synthesis has naturally inspired the extension from image to video. Subsequently, the Diffusion Transformer (DiT) [23] architecture, combined with RoPE [29] for position encoding, offers superior modeling capability and scalability, becoming the leading backbone for high-quality video generation [18, 22, 36, 47]. More recently, diffusion-based video generation has advanced toward controllability, enabling control over camera viewpoints [9, 42], motion trajectories [6, 50], and scene structures [5, 19, 41]. In this work, we focus on character video generation with precise pose control. 2.2. Character Image Animation Character image animation aims to generate photorealistic and temporally coherent video, where the appearance remains consistent with given reference image and the motion follows driving video. AnimateAnyone [15] extracts 2D skeletons from the driving video as motion guidance, and designs Pose Guider and ReferenceNet modules to control motion and appearance. Subsequent works explore various improvements. Champ [52] integrates multiple motion signals, including depth maps, SMPL [20] normals, 2 Figure 2. Overview of the proposed 3D-consistent pose. For scaling implementation, we take the clavicle or the pelvis as the central reference, applying scaling from proximal to distal along each limb in bones set B. Aug() denotes augmentation in training, Ret() denotes retargeting in inference, and ref = {Pref 1 } denotes estimated 2D keypoints in the reference image. We further incorporate hand and face controls by overlaying 2D hand and face keypoints onto the rendered sequences, and align them with the projection of 3D joints during augmentation or retargeting. For better clarity, we omit the drawing process of 2D hand and face in the figure. and 2D skeletons. Animate-X [30] introduces skeleton augmentation strategy to enable animation of anthropomorphic characters with large body-ratio discrepancies. Moreover, MimicMotion [49] address hand and facial distortions via regional loss. DanceTogether [3] extends this line of research to multi-person animation. UniAnimate-DiT [38], VACE [17] and Wan-Animate [4] replace U-Net with Transformer as the denoising backbone, significantly improving the generation quality. Despite progress, current models still struggle with complex scenarios and cross-pair animations, which hinders their deployment in real production. 3. Method 3.1. Preliminaries Latent Diffusion Models. Latent diffusion models [1, 25] reduce the computational cost of pixel-space diffusion [11, 12] by operating in compressed latent space. Given an input video x, pretrained VAE encoder [36] first maps it into latent representation z0 = E(x). During the forward diffusion process, Gaussian noise is progressively added over timesteps, formulated as: q(ztzt1) = N(cid:0)zt; (cid:112)1 βt zt1, βtI(cid:1), (1) where βt denotes the noise schedule. The denoising model εθ(zt, t, c) learns to predict the added noise conditioned on optional input (e.g., pose, text), and is optimized by: = Ezt, εN (0,I) (cid:2)ε εθ(zt, t, c) (cid:3). (2) 2 Diffusion Transformer (DiT). Traditional diffusion models typically rely on U-Net as denoising backbones. Recent works propose Diffusion Transformer (DiT) [23] architecture, which supports variable input resolution and sequence length. The DiT combines the generative power of diffusion models with the flexibility of Transformers [34], enabling better scalability and stronger modeling capacity. 3.2. 3D-Consistent Pose Conditioning 3D-Consistent Pose Representation. Recent methods [4, 15, 17, 30, 38] that employ 2D keypoint-based skeletons as motion controls perform well on simple actions. However, they often struggle under complex studio-level production scenarios, leading to abnormal human kinematics and violation of motion physics. 2D keypoints cannot encode 3D occlusion, making it difficult for models to generate realistic motion videos from incomplete or ambiguous pose signals. Another challenge arises from the discrepancy between the reference image and driving pose, which often differ in body shape or identity. Previous attempts [30, 38] mitigate the discrepancy through 2D skeleton scaling in training or heuristic retargeting in testing, yet 2D-derived adaptation inherently suffers from deformation 3 Figure 3. Overview of SCAILs model architecture. SCAIL builds upon I2V model and incorporate pose control as an explicit context for the model to learn spatial-temporal motion. To accommodate to the training setting where reference image and video input are sampled from different parts of the video, we modify the I2V models input structure by concatenating the reference image at the beginning of the sequence and initiating generation from = 1, using the original I2V pattern to inject the reference CLIP feature. To help the model better distinguish the conditional tokens and the noisy video sequence, we leverage the original mask mechanisim of Wan-I2V model architecture, applying an all-one mask for the reference image and the driving sequence, and an all-zero mask for the noisy video sequence. due to inconsistency with 3D motion dynamics and further amplifies estimation noise and inaccuracies. We argue that an ideal driving pose representation should perceive depth and occlusion, remain identity-agnostic, and retain the potential to scale to multi-person settings. To achieve that, we employ NLFPose [26] to estimate 3D body keypoints = {Ji,j}T,N i=1,j=1, where Ji,j denotes the predicted 3D joint at frame i, and connect them according to the skeletal topology in 3D space. We discard SMPL meshlike representations, which are inherently human-specific and offer limited controllability for augmentation or retargeting, often resulting in identity leakage. To retain occlusion and depth cues lost in 2D projection, we render 3D skeletons as cylindrical segments to provide motion-driving signals. To address the disparity between the reference image and the driving pose, the framework should enable precise, identity-agnostic motion control and transfer motion naturally respecting the initial conditions present in the reference image. To achieve these objectives, we introduce two-stage optimization strategy leveraging the 3D prior of our representation. During inference, we perform spatial alignment using projection loss (Lproj) to remap the predicted 3D driving poses to the reference 2D frame. This is to help the model anchor where the motion should be transferred to. Specifically, we optimize the following objective to refine the camera projection matrix Cam 3 : Lproj ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 (cid:13) (cid:13) (cid:13)Π (cid:16) Cam 33J0,j (cid:17) Pref (cid:13) 2 (cid:13) (cid:13) 2 , (3) 4 where Π() is the perspective projection, is joint number, and Pref is the estimated 2D joints of the reference frame. During training, we decouple the augmentation process to body figure rescale and camera manipulation. To achieve identity decoupling, we synthesize set of per-subject scale parameters Si and randomly sample the parameters during training to simulate different body proportions. Since the underlying estimated skeleton topology and relative bone directions remain consistent, the scaled skeletons still faithfully represent the same motions. As the alignment stage in inference may introduces approximation error, we also exert augmentation on camera parameters Cam33 to enhance the models robustness towards camera variance. Since in training we sample reference image from other parts of the video, we restrict the augmentation scale in camera manipulation within limited range to provide anchor for where motion happens, so that our method can strike balance between the controllability of location and natural motion transfer in further generation. Full-Context Driving Pose Injection. Our model builds upon DiT-based Image-to-Video (I2V) model [36] and inject pose control signals for motion guidance. The common practice for pose information injection is through channel concat [4, 51] or using pose adapter [38], where the driving pose sequence is embedded and added to the noisy video latents. We first implement the channel concat approach as illustrated in Figure 4, following [51]. While this method shows decent pose-following capability in generation, we find it tend to generate unnatural human pose when the motion is complex despite conditioned on accurate and unambiguous pose. typical example is the turning motion, where the model often fails to correctly distinguish between front and back views, generating awkward postures. We assume that this limitation stems from the per-frame addition scheme, which fails to provide sufficient temporal context for motion understanding. Since motion is inherently sequential, many actions can only be interpreted correctly within temporal context. Therefore, we design new strategy termed full-context pose injection, which enables the model to reason over the entire pose sequence. Specifically, we directly concatenates conditional pose tokens with noisy video tokens to facilitate spatial-temporal interactions between the two modality. As shown in Figure 4, this strategy handles motion correctly in complex scenarios. To mitigate the sequence length increase introduced by our context-aware injection scheme, we apply spatial downsampling to the pose video. Empirically, we find that with 2 downsampling the pose following ability is nearly unaffected. Thus, we adopt this as the default setting in both training and testing, achieving balance between generation quality and efficiency. Shifted RoPE for Pose Context. Conventional pipelines typically extract poses that are spatially aligned with the original video. However, in our setting, augmentations such as scaling and camera transformations often prevent the extracted pose context to be spatially aligned with the driving video. To address this misalignment, we introduce Pose-Shifted RoPE mechanism, which enables the model to effectively retrieve driving signals from the augmented full-sequence pose representation. As illustrated in Figure 3, conventional 3D RoPE encodes position in the form (t, h, w), where is the temporal frame index and h, denote spatial dimensions, each starting from zero. In our design, Pose-Shifted RoPE applies shift along the width dimension specifically to pose representations. The positional vectors for each driving pose token are defined as: Pos = [t, h, Wmax : Wmax + shiftW ], (4) where shiftW is constant shift magnitude and Wmax is the width of the reference image. As show in Figure 3, this design helps the model to distinguish driving pose tokens from reference image tokens and noisy video tokens in conjunction with the modified I2V Mask and reference image token injection strategy, enhancing the overall performance. Considering the modality gap between the noisy video tokens and the driving pose tokens, we observe that the model performs best particularly when the shift distance is relatively large constant. To accommodate different downsampling ratios for pose context, mean-pooling on the 3D-RoPE [29] frequencies is performed according to the applied ratio. Figure 4. Exploration of different strategies for pose injection. 3.3. Data Collection Data Source. Our dataset is composed of three primary data sources: (1) samples retrieved from our internal base model training data and other downstream tasks, (2) large collection of high-resolution dance videos from Bilibili and YouTube, and (3) additional sports videos such as gymnastics and figure skating. To ensure diversity, we maintain certain proportion of stylized content, including 3D and 2D animations from source (1), as well as MMD and Live2D animations from source (2). Filtering and Pose Extraction Pipeline. We develop curated pipeline to construct high-quality dataset for character animation. To meet the requirements for generating studio-grade character animation, we have established the following criteria for data filtering: (1) the character should be the primary focus of each frame, (2) the person should exhibit explicit motion, and (3) the pose should be visually complete, covering either the upper body or the full body. We first detect human presence using YOLO [33] and discard clips where characters are not the main subjects. Next, 2D keypoints are extracted using DWPose [46], retaining only half-body or full-body videos. After this stage, multi-person videos can be further separated based on the number of bounding boxes detected by YOLO [33]. We further employ large language model [7] to analyze captions, complementing the complex rule-based logic of multi-person motion bounding box checking. For multiperson videos, we employ Samurai [45] to track and split the mask of each character, generating multiple singlehuman video splits. Subsequently, 3D keypoints are extracted using NLFPose [26] for each video that contain one main character. To meet the motion criteria, we compute the motion speed from the estimated 3D keypoints and discard samples with limited motion, resulting in clean and motion-rich training set. The motion speed is calculated as: ="
        },
        {
            "title": "1\nT",
            "content": "T 1 (cid:88) (cid:88) t=1 (cid:13) (cid:13)Jt+1,j Jt,j (cid:13) (cid:13) (5) 5 Figure 5. The data curation pipeline. We perform character filtering and motion-speed filtering to construct high-quality training data. where refers to the joints in the screen and Jt = Jt Jroot denotes the 3D human joint positions relative to the body center at frame t. Finally, The 3D skeletons are represented as cylindrical segments in 3D space and rendered using the Taichi [16] renderer to preserve spatial relationships. For multi-person data, each split is independently processed by NLFPose to estimate 3D skeletons, which are then composed together in 3D space and rasterized on the 2D canvas. This procedure preserves inter-person occlusion relationships, benefiting from NLFPoses accurate depth estimation. In practice, we find that our proposed multi-stage pipeline provides more accurate estimation than direct multi-person pose estimation methods like PromptHMR [39], especially in wild cases involving complex interaction. We collect around 250K high-quality motion-rich videopose pairs using the filtering pipeline, among which 20K pairs featuring multiple characters. To further enhance models performance on complex motions, we select 12K samples with the highest motion speed from dance, general motion, and obtain finetuning set of 4,000 high-dynamic videos with minimal blur after manual clarity inspection. 3.4. Studio-Bench for Evaluation To evaluate the model capabilities under studio-grade standards, we propose Studio-Bench, new evaluation benchmark tailored for Studio-Grade character animation. Previous evaluations [15, 31, 38, 44, 49, 52] primarily focus on simple actions, which fail to capture the challenges present in film production, such as complex and dynamic motions, multi-person interactions, and cross-domain animations. Specifically, our benchmark consists of two parts: SelfDriven and Cross-Driven. The first part evaluates motion adherence under complex actions. We examine whether the generated videos maintain correct body structure during large-scale motions, and whether the model can accurately capture inter-person relationships in multi-character scenes, totally containing 130 clips. Quantitative metrics can be computed by directly comparing generated results with paired ground-truth videos for this subset. The second part measures the models transferability when the reference image and driving video differ in identity or domain. We manually select set of real reference images to test generalization, and further construct additional reference images using an image generation tool [27] to introduce variations in style and body shape, obtaining 120 single-character pairs and 10 multi-character pairs. As we simulate in-thewild reference-driving discrepancy, pose retarget should be enabled for single-character pairs in this subset. We incorporate the retarget logic of [38] for VACE [17] which do not natively support retargeting. For this subset, we conduct user study to assess the generation quality. All quantitative results on the subset are based on the 120 single-character pairs to ensure fairness considering some baselines incompatibility with multi-character settings. Evaluation samples are strictly excluded from the training data. 4. Experiments 4.1. Implementation Details We train two versions (1.3B and 14B) of our model. The 1.3B model is finetuned from the Wan2.1-1.3B-Fun-Inp backbone on our pretraining dataset for 6,000 steps with batch size of 96 and learning rate of 1e-5, using 32 NVIDIA H100 GPUs for approximately two days. For the larger 14B model, we finetune it from the Wan2.1-I2V-14B backbone in two stages: during the pretraining stage, we train for 8,000 steps with batch size of 96 at learning 6 Methods Self-Driven Animation Cross-Driven Animation PSNR SSIM LPIPS FVD Mot-Acc Kin-Consis Phy-Consis ID-Sim UniAnimate-DiT [38] VACE [17] Wan-Animate [4] SCAIL-14B (Ours) 17.79 16.73 18.54 19.22 0.637 0.588 0. 0.660 0.242 0.263 0.221 0.206 362.27 264.71 187.61 176.16 2.5% 9.2% 35.0% 53.3% 1.7% 14.2% 28.3% 55.8% 0.8% 18.3% 24.2% 56.7% 1.7% 32.5% 20.0% 45.8% Table 1. Quantitative comparison for SCAIL-14B and baselines. All compared methods are built upon 14B Wan [36] foundation models. iors like hovering in midair. (4) Identity Similarity, measuring the consistency of the subjects appearance with the reference image. We convey detailed blinded user study to collect these metrics, letting users vote for the best in baseline comparison. We also conduct Win/Tie/Lose study to evaluate our performance between two commonly used open-source frameworks, as well as the closed-source commercial product Viggle [35], omitting UniAnimate-DiT[38] due to lack of competitiveness under studio-grade demandings. Viggle is widely believed to rely on 3D foundation model rather than video diffusion and can be strong baseline for frame-by-frame motion accuracy under complex scenarios. To ensure methodological rigor, we note that the user study for selecting the best-performing model in Table 1 and the win/tie/lose evaluation in Figure 6 were conducted on different batches of participants, allowing the two sets of results to serve as cross-validations. Results in Table 1 show that our model performs better than current open-source works on Studio-Bench. Under the best-model selection scheme, our method shows significant improvements across all metrics, and the win/tie/lose study further highlights its advantages in motion-transfer accuracy over other video diffusion methods and state-of-the-art motion naturalness in the metrics of Kinesiology Consistency and Physical Consistency. 4.3. Qualitative Evaluation Figure 7 shows qualitative comparisons across both selfdriven and cross-driven animation settings for singlecharacter animation. In self-driven cases, SCAIL produces animations with stable structure and accurate limb articulation, particularly in challenging motions such as spinning and bending. Baseline methods often exhibit incomplete pose following or limb abnormalities such as incorrect legs in ballet dancing. In cross-driven cases, our method still demonstrates stronger motion transferability even when large discrepancies exist between the character image and the first frame of the driving video. In contrast, prior methods produce artifacts such as body-shape drift, inconsistent textures, and structural distortions like armbody interpenetration during acrobats. Figure 6. User study for comparing our model with popular community and commercial projects. rate of 1e-5; after convergence, we perform an additional finetuning stage for 400 steps with the same batch size and reduced learning rate of 4e-6. Training of the 14B model is conducted on 128 NVIDIA H100 GPUs for over four days with sequence parallelism enabled. All models are optimized using AdamW [21]. During inference, we set the classifier-free guidance (CFG) scale [10] to 4, offering favorable balance between pose following and video fidelity. 4.2. Quantitative Evaluation We conduct quantitative comparison with state-of-the-art methods on our proposed Studio-Bench. For self-driven part we employed several widely-used quantitative metrics, including PSNR [14], SSIM [40], LPIPS [48], and FVD [32]. To evaluate the generated results in the second subset, we design four metrics: (1) Motion Accuracy, which measures how faithfully the generated motion follows the driving signal in frame-by-frame manner. (2) Kinesiology Consistency, evaluating whether joint rotations and body movements remain anatomically feasible and temporally coherent, penalizing sudden twists or physically impossible poses that break natural motion continuity. (3) Physical Consistency, assessing whether the generated motions comply with basic physical constraints such as gravity, support, and momentum conservation, penalizing unrealistic behav7 Figure 7. Qualitative comparison for single-character animation. Rendered pose in Cross-Driven Animation are omitted for clarity. Zoom in for better visualization. Figure 8. Qualitative comparison for dual-character animation. Zoom in for better visualization. 8 Methods Self-Driven Animation PSNR SSIM LPIPS FVD 18.08 SCAIL-1.3B (Ours) SCAIL-1.3B w/o P-RoPE 17.79 SCAIL-1.3B w/o full-context 17.69 17.01 MimicMotion [49] 0.639 0.637 0.626 0.630 0.249 228.62 0.280 269.35 0.262 263.63 0.314 334.24 Table 2. Ablation study conducted on SCAIL-1.3B. The w/o fullcontext denotes using the channel-concatenation strategy. cies and reason plausible human poses. Our architecture is designed to enable extensive interaction between noisy video tokens and pose tokens, and experiments show the robustness this design achieves when facing pose-estimation errors. Specifically, case (2) shows that through this strategy, when the estimator misidentifies the left part and the right part and extracts posture like forward lunge in running driving video, our model with full-context pose injection can still generate correct running posture based on the global motion, as conditions from the reference frame and estimations in other frames clearly reflect the semantic context of running along the river. Ablation on Pose-Shifted RoPE. Removing Pose-Shifted RoPE (P-RoPE) leads to noticeable performance degradation  (Table 2)  , especially in LPIPS and FVD, confirming that precise pose-aware positional encoding is crucial for maintaining image quality and temporal motion smoothness. Figure 9 shows that P-RoPE can strengthen the correspondence between motion cues and spatial structure, resulting in more accurate hand articulation and more reasonable foot grounding. Results from case (1) demonstrate that with full-context injection, P-RoPE yields the strongest disentanglement of character identity and pose guidance, enabling the model to faithfully preserve the subjects appearance while following motion patterns. 5. Conclusion In this work, we present SCAIL, novel framework for studio-grade character image animation. By introducing scalable and robust 3D pose representation and leveraging novel full-context pose injection with shifted RoPE, we enhances spatiotemporal reasoning within DiT architecture, allowing the model to generate structurally accurate and temporally consistent animations under challenging scenarios. With curated data pipeline and comprehensive StudioBench, we push character animation towards productionlevel standards. Experiments show that SCAIL achieves state-of-the-art performance in both self-driven and crossdriven animation. We believe this work provides solid step toward practical, production-ready character animation. Figure 9. Visualization of ablation study. Figure 8 presents qualitative comparisons for multicharacter animations, where issues caused by occlusion and estimation error become more pronounced. In relatively clean cases without severe occlusionssuch as the dancing scenario in the castlebaseline methods mainly struggle with identity and orientation ambiguities during character turning, similar to their failure patterns in single-character animation. However, as complexity increases, as illustrated by the last two cases of Figure 8, pose overlapping makes it difficult for the baselines to correctly distinguish human limbs, causing the models to generate severe artifacts where body parts of different characters merge during motion. Furthermore, in multi-character settings, both 2D and 3D pose estimations may occasionally fail due to heavy occlusions (in the shown case they detect only the front character in certain frame), posing significant challenges for temporal and spatial reasoning. In contrast to the baselines which are unable to infer coherent actual poses in such scenarios, our model captures global pose-identity relationships and generates plausible results. These results highlight SCAILs strong ability to handle both large motion variations and cross-domain appearance gaps, producing videos that are more natural and visually appealing. 4.4. Ablation Study We conduct ablation studies on SCAIL-1.3B model to evaluate the contribution of each component. For reference, we also compare against the U-Net based MimicMotion [49], highlighting the advantage of our DiT-based architecture. In the main paper we mainly focus on ablations on model architecture, and more ablations on the pose condition would be provided in Appendix A2. Ablation on Full-Context Driving Pose Injection. As shown in Table 2, replacing channel concatenation with the proposed full-context injection consistently improves all the metrics. We observe that generated videos exhibit fewer artifacts like limb tearing and structural collapse (shown in Figure 9), indicating that our context-aware injection strategy enables the model to capture global motion dependen-"
        },
        {
            "title": "References",
            "content": "[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 3 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [3] Junhao Chen, Mingjin Chen, Jianjin Xu, Xiang Li, Junting Dong, Mingze Sun, Puhua Jiang, Hongxiang Li, Yuhang Yang, Hao Zhao, et al. Dancetogether! identity-preserving multi-person interactive video generation. arXiv preprint arXiv:2505.18078, 2025. 3 [4] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055, 2025. 1, 2, 3, 4, 7 [5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 73467356, 2023. [6] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multientity motion in video generation. In The Thirteenth International Conference on Learning Representations, ICLR, 2025. 2 [7] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. 5 [8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [9] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. 1 [14] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. 7 [15] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 1, 2, 3, 6 [16] Yuanming Hu. Taichi: An open-source computer graphics library. arXiv preprint arXiv:1804.09293, 2018. 6, 1 [17] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3, 6, 7, 1 https : / / kling . Kling. kuaishou.com/en, 2024. 2 [18] Kuaishou AI Team. [19] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: layout-guided multi-view driving scenarios video generation In European Conference on with latent diffusion model. Computer Vision, pages 469485. Springer, 2024. 2 [20] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. ACM Trans. Graph., 34(6):248:1 248:16, 2015. 2 [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR, 2019. 7 [22] OpenAI. Sora. sora/, 2024. 2 https : / / openai . com / index / [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3 [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, ICLR, 2024. 2 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [26] Istvan Sarandi and Gerard Pons-Moll. Neural localizer fields for continuous 3d human pose and shape estimation. Advances in Neural Information Processing Systems, 37: 140032140065, 2024. 4, 5 [27] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. 6 [28] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR, 2021. 10 [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2, 5 [30] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. In The Thirteenth International Conference on Learning Representations, ICLR, 2025. 1, 3 [31] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2109621106, 2025. 1, 2, 6 [32] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [33] Rejin Varghese and Sambath. Yolov8: novel object detection algorithm with enhanced performance and robustness. In 2024 International conference on advances in data engineering and intelligent computing systems (ADICS), pages 16. IEEE, 2024. 5 [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [35] Viggle AI. Viggle. https://viggle.ai/, 2024. 7 [36] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, 4, 7 [37] Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. arXiv preprint arXiv:2405.18156, 2024. 2 [38] Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya and Nong Unianimate-dit: Human image animation with arXiv preprint Zhang, Changxin Gao, Yuehuan Wang, Sang. large-scale video diffusion transformer. arXiv:2504.11289, 2025. 1, 2, 3, 4, 6, 7 [39] Yufu Wang, Yu Sun, Priyanka Patel, Kostas Daniilidis, Michael J. Black, and Muhammed Kocabas. Prompthmr: In Proceedings of the Promptable human mesh recovery. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11481159, 2025. 6 [40] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [41] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video generation using textual and structural guidance. IEEE Transactions on Visualization and Computer Graphics, 31 (2):15261541, 2024. 2 [42] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 2 [43] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35:3857138584, 2022. 2 [44] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 1, 6 [45] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. [46] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. 2, 5, 1 [47] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, ICLR, 2025. 1, 2 [48] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [49] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with In International Conferconfidence-aware pose guidance. ence on Machine Learning, 2025. 1, 2, 3, 6, 9 [50] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 20632073, 2025. 2 [51] Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Weihua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Simple yet strong baseline towards controllable character animation in the wild. arXiv preprint arXiv:2504.14977, 2025. 2, 4 [52] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. 1, 2, 6 SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations"
        },
        {
            "title": "Supplementary Material",
            "content": "A1. Details on Pose Conditioning Rendering Details. To preserve spatial relationships between joints, we represent the human skeleton using cylindrical structures. For efficient rendering of multiple skeletons, we employ 3D rendering pipeline that first converts the cylinders into spatial voxels, followed by ray marching implemented via [16]. This strategy is highly optimized for modern GPUs, introducing negligible computational overhead. To facilitate person discrimination in multi-person motion sequences, we assign distinct color schemes to each individual. This enables the model to directly learn how to distinguish characters from the representation. We observe that this design effectively alleviates identity switching when the positions of the people interchange. Augmentation Details. As our augmentation strategy is designed to maximally preserve the original motion, we use high augmentation rate of 0.8 to achieve the balance between the pose-following accuracy and motion transfer robustness. During training, the overlaid 2D hand and face keypoints extracted by DWPose [46] will go through additional augmenation after the 3D adaptation process where 2D keypoints are shifted to match the reformed 3D skeleton after body rescaling and camera manipulation. This step is to minimize the unintended influence of the 2D facial and hand signals on the 3D pose representation. Performance Tradeoff. For the injection of the representation in full-context settings, we observe that the rendered pose sequence is relatively sparse as most frame areas consist of non-informative black pixels. In our method, spatial pooling to pose sequence can serve as simple workaround to effectively reduce the contextual pose tokens to 1/4 and preserves accurate pose-following capability. In addition, full-context pose injection introduces no new parameters except the additional patchify layer to the original model, offering more streamlined architecture compared to stacking additional DiT layers in residual context-tuning methods [17]. In general, considering the significant motion error reduction, we conclude that the modest efficiency trade-off is acceptable, particularly in studio-grade scenarios which prioritize stringent accuracy and stability. A2. More Ablation Studies We conduct further ablation studies on SCAIL-1.3B model to evaluate the contribution of our proposed components. All training settings, including learning rate, data, batch size, and training steps, are kept exactly the same across Methods Self-Driven Animation PSNR SSIM LPIPS FVD SCAIL-1.3B (Ours) SCAIL-1.3B w/ 2D Pose 18.08 17.08 0.639 0.619 0.249 0.284 228.62 295. Table A1. Ablation studies to validate the 3D pose representation on the self-driven subset of Studio-Bench. Figure A1. Ablation studies on pose representation. Anomalies in the human body or deviations from correct posture are boxed. all models in our ablation studies. Additional user studies are conducted to collect users preference towards different configurations for the SCAIL-1.3B model. A2.1. Ablation on the Pose Representation To validate the effectiveness of our proposed 3D-consistent pose representation, we compare with 2D keypoints-based pipeline (extracted using DWPose [46] and denoted as w/ 2D Pose). Specifically, we implement the 2D augmentation strategy as close as to the 3D version, with similar figure settings and same augmentation ratio of 0.8. Faces and hands are also augmented identically to the 3D version and retargeting logic from [38] are applied during inference. All model architectures keep the same using full-context pose injection and P-RoPE. As shown in Table A1, the estimation noise and the 2D-3D mismatch noise introduced by the pipeline significantly undermine the models performance in the self-driven subset of our challenging Studio-Bench, which involves high ratio of complex motions. Figure A3. User study results of ablation on 3D Augmentation. Figure A2. Ablation studies on pose retargeting. 2D Retarget are visualized for comparison. Regions where the body proportions deviate from the reference image are boxed. For the cross-driven subset, 2D pose can easily lead to distorted limbs in generation, as seen in case (1) of Figure A1. Case (2) indicates that when model needs to distinguish the front and back of limbs, the inherent ambiguity of 2D pose can result in incorrect pose interpretations. Qualitative results from the cross-driven subset and quantitative results from the self-driven subset together demonstrate the overall effectiveness of our proposed 3D-based solution. Figure A2 can also demonstrate the unreasonable scaling factors introduced by 2D-based retargeting process during the inference step, which we will discuss in the next section of retarget ablations. A2.2. Ablation on 3D-Consistent Adaptation Furthermore, we validate the key component in our pose representation: 3D-Consistent Adaptation. 3D-Consistent Adaptation includes 3D Retarget in cross-driven inference and 3D Augmentation in training. Ablation on 3D Retarget. Figure A2 shows the comparison of the driving pose with and without 3D Retarget. 3D Retarget helps transfer the motion to the person without introducing position change. Compared to 2D Retarget, 3D Retarget keeps the original motion without introducing limb length distortion. Note that in our Studio-Bench, we only include cases where 2D Retarget works well for fair comparison of the the model itselfs performance against other baselines. In wild scenarios, however, 3D information and camera parameters can help create highly robust retarget rules that are suitable for production-level use. Ablation on 3D Augmentation. 3D augmentation is central to our methods ability to adapt to different characters. We conducted experiments to evaluate the impact of using 3D augmentation in the context of self-driven animation. The results indicated that even with high augmentation raFigure A4. Qualitative results of ablation on 3D Augmentation. tio, there was no significant difference in the metrics compared to when augmentation was not used. This is because 3D augmentation effectively preserves motion information by only altering figure shape and maintaining the temporal motion semantics. Methods Self-Driven Animation PSNR SSIM LPIPS FVD SCAIL-1.3B (Ours) w/o augmentation 18.08 18.09 0.639 0.635 0.249 0.251 228.62 230.31 Table A2. Ablation studies to validate the augmentation strategy on the self-driven subset of Studio-Bench. In the case of cross-driven animation, 3D augmentation significantly mitigated identity leakage, particularly notable in characters with substantial body shape differences, as illustrated in Figure A4. To quantify this improvement, we conduct user study comparing two groups: with and without 3D augmentation. The results reveal that 3D augmentation clearly enhances the metric of Physical Consistency and Identity Similarity, endowing our model with better generalization capabilities when handling in-the-wild characters. A3. Details on Evaluation A3.1. Details on Studio-Bench To comprehensively evaluate studio-grade scenarios, we curate an diverse set of motion sequences in our StudioBench, as illustrated in Figure A5. The motion collection in our dataset emphasizes complex human-body configurations, covering wide spectrum of challenging inputs, including dance, sports, martial-arts, acrobats and so on. In addition to isolated single-person motions, the test set also 2 character figures appear together in Figure A7, the issues with baseline models become even more pronounced. Another point worth noting is the scenario of reverse driving, where an anime characters motion is used to drive real person. While most user inputs involve real person driving another figure, this less common use case presents demand of making real person mimic an anime posture. We found that previous methods produce strange proportions under such inputs in Figure A8 while our model is still capable of handling this task. These comparisons highlight the strong potential of our approach for studio-grade applications where character compatibility for diverse motion types are critical requirements. A4. Discussion A4.1. Limitations and Future Work Although we have adopted relatively effective multiperson pose extracting pipeline, the accuracy of multiperson pose estimation is still not as precise as that for single-person scenarios. While we are able to make the model more robust to inaccurate poses through tailored model architecture design and sufficient data, we still look forward to advances in the field of multi-person pose estimation to further improve the fidelity of motion replication. Moreover, our SCAIL model currently relies on facial landmarks to achieve face control. Such representation is inherently limited in fine-grained facial expression. As our work primarily targets addressing the challenges including instability and motion artifacts in studio-grade video generation, enhancing the expressiveness of facial control is left for future exploration. Specifically, future work will focus on improving the accuracy and fidelity of fine-grained details, such as hands and facial expressions, to further elevate the models overall quality. A4.2. Ethical Considerations Our approach is designed to produce studio-grade, highfidelity character animation, enabling professional workflows in virtual production and cinematic pipelines. As our method advances character animation to new level of realism and expressiveness, the potential for misuse, particularly in generating misleading or harmful digital content, becomes an important consideration. Despite these concerns, we believe that fully open-sourcing our model will bring substantial value to the community. By promoting transparency and broad accessibility, we aim to encourage wide range of responsible, creative, and innovative works. Figure A5. Visualization of the distribution of data annotations in Studio-Bench. We categorize and annotate videos based on motion types. The annotation of single video can contain multiple tags, such as turning and ballet. contains small portion of interactions between the person and the environment, as well as several cases of multiperson interactions like dual dancing. We also include certain portion of fine-grained motions which are commonly featured in advertisement poses and iconic movie gestures, to evaluate the models all-around capability. For the construction of cross-driven cases, we intentionally select reference character images to cover diverse figure shapes and different facial characteristics. On top of these real human references, we additionally introduce approximately 40 non-real characters. Most of them originate from 3D animated productions, while others include 2D animated characters, plush toys, anime figurines, and various stylized representations. A3.2. More Examples on Studio-Bench Example from the main paper demonstrate that our model can handle both fine-grained motions and highly complex in-the-wild motions. Figure A6 and Figure A7 will provide more cases with nonstandard figures to show our models generalization ability across wide range of subjects and artistic styles. Figure A6 demonstrates that SCAIL can produce accurate limb motions that respect the features of nonstandard figures such as thin-limb anime characters. Furthermore, when the driving image is drastically different from the standard human figure (such as plush toy with very short body), SCAIL avoids the undesirable changes in body proportions that often plague baseline models. When the dual challenges of complex motions and non-standard 3 Figure A6. Comparison of models ability to preserve body structure for non-standard character figures. Figure A7. Visualization of our models performance under both high-dynamic motion and non-standard character figures. Figure A8. Visualization of our models performance under reverse driving settings."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Z.ai"
    ]
}