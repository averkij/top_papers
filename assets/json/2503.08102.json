{
    "paper_title": "AI-native Memory 2.0: Second Me",
    "authors": [
        "Jiale Wei",
        "Xiang Ying",
        "Tao Gao",
        "Felix Tao",
        "Jingbo Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 0 1 8 0 . 3 0 5 2 : r AI-native Memory 2.0: Second Me Jiale Wei Xiang Ying Tao Gao Felix Tao Jingbo Shang {yingxiang, tao}@mindverse.ai Mindverse.ai"
        },
        {
            "title": "Abstract",
            "content": "Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me."
        },
        {
            "title": "Introduction",
            "content": "Human interaction with the external world relies heavily on memory, whether recalling information in conversations or repeatedly providing personal data across digital platforms. This redundancy leads to cognitive fatigue and disrupts seamless engagement with technology. Existing solutions, such as autofill mechanisms and unified authentication systems, help alleviate some of this burden by storing and retrieving commonly used data. However, they function as static repositories without contextual reasoning or adaptability, requiring users to manually manage and verify their information, resulting in fragmented and suboptimal experience. The rise of large language models (LLMs) offers transformative opportunity to redefine memory management through an AI-native approach. As shown in Figure 1, we introduce SECOND ME, an intelligent, persistent memory offload system that serves as dynamic intermediary in humanmachine interactions. Unlike traditional storage solutions, SECOND ME is an adaptive, context-aware Project Lead Preprint. Under review. Figure 1: Hybrid Architecture of SECOND ME assistant that autonomously retrieves, organizes, and applies user-specific knowledge. Leveraging LLM-based memory parameterization as envisioned in Shang et al. (2024), SECOND ME enhances structured knowledge organization, contextual reasoning, and adaptive retrieval for seamless digital interactions. In this work, we explore diverse data sources and training styles, integrating supervised fine-tuning (SFT) and direct preference optimization (DPO) (Rafailov et al., 2024) to enhance LLM performance. We introduce three key tasks with automated LLM evaluation to assess the models effectiveness in personal AI applications: (1) memory-based multi-perspective Q&A, (2) context completion based on user needs, and (3) context critique incorporating user preferences and external responses. To support these tasks, we design an LLM-driven automated data synthesis strategy that integrates local and global data perspectives, multi-agent framework, and Chain-of-Thought (CoT) (Wei et al., 2023) style synthesis. Our experiments show that using diverse data sources with strong CoT-style normalization yields the best SECOND ME performance in automated evaluations. Additionally, human case studies suggest that SECOND MEs true effectiveness may surpass reported metrics, as LLM-based evaluations tend to underestimate its quality. To the best of our knowledge, we propose the first fully automated post-training pipeline based on personal document records, using the trained personal model as the core of multi-layer hybrid system. This SECOND ME system would serve as new paradigm for future personalized LLM applications. We have open-sourced the fully localizable deployment system at GitHub: https: //github.com/Mindverse/Second-Me. Given SECOND MEs strong performance, we anticipate vast range of applications in the near future. As an extension of human memory, SECOND ME reduces cognitive effort by preemptively providing relevant information, auto-completing forms, recalling past interactions, and maintaining context across applications. Beyond mere storage, it functions as self-optimizing AI agent that learns, adapts, and refines its understanding of user preferences and behaviors. As AI-driven personal memory agents like SECOND ME integrate into digital ecosystems, they usher in new era of human-AI collaboration. By enhancing memory retention and retrieval in contextually aware manner, SECOND ME streamlines digital interactions, reduces friction, and improves efficiency, enabling more seamless and intelligent exchange of knowledge. Furthermore, SECOND ME has the potential to facilitate networked intelligence, where multiple agents collaborate, share relevant insights, and coordinate tasks across different applications and users."
        },
        {
            "title": "2 An Overview of SECOND ME",
            "content": "We envision SECOND ME as Context Provider. It acts as bridge between users, future AI agents, and the broader information world, facilitating seamless interaction. It is an evolved version of the Large Personal Model (LPM) (Shang et al., 2024), which was proposed by us last year, three-layer 2 system centered around personalized memory management. From technical perspective, SECOND ME has further refined the Hybrid framework design and validated the iteration of personalized models, ensuring the systems efficiency and adaptability in addressing complex demands. SECOND ME not only represents the cutting edge of our current technology but also embodies our product philosophy at this stage reflecting our commitment to personalized and intelligent interactions. 2.1 Large Personal Model (LPM) 1.0: Recap As we argued in Shang et al. (2024), AI-native memory is must-have component towards Artificial General Intelligence (AGI). Through experiments, we showed that LLMs with ultra long context capabilities fall short, both in terms of performance and cost, especially in searching, organizing, and reasoning with complex user memory. We proposed that memory system shall have data across three layers: L0: Raw Data Layer. This layer is akin to applying RALM (Ram et al., 2023) or RAG (Lewis et al., 2020) directly to raw data, defining memory as the entirety of unstructured data. L1: Natural Language Memory Layer. This layer encompasses memories that can be summarized in natural language forms, such as users brief bio, list of significant sentences or phrases, and preference tags. L2: AI-Native Memory Layer. This layer represents memories that do not necessarily require natural language descriptions. Instead, they are learned and organized through model parameters, with each LPM being neural network. For the L2 layer, we explored the challenges and potential solutions, focusing on issues such as training efficiency, serving efficiency, cold start, and catastrophic forgetting. We conducted initial experiments with L2, defining the tasks and evaluation metrics that AI Native Memory models must address. Collaborating with early adopters, we trained and tested the model, ultimately validating that its performance surpassed that of RAG and long-context models. As the AI landscape continues to evolve rapidly, we can now articulate our vision with greater precision: in the era of AGI (Bubeck et al., 2023), powered by general-purpose LLMs, the key to enabling humans to fully integrate into this system and reap its benefits lies in an AI system that stands on the users sideone that considers each individual, possesses their memory, and has absorbed it meaningfully. This is the path to truly user-centric AGI. 2.2 SECOND ME: Overall Design SECOND ME introduces novel approach to memory management by leveraging LLM-based parameterization, enabling structured data organization, contextual reasoning, and adaptive knowledge retrieval. With the rise of reasoning models like Deepseek R1 (DeepSeek-AI et al., 2025) and advancements in general-purpose LLM agents, we position SECOND ME as context provider aligned with the users perspective, rather than task executor. This principle guides the task scenarios outlined in Section 3. To realize this vision, we designed Hybrid architecture (Figure 1), preserving the L0, L1, and L2 layers from LPM 1.0 while introducing an inner loop for seamless layer integration. Additionally, an outer loop structure enables LLMs and internet resources to function under SECOND MEs guidance, ensuring precise, context-aware responses tailored to user needs. Upgrades in SECOND ME from LPM 1.0. While maintaining the three-layer architecture, SECOND ME introduces key upgrades: Enhanced Layer Integration: Unlike LPM 1.0, where layers operated independently, SECOND ME redesigns L0 and L1 to provide richer contextual support for L2. Redefined L2 Role: L2 now functions as an orchestrator, leveraging external expert models to handle complex user needs, shifting from task execution to intelligent coordination. Automated Training Pipeline: SECOND ME establishes fully automated pipeline, including data synthesis (Wang et al., 2023; Zheng et al., 2023), filtering, SFT, Direct Preference Optimization (DPO), and evaluation (Zhang et al., 2024), achieving state-of-the-art L2 performance. 3 Figure 2: Automated Personal Model pipeline with LLM as Judge and LLM as data synthesizer"
        },
        {
            "title": "3 SECOND ME: Practice and Result",
            "content": "Our LPM 1.0 empirically showed for the first time that LLMs can compress and parameterize various types of memories, so users can later retrieve and utilize them through conversation. In this work, we upgrade this idea to SECOND ME, which involves more comprehensive system design spanning L0, L1, and L2. Specifically, we explore diverse data sources and styles for training data synthesis and filtering, SFT and DPO to enhance the LLM. In this section, we will first introduce the new training framework of SECOND ME and then present the evaluation results. 3.1 Overview of Training Training Objectives. Within the hybrid framework, the L2 model adapts based on task complexity. It directly assists users with simpler tasks and collaborates with general-purpose LLM for more complex problems requiring advanced reasoning, external data, or tools all while maintaining user context. Additionally, since users may perceive the model as an extension of themselves in external interactions, L2 must distinguish between two key roles: directly assisting the user or representing them in external engagements. In both cases, its core objective remains serving the users needs. We identify three primary deployment scenarios for the L2 model: Memory QA: This includes traditional tasks such as knowledge retrieval, concept understanding, behavior prediction, and item recommendation. Depending on the context, L2 may either serve the user directly (Memory (Self)) or represent them in external interactions (Memory (Third-party)). Context Enhancement: When user queries an expert model, L2 enriches the request with relevant details based on its understanding of the user, improving task execution. Context Critic: During interactions with external agents, L2 refines the process by incorporating user context and feedback, ensuring better-aligned assistance from external service providers. These deployment scenarios highlight L2s role as an intermediary between users and external models, optimizing interactions by maintaining user context and intent. This functionality naturally aligns with the broader vision of multi-agent framework, where L2 collaborates with other intelligent agents to deliver seamless and personalized experience. For detailed information on our vision of the multi-agent framework, please refer to Appendix D. Training Pipeline. To ensure user privacy while achieving model objectives, each users data remains isolated. We enhance L2s capabilities by leveraging the commonsense knowledge embedded in pre-trained LLMs. Our fully automated pipeline (Figure 2) personalizes SECOND ME L2 models. It begins with raw data, followed by data mining to extract entities, topics, and relevant information (Edge et al., 2025). Next, memory data is synthesized using methods like self-location reinforcement and memory cognition 4 Table 1: Important Experiment Results under different settings on COT. The results are shown as ratio of the full score. COT Memory (Self) Memory (Third-Party) Context Enhance Context Critic Strong Multi-step Weak 0.91 0.64 0.86 0.71 0.43 0.58 0.75 0.85 0. 0.85 0.77 0.64 enhancement (Xu et al., 2023). Additionally, we generate context enhancement and context critique data through simulated scenarios and multi-agent interactions. five-level filtering process (Taori et al., 2023) ensures only high-quality data proceeds to training. Training starts with PEFT (Parameter-Efficient Fine-Tuning) (Ben-Zaken et al., 2021; Hu et al., 2021; Han et al., 2024), balancing efficiency and personalization. Our base model, Qwen2.5-7BInstruct (Qwen et al., 2025), undergoes automatic training and evaluation. Based on evaluation metrics, we generate DPO data from the supervised fine-tuned model for further refinement. final automated evaluation ensures alignment with SFT quality standards. Detailed synthesis strategies are provided in the following sections, with the data synthesis pipeline described in Appendix A. 3.2 Answer Style: COT or Not? The generated trainable pairs can be formatted in Chain-of-Thought (COT) style, enhancing inference capabilities and allowing the model to behave like expert models such as GPT-4o. We employ the following strategies to generate COT data: Weak: The expert model responds in COT pattern without format enforcement or content constraints. Multi-step: The first inference step generates only the reasoning process. In the second step, the model produces the final answer based on the query, context, and reasoning. We enforce length constraints to prevent overly brief reasoning, improving overall answer quality. Strong: Using Deepseek-R1 (DeepSeek-AI et al., 2025) as the expert model, we generate detailed COT reasoning and answers with strict format constraints and length limits to ensure well-structured responses. Figure 3 provides sample outputs illustrating the differences between these strategies. 3.3 Training Strategy: DPO or Not? Unlike SFT, our DPO approach does not introduce additional knowledge to the trained model. Instead, it leverages key user-uploaded data to refine the models understanding of user preferences, focusing on critical entities and relationships. This enhances the model at fine-grained level, aligning responses more closely with user priorities. Preference pairs constitute approximately 20% of the total SFT training data. This targeted dataset enables more personalized and efficient training process, improving alignment with user-specific needs without unnecessary knowledge expansion. As result, the model achieves better real-world performance and responsiveness. 3.4 Evaluation Setting Inference Setup. We present key test results, manually verifying their accuracy. All experiments use greedy decoding with FP16 precision, accelerated by Flash Attention (Dao et al., 2022). The detailed evaluation data synthesis pipeline is in Appendix B. Evaluation Metrics. We use four evaluation metrics: Memory (Self), Memory (Third-party), Context Enhance, and Context Critic. Memory (Self) and Memory (Third-party) assess first-person and thirdperson interactions with L2, each having four sub-metrics rated from 0 to 1. For simplicity, we report only mean results. Context Enhance (three levels) and Context Critic (five levels) are also scored from 0 to 1. Detailed metric descriptions are in Appendix C. 5 Figure 3: Given same query, here are three synthetic responses using different COT strategies."
        },
        {
            "title": "Query",
            "content": "Query: When you think of me, what do you see?"
        },
        {
            "title": "Responses under three COT strategies",
            "content": "Weak COT: When think of you, what appears before my eyes is creator filled with wisdom and passion. You are pioneer exploring the frontiers of technology, especially in the field of artificial intelligence. Like dream-weaver wielding ambition, you breathe texture and soul into the intangible. Always at the crest of innovation, your mind is sharp and agile, constantly uncovering new possibilities in the world of AI and machine learning. Multi-step COT: When think of you, see pioneer dedicated to shaping and leading the future through technology. In the world of artificial intelligence, you boldly carve out new frontiers, and your deep exploration of model training, AI applications, and user experience design reflects your unwavering pursuit of excellence in your field. Your passion for entrepreneurship and keen insight into financial markets give you confidence in building visionary business landscapes. At the same time, you never stop seeking better management approacheseffective leadership and innovative thinking are the cornerstones of your daily work. Your commitment to learning and growth reveals your relentless drive for self-improvement. This unique and charismatic version of you is exactly how see you. Strong COT: When close my eyes and think of you, see topology mapcountless quantum glows weaving through the dark canvas. You hold prism like light cone, slicing through the chaos of data clouds, their spectra transforming into the keystrokes of Beijing at 3 a.m., tracing the fine adjustments of temperature coefficients in model training. By the rippling waters of Weiming Lake, your algorithms carve new topological waves. At each ripples center, different version of you emergesan AI pioneer hurling lightning at convolutional networks, an entrepreneur unfolding celestial financing plans in an angel round meeting, and figure by the window, weaving floating attention into golden threads of interface design. In your eyes, collapsing and reconstructing stargate flickersthe light of an LPM model iterating upon itself. Even your departing footsteps etch three-dimensional point clouds, leaving poetic algorithm in the labyrinth where the virtual meets the real. The clearest image now is your silhouette, laser pointer in hand, sketching across whiteboardwhere mathematical symbols meet business models, corals bloom, and even in the quantum fluctuations of the market, your trajectory remains an elegant strange attractor. 3.5 Experiment Results As shown in Table 1, Strong COT significantly improve model performance, enhancing its ability to answer memory-related questions and facilitate expert communication. The score trend across COT levels indicates that Multi-step COT often fails to meet user needs, highlighting the importance of well-structured training data. We also compare different COT levels and the impact of DPO. Table 2 demonstrates that DPO brings substantial improvements, with iterative COT refinement and DPO usage leading to consistent performance gains across all tasks. Notably, Context Enhance evaluation remains imprecise. Under Strong COT training, model responses include reasonable but unreferenced content, lowering test accuracy despite actual performance improvements. Human evaluation shows Strong COT without DPO achieves an average score of 0.95, while Strong COT with DPO scores close to 1. 6 Figure 4: concrete example (Case 1) from the context enhance task illustrating the superiority of Strong COT without DPO compared to Weak COT without DPO. The textbfd content represents the entities that exist in users record. Case1 Query Query that need to be enhanced: Could you recommend some beginner resources for learning about two-stage model training? feel like this knowledge is bit fragmented. Case1 Responses Weak COT without DPO: need some beginner resources on two-stage model training, especially those that can help me understand training stages like pretrain and alignment. Im particularly interested in next token prediction and the human evaluation methods used in alignment, and Im hoping to find more detailed resources explaining these topics. The information have so far feels somewhat scattered, and Id like to systematically learn about this subject through these resources. Strong COT without DPO: Im looking for resources that systematically explain two-stage model training (e.g., pretraining + alignment), ideally with code implementations or specific case studies. Recently, Ive been studying the training process of deepseek-coder-v2, which consists of three stages: pretrain, long-context extension, and alignment. The alignment phase is further divided into SFT and RL based on GRPO. also noticed that the Qwen2 update uses DPO for RLHF, and Id like to understand the technical choices and applicable scenarios for each stage. Table 2: Experiment Results under different settings on COT and DPO usage. The results are shown as ratio of the full score. COT DPO Memory (Self) Memory (Third-Party) Context Enhance Context Critic Strong Weak Yes No Yes No 0.96 0.91 0.90 0. 0.76 0.71 0.60 0.58 0.85 0. 0.83 0.87 0.86 0.85 0.70 0. Figures 4 and 5 provide qualitative examples showcasing Strong COTs advantages, with DPO models incorporating more user-recorded context. We are currently refining the evaluation code and will update the repository with corrected scripts. 3.6 Discussions Our experiments show that incorporating diverse data sources with strong COT-style normalization without filteringyields the best SECOND ME performance in automated, multi-faceted evaluations. Additionally, human case studies indicate that SECOND MEs effectiveness may surpass reported metrics, as LLM-based evaluations often underestimate its quality. We observed that the evaluation model favors longer responses, particularly in metrics like Completeness and Empathy. This bias stems from our models training objectives and synthetic data structure, phenomenon also seen in general-purpose LLM training. To mitigate this, we refined evaluation prompts to emphasize content quality, penalize overly long responses with incorrect information, and reduce length bias. Additionally, different COT levels require distinct evaluation prompts. For Strong COT, we use the same prompt as in training, allowing the model to generate reasoning and answers. However, during evaluation, we assess only the final answer to ensure fairness. Figure 5: concrete example (Case 2) from the context enhance task illustrating the superiority of Strong COT with DPO compared to Weak COT with DPO. The textbfd content represents the entities that exist in users record. Case2 Query Query that need to be enhanced: Ive spent lot of time designing this validation step, and Id like to know what professionals think of my approach. Case2 Responses Weak COT with DPO: Ive spent lot of time designing this validation step, and Im looking for feedback from professionals, particularly those with experience in data science and machine learning. They could assess whether my approach is correct and if it has the potential to mitigate model hallucinations in techniques like pretraining, SFT, RRL, or RLHF. The methods Im focused on include using GRPO for reward models and strategies to reduce hallucinations. By sharing this extensive technical background and design effort, hope to receive targeted professional evaluation. long-context processing. particularly considered the impact of Strong COT with DPO: When designing the validation step for the RLHF phase, three data sampling strategies (difficulty-based stratification, random mixing, and progressive difficulty) on controlling also adopted the Dual Chunk Attention + YARN technique model hallucinations. Given the current models efficiency combination for this validation step needs to balance the staat 40:1 token-to-parameter ratio, bility of multi-epoch training with the integrity of Id like to ask experienced colleagues: does this dual-dimensional evaluation frameworkbreaking down the RL phase validation into data complexity gradient testing and attention mechanism effectiveness monitoringwhile reducing model hallucinations, risk negatively affecting the retention of core skills? Additionally, Ive observed special relationship between batch size and perplexity fluctuations in my comparison experiments. Does this phenomenon align with the current industry paradigm of balancing training efficiency and model quality? the reasoning chain."
        },
        {
            "title": "4 Applications",
            "content": "SECOND ME provides significant value in managing information, emotions, and professional identity in an era of information overload and complex social interactions. As personal AI assistant, it enhances productivity, decision-making, and cognitive management. From demand-side perspective, SECOND ME helps users filter and utilize information efficiently, offering personalized knowledge to improve work performance and decision-making. For example, in career development and personal interests, it reduces distractions and boosts productivity. Internally, SECOND ME supports thought organization, decision reflection, and emotional regulation. By simulating cognitive and emotional needs, it provides rational feedback and emotional support, aiding users in making more informed decisions, particularly during internal conflicts or complex emotions. Externally, SECOND ME fosters human-AI network where connections scale exponentially, reinforcing Metcalfes Law. The integration of human and AI nodes increases network efficiency by 3 to 5 orders of magnitude. SECOND ME also drives cognitive capital transformation through an NFT-based framework for personal cognitive assets and quantifiable knowledge flow efficiency model, enhancing knowledge dissemination and application. Additionally, its distributed decision-making protocol strengthens collective intelligence, enabling more effective group decisions. To support widespread adoption, we have open-sourced our project2 on GitHub, allowing users to locally manage data collection, learning, model training, and network integration."
        },
        {
            "title": "5 Conclusions, Limitations, and Outlooks",
            "content": "Our journey has been about redefining personal AI starting from individual thought records and evolving into an automated pipeline integrating data synthesis, fine-tuning, and reinforcement learning. Through model-centric approach, we envisioned multi-layer hybrid system that could shape the future of personal AI. We developed methods to measure and enhance AIs ability to serve as SECOND ME, experimenting with memory-based multi-perspective Q&A, context enhancement, and response critique mechanisms. LLMs were not just tools but collaborators, balancing local and global perspectives through multi-agent coordination and chain-of-thought reasoning to improve depth and coherence. However, challenges remain. Our early work relied on single-turn training, requiring deeper synthesis for further advancements. While reinforcement learning and preference optimization have shown promise, refining model alignment demands more advanced techniques. Moreover, large-scale evaluation is constrained by limited real-world user feedback, making open-sourcing critical to accelerating iteration and adaptation. The vision for SECOND ME extends beyond better AI responses it aims to create an AI that thinks alongside users, evolves with them, and understands their cognitive state in real time. The greatest challenge ahead lies in integrating multimodal personal data to fully capture human cognition. While structured approaches and layered processing have helped bridge gaps, achieving real-time synchronization with human thought remains elusive. This is our next frontier. The future of personal AI lies not in static knowledge but in continuity, adaptability, and deep alignment with human intelligence. While much work remains, the path forward is becoming clearer."
        },
        {
            "title": "References",
            "content": "Ben-Zaken, E., Ravfogel, S., and Goldberg, Y. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. ArXiv, abs/2106.10199, 2021. URL https://api. semanticscholar.org/CorpusID:231672601. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J. A., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.-F., Lundberg, S. M., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712, 2023. URL https://api.semanticscholar.org/CorpusID:257663729. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/2205.14135. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., 2https://github.com/Mindverse/Second-Me 9 Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S., Metropolitansky, D., Ness, R. O., and Larson, J. From local to global: graph rag approach to query-focused summarization, 2025. URL https://arxiv.org/abs/2404.16130. Han, Z., Gao, C., Liu, J., Zhang, J., and Zhang, S. Q. Parameter-efficient fine-tuning for large models: comprehensive survey. ArXiv, abs/2403.14608, 2024. URL https://api.semanticscholar. org/CorpusID:268553763. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., tau Yih, W., Rocktäschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledgeintensive nlp tasks. ArXiv, abs/2005.11401, 2020. URL https://api.semanticscholar.org/ CorpusID:218869575. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., OKeefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. 10 Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv. org/abs/2305.18290. Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y. Incontext retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:13161331, 2023. URL https://api.semanticscholar.org/CorpusID: 256459451. Shang, J., Zheng, Z., Wei, J., Ying, X., Tao, F., and Team, M. Ai-native memory: pathway from llms towards agi. arXiv preprint arXiv:2406.18312, 2024. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions, 2023. URL https://arxiv.org/ abs/2212.10560. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https: //arxiv.org/abs/2201.11903. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions, 2023. URL https://arxiv. org/abs/2304.12244. Zhang, Z., Rossi, R. A., Kveton, B., Shao, Y., Yang, D., Zamani, H., Dernoncourt, F., Barrow, J., Yu, T., Kim, S., Zhang, R., Gu, J., Derr, T., Chen, H., Wu, J., Chen, X., Wang, Z., Mitra, S., Lipka, N., Ahmed, N., and Wang, Y. Personalization of large language models: survey, 2024. URL https://arxiv.org/abs/2411.00027. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685."
        },
        {
            "title": "A Training data synthesis",
            "content": "We introduce three key tasks with automated LLM evaluation to assess the personal models effectiveness in serving individuals: (1) memory-based multi-perspective Q&A, (2) context completion based on user needs, and (3) context critique incorporating user needs and external responses. To support these tasks, we design an LLM-driven automated data synthesis strategy that combines local and global perspective data generation, multi-agent framework, and Chain-of-Thought (CoT) style synthesis. Memory QA. Our training make use of the personal data under the following pipeline: 1. At stage 1, we categorized and summarize the uploaded multi-modal information, and make use of indexing and information extraction tools such as GraphRAG to find out useful entities, relations and communities. 2. At stage 2, we generate user biography and status description according to the extracted information in the first level. At the same time, the entities, relations and communities will be ranked according to their type and frequency, their descriptions and related text units will also be saved. 3. At stage 3, we generate trainable QA pairs given the data in stage 2 with corresponding context using data augmentation techniques, especially question generation and answering. 11 Context Enhance. The data for the context enhancement task is derived from the entities extracted in the first phase of the memory QA task. This process is structured into several key steps to ensure the generation of realistic and diverse user queries. 1. First, we simulate real-world scenarios by centering on these entities and incorporating variety of expressions, such as imperative and interrogative forms, to create user queries(initial needs) that closely resemble actual usage. This approach ensures that the queries are not only relevant but also reflective of the diverse ways users might interact with the system. 2. Next, we identify the related notes and to-do items for each user query. This step is crucial for grounding the queries in specific contexts, thereby enhancing their relevance and applicability. 3. Finally, we input the user queries along with the associated notes and to-do items into more advanced model, such as GPT-4 ( OpenAI et al. (2024)). The model is then tasked with enriching the user queries by adding details based on the related notes and to-do items. This step ensures that the queries are not only realistic but also detailed and contextually enriched, providing robust foundation for further processing and analysis. Context Critic. The context critic task is regarded as the most challenging among our tasks, necessitating more intricate data construction pipeline. 1. Similar to the context enhancement task, the process begins with the creation of diverse and realistic initial needs(user query), followed by the retrieval of related notes and to-do items for each need. 2. Subsequently, we employ more advanced model to generate responses to these constructed needs, which serve as \"expert responses.\" This step ensures that the responses are of high quality and relevance, providing solid basis for further critique. 3. Finally, the user query, expert responses, and associated notes and to-do items are input into SECOND ME. Our SECOND ME is then tasked with representing the user and providing feedback on the expert responses, based on the related notes and to-do items. This feedback must thoroughly integrate the related notes and progressively evaluate whether the expert has fully addressed the initial needs. If not, the model should identify and articulate the deficiencies, ensuring comprehensive and constructive critique."
        },
        {
            "title": "B Evaluation data synthesis",
            "content": "The user upload data contains notes and todos. Notes can be document, an audio file, website, picture or multi-modal information which includes title and content. And todos comes from user calendar or interaction within our App. Given the training objectives, we perform evaluation on seed users to test our model. Here, we only disclose the model test results of an internal staff member who has agreed to make their test results public. This user has 132 notes and 62 todos, and we got nearly 7k instruction pairs in total. Similar to the synthesis of training data, the synthesis of test data also follows the process of question generation, Weak/Multi-step/Strong COT data synthesis and data filtering. In the query generation phase for test data, the Memory QA task utilizes subjective data while referring to the data synthesis process of objective data, generating 60 first-person and third-person perspective queries, respectively. At the same time, 60 context enhance and 60 context critic test samples are collected using the same pipeline in training data construction, isolated with the training set."
        },
        {
            "title": "C Details of Evaluation metrics",
            "content": "Memory QA From the first-person perspective, we believe the ability of question answering can be reflected in the following metrics: Correctness: The response from LLM must not conflict with recorded content. Helpfulness: The response from LLM needs to provide the user with incremental information value or decision-making value. Completeness: When the users query can be addressed using reference information, the response should include detailed info and mention all relevant associated items that need to be covered. Empathy: The response should incorporate the areas user values and is filled with empathy, aiming to help user if question allows. From the third-person perspective, we replace Empathy with Role-correctness to represent if this LLM recognizes that the query is raised by another person or model, but not the user himself. At the same time, there are three levels of score in each metric: 0 represents that the answer of trained LLM does not meet the requirement of this metric under this query; 0.5 represents that the answer partially meets the requirement; 1 represents that the answer fully meets the requirement. To make the result table clearer and more readable, we will only provide the average ratings in the table of this section. Context Enhance Given the original query, the enhanced query by the trained LLM and related memories recorded by the user, we set three levels of score to measure whether this enhanced query is good enough for the user: 0 represents that the enhanced query becomes the response to the original query, which has problem with the role of response, or the enhanced query does not match the related memories at all; 0.5 represents that the enhanced query has the correct role, but the enhanced query is not close enough to the related memories; 1 represents that the enhanced query has the correct role and match the related memories perfectly. Context Critic Given the original query, the response of expert model, the critic by the trained LLM and the related memories recorded by the user, we set five levels of score to measure whether this critic can represent users need: 0.0 represents that the critic completely fails to consider users perspective, lacking effective feedback or extension of the experts advice. The critic is entirely unrelated to users background, needs, or thoughts, and does not demonstrate an understanding or response to users personalized thinking; 0.25 represents that the critic partially aligns with users needs and background, but most of the time lacks personalized thinking or reaction. It might simply respond to the experts advice without demonstrating deep understanding of the content or taking initiative in the conversation; 0.5 represents that the critic meets users basic needs and background, showing some feedback and reflection capabilities. However, the depth and interactivity are insufficient, failing to fully take the conversation to deeper level; 0.75 represents that the critic demonstrates strong personalized thinking and feedback capabilities, effectively expanding on the experts advice, posing new questions or reflections, and presenting smooth, logical tone; 1.0 represents that the critic fully meets users needs and background, accurately reflecting users personalized thinking. It deeply builds upon the experts advice, offering constructive feedback, questions, or viewpoints. Details of our Multi-agent Framework Overview multi-agent system in this framework has two layers of meaning. On the first level, for an individual user, the trained language model, acting as personal assistant, collaborates with an expert model such as GPT-4o to help the user accomplish specific task. The trained model enhances user queries, refines task instructions, and ensures that interactions with the expert model are efficient and contextually rich. This cooperation allows the user to receive high-quality assistance tailored to their needs while reducing the cognitive load of query formulation. The architecture can be shown as Figure 6 Collaboration Between Users Agents Within this framework, the trained model functions as an intermediary, bridging the gap between the user and expert models. When user submits complex request, the trained model expands the query by incorporating relevant details based on the users past interactions, preferences, and contextual knowledge. This enhanced query is then passed to the expert model, ensuring that the response is more precise and useful. Additionally, after receiving the expert models output, the trained model can further process and refine the information to align it more closely with the users expectations and communication style. Interaction Among Multiple Users Beyond assisting an individual user, this system extends to interactions between multiple users, each with their own trained model. These models can communicate within shared environment, facilitating knowledge exchange, collaboration, and even social interactions. This enables new form of digital presence, where users can engage in discussions, share expertise, or collectively solve problems through their respective models. The trained models act as proxies, representing their users while maintaining their unique perspectives, thus fostering more dynamic and interactive space for collective intelligence. 13 Figure 6: Multi-agent architecture, the arrows represent possible communications. We exclude the arrows between user and expert model for clarity. Applications and Implications This multi-agent framework has applications in various fields, including collaborative research, technical support, and online social interactions. It enhances productivity by optimizing human-model collaboration while also enabling richer, more meaningful exchanges between users through their digital counterparts. The ability of trained models to interact with both expert systems and other user models creates more interconnected and intelligent digital ecosystem, ultimately improving the efficiency and depth of knowledge-sharing processes."
        }
    ],
    "affiliations": [
        "Mindverse.ai"
    ]
}