{
    "paper_title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts",
    "authors": [
        "Xiaoqiang Wang",
        "Suyuchen Wang",
        "Yun Zhu",
        "Bang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move beyond fast System-1 responses and engage in deliberative System-2 reasoning. However, this comes at the cost of significant inefficiency due to verbose intermediate output. Recent latent-space reasoning methods improve efficiency by operating on hidden states without decoding into language, yet they treat all steps uniformly, failing to distinguish critical deductions from auxiliary steps and resulting in suboptimal use of computational resources. In this paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that dynamically allocates computation across reasoning steps through shortcut paths in latent space. Specifically, System-1.5 Reasoning introduces two types of dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the vertical depth by early exiting non-critical tokens through lightweight adapter branches, while allowing critical tokens to continue through deeper Transformer layers. The step shortcut (SS) reuses hidden states across the decoding steps to skip trivial steps and reason horizontally in latent space. Training System-1.5 Reasoning involves a two-stage self-distillation process: first distilling natural language CoT into latent-space continuous thought, and then distilling full-path System-2 latent reasoning into adaptive shortcut paths (System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20x and reducing token generation by 92.31% on average."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 2 2 6 9 8 1 . 5 0 5 2 : r System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts Xiaoqiang Wang1, Suyuchen Wang1,2 Yun Zhu3 Bang Liu1,2,4 1DIRO & Institut Courtois, Université de Montréal 2Mila - Quebec AI Institute; 3Apple; 4Canada CIFAR AI Chair {xiaoqiang.wang, suyuchen.wang, bang.liu}@umontreal.ca"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move beyond fast System-1 responses and engage in deliberative System-2 reasoning. However, this comes at the cost of significant inefficiency due to verbose intermediate output. Recent latent-space reasoning methods improve efficiency by operating on hidden states without decoding into language, yet they treat all steps uniformly, failing to distinguish critical deductions from auxiliary steps and resulting in suboptimal use of computational resources. In this paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that dynamically allocates computation across reasoning steps through shortcut paths in latent space. Specifically, System-1.5 Reasoning introduces two types of dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the vertical depth by early exiting non-critical tokens through lightweight adapter branches, while allowing critical tokens to continue through deeper Transformer layers. The step shortcut (SS) reuses hidden states across the decoding steps to skip trivial steps and reason horizontally in latent space. Training System-1.5 Reasoning involves two-stage self-distillation process: first distilling natural language CoT into latentspace continuous thought, and then distilling full-path System-2 latent reasoning into adaptive shortcut paths (System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20 and reducing token generation by 92.31% on average."
        },
        {
            "title": "Introduction",
            "content": "Foundational large language models (LLMs) (Ouyang et al., 2022; Team et al., 2023; Dubey et al., 2024; Hurst et al., 2024; Meta, 2025) has revolutionized natural language processing (Liang et al., 2022; Srivastava et al., 2023; Wang et al., 2024) and demonstrated strong potential in diverse agent applications (Liu et al., 2025), automating real-world tasks across both digital (OpenAI, 2022; Wang et al., 2023; Hong et al., 2023; Wang & Liu, 2024; Qin et al., 2025; Wang et al., 2025) and physical environments (Brohan et al., 2023; Driess et al., 2023; Zheng et al., 2024; Team et al., 2024). Recent advances in test-time scaling paradigms (Snell et al., 2024; Zhang et al., 2025) and the emergence of reasoning-oriented LLMs, also referred to as Large Reasoning Models (LRMs) (Jaech et al., 2024; Guo et al., 2025; Team, 2025; Abdin et al., 2025), have leveraged Chain-of-Thought (CoT) (Wei et al., 2022) to extend LLM reasoning capabilities (Chen et al., 2025; Wu et al., 2025). This approach facilitates transition from fast and heuristic-driven System-1 reasoning to slower and deliberate System-2 reasoning (Li et al., 2025b), and has significantly improved performance in competitive Corresponding author. Figure 1: Comparison of (a) our proposed System-1.5 Reasoning, (b) chain-of-thought (CoT) reasoning, (c) early-exit (Elbayad et al., 2019; Elhoushi et al., 2024), (d) compressed latent-space reasoning (e.g. , Coconut (Hao et al., 2024) and CCoT (Cheng & Van Durme, 2024)), and (e) extra latent reasoning approaches that delay output (e.g. , pause token (Goyal et al., 2023) and filler token (Pfau et al., 2024)). System-1.5 Reasoning enables flexible latent reasoning along both the horizontal and vertical dimensions through dynamic shortcuts. mathematics (Hendrycks et al., 2021; Li et al., 2024) and code generation (Jimenez et al., 2023; Jain et al., 2024). However, CoT reasoning incurs substantial computational costs during inference due to the need to generate long reasoning chains before producing the final answer. It also suffers from overthinking phenomena (Chen et al., 2024; Team et al., 2025), resulting in redundant and verbose outputs even for simple problems, where excessive reasoning leads to unnecessary token generation and escalates inefficiency. To address it, one line of work focuses on language-space efficiency, such as imposing token budgets during prompt input (Xu et al., 2025a) or decoding interventions (Muennighoff et al., 2025) to constrain the length or complexity (Lee et al., 2025) of generated explanations. Another direction emphasizes efficient decoding, such as speculative decoding (Leviathan et al., 2023) and optimized sampling (Sun et al., 2024b; Fu et al., 2024; Li et al., 2025a). However, many tokens generated during chain-of-thought (CoT) reasoning primarily serve textual fluency and coherence rather than contributing meaningfully to actual reasoning advancement (Song et al., 2025; Luo et al., 2025a). Consequently, recent approaches have explored operating reasoning in latent space. Methods such as the pause token (Goyal et al., 2023) and filler token (Pfau et al., 2024) insert special tokens and ignore their language-space outputs to allow for additional latent computation, while Coconut (Hao et al., 2024) and CCoT (Cheng & Van Durme, 2024) compress explicit CoT into hidden states and feed these hidden states, rather than generated tokens, back into the model as subsequent embeddings. By bypassing language constraints, these internal computations enable more compact and flexible reasoning within the latent space. While these methods improve efficiency, as illustrated in Figure 1, they introduce unidirectional optimization bias, promoting either universally fast reasoning through compressed latent-space reasoning (e.g. , Coconut and CCoT) for all reasoning steps or universally slow reasoning that delays outputs at every step (e.g. , pause token and filler token). This uniform treatment fails to differentiate between critical deductions and auxiliary steps, leading to inefficient allocation where trivial and complex steps receive nearly equal computational budgets. For example, CoT sequence often includes distinct phases such as problem restatement, exploration, and result verification, each requiring different levels of reasoning effort (Luo et al., 2025b). 2 This raises natural question: Can we dynamically tailor computation across different reasoning steps to maximize efficiency while preserving performance? Ideally, models should reason quickly (System-1) on non-critical steps and more carefully (System-2) on critical ones. Motivated by this, we propose System-1.5 Reasoning, which adaptively combines fast and slow reasoning paths in latent space to handle varying step complexities in the language space. System-1.5 Reasoning introduces model depth shortcut (DS) and decoding step shortcut (SS) to adaptively allocate computation along the vertical and horizontal paths in latent space. Specifically, the depth shortcut is implemented by augmenting each Transformer layer with routeradapter module, which dynamically determines whether token continues through deeper standard layers or exits early via lightweight adapter branch. This depth shortcut mechanism enables flexible vertical computation along the model depth, allowing non-critical reasoning steps to be processed with fewer layers while critical steps continue deeper into the model. In addition, the step shortcut enables latent-space skipping across the horizontal dimension of decoding length, where early-exited hidden states at given layer are directly copied to the next decoding step, instead of restarting latent computation from the first layer as in standard Transformers. By supporting adaptive reasoning along both vertical and horizontal paths, System-1.5 Reasoning maximizes flexibility in latent-space reasoning and better mirrors human thinking, where difficult reasoning steps are handled with deliberate, reflective System-2 thinking, simple steps are processed quickly through heuristic System-1 thinking, and trivial steps are naturally skipped without thinking. To enable latent-space shortcut reasoning, we train System-1.5 Reasoning via two-stage distillation process. The first stage, language-to-latent distillation, aligns the latent-space behavior of student model with the language-space CoT reasoning of teacher model. This process leverages full teacher-forcing and allows efficient parallel training, avoiding the step-wise scheduling complexities of curriculum learning approaches such as iCoT (Deng et al., 2024) and Coconut (Hao et al., 2024). The second stage, System-2 to System-1.5 distillation, further compresses full-depth reasoning trajectories into shortcut execution path by leveraging reasoning criticality estimation in the language space. Specifically, we freeze the original transformer parameters and tune only the router-adapter module using an early-exit loss that encourages non-critical tokens to exit at earlier layers and critical tokens to proceed to deeper layers, while maintaining consistency between the hidden states of the full path and those of the shortcut-exited path. We validate our approach on challenging reasoning datasets such as GSM8K (Cobbe et al., 2021). Experiments demonstrate that System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20 and reducing token generation by 91.0% on average. These results highlight the promise of System-1.5 Reasoning in improving the efficiency and scalability of the LLM reasoning."
        },
        {
            "title": "2 System-1.5 Reasoning",
            "content": "We frame System-1.5 Reasoning as adaptive and dynamic latent-space reasoning guided by criticality analysis of language-space reasoning. As shown in Figure 2, the core idea is to dynamically allocate computation in latent space through two types of dynamic shortcuts, model depth shortcut (DS) and decoding step shortcut (SS), to handle varying step complexities in the language space. Specifically, by inserting standard router-adapter module into each vanilla Transformer layer, the dynamic depth shortcut enables simple steps to be processed through shallow layers, while complex steps are routed through deeper layers for more extensive computation. In parallel, the dynamic step shortcut allows trivial steps to be skipped by copying hidden states at early exit points and directly reusing them as the hidden states for the next decoding step at the same layer. To train System-1.5 Reasoning effectively in latent space, we employ two-stage distillation process. First, we perform language-to-latent distillation by fine-tuning vanilla Transformer layers, aligning the student models hidden states, which reason in latent space with those of the CoT-trained teacher model. Then, with the vanilla Transformer parameters frozen, we perform System-2 to System1.5 distillation by training the router-adapter modules. Specifically, we leverage atomic thought decomposition (Teng et al., 2025) to estimate reasoning criticality step by step. We decompose the CoT into directed acyclic graph (DAG) of self-contained subquestions. In this graph, independent nodes are labeled as non-critical, while derived nodes that require logical integration are labeled as 3 Figure 2: System-1.5 Reasoning is trained through two-stage distillation process: (1) Languageto-latent alignment, where the model learns to reason in latent space by minimizing consistency loss between language-space reasoning model (System-2 teacher) and latent-space reasoning model (System-2 student); and (2) Shortcut learning, where dynamic shortcuts are trained by applying an early-exit loss that encourages non-critical steps to exit earlier and critical steps to proceed deeper. The model is simplified to 2-layer Transformer for illustration purposes. critical. Based on this, we apply an early-exit loss that encourages tokens of non-critical steps to exit earlier and tokens of critical steps to proceed to deeper layers. 2.1 Dynamic Shortcut Architecture Formally, given an input text sequence = x1, , xt, , xT , we denote the vanilla hidden states at the l-th layer as Hl = hl,1, , hl,t, , hl,T , where is the sequence length and {0, 1, , L} with being the total number of Transformer layers. The initial hidden states are given by H0 = Embed(X), corresponding to the embedding layer. For 1, denoting fl() as the operation of single Transformer layer, the hidden state at layer is updated as Hl = fl(Hl1). Depth shortcut. To adaptively determine how many of the initial Transformer layers are needed for each token, i.e. , when to trigger early exit, we employ lightweight router-adapter module, as shown in Figure 2. At each Transformer layer l, the router module Rl dynamically decides whether token should continue through the standard Transformer layer fl for deeper processing, or exit early through an adapter branch gl. Formally, during training, the output at layer for step is expressed as weighted combination of the adapter and Transformer outputs, where the weight is determined by the binary router Rl, implemented as feed-forward network (FFN) layer followed by sigmoid activation: hsystem-1.5-ds-training l,t = gl1(hl1,t) + fl(hl1,t) (1 w) = Rl(hl1,t) (1) (2) During inference, the output of the router Rl serves as confidence score for early exit, which is compared against predefined depth exit threshold λdepth to determine whether to halt computation at the current layer for the given decoding step. hsystem-1.5-ds-inference l,t = (cid:26)gl1(hl1,t), fl(hl1,t), if Rl(hl1,t) > λdepth, otherwise. (3) Step shortcut. The step shortcut is motivated by the observation that, in standard decoding, each step still forces the model to process from the first layer. In other words, while depth shortcuts allow adaptive reasoning along the vertical path in latent space, enabling exits at intermediate layers rather than always reaching the final layer, the model must still sequentially process every decoding step. To address this, we introduce dynamic shortcuts along the decoding steps, allowing the model 4 to adaptively skip steps and reason horizontally in latent space. By supporting adaptive reasoning along both vertical and horizontal paths, this design maximizes flexibility in latent-space reasoning and enables System-1.5 Reasoning to better mirror human thinking, where trivial steps are often skipped (step shortcut), difficult steps are handled with deliberate and reflective reasoning, and simple steps are processed quickly through heuristic reasoning (depth shortcut). More importantly, since System-1.5 Reasoning operates entirely in latent space, where new decoding steps rely solely on the hidden states from the previous step without introducing new input language tokens, the practice of step shortcut avoids the problem of partial observation of the input context (i.e. , skipping token input at the current step) in token compression methods. This allows System-1.5 Reasoning to achieve high computational efficiency while preserving all crucial information and thinking steps. Formally, similar to the depth shortcut, during training, the output at layer and step is computed as weighted combination of the hidden state from step t1 at the same layer (note that the step shortcut is not applied when = 0) and the hidden state at step from the vanilla Transformer: hsystem-1.5-ss-training l,t = gl(hl,t1) + fl(hl1,t) (1 w) (4) where the weight is determined by the router Rl as Eq. 2. By merging Eq. 1 and Eq. 4, we obtain: hsystem-1.5-training l,t = (cid:16) gl1(hl1,t) (cid:125) (cid:123)(cid:122) (cid:124) depth shortcut + gl(hl,t1) (cid:124) (cid:125) (cid:123)(cid:122) step shortcut (cid:17) + fl(hl1,t) (cid:123)(cid:122) (cid:125) (cid:124) vanilla path (1 w) (5) During inference, as determined by Eq. 3, if decoding step halts computation at an intermediate layer, the hidden state at that layer is directly passed to the next decoding step within the same layer to continue reasoning. If computation halts at the final layer (i.e. , after traversing all the Transformer layers via dynamic shortcuts, termed as reasoning cycle), we either (1) feed the final hidden state back for the next reasoning cycle or (2) generate the final output. Borrowing the latent reasoning setting from (Hao et al., 2024), we apply fixed number of latent reasoning steps, denoted as the decoding step constant λstep. By combining depth exit threshold λdepth in Eq. 3 and decoding step constant λstep to control computation along both the model depth and decoding steps, this approach provides flexible computational budget for test-time scaling (see Section 3.2). 2.2 Two-Stage Distillation: Language-to-Latent Alignment and Shortcut Learning Training model to reason in latent space with shortcut paths poses two challenges: the latent property and the adaptive property. (1) Vanilla Transformer layers in pre-trained LLMs are optimized for next-token prediction in the language space and are not inherently capable of reasoning in latent space. How can we align latent-space reasoning with language-space CoT? (2) Adaptive reasoning needs the model to dynamically decide varying reasoning paths in the latent space. How can we potentially leverage the characteristics of language-space reasoning steps (e.g. reasoning step criticality) to guide the model toward learning adaptive reasoning via shortcut paths? As shown in Figure 2, for the latent property challenge, we employ language-to-latent alignment through hidden-state distillation. Specifically, we align the reasoning processes between languagespace teacher model and latent-space student model. The teacher model, denoted as Mθteacher, is trained with standard CoT fine-tuning to generate both intermediate steps and final answers in the language space. The student model, denoted as Mθstudent, learns to reason directly in latent space. For the adaptive property challenge involving dynamic shortcut decisions, we leverage atom-ofthought (Teng et al., 2025) to decompose the original CoT into directed acyclic graph (DAG) of self-contained subquestions. Independent nodes are identified as non-critical steps (requiring less computation), while derived nodes requiring logical integration are identified as critical steps (requiring deeper reasoning). Based on this, we introduce shortcut learning: We first initialize the System-1.5 model, denoted as Mϕsystem-1.5 , by inheriting the parameters from Mθstudent. We then freeze the Transformer parameters and insert an adapter-router module into each Transformer layer to enable fine-tuning for adaptive reasoning. Lastly, we apply an early-exit loss that encourages non-critical steps to exit at shallower layers while allowing critical steps to proceed deeper into the model. Language-to-latent alignment. During training, we extract the last-layer hidden states from the teacher model, apply stop-gradient, and use them as ground-truth features for the student model. These features are then fed into the student model together with the question input, enabling teacher-forcing and ensuring training efficiency. Formally, given an input sequence = x1, . . . , xT , we further represent it as the concatenation of three segments, the question Q, intermediate reasoning steps R, and the answer A, denoted as = : : = q1, . . . , qTQ, r1, . . . , rTR , a1, . . . , aTA, where TQ, TR, and TA correspond to the lengths of each segment. The last-layer hidden states from the System-2 teacher and student models, denoted by hteacher hstudent L,t L,t respectively, are aligned by minimizing the following mean squared error (MSE) loss: and Lconsistency θstudent ="
        },
        {
            "title": "1\nTA",
            "content": "TQ+TR+TA (cid:88) t=TQ+TR+1 MSE(sg[hteacher L,t ], hstudent L,t ) (6) where sg[] denotes the stop-gradient operation applied to the teacher model. In parallel, the System-2 teacher model is supervised by the standard negative log-likelihood (NLL) loss over both intermediate reasoning steps and final answers, while the System-2 student model is supervised by NLL loss over final answer generation only (i.e. , performing latent reasoning for intermediate steps): LLM θteacher = TQ+TR (cid:88) t=TQ+1 log Mθteacher (rt r<t, Q) TQ+TR+TA (cid:88) t=TQ+TR+1 log Mθteacher (at a<t, R, Q) (7) LLM θstudent = TQ+TR+TA (cid:88) t=TQ+TR+1 log Mθstudent (cid:0)at a<t, teacher , Q(cid:1) (8) = {hteacher L,TQ where teacher L,TQ+TR1} denotes the extracted last-layer hidden states of the reasoning steps, provided as input to the student model to enable teacher-forcing. The overall loss for language-to-latent alignment is thus: L,TQ+1, , hteacher , hteacher L1 = LLM θteacher + LLM θstudent + αLconsistency θstudent (9) where θteacher and θstudent refer to the original Transformer parameters of the System-2 teacher and System-2 student models, respectively. Shortcut learning. Given dataset with labeled intermediate reasoning steps = r1, r2, , rTR and estimated criticality binary labels for each step (r1, c1), . . . , (rTR , cTR ), we define an early-exit loss (Elbayad et al., 2019; Elhoushi et al., 2024) that enforces consistency between the intermediate hidden states of the System-1.5 model and the final-layer hidden states of the System-2 student model (which has been trained through language-to-latent alignment): Learly-exit ϕsystem-1.5 = (cid:88) TQ+TR (cid:88) l=1 t=TQ+1 el,tMSE(sg[hstudent l,t ], hsystem-1.5-training l,t ) (10) where el,t is the early-exit weight for the l-th layer at step t, designed to distinguish critical and non-critical steps. Specifically, we define el,t to encourage non-critical steps (ct = 0) to exit earlier, and critical steps (ct = 1) to exit later, according to: el,t = (1 ct) (cid:32) + ct 1 (cid:80)l (cid:80)L i=1 j=1 (cid:80)l (cid:80)L i=1 j=1 (cid:33) . (11) In parallel, the System-1.5 model is also supervised by NLL loss over the final answer generation: LLM ϕsystem-1.5 = TQ+TR+TA (cid:88) t=TQ+TR+1 log Mϕsystem-1.5 (cid:0)at a<t, student , Q(cid:1) (12) 6 Table 1: Quantitative results on reasoning tasks, reported in terms of accuracy (Acc.), number of decoding steps before generating the final answer (# Steps), average FLOPs reduction rate per step relative to CoT (FLOPs r.), and overall inference speedup relative to CoT measured by wall-clock time. Best and second-best results are highlighted with bold and underline, respectively. Method CoT LITE LayerSkip iCoT Coconut CODI pause token System-1.5 Acc. (%) GSM8K # Steps FLOPs r. Speedup Acc. (%) GSM-HARD # Steps FLOPs r. Speedup Acc. (%) StrategyQA # Steps FLOPs r. Speedup 46.94 44.51 43.20 32.14 36.75 43.78 46. 46.66 26 28 32 2 2 2 38 2 - 1.84 1.77 1.02 1.02 1.02 1.00 1.95 - 1.61 1.45 13.45 11.98 13.37 0.8 20.27 38.32 37.01 36.55 23.17 28.25 35.91 38. 38.28 26 28 33 4 4 4 42 4 - 1.56 1.32 1.02 1.02 1.02 1.00 1.76 - 1.44 1.03 6.17 7.07 6.93 0.89 12.45 47.62 46.15 42.54 34.42 38.67 45.12 48. 48.61 52 42 49 2 2 2 61 2 - 1.96 1.8 1.02 1.02 1.02 1.00 2.12 - 2.36 1.86 26.47 27.25 25.96 0.82 55.65 where student shortcut learning is then given by: extracts the System-2 student model to enable teacher-forcing. The overall loss for L2 = LLM ϕsystem-1.5 + βLearly-exit ϕsystem-1.5 (13) where ϕsystem-1.5 refers to the parameters of the router-adapter modules. The underlying Transformer parameters are initialized from θstudent and are kept frozen during shortcut learning."
        },
        {
            "title": "3 Experiments",
            "content": "Datasets. We evaluate the effectiveness and efficiency of System-1.5 Reasoning on two reasoningintensive tasks: mathematical reasoning and common sense reasoning. For mathematical reasoning, we train on the augmented GSM8K dataset (Deng et al., 2023), which extends the original GSM8K (Cobbe et al., 2021) with larger set of grade school-level math problems. For commonsense reasoning, we use StrategyQA (Geva et al., 2021), which contains multihop questions annotated with supporting facts. Each sample includes question, decomposed subquestions, and yes/no answer. We merge the annotated facts and subquestions as coherent CoT sequence for training. For both datasets, we label each reasoning step with criticality annotations using atomic thought decomposition, as described in Section 2.2, to distinguish between critical and non-critical steps. We train models on the official training splits and evaluate performance on the respective test sets for in-domain evaluation. Additionally, for mathematical reasoning, we conduct out-of-domain evaluation on GSM-HARD (Gao et al., 2023), dataset with increased reasoning difficulty designed to test generalization beyond the original GSM8K. Baselines. In addition to CoT fine-tuning, we compare System-1.5 Reasoning against six efficient reasoning methods based on supervised fine-tuning, consisting of: (1) language-space conditional computation methods: LITE (Varshney et al., 2023) and LayerSkip (Elhoushi et al., 2024), which improve efficiency by applying early exit mechanisms; (2) Latent-space compressed reasoning: iCoT (Deng et al., 2024), Coconut (Hao et al., 2024), and CODI (Shen et al., 2025), which compress natural language CoT into compact continuous latent representations; (3) Latent-space extended reasoning methods: pause token (Goyal et al., 2023), which insert extra latent states to delay output and allow for extended internal reasoning. Since the official implementations of iCoT and Coconut are based on GPT-2 124M (Radford et al., 2019), while LayerSkip builds on the LLaMA 2 (Touvron et al., 2023) and LLaMA 3 (Grattafiori et al., 2024) series model, we implement two backbone versions of System-1.5 Reasoning, one using GPT-2 124M and another using LLaMA 3.2 1B, to ensure fair comparisons between different baselines. 3.1 Main Results System-1.5 Reasoning outperforms previous state-of-the-art methods in latent-space reasoning in both accuracy and efficiency. As shown in Table 1, compared to iCoT, Coconut, CODI, and pause token, System-1.5 Reasoning achieves higher accuracy and greater overall speedup. In GSM8K, compared to original CoT reasoning, latent-space reasoning reduces intermediate token generation by 92.31% during inference. This reduction is even more pronounced on StrategyQA, reaching 96.15%. Additionally, compared to early-exit methods, System-1.5 Reasoning achieves 1.95 reduction in average FLOPs per decoding step, due to its dynamic step shortcut mechanism, 7 which allows hidden states from early-exit layers to be directly copied and reused in the next decoding step. In contrast, early-exit methods still reprocess these states starting from the first layer. By combining early exits along the model depth and shortcut copying across decoding steps, System-1.5 Reasoning achieves an overall inference speedup of 20.27 on GSM8K, further improving upon the approximately 10 speedups achieved by previous latent reasoning methods. On StrategyQA, it further accelerates inference by over 55, significantly enhancing reasoning efficiency. System-1.5 Reasoning matches CoT fine-tuning on challenging mathematical reasoning tasks and outperforms it on text-rich commonsense reasoning. On GSM8K and GSM-HARD, System-1.5 Reasoning achieves 46.94% and 38.32% accuracy respectively, closely matching CoT fine-tuning results of 46.67% and 38.28%. On StrategyQA, task requiring reasoning over multiple pieces of textual evidence, System-1.5 Reasoning achieves 48.61% accuracy, outperforming CoTs 47.36%. Similar improvements are observed for latent-space reasoning methods such as pause token, highlighting the promising potential of System-1.5 Reasoning in reducing dependency on explicit textual tokens and enhancing the models inherent reasoning capabilities. Figure 3: Ablation results on optimizing System1.5 (shown in solid color) using different System-2 students (shown in light color), namely CoconutSystem-1.5 and CODI-System-1.5, as well as alternative learning strategies: joint learning of language-to-latent alignment and shortcut learning, and full-parameter shortcut learning (SL). 3.2 In-depth Analysis Direct distillation from language to latent is more effective for shortcut learning in System-1.5 Reasoning. We modify the language-to-latent alignment into curriculumbased learning process, following the approach used in Coconut. This curriculum consists of multiple stages where natural language thoughts are progressively replaced with latentspace thoughts to train the latent reasoning model (analogous to using fine-tuned Coconut model as the System-2 student). We then apply the same shortcut learning procedure to train System-1.5 Reasoning. We follow Coconuts original training schedule, using six epochs in the initial stage and three in each subsequent stage. For comparison, we also evaluate CODI, distillation-based latent reasoning model that performs competitively in Table 1, as the System-2 student for shortcut learning. Figure 3 shows the results using different System-2 students to train System-1.5 model. We observe significant accuracy drop in the final System-1.5 Reasoning performance when using Coconut as the System-2 student. In addition to Coconuts suboptimal performance observed in Table 1, another possible reason for the gap is that its hard curriculum schedule between language and latent-space reasoning limits the flexibility and completeness of latent-space modeling. In contrast, System-1.5 Reasoning leverages both horizontal and vertical shortcut across decoding steps and model depth, requiring more flexible latent reasoning structure that hard curriculum distillation cannot effectively support. Figure 4: Controllable test-time scaling by tuning the depth exit threshold λdepth and decoding step constant λstep in System-1.5 Reasoning. The red grid-aligned Pareto boundary highlighting the frontier of configurations surpassing the CoT baseline (46.94% accuracy in Table 1). Joint learning and full-parameter shortcut learning degrade the performance of System-1.5 Reasoning. We further investigate the training strategy of System-1.5 Reasoning by exploring two variants: (1) Joint learning, where languageto-latent alignment and shortcut learning are conducted simultaneously, i.e. , distilling the hidden states of natural language thought directly into the shortcut-exited hidden states; and (2) Full8 parameter shortcut learning, where instead of freezing the original Transformer parameters, all model parameters are updated during shortcut learning. As shown in Figure 3, both joint learning and full-parameter shortcut learning degrade the final performance of System-1.5 Reasoning, with joint learning exhibiting more pronounced drop. We attribute this to optimization conflicts between the two groups of parameters: Transformer parameters responsible for latent reasoning and router parameters responsible for shortcut routing. These conflicts likely hinder the models ability to simultaneously preserve flexible latent reasoning paths and optimize dynamic shortcut decisions. System-1.5 Reasoning enables flexible budget-controllable test-time scaling. Unlike languagespace reasoning, where scaling test-time computation often requires exploring complex structural dependencies (e.g. , tree-like exploration) or enforcing solution consistency across multiple trajectories (Zhang et al., 2025), System-1.5 Reasoning allows for fine-grained control over computational budgets through simple threshold tuning during inference. Specifically, System-1.5 Reasoning inherently introduce two control parameters: the depth exit threshold λdepth, which adjusts the adaptive computation depth for each decoding step (vertically across model layers, where λdepth = 1 forces the computation to proceed to the final layer as in standard Transformer) , and the decoding step constant λstep, which determines when to halt intermediate latent thought generation and output final answer (horizontally across decoding steps). Figure 4 shows the performance under different configurations of λdepth and λstep. We observe that performance is approximately equally sensitive to adjustments along both dimensions, further validating the motivation for adaptive reasoning across both the model depth and decoding steps. Moreover, depth scaling saturates more quickly, and deeper reasoning demands significantly higher training computation, suggesting synergistic relationship between train-time scaling and flexible test-time scaling in System-1.5 Reasoning."
        },
        {
            "title": "4 Related Works",
            "content": "Efficient reasoning models. Efficient reasoning models aim to mitigate the inefficiencies caused by verbose outputs and the overthinking phenomenon (Chen et al., 2024; Team et al., 2025). One line of work focuses on improving language-space efficiency, using length budgeting through prompting (Lee et al., 2025) and fine-tuning (Liu et al., 2024; Yu et al., 2024; Luo et al., 2025a). For example, Chainof-Draft (Xu et al., 2025a) applies token budget-aware prompting to guide the model toward more concise reasoning, while S1 (Muennighoff et al., 2025) introduces budget-forcing strategy to terminate the thinking process early. Another line of work focuses on compressing language-space reasoning into latent space, exemplified by iCoT, Coconut (Hao et al., 2024), and CCoT (Cheng & Van Durme, 2024), which train models to reason through compact internal representations. Pause token (Goyal et al., 2023) and filler token (Pfau et al., 2024) further extend latent-space reasoning by inserting special tokens that delay output and allow for additional latent computation. More recently, Token Assorted (Su et al., 2025) mixes latent and language thoughts by abstracting early steps into discrete latent codes, SoftCoT (Xu et al., 2025b) mitigates catastrophic forgetting in latent reasoning via prompt tuning, and CODI (Shen et al., 2025) applies hidden-state distillation to enhance latent reasoning performance. System-1.5 Reasoning builds upon latent-space reasoning but further optimizes efficiency by adaptively allocating computation to handle reasoning steps with varying complexity. Conditional computation. Conditional computation techniques aim to selectively apply heavy computation only to the important parts of an input, thereby reducing unnecessary resource usage. One line of work focuses on system or model switching (Qu et al., 2025), where inputs are routed between different reasoning systemsfor example, fast System-1 and slower, more deliberate System-2. System-1.x (Saha et al., 2024) combines linear reasoning chains and search trees to enable fast and accurate planning in maze navigation tasks, while FaST (Sun et al., 2024a) uses switch adapters to dynamically toggle between System-1 and System-2 modes based on task complexity factors such as visual uncertainty or occlusion. Another line of work explores sparse activation via mixture-of-experts models (Jiang et al., 2024; Raposo et al., 2024), where only subset of the model is activated during inference. Beyond expert routing, dynamic depth models adaptively apply only portion of the Transformer layers. Early exit (EE) and skip-layer techniques are common approaches: DeeBERT (Xin et al., 2020) and CascadeBERT (Li et al., 2021) apply EE in encoder9 only architectures, while more recent decoder-only LLMs like LITE (Varshney et al., 2023) and SkipDecode (Del Corro et al., 2023) employ confidence-based exits to speed up inference. Other approaches, such as CoLT5 (Ainslie et al., 2023) and LayerSkip (Elhoushi et al., 2024), perform binary routing to skip redundant attention layers or entire blocks. System-1.5 Reasoning draws inspiration from this line of work and extends it to the latent reasoning space. It not only adapts computation along the model depth via early exits but also introduces dynamic shortcuts across decoding steps, enabling reasoning to proceed efficiently both vertically and horizontally."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce System-1.5 Reasoning, novel reasoning framework that improves inference efficiency in large language models by enabling dynamic shortcut reasoning within latent space. System-1.5 Reasoning adaptively allocates computation based two forms of latent-space shortcuts: depth shortcuts, which modulate layer-wise computation, and step shortcuts, which streamline decoding steps. To support this adaptive reasoning, System-1.5 Reasoning is trained via twostage distillation process: first aligning language-space and latent-space reasoning, and then learning shortcut execution paths guided by stepwise reasoning criticality. This training paradigm ensures that the model captures both the expressiveness of System-2 reasoning and the efficiency of System-1-style shortcuts. Experimental results on challenging reasoning benchmarks demonstrate that System-1.5 Reasoning preserves the accuracy of traditional chain-of-thought methods while accelerating inference by more than 20. These results highlight the potential of System-1.5 Reasoning as an effective and scalable latent-space reasoning framework for future LLM deployment."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontanon, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. CoLT5: Faster long-range transformers with conditional computation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 50855100, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.309. URL https://aclanthology.org/2023. emnlp-main.309/. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-ofthought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. arXiv preprint arXiv:2307.02628, 2023. 10 Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv: 2311.01460, 2023. Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning, pp. 84698488, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. arXiv preprint arXiv:1910.10073, 2019. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. LayerSkip: Enabling early exit inference and self-speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1262212642, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.681. URL https://aclanthology.org/2024.acl-long.681/. Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, and Hao Zhang. Efficiently serving llm reasoning programs with certaindex. arXiv preprint arXiv:2412.20993, 2024. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. doi: 10.1162/tacl_a_00370. URL https://aclanthology.org/2021.tacl-1.21/. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. 11 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning, pp. 19274 19286, 2023. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, and Xu Sun. CascadeBERT: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 475486, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.43. URL https://aclanthology.org/2021.findings-emnlp.43/. Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, and Qipeng Guo. Fastmcts: simple sampling strategy for data synthesis. arXiv preprint arXiv:2502.11476, 2025a. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025b. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990, 2025. Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025a. 12 Yijia Luo, Yulin Song, Xingyao Zhang, Jiaheng Liu, Weixun Wang, GengRu Chen, Wenbo Su, and Bo Zheng. Deconstructing long chain-of-thought: structured reasoning optimization framework for long cot distillation. arXiv preprint arXiv:2503.16385, 2025b. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL https://openai.com/ blog/chatgpt/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022. Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal. System-1. x: Learning to balance fast and slow planning with language models. arXiv preprint arXiv:2407.14414, 2024. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275, 2025. Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. Visual agents as fast and slow thinkers. arXiv preprint arXiv:2408.08862, 2024a. 13 Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290, 2024b. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for markov llm test-time scaling. arXiv preprint arXiv:2502.12018, 2025. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Accelerating llama inference by enabling intermediate layer decoding via instruction tuning with lite. arXiv preprint arXiv:2310.18581, 2023. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Xiaoqiang Wang and Bang Liu. Oscar: Operating system control via state-aware reasoning and re-planning. arXiv preprint arXiv:2410.18963, 2024. Xiaoqiang Wang, Lingfei Wu, Tengfei Ma, and Bang Liu. FAC2E: Better understanding large language model capabilities by dissociating language and cognition. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1322813243, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.734. URL https: //aclanthology.org/2024.emnlp-main.734/. Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. R3mem: Bridging memory retention and retrieval via reversible compression. arXiv preprint arXiv:2502.15957, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. Sifan Wu, Huan Zhang, Yizhan Li, Farshid Effaty, Amirreza Ataei, and Bang Liu. Seeing beyond words: Matvqa for challenging visual-scientific reasoning in materials science. arXiv preprint arXiv:2505.18319, 2025. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 22462251, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.204. URL https://aclanthology.org/2020.acl-main.204/. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025a. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025b. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025. Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning generalist model for embodied navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1362413634, 2024."
        }
    ],
    "affiliations": [
        "Apple",
        "Canada CIFAR AI Chair",
        "DIRO & Institut Courtois, Université de Montréal",
        "Mila - Quebec AI Institute"
    ]
}