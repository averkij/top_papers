{
    "paper_title": "Branched Schrödinger Bridge Matching",
    "authors": [
        "Sophia Tang",
        "Yinuo Zhang",
        "Alexander Tong",
        "Pranam Chatterjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 0 0 9 0 . 6 0 5 2 : r Branched Schrödinger Bridge Matching Sophia Tang1, Yinuo Zhang2, Alexander Tong3,4, Pranam Chatterjee5,6, 1Department of Computer and Information Science, University of Pennsylvania 2Center of Computational Biology, Duke-NUS Medical School 3Mila, Quebec AI Institute, 4Université de Montréal 5Department of Biomedical Engineering, Duke University 6Department of Computer Science, Duke University Corresponding author: pranam.chatterjee@duke.edu"
        },
        {
            "title": "Abstract",
            "content": "Predicting the intermediate trajectories between an initial and target distribution is central problem in generative modeling. Existing approaches, such as flow matching and Schrödinger Bridge Matching, effectively learn mappings between two distributions by modeling single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from common origin to multiple distinct outcomes. To address this, we introduce Branched Schrödinger Bridge Matching (BranchSBM), novel framework that learns branched Schrödinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations."
        },
        {
            "title": "Introduction",
            "content": "While generative frameworks such as denoising diffusion [Austin et al., 2021] and flow matching [Lipman et al., 2023] have demonstrated strong performance in learning mappings from noisy priors to clean data distributions, many real-world problems call for different paradigm. Tasks like crowd navigation and modeling cell-state transitions under perturbation involve learning transport map between two empirically observed endpoint distributions, rather than sampling from predefined prior. The Schrödinger Bridge (SB) [Schrödinger, 1931] problem seeks to identify an optimal stochastic map between pair of endpoint distributions that minimizes the KullbackLeibler (KL) divergence to an underlying reference process. Schrödinger Bridge Matching (SBM) solves the SB problem by parameterizing drift field that matches mixture of conditional stochastic bridges between endpoint pairs that each minimize the KL divergence from known reference process. Extensions such as Generalized SBM [Liu et al., 2023a] reformulate the standard SBM as conditional stochastic optimal control (CondSOC) problem, learning drift fields that minimize kinetic energy alongside task-specific state cost. Typically, SBM assumes conservation of mass from the initial to the target distribution, which fails to capture dynamical population behaviors such as growth and destruction of mass, commonly seen in single-cell population data. Furthermore, prior works focus on transporting samples from pair of unimodal initial and target distributions via single, continuous trajectory, without accounting for branching dynamics [Tong et al., 2024a, Theodoropoulos et al., 2024, Liu et al., 2022, De Bortoli et al., 2021a], where uniform population follows branched trajectory that diverges toward multiple distinct target distributions. The notion of branching is central to many real-world systems. For example, when homogeneous cell population undergoes perturbation such as gene knockouts or drug treatments, it frequently induces fate bifurcation as the cell population splits into multiple phenotypically distinct outcomes or commits to divergent cell fates [Shalem et al., 2014, Zhang et al., 2025a]. These trajectories are observable in single-cell RNA sequencing (scRNA-seq) data, where each subpopulation independently evolves and undergoes growth or contraction along its trajectory toward distinct terminal state. In this work, we introduce Branched Schrödinger Bridge Matching (BranchSBM), novel framework for learning stochastic transport maps from an unimodal initial distribution to multiple target distributions via branched trajectories. BranchSBM solves the branched Schrödinger Bridge problem by parameterizing diverging velocity fields and branch-specific growth rates, which together define set of conditional stochastic bridges from the common source to each terminal distribution. This formulation enables the modeling of population-level stochastic processes that account for both transport and growth dynamics, jointly minimizing energy across branches while matching the mass and structure of each target distribution. Our main contributions can be summarized as follows: 1. We define the Branched Generalized Schrödinger Bridge problem and introduce BranchSBM, novel matching framework that learns optimal branched trajectories from an initial distribution π0 to multiple target distributions {πt,k}. 2. We derive the Branched Conditional Stochastic Optimal Control (CondSOC) problem as the sum of Unbalanced CondSOC objectives and leverage multi-stage training algorithm to learn the optimal branching drift and growth fields that transport mass along branched trajectory. 3. We demonstrate the unique capability of BranchSBM to model dynamic branching trajectories while matching multiple target distributions across various problems, including 3D navigation over LiDAR manifolds (Section 5.1), modeling differentiating single-cell population dynamics (Section 5.2), and predicting heterogeneous cell states after perturbation (Section 5.3)."
        },
        {
            "title": "2 Preliminaries",
            "content": "Schrödinger Bridge Given reference probability path measure Q, the Schrödinger Bridge (SB) problem aims to find an optimal path measure PSB that minimizes the Kullback-Leibler (KL) divergence with while satisfying the boundary distributions P0 = π0 and P1 = π1. PSB = min {KL(PQ) : P0 = π0, P1 = π1} (1) where is commonly defined as standard Brownian motion. For an extended background and formal definition of Schrödinger Bridges, refer to Definition 3 and Appendix A.1. Generalized Schrödinger Bridge Problem The solution to the standard SB problem minimizes the kinetic energy of the conditional drift term ut(Xt) that preserves the endpoints drawn from the coupling (x0, x1) π0,1 defined as (cid:90) 1 0 min ut Eptut(Xt)2dt s.t. (cid:26)dXt = ut(Xt)dt + σdBt X0 π0, X1 π (2) where dBt is standard d-dimensional Brownian motion. The evolution of the marginal probability density pt of the state variable Xt is governed by the Fokker-Planck equation [Risken, 1996] given by pt = (ptut) + 1 2 σ2pt, p0 = π0, p1 = π (3) where we say ut generates pt. To define more complex systems where the optimal dynamics cannot be accurately captured by minimizing the standard squared-Euclidean cost in entropic OT [Vargas et al., 2021], the Generalized Schrödinger Bridge (GSB) problem introduces an additional non-linear state-cost Vt(Xt) [Chen et al., 2021a, Chen and Georgiou, 2016, Liu et al., 2022]. The minimization objective becomes (cid:90) 1 0 min ut Ept (cid:20) 1 2 (cid:21) ut(Xt)2 + Vt(Xt) dt (4) such that ut, pt satisfy the FP equation in (3). The state cost can also be interpreted as the potential energy of the system at state Xt. Figure 1: Branched Schrödinger Bridge Matching (A) Stage 1 trains correction term that learns the optimal interpolant conditioned on endpoints (B) Stage 2 and 3 trains separate flow and growth network for each branch independently (C) Stage 4 jointly optimizes the flow and growth networks to minimize the energy, mass, and matching loss."
        },
        {
            "title": "3 Branched Schrödinger Bridge Matching",
            "content": "To model branched trajectories from an initial distribution π0 to multiple target distributions {πt,k}, we introduce Branched Schrödinger Bridge Matching (BranchSBM), unique matching framework that models branching of diverging distributions along the Schrödinger bridge path. First, we define the Unbalanced GSB Problem and prove that it can be tractably solved as Unbalanced Conditional Stochastic Optimal Control (CondSOC) problem over paired endpoint samples in the dataset. Then, we formulate the Branched GSB problem as the solution to the sum of Unbalanced GSB problems for each branch. Finally, we solve the Branched GSB problem by parameterizing the velocity and growth rates of each branch with neural networks. 3.1 Unbalanced Conditional Stochastic Optimal Control Unbalanced Generalized Schrödinger Bridge Problem Extending the definition of the Generalized Schrödinger Bridge (GSB) problem in Equation 4, we define the Unbalanced GSB problem by scaling the minimization objective by time-dependent weight wt(Xt) = w0 + (cid:82) 0 gs(Xs)ds that evolves according to time-varying growth rate gt(Xt) : Rd [0, 1] R. (cid:90) 1 0 Ept (cid:20) 1 2 min ut,gt ut(Xt)2 + Vt(Xt) (cid:21) wt(Xt)dt s.t. dXt = ut(Xt)dt + σdBt X0 π0, X1 = π1 w0(X0) = 0 , w1(X1) = 1 (5) Unbalanced Conditional Stochastic Optimal Control (CondSOC) Now, we show that we can solve the Unbalanced GSB problem as an Unbalanced CondSOC problem where the optimal drift ut and growth gt minimize the expectation of the objective in (5) conditioned on pairs of endpoints. Proposition 1 (Unbalanced Conditional Stochastic Optimal Control). Suppose the marginal density can be decomposed as pt(Xt) = (cid:82) pt(Xtx0, x1)p0,1(x0, x1)dπ0,1, where π0,1 is fixed joint coupling of the data. Then, we can identify the optimal drift that solves the Unbalanced GSB problem in (5) by minimizing the Unbalanced Conditional Stochastic Optimal Control objective given by and growth π0,1 (cid:20) 1 2 s.t. dXt = ut(Xtx0, x1)dt + σdBt, X0 = x0, X1 = x1 w0(X0) = (cid:21) ut(Xtx0, x1)2 + Vt(Xt) E(x0,x1)π0,1 Ept0,1 min ut,gt (cid:20)(cid:90) 1 0 wt(Xt)dt (cid:21) 0 , w1(X1) = 1 (6) (7) where wt = w0 + (cid:82) 0 gs(Xs)ds is the time-dependent weight initialized at the growth rate, and π0,1 is the weighted coupling of paired endpoints (x0, 0, ut is the drift, gt is 0, x1, 1) π0,1. The proof is provided in Appendix C.1. This defines the objective for us to tractably solve the Unbalanced GSB problem by conditioning on finite set of endpoint pairs in the dataset. 3.2 BranchSBM: Sum of Unbalanced CondSOC Problems Branched Generalized Schrödinger Bridge Problem Given the Unbalanced GSB problem, we define the Branched GSB problem as minimizing the sum of Unbalanced GSB problems across all branches. All mass begins along primary path indexed = 0 with initial weight 1. Over [0, 1], mass is transferred across secondary branches with initial weight 0 and target weight w1,k such that it minimizes the objective defined as min {ut,k,gt,k}K k=0 (cid:26) Ept, (cid:90) 1 0 (cid:20) 1 2 ut,0(Xt,0)2 + Vt(Xt,0) (cid:21) wt,0 + (cid:88) k=1 Ept,k (cid:20) 1 2 ut,k(Xt,k)2 + Vt(Xt,k) (cid:21) (cid:27) wt,k dt s.t. dXt,k = ut,k(Xt,k)dt + σdBt, X0 = x0, X1,k = x1,k, w0,k = δk=0, w1,k = w1,k (8) When total mass across branches is conserved, we enforce (cid:80)K k=0 wt,k = 1 for all [0, 1], which constrains the growth rates such that gt,0(Xt,0) + (cid:80)K k=1 gt,k(Xt,k) = 0. This ensures that mass lost from the primary branch (when gt,0 < 0) is redistributed among the secondary branches (where gt,k > 0). The primary branch evolves from initial weight of 1 according to wt,0 = 1 + (cid:82) 0 gs(Xs,0)ds and the secondary branches grow from the primary branch from weight 0 according to wt,k = (cid:82) 0 gs(Xs,k)ds. Branched Conditional Stochastic Optimal Control Following similar procedure as shown for the Unbalanced GSB problem, we can reformulate the Branched GSB problem as solving the Branched CondSOC problem where we optimize set of parameterized drift {ut,k}K k=0 and growth {gt,k}K k=0 networks by minimizing the energy of the conditional trajectories between paired samples (x0, {x1,k}K Proposition 2 (Branched Conditional Stochastic Optimal Control). For each branch, let pt,k(Xt,k) = Ep0,1k [pt,k(Xt,kx0, x1,k)], where π0,1,k is the joint coupling distribution of samples x0 π0 from the initial distribution and x1,k π1,k from the kth target distribution. Then, we can identify the set of optimal drift and growth functions {u t,k}K k=0 that solve the Branched GSB problem in (3.2) by minimizing sum of Unbalanced CondSOC objectives given by k=0) {p0,1,k}K t,k, k=0. min {ut,k,gt,k}K k=0 E(x0,x1,0)π0,1,0 (cid:90) 1 0 (cid:26) Ept0,1, (cid:20) 1 2 ut,0(Xt,0)2 + Vt(Xt,0) (cid:21) wt,0 + (cid:88) k=1 E(x0,x1,k)π0,1,k (cid:90) 1 0 Ept0,1,k (cid:20) 1 ut,k(Xt,k)2 + Vt(Xt,k) (cid:21) (cid:27) wt,k dt (9) s.t. dXt,k = ut,k(Xt,k)dt + σdBt, X0 = x0, X1,k = x1,k, w0,k = δk=0, w1,k = w1,k (10) where wt,0 = 1 + (cid:82) (cid:82) 0 gs,k(xs,k)ds are the weights of the secondary branches initialized at 0. 0 gs,1(xs,1)ds is the weight of the primary paths initialized at 1 and wt,k = The proof is given in Appendix C.2. This defines the objective for us to tractably solve the Branched GSB problem in Section 4 by conditioning on discrete set of branched endpoint pairs in the dataset. Remark 1. When gt,0(Xt,0) = 0 and gt,k(Xt,k) = 0 for all [0, 1] and {0, . . . , K}, then the Branched CondSOC problem is the solution to the single path GSB problem."
        },
        {
            "title": "4 Learning BranchSBM Using Neural Networks",
            "content": "Given an initial data distribution π0 and + 1 target distributions {π1,k}K the optimal drift and growth fields {u Proposition 2 by parameterizing {uθ t,k}K k=0 with neural networks. k=0, we aim to learn k=0 that solve the Branched CondSOC problem in t,k, t,k}K t,k, gϕ 4.1 Branched Neural Interpolant Optimization Since the optimal trajectory under the state cost Vt(Xt) follows non-linear cost manifold, given pair of endpoints (x0, x1,k), we train neural path interpolant φt,η(x0, x1,k) : Rd Rd [0, 1] Rd 4 that defines the intermediate state xt,η,k and velocity xt,η,k = txt,η,k at time t, which minimizes (2). We define xt,η,k to be bounded at the endpoints as given by xt,η,k = (1 t)x0 + tx1,k + t(1 t)φt,η(x0, x1,k) (11) xt,η,k = x1 x0 + t(1 t) φt,η(x0, x1,k) + (1 2t)φt,η(x0, x1,k) (12) To optimize φt,η(x0, x1,k) such that it predicts the energy-minimizing trajectory, we minimize the trajectory loss Ltraj defined as (cid:88) (cid:90) 1 Ltraj(η) = E(x0,x1,k)π0,1,k xt,η,k2 (cid:21) 2 + Vt(xt,η,k) dt (cid:20) 1 2 (13) k=0 0 After convergence, Stage 1 returns the network φ t,η(x0, x1,k) that generates the optimal conditional velocity t,η,k which defines the matching objective in Stage 2. In Stage 2, we parameterize set t,k(xt,k) : Rd [0, 1] Rd that generates the mixture of bridges defined in of neural drift fields uθ Stage 1 by minimizing the conditional flow matching loss [Lipman et al., 2023, Tong et al., 2024b]. Lflow(θ) = (cid:88) (cid:90) k=0 0 E(x0,x1,k)π0,1,k (cid:13) (cid:13) t,η,k uθ t,k(xt,k)(cid:13) 2 (cid:13) dt (14) Proposition 3 (Solving the GSB Problem with Stage 1 and 2 Training). Stage 1 and Stage 2 training yield the optimal drift (Xt) that solves the GSB problem in (4). (Xt) that generates the optimal marginal probability distribution The proof is provided in Appendix C.3. Since the drift for each branch uθ t,k(Xt) are trained independently in Stage 2, we can extend this result across all + 1 branches and conclude that the sequential Stage 1 and Stage 2 training procedures yields the optimal set of drifts {u k=0 that generate the optimal probability paths {p k=0 that solves the GSB problem for each branch. t,k}K t,k}K 4.2 Learning the Energy-Minimizing Branching Dynamics Branched Energy Loss To solve the Branched CondSOC problem defined in Proposition 2, we minimize branched energy loss Lenergy defined as Lenergy(θ, ϕ) = (cid:90) 0 {pt,k}K k=0 (cid:26) (cid:20) 1 2 (cid:124) uθ t,0(xt,0)2 + Vt(xt,0) (cid:123)(cid:122) primary trajectory (cid:21) wϕ t,0 + (cid:125) (cid:20) 1 2 (cid:88) k=1 (cid:124) s.t. wϕ t,0 = 1 + (cid:90) 0 s,1(xs,1)ds, wϕ gϕ t,k = (cid:90) gϕ s,k(xs,k)ds uθ t,k(xt,k)2 + Vt(xt,k) (cid:21) (cid:27) dt wϕ t,k (cid:123)(cid:122) branches (cid:125) (15) where (x0, x1,0) are the endpoints of the primary path. At time = 0, the primary path has weight 1 and the branches have weights 0. Over [0, 1], the weight of the primary path changes according to gϕ t,k(xt,k) 0 (Lemma 2). Intuitively, the branched energy loss optimizes the branching growth rates such that they are non-zero when branching is favored over the primary path. t,0(xt,0) and supplies mass to the branches, which grow at rates gϕ Weight Matching Loss We define weight matching loss Lmatch that aims to minimize the difference between the predicted weights of each branch at = 1, obtained by integrating the growth function gϕ t,k(Xt) over [0, 1], and the true weights of each terminal distribution {w 1,k}K k=0. Lmatch(ϕ) = (cid:88) k= Ep1,k (cid:16) wϕ 1,k(x1,k) 1,k (cid:17) , s.t. wϕ 1,k(x1,k) = w0,k + (cid:90) 1 0 gϕ t,k(xt,k)dt (16) where 1,k = Nk/Ntotal is the fraction of the population in the kth target distribution. Mass Conservation Loss To ensure that the growth rate satisfies conservation of total mass at all times [0, 1], we define mass loss Lmass that enforces the sum of the weights of all + 1 branches matches the true total weight at time denoted as wtotal . (cid:88) (cid:34)(cid:18) (cid:88) (cid:90) 1 (cid:19) (cid:16) (cid:17) (cid:35) wϕ t,k(xt,k) wtotal + max 0, wϕ t,k(xt,k) dt (17) Lmass(ϕ) = {pt,k}K k=0 k=0 k=0 5 where max(0, wϕ balanced branched SBM problem where the total mass is conserved, we have wtotal t,k) assigns an additional linear penalty for negative weight predictions. For the = 1. Training the Growth Networks weights of the flow networks {uθ additional growth penalty term gϕ t,k}K t,k2 In Stage 3, we train the growth networks {gϕ k=0 by fixing the k=0 and minimizing the weighted combined loss Lgrowth with an t,k}K 2 to ensure coercivity of Lgrowth Lgrowth(ϕ) = λenergyLenergy(θ, ϕ) + λmatchLmatch(ϕ) + λmassLmass(ϕ) + λgrowth (cid:88) k=0 gϕ t,k2 2 (18) We show in Lemma 2 that the optimal growth rates across the secondary branches are nondecreasing; however, mass destruction can still be modeled by defining an additional branch with target weight equal to the ratio of mass lost over [0, 1]. To ensure that the set of optimal growth functions exists, we establish Proposition 4 (see proof in Appendix C.4). Proposition 4 (Existence of Optimal Growth Functions). Assume the state space Rd is bounded domain within Rd. Let the optimal probability density of branch be known non-negative function bounded in [0, 1], denoted as t,k : [0, 1] [0, 1] L(X [0, 1]). By Lemma 2, we can define the set of feasible growth functions in the set of square-integrable functions L2 as := {g = (gt,0, . . . , gt,K) L2(X [0, 1]) gt,k(x) : [0, 1] , gt,k(x) 0} (19) Let the growth loss be the functional L(g) : L2(X [0, 1]) R. Then, there exists an optimal function = (g t,k such that L(g) = inf gG L(g) which can be obtained by minimizing L(g) over G. t,K) L2 where t,0, . . . , Final Joint Training In the final Stage 4, we train the weights for both the flow and growth networks {uθ k=0 by minimizing Lgrowth from Stage 3 in addition to reconstruction loss Lrecons that ensures the endpoint distribution at time = 1 is maintained. t,k, gϕ t,k}K Lrecons(θ) = (cid:88) k=0 Ep1,k (cid:88) max x1Nn(x1,k) (cid:18) 0, x1,k x1,k2 ϵ (20) (cid:19) where Nn(x1,k) is the set of n-nearest neighbors to the reconstructed state x1,k p1,k at time = 1 from the data points x1,k π1,k at time = 1. Our multi-stage training scheme decomposes the Branched CondSOC problem into two parts. We first independently learn an optimal drift field for each branch, which is vector field over the state space that propagates mass flow in the direction of each target distribution. Then, we fix the drift fields and learn the growth dynamics that determine the optimal distribution of mass over the branches."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate BranchSBM on variety of branched matching tasks with different state costs Vt(Xt), including multi-path LiDAR navigation (Section 5.1), modeling differentiating single-cell population dynamics (Section 5.2), and predicting heterogeneous cell-states after perturbation (Section 5.3). For all tasks, we leverage the multistage training approach in Section E.1 to train set of flow {uθ t,k}K k=0 for each branch. We demonstrate that BranchSBM can accurately learn branched Schrödinger bridges with diverse state costs and data types. k=0 and growth neural networks {gϕ t,k}K 5.1 Branched LiDAR Surface Navigation First, we evaluate BranchSBM for navigating branched paths along the surface of 3-dimensional LiDAR manifold, from an initial distribution to two distinct target distributions (Figure 3). Setup We define single initial Gaussian mixture π0 and two target Gaussian mixtures π1,0, π1,1 on either side of the mountain (Figure 3). We sample 5000 points i.i.d. from each of the Gaussian mixtures and assign all endpoints target weight of w1,0 = w1,1 = 0.5. To ensure trajectories follow the LiDAR manifold, we define the state cost LAND (Xt) as the data-dependent LAND metric [Kapusniak et al., 2024, Arvanitidis et al., 2016], which assigns lower costs in regions near coordinates in the LiDAR dataset. Further experimental details are provided in Appendix E.3. 6 surface distances Table 1: Benchmark of BranchSBM single-branch SBM on against navigation. multi-path (W1 Wasserstein and W2) between the reconstructed and ground-truth distributions with Nsteps = 100 Euler steps at time = 1 from validation samples in the initial distribution. Results are averaged over 5 independent runs. Model W1 () W2 () Single Branch SBM 0.9750.009 0.2390.001 BranchSBM 1.2850.007 0.3090.003 Figure 2: Plot of weight (left) and energy (right) calculated with (15) of each branch over time [0, 1]. Mass is transferred from the primary branch to branch 1, and both converge to the target weight of 0.5 at = 1. Both plots represent the average over trajectories from samples in the validation set. Figure 3: Application of BranchSBM on Learning Branched Paths on LiDAR Manifold. Plots of the initial and target distributions, learned interpolants, and learned branched trajectories on the LiDAR manifold. Results We show that BranchSBM can learn distinct, non-linear branched paths that curve along the 3-dimensional LiDAR manifold while minimizing the kinetic energy and state-cost. From the mass and energy curves in Figure 2, we see that mass begins in the primary branch (branch 0) and is gradually transferred to the secondary branch (branch 1) over [0, 1], with both curves converging to the target weight of 0.1 at = 1. As mass is transferred, the slope of the cumulative energy curve decreases in branch 0 and increases in branch 1, reflecting the true energy dynamics. In Figure 3, we observe that the branching occurs at the edge of the inclined mountain, indicating that the model can determine the optimal branching time based on the paths of lowest potential energy. As shown in Table 1, BranchSBM reconstructs the endpoint distributions with significantly higher accuracy in comparison to single-branch SBM. In total, we demonstrate the capability of BranchSBM to learn branched trajectories on complex 3D manifolds. 5.2 Differentiating Single-Cell Population Dynamics BranchSBM is uniquely positioned to model single-cell population dynamics where homogeneous cell population (e.g., progenitor cells) differentiates into several distinct subpopulation branches, each of which independently undergoes growth dynamics. Here, we demonstrate this capability on mouse hematopoiesis data. Setup We use dataset consisting of mouse hematopoiesis scRNA-seq data analyzed by lineage tracing technique from [Sha et al., 2023, Weinreb et al., 2020]. This data contains three time points ti for {0, 1, 2} that are projected to two-dimensional representations R2 referred to as force-directed layouts or SPRING plots. From the plotted data, we can observe two clear branches that indicate the differentiation of progenitor cells into two distinct cell fates (Figure 4). We use k-means clustering to define two distinct target distributions π1,0 and π1,1 of samples at time t2 and set their target weights equal to w1,0 = w1,1 = 0.5 due to the equal ratio of cells (Figure 9). We used samples across all time steps ti for {0, 1, 2} to define the data manifold via the LAND metric LAND . BranchSBM was trained on pairs sampled only from t0 and t2, and samples from t1 were t,η held out for evaluation. For comparison, we trained single-branch SBM model with both clusters at t2 as the target distribution. 7 Figure 4: Application of BranchSBM on Modeling Differentiating Single-Cell Population Dynamics. Mouse hematopoiesis scRNA-seq data is provided for three time points t0, t1, t2. (A) Simulated states (top) and trajectories (bottom) at time t1 using single-branch SBM. (B) Simulated states with BranchSBM at t1 (t = 0.5) and (C) t2 (t = 1). (D) Learned trajectories over the interval [t0, t2] on validation samples. Table 2: Results for Modeling Single-Cell Differentiation. Wasserstein distances (W1 and W2) between simulated and ground-truth cell distributions at time t1 and t2 on the validation dataset. BranchSBM reconstructs both intermediate and terminal states significantly better than single-branch SBM. Results are averaged over 5 independent runs. Results After evaluating the reconstructed distributions at the intermediate held-out time point t1 and final time point t2 (t = 1) from simulating validation samples from the initial distribution x0 π0. In Figure 4A, we observe that single-branch SBM trained with single target distribution p1 containing both terminal fates fails to learn distinct branched trajectories, and the simulated cell states at time t2 do not reach either of the terminal distributions. In contrast, we show that BranchSBM simulates branched states at intermediate time steps not included in the training data while accurately reconstructing both target distributions with significantly lower 1-Wasserstein and 2-Wasserstein distances compared to the singlebranch SBM model (Figure 4B-D; Table 2). 0.3660.034 0.2100.042 0.5820.020 0.9400.075 0.7030.008 1.0370.074 0.4790.044 0.2650.046 Single Branch SBM BranchSBM W2 () W1 () W2 () W1 () Time Model t1 t2 5.3 Cell-State Perturbation Modeling Predicting the effects of perturbation on cell state dynamics is crucial problem for therapeutic design. In this experiment, we leverage BranchSBM to model the trajectories of single cell line from single homogeneous state to multiple heterogeneous states after drug-induced perturbation. We demonstrate that BranchSBM is capable of capturing the dynamics of high-dimensional gene expression data and learning branched trajectories that accurately reconstruct diverging perturbed cell populations. Setup For this experiment, we extract the data for single cell line (A-549) under two drug perturbation conditions selected based on cell abundance and response diversity from the Tahoe100M dataset [Zhang et al., 2025a]. Clonidine at 5 µL was selected first due to having the largest number of cells at this dosage, while Trametinib was chosen as the second drug based on its secondhighest cell count under the same condition. Since both drugs had over 60K genes, we selected the top 2000 highly variable genes (HVGs) based on normalized expression and performed principal component analysis (PCA) to find the top PCs that capture the variance in the data. We set the initial distribution at = 0 to be control DMSO-treated cell population and the target distributions at = 1 to be distinct clusters in the drug-treated cell population. After clustering, we identified two divergent clusters in the Clonidine-perturbed population and three in the Trametinib-perturbed population (Appendix Figure 10). To determine the weights of each branch, we take the ratio of each 8 Figure 5: Results for Clonidine Perturbation Modeling. (A) Gene expression data of DMSO control (set to = 0) and cell states (set to = 1) after Clonidine perturbation with two distinct endpoints (pink and purple). (B) The simulated trajectories for single-branch SBM on the top 50 PCs with both clusters. All samples take the low-energy path without reaching the second cluster. (C) The simulated endpoints of the top 50, 100, and 150 PCs at = 1 on the validation data for each branch. cluster with respect to the total perturbed cell population (Appendix Table 8). For both experiments, we simulated the top 50 PCs, which capture approximately 38% of the variance in the dataset. To further evaluate the scalability of BranchSBM on simulating trajectories in high-dimensional state spaces, we simulated the top 100 and 150 PCs for Clonidine and compared the performance across dimensions. Finally, we benchmarked both experiments against single-branch SBM, where we parameterize single branch with conserved mass that learns the trajectory from the initial distribution to the concatenation of clusters in the perturbed distribution. Table 3: Results for Clonidine Perturbation Modeling for Increasing Principal Component Dimensions. Maximum-mean discrepancy (MMD) across all PCs and Wasserstein distances (W1 and W2) of top 2 PCs between ground truth and reconstructed distributions at = 1 simulated from the validation data at = 0. Results for single-branch SBM (50 PCs) and BranchSBM (2 branches) were averaged over 5 independent runs. Model RBF-MMD () Single Branch SBM (50 PCs) Given that the intermediate trajectory between the control and perturbed state is unknown, we assume that the optimal trajectory both minimizes the kinetic energy of the drift field while minimizing the distance from the space of feasible cell states. We define the state cost Vt(Xt) with the RBF metric [Kapusniak et al., 2024, Arvanitidis et al., 2016], which pushes the intermediate trajectory to lie near states represented in the dataset. Further details are provided in Appendix E.5. BranchSBM 50 PCs 100 PCs 150 PCs 0.0650.001 0.0530.002 0.0830.001 0.2790.024 W1 () 5.1240.509 1.0760.085 1.8320.174 1.7220.064 W2 () 6.1490.463 1.2240.097 2.0370.174 1.9310.035 Clonidine Perturbation Results After multi-stage training of BranchSBM with {50, 100, 150} PCs and two branched endpoints (Figure 5A), we simulated the final perturbed state of each branch at time = 1 from the samples in the initial validation data distribution x0 π0 corresponding to the control DMSO condition. In Figure 5C, we demonstrate that BranchSBM accurately reconstructs the ground-truth distributions of endpoint 0 (top row) and endpoint 1 (bottom row) across increasing PC dimensions, capturing the location and spread of the dataset. To prove the necessity of our branched framework, we simulate the target distribution with only single endpoint distribution p1 containing both clusters with single-branch SBM and show that it only reconstructs the population of cells in endpoint 0, which represent cells closest to the control cells along PC2, and fails to differentiate cells in cluster 1 that differ from cluster 0 in higher-dimensional PCs (Figure 5B). Concretely, BranchSBM used across all PC dimensions outperforms single-branch SBM on only 50 PCs  (Table 3)  , indicating that BranchSBM is required to model complex perturbation effects in high-dimensional gene expression spaces. 9 Figure 6: Results for Trametinib Perturbation Modeling with BranchSBM. (A) Gene expression data of DMSO control (t = 0) and cells after treatment with 5µL Trametinib (t = 0) with three distinct endpoints (purple, turquoise, and pink). (B) The simulated endpoints of the top 50 PCs at = 1 on the validation data for each branch. (C) The evolution of cumulative energy across [0, 1] calculated as (15) along each branched trajectory after Stage 3 (growth with fixed drift) and Stage 4 (joint) training. (D) The evolution of mass across [0, 1] along each branched trajectory with target weights of w1,0 = 0.603, w1,1 = 0.255 and w1,2 = 0.142. Trametinib Perturbation Results We further show that BranchSBM can scale beyond two branches, by modeling the perturbed cell population of Trametinib-treated cells, which diverge into three distinct clusters (Figure 6A). We trained BranchSBM with three endpoints and single-branch SBM with one endpoint containing all three clusters on the top 50 PCs. After simulating the trajectories over time [0, 1] on the validation cells in the control population, we show that BranchSBM generates clear trajectories to all three branched endpoints (Figure 6B) and reconstructs the overall target distribution with lower error compared to single-branch SBM  (Table 4)  . In Figure 6C and D, we plot the evolution of cumulative energy calculated in Lenergy(θ, ϕ) (15) and weight of each branch over [0, 1], demonstrating that BranchSBMs multistage training scheme effectively learns the optimal trade-off between minimizing the energy across trajectories and matching the target weights of each branch. Table 4: Results for Trametinib Perturbation Modeling. Maximummean discrepancy (MMD) across all 50 PCs and Wasserstein distances (W1 and W2) of top 2 PCs between ground truth and reconstructed distributions at = 1 simulated from the validation data at = 0. Results were averaged over 5 independent runs. Single Branch SBM BranchSBM 0.2460.013 0.0530. 5.4280.234 0.8380.061 6.4260.186 0.9730.050 RBF-MMD () W1 () W2 () Model"
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce Branched Schrödinger Bridge Matching (BranchSBM), novel matching framework that solves the Generalized Schrödinger Bridge (GSB) problem from an initial distribution to multiple weighted target distributions through the division of mass across learned branched trajectories. By framing the branched SBM problem as the sum of Unbalanced Conditional Stochastic Optimal Control tasks, we parameterize the velocity and growth rates of each branch with neural networks to predict the trajectories of dynamical systems without requiring simulation. Through applications to nonlinear 3D navigation, cell differentiation, and perturbation-induced gene expression, we demonstrate that BranchSBM provides unified and flexible framework for modeling complex branched dynamics across biological and physical systems."
        },
        {
            "title": "Declarations",
            "content": "Acknowledgments We thank the Duke Compute Cluster, Pratt School of Engineering IT department, and Mark III Systems for providing database and hardware support that has contributed to the research reported within this manuscript. Author Contributions S.T. devised and developed model architectures and theoretical formulations, and trained and benchmarked models. Y.Z. advised on model design and theoretical framework, and processed data for training. S.T. drafted the manuscript and S.T. and Y.Z. designed the figures. A.T. reviewed mathematical formulations and provided advising. P.C. designed, supervised, and directed the study, and reviewed and finalized the manuscript. Data and Materials Availability The codebase will be freely accessible to the academic community at https://huggingface.co/ChatterjeeLab/BranchSBM. Funding Statement This research was supported by NIH grant R35GM155282 to the lab of P.C. Competing Interests P.C. is co-founder of Gameto, Inc. and UbiquiTx, Inc. and advises companies involved in biologics development and cell engineering. P.C.s interests are reviewed and managed by Duke University in accordance with their conflict-of-interest policies. S.T., Y.Z., and A.T. have no conflicts of interest to declare."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. International Conference on Learning Representations, 2023. Erwin Schrödinger. Über die umkehrung der naturgesetze. Verlag der Akademie der Wissenschaften in Kommission bei Walter De Gruyter . . . , 1931. Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos A. Theodorou, and Ricky T. Q. Chen. Generalized schrödinger bridge matching. International Conference on Learning Representations, 2023a. Alexander Y. Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, and Yoshua Bengio. Simulation-free Schrödinger bridges via score and flow matching. Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, 238:12791287, 2024a. Panagiotis Theodoropoulos, Nikolaos Komianos, Vincent Pacelli, Guan-Horng Liu, and Evangelos A. Theodorou. Feedback schrödinger bridge matching. The Thirteenth International Conference on Learning Representations, 2024. Guan-Horng Liu, Tianrong Chen, Oswin So, and Evangelos A. Theodorou. Deep generalized schrödinger bridge. Advances in Neural Information Processing Systems, 2022. Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 2021a. Ophir Shalem, Neville Sanjana, Ella Hartenian, Xi Shi, David Scott, Tarjei Mikkelsen, Dirk Heckl, Benjamin Ebert, David Root, John Doench, et al. Genome-scale crispr-cas9 knockout screening in human cells. Science, 343(6166):8487, 2014. Jesse Zhang, Airol Ubas, Richard de Borja, Valentine Svensson, Nicole Thomas, Neha Thakar, Ian Lai, Aidan Winters, Umair Khan, and Matthew G. et al. Jones. Tahoe-100m: giga-scale single-cell perturbation atlas for context-dependent gene function and cellular modeling. bioRxiv, 2025a. 11 Hannes Risken. Fokker-Planck Equation, page 6395. Springer Berlin Heidelberg, 1996. ISBN 9783642615443. Francisco Vargas, Pierre Thodoroff, Neil D. Lawrence, and Austen Lamacraft. Solving schrödinger bridges via maximum likelihood. arXiv preprint arXiv:2106.02081, 2021. Yongxin Chen, Tryphon T. Georgiou, and Michele Pavon. The most likely evolution of diffusing and vanishing particles: Schrodinger bridges with unbalanced marginals. arXiv preprint arXiv:2108.02879, 2021a. Yongxin Chen and Tryphon Georgiou. Stochastic bridges of linear systems. IEEE Transactions on Automatic Control, 61(2):526531, 2016. doi: 10.1109/TAC.2015.2440567. Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid RectorBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024b. Kacper Kapusniak, Peter Potaptchik, Teodora Reu, Leo Zhang, Alexander Tong, Michael Bronstein, Avishek Joey Bose, and Francesco Di Giovanni. Metric flow matching for smooth interpolations on the data manifold. Advances in Neural Information Processing Systems, 2024. Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. locally adaptive normal distribution. Advances in Neural Information Processing Systems, 2016. Yutong Sha, Yuchi Qiu, Peijie Zhou, and Qing Nie. Reconstructing growth and dynamic trajectories from single-cell transcriptomics data. Nature Machine Intelligence, 6(1):2539, 2023. Caleb Weinreb, Alejo Rodriguez-Fraticelli, Fernando D. Camargo, and Allon M. Klein. Lineage tracing on transcriptional landscapes links state to fate during differentiation. Science, 367(6479), 2020. L. C. G. Rogers and David Williams. Diffusions, Markov Processes and Martingales. Cambridge University Press, 2000. ISBN 9780511805141. Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrödinger bridge matching. Advances in Neural Information Processing Systems, 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. International Conference on Learning Representations, 2023b. Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 2021b. Alexander Korotin, Nikita Gushchin, and Evgeny Burnaev. Light schrödinger bridge. arXiv preprint arXiv:2310.01174, 2023. Charlotte Bunne, Ya-Ping Hsieh, Marco Cuturi, and Andreas Krause. The schrödinger bridge between gaussian measures has closed form. International Conference on Artificial Intelligence and Statistics, 2022a. Lénaïc Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Unbalanced optimal transport: Dynamic and kantorovich formulations. Journal of Functional Analysis, 274(11): 30903123, 2018. ISSN 0022-1236. Jun Hyeong Kim, Seonghwan Kim, Seokhyun Moon, Hyeongwoo Kim, Jeheon Woo, and Woo Youn Kim. Discrete diffusion schrödinger bridge matching for graph transformation. International Conference on Learning Representations, 2024. Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schrödinger bridge. International Conference on Machine Learning, 2021. Stefano Peluchetti. Bm2: Coupled schrödinger bridge matching. arXiv preprint arXiv:2409.09376, 2024. 12 Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion schrödinger bridges. Uncertainty in Artificial Intelligence, 2023. Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, and Alexander Korotin. Light and optimal schrödinger bridge matching. In Forty-first International Conference on Machine Learning, 2024. Michele Pavon, Giulio Trigila, and Esteban Tabak. The data-driven schrödinger bridge. Communications on Pure and Applied Mathematics, 74(7):15451573, 2021. Jhanvi Garg, Xianyang Zhang, and Quan Zhou. Soft-constrained schrödinger bridge: stochastic control approach. In International Conference on Artificial Intelligence and Statistics, pages 44294437. PMLR, 2024. Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, and Arnaud Doucet. Schrodinger bridge flow for unpaired data translation. Advances in Neural Information Processing Systems, 37: 103384103441, 2024. Yunyi Shen, Renato Berlinghieri, and Tamara Broderick. Multi-marginal schr\" odinger bridges with iterative reference refinement. arXiv preprint arXiv:2408.06277, 2024. Tianrong Chen, Guan-Horng Liu, Molei Tao, and Evangelos Theodorou. Deep momentum multi-marginal schr\" odinger bridge. arXiv preprint arXiv:2303.01751, 2023. Maxence Noble, Valentin De Bortoli, Arnaud Doucet, and Alain Durmus. Tree-based diffusion schrödinger bridge with applications to wasserstein barycenters. Advances in Neural Information Processing Systems, 36:5519355236, 2023. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transportation distances. Advances in Neural Information Processing Systems, 2013. Christian Léonard. survey of the schrödinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems-A, 2014. Michele Pavon, Esteban Tabak, and Giulio Trigila. The data-driven schroedinger bridge. arXiv, 2018. Yongxin Chen, Tryphon Georgiou, and Michele Pavon. On the relation between optimal transport and schrödinger bridges: stochastic control viewpoint. Journal of Optimization Theory and Applications, 2016. Tianrong Chen, Guan-Horng Liu, and Evangelos A. Theodorou. Likelihood training of schrödinger bridge using forward-backward sdes theory. International Conference on Learning Representations, 2021b. Zhenyi Zhang, Tiejun Li, and Peijie Zhou. Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport. International Conference on Learning Representations, 2025b. Frederike Lübeck, Charlotte Bunne, Gabriele Gut, Jacobo Sarabia del Castillo, Lucas Pelkmans, and David Alvarez-Melis. Neural unbalanced optimal transport via cycle-consistent semi-couplings. arXiv preprint arXiv:2209.15621, 2022. Matteo Pariset, Ya-Ping Hsieh, Charlotte Bunne, Andreas Krause, and Valentin De Bortoli. Unbalanced diffusion schr\" odinger bridge. arXiv preprint arXiv:2306.09099, 2023. Peter Lippmann, Enrique Fita Sanmartín, and Fred Hamprecht. Theory and approximate solvers for branched optimal transport with multiple sources. Advances in Neural Information Processing Systems, 35:267279, 2022. Aymeric Baradat and Hugo Lavenant. Regularized unbalanced optimal transport as entropy minimization with respect to branching brownian motion. arXiv preprint arXiv:2111.01666, 2021. 13 Christian Kramme, Alexandru M. Plesa, Helen H. Wang, Bennett Wolf, Merrick Pierson Smela, Xiaoge Guo, Richie E. Kohman, Pranam Chatterjee, and George M. Church. An integrated pipeline for mammalian genetic screening. Cell Reports Methods, 1(6):100082, October 2021. Atray Dixit, Oren Parnas, Biyu Li, Jenny Chen, Charles Fulco, Livnat Jerby-Arnon, Nemanja Marjanovic, Danielle Dionne, Tyler Burks, Raktima Raychowdhury, et al. Perturb-seq: dissecting molecular circuits with scalable single-cell rna profiling of pooled genetic screens. cell, 167(7): 18531866, 2016. George Gavriilidis, Vasileios Vasileiou, Aspasia Orfanou, Naveed Ishaque, and Fotis Psomopoulos. mini-review on perturbation modelling across single-cell omic modalities. Computational and Structural Biotechnology Journal, 23:1886, 2024. Mutsumi Kobayashi, Misato Kobayashi, Junko Odajima, Keiko Shioda, Young Sun Hwang, Kotaro Sasaki, Pranam Chatterjee, Christian Kramme, Richie Kohman, George Church, et al. Expanding homogeneous culture of human primordial germ cell-like cells maintaining germline features without serum or feeder layers. Stem Cell Reports, 17(3):507521, 2022. Merrick Pierson Smela, Christian Kramme, Patrick RJ Fortuna, Jessica Adams, Rui Su, Edward Dong, Mutsumi Kobayashi, Garyk Brixi, Venkata Srikar Kavirayuni, Emma Tysinger, et al. Directed differentiation of human ipscs to functional ovarian granulosa-like cells via transcription factor overexpression. Elife, 12:e83291, 2023. Merrick Pierson Smela, Christian Kramme, Patrick RJ Fortuna, Bennett Wolf, Shrey Goel, Jessica Adams, Carl Ma, Sergiy Velychko, Ursula Widocki, Venkata Srikar Kavirayuni, et al. Rapid human oogonia-like cell specification via transcription factor-directed differentiation. EMBO Reports, pages 130, 2025. Grace Hui Ting Yeo, Sachit Saksena, and David Gifford. Generative modeling of singlecell time series with prescient enables prediction of cell trajectories with interventions. Nature communications, 12(1):3222, 2021. Yuhui Zhang, Yuchang Su, Chenyu Wang, Tianhong Li, Zoe Wefers, Jeffrey Nirschl, James Burgess, Daisy Ding, Alejandro Lozano, Emma Lundberg, et al. Cellflow: Simulating cellular morphology changes via flow matching. arXiv preprint arXiv:2502.09775, 2025c. Martin Rohbeck, Charlotte Bunne, Edward De Brouwer, Jan-Christian Huetter, Anne Biton, Kelvin Y. Chen, Aviv Regev, and Romain Lopez. Modeling complex system dynamics with flow matching across time and conditions. The Thirteenth International Conference on Learning Representations, 2025. Lazar Atanackovic, Xi Zhang, Brandon Amos, Mathieu Blanchette, Leo Lee, Yoshua Bengio, Alexander Tong, and Kirill Neklyudov. Meta flow matching: Integrating vector fields on the wasserstein manifold. arXiv preprint arXiv:2408.14608, 2024. Dongyi Wang, Yuanwei Jiang, Zhenyi Zhang, Xiang Gu, Peijie Zhou, and Jian Sun. Joint velocitygrowth flow matching for single-cell dynamics modeling. arXiv preprint arXiv:2505.13413, 2025. Sayali Anil Alatkar and Daifeng Wang. Artemis integrates autoencoders and schrodinger bridges to predict continuous dynamics of gene expression, cell population and perturbation from time-series single-cell data. bioRxiv, pages 202501, 2025. Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Trajectorynet: dynamic optimal transport network for modeling cellular dynamics. In International conference on machine learning, pages 95269536. PMLR, 2020. Charlotte Bunne, Stefan Stark, Gabriele Gut, Jacobo Sarabia Del Castillo, Mitch Levesque, Kjong-Van Lehmann, Lucas Pelkmans, Andreas Krause, and Gunnar Rätsch. Learning single-cell perturbation responses using neural optimal transport. Nature methods, 20(11):17591768, 2023. Alice Driessen, Benedek Harsanyi, Marianna Rapsomaniki, and Jannis Born. Towards generalizable single-cell perturbation modeling via the conditional monge gap. arXiv preprint arXiv:2504.08328, 2025. 14 Guillaume Huguet, Daniel Sumner Magruder, Alexander Tong, Oluwadamilola Fasina, Manik Kuchroo, Guy Wolf, and Smita Krishnaswamy. Manifold interpolating optimal-transport flows for trajectory inference. Advances in neural information processing systems, 35:2970529718, 2022. Dominik Klein, Théo Uscidda, Fabian Theis, and Marco Cuturi. Genot: Entropic (gromov) wasserstein flow matching with applications to single-cell genomics. Advances in Neural Information Processing Systems, 37:103897103944, 2024. Toshiaki Yachimura, Hanbo Wang, Yusuke Imoto, Momoko Yoshida, Sohei Tasaki, Yoji Kojima, Yukihiro Yabuta, Mitinori Saitou, and Yasuaki Hiraoka. scegot: single-cell trajectory inference framework based on entropic gaussian mixture optimal transport. BMC bioinformatics, 25(1):388, 2024. Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. Cell, 176(4):928943, 2019. Stephen Zhang, Anton Afanassiev, Laura Greenstreet, Tetsuya Matsumoto, and Geoffrey Schiebinger. Optimal transport analysis reveals trajectories in steady-state systems. PLoS computational biology, 17(12):e1009466, 2021. Charlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi. Proximal optimal transport modeling of population dynamics. In International Conference on Artificial Intelligence and Statistics, pages 65116528. PMLR, 2022b. Stathis Megas, Daniel Chen, Krzysztof Polanski, Moshe Eliasof, Carola-Bibiane Schönlieb, and Sarah Teichmann. Estimation of single-cell and tissue perturbation effect in spatial transcriptomics via spatial causal disentanglement. In The Thirteenth International Conference on Learning Representations, 2025. Yusuf Roohani, Kexin Huang, and Jure Leskovec. Predicting transcriptional outcomes of novel multigene perturbations with gears. Nature Biotechnology, 42(6):927935, 2023. Jayoung Ryu, Charlotte Bunne, Luca Pinello, Aviv Regev, and Romain Lopez. Cross-modality matching and prediction of perturbation responses with labeled gromov-wasserstein optimal transport. arXiv preprint arXiv:2405.00838, 2024. Barbora Benešová and Martin Kružík. Weak lower semicontinuity of integral functionals and applications. arXiv, 2016. Bernard Dacorogna. Direct Methods in the Calculus of Variations. Springer New York, 1989. ISBN 9780387357799. Ivar Ekeland and Roger Temam. Convex analysis and variational problems. SIAM, 1999. Bernard Beauzamy. Introduction to Banach spaces and their geometry, volume 68. Elsevier, 2011. Michael Steele. The Cauchy-Schwarz master class: an introduction to the art of mathematical inequalities. Cambridge University Press, 2004. Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2008. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. International Conference for Learning Representations, 2014. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference for Learning Representations, 2017."
        },
        {
            "title": "Outline of Appendix",
            "content": "In Appendix A, we provide an extended background on the relevant theory for learning optimal stochastic bridges (A.1) and simulating trajectories on the data manifold (A.2). In Appendix B, we discuss the relationship between our proposed formulation for BranchSBM and previous related works. Appendix provides the theoretical basis for Sections 3 and 4, including formal proofs for Proposition 1 (C.1), Proposition 2 (C.2), Proposition 3 (C.3), and Proposition 4 (C.4). In Appendix E, we describe further details on experiments and hyperparameters used including specific details for each experiment, including multi-path LiDAR navigation (E.3), modeling differentiating single-cells (E.4), and modeling cell-state perturbations (E.4). Finally, we provide the pseudo code for the multi-stage training algorithm in Appendix F. Notation We denote the state space as Rd and time interval as [0, 1]. The branches are indexed with {0, . . . , K}. We denote initial data distribution at time = 0 as π0 and the terminal data distributions at = 1 as {π1,k}K k=0. The joint data distribution is denoted π0,1,k and pair of samples is given by (x0, x1,k) π0,1,k. For simplicity, we denote d(x0, x1,k) = dπ0,1,k. Let ut,k(Xt) denote the marginal velocity field, gt,k(Xt) denote the growth rate, and pt,k(Xt) denote the marginal probability density, where we sometimes drop the input Xt for simplicity. In addition, we denote the conditional velocity field and probability density as ut0,1,k ut0,1,k(Xtx0, x1,k) and pt0,1,k pt0,1,k(Xtx0, x1,k) respectively. The optimal values for any quantity are superscripted with symbol. We denote the parameterized flow neural networks as uθ t,k with parameters θ and the growth neural networks with gϕ t,k with parameters ϕ. In the context of unbalanced endpoint distributions, we denote the true initial weight of sample as 0 and the final weight of sample from the kth target distribution as 1,k. The predicted weights generated from the growth dynamics are given by wt(Xt), and we seek to match the predicted weight at time = 1 given by w1,k(X1,k) to the true weight 1,k. L2 denotes the space of square integrable functions and L2 be the L2-norm in function space. denotes the space of essentially bounded functions such that < ."
        },
        {
            "title": "A Extended Theoretical Background",
            "content": "A.1 Learning Optimal Stochastic Bridges Pinned-Down Stochastic Bridges Let be Markovian reference path measure that evolves over [0, 1] according to drift field ft(Xt) : Rd Rd and stochastic d-dimensional Brownian motion Bt Rd, via the SDE dXt = ft(Xt)dt + σtdBt, X0 π0 (21) Given Q, consider stochastic process (Xt)t[0,1] over the time interval [0, 1] pinned-down at the initial point X0 = x0 and final point X1 = x1 denoted as Q0,1(x0, x1). Due to the endpoint conditions, Q0,1 is not necessarily Markov, and evolves via the SDE dXt = {ft(Xt) + σ2 log Q1t(x1Xt)}dt + σtdBt, X0 = x0 (22) where log Q1t(x1Xt) is non-Markovian score function that corrects the drift field ft(Xt) of the reference process such that it points toward the target endpoint x1. Since log Q1t(x1Xt) is the log-likelihood that the final state satisfies the condition X1 = x1, the gradient defines how the log-likelihood changes with respect to the changes in the state at time t. The drift moves in the direction of the largest increase in log-likelihood given by the score function, which ensures that the process satisfies X1 = x1 following the theory of Doobs h-transform [Rogers and Williams, 2000]. Now, we can define the conditional probability distribution pt as mixture of pinned-down stochastic bridges over pairs of endpoints in the data coupling π0,1 = π0 π1 given by pt(x) = π0,1Qt0,1(xx0, x1) = (cid:90) Qt0,1(xx0, x1)dπ0,1 (23) To simplify notation, we denote each conditional bridge as Qt0,1(Xtx0, x1) = pt0,1(Xtx0, x1) and the joint distribution π0,1 = p0,1(x0, x1). Now, we can rewrite the marginal pt as (cid:2)pt0,1(Xtx0, x1)(cid:3) pt0,1(Xtx0, x1)p0,1d(x0, x1) = Ep0, pt(Xt) = (24) (cid:90) 16 log Q1t(x1Xt)} as the Furthermore, we denote ut0,1 ut0,1(Xtx0, x1) = {ft(Xt) + σ2 conditional drift that generates pt0,1 pt0,1(Xtx0, x1), that satisfies the conditional FokkerPlanck equation pt0,1 = (ut0,1pt0,1) + 1 2 σ2pt0,1 (25) Definition 1 (Reciprocal Class). Given our definition of the conditional bridge Q0,1, we can define the reciprocal class, denoted R(Q), of the reference measure as the class of path measures that share the same bridge as Q, defined as R(Q) = {Π Π(Xtx0, x1) = Q(Xtx0, x1)}. Markovian Projections Given mixture of conditional stochastic bridges Π = Π0,1Q0,1 under the reference measure that require knowledge of the joint distribution p0,1, we aim to project Π to the space of Markovian measures M, where the drift dynamics ut(Xt) is only dependent on the current state Xt and require no knowledge on the endpoints. This allows us to parameterize Markovian drift uθ (Xt) that can transport samples from the initial distribution x0 π0 to samples from the target distribution x1 π1. To do this, we define the Markovian projection of Π [Shi et al., 2023, Liu et al., 2023b]. Definition 2 (Markovian Projection). Given conditional bridge Q0,1 that evolves via the SDE in (22), we define Markovian projection of the mixture of bridges Π = Π0,1Q0,1 as Markov process = projM(Π) with the same marginals as Π such that Xt Πt for all [0, 1], X1 π1 and evolves via the SDE dXt = {ft(Xt) + (Xt)}dt + σtdBt (Xt) = σ2EΠ1t (cid:2)xt log Q1t(X1Xt)Xt = xt (cid:3) (26) (27) where Π1t is the conditional distribution of X1 under the mixture of bridges Π and xt log Q1t(X1Xt) points in the direction of greatest increase in the log-likelihood of the target endpoint X1 π1 under the reference process Q. In addition, the Markov projection minimizes the KL-divergence with the mixture of bridges = arg minMM KL(ΠM) and can be obtained by parameterizing vθ (Xt) and minimizing the dynamic formulation given by KL(ΠM) = E(x0,x1)Π0,1 ExtΠt0,1 (cid:90) 0 (cid:20) 1 2σ2 (cid:13) (cid:13)σ2 xt log Q1t(x1xt) vθ 2 (cid:21) (xt)(cid:13) (cid:13) dt (28) In general, the Markovian projection of reference measure does not preserve the conditional bridge and is not in the reciprocal class R(Q). The unique path measure that is the Markovian projection of Q, is in the reciprocal class R(Q), and preserves the endpoint distributions is called the Schrödinger Bridge. Definition 3 (Schrödinger Bridge). Given reference measure Q, initial distribution π0, and final distribution π1. path measure is the unique Schrödinger bridge if it satisfies 1. is the Markovian projection of Q, such that = projM(Q). 2. is in the reciprocal class of Q, i.e. R(Q), such that it preserves the conditional bridge P(Xtx0, x1) = Q(Xtx0, x1). 3. preserves the endpoint distributions P0 = π0 and P1 = π1. The goal of Schrödinger Bridge Matching (SBM) is to estimate the Schrödinger Bridge that transports samples from an initial distribution π0 to final distribution π1 given the optimal reference dynamics. We further discuss previous approaches to solving the SB problem in Appendix B. A.2 Simulating Trajectories on the Data Manifold Riemannian Manifolds and Metrics Since the interpolant xt,η learned in Stage 1 is defined by minimizing non-linear state cost Vt(Xt), the resulting velocity field uθ (Xt) lies on the tangent bundle TxΩ of smooth d-dimensional manifold Ω Rd called Riemannian manifold. Intuitively, Riemannian manifold can be thought of as smooth surface where the local curvature around point Ω can be approximated by tangent space TxΩ that defines the set of directions in which 17 can move along the manifold. These directions are defined by set of tangent vectors TxΩ, which pushes the point along the manifold. In Stage 2, we seek to parameterize vector field uθ (Xt) that minimizes the angle from the tangent vector xt,η = txt,η at each point on the manifold optimized in Stage 1. To do this, we must define the concepts of length and angles on Riemannian manifolds. First, we define location-dependent inner product in Riemannian manifolds known as the Riemannian metric gx : TxΩ TxΩ that maps two vectors u, TxΩ to scalar that describes the relative direction and length of the two vectors. Formally, the Riemannian metric can be written as the billinear and positive definite function gx(u, v) = uG(x)v = u, Gv s.t. (cid:26)u = 0 gx(u, u) > 0 0 (29) which defines the norm of tangent vector as ugx = (cid:112)gx(u, u). Now, we can decompose the Riemannian norm of the tangent vector xt,ηgx to get the loss defined in (13) as follows Ltraj(η) = Et,(x0,x1)p0,1 (cid:2) xt,η2 gx (cid:3) = Et,(x0,x1)p0,1 xt,η, G(xt,η) xt,η = Et,(x0,x1)p0,1 = Et,(x0,x1)p0,1 where the state cost is defined as Vt(xt,η) = xt,η, (G(xt,η) I) xt,η. 2 + xt,η, (G(xt,η) I) xt,η(cid:3) 2 + Vt(xt,η)(cid:3) (cid:2) xt,η2 (cid:2) xt,η2 (30) (31) (32) (33) Data-Dependent State Cost Following Kapusniak et al. [2024], we define the metric matrix G(xt,η) described previously as the data-dependent LAND and RBF metrics of the form GLAND(x, D) = GRBF(x, D) = (diag(h(x)) εI)1 which assigns higher cost (i.e. G(x) is larger) when moves away from the support of the dataset D. Specifically, given dataset i=1, we define the elements hLAND(x) Rd that scales down each dimension = {xi}N {1, . . . , d} in the LAND metric as hLAND (x) = (cid:88) i=1 (xj xj)2 exp (cid:18) xi2 2σ (cid:19) (34) where the exp term is positive when there is high concentration of data points around the point (i.e. xi is small) and approaches 0 as the concentration of data around decreases (i.e. xi is large). Writing xt,η, G(xt,η) xt,η in terms of hj(x), we get Ltraj(η) = Et,(x0,x1)p0,1 xt,η, G(xt,η) xt,η = Et,(x0,x1)p0, (cid:88) j=1 ( xt,η)2 hj(xt,η) + ε (35) (36) When hj(x) is large, the loss is minimized, and when hj(x) is small, the loss is scaled up. While the LAND metric effectively defines the data manifold in low dimensions, in high-dimensional state spaces, setting suitable variance σ in hLAND (xt) to ensure that the path does not deviate far from the data manifold without overfitting is challenging. To overcome this limitation, the RBF metric clusters the dataset into Nc clusters with centroids denoted as ˆxn Rd and trains set of parameters {ωα,n}Nc n=1 to enforce hj(xi) 1 for all points in the dataset such that points within the data manifold are also assigned hj(x) 1. Specifically, hRBF (cid:18) is defined as (cid:19) hRBF ωn,j(x) exp ˆxn2 λn,j 2 (x) = λn = Nc(cid:88) n=1 (cid:32) 1 2 κ Cn (cid:33)2 (cid:88) ˆxn2 xCn where Cn denotes the nth cluster, κ is tunable hyperparameter, and λn,j is the bandwidth of cluster the jth dimension. To train the parameters, we minimize the following loss (37) (38) (cid:0)1 hRBF (xi)(cid:1) (39) LRBF({ωα,n}) = (cid:88) xiD 18 In our experiments, we use the LAND metric for the LiDAR and mouse hematopoiesis datasets with dimensions = 2 and = 3 respectively, and the RBF metric for the perturbation modeling experiment with gene expression data of dimensions {50, 100, 150}. Sampling on the Data Manifold In Riemannian geometry, each Euclidean step tuθ (x) following the tangent vector uθ (x) along the manifold requires mapping back to the manifold via an exponential map Xt+t = expx(t uθ (x)). In general, computing the exponential map expx under the Riemannian metric G(x) requires simulation of the geodesic flow (i.e., approximating the final state at = 1 under the initial conditions x0 = and x0 = uθ While the manifold induced by the data-dependent metric G(x, D) behaves as curved manifold that follows an optimal cost structure, its underlying space is still Euclidean. G(x, D) just assigns varying costs of moving in the Euclidean space Rd. Therefore, we can avoid computing the exponential map and generate trajectories with simple Euclidean Euler integration following (x)). xt+t = xt + uθ (xt) (40) where = 1/Nsteps is the discretized step size."
        },
        {
            "title": "B Comparison to Existing Works",
            "content": "In this section, we discuss the relationship between our proposed formulation for Branched Schrödinger Bridge Matching and related previous works. We establish reasons why BranchSBM is the theoretically optimal formulation to solve the problem of modeling stochastic dynamical systems with diverging trajectories over time by modeling branched Schrödinger bridges. We conclude by introducing an alternative perspective of the Branched GSB problem defined in Section 3.2 as the problem of modeling probabilistic trajectories of dynamic systems with nondeterministic states, which BranchSBM is uniquely positioned to solve. B.1 Modeling Branched Schrödinger Bridges Schrödinger Bridge Matching Computational methods for solving the Schrödinger Bridge (SB) problem for predicting trajectories between initial and target distributions have been extensively studied in existing literature [De Bortoli et al., 2021b, Chen et al., 2021a, Korotin et al., 2023, Bunne et al., 2022a, Chizat et al., 2018, Liu et al., 2023a, 2022, Shi et al., 2023, Kim et al., 2024, Wang et al., 2021, Tong et al., 2024a, Peluchetti, 2024, Bunne et al., 2022a, Somnath et al., 2023, Gushchin et al., 2024, Pavon et al., 2021, Garg et al., 2024, De Bortoli et al., 2024, Shen et al., 2024, Chen et al., 2023, Noble et al., 2023]. Previous work has framed the SB problem as an entropy-regularized Optimal Control (EOT) problem [Cuturi, 2013, Léonard, 2014, Pavon et al., 2018] or stochastic optimal control (SOC) problem [Chen et al., 2016, 2021b, Liu et al., 2023a], which we build on in this work. Conditional Stochastic Optimal Control Several works [Chen et al., 2016, 2021b, Liu et al., 2023a] have reframed the SB problem as Conditional Stochastic Optimal Control (CondSOC) problem, which takes the canonical form (cid:90) 1 min ut Ept (cid:20) 1 2 ut(Xt)2 + Vt(Xt) (cid:21) dt + Ep1 [ϕ(X1)] (41) s.t. dXt = ut(Xt)dt + dBt, X0 π0 (42) where ϕ(X1) : acts as reconstruction loss that enforces that the distribution of X1 p1 matches the true distribution π1. Due to the intractability of π0, π1, GSBM [Liu et al., 2023a] uses spline optimization to learn an optimal Gaussian probability path Id) using only samples x0 π0 and x1 π1 from the initial and terminal distributions. Although GSBM does not require knowledge of the densities, it follows an iterative optimization scheme that alternates between matching the drift ut given fixed marginal pt, and updating the marginal given the drift. This strategy can get stuck in suboptimal solutions and is sensitive to the initialization of uθ . GSBM is also limited to learning unimodal Gaussian paths between one source and one target distribution with balanced mass, making it unsuitable for modeling tasks with multimodal terminal distributions and splitting of mass over multiple distinct paths. t0,1 = (µt, γ2 19 Regularized Unbalanced Optimal Transport Several previous works have studied the unbalanced optimal transport problem [Zhang et al., 2025b, Chen et al., 2021a, Lübeck et al., 2022, Pariset et al., 2023]; however, these approaches address fundamentally different setting from the unbalanced Generalized Schrödinger Bridge (GSB) problem considered in this work. Specifically, DeepRUOT [Zhang et al., 2025b] solves the Regularized Unbalanced Optimal Transport (RUOT) problem by parameterizing the canonical stochastic bridge SDE dXt = ft(Xt)dt + σtdBt with probability flow ODE given by (cid:26) dXt = ft(Xt) + 1 2 (cid:124) (Xt) x log pθ σ2 (cid:123)(cid:122) (Xt) uθ dt (cid:27) (cid:125) (43) where uθ (Xt) is the drift of the probability flow ODE that is learned along with the probability density pθ (Xt) to derive the drift of the SDE ft(Xt). Unlike GSBM [Liu et al., 2023a], which enforces hard constraint on the endpoints, DeepRUOT models probability density flow by minimizing reconstruction loss that encourages alignment with both intermediate and terminal distributions. While effective when intermediate distributions are observed, the method fails to learn meaningful trajectories in settings where these intermediate snapshots are unavailable, limiting its applicability in many real-world scenarios. Learning Diverging Trajectories with Single Target SBM To model diverging trajectories with single-branch SBM where the target distribution is multi-modal, we can follow set of Nsamples samples from the initial distribution π0 to determine the distribution of samples that end at each of the modes of the target distribution π1. While this can estimate the splitting of mass across different trajectories, it does not explicitly learn the optimal distribution of mass in the latent space over time. Furthermore, if the path toward specific cluster in the target distribution has lower potential than that of the other clusters, mode collapse could occur, where all samples follow the same trajectory without reaching the other clusters. BranchSBM learns to generate the correct mass distribution over each of the target states by optimizing the growth term with respect to the matching loss Lmatch defined in Equation 16. With the standard SBM formulation, it is also challenging to determine the time at which branching occurs and the population diverges toward different targets, as all samples follow stochastic trajectories. With BranchSBM, we model population-wide branching dynamics with growth networks that are trained to predict the origin of branch from having zero mass (wt,k = 0) and the rate at which it grows/shrinks over time twt,k = gt,k(Xt,k). The mass of branch at any given time step can be simulated with wt,k(Xt,k) = w0,k + (cid:82) s,k(Xs,k)ds. 0 gϕ Branching Dynamics While branched dynamics have been explored in the context of optimal transport [Lippmann et al., 2022] and Brownian motion [Baradat and Lavenant, 2021], no previous work has explicitly formulated or solved the branched Schrödinger bridge problem that seeks to match an initial distribution to multiple terminal distributions via stochastic bridges. For instance, the concept of Branching Brownian Motion (BBM) introduced in Baradat and Lavenant [2021] models population of particles that each independently follow stochastic trajectories according to standard Brownian motion with positive diffusivity ν > 0. To model branching dynamics, each particle has an independent branching rate λ > 0 that determines the probability that the particle undergoes branching event, defined as the particle dying and generating new particles that then evolve independently. The number of generated particles is sampled from probability distribution over non-negative integers = (pk)kN, where pk is the probability of generating particles at the branching event and (cid:80) kN pk = 1. Given pk, qk = λpk defines the rate of branching events that generate new particles and define new probability measure = (qk)kN called the branching mechanism. While BBM defines branching as the generation of additional particles from single particle following an independent, temporal probability measure q, this model fails to model the division of mass across multiple trajectories, where total mass remains constant but the mass of each branch changes. In addition, BBM assumes that each branched particle undergoes independent Brownian motion, without explicitly defining terminal state or branch-specific drift. For these reasons, the BBM model is unsuitable for modeling branching in the majority of real-world contexts, such as cell state transitions, 20 where undifferentiated cells split probabilistically into distinct fates rather than proliferating in number. In such systems, branching reflects redistribution of probability mass over developmental trajectories, governed by underlying regulatory patterns rather than purely stochastic reproduction. Where existing frameworks fall short is in modeling meaningful energy-aware, conditional stochastic trajectories with unbalanced and multi-modal dynamics, which we address in this work. Specifically, we formulate the Unbalanced CondSOC problem followed by the Branched CondSOC problem that defines set of optimal drifts and growth fields that define set of branched trajectories following optimal energy-minimizing trajectories defined by the state cost Vt(Xt). Instead of spline optimization, we leverage parameterized network φ t,η(x0, x1) that learns to predict an optimal interpolating path given pair of endpoints. k=0 that evolve co-currently via learned growth rates {gϕ To model dynamic growth of mass along branched trajectories, we initialize normalized population weight of 1 at primary branch (k = 0), that can split across branches and generate weights {wt,k}K k=0. This approach relies on learning the growth rate over an entire population of samples across all branches rather than learning independent growth rates of individual samples, which enforces stronger constraints during training to ensure that the model captures true population growth dynamics. t,k}K B.2 Modeling Perturbation Responses In single-cell transcriptomics, perturbations such as gene knockouts, transcription factor induction, or drug treatments frequently induce cell state transitions that diverge into distinct fates, reflecting differentiation, resistance, and off-target effects [Shalem et al., 2014, Kramme et al., 2021, Dixit et al., 2016, Gavriilidis et al., 2024, Zhang et al., 2025a, Kobayashi et al., 2022, Smela et al., 2023, Pierson Smela et al., 2025, Yeo et al., 2021]. Trajectory inference methods for modeling cell-state dynamics with single-cell RNA sequencing data, including flow matching [Zhang et al., 2025c, Rohbeck et al., 2025, Atanackovic et al., 2024, Tong et al., 2024a, Wang et al., 2025], Schrödinger Bridge Matching [Alatkar and Wang, 2025, Tong et al., 2024a], and optimal transport [Zhang et al., 2025b, Tong et al., 2020, Bunne et al., 2023, Driessen et al., 2025, Huguet et al., 2022, Klein et al., 2024, Yachimura et al., 2024, Schiebinger et al., 2019, Zhang et al., 2021, Bunne et al., 2022b], have been widely explored. While many algorithms have been developed to predict perturbation responses [Bunne et al., 2023, Megas et al., 2025, Rohbeck et al., 2025, Roohani et al., 2023, Ryu et al., 2024], they either predict only the terminal perturbed state without modeling the intermediate cell-state transitions or model single unimodal perturbation trajectories. Schrödinger Bridge Matching frameworks like [SF]2M [Tong et al., 2024a] and DeepGSB [Liu et al., 2022] have been shown to effectively model stochastic transitions in biological systems, offering better scalability and sampling efficiency than classical OT. However, these models are still limited to modeling trajectories between single pair of boundary distributions, limiting their ability to represent divergent trajectories arising from the same perturbed initial state. This is key limitation when modeling processes like fate bifurcation post-perturbation, where cell population exposed to the same stimulus may split into multiple phenotypically distinct outcomes. BranchSBM extends the SBM framework to support multiple terminal marginals, enabling modeling of stochastic bifurcations in mathematically principled way. By learning mixture of conditional stochastic processes from common source to multiple target distributions, BranchSBM can capture the heterogeneity and uncertainty of cell fate decisions under perturbation. Moreover, it retains the empirical tractability of previous SB-based models, requiring only samples from distributions, and ensures that intermediate trajectories lie on the manifold of feasible cell states. B.3 Modeling Probabilistic Trajectories with BranchSBM We conclude the discussion with an alternative interpretation of the Branched Schrödinger Bridge problem that deviates from the branching population dynamics problem. We instead consider Branched SB as probabilistic trajectory matching problem, where each branch is one of multiple possible trajectories that sample Xt could follow. Since single-path SBM learns only single deterministic drift field ut(Xt) that determines the direction and flow of the SDE dXt = uθ (Xt)dt + σdBt, the probabilistic aspect of the trajectory remains restricted to Brownian motion via σdBt. This fails to capture probabilistic dynamics where probability densities begin concentrated at single state but evolve into multi-modal probability densities {pt,k(Xt)}K k=0 over multiple different states, each 21 of which evolve according to an SDE dXt = ut,k(Xt)dt + σkdBt with an independent drift term ut,k(Xt) and noise scaling σk. k=0, each with probability wt,k such that (cid:80)K Where other SBM frameworks fall short, BranchSBM is capable of modeling multiple probabilistic trajectories, where system begins at single deterministic state X0 = x0 with probability w1,0 = 1 and evolves via multiple probabilistic trajectories that diverge in the state space governed by the SDEs dXt = ut,k(Xt)dt+σkdBt. At time t, the system exists in non-deterministic superposition of states {Xt,k, wt,k}K t,k = 1. In addition, BranchSBM can model the evolution of the probability weights wϕ t,k by parameterizing the probabilistic growth rates {gϕ t,k = 1 at all times [0, 1] by minimizing the mass loss Lmass(ϕ) (17). This problem is prevalent in many biological and physical systems, where system does not exist in single deterministic state, but rather superposition over distribution of states. k=0 that preserve conservation of probability mass (cid:80)K k=0 wϕ k=0 wϕ t,k}K Table 5: BranchSBM enables robust modeling of branched Schroödinger bridges compared to existing frameworks. BranchSBM can model both branching and unbalanced trajectories, follows intermediate trajectories governed by task-specific state cost, requires only endpoint samples for training, and samples trajectories from only single sample from the starting distribution. Method Models Branching Models Unbalanced Intermediate Dynamics Requirements for Training Requirements for Sampling Generalized SBM Liu et al. [2023a] No No DeepRUOT [Zhang et al., 2025b] Requires simulating multiple samples Growth rate gϕ (Xt) Branched growth rates t,k(Xt)}K {gϕ k=0 BranchSBM (Ours) Simulates divergent trajectories and terminal states from single sample x"
        },
        {
            "title": "C Theoretical Proofs",
            "content": "C.1 Proof of Proposition 1 Entropic OT with learned drift uθ Regularized OT with learned drift uθ and density pθ Branched drifts {uθ k=0 that t,k}K minimize state-cost Vt Samples from π0, π1 Endpoints (x0, x1) pθ 0,1 Samples from π0, π1 Sample x0 π0 Samples from π0 and clusters {π1,k}K k=0 Sample x0 π Proposition 1 (Unbalanced Conditional Stochastic Optimal Control) Suppose the marginal density can be decomposed as pt(Xt) = (cid:82) pt(Xtx0, x1)p0,1(x0, x1)dπ0,1, where π0,1 is fixed joint coupling of the data. Then, we can identify the optimal drift that solves the Unbalanced GSB problem in (5) by minimizing the Unbalanced Conditional Stochastic Optimal Control objective given by and growth π0,1 (cid:20) 1 2 s.t. dXt = ut(Xtx0, x1)dt + σdBt, X0 = x0, X1 = x1 w0(X0) = ut(Xtx0, x1)2 + Vt(Xt) E(x0,x1)π0, Ept0,1 min ut,gt (cid:20)(cid:90) 1 (cid:21) 0 wt(Xt)dt (cid:21) 0 , w1(X1) = 1 (44) (45) where wt = w0 + (cid:82) the growth rate, and π0,1 is the weighted coupling of paired endpoints (x0, w0, x1, w1) π0,1. 0 gs(Xs)ds is the time-dependent weight initialized at 0, ut is the drift, gt is Proof. We define the Unbalanced Generalized Schrödinger Bridge problem as the solution (ut, pt, gt) to the energy minimization problem such that the unbalanced Fokker-Planck equation is satisfied. (cid:90) 1 0 (cid:40) min ut,gt s.t. Ept (cid:20) 1 2 ut(Xt)2 + Vt(Xt) w0 + (cid:21) (cid:18) (cid:19) gs(Xs)ds dt (cid:90) 0 pt(Xt) = (ut(Xt)pt(Xt)) + σ2 p0 = π0, p1 = π1 2 pt(Xt) + gt(Xt)pt(Xt) (46) (47) Under the assumption that the joint probability p0,1(x0, x1) is fixed over all times [0, 1] and the marginal probability can be factorized as pt(Xt) = Ep0,1 [pt(Xtx0, x1)], we can decompose the 22 minimization objective into (cid:90) 0 Ept (cid:20) 1 2 = ut(Xt)2 + Vt(Xt) (cid:21) (cid:18) w0 + (cid:90) 0 (cid:19) gs(Xs)ds dt (48) (cid:90) 1 0 Ep0,1 Ept0,1 = Ep0, (cid:90) 1 0 Ept0,1 (cid:20) 1 2 (cid:20) 1 2 ut(Xt)2 + Vt(Xt) (cid:21) (cid:18) (cid:90) w0 + (cid:19) gs(Xs)ds dt (law of total expectation) ut(Xt)2 + Vt(Xt) (cid:21) (cid:18) 0 (cid:90) 0 w0 + (cid:19) gs(Xs)ds dt (Fubinis theorem) which can be solved by identifying the conditional drift ut that minimizes the expected objective value over all endpoint samples (x0, x1) p0,1. Under similar assumptions, we can decompose all terms in the Fokker-Planck equation. For the left-hand side, we have: pt(Xt) = (cid:90) π0,1 pt(Xtx0, x1)p0,1(x0, x1)dπ0,1 = E(x0,x1)π0,1 (cid:21) pt(Xtx0, x1) (49) (cid:20) For the divergence term, we have: (ut(Xt)pt(Xt)) = ut(Xt) (cid:32) pt(Xtx0, x1)p0,1(x0, x1)dπ0, (cid:33) (cid:90) π0,1 (cid:33) ut(Xt)pt(Xtx0, x1)p0,1(x0, x1)dπ0,1 (50) (cid:32)(cid:90) = π0,1 By Fubinis Theorem and the linearity of the divergence operator, we can switch the order of integration to get (ut(Xt)pt(Xt)) = (cid:18) (cid:19) ut(Xt)pt(Xtx0, x1) p0,1(x0, x1)dπ0,1 (cid:90) π0, = E(x0,x1)π0,1 (cid:20) (cid:0)ut(Xt)pt(Xtx0, x1)(cid:1) (cid:21) (51) For the Laplacian term, we have: σ2 pt(Xt) = = = = σ2 2 σ2 2 σ2 2 σ2 (pt(Xt)) (cid:32) (cid:90) π0, pt(Xtx0, x1)p0,1(x0, x1)dπ0,1 (cid:33) (cid:90) (cid:18) π0,1 pt(Xtx0, x1) p0,1(x0, x1)dπ0,1 (cid:19) E(x0,x1)π0,1 (cid:2)pt0,1(Xtx0, x1)(cid:3) For the growth term, we have: gt(Xt)pt(Xt) = gt(Xt) (cid:90) π0,1 pt(Xtx0, x1)p0,1(x0, x1)dπ0,1 (cid:90) = π0, gt(Xt)pt(Xtx0, x1)p0,1(x0, x1)dπ0,1 = E(x0,x1)π0,1 (cid:20) gt(Xt)pt(Xtx0, x1) (cid:21) (52) (53) Combining all the terms of the Fokker-Planck equation, we have shown that (47) can be rewritten as pt(Xtx0, x1) = (cid:0)utpt(Xtx0, x1)(cid:1) + pt0,1(Xtx0, x1) + gtpt(Xtx0, x1) (54) Therefore, the Unbalanced GSB problem is equivalent to solving the Unbalanced CondSOC problem, and we conclude our proof of Proposition 1. C.2 Proof of Proposition 2 Proposition 2 (Branched Conditional Stochastic Optimal Control) For each branch, let pt,k(Xt,k) = Ep0,1k [pt,k(Xt,kx0, x1,k)], where π0,1,k is the joint coupling distribution of samples x0 π0 from the initial distribution and x1,k π1,k from the kth target distribution. Then, we can identify the set of optimal drift and growth functions {u k=0 that solve the Branched GSB problem in (3.2) by minimizing sum of Unbalanced CondSOC objectives given by t,k, t,k}K min {ut,k,gt,k}K k= E(x0,x1,0)π0,1,0 (cid:90) 1 0 (cid:26) Ept0,1,0 (cid:20) 1 ut,0(Xt,0)2 + Vt(Xt,0) (cid:21) wt,0 + (cid:88) k= E(x0,x1,k)π0,1,k (cid:90) 1 0 Ept0,1,k (cid:20) 1 2 ut,k(Xt,k)2 + Vt(Xt,k) (cid:21) (cid:27) wt,k dt (55) s.t. dXt,k = ut,k(Xt,k)dt + σdBt, X0 = x0, X1,k = x1,k, w0,k = δk=0, w1,k = w1,k (56) 0 gs,1(xs,1)ds is the weight of the primary paths initialized at 1 and wt,k = where wt,0 = 1 + (cid:82) (cid:82) 0 gs,k(xs,k)ds are the weights of the secondary branches initialized at 0. Proof. We extend the proof of Proposition 1 to the branching case by defining each branch as an independent Unbalanced Generalized Schrödinger Bridge problem in (5) given by min ut,k,gt,k (cid:40) (cid:90) 1 Ept,k (cid:20) 1 2 ut,k(Xt,k)2 + Vt(Xt,k) (cid:21) (cid:18) w0,k + (cid:90) 0 (cid:19) gs,k(Xs,k)ds dt s.t. pt,k(Xt,k) = (ut,k(Xt)pt,k(Xt,k)) + σ2 p0 = π0, pt,k = πt,k 2 pt,k(Xt,k) + gt,k(Xt,k)pt,k(Xt,k) (57) (58) such that each branch independently solves the Fokker-Planck equation defined as (ut,kpt,k) + σ2 the global Fokker-Planck equation pt,k = 2 pt,k. Now, we show that the sum of unbalanced CondSOC problems still satisfies where we define pt as the weighted sum of the branched distributions given by pt = (utpt) + σ2 2 pt pt(Xt) = (cid:88) k=0 wt,kpt,k(Xt) (59) (60) To obtain an expression for the global Fokker-Planck equation, we differentiate both sides and substitute the branched FPE as follows (cid:34) (cid:88) (cid:35) wt,kpt,k t t pt = pt = pt = pt = pt = (cid:88) k=0 (cid:88) k=0 (cid:88) k=0 (cid:88) k=0 k=0 (cid:20) wt,k (cid:18) (cid:19) pt,k + (cid:18) (cid:19) (cid:21) wt,k pt,k (cid:20) (cid:18) wt,k (ut,kpt,k) + (cid:19) (cid:21) pt,k + gt,kpt,k σ2 2 (substitute branched FPE and twt,k = gt,k) (cid:20) wt,k (ut,kpt,k) + wt,k pt,k + gt,kpt,k (cid:21) σ2 2 (wt,k (ut,kpt,k)) + (cid:88) k=0 wt,k σ2 2 pt,k + (cid:88) k=0 gt,kpt,k (61) By linearity of the Laplacian, the diffusion term can be rewritten as (cid:88) k= wt,k σ2 2 pt,k = σ2 2 (cid:32) (cid:88) k=0 (cid:124) (cid:33) wt,kpt,k = σ2 pt (cid:123)(cid:122) pt (cid:125) (62) 24 Since the global growth term (cid:80)K k=0 gt,kpt,k is the sum of the growth dynamics across all branches and is separate from the drift and diffusion dynamics, it doesnt alter the direction or motion of particles along the branched fields. Thus, both the diffusion and growth terms satisfy the global FPE. Now, we set the divergence term in (61) equal to the global divergence (utpt) and derive the expression for the total drift field ut(Xt) that satisfies the global FPE. By linearity of the divergence operator, we get (utpt) = (cid:88) k=0 (wt,k (ut,kpt,k)) (utpt) = (utpt) = (cid:33) wt,kut,kpt,k (cid:32) (cid:88) k=0 1 pt (cid:88) k=0 (cid:32) (cid:124) (cid:33) wt,kut,kpt,k pt (cid:123)(cid:122) ut (cid:125) (63) Under the global FPE constraint, the drift ut is defined as the mass-weighted average of the drift fields for each branch, given by ut(Xt) = 1 k=0 wt,k(Xt)ut,k(Xt)pt,k(Xt). Intuitively, this means that in the global context, for any Xt = xt, the drift of state Xt along the dynamics of branch is scaled by the weight of xt at time along branch and the ratio of probability density of xt under branch over the total probability density of xt across all branches. Therefore, our definition of the weighted drift decoupled over individual branches satisfies the global FPE equation in (59), and this concludes the proof of Proposition 2. pt(Xt) (cid:80)K Remark 1 (Reduction to Single Path GSBM) When gt,0(Xt,0) = 0 and gt,k(Xt,k) = 0 for all [0, 1] and {1, . . . , K}, then the Branched CondSOC problem is the solution to the single path GSB problem given by (cid:90) min ut Ept0,1 (cid:20) 1 2 (cid:21) ut(Xt)2 + Vt(Xt) dt s.t. (cid:26)dXt = ut(Xt)dt + σdBt X0 π0, X1 π1 (64) (65) where the probability density pt0,1(Xtx0, x1) is conditioned explicitly on pair of endpoints (x0, x1) π0,1 drawn from the joint distribution. C.3 Proof of Proposition Proposition 3 (Solving the GSB Problem with Stage 1 and 2 Training) Stage 1 and Stage 2 training yield the optimal drift (Xt) that generates the optimal marginal probability distribution (Xt) that solves the GSB problem in (4). Proof. Let the marginal probability distribution optimal solution to the GSB problem. It suffices to show that (Xt) and corresponding drift (Xt) define the 1. Given (x0, x1) π0,1, Stage 1 training with the trajectory loss Ltraj(η) (13) yields the interpolant 2. Given t,η and time-derivative (Xt). (Xt), Stage 2 training with the explicit flow matching loss Lflow(θ) (14) yields the t,η that define optimal drift (Xt). To prove Part 1, we establish the following Lemma. Lemma 1. Given the Markovian reference process with drift unconstrained GSB problem in (4), Stage 1 training returns the velocity field reciprocal projection Π = projR(Q)(P) of the path measure = p0,1. (Xt) that is the minimizer of the t,η that generates Proof of Lemma. It suffices to prove that endpoint constraints X0 = x0 and X1 = x1. t,η minimizes the KL-divergence with (Xt) under 25 Consider the unconstrained Markovian drift v = arg min vt (cid:90) 1 0 Ept that is the minimizer of the energy function given by (cid:20) 1 (cid:21) 2 vt(Xt)2 + Vt(Xt) (66) dt The class of interpolants is given by parameters η is defined as xt,η = (1 t)x0 + tx1 + t(1 t)φt,η(x0, x1) xt,η = x1 x0 + t(1 t) φt,η(x0, x1) + (1 2t)φt,η(x0, x1) (67) (68) Stage 1 training yields the optimal interpolant points [0, 1], defined as t,η that minimizes the energy function across all time t,η = arg min t,η (cid:90) 1 Ept (cid:20) 1 2 t,η(x0, x1)2 + Vt(x (cid:21) t,η) dt (69) t,η is the velocity field corresponding to Π = p0,1Q0,1, which is Therefore, we aim to prove that the reciprocal projection of p0,1 onto the class of path measures that share the same bridge marginals conditioned on pair of endpoints, called the reciprocal class R(Q) [Léonard, 2014]. By definition, the reciprocal projection Π is defined as the minimizer of the KL-divergence with = p0,1 that lies within the reciprocal class R(Q) of measures that match the bridge conditions of Q. Π = arg minΠR(Q)KL(PΠ) The reciprocal projection matches the endpoint constraints of while following the bridge conditionals Q0,1 = Q(X0 = x0, X1 = x1) [Léonard, 2014]. Therefore, we can write the generating velocity field (70) t,η as t,η(x0, x1) = ExtQ [ xtX0 = x0, X1 = x1] (71) generating and Since both to determining reference process and all path measures = p0,1 that preserve endpoint constraints given by t,η minimize the same energy function, the objective in (70) reduces t,η that is the minimizer of the dynamic formulation of KL divergence between the t,η = arg min xt,η KL(PQ) = arg min xt,η E(x0,x1)p0,1 (cid:20)(cid:90) 1 0 1 (cid:21) (72) (73) t,η (xt,η)2dt Therefore, we conclude that By Lemma 1, we know that endpoint constraints and preserving the coupling p0,1. Therefore, we can define conditional drift that generates the conditional probability distribution the Fokker-Planck equation t,η generates the reciprocal projection Π = p0,1Q0,1. is the optimal drift energy function in the GSB problem without while t,η(x0, x1) as the t0,1(Xtx0, x1) that satisfies t,η is the reciprocal projection that follows the dynamics of t0,1(Xtx0, x1) t pt0,1 = (ut0,1pt0,1) + 1 2 σ2pt0,1 (74) t0,1(Xtx0, x1) is the optimal bridge that solves the GSB problem for pair of endpoints Given that (x0, x1) p0,1, we can define the marginal probability distribution as the mixture of bridges (cid:105) = E(x0,x1)π0,1 (cid:104) t0,1(Xtx0, x1) (75) which concludes the proof of Part 1. For Part 2 of the proof, we aim to show that Stage 2 training yields the optimal Markovian drift (Xt) that generates (Xt) in terms of the conditional bridge pt0,1 and drift field ut0,1 to extract an expression for (Xt) that satisfies (Xt). To do this, we write the Fokker-Planck equation for it. Starting from the definition of , we have (cid:90) = t0,1p0,1dx0,1 p = (cid:90) (cid:18) (cid:19) t0,1 p0,1dx0,1 (cid:90) (cid:18) (ut0,1p t0,1) + (cid:19) 1 2 σ2p t0, p0,1dx0,1 = = (cid:90) (cid:16) (cid:17) t0,1) p0,1dx0,1 + (cid:90) (cid:18) 1 2 (cid:19) σ2p t0,1 p0,1dx0,1 (cid:90) σ2 t0,1p0,1dx0,1 (ut0,1p (cid:90) = (ut0,1p t0,1)p0,1dx0,1 + = (cid:90) (cid:124) (ut0,1p t0,1)p0,1dx0,1 (cid:123)(cid:122) (cid:125) (u ) + 1 2 1 2 σ2 (cid:90) (cid:124) t0,1p0,1dx0,1 (cid:125) (cid:123)(cid:122) t (76) For (76) to satisfy the Fokker-Planck equation, we set the first term equal to (u to get ) and solve for (cid:90) u = (u t0,1p (cid:104) t0,1)p0,1dπ0,1 (cid:105) Eπ0,1 = t0,1 t0,1p p (77) (78) Therefore, the optimal Markovian drift (or Markovian projection) is the average of the conditional drifts defined in part 1 as t,η over the joint distribution p0,1. This means that the minimizer of the conditional flow matching loss Lflow(θ) in (14) defined as the expected mean-squared error between Markovian drift field ut(Xt) and t,η over pairs (x0, x1) p0,1 in the dataset is the optimal drift (Xt) that solves the GSB problem. t0,1 u (Xt) = arg min θ Lflow(θ) = arg min uθ (cid:90) 1 E(x0,x1)p0,1 t,η uθ (Xt)2dt (79) which concludes the proof of Proposition 3. Since the drift for each branch uθ t,k(Xt) are trained independently in Stage 2, we can extend this result across all + 1 branches and conclude that the sequential Stage 1 and Stage 2 training procedures yields the optimal set of drifts {u k=0 that t,k}K generate the optimal probability paths {p k=0 that solves the GSB problem for each branch. t,k}K C.4 Proof of Proposition 4 t,k : Rd Lemma 2. Suppose the optimal drift field that minimizes the GSB problem in (4) is well-defined over the state space Rd for each branch. Then, the optimal weight t,k at any of the secondary branches {1, . . . , K} is non-decreasing over the interval [0, 1]. Equivalently, the optimal growth rates t,k : Rd Rd and probability density t,k(Xt) 0 for all [0, 1]. Proof. We will prove this lemma by contradiction. Suppose there exists branch that decreases in weight over the time interval [t1, t2] for 0 t1 < t2 1, such that wt1,k > wt2,k. We know that the target weight at time = 1 is non-negative w1,k 0 and the total weight across all branches is conserved (i.e. (cid:80)K > 0 for all [0, 1]). In this proof, we let Ek(t1, t2) = (cid:82) t2 denote the energy of following the t1 dynamics of the kth branch over the time interval [t1, t2]. ) and non-decreasing (i.e. twtotal t,k(Xt)2 k=0 wt,k(Xt) = wtotal (cid:104) 1 2 2 + Vt(Xt) (cid:105) Then, there can only be two possible reasons for the loss of mass along branch k: (1) the mass is destroyed, or (2) the mass is transferred to different branch = k. Case 1. The destruction of mass directly violates the assumption that the total mass across all branches is conserved and non-decreasing. So, we only need to consider the possibility of Case 2. Case 2. Suppose the mass is transferred to different branch = over the interval [t1, t2]. By Proposition 3, Stage 1 and 2 training yields the optimal velocity fields {u t,k(Xt)}K k=0 that generate the optimal interpolating probability density {p k=0 that independently minimize the GSB problem in (4). t,k(Xt)}K If mass is transferred from branch to branch over the interval [t1, t2], it must be compensated for over time [t2, 1] to reach the target weight w1,k > 0. Then, without loss of generality, we can consider two sub-cases: (1.1) the mass is compensated from the primary branch, and (1.2) the mass that diverges to the jth branch returns to the kth branch following continuous trajectory. Case 1.1. Since all mass along the secondary branches originates from the primary branch, this implies that there exists positive weight > 0 that first follows the dynamics of branch and is transferred to branch j, contributing to the final weight of the jth endpoint, and the total weight supplied from the primary branch to branch is w1,k + w. Given that each branch has no capacity constraints, it follows that the dynamics along branch over [0, t1] and branch over [t1, 1] is optimal for all mass reaching endpoint j. Formally, we express this in terms of energy as Ek(0, t1) + Ej(t1, 1) < Ej(0, 1) (80) which contradicts the assumption that the dynamics of branch given by (u the state space . t,j, t,j) are optimal over Case 1.2. In this case, there exists positive mass that follows the dynamics of branch over the interval [0, t1], the dynamics of branch over [t1, t2], and back to branch over [t2, 1]. Similarly to Case 1.1, given that each branch has no capacity constraints, this implies that this concatenation of dynamics is optimal for all mass reaching endpoint k. Expressing in terms of energy, we have Ek(0, t1) + Ej(t1, t2) + Ek(t2, 1) < Ek(0, 1) (81) which contradicts the assumption that the dynamics of branch given by (u the state space . t,k, t,k) are optimal over Given that both sub-cases lead to contradiction of the optimality assumption, we conclude that mass along each branch cannot be transferred to another branch and is non-decreasing over [0, 1]. Note that we do not need to consider the case where the mass is compensated from another secondary branch ℓ = j, as this would imply that mass is transferred from branch ℓ to branch k, which is not possible under the same argument. Proposition 4 (Existence of Optimal Growth Functions) Assume the state space Rd is bounded domain within Rd. Let the optimal probability density of branch be known non-negative t,k : [0, 1] [0, 1] L(X [0, 1]). By Lemma 2, we function bounded in [0, 1], denoted as can define the set of feasible growth functions in the set of square-integrable functions L2 as := {g = (gt,0, . . . , gt,K) L2(X [0, 1]) gt,k(x) : [0, 1] , gt,k(x) 0} (82) Let the growth loss be the functional L(g) : L2(X [0, 1]) R. Then, there exists an optimal function = (g t,k such that L(g) = inf gG L(g) which can be obtained by minimizing L(g) over G. t,K) L2 where t,0, . . . , Proof. This proof draws on several concepts from functional and convex analysis. For comprehensive background on these concepts, see Benešová and Kružík [2016]. We follow the direct method in the calculus of variations [Dacorogna, 1989] for proving there exists minimizer for the functional Lgrowth(g) in (18) with the following steps: 1. Show that the set of feasible growth functions is convex and closed under the weak topology of the set of square integrable functions L2 (Lemma 3). 2. Show that the minimizing sequence {g(n)} has weakly convergent subsequence (Lemma 4). 3. Show that the functional Lgrowth(g) is weakly lower semi-continuous (Lemmas 5, 6, 7, 8). We prove each with sequence of Lemmas. 28 Lemma 3. The set of feasible growth functions := {gt,k(x) : [0, 1] L2(X [0, 1]) gt,k(x) 0} is convex and closed over the weak topology of L2. Proof. We first prove convexity and then closure under the weak topology of L2. Proof of Convexity. To prove that the set of functions is convex, we first define what it means for set of functions to be convex. Definition 4 (Convex Set). set of functions is said to be convex if any convex combination of two functions in the set g1, g2 is also in the set. Formally, if g, G, λ [0, 1], gλ = λg + (1 λ)g Recall our definition of as the set of functions that return strictly non-negative values: := {gt,k(x) : [0, 1] L2(X [0, 1]) gt,k(x) 0} Given any λ [0, 1], we have t,k(x) = λgt,k(x) + (1 λ)g gλ gλ t,k(x) λ 0 + (1 λ) 0 gλ t,k(x) 0 t,k(x) (83) (84) (85) which means gλ t,k and is convex. Proof of Closure. First, we define what it means to be closed under the weak topology of L2. Definition 5 (Closure Under Weak Topology of L2). set of functions is said to be closed in the weak topology of L2 if the statement is true: if sequence of functions {g(n) : g(n) G} indexed by n, converges to some function g() L2 as , then g() G. To show that is closed under weak topology of L2, we need to show that all sequences {g(n) t,k : g(n) t,k G} converge to as , such that 0. The proof follows directly from Fatous Lemma [Ekeland and Temam, 1999] which states that given sequence of non-negative, measurable functions {g(n) t,k G}, the following is true lim inf g(n) t,k (x)dx lim inf (cid:90) g(n) t,k (x)dx (Fatous Lemma) t,k : g(n) (cid:90) 0 meaning that the limit of converging sequence of non-negative functions g() over the state space , and thus is in the set of feasible growth functions g() proof. Lemma 4. Given minimizing sequence {g(n) G} under the functional L(g) : L2 such that L(g(n)) inf gG L(g), there exists subsequence {g(ni) G} that converges weakly in L2 to some limit G. t,k is also non-negative t,k G, concluding our Proof. It suffices to show the following: 1. The functional {g(n)} is bounded in L2 such that there exists positive value where g(n)L2 for all n. 2. The space of feasible functions is reflexive, such that all bounded sequences have weakly convergent subsequence in L2. The proof of Part 1 follows from the growth penalty term in L(g) defined as the squared-norm g2 L2 of the growth functional. This term ensures that L(g) is coercive, such that g(n)L2 = L(g(n)) which ensures that the sequence does not diverge to infinity in norm without incurring penalty from the loss functional. Given that L(g(n)) inf gG L(g), where inf gG L(g) < , it follows from coercivity that g(n)L2 does not diverge to infinity and {g(n)} is bounded in L2. (86) 29 The proof of Part 2 follows from the well-established result that for 1 < < , the space Lp is reflexive [Beauzamy, 2011]. Therefore, by reflexivity of L2, we have that every minimizing sequence {g(n)} has weakly convergent subsequence {g(ni)}, such that g(ni) in L2, concluding our proof. Before proving that each component of the loss functional is weakly lower semi-continuous, we establish the definitions for weakly continuous and weakly lower semi-continuous functionals. Definition 6 (Weakly Continuous Functionals). functional : L2 is said to be weakly continuous if it satisfies = g(n) = L(g(n)) L(g) (87) such that if sequence {g(n) : g(n) G} converges g(n) as , then the functional also converges L(g(n)) L(g). Definition 7 (Weakly Lower Semi-Continuous Functionals). functional : L2 is said to be weakly lower semi-continuous (w.l.s.c.) if it satisfies = g(n) = lim inf L(g(n)) L(g) (88) such that if sequence {g(n) : g(n) G} converges g(n) as , then the functional is lower bounded by L(g). By definition, weak continuity implies w.l.s.c. Lemma 5. The functional Lmatch(g) : L2 defined as Lmatch(g) = = = (cid:88) k=0 (cid:88) k=0 (cid:88) k=0 (cid:18) Ep t,k w0,k + (cid:90) 0 (cid:19)2 gt,kdt 1,k (cid:18) w0,k + (cid:90) 1 0 Ep t,k [gt,k]dt 1,k (cid:19)2 (cid:18)(cid:90) 1 (cid:90) 0 t,kgt,kdxdt + (cid:19)2 (linearity of expectation) (89) where = w0,k 1,k is constant, is weakly lower semi-continuous in L2 (w.l.s.c.). Proof of Lemma. First, we show that the map ϕ(gt,k) = (cid:82) t,kgt,kdx is bounded linear functional in L2. By linearity of integration, we have that for two functions g, G, ϕ(cg + cg) = cϕ(g) + cϕ(g), so ϕ() is linear map. To establish that ϕ() is bounded in L2, we must show that ϕ(g) CgL2. By the Cauchy-Schwarz Inequality [Steele, 2004], we have p ϕ(gt,k) = t,k, gt,k = (cid:90) (cid:12) (cid:12) (cid:12) (cid:12) p t,kgt,kdx (cid:12) (cid:12) (cid:12) (cid:12) t,kL2 gt,kL2 (90) t,k, gt,k L2. Since which is valid as t,kL2 is fixed constant with respect to gt,k, we have shown that ϕ(gt,k) Cgt,kL2 and gt,k is bounded. By the definition of weak topology on L2, all bounded linear functionals are weakly continuous, such that limn ϕ(g(n)) ϕ(g). Given that ϕ(g) is weakly continuous, as g(n) g, we have ϕ(g(n)) ϕ(g). Since the square function ψ() = ()2 is convex and continuous and bounded below by ψ 0, the function does not contain discontinuities and limn(ϕ(g(n)) c)2 (ϕ(g) c)2, which is the definition of w.l.s.c. functional. Since the sum of w.l.s.c. functionals is also w.l.s.c., we conclude our proof. Lemma 6. The functional Lenergy(g) : L2 defined as Lenergy(g) = = (cid:90) 1 0 (cid:90) 1 Ep t,k (cid:20) 1 2 (cid:18) α(t) w0,k + ut,k(Xt)2 + Vt(Xt) (cid:21) (cid:18) w0,k + (cid:90) 0 (cid:19) gs,k(Xt)ds dt (cid:90) (cid:90) 0 (cid:19) s,kgs,kdxds dt (linearity of expectation) where α(t) is constant not dependent on gt,k, is weakly lower semi-continuous in L2 (w.l.s.c.). Proof of Lemma. Following similar proof as Lemma 5, we have that the map ϕ(gs,k) = (cid:82) s,kgs,kdx is bounded linear functional, and thus is weakly continuous in L2. Given that 30 ϕ(g) is bounded, [0, 1], and linearity of integration holds, the integral of ϕ(g) over the interval [0, t] remains bounded linear functional in L2, and thus is weakly continuous in L2. Following similar logic, the outer integral (cid:82) 1 dt is also bounded linear functional given that both α(t) and w0,k are constants, and we have shown that Lenergy(g) is weakly continuous. Since weak continuity implies w.l.s.c. and the sum of w.l.s.c. functionals is also w.l.s.c., we conclude our proof. Lemma 7. The mass loss functional Lmass(g) : L2 defined as (cid:17) 0 ϕ(g)ds (cid:16) w0,k + (cid:82) 0 α(t) Lmass(g) = (cid:90) 1 0 {pt,k}K k=0 (cid:34) (cid:88) k=0 wϕ t,k(xt,k) wtotal (cid:35)2 dt (cid:35)2 (cid:90) 1 (cid:34) (cid:88) 0 k=0 (cid:18) w0,k + (cid:90) 0 (cid:19) Ept,k [gt,k] = = wtotal dt (linearity of expectation) (cid:34) (cid:90) 0 1 + (cid:88) (cid:90) k=0 (cid:35)2 Ept,k [gt,k] wtotal dt (91) where wtotal is constant not dependent on gt,k, is weakly lower semi-continuous in (L2)K+1 (w.l.s.c.). Note that we do not need to account for the negative penalty loss as we assume the growth function is strictly positive. Proof of Lemma. Following similar proof as Lemma 5, we have that the map ϕ(g) = (cid:82) s,kgs,kdx is bounded linear functional, and thus is weakly continuous in L2. Since the sum of bounded linear functionals across + 1 branches is also weakly continuous in L2, following similar logic to Lemma 5, the composition of weakly continuous functional with the convex and continuous square map is w.l.s.c.. Since the sum of w.l.s.c. functionals is also w.l.s.c., Lmass(g) is w.l.s.c., which concludes our proof. Lemma 8. The combined Stage 3 training loss Lgrowth(g) = λenergyLenergy(g) + λmatchLmatch(g) + λmassLmass(g) + λgrowthgL2 is weak lower semi-continuous in L2 (w.l.s.c.). Proof of Lemma. Given the weighted sum of w.l.s.c. functionals (cid:80)M limn Li(g(n)) Li(g) for constants {λi}M follows easily that i=1 λiLi(g) that each satisfy i=1 for converging sequence g(n) as , it lim (cid:88) i=1 Li(g(n)) (cid:88) i=1 Li(g) (92) By definition, the norm in L2 is lower semi-continuous in the strong topology and thus the weak topology. Since Lenergy, Lmatch, and Lmass are w.l.s.c. by Lemmas 5, 6, and 7, we conclude that Lgrowth(g) is w.l.s.c., which concludes the proof. In total, we have shown: 1. The set of feasible growth functions is convex and closed under the weak topology of the set of square integrable functions L2 (Lemma 3). 2. The minimizing sequence {g(n)} has weakly convergent subsequence (Lemma 4). 3. The functional L(g) is weakly lower semi-continuous (Lemmas 5, 6, 7, 8). Thus, by the direct method in the calculus of variations, we have shown = (g t,0, . . . , t,1) s.t. L(g) = inf gG L(g) (93) which concludes the proof of Proposition 4."
        },
        {
            "title": "D Additional Experiments and Discussions",
            "content": "D.1 Comparison to Single-Branch Schrödinger Bridge Matching Setup For each experiment, we compared the performance of BranchSBM against single-branch SBM. Instead of clustering the data at = 1 into distinct endpoint distributions, we left the unclustered 31 data as single target distribution p1 and let the model learn the optimal Schrödinger Bridge from the initial distribution π0. For the single-branch task, we assume mass conservation and set the weights for all samples from π0 and π1 to 1.0, while keeping the model architecture, state-cost Vt, and hyperparameters equivalent to BranchSBM. Since single-branch SBM does not require modeling the growth of separate branches, we train only Stages 1 and 2 to optimize the drift field uθ of the single branch. For evaluation, the trajectories of validation samples from the initial distribution x0 π0 were simulated over [0, 1] and compared with the ground truth distribution at = 1. For BranchSBM, we take the overall distribution generated across all branches and compare it to the overall ground truth distribution. Modeling Mouse Hematopoiesis Differentiation In Figure 7, we provide side-by-side comparison of the reconstructed distributions at time t1 and t2 using BranchSBM (top) with two branches and single-branch SBM (bottom), as well as the learned trajectories over the time interval [t1, t2]. While single-branch SBM produces samples that loosely capture the two target cell fates, the resulting distributions display high variance and fail to align with the true differentiation trajectories. In contrast, BranchSBM generates intermediate distributions that are sharply concentrated along the correct developmental paths, more faithfully reflecting the underlying branching structure of the data. Figure 7: Comparison of BranchSBM to Single-Branch SBM for Cell-Fate Differentiation. Mouse hematopoiesis scRNA-seq data is provided for three time points t0, t1, t2. (A, B) Distribution of simulated cell states at time (A) t1 and (B) t2 across both branches for BranchSBM (top) and single-branch SBM (bottom). (C) Learned trajectories of BranchSBM and single-branch SBM over the [t1, t2] on validation samples. Modeling Clonidine and Trametinib Perturbation For both Clonidine and Trametinib, we performed the single-branch experiment on the top 50 principal components identified by PCA. After training, we simulated all the validation samples from the initial distribution π0 by integrating the single velocity field uθ to = 1. We evaluated the performance of the single-branch parameterization by computing the RBF-MMD (99) of all PCs and W1 (97) and W2 (98) distances of the top-2 PCs of the simulated samples at time = 1 with the ground truth data points. In Table 3, we show that BranchSBM with two branches trained on gene expression vectors across all dimensions {50, 100, 150} outperforms single-branch SBM on dimension = 50 in reconstructing the distribution of cells perturbed with Clonidine. In Table 4, we further show improved performance of BranchSBM with three branches compared to single-branch SBM. In Figure 8A and 8B, we see that single-branch SBM only reconstructs cluster 0, while failing to generate samples from cluster 1 for Clonidine perturbation or clusters 1 and 2 for Trametinib perturbation. The endpoints for both perturbation experiments are clustered largely on variance in the first two or three principal components (PCs), where PC1 captures the divergence from the control state to the perturbed clusters and PC2 and higher captures the divergence between clusters in the 32 perturbed population, where cluster 0 is closest to the control state along PC2 and cluster 1 and 2 are farther from the control. From Figure 8A and B, we can conclude that single-branch SBM is not expressive enough to capture the complexities of higher-dimensional PCs and follows the most obvious trajectory from the control to cluster 0, resulting in an inaccurate representation of the perturbed cell population. In contrast, we demonstrate that BranchSBM is capable of stimulating trajectories to both clusters in the population perturbed with Clonidine (Figure 8B) and three clusters in the population perturbed with Trametinib (Figure 8D), generating branched distributions that accurately capture the location and spread of the perturbed cell distribution in the dataset. Figure 8: Comparison of BranchSBM to Single-Branch SBM for Perturbation Modelling. (A, B) Clonidine perturbation trajectories with two target clusters generated by (A) single-branch SBM and (B) BranchSBM from the validation data. (C, D) Trametinib perturbation trajectories with three target clusters generated by (C) single-branch SBM and (D) BranchSBM. In both experiments, single-branch SBM only generated states in cluster 0 and not cluster 1 or 2, whereas BranchSBM reconstructed all perturbed clusters via branched trajectories. D.2 Effect of Final Joint Training on Losses In Table 6, we show the final losses after convergence, summed across each branch and averaged over the batch size, for Stage 3 training of the only the growth networks and Stage 4 joint training of the flow and growth networks discussed in Section 4. All losses are calculated exactly as shown in Section 4. Crucially, we find that the final joint training stage refines the parameters of both the flow and growth networks simultaneously to minimize the energy loss Lenergy(θ, ϕ) (15) while ensuring the growth parameters maintain minimal losses across Lmatch(ϕ) (16) and Lmass(ϕ) (17) for all experiments. This indicates that jointly optimizing both the drift and growth dynamics leads to further refinement towards modeling the optimal branching trajectories in the data. 33 Table 6: Validation Losses for Stage 3 and 4 Training Across Experiments. Losses are summed across both branches and averaged over batch size. The final Stage 4 joint training stage that refines both the flow and growth networks simultaneously minimize the energy loss Lenergy(θ, ϕ) (15) from Stage 3 while ensuring the growth parameters maintain minimal losses across Lmatch(ϕ) (16) and Lmass(ϕ) (17) Experiment Lenergy(θ, ϕ) LiDAR Mouse Hematopoiesis Chlonidine Perturbation Trametinib Perturbation 1.276 2.209 36.469 35.834 Stage 3 Lmass(θ, ϕ) 3.0 105 1.2 104 0.030 0.023 Lmatch(θ, ϕ) Lenergy(θ, ϕ) 0.007 0.054 0.109 0.078 0.768 1.918 25.798 32.843 Stage 4 Lmass(θ, ϕ) 2.0 105 5.0 105 0.053 0.017 Lmatch(θ, ϕ) 0.102 0.076 0.153 0."
        },
        {
            "title": "E Experiment Details",
            "content": "E.1 Multi-Stage Training To ensure stable training while incorporating all loss functions, we introduce multi-stage training approach (Algorithm 1). Stage 1 First, we train neural interpolant φt,η(x0, xt,k) : Rd Rd [0, 1] Rd that takes the endpoints of branched coupling and defines the optimal interpolating state Xt at time by minimizing the energy function Ltraj(η) in (13). This is used to calculate the optimal conditional velocity xt,η,k that preserves the endpoints X0 = x0 and X1,k = x1,k for the flow matching objective in Stage 2. Stage 2 Next, we train set of flow networks {uθ trajectories for each branch with the conditional flow matching objective Lflow in (14). k=0 that generate the optimal interpolating t,k}K Stage 3 We freeze the parameters of the flow networks and only train the growth networks {gϕ by minimizing Lgrowth in (18). t,k}K k=0 Stage 4 Finally, we unfreeze the parameters of both the flow and growth networks and jointly train {uθ k=0 by minimizing the growth loss Lgrowth in (18) from Stage 3 in addition to the distribution reconstruction loss Lrecons in (20). t,k, gϕ t,k}K E.2 General Training Details Model Architecture We parameterized the branched trajectory φt,η(x0, x1) with 3-layer MLP with Scaled Exponential Linear Unit (SELU) activations. The endpoint pair (x0, x1) and the time step are concatenated into single (2d + 1)-dimensional vector and used as input to the model. Similarly, we parameterize each flow network uθ t,k(xt) with the same 3-layer MLP and SELU activations but takes the interpolating state xt and time concatenated into (d + 1)-dimensional vector. t,k(xt) and growth network gθ To ensure that the growth rates of all secondary branches are non-negative (i.e. for all {1, . . . , K}, gt,k(Xt) 0), we apply an additional softplus activation to the output of the 3-layer MLP in the growth networks, defined as softplus() = log(1 + exp()), which is smooth function that transforms negative values to be positive near 0. For the growth network of the primary branch (k = 0), we allow for both positive and negative growth, as all mass starts at the primary branch and flows into the secondary branches, but the primary branch itself can grow as well, depending on whether mass is conserved. State Cost Vt Depending on the dimensionality of the data type, we set the state cost Vt(Xt) : Rd [0, +] to be either the LAND or RBF metric discussed in Appendix A.2. For the experiments on LiDAR (d = 3) and Mouse Hematopoiesis scRNA-seq (d = 2) data, we used the LAND metric (34) with hyperparameters σ = 0.125 and ε = 0.001. To avoid the task of setting suitable variance σ for the high-dimensional gene expression space {50, 100, 150}, we use the RBF metric (38) that learns parameters to ensure the regions within the data manifold have low cost and regions far from the data manifold have high cost. Using the training scheme in Kapusniak et al. [2024], we identified Nc cluster centers with k-means clustering, and trained the parameters {ωj,n}Nc n=1 by minimizing LRBF (39) on the training data. We found that setting the number of cluster centers Nc too low resulted in non-decreasing LRBF and that increasing Nc for higher dimensions enabled more effective training. Furthermore, we found that modeling higher-dimensional PCs required setting larger κ, which determines the spread of the RBF kernel around each cluster center. The specific values for Nc and κ depending on the dimension of principal components are provided in Table 9. Optimal Transport Coupling Since our experiments consist of unpaired initial and target distributions and we seek to minimize the energy of the interpolating bridge, we define pairings (x0, x1,k) using the optimal transport plan π 0,1,k that minimizes the distance between the initial distribution π0 and each target distribution π1,k in probability space. Specifically, we define π 0,1,k as the 2-Wasserstein transport plan [Villani et al., 2008] between π0 and π1,k defined as π 0,1,k = arg minπ0,1,kΠ (cid:90) π0π1,k x0 x1,k 2dπ(x0, x1,k) (94) where π0 π1,k is the set of all possible couplings between the endpoint distributions. For each of the branches, the dataset was paired such that (x0, x1,k) π 0,1,k. Training We train each stage for maximum of 100 epochs. For Stage 1, we used the Adam optimizer [Kingma and Ba, 2014] with learning rate of 1.0 104 to train φt,η(x0, x1). For Stage 2, 3, and 4, we used the AdamW optimizer [Loshchilov and Hutter, 2017] with weight decay t,k and growth network gϕ 1.0 105 and learning rate 1.0 103 to train each flow network uθ t,k. All experiments were performed on one NVIDIA A100 GPU. We trained on the LiDAR and mouse hematopoiesis data with batch size of 128 and the Clonidine and Trametinib perturbation data with batch size of 128, each divided with 0.9/0.1 train/validation split. All hyperparameters across experiments are provided in Table 9. Computational Overhead Although we train + 1 flow and growth networks, the overall training time remains comparable to that of single-branch SBM, since the networks for each branch is trained only on the subset of data corresponding to its respective target distribution. While the method does incur higher space complexity, we find that simple MLP architectures suffice for strong performance, suggesting that scalability is not major concern. BranchSBM also significantly reduces inference time, as predicting branching population dynamics requires simulating only single sample from the initial distribution, unlike other models that require simulating large batches of samples. E.3 LiDAR Experiment Details LiDAR Data We used the same LiDAR manifold from Liu et al. [2023a], Kapusniak et al. [2024]. The data is collection of three-dimensional point clouds within 10 unit cubes [5, 5]3 R3 that span the surface of Mount Rainier. Given any point R3 in the three-dimensional space, we project it onto the LiDAR manifold by identifying the k-nearest neighbors {x1, . . . , xk} and fitting 2D tangent plane to the set of neighbors arg min a,b,c 1 (cid:88) i= exp (cid:18) xi τ (cid:19) (ax(x) + bx(y) + x(z) )2 (95) where τ = 0.001 following Liu et al. [2023a]. Then, we solve for the tangent plane ax + by + = using the MoorePenrose pseudoinverse from = 20 neighbors. From the tangent plane, we can project any point to the LiDAR manifold with the function π(x) defined as π(x) = (cid:19) (cid:18) xv + v2 v, where = (cid:35) (cid:34) 1 (96) Synthetic Distributions To reformulate the experiment in Liu et al. [2023a] as branching problem, we define single initial distribution and two divergent target distributions. Specifically, we define single initial distribution π0 = (µ0, σ0) as mixture of four Gaussians and two target distributions π1,0, π1,1 on either side of the mountain both as mixtures of three Gaussians. The exact parameters of each Gaussian are given in Table 7. We sample total of 5000 points i.i.d. {xi fold with the projection function π(x) in (96). i=1 π1,0, and {xi 1,1}5000 1,0}5000 from each of the Gaussian mixtures {xi i=1 π0, i=1 π1,1. The data points are projected to the LiDAR mani0}5000 Table 7: Synthetic Gaussian mixture distribution parameters for LiDAR experiment. 5000 datapoints are drawn i.i.d. from each of the Gaussian mixtures and paired randomly (x0, x1,0, x1,1) to define the training dataset. visualization of the training data on the LiDAR manifold is provided in Figure 3. Distribution µ (µ0, σ0) π0 π1,0 (µ1,0, σ1,0) π1,1 (µ1,1, σ1,1) (4.5, 4.0, 0.5), (4.2, 3.5, 0.5), (4.0, 3.0, 0.5), (3.75, 2.5, 0.5) (2.5, 0.25, 0.5), (2.25, 0.675, 0.5), (2, 1.5, 0.5) (2, 2, 0.5), (2.6, 1.25, 0.5), (3.2, 0.5, 0.5) σ 0.02 0.03 0.03 Evaluation Metrics To determine how closely the simulated trajectories match the ground truth trajectories, we compute the Wasserstein-1 (W1) and Wasserstein-2 (W2) distances defined as (cid:90) (cid:18) (cid:19) W1 = min πΠ(p,q) y2dπ(x, y) (97) (cid:18) W2 = min πΠ(p,q) (cid:90) y2 2dπ(x, y) (cid:19)1/2 (98) where denotes the ground truth distribution and denotes the predicted distribution. After training the velocity and growth networks on samples from the initial Gaussian mixture π0 and target Gaussian mixtures π1,0 and π1,1, we evaluate W1 and W2 of the reconstructed distribution simulated from the validation points in the initial distribution π0 against the true distribution at = 1. E.4 Differentiating Single-Cell Experiment Details Mouse Hematopoiesis scRNA-seq Data We used the mouse hematopoiesis dataset from Zhang et al. [2025b] consisting of three timesteps t0, t1, t2, with total of 1429 cells at t0, 3781 cells from t1, and 5788 cells from t2. The data points at t0 form homogeneous cluster, while the data points at t2 are clearly divided into two distinct cell fates. We performed k-means clustering with = 2 clusters to create branching on the cells at t2, splitting the cells into two clusters: endpoint 0 with 2902 cells and endpoint 1 with 2886 cells. Since the two endpoints are near equal in ratio, we set the final weights of both endpoints as 0.5 (i.e. w1,0 = w1,1 = 0.5). To match the size of the t0 samples, we randomly sampled 1429 samples from each of these two clusters and used them as the endpoints for branches 0 and 1, respectively. Training and validation follow 0.9/0.1 split ratio. Evaluation Metrics To determine how closely the reconstructed distributions along the trajectory match the ground truth distributions, we compute the 1-Wasserstein (W1) (97) and 2-Wasserstein (W2) (98) distances similar to the LiDAR experiment. After training the velocity and growth networks on samples from the initial cell distribution πt1 and differentiated target cell distributions πt2,0 and πt2,1, we evaluate W1 and W2 between the reconstructed branched distributions at the intermediate time t1 (pt1,0 and pt1,1) and the target distributions at the final time t2 (pt2,0 and pt2,1) simulated from the validation points in the initial distribution π0 and the true distributions πt1 and πt2 . E.5 Cell-State Perturbation Modeling Experiment Details Tahoe Single-Cell Perturbation Data The Tahoe-100M dataset consists of 50 cell lines and over 1000 different drug-dose conditions [Zhang et al., 2025a]. For this experiment, we extract the data for single cell line (A-549) under two drug perturbation conditions selected based on cell abundance and response diversity. 36 Figure 9: Mouse Hematopoiesis Single-Cell RNA Sequencing Data Plotted by Time Point. Real scRNA-seq data is projected to 2D force-directed SPRING plots [Sha et al., 2023, Weinreb et al., 2020, Zhang et al., 2025b]. There is clear divergence of cell fate between times t0 (left), t1 (middle), and t2 (right) from the initial homogeneous progenitor cells into two distinct cell fates (shown in pink and purple in the t2 plot). Cells at time t2 are clustered into endpoint 0 (pink; 2902 cells) and endpoint 1 (turquoise; 2886 cells). Clonidine at 5 µL was selected first due to having the largest number of cells at this dosage, while Trametinib was chosen as the second drug based on its second-highest cell count under the same condition. For both drugs, we selected the top 2000 highly variable genes (HVGs) based on normalized expression and projected the data into 50-dimensional PCA space, which captured approximately 38% of the total variance in both cases. We applied K-nearest neighbor (K = 50) and conducted Leiden clustering separately for drugged versus DMSO control conditions. The most abundant DMSO cluster was selected as the initial state (t = 0). For Clonidine, we identified two clusters that were most distinct from the DMSO control along PC1 and PC2, respectively. These were selected as two distinct endpoints for branches 1 and 2 at = 1. We applied centroid-based sampling to obtain balanced training sets of 1033 cells per cluster (Figure 10). For Trametinib, we extended the branching up to three endpoints. From its Leiden clustering results, we identified three clusters that were the most divergent from the DMSO control clusters along PC1, PC2, and PC3, respectively. All selected clusters contained at least 100 cells and were subsampled to 381 cells each for branch training. The remaining cells were clustered with K-means into three (Clonidine) or four (Trametinib) groups to construct the metrics dataset. The training and validation dataset split followed 0.9/0.1 ratio. The final visualization utilized the first two principal components. Table 8: Training cluster cell counts for perturbation experiments. Clonidine Trametinib Cluster 0 Cluster 1 Cluster 0 Cluster 1 Cluster 2 Original Cell Count Initial Weight w0,k Target Weight w1,k 1675 1.0 0. 1033 0 0.381 1622 1.0 0.603 686 0 0.255 381 0 0.142 Evaluation Metrics To quantify the alignment of the reconstructed and ground-truth distributions for the cell-state perturbation experiment on principal component (PCs) dimensions {50, 100, 150}, we calculate the Maximum Mean Discrepency with the RBF kernel (RBFMMD) on all predicted PCs and the 1-Wasserstein (W1) (97) and 2-Wasserstein (W2) (98) distances on the top-2 PCs. Figure 10: Clustered Cell-State Perturbation Data from the Tahoe-100M Dataset. PCA was conducted on all cells for the control DMSO-treated and two drug-treated populations, and clustered. Plots show divergence along the top 2 PCs. (A) Clonidine-treated cells (5µL) are plotted in pink (endpoint 0) and turquoise (endpoint 1), and the distribution of control cells is plotted in navy. (B) Trametinib-treated cells (5µL) are plotted in purple (endpoint 0), turquoise (endpoint 1), and pink (endpoint 2), and the distribution of control cells is plotted in navy. Given the predicted distribution and true distribution and samples from each distribution {xi p}n i=1, the RBF-MMD between and is calculated as i=1 and {yi q}n MMD(p, q) = 1 n (cid:88) (cid:88) i=1 j=1 kmix(xi, xj) + 1 n (cid:88) (cid:88) i=1 j=1 kmix(yi, yj) 2 n (cid:88) (cid:88) i=1 j=1 kmix(xi, yj) (99) where kmix(, ) is mixture of RBF kernel functions defined as kmix(x, y) = 1 Σ (cid:88) σΣ (cid:18) exp (cid:19) y2 2σ2 (100) where Σ = {0.01, 0.1, 1, 10, 100} is the set of values that determine how much the distances between pairs of points are scaled when computing the overall discrepancy. The equations for 1-Wasserstein (W1) and 2-Wasserstein (W2) distances are provided in (97) and (98) respectively. 38 E.6 Hyperparameter Selection and Discussion In this section, we present the hyperparameters used in each experiment. While the model architecture remained largely the same across experiments, we increased the hidden dimension to 1024 for dimensions {50, 100, 150}. For low-dimensional data, we found that increasing model complexity underperforms in comparison to lower hidden dimensions, and we established that hidden dimension of 64 achieves relatively optimal performance for {2, 3}. While beyond the scope of this study, we believe that further exploration of diverse model architectures and hyperparameter tuning could improve the performance of BranchSBM. Exploration of diverse task-dependent state-costs for novel applications is another exciting extension of our work. Table 9: Hyperparameter settings for different datasets. The Clonidine perturbation experiment is split into three columns for each of the three dimensions of principal components (PCs) used {50, 100, 150}. Parameter Dataset LiDAR Mouse Hematopoesis scRNA Clonidine Perturbation 100PCs 150PCs 50PCs Trametinib Perturbation branches data dimension batch size λenergy λmass λmatch λrecons λgrowth Vt RBF Nc RBF κ hidden dimension lr φt,η lr uθ lr gϕ 2 3 128 1.0 100 1.0 103 1.0 0.01 LAND - - 64 1.0 104 1.0 103 1.0 103 2 2 128 1.0 100 1.0 103 1.0 0.01 LAND - - 64 1.0 104 1.0 103 1.0 103 50 150 1.5 300 3.0 2 100 32 1.0 100 1.0 103 1.0 0.01 RBF 300 2.0 1024 1.0 104 1.0 103 1.0 103 3 50 32 1.0 100 1.0 103 1.0 0.01 RBF 150 1.5 1024 1.0 104 1.0 103 1.0"
        },
        {
            "title": "F Training Algorithm",
            "content": "Here, we provide the pseudocode for BranchSBMs multi-stage training algorithm for stable optimization of the velocity and growth networks over the branched trajectories. Algorithm 1 Multi-Stage Training of BranchSBM 0,1,k, U(0, 1) k, (x0, x1,k) π for = 0 to do 1: Stage 1: Learning the Branched Neural Interpolants 2: while Training do 3: 4: 5: 6: 7: xt,η,k (1 t)x0 + tx1,k + t(1 t)φt,η(x0, x1,k) xt,η,k x1 x0 + t(1 t) φt,η(x0, x1,k) + (1 2t)φt,η(x0, x1,k) Compute Vt(xt,η,k) given the task-specific definition Ltraj(η) (cid:82) 1 Update φt,η using gradient ηLtraj(η) 2 xt,η,k2 + Vt(xt,η,k)(cid:3) dt (cid:2) 1 0 8: 9: end for 10: 11: end while 12: Stage 2: Initial Training of Velocity Networks 13: while Training do 14: 15: 16: 17: Initialize + 1 flow networks {uθ for = 0 to do t,k(Xt)}K k= Calculate xt,η,k and xt,η,k with the trained network φ Lflow(θ) xt,η,k uθ Update uθ t,k(xt,η,k)2 2 t,η(x0, x1,k) Freeze parameters of flow networks and initialize + 1 growth networks {gϕ for = 0 to 1 do t,k(Xt)}K k= t,k using gradient θLflow(θ) 18: end for 19: 20: end while 21: Stage 3: Initial Training of Growth Networks 22: while Training do 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: for = 0 to do xt,k (cid:82) 0 uθ if = 0 then t,k 1 + (cid:82) wϕ t,k (cid:82) wϕ end if Lenergy(ϕ) Lenergy(ϕ) + (cid:82) t+t s,k(xs,k)ds t,k(xs,k)ds t,k(xs,k)ds 0 gϕ 0 gϕ else end for Lmass (cid:16)(cid:80)K k=0 wϕ t,k wtotal(cid:17)2 (cid:104) 1 2 uθ t,k2 + Vt(xt,k) (cid:105) wϕ t,k 33: 34: 35: 36: 37: 38: (cid:17)2 k=0 (cid:16) wϕ end for Lmatch (cid:80)K Lrecons(θ) (cid:80)K Lgrowth(ϕ) λenergyLenergy(θ, ϕ) + λmatchLmatch(ϕ) + λmassLmass(ϕ) + λgrowth Update gϕ x1,kNn(x1,k) max(0, x1,k x1,k2 ϵ) 1,k(x1,k) 1,k (cid:80) t,k using gradient ϕLgrowth(ϕ) k=0 (cid:80)K k=0 gϕ t,k2 2 39: 40: end while 41: Stage 4: Final Joint Training 42: while Training do 43: 44: 45: 46: end while Unfreeze parameters of flow networks {uθ Repeat steps of Stage 3 and calculate Ljoint(θ, ϕ) Lgrowth(θ, ϕ) + Lrecons(θ) Jointly update uθ t,k(Xt)}K t,k for all branches using gradients θLjoint(θ, ϕ) and ϕLjoint(θ, ϕ) t,k and gϕ k="
        }
    ],
    "affiliations": [
        "Center of Computational Biology, Duke-NUS Medical School",
        "Department of Biomedical Engineering, Duke University",
        "Department of Computer Science, Duke University",
        "Department of Computer and Information Science, University of Pennsylvania",
        "Mila, Quebec AI Institute",
        "Université de Montréal"
    ]
}