{
    "paper_title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
    "authors": [
        "Samira Abnar",
        "Harshay Shah",
        "Dan Busbridge",
        "Alaaeldin Mohamed Elnouby Ali",
        "Josh Susskind",
        "Vimal Thilak"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts model's performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 2 0 7 3 2 1 . 1 0 5 2 : r Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models Samira Abnar Apple Harshay Shah MIT Dan Busbridge Apple Alaaeldin Mohamed Elnouby Ali Apple Josh Susskind Apple Vimal Thilak Apple Abstract Scaling the capacity of language models has consistently proven to be reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs) , which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts models performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures."
        },
        {
            "title": "1 Introduction",
            "content": "Empirical scaling laws for language model pretraining (Kaplan et al., 2020; Hoffmann et al., 2022; OpenAI, 2023; OpenAI et al., 2024; Gemini Team et al., 2024; Henighan et al., 2020; Clark et al., 2022; Yun et al., 2024; Ludziejewski et al., 2024) have demonstrated that proportionally increasing model capacity, along with data and total compute budget, consistently decreases pretraining loss (i.e., perplexity), improves downstream task performance (Devlin et al., 2019; Brown et al., 2020; BIG-bench authors, 2023) and unlocks emergent capabilities (Wei et al., 2022a). recurring notion in these studies is that model capacity is well quantified by the total number of model parameters. However, the number of parameters is not the only means to increase model capacity compute per example (i.e., fixed-sized input), measured in FLoating OPerations (FLOPs), also plays significant role (Clark et al., 2022). In fact, several mechanisms (Shazeer et al., 2017; Dehghani et al., 2019; Wei et al., 2022b; Goyal et al., 2024; Csordas et al., 2024) allow for independent variation of the number of parameters or FLOPs per example within model. For instance, Sparse Mixture-of-Experts (MoE) models (Shazeer et al., 2017) introduce FLOP-free parameters by leveraging sparsity, where only subset of expert modules is activated for each input. When studying scaling laws for specific classes of models, e.g., vanilla transformers, the total number of parameters can serve as reasonable relative estimator of FLOPs per example. Therefore, Core contributors using the number of parameters as measure of model capacity in scaling law studies is appropriate. However, in scenarios or for architectures where the number of parameters and FLOPs per example are not directly linked, it is essential to jointly consider the effects of these variables on scaling model capacity (Clark et al., 2022). We therefore ask Can we draw scaling laws for the optimal trade-off between parameter count and FLOPs per example? To address this question, we study sparse Mixture-of-Expert Transformers (MoEs) (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022; Zoph et al., 2022; Muennighoff et al., 2024) in the context of language modeling. Existing scaling law studies for MoEs, investigate the role of variables like number and granularity (Ludziejewski et al., 2024)of experts, underlying dense model size and inference compute in predicting the performance of the models under different conditions such as training or inference compute optimality (Du et al., 2021; Clark et al., 2022; Yun et al., 2024; Ludziejewski et al., 2024). In this paper, we focus on the interaction between FLOPs per example and total parameter count, and their impact on model performance in MoEs, through large-scale empirical study. We define sparsity as the ratio of inactive experts to the total number of experts, which controls the ratio of the total number of parameters to FLOPs per example in MoEs. We evaluate loss and downstream metrics for different sparsities, model sizes, and compute budgets. Through qualitative and quantitative analysis to derive scaling laws which disentangle total parameters vs FLOPs per example in MoEs, we can estimate the optimal sparsity level under the setting where both total training FLOPs and total number of parameters are given and fixed. Generally, we find that: During pretraining, increasing models capacity by adding more parameters yields greater benefits than increasing FLOPs per example. We observe that the size of compute-optimal models increases as we increase the training budget (measured in terms of total FLOPs) while the active number of parameters, hence FLOPs per example, decrease for compute-optimal models. During inference, FLOPs per example seem to play more important role2. For many tasks, upstream performance is good predictor of downstream performance and the relationship between upstream and downstream performance is not impacted by the sparsity level. However, we observe that for models with the same perplexity on the pretraining data distribution, sparser models, i.e., models with fewer number of active parameters, perform worse on specific types of downstream tasks that presumably require more reasoning. Our results, in line with findings from previous relevant studies (Ludziejewski et al., 2024; He, 2024) on scaling laws for MoEs, show increasing sparsity level leads to better performance and efficiency during pretraining. Considering the various methods to increase compute per example during inference adaptively conditioned on task or example complexity, we conclude that MoEs approaches, which reduce the unit compute cost (i.e., FLOPs per token) by increasing the sparsity level, hold significant promise given their potential to enhance efficiency in both pretraining and inference."
        },
        {
            "title": "MoEs",
            "content": "Is there an optimal trade-off between parameter count and FLOPs per example in MoEs under the setting where the training compute budget (i.e., total training FLOPs) is fixed? 2A relevant discussion here is the recent trend of increasing test-time compute, e.g., OpenAI o1 model (OpenAI et al., 2024), achieved by generating more tokens as way for introducing parameter-free-FLOPs. 2 (a) IsoFLOP surface over sparsity and total parameters (b) IsoFLOP surface over sparsity and active parameters Figure 1: IsoFLOP surface over observed pretraining loss L, model size (in terms of total and active parameters Na), and sparsity S. We fit polynomial function mapping (or Na), S, and their interaction to L, using empirical data. Both fits achieve an MSE loss of 0.0001 on held-out set. These results indicate that for fixed compute budget, increasing model sparsity leads to reduction in pretraining loss. When considering optimal model size, we observe opposite trends for total parameters (N) (Figure a) versus active parameters (Na) (Figure b). (Figure 8 includes results Appendix D.1 for different total compute budgets C.) Intuitively, under infinite data setting, scaling model capacity along with the training compute budget leads to performance improvements. Previous scaling law studies suggest that, conditioned on training compute budget measured in FLOPs denoted by C, the optimal number of parameters, (C), exhibits power-law relationship with (Hoffmann et al., 2022): (C) = arg min L(N ; C) (1) Our goal is to study how to optimally trade-off FLOPs per example and total parameters in MoEs. In MoEs the balance between parameters and FLOPs can be expressed through the sparsity level, S. We define as the ratio of non-active to total number of experts, i.e., = EK ; where is the total number of experts and is the number of selected experts per token. We can vary the sparsity level by either changing the number of active experts or total number of experts E. 3 Essentially, for models with the same , the model with higher will have fewer active parameters Na, resulting in fewer FLOPs per example. For more details on the notations and experimental settings see Appendix and Appendix B. (N , S) = arg min N,S L(N, S; C) (2) To simplify the problem of understanding the joint role of and in predicting L, we break the problem, Equation 2, into two parts: 1. \"How does the sparsity level impact the scaling laws of the relationship between and for training-compute optimal models?\" To address this question in 2.1, we fix and vary , studying how optimal and Na change for different values of S: = arg min L(N ; C, S) (3) 3Sparsity level determines the number of active parameters given the total number of parameters and we use the active number of parameters as proxy for FLOPs per example, as 6NaD provides good estimate of the total FLOP count for MoEs; see Appendix for details. Figure 2: IsoFLOP slices along Sparsity and Model Size (C = 1e20). We use fitted isoFLOP surfaces (Section 2) to analyze how sparsity and model size impact the loss for fixed compute budget. We identify optimal points by (a) fixing and varying S, (b) fixing and varying and (c) fixing and varying active parameters Na. Observe that (a) the optimal sparsity increases with increasing model size and converges to 1 while (b) and (c) show that the optimal model size and active parameter count Na increase and decrease respectively with increasing sparsity levels. (see Figure 9 in Appendix D.1 for other total training compute budgets.) 2. \"Is there an optimal balance between total number of parameters and the sparsity level under fixed training-compute budget?\" To address this question in 2.2, we fix and vary S, studying how optimal changes across different values of : = arg min L(S; C, ) (4) As the first step, considering fixed training compute budget C, we fit 3D surface, referred to as the IsoFLOP surface, in Figure 1a, using polynomial function, following approach II of Hoffmann et al. (2022). Compared to Hoffmann et al. (2022) we include the sparsity variable and fit single 3d IsoFLOP surface across all data points, rather than fitting separate 2d IsoFLOP curves for fixed sparsity levels or model sizes. We conducted grid search to determine the optimal polynomial degree for , S, and the interaction term S, finding that degree of (2, 2, 2) resulted in the lowest cross-validation error. Both and are in log space (see Appendix for more details). As seen in Figure 1a, the IsoFLOP surface plot is parabolic along model size, suggesting that the findings of Hoffmann et al. (2022) extend to MoEs across different sparsity levels, i.e., L(N ; C, S) is parabolic, with its optimal solution located at the turning point. When considering the total number of parameters , the optimal value increases as the sparsity level increases, while for the active number of parameters Na the optimal value decreases with the sparsity level. This indicates that by increasing the sparsity level the training compute optimal models are larger but have fewer FLOPs per example, i.e., lower inference cost. Moreover, along sparsity, the pretraining loss decreases monotonically, indicating that, for the same compute budget, sparser models achieve better pretraining performance. We observe the same pattern across different training compute budgets (See Appendix D.1). To better understand and explain these observations, we examine slices of the IsoFLOP surface along the axes of and separately in 2.1 and 2.2, respectively."
        },
        {
            "title": "2.1 Optimal Model Size for Fixed Sparsity Level",
            "content": "Here we examine how sparsity influences scaling laws governing the relationship between , Na and for training-compute optimal models, i.e. how does and , for given C, (Equaa tion 3), change as we increase S? Looking at slices of the IsoFLOP surface along the model size dimension, in Figure 2b and Figure 2c, we observe how the IsoFLOP curves shift along loss and model size. Considering the training-compute optimal model, for fixed compute budget, loss decreases as we increase sparsity. Furthermore, while sparser models have larger compared to 4 Figure 3: Effect of compute budget on model size, number of active parameters and loss with sparsity. Across all compute budgets, we observe that (a) the optimal model size increases with sparsity, (b) the optimal number of active parameters Na decreases with sparsity, and (c) the loss decreases with sparsity. denser models, as seen in Figure 2b, they have smaller active parameter count Na; hence, fewer FLOPs per example. Intuitively, more parameters in total increase the capacity of the sparser models to fit the data, while fewer number of active parametes, hence fewer FLOPs per example, allow the model to be trained with more tokens, i.e., higher D, for the same training compute budget."
        },
        {
            "title": "2.2 Optimal Sparsity Level for Fixed Model Size",
            "content": "In this section we aim to understand the dynamics between the total number of parameters and FLOPs per example in MoEs. In Section 2.1 we are considering the case where there is no bound on the total number of parameters. In this case, we observe that under fixed training compute budget in terms of FLOPs, it is better to train sparser models with higher total number of parameters. However in practical scenarios it is reasonable to assume that there would be some bounds on the memory and hence the total number of parameters of model. This leads us to fundamental question: Is there an optimal balance between the total number of parameters and and FLOPs per example under fixed training-compute budget? Thus, we investigate the optimal sparsity level when total number of parameters is fixed. Specifically, we ask: Given and C, How does change as we vary ? To address this, we look into slices of the IsoFLOP surface along the sparsity dimension. As we can see in Figure 2a, for fixed training compute budget and fixed model size L(S; N, C) exhibits parabolic profile, reaching its optimum value at the vertex where = S. It is noteworthy that for given total training compute, there is threshold value Nth for the total number of parameters, where for larger models, models with > Nth, increasing sparsity always has positive impact, i.e., optimal sparsity level approaches 1.0. More accurately, for fixed compute budget the optimal sparsity level increases with model size and converges to 1 as the model size grows (see Figure 4 in D.2 in the Appendix for more details). Note that the optimal model, here is not the largest model, i.e., there is compute optimal model size in terms of total parameters even after sparsity is introduced, and increasing total number of parameters would lead to under-training if training compute budget is fixed. These results highlight the importance of balancing the number of parameters with FLOPs per example in MoEs. Intuitively, when the total number of parameters is small, higher sparsity results in fewer active parameters, and thus fewer FLOPs per example. This reduction in FLOPs per example may lead to inefficiencies during both training and inference. Conversely, when the total number of parameters is large, for reasonable amount of FLOPs per example, fixed compute budget may not allow sufficient training on enough tokens to make use of the models additional capacity."
        },
        {
            "title": "3 Impact of Training Compute Budget on the Interaction be-",
            "content": "tween Model Parameters and Sparsity Does increasing compute budget impact the interaction between the parameters and FLOPs per example in MoEs and how they contribute to models capacity? In other words, does the recipe for optimally increasing model capacity, i.e., optimal sparsity level for MoEs change as we scale up the total training compute? To answer this question. parameters, , the number of active parameters, different compute budgets. in Figure 3 we illustrate the trends for changing the total number of , and the loss, L, with sparsity level across Figure 3c shows that the optimal sparsity level approaches 1 across all compute budgets used in our experiments. There is no significant difference observed in the slope of the loss vs sparsity curves across different training compute budgets used in our experiments. This observation suggests that there is no diminishing effect of sparsity on the pretraining loss as we increase training compute budget, i.e., if there is no constraint on the model size, sparsity improves the performance of the model across all training budgets. In Figure 3a and Figure 3b, , we see consistent trend of increasing and decreasing Na for compute optimal models as sparsity level increases across all training compute budgets. Moreover, as can be seen in Figure 4, when model size in terms of total number of parameters is fixed, optimal sparsity level increases with training compute budget as well as model size as discussed in Section 2.2. Figure 4: Effect of training budget and total parameters on MoE sparsity. Optimal MoE sparsity changes with respect to the total number of parameters and the training budget C. The x-axis represents the total parameters on logarithmic scale, and the y-axis shows the optimal MoE sparsity S."
        },
        {
            "title": "4 Effect of MoE Sparsity on Downstream Task Performance",
            "content": "In this section, we study how sparsity affects the relationship between upstream and downstream performance of MoEs. In other words, does sparsity impact the relative gains from improvements in pretraining tasks on downstream tasks? We use downstream tasks from the evaluation suite in llm-foundry4 for benchmarking our pretrained models, specifically in an in-context few-shot learning setup. This setup focuses on evaluating models ability to learn and adapt to new tasks with limited examples. The downstream task are devided into four pre-defined categories namely: language understanding, world knowledge, reading comprehension, and symbolic reasoning to help us systematically test whether the downstream vs upstream performance trend remains the same or is different as we vary sparsity values. We observe from Figure 5a (language understanding), Figure 5c (commonsense reasoning), and Figure 5d (world knowledge) that, in an in-context few-shot learning setting, there is strong correlation between upstream (pretraining) loss and downstream performance (error) across all these tasks. For these tasks, downstream performance in the few-shot setting is predictable based 4Github repository: https://github.com/mosaicml/llm-foundry 6 Figure 5: Effect of sparsity on downstream vs upstream performance. Downstream error shows tight relationship with pretraining (upstream) loss across downstream tasks across all sparsity levels. on upstream performance, regardless of the sparsity level. This indicates that, in the context of these tasks, the optimal sparsity level follows the same trend as the optimal sparsity observed during pretraining. However, Figure 5b (reading comprehension) shows an example of task where models with higher sparsity transfer more poorly compared to denser models. This decrease in the transfer performance of sparser models on these tasks may be due to the lower inference-time compute in sparser models compared to their denser counterparts for similar pretraining loss. Further analysis is needed to verify this intuition. If fewer FLOPs per example are the reason behind the worse transfer performance in sparser models, this effect might diminish with larger total training compute budget, as the optimal active number of parameters increases. Moreover, one can use approaches like chain-of-thought reasoning (Wei et al., 2022b) to independently increase FLOPs per example during inference time. While our results may indicate that there may be no additional benefit obtained via sparsity in MoEs beyond the efficiency gains for pretraining, we caution the reader that this suggestion may be an artifact of the scale of our experiments. In the end, since, as shown in 2, sparser models are more efficient both in terms of training and inference cost (when measured in terms of theoretical FLOPs), we can reach better pretraining performance with higher sparsity levels at lower cost, which can translate to better downstream performance."
        },
        {
            "title": "5 Incorporating Sparsity into Scaling Laws",
            "content": "The scaling laws proposed by Kaplan et al. (2020) provide framework for predicting loss in dense models by establishing power-law relationship between loss L, number of parameters and dataset size D, where and interact linearly. Formally, the relationship is given by: L(N, D) = Dβ + α + Here, the term α captures the inverse relationship between model size and loss, where an increase in model size leads to reduction in loss. The exponent α quantifies the rate of this decrease; larger α suggests steeper reduction in loss with increasing model size. Similarly, the term Dβ indicates the impact of dataset size on loss, with larger datasets contributing to lower loss values. The exponent β measures this relationship, where larger β implies greater benefit from increased data. The constant represents an asymptotic minimum for the loss, as both model size and dataset size approach infinity. (5) For dense models with fixed total training FLOPs, C, the parameters and are interrelated through the equation for estimating FLOPs per example, given as = 6N for transformers. However, in MoEs (Mixture of Experts models), this relationship involves the active number of parameters Na rather than the total parameter count . Thus, and Na define the total training 7 FLOPs rather than and . Given the analysis conducted in 2, we know that if the total number of parameters is fixed, the optimal sparsity level, i.e., active number of parameters would depend on . Motivated by this observation, we suggest the following parametric form that includes multiplicative interaction between and or Na to predict the loss: L(N, D, S) = α + Dβ + (1 S)λ + (1 S)δ γ + (6) The term (1 S) in the above equation provides rough estimate of the percentage of number of active parameters. If the exponent for the multiplicative terms is the same then that term provides an approximate estimate of the number of active parameters. By incorporating sparsity into the scaling law equation, we can eliminate the need for parameters specific to MoEs, such as the total and active number of experts. As demonstrated by Frantar et al. (2024), this formulation also holds for other sparsity mechanisms, such as weight sparsity, where individual neural network connections are pruned. We use the recipe described by Hoffmann et al. (2022) and use the L-BFGS algorithm to fit the coefficients in equation 6 using Huber loss with δ = 103. Optimal coefficient values were determined through grid search (see Table 2 for search values). The results of data fitting and validation are shown in Figure 6. The estimated values are shown in Table 3 in Appendix E. (a) Fit on data used to estimate coefficients. (b) Validating scaling law on held-out dataset. Figure 6: Scaling law fit on data obtained from training compute-optimal models. Figure 6(a) shows the fit on the data used to estimate the coefficients for equation 6, while Figure 6(b) validates these coefficients on held-out dataset. All data points with = 0.98 were excluded from the fitting process for out-of-sample validation."
        },
        {
            "title": "6 Discussion",
            "content": "Our findings amplify the findings of Ludziejewski et al. (2024) and further justify the effort to work toward MoEs with experts larger in number and smaller in size (He, 2024). For downstream tasks which their performance is predictable given the pretraining loss (i.e., perplexity), sparsity potentially provides efficiency gains both during pretraining and inference. Here is summary of our observations as discussed in Sections 2 to 5 : Larger, Sparser Models Perform Better under Fixed Compute Budget: When memory and communication overheads are disregarded, increasing sparsity while proportionally expanding the total number of parameters consistently leads to lower pretraining loss, even when constrained by fixed training compute budget (see 2). 8 Optimal Sparsity for Fixed Model Size: For any given number of parameters and under fixed training compute budget, model performance as function of sparsity exhibits parabolic pattern, reaching its peak at an optimal sparsity level (see 2.2). Specifically, the optimal sparsity level: Increases with the total number of parameters approaching 1.0 for larger models. i.e., if model is relatively small for given training compute budget, sparsifying it more than threshold will hurt its performance. On the other hand, if model is relatively large for given compute budget, further sparsifying it helps as it leads to increase in the number of tokens the model is trained on under the given training budget constraints (see 2.2). Increases across all model sizes as the training compute budget increases (see D.1 and D.2). Effect of Sparsity on Scaling Laws for Optimal Model Size: For any specific sparsity level, performance of the models as function of their size exhibits parabolic behavior under fixed training compute budget. i.e., the model reaches its optimal performance at vertex, that indicates optimal model size. Under these conditions: The optimal active number of parameters decreases as the sparsity level increases, leading to smaller FLOPs per example and more efficient inference even though the total number of parameters increases (see 2.1). While the trend of increasing active number of parameters is similar across all training compute budgets; the optimal active number of parameters decrease more rapidly with sparsity as the training compute budget increases (see 3). Effect of Sparsity on Downstream Performance: For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2019), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022b; Goyal et al., 2024) may address this gap. Parametric Scaling Law: We propose parametric form for scaling laws that accounts for sparsity. The model coefficients are estimated using the empirical data obtained by training compute-optimal models. An interesting observation from Appendix is that the exponent for sparsity term λ is negative which is consistent with our intuition that sparser models lead to lower perplexity."
        },
        {
            "title": "6.1 Limitations",
            "content": "In our analysis, similar to other scaling law studies (Kaplan et al., 2020; Hoffmann et al., 2022), we have measured the costs for both training and inference exclusively in terms of FLOPs. While there may be discrepancies between actual computational costs and theoretical FLOPs due to hardware specifications, infrastructure, and implementation details, it is reasonable to abstract away from these factors when comparing similar models under fixed conditions. However, an important aspect not accounted for in this study is the cost associated with memory usage and communication overhead, which could potentially increase as we raise the sparsity level. Incorporating these factors is challenging because they are highly dependent on the hardware used. To address this limitation to some extent, in Section 2.2 we investigate the optimal sparsity level under the setting where total number of parameters is fixed. Despite the limitation with using an approximate method to quantify FLOPs, our findings highlight the importance of investing in methods to enhance the efficiency of sparse Mixture-of-Experts models. By increasing model capacity through additional parameters while minimizing per-unit computation costs, these models have the potential to improve both efficiency and performance. The availability of GPUs with larger memory, for e.g., the recently introduced H200 GPU chip with 141 GB of memory as well as improving the efficiency of training and deployment pipelines (Authors, 2025) suggest that there is significant interest in developing efficient implementations for 9 MoEs."
        },
        {
            "title": "7.1 Scaling Laws for Language Models",
            "content": "Scaling laws have proven to be powerful framework for understanding and predicting the performance of language models. Existing studies, such as Kaplan et al. (2020) and Hoffmann et al. (2022), reveal that power-law relationships govern model performance as function of factors like model size, data size, and compute budget, offering predictable performance improvements with increased resources. Hoffmann et al. (2022) emphasizes the critical balance between model size and the number of training tokens when the training compute budget is fixed, showing that scaling the model without corresponding data increases can lead to suboptimal performance. Additionally, DeepSeek-AI (2024) explores more nuanced scaling behaviors by incorporating data quality, demonstrating that higherquality data allows for more efficient scaling, and thus, larger portion of the compute budget should be allocated to increasing model size. Recent work extends scaling law analysis to specialized contexts, including over-training (Gadre et al., 2024), downstream task performance, and multilingual or multi-modal settings, where scaling laws provide valuable insights and can be adapted to address specific challenges."
        },
        {
            "title": "7.2 Scaling Laws for MoEs",
            "content": "Mixture-of-Experts (MoE) models (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022; DeepSeek-AI, 2025) have emerged as powerful architecture for language modeling, primarily because they decouple computational cost from parameter count. This separation between parameters and FLOPs per token in MoE architectures calls for scaling laws that can accurately factor in the contributions of both. Previous research on the scaling behavior of MoE models has established foundational scaling laws, incorporating factors such as total parameter count, the number of experts, and the granularity of these experts (Clark et al., 2022; Ludziejewski et al., 2024; Wang et al., 2024). However, these studies typically assume fixed configuration for other critical variables influencing FLOPs per token, such as the number of active experts per input. In contrast, we propose generalized scaling law that considers variables like active parameter count and sparsity level, thereby expanding the applicability of MoE scaling laws. common theme in the literature suggests that training sparser modelsachieved by increasing the number of smaller expertsoffers significant gains in efficiency for both pretraining and inference phases. Through comprehensive large-scale study, we provide empirical evidence for this, analyzing the impact of sparsity level on efficiency and defining optimal configurations. Supporting this, Du et al. (2021) demonstrates GLaMs superior efficiency and performance compared to GPT-3, showing that MoE architectures can achieve high performance with significantly lower computational and energy costs. Further insights are offered by Clark et al. (2022), who analyze scaling behaviors across various MoE routing techniques. While their study finds that MoEs generally outperform dense models, it also notes diminishing benefits as base model sizes grow. Ludziejewski et al. (2024) challenge this conclusion, attributing the diminished returns partly to the fixed number of training tokens across models and constant expert sizes. By introducing \"granularity\" and adjusting training durations, they demonstrate that MoEs can outperform dense models across any compute budget, debunking the notion of diminishing returns for MoEs with adaptive expert configurations. More recently, Jelassi et al. (2024) finds that, on downstream tasks, MoEs 10 scale efficiently with the number of experts (i.e., increasing sparsity) on memorization tasks, but their reasoning capabilities saturate and lag behind dense models on tasks requiring complex reasoning when compared based on total number of parameters. Another approach by He (2024) explores the benefits of training MoEs with larger numbers of smaller experts rather than the conventional setup of fewer, larger experts. They introduce Parameter Efficient Expert Retrieval (PEER), novel routing mechanism designed to tackle the computational and optimization challenges that arise when handling high number of experts, thus enabling efficient scaling of MoE models. Lastly, Yun et al. (2024) draws attention to the increased inference costs associated with scaling MoEs by adding experts. While additional experts may not substantially affect training costs, they can inflate inference costs, thereby diminishing deployment efficiency. To address this, the study proposes an over-trained budget allocation strategy, optimizing MoE models for both performance and efficiency in deployment."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we investigated the optimal trade-off between parameters and compute per example for maximizing model capacity. Our findings indicate that sparsity, as knob that controls FLOPs per example in MoEs, is powerful mechanism for optimizing model performance under constrained training compute budgets. By balancing the total number of parameters, compute, and sparsity, MoEs can be scaled more effectively. These insights provide valuable guidance for scaling language models, especially for MoEs, where the trade-offs between parameters and FLOPs must be carefully managed. MoEs were originally introduced to allow increasing model capacity without significant increase in inference cost. Our experiments show that under fixed total training compute budget increasing sparsity in MoEs leads to smaller FLOPs per example, higher number of parameters, and lower pretraining loss simultaneously. In other words, in the context of MoEs, if there are no constraints on the total number of parameters, increasing the capacity of the model through parameter count seem to be the optimal strategy if lower pretraining loss is the main goal. On the other hand, when comparing how well the pretraining performance transfers to various downstream tasks, denser models exhibit better transfer performance on certain types of task that potentially rely on deeper processing of the input vs the knowledge stored in the parameters of the model. This potentially signals the importance of the role of FLOPs per example in increasing the capacity of the model during inference. This observation reveals an interesting direction to improve the performance efficiency of MoEs at inference time. Future work will focus on determining the optimal balance between FLOPs per example and parameter count, with an emphasis on conducting in-depth analyses of model performance across diverse downstream tasks. key direction will involve exploring strategies to balance parameter allocation and computational demands to minimize inference costs. Developing scaling law studies to identify optimal approaches for achieving efficiency and performance during inference represents critical area for further investigation. Another important avenue will be to examine how the findings on the role of sparsity in MoEs generalize to architectures or approaches that employ different mechanisms for independently adjusting FLOPs per example and the number of trainable parameters. Additionally, an intriguing direction for future exploration is the study of scaling behaviors in models that enable negative sparsity values through parameter sharing."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors would like to thank Vaishaal Shankar, Fartash Faghri, Skyler Seto, Mustafa Shukor, Amitis Shidani, David Grangier, Etai Littwin, Alexander Toshev and Preetum Nakkiran for their insightful discussions, feedback and technical support that significantly contributed to the development of this paper."
        },
        {
            "title": "References",
            "content": "N. Authors. Nemo: toolkit for conversational ai and large language models. https:// github.com/NVIDIA/NeMo, 2025. BIG-bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities ISSN 2835-8856. URL of language models. Transactions on Machine Learning Research, 2023. https://openreview.net/forum?id=uyTL5Bvosj. S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are fewshot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. A. Clark, D. d. l. Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud, G. v. d. Driessche, E. Rutherford, T. Hennigan, M. Johnson, K. Millican, A. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan. Unified scaling laws for routed language models. In Proceedings of the 39th International Conference on Machine Learning. PMLR, 2022. R. Csordas, K. Irie, J. Schmidhuber, C. Potts, and C. D. Manning. Moeut: Mixture-of-experts universal transformers. ArXiv, abs/2405.16039, 2024. URL https://api.semanticscholar. org/CorpusID:270063139. DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. ArXiv, abs/2401.02954, 2024. URL https://api.semanticscholar.org/CorpusID: 266818336. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. https://github.com/deepseek-ai/DeepSeek-R1, Jan. 2025. Accessed: 202501-21. M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=HyzdRiR9Y7. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of language models with mixture-of-experts. ArXiv, abs/2112.06905, 2021. URL https://api.semanticscholar.org/CorpusID:245124124. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23(1), jan 2022. ISSN 1532-4435. 13 E. Frantar, C. R. Ruiz, N. Houlsby, D. Alistarh, and U. Evci. Scaling laws for sparsely-connected foundation models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=i9K2ZWkYIP. S. Y. Gadre, G. Smyrnis, V. Shankar, S. Gururangan, M. Wortsman, R. Shao, J. Mercat, A. Fang, J. Li, S. Keh, R. Xin, M. Nezhurina, I. Vasiljevic, J. Jitsev, A. G. Dimakis, G. Ilharco, S. Song, T. Kollar, Y. Carmon, A. Dave, R. Heckel, N. Muennighoff, and L. Schmidt. Language models scale reliably with over-training and on downstream tasks. CoRR, abs/2403.08540, 2024. URL https://doi.org/10.48550/arXiv.2403.08540. T. Gale, D. Narayanan, C. Young, and M. Zaharia. MegaBlocks: Efficient Sparse Training with Mixture-of-Experts. Proceedings of Machine Learning and Systems, 5, 2023. Gemini Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models, 2024. URL https: //arxiv.org/abs/2312.11805. S. Goyal, Z. Ji, A. S. Rawat, A. K. Menon, S. Kumar, and V. Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ph04CRkPdC. X. O. He. Mixture of million experts. arXiv preprint arXiv:2407.04153, 2024. T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray, C. Hallacy, B. Mann, A. Radford, A. Ramesh, N. Ryder, D. M. Ziegler, J. Schulman, D. Amodei, and S. McCandlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv: Arxiv-2010.14701, 2020. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre. An empirical analysis of compute-optimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3001630030. Curran Associates, URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf. Inc., 2022. S. Jelassi, C. Mohri, D. Brandfonbrener, A. Gu, N. Vyas, N. Anand, D. Alvarez-Melis, Y. Li, S. M. Kakade, and E. Malach. Mixture of parrots: Experts improve memorization more than reasoning. arXiv preprint arXiv:2410.19034, 2024. J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/pdf/2001.08361.pdf. D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=qrwe7XHTmYb. J. Ludziejewski, J. Krajewski, K. Adamczewski, M. Pióro, M. Krutul, S. Antoniak, K. Ciebiera, K. Król, T. Odrzygóźdź, P. Sankowski, M. Cygan, and S. Jaszczur. Scaling laws for fine-grained mixture of experts. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. URL https://openreview.net/forum?id=Iizr8qwH7J. N. Muennighoff, L. Soldaini, D. Groeneveld, K. Lo, J. Morrison, S. Min, W. Shi, P. Walsh, O. Tafjord, N. Lambert, Y. Gu, S. Arora, A. Bhagia, D. Schwenk, D. Wadden, A. Wettig, B. Hui, T. Dettmers, D. Kiela, A. Farhadi, N. A. Smith, P. W. Koh, A. Singh, and H. Hajishirzi. Olmoe: Open mixtureof-experts language models, 2024. URL https://arxiv.org/abs/2409.02060. 14 OpenAI. Gpt-4 technical report. PREPRINT, 2023. OpenAI, :, A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, A. Iftimie, A. Karpenko, A. T. Passos, A. Neitz, A. Prokofiev, A. Wei, A. Tam, A. Bennett, A. Kumar, A. Saraiva, A. Vallone, A. Duberstein, A. Kondrich, A. Mishchenko, A. Applebaum, A. Jiang, A. Nair, B. Zoph, B. Ghorbani, B. Rossen, B. Sokolowsky, B. Barak, B. McGrew, B. Minaiev, B. Hao, B. Baker, B. Houghton, B. McKinzie, B. Eastman, C. Lugaresi, C. Bassin, C. Hudson, C. M. Li, C. de Bourcy, C. Voss, C. Shen, C. Zhang, C. Koch, C. Orsinger, C. Hesse, C. Fischer, C. Chan, D. Roberts, D. Kappler, D. Levy, D. Selsam, D. Dohan, D. Farhi, D. Mely, D. Robinson, D. Tsipras, D. Li, D. Oprica, E. Freeman, E. Zhang, E. Wong, E. Proehl, E. Cheung, E. Mitchell, E. Wallace, E. Ritter, E. Mays, F. Wang, F. P. Such, F. Raso, F. Leoni, F. Tsimpourlas, F. Song, F. von Lohmann, F. Sulit, G. Salmon, G. Parascandolo, G. Chabot, G. Zhao, G. Brockman, G. Leclerc, H. Salman, H. Bao, H. Sheng, H. Andrin, H. Bagherinezhad, H. Ren, H. Lightman, H. W. Chung, I. Kivlichan, I. OConnell, I. Osband, I. C. Gilaberte, I. Akkaya, I. Kostrikov, I. Sutskever, I. Kofman, J. Pachocki, J. Lennon, J. Wei, J. Harb, J. Twore, J. Feng, J. Yu, J. Weng, J. Tang, J. Yu, J. Q. Candela, J. Palermo, J. Parish, J. Heidecke, J. Hallman, J. Rizzo, J. Gordon, J. Uesato, J. Ward, J. Huizinga, J. Wang, K. Chen, K. Xiao, K. Singhal, K. Nguyen, K. Cobbe, K. Shi, K. Wood, K. Rimbach, K. Gu-Lemberg, K. Liu, K. Lu, K. Stone, K. Yu, L. Ahmad, L. Yang, L. Liu, L. Maksin, L. Ho, L. Fedus, L. Weng, L. Li, L. McCallum, L. Held, L. Kuhn, L. Kondraciuk, L. Kaiser, L. Metz, M. Boyd, M. Trebacz, M. Joglekar, M. Chen, M. Tintor, M. Meyer, M. Jones, M. Kaufer, M. Schwarzer, M. Shah, M. Yatbaz, M. Y. Guan, M. Xu, M. Yan, M. Glaese, M. Chen, M. Lampe, M. Malek, M. Wang, M. Fradin, M. McClay, M. Pavlov, M. Wang, M. Wang, M. Murati, M. Bavarian, M. Rohaninejad, N. McAleese, N. Chowdhury, N. Chowdhury, N. Ryder, N. Tezak, N. Brown, O. Nachum, O. Boiko, O. Murk, O. Watkins, P. Chao, P. Ashbourne, P. Izmailov, P. Zhokhov, R. Dias, R. Arora, R. Lin, R. G. Lopes, R. Gaon, R. Miyara, R. Leike, R. Hwang, R. Garg, R. Brown, R. James, R. Shu, R. Cheu, R. Greene, S. Jain, S. Altman, S. Toizer, S. Toyer, S. Miserendino, S. Agarwal, S. Hernandez, S. Baker, S. McKinney, S. Yan, S. Zhao, S. Hu, S. Santurkar, S. R. Chaudhuri, S. Zhang, S. Fu, S. Papay, S. Lin, S. Balaji, S. Sanjeev, S. Sidor, T. Broda, A. Clark, T. Wang, T. Gordon, T. Sanders, T. Patwardhan, T. Sottiaux, T. Degry, T. Dimson, T. Zheng, T. Garipov, T. Stasi, T. Bansal, T. Creech, T. Peterson, T. Eloundou, V. Qi, V. Kosaraju, V. Monaco, V. Pong, V. Fomenko, W. Zheng, W. Zhou, W. McCabe, W. Zaremba, Y. Dubois, Y. Lu, Y. Chen, Y. Cha, Y. Bai, Y. He, Y. Zhang, Y. Wang, Z. Shao, and Z. Li. Openai o1 system card. arXiv preprint arXiv: 2412.16720, 2024. P. Rajpurkar, R. Jia, and P. Liang. Know what you dont know: Unanswerable questions for SQuAD. In I. Gurevych and Y. Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https: //aclanthology.org/P18-2124. S. Reddy, D. Chen, and C. D. Manning. CoQA: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. doi: 10.1162/tacl_ a_00266. URL https://aclanthology.org/Q19-1016. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id= B1ckMDqlg. Together Computer. Redpajama: An open source recipe to reproduce llama training dataset. https://github.com/togethercomputer/RedPajama-Data, Apr. 2023. Accessed: YYYY-MM-DD. S. Wang, Z. Chen, B. Li, K. He, M. Zhang, and J. Wang. Scaling laws across model architectures: comparative analysis of dense and MoE models in large language models. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 55835595, Miami, Florida, USA, Nov. 15 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.319. URL https://aclanthology.org/2024.emnlp-main.319/. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. URL https://openreview.net/forum?id=yzkSU5zdwD. Survey Certification. J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022b. URL https://openreview.net/forum?id=_VjQlMeSB_J. M. Wortsman, P. J. Liu, L. Xiao, K. Everett, A. Alemi, B. Adlam, J. D. Co-Reyes, I. Gur, A. Kumar, R. Novak, et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. L. Yun, Y. Zhuang, Y. Fu, E. P. Xing, and H. Zhang. Toward inference-optimal mixture-of-expert large language models. arXiv preprint arXiv:2404.02852, 2024. B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. ST-MoE: designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 16 18 18 18 20 22 24 24 24"
        },
        {
            "title": "Appendices",
            "content": "A Preliminaries A.1 Notation and Terminology . . A.2 Mixture-of-Expert (MoE) Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experimental Setup Estimating Mixture-of-Expert (MoE) FLOPs Additional Analysis Interplay between parameters and FLOPs per example . . . D.1 . D.2 Effect of training budget and model size on optimal MoE sparsity . D.3 Effect of sparsity on downstream task performance . . . D.4 Comparing IsoFLOP Surface Analysis with Independent 2d IsoFLOPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Incorporating Sparsity into Scaling Laws"
        },
        {
            "title": "A Preliminaries",
            "content": "A.1 Notation and Terminology To aid readability, we provide list of key symbols used throughout this paper. Symbol Na L E α, β, γ, λ, δ, a, b, c, d, Coefficients in the parametric scaling law equation Description Total number of model parameters Active number of model parameters Sparsity level (ratio of non-active to total experts) Optimal sparsity level Pretraining Loss (Categorical Cross-Entropy) Optimal pretraining loss Total training compute budget (in FLOPs) Optimal total number of parameters Optimal active number of parameters Expansion factor (number of experts per MoE layer) Number of selected experts per token Granularity of experts (size relative to base MLP) Dataset size (number of training tokens) In this paper, we use the term \"compute\" in general sense to refer to computational cost. Unless otherwise specified, \"compute\" and \"FLOPs\" (Floating Point Operations) are used interchangeably to quantify this cost. A.2 Mixture-of-Expert (MoE) Transformers Mixture-of-Experts Transformers modify the standard transformer architecture by introducing in the MLP layer. In this design, the experts are MLP (Multi-Layer Perceptron) modules that follow the attention mechanism and are selectively activated for each token. gating mechanism determines which MLP experts are most relevant for each token, ensuring that only subset of experts (top-k) is active at any given time, while the rest remain inactive. Below, we provide the notations used throughout the paper for various terms related to training MoEs. In MoEs, we distinguish between total and active parameters, Total and Active Parameters: denoted by and Na, respectively. The total parameter count, , includes all parameters of the network, encompassing both the experts and the rest of the architecture. The active parameter count, Na, refers to the parameters associated with the active portion of the experts, along with the rest of the network that is always utilized. In MoEs, the gating mechanism assigns tokens to subset of experts Top-k Expert Selection: using top-k selection process, where denotes the number of experts activated for each token. The gate computes relevance score for each expert, and the top experts with the highest scores are selected and activated. This selective activation limits the computational overhead by ensuring that only fraction of the experts are used per token. Expansion Factor and Granularity: The expansion factor, typically denoted by E, represents the increase in model capacity due to the inclusion of multiple experts, measured as multiplicative factor relative to the base dense model. The granularity, G, determines the size of each expert relative to the size of the MLP module in the base dense model. The total number of experts in the model is given by G, where scales the capacity and controls the level of granularity. In general, sparsity is defined as the ratio of inactive to total parameters. However, Sparsity (S): in the context of MoEs, we focus on the sparsity of the MLP modules specifically. Therefore, we define the sparsity level as the ratio of inactive to total experts, given by: = number of non-active experts number of total experts . (7) This definition provides an interpretable measure of sparsity but cannot be directly used to calculate the active parameter count Na due to the contribution of other parameters in the model that remain unsparsified."
        },
        {
            "title": "B Experimental Setup",
            "content": "We train and evaluate auto-regressive sparse Mixture-of-Experts (MoE) language models of varying sizes and configurations on subsets of the RedPajamaV1 dataset Together Computer (2023). The key variables we explore in our experiments are total model parameters , training compute budget C, and the MoE sparsity S. Pre-training data. Our models are pre-trained on subsets of the RedPajamaV1 dataset5 Together Computer (2023), which attempts to replicate the LLaMA pre-training data recipe and comprises 1.2 trillion tokens from sources such as Common Crawl, C4, GitHub, and Wikipedia. In all our experiments, the effective dataset size is adjusted based on the training compute budget and the model size . We tokenize the data using the GPT-NeoX tokenizer Black et al. (2022), which has vocabulary size of 50, 432 tokens. Model and tokenizer. We use auto-regressive transformer-based MoE language models in order to study compute-parameter trade-offs by varying MoE sparsity. We use the Megablocks library Gale et al. (2023) to train dropless MoEs in which the routing mechanism ensures that all tokens are efficiently routed without being dropped due to routing capacity constraints. Optimizer and scheduler. We optimize our models using the scale-free Adam optimizer6 with variable learning rate, weight decay of 1 105, and fixed Adam-specific parameters β = (0.9, 0.95) and ε = 1 108. We use learning rate scheduler consisting of linear warmup phase followed by cosine decay. The warm-up phase increases the learning rate from 0 to the base learning rate over fraction of the total training steps (selected from {0.1, 0.05, 0.02}). After warm-up, the learning rate decays following cosine schedule for the remaining training steps. Fitting IsoFLOP surfaces. Recall that in Section 2, we fit isoFLOP surfaces to predict pretraining loss as polynomial function of model size and MoE sparsity for fixed training budget C. The polynomial function takes the form L(N, S) = α1(cid:88) i=1 ai ˆN + α2(cid:88) i=1 bi ˆSi + α3(cid:88) i=1 ci( ˆN ˆS)i + (8) where ˆN = log and ˆS = log(1 S)we find that applying log transformations improves the fit of the resulting IsoFLOP surface. Through grid search over the polynomial coefficients α1, α2, α3 {0, 1, 2, 3, 4}, we found that the best fit was obtained for α = β = γ = 2, i.e., quadratic polynomial over ˆN and ˆS. We evaluate the fitted IsoFLOP surfaces in Figure 1 by (a) re-running the fitting procedure = 100 times on randomly subsampled data and (b) evaluating the Pearson correlation between the true and predicted pretraining loss values on set of held-out data points. Hyperparameters. We fix subset of hyperparameters for which changing values in preliminary experiments (a) did not significantly improve pre-training loss, (b) the optimal value remained the same across several model configurations, or (c) in order to reduce the search space (i.e., limited compute resources). Specifically, we first opted to use z-router loss Zoph et al. (2022) and qk-normalization Wortsman et al. (2023) in order to stabilize training for large MoEs. Second, we fixed MoE router jitter noise to 0, as it did not improve performance. We also fixed our batch size to 1024 for all model sizes. 5GitHub repository: https://github.com/togethercomputer/RedPajama-Data 6Scale-free Adam: https://fabian-sp.github.io/posts/2024/02/decoupling/ 20 We swept over hyperparameters that, when adjusted, (a) significantly improved pre-training loss and (b) the optimal values varied across different model configurations. We increase the MoE sparsity by decreasing the number of active experts and/or increasing the number of total experts. We also varied the MoE granularity Ludziejewski et al. (2024), MoE load balancing regularizer, Adam learning rate, and linear warm-up steps (fraction) in order to improve pre-training loss. The table below summarizes our hyperparameter sweeps: Table 1: Hyperparameter configurations and search spaces Hyperparameter Configuration Search Space Sparsity Level Number of Total Experts Number of Active Experts Granularity Learning Rate Load Balancing Factor Warm-up Steps Batch Size Jitter Noise z-Loss z-Router Loss QK Norm Tuned Tuned Tuned Tuned Tuned Tuned Tuned Constant Constant Constant Constant Constant {0, 25, 50, 75, 90, 95, 98}% Adjusted depending on sparsity Adjusted depending on sparsity {1, 2} [0.003, 0.002, 0.001] {0.02, 0.05} {2, 5, 10}% 1024 0 0 0.001 Applied It is also noteworthy that, in this paper, we have prioritized training compute-optimal models, in contrast to many published results on large language models (LLMs), which often rely on overtrained models. As result, the performance of the models we use for the analysis in this paper is not directly comparable to those of other studies, where they overtrain smaller language models, to reduce the cost of inference relative to training. 21 Estimating Mixture-of-Expert (MoE) FLOPs Similar to prior work on scaling laws (e.g., Kaplan et al. (2020); Hoffmann et al. (2022); Ludziejewski et al. (2024)), we use theoretical FLOP estimates as proxies for training and inference costs of language models. In this section, we (a) outline our methodology for estimating FLOPs for MoEs and (b) show that the proposed estimator closely approximates empirical FLOPs of large-scale MoEs. Setup and notation. Consider an MoE model with nlayers MoE layers, each with an embedding dimension of dmodel. We denote the number of total experts and active experts in each MoE layer by Etotal and Eactive respectively. Following Ludziejewski et al. (2024), we let denote the MoE granularity, which defaults to 1 and controls the size of each expert relative to the size of feedforward layer in an equivalent dense transformer. In order to change sparsity in more granular manner, we treat the number of active experts as an independent variable that does not scale with granularity G. In our experiments, we use vocabulary size nvocab = 50, 432, context length nctx of 2048, and GLU modules (Gated Linear Units) (Shazeer et al., 2017) over feed-forward modules as the architecture of choice for MoE experts. We also set the (a) hidden dimension of each GLU expert dffn to 4 dmodel and (b) instantiate MoEs where the number of attention heads nheads times the dimensionality for each head dhead equals dmodel, i.e., nheadsdhead = dmodel. Estimating module-specific FLOPs. To estimate the FLOPs of given MoE model, we first individually estimate the FLOPs per token incurred by forward and backward pass through every module in MoEs. Then, we aggregate these estimates to obtain the final estimator for the FLOPs per token incurred by forward and backward pass through the model. Like in prior work on scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), we take two-step approach to estimate module-specific FLOPs. Given module, we first estimate the number of parameters in the module and then scale this with an appropriate constant corresponding to the number of add-multiply operations per parameter through forward and backward pass of the given module. We also omit non-leading terms such as non-linearities, biases, and layer normalization in our estimation. We estimate the FLOPs per token for attention modules, MoE routers, MoE experts, and the final un-embedding layer as follows: 1. Attention module. We estimate the FLOPs incurred via the QKV (and final) projections, attention logits, and attention values of all heads in multi-head attention module as follows. QKV (and final) projections. These projections involve 4 dmodelnheadsdheads = 4d model parameters. Following Kaplan et al. (2020), we use the multiplicative constant = 6 to account for the add-multiply operations per parameter in forward and backward pass through linear modules, resulting in FLOPs-per-token estimate of 4 d2 model. Attention logits. The FLOPs required to compute the attention logits for all nctx tokens ctxdmodel FLOPs, making the FLOP-per-token estimate equal to nctxdmodel. Attention values. The computation of attention values requires per-token weighted equals n2 sum over nctx dmodel-dimensional vectors, making the estimate nctxdmodel. 2. MoE module. Given an MoE layer, we estimate the FLOPs incurred by its router and all experts separately. Router. The MoE routing linearly maps dmodel-dimensional token embedding to Etotaldimensional logit vector, which is subsequently used to map the token to Eactive active experts. Following Ludziejewski et al. (2024), we use multiplicative constant = 14 that accounts for the add-multiply-route operations per router parameter. The resulting FLOP estimate equals dmodelEtotal Experts. Each MoE experts corresponds to GLU module (Shazeer et al., 2017) with dffn = 4 dmodel. Since there are Eactive active experts with granularity G, each involving 22 three linear projections, this results in FLOP estimate of 1/G 3 Eactive dmodeldffn = 12C/G Eactive d2 model. 3. Un-embedding layer. The un-embedding linear layer maps the final dmodel-dimensional embedding of token to nvocab-dimensional logits, making the FLOPs-per-token nvocabdmodel. Estimating MoE FLOPs. We can aggregate the module-level FLOP estimates described above to estimate the FLOPs per token required for single forward and backward pass through given MoE model as follows: nlayer (cid:0)4Cd2 model + 2Cdmodelnctx + 12C/GEactived2 model + RdmodelEtotal (cid:1) + Cnvocabdmodel When Etotal/dmodel is small, which is typically the case in practice, the FLOPs induced by MoE routing can be ignored as they contribute negligibly to the estimator. This allows us to simplify the estimator to: MoE FLOPs per token := nlayersd2 model (cid:16) 4 + 2nctx dmodel + 12Eactive + nvocab dmodelnlayers (cid:17) (9) For standard dense Evaluating 6NaD as FLOPs-per-token estimator in MoE Models transformers, the FLOPs are often estimated as 6N (Kaplan et al., 2020; Hoffmann et al., 2022). Given that is fixed and not adjusted dynamically, can serve as relative estimator of FLOPs per token for dense transformer models. To adapt the 6N estimator for MoE models, we replace with Na (the active number of parameters)the number of parameters used in every forward and backward pass. In Figure 7, we evaluate the accuracy of the 6NaD estimator by plotting the ratio between the MoE FLOPs estimator described in Equation 9 and 6NaD as function of model size and fixed context length = 2048. The results show that, across all sparsity levels, the ratio remains close to one, and the gap between the two estimators decreases as model size increases. Figure 7: Accuracy of 6NaD FLOPs Estimator for MoEs. Ratio of the MoE FLOPs estimator (Equation 9) to the 6NaD estimator as function of the total number of parameters, for fixed context length of = 2048, used in our experiments."
        },
        {
            "title": "D Additional Analysis",
            "content": "D.1 Interplay between parameters and FLOPs per example Recall that in Section 2, we showed that isoFLOP curves were predictive of pretraining loss for different parameter counts and sparsity levels. In this section, we show similar results with additional training compute budgets. 1. In Figure 8, we first show that IsoFLOP surfaces mapping model size and sparsity level to pre-training loss are predictive in similar way for all training compute budgets that we consider, ranging from 3e19 to 1e21 FLOPs. 2. In Figure 9, we analyze the fitted IsoFLOP surfaces (one for each training budget) and find that the (a) effect of model size on optimal MoE sparsity and (b) the effect of MoE sparsity on the optimal total and active parameters, and , is similar for all training budgets. D.2 Effect of training budget and model size on optimal MoE sparsity Recall that in Section 3, we demonstrated how the relationship between optimal total parameters , optimal active parameters a, and optimal pretraining loss predictably changes as function of sparsity and training budget C. In this section, we use the fitted isoFLOP surfaces to analyze how the optimal MoE sparsity changes as function of total parameters and training budget C, as shown in Figure 4. Our main findings are: Across all training budgets (ranging from 3e19 to 1e21 FLOPs), increasing the total parameters leads to an increase in the optimal sparsity level S. For fixed model size (i.e., total parameters ), increasing the training budget generally reduces the optimal sparsity level S. The relationship between model size and optimal is not linear. For smaller models (up to about 500 106 parameters), the optimal sparsity remains at 0 (i.e., dense) for most compute budgets. D.3 Effect of sparsity on downstream task performance In Section 4, we analyzed the relationship between upstream pre-training loss and downstream task performance across different MoE sparsity levels. We found that language understanding and world knowledge tasks generally showed strong correlation between upstream and downstream performance, while reading comprehension tasks seemed to favor denser models to some extent. In this section, we provide additional plots for broader range of tasks within each category to further support our findings. We consider the following tasks: Common Sense Reasoning: PIQA, CommonSenseQA, OpenBookQA, COPA Language Understanding: LAMBADA, HellaSwag, Winograd, Winogrande Reading Comprehension: SQuAD, CoQA, BoolQ World Knowledge: TruthfulQA, ARC-Easy, ARC-Challenge Figure 10 shows the relationship between upstream pre-training loss and downstream task performance for these additional tasks. Each row corresponds to task category and each subplot represents different task, with points colored according to MoE sparsity S. The x-axis represents the upstream pre-training loss, while the y-axis shows the downstream task performance metric (usually accuracy or error rate). These results supplement our main findings from Section 4: Figure 8: IsoFLOP surfaces over total parameters , MoE sparsity S, and pretraining loss for different compute budgets. The rows correspond to IsoFLOP surface fitted using models trained with budget of 3e19, 6e19, 1e20, 3e20, and 1e21. The subplots on the left visualize IsoFLOP surfaces mapping total parameters and sparsity level to pretraining loss L. The subplots on the right correlate the ground-truth pretraining loss with the estimated pretraining loss on held-out data. Taken together, these results show that isoFLOP surfaces are accurate proxies for understanding how model size and MoE sparsity jointly impact pretraining loss. 25 Figure 9: Optimal MoE configurations predictably change with training compute budget. Each row corresponds to an analysis of how optimal MoE sparsity S, total parameters , and active parameters change for given training budget. The subplots on the left show that (a) increasing the training budget increases the model size (denoted with black dots) with the minimum pretraining loss and (b) for models smaller than threshold (which increases with training budget), dense models (i.e., 0% sparsity) fare better than sparse MoEs. The subplots in the second and third panel show that (a) increasing MoE sparsity increases the optimal total parameters and decreases the optimal active parameters . In both cases, for fixed sparsity level, increasing the budget shifts increases the optimal total and active parameters. We observe consistent trends across tasks within each category, with language understanding and world knowledge tasks showing strong correlations between upstream and downstream performance regardless of sparsity. Reading comprehension tasks continue to show slight advantage for denser models, while common sense reasoning tasks (which can be considered part of the symbolic problem-solving category) show more varied relationships between upstream and downstream performance. D.4 Comparing IsoFLOP Surface Analysis with Independent 2d IsoFLOPs Recall that in Section 2, we used IsoFLOP surfaces that predict pre-training loss across varying parameter counts and sparsity levels to understand how optimal sparsity and optimal model size depend on each other. In this section, we evaluate whether these findings remain consistent when we do not rely on fitted IsoFLOP surfaces. Specifically, similar to Approach II in Hoffmann et al. (2022), we directly fit univariate quadratic functions that map model size to pre-training loss L, independently for each sparsity level and training compute budget. We then assess these univariate fits to determine whether our findings in Section 2 hold. In Figure 12, each row shows how the optimal total and active parameters change as function of MoE sparsity for fixed training budgets. As in our findings from Section 2 (Figure 2), increasing sparsity increases the optimal total parameters while decreasing the optimal active parameters. Moreover, larger compute budgets still result in higher optimal total and active parameters, regardless of the sparsity level. Furthermore, in Figure 11, we observe that across all training compute budgets, increasing sparsity reduces the optimal pre-training loss. This is consistent with the trends identified in Section 3 (Figure 3), thereby validating our earlier results. 26 Figure 10: Downstream task performance vs. upstream pre-training loss. Each subplot shows the relationship between upstream pre-training loss (x-axis) and downstream task performance (y-axis) for specific task. Similar to our results in Section 4, we find that the MoE sparsity level does not change the relationship between upstream pre-training loss and downstream task performance. 27 Figure 11: Effect of MoE sparsity on pretraining loss across different training compute budgets. As sparsity increases, the validation loss decreases for all compute budgets, with larger budgets (darker lines) achieving lower losses at each sparsity level. This trend is consistent with the findings from Section 3, demonstrating that increasing sparsity reduces the optimal pretraining loss across all compute budgets. Figure 12: Effect of MoE sparsity on optimal total and active parameters across different training compute budgets. Each row shows the change in total and active parameters as function of sparsity level for fixed training budgets. Increasing sparsity leads to an increase in the optimal total parameters while reducing the optimal active parameters, consistent with our findings in Section 2 (Figure 2). Larger training compute budgets result in higher optimal (total and active) parameters across all sparsity levels."
        },
        {
            "title": "E Incorporating Sparsity into Scaling Laws",
            "content": "Table 2 shows the parameters used to initialize L-BFGS used to fit the proposed parametric scaling law given in Equation 6. Table 3 shows the estimated parameters for the parameteric model. We use held out dataset that consists of data points for models with sparsity value = 0.98 to validate the performance of the estimated model coefficients. The mean squared error and the Huber loss error on the dataset used to fit the model is 0.00056 and 0.0036 respectively and 0.0058 and 0.0011 respectively on the out-of-sample validation set. Table 2: Initial values used to estimate coefficients in Equation 6. Coefficients Initial Values log(a), log(b), log(c), log(d) α, β, γ λ, δ log(e) [0, 10, 20] [0, 0.25, 0.5, 0.75, 1, 1.25] [1, 0.5, 0, 0.5, 1] 1. Table 3: Estimated values for coefficients in Equation 6. Coefficient Estimate α β λ δ γ d 0.5962 0.3954 -0.1666 0.1603 0.1595 16612.50 5455.67 0.4598 17.26 0."
        }
    ],
    "affiliations": [
        "Apple",
        "MIT"
    ]
}