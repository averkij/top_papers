{
    "paper_title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model",
    "authors": [
        "Jingcheng Hu",
        "Yinmin Zhang",
        "Qi Han",
        "Daxin Jiang",
        "Xiangyu Zhang",
        "Heung-Yeung Shum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes."
        },
        {
            "title": "Start",
            "content": "2025-4-1 Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model Jingcheng Hu1,2, Yinmin Zhang1, Qi Han1, Daxin Jiang1, Xiangyu Zhang1, Heung-Yeung Shum2 1StepFun, 2Tsinghua University GitHub: https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, HuggingFace: https://huggingface.co/Open-Reasoner-Zero."
        },
        {
            "title": "Abstract",
            "content": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that minimalist approach, vanilla PPO with GAE (洧랝 = 1, 洧 = 1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiencyrequiring only tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes. 5 2 0 2 1 3 ] . [ 1 0 9 2 4 2 . 3 0 5 2 : r Figure 1: Evaluation performance of Open-Reasoner-Zero-{7B, 32B} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only tenth of the training steps. *Work done during internship at StepFun."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Scale-up Reinforcement Learning from Base Model 2.1 Basic Settings . 2.1.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 Reward Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.3 RL Algorithm . 2.2 Key Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experiments 3.1 Training Details and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Training Results . 3.3 Ablation Study . . 3.4 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Conclusion and Discussions 5 Acknowledgements More Ablation Studies More Evaluation Results 3 4 4 4 6 7 8 8 9 12 13 14 17 19 1. Introduction Large-scale reinforcement learning (RL) training of language models on reasoning tasks has emerged as promising paradigm for mastering complex problem-solving skills. Recent breakthroughs, particularly OpenAIs o1 [1] and DeepSeeks R1-Zero [2], have demonstrated remarkable training time scaling phenomenon: as the training computation scales up, both the models benchmark performance and response length consistently and steadily increase without any sign of saturation. Inspired by these advancements, we aim to explore this new scaling phenomenon by conducting large-scale RL training, even applying it directly to base models, an approach we refer to as Reasoner-Zero training. In this work, we introduce Open-Reasoner-Zero (ORZ), the first open-source implementation of large-scale reasoning-oriented RL training on large language models (LLMs) with our best practices, designed to be robust, scalable and simple-to-follow. Under Reasoner-Zero paradigm, LLMs are trained to master diverse reasoning skills under verifiable rewards, spanning arithmetic, logic, coding and common-sense reasoning (e.g., scientific problems, numerical reasoning, natural language understanding and even creative writing). While DeepSeeks R1-Zero outlined their training pipeline briefly, we provide comprehensive study of our training strategy, with in-depth insights into overcoming common challenges such as training instability, stagnating response length, benchmark performance plateaus, and reward design. Our goal is to democratize advanced RL training techniques accessible to the broader research community. Our proposed Open-Reasoner-Zero outperforms the DeepSeek-R1-Zero, with the same Qwen-32B base, on AIME24, MATH500, and GPQA Diamond, yet only 1/10 the training steps. Through extensive ablation studies, we summarize some key findings and lessons learned from our exploration. Specifically, vanilla PPO using GAE (洧랝 = 1 and 洧 = 1) and without any KL-related regularization, combined with straightforward rule-based reward, is sufficient to achieve steady scalability in both response length and benchmark performances across varying model sizes and training data scales. ORZs stable scaling resonates well with the bitter lesson [3]: the most significant performance improvements stem from the scale of training data, model size, and training iterations, rather than the complexity of design choices. The most critical thing is how to design simple and effective RL algorithm to scale up the training process. We release all of our training resources, including code, parameters, data, and model weights. We are excited to share this breakthrough of scale-up RL, along with the lessons we learned with the research community. This sharing empowers researchers to go beyond merely using the prepared results (e.g., APIs or model weights) and instead become active participants in this transformative moment in AI development themselves. Additionally, we provide recommended practices for 0.5B and 1.5B model variants to assist wider range of researchers. Our primary contributions are as follow: 1. We provide fully open-source implementation of large-scale RL training directly on base LLM, strategy we refer to as Open-Reasoner-Zero. 2. We share empirical insights and lessons we learned from frustrating failures and exciting breakthroughs during our scaling-up journey. 3. We release comprehensive training resources including code, parameter settings, data, and model weights to the research community. Our findings indicate that model performance continues to improve with more training data, showing no signs of saturation. In light of this, we are releasing our training datasets and calling on the community to contribute additional data to collectively advance the frontiers of LLMs. 3 Figure 2: Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - {0.5B, 1.5B, 7B, 32B}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe. 2. Scale-up Reinforcement Learning from Base Model In this section, we describe the strategy and critical components for scale-up reasoning-oriented reinforcement learning (RL) directly from base model. First, we introduce the basic yet critical settings for our scale-up RL training from base model, including data curation, reward function, and detailed settings of the Proximal Policy Optimization (PPO) [4] algorithm. We then discuss key insights derived from our comprehensive ablation experiments that enable successful scale-up RL training. 2.1. Basic Settings We conduct our experiments utilizing the Qwen2.5-{7B, 32B} as our base model [5], and directly starting the large-scale RL training without any fine-tuning (e.g., distillation or SFT) [6, 7]. Building upon the Qwen2.5-{7B, 32B} base model, we scale up the standard PPO algorithm [4] for reasoning-oriented RL training, with careful consideration of scalability and robustness. Our training data comprises tens of thousands of carefully curated question and answer pairs consisting of Math and Reasoning tasks, designed specifically for enhancing models capability in diverse and complex problem-solving scenarios. Inspired by DeepSeek-R1 [2], we design our prompt template to elicit the model to utilize inference computation, gradually mastering the reasoning ability for complex tasks, as shown in Table 1. Furthermore, we develop an efficient and easy-to-use large-scale RL training framework based on OpenRLHF [8], by introducing more flexible trainer, enabling GPU collocation generation, and training with offload and backload support. In the following sections, we provide detailed settings for our scale-up RL training from base model. 2.1.1. Dataset In this section, we introduce our carefully curated dataset, detailing its source description, cleaning process, and scaling insights for future directions. High-quality training data are crucial for scalable Reasoner-Zero training. We identify three key aspects in our data recipe: 4 conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>. And your final answer will be extracted automatically by the boxed{} tag. {{prompt}} Assistant: <think> Table 1: Template for Open-Reasoner-Zero. prompt will be replaced with the specific reasoning question during training. quantity, diversity, and quality. Following these key aspects, we curate our dataset through comprehensive collection and cleaning process: We collect public data from various sources, including AIME (up to 2023), MATH [9], Numina-Math collection [10], Tulu3 MATH [11], OpenR1-Math-220k [12] and other opensource datasets. Based on source and problem difficulty, we retrieve AMC, AIME, Math, Olympiads, and AoPS forum components as our difficult level prompts to ensure appropriate difficulty levels. We synthesize additional reasoning tasks using programmatic approaches to augment the dataset. We exclude problems that are challenging to evaluate with our rule-based reward function, such as multiple-choice and proof-oriented problems, ensuring accurate and consistent reward computation during training. We implement model-based filtering strategy based on heuristic evaluation of problem difficulty. Specifically, we use LLM to assess the pass rate of each problem, removing samples with either too high or zero pass rates. The final curated data consists of approximately 129k samples spanning mathematics and reasoning domains. This collection is specifically designed to enhance models capabilities in complex problem-solving tasks, carefully balancing quantity, diversity, and quality. Additionally, we leverage the training process of our 32B model itself to identify challenging and high-quality prompts. We initially train the 32B model for 1100 steps with data sampled from the complete 129k-sample dataset. Subsequently, we pinpoint particularly difficult prompts, defined as those where the model achieves fewer than 4 correct answers out of total of 64 attempts, resulting in approximately 13k challenging prompts. These identified prompts are then selectively used in final training stage of 100 additional steps, aiming to address the models weakest areas and substantially enhance its performance on the most difficult reasoning tasks. In the future, we aim to expand our dataset by collaborating with the research community to encourage researchers to voluntarily contribute additional data across various domains, from advanced mathematics and reasoning tasks to competitive programming and software engineering tasks. More detailed discussions on dataset collection and processing are provided in the appendix. 5 from math_verify import verify, parse verify(parse(ground_truth), parse(model_output)) Figure 3: The code snippet for verifying the mathematical correctness of generated answers using the Math-Verify library. 2.1.2. Reward Function Unlike DeepSeek-R1-Zero [2], our scale-up RL training employs simple minimalist rule-based reward function that solely checks answer correctness, without any additional format rewards. Specifically, this reward function is designed to extract the content between <answer> and </answer> tags during training and compare it with the reference answer. To maintain clarity and simplicity in scale-up RL, we implement binary reward scheme - awarding reward of 1 for exact matches with the reference answer, and 0 for all other cases. To ensure rigorous and consitent assessment in evaluation, we adopt the widely-used Math-Verify1 library and its usage as shown in Figure 3. Surprisingly, we found that with our designed prompt, even unaligned base model can yield well-formatted responses in high probability. Then during early training stages, the base model can quickly learn and reinforce the correct format for reasoning and answering incentivized by our simple rule-based reward function alone, as shown in Figure 4. More importantly, our preliminary experiments revealed that complicated reward functions were not only unnecessary, but could leave potential room for reward hacking. 2.1.3. RL Algorithm We adopt the Proximal Policy Optimization (PPO) algorithm [4] as the RL algorithm for our scale-up training, unlike GRPO used in DeepSeek-R1-Zero. Specifically, for each question 洧 (i.e., prompt), the model generates group of responses {洧녶1, 洧녶2, ..., 洧녶洧녵} and receives corresponding rewards {洧1, 洧2, ..., 洧洧녵} based on the rule-based reward function, where 洧녵 represents the number of sampled trajectories (i.e., rollout size per prompt). For each response 洧녶洧녰 at time step 洧노 (i.e., token 洧노), let 洧멇롐 denote the state at time 洧노 which comprises the question and all previously generated tokens, and 洧녩洧노 denote the token generated at that step. We compute the advantage estimation 틙洧냢洧노 for each token using Generalized Advantage Estimation (GAE) [13]. Generally, GAE provides trade-off between bias and variance in the advantage estimation by combining multiple n-step advantage estimates through an exponentially weighted average controlled by the parameter 洧랝. The advantage is computed as 틙洧냢洧노 = 洧洧노 + (洧쮫롚)洧洧노+1 + ... + (洧쮫롚)洧녢 洧노1洧洧녢 1, where 洧洧노 = 洧洧노 + 洧쮫롐 (洧멇롐+1) 洧녤 (洧멇롐) is the TD (temporal difference) residual and 洧 is the discount factor that determines how much future rewards are valued relative to immediate rewards. The PPO algorithm updates the policy model parameters 洧랚 to maximize the expected reward and value model parameters 洧랯 to minimize the value loss by optimizing the following objective function: JPPO(洧랚) = E洧노,洧멇롐,洧녩洧노洧랢洧랚 old [min( 틙洧냢洧노, clip( 洧랢洧랚(洧녩洧노 洧멇롐) (洧녩洧노 洧멇롐) 洧랢洧랚 old , 1 洧랬, 1 + 洧랬) 틙洧냢洧노)], old 洧랢洧랚(洧녩洧노 洧멇롐) 洧랢洧랚 (洧녩洧노 洧멇롐) 1 2 E洧노,洧멇롐,洧녩洧노洧랢洧랚 Jvalue(洧랯) = [(洧녤洧랯(洧멇롐) 洧녠洧노)2], old (1) (2) where 洧랬 is the clipping parameter, 洧랢洧랚 is the current policy, 洧랢洧랚 update, 洧녤洧랯 is the value function, and 洧녠洧노 = (cid:205)洧녢 洧노 洧녲=0 old is the old policy before the 洧쮫롐떯롐洧노+洧녲 is the discounted return. We instantiate the 1https://github.com/huggingface/Math-Verify Figure 4: Percentage of responses following the reasoning format. Results demonstrate rapid adoption of structured reasoning patterns even by the base model using only simple rule-based reward function. Our findings suggest that complicated reward functions are unnecessary for training Reasoner-Zero models. PPO algorithm with carefully tuned hyperparameters: GAE parameter 洧랝 = 1.0, discount factor 洧 = 1.0, and clipping parameter 洧랬 = 0.2. 2.2. Key Findings In this study, we explore best practices for reasoning-oriented RL training with an emphasis on stability and scalability. We conduct extensive experiments across the design space of Reasoner-Zero training. Here are the key findings from our experiments: RL Algorithm Key Implementations: Our empirical studies demonstrate that vanilla PPO provides remarkably stable and robust training process across different model scales and training duration without requiring additional modifications. Through extensive experiments, we identified that the GAE parameters play critical role in PPO for reasoning tasks. Specifically, setting 洧랝 = 1.0 and 洧 = 1.0, while typically considered suboptimal in traditional RL scenarios, achieves the ideal balance for scale-up RL training. Minimal Reward Function Design: We show that simple rule-based reward function is not only sufficient but optimal, as minimal design leaves no room for potential reward hacking. Notably, even unaligned base models quickly adpots to desired format, suggesting this is straightforward task without requiring complex reward engineering. Loss Function: We achieve stable training without relying on any KL-based regularization techniques (e.g., KL shaped rewards and loss), different from the de facto RLHF community [14] and Reasoner model [15, 2]. This also offers promising potential for further large-scaling RL. Scale up Training Data: We identify that scaling up data quantity and diversity is crucial for Reasoner-Zero training. While training on limited academic datasets like MATH leads to quick performance plateaus, our curated large-scale diverse dataset enables continuous scaling without signs of saturation on both training and test sets. 7 Figure 5: Comparison of training and evaluation reward and average response length for the Open-Reasoner-Zero 7B model. All of benchmarks experience sudden increase in reward and response length at certain point, phenomenon like emergent behavior. 3. Experiments In this section, we present comprehensive experimental results and analysis of our OpenReasoner-Zero models. We begin by the training setup and hyperparameters, followed by an in-depth analysis of traning results, and ablation studies. Finally, we discuss the evaluation results and provide detailed analysis of the training process. 3.1. Training Details and Hyperparameters We initialize both our policy and critic networks with Qwen-2.5 base models (7B and 32B variants), where value head is random initialized from ( 5) with no bias term. The policy and critic do not share weights during training. For both policy and critic networks, we employ AdamW optimizer with 洧띻 = [0.9, 0.95] without weight decay. The learning rates are set to 1 106 and 5 106 for the policy and critic networks, respectively. The learning rate scheduler are both constant learning rate with linear warm-up of 50 optimizer steps. We employ sample packing during training. 5, Each generation step contains 128 unique prompts sampled from the dataset, and generating 64 responses per prompt with temperature and top-p both set to 1.0. To maintain training stability, we implement strict on-policy optimization for the policy network, where each generation corresponds to exactly one optimization step. The critic network, being less sensitive to off-policy updates, processes the experiences in 12 mini-batches, effectively performing 12 optimization steps per iteration. We apply batch level advantage normalization in the training. For the 32B variant, we introduce an additional \"annealing\" training stage inspired by analogous practices in large language model pre-training [16]. Specifically, we utilize the mined challenging prompts detailed in Section 2.1.1 and apply linear decay schedule to the learning rate, reducing it to 3 107 over 100 training steps. This targeted training phase is explicitly designed to enhance the models capability on more complex reasoning tasks. Notably, our training process operates stably without any KL-related regularization terms or entropy bonuses, demonstrating that vanilla PPO can achieve stable training without these commonly used stabilization techniques. To comprehensively evaluate our models reasoning capabilities, we conduct experiments on diverse benchmarks spanning mathematical reasoning and general problem solving. These include AIME2024, AIME2025 [17], MATH500 [9] and GPQA DIAMOND [18] datasets. For each benchmark, we report the average accuracy across 16 samples per question as our primary evaluation metric. Moreover, we also assess the models general capabilities through evaluations on MMLU [19] and MMLU_PRO [20] benchmarks to provide comprehensive understanding of their performance across diverse tasks. 3.2. Training Results In this section, we present the key findings from our experimental training results. We evaluate the training process from multiple perspectives, including reward in training sets, average response lengths, and generation quality metrics. These metrics provide holistic view of model performance and learning dynamics. Training Curves. Figure 2 shows the training reward and average response length curves of our experiments for both Open-Reasoner-Zero 7B and 32B, while Figure 5 shows the reward/accuracy and average response length curves of our experienments for Open-Reasoner-Zero 7B on training and evaluation sets. The training reward curve and response length curve represent the average reward of the generated responses and the average length of the generated responses at each generation step, respectively. We observe consistent improvements in these metrics throughout training across both models and all benchmarks, with notable observations: OpenReasoner-Zero exhibits an intriguing \"step moment\" phenomenon, where response metrics suddenly increase during training, revealing emergent reasoning capabilities. Quality Analysis. Here we provide some qualitative analysis of the generated responses from our Open-Reasoner-Zero models. To analyze the models reflection capabilities and observe the Aha moment like DeepSeek-R1-Zero, we identify five representative reflection patterns (\"wait,\", \"recheck\", \"retry\", \"alternatively,\", and \"however,\"), following methodology similar to [21]. We count the number of responses containing any of these patterns as reflection responses, and identify the average correct reflection length (the length of responses containing reflection patterns that achieve correct answers). As shown in Figure 6, the average correct reflection length consistently exceeds the average response length throughout the training process, indicating that responses containing reflection patterns utilize more \"thinking time\" to achieve correct answers, similar to the test-time scale described in OpenAI o1. particularly noteworthy phenomenon emerges around step 680, where we observe simultaneous acceleration in three metrics: the reward, average correct reflection length, and average response length. Through manual inspection of model outputs before and after step 680, we observed qualitatively more pronounced reflection patterns in the latter responses. This emergent behavior warrants further 9 Figure 6: Reflection patterns in generated responses. The Average Correct Reflection Length consistently exceeds the Average Response Length throughout the training process. particularly noteworthy phenomenon emerges around step 680, where we observe simultaneous acceleration in three metrics: Reward in training set, Average Correct Reflection Length, and Average Response Length. investigation, and we are currently conducting detailed analyses to understand the underlying mechanisms of this phenomenon. For comprehensive quantitative and qualitative analyses, please refer to our detailed documentation available at Notion2. 3.3. Ablation Study We present ablation studies over key training strategies and hyperparameters that enable successful scaling of RL training directly from base model. More comprehensive ablation studies are available in appendix. GAE Analysis. We compare different GAE 洧랝 combinations. From the experimental results, we find that GAE 洧랝=1.0 performs best in terms of training stability and final performance. Specifically, in the training reward, the GAE 洧랝=1.0 curve rises quickly in the early stage and remains stable, finally converging to about 0.8; while the GAE 洧랝=0.95 curve rises slowly and fluctuates. In the Response Length, the GAE Lambda=1.0 curve maintains reasonable level during the training process; while the GAE 洧랝=0.95 curve shows an unstable trend, leading to PPO learning instability. These results indicate that GAE 洧랝=1.0 can better balance the training stability and generation quality. Moreover, discount factor (洧) set to 1.0 also has significant impact on the scale-up RL training. Less than 1.0 will result in penalty for long-term reward, leading to decrease in response length decrease and struggling to improve the final performance. KL Constrains Analysis. We evaluate different combinations of KL Loss and KL Penalty for the Open-Reasoner-Zero 7B model, analyzing their impact on evaluation metrics and response 2Notion: Comprehensive quantitative and qualitative analyses. 10 Figure 7: Comparison of different GAE 洧랝 values. GAE 洧랝 = 1.0 shows better stability and performance compared to 洧랝 = 0.95 for both training reward and response length. Figure 8: Comparisons to applying KL-related regularizations. Notably, training without KL constraints demonstrates superior average benchmark performance and length scaling property, compared to models trained with KL Loss and KL Penalty. Performance is evaluated on MATH500, AIME2024, and GPQA DIAMOND benchmarks using pass@1 metric. length of the training set. This analysis is particularly important since reward shaping can introduce additional effects on training rewards. Our experimental results demonstrate that removing both KL Loss and KL Penalty yields optimal training stability and final performance. Both KL Loss and KL Penalty mechanisms not only slow down the training process but also consume computational resources that could be better utilized for reward optimization. Furthermore, eliminating these components reduces hyperparameter tuning burden and implementation complexity, which is crucial for scaling up RL training effectively. Data Scale. We compare different data scales for training, ranging from 7.5k to 30k samples. As shown in Figure 9, larger data scales consistently lead to better performance in both training reward and response length for both training and evaluation sets. This result suggests that data scale plays crucial role in training performance, and increasing the training data scale can effectively improve the models reasoning capabilities. More comprehensive ablation studies including data quantity, quality, and diversity are available in the appendix. 11 Figure 9: Data scale ablation study. Training data from math train 7.5k to Open-ReasonerZero 57k, we observe consistent increase in both training reward and response length for training and evaluation set, indicating that data scale plays crucial role in training performance. Performance is evaluated on MATH500 benchmark using pass@1 metric. Model AIME 2024 AIME 2025 MATH500 GPQA Diamond QwQ-32B-preview DeepSeek-R1-Zero-Qwen-32B Open-Reasoner-Zero-32B 50.0 47.0 48.1 33.5 - 36. 90.6 91.6 92.2 54.5 55.0 55.5 Table 2: Comparison of Open-Reasoner-Zero-32B with DeepSeek-R1-Zero-Qwen-32B and QwQ32B-Preview on reasoning-related benchmarks. DeepSeek-R1-Zero-Qwen-32B results are from [2], and no AIME2025 results are provided. Reasoner-Zero Training on Smaller Models To demonstrate the robustness and versatility of our Open-Reasoner-Zero training methodology, we extend the same training pipeline to smallerscale models, specifically Qwen-2.5-0.5B and Qwen-2.5-1.5B. The evaluation results, presented in Figure 10, clearly indicate that our minimalist RL approach consistently improves reasoning capabilities even at substantially smaller model sizes. Remarkably, meaningful performance gains are observable even at the scale as small as 0.5B parameters. By releasing these smallerscale baseline models, we aim to facilitate further research, experimentation, and accessibility within the broader research community. 3.4. Evaluation Results In this section, we list our main experimental results. In our 32B experiments, Open-Reasoner-Zero demonstrates significant improvements in both training efficiency and model performance, as shown in Figure 1. The model achieves superior response length and accuracy across all benchmarks, notably outperforming DeepSeekR1-Zero-Qwen2.5-32B on AIME2024, MATH500, and GPQA Diamond benchmark, while only requiring an order of magnitude fewer training steps. We further illustrate the training dynamics of Open-Reasoner-Zero models across various sizes in Figure 2. Training Reward and Response Length demonstrate consistent and steady growth across all scales, highlighting the scalability of our minimalist reinforcement learning 12 Model MMLU MMLU_PRO Qwen2.5-32B-Base Qwen2.5-32B-Instruct Open-Reasoner-Zero-32B 83.3 83.2 84.9 55.1 69.2 74.4 Table 3: Generalization performance of Open-Reasoner-Zero models on MMLU and MMLU_PRO benchmarks. Through solely scaling up RL training on reasoning-oriented tasks, Open-Reasoner-Zero achieves superior performance on both benchmarks, surpassing Qwen2.5Instruct without any additional instruction tuning. This demonstrates the remarkable effectiveness of our training pipeline in enhancing model generalization capabilities. approach. Interestingly, the Response Length curve of the ORZ-32B model exhibits noticeable fluctuations, yet these fluctuations do not negatively impact training stability or the continuous growth of reward. This phenomenon indicates the robustness of our method against temporary variations in generated sequence lengths and motivates further investigation into understanding and leveraging this behavior in future work. To gain deeper insights into how model reasoning abilities develop during training, we analyze the detailed training dynamics of Open-Reasoner-Zero-7B across different benchmarks, as shown in Figure 5. The accuracy generally shows steady increase during training, while the response length exhibits more dramatic growth patterns. Notably, we observe an interesting emergent phenomenon during evaluation where both the reward and response length exhibit sudden, step-function-like increases at certain points, which we refer to as \"step moments\". This suggests that the models progressively master more detailed and comprehensive reasoning capabilities as training advances. This pattern is particularly pronounced in GPQA Diamond and AIME2024, where response lengths increase substantially in later training steps. Fianlly, we present the generalization capabilities of our models on comprehensive benchmarks like MMLU and MMLU_PRO. As shown in Table 3, Open-Reasoner-Zero-32B models demonstrate strong generalization capabilities, significantly outperforming Qwen2.5-Instruct32B on MMLU, MMLU_PRO through pure scale-up RL training on reasoning-oriented tasks, without any additional instruction tuning. 4. Conclusion and Discussions In this work, we present Open-Reasoner-Zero (ORZ), the first open-source implementation of large-scale reasoning-oriented RL training, focusing on scalability, simplicity, and accessibility. Through extensive experiments, our best practice demonstrates that vanilla PPO with GAE (洧랝 = 1, 洧 = 1) and straightforward rule-based reward function, without any KL regularization, is sufficient to scale up in both response length and benchmark performance on reasoning tasks with surprising generalization capabilities, achieving competitive results compared to DeepSeek-R1-Zero pipeline. We provide comprehensive analysis of the key components and settings required for successful large-scale RL training, along with critical insights into scaling up PPO. By releasing our complete training resources, we aim to enable broader participation in this pivotal moment of AI development. We believe we are at an early stage of this new scaling trend, and we are excited to share our findings and experiences with the community. Recall bitter lesson from the past: the only thing that matters in the long run is what scales up effectively with increased computation and data. This fundamental insight continues to Figure 10: Evaluation performance of Open-Reasoner-Zero-{0.5B, 1.5B}. We report the average accuracy on the benchmark dataset for each question with 16 responses. guide our research direction. In the future, we plan to further explore the following directions for continuously scaling up reasoning-oriented RL: Data Scaling: We will investigate how to effectively scale up by increasing the quantity, quality and diversity of training data. By open sourcing our own training dataset, we hope to encourage the research community to contribute and share more training data. Model Scaling: We will explore how to scale up model architectures to improve reasoning abilities. We will investigate how multimodal models can enable richer reasoning across different modalities, and how extended sequence lengths can allow for more complex multi-step reasoning. Test Time Scaling: We will explore how to scale up test time computation. We will investigate how multi-turn interactions can enhance contextual reasoning abilities, how value model can assess reasoning trajectories, and how multi-agent scenarios can lead to more sophisticated reasoning strategies. Scenario Scaling: We will explore how to scale up the complexity of reasoning for general scenarios. Our focus will be on generalizing reasoning capabilities to increasingly diverse tasks spanning creative writing, scientific discovery, and social interaction domains. 5. Acknowledgements This work was supported by computing resources and infrastructure provided by StepFun. We are grateful for our colleagues from StepFun and Tsinghua University for their valuable feedback and contributions."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-r eason-with-llms/, 2025. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. [4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [5] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [6] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.n otion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-S caling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. [7] Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, and Kai Chen. Exploring the limit of outcome reward for learning mathematical reasoning, 2025. [8] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. OpenarXiv preprint rlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv:2405.11143, 2024. [9] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, 2021. [10] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggin gface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ai mo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [11] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. [12] Loubna Ben Allal, Lewis Tunstall, Anton Lozhkov, Elie Bakouch, Guilherme Penedo, and Gabriel Mart칤n Bl치zquez Hynek Kydlicek. Open r1: Evaluating llms on uncontaminated math competitions, February 2025. 15 [13] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [14] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [15] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms. 2025. [16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Mislav Balunovic, Jasper Dekoninck, and Martin Vechev Ivo Petrov, Nikola Jovanovic. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. [18] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations. [20] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems, NeurIPS 2024, 2024. [21] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [22] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [23] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. 16 A. More Ablation Studies In this section, we present additional ablation studies conducted during our exploration of scaling up RL training. Notably, our ablation experiments were conducted during our efforts to scale up RL training, with some experiments employing different basic training strategies to explore various aspects of the training process. More Ablations over Data Curation. Based on our analysis of data quality issues, we conduct comprehensive ablation studies to evaluate how different data curation strategies affect model training stability and performance. Motivated by OpenR1s finding [12] that SFT performance degradation on Chinese subsets was due to simpler question patterns, we experiment with two data curation approaches: using English-only data versus using both English and Chinese data. Our results demonstrate that the English-only dataset yields superior training stability and final model performance. While most of our experiments utilize the full dataset including Chinese content, we make the final Open-Reasoner-Zero 57k dataset publicly available as it provides broader applicability across diverse tasks. Figure 11: Data Curation Ablation Study. CN represents Chinese data and EN represents English data. Our results demonstrate that the English-only dataset yields superior training stability and final model performance. Figure 12: Comparison of different Prompt, Rollout, Batch Size combinations. U.S. represents Update steps of model parameters in each generation steps. On policy update setting performs better than off policy counterpart on both training reward and response length. 17 Sampling Strategy. We compare different sampling strategies, including temperature T=0.6, 1.0, 1.2 and T=0.6 topp=0.95, with different model initialization, training dataset and training hyperparameters compared to our main settings. Specifically, we use Qwen2.5-Math-7B[22] as initialization here and use MATH train set as training data. As for the training hyperparameters, here we adopt 1024 unique prompts and 8 responses for each prompt in each generation. We process the experiences into at most 8 mini-batches for both policy and critic training. From the experimental results, we find that most basic sampling strategy works well compared to changing temperature or topp. Considering the scalability of training recipe, we finally opt for the most basic sampling strategy that and topp both equal to 1.0. Figure 13: Comparison of different KL Loss, KL Penalty, and GAE 洧랝 values. More Ablations over KL Loss & KL Penalty & GAE 洧랝 Analysis. We analyze the GAE Lambda and KL Loss for the LLaMA3.1-Instruct-SFT model [16]. This model is trained on STILL-2 data [23]. From the experimental results, we find that the combination of GAE 洧랝=1.0 and no KL Loss performs best in terms of training stability and final performance. As shown in the figure, this configuration shows the most stable performance in both training reward and response length. Additionally, our early experiments also found that introducing KL Penalty (similar to reward shaping in RLHF) significantly affected the reasoning ability of the model. Based on these findings and for scalability, we finally chose the training strategy of not using KL constrains. Figure 14: Comparison of different sampling strategies. represents temperature and topp represents top-p sampling. 18 B. More Evaluation Results In this section, we provide detailed results from evaluating Open-Reasoner-Zero models of varying parameter counts (0.5B, 1.5B, 7B, and 32B) across multiple reasoning-oriented benchmarks. Specifically, we report performance on AIME 2024, AIME 2025, MATH500, and GPQA Diamond. The results (see Table 4) clearly demonstrate consistent improvements in reasoning ability with increased model size, highlighting the strong scaling properties of our minimalist reinforcement learning setup. We release these comprehensive evaluation results as reference to facilitate further research and reproducibility. Model AIME 2024 AIME 2025 MATH500 GPQA Diamond Open-Reasoner-Zero-0.5B Open-Reasoner-Zero-1.5B Open-Reasoner-Zero-7B Open-Reasoner-Zero-32B 1.0 3.5 17.9 48.1 0.2 1.0 15.6 36.0 31.0 58.0 81.4 92. 12.1 16.8 36.6 55.5 Table 4: Detailed benchmark performance across Open-Reasoner-Zero model sizes. Performance metrics are provided for four reasoning-oriented benchmarks: AIME 2024, AIME 2025, MATH500, and GPQA Diamond. As model size increases from 0.5B to 32B parameters, we observe consistent and substantial improvements, underscoring the scalability and effectiveness of our minimalist reinforcement learning approach."
        }
    ],
    "affiliations": [
        "StepFun",
        "Tsinghua University"
    ]
}