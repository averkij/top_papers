{
    "paper_title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models",
    "authors": [
        "Zihao Zhu",
        "Xinyu Wu",
        "Gehan Hu",
        "Siwei Lyu",
        "Ke Xu",
        "Baoyuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify a failure mode in current safety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing a dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving a superior safety-utility balance without compromising reasoning capabilities. Our work establishes a new direction for building more robust and reliable reasoning models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 6 2 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Under review",
            "content": "ADVCHAIN: ADVERSARIAL CHAIN-OF-THOUGHT TUNING FOR ROBUST SAFETY ALIGNMENT OF LARGE REASONING MODELS Zihao Zhu1 Xinyu Wu1 Gehan Hu1 Siwei Lyu2 Ke Xu3 Baoyuan Wu1 1The Chinese University of Hong Kong, Shenzhen 2State University of New York at Buffalo 3Huawei International, Singapore zihaozhu@link.cuhk.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify failure mode in current safety CoT tuning methods: the snowball effect, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving superior safety-utility balance without compromising reasoning capabilities. Our work establishes new direction for building more robust and reliable reasoning models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Reasoning Models (LRMs), which excel at complex problem-solving through explicit Chainof-Thought (CoT) reasoning, represent significant advance in artificial intelligence (Guo et al., 2025; Yang et al., 2025; Team, 2025; OpenAI, 2024). By generating sequence of intermediate reasoning steps before producing final answer, these models achieve remarkable performance on tasks requiring logic, planning, and explanation (Plaat et al., 2024; Xu et al., 2025a; Chen et al., 2025). However, the multi-step nature of CoT reasoning also introduces new attack surface, where single flawed intermediate step can derail an otherwise safe process and corrupt the final outcome, presenting unique and critical challenges for the safety alignment of LRMs (Kuo et al., 2025). The prevailing paradigm for achieving this, known as Safety CoT Tuning, involves fine-tuning models on curated refusal demonstrations (Jiang et al.; Wang et al., 2025b). In this paradigm, models learn to imitate idealized reasoning chains that safely analyze and reject harmful requests. While effective at eliciting correct refusals on standard benchmarks, we demonstrate that this approach inadvertently instills critical vulnerability. We term this failure mode the Snowball Effect in CoT alignment: small, initial deviation in reasoning step progressively amplifies throughout the chain, leading to catastrophic outcomes as models cannot self-correct. This effect manifests in two detrimental forms: snowballing escalation of harmfulness for malicious prompts, where reasoning drifts from safe analysis to harmful compliance, and snowballing escalation of over-refusal for benign prompts, where misplaced caution derails helpful intent. We empirically validate this effect through stepwise evaluation of reasoning chains. For harmful prompts, models often begin with safe analysis but are unable to prevent gradual descent into Corresponding Author"
        },
        {
            "title": "Under review",
            "content": "generating unsafe content. Conversely, for ambiguous but benign prompts, models initially engage constructively but are often trapped by escalating self-doubt, resulting in unnecessary refusals. These dual phenomena reveal that current alignment methods, by teaching models to merely imitate flawless scripts, fail to equip them with the essential capability of dynamic self-correction. To address this limitation, we propose new alignment paradigm adversarial CoT tuning, named AdvChain. Instead of training on exclusively perfect reasoning paths, AdvChain explicitly teaches models to recognize and recover from their own flawed reasoning. Our approach is adversarial because it involves fine-tuning models on novel dataset containing intentionally flawed CoT trajectories that are subsequently corrected. This dataset comprises two key types of self-correcting samples: Temptation-Correction samples, which teach the model to halt and reverse drift towards harmful compliance, and Hesitation-Correction samples, which teach it to overcome unnecessary caution and continue providing helpful responses. By training the model with these samples, we aim to break the cognitive inertia that allows the snowball effect to grow unchecked. Extensive experiments demonstrate that AdvChain effectively counteracts the snowball effect. Models tuned with our method show significantly enhanced robustness against both harmful requests and sophisticated CoT hijacking, while simultaneously reducing over-refusal on benign prompts. Furthermore, AdvChain achieves these gains with high data efficiency, comparable with the performance of models trained on 15 more data, without compromising core reasoning capabilities. Our main contributions are as follows: (1) We identify and empirically validate the Snowball Effect in current CoT alignment, characterizing its dual manifestations of escalating harmfulness and over-refusal. (2) We propose adversarial CoT tuning ( AdvChain) to train LRMs actively recover (3) We construct an adversarial safety reasoning dataset featuring from flawed reasoning steps. temptation-correction and hesitation-correction samples. (4) Extensive evaluation demonstrate that AdvChain is more robust against attacks and less prone to over-refusal."
        },
        {
            "title": "2 BACKGROUND AND PRELIMINARIES",
            "content": "2.1 LARGE REASONING MODELS AND CHAIN-OF-THOUGHT Large Reasoning Models (LRMs) represent an evolution of Large Language Models (LLMs), specifically optimized for complex, multi-step problem-solving (Chen et al., 2025; Xu et al., 2025a; Patil & Jadon, 2025; Liu et al., 2025; Li et al., 2025). Unlike models that produce immediate answers, LRMs excel by generating sequence of intermediate reasoning steps, process known as Chain of Thought (CoT) (Wei et al., 2022), before arriving at final conclusion. This explicit reasoning process, analogous to human cognition, significantly enhances models performance on tasks requiring logical deduction, planning, and mathematical reasoning , as demonstrated by models like DeepSeek-R1 (Guo et al., 2025), Qwen3 (Yang et al., 2025), QwQ (Team, 2025), o1 series OpenAI (2024). Formally, given user prompt x, an LRM with parameters θ generates an output = Mθ(x). This output can be decomposed into tuple = (c, a), where = (c1, c2, . . . , cn) is the reasoning chain, representing the sequence of intermediate thought steps, and is the final answer derived from this reasoning chain. While CoT provides valuable transparency into the models reasoning process, it also introduces new attack surface that require specialized alignment approaches (Xu et al., 2025b; Zhou et al., 2025a; Zheng et al., 2025; Arrieta et al., 2025; Zhu et al., 2025). 2.2 SAFETY ALIGNMENT OF LARGE REASONING MODELS The primary goal of safety alignment is to ensure that models outputs adhere to predefined set of safety principles (e.g., avoiding the generation of harmful, unethical, or biased content) across wide variety of harmful inputs, denote as Xharm (Wang et al., 2023; Ma et al., 2025). prominent method for aligning LRMs is safety CoT tuning (Wang et al., 2025a). Recent approaches include STAR-1 (Wang et al., 2025b) which uses policy-grounded reasoning samples, RealSafe-R1 (Zhang et al., 2025a) with 15k safety-aware trajectories, SafeChain (Jiang et al.) featuring CoT-style safety training, and UnsafeChain (Tomar et al., 2025) focusing on hard case reasoning. These methods finetune models on curated datasets Dalign containing safety demonstrations. Each example comprises tuple (xharm, csafe, asafe), where xharm is harmful prompt, csafe is safe reasoning chain identifying risks and justifying refusal, and asafe is the final safe response. By learning to imitate these idealized"
        },
        {
            "title": "Under review",
            "content": "Figure 1: Empirical validation of the Snowball Effect in CoT alignment of current LRMs. (a)-(b): Snowballing escalation of harmfulness. (c)-(d): Snowballing escalation of over-refusal. reasoning patterns, the model is expected to internalize the underlying safety principles (Zhang et al., 2025b; Zhou et al., 2025b; Wang et al., 2025a). The model learns through standard language modeling objectives to internalize safety constraints. However, as we demonstrate in subsequent sections, these approaches have fundamental limitations that necessitate more robust alignment paradigms. This paradigm involves fine-tuning the large reasoning models on curated dataset, Dalign, composed of high-quality safety demonstrations. Each example in this dataset is tuple (xharm, csafe, asafe), where xharm is potentially harmful user prompt, csafe is model-generated safe reasoning chain that identifies risks and justifies refusal according to safety policies, and asafe is the final, succinct, and safe response, which is typically refusal. The model is then fine-tuned on this dataset by maximizing the standard language modeling objective. By learning to imitate these safety-oriented reasoning patterns, the model is expected to internalize safety alignment capabilities."
        },
        {
            "title": "3 THE SNOWBALL EFFECT IN COT ALIGNMENT",
            "content": "We identify critical failure mode resulting from current CoT alignment methods, which we term the snowball effect, which describes process where small, initial deviation in an intermediate reasoning step progressively amplifies as the reasoning chain unfolds. It occurs because alignment often fails to equip models with the ability to self-correct minor errors, allowing these mistakes to compound and ultimately corrupt the final output. In this section, we empirically demonstrate that this effect manifests in two primary, detrimental forms: snowballing escalation of harmfulness for harmful prompts and snowballing escalation of over-refusal for benign prompts. 3.1 SNOWBALLING ESCALATION OF HARMFULNESS First, our analysis uncovers the critical manifestation of the snowball effect, which we term snowballing escalation of harmfulness. This describes process where the model initiates safe and valid reasoning path, but minor deviation in an intermediate step acts as seed for the snowball. Once flawed step occurs, it begins to gather momentum, progressively corrupting subsequent reasoning and amplifying the initial error into fully harmful conclusion and output. Stepwise Safety Analysis. We quantitatively analyze this phenomenon through stepwise evaluation of reasoning chains generated by DeepSeek-R1-7B and its safety-aligned counterpart STAR1-7B (Wang et al., 2025b) on harmful prompts from WildJailbreak benchmark. Each reasoning chain is decomposed into individual steps using rule-based newline separation (nn), and each step receives an independent assessment by GPT-4o on 5-point safety scale (1 = completely safe, 5 = clearly harmful). We specifically isolate cases where the initial reasoning steps are rated as safe (score 2) but the final answer is judged harmful by LlamaGuard3 (Llama Team, 2024). For comparative analysis across varying reasoning lengths, the position of each step is normalized to relative scale from 0.1 to 1.0. This allows us to track the evolution of safety scores and identify the escalation pattern. Findings. The results, illustrated in Figure 1 (a)-(b), provide empirical evidence for the snowballing escalation of harmfulness. The process does not begin as overtly harmful. Actually, the initial reasoning steps maintain low score, often averaging below 1.5, as the model correctly identifies the users query and initiates seemingly legitimate analysis. This represents the small, seeding snow-"
        },
        {
            "title": "Under review",
            "content": "Figure 2: The framework of our proposed AdvChain, which consists of two stages: (a) constructing an adversarial safety reasoning dataset with Temptation-Correction (T-C) and Hesitation-Correction (H-C) samples, and (b) the adversarial CoT fine-tuning to instill dynamic self-correction capabilities. ball. However, as the reasoning progresses, the safety score rapidly escalates, frequently exceeding 4.0 in the final steps. This finding directly shows how subtle nudge towards harmful path can initiate snowballing process that the model is unable to stop. This suggests he safety logic learned through conventional alignment is superficial. It lacks the robustness to halt this internal escalation, demonstrating critical failure to correct its path once harmful trajectory has begun. 3.2 SNOWBALLING ESCALATION OF OVER-REFUSAL We identify another critical manifestation of the snowball effect: snowballing escalation of overrefusal. This phenomenon describes process where the model begins with helpful and appropriate reasoning path for benign prompt, but minor, unnecessary hesitation about safety acts as the initial seed. Once this seed of doubt is planted, it progressively amplifies throughout the reasoning chain, transforming potentially helpful response into an unnecessary refusal. Stepwise Helpfulness Analysis. To trace this process, we conduct stepwise analysis analogous to the one in the previous section. We analyze the reasoning chains from over-refused responses to the benign prompts within the WildJailbreak benchmark. Each reasoning step is scored by GPT-4o on 5-point helpfulness scale, where score of 1 indicates explicit refusal or reasoning termination and 5 represents actively helpful reasoning. We evaluate both the base DeepSeek-R1-7B and safetyaligned STAR-1-7B models. We filter for cases where the model initially attempts to be helpful (initial score 4) to precisely observe how the escalation of over-refusal unfolds from correct starting point. Findings. Our analysis, shown in Figure 1 (c)-(d), illustrates this snowballing process of overrefusal. The reasoning typically begins with helpful steps, with the initial phase of the CoT averaging helpfulness score above 4.5 as the model correctly understands and attempts to address the users request. However, during the process, once point of hesitation regarding safety is introduced, and from there, the helpfulness score progressively decreases. In the latter half of the chain, the score often plummets below 2.0. This demonstrates the snowball effect in action: minor, misplaced doubt about potential policy violation gets amplified, causing the models internal dialogue to shift from problem-solving to defensive risk aversion. The initial helpful intent is completely derailed, leading to an unnecessary refusal and significantly reduced practical utility. 3.3 THE CORE ISSUE: LACK OF ROBUST AND ADAPTIVE REASONING The dual phenomena of escalating harmfulness and escalating over-refusal reveal the core issue with conventional alignment: it induces cognitive inertia but fails to instill robust, adaptive reasoning. Current safety tuning methods primarily teach models to replicate idealized, error-free reasoning scripts. This approach trains models to recognize the form of correct reasoning chain, but critically, it provides no training signals for how to recover from mistake. This lack of error-correction capability is what allows the snowball effect to take hold. It is trapped by its own cognitive inertia, allowing the snowball of flawed reasoning to grow unchecked until it corrupts the final output."
        },
        {
            "title": "4 METHODOLOGY: ROBUST ALIGNMENT VIA ADVERSARIAL COT TUNING",
            "content": "To counteract the snowball effect in CoT alignment, we propose adversarial CoT tuning, named AdvChain, which is new alignment paradigm focused on dynamic self-correction."
        },
        {
            "title": "4.1 OVERVIEW: FROM IMITATION SCRIPTS TO DYNAMIC CORRECTION",
            "content": "Our approach is founded on the insight that true robustness comes not from flawlessly imitating merely idealized, error-free reasoning scripts, but from the dynamic ability to recognize and recover from ones own cognitive errors. We shift the alignment paradigm from preventing flawed thoughts to actively correcting them, thereby breaking the cognitive inertia that allows the snowball effect to grow unchecked. The core of our method, AdvChain, is to build this self-correction capability directly into the models reasoning process. The method is adversarial because we intentionally expose the model to flawed, internally generated reasoning steps that act as attacks on its own thought process. The framework of our method is shown in Figure 2. It consists of two primary stages: (1) the programmatic construction of novel adversarial safety reasoning dataset containing examples of internal errors and their corrections; and (2) fine-tuning the LRM on this dataset. By training on these self-correcting trajectories, our method directly targets the cognitive inertia identified previously, aiming to cultivate more resilient and practical alignment. 4.2 ADVERSARIAL SAFE REASONING DATASET Our dataset is constructed by programmatically rewriting existing reasoning chains to create adversarial examples of flawed internal reasoning. We use powerful teacher model, guided by detailed instructional prompts, to inject specific cognitive errors and their subsequent corrections into existing CoTs. This process yields two novel types of training samples, Temptation-Correction (T-C) samples and Hesitation-Correction (H-C) samples, each designed to address specific failure mode identified in Section 3. Temptation-Correction Samples to Halt Harmfulness Escalation. To directly counter the snowballing escalation of harmfulness, we first create temptation-correction samples that move beyond perfect refusals by simulating an internal temptation to act maliciously, thereby creating an adversarial attack within the reasoning path itself, which the model must then learn to overcome. The generation process is as follows: Stage 1: Generating Base Safe Reasoning Path. For given harmful prompt xharm, the teacher model is first prompted to generate standard, safe refusal CoT, csafe = (c1, c2, . . . , cn), which serves as the foundational context for adversarial modifications. Stage 2: Injecting the Temptation Step. The base CoT is then provided to the teacher model to inject harmful temptation phase at logically coherent insertion point k. This injected text, denoted as ctemp, serves as an adversarial thought process, where the reasoning begins to rationalize the harmful request and explore how to respond to it, marking the turning point from safe to an unsafe reasoning path. Stage 3: Injecting the Correction Step. In subsequent step, the teacher model is prompted to generate strong correction step, ccorr, that explicitly identifies the danger of ctemp, refutes the flawed justification, and steers the reasoning back towards safe refusal. Stage 4: Assemble the Trajectory. The final chain is assembled as cadv = (c1:k, ctemp, ccorr, ck+1:n), where c1:k is the initial part before inserted point and ck+1:n is the remainder. This chain can be further polished to ensure overall coherence and fluency. The final summary remains safe refusal. Hesitation-Correction Samples to Counter Helpfulness Decay. To address the snowballing escalation of over-refusal, we create hesitation-correction samples. These simulate unnecessary hesitation when faced with an ambiguous but benign request. The process mirrors the one above: Stage 1: Generating Base Helpful Reasoning Path. The process begins with benign prompt and generate its corresponding standard, helpful CoT, chelp = (c1, c2, . . . , cn)."
        },
        {
            "title": "Under review",
            "content": "Stage 2: Injecting the Hesitation Step. At an appropriate insertion point k, the model injects an overcautious hesitation phase chesi, where the model incorrectly misinterprets the safe prompt as harmful and temporarily decides to refuse. Stage 3: Injecting the Correction Step. correction step ccorr is then inserted, in which the reasoning identifies the hesitation as false positive and steers the process back to original path. Stage 4: Assemble the Trajectory. The components are assembled into the final CoT, cadv = (c1:k, chesi, ccorr, ck+1:n), and then be polished."
        },
        {
            "title": "4.3 ADVERSARIAL COT TUNING",
            "content": "Our constructed dataset, Dadv , which contains combination of both temptation-correction and hesitation-correction samples, is subsequently used to fine-tune base LRM. The models parameters θ are optimized using standard autoregressive objective over this new dataset. Specifically, for each sample (x, cadv, s) Dadv, we maximize the log-likelihood of the model generating the entire self-correcting reasoning path and final summary: maxθ log (cadv, x; θ) . This adversarial CoT tuning process compels the model to internalize the mechanism of error identification and recovery, equipping it with the tools necessary to actively halt the snowball effect. (x,cadv,a)Dadv (cid:80)"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP Base Models. Our experiments are conducted on diverse set of open-source LRMs to ensure broad applicability. We use two models from the DeepSeek-R1 family (1.5B and 7B) and three from the Qwen3 family (0.6B, 1.7B, and 4B). These models were chosen for their strong baseline reasoning capabilities and their open availability. Implementation Details. We construct adversarial safe reasoning dataset Dadv by leveraging and rewriting existing high-quality data, where harmful prompts for temptation-correction samples are sourced from STAR-1k, while benign prompts for hesitation-correction samples are sourced from STAR-benign-915. To streamline the process, the original reasoning chains and final summaries from these datasets are used directly as the base context for our adversarial injection, replacing the generation process in step 1. Our final dataset contains 1000 samples, comprising 800 temptationcorrection and 200 hesitation-correction examples, keeping the total sample size consistent with baselines. For the safety CoT tuning, we performed full supervised fine-tuning for 5 epochs with batch size of 128. We used the AdamW optimizer with learning rate of 1e-4, max sequence length of 8192, and warm-up ratio of 5%. All experiments are performed on 8 NVIDIA RTX4090 GPUs. The adversarially safe tuned models are referred as AdvChain-R1 and AdvChain-Qwen3 according to their base models respectively. Evaluation Datasets. To comprehensively assess model performance, we utilize suite of benchmarks targeting four key areas. (1) General Safety: To evaluate the models ability to refuse direct harmful requests, we use HarmBench (Mazeika et al., 2024), StrongReject (Souly et al., 2024), and the vanilla harmful subset of WildJailbreak (Jiang et al., 2024). (2) Adversarial Robustness: To test against more sophisticated attacks, we use benchmarks containing stealthy requests and jailbreak tactics from SafeUnlearning (Zhang et al., 2024) and the adversarial harmful subset of WildJailbreak. (3) Over-Refusal: To measure cognitive rigidity, we evaluate on the safe subset of XSTest (Rottger et al., 2024) and vanilla benign and adversarial benign subsets of WildJailbreak. (4) Reasoning Capabilities: To ensure our method preserves core abilities, we evaluate all models on suite of reasoning tasks, including Math500 (Hendrycks et al., 2021), AIME2024 (Mathematical Association of America, 2024), and LiveCodeBench (Jain et al., 2024). Baselines. Our comparisons are against following prominent alignment methods: STAR-1 (Wang et al., 2025b), which guides model to generate safe CoT by providing the safety policy along with the prompt and verify the safety of the reasoning; SafeChain (Jiang et al.), which employs generate-then-filter strategy where powerful model generates multiple responses that are subsequently filtered by safety classifier; UnsafeChain (Tomar et al., 2025), which focuses on hard cases by rewriting the failed refusals of base model into safe demonstrations. These three methods are all fine-tuned on 1,000 data samples using their default parameters. Moreover, we compare"
        },
        {
            "title": "Under review",
            "content": "Table 1: Comparison of safety performance across harmful and jailbreak prompt benchmarks. Dataset Model DeepSeek-R1-1.5B STAR-1(1k) SafeChain (1k) UnsafeChain (1k) RealSafe-R1 (15k) AdvChain (1k) DeepSeek-R1-7B STAR-1(1k) SafeChain (1k) UnsafeChain (1k) RealSafe-R1 (15k) AdvChain (1k) Qwen3-1.7B STAR-1(1k) SafeChain (1k) UnsafeChain (1k) AdvChain (1k) Qwen3-4B STAR-1(1k) SafeChain (1k) UnsafeChain (1k) AdvChain (1k) Harmful Prompts JailBreak Prompts HarmBench StrongReject WJ-VaniHarm SafeUnlearning WJ-AdvHarm ASR RR ASR RR ASR RR ASR RR ASR RR 81.50 19.50 33.50 23.00 6.00 9.50 51.00 8.00 38.00 26.00 2.00 4.50 43.00 18.00 47.50 50.50 5. 24.00 2.50 33.00 17.00 4.00 28.00 65.50 72.50 64.50 96.00 86.50 53.50 83.50 60.00 63.50 96.00 92.00 61.50 78.00 55.00 60.50 90.50 79.00 95.50 59.50 75.00 93.50 78.00 29.50 33.50 30.00 2.50 9. 45.05 6.00 38.00 27.00 2.50 2.00 31.00 9.50 39.00 44.00 3.00 9.50 0.50 21.00 7.50 1.00 26.50 57.00 66.00 52.00 94.50 90.00 49.84 95.00 62.00 63.50 97.50 95.00 72.00 79.50 53.00 66.50 91. 87.00 97.50 68.00 82.50 95.00 46.60 20.67 36.00 28.00 0.20 3.33 28.46 11.67 24.67 12.67 0.20 2.00 19.00 4.67 20.67 18.67 1.33 6.60 0.67 12.00 11.33 0.67 26.60 47.33 29.33 38.00 96.80 86. 31.80 88.33 37.33 58.67 99.20 86.67 47.60 68.67 42.67 45.33 84.67 61.60 90.00 46.67 50.00 92.00 88.60 43.49 50.47 45.81 2.33 11.63 45.35 28.05 39.65 34.86 8.14 9.30 87.21 70.93 62.79 79.07 16. 79.07 37.21 36.05 52.33 17.44 34.88 53.72 43.60 48.84 96.51 84.88 54.65 65.12 59.30 52.77 98.84 89.53 37.21 47.67 48.84 43.02 81.40 75.58 86.05 70.93 67.44 83.72 75.40 27.33 23.30 19.33 4.40 11. 26.00 17.33 24.00 19.33 4.80 9.00 29.00 23.33 25.33 27.33 14.00 24.80 13.33 19.33 20.66 10.68 19.00 64.00 58.67 68.40 93.60 86.50 5.80 44.67 22.00 26.00 94.80 80.40 15.20 22.67 19.33 25.33 43. 23.20 44.67 33.33 43.33 74.67 against RealSafe-R1 (Zhang et al., 2025a), an advanced safety-tuned model that was fine-tuned on larger dataset of 15k safety-aware reasoning trajectories. As its training data is not public, we use its released model checkpoint. Evaluation Metrics. Our evaluation applies the following metrics: (1) Attack Success Rate (ASR,%): For given harmful prompt, we use LlamaGuard3 to adjudicate the safety of the models final summary. response that is judged as unsafe denotes successful attack. The ASR is the percentage of prompts that elicit an unsafe response. (2) Refusal Rate (RR,%): The RR is the proportion of prompts that the model provides refusal response, which is identical with over-refusal rate (ORR) for benigns. It is calculated by using keyword matching to identify refusal phrases. (3)Pass@1 (%): For the general reasoning benchmarks, this metric represents the percentage of problems for which the model generates correct solution in single attempt. 5.2 EVALUATION OF SAFETY AND ROBUSTNESS In this section, we evaluate the effectiveness of AdvChain in enhancing model safety, focusing on its ability to resist both standard attacks and manipulation of its reasoning process. Performance on General Safety Benchmarks. We first evaluate the models on broad suite of safety benchmarks to establish their fundamental resilience against common threats, including both direct harmful requests and more sophisticated jailbreak prompts. The results, summarized in Table 1, consistently demonstrate the superior performance of our AdvChain models. Across all model families and sizes, AdvChain achieves significantly lower Attack Success Rate (ASR) compared to baseline methods like STAR-1, SafeChain, and UnsafeChain, which are trained on the same volume of data (1k). This robust defense is likely because training the model to actively correct harmful reasoning paths provides more principled safety understanding than simply memorizing refusal patterns. Furthermore, AdvChains performance is on par with RealSafe-R1, despite the latter being fine-tuned on 15 larger dataset (15k). This highlights that our adversarial CoT tuning is highly data-efficient method for achieving safety alignment and generalizes effectively against wide range of attack vectors."
        },
        {
            "title": "Under review",
            "content": "Table 2: Results against CoT-Hijacking attack. Table 3: Results on Benign Prompts. Dataset Model CoT-Hijack ASR (%) RR (%) Dataset Model XSTest WJ-Benign ORR (%) ORR (%) DeepSeek-R1-7B STAR-1 SafeChain-R1-7B UnsafeChain RealSafe-R1 (15k) AdvChain (Ours) Qwen3-4B STAR-1 SafeChain UnsafeChain AdvChain (Ours) 74.67 54.67 44.00 60.67 14.67 9.33 30.00 12.67 14.00 39.33 8.67 35.33 43.33 58.00 45.33 84.67 74.00 72.67 82.67 68.00 62.00 84.00 DeepSeek-R1-7B STAR-1 SafeChain-R1-7B UnsafeChain RealSafe-R1 (15k) AdvChain (Ours) Qwen3-4B STAR-1 SafeChain UnsafeChain AdvChain (Ours) 16.80 42.00 28.80 24.80 66.40 18.00 10.80 26.80 15.40 16.00 12.50 10.40 33.33 14.67 21.33 60.60 12.67 16.00 22.00 20.67 22.33 18.00 Robustness against Adaptive CoT Hijacking. To more directly probe the stability of the reasoning process, we design and evaluate the models against an Adaptive CoT Hijacking Attack. This attack moves beyond standard prompts to measure models ability to maintain safe reasoning path when its own thought process is adversarially manipulated. To this end, we construct CoT-Hijack dataset, comprising 150 samples and thought prefixs. The construction process targets samples where base model (DeepSeek-R1-7B) initially produces correct and safe refusal. For each of these successful refusals, we take its safe reasoning chain and use powerful teacher model to rewrite it. The rewriting involves strategically inserting malicious pivot thought that subtly shifts the reasoning from refusal towards compliance. This creates hijacked reasoning prefix, which is then presented to the target model to continue the thought process. successful attack occurs if the models final response is harmful. The results of this targeted attack, shown in Table 2, reveal stark difference in resilience. Our AdvChain models demonstrate robust reasoning, achieving significantly lower ASR than the baseline models. In contrast, the conventionally aligned models prove to be highly fragile, easily having their reasoning hijacked by the adversarial prefix. This finding directly validates that training on temptation-correction samples builds form of cognitive immunity to internal reasoning manipulation, which is crucial capability that models trained only on perfect refusal paths lack. 5.3 EVALUATION OF OVER-REFUSAL AND GENERAL CAPABILITIES In this section, we evaluate AdvChains impact on the models utility, specifically its tendency for over-refusal and its core reasoning abilities. Reduced Over-Refusal on Benign Prompts. common side effect of safety alignment is an increase in over-refusal, where models incorrectly reject safe, nuanced prompts. We assess this by measuring performance on the XSTest and WildJailbreak benign benchmarks. The results, reported in Table 3, show that our AdvChain models are significantly more practical. They exhibit much lower Over-Refusal Rate (ORR) compared to the conventionally aligned baselines, which show strong tendency to be overcautious. This demonstrates that training with our Hesitation-Correction samples successfully mitigates the snowballing, improving cognitive flexibility and allowing our models to break the typical safetyutility trade-off. Table 4: Results on mathematics and coding datasets. Math-500 AIME 2024 LiveCodeBench DeepSeek-R1-7B AdvChain (Ours) Qwen3-4B AdvChain (Ours) 51.30 49.33 92.80 93.40 37.60 36.50 97.00 96. 53.03 52.40 71.35 69.50 Model Preserved Reasoning Capabilities. Finally, it is crucial to verify that our alignment method does not degrade the models core problemsolving abilities. We evaluate all fine-tuned models on suite of challenging reasoning benchmarks covering mathematics and coding. As shown in Table 4, our AdvChain models achieve Pass@1 scores on par with their original base models. This confirms that Adversarial CoT Tuning successfully instills robust safety and improves helpfulness without sacrificing the essential reasoning capabilities that make these models powerful in the first place."
        },
        {
            "title": "Under review",
            "content": "Figure 3: Comparison of stepwise reasoning patterns between differnet training datasets. Figure 4: Impact of different types of data composition on safety alignment and over-refusal."
        },
        {
            "title": "6 ANALYSIS AND DISCUSSION",
            "content": "Structural Analysis of Reasoning Patterns. To understand the fundamental differences between our method and conventional alignment, we conduct structural analysis of the reasoning patterns between our temptation-correction samples and other standard safety datasets. For each reasoning chain, we first decompose it into atomic reasoning steps using newline separator. Then, powerful external LLM adjudicates the safety of each step on 5-point scale as used in Section 3.1. The results are shown in in Figure 3, reveal striking contrast. The STAR-1 exhibit flat and consistently low safety score, remaining in safe state from beginning to end, reinforcing an idealized path but providing no information on how to handle errors. In contrast, our temptation-correction samples feature distinct peak-like pattern: the score begins at low level, rises during the reasoning chains, and back to safe state. This dynamic trajectory provides an explicit training signal for self-correction, teaching the model the process of error recovery rather than mere imitation of an idealized safe form. Impact of Data Composition. We conduct an ablation study to examine the specific contribution of each type of our constructed dataset. We keep the total training size fixed at 1,000 samples but vary the ratio of temptation-correction to hesitation-correction samples. The results in Figure 4 show trade-off. As the proportion of T-C samples increases, the models robustness against attacks improves, leading to lower ASR. Conversely, higher proportion of H-C samples leads to lower refuse rate on benign prompts, indicating reduced over-refusal. This finding demonstrates that each component of our dataset serves specialized and complementary purpose: T-C data is critical for building resilience against harmful prompts, while H-C data is essential for maintaining helpfulness and reducing false positives. Limitations and Future Work. Our method demonstrates promising results but faces several limitations. First, the generated adversarial examples depend on the quality of the teacher model, which may not cover all potential safety violations. Second, our method currently addresses single-turn reasoning corrections, while attacks may involve more sophisticated, multi-step manipulations. Future work should explore more efficient methods for generating adversarial CoT examples, extend the framework to diverse scenarios, and investigate continual learning approaches to maintain robustness against evolving threats. These directions would help create more autonomous and adaptive safety alignment systems."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This work identifies the snowball effect as critical vulnerability in current safety alignment methods for Large Reasoning Models. To address this limitation, we introduce AdvChain, novel adversarial CoT tuning framework that teaches models dynamic self-correction through training on welldesigned Temptation-Correction and Hesitation-Correction samples. Our approach demonstrates that learning from reasoning chains containing intentional errors and their corrections enables models to effectively halt reasoning degradation. This results in significantly enhanced robustness against attacks while substantially reducing over-refusal. By breaking the conventional safety-utility tradeoff without compromising core reasoning capabilities, AdvChain establishes promising direction for developing more reliable and practical reasoning models."
        },
        {
            "title": "REFERENCES",
            "content": "Aitor Arrieta, Miriam Ugarte, Pablo Valle, Jose Antonio Parejo, and Sergio Segura. o3-mini vs deepseek-r1: Which one is safer? arXiv e-prints, 2025. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-ofthought for reasoning large language models. arXiv e-prints, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Neural Information Processing Systems, 2021. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free In The Thirteenth International Conference on evaluation of large language models for code. Learning Representations, 2024. Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, and Radha Poovendran. Safechain: Safety of language models with long chain-of-thought reasoning capabilities. In International Conference on Learning Representations. Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, et al. Wildteaming at scale: From inthe-wild jailbreaks to (adversarially) safer language models. Advances in Neural Information Processing Systems, 2024. Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Hai Li, and Yiran Chen. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv e-prints, 2025. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv e-prints, 2025. Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, and Yue Zhang. Logical reasoning in large language models: survey. arXiv e-prints, 2025. AI @ Meta Llama Team. The llama 3 herd of models, 2024. Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, et al. Safety at scale: comprehensive survey of large model safety. arXiv e-prints, 2025. Mathematical Association of America. American invitational mathematics examination (aime), 2024. Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. In International Conference on Machine Learning, 2024. OpenAI. Openai o1 system card. arXiv e-prints, 2024. Avinash Patil and Aryan Jadon. Advancing reasoning in large language models: Promising methods and approaches. arXiv e-prints, 2025. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. arXiv e-prints, 2024."
        },
        {
            "title": "Under review",
            "content": "P Rottger, Kirk, Vidgen, Attanasio, Bianchi, Hovy, et al. Xstest: test suite for identifying exaggerated safety behaviours in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2024. Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et al. strongreject for empty jailbreaks. In International Conference on Learning Representations, 2024. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. Raj Vardhan Tomar, Preslav Nakov, and Yuxia Wang. Unsafechain: Enhancing reasoning model safety via hard cases. arXiv e-prints, 2025. Cheng Wang, Yue Liu, Baolong Bi, Duzhen Zhang, Zhong-Zhi Li, Yingwei Ma, Yufei He, Shengju Yu, Xinfeng Li, Junfeng Fang, et al. Safety in large reasoning models: survey. arXiv e-prints, 2025a. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: survey. arXiv e-prints, 2023. Zijun Wang, Haoqin Tu, Yuhan Wang, Juncheng Wu, Jieru Mei, Brian Bartoldson, Bhavya Kailkhura, and Cihang Xie. Star-1: Safer alignment of reasoning llms with 1k data. arXiv eprints, 2025b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 2022. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv e-prints, 2025a. Zhiyuan Xu, Joseph Gardiner, and Sana Belguith. The dark deep side of deepseek: Fine-tuning attacks against the safety alignment of cot-enabled models. arXiv e-prints, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv e-prints, 2025. Yichi Zhang, Zihao Zeng, Dongbai Li, Yao Huang, Zhijie Deng, and Yinpeng Dong. Realsafe-r1: Safety-aligned deepseek-r1 without compromising reasoning capability. arXiv e-prints, 2025a. Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, and Minlie Huang. Safe unlearning: surprisingly effective and generalizable solution to defend against jailbreak attacks. arXiv preprint arXiv:2407.02855, 2024. Zhexin Zhang, Xian Qi Loye, Victor Shea-Jay Huang, Junxiao Yang, Qi Zhu, Shiyao Cui, Fei Mi, Lifeng Shang, Yingkang Wang, Hongning Wang, et al. How should we enhance the safety of large reasoning models: An empirical study. arXiv e-prints, 2025b. Baihui Zheng, Boren Zheng, Kerui Cao, Yingshui Tan, Zhendong Liu, Weixun Wang, Jiaheng Liu, Jian Yang, Wenbo Su, Xiaoyong Zhu, et al. Beyond safe answers: benchmark for evaluating true risk awareness in large reasoning models. arXiv e-prints, 2025. Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, and Xin Eric Wang. The hidden risks of large reasoning models: safety assessment of r1. In International Conference on Machine Learning, 2025a. Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth Srinivasa, Aosong Feng, Dawn Song, and Xin Eric Wang. Safekey: Amplifying aha-moment insights for safety reasoning. arXiv e-prints, 2025b. Zihao Zhu, Hongbao Zhang, Ruotong Wang, Xu Ke, Lyu Siwei, and Baoyuan Wu. To think or not to think: Exploring the unthinking vulnerability in large reasoning models. In NeurIPS 2025 Workshop on Foundations of Reasoning in Language Models, 2025."
        }
    ],
    "affiliations": [
        "Huawei International, Singapore",
        "State University of New York at Buffalo",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}