{
    "paper_title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "authors": [
        "Yuan-Kang Lee",
        "Kuan-Lin Chen",
        "Chia-Che Chang",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/"
        },
        {
            "title": "Start",
            "content": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes Yuan-Kang Lee1,3* Kuan-Lin Chen2,3* Chia-Che Chang1 Yu-Lun Liu3 1MediaTek Inc. 2National Taiwan University 3National Yang Ming Chiao Tung University 6 2 0 J 8 ] . [ 1 9 4 2 5 0 . 1 0 6 2 : r Figure 1. Our method achieves optimal parameter tuning for automatic white balance (AWB) in nighttime scenes through hybrid combination of novel statistical color constancy algorithm and reinforcement learning. Due to the complex lighting conditions in night scenes and the presence of significant noise in the images, traditional AWB tuning faces relatively difficult and time-consuming challenges when adapting parameters for night scene images. Our method can optimize parameters for different night scene images at faster speed without requiring prior knowledge of illumination ground-truth, and has better cross-sensor generalization advantages."
        },
        {
            "title": "Abstract",
            "content": "Nighttime color constancy remains challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statisti- *The first two authors contributed equally to this work. cal algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieve superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/ research/rl-awb/ 1. Introduction Auto White Balance (AWB) is fundamental component of camera image signal processing (ISP) pipelines that estimates scene illumination and corrects color casts, ensuring white objects appear neutral across varying lighting conditions [10, 22]. While existing methods achieve robust performance in well-lit daytime scenarios [2, 8, 13], nighttime environments present fundamentally different challenge. Low illumination, high ISO settings, and severe chromatic noise violate the statistical assumptions underlying conventional AWB algorithms [46, 55], leading to highly unstable illuminant estimates. This instability is further exacerbated in cross-sensor deployment, where the same model often produces significant and unpredictable color shifts across different camera sensors and ISP configurations [2, 18]. Addressing robust cross-sensor nighttime AWB is therefore critical for real-world applications including mobile photography, surveillance systems, and automotive imaging. The core difficulty of nighttime AWB stems from the breakdown of reliable color statistics. Statistical methods [10, 22, 55] assume sufficient scene diversity and stable gray pixel detection, which fail under extreme low-light conditions where sensor noise dominates signal [17]. Deep learning approaches [8, 13, 28], while effective in daytime scenarios, require extensive labeled nighttime training data and suffer from severe generalization degradation when deployed on unseen camera sensors [2, 30]. Furthermore, nighttime scenes exhibit heightened sensitivity to algorithmic parameters, small variations in parameter selection can lead to dramatically different illuminant estimates. To address these challenges, we present RL-AWB, the first framework that integrates reinforcement learning into automatic white balance for nighttime color constancy. Our approach fundamentally differs from existing paradigms by formulating AWB as sequential decision-making problem, where an RL agent learns adaptive parameter selection policies for novel statistical illuminant estimator. This hybrid design preserves the interpretability and sensor-agnostic nature of statistical methods [22, 46] while gaining the adaptive capability of learning-based approaches [2, 8], all with minimal training data requirements. In summary, we make the following contributions: We develop SGP-LRD (Salient Gray Pixels with Local Reflectance Differences), nighttime-specific color constancy algorithm that achieves state-of-the-art illumination estimation on public nighttime benchmarks. We design the RL-AWB framework with Soft Actor-Critic training and two-stage curriculum learning, enabling adaptive per-image parameter optimization with exceptional data efficiency. We contribute LEVI, the first multi-camera nighttime dataset comprising 700 images from two sensors, enabling rigorous cross-sensor color constancy evaluation. Extensive experiments demonstrate superior cross-sensor generalization over state-of-the-art methods with only 5 training images per dataset. 2. Related Work Statistical Color Constancy. Statistical methods exploit scene statistics via achromatic averages (Gray World [10]), maximum responses (Max-RGB), unified by Minkowski norms [22]. Edge-based approaches [55] use derivatives with photometric weighting [25] and local statistics [21]. Recent advances include grayness indices [46], FFT acceleration (700 FPS) [9], spectral methods [31], and attention mechanisms [33]. Unlike these methods using fixed parameters or heuristics, we employ reinforcement learning to automatically learn optimal parameter adjustments for varying nighttime conditions, combining statistical efficiency with adaptive optimization. Learning-Based Color Constancy. Deep learning approaches feature CNNs with log-chrominance localization and confidence pooling [8, 28], enhanced by cascades [66], contrastive learning [40], and adaptation [2]. Recent diffusion methods [13] achieve 4.325.22 worst-25% error via inpainting, while self-supervised [44] and uncertaintyaware [11] approaches reduce supervision. Extensions address multi-illuminant [37], post-editing [1], mixed lighting [3], and brightness [16, 62]. In contrast to these data-hungry approaches requiring extensive labeled nighttime data, our method achieves cross-dataset generalization through sample-efficient RL tuning interpretable statistical parameters, combining both paradigms. Nighttime and Low-Light Color Constancy. Nighttime scenes present unique challenges that fundamentally violate daytime color constancy assumptions: mixed illumination from multiple light sources, severely low light levels, and elevated sensor chroma noise [14]. Early methods [15, 26, 58] targeted brightness. Recent work employs transformers [12, 63], adaptive masking [36], synthetic data [45], noise-resistant detection [17], and joint restoration [41, 60, 65, 73]. However, these rely on pseudo-labels with error propagation or fixed parameters. We formulate nighttime AWB as sequential decision-making, where RL learns dynamic parameter adjustment strategies. Reinforcement Learning for ISP. RL enables adaptive ISP policies for exposure [67], color enhancement with WB control [43], rapid convergence [34] (5 frames, 1ms), and artifact filtering [7]. Applications span pixel-wise correction [23], software parameters [32], personalization [64, 72], sequential optimization [50], and module selection [57, 70]. These validate RL for ISP but focus on daytime scenarios with camera settings as actions. We extend to nighttime AWB where actions control statistical algorithm parameters, requiring noise-aware rewards and robust optimization. Cross-Sensor Generalization. Cross-sensor generalization remains challenge in color constancy, as different camera sensors exhibit varying spectral responses and ISP charac2 Figure 2. Overview of the proposed RL-AWB framework. (A) Given an input image, the proposed nighttime color constancy algorithm SGP-LRD estimates the scene illuminant conditioned on two hyperparameters (gray-pixel sampling percentage and Minkowski order p). (B) SAC agent selects parameter updates based on image statistics and current AWB settings. (C) The policy outputs one action per parameter; actions are sampled, squashed by tanh to [1, 1], and rescaled to valid ranges. (D) The rescaled actions update the two hyperparameters and are applied to SGP-LRD to produce the illuminant estimate. Repeat until the termination criterion is met. teristics [39]. Methods evolved from dataset evaluation [18] through fine-tuning [2], embeddings [30], to domaininvariant learning [52, 71]. Calibration-light methods use dual-mapping [68] (single D65), self-supervision [20], multidomain architectures [61], and HDR [4]. Test-time adaptation [29, 38, 51, 56] enables training-free deployment. Unlike methods requiring test-time multi-image access or camera-specific calibration, we achieve generalization through (1) sensor-agnostic statistical algorithms and (2) curriculum learning exposing agents to diverse conditions, enabling few-shot inference on unseen cameras. Hybrid Statistical-Learning Approaches. Hybrid methods combine classical algorithms with deep learning via unfolding [69], learned parameter dictionaries [19], energy landscapes [6], and hyperparameter prediction [35]. Our work follows this paradigm, combining statisticallygrounded illumination estimation with RL-based parameter optimization and preserving interpretability. Curriculum Learning in RL. Curriculum learning improves efficiency through progressive difficulty [42], including backward-forward [53] and strategic sampling [5, 48]. Our two-stage approach: (1) single-instance stabilization addresses cold starts, (2) cyclic multi-instance balances exploration and stability. 3. Method 3.1. Nighttime Color Constancy Algorithm Salient Gray Pixel Detection. Under the narrow-band spectral assumption with uniform illumination, the linear image is modeled as = WL+δ, where is the white-balanced image, is illumination, and δ is noise. Neglecting δ and applying log-transform followed by local contrast operator C{} (Laplacian of Gaussian) yields: C{log(I (x,y) )} = C{log(W (x,y) )}. (1) 3 This shows local contrast depends solely on surface reflectance. Let i(x, y) denote the local contrast at (x, y) for channel {R, G, B}. Following Qian et al. [47], achromatic pixels have contrast vectors (x, y) = [R, G, B]T aligned with the gray direction = [1, 1, 1]T. Grayness is measured via angular error: reflectance difference at position (i, j) is then defined as: (cid:88) fc(x) / i,j i,j = xW i,j max xWi,j fc(x) . (4) G(x, y) = cos1 (cid:18) (x,y), (x,y)g2 (cid:19) . (2) The top % pixels ranked by G(x, y) are selected as gray candidates. We then apply two-layer filtering process to refine this candidate set, mitigating the adverse effects of noise and chromatic outliers prevalent in low-light imagery: Local Variance Filtering. For each pixel (x, y) in the initial detected gray pixel mask, we compute the variance across the logarithmic RGB channels. Pixels where the intra-pixel variance is too small often lack reliable color information. By applying lower bound threshold, VarTh, we filter out these unreliable candidates. Color Deviation Filtering. The second stage filter out gray pixels that are too distant from the dominant color cast of the scenes illumination. We first compute the mean logarithmic intensity for each channel of the image: = [ MR, MG, MB]T, and then calculate the maximum absolute color deviation, X(x, y), for each pixel from this mean. We define threshold TC = ColorTh min(M). Any initial detected gray pixel exceeding this deviation is removed. Following these two refinement layers to the initial detected gray pixels, we obtain the Salient Gray Pixels (SGPs). Gray-pixel Confidence Weighting. Let R(i, j), G(i, j), and B(i, j) denote the normalized pixel intensities at location (i, j) in the three channels, respectively. The luminance map for SGPs is then computed as LM (i, j) = (R(i, j) + G(i, j)+B(i, j))/3. The skewness value γLM quantifies the asymmetry of the brightness distribution and guides the selection of an adaptive exponent parameter E. We set = 1.0 for highly skewed distributions (γLM > 1.5), = 2.0 for moderate skewness (0.2 < γLM 1.5), and = 4.0 for uniform illumination (γLM 0.2). Let LM denotes the mean luminance of non-zero SGPs. The gray-pixel confidence weight is then computed as: WSGP (i, j) = 1 exp (cid:34) (cid:19)E (cid:35) (cid:18) LM (i, j) LM . (3) For pixels where the maximum value is zero, we set i,j = 0 to exclude invalid regions from the computation. Finally, the illuminant is estimated as: 1/p (cid:88) ˆec = (cid:88) Ω Ω (cid:0)µi,j (cid:0)N i,j WSGP (i, j)(cid:1)p WSGP (i, j)(cid:1)p , (5) where Ω represents all valid pixel positions (N i,j > 0), µ is the mean intensity of SGPs in Wi,j, and is the Minkowski norm parameter. The numerator accumulates weighted SGP intensity, and the denominator accumulates weighted normalized local differences. Our proposed algorithm, SGP-LRD (Salient Gray Pixels with Local Reflectance Differences), addresses nighttime color constancy through three key design principles: Reliability amplification: Spatially coherent gray regions are sampled repeatedly across overlapping windows, naturally amplifying high-SNR grayness signals. Implicit noise filtering: While spurious pixels from sensor noise appear in limited windows with minimal contribution, genuine gray regions exhibit consistent responses across neighboring windows, naturally distinguishing signal from noise through spatial redundancy. Spatial prior exploitation: The overlapping design encodes the natural scene prior that reliable achromatic surfaces exhibit spatial continuity, ensuring that illumination estimates are dominated by high-confidence information. 3.2. RL-AWB Framework Algorithm Parameters. Two parameters critically determine the performance of our SGP-LRD: the gray pixel candidate selection threshold % and the Minkowski norm exponent p. Lower % for gray-rich scenes (higher purity); raise it when gray cues are sparse (better coverage). Small yields near-uniform weighting; large emphasizes highconfidence pixels, it helpful when detections are reliable but brittle in ambiguous/low-light scenes. In low-light nighttime images, the optimal (N, p) configuration is inherently scene-dependent. We address this challenge through reinforcement learning framework that learns to adaptively select algorithm parameters based on scene characteristics. Pixel-wise Local Reflectance Difference. For each position (i, j) in an image, local window Wi,j of size 3 3 is centered at that pixel. Let fc(x) denote the detected SGP intensity for channel c, and i,j represent the set of nonzero pixels within Wi,j. The pixel-wise normalized local State Design. Differs from RL-AE control [34]: the groundtruth illuminant is unavailable at deployment in RL-AWB and thus cannot appear in the state. We therefore encode rich chromatic statistics without privileged labels and add compact history descriptor for recent adjustments. 4 Illumination-related Features. Following Barron [8], we represent an image with log-chrominance (RGB-uv) histogram Rmm3 (granularity = 60). We then apply ℓ1-normalization to H, followed by element-wise square-root, and flatten it to sWB R3m2 . Parameter-related Features. To capture the trajectory of parameter adjustments, analogous to how humans consider past tuning attempts, we append compact history vector hs R11 that encodes recent actions for both % and p, along with normalized timestep counter. Two-branch Backbone. The full state is st = (sWB, hs). Both actor and critic employ two-branch MLP encoders that map each input to 64-dimensional embeddings zWB and zhist, which are then fused. The actor outputs µ and log σ2 R2 for reparameterized sampling of the two continuous actions, while twin critics compute Q-values with minimum selection to reduce overestimation. Action Design. We use relative, continuous actions to jointly tune the gray-pixel percentage and Minkowski order p: paramt+1 = paramt + at, with param {N, p}. Actions are sampled from the policy, squashed by tanh to [1, 1], and rescaled to valid ranges (e.g., a(N ) [0.6, 0.6], a(p) [4, 4]). This yields smooth updates and coordinated adaptation. Reward Design. We measure the quality of illuminant estimation by the angular error. To stabilize the training across images with different initial errors E0, the main reward signal is the relative error improvement: = arccos (cid:18) ˆe, ˆe (cid:19) , Rerr = E0 Et (cid:18) E0 c1 E0 + ϵ + (cid:19)α , (6) (7) (cid:113) where ϵ = 103, c1 is the average initial error and α = 0.6. To discourage overly large moves, we add an action cost (a1/0.6)2 + (a2/4)2, where λ = 0.1 and Ract = λ a1, a2 control the gray-pixel selection percentage % and Minkowski order p. difficulty-aware relaxation scales the penalty: Rstep = Rerr + (1 E0/c2) Ract, where c2 is the maximum initial error. Episodes stop after three steps of estimation stability. At termination, we add bonus Rρ {+50, +30, +20, +10, 10} for improvement ratio ρ = Et/ max(E0, 1012) in ranges [0, 0.8), [0.8, 0.9), [0.9, 0.95), [0.95, 1.0), 1.0, yielding Rfinal = Rstep + Rρ. Optimization. We adopt the off-policy Soft Actor-Critic (SAC) algorithm [27] with stochastic policy and critics implemented with twin Q-value heads. Figure 3. Sample images from the proposed LEVI dataset with their corresponding Color Checker mask annotations. The dataset captures diverse nighttime scenes with complex mixed lighting, low illumination, and high ISO conditions. 3.3. Curriculum Learning Stage 1: Single-Image Parameter Tuning. Stage 1 uses fixed training image to train the agent on error reduction through sequential parameter adjustments and termination detection. Once behavior stabilizes and the agent reliably stops at convergence, we proceed to Stage 2. Stage 2: Multi-Image Adaptive Tuning. We use curriculum pool Dc = {x1, . . . , xM } (M =5). For each training data xi, the agent runs 5 consecutive episodes, then cycles to xi+1 (wrapping after xM ). This cyclic schedule reduces environment resets, captures short-horizon patterns in the replay buffer, and exploits SACs off-policy experience reuse for stable updates. 4. LEVI Dataset Prior to our work, the NCC dataset [17] was the only public nighttime color constancy benchmark, containing 513 images from single camera. To enable cross-sensor evaluation, we introduce the Low-light Evening Vision Illumination (LEVI) dataset. The first multi-camera nighttime benchmark comprising 700 linear RAW images from two systems: iPhone 16 Pro (images #1370, 43202160, 12-bit) and Sony ILCE-6400 (images #371700, 60004000, 14-bit), with ISO ranging from 500 to 16,000. Each scene contains Macbeth Color Checker with manual annotations. Groundtruth illuminants are computed as median RGB values of non-saturated achromatic patches. All images are black-level corrected and converted to linear RGB. Fig. 3 shows sample images with Color Checker masks; Fig. 4 and Fig. 5 compare the illuminant and luminance distributions of the NCC and LEVI datasets. LEVI complements NCC by cov5 upper bound. Illumination estimation performance is evaluated using the angular error (in degrees) between estimated and ground-truth illuminants, where lower values indicate better estimation accuracy. Each model is evaluated on the complete test set using angular error; results appear in Tab. 1. Among statistical methods, SGP-LRD achieves the lowest errors on both NCC and LEVI. Furthermore, SGP-LRD with RL-AWB parameter tuning outperforms the same algorithm using manually optimized fixed parameters, highlighting the benefit of instance-level parameter tuning over dataset-level optimization. Under the 5-shot setting, RL-AWB attains the best median and mean angular errors, offering superior accuracy-data trade-off in the few-shot regime. Cross-dataset generalization. As summarized in Tab. 2, all existing learning-based baselines suffer from substantial performance drop under cross-dataset evaluation: for both NCCLEVI and LEVINCC, the median and worst-25% errors increase markedly compared to their in-dataset results. This degradation reflects the impact of domain and sensor shift, the two datasets differ in scene content and camera characteristics, making models trained on one dataset difficult to generalize to the other. Rather than directly regressing illuminant RGB as in deep learning-based methods, RLAWB focuses on adaptively tuning the control parameters of SGP-LRD on per-image basis. Illumination estimation is then performed by the underlying statistical model, whose inherent robustness to distribution shifts supports reliable cross-dataset generalization. These results show that combining statistical estimation with reinforcement learning effectively mitigates the severe generalization degradation typical of purely learning-based methods. Fig. 6 shows qualitative results of cross-sensor performance on several nighttime scenes from the NCC and LEVI datasets. Beyond nighttime color constancy, we further examine whether the proposed method generalizes to daytime and indoor scenarios. To adapt SGP-LRD to well-lit daytime datasets, we remove the local variance filtering and color deviation filtering modules, and subsequently perform our RL-based parameter tuning. The evaluation results on the Gehler-Shi dataset [24, 49] are shown in Tab. 3. Despite being tailored for low-light nighttime scenes, RL-AWB achieves state-of-the-art generalization capability compared with other baselines. 5.3. Ablation Studies Figure 4. Illuminant distribution over all the collected nighttime images in the LEVI and NCC datasets. Figure 5. Normalized mean luminance histogram over all the collected nighttime images in the LEVI and NCC datasets. ering broader lighting conditions and containing more lowluminance nighttime images, offering new benchmark for low-light color constancy evaluation. 5. Experiments 5.1. Implementation Details We normalize image resolutions across datasets by downsampling: iPhone 16 Pro captures in LEVI are resized by 0.25, while Sony ILCE-6400 images in LEVI and all NCC images are resized by 0.125. The performance of color constancy methods are evaluated by the standard angular error metric (measured in degrees). RL-AWB (SAC) is trained on an Intel Core i5-13600K CPU. Training batch size is 256, γ = 0.99, τ = 0.005, learning rate 3 104, and 16 parallel environments over 150,000 timesteps, with updates starting after 100 initial steps. 5.2. Results and Comparisons In-domain results. Following our ablation study (Sec. 5.3), we train RL-AWB with only five images per dataset. The same 5-shot protocol is applied to all deep learning baselines (C4 [66], C5 (5) [2], PCC [59]), with an additional fully-trained C5 using its 3-fold protocol (C5 (full)) as an Effect of training data number. We vary the Stage-2 curriculum pool size {3, 5, 7, 9, 15} (Tab. 4). We adopt = 5 as the best trade-off. RL adjustment trajectories. Fig. 7 shows stepwise corrections, as the agent updates SGP-LRD parameters, outputs approach the white-balanced target. Table 1. In-dataset evaluation results on the NCC and LEVI datasets. Angular error in degrees. NCC Dataset LEVI Dataset Method Median Mean Tri-mean B-25% W-25% Median Mean Tri-mean B-25% W-25% Statistical GE-1st [55] GE-2nd [55] Mean Shift [47] GI [46] BCC [54] RGP [17] SGP-LRD (Ours) Learning-based 4 [66] 5 (default) [2] 5 (5) [2] PCC [59] RL-AWB (Ours) 4.14 3.58 2.48 3.13 3.06 2.22 2.12 6.24 5.74 5.56 4.65 1.98 5.17 4.64 3.52 4.52 3.81 3.33 3.11 7.88 6.52 7.11 5.77 3.07 4.35 3.78 2.70 3.40 3.23 2.44 2. 6.81 5.83 6.05 5.00 2.24 1.25 1.11 0.80 0.91 1.05 0.68 0.68 1.25 1.36 1.91 1.35 0.69 10.87 9.93 8.02 10.60 7.78 7.81 7.22 17.42 13.29 14.66 12.03 7.22 3.94 4.17 3.12 3.10 4.23 3.21 3. 7.01 4.43 2.46 4.01 3.01 4.31 4.49 3.34 3.42 4.53 3.56 3.25 8.22 4.84 3.50 5.18 3.22 3.97 4.19 3.14 3.15 4.28 3.29 3.07 7.37 4.49 2.56 4.25 3.03 1.82 1.80 1.54 1.49 2.52 1.63 1. 2.76 1.89 1.08 1.48 1.43 7.45 7.76 5.52 5.91 7.06 6.12 5.46 15.75 8.60 7.80 10.91 5.32 Table 2. Cross-dataset evaluation results between the NCC and LEVI datasets. Angular error in degrees. Train Test NCC LEVI LEVI NCC"
        },
        {
            "title": "Mean",
            "content": "Tri-mean B-25% W-25% Median"
        },
        {
            "title": "Mean",
            "content": "Tri-mean B-25% W-25% 4 [66] 5 (5) [2] 5 (full) [2] PCC [59] RL-AWB (Ours) 13.18 9.40 9.12 20.69 3.03 13.52 10.93 11.65 19.37 3.24 13.18 9.36 9.85 19.92 3. 8.48 4.36 3.76 9.47 1.45 19.40 20.61 23.39 27.44 5.36 13.98 11.38 4.47 9.81 1.99 16.68 13.11 5.46 10.85 3.12 15.28 11.81 4.68 10.37 2.25 8.40 4.48 1.70 4.34 0. 28.19 24.33 10.88 18.38 7.39 Note. C5(5) and C5(full) are both trained using the official implementation [2]. C5(5) denotes the few-shot setting with only five training images per dataset, whereas C5(full) follows the original 3-fold protocol using all available training images in the datasets. Table 3. Evaluation results on the Gehler-Shi dataset. Angular error in degrees. 4, 5, and the proposed RL-AWB are trained on the NCC dataset and evaluated on the GehlerShi dataset. Compared with our SGP-LRD, the proposed RL-AWB framework achieves reduction of 5.9% in the median angular error and 9.8% in the best-25% angular error, showing that RL-AWB generalizes well across low-light and well-illuminated images."
        },
        {
            "title": "Mean",
            "content": "B-25% W-25% 2.29 2.73 2.25 5.62 3.34 2.38 2.24 3.82 3.65 3.76 6.52 3.97 3.64 3.50 0.49 0.91 0.52 2.43 1. 0.51 0.46 9.61 8.00 9.38 11.97 7.80 8.89 8."
        },
        {
            "title": "Method",
            "content": "GI [46] BCC [54] RGP [17] 4 [66] 5 [2] SGP-LRD (Ours) RL-AWB (Ours) 6. Conclusion Table 4. Ablation on curriculum pool size (SAC). Ablation on the curriculum pool size exhibits U-shaped performance trend. Small pools (< 5 images) lack sufficient diversity to learn robust policies, while large pools ( > 5 images) reduce per-sample visitation under fixed replay budget, leading to excessive exploration noise. NCC Dataset LEVI Dataset Median Mean Worst 25% Median Mean Worst 25% 3 5 7 9 15 2.16 1.98 2.09 2.13 2.24 3.29 3.07 3.19 3.23 3.21 7.69 7.22 7.54 7.63 7.47 3.05 3.01 3.04 3.03 3. 3.28 3.22 3.28 3.23 3.24 5.55 5.32 5.53 5.39 5.41 Table 5. Ablation study on DRL algorithms (5 training images). NCC Dataset LEVI Dataset Median Mean Worst 25% Median Mean Worst 25% PPO SAC 2.16 1.98 3.20 3.07 7.51 7.22 3.09 3.01 3.27 3. 5.49 5.32 This study is the first to apply reinforcement learning to color constancy, demonstrating that DRL can be effectively used for white balance tuning. Our work makes three contributions: (1) SGP-LRD, novel statistical algorithm for nighttime color constancy, (2) RL-AWB, reinforcement learning framework for adaptive parameter optimization, and (3) LEVI, the first multi-camera nighttime color constancy 7 Figure 6. Qualitative comparison of cross-dataset performance. Images are gamma-corrected for visualization. Top: train on LEVI, test on NCC; bottom: train on NCC, test on LEVI. Learning-based methods degrade under cross-dataset shift, while RL-AWB remains stable. Figure 7. Illustration of the RL-AWB auto-tuning process for representative nighttime scenes. For each image, we visualize the initial input, several intermediate correction results along the trajectory of the RL policy, and the final output, together with the corresponding angular error at each step. As the agent iteratively updates the SGP-LRD parameters, the corrected images gradually approach the groundtruth white-balanced results, and the angular error decreases. Note that the images shown are gamma-corrected for visualization. dataset. Experiments on both nighttime and daytime datasets demonstrate that the proposed method achieves competitive performance compared to existing baselines, while exhibiting strong cross-sensor robustness. 8 Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628E-A49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Afifi and Michael Brown. Deep white-balance In Proceedings of the IEEE/CVF Conference on editing. computer vision and pattern recognition, pages 13971406, 2020. 2 [2] Mahmoud Afifi, Jonathan Barron, Chloe LeGendre, Yun-Ta Tsai, and Francois Bleibel. Cross-camera convolutional color constancy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19811990, 2021. 2, 3, 6, 7, 15, 16 [3] Mahmoud Afifi, Marcus Brubaker, and Michael Brown. Auto white-balance correction for mixed-illuminant scenes. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 12101219, 2022. 2 [4] Mahmoud Afifi, Zhenhua Hu, and Liang Liang. Optimizing illuminant estimation in dual-exposure hdr imaging. In European Conference on Computer Vision, pages 202219. Springer, 2024. 3 [5] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. 3 [6] Min Bai and Raquel Urtasun. Deep watershed transform for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52215229, 2017. 3 [7] Chandrajit Bajaj, Yunhao Yang, and Yi Wang. Reinforcement learning of self-enhancing camera image and signal processing. In Advances in Data-driven Computing and Intelligent Systems: Selected Papers from ADCIS 2022, Volume 2, pages 281303. Springer, 2023. [8] Jonathan Barron. Convolutional color constancy. In Proceedings of the IEEE International Conference on Computer Vision, pages 379387, 2015. 2, 5 [9] Jonathan Barron and Yun-Ta Tsai. Fast fourier color constancy. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 886894, 2017. 2 [10] Gershon Buchsbaum. spatial processor model for object colour perception. Journal of the Franklin institute, 310(1): 126, 1980. 2 [11] Marco Buzzelli and Simone Bianco. Uncertainty estimation in color constancy. Pattern Recognition, 160:111175, 2025. 2 [12] Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, Radu Timofte, and Yulun Zhang. Retinexformer: One-stage retinexbased transformer for low-light image enhancement. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1250412513, 2023. 2 [13] Chen-Wei Chang, Cheng-De Fan, Chia-Che Chang, Yi-Chen Lo, Yu-Chee Tseng, Jiun-Long Huang, and Yu-Lun Liu. Gcc: Generative color constancy via diffusing color checker. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1086810878, 2025. 2 [14] Ke-Chi Chang, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, ChiaPing Chen, Yu-Lin Chang, and Hwann-Tzong Chen. Learning In European Conference on camera-aware noise models. Computer Vision, pages 343358. Springer, 2020. 2 [15] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. LearnIn Proceedings of the IEEE coning to see in the dark. ference on computer vision and pattern recognition, pages 32913300, 2018. [16] Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Wen-Hsiao Peng, and Yen-Yu Lin. Learning continuous exposure value representations for single-image hdr reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1299013000, 2023. 2 [17] Cheng Cheng, Kai-Fu Yang, Xue-Mei Wan, Leanne Lai Hang Chan, and Yong-Jie Li. Nighttime color constancy using robust gray pixels. Journal of the Optical Society of America A, 41(3):476488, 2024. 2, 5, 7, 15 [18] Dongliang Cheng, Dilip Prasad, and Michael Brown. Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution. Journal of the Optical Society of America A, 31(5):10491058, 2014. 2, 3 [19] Marcos Conde, Steven McDonagh, Matteo Maggioni, Ales Leonardis, and Eduardo Perez-Pellitero. Model-based image signal processors via learnable dictionaries. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 481 489, 2022. 3 [20] Xiaodong Cun, Zhendong Wang, Chi-Man Pun, Jianzhuang Liu, Wengang Zhou, Xu Jia, and Houqiang Li. Learning enriched illuminants for cross and single sensor color constancy. arXiv preprint arXiv:2203.11068, 2022. 3 [21] Marc Ebner. Color constancy based on local space average color. Machine Vision and Applications, 20(5):283301, 2009. [22] Graham Finlayson and Elisabetta Trezzi. Shades of gray and colour constancy. In Color and imaging conference, pages 3741. Society of Imaging Science and Technology, 2004. 2 [23] Ryosuke Furuta, Naoto Inoue, and Toshihiko Yamasaki. Pixelrl: Fully convolutional network with reinforcement learning for image processing. IEEE Transactions on Multimedia, 22 (7):17041719, 2019. 2 [24] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom Minka, and Toby Sharp. Bayesian color constancy revisited. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 18. IEEE, 2008. 6 [25] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Improving color constancy by photometric edge weighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(5):918929, 2011. 2 [26] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference 9 deep curve estimation for low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17801789, 2020. [27] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. International Conference on Machine Learning (ICML), 2018. 5 [28] Yuanming Hu, Baoyuan Wang, and Stephen Lin. Fc4: Fully convolutional color constancy with confidence-weighted pooling. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 40854094, 2017. 2 [29] Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, and Eric Xing. Efficient test-time adaptation of vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1416214171, 2024. 3 [30] Dongyoung Kim, Mahmoud Afifi, Dongyun Kim, Michael Brown, and Seon Joo Kim. Ccmnet: Leveraging calibrated color correction matrices for cross-camera color constancy. arXiv preprint arXiv:2504.07959, 2025. 2, 3 [31] Samu Koskinen, Erman Acar, and Joni-Kristian Kamarainen. Single pixel spectral color constancy. International Journal of Computer Vision, 132(2):287299, 2024. 2 [32] Satoshi Kosugi and Toshihiko Yamasaki. Unpaired image enhancement featuring reinforcement-learning-controlled image editing software. In Proceedings of the AAAI conference on artificial intelligence, pages 1129611303, 2020. 2 [33] Firas Laakom, Nikolaos Passalis, Jenni Raitoharju, Jarno Nikkanen, Anastasios Tefas, Alexandros Iosifidis, and Moncef Gabbouj. Bag of color features for color constancy. IEEE Transactions on Image Processing, 29:77227734, 2020. [34] Kyunghyun Lee, Ukcheol Shin, and Byeong-Uk Lee. Learning to control camera exposure via reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29752983, 2024. 2, 4 [35] Jiacheng Li, Chang Chen, Wei Huang, Zhiqiang Lang, Fenglong Song, Youliang Yan, and Zhiwei Xiong. Learning steerable function for efficient image resampling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 58665875, 2023. 3 [36] Shuwei Li and Robby Tan. Nightcc: Nighttime color conIn Proceedings of stancy via adaptive channel masking. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2552225531, 2024. 2 [37] Shuwei Li, Jikai Wang, Michael Brown, and Robby Tan. Transcc: Transformer-based multiple illuminant color constancy using multitask learning. CoRR, 2022. 2 [38] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808 21820, 2021. 3 [39] Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Single-image hdr reconstruction by learning to reverse the camera pipeline. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1651 1660, 2020. 3 [40] Yi-Chen Lo, Chia-Che Chang, Hsuan-Chao Chiu, Yu-Hao Huang, Chia-Ping Chen, Yu-Lin Chang, and Kevin Jou. Clcc: Contrastive learning for color constancy. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 80538063, 2021. [41] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and robust low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 56375646, 2022. 2 [42] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: framework and survey. Journal of Machine Learning Research, 21(181):150, 2020. 3 [43] Jongchan Park, Joon-Young Lee, Donggeun Yoo, and In So Kweon. Distort-and-recover: Color enhancement using deep In Proceedings of the IEEE conreinforcement learning. ference on computer vision and pattern recognition, pages 59285936, 2018. 2 [44] Ruo Peng and Chenye Wu. Cccg: Self-supervised color constancy with collaborative generative network. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, pages 20552060, 2024. 2 [45] Abhijith Punnappurath, Abdullah Abuolaim, Abdelrahman Abdelhamed, Alex Levinshtein, and Michael Brown. Dayto-night image synthesis for training nighttime neural isps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1076910778, 2022. 2 [46] Yanlin Qian, Joni-Kristian Kamarainen, Jarno Nikkanen, and In Proceedings of Jiri Matas. On finding gray pixels. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80628070, 2019. 2, 7, 15 [47] Yanlin Qian, Said Pertuz, Jarno Nikkanen, Joni-Kristian Kamarainen, and Jiri Matas. Revisiting gray pixel for statistical illumination estimation. In Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP), pages 3646, 2019. 4, 7, [48] Tom Schaul, John Quan, Ioannis Antonoglou, and David arXiv preprint Prioritized experience replay. Silver. arXiv:1511.05952, 2015. 3 [49] Lilong Shi and Brian Funt. Re-processed version of the gehler color constancy dataset of 568 images. Simon Fraser University, 2010. Technical Report. 6 [50] Xinyu Sun, Zhikun Zhao, Lili Wei, Congyan Lang, Mingxuan Cai, Longfei Han, Juan Wang, Bing Li, and Yuxuan Guo. Rlseqisp: Reinforcement learning-based sequential optimization In Proceedings of the AAAI for image signal processing. Conference on Artificial Intelligence, pages 50255033, 2024. 2 [51] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with selfsupervision for generalization under distribution shifts. In International conference on machine learning, pages 9229 9248. PMLR, 2020. [52] Yuxiang Tang, Xuejing Kang, Chunxiao Li, Zhaowen Lin, and Anlong Ming. Transfer learning for color constancy via 10 In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1272512732, 2020. 2, 6, 7, 15, 16 [67] Runsheng Yu, Wenyu Liu, Yasen Zhang, Zhi Qu, Deli Zhao, and Bo Zhang. Deepexposure: Learning to expose photos with asynchronously reinforced adversarial learning. Advances in neural information processing systems, 31, 2018. 2 [68] Shuwei Yue and Minchen Wei. Effective cross-sensor color constancy using dual-mapping strategy. Journal of the Optical Society of America A, 41(2):329337, 2024. 3 [69] Jian Zhang and Bernard Ghanem. Ista-net: Interpretable optimization-inspired deep network for image compressive sensing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18281837, 2018. 3 [70] Shuyang Zhang, Jinhao He, Yilong Zhu, Jin Wu, and Jie Yuan. Efficient camera exposure control for visual odometry via deep reinforcement learning. IEEE Robotics and Automation Letters, 2024. 2 [71] Zhifeng Zhang, Xuejing Kang, and Anlong Ming. Domain In IJCAI, pages adversarial learning for color constancy. 16931699, 2022. 3 [72] Ming Zhao, Pingping Liu, Tongshun Zhang, and Zhe Zhang. Ref-lle: Personalized low-light enhancement via reference-guided deep reinforcement learning. arXiv preprint arXiv:2506.22216, 2025. 2 [73] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and deblurring in the dark. In European conference on computer vision, pages 573589. Springer, 2022. statistic perspective. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 23612369, 2022. 3 [53] Stone Tao, Arth Shukla, Tse-kai Chan, and Hao Su. Reverse forward curriculum learning for extreme sample and demonstration efficiency in reinforcement learning. arXiv preprint arXiv:2405.03379, 2024. 3 [54] Oguzhan Ulucan, Devrim Ulucan, and Marc Ebner. Blockbased color constancy: The deviation of salient pixels. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 7, 15 [55] Joost Van De Weijer, Theo Gevers, and Arjan Gijsenij. Edgebased color constancy. IEEE Transactions on image processing, 16(9):22072214, 2007. 2, 7, 15 [56] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 3 [57] Yujin Wang, Tianyi Xu, Zhang Fan, Tianfan Xue, and Jinwei Gu. Adaptiveisp: Learning an adaptive image signal processor for object detection. Advances in Neural Information Processing Systems, 37:112598112623, 2024. 2 [58] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. [59] Yue Wei, Jicheng Liu, Shuwei Zhang, Kai-Hong Yue, Li Li, and Hongmin Cai. Color constancy from pure color view. Journal of the Optical Society of America (JOSA A), 40(3): 602610, 2023. 6, 7, 15, 16 [60] Yuhui Wu, Chen Pan, Guoqing Wang, Yang Yang, Jiwei Wei, Chongyi Li, and Heng Tao Shen. Learning semantic-aware knowledge guidance for low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16621671, 2023. 2 [61] Jin Xiao, Shuhang Gu, and Lei Zhang. Multi-domain learning for accurate and few-shot color constancy. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 32583267, 2020. 3 [62] Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, and Meie Fang. Boosting illuminant estimation in deep color constancy through brightness robustness enhancement. Pattern Recognition, page 112153, 2025. 2 [63] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia. Snr-aware low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1771417724, 2022. 2 [64] Huan Yang, Baoyuan Wang, Noranart Vesdapunt, Minyi Guo, and Sing Bing Kang. Personalized exposure control using adaptive metering and reinforcement learning. IEEE transactions on visualization and computer graphics, 25(10):2953 2968, 2018. [65] Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang, and Jiayi Ma. Diff-retinex: Rethinking low-light image enhancement with generative diffusion model. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12302 12311, 2023. 2 [66] Huanglin Yu, Ke Chen, Kaiqi Wang, Yanlin Qian, Zhaoxiang Zhang, and Kui Jia. Cascading convolutional color constancy. 11 A. Overview This document provides supplementary material for the paper RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes to complement the main manuscript. Sections and provide comprehensive implementation details of the proposed color constancy method, SGP-LRD, and the RL-AWB framework, respectively. Section offers detailed description of the proposed night-time color constancy dataset, LEVI. Additional quantitative results are reported in Section E, while further ablation studies are presented in Section F. Finally, Section concludes the supplementary material. B. Details of SGP-LRD Following the initial gray pixel selection, we employ twolayer filtering process to refine the candidate set, aiming to mitigate the adverse effects of sensor noise and chromatic outliers, which are prevalent in low-light imagery: Noise Mitigation via Local Variance Filtering. The first stage addresses the issue of pure noise, which can often produce achromatic-like responses in extremely dark areas. For each pixel (x,y) in the initial detected gray pixel mask, we compute the variance across the logarithmic RGB channels. Pixels where the intra-pixel variance is too small often lack reliable color signal and are primarily sensor noise. By applying lower bound threshold, arT h, we filter out these unreliable candidates: (cid:40) Mask1(x, y) = if Var{log(I(x, y))} > arT 1, 0, otherwise (8) Chromatic Outlier Elimination via Color Difference Filtering. This stage aims to filter out initial detected gray pixels that are too distant from the dominant color cast of the scenes illumination. We first compute the mean logarithmic intensity for each channel of the image: = [ MR, MG, MB]T, and then calculate the maximum absolute color deviation, X, for each pixel from this mean: X(x, y) = max i{R,G,B} (cid:0) log(Ii(x, y)) Mi(cid:1) (9) Applying these two refinement layers to the initially detected gray pixels yields the Salient Gray Pixels (SGP). Gray-pixel Confidence Weighting. fundamental challenge in nighttime color constancy is that pixels in underexposed areas suffer from low SNR, compromising the reliability of their color measurements. To address this spatiallyvarying reliability, we introduce luminance-adaptive confidence measure that weights gray-pixel candidates according to their local signal quality. For given image with bit depth representation, we first normalize the pixel values to the range [0,1], and then the luminance map for SGPs is computed as: LM (x) = R(x) + G(x) + B(x) 3 (12) We analyze the intensity distribution characteristics through skewness calculation on non-zero pixels. The skewness value sLM guides the selection of an adaptive exponent parameter E: = 1.0 2.0 4.0 if sLM > 1.5 if 0.2 < sLM 1.5 if sLM 0. (13) This adaptive selection responds to different scene brightness distributions: higher skewness indicates more lowintensity pixels, while lower skewness suggests more uniform illumination. The gray-pixel confidence weight is then computed as: WSGP (x) = 1 exp (cid:34) (cid:19)E (cid:35) (cid:18) LM (x) LM (14) Pixel-wise Local Reflectance Difference. We design pixel-wise sliding window approach for local normalization. For each pixel position (i, j), we define local window Ni,j of size centered at that pixel. Let fc(x) denote the detected SGPs intensity for channel c. We denote i,j as the set of non-zero pixels within the local window Wi,j. The pixel-wise normalized local reflectance difference at position (i, j) is defined as: To discard pixels that are outliers relative to the dominant scene color, we define an adaptive threshold TC. Any initial detected gray pixel exceeding this deviation is removed: i,j = TC = ColorT min(M) Mask2(x, y) = (cid:40) if X(x, y) TC 1, 0, otherwise (10) (11) 12 (cid:35) / i,j (cid:34) fc(x) (cid:80) xW i,j max xWi,j fc(x) if max xWi,j fc(x) > 0 0 otherwise (15) where the numerator represents the mean of non-zero elements in the local window, computed as the sum of non-zero pixels divided by their count i,j, and the denominator is the maximum value within the window. This formulation ensures that each pixels local context is normalized by its surrounding maximum, providing spatially adaptive measure of local reflectance variation. Integrating the gray-pixel confidence weights into the estimation framework, we formulate the illuminant estimation as: ˆec = (cid:80) (i,j)Ω (cid:80) (i,j)Ω (cid:0)µi,j (cid:16) i,j WSGP (i, j)(cid:1)p (cid:17)p WSGP (i, j) 1/p (16) where Ω represents all valid pixel positions (where i,j > 0), (i, j) is the gray-pixel confidence weight at position (i, j), and is the Minkowski norm parameter. The graypixel confidence weight (i, j) ensures that pixels in wellilluminated, reliable regions contribute more to the final estimate, while uncertain or poorly-lit regions have reduced influence. Finally, the estimated illuminant vector is then normalized: ˆe = (ˆeR, ˆeG, ˆeB) (ˆeR, ˆeG, ˆeB) (17) The pixel-wise sliding window strategy is designed to exploit the spatial distribution characteristics of reliable SGPs in natural scenes. C. Details of RL-AWB Python Environment Setting. We implement RL-AWB in Python, using Stable-Baselines3 to construct and train the SAC agent, and PyTorch to define custom policy and value networks. The SAC networks are implemented as dualhead multi-layer perceptrons (MLPs) built mainly from fully connected layers. During SAC training, we rely solely on the CPU. Our experiments are run on machine equipped with an Intel Core i5-13600K processor and 40 GB of system memory. In contrast, during environment interaction, the input images are processed by the SGP-LRD algorithm, for which we leverage single NVIDIA RTX 3080 GPU with 10 GB of VRAM to accelerate the computations. Soft Actor-Critic (SAC) Algorithm Optimization. We adopt Soft Actor-Critic (SAC) to optimize the parameters of our AWB algorithm. SAC is an off-policy deep reinforcement learning method whose core idea is to jointly maximize the expected return and the policy entropy, thereby encouraging exploration and improving training stability. It follows an actorcritic architecture with two critic networks Qθ1 , Qθ2 and stochastic actor πϕ."
        },
        {
            "title": "The critic loss is defined as",
            "content": "JQ(θi) = E(st,at,rt,st+1)D (cid:104) 1 2 (cid:0)Qθi(st, at) yt (cid:1)2(cid:105) , (18) {1, 2}. where denotes the replay buffer. The target value yt is given by yt = rt + γ Eat+1πϕ (cid:104) min j{1,2} Qθj (st+1, at+1) (cid:105) α log πϕ(at+1 st+1) . (19) Here, γ (0, 1) is the discount factor, α is the temperature parameter that controls the strength of exploration, and rt, st, at are the reward, state, and action at time step t, respectively. Using min(Qθ1, Qθ2) in the target mitigates overestimation of the Q-value and stabilizes learning. The entropy term α log πϕ(at+1 st+1) encourages the policy to remain sufficiently stochastic."
        },
        {
            "title": "The actor is updated by minimizing",
            "content": "Jπ(ϕ) = EstD, εN (cid:2)α log πϕ(at st)Qθ(st, at)(cid:3) (20) where at is sampled via the reparameterization trick from noise variable ε, and Qθ denotes slowly updated target critic. This objective strikes balance between achieving high Q-values and preserving sufficient randomness for exploration: if certain action yields large Q-value, the actor increases its probability; however, when the policy becomes overly deterministic, the entropy term acts as regularizer that penalizes low-entropy behavior and enforces diversity in the actions. D. Datasets Low-light Evening Vision Illumination (LEVI) Dataset. Given the limited availability of nighttime color constancy datasets, we constructed new dataset to facilitate research in this challenging domain. Some samples are shown in Fig. 8. Prior to our work, the NCC dataset was the only publicly available dataset specifically designed for nighttime illuminant estimation, containing 513 nighttime images with corresponding ground-truth illuminants. While the NCC dataset has been valuable for initial algorithmic development, it was captured using single camera model, limiting its utility for evaluating cross-sensor generalization, critical requirement for practical AWB systems that must operate across diverse imaging devices. Our dataset addresses this limitation and introduces several key improvements. First, it is the first nighttime color constancy dataset captured with multiple camera systems, enabling rigorous evaluation of cross-sensor generalization performance. The dataset comprises 700 nighttime images: images #1#370 were captured using an iPhone 16 Pro at resolution of 4320 Figure 8. Example nighttime scenes from the LEVI dataset. Our LEVI dataset covers diverse nighttime environments and illuminant conditions across multiple camera sensors. 2160 pixels with 12-bit depth, while images #371#700 were captured using Sony ILCE-6400 at 6000 4000 pixels with 14-bit depth. The ISO values range from approximately 500 to 16,000, covering wide spectrum of low-light conditions commonly encountered in nighttime photography. Second, to ensure accurate ground-truth illuminant estimation, standard Macbeth Color Checker was placed in each scene. Unlike datasets where illuminants are estimated indirectly or from limited reference points, our dataset provides high-precision, manually annotated color checker masks for every image. The ground-truth illuminant for each image is computed as the median RGB values of the non-saturated achromatic patches on the color checker, ensuring that the provided illuminants reliably represent the dominant scene illumination. All images are preprocessed by correcting for black level and converting to linear RGB space to facilitate algorithmic development. Third, we provide comprehensive metadata for each image, including focal length (mm), F-number, exposure time (s), and ISO settings. This additional information enables researchers to analyze the relationship between camera settings and illuminant estimation performance, potentially leading to more robust AWB algorithms that can adapt to different capture conditions. The combination of multi-camera acquisition, precise ground-truth annotations, and detailed metadata makes our dataset valuable resource for advancing nighttime color constancy research. E. Additional Quantitative Results E.1. Reproduction Angular Error In addition to the recovery angular error used in the main paper, we also adopt the reproduction angular error to evaluate the discrepancy between the predicted illuminant ˆe and the ground-truth illuminant e. The recovery angular error measures the directional error of illuminant estimation, whereas the reproduction angular error assesses whether neutral gray surface is reproduced as neutral after applying the estimated gains. In practice, we first compute the per-channel ratio vector LL = , (21) ˆe and then measure the angle between LL and the ideal white vector [1, 1, 1]: rep = arccos (cid:18) LL, [1, 1, 1] (cid:19) . (22) 3 LL2 Smaller angles indicate that the corrected image is closer to perceptual white. These two metrics complement each other: recovery angular error reflects the accuracy of illuminant 14 Table 6. In-dataset evaluation results on the NCC and LEVI datasets. Reproduction angular error in degrees. NCC Dataset LEVI Dataset Method Median Mean Tri-mean B-25% W-25% Median Mean Tri-mean B-25% W-25% Statistical GE-1st [55] GE-2nd [55] Mean Shift [47] GI [46] BCC [54] RGP [17] SGP-LRD (Ours) Learning-based 4 [66] 5 (default) [2] 5 (5) [2] PCC [59] RL-AWB (Ours) 5.51 4.92 3.44 4.13 4.01 3.12 2.92 8.91 7.68 7.52 6.01 2.71 6.64 6.05 4.69 5.80 4.91 4.51 4.15 9.82 9.55 8.83 5.18 4. 5.69 5.15 3.62 4.47 4.18 3.38 3.09 8.96 8.35 7.88 6.28 3.04 1.69 1.52 1.10 1.23 1.36 0.94 0.97 1.75 1.57 2.52 1.70 0.97 13.46 12.58 10.46 13.09 9.87 10.39 9.53 19.88 20.28 17.19 15.10 9. 5.40 5.78 5.20 5.11 5.38 5.35 5.10 10.57 5.99 3.64 6.28 5.07 5.96 6.14 5.64 5.71 5.87 6.07 5.62 12.08 6.51 4.78 7.37 5.60 5.53 5.80 5.27 5.24 5.49 5.47 5.16 11.22 6.03 3.75 6.48 5. 2.56 2.67 2.53 2.50 3.20 2.70 2.24 3.88 2.64 1.48 2.23 2.31 10.26 10.36 9.55 9.91 9.38 10.67 9.92 22.51 11.39 10.24 14.46 9.80 estimation, while reproduction angular error reflects the perceptual quality after AWB correction. In the main paper, we report the primary comparisons using the recovery angular error; in this supplementary material, we additionally provide results under the reproduction angular error to complete the performance evaluation. E.2. In-dataset Quantitative Comparison Table 6 reports in-dataset reproduction angular errors on the NCC and LEVI datasets. Among statistical methods, SGP-LRD achieves the lowest errors on NCC across all statistics (median, mean, tri-mean, and best/worst 25%), and also attains the best or near-best performance on LEVI. This confirms the robustness of SGP-LRD for nighttime color estimation. For learning-based approaches, our RL-AWB clearly outperforms C4, C5, and PCC on NCC, achieving the lowest median, tri-mean, and worst-25% errors, which indicates both strong average performance and improved stability on challenging scenes. On LEVI, RL-AWB obtains reproduction angular errors comparable to the best C5(5) configuration while remaining competitive on the worst-25% statistic. Overall, the combination of SGP-LRD and RL-AWB yields strong and interpretable nighttime AWB solution when trained and evaluated within the same dataset. E.3. Cross-dataset Generalization Comparison Table 7 summarizes the cross-dataset reproduction angular errors when training on one dataset and testing on the other. When trained on NCC and evaluated on LEVI, existing learning-based methods (C4, C5, PCC) suffer from noticeable degradation in both median and worst-25% errors. In contrast, RL-AWB substantially reduces the error and achieves the best performance across all reported statistics. The opposite direction, training on LEVI and testing on NCC, shows the same trend: RL-AWB consistently attains the lowest median, tri-mean, and worst-25% errors among all competing methods. These results indicate that, despite being trained on single dataset, RL-AWB generalizes well across sensors and scene distributions, providing stable nighttime white balance on unseen datasets and clearly outperforming fully supervised learning-based baselines. F. Additional Ablation Studies Effect of model architecture. We then study the impact of the backbone architecture by comparing single-branch and dual-branch designs under the same SAC configuration. As shown in Table 8, the dual-branch variant consistently achieves lower errors. This is because our state comprises not only high-dimensional WB-sRGB histogram (10 800 dimensions) but also low-dimensional adjustment history (11 dimensions) encoding recent parameter values and the current step index. In single-branch network, directly concatenating these two parts tends to dilute the influence of the low-dimensional signals. The dual-branch design, on the other hand, processes the histogram and history through separate MLPs to obtain two 64-dimensional embeddings, which are then concatenated and fused. This structure preserves the adjustment-related information more effectively, leading to better AWB parameter updates. G. Future Work We plan to extend RL-AWB in several directions. First, the current agent controls only two AWB parameters, whereas the underlying SGP-LRD pipeline exposes multiple tunable parameters. Naively expanding the action space would sub15 Table 7. Cross-dataset evaluation between NCC and LEVI. Reproduction angular error in degrees. Train Test NCC LEVI LEVI NCC Method Median Mean Tri-mean B-25% W-25% Median Mean Tri-mean B-25% W-25% 4 [66] 5 (5) [2] 5 (full) [2] PCC [59] RL-AWB (Ours) 15.09 11.34 10.30 18.60 5.10 15.85 12.55 12.44 19.66 5.62 15.19 11.30 10.88 18.42 5.19 8.48 5.36 4.20 10.79 2.35 24.55 22.15 24.26 31.03 9.80 13.80 13.08 5.91 11.52 2. 16.08 14.38 7.78 11.61 4.19 14.79 13.57 6.51 11.22 3.10 8.62 5.08 1.95 4.48 0.97 26.03 25.45 16.37 19.61 9.66 Note. C5(5) and C5(full) are both trained using the official implementation [2]. C5(5) denotes the few-shot setting with only five training images per dataset, whereas C5(full) follows the original 3-fold protocol using all available training images in the datasets. Table 8. Ablation study on network architecture (settings: SAC algorithm, 5 training images). NCC Dataset LEVI Dataset Median Mean Worst 25% Median Mean Worst 25% Single Dual 2.11 1.98 3.25 3. 7.67 7.22 3.06 3.01 3.29 3.22 5.48 5.32 stantially increase training complexity and cost. To address this, we plan to investigate structured and hierarchical policies, as well as low-dimensional latent action representations, to efficiently coordinate multiple ISP parameters. Second, while RL-AWB consistently reduces overall angular error, it may still over-correct small number of challenging nighttime scenes, resulting in visually degraded outputs. Future work will therefore explore safety-aware reward formulations and constrained optimization strategies, such as penalizing abrupt parameter changes or incorporating preference-based regularization, to explicitly mitigate such failure cases. Third, the current implementation combines GPU-accelerated environment simulation with CPU-based reinforcement learning updates. Moving toward fully GPUresident training pipeline with batched rollouts could further reduce wall-clock training time, and enable joint optimization across nighttime and daytime data, ultimately facilitating unified all-time AWB agent."
        }
    ],
    "affiliations": [
        "MediaTek Inc.",
        "National Taiwan University",
        "National Yang Ming Chiao Tung University"
    ]
}