{
    "paper_title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "authors": [
        "Xinyu Liu",
        "Hangjie Yuan",
        "Yujie Wei",
        "Jiazheng Xing",
        "Yujin Han",
        "Jiahao Pan",
        "Yanbiao Ma",
        "Chi-Min Chan",
        "Kang Zhao",
        "Shiwei Zhang",
        "Wenhan Luo",
        "Yike Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 4 2 9 9 0 . 2 1 5 2 : r ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning Xinyu Liu1,6, Hangjie Yuan2*, Yujie Wei3,6, Jiazheng Xing2, Yujin Han4,6, Jiahao Pan1, Yanbiao Ma5, Chi-Min Chan1, Kang Zhao6, Shiwei Zhang6, Wenhan Luo1, Yike Guo1 1HKUST 2ZJU 3FDU 4HKU 5RUC 6Tongyi Lab"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reasoninformed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within single architecture. The models internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generators reasoning behavior during training. Extensive experiments on RVEBench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving 32% improvement of Overall score in reasoning-informed video editing subset over state-of-the-art methods. Our code and models are available at: https://github.com/Liuxinyv/ ReViSE *Project leader. Corresponding author. 1 Instruction-guided video editing [1, 7, 8, 33, 42, 47, 58] aims to modify source video according to given textual instruction. Recent unified generation models [12, 30, 35, 56] have shown impressive editing capabilities, yet most existing approaches remain limited to literal transformations such as adding, removing, or replacing visual elements. They often struggle with reason-informed editing that require understanding of the underlying processes or physical dynamics within the scene. As shown in Figure 1, while prior editing models are capable of literal tasks like remove the boat, they fail on understanding Imagine the scene an hour after the boat departed., which requiring temporal reasoning. This type of command remains significant challenge for unified video generation models. To address this, we introduce the Reason-Informed Video Editing (RVE) task. RVE challenges models to move beyond simple pattern recognition by integrating implicit reasoning about physical plausibility and causal dynamics. Consequently, it demands edits that are not only semantically faithful to the instruction but also logically and physically coherent with the scenes context. To systematically evaluate the RVE task, we introduce RVE-Bench, benchmark designed to probe two distinct subsets of RVE samples, as shown in Figure 2. The first subset, termed Reasoning-Informed Video Editing, challenges models ability to to incorporate implicit reasoning and world knowledge, essential for understanding and generating realistic video content. For example, the command Let the buildings facade reflect the cool tones of the sky requires the model to leverage knowledge of environmental lighting and material properties to produce plausible visual transformation. The second subset, In-Context Video Generation, assesses models capacity to parse and execute complex instructions within rich context. For instance, the instruction Imagine if the man in the red hoodie... decided to sit down and join the conversation. hints the mans motivation, which requires the models reasoning and contextual generation capabilities to predict next Figure 1. Current video unified models excel at understanding tasks with powerful internal VLM, but significant gap remains in their ability to guide generator for reason-informed editing. Our ReViSE bridges this gap by enabling the VLM to produce self-corrective feedback, iteratively refining editing direction. behavior. Building upon this foundation, we observed that strong understanding and generation capabilities do not guarantee proficiency in reasoning-driven editing. While Supervised Fine-Tuning (SFT) improves instruction following, it proves insufficient for enhancing reasoning. Specifically, SFT on our RVE dataset yielded limited gains in reasoningdriven editing, as it primarily refines editing skills without internalizing the real-world dynamics. To bridge this gap, we introduce the ReViSE, SelfReflective Reasoning (SRF) framework that empowers connector-based unified video models to refine their reasoning through intrinsic differential feedback. In contrast to Reinforcement Learning methods [4, 24, 48], reliant on expensive external reward models, ReViSE leverages solely intrinsic supervision. An internal understanding module acts as critic, evaluating the logical consistency between generated edits and the given instruction, and providing probability score of predicting Yes to guide the generator with differential feedback. By concurrently optimizing reasoning-guided supervision and flow-matching loss, ReViSE enhances both semantic alignment and generation realism, effectively integrating reasoning and editing. Extensive experiments on RVE-Bench show that ReViSE substantially improves reasoning accuracy and visual fidelity compared with state-of-the-art (SOTA) methods. These results validate that our self-reflective learning paradigm can effectively bridge the gap between high-level reasoning and video editing, opening new avenues for creating unified, reasoning-aligned generative models. In summary, our main contributions are as follows. We propose RVE-Bench, the first comprehensive benchmark designed for Reasoning-Informed Video Editing (RVE), which requires models to demonstrate an implicit understanding of complex real-world dynamics. We propose the ReViSE, an end-to-end, self-supervised paradigm. ReViSE enables unified models to correct their reasoning errors via leveraging an internal VLM as an intrinsic critic, providing differential guidance without external reward models. We conduct extensive experiments that not only establish state-of-the-art performance on RVE-Bench but also provide in-depth analysis, validating the effectiveness of our self-reflective mechanism. 2. Related Work 2.1. Instructional Video Editing Recent advances in video editing [3, 7, 8, 21, 25, 29, 29, 32, 34, 37, 39, 51, 52, 61, 63] have increasingly leveraged diffusion-based generative models [58, 13, 16, 27, 43, 44, 49] to achieve high-quality, temporally consistent, and controllable video synthesis. growing body of research focuses on extending image diffusion models to the video domain, emphasizing instruction following and adaptation efficiency. Early efforts such as Tune-A-Video [44] pioneered one-shot tuning of pre-trained image diffusion models for text-to-video generation. To further enhance temporal alignment and domain generalization, InsV2V [10] introduces video-to-video transfer framework using synthetic datasets for improved stability. Recently, large-scale synthesized datasets such as InsViE [46], Senorita-2M [62] and Ditto [1] have been introduced to facilitate instructionguided video editing, providing diverse and high-quality examples for training and evaluation. Beyond isolated models, unified frameworks have been explored to integrate VLMs to enhance instruction comprehension and crossmodal grounding for editing. For instance, VEGGIE [54] parses complex natural-language instructions and reasons over spatio-temporal regions to produce semantically consistent, context-aware edits; meanwhile, Omni-Video [35] tightly couples VLM-based instruction interpretation with Figure 2. Overview and statistics of RVE-Bench. (a) Examples from two subsets: Reasoning-Informed Video Editing (top) and InContext Video Generation (bottom). (b) Distribution of reasoning categories across the two subsets. (c) Word cloud of instruction keyword frequencies. diffusion-based generation in single end-to-end trainable system, improving faithfulness in instruction following and alignment of edits to semantic intent. 2.2. Reasoning Visual Generation and Editing Recent efforts in reason-informed visual generation and editing [9, 23, 28, 41, 50, 57] focus on bridging the gap between perceptual quality and semantic coherence. Benchmarks like WISE [28], R2I-Bench [9], and WorldGenBench [57] evaluate text-to-image models ability to reason about world knowledge, spatial relationships, and causal logic, revealing widespread failures in factual accuracy and commonsense understanding. RISE [59] and EditWorld [50] extend this to image editing, assessing whether edits preserve physical plausibility and contextual consistency, while Science-t2i [23] specifically targets scientific accuracy in generated images. Together, these works advance the frontier of reasoning-grounded visual generation. Recent Reinforcement Learning methods [11, 14, 15, 19, 22, 26, 55, 60] have focused on improving model alignment without relying on external reward models. Methods like CSR [60] leverage internal consistency and crossmodal alignment as implicit rewards, enabling iterative selfimprovement in multimodal understanding. However, these approaches do not explore the potential of the reasoning capability of VLMs for the RVE task. 3. Method In the following section, we will first describe the construction of our RVE-Dataset and RVE-Benchmark, then present the ReViSEs architecture, and finally discuss training objectives and implementation details. 3.1. Data Curation and Benchmark Construction This section describes our data curation strategy for building the RVE-Dataset, which enables the model to acquire deeper and more reasonable editing capabilities. We collect total of about 5.6W editing samples to support model training. To enable systematic evaluation of reasoninginformed video editing, we construct RVE-Bench, benchmark consisting of two complementary subsets: ReasoningInformed Video Editing and In-Context Video Generation. These two subsets represent complementary dimensions of reasoning-informed instructional video editing: The first focuses on explicit causal and physical reasoning within localized edits, while the second emphasizes implicit contextual and spatial-temporal reasoning across complex scenes. Specific category information can be found in the Appendix. 3 ity using SigLIP [36] embeddings with similarity score. Each cluster contains up to six clips. Within each cluster, the clip with the lowest average similarity to others is used as the target video, and the rest serve as source videos. For each selected triplet, we utilize GPT-4o to generate reason-informed editing instructions describing narrative, causal, or commonsense relations, divided into four reasoning types: camera, causal, emotional, and commonsense. By combining recaptioned reasoning-enriched instructions of the existing dataset with real video pairs, our dataset achieves both visual diversity and semantic depth. This dual-source design bridges the gap between pixel-level editing and reasoning-level understanding, providing strong foundation for training models capable of reason-aware and physically plausible video editing. 3.1.3. RVE-Bench and Evaluation Metrics To facilitate systematic evaluation of RVE task, we construct and introduce RVE-Bench, the first comprehensive benchmark designed for this task. It comprises 1,000 unique triplets, each containing source video, textual instruction, and corresponding edited video. Inspired by recent work in automated evaluation [20], we propose robust, reasoning-aware evaluation framework utilizing GPT4o. Our framework assesses generated videos from two primary aspects: Semantic Consistency (SC) and Perceptual Quality (PQ). SC measures how faithfully the edit adheres to the instruction and PQ evaluates the visual and temporal integrity of the edited video. Each aspect is further broken down into two fine-grained sub-metrics: 1. Edit Accuracy (EA): evaluates whether the edit aligns with the instructions. 2. Preservation Consistency (PC): assesses the consistency of non-edited regions with the source video to avoid over-editing. 3. Generation Naturalness (GN): evaluates the smoothness and natural flow of the video. 4. Generation Realism (GR): measures the visual fidelity of the generated frames, focusing on the absence of artifacts, distortions, or unnatural textures. For each pair, GPT-4o provides score from 0 to 10 for all four sub-metrics. The Overall score for method is calculated by first taking the geometric mean of the two semantic scores and the two perceptual scores, respectively, and then averaging these overall scores across the entire benchmark. However, for the In-Context Video Generation task, which often involves complex and creative transformations, such as changing logs into chips, the PC metric is not applicable. Therefore, in this experimental setting, the PC score is not calculated for this subset. 3.2. Self Reflective Learning As illustrated in Figure 4, our proposed ReViSE framework consists of two core components: Generation module reFigure 3. Overview of the data construction pipeline for the InContext Video Generation subset of RVE-Bench. 3.1.1. Reasoning-Informed Video Editing Subset This subset is built upon existing instruction-based video editing datasets (e.g., Ditto-1M [1]). While the original instructions typically describe surface-level pixel manipulations, such as remove the cup, add lamp, they often lack the implicit causal or commonsense reasoning required for real-world understanding. To address this limitation, we employ GPT-4o [17] to recaption the original instructions into reasoning-informed instructions. The rewritten captions introduce implicit causal, temporal, and physical reasoning cues while maintaining consistency with the paired source and target videos. For example, an instruction such as replace the poached egg with fried egg will be rewritten as Reflect the diners preference for the crisp texture and richer flavor of cooked yolk. This process generates set of high-quality, reasoning-aware instructionvideo triplets, encompassing four reasoning types: causal, spatial, temporal, and commonsense reasoning, thereby significantly increasing the semantic complexity and diversity of reasoning in the editing task. 3.1.2. In-Context Video Generation Subset While the first subset enhances implicit reasoning alignment, the second focuses on contextual understanding across scenes. As illustrated in Figure 3, we construct this subset directly from movie data, leveraging their naturally rich temporal continuity and semantic diversity. Each movie is first segmented into shots using automatic scene cut detection [53]. Then, we employ QwenVL-32B [2] to generate video-level captions. We randomly select two clips from nearby scenes as the source and target pair, and use the video caption as the instruction template. To ensure semantic relevance, we perform clustering based on visual similar4 Figure 4. Overview of ReViSEs framework. Given reasoning instruction and source video, the internal VLM encodes them into textual tokens and visual features: the textual tokens support understanding, while the visual features pass through vision head and adapter to condition DiT for video edits. The proposed Self-Reflective Learning utilizes the internal VLM to evaluate the edited video from four dimensions and produces self-corrective feedback by answering yes or no, guiding the DiT training and refining the editing results. sponsible for editing the video, and an internal understanding module that evaluates the edit accuracy and provides self-reflective feedback. This synergy allows the model to iteratively refine its reasoning capabilities during training. 3.2.1. Architecture In connector-based unified models, [35] demonstrates the effectiveness of leveraging pretrained Vision-Language Models (VLMs) (e.g., ViLA [45]) and the generative diffusion model (e.g., Wan [38]) for robust video understanding and generation. These models typically consist of an understanding module that processes multimodal inputs to generate semantic representations grounded in world knowledge. Our method builds upon such connector-based unified model [35]. We first process the multimodal inputs through separate encoders. Specifically, the source video yi is passed through the video encoder to produce visual latent representation vi, while the textual instruction ci is processed by the T5 encoder to produce sequence of textual tokens ti. Additionally, the understanding module processes both the visual and textual inputs to generate multimodel semantic representation ui: vi = Evid(yi), ti = Etext(ci), ui = U(yi, ci). These representations are then combined to form unified conditioning signal ci: ci = fC(vi, ti, ui), where fC is the connector function that transforms the combined embeddings into generator-compatible conditioning signal. This signal is then injected into each DiT block [31] to guide the generation process. Given the conditioned signal ci, the generator Gϕ predicts the video content. The generator takes noisy latent variable zt at timestep t, and the generator output at time is conditioned on ci: vϕ(zt, ci) = Gϕ(zt, t, ci). (1) The generator may produce noisy videos that are difficult for the VLM to interpret. In such cases, rather than directly feeding Gϕ(zt), we estimate cleaner version of the video: ˆx0 = zt vϕ(zt, ci). (2) Here, ˆx0 represents the clean video that is estimated from the noisy latent zt. This provides stable and coherent visual input for the subsequent evaluation. 3.2.2. Self-Reflection cornerstone of our self-reflective framework is enabling the models internal understanding module to serve as stable and reliable verifier. To achieve this, we incorporate QA-based verification mechanism, which allows the model to validate the accuracy and consistency of its edits leveraging the VLM. To ensure reliable verification, we design system prompt that directs to reason through the video editing process before predicting final Yes/No probability score. For each editing instruction, we require the VLM to evaluate the edit results across four key dimensions: 1) 5 Algorithm 1 Self-Reflective Learning Require: Generator Gϕ, Understanding Module U, Instruction ci, Source video yi 1: while training do 2: Sample timestep Estimate clean latent ˆx0 Decode edited video ˆy0 D(ˆx0) Reflect: Evaluate (ˆy0, yi, ci) with to obtain reasoning loss Lreason Compute base Flow Matching loss: LFM (cid:2)vϕ vtarget2(cid:3) Compute final USO objective: LUSO LFM + λ LLreason Update generator parameters: ϕ ϕ ηϕLUSO 3: 4: 5: 6: 7: 8: 9: end while 10: return Refined generator ϕ edit accuracy; 2) preservation consistency; 3) generation naturalness; 4) generation realism, providing comprehensive assessment of the overall generation quality, as detailed in Section 3.1.3. The VLM responds Yes only if all four dimensions are satisfied; otherwise, it responds No. The system prompt template is shown in Figure 4. To enhance interpretability and reliability, is further instructed to first provide concise chain-of-thought reasoning before producing the final Yes/No response, ensuring that the evaluation is both transparent and well-grounded. 3.2.3. Training Objective To integrate the self-reflective feedback from U, we explore two distinct optimization strategies built upon generative loss: Unified Semantic Optimization (USO) and Reward Weighted Optimization (RWO). Both strategies are based on the same reward signal, but each focuses on different optimization goals: USO improves the overall generation performance by unifying the optimization of semantic consistency and perceptual quality, while RWO refines the models reasoning process through weighted adjustments, thereby more precisely optimizing the editing. Unified Semantic Optimization. As shown in Algorithm 1, Our first strategy enhances the standard training objective by incorporating an auxiliary reasoning loss from our critic. This is achieved by adding the feedback from the Understanding Module as compulsory constraint. The total loss function combines flow matching loss Lreason and proposed reasoning loss Lreason. Formally, the generator Gϕ generates an edited video ˆy given the source video yi and an instruction c. The system prompt Sq is then injected into the understanding module U, which processes the multimodal inputs and provides reasoning-based feedback. The objective Lreason is defined as binary cross-entropy loss based on the logit difference between the yes and no tokens: ℓ(j) aj and ℓ(j) Lreason = log p(aj), where p(aj) = σ(ℓ(j) aj aj ), (3) where ℓ(j) aj denote the logits of the correct and opposite answer tokens, respectively, and σ() is the sigmoid function. This formulation normalizes the logits within the yes and no space, ensuring stability and effectiveness during training. Specifically, this combination aims to balance the continuity of video editing with the accuracy of the edits, enhancing the overall quality of the generated videos. We define the total loss function as: LUSO = LFM + λ Lreason, (4) This USO approach jointly optimizes for generative fidelity via LFM and high-level semantic alignment via Lreason. Reward Weighted Optimization. Our second strategy, RWO, offers more direct way to incorporate the critics feedback by re-weighting the generative loss itself. Instead of adding new loss term, RWO modulates the contribution of each training sample to the LFM based on the critics evaluation. To ensure training stability, the RWO objective combines reward-weighted loss term with regularizing component of the original FM loss: LRWO = (cid:2)w(pyes) vϕ vtarget2(cid:3) + λc LFM, where vϕ is the models predicted velocity and vtarget is the ground-truth velocity. The weighting function is defined as w(pyes) = 1 pyes, where pyes is the yes probability from the critic U. This dynamically scales the loss, compelling the generator to focus more on correcting samples that the critic identifies as incorrect. RWO thus transforms the critics score into scalar signal that directly influences the generative process. (5) 4. Experiments On the RVE-Bench, we compare our proposed method with several state-of-the-art baselines, including OmniVideo [35], InsV2V [10], InsViE [46], and VACE [18]. We present detailed quantitative and qualitative results that demonstrate the effectiveness of self-reflective learning across reason-informed instructional editing categories. The results are evaluated across six key metrics: ViCLIPT [40], EA, PC, GN, GR, and Overall score, providing comprehensive assessment of models performance. 4.1. Experimental Results Quantitative Results. As shown in Table 1, we evaluate five models on RVE-Bench across four reasoning categories: Causal, Temporal, Spatial, and Commonsense Reasoning. Our ReViSE consistently outperforms other models on key metrics, with the most significant improvement Figure 5. Qualitative comparison of ReViSE and other baselines on RVE-Bench. in the EA aspect. For example, ReViSE achieves 38% improvement in Overall score (3.6756 5.0786) in temporal reasoning and 32% improvement in Overall score (4.9384 5.1112) on the first subset, demonstrating its superior ability to interpret and execute reasoning-informed instructions. While InsV2V and InsViE score higher in PC, perfect PC score may indicate minimal editing, which doesnt fully address the reasoning task. ReViSE balances overall PC (5.4086) with meaningful changes, ensuring both high semantic fidelity and substantial edits where needed. ReViSE excels across all reasoning categories, delivering the best overall performance. Qualitative Results. Qualitative results in Figure 5 demonstrate ReViSEs effectiveness in RVE task. For instance, ReViSE successfully transforms background into depth forest from the instruction ran into the depth forest, while maintaining visual consistency and avoiding artifacts. These results confirm ReViSEs strong performance in both visual quality and semantic accuracy. When instructed to reflect the effects of cold air meets the wet ground and causes moisture, ReViSE generates foggy scene that logically aligns with the physical process described in the instruction. This ability to generate feasible editing results through commonsense reasoning further highlights the robustness of ReViSE in handling diverse real-world scenarios. Figure 5 (b) showcases the qualitative results for InContext Video Generation, where the instructions require rich contextual reasoning ability. For instance, the instruction Imagine if the logs underwent mechanical ... wood chips onto growing pile demands an understanding of the transformation from logs to chips and the subsequent Table 1. Comparison results on RVE-Bench. The best and second best results are shown in bold and underlined respectively. Method ViCLIPT Temporal Reasoning VACE [18] InsViE[46] InsV2V [10] Omni-Video [35] ReViSE (Ours) Causal Reasoning VACE [18] InsViE[46] InsV2V [10] Omni-Video [35] ReViSE (Ours) Spatial Reasoning VACE [18] InsViE[46] InsV2V [10] Omni-Video [35] ReViSE (Ours) 0.1623 0.1651 0.1683 0.1674 0.1684 0.1643 0.1670 0.1720 0.1744 0.1758 0.1549 0.1636 0.1668 0.1561 0. Commonsense Reasoning VACE [18] InsViE[46] InsV2V [10] Omni-Video [35] ReViSE (Ours) 0.1697 0.1730 0.1765 0.1778 0.1826 Reasoning-Informed Video Editing GR GN PC EA 3.0746 3.4179 3.8955 3.9104 4.4179 2.1786 2.6667 3.9048 5.1084 6.0402 1.8333 1.8333 2.5000 5.6667 6.6667 4.6160 5.2167 4.9430 4.6782 6.4639 6.7164 6.7313 8.1493 2.7761 4. 7.0119 7.0238 7.9286 2.6988 5.8330 6.1667 8.1667 7.6667 4.5000 4.3333 6.6198 5.8859 7.4221 2.4483 4.4905 6.5463 6.3731 6.5970 7.3134 6.6119 6.7463 5.9643 6.2619 7.0361 6.5835 6.5000 6.1667 6.1667 6.6667 7. 6.5217 6.1103 6.3688 7.3372 6.5885 7.7164 6.0746 7.0149 7.8806 7.2836 6.9762 6.0476 6.8810 7.6988 7.2072 8.3333 6.5000 7.3333 8.3333 7.5000 7.4183 6.3612 7.3240 8.2720 7.3832 Overall 3.2471 2.8958 3.6756 2.9187 5.0786(+38%) 2.3032 2.5115 3.7026 3.1360 5.4782(+30%) 2.1985 1.7743 2.9308 4.5506 4.9054(+9%) 4.2975 3.9263 4.3150 2.9536 4.3961(+2%) In Context Video Generation GR GN EA ViCLIPT Camera Reasoning 0.2120 0.2129 0.2188 0.2150 0.2193 3.6154 3.6703 3.9780 5.7912 6.4505 Causal Reasoning 0.2130 0.2121 0.2219 0.2155 0.2259 3.2381 2.7823 3.6122 5.2619 5. Emotional Reasoning 0.2088 0.2089 0.2168 0.2121 0.2181 3.6471 3.2255 3.2745 4.8039 5.3627 Commonsense Reasoning 0.2118 0.2122 0.2208 0.2157 0.2260 3.7647 3.3137 4.4706 5.9216 5. 5.5055 4.7692 5.9780 7.1429 6.9231 5.8095 5.0646 5.7687 7.2483 6.8299 6.2941 4.8824 5.9706 7.0882 6.7059 6.2745 5.0980 6.0588 7.3529 7.0980 4.2857 3.6264 5.5714 7.1758 6.6484 4.6837 4.2517 5.5442 7.6327 6. 5.5000 4.2647 5.8431 7.3137 6.4902 4.8627 4.1176 5.9412 7.2549 6.7059 Overall 3.3408 2.8685 3.6895 5.7630 5.9268 3.0712 2.4028 3.3804 5.4782 5.6354 3.6523 2.6626 3.3410 4.9384 5. 3.6547 2.8524 4.1007 5.7932 5.7623 Table 2. Ablation results of training objectives of ReViSE on Reasoning-Informed Video Editing subset of RVE-Bench. Method VACE InsViE InsV2V Omni-Video SFT RWO USO (ReViSE) ViCLIPT EA PC GN GR Overall 0.1658 0.1688 0.1731 0. 0.1721 0.1781 0.1771 2.6309 3.2111 3.7815 4.8506 4.9383 5.3988 6.1605 6.7198 6.4432 7.9123 2.9716 4.1160 4.5099 5.4086 6.6062 6.1679 6.4580 6. 6.7481 6.3370 6.6185 7.3120 6.2481 7.2012 7.1519 7.3495 6.9519 7.3423 2.7195 2.6281 3.5387 3.1331 3.7009 3.7554 4.6689(+32%) arrangement of the chips. ReViSE is the only method that accurately interprets the instruction, generating realistic video of logs transforming into chips and accumulating into growing pile. This result demonstrates ReViSEs unique ability to handle complex in-context generation tasks that require reasoning and dynamic context interpretation. Results on Conventional Video Editing To evaluate the performance of ReViSE on standard video editing tasks, we randomly selected 809 samples from the Ditto-1M dataset [1]. ReViSE was compared with four representative baselines, and the quantitative results in Table show that it consistently outperforms all competitors across four key metrics, achieving 36.7% gain in the Overall score over the previous state of the art. Although the radar plot in Figure 7 reports lower Preservation Consistency (PC) for ReViSE, this should not be interpreted as inferior editing quality. higher PC reflects fewer visual changes, rather than better edits. ReViSE achieves an average PC score of 5.0963, which lies within desirable range that balances faithful execution of the editing instruction with consistency to the source video, thereby ensuring both semantic accuracy and visual coherence. Qualitative comparisons in Figure 6 further illustrate that ReViSE not only excels in conventional editing scenarios but also provides reliable foundation for more challenging reasoning-aware video editing tasks. In the figure, both the white and red boxes highlight the regions that require editing. This robustness demonstrates the generality of our ReViSE across both traditional and reasoning-driven video editing. Table 3. Comparison results of conventional video editing on Ditto-1M [1]. Method ViCLIPT EA PC GN GR Overall VACE [18] InsViE [46] InsV2V [10] Omni-Video [35] ReViSE (Ours) 0.1721 0.1733 0.1828 0.1851 0. 1.4741 2.2185 3.2395 3.6901 5.0963 5.7444 5.4185 6.6247 3.1321 4.6852 5.9753 5.2420 5.7432 6.5235 6.5259 6.2383 5.2840 5.8864 7.0222 7.1543 1.3958 1.6960 2.7871 2.5508 3.8109(+36.7%) Ablation of Training Objectives. Table 2 compares training objective variants for RVE task on the RVEBenchmark. While SFT and RWO improve editing performance, USO achieves superior results across all metrics. Specifically, SFT improves edit accuracy by only 1.8%, whereas USO and RWO achieve gains of 27% and 11%, 8 Figure 6. Qualitative comparisons between ReViSE and other representative methods on conventional video editing. Figure 7. Comparison results of conventional video editing on Ditto-1M [1]. respectively. Both USO and RWO leverage self-reflective learning, refining edits by verifying adherence to reasoning instructions. RWO scales the flow matching loss using an alpha factor derived from the yes token probability score, acting as regularization. However, this approach does not explicitly decouple edit generation and instruction satisfaction and may not be sufficient to effectively balance these two objectives. In contrast, USO optimizes both flow matching and unified semantic losses, enabling complementary optimization based on distinct gradient, leadFigure 8. Qualitative ablation results of training objectives. ing to balanced improvement. Qualitative ablation results in Figure 8, further confirm that our approach not only improves reasoning editing accuracy but also enhances the visual realism of the generated videos, demonstrating the effectiveness of our unified framework. 5. Conclusion In this work, we introduced the Reason-Informed Video task and developed ReViSE, SelfEditing (RVE) 9 Reflective Reasoning (SRF) framework, to bridge the gap between reasoning and video editing. By integrating reasoning, generation, and internal evaluation within unified architecture, ReViSE leverages an internal VLM to provide intrinsic feedback and refine the video editing process. Additionally, we propose RVE-Bench, the first benchmark specifically designed for the RVE task, featuring two categories that encompass diverse reasoning dimensions and real-world contexts. Our extensive experiments on the RVE-Bench benchmark demonstrate that ReViSE significantly improves reasoning accuracy and visual fidelity, achieving marginal gain over state-of-the-art methods."
        },
        {
            "title": "References",
            "content": "[1] Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Ka Leong Cheng, Shuailei Ma, Yanhong Zeng, Zichen Liu, et al. Scaling instruction-based video editing with high-quality synthetic dataset. arXiv preprint arXiv:2510.15742, 2025. 1, 2, 4, 8, 9, 5 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 3 [3] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Anylength video inpainting and editing with plug-and-play context control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 2 [4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 2 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI, https://openai. com / research / video - generation - models - as-world-simulators, 2024. [7] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. 1, [8] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2304023050, 2023. 1, 2 [9] Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, Yuguang Yao, Joy Rimchala, Jiaxin Zhang, and Lifu Huang. R2ibench: Benchmarking reasoning-driven text-to-image generation. arXiv preprint arXiv:2505.23493, 2025. 3 [10] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoarXiv preprint to-video transfer using synthetic dataset. arXiv:2311.00213, 2023. 2, 6, 8, 5 [11] Eugene Choi, Arash Ahmadian, Matthieu Geist, Oilvier Pietquin, and Mohammad Gheshlaghi Azar. Selfimproving robust preference optimization. arXiv preprint arXiv:2406.01660, 2024. 3 [12] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 1 [13] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In CVPR, pages 73467356, 2023. [14] Tsu-Jui Fu, Xin Eric Wang, Scott Grafton, Miguel Eckstein, and William Yang Wang. Sscr: Iterative language-based image editing via self-supervised counterfactual reasoning. arXiv preprint arXiv:2009.09566, 2020. 3 [15] Yujin Han, Hao Chen, Andi Han, Zhiheng Wang, Xinyu Liu, Yingya Zhang, Shiwei Zhang, and Difan Zou. Turning internal gap into self-improvement: Promoting the arXiv generation-understanding unification in mllms. preprint arXiv:2507.16663, 2025. 3 [16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2023. 2 [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 4 [18] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 6, 8, 5 [19] Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, and Xihui Liu. Srum: Fine-grained selfrewarding for unified multimodal models. arXiv preprint arXiv:2510.12784, 2025. 3 [20] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, 2024. 4 [21] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. [22] Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, and Xun Huang. Learning an image editing model without image editing pairs. arXiv preprint arXiv:2510.14978, 2025. 3 [23] Jialuo Li, Wenhao Chai, Xingyu Fu, Haiyang Xu, and Saining Xie. Science-t2i: Addressing scientific illusions in image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 27342744, 2025. 3 10 [24] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 2 [25] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. 2 [26] Grace Luo, Jonathan Granskog, Aleksander Holynski, and arXiv Trevor Darrell. Dual-process image generation. preprint arXiv:2506.01955, 2025. 3 [27] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 91179125, 2023. 2 [28] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. 3 [29] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing via image-tovideo diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [30] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 1 [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [32] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. 2 [33] Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Instructvid2vid: Controllable video and Yueting Zhuang. In 2024 IEEE editing with natural language instructions. International Conference on Multimedia and Expo (ICME), pages 16. IEEE, 2024. 1 [34] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 2 [35] Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, and Hao Li. Omni-video: Democratizing uniarXiv preprint fied video understanding and generation. arXiv:2507.06119, 2025. 1, 2, 5, 6, 8 [36] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [37] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video obIn Proceedings ject insertion with precise motion control. of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 2 [38] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 5 [39] Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, and Raj Sodhi. Lave: Llm-powered agent assistance and language augmentation for video editing. In Proceedings of the 29th International Conference on Intelligent User Interfaces, pages 699714, 2024. 2 [40] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 6 [41] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 3 [42] Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, and Peter Vajda. Fairy: Fast parallelized instruction-guided video-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82618270, 2024. [43] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2 [44] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pages 76237633, 2023. 2 [45] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 5, 3 [46] Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, and Lei Zhang. Insvie-1m: Effective instruction-based video editing with elaborate dataset construction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1669216701, 2025. 2, 6, 8, 5 [47] Zhen Xing, Qi Dai, Zihao Zhang, Hui Zhang, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Vidiff: Translating videos via multi-modal instructions with diffusion models. arXiv preprint arXiv:2311.18837, 2023. 1 11 [61] Shaobin Zhuang, Zhipeng Huang, Binxin Yang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Chong Sun, Zheng-Jun Zha, Chen Li, and Yali Wang. Get in video: Add anything you want to the video. arXiv preprint arXiv:2503.06268, 2025. 2 [62] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Se norita-2m: high-quality instructionbased dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. [63] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Rong Xiao, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1106711076, 2025. 2 [48] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 2 [49] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 2 [50] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024. 3 [51] Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. Videograin: Modulating space-time attention for multigrained video editing. In The Thirteenth International Conference on Learning Representations, 2025. 2 [52] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 2 [53] Jun Yu and Mandyam Srinath. An efficient method for scene cut detection. Pattern Recognition Letters, 22(13): 13791391, 2001. [54] Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit Bansal. Veggie: Instructional editing and reasoning video concepts with grounded generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15147 15158, 2025. 2 [55] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel AlInstructvideo: Instructing video difbanie, and Dong Ni. In Proceedings of fusion models with human feedback. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024. 3 [56] Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, et al. Lumos-1: On autoregressive video generation from unified model perspective. arXiv preprint arXiv:2507.08801, 2025. 1 [57] Daoan Zhang, Che Jiang, Ruoshi Xu, Biaoxiang Chen, Zijian Jin, Yutian Lu, Jianguo Zhang, Liang Yong, Jiebo Luo, and Shengda Luo. Worldgenbench: world-knowledgeintegrated benchmark for reasoning-driven text-to-image generation. arXiv preprint arXiv:2505.01490, 2025. 3 [58] Zhenghao Zhang, Zuozhuo Dai, Long Qin, and Weizhi Wang. Effived: Efficient video editing via text-instruction diffusion models. arXiv preprint arXiv:2403.11568, 2024. 1 [59] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 3 [60] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. Advances in Neural Information Processing Systems, 37:5150351531, 2024. 3 2 2 2 3 3 3"
        },
        {
            "title": "Contents",
            "content": "A. Detailed Descriptions of RVE-Bench A.1. Reasoning-Informed Video Editing . . A.2. In-Context Video Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Training Details C. Effect of Self-Reflection Strength λ D. Reliability of Understanding Module E. More Qualitative Results F. Examples of Evaluation Results G. Limitation 1 A. Detailed Descriptions of RVE-Bench To comprehensively evaluate reasoning in advanced video models, we introduce RVE-Bench including 1k samples, which is composed of two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. To address the limitations of existing benchmarks that rely on explicit, pixel-focused instructions, we developed the reasoning-informed video editing subset. This was achieved by systematically reformulating standard editing instructions into sophisticated prompts that necessitate an implicit understanding of real-world dynamics. The second subset, in-context video generation, is built from real movie data to specifically target the challenge of contextually-enriched video generation, requiring models to create new scenes based on complex narrative and cinematic reasoning. Together, they provide robust benchmark for advanced video reasoning editing. We provide detailed breakdown of both subsets in Table 4, which specifies their data sources, reasoning categories, and instruction examples. A.1. Reasoning-Informed Video Editing The Reasoning-Informed Video Editing subset is systematically structured to evaluate model capabilities across four deep understanding and precise reasoning dimensions: causal, spatial, temporal, and commonsense reasoning. In total, this subset contains 809 meticulously annotated samples. Causal reasoning involves understanding transformative changes driven by external factors, such as glass shattering upon impact. This domain covers three representative forms of causal effects: (1) structural deformation, where external forces alter the shape of an object; (2) state transition, in which an object changes state due to manipulation or environmental shifts (e.g., melting, freezing, or burning); and (3) physical manifestation, where visible outcomes emerge from underlying physical laws triggered by specific stimuli. Spatial reasoning reflects how scene appears from different viewing angles or how various components assemble into new structures. This domain includes three (1) object arrangement, representative subcategories: which assesses the sequencing and positioning of objects based on attributes such as size, shape, or color; (2) viewpoint generation, which evaluates the ability to synthesize plausible novel views from different angles; and (3) layout reasoning, which examines whether the model can understand and manipulate overall spatial configurations while maintaining structural coherence. Temporal reasoning focuses on how objects and scenes naturally evolve over time rather than changing abruptly for visual convenience. This domain encompasses four representative forms: (1) life progression, such as growth, aging, or developmental stages; (2) environmental cycles, including seasonal or weather-driven transitions; (3) material state change, such as rusting, drying, melting, or fading; and (4) societal transformation, where human activities or cultural contexts evolve across time. Commonsense reasoning requires generated edits to follow everyday expectations about how humans, objects, and environments behave in the real world. This domain covers four representative aspects: (1) physical plausibility, such as knowing that snow melts when temperatures rise or that unsupported objects fall; (2) environmental response, where changes in weather, lighting, or surroundings influence how the scene evolves; (3) social convention, understanding typical human reactions, routines, and cultural norms; and (4) functional usage, predicting how everyday objects are normally used and how they should behave in context. A.2. In-Context Video Generation The In-Context Video Generation subset focuses on narrative and cinematic challenges, categorized into four distinct reasoning types: causal, camera, emotional, and commonsense reasoning. This subset comprises 200 carefully annotated samples. Causal reasoning focuses on narrative progression, where events unfold as plausible consequences of character motivations, intentions, or interactions. It is conceptually analogous to causal reasoning in section A.1, but adapted from physical world causality to narrative causality. Camera reasoning involves instructions that require altering the perspective from which scene is viewed, rather than changing the scenes content. This reasoning encompasses three key dimensions: It involves three core dimensions: (1) spatial reasoning involves determining the optimal camera position or orientation. For example, shifting from wide shot to close-up or rotating the camera to reveal hidden element; (2) temporal reasoning focuses on how the camera should adapt as the narrative unfolds over time. An instance of this could be transitioning the viewpoint as the story jumps forward, such as ten years later; and (3) narrative intent interpretation entails selecting cinematic techniques that align with the narratives goals. Emotional reasoning requires understanding how characters internal affective states drive the evolution of their actions and interactions. It involves: (1) emotiondriven actions, where internal feelings determine what character does next (e.g., shyness leading to avoidance, attraction leading to intimacy); (2) relationship modulation, adjusting interpersonal distance, orientation, and interaction intensity according to the emotional climate; 2 and (3) atmospheric reflection, shaping the surrounding visual or cinematic tone so that it resonates with the emotional undercurrent of the scene. Commonsense reasoning ensures that generated scene developments adhere to everyday expectations about human behavior and the physical and social world. It follows the same fundamental notion of commonsense as in the edit-oriented subset. B. Training Details During training, we unfreeze and simultaneously fine-tune the vision adapter and diffusion module. The learning rate is set to 3 106, with global batch size of 16 for videos. For self-reflective learning, the hyperparameter λ is set to 0.5. We perform Self-Reflective Reasoning (SRF) on base model with eight NVIDIA H800 GPUs, each equipped with 80 GB of memory, requiring approximately 20 hours per epoch. In the SRF process, we decode two selected frames from the estimated clean latent space and design prompts to leverage ViLA [45] for evaluating editing quality. The prediction probability score from the understanding branch is used to calculate the cross-entropy loss, which acts as the reason loss to constrain the generative loss, iteratively optimizing the editing behavior during training. Detailed hyperparameter configurations are presented in Table 5. C. Effect of Self-Reflection Strength λ Unified Semantic Optimization (USO) combines the flowmatching objective LFM with the reasoning-aware objective Lreason through balancing hyperparameter λ. To understand how this weight shapes editing behavior, we conduct an ablation study on the reasoning-informed video editing subset and report results across five dimensions: Editing Accuracy (EA), Preservation Consistency (PC), Generation Naturalness (GN), Generation Realism (GR), and Overall Quality under four reasoning categories (temporal, causal, spatial, and commonsense), as summarized in Figure 10 and Table 6. In general, the model exhibits clear sensitivity to the strength of self-reflective supervision. Starting from λ = 0.10, increasing λ generally improves EA, GN, GR, and the Overall score in most reasoning types, indicating that internal VLM feedback indeed provides useful training signals beyond pure reconstruction. In particular, λ = 0.75 consistently delivers the best or near-best Overall performance in temporal, causal, and commonsense reasoning, and also remains competitive for spatial reasoning. This setting strikes balance in which the reflection is strong enough to enforce reasoning consistency, while still preserving the still region of the source video. Qualitative comparisons in Figure 9 further illustrate how the strength of the reflection influences the balance between semantic alignment and content preservation. When Figure 9. Visualized qualitative comparison under varying λ. λ = 0.10, the reasoning signal is too weak, resulting in semantic under-editing. As λ increases, semantic accuracy improves. However, intermediate values such as λ = 0.50 start to introduce background distortions, suggesting overediting. At λ = 0.75, edits become both precise and minimally intrusive. Only one bird is added, and the sunset background remains intact, representing balanced optimum. When λ reaches 1.0, the reward term becomes dominant and leads to instability. This can result in duplicating the sunset, which satisfies the instruction but disrupts scene coherence. We therefore adopt λ = 0.75 as the default configuration in all experiments. D. Reliability of Understanding Module To examine whether the internal VLM can act as reliable evaluator during self-reflection training, we compared its assessment behavior with multiple external VLMs using two metrics: (1) final decision agreement (yes/no) and (2) reasoning similarity (cosine similarity between textual rationales). We randomly sampled 100 edited-video records and logged the understanding modules intermediate outputs, including reasoning text and binary judgments. As shown in Table 8, the internal VLM achieves high consistency with external evaluators across both metrics. QwenVL [2] tends to respond more leniently, which includSubset Source Reasoning Type Instruction Example Table 4. Composition of the RVE-Bench. Reasoning-Informed Video Editing Ditto-1M [1] Causal Spatial Temporal Commonsense Causal Camera In-Context Video Generation Collected Movie Data Emotional Commonsense What if the drink had cooled down just enough to be sipped comfortably? What will it be like if the perspective is shifted to roadside view, revealing new home for local birds? Imagine that the spring thaw brought vibrant, mossy life to the rocky canyon walls. Let the buildings facade be designed to reflect the cool tones of the sky. What would happen if close-up shot zoomed in from their full bodies to just their hands and lower torsos? Imagine if the focus shifted to the jets cockpit door, which slowly closed under the control of internal mechanisms. Imagine that the young mans simple smile should transform into blossoming, radiant grin as he spots someone he likes, before he shyly turns away in moment of sweet bashfulness. Imagine if the geysers eruption intensified, creating dramatic scene as the powerful jet of water and steam soared against the serene backdrop of forested hills. Figure 10. Ablation results of self reflection strength λ across causal, spatial, temporal, and commonsense reasoning. ing more yes predictions, whereas the internal VLM exhibits evaluation patterns closer to GPT-4o [17], providing stricter and more instruction-focused decisions. These results indicate that the internal VLM serves as reliable evaluator, generating stable and logically grounded 4 feedback signals that guide self-reflection training without requiring external VLMs or human annotation. at reason-enriched video editing, where the correct edit is not visually explicit in the instruction but must be inferred through implicit reasoning. ReViSE identifies the latent causal and semantic implications behind the instruction and applies modifications that are logically grounded while preserving unrelated content. In contrast, baseline models struggle with such implicit reasoning: they either perform literal edits that fail to capture the intended transformation, or they introduce visually inconsistent modifications due to the lack of world-model understanding. F. Examples of Evaluation Results To demonstrate the practical application of our proposed evaluation framework, we utilized GPT-4o to score diverse set of editing examples of RVE-Bench. Figure 12 and Figure 13 showcases four such quantitative results. Each example is evaluated against our four proposed dimensions, yielding scores and detailed rationales for both Semantic Consistency (SC) and Perceptual Quality (PQ), along with final Overall Score. These examples highlight how our metrics effectively capture nuances in edit accuracy, realism, and visual fidelity across all reasoning categories. G. Limitation ReViSE has demonstrated significant advancements in reasoning-informed video editing. However, it still inherits some limitations. For instance, the editing performance is primarily constrained by the capabilities of the base model. While ReViSE effectively integrates reasoning with visual transformation, it cannot fully exploit its potential without more powerful foundational models. Competing methods benefit from expansive models like Wan-5B/14B, which enhance performance due to their scale and training. Unfortunately, the resources required for such large models are beyond our current capacity, indicating that access to these models could further elevate ReViSEs capabilities."
        },
        {
            "title": "ReViSE",
            "content": "Optimization Optimizer Learning rate Weight decay Warmup steps Epoch Gradient Accumulation steps Per-GPU batch size Loss weights λ (Strength of SRF guidance) CFG Ratio"
        },
        {
            "title": "Data Construction\nVideo Resolution\nTraining Data Size\nTesting Data Size",
            "content": "AdamW 3 106 0.0001 500 10 1 32 (SFT), 2 (SRF) 0.75 0.2 352 640 17 56.3k 1k Table 5. Hyperparameter configurations for training ReViSE. Table 6. Performance comparison with different λ settings of RVE-Bench. λ 0.10 0.25 0.50 0.75 1.00 Reasoning-Informed Video Editing Subset ViCLIPT 0.1784 0.1747 0.1801 0.1771 0.1767 EA PC GN GR Overall 5.4247 5.5679 5.6296 6.1605 5.8593 4.5099 4.5765 4.1790 5.4086 5.4914 6.2407 6.221 6.5407 6.6185 6.5321 6.7790 6.8099 7.1617 7.3123 7. 3.8174 3.9219 3.9552 4.6689 4.4779 Table 7. Comparison results of conventional video editing on Ditto-1M [1]. Method ViCLIPT EA PC GN GR Overall VACE [18] InsViE [46] InsV2V [10] Omni-Video [35] ReViSE (Ours) 0.1721 0.1733 0.1828 0.1851 0.1877 1.4741 2.2185 3.2395 3.6901 5. 5.7444 5.4185 6.6247 3.1321 4.6852 5.9753 5.2420 5.7432 6.5235 6.5259 6.2383 5.2840 5.8864 7.0222 7.1543 1.3958 1.6960 2.7871 2.5508 3.8109(+36.7%) Table 8. Reliability of the internal VLM. We report yes/no decision agreement (%) and reasoning similarity (cosine) between the internal VLM and external VLMs. External VLMs Acc Sim QwenVL-2.5-32B QwenVL-2.5-72B GPT-4o 65.6 66.1 74.6 0.74 0.70 0.72 E. More Qualitative Results We provide additional qualitative results in Figure 11 on RVE-Bench which further demonstrate that ReViSE excels 5 Figure 11. More qualitative comparisons between ReViSE and other representative methods. 6 Evaluation Results of Reasoning-Informed Video Editing Subset Example 1: Instruction: Remove the small hummingbird hovering near the edge of hanging flower pot. Reasoning instruction: What if the hummingbird finished its visit and flew away from the flower pot? type: Causal Reasoning Edits SC: score: [8.0, 9.0], reasoning: The edited video successfully depicts the hummingbird flying away from the flower pot, which aligns well with the editing instruction. The rest of the scene, including the woman and the environment, remains unchanged, indicating minimal and effective editing. PQ: score: [8.0, 9.0], reasoning: The video looks natural with appropriate lighting and shadows. The environment and colors are consistent, adding to the realism. There are minimal artifacts present, with no notable distortions or disharmony among elements. score: 8.0 Example 2: Instruction: Remove the small, glowing seashell embedded in the sand near the foreground. Reasoning instruction: What if the scenes focus shifted entirely to the distant coastal view? type: Spatial Reasoning Edits SC: score: [5.0, 9.0], reasoning: The edited video shifts the focus slightly towards the distant coastal view by reducing the prominence of the foreground elements, but it still does not make the coastal view entirely the focus as the instruction suggestshence moderate editing success. The video changes minimally, maintaining most of the original structure leading to high degree of overediting score. PQ: score: [9.0, 9.0], reasoning: The video editing effectively focuses on the coastal view, providing natural and consistent setting. The imagery is clear, with minimal distortion or visible artifacts, maintaining realism throughout the scene. score: 6.7082 Example 3: Instruction: Introduce single, glowing red drone hovering silently above the windmill, rotating slowly. Reasoning instruction: What if modern surveillance device appeared to monitor the traditional landscape? type: Causal Reasoning Edits SC: score: [9.0, 8.0], reasoning: The edited video effectively introduces modern surveillance device into the traditional landscape, as instructed. The design and placement of the device do not drastically alter the scene, maintaining the original landscapes integrity. However, the surveillance device is still noticeable addition, slightly reducing the overediting score. PQ: score: [6.0, 7.0], reasoning: The video depicts modern surveillance device in traditional landscape. The device does not blend seamlessly with the environment, affecting naturalness. Lighting and shadows could be improved for better integration. Artifacts are minimal but include potential minor distortions in alignment and integration of the device. score: 6.9282 Example 4: Instruction: Clear the mist rising from the river surface, restoring the original clarity of the scene. Reasoning instruction: What if the morning mist over the river evaporated, revealing clearer view of the horizon? type: Commonsense Reasoning Edits SC: score: [6.0, 9.0], reasoning: The edited video successfully reduces the mist slightly, revealing clearer view of the horizon, but the change is subtle and not dramatically clearer than in the original. The overall composition remains similar with minimal changes to the original scene. PQ: score: [7.0, 8.0], reasoning: The video appears mostly natural with the mist revealing the horizon, however, the mist clearing effect seems slightly abrupt. Artifacts are minimal but theres minor distortion in how the mist clears. score: 6.4807 7 Figure 12. Evaluation results of the reasoning-informed video editing subset. Each example reports SC, PQ and Overall score together with textual rationales. Evaluation Results of In-Context Video Gneration Subset Example 1: Instruction: Imagine if researcher observing the baboons decided to step into the scene, providing human perspective against the backdrop of the majestic mountains. type: narrative commonsense SC: score: [8.0, 7.0], reasoning: The edited video successfully adds researcher in the background among the baboons, fulfilling the instruction to include human perspective. The human figure is clearly visible against the mountain backdrop, indicating effective execution. However, the terrain and scenery differ from the original, implying noticeable edits, thus slightly decreasing the overediting score. PQ: score: [7.0, 6.0], reasoning: The scene with the baboons and mountains appears relatively natural, with appropriate lighting and shadows. However, the human figure seems slightly out of place in terms of scale and blending with the environment. There are minor artifacts visible, such as slight blurring around the baboons and the human figure not fully harmonizing with the background. score: 6.4807 Example 2: Instruction: Imagine if the monkeys climbing attracted the attention of nearby leopard, prompting the predator to ascend the tree in pursuit. type: narrative causal SC: score: [8.0, 7.0], reasoning: The edited video successfully shows leopard climbing tree, which matches the instruction. The scene feels consistent with the original setting, supporting wildlife behavior. However, the edited video features significant new content, indicating some overediting in creating the dramatic predator-prey scenario. PQ: score: [3.0, 2.0], reasoning: The scene is poorly constructed with visible distortions and unnatural interactions; the leopards position on the tree is awkward and lacks realistic movement, presence of visual artifacts such as mesh-like distortion around the leopard, and mismatched lighting. score: 3.7417 Example 3: Instruction: Imagine if the event transitioned from static car showcase to dynamic car parade, where the focus shifts from appreciating design details to experiencing the thrill of cars in motion. type: narrative camera SC: score: [10.0, 5.0], reasoning: The edited video successfully transitions from static car showcase to dynamic car parade, capturing the thrill of the cars in motion as intended. However, the change is significant from the original, which impacts the degree of overediting. PQ: score: [3.0, 2.0], reasoning: The video lacks naturalness due to the unnatural motion blur on the cars, suggesting an unrealistic speed. The artifacts are significant, with noticeable blurring and merging of the cars outlines, which disrupts the visual coherence and clarity. score: 3.1623 Example 4: Instruction: Imagine if the young womans speech inspired the audience, leading them to become more engaged and animated, resulting in shift from passive listening to active participation in the event. type: emotional causal SC: score: [9.0, 3.0], reasoning: The edited video successfully shows an audience that is much more engaged and animated compared to the original, following the editing instruction to depict active participation. However, there is significant change in the setting and audience, indicating some degree of overediting. PQ: score: [8.0, 8.0], reasoning: The video portrays natural setting of an engaged audience at an event, and the movements of clapping appear fluid. However, there is slight jerkiness in hand movements, which affects the naturalness. The video is free from noticeable artifacts like distortions or mismatches in elements. score: 4.899 Figure 13. Evaluation results of the in-context video editing subset. Each example reports SC, PQ and Overall score together with textual rationales."
        }
    ],
    "affiliations": [
        "FDU",
        "HKU",
        "HKUST",
        "RUC",
        "Tongyi Lab",
        "ZJU"
    ]
}