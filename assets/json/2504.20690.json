{
    "paper_title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "authors": [
        "Zechuan Zhang",
        "Ji Xie",
        "Yu Lu",
        "Zongxin Yang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/."
        },
        {
            "title": "Start",
            "content": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer Zechuan Zhang1 Ji Xie1 Yu Lu1 Zongxin Yang2 Yi Yang1 1 ReLER, CCAI, Zhejiang University 2 DBMI, HMS, Harvard University Project Page: https://river-zhang.github.io/ICEdit-gh-pages 5 2 0 2 9 ] . [ 1 0 9 6 0 2 . 4 0 5 2 : r Figure 1. We present In-Context Edit, novel approach that achieves state-of-the-art instruction-based editing using just 0.5% of the training data and 1% of the parameters required by prior SOTA methods. The first row illustrates series of multi-turn edits, executed with high precision, while the second and third rows highlight diverse, visually impressive single-turn editing results from our method."
        },
        {
            "title": "Abstract",
            "content": "Instruction-based image editing enables robust image mod- : the corresponding author. ification via natural language prompts, yet current methods face precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this 1 dilemma by leveraging large-scale Diffusion Transformer (DiT) enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our methods superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes new paradigm that enables highprecision yet efficient instruction-guided editing. 1. Introduction In recent years, instruction-based image editing has gained considerable attention for its ability to transform and manipulate images using natural language prompts. The main advantage of instruction-based editing is its ability to generate precise modifications with minimal textual instructions, thereby opening new possibilities for both automated image processing and user-driven content creation. Existing approaches to instruction-based image editing generally fall into two categories: finetuning-based methods and training-free techniques. Finetuning-based methods [3, 10, 12, 28, 39, 47, 48] typically involve retraining pretrained diffusion models on large-scale editing datasets, enabling the model to learn instruction-following capabilities. In contrast, training-free techniques [1, 13, 19, 21, 30, 43, 51] avoid retraining process by mechanisms like image inversion, prompt swapping (substituting generative prompts for instructions) or manipulating attention weights. Despite demonstrating impressive capabilities, current image editing methods face critical trade-off between precision and efficiency. Finetuning-based methods deliver precise instruction-following results through structural modifications (e.g., reference image processing modules, channel adjustments) combined with extensive training on large-scale datasets (450K [3] to 10M [39] samples), while requiring substantial computational resources that compromise efficiency. Conversely, training-free methods eliminate architectural modifications and dataset training, but struggle to interpret complex instructions, leading to reduced precision that limits their practical utility. Despite the dilemma above, recent advances in diffusion transformers (DiT) [7, 8, 35] suggest promising pathway. DiT architectures exhibit two critical properties: (1) Scalable Generation Fidelity: Larger DiT variants (e.g., FLUX [22]) achieve state-of-the-art text-to-image alignFigure 2. Data Efficiency increases inversely with the amount of training data, while CLIP score reflects editing performance. Our method achieves high editing precision with fewer training data. ment without auxiliary modules, enabling direct adaptation to tasks like reference-guided synthesis [41] and identity- (2) Intrinsic Contextual Awarepreserved editing [17]. ness: DiTs inherently establish bidirectional interactions between reference and generated content through attention mechanisms, enabling simultaneous processing of sourcetarget images [17, 49] without dedicated alignment networks. This prompts key inquiry: Can DiTs generation capacity and contextual awareness directly address instruction-based image editing, while balancing precision and efficiency through intrinsic capabilities rather than external complexity? Our investigation reveals two critical insights through exploration of DiTs editing capabilities. First, we implement training-free in-context editing paradigm based on DiTs, where the model generates edited outputs (right panel of diptych) by processing in-context prompts alongside the source image (left panel). This approach demonstrates two key advantages: (1) eliminating architectural modifications for reference image encoding, and (2) tuningfree instruction compliance through contextual understanding. While persistent failure cases remain, the achieved advantages establish robust baseline that facilitates efficient fine-tuning for precision enhancement. Second, we observed that the choice of initial noise significantly impacts the quality of the result. Certain types of noise lead to better results, highlighting the importance of selecting better initial noises for improved performance. Building on these observations, we explore two strategies to enhance both precision and efficiency in the editing paradigm: LoRA-MoE Hybrid Tuning: Integrating parameter-efficient LoRA adapters with mixture-of-experts (MoE) routing within the DiT framework, this approach dynamically activates task-specific experts during editing. Trained on minimal publicly available data, it improves editing success rates across diverse scenarios without architectural modifications or large-scale retraining. Early Filter Inference Time Scaling: Leveraging vision2 language models (VLMs) as quality evaluators, we identify better initial noise distributions within early denoising steps of rectified flow models. This strategy selects noise candidates aligned with textual instructions, improving both robustness and output quality. Our experiments on the Emu Edit [39] and MagicBrush benchmarks demonstrate three critical advancements. First, the method achieves superior data and parameter efficiency, outperforming SOTA approaches while utilizing only 0.5% training data and 1% trainable parameters. Second, it outperforms recent DiT-based models, confirming the effectiveness of our in-context editing paradigm. Third, VIE-score evaluation [20] shows competitive performance against commercial systems (78.2 vs. SeedEdits 75.7), proving practical viability. These results establish an unprecedented balance between precision and efficiency (as shown in Fig. 2). In summary, our contributions include: We explore the editing capabilities of the large pretrained diffusion transformer, e.g. FLUX, and introduce novel in-context editing paradigm that enables effective instructive image editing without modifying the models architecture or requiring extensive fine-tuning. We propose LoRA-MoE Hybrid Finetuning for parameterefficient editing task adaptation, integrated with an Early Filter Inference Time Scaling strategy that employs VLMbased noise pruning. This co-design synergistically enhances editing precision while maintaining the efficiency of our in-context edit framework. Our experiments demonstrate that our method achieves state-of-the-art editing performance while requiring only 0.5% of the training data and 1% of the trainable parameters compared to previous approaches. This effectively resolves the long-standing precision-efficiency trade-off that has challenged prior methods. 2. Related Work Training-free editing techniques. Since the emergence of Diffusion Models, numerous training-free image editing methods [5, 13, 21, 31, 42, 51] have gained attention. RF-Solver [43] improves inversion precision in Rectifiedflow models by mitigating ODE-solving errors and leverages MasaCtrl [5] for image editing. StableFlow [1] identifies critical MM-DiT Blocks through ablation studies, injecting features only into these blocks to enhance editing capabilities. However, these methods face two key limitations: 1) manually designed modules restrict generation ability, hindering complex instruction understanding and reducing success rates; 2) editing requires carefully crafted prompts, limiting generalizability and scalability. Finetuning-based editing methods. Most current editing models modify architectures and fine-tune on high-quality datasets [3, 4448]. InstructPix2Pix [3] fine-tunes diffusion UNet using original images as input. MGIE [10] Figure 3. Attention Map Visualization of Edit Instructions (3.1). We computed the attention values for the selected text by aggregating sums and averages across different steps and layers. enhances instruction understanding by integrating Multimodal Large Language Model (MLLM) to encode and inject instructions into the diffusion model. However, gap exists between the embedding spaces of generative prompts and editing instructions, reducing the generalization ability of Diffusion Models and necessitating large datasets to bridge it. For instance, InstructPix2Pix generated 450K pairs, Emu Edit [39] collected nearly 10M pairs, and FluxEdit [34] used 1.2M pairs from [44] based on FLUX [22], yet the editing results remain suboptimal. 3. Method In this section, we begin by exploring the in-context editing capabilities within the original DiT generative models, and propose our in-context edit framework for instructionbased image editing (3.1). After conducting thorough analysis, we introduce LoRA-MoE hybrid fine-tuning (3.2) to our framework with small editing dataset, which significantly improves editing quality and success rate. Finally, we present an early filter inference time scaling strategy (3.3) to select better initial noise for enhanced generation quality during inference. 3.1. Exploration of DiTs In-context Edit Ability In-Context Generation with Edit Instructions. Inspired by recent work [16, 17, 41, 49] showing the strong contextual abilities of large-scale DiT models, we explore if image editing can be done using in-context generation. To do this, we add edit instructions to generative prompt designed for in-context editing. Specifically, we design prompts in the form: side-by-side image of the same {subject}: the left depicts the original {description}, while the right mirrors the left but applies {edit instruction}. We term this formulation the in-context edit prompt (IC prompt). Leveraging the T5 text encoder [36]widely adopted in DiTs for its robust sentence-level semantic understanding [11]this approach effectively interprets such extended prompts, enabling precise and contextually coherent edits. 3 Figure 4. Exploration of Two Training-Free In-Context Edit Structures (3.1). Example images for each framework are edited outputs from them. Despite some artifacts, they demonstrate potential for instruction-based editing tasks. As illustrated in Fig. 3, the in-context edit prompt (IC prompt) empowers DiT models to generate edited outputs in diptych format: the left side features an image aligned with the description, while the right side shows the same image adjusted per the edit instruction. To elucidate this mechanism, we examined the attention map of the edit prompt within the IC prompt, which reveals notably high attention values in regions slated for modification. This evidence suggests that the DiT model adeptly interprets and executes the edit instructions embedded in the IC prompt, enabling it to comprehend editing instruction and perform accordingly without requiring extensive fine-tuning. In-Context Edit Framework Building on the insights above, we propose an editing framework wherein designating the left side as reference image enables seamless application of edits to the right side. Specifically, we introduce two training-free frameworks based on text-to-image (T2I) DiT and inpainting DiT, respectively, as depicted in Fig. 4. For the T2I DiT framework, we devise an implicit reference image injection method. We first perform image inversion [1, 5, 29, 43] on the reference image, retaining attention values across layers and steps. These values are then injected into the tokens representing the left side of the diptych for image reconstruction, while the right side is generated based on the edit instruction within the predefined IC prompt during in-context generation. In contrast, the inpainting DiT framework offers more Figure 5. We augment the inpainting frameworks editing capabilities through LoRA-MoE hybrid tuning, integrating parameterefficient adaptation with dynamic expert routing for specialized feature processing and dynamic computation (3.2). straightforward approach. As it accepts both reference image and masks, we preset side-by-side image with the left side as the reference and the right side masked, guiding the inpainting process with the same IC prompt. Figure 4 illustrates both frameworks operations, with example outputs showing their capacity to retain the reference images identity during edits. However, experiments in Tab. 3 reveal that neither framework consistently delivers stable, robust results across varied editing tasks, limiting their real-world applicability. Moreover, the T2I DiT approach requires an additional inversion step, raising computational demands compared to the simpler inpainting framework. Thus, we consider the inpainting-based framework more viable candidate for further refinement. 3.2. LoRA-MoE Hybrid Fine-tuning Based on the analysis above, we summarize our method as function that maps the source image Is and edit instruction Te to the target edited output It: It = E(Is, Te) = D(IIC, M, TIC) (1) where is the inpainting diffusion transformer, IIC represents the in-context image input, with the source image Is placed on the left side and the right side masked by fixed 4 binary mask . The editing instruction Te is transformed into the in-context edit prompt TIC. LoRA Tuning. To strengthen the editing capabilities of this framework, we assemble compact editing dataset (50K samples) from publicly available sources (see Sec. 4) and employ LoRA fine-tuning [15, 49] on the multi-modal DiT blocks for parameter-efficient adaptation. This method delivers significant gains in editing success rate and quality, despite the limited dataset size. Nonetheless, challenges persist with certain tasks, e.g. style-changing and removal, which reduce the overall success rate. These findings lead us to contend that single LoRA structure is limited in its capacity and insufficient for addressing diverse editing tasks. Different editing tasks demand distinct latent feature manipulations, and mastering these varied patterns in parallel poses significant challenge. Previous LoRA tuning often focus on specific tasks, training separate weights for distinct goals, which highlights the constraints of unified LoRA model for comprehensive editing scenarios. Mixture of LoRAs. To address this limitation, we draw inspiration from recent progress in large language models, where mixture-of-experts (MoE) architectures [4, 18, 23, 25] adeptly handle diverse input patterns using specialized expert networks. The MoE paradigm offers two key advantages for our task: (1) Specialized Processing, enabling individual experts to focus on distinct feature manipulations, and (2) Dynamic Computation, allowing specific expert selection via routing mechanisms [38]. This enhances model capacity without sacrificing computational efficiency. Leveraging these advantages, we propose hybrid LoRA-MoE structure within the DiT block, incorporating parallel LoRA experts into the output projection layer of the multi-modal (MM) attention block, while using standard LoRA for parameter-efficient tuning in other layers. trainable routing classifier dynamically selects the most suitable expert for feature transformation, guided by visual token content and text embedding semantics. Specifically, we set up experts, each corresponding to LoRA module with rank and scaling factor α. For each input token, routing classifier predicts the selection probability for each expert, where [1, ]. The output of the MoE-LoRA structure is computed as follows: Output = BaseLayer(x) + α (cid:88) i=1 G(x)i Bi Ai (2) Here, Bi Rdr and Ai Rrk with min(d, k) represent the learned weights for the i-th LoRA expert, and Rk is the input tokens. The routing classifier assigns selection probability G(x)i to each expert, and the final output is the weighted sum of the experts outputs. In our implementation, we use sparse MoE setup where only the Illustration of Inference-Time Scaling Strategy Figure 6. (3.3). The upper rows demonstrate that edit success can be assessed within few initial steps. These early results are used to filter the optimal initial noise with VLM judges. top-k experts are chosen: G(x)i = softmax(TopK(g(x), k))i (3) where the TopK(, k) function retains only the top-k entries of vector at their original values, while setting all other entries to . This ensures efficient expert usage, minimizing computational overhead while maintaining flexibility for diverse editing tasks. 3.3. Early Filter Inference Time Scaling During inference, we find that initial noise significantly shapes editing outcomes, with some inputs producing results better aligned with human preferences (see Fig. 10), pattern supported by recent studies [27, 50]. This variability drives us to investigate inference-time scaling to improve editing consistency and quality. In instruction-based editing, we observe that success in instruction alignment often become evident in few inference steps (see Fig. 6), characteristic compatible with rectified flow DiT models [24, 26]. These models traverse latent space efficiently, delivering high-quality outputs with few denoising stepssometimes as few as one [9]. Thus, unlike generation tasks that demand more steps for detail and quality, we can evaluate edit success with only few steps. Based on this insight, we propose an Early Filter Inference Time Scaling strategy. We start by sampling initial noise candidates and generating preliminary m-step edit for each, where (the full denoising steps). visual large language model (VLM) then assesses these early outputs for instruction compliance, using bubble sortFigure 7. Comparison with baseline models on the Emu Edit test set (4.1). Our method demonstrates superior performance in both edit-instruction accuracy and preservation of non-edited regions compared to the baseline models. (cid:252) Zoom in for detailed view. Table 1. Quantitative results on MagicBrush test set (4.1). Following [48], all metrics are calculated between the edited image and GT edited image provided by the MagicBrush [47]. * indicates the methods cannot directly process instructive prompts and instead rely on output captions provided by the dataset. Methods Train. Params L1 CLIP-I DINO InstructP2P (SD1.5) MagicBrush (SD1.5) UltraEdit (SD3) FluxEdit (FLUX) FLUX.1 Fill (FLUX) RF-Solver Edit* (FLUX) ACE++ (FLUX) ICEdit (ours) (FLUX) 0.9B 0.9B 2.5B 12B - - 12B 0.2B 0.114 0.074 0.066 0.114 0.192 0.112 0.195 0. 0.851 0.908 0.904 0.779 0.795 0.766 0.741 0.928 0.744 0.847 0.852 0.663 0.669 0.675 0.591 0.853 inspired pairwise comparison to iteratively pinpoint the top candidate, akin to selecting the maximum value (see Fig. 6). This optimal seed is subsequently refined with n-step denoising to produce the final image. Our approach quickly identifies good noise early, while VLM selection ensures the output aligns with human preferences. Further details are provided in the supplementary materials (Sup. Mat.). 4. Experiment Implementation Details. We adopt FLUX.1 Fill, the leading open-source DiT-based inpainting model, as our backbone. To fine-tune our hybrid LoRA-MoE module, we assembled concise editing dataset from public sources. Initially, we employed the MagicBrush dataset [47], which includes 9K editing samples, but identified its limitations: 1) an imbalanced mix of editing types, 2) lack of stylefocused data, and limited domain diversity. To address these issues, we augmented it with roughly 40K samples from the open-source OmniEdit dataset [44], forming our final training set. Our model configuration features LoRA rank of 32, incorporates four experts in the MoE module, and uses TopK value of 1. In our inference-time scaling strategy, we utilize Qwen-VL-72B [2] to assess image outputs as the evaluator. Additional details on datasets, model parameters, and comparative studies are provided in Sup. Mat.. Evaluation Settings. We performed thorough evaluations on Emu [39] and MagicBrush test set [47]. For MagicBrush, which includes ground truth (GT) edited results, we closely follow [47, 48] to calculate metrics such as CLIP [14, 37], DINO [6, 33], and L1, measuring the divergence between our models results and GT. Conversely, the Emu test set lacks GT edited results; we adhere to [39, 48] for baseline assessments and supplement these with GPT4o [32], following [44], to judge editing success (see Sup. Mat.). To ensure fair comparisons, all models are evaluated using single default noise input, and NOT using our proposed Early Filter Inference Time Scaling technique. As highlighted in [20, 44, 45], conventional metrics such 6 Table 2. Quantitative results on Emu Test set (4.1). Following [39, 48], we compute CLIP-I and DINO scores between the source and edited image, while CLIP-out measures the distance between the output caption and the edited image. Additionally, we employ GPT-4o to evaluate the edited results. Methods Data Usage CLIP-I CLIP-Out DINO GPT InstructP2P [CVPR23] MagicBrush [NeurIPS23] EmuEdit [CVPR24] UltraEdit [NeurIPS24] FluxEdit [huggingface] FLUX.1 Fill [huggingface] RF-Solver Edit* [arXiv25] ACE++ [arXiv25] ICEdit (ours) 0.45M 0.47M 10M 3M 1.2M - - 54M 0.05M 0.856 0.877 0.877 0.880 0.852 0.794 0.797 0.791 0.907 0.292 0.298 0.306 0.304 0.282 0.273 0.309 0.280 0.305 0.773 0.807 0.844 0.847 0.760 0.659 0.683 0.687 0. 0.36 0.48 0.72 0.54 0.22 0.24 0.32 0.24 0.68 Figure 9. In human-centric image editing, SeedEdit prioritizes aesthetics at the expense of identity consistency, whereas our approach ensures more precise edits aligned with the intended goals. Table 3. Ablation study on model structure (4.2). We evaluation the performance of different ablation settings on Emu test set. Settings Params CLIP-I CLIP-T GPT Training-free w/o IC prompt Training-free w/ IC prompt Only MoE module LoRA (r=64) w/ IC prompt Ours w/o IC prompt Ours - - 130M 240M 214M 214M 0.681 0.794 0.929 0. 0.896 0.907 0.258 0.273 0.300 0.301 0.300 0.305 0.14 0.24 0.51 0.60 0.62 0.68 Edit, despite requiring far less training data. Compared to DiT-based models with identical backbones, our approach uses fewer samples and parameters yet delivers superior performance, highlighting its efficiency and effectiveness. Qualitative results are shown in Fig. 7 and Sup. Mat. VIE-Score Evaluation. As shown in Fig. 8, our model significantly outperforms open-source SOTA methods in both editing accuracy and visual quality. Through random seed testing, our performance approaches that of SeedEdit, and with our inference scaling strategy, it surpasses SeedEdit in overall score. While SeedEdit achieves higher PQ scoreslikely due to its polished, commercially appealing outputsit often fails in identity preservation in unedited regions. In contrast, our method maintains superior fidelity in these areas, as demonstrated in Fig. 9. 4.2. Ablation Study Model Structure. We validate our approach through experiments with various configurations, as detailed in Tab. 3. The in-context edit prompt (IC prompt) proves critical: it Figure 8. We employ the VIE-score to evaluate human preference alignment and quantify improvements from our inferencetime scaling strategy (w/ Inf. Sca.) (4.1 and 4.2). as CLIP [14, 37] and DINO [6, 33] often misalign with human preferences. To more accurately assess our models editing performance and visual quality, we also compute the VIE-Score [20]. This metric consists of two components: the SC score, which evaluates instruction adherence and preservation of unedited regions, and the PQ score, which measures visual quality independently of the source image and instructions. The overall score is derived as Overall = SC PQ. We leverage this metric to measure the gains from our inference-time scaling strategy and benchmark our approach against SeedEdit [40], top-tier closed-source commercial model. 4.1. Comparisons with State-of-the-Art Results on MagicBrush and Emu Test Sets. We evaluate our model against UNet-based [3, 39, 47] and DiTbased [28, 34, 43, 48] methods, as detailed in Tables 1 and 2. Our model achieves SOTA-comparable performance on both datasets, with outputs on MagicBrush (Tab. 1) aligning closely with GT and demonstrating robust editing proficiency. On Emu (Tab. 2), it matches SOTA text alignment while better preserving image faithfulness. Notably, our GPT-based evaluation scores significantly outperform open-source models and approach the closed-source Emu 7 Figure 10. Ablation on Inference-Time Scaling (4.2). Our strategy significantly enhances edit quality. For example, with the instruction get rid of the helmet, default fixed seed incorrectly removes the characters heada flawed outcome prevented by VLM filtering. Figure 11. Our method achieves more harmonious editing results by automatically incorporating shadow effects and style alignment, leading to significantly improved outcomes (4.3). significantly outperforms direct edit instructions in trainingfree models, and fine-tuning with IC prompts further enhances its editing capacity. Our LoRA-MoE design surpasses standard LoRA fine-tuning, achieving better editing quality and success rates (13% GPT score increase) with fewer parameters, highlighting its efficiency. Moreover, limiting adaptation only to the out. proj. layer (Only MoE) results in performance degradation, showcasing the necessity of finetuning across all model modules. Inference-Time Scaling. As illustrated in Figures 8 and 10, our inference-time scaling strategy significantly enhances editing performance, achieving 19% increase in SC score and 16% boost in overall VIE-score. When generating edits with fixed or random seed, the model produces viable results, though not always optimal. By employing VLMs to screen early-stage outputs from multiple seeds and selecting the best candidate, we achieve superior editing quality. Further comparisons are detailed in Sup. Mat. Data Efficiency. As shown in Fig. 2 and Tab. 2, our method achieves significant improvement with only 0.05M training samples compared to our training-free framework (refer to FLUX.1 fill), which is far fewer than the 10M samples used by SOTA models. This highlights the effectiveness of our Figure 12. Applications (4.3). Without additional tuning, our method demonstrates robust generalization across diverse tasks. framework and the efficiency of our fine-tuning approach. 4.3. Application Harmonious Editing. As shown in Figs. 1 and 11, our method produces harmonious edits that seamlessly align with the original image. The model intelligently adapts to the surrounding context during editing, resulting in more natural and realistic outcomesa capability that previous methods struggle to achieve. Diverse Tasks. Our method serves as versatile imageto-image framework, applicable to real-world tasks such as hand refinement and relighting, as demonstrated in Fig. 12. Future fine-tuning with task-specific datasets could further broaden its applicability across diverse scenarios. 5. Conclusion In this paper, we present In-Context Edit, novel DiT-based instruction editing method that delivers state-of-the-art performance with minimal fine-tuning data, achieving an unmatched balance of efficiency and precision. We first explore the inherent editing potential of generative DiTs in training-free context, then propose hybrid LoRA-MoE fine-tuning strategy to boost stability and quality. Additionally, we introduce an inference-time scaling approach, using VLMs to select optimal early-stage outputs from multiple seeds, enhancing edit outcomes. Extensive experiments confirm our methods effectiveness and showcase superior results. We believe this efficient, precise framework offers new insights for instruction-based image editing and plan to refine it further in future work."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel CohenOr. Stable flow: Vital layers for training-free image editing, 2024. 2, 3, 4 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2, 3, 7 [4] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts. arXiv preprint arXiv:2407.06204, 2024. 5 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing, 2023. 3, 4 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. 6, [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2 [9] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter arXiv Abbeel. One step diffusion via shortcut models. preprint arXiv:2410.12557, 2024. 5 [10] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 2, 3 [11] Daiheng Gao, Shilin Lu, Shaw Walters, Wenbo Zhou, Jiaming Chu, Jie Zhang, Bang Zhang, Mengxi Jia, Jian Zhao, Zhaoxin Fan, et al. Eraseanything: Enabling concept erasure in rectified flow transformers. arXiv preprint arXiv:2412.20413, 2024. [12] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. Ace: Allround creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086, 2024. 2 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. CoRR, abs/2208.01626, 2022. 2, 3 [14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 75147528. Association for Computational Linguistics, 2021. 6, 7 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. 5 [16] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. arXiv preprint arxiv:2410.15027, 2024. 3 [17] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. 2, 3 [18] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 5 [19] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. image synthesis evaluation. [20] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for arXiv preprint conditional arXiv:2312.14867, 2023. 3, 6, 7 [21] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free arXiv text-based editing using pre-trained flow models. preprint arXiv:2412.08629, 2024. 2, 3 [22] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. Accessed: 2024-11-12. 2, 3 [23] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 5 [25] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu 9 Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023. 2, 3, 6, 7 [40] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. [41] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. arXiv preprint arXiv:2411.15466, 2024. 2, 3 [42] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation, 2022. 3 [43] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 2, 3, 4, 7 [44] Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. arXiv preprint arXiv:2411.07199, 2024. 3, 6 [45] Yingjing Xu, Jie Kong, Jiazhi Wang, Xiao Pan, Bo Lin, and Qiang Liu. Insightedit: Towards better instruction following for image editing. arXiv preprint arXiv:2411.17323, 2024. 6 [46] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified arXiv preprint high-quality image editing for any idea. arXiv:2411.15738, 2024. [47] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024. 2, 6, 7 [48] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2025. 2, 3, 6, 7 [49] Tan Zhenxiong, Liu Songhua, Yang Xingyi, Xue Qiaochu, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 2024. 2, 3, 5 [50] Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, and Zeke Xie. Golden noise for diffusion models: learning framework. arXiv preprint arXiv:2411.09502, 2024. 5 [51] Tianrui Zhu, Shiyi Zhang, Jiawei Shao, and Yansong Tang. Kv-edit: Training-free image editing for precise background preservation, 2025. 2, 3 Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 5 [27] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. 5 [28] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 2, 7 [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 4 [30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 2 [31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 report. Gpt-4 technical arXiv preprint [32] OpenAI. arXiv:2303.08774, 2023. 6 [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 6, 7 [34] Sayak Paul. Flux.1-dev-edit-v0, 2025. Accessed: 2025-0221. 3, 7 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2 [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. CoRR, abs/1910.10683, 2019. 3 [37] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields: Weakly supervised semantic fields for robotic memory. arXiv preprint arXiv:2210.05663, 2022. 6, 7 [38] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538, 2017. 5 [39] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman."
        }
    ],
    "affiliations": [
        "DBMI, HMS, Harvard University",
        "ReLER, CCAI, Zhejiang University"
    ]
}