{
    "paper_title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
    "authors": [
        "Daeun Lee",
        "Subhojyoti Mukherjee",
        "Branislav Kveton",
        "Ryan A. Rossi",
        "Viet Dac Lai",
        "Seunghyun Yoon",
        "Trung Bui",
        "Franck Dernoncourt",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 0 7 1 0 . 2 1 5 2 : r STREAMGAZE: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos Daeun Lee1 Subhojyoti Mukherjee2 Branislav Kveton2 Ryan A. Rossi2 Viet Dac Lai2 Seunghyun Yoon2 Trung Bui2 1UNC Chapel Hill Franck Dernoncourt2 Mohit Bansal1 2Adobe Research"
        },
        {
            "title": "Project page",
            "content": "Figure 1. STREAMGAZEs task taxonomy. We introduce STREAMGAZE, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. We introduce gaze-guided past , present , and proactive streaming video understanding tasks."
        },
        {
            "title": "Abstract",
            "content": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within streaming setting. To fill this gap, we introduce STREAMGAZE, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. STREAMGAZE introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build STREAMGAZE, we develop gazevideo QA gener1 ation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all STREAMGAZE tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding. 1. Introduction mirroring the constraints faced by real streaming agents. Recent advances in Video Large Language Models (VideoLLMs) [12, 26, 33] have significantly improved multimodal understanding of dynamic visual environments. growing line of work now investigates streaming video understanding [2, 4, 29, 34], where models must process temporally incoming frames and respond in real time without access to future context. Such capability is essential for real-world applications such as robotics, embodied agents, and ARglass assistants, where perception and decision-making must occur continuously as events unfold. Benchmarks play critical role in tracking progress in this rapidly developing domain. While several streaming benchmarks exist [2, 4, 28, 31, 36], they capture only some challenges faced by realistic streaming agents. As summarized in Table 1, some benchmarks [29, 31] focus primarily on past or present recognition, leaving out proactive understanding, the ability to anticipate future events and user intentions. Moreover, existing benchmarks rarely incorporate the human perceptual signals that actually drive real-world decision-making, even when they rely on egocentric video sources and implicitly assume AR-glasses usage scenarios. Among these perceptual cues, eye gaze is the most direct and reliable indicator of where user is attending, what they are processing, and what they are likely to do next [7, 10, 11, 35]. Omitting gaze removes the key perceptual signal humans use to filter relevant information, plan actions, and anticipate what will happen next. This leads to benchmark evaluations that diverge from the way humans actually perceive and reason about dynamic environments. However, incorporating gaze into video understanding has been challenging [7, 19]. Unlike conventional QA that relies solely on visible content, gaze-guided QA requires interpreting dynamic gaze trajectories, modeling how attention shifts over time, and grounding these signals within moving egocentric viewpoint. Raw gaze streams are noisy and contain fast, unstable fluctuations [9, 16, 24], while egocentric videos continuously shake and rotate with user motion, making even \"what the user is looking at\" nontrivial spatio-temporal grounding problem. These challenges are further amplified in the streaming setting, where models must reason causally using only past and currently observed frames. Together, these factors make automatic construction of temporally grounded QA pairs highly challenging. To close this gap, we introduce STREAMGAZE, the first benchmark for gaze-guided streaming video understanding. As shown in Figure 1, STREAMGAZE provides unified suite of gaze-conditioned tasks spanning past, present, and proactive reasoning, enabling comprehensive evaluation of how well MLLMs use gaze for temporal understanding and future prediction. Solving these tasks requires models to interpret gaze online, follow shifting attention over time, and infer intention using only past and currently observed frames, To construct STREAMGAZE, we develop semiautomatic gaze-guided data generation pipeline that aligns gaze trajectories with egocentric videos and converts them into temporally grounded QA pairs. We first detect stable gaze moments (i.e., fixations) throughout each video and extract objects within the corresponding gaze regions using region-specific visual prompting. Next, we capture the temporal dynamics of gaze (i.e., scanpaths) to model how users attention shifts across spatial regions over time. Building on these signals, we automatically generate past, present, and proactive streaming tasks using an LLM, followed by human verification for quality assurance. Across all STREAMGAZE tasks, we observe substantial performance gaps between state-of-the-art MLLMs (e.g., GPT-4o, InternVL-3.5) and both human performance and specialized streaming video understanding models. In particular, current MLLMs struggle to leverage gaze signals for temporal reasoning and proactive inference, revealing fundamental limitations in gaze-conditioned understanding. We further conduct detailed analyses of gaze-prompting strategies, gaze-based reasoning behaviors, and task-specific proactive responses on STREAMGAZE. Together, these evaluations position STREAMGAZE as the first comprehensive testbed for assessing gaze-driven causal reasoning and proactive prediction in streaming video scenarios. Our contributions are threefold: We propose the first gaze-guided data construction pipeline that integrates gaze trajectories with egocentric video to produce spatio-temporally aligned, gaze-guided QA pairs. Our pipeline models full scanpath dynamics, tracking how attention evolves over time and how new objects enter the FOV through ego-motion, enabling truly streaming, temporally grounded supervision that static gazebased pipelines cannot provide. We introduce STREAMGAZE, the first benchmark specifically designed for streaming gaze-guided video understanding, comprising 8521 QA pairs across 10 tasks spanning past, present, and proactive timestamps. This represents larger and more diverse evaluation suite compared to existing streaming QA benchmarks. We evaluate state-of-the-art MLLMs on STREAMGAZE, uncovering substantial and consistent gaps relative to human performance. Our in-depth analyses of gazeprompting strategies, reasoning patterns, and task-specific behaviors reveal that current MLLMs struggle to interpret raw gaze signals, overly rely on frame-local visual cues, and fail to generalize across different temporal reasoning requirements. These findings provide concrete design guidance for future gaze-aware streaming models. 2 Table 1. Comparison of streaming video understanding benchmarks with STREAMGAZE. MC: Multiple-choice; OE: Open-ended; All-time: Covers past, present, and future tasks; Proac.: Proactive. marks the presence of feature, and indicates partial inclusion. Dataset Avg video (s) QA pairs Annotation Proac. Gaze Alltime Ego. QA type MC MC 25040 1757 OE 4500 MC+OE 7374 2814 MC+OE 2290 1020 1800 OE OE OE OE Human Auto Auto & Human Auto & Human Auto & Human Auto & Human Auto Auto & Human Auto & Human 8521 MC+OE Auto & Human Gaze-based QA benchmarks GazeVQA [7] EgoGazeVQA [19] 150 52 Streaming QA benchmarks StreamingBench [13] SVBench [31] OVO-Bench [15] OmniMMI [28] ProAssist [36] StreamBench [29] Vispeak-Bench [4] STREAMGAZE (Ours) 268 147 428 324 710 270 21 815 2. Related Work Streaming QA benchmarks. Streaming video understanding requires models to interpret and respond to temporally incoming frames without access to future context. StreamingBench [13] and OVO-Bench [15] benchmark online video understanding across past, present, and future tasks, providing comprehensive evaluation of temporal reasoning capabilities. Meanwhile, OmniMMI [28], ProAssist [36], and ViSpeak-Bench [4] extend this paradigm to interactive dialogue settings, assessing how effectively MLLMs perceive user intentions and sustain coherent interactions in streaming contexts. In contrast, our proposed STREAMGAZE introduces novel dimension by integrating eye-gaze dynamics into the streaming setting, enabling systematic evaluation of how models leverage human gaze for temporal reasoning, intention inference, and proactive prediction. Gaze-based QA benchmarks. GazeVQA [7] introduces the first gaze-based VQA dataset designed for collaborative interactions in assembly processes. EgoGazeVQA [19] focuses primarily on intent understanding in short egocentric videos, but relies only on frame-wise gaze signals without extracting fixations or establishing spatio-temporal grounding for QA generation. Moreover, none of these works address the online setting that encompasses past, present, and proactive tasks, an essential component for real-world gaze-guided applications. In contrast, STREAMGAZE is the first benchmark to integrate gaze dynamics into the streaming setting, enabling systematic evaluation of how well MLLMs perform temporal reasoning and proactively align with human attention over time. 3. Gaze-Guided Streaming Data Construction We design novel data construction pipeline to build gazeguided streaming video understanding tasks. In contrast to EgoGazeVQA [19], which processes short clips with static per-frame gaze points, our pipeline extracts the full scanpath, 3 Figure 2. MLLMs performance across STREAMGAZE tasks. sequence of fixations that captures how attention shifts over time and how objects enter the users FOV (Field of View) through ego-motion. This enables streaming and temporally grounded data construction beyond static-gaze settings. We first preprocess raw gaze trajectories by projecting them using camera parameters (Section 3.1) and extract stable fixation moments that capture meaningful, attentiondriven events (Section 3.2). Next, we divide each frame into FOV and out-of-FOV regions and extract the corresponding objects. Finally, we construct scanpaths and conduct human verificationnot only to ensure quality and consistency, but also to confirm that our pipelines data construction closely correlates with human annotations (Section 3.3). 3.1. Preprocessing We use three public egocentric video datasets spanning diverse domains: EGTEA+ (cooking) [11], EgoExoLearn (cooking, lab) [6], and HoloAssist (assembly) [27]. Each video is represented as sequence of frames, = {vt}T t=1, where is the number of frames and vt is the frame at time t. To obtain the gaze trajectory = {(xt, yt)}T t=1, where (xt, yt) denotes the gaze coordinate on the image plane corresponding to frame vt, we project raw gaze in world coordinates onto the 2D image plane using officially provided camera parameters. For datasets that directly provide 2D gaze coordinates (e.g., [6, 11]), we use them as given. 3.2. Fixation Extraction Based on the unified gaze trajectory obtained in Sec. 3.1, we identify query moments within Vtime points that likely correspond to meaningful user attentionfor streaming video QA tasks. This step is essential for gaze-guided QA generation, as it identifies the most informative moments at which user attention should be queried. We primarily target fixation moments, intervals where the gaze remains relatively stable within localized region, as they more reliably capture users visual attention compared to rapid eye shifts (i.e., Figure 3. Gaze-guided streaming data construction pipeline for STREAMGAZE. Given egocentric video sources and raw gaze projections (Sec. 3.1), we first extract fixation moments across the entire video (Sec. 3.2). Next, we divide each frame into FOV and out-of-FOV regions and extract objects within the gaze area (Sec. 3.3). Finally, we construct scanpaths and generate streaming QA pairs (Sec. 4). saccadic) [7, 8, 23]. From raw gaze trajectory G, we first identify fixation intervals. Each fixation is characterized by spatial centroid (xi, yi) and temporal span [ts and te denote the start and end timestamps. If fixations are detected, the resulting set is written as ] [1, ], where ts , te = {fi = (xi, yi, ts , te )}N i=1. (1) , te Each fixation is represented by its spatial centroid (xi, yi) and temporal span [ts ] [1, ], corresponding to the start and end timestamps of the fixation. The spatial centroid is obtained by averaging all gaze points within the interval. To extract F, we apply two criteria: (i) point-wise stability and (ii) scene consistency. Point-wise stability. We identify [ts ] as fixation when the gaze points remain spatially concentrated and temporally stable. We enforce spatial stability by requiring that , te dt = (xt, yt) (xi, yi)2 rthresh, [ts , te ], (2) where rthresh denotes the maximum spatial dispersion allowed around the fixation centroid (xi, yi). To ensure comparability across datasets with different resolutions, we normalize rthresh by the frame width. Furthermore, we ensure temporal stability by enforcing minimum duration: ts te τdur, (3) where τdur specifies the minimum time required for valid fixation. Scene consistency. Even if fixation satisfies spatial and temporal stability, abrupt scene changes may occur due to camera motion or cuts. To ensure that each fixation corresponds to visually continuous segment, we compute framewise HueSaturation histograms [22] Ht for all frames in Vi = {vt}te corresponding to fixation fi. Each Ht is normalized histogram over the hue and saturation channels of frame vt. We then measure the minimum Pearson correlation between consecutive histograms: t=ts Smin = min , te t[ts 1] ρ(Ht, Ht+1), (4) where ρ() denotes the Pearson correlation between normalized histograms. fixation is retained only if Smin τscene, ensuring that scene-consistent fixations are preserved while discontinuous segments are discarded. 3.3. Object Extraction by Gaze Area Given fixation fi, we extract objects according to their locations within or outside the FOV from fixation video clip Vi defined in Sec. 3.2. Our objective is to distinguish objects that lie inside the users FOV from those outside it, enabling controllable task difficulty and gaze-grounded QA generation. Definition of FOV and out-of-FOV regions. Motivated by classical eye-tracking literature, which models the foveal and parafoveal regions as circular areas around the gaze point and scales their pixel radius by screen resolution [7, 9, 16, 19, 24], we define the FOV region in each frame vt as circular patch: Rfov i,t = {(x, y) (x, y) (xi, yi)2 τf ov}, (5) where τf ov denotes the FOV radius and (x, y) is pixel coordinate on frame vt and (xi, yi) is spatial centroid defined in Sec. 3.2. Following the normalized dispersion strategy commonly used for fixation modeling [7], we target consistent FOV size across video sources, ensuring that FOV extraction captures comparable spatial extent regardless of video resolution. The remaining portion of the frame defines the out-of-FOV region:"
        },
        {
            "title": "Rout",
            "content": "i,t = vt Rfov i,t . (6) Across the entire fixation interval, these per-frame regions form two time-indexed sequences:"
        },
        {
            "title": "Rfov",
            "content": "i = {Rfov i,t }te t=ts ,"
        },
        {
            "title": "Rout",
            "content": "i = {Rout i,t }te t=ts . (7) Region-specific visual prompting. To extract objects from each region, we employ MLLM (InternVL3.5-38B) with spatially guided visual prompts. For FOV region Rfov i,t , we crop circular patch centered at (xi, yi) with radius rfov, and overlay small red dot at the fixation center to explicitly indicate the users point of gaze. This patch is then provided to the model as the inside-FOV visual input. For the out-ofFOV region Rout i,t , we take the original frame vt and mask the circular FOV area by replacing all pixels within radius τf ov with solid black disk (see Fig. 3). This removes all gaze-relevant content while preserving the surrounding contextual background, ensuring that the model extracts only non-attended objects. Given the region sequences Rfov outputs two corresponding object sets: and Rout , the MLLM Ofov = MLLM(Rfov ), Oout = MLLM(Rout ), (8) where each detected object is accompanied by brief (12 sentence) caption used for downstream QA generation. Scanpath generation. Based on the fixation-level object sets, we construct scanpath that represents the temporal evolution of gaze-guided object observations. This scanpath preserves the temporal order of fixations, thereby representing how attention shifts across different spatial regions and semantic contexts in the video. Given sequence of fixations defined in Sec. 3.2, we define = {(Ofov i=1 where each element corresponds to the FOV and out-of-FOV object sets extracted from the fixation clip Vi. For each fixation fi, the corresponding object sets Ofov provide the local visual context, while the sequential arrangement of these fixations forms gaze-conditioned trajectory over time. This ordered sequence serves as structured representation of user perception, which will be used to generate temporally grounded QA pairs in subsequent steps. and Oout )}N , Oout i Human verification. We finally employ human annotators to verify the generated scanpaths and extracted objects. Annotators review each fixation episode and determine 5 i and Oout whether each Ofov should be included or excluded. In particular, for Ofov , annotators correct any mislabeled or missing objects and captions. Only human-verified scanpaths and object sets are used for the final benchmark. This verification process ensures high-quality yet scalable annotations, achieving an average correctness rate of approximately 83%. Please refer to Sec. A.5 for more details. 4. STREAMGAZE We now introduce the STREAMGAZE QA generation process and its task taxonomy spanning past, present, and proactive tasks, designed to quantitatively evaluate gaze-guided streaming video understanding. 4.1. Benchmark Overview Benchmark overview. STREAMGAZE comprises over 8,521 QA pairs from 285 videos. We also provide detailed statistics in Section D. As shown in Table 1, STREAMGAZE covers 10 tasks including past , present , and proactive . Also containing gaze modality corresponding with egocentric videos, representing realistic scenario. Problem setup. We formulate gaze-guided streaming video understanding task, where the model must answer timesensitive questions while observing temporally incoming frames and user gaze. At query time tq, the MLLM receives the videogaze context (V, G) (see Sec. 3.1) within specified temporal range and produces an answer for question Q. We denote the short temporal window by ω, where ω is 60 seconds following [13, 15]. Based on the accessible portion of the stream, we define novel gaze-based three streaming tasks as follows: Past: = M(cid:0)Q; V[0, tq], G[0, tq] (cid:1) , Present: = M(cid:0)Q; V(tqω, tq], G(tqω, tq] (cid:1) , Proactive: = M(cid:0)Q; V(tq, ], G(tq, ] (cid:1) . 4.2. Task Taxonomy Given from Sec. 3.3, we construct QA pairs that require spatio-temporal reasoning over gaze transitions. 4.2.1. Past Task Past tasks focus on temporal reasoning over gaze and capture the dynamic characteristics of the users gaze, modeling how attention shifts across objects over time. Non-Fixated Object Identification (NFI): Evaluates implicit visual awareness by identifying objects that were visible but never directly fixated. For each timestamp t, we sample one never-gazed object from Oout as the correct answer and three visible objects from Ofov as distractors. See Sec. A.4 for more details. Object Transition Prediction (OTP): Assesses temporal continuity in gaze behavior by predicting the next object to be fixated. Given the current fixation on object group Ofov within the scanpath S, the correct answer is the next newly attended object Ofov i+1, while distractors are sampled from the global object pool excluding the current group. See Sec. A.4 for more details. Gaze Sequence Matching (GSM): Measures how well models capture human-like scanpath patterns through sequential gaze transitions. We extract triplets of consecutive fixation groups Ofov i1 Ofov i+1 as correct transitions, while negative options include shuffled orderings of the same groups or randomly sampled triplets that preserve transition structure. Ofov Scene Recall (SR): Tests contextual memory by recalling background objects previously visible during fixation. At each fixation Ofov , we sample the correct answer from background objects visible at other timestamps = (cid:83) Oout but not at the current one, with three visible background objects as distractors. t=i Oout 4.2.2. Present Task Present tasks capture the users perceptual state and intention. We control question difficulty using both Ofov and Oout . i Object Identification (OI, Easy/Hard): Evaluates recognition of the currently attended object within the fixation region Ofov . The correct answer corresponds to the fixated object itself. For the easy setting, distractors are randomly sampled from earlier object pools, while for the hard setting, distractors are chosen from visually similar Oout appearing in the same frame. Object Attribute Recognition (OAR): Assesses finegrained perceptual understanding by predicting visual attributes (e.g., color, shape, or texture) of the currently fixated object within Ofov . To avoid overlapping or ambiguous attribute options, we use Qwen3-VL-30B to generate distinct and contextually consistent distractors. Future Action Prediction (FAP): Models intention inference by predicting the users next action from the recent gaze-conditioned context {Ofov }, where < denotes the number of past fixation steps considered. Ground-truth future actions are derived from fine-grained human action captions aligned with gaze sequences in the original video sources. Semantically related but incorrect action descriptions are generated by Qwen3-VL-30B to serve as distractors. ik, . . . , Ofov 4.2.3. Proactive Task Proactive tasks involve gaze-based reasoning, where the model anticipates or assists future user behavior based on gaze and contextual cues. Unlike prior proactive settings [13, 15, 36], we leverage gaze as an anticipatory signal to predict upcoming attention or object events. Gaze-Triggered Alert (GTA): Triggers an alert when the user gaze specified object within Rfov . This task evaluates the models ability to indicate users fixation. 6 Object Appearance Alert (OAA): Triggers an alert when the specified object first appears in the peripheral region Rout . This task assesses whether the model can proactively recognize and react to newly emerging objects in streaming video. Overall, we filter out ambiguous questions through both Qwen3-VL-30Bbased validation and human verification. Detailed task-specific configurations and generation procedures are provided in Sec. A.4. 5. Experiments 5.1. Experimental Setup Baselines. We evaluate four categories of existing models in zero-shot setting on STREAMGAZE: (1) Closedsource MLLMs, including GPT-4o [17], Claude Sonnet4 and Opus4 [18]; (2) Gaze-based models, including AssistGaze [7]; (3) Open-source MLLMs, including Qwen2.5VL [1], InternVL3.5 [26], MiniCPM-V [32], Kangaroo [14] and VITA-1.5 [3]; (4) Open-source streaming MLLMs, including Flash-VStream [34], VideoLLM-Online [2], Dispider [21] and ViSpeak [4]. Following prior streaming benchmarks [13, 15], because non-streaming models cannot ingest live video streams, we convert all streaming tasks into offline inference by providing each model with the corresponding video clip. We additionally report human oracle performance for all tasks. Gaze input strategy. To incorporate gaze into each model, we employ visual prompting strategy inspired by [19]. For every video, we overlay (i) green dot indicating the users gaze center and (ii) red circular region representing FOV. We also prepend an instruction prompt that explicitly explains these cues to the model, e.g., The users gaze center is indicated by green dot. The red circle represents the area where the users gaze is focused. Full instruction templates are provided in the Sec. B. Evaluation metrics. For past and present tasks, which are multiple-choice questions, we use Accuracy based on exact matching (with optional fuzzy matching for semantically equivalent responses). In particular, for VideoLLMonline [2], which generates free-form text responses, we applied keyword-based matching by checking if the ground truth option keyword appeared in the response using regular expressions. For proactive remind tasks, we adopt multi-triggering query protocol following [13, 15] to simulate online decision-making. At each timestamp, the model is prompted with the cumulative video observed so far and must answer Yes if the target object has appeared in the field of view and No otherwise. We use accuracy as the primary metric, as it penalizes premature or excessive triggers and thus best reflects the reliability of proactive alerts. Additional recall results are provided in Sec. C. Table 2. Comparison of various MLLMs across STREAMGAZE tasks. We evaluate four categories of models, reporting accuracy for past and present tasks and precision for proactive tasks. Gaze information is provided to each model through visual prompting. Each section is sorted by overall performance, and the best per task across all models (excluding Human) is bolded. Params Frames Past Present Proactive NFI OTP SR GSM OI (E) OI (H) OAR FAP GTA OAA Overall Method Human GPT-4o [17] Claude Sonnet4 [18] Claude Opus4 [18] - - - - AssistGaze [7] 26M - 0.700 0.889 0. 0.707 0.960 0.920 0.800 0.840 0. 0.780 0.827 Closed-Source MLLMs 16 16 16 32 0.601 0.500 0. 0.449 0.554 0.392 0.535 0.425 0.460 0.580 0.325 0.166 0.729 0.521 0.431 0.730 0.533 0.436 0.596 0.561 0. 0.370 0.439 0.351 0.597 0.535 0.466 0.149 0.350 0.490 0.535 0.474 0.399 GazeQA-based Fine-tuned Models 0. 0.131 0.310 0.294 0.278 0.250 0. 0.254 N/A N/A 0.223 Qwen2.5-VL [25] InternVL3.5 [26] VITA 1.5 [3] MiniCPM-V [32] Kangaroo [14] ViSpeak [4] Dispider [21] Flash-VStream [34] VideoLLM-online [2] 7B 8B 7B 8B 7B 7B 7B 7B 8B Adaptive Adaptive 16 32 64 1 fps 1 fps 1 fps 2 fps 0.518 0.490 0.474 0.430 0.363 0.463 0.366 0.249 0. Open-Source MLLMs 0.350 0.311 0.365 0.374 0.365 0.450 0.573 0.346 0.354 0.319 0.483 0.548 0.378 0.296 0.275 0.590 0.627 0.455 0.334 0.454 Open-Source Streaming MLLMs 0.358 0.365 0.202 0.000 0.417 0.381 0.336 0.000 0.473 0.263 0.220 0.000 0.572 0.336 0.289 0.006 0.558 0.628 0.396 0.379 0.484 0.581 0.338 0.147 0. 0.548 0.466 0.437 0.438 0.402 0.406 0.353 0.044 0.002 0.391 0.372 0.370 0.345 0.412 0.309 0.321 0.280 0.000 0.486 0.373 0.351 0.480 0.242 0.635 0.252 0.443 0. 0.407 0.051 0.267 0.216 0.198 0.458 0.261 0.217 0.333 0.478 0.444 0.384 0.365 0.351 0.467 0.323 0.243 0.080 5.2. Overall Performance In Table 2, we report performances from all STREAMGAZE tasks from baseline MLLMs. Our evaluation brings several important findings regarding gaze-guided streaming video understanding as follows: General-purpose MLLMs struggle to leverage gaze for long-term temporal reasoning. STREAMGAZEs past and present tasks require integrating gaze information over longer temporal windows, where models must aggregate multiple gaze-conditioned observations rather than rely on single-frame cues. Although they outperform traditional gaze-based models [7], open-source MLLMs still struggle to utilize gaze signals effectively. Even when provided with gaze as visual prompt, their limited model capacity hinders the incorporation of gaze information during inference. Furthermore, these models often rely on key-frame or sparsely sampled inputs, causing them to process frames independently rather than as continuously evolving stream. As result, they fail to accumulate gaze evidence over time and cannot maintain coherent long-term temporal reasoning. Streaming MLLMs struggle with gaze-guided proactive tasks. As observed in prior work [4, 15], dialogue-based streaming models such as VideoLLM-online [2] often fail to follow task instructions and instead produce generic descriptions (e.g., You look at the camera.), due to their conversational training data. In contrast, non-dialogue streaming model, ViSpeak [4] outperforms open-source non-streaming MLLMs on proactive tasks, demonstrating the benefit of frame-by-frame online processing. However, other streaming MLLMs perform substantially worse, and all models remain far below human performance, indicating that robust gaze-guided proactive reasoning is still an open challenge. This limitation likely arises from the absence of explicit mechanisms for linking gaze shifts to predictive temporal reasoning within current streaming architectures. Gaze-based model fails to generalize to streaming settings. Although explicitly designed to utilize gaze information, AssistGaze [7] achieves only 0.223 average accuracy on our benchmark. While such models can effectively exploit gaze cues in static or short-range scenarios, they struggle to maintain temporal coherence, integrate gaze shifts over time, and track evolving user intentions. Furthermore, since AssistGaze is trained on narrowly defined GazeVQA tasks, it lacks the flexibility to generalize to proactive or temporally extended streaming tasks. substantial gap remains between human and model performance. Humans achieve an average accuracy of 0.827, substantially higher than all evaluated MLLMs. In particular, the large disparity between Scene Recall (SR) and Object Transition Prediction (OTP) suggests that current models over-focus on foreground gaze objects while neglecting the surrounding scene context and struggle to model gaze transitions. This pronounced humanmodel gap underscores the need for future models to develop broader situational perception and richer temporal memory beyond object-centric cues. Table 3. Ablation of gaze input prompting. We evaluate the effect of gaze input strategies on Qwen2.5-VL. Table 4. Ablation of different reasoning strategies. We evaluate the effectiveness of text, gaze, and visual reasoning on GPT-4o. Past Present Proactive Avg. Qwen2.5-VL [1] (w/o gaze) + test prompt + visual prompt + salience map 0.423 0.403 0.398 0.394 0.500 0.499 0.503 0.546 0.384 0.341 0.342 0.386 0.446 0.429 0.429 0.454 Reasoning Gaze Visual Text Past Present NFI 0.52 0.10 0.36 0.44 SR OI (Easy) 0.60 0.62 0.62 0. 0.64 0.68 0.72 0.72 FAP 0.41 0.47 0.40 0.44 Avg. 0.542 0.467 0.525 0.565 5.3. Additional Analysis We provide further analyses to better understand the behavior and characteristics of STREAMGAZE. Ablation of gaze input prompting. We analyze different strategies for incorporating gaze information into MLLMs using Qwen2.5-VL (7B), as shown in Table 3. Specifically, we compare three prompting methodstextual prompt, visual prompt, and salience mapto examine which representation most effectively enables the model to leverage gaze-specific cues, following [19]. In the textual prompt, fixation coordinates are provided alongside the question for each video clip. In the visual prompt, green dot indicating the gaze center and red circular region marking the FOV area are overlaid on each frame. In the salience-map prompt, the entire gaze trajectory is aggregated into single heatmap, which is then provided as an additional image input. Among these methods, the salience-map prompt achieves the best performance, particularly on present and proactive tasks, suggesting that spatially aggregated cues are more compatible with the model than raw coordinates or frame-level overlays. However, none of the prompting strategies consistently outperform the gaze-free baseline, indicating that Qwen2.5-VL is not inherently equipped to interpret or reason over raw gaze signals. More details are in Sec. C. Gaze-based reasoning in MLLMs. To investigate how MLLMs utilize gaze signals during inference, we ablate the contributions of text-, gaze-, and visual-based reasoning using GPT-4o in Table 4. For textual reasoning, we apply standard chain-of-thought prompting (e.g., Lets think step by step). For gaze-based reasoning, GPT-4o is first instructed to estimate the gaze point before generating an answer. For visual reasoning, the model is guided to identify visible objects with bounding boxes and use this visual evidence when answering. The best average performance is achieved when all reasoning strategies are combined. However, their effectiveness varies across tasks: incorporating visual cues benefits Scene Recall (SR) by improving contextual grounding, but can hinder Non-Fixated Object Identification (NFI), where excessive focus on gaze regions may suppress exploration of unseen objects. More details and qualitative examples are in Sec. C. Task-Specific Behavior in STREAMGAZE. As shown in Figure 4 and Tab. 4, interestingly different STREAMGAZE tasks respond unevenly to the same input gaze or reasoning Task-wise effect of gaze input strategies on Figure 4. STREAMGAZE. Different tasks respond unevenly to textual, visual, and salience-map prompts, revealing strong task-specific behavior in streaming gaze understanding. Figure 5. Offline MLLMs behavior on STREAMGAZE proactive tasks. We evaluate type1 (false positive) and type2 (false negative) error from Qwen2.5-VL, InternVL-3.5, and GPT4o. strategies. These patterns indicate that each STREAMGAZE task require distinct visual, temporal, and attentional requirements, and therefore single, uniform prompting or reasoning strategy is insufficient. more adaptive, task-aware, or dynamically integrated approach is needed to effectively model the full spectrum of streaming gaze understanding. Proactive Behavior of MLLMs. In Figure 5, we analyze how offline MLLMs behave on STREAMGAZE proactive remind tasks by measuring Type 1 (false positive) and Type 2 (false negative) error rates across each task. This evaluation reveals strong model-specific biases: InternVL3.5-8B consistently over-triggers with very high false positive rates, GPT4o deteriorates as tasks become harder with rising errors of both types, while Qwen2.5-VL-7B shows the most balanced pattern, remaining accurate in hard settings despite conservative behavior on easy ones. These results demonstrate that proactive gaze prediction is highly sensitive to model characteristics and underscore the need to examine error-type distributions when designing reliable gaze-based assistive systems. 6. Conclusion In this work, we introduce STREAMGAZE, the first benchmark for evaluating gaze-guided temporal and proactive rea8 soning in streaming video understanding. Our gazevideo QA pipeline provides human-aligned supervision across diverse past, present, and proactive tasks. Experiments reveal substantial gaps between state-of-the-art MLLMs and human annotators, especially in intention modeling and proactive prediction. We hope STREAMGAZE advances the development of more human-aligned, attention-aware streaming models."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 8 [2] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. 2, 6, 7 [3] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025. 6, 7 [4] Shenghao Fu, Qize Yang, Yuan-Ming Li, Yi-Xing Peng, KunYu Lin, Xihan Wei, Jian-Fang Hu, Xiaohua Xie, and Wei-Shi Zheng. Vispeak: Visual instruction feedback in streaming videos. arXiv preprint arXiv:2503.12769, 2025. 2, 3, 6, 7, 5 [5] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 2, 6 [6] Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, et al. Egoexolearn: dataset for bridging asynchronous ego-and exo-centric view of procedural activities in real world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22072 22086, 2024. 3, 2, 5, 6 [7] Muhammet Ilaslan, Chenan Song, Joya Chen, Difei Gao, Weixian Lei, Qianli Xu, Joo Lim, and Mike Shou. Gazevqa: video question answering dataset for multiview eye-gaze taskoriented collaborations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1046210479, 2023. 2, 3, 4, 5, 6, [8] Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, and Koichiro Yoshino. gaze-grounded visual question answering dataset for clarifying ambiguous japanese questions. arXiv preprint arXiv:2403.17545, 2024. 4 [9] Lisa Kroell and Martin Rolfs. Foveal vision anticipates defining features of eye movement targets. Elife, 11:e78106, 2022. 2, 4 [10] Bolin Lai, Fiona Ryan, Wenqi Jia, Miao Liu, and James Rehg. Listen to look into the future: Audio-visual egocentric 9 gaze anticipation. Vision, pages 192210. Springer, 2024. 2 [11] Yin Li, Miao Liu, and James Rehg. In European Conference on Computer In the eye of the beholder: Gaze and actions in first person video. IEEE transactions on pattern analysis and machine intelligence, 45(6): 67316747, 2021. 2, 3, 5 [12] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 2 [13] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628, 2024. 3, 5, 6 [14] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 6, 7 [15] Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1890218913, 2025. 3, 5, 6, 7, 4 [16] Antje Nuthmann and Teresa Canas-Bajo. Visual search in naturalistic scenes from foveal to peripheral vision: comparison between dynamic and static displays. Journal of Vision, 22(1):1010, 2022. 2, [17] OpenAI. Gpt-4 technical report, 2024. 6, 7 [18] Anthropic PBC. Introducing claude 4. Online blog post, 2025. https://www.anthropic.com/news/claude4 (accessed 2025-11-14). 6, 7 [19] Taiying Peng, Jiacheng Hua, Miao Liu, and Feng Lu. In the eye of mllm: Benchmarking egocentric video intent understanding with gaze-guided prompting. arXiv preprint arXiv:2509.07447, 2025. 2, 3, 4, 6, 8, 5 [20] Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Kumar Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, et al. Hd-epic: highly-detailed egocentric video dataset. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2390123913, 2025. 5, 6 [21] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2404524055, 2025. 6, 7 [22] Nisreen I. Radwan, Nancy M. Salem, and Mohamed I. El Adawy. Histogram correlation for video scene change detection. In Advances in Computer Science, Engineering & Applications: Proceedings of the Second International Conference on Computer Science, Engineering and Applications (ICCSEA 2012), May 2527, 2012, New Delhi, India, Volume 1, pages 647655, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. [36] Yichi Zhang, Xin Luna Dong, Zhaojiang Lin, Andrea Madotto, Anuj Kumar, Babak Damavandi, Joyce Chai, and Seungwhan Moon. Proactive assistant dialogue generaarXiv preprint tion from streaming egocentric videos. arXiv:2506.05904, 2025. 2, 3, 6 [23] Jun Rekimoto. Gazellm: Multimodal llms incorporating human visual attention, 2025. 4 [24] Emma EM Stewart, Matteo Valsecchi, and Alexander Schütz. review of interactions between peripheral and foveal vision. Journal of vision, 20(12):22, 2020. 2, 4 [25] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 7 [26] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 2, 6, 7 [27] Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, et al. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2027020281, 2023. 3, 1, 2, [28] Yuxuan Wang, Yueqian Wang, Bo Chen, Tong Wu, Dongyan Zhao, and Zilong Zheng. Omnimmi: comprehensive multimodal interaction benchmark in streaming video contexts. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1892518935, 2025. 2, 3 [29] Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, and Huchuan Lu. Streaming video understanding and multi-round interaction with memory-enhanced knowledge. arXiv preprint arXiv:2501.13468, 2025. 2, 3 [30] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 3, 4 [31] Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. Svbench: benchmark with temporal multi-turn dialogues for streaming video understanding. arXiv preprint arXiv:2502.10810, 2025. 2, 3 [32] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6, 7 [33] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2 [34] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memorybased real-time understanding for long video streams, 2024. 2, 6, 7 [35] Mengmi Zhang, Keng Teck Ma, Joo Hwee Lim, Qi Zhao, and Jiashi Feng. Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 43724381, 2017. 10 STREAMGAZE: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos"
        },
        {
            "title": "Supplementary Material",
            "content": ". A. STREAMGAZE Data Construction Details . . . . . . A.1. Gaze Projection . . . A.2. Fixation Extraction . A.3. Object Extraction by Gaze Area. . . A.4. QA Generation and Filtering . . . A.5. Human Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. STREAMGAZE Evaluation Details C. Additional Analysis C.1. Fine-tuning Results . . . . C.2. Details of Gaze Input Prompting . C.3. Details of Gaze-based Reasoning . . . . . . . D. STREAMGAZE Data Details . . D.1. Data Statistics D.2. QA Examples . . . . . . . . . . . . . . . . . . . . . E. Limitations and Future Work F. License . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 2 2 3 4 5 5 5 6 6 7 7 8 8 A. STREAMGAZE Data Construction Details In this section, we elaborate details about STREAMGAZE data construction pipeline. We first introduce details of gaze projection (Sec. A.1) during preprocessing Sec. 3.1 and fixation extraction (Sec. A.2). Then, we introduce object extraction details with prompt (Sec. A.3) and QA generation details by each task (Sec. A.4). Finally, we introduce human annotation process details (Sec. A.5). We also provide the full data construction procedure in Algorithm 1. A.1. Gaze Projection To obtain frame-level gaze positions on the egocentric RGB video, we convert each 3D gaze ray into 2D pixel coordinate using camera poses and calibrated intrinsics for the HoloAssist dataset [27]. This method follows the handeye projection procedure implemented in official code.1 Each gaze sample provides world-coordinate origin o(t) and direction d(t), while the camera stream provides per-frame extrinsic matrices and intrinsics. Temporal alignment. All signals, including video frames, camera poses, and gaze samples, share common timestamp domain. For each video frame at time t, we select 1https : / / github . com / taeinkwon / PyHoloAssist / blob/main/hand_eye_project.py 1 Algorithm 1 Gaze-Guided Data Construction Pipeline = {vt}1:T , = {(xt, yt)}T 1: Input: t=1, rthresh, τdur, τscene 2: # Fixation Extraction 3: 4: for each candidate interval [ts Compute centroid (xi, yi) 5: , te if [ts 6: ts if te 7: Smin minte if Smin τscene then , te ] do ], (xt, yt) (xi, yi)2 rthresh then τdur then 1 t=ts ρ(Ht, Ht+1) {(xi, yi, ts , te )} 8: 9: 10: end if end if 11: 12: 13: 14: end for end if 15: # Object and scanpath extraction 16: for each fi do . . . te for = ts 17: Define Rfov 18: 19: 20: do i,t and Rout i,t MLLM(Rfov ) MLLM(Rout ) end for Ofov Oout 21: 22: end for 23: {(Ofov , Oout )}N i=1 24: # QA Generation the nearest timestamped camera pose and gaze measurement: (ot, dt) = (o(t), d(t)) where = arg min τ τ t. This nearest-neighbor alignment ensures consistent pairing despite differing sampling rates. 3D gaze point construction. Given the aligned 3D gaze ray, we form virtual fixation point by moving fixed distance deye along the normalized gaze direction: pworld = ot + deye dt dt . as the gaze target for frame vt. We treat pworld World-to-camera transformation. Let world cam (t) be the camera pose associated with frame vt. We convert the world point into camera coordinates via pt,cam = Taxis cam (t)(cid:1)1 (cid:20)pworld (cid:0)T world 1 (cid:21) . Next, we measure visual continuity between consecutive frames by computing the histogram correlation using Pearsons coefficient: (cid:80) h,s(Ht(h, s) µt) (Ht+1(h, s) µt+1) h,s(Ht(h, s) µt)2 (cid:113)(cid:80) Pinhole projection. With the intrinsic matrix ρ(fi(t), fi(t + 1)) = = fx 0 0 0 fy 0 , cx cy we project the 3D camera-coordinate point (Xt, Yt, Zt) onto the image plane: ut ="
        },
        {
            "title": "Xt\nZt",
            "content": "fx + cx, vt ="
        },
        {
            "title": "Yt\nZt",
            "content": "fy + cy. point is marked as in-frame if (ut, vt) lies within the image boundaries. Output. For each frame vt, the resulting projected coordinates (ut, vt) define the 2D gaze position on the image plane. We therefore set (xt, yt) = (ut, vt) to construct the gaze trajectory = {(xt, yt)}T t=1 used in the main paper. In addition, we store the original gaze validity flag and an in-frame indicator. For visualization, we simply draw dot at (xt, yt) on the RGB frame. A.2. Fixation Extraction Handling short gaze interruptions. Human gaze often contains brief microsaccades or flicks that momentarily violate spatial stability. To avoid prematurely terminating fixation, we permit short interruptions within fixation interval. For each timestamp t, if the gaze deviation exceeds the spatial threshold (dt > τthresh), we examine the temporal gap between consecutive gaze samples: = (t 1). If 200ms (a short interruption threshold), the deviation is treated as transient flick rather than fixation termination, and the fixation interval continues without interruption. Scene consistency. To ensure that each detected fixation corresponds to visually coherent segment, we further apply scene-consistency filter based on frame-wise HueSaturation histogram similarity. For fixation interval fi = (xi, yi, ts ), we uniformly sample frames from the corresponding video subsequence Vi = {vt}te and convert each sampled frame into the HSV color space. For every frame vt, we compute normalized 2D histogram over the HueSaturation channels: , te t=ts Ht(h, s) = HistHSV (vt(h, s)), [0, 180], [0, 256], with (cid:80) h,s Ht(h, s) = 1. 2 (cid:113)(cid:80) . h,s(Ht+1(h, s) µt+1)2 (9) where µt and µt+1 denote the mean values of Ht and Ht+1."
        },
        {
            "title": "Collecting similarities across the entire episode gives",
            "content": "S = (cid:8)ρ(fi(t1), fi(t2)), . . . , ρ(fi(tN 1), fi(tN ))(cid:9). (10) Smin = min(S). (11) fixation is retained only if Smin τscene, where τscene is an empirically set threshold (typically τscene 0.9). Intervals failing to satisfy this constraint are considered to include scene changes and are discarded. A.3. Object Extraction by Gaze Area. FOV definition. When camera intrinsics are available [27], we compute the horizontal field-of-view (HFOV) using the intrinsic matrix and frame image width Wpixels: HF OVrad = 2 arctan (cid:19) (cid:18) Wpixels 2fx HF OVdeg = HF OVrad 180 π If intrinsics are unavailable [5, 6, 11], we assume canonical HF OVdeg = 90. Following standard visual-angle conventions in [9, 16, 24], the perifoveal region spans approximately 515. For our FOV threshold, we adopt the upper perifoveal bound and set rdeg = 15, which specifies the angular radius that is later converted into the pixel threshold τfov. We then convert this angular radius into pixel radius via px/deg = Wpixels HF OVdeg , τfov = rdeg px/deg. This τfov serves as the pixel threshold used in Sec. 3.3 to determine whether an object lies inside or outside the users FOV. Region-specific visual prompting. To extract objects conditioned on human gaze, we design region-specific visual prompting strategies for InternVL-3.5 (38B). As shown in Fig. 17 and Fig. 18, we provide two distinct prompts: one for identifying objects within the users FOV and another for discovering objects located outside the FOV. For the in-FOV setting, we provide the model with the fixation-aligned video clip and explicitly instruct it to (i) generate brief scene-level description, (ii) identify the precisely gazed object indicated by the green dot, and (iii) describe additional objects located within the perifoveal region. The prompt enforces consistent naming by providing an object pool and requires detailed, attribute-rich captions for each recognized object. For the out-of-FOV setting, we mask the central region of the frame and prompt the model to detect only objects outside the masked area. The prompt includes similar constraints on naming, appearance grounding, and relational descriptions, while removing all references to the gazed object. Both prompts further include human-centric rulese.g., describing persons only when their full or upper body is visible, and requiring identity strings such as person wearing [clothing description] rather than generic labels. These region-specific prompts guide the MLLM to reliably extract spatially grounded object sets that are aligned with gaze-conditioned context, enabling consistent FOVaware and out-of-FOV object discovery across datasets. A.4. QA Generation and Filtering In this section, we provide details of QA generation and filtering for each STREAMGAZE task. We also elaborate how we decide query time tq for each task. A.4.1. Non-Fixated Object Identification (NFI) NFI evaluates whether model can identify objects that were visible in the scene but never directly fixated by the user. i(Ofov Oout QA generation and filtering. We collect all FOV/outFOV objects and define the scene-wide object pool as = (cid:83) ). Objects that were visible yet never fixated are defined as Ni = Ofov . For each index i, if Ni is nonempty, we sample the correct answer from Ni and choose three distractors from the prefix FOV object set Ofov . Since NFI does not require visual verification, we rely directly on human-verified scanpaths and perform only formatting and consistency checks. A.4.2. Scene Recall (SR) SR evaluates contextual memory by determining which background object was previously visible but not visible during the current fixation. QA generation and filtering. We construct the global background pool as = (cid:83) Oout . At fixation i, the correct answer is sampled from Oout , while distractors are choi sen from the currently visible background objects Oout . For filtering, we apply three consistency checks with Qwen3-VL: (1) the question must be clear and unambiguous, (2) the selected answer must be an object that was never fixated, and (3) the answer object must appear in the video. Query time. We define the query timestamp tq as the moment when the user first begins to fixate on any of the objects mentioned in the question. This ensures that the model is evaluated exactly when the relevant scene context becomes available to the user. A.4.3. Object Transition Prediction (OTP) OTP evaluates temporal continuity in gaze by predicting the next newly attended object. and Ofov i+1 that does not belong to Ofov QA generation and filtering. For consecutive timestamps (i, + 1), we use Ofov i+1. The correct answer is the first object in Ofov , corresponding to the newly attended object. Distractors are sampled from the global object pool, excluding both Ofov and the correct answer. For filtering, since OTP transitions come directly from human-verified scanpaths, no additional visual filtering is required. Query time. We define the query timestamp tq as the moment when the fixation on the current object group Ofov first begins, marking the onset of the transition. To compute the response window, we use the full temporal span of the transition event: we start two seconds before Ofov becomes visible in the FOV and end two seconds after the fixation on the newly attended object Ofov i+1 concludes. This window provides continuous context across the transition, capturing both the preceding fixation pattern and the complete fixation period of the next object. Query time. Let fi denote the set of distinct fixation objects observed up to timestamp as defined in Sec. 3.2. We define the query timestamp tq as the earliest time at which three unique fixation objects have been accumulated: A.4.4. Gaze Sequence Matching (GSM) GSM evaluates whether models capture sequential gaze patterns across time. tq = min{ fi 3 }. At this point, we determine which objects have already been seen and which have not. To compute the response window, we gather all intervals in which the distractor objects (i.e., previously fixated objects) appear within the FOV. We then take the earliest and latest of these intervals and expand the window by two seconds on both ends, producing consistent response interval that fully captures the temporal range in which distractor objects are visible. i+1, Ofov i+1 Ofov QA generation and filtering. We extract consecutive , Ofov 3-step fixation sequences [Ofov i+2] and serialize them as Ofov Ofov i+2 to form the correct sequence. Three negative options are generated: one using shuffled ordering of the same groups, and two using random groups sampled from the global object pool while preserving structural validity. To prevent ambiguous QA pairs, we automatically remove samples with Qwen3-VL [30] where any distractor sequence overlaps with the correct sequence in more than 50% of segments. Query time. We define the query timestamp tq as the end of the third fixation group Ofov i+2, which marks the completion of the three-step transition sequence used for the question. The response window spans the entire sequence interval, beginning from the onset of the first fixation group Ofov and extending to the end of the third group Ofov i+2. This window provides continuous temporal context for evaluating whether the model can recognize the correct sequential gaze pattern across all three segments. A.4.5. Object Identification (OI, Easy/Hard) OI (Easy/Hard) evaluates whether models can correctly recognize the object currently fixated within the gaze-aligned cropped region Ofov . QA generation and filtering. In the easy task, distractors are sampled from all objects appearing anywhere in the video, excluding the target object and all objects within the cropped FOV region. In the hard task, distractors are drawn exclusively from Oout , i.e., objects visible in the same frame but outside the fixation region. If fewer distractors than required are available, the corresponding sample is discarded to avoid trivial or ambiguous questions. Query time. Each prediction is evaluated within the response interval aligned with the corresponding fixation episode. We therefore define the query timestamp tq as the start time of the fixation. A.4.6. Object Attribute Recognition (OAR) OAR evaluates fine-grained perceptual understanding by asking the model to infer specific visual attribute (e.g., color, material, shape, texture, size, or state) of the object currently fixated. QA generation and filtering. To construct reliable attribute questions, we employ constrained LLM-based template system. We define fixed dictionary of attribute types and pair each type with corresponding question template (e.g., color What color is this object?). Given the caption of Ofov , Qwen3-VL-30B [30] is prompted to: (1) select the most appropriate attribute type, (2) extract concise correct answer grounded in the caption, and (3) generate three visually plausible but incorrect distractors that do not overlap semantically with the correct attribute. To remove ambiguous or semantically entangled distractors, we apply an additional verification step using Qwen3-VL [30], retaining only unambiguous multiple-choice sets. Query time. Similar to the OI task, we define the query timestamp tq as the start time of the fixation. A.4.7. Future Action Prediction (FAP) FAP evaluates proactive intention inference by predicting the users upcoming action based on the recent sequence of fixations. QA generation and filtering. For each fixation index i, we extract 3-step fixation sequence capturing the shortterm progression of the users attention. We then align this sequence with the official action annotations of the video and identify the earliest future action whose timestamp occurs at least small temporal margin (3 seconds to 1 minute) after the fixation ends. This action is assigned as the correct label. Distractor options are sampled from other actions within the same video, while removing near-duplicate or semantically ambiguous actions to ensure clear discrimination. Query time. We define the query timestamp tq as the moment when the fixation sequence ends, ensuring that the model makes predictions immediately after observing all relevant gaze information. The response window begins two seconds before the actual action timestamp, preventing temporal leakage while anchoring the prediction to the correct action segment. A.4.8. Gaze-Triggered Alert (GTA) GTA evaluates whether model can proactively monitor the users fixation and trigger an alert as soon as specified target object enters the fixation region Rfov . For each video, we first gather all objects that were directly fixated at least once, excluding background furniture or static structures using Qwen3-based filtering [30]. We then sample multiple evaluation timestamps relative to the objects first fixation moment (e.g., 20, 10, t, + 10, + 20 seconds), following the multi-query design of OVO-Bench [15]. At each timestamp, we determine whether the user is currently fixating on the object and assign binary label accordingly (type 1 for fixation, type 0 for non-fixation). For quality control, we manually verify the fixation labels to ensure accuracy. A.4.9. Object Appearance Alert (OAA) OAA evaluates whether the model can proactively detect objects that appear in the peripheral region Rout , i.e., objects that are visible in the frame but have not yet entered the users fixation. We extract all Oout at each timestamp, removing background furniture or static structures using Qwen3-based filtering [30]. For each selected object, we generate evaluation timestamps (e.g., 20, 10, t, + 10, + 20 seconds) relative to its first appearance, using offsets centered around the event boundary. At each timestamp, we determine whether the object is currently visible in the frame but outside the fixation region, assigning type 1 when present and type 0 otherwise. For quality control, we use Qwen3-VL [30] to confirm that the object indeed appears within the 10-second evaluation window and additionally perform manual verification to ensure accuracy. A.5. Human Verification As shown in Fig. 22, we employ three annotators and provide them with an HTML-based interface for verification. 4 Figure 6. Data statistics of STREAMGAZE. We report domain proportion, video length, world cloud for FOV extracted objects. way multiple-choice questions. Because models may output answers in diverse natural-language formats, we extract the predicted option by deterministic parsing rules, prioritizing concise outputs such as final standalone letter or expressions like the answer is B, while also supporting formats such as C. pot. Accuracy is reported as the proportion of correctly answered questions over the total number of questions for each task. Proactive evaluation. The two proactive tasks follow an OVO-Benchstyle configuration [15], where evaluation is conducted at series of checkpoints rt. Each rt corresponds to an evaluation timestamp sampled around the target objects first fixation time (e.g., 20, 10, t, + 10, + 20 seconds), representing moments at which proactive assistant should determine whether the user is attending to the target object. At each checkpoint, the model receives the cumulative video prefix [0, rt] and outputs binary yes/no decision indicating whether the object lies within the fixation region. Each checkpoint is labeled as positive case if the target is fixated and negative otherwise, and we compute accuracy across all checkpoints. This setup evaluates whether model can continuously monitor gaze signals and trigger alerts with precise temporal fidelity. C. Additional Analysis C.1. Fine-tuning Results To better understand the underlying failure of MLLMs and assess whether performance can be improved through gazesupervised learning, we conduct LoRA-based fine-tuning experiments. Specifically, we fine-tune ViSpeak [4] on our gaze-guided training set to quantify the potential performance gains achievable through targeted adaptation. Fine-tuning dataset. We construct four variants of training data for our fine-tuning experiments, drawing from both external resources and our own automatically generated data. First, we include external gaze-based QA datasets such as HD-EPIC [20] (2k) and EgoGazeVQA [19] (1.2k), which provide high-quality gazequestionanswer supervision despite not being strictly streaming-based. Second, Figure 7. Task proportion of STREAMGAZE. We summarize the sample distribution for each task in STREAMGAZE. Table 5. Human verification results per each video source. We report object inclusion/modification ratio after human verification. Inclusion Ratio (%) Modified Ratio (%) EGTEA-Gaze [11] HoloAssist [27] EgoExoLearn [6] 81.77 67.88 84.31 6.89 9.99 7.24 Each annotator is shown the fixation-level video clip along with the objects extracted by the MLLM: the precisely gazed object (marked by green dot), other objects within the FOV region, and out-of-FOV objects. (Details of the provided instructions are shown in Fig. 24.) Annotators then determine whether each object should be included or excluded based on the visual evidence. For the gazed (pointing) object, annotators additionally correct or rewrite its identity and detailed caption when necessary. We report the inclusion ratio and modification ratio in Tab. 5. The inter-annotator agreement, measured using Fleiss Kappa, is 0.60, indicating consistency among annotators. B. STREAMGAZE Evaluation Details Multiple-choice question evaluation. For all past and present tasks, we construct the models input clip according to the prior streaming benchmarks [13, 15]. Past tasks receive the full history from the beginning of the video up to the query timestamp [0, tq], enabling long-range reasoning. Present tasks use 60-second sliding window [tq 60, tq] (clamped to start at 0), reflecting real-time processing with limited recent context. All these tasks are evaluated as four5 Table 6. Fine-tuning results on ViSpeak [4]. We construct four variants of training data for our fine-tuning experiments, drawing from both external resources [19, 20] and our own automatically generated data based on Ego4D-Gaze [5] and EgoExoLearn [6]. External [19, 20] Fine-tuning datasets EgoExoLearn [6] Ego4D-Gaze [5] NFI OTP SR GSM OI (E) OI (H) OAR FAP GTA OAA Past Present Proactive 0.526 0.467 0.592 0.517 0.583 0.486 0.477 0.563 0.545 0. 0.341 0.300 0.469 0.495 0.495 0.418 0.347 0.385 0.373 0.388 0.635 0.368 0.622 0.540 0.601 0.477 0.562 0.616 0.452 0.641 0.413 0.420 0.544 0.496 0.575 0.489 0.392 0.359 0.351 0. 0.504 0.147 0.474 0.487 0.409 0.502 0.381 0.792 0.737 0.651 Overall 0.479 0.386 0.542 0.500 0.521 we leverage QA pairs generated by our data construction pipeline. Because the scanpath inclusion ratio between our pipeline and human annotators is approximately 78% (see Tab. 5), we further apply the same pipeline to EgoExoLearn [6] and Ego4D-Gaze [5], which do not overlap with STREAMGAZE video sources, to automatically synthesize additional STREAMGAZE-style training data. This serves as form of data augmentation, expanding coverage and increasing the diversity of gaze-conditioned supervision. Implementation details. The base model is ViSpeak, built on Qwen2.5-Instruct with an InternViT-300M vision tower and frozen VITA-1.5 audio encoder. We enable LoRA with rank 128 and α = 256, and train the MLP-based multimodal projector with learning rate of 1105. We use batch size of 2 per device (gradient accumulation 2), maximum sequence length of 5500 tokens, and train for 2 epochs. Negative sampling for proactive task fine-tuning { \"conversations\": [ {\"from\": \"human\", \"value\": \"Monitor and alert when gaze <screen>\", \"time\": 91.0}, {\"from\": \"gpt\", \"value\": \"\", \"time\": 2783.0}, {\"from\": \"gpt\", \"value\": \"\", \"time\": 2793.0}, {\"from\": \"gpt\", \"value\": \"You are now gazing <screen>.\", \"time\": 2803.0} ], \"proactive\": true } Figure 8. Example of negative sampling strategy for proactive task fine-tuning. We apply negative sampling to fine-tune ViSpeak. Proactive task fine-tuning. For proactive reminder tasks, we fine-tune the model using negativepositive sampling strategy together with ViSpeaks informative head. As illustrated in Fig. 8, all timestamps before the target objects first fixation are treated as negative instances, where the model is trained to output an empty response so that it learns when it should remain silent. At the first fixation timestamp, positive instance is provided and the model is trained to generate the alert message, teaching it the exact moment notification should be triggered. Each training example is formatted as short multi-turn sequence in which early turns contain empty responses and the first positive turn contains the alert (e.g., You are now gazing <screen>.\"), clearly marking the transition from non-trigger to trigger conditions. Quantitative results. As shown in Tab. 6, training only on external gaze QA datasets lowers the overall accuracy, indicating that these datasets alone do not transfer effectively to the streaming gaze reasoning setting. In contrast, adding automatically generated STREAMGAZE-style data leads to clear improvements. EgoExoLearn is particularly helpful because it is in-domain with respect to egocentric manipulation scenes, making its synthesized gaze supervision more aligned with the target distribution. Incorporating Ego4D-Gaze provides additional gains, and combining all data sources yields the best overall accuracy of 0.521. However, because STREAMGAZE covers wide range of tasks, the resulting data distribution is inherently imbalanced, so performance does not increase uniformly across all tasks. Overall, these results show that in-domain synthetic gaze supervision is essential for improving ViSpeak, while external datasets alone are insufficient for this setting. C.2. Details of Gaze Input Prompting We provide each gaze-prompting instruction in Fig. 9, inspired by [19], to obtain the Qwen2.5-VL performances reported in Tab. 3. For the text gaze prompting setting, we extract the users fixation center prior to the query time and embed the corresponding fixation coordinates into the textual prompt. To construct salience-map prompt, we follow Algorithm 1 from Appendix of [19] without modification, compressing the entire gaze trajectory into single salience heatmap. We then prepend this salience map to the input video frames so that the MLLM jointly attends to both the visual stream and the gaze-derived salience distribution. Tab. 7 also provide detailed performance per each task on GPT-4o, InternVL-3.5 and ViSpeak. C.3. Details of Gaze-based Reasoning We illustrate the input formats for gaze-, text-, and visualbased reasoning in Figs. 19 to 21. We further provide qualitative comparisons between text-only reasoning and gazeor visual-guided reasoning in Figs. 10 and 11. In Fig. 10, the gaze-based reasoning correctly identifies the attended object 6 Figure 9. Gaze prompting strategies for STREAMGAZE We adopt three strategies for inputting gaze information to MLLMs. Table 7. Detailed performance of gaze input prompting across each STREAMGAZE task. We ablate effect of gaze input prompting on GPT-4o [17], InternVL3.5 [26] and ViSpeak [4]. Method Params Frames Past Present Proactive NFI OTP SR GSM OI (E) OI (H) OAR FAP GTA OAA GPT-4o [17] + visual prompt InternVL3.5 [26] + visual prompt ViSpeak [4] + visual prompt - - 8B 8B 7B 7B 16 16 Adaptive Adaptive 1 fps 1 fps 0.601 0.601 0.469 0.490 0.493 0. 0.459 0.449 0.370 0.311 0.374 0.358 0.507 0.535 0.526 0.573 0.417 0. 0.607 0.580 0.510 0.548 0.521 0.473 0.725 0.729 0.626 0.627 0.591 0. 0.731 0.730 0.637 0.628 0.560 0.581 0.594 0.596 0.463 0.466 0.438 0. 0.375 0.370 0.382 0.372 0.336 0.309 0.608 0.597 0.376 0.373 0.625 0. 0.148 0.149 0.048 0.051 0.334 0.458 Overall 0.536 0.535 0.441 0. 0.469 0.467 by leveraging the fixation location, whereas the text-only reasoning fails due to relying solely on language priors without visual grounding. In Fig. 11, the visual-based reasoning succeeds by using the detected object region to determine which background item was outside the users field of view, while the text-only reasoning struggles because it lacks explicit spatial grounding. D. STREAMGAZE Data Details D.1. Data Statistics Overall statistics. We provide STREAMGAZE data statistics in Figs. 6 and 7 and Tab. 8. From Fig. 7: (a) The videos cover three domainscooking, laboratory work, and assemblywith cooking comprising the majority. This distribution reflects realistic egocentric environments while still offering diverse interaction patterns. (b) The word cloud highlights frequently appearing FOV objects such as hands, knives, gloves, bottles, and cutting boards, indicating that the extracted objects largely correspond to action-related tools and manipulable items. (c) Video lengths vary from just few minutes to more than half an hour, with most clips between 5 and 15 minutes. This range provides both short segments for immediate reasoning and longer sequences suitable for multi-step or extended interactions. Attribute statistics of OAR task. For the OAR (Object Attribute Recognition) task, we generate attribute-focused questions centered on the gazed objects properties. The distribution covers color (65.61%), material (19.3%), state (5.5%), texture (5.3%), and shape (4.1%), reflecting the dominant attribute types encountered in egocentric manipulation scenes. 7 Table 8. Statistics of StreamGaze tasks. Task # Samples Gaze Sequence Matching Non-Fixated Object Identification Object Transition Prediction Scene Recall Future Action Prediction Object Attribute Recognition Object Identification (Easy) Object Identification (Hard) Gaze-Triggered Alert Object Appearance Alert Total 186 650 494 211 921 1,419 1,487 1,005 283 1,865 8,521 D.2. QA Examples We provide data examples per each STREAMGAZE task for better understanding in Figs. 12 to 16. An orange question mark in each task figure denotes query time. E. Limitations and Future Work Although STREAMGAZE offers the first comprehensive benchmark for gaze-guided streaming reasoning, some limitations suggest promising avenues for future work. STREAMGAZE spans ten heterogeneous tasks with inherently imbalanced sample sizes, and future work could explore curriculum or task-balanced training to improve uniformity across tasks. The benchmark is also grounded in manipulation-centric egocentric environments, leaving broader domains such as social interaction, navigation, and outdoor activities largely unexplored. F. License We provide licenses of datasets and models used in our work in Tab. 9. Table 9. Licenses of datasets and models. Resource License EgoExoLearn HoloAssist ViSpeak InternVL 3.5 Qwen3 MIT License (link) CDLAv2 License (link) Apache-2.0 (link) Apache-2.0 (link) Apache-2.0 (link) 8 Figure 10. Qualitative comparison with text reasoning and gaze-based reasoning. We visualize examples from the scene reconstruction task comparing (a) text and (b) gaze-based reasoning. Figure 11. Qualitative comparison with text reasoning and visual-based reasoning. We visualize examples from the object identification task comparing (a) text and (b) visual-based reasoning. Figure 12. STREAMGAZE data example for OTP and NFI task. Figure 13. STREAMGAZE data example for SR and GSM task. 10 Figure 14. STREAMGAZE data example for OI (Easy/Hard) task. Figure 15. STREAMGAZE data example for OAR and FAP task. Figure 16. STREAMGAZE data example for GTA and OAA task."
        },
        {
            "title": "Prompt for FOV region object extraction",
            "content": "Context: {action_caption} Known objects: {object_pool} IMPORTANT: Reuse these exact names if you see the same objects. Only use new names for clearly different objects. Analyze this video clip and return JSON with this exact structure: { \"scene_caption\": \"Brief 2-3 sentence description of the overall scene and whats happening\", \"gaze_object\": { \"object_identity\": \"name of object at coordinate (x, y)\", \"detailed_caption\": \"Natural 3-5 sentence description of this objects appearance and characteristics\" }, \"other_objects\": [ \"object_identity\": \"object_name (e.g., cup, person wearing blue shirt)\", \"detailed_caption\": \"Two sentences describing the objects appearance, attributes, and spatial position relative to other objects\" { } ] } Person rules: Only include if full/upper body visible (not just hands/arms). CRITICAL: object_identity must be \"person wearing [clothing description]\" NOT just \"person\". Return only valid JSON. Be concise and direct. Figure 17. Prompt for FOV region object extraction. 13 Prompt for out-of-FOV region object extraction Context: {action_caption} Known objects: {object_pool} IMPORTANT: Reuse these exact names if you see the same objects. Only use new names for clearly different objects. Analyze this video clip (with the center region masked) and return JSON with this structure: { \"other_objects\": [ \"object_identity\": \"object_name (e.g., bottle, person wearing green jacket)\", \"detailed_caption\": \"Two sentences describing the objects appearance, attributes, and spatial position\" { } ] } Focus only on objects OUTSIDE the black masked region. Person rules: Only include if full/upper body visible (not just hands/arms). CRITICAL: object_identity must be \"person wearing [clothing description]\" NOT just \"person\". Return only valid JSON. Figure 18. Prompt for out-of-FOV region object extraction. Prompt for text-based reasoning You are an expert at gaze-conditioned streaming video reasoning. Rules: 1) Only use visual evidence up to the given timestamp. 2) Do your step-by-step reasoning ONLY inside <think>...</think>. 3) Give exactly ONE final choice (A/B/C/D) or yes/no with its short text inside <answer>...</answer>. {Question} Figure 19. Prompt for text-based reasoning. 14 Prompt for gaze-based reasoning You are an expert at gaze-conditioned streaming video reasoning. Rules: 1) Only use visual evidence up to the given timestamp (no future leakage). 2) Do your step-by-step reasoning ONLY inside <think>...</think>. 3) Return exactly three tags in order: <gaze>...</gaze> <think>...</think> <answer>...</answer>. 4) In <gaze>, estimate the most recent gaze center and FOV radius from the green dot and red circle in the frames up to the timestamp. 5) If the green dot/Red circle is not visible, output \"unknown\" in <gaze> but still reason and answer using other available evidence. 6) In <answer>, give exactly ONE final choice (A/B/C/D) or Yes/No with short text. {Question} Figure 20. Prompt for gaze-based reasoning. Prompt for visual-based reasoning You are an expert at gaze-conditioned streaming video reasoning. Rules: 1) Only use visual evidence up to the given timestamp (no future leakage). 2) Produce your reasoning ONLY inside <think>...</think> and never reveal reasoning outside the tag. 3) Return exactly FOUR tags in this order: <gaze>...</gaze> <objects>...</objects> <think>...</think> <answer>...</answer>. 4) In <gaze>, estimate the most recent gaze center and FOV radius from the green dot and red circle, or output \"unknown\" if not visible. 5) In <objects>, ground the most relevant objects inside or closest to the gaze/FOV region; for each object provide: object_name, bbox_x1, bbox_y1, bbox_x2, bbox_y2 normalized to [0,1], or output \"none\" if no relevant object appears. 6) In <think>, use the gaze and grounded objects to perform step-by-step reasoning and explain exclusion of irrelevant options. 7) In <answer>, provide exactly ONE final label (A/B/C/D or Yes/No) with short justification. {Question} Figure 21. Prompt for visual-based reasoning. 15 Figure 22. HTML for human verification of STREAMGAZE data construction. 16 Figure 23. HTML for human oracle evaluation of STREAMGAZE. Figure 24. Instructions provided to human annotators."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "UNC Chapel Hill"
    ]
}