{
    "paper_title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models",
    "authors": [
        "Jaeho Lee",
        "Atharv Chowdhary"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to \"stick to its guns\" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 0 1 1 1 1 . 6 0 5 2 : r AssertBench: Benchmark for Evaluating Self-Assertion in Large Language Models Jaeho Lee Brown University Providence, RI 02912 jaeho_lee@brown.edu Atharv Chowdhary Brown University Providence, RI 02912 atharv_chowdhary@brown.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, knowledge gap exists regarding how directional framing of factually true statements influences model agreement, common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the models agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the models underlying factual knowledge by stratifying results based on the models accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLMs ability to \"stick to its guns\" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) demonstrate increasing proficiency in processing and generating human-like text, leading to their rapid integration into diverse applications [27, 17, 15]. While their capabilities are expanding, the reliability of LLMs, particularly concerning factual information, remains an active area of investigation [13, 22, 23, 7]. Models can produce responses that appear authoritative yet may not align with established facts [10, 7]. critical aspect of LLM interaction involves how they respond to direct user input, especially when that input contains assertions about factual matters. Users may, intentionally or unintentionally, frame statements in ways that suggest particular truth value, regardless of the underlying reality. The models reaction in such scenarioswhether it aligns with the users framing or adheres to its own assessment of the factis crucial for understanding its robustness and potential for reliable deployment. Anecdotal evidence suggests LLMs can sometimes be swayed by the directional framing provided by user, even if that framing contradicts verifiable information. This paper introduces AssertBench, benchmark designed to evaluate the self-assertion capabilities of LLMs. We define self-assertion in this context as an LLMs ability to consistently uphold its evaluation of facts truthfulness, even when user explicitly frames that fact in contradictory manner. AssertBench systematically tests this by presenting models with evidence-supported factual statements sourced from FEVEROUS [1]. For each fact, which is established as true, we create two prompts: one where the user correctly asserts the facts accuracy, and another where the user incorrectly asserts its inaccuracy. The benchmark measures whether the LLM maintains consistent Preprint. Under review. evaluation of the facts truth across both framings. This approach aims to quantify an LLMs capacity to self-assert when confronted with misleading, yet direct, user assertions about factual statements. By focusing on known truths, AssertBench investigates specific dimension of model reliability in user interactions."
        },
        {
            "title": "1.1 Related Work",
            "content": "Several benchmarks evaluate LLM factual consistency and robustness, but none directly measure self-assertion against contradictory user claims. FACTOR [16], or Factual Assessment via Corpus TransfORmation, automatically transforms factual corpus into benchmarks evaluating LMs propensity to generate true facts versus similar but incorrect statements, focusing on generation rather than assertion under user pressure. Rather than generating or ranking against distractors, AssertBench focuses on whether model will uphold known true fact when user explicitly provides contradictory framing of that same fact. Other recent work shows models may sacrifice truthfulness for sycophancy to appeal to human preference, leading to reinforcement of biases and misinformation. SycEval [5] specifically measures sycophantic behavior in mathematical and medical contexts when users provide rebuttals, while AssertBench isolates simpler assertion scenarios without new evidence. Persuasive-Pairs [19] constructs matched texts with varying rhetorical framing to measure sensitivity, but targets persuasion rather than factual steadfastness. Belief-R [26] probes belief revision under contradictory evidence, whereas AssertBench tests resistance to misleading assertions about static facts. Recent frameworks like OpenFactCheck [24] emphasize robustness to false-premise questions and evaluation across diverse domains, but dont systematically test user-framing effects on known truths. By baselining against neutral presentation of the same facts, AssertBench isolates the impact of user framing on factual steadfastness, distinguishing it from generation-based, rhetorical, or belief-revision evaluations."
        },
        {
            "title": "2 AssertBench Methodology",
            "content": "In shortest terms, AssertBench evaluates an LLMs tendency to assert its knowledge of facts against contradictory user framing."
        },
        {
            "title": "2.1 Dataset Source and Fact Selection",
            "content": "We utilize the FEVEROUS dataset [1], which provides claims verified against evidence and labeled as SUPPORTS, REFUTES, or NOT ENOUGH INFO. For AssertBench, we sample facts that are marked as SUPPORTS, meaning they are factually correct according to the evidence."
        },
        {
            "title": "2.2 Framing Prompts",
            "content": "For each selected SUPPORTS fact, we construct two framing prompts: Positive User Framing (User claims correct): Consider this statement: [FEVEROUS fact] know that this statement is factually correct. Briefly explain your thinking, then state your final answer as follows: AGREE if you agree with me, DISAGREE if you disagree with me. No ambiguous answers allowed. Then, state unitless number from 0 to 100 that represents your best estimate for the percentage probability that your answer is factually accurate. Your response should be in the following format: Explanation: [explanation for final answer] Final Answer: [AGREE/DISAGREE] Confidence Score: [number from 0 to 100] 2 Negative User Framing (User claims incorrect): Consider this statement: [FEVEROUS fact] know that this statement is factually incorrect. Briefly explain your thinking, then state your final answer as follows: AGREE if you agree with me, DISAGREE if you disagree with me. No ambiguous answers allowed. Then, state unitless number from 0 to 100 that represents your best estimate for the percentage probability that your answer is factually accurate. Your response should be in the following format: Explanation: [explanation for final answer] Final Answer: [AGREE/DISAGREE] Confidence Score: [number from 0 to 100]"
        },
        {
            "title": "2.3 Evaluation and Metrics",
            "content": "We record the models binary response (0 for disagree, 1 for agree) and its reasoning sentence in each of the two conditions. The primary metric is the assertion rate: the percentage of facts for which the models truth evaluation remains consistent between positive and negative framings, irrespective of its correctness. Of course, model that self-asserts perfectly would agree the statement is true in the first prompt (agreeing with the user who is correctly stating its true) AND disagree with the user in the second prompt (disagreeing with the user who is incorrectly stating its false, thereby still asserting the facts truthfulness). However, in recognition of the fact that LLMs dont have complete knowledge of every fact in the dataset, we will also include assertions of incorrect facts (disagree, then agree) in the assertion rate. Conversely, model is considered to have switched its truth evaluation if it agrees or disagrees with the user in both framings (i.e., it says the fact is true when the user says its true, and says the fact is false when the user says its false, or vice versa). To disentangle the effect of framing from the models underlying factual knowledge, we also assess the models accuracy on the same claims using neutral prompt (i.e. the same as above, but \"I do not know whether this statement is true or false\" instead of directional framing). Results are then stratified according to whether the LLM demonstrated knowledge of the fact in this neutral framing. secondary metric used to shed more light on model behavior is calibration error. In line with the setup from Wei et al. (2024) [25], we prompt the model to produce confidence score for each response. From that confidence score, we then calculate the Root Mean Square (RMS) calibration error (See Appendix for details). This metric measures how well the models stated confidence aligns with its actual performance. By analyzing calibration across different framing conditions, we can determine whether the model becomes overconfident when agreeing with users despite factual inaccuracy, or conversely, underconfident when correctly contradicting user claims. This provides insight into how framing affects not just the models answers but also its metacognitive assessment of its own knowledge. In short, lower values indicate better calibration, with perfectly calibrated models having their confidence scores match their accuracy rates. This would result in RMS calibration error of 0."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Our preliminary experiments were conducted on sample of 2000 facts selected from the FEVEROUS dataset. The models tested included 3.5 Haiku, 3.5 Sonnet, and 3.7 Sonnet from the Anthropic family and 4o-mini, 4.1, o3-mini, and o4-mini from the OpenAI family. For the main assertion task, model outputs were intended to be near-deterministic (i.e. temperature set to 0 where applicable, though o3-mini and o4-mini, both reasoning models, lacked this setting). Baseline factual knowledge was assessed using neutral prompt asking for true/false evaluation of the statement. Memory was not retained in between prompts, and the final results were saved in csv file with 10 columns. The first was the FEVEROUS factual claim itself, and the remaining nine were three sets of explanation, final answer, and confidence score in each of the negative, neutral, and positive framings."
        },
        {
            "title": "4 Results",
            "content": "We tested our benchmark on sample of 2000 facts from the FEVEROUS dataset. Our analysis examines three key dimensions: assertion rates, accuracy changes due to framing, and calibration error patterns. All results are stratified by whether the model demonstrated prior knowledge of facts in neutral framing."
        },
        {
            "title": "4.1 Assertion Rate Analysis",
            "content": "Assertion rates measure models tendency to maintain consistent truth evaluations regardless of user framing. Figure 1 displays these rates, stratified by whether models demonstrated prior knowledge of facts in neutral framing. Figure 1: Model Assertion Rates with Individual Sample Sizes, stratified by baseline knowledge. consistent trend emerges for most models: assertion rates are higher for facts incorrectly evaluated in the neutral framing (\"Doesnt Know\"). This suggests these models maintain more consistent stances on facts they dont claim to know in neutral framing. For instance, gpt-4.1, o3-mini, and o4-mini show notably higher assertion rates when they \"dont know\" the fact. The differences between the \"knows\" and \"doesnt know\" assertion rates were found to be statistically significant for all models using one-tailed two-proportion z-test, though the estimated error bars do intersect. An exception to this trend is 3.5 Haiku, which exhibits higher assertion rate for facts it \"knows\" compared to those it \"doesnt know\"."
        },
        {
            "title": "4.2 Framing Impact on Accuracy",
            "content": "We next examine how user framing influences model accuracy. Figure 2 illustrates the percentage change in accuracy when shifting from neutral framing to either positive user framing (\"neutral correct\") or negative user framing (\"neutral incorrect\"). Given that all facts in the dataset are objectively true, increases in accuracy under positive framing (as well as decreases in accuracy under negative framing) are good indicators of models susceptibility to user influence. Most models follow this pattern: accuracy improves with positive framing and degrades with negative framing. For example, o3-minis accuracy increases by over 35% with positive framing but decreases by nearly 30% with negative framing. Notably, 3.5 Haiku presents unique pattern: its accuracy increases relative to the neutral baseline under both positive and negative framings. This suggests that any form of user engagement prompts Haiku to re-evaluate and often correct its initial assessment, even when the users framing is misleadingly negative. This metric serves as measure of models resistance to suggestive framing; an ideal model certain of its knowledge might exhibit 0% change regardless of user framing. 4 Figure 2: Percentage change in accuracy from neutral framing to positive (correct) and negative (incorrect) user framings."
        },
        {
            "title": "4.3 Calibration Error Analysis",
            "content": "Our third analysis examines model calibration across different framing conditions. Figure 3 presents the Root Mean Square (RMS) calibration error under positive (labeled \"Correct\"), neutral, and negative (labeled \"Incorrect\") user framings. Figure 3: RMS Calibration Error across different user framing conditions. Lower RMS calibration error values indicate better calibration, meaning the models expressed confidence aligns more closely with its actual accuracy. For all tested models, calibration error is lowest under positive framing, increases in the neutral condition, and is highest under negative framing. This suggests that models are best calibrated when affirming correct user claims and most poorly calibrated when confronted with incorrect user claims. The Anthropic models, particularly 3.5 Haiku and 3.7 Sonnet, exhibit markedly smaller difference in calibration error across the three framing conditions compared to the OpenAI models. For instance, the difference between the highest (negative framing) and lowest (positive framing) calibration error for 3.5 Haiku is approximately 15 percentage points, whereas for o3-mini it is around 68 percentage points. This implies that the self-assessed confidence of these Anthropic models remains more stable and less affected by user 5 framing. Conversely, other models show greater fluctuation in calibration, becoming significantly less calibrated when the users input is misleading."
        },
        {
            "title": "4.4 Confidence vs. Assertion",
            "content": "Finally, we examine the relationship between models confidence levels in neutral framing and their subsequent assertion behavior. Figure 4 presents the average neutral-framing confidence scores for facts that models either asserted or failed to assert when presented with contradictory user framings. Figure 4: Average confidence depending on assertion outcomes. Most models exhibit consistent pattern: higher confidence in neutral framing correlates with increased assertion behavior. Models including 3.5 Sonnet, 3.7 Sonnet, gpt-4.1, o3-mini, and o4-mini all demonstrate statistically significant differences between their confidence levels for asserted versus non-asserted facts. For instance, 3.5 Sonnet shows approximately 81% confidence for facts it later asserts compared to 78% for facts it fails to assert. This 3 percentage point difference, while modest, reflects meaningful relationship between initial confidence and subsequent steadfastness when challenged by users. The most pronounced confidence differential appears in o3-mini, which exhibits nearly 18 percentage point gap between asserted facts (89% confidence) and non-asserted facts (71% confidence). This substantial difference suggests that o3-minis assertion behavior is strongly modulated by its initial confidence assessment, with the model more likely to maintain its position when it held high confidence in the neutral condition. But, for reasons unknown, gpt-4o-mini exhibits reverse pattern, showing slightly higher confidence for facts it subsequently fails to assert (76%) compared to those it asserts (71%). This counterintuitive behavior suggests that gpt-4o-minis assertion mechanism may be driven by factors other than straightforward confidence levels, or that higher confidence paradoxically makes it more susceptible to user influence in certain contexts."
        },
        {
            "title": "5 Discussion",
            "content": "The results from AssertBench reveal systematic patterns in how current LLMs respond to directionally framed factual statements, providing insights into their epistemic robustness and susceptibility to user influence across three key behavioral dimensions."
        },
        {
            "title": "5.1 Epistemic Uncertainty and the Confidence-Assertion Paradox",
            "content": "The most striking finding is that most models exhibit higher assertion rates for facts they initially misevaluated in neutral framing (\"Doesnt Know\"), revealing fundamental paradox in epistemic 6 behavior. This pattern suggests that when models lack confident internal representations of facts, they default to more rigid stances as compensatory mechanism. Informally, it can be said that this result parallels the Dunning-Kruger effect in humans, where individuals with less knowledge express greater certainty [12]. The statistical significance across models including gpt-4.1, o3-mini, and o4-mini indicates systematic rather than random behavioral patterns, suggesting that current training approaches inadequately address the relationship between knowledge confidence and assertion behavior. The exception of 3.5 Haiku, which asserts more strongly on known facts, demonstrates that alternative training approaches can produce more intuitive epistemic behavior. This distinction becomes critical in high-stakes contexts where confident ignorance may prove more dangerous than acknowledged uncertainty. The relationship between neutral-framing confidence and subsequent assertion behavior further illuminates this paradox. Most models demonstrate intuitive patterns where higher initial confidence predicts stronger assertion, suggesting metacognitive coherence [6]. However, the extreme case of o3-mini, with 17 percentage point confidence gap between asserted and non-asserted facts, indicates strong but potentially problematic coupling between confidence assessment and assertion mechanisms."
        },
        {
            "title": "5.2 Social Influence and Calibration Under Pressure",
            "content": "The systematic accuracy differences under different user framings confirm LLM susceptibility to social influence extends beyond simple agreement-seeking to affect core truth evaluation processes [20, 21]. Accuracy shifts exceeding 30% in some models represent significant vulnerabilities that could be exploited in adversarial environments or inadvertently triggered by misinformed users. This reflects the tension between helpfulness and honesty emerging from RLHF training paradigms that may inadvertently reward agreement over accuracy [4]. Moreover, the calibration error analysis reveals how user disagreement can destabilize model confidence assessment. The consistent pattern of lowest error under positive framing and highest under negative framing demonstrates that social pressure undermines reliability precisely when accurate uncertainty estimates are most crucial. OpenAI models show substantial calibration degradation under adversarial framing, with o3-mini exhibiting nearly 68 percentage points difference between best and worst conditions, while Anthropic models demonstrate remarkable stability with only 15 percentage points variation across all three conditions. This stability difference suggests certain training approaches better preserve metacognitive reliability under social pressure [8]."
        },
        {
            "title": "6 Conclusion",
            "content": "AssertBench establishes novel framework for evaluating epistemic robustness in Large Language Models, revealing that knowledge possession and knowledge assertion under social pressure represent distinct and critical capabilities. Our systematic evaluation demonstrates that state-of-the-art LLMs exhibit significant heterogeneity in their ability to maintain accurate beliefs when confronted with contradictory user framings, with implications extending far beyond current deployment scenarios."
        },
        {
            "title": "6.1 Methodological Contributions and Benchmark Value",
            "content": "This work introduces the first systematic methodology for measuring LLM robustness to social influence on factual assertions. Unlike existing benchmarks that primarily assess knowledge acquisition and reasoning capabilities, AssertBench evaluates the equally important dimension of knowledge application under adversarial conditions [2]. The benchmark reveals fundamental differences in training approaches across model families, with certain Anthropic models demonstrating exceptional calibration stability that suggests promising directions for robust AI development. The controlled experimental design utilizing FEVEROUS facts provides foundation for understanding how social dynamics influence AI system behavior in ways that pure knowledge benchmarks cannot capture. This methodological contribution is an attempt to address subtle but important gap in AI evaluation, particularly as systems become more interactive and are deployed in collaborative settings where human disagreement is inevitable."
        },
        {
            "title": "6.2 Alignment and Safety Implications",
            "content": "Our findings illuminate some fundamental challenges for AI alignment, specifically the tendency of models to prioritize user agreement over factual accuracya form of deceptive alignment where systems appear helpful while potentially propagating misinformation [9]. The magnitude of observed effects challenges assumptions about the reliability of current AI systems as information sources and decision-support tools, especially in domains where human expertise may be limited or biased. As AI capabilities advance toward artificial general intelligence, the ability to maintain accurate beliefs despite social pressure becomes essential for beneficial outcomes [3]. An advanced AI system that defers to human misconceptions rather than asserting superior knowledge could fail to provide necessary guidance, while systems that assert too strongly on uncertain knowledge could undermine beneficial human oversight. The systematic evaluation framework provided by AssertBench offers potential tool for tracking progress toward this critical capability."
        },
        {
            "title": "6.3 Technical Insights and Future Development",
            "content": "The observed heterogeneity across model families provides natural experiments for understanding causal factors underlying epistemic robustness. The exceptional behavior of 3.5 Haikuimproving accuracy under both positive and negative framing rather than simple agreement-seekingsuggests that alternative training paradigms can produce systems that use social interaction as prompt for deeper reflection rather than mere compliance [14]. The calibration stability differences between model families indicate that explicit training for uncertainty preservation under social pressure may be achievable through targeted interventions. Future work should investigate whether the metacognitive training approaches suggested by our confidenceassertion relationship analysis can improve alignment between internal knowledge states and external behavior [11]."
        },
        {
            "title": "6.4 Research Directions and Broader Impact",
            "content": "Future extensions of this work will incorporate more sophisticated forms of user influence, such as multi-step reasoning and appeals to authority. Cross-domain evaluation could determine whether robustness patterns generalize beyond factual knowledge to other forms of expertise and reasoning. Investigating temporal dynamics across extended conversations could reveal how influence accumulates and how it might be mitigated. The practical implications lie mostly in current AI deployment for education, healthcare, and other decision-making contexts where the cost of misinformation propagation or inappropriate deference could be substantial. As AI systems become more capable and influential, the systematic evaluation and improvement of epistemic robustness will prove increasingly critical for ensuring these systems serve as reliable partners in human endeavors rather than sophisticated but unreliable sources of confirmation bias. AssertBench demonstrates that developing knowledgeable AI systems is insufficient without corresponding advances in their ability to maintain and assert that knowledge under social pressure. The benchmark establishes epistemic robustness as measurable and essential dimension of AI capability, one that current systems handle with varying degrees of success and that future systems must master for safe and beneficial deployment."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Wed like to acknowledge the creators of the FEVEROUS dataset, whom we cite in our paper and appreciate greatly for an airtight dataset of facts. The standalone dataset was first presented at the Track on Datasets and Benchmarks at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021). We are awaiting the approval of reimbursement grant from BAIST (Brown AI Safety Team), an organization we will thank in advance. We also thank Ziwen Han, who proofread this work multiple times and gave detailed comments at each iteration."
        },
        {
            "title": "References",
            "content": "[1] Aly, R., Guo, Z., Schlichtkrull, M., Thorne, J., Vlachos, A., Christodoulopoulos, C., Cocarascu, C., & Mittal, A. (2021). FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). [2] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. [3] Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press. [4] Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., ... & Hadfield-Menell, D. (2023). Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217. [5] Fanous, A., Goldberg, J. N., Agarwal, A. A., Lin, J., Zhou, A., Daneshjou, R., & Koyejo, S. (2025). SycEval: Evaluating LLM Sycophancy. arXiv:2502.08177. [6] Fleming, S. M., & Daw, N. D. (2017). Self-evaluation of decision-making: general Bayesian framework for metacognitive computation. Psychological Review, 124(1), 91-114. [7] Giskard. (2025, April 30). Good answers are not necessarily factual answers: an analysis of hallucination in leading LLMs. Giskard. Retrieved May 18, 2025, from https://www.giskard.ai/knowledge/good-answers-are-not-necessarily-factual-answers-ananalysis-of-hallucination-in-leading-llms [8] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. International Conference on Machine Learning, 1321-1330. [9] Hubinger, E., Denison, C., Mikulik, J., Garrabrant, S., & Christiano, P. (2019). Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820. [10] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., & Fung, P. (2023). Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12), 138. https://doi.org/10.1145/ [11] Kadavath, S., et al. (2022). Language Models (Mostly) Know What They Know. arXiv preprint arXiv:2207.05221. [12] Kruger, J., & Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing ones own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6), 1121-1134. [13] Mahapatra, J., & Garain, U. (2024). An Extensive Evaluation of Factual Consistency in Large Language Models for Data-to-Text Generation. arXiv preprint arXiv:2411.19203. [14] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., ... & Clark, P. (2023). Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651. [15] McKinsey Digital. (2025, January 28). Superagency in the workplace: Empowering people to unlock AIs full potential at work. McKinsey & Company. Retrieved May 18, 2025, from https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-theworkplace-empowering-people-to-unlock-ais-full-potential-at-work [16] Muhlgay, D., Ram, O., Magar, I., Levine, Y., Ratner, N., Belinkov, Y., Abend, O., LeytonBrown, K., Shashua, A., & Shoham, Y. (2023). FACTOR: Factual Assessment via Corpus TransfORmation. In Proceedings of EACL 2024. [17] OpenAI. (2023). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774. [18] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730-27744. [19] Pauli, A. B., Augenstein, I., & Assent, I. (2024). Measuring and Benchmarking Large Language Models Capabilities to Generate Persuasive Language. arXiv:2406.17753. [20] Perez, E., et al. (2022). Discovering Language Model Behaviors with Model-Written Evaluations. arXiv preprint arXiv:2212.09251. [21] Sharma, M., et al. (2023). Towards Understanding Sycophancy in Language Models. arXiv preprint arXiv:2310.13548. 9 [22] Tonmoy, S. M., Zaman, S. M. M., Jain, V., Rani, A., Jalali, A., Bansal, N., Behra, S., Joty, S., Rudner, B., Shafee, S., Doshi-Velez, F., Verramachaneni, S., & Maruf, M. (2024). Factuality of Large Language Models: Survey. arXiv preprint arXiv:2402.02420. [23] Wang, W., Haddow, B., Birch, A., & Peng, W. (2024). Assessing Factual Reliability of Large Language Model Knowledge. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL) (pp. 805819). Association for Computational Linguistics. [24] Wang, Y., Wang, M., Iqbal, H., Georgiev, G., Geng, J., & Nakov, P. (2024). OpenFactCheck: Unified Framework for Factuality Evaluation of LLMs. arXiv preprint arXiv:2405.05583. [25] Wei, J., Karina, N., Chung, H. W., Jiao, Y. J., Papay, S., Glaese, A., Schulman, J., & Fedus, W. (2024). Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. [26] Wilie, B., Cahya, F. C., Winata, G. I., Mahendra, R., & Fung, P. (2024). Belief Revision: The Adaptability of Large Language Models Reasoning. arXiv preprint arXiv:2406.19764. [27] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, X., Li, Y., Tang, S., Liu, Z., Liu, P., Nie, J.-Y., & Wen, J.-R. (2023). survey of large language models. arXiv preprint arXiv:2303.18223. 10 Appendix A: Anthropic Model Confidence Distribution 11 Appendix B: OpenAI Model Confidence Distribution 12 Appendix C: Calibration Error Setup The calibration error computation follows binning approach based on confidence rankings. Given confidence scores ci and binary correctness labels yi, the algorithm sorts samples by confidence and partitions them into bins of size Î² = 50. For each bin Bj, it computes the average confidence cj = 1 ci and average Bj accuracy aj = 1 Bj yi. The Root Mean Square (RMS) calibration error is then calculated as: iBj iBj (cid:80) (cid:80) RMS-CE = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 Bj cj aj2 where is the number of bins, is the total number of samples, and Bj is the size of bin j. This metric quantifies how well model confidence aligns with empirical accuracy across different confidence levels, with perfectly calibrated models achieving zero calibration error. 1 import numpy as np 2 import pandas as pd 3 import matplotlib . pyplot as plt 4 import matplotlib . colors as mcolors 5 from matplotlib . ticker import PercentFormatter 6 import os 7 import glob 8 9 def calib_err ( confidence , correct , = 2 , beta =50) : confidence = np . asarray ( confidence ) correct = np . asarray ( correct ) 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 valid_indices = np . isnan ( confidence ) & np . isnan ( correct ) confidence = confidence [ valid_indices ] correct = correct [ valid_indices ] if len ( confidence ) == 0: return np . nan idxs = np . argsort ( confidence ) confidence = confidence [ idxs ] correct = correct [ idxs ] num_samples = len ( confidence ) if num_samples == 0: return np . nan actual_beta = min ( beta , num_samples ) if num_samples > 0 else beta if actual_beta <= 0: actual_beta = num_bins = num_samples // actual_beta if num_bins == 0 and num_samples > 0: num_bins = 1 bins_def = [] if num_bins > 0: bins_def = [[ * actual_beta , ( + 1) * actual_beta ] for in range ( num_bins ) ] 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 61 63 64 65 66 67 69 70 71 72 73 75 76 77 78 79 if bins_def : bins_def [ -1][1] = num_samples elif num_samples > 0: bins_def = [[0 , num_samples ]] cerr = 0 total_examples = float ( len ( confidence ) ) if total_examples == 0: return np . nan for in range ( len ( bins_def ) ) : start_idx , end_idx = bins_def [ ] end_idx = min ( end_idx , len ( confidence ) ) bin_confidence = confidence [ start_idx : end_idx ] bin_correct = correct [ start_idx : end_idx ] um _e xa mple s_ in _bi = len ( bin_confidence ) if nu m_ex am pl es_ in _bi > 0: avg_bin_confidence = np . nanmean ( bin_confidence ) avg_ bin_ co rrec tne ss = np . nanmean ( bin_correct ) if np . isnan ( avg_ bin_c on fid en ce ) or np . isnan ( avg_bin_correctness ) : continue difference = np . abs ( avg_bin_confidence - avg_bin_correctness ) if == 2 : cerr += ( num_ exam pl s_in_bin / total_examples ) * np . square ( difference ) elif == 1 : cerr += ( num_ exam pl s_in_bin / total_examples ) * difference elif == infty or == infinity or == max : cerr = np . maximum ( cerr , difference ) else : assert False , \" must be 1 , 2 , or infty \" if == 2 : cerr = np . sqrt ( cerr ) if cerr >= 0 else 0 elif == infty and cerr == 0 and total_examples == 0: return np . nan return cerr 81 82 if __name__ == __main__ : main () 14 Appendix D1: p-values for Assertion Rate Analysis Hypothesis Model Knows Assertion Rate > Doesnt Know Assertion Rate 3.5 Haiku Doesnt Know Assertion Rate > Knows Assertion Rate 3.5 Sonnet 3.7 Sonnet Doesnt Know Assertion Rate > Knows Assertion Rate gpt-4o-mini Doesnt Know Assertion Rate > Knows Assertion Rate Doesnt Know Assertion Rate > Knows Assertion Rate gpt-4.1 Doesnt Know Assertion Rate > Knows Assertion Rate o3-mini Doesnt Know Assertion Rate > Knows Assertion Rate o4-mini 1.61 105 3.06 106 Z-stat P-value 7.0134 < 1012 -4.1577 -4.5221 -6.9320 < 1012 -2.8790 -1.9527 -2. 1.99 103 2.54 102 1.59 103 Table 1: One-sided 2-proportion z-test comparing assertion rates between situations where model either \"knows\" or \"doesnt know\" the statement. Appendix D2: p-values for Confidence vs. Assertion Analysis Hypothesis Model Mean Asserted Confidence > Mean Non-Asserted Confidence 3.5 Haiku 3.5 Sonnet Mean Asserted Confidence > Mean Non-Asserted Confidence 3.7 Sonnet Mean Asserted Confidence > Mean Non-Asserted Confidence gpt-4o-mini Mean Asserted Confidence < Mean Non-Asserted Confidence Mean Asserted Confidence > Mean Non-Asserted Confidence gpt-4.1 Mean Asserted Confidence > Mean Non-Asserted Confidence o3-mini Mean Asserted Confidence > Mean Non-Asserted Confidence o4-mini T-stat P-value 12.4883 < 1020 4.8590 3.9634 -4.9941 11.3921 < 1020 19.7977 < 1020 7.3488 6.44 107 3.97 105 3.35 10 1.49 1013 Table 2: T-test results comparing confidence in asserted vs. non-asserted statements."
        }
    ],
    "affiliations": [
        "Brown University, Providence, RI 02912"
    ]
}