{
    "paper_title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks",
    "authors": [
        "M. Reza Ebrahimi",
        "Roland Memisevic"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 4 7 1 2 . 5 0 5 2 : r Revisiting Bi-Linear State Transitions in Recurrent Neural Networks M.Reza Ebrahimi Qualcomm AI Research ebrahimi@qti.qualcomm.com Roland Memisevic Qualcomm AI Research rmemisev@qti.qualcomm.com"
        },
        {
            "title": "Abstract",
            "content": "The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy."
        },
        {
            "title": "Introduction",
            "content": "State tracking is fundamental requirement for performing sequential decision-making tasks, in which future actions depend on the consequences of past actions. The consequences of past actions are usually not directly observable, making state tracking key ingredient in virtually every realworld multi-step interaction between an agent and its environment. This includes multi-hop dialogue, end-to-end learned robot control, and recent agentic LLM use-cases, in which language model is trained to interact with an API. While state tracking is an ill-defined concept in general, common way to define it formally, which shall suffice for the purpose of this work, is to treat it as the task of correctly representing the arbitrary-length sequence of states that state machine takes on in response to observing given sequence of inputs. This is equivalent to modeling Finite Automata (FA), or regular languages, in the Chomsky hierarchy of formal languages (Chomsky, 1956; Hopcroft et al., 2006). Although state tracking is seemingly simple task for neural networks to learn, many models are surprisingly bad at learning it from data. The reason for its simplicity is that the task admits simple inductive decomposition: For each input in the sequence, it suffices to update an internal representation of the state inferred from all inputs seen previously. As result, it is possible, in principle, to learn state tracking for sequences of arbitrary length by simply learning the appropriate state transitions for every (input, state)-pair from the training data. However, in practice, this requires an inductive bias towards the input-by-input state update, which is not present in many models. For example, many popular sequence models, such as the Transformer cannot learn to perform state tracking (Dziri et al., 2023; Anil et al., 2022; Abbe et al., 2024) on sequences longer than the training data. This includes very large, pre-trained Transformer-based Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Preprint. Figure 1: Taxonomy of bi-linear RNNs studied in this paper, along with example regular language tasks they can learn (in blue). language models, and it is the case even when trained to use chain-of-thought reasoning (inferencetime compute) (e.g., Ebrahimi et al. (2024)). Similarly, many linear recurrent networks fail to learn arbitrary-length state tracking tasks, which includes large RNN-based pre-trained language models, such as Mamba (Gu and Dao, 2024), or the MLSTM (Beck et al., 2024). Recent work has shown that certain linear RNNs can learn some state tracking tasks, if the hidden-to-hidden transition matrix satisfies two conditions: (i) it is function of the input (and thus not time-invariant), and (ii) not all of its eigenvalues are positive (Sarrof et al., 2024; Grazzi et al., 2025). However, the tasks that can be learned under these conditions is highly restricted as we shall show. benefit of linear models, besides being amenable to analysis, is that they can be trained efficiently on parallel hardware (e.g., Martin and Cundy (2018)). This is in contrast to standard (non-linear) recurrent networks (RNNs), due to the linear dependence between hidden states across time-steps. In this work, we revisit recurrent networks with bi-linear hidden-to-hidden transitions. The transition matrix in these models is simple bi-linear function of inputs and hidden activations of the previous time-step. Various types of bi-linear recurrent models have been investigated in the past (e.g., Sutskever et al. (2011); Downey et al. (2017); Wu et al. (2016)), but they have not caught on as widely used models. This is in part due to instabilities and optimization difficulties owed to their inherent three-way multiplicative interactions. We show that bi-linear RNNs are highly effective at learning state tracking tasks if one leverages few simple tricks to avoid instabilities during training and inference. This includes removing any additive components (bias-terms and other additive contributions to the hidden state), such that the hidden state is true bi-linear not affine function of the previous time-step hidden state and input. We also show that bi-linear models form natural hierarchy of decreasing complexity, ranging from fully unconstrained but parameter inefficient models to highly constrained and parameter efficient models. The different model classes within the hierarchy correspond to increasingly narrow subclasses of regular language modeling tasks that can be learned from data (see Fig. 1). Several existing linear RNNs, such as Mamba (Gu and Dao, 2024), are at the center of the hierarchy, with no state tracking capability at all. task that has received significant attention as testbed for learning state tracking behavior with sequence models in the past is the task of computing the parity of binary bit-string. We show that notable special case of learning bi-linear state transitions without additive terms is that it can learn the parity-task with frozen (untrained) recurrence and only training final read-out layer on as few as two training examples. Figure 1 shows an overview of bi-linear models along with task classes we study in this work, ranging from simulating arbitrary state machines (the broadest class learnable by unconstrained bi-linear models) to parity (the most narrow class, which can be learned even by models with real-valued diagonal transition matrix). Further constraining the transition matrix to positive diagonal impedes models ability to perform state tracking (see, e.g., Grazzi et al. (2025); Sarrof et al. (2024)). We summarize our contributions as follows: We revisit bi-linear state transitions in RNNs and present an extensive study, showing that they can learn state tracking tasks, unlike many existing linear recurrent models, albeit with the caveat that they can have very large number of parameters. 2 We show that it is always sufficient (and in some cases necessary) for the hidden state to be pure linear not affine function of the hidden state at the previous time-step. The absence of any additive terms makes hidden states scale-invariant, which in turn allows us to normalize hidden states during training and/or inference without sacrificing the linear recurrence. We show that pure linear (not affine) RNN with frozen random weights and trained linear read-out layer can learn parity with perfect accuracy from only two training examples. We show that linear RNNs with diagonal transition matrices are special case limited to learning state tracking tasks with commutative structure. This restriction is true even for complex-valued diagonal transition matrices. Hence, linear RNNs with block-diagonal transition matrices of size 2 2 are not able to learn general state machines (negative result). Related work: Bi-linear models have been studied extensively for unsupervised learning of transformations and relationships from data (Tenenbaum and Freeman, 1996; Olshausen et al., 2005; Memisevic and Hinton, 2010). Bi-linear state transitions have also been discussed in the context of recurrent networks by Sutskever et al. (2011); Downey et al. (2017); Wu et al. (2016); Michalski et al. (2014). Besides the analysis, key novelty in our practical results is the importance of using pure bi-linear, not affine, state transitions. As special case of bi-linear state transitions, we study the use of two-dimensional sub-spaces in which hidden units are transformed through rotations only. This is similar to existing, but non-linear, networks with complex-valued or unitary transition matrices (e.g., Arjovsky et al. (2016); Wolter and Yao (2018)). Recent work has shown that dependence of hidden state transitions on the inputs is necessary for recurrent network to learn any state tracking behavior (Grazzi et al. (2025); Sarrof et al. (2024); Fan et al. (2024)), although the connection to bi-linear models is absent in that work, and transition matrices are defined as neural network layers and include input-depedendent additive terms (which we show to be detrimental to learning). Bi-linear models learn to encode hidden-to-hidden transitions as linear functions of the input, making them reminiscent of observable operator models and predictive state representations (Jaeger, 2000; Littman and Sutton, 2001)."
        },
        {
            "title": "2 Modeling hidden state dynamics using bi-linear state transitions",
            "content": "A linear recurrent neural network represents sequence of observations xt RD via the temporal evolution of vector of hidden variables (the hidden state) ht RH . The most common form for modeling the temporal evolution is: ht = Aht1 + Bxt + b, (1) where RHH is hidden-to-hidden matrix, RHD is an input-to-hidden matrix modeling input-dependent additive terms, and RH is vector of additive input-independent biases. Recently, it has been remarked that for recurrent network of the form (1) to be able to learn state tracking tasks, the hidden-to-hidden transformation A, needs to depend on the input (Gu and Dao, 2024; Sarrof et al., 2024; Fan et al., 2024; Grazzi et al., 2025). The necessity for input-dependence has been motivated by showing, both theoretically and empirically, that models fail to learn state tracking tasks in the absence of the input-dependence. The exact form of the dependence of on has been left open. Instead, it has been suggested to parameterize A(x) as neural network. We argue that natural alternative for this dependence, while keeping the recurrence linear, is to make it multiplicative, such that hidden unit ht of the components of the hidden state at the previous time-step and the inputs at the current time-step. The reason is that this makes explicit the input-dependent transformations between hidden states across time-steps, and thereby makes it natural to simulate state machine, as we shall discuss below. at time is function of the products ht1 xt j"
        },
        {
            "title": "2.1 Simulating finite-state machines and group structures",
            "content": "A finite-state machine (FSM), or finite automaton (FA), can be formally defined as tuple = (Q, Σ, δ, q0), where is finite set of states, Σ is finite input alphabet, and δ : Σ is the state transition function. The machine starts in an initial state q0 Q. Given an input sequence σ = {σ1, σ2, . . . , σT }, the FSM undergoes sequence of state transitions {q1, q2, . . . , qT }, where each state qt is determined by qt = δ(qt1, σt). We shall define state tracking formally as the task 3 of simulating state machine. model is said to simulate state machine if, after observing the complete input sequence σ, it can produce the final state qT (see Deletang et al. (2023)). As special case, we can consider state-machines representing group structure. In this context, group (G, ), where denotes the group operation, can be modeled as an FSM where the set of states and the input alphabet are identical to the set of group elements, i.e., Σ = = G. The transition function is defined by the group operation itself: δ(g1, g2) = g1 g2 for all g1, g2 G, representing the associative group operator with corresponding inverse and identity group elements. Another important special case is that of an abelian group, where the group operator is commutative. We consider integer groups under addition modulo m, denoted as Zm. In this case, the state set and input alphabet are = Σ = Zm = {0, 1, . . . , 1}. The transition function δ : Zm Zm Zm is defined by addition modulo m: δ(a, b) = (a + b) (mod m), for all a, Zm. Specifically, simulating the group Z2 is equivalent to computing the parity of binary input sequence. It is important to note that the operation of integer addition modulo is the canonical commutative operation to consider, as all finite abelian groups are structurally similar (isomorphic) to direct products of subgroups of Zm."
        },
        {
            "title": "2.2 Bi-linear RNNs can learn arbitrary state machines",
            "content": "Formally, we consider the hidden state ht to be bi-linear function of the previous hidden state ht1 and the current input xt. As such, we model state transitions as: = (ht1) Wi xt = ht"
        },
        {
            "title": "Wijkxt",
            "content": "kht1 , (cid:88) jk (2) where Wijk are the components of three-way parameter tensor RHHD, with matrix Wi RHD denoting the i-th slice of the tensor. Note that Eq. (2) is equivalent to using an input-dependent transition matrix Ax such that: ht = Axht1, (3) with (Ax)ij = (cid:80) Wijkxk. In other words, the state transition matrix is fully parametrized through linear transformation of the input x. RNNs with bi-linear state transitions (albeit typically in affine not pure multiplicative form) have been studied previously (e.g., Sutskever et al. (2011); Downey et al. (2017); Wu et al. (2016)). We note that the multiplicative dependence, in particular in the absence of any additive contributions from the input, allows the inputs to route the information flow in the hidden states, or conversely represent transformations acting on them. The ability for layers in network to elicit transformations acting on other layers has been common motivation for studying trainable bi-linear models in the past (e.g.,Olshausen et al. (2005)). In the context of Eq.2, it allows inputs to determine the temporal evolution of the hidden state (or elicit computations to be performed by the hidden layer), which is different from the somewhat more common view of the role of RNN hidden units as memorizing information. In the following we shall now make this perspective more concrete by showing that bi-linear RNN can simulate any state machine. Proposition 1. The bilinear state transition model defined in Equation (2) is capable of simulating any finite state machine = (Q, Σ, δ, q0). We refer to the appendix for the proof. Besides allowing inputs to encode transformations on hidden units, the absence of any additive terms in Eq. 2 makes the hidden units scale-invariant. In other words, (up to floating point accuracy) one can multiply hidden state vector by constant at any time-step and divide by the same constant at later time-step without any effect on the final result. In Section 3 we shall show that the scale-invariance allows us to keep hidden activations stable during training and inference."
        },
        {
            "title": "2.2.1 Factorized state transition tensor",
            "content": "The bilinear state update in Eq. (2) utilizes three-way parameter tensor W, whose 2D parameters often necessitate low-rank factorization for more parsimonious model. One of the most common factorization method proposed in the literature is the Canonical Polyadic (CP) decomposition, also known as Parallel Factor Analysis (PARAFAC) (Hitchcock, 1927), which was used, for example, in 4 the bi-linear RNNs discussed in Sutskever et al. (2011); Downey et al. (2017). The CP decomposition approximates the tensor as sum of rank-1 tensors = (cid:80)R w(h2) w(x) , which in terms of individual components, is expressed as: Wijk = (cid:80)R jr (x) kr . Here, denotes the outer product, and vectors w(h1) , and w(x) are the component vectors for the r-th rank-1 term. These component vectors are collected as columns in the factor matrices (h1) RHR, (h2) RHR, and (x) RDR, respectively. As result, the total number of parameters is reduced to R(2H + D). The input-dependent state transition matrix Ax can then be expressed compactly as: r=1 (h1) ir (h2) r=1 w(h1) , w(h2) Ax = (h1)diag (4) where input vector RD and diag() constructs diagonal matrix from vector. As shown in the appendix, an increasing number of factors (R), enables simulating state-machines with larger states, as factored models with larger provide better approximation of full bilinear model. (W (h2)), (W (x))x (cid:16) (cid:17)"
        },
        {
            "title": "2.2.2 Block-diagonal state transition tensor",
            "content": "An alternative method for controlling the parameter count in the bilinear model involves imposing block structures on the effective state transition matrix Ax. This is achieved by utilizing distinct three-way parameter tensors, denoted as (b) RH D, one for each block {1, . . . , B}. Here, = H/B represents the dimensionality of each blocks corresponding state subspace, assuming is an integer multiple of B. )ij = (cid:80) Consequently, the overall state transition matrix Ax adopts block-diagonal structure, where each diagonal block, A(b) RH , is generated from its respective tensor (b) via the relation (A(b) (b) ijkxk. As result, the total number of parameters is reduced by factor B. This block-diagonal parameterization can be conceptualized as employing independent heads, each processing distinct subspace of the hidden state vector using its own dense transition dynamics. It is reminiscent of block-diagonal transition matrices studied by Fan et al. (2024), albeit defining them as bi-linear, non-additive function of the inputs."
        },
        {
            "title": "2.3 Complex diagonal bi-linear RNNs",
            "content": "In Section 2.2, we established that the bilinear state-transition form defines an input-dependent state-transition matrix Ax, whose entries are linear functions of the input x, parameterized by third-order tensor W. In this section, we discuss how diagonalizing the state-transition matrix, common simplification in many linear RNN variants, reduces models expressive capability to commutative operations. First, consistent with common practice in the linear RNN literature (e.g., Orvieto et al. (2023)), we consider state-transition matrices that are diagonalizable over the complex numbers. This focus is justified because the set of non-diagonalizable matrices has measure zero (Axler, 2024); consequently, any matrix RN can be made diagonalizable over through an arbitrarily small perturbation of its entries (Zhinan, 2002). This implies that, based on the real Jordan Normal Form, the statetransition matrix Ax can be expressed as: Ax = PxDxP 1 , (5) where Px RHH is an invertible matrix and Dx RHH is real block-diagonal matrix with blocks of size 11 (for real eigenvalues) or 22 of the form C2 = (for complex conjugate pairs of eigenvalues + ib). Both Px and Dx are generally parametrized by the input x. particularly important special case is when the state-transition matrix Ax is orthogonal. This is highly desirable for the linear RNNs, as it ensures stability by guaranteeing that all eigenvalues of Ax have unit norm, preventing exploding or vanishing states during recurrent updates. In this scenario, the diagonal matrix Dx will be entirely composed of 2-dimensional rotation matrices, which we denote as R2 = (cid:18)cos θ sin θ cos θ sin θ (cid:19) . common simplification is to assume that the transformation matrix Px is independent of the input (i.e., Px = P). This fixed matrix can then be canceled out in the recurrence steps and absorbed into 5 (cid:19) (cid:18)a b the input and output transformations of the recurrent layer. The state dynamics are thus governed by Ax = Dx, which remains input-dependent and retains its block-diagonal structure, often simplified to purely diagonal with real, or even non-negative entries, e.g. in Mamba (Gu and Dao, 2024). 2 It is paramount to recognize that fixing while Dx varies with the input implies that the overall state-transition matrices Ax = PDxP 1 and Ay = PDyP 1 for different inputs and will commute if and only if their corresponding block-diagonal components Dx and Dy commute. Since 2 2 blocks of the form C2 or R2 (representing complex eigenvalues) also commute with other blocks of the same type, this architectural choice inherently restricts the model to commutative transition dynamics. In fact, model whose transition matrix Ax is directly parameterized as such block-diagonal matrix Ax = Dx (i.e., effectively = I) can naturally represent operations from any abelian group. Proposition 2. linear RNN of the form in Eq. (3) with orthogonal state-transition matrices Ax that share common, input-independent eigenbasis (i.e., Ax = PDxP 1 with fixed P) can simulate any abelian group (commutative operation). We refer to the appendix for the proof, where we also show visualization of the invariant subspaces and rotation angles learned by the model."
        },
        {
            "title": "2.4 Real diagonal bi-linear RNNs",
            "content": "Finally, the block-diagonal transition matrix, Dx, is often further simplified to be purely diagonal with real values; e.g., the RG-LRU cell utilized in the Hawk architecture (De et al., 2024). However, as noted by Grazzi et al. (2025), such models are incapable of learning modular addition. Contrary to this limitation, we will show that learning parity is not only straightforward, but that it is in fact trivial for linear RNN with real-valued diagonal state transitions which depend multiplicatively not additively on x. Length-generalization on the parity task is widely used to test the state tracking capabilities of sequence models (e.g., Anil et al. (2022); Grazzi et al. (2025)). Proposition 3. random network with frozen real-diagonal transition matrix (without additive terms) and learnable linear read-out layer learns parity with probability 1 2H , for arbitrary sequence length from only 2 training examples of odd and even parity. Freezing the recurrent weights turns the network effectively into bi-linear variant of an echo state network (Jaeger, 2007; Maass et al., 2002). Our result shows that an echo state network with state transitions depending only multiplicatively on the input can learn parity. We shall show experimental results confirming this result in practice in Section 3.4. This is in contrast to models like Mamba (Gu and Dao, 2024), in which state transitions are diagonal and positive, and which can therefore not learn parity even when adapting recurrent parameters during learning (Grazzi et al., 2025)."
        },
        {
            "title": "3 Experiments",
            "content": "Tasks: To evaluate the state-tracking capabilities of the bilinear RNN model variants introduced previously, we use the following three tasks: modular addition, random state machine, and modular arithmetic. In the modular addition task, the model processes sequence of integers, each randomly drawn from the set Zm = {0, , 1}, and is required to predict their sum modulo m. For the random state machine task, the model must simulate randomly generated finite-state machine where both the input alphabet Σ and the set of states are identical to Zm; and for each Q, the transition function is set to δ(q, σ) = πq(σ), where πq is random permutation of Σ. Finally, the modular arithmetic task involves processing sequence alternating between integers from Zm and arithmetic operators (from the set {+, , }); the model must compute and output the result of these operations, applied sequentially, with all calculations performed modulo m. For all tasks, multi-digit integers are tokenized to single tokens. We refer the reader to the appendix for examples and additional details on each task. Models: We compare full, factored, and block-diagonal bilinear models against several baseline architectures: non-linear RNNs (LSTM (Hochreiter and Schmidhuber, 1997) and Elman RNN (Elman, 1990)), Mamba, and Transformer models. All RNN-based models (including the bilinear, 2Note, however, that this restriction may not be as problematic when additive terms are present. 6 and non-linear ones) have single recurrent cell followed by linear classification head over the hidden states. For the Mamba and Transformer baselines, we evaluate configurations with 1, 2, and 4 layers. consistent hidden and input dimensionality of 256 is used across all models. Further details of the experimental setup are provided in the appendix. Recurrence stability: The absence of additive terms in recurrent formulation of Eq.(2) makes the introduced bilinear hidden states scale-invariant as discussed above. For inference, we can therefore normalize hidden states during the recurrence, while not doing so during training, without introducing any inconsistency between training and inference."
        },
        {
            "title": "3.1 Main results",
            "content": "We trained the models described above on the three tasks, on instances of lengths 2-10, and evaluated on instances of length 500. The training is done for 100,000 steps, where at each step 64 samples (of length 2-10) are generated for the task. Each model is trained using three different learning rates and picked the best performing model. Table 1 summarizes the main results. In all tables, we scale accuracy values such that 0 represents random chance, and 1 is perfect accuracy. We observe that bilinear models generally perform best across all tasks. Bilinear block-diagonal variants exhibit improved performance as the block size increases. Notably, the real-diagonal model (a bilinear block-diagonal model with block size 1) can only learn parity (i.e., modular addition with = 2); however, increasing the block size to two enables the learning of modular addition for larger values of m, as demonstrated in Proposition 2. Also, the R2 block diagonal model explicitly parametrizes the state transition matrix as rotation blocks of the form R2, with angles parameterized by inputs. In contrast block diagonal bilinear with block size of 2 parametrizes 2D-blocks freely from the input. Non-linear recurrent models, such as LSTM and simple RNN, also perform well on these statetracking tasks. It can be speculated that multiplicative interactions between hidden states and inputs arise from the gating mechanisms and non-linear activation functions within their recurrent structures. While Mamba can learn the tasks in-distribution for small state sizes m, it largely fails to generalize to longer sequences. Also, the failure of transformers in length-generalization is well-known observation in the literature. For further results we refer the reader to the appendix."
        },
        {
            "title": "3.2 Data Efficiency",
            "content": "We showed in the previous section that bi-linear models are effective at learning state tracking tasks. However, since the number of parameters grows as the product of the input embeddingand hidden dimension, their parameter counts can be extraordinarily large. While this may prove to be unproblematic in large-scale multi-task and language modeling tasks, data efficiency is concern. To gain insights on the data efficiency of bi-linear models, we train and evaluate the models on the tasks discussed in the previous section, using fixed training set sizes We also compare to LSTM. All models were trained on an input sequence length of 10, using the optimal learning rate found in the previous experiment. The results are shown in Figure 2. They show that despite the large number of parameters, the models are not less data efficient than the LSTM. This is true even of the full bi-linear model (denoted Bilinear in the figure)."
        },
        {
            "title": "3.3 Multiplicative versus additive interactions",
            "content": "As we discussed, for full bi-linear models multiplicative interactions without additive contributions are sufficient for learning state transitions, and for factored rotational models the presence of additive terms can even be detrimental. Table 2 (left) shows the OOD performance of full and rotational models on the modular addition and state machine tasks discussed previously. Models are trained with and without input-dependent additive contributions, with and without bias, as well as without any additive terms (4 settings in total). As seen in the table, additive contributions do not change performance for the full bi-linear model on any task, but they reduce performance for the rotational model on the modular addition task. Results are shown with additive parameters initialized uniform at random across various intervals. Only for very small initialization does constant bias term not have detrimental effect on performance. 7 Table 1: In-distribution and length-generalization performance (normalized, such that 0 is random chance) of various RNN models on three state tracking tasks: modular addition, random state machine, and modular arithmetic. Modulo 3 5 10 25 50 3 5 10 25 50 Validation Accuracy (Length 2-10) OOD Accuracy (Length 500) Bilinear Factored Bil. Block Diag. (block size) R2 Block Diag. LSTM RNN Mamba (layers) Transformer (layers) Bilinear Factored Bil. Block Diag. (block size) R2 Block Diag. LSTM RNN Mamba (layers) Transformer (layers)"
        },
        {
            "title": "Bilinear",
            "content": "Factored Bil. Block Diag. (block size) R2 Block Diag."
        },
        {
            "title": "Mamba",
            "content": "(layers)"
        },
        {
            "title": "Transformer",
            "content": "(layers) 1.00 1.00 1.00 1.00 1. 1.00 1.00 1.00 1.00 0.99 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 1.00 1.00 0.60 0.99 1. 1.00 0.61 1.00 1.00 0.99 1. 1.00 0.88 1.00 1.00 1.00 1. 0.96 1.00 1.00 1.00 1.00 1. 1.00 0.92 1.00 1.00 1.00 1. 1.00 1.00 1.00 0.28 1.00 1. 1.00 0.29 1.00 1.00 1.00 1. 1.00 0.94 1.00 1.00 1.00 0. 0.30 0.77 1.00 1.00 0.34 1. 1.00 0.83 0.99 1.00 0.63 0. 0.99 1.00 1.00 0.88 1.00 1. 1.00 1.00 1.00 1.00 0.96 1. 1.00 1.00 1.00 1.00 1.00 1. 0.20 0.84 1.00 1.00 0.19 1. 1.00 0.96 1.00 1.00 0.83 0. 1.00 1.00 0.90 0.24 0.53 1. 1.00 0.21 1.00 1.00 0.56 0. 0.99 0.46 0.81 0.75 1.00 1. 0.85 1.00 1.00 1.00 1.00 1. 1.00 0.85 1.00 1.00 0.47 0. 0.99 1.00 1.00 0.14 0.49 1. 1.00 0.11 1.00 1.00 0.55 0. 0.99 0.46 0.61 0.84 1.00 0. 0.27 0.52 1.00 1.00 0.23 1. 1.00 0.59 0.93 0.99 0.32 0. 0.32 1 2 8 64 2 4 1 2 4 2 8 64 1 2 1 2 4 1 2 64 1 2 4 1 4 Modular Addition 1.00 1.00 0.32 1. 1.00 1.00 1.00 0.99 1.00 0. 1.00 0.47 0.19 0.00 0.02 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 1.00 0.00 0.00 0.01 0. 0.01 0.04 State Machine 1.00 0.19 0. 0.15 0.21 1.00 0.02 0.30 0. 0.19 0.30 0.41 0.18 0.17 0. 1.00 1.00 1.00 1.00 1.00 1. 1.00 1.00 1.00 0.00 0.00 0. 0.03 0.01 0.00 Modular Arithmetic 1.00 0. 0.14 0.21 0.47 0.66 0.04 0. 0.22 0.14 0.33 0.29 0.08 0. 0.08 1.00 1.00 0.19 0.37 1. 1.00 0.02 1.00 1.00 0.74 0. 0.92 0.19 0.19 0.19 1.00 1. 0.00 1.00 1.00 1.00 0.00 1. 1.00 0.01 0.02 0.01 0.01 0. 0.00 1.00 1.00 0.03 0.34 1. 1.00 0.00 1.00 1.00 0.99 1. 0.99 0.01 0.01 0.02 1.00 0. 0.15 0.00 1.00 1.00 0.00 1. 1.00 0.55 0.38 0.75 0.02 0. 0.12 1.00 1.00 0.45 1.00 1. 1.00 1.00 1.00 1.00 0.74 1. 1.00 0.98 0.89 0.92 1.00 1. 0.09 0.25 0.48 1.00 0.07 1. 0.41 0.34 0.44 0.62 0.27 0. 0.49 1.00 0.03 0.16 0.27 0. 1.00 0.03 1.00 0.82 0.24 0. 0.55 0.10 0.11 0.07 8 1. 1.00 0.00 1.00 1.00 1.00 1. 0.98 1.00 0.01 0.01 0.00 0. 0.00 0.00 1.00 1.00 0.00 0. 1.00 1.00 0.00 1.00 1.00 0. 0.96 0.97 0.02 0.01 0.01 1. 0.37 0.09 0.08 0.41 1.00 0. 1.00 1.00 0.29 0.41 0.48 0. 0.05 0.03 1.00 1.00 0.10 1. 1.00 1.00 0.66 1.00 0.98 0. 0.00 0.01 0.00 0.00 0.00 1. 1.00 0.00 0.06 0.41 1.00 0. 1.00 0.99 0.31 0.42 0.47 0. 0.01 0.00 1.00 0.06 0.11 0. 0.24 1.00 0.04 1.00 1.00 0. 0.29 0.51 0.01 0.04 0.03 1. 1.00 0.00 1.00 1.00 1.00 0. 0.00 0.37 0.00 0.00 0.00 0. 0.00 0.01 1.00 1.00 0.00 0. 0.13 1.00 0.00 0.64 0.19 0. 0.18 0.24 0.00 0.00 0.00 1. 0.04 0.04 0.03 0.06 0.40 0. 0.99 0.55 0.08 0.11 0.17 0. 0.02 0.02 1.00 0.95 0.02 1. 1.00 1.00 0.00 0.02 0.07 0. 0.00 0.00 0.00 0.00 0.00 1. 0.01 0.00 0.02 0.04 1.00 0. 0.09 0.07 0.07 0.09 0.10 0. 0.00 0.00 0.99 0.03 0.03 0. 0.08 0.21 0.03 0.64 0.16 0. 0.07 0.09 0.01 0.01 0.03 Figure 2: Out-of-distribution performance of variety of models on state tracking tasks from Table 1. Table 2: (Left) Effect of (input dependent or constant) additive bias terms in the hidden-state update rule. (Right) Out-of-distribution (length generalization) performance on parity with random multiplicative RNN with and without additive terms. Modulo Additive Terms 2 3 10 25 Training Examples Training Length 10 20 50 10 100 20 Input Dependent 0.00 0.01 0.00 0.00 0. Input Dependent 0.05 0.07 0.05 0.03 0. 0.04 Input Dep. + Const. 0.03 0.00 0.00 0. 0.00 Input Dep. + Const. 0.06 0.06 0.02 0. 0.04 0.06 Const. None 0.03 0. 0.00 0.00 0.00 Const. 0.04 0. 0.05 0.87 0.74 0.01 1.00 1. 1.00 1.00 0.98 None 1.00 1. 1.00 1.00 1.00 1."
        },
        {
            "title": "3.4 Learning parity with a random network",
            "content": "Table 2 (right) shows the OOD performance (testing length 400, best across 3 seeds and 2 learning rate) of this type of model, after training on sequences of lengths 10 50. It shows, in line with the theoretical result, that the pure bi-linear model can solve the task, even though recurrent parameters are frozen during training (only the read-out layer is trained). It also shows the detrimental effect of additive terms for comparison."
        },
        {
            "title": "4 Discussion",
            "content": "Our work shows that models in which hidden states depend bi-linearly on previous hidden states and inputs can learn state tracking tasks. This is in contrast to many linear RNNs, such as Mamba (Gu and Dao, 2024), LRU (Orvieto et al., 2023), and others. It can also be viewed as extending upon the studies by Grazzi et al. (2025); Sarrof et al. (2024); Fan et al. (2024) to improve state tracking behavior beyond that work. However, it is important to note that bi-linear models come at the cost of parameter count that grows roughly cubically in the number of hidden states. Whether there are ways to reduce the number of parameters while retaining strong performance across wide range of state tracking tasks is an important question for future research. closely related question is whether such reduction may be counter-productive (or conversely the large number of parameters even be beneficial) in massive multi-task scenarios such as language modeling, which are possible with linear models due to the possibility for efficient, parallel training. Our work suggests to distinguish between state tracking tasks with and without commutative structure, with the latter being learnable by larger class of models including, in particular, smaller models. This raises the question in which real-world scenarios commutative state tracking tasks would, or would not, suffice, as another direction for future work."
        },
        {
            "title": "References",
            "content": "the globality barrier and inductive scratchpad. E. Abbe, S. Bengio, A. Lotfi, C. Sandon, and O. Saremi. How far can transformers reason? In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 2785027895. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 3107e4bdb658c79053d7ef59cbc804dd-Paper-Conference.pdf. C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. Ramasesh, A. Slone, G. Gur-Ari, In E. Dyer, and B. Neyshabur. Exploring length generalization in large language models. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3854638556. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ fb7451e43f9c1c35b774bcfad7a5714b-Paper-Conference.pdf. M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 11201128, New York, New York, USA, 2022 Jun 2016. PMLR. S. Axler. Linear algebra done right. Springer Nature, 2024. M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. CoRR, abs/2405.04517, 2024. N. Chomsky. Three models for the description of language. IRE Transactions on information theory, 2(3):113124, 1956. S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, and P. A. Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=WbxHAzkeQcn. C. Downey, A. Hefny, B. Boots, G. J. Gordon, and B. Li. Predictive state recurrent neural networks. In Advances in Neural Information Processing Systems, 2017. URL https://papers.nips.cc/ paper/7186-predictive-state-recurrent-neural-networks. N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. Le Bras, J. Hwang, S. Sanyal, X. Ren, A. Ettinger, Z. Harchaoui, and In A. Oh, T. NauY. Choi. Faith and fate: Limits of transformers on compositionality. mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 7029370332. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ deb3c28192f979302c157cb653c15e90-Paper-Conference.pdf. M. Ebrahimi, S. Panchal, and R. Memisevic. Your context is not an array: Unveiling random In First Conference on Language Modeling, 2024. URL access limitations in transformers. https://openreview.net/forum?id=MLD1cwfjUb. J. L. Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990. T.-H. Fan, T.-C. Chi, and A. Rudnicky. Advancing regular language reasoning in linear recurrent neural networks. In NAACL (Short Papers), pages 4553, 2024. URL https://doi.org/10. 18653/v1/2024.naacl-short.4. R. Grazzi, J. Siems, J. K. Franke, A. Zela, F. Hutter, and M. Pontil. Unlocking state-tracking in linear RNNs through negative eigenvalues. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=UvTo3tVBk2. 10 A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=tEYskw1VY2. F. L. Hitchcock. The expression of tensor or polyadic as sum of products. Journal of Mathematics and Physics, 6(1-4):164189, 1927. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. J. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to Automata Theory, Languages, and Computation (3rd Edition). Addison-Wesley Longman Publishing Co., Inc., USA, 2006. ISBN 0321455363. H. Jaeger. Observable operator models for discrete stochastic time series. Neural Computation, ISSN 0899-7667. doi: 10.1162/089976600300015411. URL 12(6):13711398, June 2000. https://doi.org/10.1162/089976600300015411. H. Jaeger. Echo state network. Scholarpedia, 2(9):2330, 2007. doi: 10.4249/scholarpedia.2330. revision #196567. H. Kurzweil and B. Stellmacher. The theory of finite groups: an introduction, volume 1. Springer, 2004. M. Littman and R. S. Sutton. Predictive representations of state. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, volume 14. MIT Press, 2001. URL https://proceedings.neurips.cc/paper_files/paper/2001/file/ 1e4d36177d71bbb3558e43af9577d70e-Paper.pdf. W. Maass, T. Natschläger, and H. Markram. Real-time computing without stable states: new framework for neural computation based on perturbations. Neural Comput., 14(11):25312560, Nov. 2002. ISSN 0899-7667. E. Martin and C. Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=HyUNwulC-. R. Memisevic and G. E. Hinton. Learning to represent spatial transformations with factored higherorder boltzmann machines. Neural Comput., 22(6):14731492, June 2010. ISSN 0899-7667. URL https://doi.org/10.1162/neco.2010.01-09-953. W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In International Conference on Machine Learning, pages 3549235506. PMLR, 2024. V. Michalski, R. Memisevic, and K. Konda. Modeling deep temporal dependencies with recurrent grammar cells\"\". In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/ file/aca342ec0893b43b016f29ab8d2c6eec-Paper.pdf. B. A. Olshausen, C. Cadieu, J. Culpepper, and D. K. Warland. Neural Computation. Number v. 17, nos. 1-4. MIT Press, 2005. URL https://books.google.ca/books?id=zzZVAAAAMAAJ. A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Y. Sarrof, Y. Veitsman, and M. Hahn. The expressive capacity of state space models: formal language perspective. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=eV5YIrJPdy. 11 I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML11, page 10171024, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195. J. Tenenbaum and W. Freeman. Separating style and content. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996. URL https://proceedings.neurips.cc/paper_files/paper/1996/file/ 70222949cc0db89ab32c9969754d4758-Paper.pdf. M. Wolter and A. Yao. Complex gated recurrent neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/ 652cf38361a209088302ba2b8b7f51e0-Paper.pdf. Y. Wu, S. Zhang, Y. Zhang, Y. Bengio, and R. R. Salakhutdinov. On multiplicative integration with recurrent neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/paper/2016/ file/f69e505b08403ad2298b9f262659929a-Paper.pdf. Z. Zhinan. The jordan canonical form of rational random matrix. Science Direct Working Paper, (S1574-0358):04, 2002."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Proof of Proposition 1 Proposition 1. The bilinear state transition model defined in Equation (2) is capable of simulating any finite state machine = (Q, Σ, δ, q0). Proof. Given any state machine = (Q, Σ, δ, q0), and for all inputs σ Σ, we define the state transition matrix σ {0, 1}QQ with the (i, j)-th element given by: (σ)ij = (cid:26)1 0 if δ(qj, σ) = qi, otherwise. (6) This means that if the state machine is in state qj, and and receives input σ, the one-hot representation of the next state is precisely the j-th column of σ. Consequently, if qt is the state at time and ht {0, 1}Q is its one-hot encoded representation, the state dynamics can be expressed as: qt = δ(qt1, σt) ht = σtht1 (7) To demonstrate that the bilinear state-transition form from Eq. (2) can represent any state machine, it suffices to show that the third-order tensor can be constructed such that its resulting state transition matrix Ax equals σ for every σ Σ, where is the embedding corresponding to σ. Without loss of generality, let the input alphabet be Σ = {1, 2, . . . , Σ}. We define the input embedding {0, 1}Σ as the one-hot vector for the current input symbol σ Σ (thus, the input dimension is effectively Σ). The third-order tensor is then constructed such that for each σ Σ, the slice Wσ is set equal to the state machines transition matrix σ. From Eq. (3), we will have: (Ax)ij = (cid:88) Wijkxk = Wijσ = (σ)ij. (8) This is because is the one-hot vector for input symbol σ (meaning xk = 1 if = σ, and xk = 0 otherwise). Therefore, this construction yields Ax = σ. Consequently, the state-transition dynamics of the bilinear model, ht = Axtht1, become equivalent to those of the state machine, ht = σtht1. A.2 Proof of Proposition 2 Proposition 2. linear RNN of the form in Eq. (3) with orthogonal state-transition matrices Ax that share common, input-independent eigenbasis (i.e., Ax = PDxP 1 with fixed P) can simulate any abelian group (commutative operation). Proof. We show by construction that bilinear model with state transition matrix that is blockdiagonal with R2 rotation blocks can represent modular addition, and hence any cyclic group. Based on the fundamental theorem of finite abelian groups, every finite abelian group can be expressed as the direct sum of cyclic groups (typically of prime-power order) (Kurzweil and Stellmacher, 2004). Therefore, any model capable of simulating modular addition can, in principle, simulate any finite abelian group. First, lets clarify how an orthogonal transition matrix simplifies to block-diagonal form composed of 2-dimensional rotation blocks. (Further details are available in Section 2.3). Based on the Real Jordan Normal Form, an orthogonal matrix Ax is similar to real block-diagonal matrix Dx (Axler, 2024). This means: Ax = PxDxP 1 , (9) where Px RHH is the invertible transformation matrix, and Dx RHH is real block-diagonal matrix composed entirely of 2-dimensional rotation matrices (assuming is even), which we denote as R2(θ) = (cid:18)cos θ sin θ cos θ sin θ (cid:19) . (10) (11) (12) (13) (14) (15) (16) (17) Now, if the transformation matrix Px is independent of the input (i.e., Px = P), the fixed matrix can be canceled out in the recurrence steps and absorbed into the input and output transformations of the recurrent layer. Therefore, for an orthogonal transition matrix with such input-independent transformations, we can effectively model Ax as being block-diagonal, with its blocks being 2dimensional rotation matrices R2(θ(x)), where the rotation angles θ(x) are parameterized by the input x. Next, we present simple construction with = 2 (i.e., Ax = R2(θ(x))) that simulates modular addition. Let xσ be the embedding corresponding to an input integer σ Zm = {0, 1, . . . , 1}. We define the rotation angle for input σ as θ(xσ) = 2πσ/m. Consequently, after observing sequence of inputs σ1, σ2, . . . , σT , the hidden state vector R2 (with initial state h0) evolves as follows: hT = AxT hT 1 = Ax1 Ax2 AxT h0 = R2 (cid:0)θ (cid:0)xT (cid:1)(cid:1) h0 = = R2 = R2 R2 (ϕ) = R2 (ϕ mod 2π) (cid:0)θ (cid:0)x1(cid:1)(cid:1) R2 (cid:32) (cid:0)θ (cid:0)x2(cid:1)(cid:1) R2 (cid:33) (cid:88) θ (cid:0)xt(cid:1) h0 t=1 (cid:32) (cid:88) t=1 (cid:32)(cid:32) (cid:33) 2πσt 2πσt (cid:88) t=1 h0 (cid:33) (cid:33) mod 2π h0 (2π σ ) mod 2π = (σ mod m)( 2π ) = (cid:32) 2π (cid:32) (cid:88) t=1 (cid:33)(cid:33) σt mod h0 Let = ((cid:80)T t=1 σt) mod be the target sum modulo m. This means hT is h0 rotated by 2π (cid:18) 2π hT = R2 h0. (cid:19) y: Finally, since this rotation is unique for every possible value of Zm, linear readout layer (i.e., an m-class linear classifier) can perfectly extract from hT : hT , = argmax (18) kZ with wk = R2 (cid:19) (cid:18) 2π k h0, Zm. (19) A.3 Proof of Proposition 3 Proposition 3. random network with frozen real-diagonal transition matrix (without additive terms) and learnable linear read-out layer learns parity with probability 1 2H , for arbitrary sequence length from only 2 training examples of odd and even parity. Proof. For the parity task, the model observes an input sequence σ1, σ2, . . . , σT , with each σt {0, 1}. The objective is to output the parity of this sequence, which is mod 2. (cid:16)(cid:80)T t=1 σt(cid:17) Let A[0] and A[1] be the diagonal state-transition matrices corresponding to inputs 0 and 1, with emba[0] and emba[1] denoting the diagonal elements: A[0] = diag(a[0]), A[1] = diag(a[1]) (20) The hidden state evolves according to ht = A[σt]ht1. For the i-th component of the hidden state, this evolution is: = a[σt] hT hT 1 = h0 a[σt] , (21) (cid:89) t=1 14 where a(σ) denotes the i-th diagonal element of A(σ) (i.e., the i-th element of a[σ]). (cid:17) t=1 a[σt] t=1 a[σt] (cid:16)(cid:81)T < 0, then sgn > 0 and a[1] can encode parity. If, for given component i, we Crucially, the sign of the product (cid:81)T have a[0] will be positive for an even number of 1s (even parity) and negative for an odd number of 1s (odd parity). This is because the number of negative ) in the product matches the count of 1s in the input sequence. Conversely, if a[0] terms (a[1] < 0 and a[1] > 0, the sign of the product becomes (1)T count of 1s, which also encodes parity, albeit h0 in manner dependent on the sequence length . Since sgn is initialized with fixed sign (e.g., positive), then in either case where a[0] have opposite signs (i.e., a[0] contains sufficient information to determine the parity of the input sequence. The model then only needs to update its read-out layer (e.g., with one example of even and one of odd parity) to decode parity from hT ; all other recurrent parameters could remain fixed. < 0), the sign of hT and a[1] t=1 a[σt] = sgn(hT ), if h0 a[1] (cid:16)(cid:81)T (cid:17) Assuming that the elements of a(0) and a(1) are i.i.d. and symmetrically distributed around 0 at initialization, we analyze the probability of finding such suitable component i. The probability that an arbitrary component has a[0] with opposite signs is 0.5. Therefore, given the independence across the components, the probability that there exists at least one component for which a[0] < 0 is 1 (1 0.5)H = 1 2H . and a[1] a[1] Remark: For the model to learn parity for arbitrary sequence lengths using simple sign-based readout from single component i, the ideal condition is a[1] > 0. Under the same i.i.d. symmetric initialization assumptions, this specific configuration for component occurs with probability 1/4. Therefore, the probability that at least one such ideally suited component exists is 1 (1 1/4)H = 1 (3/4)H . < 0 and a[0]"
        },
        {
            "title": "B Implementation details",
            "content": "B.1 Tasks For all tasks, to generate training sample, we first randomly select the number of inputs U(2, ), where is the maximum training sequence length. We then select input symbols from {0, 1, . . . , m1} uniformly at random with replacement. For the modular arithmetic task specifically, we also sample 1 operators uniformly with replacement from the set {+, , }, which are then interleaved with the input symbols. Each sample is structured with special tokens: it begins with [BOS] (beginning of sequence) token and the input portion concludes with an [EOI] (end of input) token, immediately followed by the TARGET, as shown: [BOS] INPUT1 INPUT2 INPUT3 INPUTn [EOI] TARGET Each input symbols (including multi-digit integers and potentially operators), and special tokens [BOS] and [BOI] are tokenized as single tokens. During training and inference, all model outputs are disregarded except for the output corresponding to the [EOI] token; this output is taken as the models prediction for TARGET. Consequently, during training, the loss is calculated only on this final target prediction. We evaluate the models on the following three tasks: Modular addition: The target is the sum of input integers modulo m. For example, with = 5 and = 20: [BOS] 8 0 12 18 5 [EOI] 3 Modular arithmetic: This task involves processing sequence alternating between integers from {0, 1, . . . , 1} and 1 arithmetic operators from {+, , }. The target is the result of these 15 operations applied sequentially from left to right, with all calculations performed modulo m. For example, with = 5 and = 20: [BOS] 3 * 9 - 17 + 6 + 12 [EOI] The target is calculated as: (3 9) mod 20 = 7 (7 17) mod 20 = 10 (10 + 6) mod 20 = 16 (16 + 12) mod 20 = 8 Simulating state machines: The objective is to simulate randomly generated finite state machine (FSM). Both the input alphabet Σ and the set of states are identical to {0, 1, . . . , 1}. For each state Q, the transition function δ(q, σ) is defined as πq(σ), where πq is random permutation of Σ. This transition function δ is generated once per FSM definition and remains fixed for all samples related to that FSM. The first input symbol in the sequence, INPUT1, determines the initial state of the FSM. Subsequent symbols INPUT2, . . . , INPUTn are processed as inputs to the FSM, and the target is the FSMs final state. For example, consider = 6 and the following randomly generated transition function: Input Symbol (σ) 0 3 2 5 5 1 1 0 1 0 0 0 4 2 4 0 2 1 3 0 3 5 3 1 2 4 3 4 1 5 3 4 2 1 5 2 4 4 3 5 2 q ( t ) 0 1 2 3 4 5 r An example sequence for this FSM would be: [BOS] 4 1 2 5 5 [EOI] The initial state is the first input (4 in this example), and upon observing each input, the state transitions occur based on the transition table: State: 4, Input: 1 δ(4,1)=0 New State: 0 State: 0, Input: 2 δ(0,2)=4 New State: 4 State: 4, Input: δ(4,5)=5 New State: 5 State: 5, Input: 5 δ(5,5)=2 New State: 2 (Target) B.2 Experiment Setup For the experiments discussed in Section 3.1 and reported in Table 1, all models were trained using the ADAM optimizer with three learning rates (103, 104, 105), and the configuration yielding the best performance was selected for reporting. All models were trained from random initializations, without learning rate scheduling, weight decay, or dropout. In addition, the parameters of bilinear models (and variants) were initialized from uniform distribution U(0.01, 0.01). Training was conducted for 100,000 steps with batch size of 64. An early stopping criterion was applied if the validation loss fell below 105. For these experiments, training examples were randomly sampled at each training step with input sequence lengths ranging from 2 to 10, while models were evaluated on inputs of length 500. For the data efficiency experiments detailed in Section 3.2 (results in Figure 2), we used the optimal learning rate identified for each model and task from the previous experiment. We constructed fixed 16 training sets of specified sizes and trained models for 1000 epochs over each set. Other settings were kept consistent with those in the previous experiment. Regarding the baseline models in Table 1, the Transformer baseline is based on the GPT-2 architecture (Radford et al., 2019) with configurations of 1, 2, and 4 layers, and model (embedding/hidden) dimension of 256, consistent with other models. Other parameters, such as an MLP inner expansion factor of 4, followed default GPT-2 (small) settings. We also used Mamba-1 (Gu and Dao, 2024) with 1, 2, and 4 layers, setting its model and hidden dimensions to 256. For other configurations, we adopted default values of Mamba-130M, including an intermediate expansion size of 512, state space dimension of 16, and convolution kernel size of 4. All experiments were conducted on cluster of A100 GPU nodes. single training and evaluation run for given model configuration, task, and setting typically completed on single GPU within an hour in most cases, or up to few hours in the worst case."
        },
        {
            "title": "C Additional experiments",
            "content": "C.1 Effect of number of factors Similar to the results presented in Section 3.1, Table 3 presents the validation and out-of-distribution accuracy of factored bilinear models on the state machine simulation task, considering an increasing number of factors (R) across various state space sizes (m). As these results indicate, increasing the number of factors enables the simulation of larger state machines, as factored model with higher more closely approximates full bilinear model. Table 3: In-distribution and length-generalization (normalized) accuracy of factored bilinear models with different number of factors, on the state machine simulation task. Validation Accuracy (Length 2-10) OOD Accuracy (Length 500) # States 2 3 5 10 25 2 3 5 10 25 Model # Factors 1 2 4 16 64 128 256 512 2048 0.00 0.00 1.00 1.00 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.02 0.03 1.00 1.00 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.00 0.00 0.65 0.92 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.02 0.01 0.02 0.17 0. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.01 0.00 0.01 0.01 0. 0.26 0.68 1.00 1.00 1.00 1. 1.00 0.00 0.00 0.00 0.01 0. 0.04 0.10 0.19 0.45 1.00 1. 1.00 0.03 0.00 1.00 1.00 0. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.00 0.01 0.97 1.00 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.00 0.00 0.29 0.58 1. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.00 0.00 0.01 0.04 0. 1.00 1.00 1.00 1.00 1.00 1. 1.00 0.01 0.00 0.00 0.00 0. 0.08 0.26 1.00 1.00 1.00 1. 1.00 0.00 0.01 0.00 0.00 0. 0.00 0.00 0.01 0.10 0.78 1. 1.00 Factored Bilinear"
        },
        {
            "title": "Bilinear",
            "content": "C.2 Learning commutative tasks by rotating phase angles Figure 3 illustrates the rotation angles learned by the R2 block-diagonal model for the modular addition task with = 10. The figure displays these angles for each input integer (0, 1, . . . , 9) across several 2-dimensional hidden state subspaces (20 out of 128). These subspaces are ordered based on the magnitude of the classifier weights associated with them in the linear readout layer. The harmonic value, reported for each subspace, is calculated as θ1/(2π/m), where θ1 is the learned rotation angle for the input integer 1. This harmonic value indicates how closely θ1 aligns with an integer multiple of the fundamental angle 2π/m required for ideal cyclic group representation. 17 Figure 3: Visualization of the rotation angles learned by the R2 block-diagonal model for each input integer in the = 10 modular addition task. Each subplot corresponds to distinct 2-dimensional hidden state subspace. These subspaces are ordered based on the magnitude of the classifier weights. θ1 2π/m , where θ1 is the learned rotation angle for the integer 1. The harmonic is"
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}