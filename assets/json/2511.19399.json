{
    "paper_title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
    "authors": [
        "Rulin Shao",
        "Akari Asai",
        "Shannon Zejiang Shen",
        "Hamish Ivison",
        "Varsha Kishore",
        "Jingming Zhuo",
        "Xinran Zhao",
        "Molly Park",
        "Samuel G. Finlayson",
        "David Sontag",
        "Tyler Murray",
        "Sewon Min",
        "Pradeep Dasigi",
        "Luca Soldaini",
        "Faeze Brahman",
        "Wen-tau Yih",
        "Tongshuang Wu",
        "Luke Zettlemoyer",
        "Yoon Kim",
        "Hannaneh Hajishirzi",
        "Pang Wei Koh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 9 9 3 9 1 . 1 1 5 2 : r DR Tulu: Reinforcement Learning with"
        },
        {
            "title": "Evolving Rubrics for Deep Research",
            "content": "Rulin Shao1, Akari Asai2,3, Shannon Zejiang Shen4, Hamish Ivison1,2, Varsha Kishore1,2, Jingming Zhuo1, Xinran Zhao3, Molly Park1, Samuel G. Finlayson1,5, David Sontag4, Tyler Murray2, Sewon Min2,6, Pradeep Dasigi2, Luca Soldaini2, Faeze Brahman2, Wen-tau Yih1, Tongshuang Wu3, Luke Zettlemoyer1, Yoon Kim4, Hannaneh Hajishirzi1,2, Pang Wei Koh1,2 1University of Washington, 2Allen Institute for AI, 3Carnegie Mellon University 4Massachusetts Institute of Technology, 5Seattle Childrens Hospital, 6University of California, Berkeley Joint first authors, Core contributors. See full author contributions here. Date: November 27, 2025 Correspondence: rulins@cs.washington.edu, akaria@allenai.org Code: github.com/rlresearch/dr-tulu Data & Models: huggingface.co/collections/rl-research/dr-tulu Blogpost: allenai.org/blog/dr-tulu"
        },
        {
            "title": "Abstract",
            "content": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems. Figure 1 Performance vs. cost of deep research models. We report average performance over 4 longform DR benchmarks (ScholarQACSv2, HealthBench, ResearchQA, and DeepResearchBench) against inference cost (USD per query on ScholarQA-CSv2). DR Tulu-8B lies on the Pareto frontier, outperforming larger open models and matching more expensive proprietary models. Full results in Table 2. 1 Figure 2 Overview of training deep research model with reinforcement learning with evolving rubrics (RLER). Left: An example of question and long-form response from DR Tulu with citations. Right: We train the policy model on dynamic set of rubrics that (1) co-evolve with the policy update (details in Figure 3) and (2) are grounded on real-world, searched knowledge from the environment. Compared to commonly-used closed-book rubrics generated purely from LM parametric knowledge (blue circle), our evolving rubrics incorporate newly searched information and are continuously tailored to the current policy models behaviors, better capturing the nuances required for long-form DR tasks."
        },
        {
            "title": "Introduction",
            "content": "Deep research (DR) models aim to produce in-depth, well-attributed answers to complex research tasks by planning, searching, and synthesizing information from diverse sources (OpenAI, 2025). Existing open DR models are either training-free, using manually designed prompts with off-the-shelf models (Li et al., 2025b,a), or trained indirectly via Reinforcement Learning with Verifiable Rewards (RLVR) on search-intensive, shortform question answering as proxy for deep research tasks (Jin et al., 2025; Nguyen et al., 2025; Liu et al., 2025). Directly training for deep research is challenging because it is hard to evaluate responses: the desiderata for good response are often underspecified (Xu et al., 2023; Krishna et al., 2021), so finite set of static rubrics cannot capture all the dimensions along which response could be good or bad. Moreover, given the knowledge-intensive nature of DR tasks, reliable evaluation requires access to vast, dynamic corpus of world knowledge, rather than relying solely on the models parametric knowledge (Gunjal et al., 2025). In this paper, we introduce Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research tasks. To address the challenge of verification in long-form tasks, DR Tulu is first finetuned on high-quality, naturally occurring user data, and then trained via new method we call Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training. As Figure 2 illustrates, at each training step, we sample several responses and search traces from the model and generate new rubrics that capture and contrast the good and bad points of these responses. This allows us to dynamically incorporate newly explored information into the rubrics and to ensure that they provide on-policy feedback that can discriminate among model responses. DR Tulu-8B outperforms the strongest open 8-32B models, including previous state-of-the-art Tongyi Deep Research 30B (Team et al., 2025), by 2.840.3 percentage points on four long-form DR benchmarksAstaBenchScholarQA-CS2 (Asai et al., 2024; Bragg et al., 2025), DeepResearchBench (Du et al., 2025), ResearchQA (Yifei et al., 2025), and HealthBench (Arora et al., 2025). In addition, it matches or exceeds proprietary systems (e.g., 2 OpenAI Deep Research, Perplexity Deep Research, and Gemini3 Pro + Search). As shown in Figure 1, DR Tulu-8B is substantially more cost-efficient than all other models; on ScholarQA-CS2, OpenAI Deep Research costs about USD 1.8 per query, whereas DR Tulu is almost 3 orders of magnitude cheaper at USD 0.0019. To assess performance on an out-of-domain, challenging real-world expert DR task, we additionally construct GeneticDiseasesQA, clinical dataset that requires models to search for and synthesize supporting evidence to assess the therapeutic eligibility of disease-causing genetic variants. On GeneticDiseasesQA, DR Tulu-8B similarly exceeds or competes with proprietary DR models; no other open models can tackle this task because they are unable to provide citations to supporting evidence. Our analysis shows that RLER improves the models ability to produce more comprehensive and in-depth long-form responses with accurate citations, yielding gains of 4.414.5 points on top of the finetuned model across the four benchmarks. Moreover, DR Tulu learns to select appropriate search tools and arguments depending on the task, instead of relying on single hard-coded search tool like in prior work (Gao et al., 2025; Bragg et al., 2025). For instance, on ResearchQA, DR Tulu uses paper search 90% of the time, whereas on DeepResearchBench, whose questions span more diverse, general-domain topics, it relies on web search and browsing about 55% of the time. We fully release our data, code, and models to support future work on long-form deep research. In particular, we provide deep research library with MCP search tools (dr-agent-lib), together with an evaluation suite, that integrates diverse search and browsing tools and allows new tools to be specified and swapped in with minimal effort. Our training and inference stack supports asynchronous tool calling, making it practical to train and evaluate models on deep research trajectories with many tool calls. We hope these resources will serve as simple, extensible foundation for future open deep research agents."
        },
        {
            "title": "2 Problem Formulation for Deep Research",
            "content": "We consider deep research model to be language model (LM) equipped with search tools. Each tool takes query and arguments, returning textual resources that can be cited in the models answer. Formally, let = {T1, T2, . . .} denote the available tools. Each tool Tk takes query with optional argument string α and returns an observation = Tk(q; α). The models policy πθ (with parameters θ) operates autoregressively over sequence of text , initialized as s0 = (the task and system instructions). Concretely, we define the models action space as { think , tool , answer , cite }, with corresponding protocol tokens: think (<think></think>) uses the LM itself to plan next steps given the current state and information. tool (<call_tool></call_tool>) invokes one of multiple search-related tools. The specific tool is chosen by setting the name attribute and tool-specific arguments. Example: <call_tool name=\"google_search\" k=\"10\" lang=\"en\">query</call_tool>. We append the tools output, in plain text, to the context for subsequent steps. answer (<answer></answer>) produces the final response and stops. cite (<cite id=\"SOURCE_ID\"></cite>) is used within the answer to wrap claims in citation tags that point to the supporting source. Ideally, these citations should be as localized as possible (e.g., to snippet within webpage vs. the entire webpage). At each step i, the model samples an action and its content or arguments, (ai, ζi) πθ( si), where ai specifies the action type: ai = think for generating reasoning text; ai = tool for calling the corresponding tool Tk with query (qi, αi); ai = answer for producing the final answer; and ai = cite for wrapping claims in citations within the final answer. If ai { think , answer , cite }, the output ζi is appended to the context, forming si+1 = si ai, ζi. If ai = tool , the model executes the tool call, receives oi = Tk(qi; αi), and updates the state as si+1 = si ai, ζi, oi. The process continues until aτ = answer , where ζτ contains the final answer. 3 generates multiple rollouts via interacting Figure 3 Training with RLER. Given training instance, the policy LM πθt with the environment. We then invoke another LM to create new rubrics based on those rollouts and the current rubrics in the rubric buffer. We score each generation against those rubrics and use that score to update model weights. We then add the new rubrics to the rubric buffer and filter it to retain only fixed number of rubrics with the highest variance among rollouts."
        },
        {
            "title": "3 RLER: Reinforcement Learning with Evolving Rubrics",
            "content": "We introduce Reinforcement Learning with Evolving Rubrics (RLER), our method for training long-form deep research models using rubrics that are specific to instances, grounded on external knowledge, and evolving with the policy model. This section introduces the preliminaries of RL with rubrics (3.1), details our rubric generation and management approach (3.2), and presents analyses demonstrating its effectiveness (3.3)."
        },
        {
            "title": "3.1 Preliminaries: RL With Rubric Rewards",
            "content": "Given question and its set of corresponding rubrics Rx = {(rx,k, wx,k)}K , where each rx,k is rubric (item) and wx,k is its corresponding weight, we assess the quality of the response with the rubric-based scoring function k=1 S(x, y) = (cid:80)K k=1 wx,k Judge(rx,k, y) k: wx,k>0 wx,k (cid:80) , (1) where, for each rubric rx,k, we use separate judge LM that returns 0, 0.5, or 1 depending on the extent to which rx,k is satisfied by the final answer in y. We compute this rubric score using only the final answer, i.e., regardless of the reasoning and search traces. Note that rubrics are instance-specific, i.e., each question has its own set of rubrics. Rubric weights can be negative to penalize undesirable aspects. During training, our goal is to optimize the expected reward over questions in the training set. In practice, we optimize this objective using the GRPO (Shao et al., 2024b) algorithm; we discuss further details in 4.2. Existing work instantiates the rubric set Rx in two main ways. The first approach is to use general rubrics, where an LM is prompted to score the response using single general rubric shared across all instances (Liu et al., 2023b; Li et al., 2024, 2025a). However, several works have shown that this approach suffers from reward hacking, where the model exploits biases in the judge rather than learning meaningful behaviors (Gunjal et al., 2025; Zeng et al., 2024). The second approach is to use an LM to generate question-specific rubrics, and then (potentially separate) LM to perform checklist-style evaluations based on those rubrics (Gunjal et al., 2025; Viswanathan et al., 2025). We refer to these rubrics as closed-book rubrics since they are generated by closed-book LM; these are therefore constrained by the generating models parametric knowledge and might not cover the necessary knowledge to assess DR outputs. In both cases, the rubrics are static: they do not adapt as the policy explores new evidence or behaviors. Algorithm 1 Reinforcement Learning with Evolving Rubrics (RLER) Require: Dataset D, policy πθ, rollout size G, max active rubrics Kmax, rubric generator Grubric 1: for each prompt do 2: (cid:0)x, Search(x)(cid:1) persist Grubric Generate initial search-based rubrics Generate Ractive 3: 4: for each training step = 1, . . . , do 5: active i=1 πθ(x) persist Rx Rollout with search {yi}G Generate Rnew Ractive Rnew persist Compute rewards with Compute std of the rewards per rubric For Ractive Grubric(x, {yi}G Ractive ; active i=1, Rx); 6: 7: 8: 9: 10: 11: Generate evolving rubrics by contrasting rollouts and update πθ (GRPO) , remove rubrics with 0 std; keep top-Kmax with highest std Manage rubric buffer"
        },
        {
            "title": "3.2 Evolving Rubrics\nDesigning rubrics for long-form deep research tasks is particularly challenging. First, long-form questions are\noften under-specified and admit many plausible ways a response could be good or bad, so a small set of fixed\ncriteria cannot capture all relevant dimensions of quality. Second, DR tasks are highly knowledge-intensive:\nreliable evaluation requires checking claims against a broad, evolving corpus of world knowledge, rather\nthan relying solely on an LM’s parametric knowledge. As a result, closed-book rubrics generated directly by\nan LM can miss critical evidence, fail to distinguish subtle errors, and are vulnerable to reward hacking by\nmodels that exploit judge biases.",
            "content": "We address these challenges by constructing rubrics that co-evolve with the policy model and are grounded on searched knowledge from the internet. Specifically, instead of trying to exhaustively enumerate all possible desiderata, our method generates rubrics tailored to the current policy models behaviors, offering on-policy feedback the model can effectively learn from. Furthermore, the rubrics are generated with retrieval, ensuring it can cover the needed knowledge to assess the generation. detailed illustration on the core RLER training process is in Figure 3. We next describe the components of our evolving-rubric framework: first, how we initialize rubrics, then how we adapt them online during training, manage the rubric buffer, and finally incorporate auxiliary format and citation rewards, following the pseudocode provided in Algorithm 1. Initial search-based rubrics. For each training prompt x, we build customized rubric buffer to store evolving rubrics that are dynamically updated during training. Before training, we initialize the rubric buffer with search-based rubrics. Specifically, for each x, we first perform Search(x) to retrieve relevant context from the internet using the original question. We then concatenate the retrieved documents with the question and feed them into an LM, Grubric, to produce set of initial rubrics that will be persistently used throughout RL training: = {R1, R2, . . . , RKs}, where Ks denotes the number of persistent rubrics. persist i= Ractive i=1 as input to Grubric, obtaining set of evolving rubrics Rnew Evolving rubrics during training. During training, we add new set of evolving rubrics to the active rubric buffer, Ractive , which are used for scoring. In each training step, for every prompt and its corresponding , where denotes the number of rollouts, we concatenate the prompt x, all sampled set of responses {yi}G responses {yi}G (including the search context and final answers), and the existing rubric pool Rx = persist (cid:1). Specifically, we instruct the LM to generate two types of evolving rubrics: (1) positive rubrics, which capture strengths or new, relevant knowledge explored by the current policy but not yet reflected in Rx, and (2) negative rubrics, which summarize common undesirable behaviors, such as reward hacking observed across responses. For example, copying retrieved results verbatim to maximize citation precision, or writing an overly long paragraph to gain rubric points and then appending many irrelevant statements with citations to separately boost citation rewards. In both cases, negative rubrics can capture and suppress such behaviors in time. The detailed prompt for evolving rubric generation is provided in Appendix A.1. = Grubric (cid:0)x, {yi}G i=1, Rx 5 Rubric Type General Rubrics Closed-book Rubrics Initial Rubrics Evolving Rubrics Uses Search Assertive Claims Factuality Frac. 0.22 0.56 0.52 / 0.94 0. 1.00 Table 1 The fraction of assertive and factual rubrics. Both the initial search-based rubrics as well as the evolving rubrics (which continue to use search, as they are generated based on the full rollouts including search traces) have higher proportion of assertive claims compared to closed-book or general rubrics. Figure 4 Effect of negative evolving rubrics. Over training, negative evolving rubrics emerge that penalize undesirable behavior such as responding in Python (right), resulting in reduction in undesirable behaviors over the course of training compared to using static closed-book rubric that does not specify such undesirable behavior (left). Rubric buffer management. Without appropriate management of rubrics, the number of rubrics would grow linearly during training as new evolving rubrics are continuously generated. To maintain compact yet informative set, we developed rubric buffer management strategy that filters, merges, and ranks rubrics based on their discriminative power. After every GRPO rollout, we score all responses {yi}G using the current active rubrics and obtain rubric-level scores. Rubrics with zero variance in their corresponding rewards are removed as they offer no discriminative value. We then compute the standard deviation for each remaining rubric and rank them by the standard deviation in descending order. To limit evaluation cost, we retain only the top Kmax rubrics with the highest standard deviation values. i=1 In addition to evolving rubrics, we introduce three auxiliary rewards Format, search, and citation rewards. format rewards, search, and citation rewardsto explicitly encourage the model to follow the correct formatting instructions, make use of searches, and provide high-quality citations that support all relevant claims. We detail these two auxiliary rewards in Appendix A.3. We combine the auxiliary rewards with the rubric rewards to form the final training reward, assigning small weights to the auxiliary components."
        },
        {
            "title": "3.3 How Do Evolving Rubrics Work?\nIn this section, we show that our initial search-based and evolving rubrics demonstrate desirable proper-\nties, such as being specific and adaptive, enabling the verification criteria to more closely approximate the\nperformance of an ideal rubric set compared to naive rubric generation methods.",
            "content": "Search-based and evolving rubrics make verification criteria more concrete and factual. Table 1 compares the specificity of four rubric types. We define rubric as assertive if it is specific and concrete about what the response should contain (e.g., The response should mention benchmarks and B), and descriptive otherwise (e.g., The response should discuss benchmarks.). Descriptive rubrics are easier to generate since they do not require factual knowledge, but they often fail to assess response quality accurately, as model may score well by superficially mentioning point or even hallucinating facts. We measure the fraction of assertive rubrics and factuality using an LM, with experimental details provided in Appendix B. As shown in Table 1, general rubrics lack specific evaluation criteria, and instance-wise rubrics generated by closed-book LM are relatively vague (only 22% are assertive). In contrast, initial search-based rubrics and evolving search-based rubrics are more concrete, with over 50% of claims being assertive. These advantages come from search-based rubrics being grounded in retrieved information, and from evolving rubrics being generated using search context, which makes them better suited for training. Evolving rubrics adjust the evaluation criteria as the policy model evolves. Static rubrics can fail to capture unexpected behaviors or insights emerging during training. As an illustration, we conducted RL training on single question, Write survey paper about RAG. (details in Appendix B). Unexpectedly, some rollouts contained Python code (e.g., Figure 15 in Appendix B), an artifact of the Qwen model that was also previously 6 reported by Shao et al. (2025); this is undesirable but hard for an initial rubric to anticipate. In contrast, evolving rubrics identify these issues and provide negative feedback about irrelevant code, leading to fewer code-containing responses during training (Figure 4)."
        },
        {
            "title": "4 DR Tulu: Training Open Deep Research Agents with RLER",
            "content": "Building on RLER, we introduce DR Tulu-8B, fully open-source deep research agent. In 4.1, we describe how we perform supervised fine-tuning (SFT) on teacher-generated trajectories to resolve the cold-start problem and teach the model basic planning, tool use, and citation skills. 4.2 further refines DR Tulu with online GRPO and RLER using asynchronous tool calls to improve its long-form reasoning and search strategies. Finally, 4.3, we detail our dr-agent-lib infrastructure, which is designed to support diverse tools within complex DR workflows and to robustly support high-throughput tool calling in RL training."
        },
        {
            "title": "4.1 Supervised Fine-Tuning for Cold Start\nRLER relies on meaningful exploration over tool-augmented trajectories, but a generic base model does not yet\nknow how to plan, invoke tools, or produce citations in the expected format, leading to low-quality rollouts.\nTo make this feasible, we first conduct SFT on trajectories produced by a strong teacher model acting as a\ntool-augmented deep research agent, which gives DR Tulu a reasonable initial search and citation strategy\nbefore online RL. Existing open deep research training data are often built for short-form, constrained tasks,\nso we construct new large-scale SFT trajectories for open-ended queries. We leverage diverse open-source user\nand task prompts, generate trajectories in an end-to-end manner, and apply lightweight rejection sampling to\nfilter them.",
            "content": "Prompt curation. For long-form, naturally occurring information-seeking questions, we derive prompts from publicly available user-assistant interaction data: SearchArena (Miroyan et al., 2025), which contains 24K real-world conversations between users and search-augmented LMs across diverse domains, and OpenScholar (Asai et al., 2024), which provides 55K scientific research-oriented queries collected from deep research assistant demo. Because real-world queries vary markedly in quality (Cao et al., 2025), we apply prompt-filtering stage in which an LM rates each prompt on 15 scale (higher is better). For SFT, we mix in moderate amount of short-form, verifiable QA so the model learns to adapt its style rather than overfit to single task: we sample from HotpotQA (Yang et al., 2018), TaskCraft (Shi et al., 2025), WebWalker-Silver (Wu et al., 2025a), and MegaScience (Fan et al., 2025), and we generate additional challenging synthetic prompts inspired by PopQA (Mallen et al., 2023). Further curation details appear in the Appendix C.1. Trajectory generation and rejection sampling. Given each curated prompt, we generate full trajectory (model thinking traces, tool calls, tool outputs, and the final response) in an end-to-end manner. We provide GPT-5 with detailed system prompt that defines the deep research workflow and exposes general web search tool, paper search tool, and web browsing tool, and ask it to produce the entire trajectory. Because GPT-5 does not expose its native internal reasoning, we instruct it to generate explicit mock thinking tokens before each tool call or answer tokens. We then apply two lightweight rejection-sampling filters to ensure that the trajectories strictly satisfy our requirements: (1) for all prompts, we verify that trajectories follow the expected tool-calling and answer formats; and (2) for short-form prompts, we discard trajectories whose final answer does not match the gold answer, following prior work (Jin et al., 2025; Li et al., 2025a). As result, we curated 16K SFT data points, including both short and long-form tasks. Appendix Table 8 shows statistics of our SFT data."
        },
        {
            "title": "4.2 Online RL with Asynchronous Tool Calls\nWe then further train DR Tulu (SFT) using RLER to allow it to explore and improve both its tool-use and answer\ncapabilities, aided by the evolving rubrics described above. We use a customized variant of GRPO (Shao\net al., 2024b) with RLER, in which we iteratively generate agentic rollouts using real tool calls, and then score\nthe model’s final answer against the evolving rubrics. This allows the model to effectively explore different",
            "content": "7 search strategies in an environment with web-enabled tools and learn how to further improve its final answers through feedback provided by the evolving rubrics. Prompt curation. We focus exclusively on long-form questions for RL training. Following the same LMbased filtering procedure used in long-form SFT data curation, we collect approximately 5K new prompts from SearchArena (Miroyan et al., 2025) and OpenScholar (Asai et al., 2024). Additionally, we sample 4K prompts from RaR (Gunjal et al., 2025) to enhance further data diversity.1 We note that, although we attempted to collect prompts from diverse sources, the questions we gathered are often still out-of-distribution (OOD) relative to those in downstream benchmarks. RL training. We train our models with GRPO (Shao et al., 2024a), building on the Open-Instruct implementation (Lambert et al., 2025). We use the basic GRPO loss, albeit using token-level loss aggregation like DAPO (Yu et al., 2025). We apply two further optimizations: we use sample packing to pack multiple rollouts into single training passes with minimal padding, and use 1-step asynchronous training (Noukhovitch et al., 2024), which means we perform generation and training steps at the same time (training on rollouts from policy one step behind our current policy), reducing training time. We additionally mask out tool output tokens from the loss, following prior work (Jin et al., 2025). We find using small KL penalty (0.001) useful for stabilizing training. We provide further hyperparameters in Appendix D.2. After generating rollouts and computing rewards, we perform the rubric buffer management steps described in 3.2 before sending the completed samples and rewards to the trainer. We also turned off the citation reward after 650 training steps, as we found it converged and did not further add to performance, whilst dramatically slowing down RL training (due to the large number of API calls required). Asynchronous tool calling. When performing tool calls during RL training, we use an asynchronous tool call setup similar to Jiang et al. (2025), wherein tool requests are sent the second given rollout triggers them, as opposed to waiting for the full batch to finish generating before sending tool calls. Once tool call is sent, we place that given generation request to sleep, allowing the inference engine to potentially continue to work on generating other responses while waiting for the tool response. This results in the generation and tool calling being overlapped wherever possible. Our tool calls are mediated by dr-agent-lib, our custom agent infrastructure, which allows us to tightly control the number of concurrent calls made to given APIs and to cache repeated queries to increase efficiency and avoid rate limits."
        },
        {
            "title": "4.3 DR Tulu Agent Infrastructure: dr-agent-lib\nDeveloping deep research (DR) agents introduces infrastructure challenges because the LM must orchestrate\ndiverse search and browsing APIs during generation. DR performance is highly sensitive to the quality of these\ntools, making it crucial to have an extensible infrastructure that allows researchers to easily customize and\nintegrate different tools and APIs. The tool backend should also be scalable and robust to handle high-volume\nconcurrent tool calls during RL training. Ideally, it should also be easy to use and support fast iteration of\nprompts and tool setups during the SFT stage.",
            "content": "To address these challenges, we developed an agent library called dr-agent-lib with three key design features. We implement unified MCP-based tool backend with collection of local and API-based web search and browsing tools (see full list in Table 11) based on the Model Context Protocol (MCP). The backend is optimized for high concurrency, including global cache for repeated tool calls and an asynchronous process lock to manage concurrent requests while respecting rate limits. It also features lightweight and flexible prompt layer, supporting the composition of different search workflows with fine-grained control over prompts and tool-calling configurations. For training, we implement an auto-search workflow (detailed in Appendix E.1) with the following tools: google_search (query top web snippets), web_browse (URL crawled page text), and paper_search (query top relevant paragraphs from open-access papers). 1For RaR prompts, we use the original rubrics provided by the dataset as the initial set of persistent rubrics, instead of generating new search-based rubrics before training. We still generate evolving rubrics for these prompts."
        },
        {
            "title": "5.1 Experimental Settings\nEvaluations. We evaluated deep research agents on four long-form, open-ended benchmarks spanning\ngeneral-domain, scientific, and medical applications: HealthBench (Arora et al., 2025), ResearchQA (Yifei\net al., 2025), AstaBench-ScholarQA-CS2 (SQAv2; Asai et al. 2024; Bragg et al. 2025), and DeepResearchBench\n(DRB; Du et al. 2025). HealthBench targets healthcare deep research; SQAv2 and ResearchQA assess synthesis\nover up-to-date scientific literature; DRB covers general-domain deep research questions; HealthBench consists\nof questions posed by individual users for healthcare professionals. For all datasets, responses are expected\nto be in long form and are evaluated using human-written or human-verified rubric items, following the\nofficial evaluation protocols. SQAv2 and DRB provide detailed performance breakdowns across diverse\naspects of answers, including relevance to the question and instruction-following. For SQAv2, we additionally\nevaluate fine-grained citation metrics such as precision and recall following official evaluations: citation\nprecision checks whether the cited sources actually support statements with citations, and citation recall\nchecks whether valid citations back all citation-worthy statements. We report both aggregated overall scores\nand per-aspect scores. We also conduct evaluations of DR Tulu on short-form QA in Analysis.",
            "content": "Baselines. We compare against several categories of deep research systems, mirroring the groupings in Table 2. (1) Open deep research models. We compared against four popular open deep research baselines, including ASearcher-7B (Gao et al., 2025), WebThinker 32B (Li et al., 2025a), Search-R1-7B (Jin et al., 2025), and the concurrent work WebExplorer-8B (Liu et al., 2025) and Tongyi Deep Research-30B (Team et al., 2025). Notably, none of these methods were evaluated on realistic long-form benchmarks, as their training solely focuses on short-form QA tasks.2 For long-form tasks, we provide the official evaluation prompts to each model and require full report-style response. (2) Fixed-pipeline deep research. We further evaluated fixed pipeline deep research systems, including WebThinker (report mode) and Ai2 ScholarQA (Singh et al., 2025). Those baselines combine LMs with inference-time only pipelines to generate long-form reports. We ran their official codes with default or recommended configurations. (3) Closed deep research: we evaluated OpenAI Deep Research, Perplexity Sonar (reasoning), Perplexity Deep Research, and Claude-Sonnet Search. We also evaluate the latest Gemini3 Pro + Search (released on November 18), which is reported in its release to achieve superior performance compared to GPT-5 and other proprietary LMs.3 Note that existing open deep research models do not provide citations by default, and proprietary deep research systems typically only link to URLs. In contrast, DR Tulu supplies snippet-level citations (short supporting passages that directly substantiate each claim). We believe such fine-grained citations make human verification substantially easier and help ensure the correctness of the responses (Liu et al., 2023a). Lastly, as baselines, we also evaluated Qwen3-8B and QwQ-32B with naive RAG inference and separately with our inference pipeline using our dr-agent-lib. We discuss the system prompts and evaluation details in Appendix E.2. Training details. We initialize our checkpoint with Qwen3-8B (Yang et al., 2025). For SFT, we run training on one H100 node (8 GPUs) and trained for 5 epochs; the hyperparameters used for SFT are described in Appendix D.1. Overall, finetuning our final SFT model took 136 GPU hours. For RL training, we use the hyperparameters described in Appendix D.2. All training runs were run on 2 H100 nodes (16 GPUs) unless otherwise stated. For rubric and citation scoring, we use GPT-4.1-mini (gpt-4.1-mini-2025-04-14) as an LM judge; we use GPT-4.1 as an rubric generator for both initial and evolving rubrics. For our final training run, we ran for 25 days, using roughly 9700 GPU hours to take 1900 training steps, or 6.8 epochs over our training data. We note that the RL run did not seem to saturate even with this long training time, and we may release future checkpoints with improved performance. We found increasing compute did not improve RL training speed, due to being limited by API rate-limits during rollouts. We show the full RL training curves for our final training run in Appendix F.1. We used Crawl4AI,4, free open-source tool, for browsing during training time to save costs. 2WebThinker includes synthetic report-generation dataset (GLAVE), but evaluation relies solely on LLM-judge ten-scale evaluations (e.g., comprehensiveness) without explicit citation or evidence checks. 3We only consider proprietary systems that provide an API for their deep research systems at the time of writing, which excludes Gemini Deep Research and Grok Deep Research. 4https://github.com/unclecode/crawl4ai 9 SQAv2 HealthBench ResearchQA DRB Average Closed Deep Research Claude-Sonnet Search Perplexity-Sonar (High) Perplexity Deep Research Gemini Deep Research Gemini 3 Pro + Search GPT-5 + Our Search GPT-5 + Search OpenAI Deep Research Naive RAG Qwen3-8B QwQ-32B Open Deep Research Models Search-R1-7B ASearcher-Web-7B WebExplorer-8B WebThinker-32B-DPO Tongyi DeepResearch-30B-A3B Fixed Pipeline Deep Research WebThinker QwQ-32B (report) WebThinker-32B-DPO (report) Ai2 ScholarQA - Claude Sonnet Open Deep Research (Ours) Qwen3-8B + Our Search DR Tulu-8B (SFT) DR Tulu-8B (RL) 67.3 69.8 61.1 74.8 79.6 40.4 41.9 22.2 26.9 42.5 32.9 46. 45.2 46.7 87.7 57.2 72.3 86.8 38.0 31.1 59.5 53.8 16.5 24.5 -0.1 -13.0 33.7 11.1 46.2 36.5 39.4 32.0 5.9 38.1 50.2 64.3 69.1 75.3 68.5 74.3 62.8 78.2 79.2 56.1 60.9 27.9 19.4 64.8 48.6 66.7 72.8 74.2 75.0 46.3 68.5 74. 34.5 40.7 42.3 48.8 46.3 50.3 50.7 46.9 33.3 40.3 9.5 7.8 36.7 23.3 40.6 37.9 40.6 36.1 18.2 39.0 43.4 57.0 51.3 65.8 64. 36.5 41.9 14.9 10.3 44.4 28.9 50.0 48.1 50.2 57.7 31.9 53.9 63.7 Table 2 Overall results. DR Tulu outperforms all open deep research models, and is competitive with proprietary systems. Rows with gray background indicate models that use closed models as backbone LMs. Bold indicates the best performance among open models. * denotes scores reported by the original benchmark authors. Except for GPT5 + our tool, we reuse the existing leaderboard results rather than rerunning the evaluations, which would cost few hundred USD per task; we leave entries as when the original benchmarks do not report the corresponding metric. denotes that the evaluation was run on 100-sample subset because the method is expensive. For open models, indicates that the training code is open-sourced, and indicates that the training data is open-sourced. None of the existing open deep research models output citations, so their citation scores on SQAv2 are 0. HealthBench scores can be negative, as HealthBench includes negative rubrics that indicate harmful responses. Inference details. We use single inference pipeline equipped with three tools by default: google_search, web_browse, and paper_search for all long-form and short-form tasks, without modifying inference pipelines for each task. Following prior work, we use the Serper Search API5 for google_search (Li et al., 2025a) and Jina browsing6 (Gao et al., 2025; Liu et al., 2025) for web_browse, instead of the cheaper Crawl4AI browsing that we used for training. We use Semantic Scholar full-text search API7 for paper_search, which returns paragraphs from relevant papers. We cap the number of tool calls at 10 per rollout to balance efficiency and performance, and we retrieve the top 10 snippets for both google_search and paper_search. For web_browse, whose outputs can be very long, we use Qwen3-8B to summarize the browsed content; we truncate the webpage during training instead of using reader to improve efficiency."
        },
        {
            "title": "5.2 Main Results\nWe report overall results across four long-form datasets in Table 2. In addition, Table 3 provides a fine-grained\nbreakdown of the two long-form datasets, SQAv2 and DRB.",
            "content": "5https://serper.dev/ 6https://jina.ai/ 7https://api.semanticscholar.org/api-docs 10 AstaBench-ScholarQA-CS2 (SQAv2) Rubric Answer Cite-P Cite-R DeepResearchBench (DRB) Comp Depth Instruction Readability Closed Deep Research Claude-Sonnet Search Perplexity Sonar Perplexity DR Gemini Deep Research Gemini3 Pro + Search GPT-5 + Search GPT-5 + Our Search OpenAI DR Naive RAG Qwen3-8B QwQ-32B Open Deep Research Search-R1-7B ASearcher-7B WebExplorer-8B WebThinker-32B-DPO Tongyi DeepResearch-30B-A3B Fixed Pipeline Deep Research WebThinker QwQ-32B (report) WebThinker-32B-DPO (report) Ai2 ScholarQA - Claude Sonnet Open Deep Research (Ours) Qwen3-8B + Our Search DR Tulu-8B (SFT) DR Tulu-8B (RL) - - 91.6 - 83.1 92.3 74.9 91.5 69.2 77.5 9.7 13.7 78.6 36.7 89.5 86.4 91.2 88. 42.8 81.4 89.6 - - 92.7 - 98.3 93.8 93.2 95.6 92.3 90.3 79.0 94.0 91.4 94.9 96.4 94.3 95.5 89.1 92.1 91.0 95. - - 47.3 - 68.5 67.8 42.5 77.4 - - - - - - - - - 92.4 53.7 65.3 88.6 - - 37.6 - 29.4 45.6 33.7 43. - - - - - - - - - 81.2 40.3 51.6 73.7 39.0 37.4 40.7 48.5 43.4 49.7 26.7 46.8 29.4 38. 5.2 5.1 33.7 19.7 39.1 36.2 39.4 35.1 14.3 36.3 41.7 37.7 36.1 39.3 48.5 44.9 51.5 21.3 45.2 27.0 34.8 2.1 1.7 28.5 12.3 34. 32.6 35.4 32.0 8.7 35.3 41.8 45.8 45.7 46.4 49.2 49.8 51.6 41.0 49.2 40.2 47.0 18.6 15.2 45.7 36.8 46.8 43.2 46.0 40. 29.5 45.5 48.2 41.5 44.7 44.3 49.4 49.0 48.5 29.4 47.1 41.1 44.6 16.8 11.8 42.2 26.3 45.4 42.9 43.5 38.9 24.4 39.5 41. Table 3 Performance breakdown for Asta-ScholarQA-CS2 and DeepResearchBench. Open deep research models and naive RAG baselines do not provide citations, indicated as - in citation columns. Rows with gray background indicate models that use closed models as backbone LMs. Bold indicates the best results among the baselines that do not use propriety models. DR Tulu-8B outperforms all open deep research models on long-form tasks. Across all four open-ended, long-form evaluation benchmarks, DR Tulu-8B consistently outperforms existing open deep research models by 13.7-53.4 points on average, including much larger 30B-scale models. In particular, open models designed and trained for constrained, short-form tasks, such as Search-R1 or ASearcher, perform poorly on realistic long-form, report-length generation tasks, yielding low overall scores. The WebThinker models, which are larger (32B), still underperform on long-form generation when used with their default inference pipeline. While the concurrent WebExplorer model as well as the previous state-of-the-art Tongyi Deep Research outperform those prior open deep research models, both the SFT and RL versions of DR Tulu-8B outperform both of them. Notably, none of the open baselines produce citations, which results in low overall scores (around 40) on SQAv2, where citation quality is core evaluation criterion. DR Tulu-8B outperforms open fixed pipeline deep research heavily engineered for report generation tasks. Open deep research systems with fixed pipelines rely heavily on human-engineered inference workflows to generate long-form reports. In contrast, DR Tulu-8B does not use separate report-generation pipeline and autonomously decides the length of answers based on given task prompts. WebThinker (report mode) performs competitively on long-form benchmarks, largely due to its pipeline that iteratively drafts and edits answers after running its default search flow with the base model. As shown in Table 4, this dedicated report generation pipeline dramatically increases response length from around 90 tokens/answer to over 4000 tokens/answer on average. However, despite producing substantially longer responses and being 4 larger in terms of model size, WebThinker-32B with the customized report mode still lags behind DR Tulu-8B on all tasks. Ai2 ScholarQA, which is specifically designed for scientific literature synthesis and uses Claude Sonnet as the backbone LM, performs competitively with DR Tulu-8B on SQAv2 and surpasses it by 0.7 points on 11 Answer Length Citations Tool Calls Cost / Query* GPT-5+ Search OpenAI Deep Research Gemini 3 Pro + Search Ai2 ScholarQA - Claude Sonnet WebExplorer-8B WebThinker-32B WebThinker-32B (report) Tongyi Deep Research-30B-A3B DR Tulu-8B (RL) 2358.7 6445.1 1310.9 2090.5 1250.4 92.2 4416.7 2138.9 1889.2 28.1 79.6 8.6 61.2 - - - - 35.8 - - 8.5 1.0 9.1 6.9 8.2 23.0 4.3 0.29 1.8 0.13 1.3 0.019 0.0037 0.015 0.032 0.0019 Table 4 Comparison of model usage statistics on SQAv2. We report answer lengths, tool usage, and citation counts across systems. - denotes this information was either not available or it was not applicable. The cost per query is estimated based on model inference on ScholarQA-CS2, following Bragg et al. (2025). More details of cost estimations are available in Appendix F.2. ResearchQA. However, this fixed pipeline struggles on DeepResearchBench and HealthBench, as it is tailored to literature synthesis rather than general deep research. As such, despite using much smaller underlying LM and single inference pipeline, DR Tulu still achieves the best overall average performance. We also found that those fixed deep research systems cannot perform simple short-form QA tasks, as even for simple factoid questions (e.g., those in SimpleQA), they attempt to generate long-form report, generalizing poorly to these types of queries (see appendix E.2 for an example). In contrast, DR Tulu can effectively answer such short-form questions, which we more comprehensively evaluate in 6.1. DR Tulu-8B matches or outperforms proprietary deep research systems. DR Tulu-8B outperforms all closed models on SQAv2. Despite performing similarly to DR Tulu on the 4 long-form benchmarks, OpenAI Deep Research generates answers that are roughly three times longer and contain about twice as many citations, as seen in Table 4. DR Tulu additionally outperforms Claude Sonnet Search and Perplexity Sonar (high-reasoning mode) and Perplexity Deep Research. We also find that, interestingly, GPT-5 and Gemini3 Pro with their internal search systems can outperform their respective deep research variants on some datasets. This is likely due to the stronger base LMs used in these setups: the OpenAI Deep Research APIs currently only expose o3/o4-based inference, and the Gemini Deep Research results on the original dataset leaderboard are based on Gemini2.5era systems. This highlights the importance of underlying model capability in addition to strong deep research pipeline. Nevertheless, DR Tulu, built on an 8B LM, performs competitively with, and in some cases even outperforms, these latest LM-backed systems. DR Tulu-8B is significantly cheaper than other proprietary and open deep research systems. Table 4 also highlights the cost advantage of DR Tulu (details of cost estimations are in Appendix F.2). Proprietary deep research systems are significantly more expensive; OpenAI Deep Research costs USD 1.80 per query on SQAv2, and Ai2 ScholarQA (using Claude Sonnet) costs USD 1.30 per query. In comparison, DR Tulu costs approximately USD 0.00008 per query, assuming the user has already paid the hardware and hosting costs (only including the cost of API calls in the query cost).8 Including the LM inference cost when running DR Tulu via OpenRouter with Qwen3-8B pricing,9 the total cost is approximately USD 0.0018 per query. This further demonstrates the strong cost-effectiveness of smaller, open-LM deep research models compared to proprietary systems, in addition to non-monetary benefits such as preserving sensitive data locally. Compared to other open deep research models, DR Tulu remains significantly cheaper despite its superior performance: Tongyi DR and Webthinker cost around USD 0.03 and USD 0.003 (for WebThinker in report mode, the cost is USD 0.015 per query) per query, respectively. This is because, beyond using smaller LM, DR Tulu can flexibly decide which tools to invoke at inference time and, on SQAv2, primarily relies on paper search, which is free. For open models, large fraction of the cost comes from tool-call APIs; DR Tulu is more cost-effective 8We find that DR Tulu calls tools about 4.3 times on average per SQAv2 query. It calls paper_search 96.8% of the time, google_search 2.4% of the time, and web_browse 0.9% of the time. S2 API is free to use. We estimate Serper search as USD 0.00075 per query and Jina browse as USD 0.00005 per query. As such, the average query costs USD 0.000079. Even if the model calls the most expensive API (Serper search) the maximum number of times allowed during evaluation (10 times), this gives maximum cost of USD 0.0075 per query. 9https://openrouter.ai/qwen/qwen3-8b Figure 5 Researching pathogenic gene variants in GeneticDiseasesQA. Left: Overall, DR Tulu-8B shows strong gains over its base model and competitive performance with closed DR models GeneticDiseasesQA, benchmark that we constructed to reflect real-world medical genetics task that is out-of-domain to our training data. We were unable to compare to other open deep research models and naive RAG baselines as they do not produce evidence citations, which are critical to GeneticDiseasesQA. Results for DR Tulu-8B (RL) and Qwen3-8B + Our Search are reported as the average of 10 trials. The remaining models are reported as the average of 3 trials, given the high costs and inference times for proprietary deep research systems. Right: Example question and response snippets generated by DR Tulu-8B. because it learns to call the right tools with only the necessary frequency, rather than simply calling tools as many times as possible. Even on DRB, where DR Tulu issues more web search and browsing calls, it still remains about 10 times cheaper than Tongyi Deep Research. Overall, this suggests that using multiple complementary search tools and allowing the model to adaptively select among them can improve both prediction quality and cost efficiency. RLER improves deep research quality across diverse aspects. We break down the performance of models on two long-form datasets, SQAv2 and DeepResearchBench in Table 3. Comparing DR Tulu (SFT) and DR Tulu (RL), we observe consistent gains from RLER across multiple aspects, including rubric coverage (+8.2 points), answer precision (+4.4), comprehensiveness (+5.4), and depth of response (+6.5). RLER also yields large improvements in citation precision and recall on SQAv2 (+23.3 and +22.1 points, respectively). These results highlight that RLER can effectively improve deep research responses along both content and attribution dimensions."
        },
        {
            "title": "5.3 Application: Researching Pathogenic Gene Variants\nWe next applied DR Tulu to a new out-of-domain deep research task motivated by a real-world challenge in\nmedical genetics: investigating disease-causing gene variants. In clinical settings, accurately identifying and\ninterpreting variants on patient genetic tests is essential for diagnosing genetic disorders and recommending\npersonalized treatment strategies (Cheerie et al., 2025). Although whole genome sequencing technologies\nhave improved the diagnostic yield of rare genetic disease, aggregating sparse information across databases,\nresearch papers, and case reports is still a key bottleneck in variant interpretation (Mastrianni et al., 2025).\nThis combination of information synthesis and biological reasoning, together with the need for evidence\nattribution and interpretability in tools for medical decision making, makes this task a natural fit for automated\ndeep research systems.",
            "content": "GeneticDiseasesQA. We create new evaluation dataset, GeneticDiseasesQA, consisting of 47 questions derived from expert-curated information about 24 disease-causing genetic variants. Question topics were centered around finding information that genetics experts use to assess the eligibility of patients with genetic variants to various gene therapy strategies (Cheerie et al., 2025), which involve reasoning about the molecular properties, disease-causing mechanisms, therapeutic approaches, etc. We show an example in Figure 5. For each question, we prompt deep research systems to generate long-form report that both answers the question and provides supporting evidence for each claim. We define the following criteria for evaluation, again using GPT-4.1 as judge to score each category: Final Answer indicates whether the expert-annotated fact(s) were mentioned in the response. Evidence Quality indicates if the type of evidence requested in the query (e.g. 13 SimpleQA 2Wiki WebWalker Avg. Naive RAG Qwen3-8B QwQ-32B Open Deep Research (Ours) Qwen3-8B + Our Search DR Tulu-8B (SFT) DR Tulu-8B (RL) 52.6 57. 70.5 75.5 80.1 18.9 34.2 44.0 66.5 68.0 8.8 10.1 27.9 31.9 39.1 26.8 33. 47.5 58.0 62.4 Table 5 Short-form results. We report short-form performance for our SFT and RL variants to analyze how each training stage affects short-form behavior. All scores are computed from top-1 predictions and evaluated with GPT-4.1 as the LLM judge under unified evaluation pipeline, and on earlier DR Tulu (RL) checkpoint evaluated at 1000 training steps. functional assays in patient-derived cells) is present within the cited statements. Evidence Synthesis indicates whether or not there was at least one statement describing the relationship between multiple sources. Evidence Support reflects the proportion of cited claims that are fully supported by the original text of the cited source. More details are in Appendix E.4. Results. We compare DR Tulu-8B (RL) with Qwen3-8B with our search tool, Ai2 ScholarQA, Gemini 3 Pro + Gemini Search, GPT-5 + OpenAI Search, and Open AI Deep Research using o4-mini.10 We do not include the open deep research models or fixed pipeline deep research models that do not provide citations, as it is key aspect of the evaluation. Figure 5 reports results on the genetic variant analysis task. DR Tulu-8B consistently outperforms the base Qwen3-8B model by large margin across all categories, demonstrating the effectiveness of our training recipe. Furthermore, DR Tulu-8B surpasses Ai2 ScholarQA Agent, which runs on top of Claude Sonnet and is specifically designed for scientific literature synthesis, and Gemini 3 Pro on overall score. DR Tulu-8B lags behind the best proprietary systems that use state-of-the-art LMs such as GPT-5 and Gemini 3 Pro on final-answer correctness, due to differences in underlying model capacity, though it does similarly on evidence support, and it outperforms Gemini 3 Pro on evidence support and evidence synthesis in particular. This highlights its strength in precisely synthesizing reliable sources, which is especially important for expert-domain tasks where users are more likely to scrutinize citations than blindly trust final responses. Altogether, these results show that DR Tulu-8B generalizes to real-world DR tasks that are unseen and out-of-domain to the training data."
        },
        {
            "title": "6 Analysis",
            "content": "We conduct set of analyses to understand DR Tulus effectiveness. Unless otherwise specified, this section uses the DR Tulu SFT checkpoint and an early DR Tulu RL checkpoint after 1k training steps."
        },
        {
            "title": "6.1 Evaluation on Short-form QA Tasks\nIn this paper, we focus on long-form, open-ended deep research, and our RLER training is applied exclusively\nto long-form tasks. However, our SFT mixture deliberately includes short-form, verifiable QA tasks that still\nrequire search, so the model learns to handle both concise answers and multi-paragraph long answers. We\ntherefore examine how well our SFT and RL models perform on standard short-form QA benchmarks to\ndetermine how well our approach generalizes to varied query types.",
            "content": "Datasets. Our short-form evaluation suite comprises SimpleQA (Wei et al., 2024), WebWalkerQA (Wu et al., 2025a), and 2Wiki (Ho et al., 2020). For short-form QA, we follow prior work (Li et al., 2025a; Wei et al., 2024) and use an LLM judge to determine answer correctness against the annotated gold, reporting 10Note that Evidence Support is computed using supporting snippets. For OpenAI and Gemini models that do not return snippets, we instead retrieve the full webpage content via Jina browsing using the provided URLs, which differs from how we compute Evidence Support for DR Tulu, Qwen3-8B, and Ai2 ScholarQA Agent. Nevertheless, all systems perform reasonably well on Evidence Support. 14 Figure 6 Ablation of SFT training data. We ablate SFT training data in terms of the mixture of data and scale of training data. We train models with varying sizes of SFT data (5%, 10%, 100%; 0% indicates the Qwen3-8B + dr-agent-lib results) as well as two SFT subsets, long-form data only (LF only) and short-form data only (SF only). Figure 7 Performance of different starting point models throughout RL training. We vary the SFT model used at the start of RL training, and then train using the same data and hyperparameters. RL gives smaller improvements to the stronger the starting point model. Further RL training further improves results. Note that the x-axis is not uniform in the gray area. judge-based accuracy. We evaluate performance with Pass@1. We randomly selected 1000 questions each from SimpleQA and 2Wiki for efficient evaluation. Results. Table 5 reports the short-form QA results. We find that DR Tulu performs competitively on short-form, verifiable QA benchmarks. Our SFT stage greatly improves over the Qwen3-8B + Our Search (dr-agent-lib) baseline, demonstrating the effectiveness of our SFT data for short-form QA. Interestingly, although the RL stage uses only long-form prompts and explicitly optimizes long-form generation quality, DR Tulu (RL) achieves further gains across datasets, improving the overall average by 4.4 points. This suggests that our RL recipe is effective not only for long-form deep research tasks, but also generalizes well to short-form QA."
        },
        {
            "title": "6.2 Analysis on Training\nIncluding both long-form and short-form SFT data is helpful. Figure 6 reports an ablation of the SFT\ntraining mixture. We analyze (i) removing the long-form or short-form subsets entirely, and (ii) subsampling\nthe full corpus uniformly at random. Many existing open deep research models are trained only on short-form\nQA, assuming that short-form supervision will transfer to long-form synthesis, while other systems are\noptimized purely for long-form reporting and struggle to produce concise answers. Our ablations show that\nneither extreme is ideal: relying only on short-form or only on long-form data is suboptimal. Removing\nlong-form data substantially degrades performance on all four long-form datasets. Conversely, removing\nshort-form data leaves HealthBench, ResearchQA, and DRB largely unchanged, but noticeably reduces\nperformance on 2Wiki and SQAv2. This suggests that short-form supervision alone does not automatically",
            "content": "15 SQAv2 HealthBench ResearchQA DRB Avg. Qwen3 8B + SFT v0.1 + RL w/ General rubrics (500 steps) + RL w/ Closed-book rubrics (500 steps) + RL w/ Initial search-based rubrics (500 steps) Qwen3 8B + SFT v0.2 + RL w/ Initial search-based rubrics (650 steps) + RL w/ Evolving rubrics (650 steps) 73.4 80.6 (+7.2) 83.2 (+9.8) 82.8 (+9.4) 76.1 83.2 (+7.1) 84.9 (+8.8) 37.5 36.0 (-1.5) 34.8 (-2.7) 37.9 (+0.4) 34.3 39.3 (+5.0) 38.9 (+4.6) 69.6 65.0 (-4.6) 65.0 (-4.6) 66.9 (-2.7) 67.8 67.8 (+0.0) 68.7 (+0.9) 39.4 34.1 (-5.3) 37.6 (-1.8) 39.3 (-0.1) 38.5 39.0 (+0.5) 40.2 (+1.7) 55.0 53.9 (-1.1) 55.2 (+0.2) 56.7 (+1.7) 54.2 57.3 (+3.1) 58.2 (+4.0) Table 6 Ablation on rubrics. We ablate the rubrics used during RL training, and the effect of adding evolving rubrics. We find that using simple rubrics can actually hurt performance, while search-based rubrics perform best. Further adding evolving rubrics gives one point improvement on average. For expensive training costs, we stopped this ablation after 650 steps and switched to training with evolving rubrics for longer. SFT v0.1 and v0.2 refer to different intermediate SFT mixtures developed during the project, which used similar mixtures to our final SFT checkpoint. yield strong long-form, open-ended, deep research performance. Although our primary focus is long-form deep research, we retain modest short-form component so that the model remains more general-purpose and handles short-form questions well, without sacrificing long-form performance. SFT scaling: saturated gains for long-form, continued gains for short-form. All tasks show clear gains as we initially increase the amount of SFT data. On 2Wiki, performance continues to improve up to 100% of the data, suggesting that short-form tasks may benefit more directly from scaling SFT data. On long-form tasks, we already observe substantial gains with just 5% of the training data, indicating that even relatively small amount of high-quality long-form trajectory data is highly beneficial. However, improvements saturate after 50%, highlighting that, unlike short-form QA, further boosting deep research agent performance on open-ended tasks is challenging if we only scale SFT data. While we observed slight drop in SQAv2 from using the full SFT dataset, we found this was largely driven by citation scores dropping, while ingredient and answer-based metrics remained similar.11 As such, we used the full-size SFT dataset to maintain strong short-form QA abilities, and as we found, RL training was able to significantly recover SQAv2 citation performance. RL benefits from stronger SFT models and longer training. We ablate the effect of using different SFT cold start datasets on RL in Figure 7, tracing performance up to 600 training steps. Beginning RL directly from Qwen3 (no SFT cold start) dramatically improves scores over Qwen3-8B with no training, but still underperforms using even small amount of high-quality SFT data (5% of our full mixture) as cold-start data for the RL training. Using larger amount of SFT data (i.e., our full SFT mixture) further improves performance. We additionally explore augmenting the SFT data with an extra on-policy SFT stage. Specifically, we run our trained model on randomly sampled prompts, apply rejection sampling to discard trajectories that do not achieve high scores on search-based rubric verification and citation verification (details in Appendix C.3), and then use the remaining trajectories for further SFT. While this slightly boosts SFT model performance, we find it ultimately weakens performance later on during RL training, underperforming using our regular SFT mixture on Healthbench and SQAv2. We also found that extended RL training was crucial to performance: in some cases, evaluations that initially seemed flat (e.g., DRB) improved with extended RL training up to 1000 and even 1900 steps. We also note that we found that higher train reward (i.e., reward during RL training) did not necessarily match with higher downstream reward, see Appendix F.5 for details. Finally, we also find that our training is robust to tool errors, with the model improving in performance even after training for an extended time with tool consistently erroring see Appendix F.1 for details and the full RL training curves. Search-based rubrics outperform closed-book rubrics. We ablate the effect of using different static rubric setups (i.e., without adding evolving rubrics) during RL training in Table 6. We run RL training for 500 steps 11We hypothesize the citation score drop is due to the presence of model calls not supported by our pipeline in the SFT data. These calls take the form of XML tags that the citation scorer picks up and penalizes (e.g. <Model name=Anthropic version=claude-3-7-sonnet-20250219>). These comprised small portion of our data, and as such are not picked up by smaller SFT subsets, but are picked up when training on the full dataset. We will further investigate this drop in future work. 16 Figure 8 Distribution of tool calls for SQAv2 (science), HealthBench (healthcare), DeepResearchBench (general domain) and SimpleQA (factoid, short-form QA). DR Tulu can adaptively choose effective tools for different tasks, relying more on paper_search for scientific questions (SQAv2), and more on google_search for general-domain questions (SimpleQA). Figure 9 Distribution of domains among web search results for SQAv2 (science), HealthBench (healthcare), DeepResearchBench (general domain) and SimpleQA (factoid, short-form QA). We show top domains returned by the google_search tool. Calculations are based on 100 samples from each task. These top domains match the evaluation domain; e.g., when evaluating on Healthbench, DR Tulu searches more for medical domain websites. on top of an intermediate SFT checkpoint using four different rubric setups: (1) general rubrics, in which we use simple prompt and LM judge to score model outputs (see Appendix D.3 for prompt); (2) closed-book rubrics, which are generated without access to any search information; (3) search-based rubrics, which are generated with knowledge from an initial search (See Section 3 for details). For these runs, we only used training samples from OpenScholar. We find that using search-based rubrics performs best overall, while using single general rubric for all samples actually results in lower performance than the SFT starting point. Evolving rubrics improve over initial rubrics alone. We additionally ablate using evolving rubrics on top of search-based rubrics in Table 6, training for 650 steps (due to compute and budget limitations) over an intermediate SFT checkpoint. We find that using evolving rubrics during training can provide boost over search rubrics alone in all long-form evaluations except Healthbench. We expect that this gap would further widen with longer training, as evolving rubrics further capture new knowledge explored by the model. 17 Model Size Long-form Multi-Search Citations Train. Code Open-Source Eval Code Train. Data Model Ckpt 7B 32B 8B 7,14,32B 8B 30B Search-R1 WebThinker WebExplorer ASearcher SFR DR Tongyi DR Ai2 ScholarQA WebWeaver SFR EDR 8B DR Tulu Table 7 Comparison with existing deep research systems. We compare our method with existing open deep research models, namely Search-R1 (Jin et al., 2025), WebThinker (Li et al., 2025a), WebExplorer (Liu et al., 2025), SFR-DeepResearch (SFR-DR; Nguyen et al. 2025), Tongyi Deep Research (Tongyi DR; Team et al. 2025), Ai2 ScholarQA (Singh et al., 2025), SFT-Enterprise Deep Research (SFR-EDR; Prabhakar et al. 2025) and WebWeaver (Li et al., 2025b). indicates tested on long-form evaluation benchmarks using specifically designed long-form report agent workflow. Rows with gray backgrounds indicate deep research systems built on proprietary backbone models. For prompt-based systems, the model size, training data, code, and model checkpoint columns are marked with since they are not available."
        },
        {
            "title": "6.3 Analysis on Inference\nTool and domain usage adapt to each task’s information needs. Figure 8 shows that paper_search (our\nscientific-paper search) dominates on SQAv2, consistent with its focus on literature understanding. In\ncontrast, Google Search is the primary tool for HealthBench, DeepResearchBench, and SimpleQA, reflecting\nthe broader, open-web information needs of these tasks. web_browse is used comparatively less across all\ndatasets, serving as a follow-up tool for reading full pages when snippet context is insufficient.",
            "content": "Figure 9 further confirms task-specific retrieval behavior. HealthBench emphasizes authoritative biomedical and public-health sites (e.g., cdc.gov, pmc.ncbi.nlm.nih.gov, ncbi.nlm.nih.gov, mayoclinic.org). DeepResearchBench mixes technical and policy sources (e.g., researchgate.net, oecd.org, github.com), consistent with deeper, exploratory research tasks. SimpleQA is dominated by general reference and social/information platforms (e.g., en.wikipedia.org, facebook.com, youtube.com). Overall, tool usage and surfaced domains align with each datasets information demands: literature-centric tasks favor scientific search and scholarly venues, whereas open-domain tasks lean on general web search and broad reference sites."
        },
        {
            "title": "7 Related Work",
            "content": "Deep research agents. Rapid adoption of commercial deep research systems has spurred numerous open efforts, but most target verifiable, short-form generation (e.g., factoid QA or lightweight web-browsing tasks). Inspired by the success of scaling online RL on verifiable domains such as code and math, many methods follow similar recipe. For example, Search-R1 (Jin et al., 2025) applies GRPO to enhance search capabilities and is trained primarily on short-form question answering. Such approaches have been explored in many recent followup studies, including WebExplorer (Liu et al., 2025) and Tongyi Deep Research (Team et al., 2025). In contrast, WebThinker (Li et al., 2025a) employs DPO and proposes report-generation workflow. Nevertheless, the majority of these works train and evaluate only on short-form outputs. Furthermore, most open deep research systems either rely on single web search tool or train separate models per search backend (Gao et al., 2025), except for the latest Tongyi Deep Research, which includes Google Scholar API (Team et al., 2025) as tool. In expert domains (e.g., healthcare, science), we find that combining multiple search tools yields substantial gains. In addition, existing open systems typically omit explicit citations, unlike proprietary counterparts. Furthermore, many of these models do not fully open-source their training data or training codebases, making it difficult to further analyze or improve their training. complementary line of work builds deep research agents by carefully designing fixed long-form generation pipelines, often on top of state-of-the-art proprietary models, including WebWeaver (Li et al., 2025b), SFT-Enterprise Deep Research (Prabhakar et al., 2025), and 18 Ai2 ScholarQA (Singh et al., 2025). Leveraging strong backbone LMs, these systems overcome some of the aforementioned limitations and are evaluated primarily on long-form generation tasks. However, their fixed pipelines sacrifice flexibility in inference flow and output style (e.g., always producing long-form reports even for simple factoid questions), and they still do not provide clear path toward open, end-to-end trainable deep research models. Table 7 summarizes these gaps. To our knowledge, our system is the first fully open deep research framework that (i) is trained and rigorously evaluated on realistic long-form tasks, (ii) natively supports multi-tool search rather than single-tool or siloed models, and (iii) produces evidence-linked citations. Furthermore, even among open deep research models, some prior studies do not fully release their training data, as shown in Table 7. We fully open-source our training and evaluation code bases, data, and new infrastructure to support flexible deep research agent developments. Rubric design for long-form generation tasks. Prior work uses human-written rubrics for evaluation (Arora et al., 2025; Asai et al., 2024), but it is costly and not scalable when applied for training. RaR (Gunjal et al., 2025) proposed to use rubrics as rewards and generate instance-wise rubrics based on reference answers from an advanced model (OpenAI o3). However, these rubrics are static and usually generated by the same model, which can only slow down reward hacking but does not resolve the issue. In addition, these approaches rely on the capabilities of the model used to generate reference answers, whose knowledge is limited and not up to date, and thus cannot meet the needs of DR tasks. Our evolving rubrics are generated based on retrieved knowledge, echoing EvalAgent (Wadhwa et al., 2025), which uses search to construct better evaluation criteria for benchmarks. Concurrent works (Rezaei et al., 2025; Jayalath et al., 2025) explore generating online rubrics by contrasting pairwise or multiple model rollouts in closed-book setting. This approach echoes the design principle of our evolving rubrics but lacks grounding in external knowledge, which leads to exploitation (reshaping model behavior based solely on its internal knowledge) rather than exploration (integrating new external knowledge while also exploiting existing knowledge). Another concurrent work, RLAC (Wu et al., 2025b), explores training critic to propose likely incorrect fact that serves similar role to rubric for factuality tasks. Compared with concurrent works, our approach focuses on more challenging setupDR tasksand generates rubrics that both co-evolve with the policy model and remain grounded in external knowledge, enabling prolonged RL training with an evolving verifier."
        },
        {
            "title": "8 Discussion and Future Work",
            "content": "In conclusion, we highlight key insights, challenges, and promising directions for future work. Evolving rubrics adapt the verifier based on the policy models capabilities. At each training step, we update our rubrics by contrasting the models current rollouts, which helps the new rubric criteria better distinguish those outputs. We can view this as making the training difficulty adaptive to the models evolving behavior. This approach aligns with the idea of training in adaptive environments, which has been previously explored by adjusting prompts during training (Zeng et al., 2025). In contrast, we adapt the environment by updating the verifier (rubrics). Future work may consider jointly adapting both prompts and rubric criteria to further improve training efficiency. new dimension of scaling verifier compute: providing more privileged information to the judge. Another perspective on RLER is that it creates new way to scale the compute used by the verifier. While prior work focuses on increasing the reasoning tokens used by the reward model, often grounded in limited context (Guo et al., 2025; Chen et al., 2025b), we instead focus on enriching the information available to the verifier. This privileged information can include, but is not limited to: (1) contrastive model responses that help the verifier better understand the policy models capabilities; (2) external knowledge searches to validate factual accuracy; (3) detailed process information showing the step-by-step reasoning behind the policys final answer. While scaling up this information often increases context length and compute costs, it can extend the verifiers capabilities far beyond what infinite reasoning tokens alone can achieve, leading to more informed and meaningful decisions under fixed compute budget. 19 Evolving rubrics can also be interpreted from the perspective of increasing knowledge coverage. Figure 10 shows an abstract visualization of the knowledge coverage of different rubric types. Search expands the knowledge covered by the rubrics beyond the parametric knowledge of the rubric generator (an LM). Furthermore, evolving rubrics generated during training fold in new evidence discovered by the deep research policy during training rollouts, capturing knowledge that requires complex reasoning and planning to obtain, and allowing the evaluation criteria to evolve with the models distribution. The train-test mismatch challenge. When developing DR Tulu, we found that models that achieved the highest training reward did not necessarily achieve the highest downstream evaluation performance, although within the same run, higher training rewards usually correlated with better downstream performance; see Appendix F.5 for more details. We conjecture that this stems from mismatch between the tasks, rubrics, and evaluation setups of the external benchmarks vs. what we used for training. For instance, RL training uses judge that differs from the judges used in downstream evaluations, which can lead to reward hacking toward preferences specific to the training-time judge. Moreover, external benchmarks often use expert-crafted or generated rubrics that may emphasize aspects not captured in our training rubrics. Some rubrics may not be clear from the question alone, making it challenging for models not trained on specific benchmarks. This underscores the value of fully open DR models like DR Tulu, which can be easily customized for downstream tasks. Figure 10 Knowledge coverage relationship visualization. An abstract visualization of the knowledge coverage relationship between closed-book rubrics, initial search-based rubrics, and evolving search-based rubrics. Adaptation to specialized domains. Our experiments with GeneticDiseasesQA demonstrate that the RLER training recipe can generalize to specialized scientific domains, even without task-specific training. While the present work focuses on deep literature search and synthesis, many areas of scientific inquiry rely on information sourced from structured, domain-specific tools that operate over modalities beyond natural language (e.g., genomic sequences, molecular structures, transcriptomics, etc.). Incorporating these specialized data sources into trainingor, better yet, training the model to flexibly use previously unseen tools just in timewould be natural next step that permits the extension of DR Tulu to more complex scientific workflows."
        },
        {
            "title": "Author Contributions",
            "content": "DR Tulu is team effort. Here, we describe each authors primary contributing roles in the project, with bolded authors taking the lead within each section: Project leads: Rulin Shao, Akari Asai Core contributors: Rulin Shao, Akari Asai, Shannon Shen, Hamish Ivison, Varsha Kishore, Jingming"
        },
        {
            "title": "Zhuo",
            "content": "RLER method development: Rulin Shao, Hamish Ivison, Shannon Shen DR Tulu data: Akari Asai, Rulin Shao, Jingming Zhuo, Varsha Kishore, Shannon Shen, Luca Soldaini DR Tulu training: Hamish Ivison, Rulin Shao, Akari Asai, Shannon Shen Infrastructure: Shannon Shen, Hamish Ivison, Rulin Shao, Luca Soldaini, Tyler Murray, Varsha Kishore Evaluations and baselines: Varsha Kishore, Shannon Shen, Rulin Shao, Akari Asai, Jingming Zhuo, Hamish Ivison, Xinran Zhao 20 GeneticDiseasesQA benchmark creation and evaluations: Molly Park, Samuel Finlayson Project mentorship: Hannaneh Hajishirzi, Pang Wei Koh, Yoon Kim, Luke Zettlemoyer, Sherry Tongshuang Wu, Scott Yih, David Sontag, Faeze Brahman, Luca Soldaini, Pradeep Dasigi, Sewon Min. Core contributors made sustained, significant contributions throughout the project. All authors contributed to project discussions, experiment planning, and writing the paper."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP-2024-001), the AI2050 program at Schmidt Sciences, and the DARPA SciFy program (Agreement No. HR00112520300). We thank Zhiyuan Zeng, Rui Xin, Stella Li, and Doug Downey for helpful discussions and feedback on the draft."
        },
        {
            "title": "References",
            "content": "Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, et al. Openscholar: Synthesizing scientific literature with retrieval-augmented lms. arXiv preprint arXiv:2411.14199, 2024. Jonathan Bragg, Mike DArcy, Nishant Balepur, Dan Bareket, Bhavana Dalvi, Sergey Feldman, Dany Haddad, Jena Hwang, Peter Jansen, Varsha Kishore, et al. Astabench: Rigorous benchmarking of ai agents with scientific research suite. arXiv preprint arXiv:2510.21652, 2025. Tianyu Cao, Neel Bhandari, Akhila Yerukola, Akari Asai, and Maarten Sap. Out of style: Rags fragility to linguistic variation. arXiv preprint arXiv:2504.08231, 2025. D. Cheerie, M. M. Meserve, D. Beijer, C. Kaiwar, L. Newton, A. L. Taylor Tavares, A. S. Verran, E. Sherrill, S. Leonard, S. J. Sanders, E. Blake, N. Elkhateeb, A. Gandhi, N. S. Y. Liang, J. T. Morgan, A. Verwillow, J. Verheijen, A. Giles, S. Williams, M. Chopra, L. Croft, H. S. Dafsari, A. E. Davidson, J. Friedman, A. Gregor, B. Haque, R. Lechner, K. A. Montgomery, M. Ryten, E. Schober, G. Siegel, P. J. Sullivan, E. F. Whittle, B. Zardetto, T. W. Yu, M. Synofzik, A. Aartsma-Rus, G. Costain, M. C. Lauffer, and N=1 Collaborative. Consensus guidelines for assessing eligibility of pathogenic dna variants for antisense oligonucleotide treatments. American Journal of Human Genetics, 112(5):975983, May 2025. doi: 10.1016/j.ajhg.2025.02.017. Epub 2025 Mar 25. Liang Chen, Xueting Han, Li Shen, Jing Bai, and Kam-Fai Wong. Beyond two-stage training: Cooperative sft and rl for llm reasoning, 2025a. https://arxiv.org/abs/2509.06948. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025b. Jonathan Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: benchmark for information-seeking question answering in ty pologically di verse languages. Transactions of the Association for Computational Linguistics, 8:454470, 2020. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976, 2025. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. 21 Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward reasoning model. arXiv preprint arXiv:2505.14674, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. https:// aclanthology.org/2020.coling-main.580/. Dulhan Jayalath, Shashwat Goel, Thomas Foster, Parag Jain, Suchin Gururangan, Cheng Zhang, Anirudh Goyal, and Alan Schelten. Compute as teacher: Turning inference compute into reference-free supervision. arXiv preprint arXiv:2509.14234, 2025. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. In COLM, 2025. Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49404957, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.393. https://aclanthology.org/2021.naacl-main.393/. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Christopher Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. In Second Conference on Language Modeling, 2025. https://openreview.net/forum?id=i1uGbfHHpH. Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. Llms-as-judges: comprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579, 2024. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025a. Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, et al. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312, 2025b. Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, et al. Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501, 2025. Nelson Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 70017025, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.467. https://aclanthology.org/2023.findings-emnlp.467/. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint, 2022. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. https://aclanthology.org/2023.acl-long.546/. Angela Mastrianni, Hope Twede, Aleksandra Sarcevic, Jeremiah Wander, Christina Austin-Tse, Scott Saponas, Heidi Rehm, Ashley Mae Conard, and Amanda K. Hall. Ai-enhanced sensemaking: Exploring the design of generative ai-based assistant to support genetic professionals. ACM Trans. Interact. Intell. Syst., August 2025. ISSN 2160-6455. doi: 10.1145/3756326. https://doi.org/10.1145/3756326. 22 Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios Angelopoulos, Trevor Darrell, Narges Norouzi, et al. Search arena: Analyzing search-augmented llms. arXiv preprint arXiv:2506.05334, 2025. Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, and Shafiq Joty. Sfr-deepresearch: Towards effective reinforcement learning for autonomously reasoning single agents. arXiv preprint arXiv:2509.06283, 2025. Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron Courville. Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models, October 2024. http://arxiv. org/abs/2410.18252. OpenAI. Deep research system card, 2025. https://openai.com/index/deep-research-system-card/. Accessed: 2025-10-21. Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, and Weiran Yao. Enterprise deep research: Steerable multi-agent deep research for enterprise analytics. arXiv preprint arXiv:2510.17797, 2025. MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Yunzhong He, and Afra Feyza Akyürek. Online rubrics elicitation from pairwise comparisons. arXiv preprint arXiv:2510.07284, 2025. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a. https://arxiv.org/abs/2402.03300. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Liu, et al. Taskcraft: Automated generation of agentic tasks. arXiv preprint arXiv:2506.10055, 2025. Amanpreet Singh, Joseph Chee Chang, Dany Haddad, Aakanksha Naik, Jena D. Hwang, Rodney Kinney, Daniel Weld, Doug Downey, and Sergey Feldman. Ai2 scholar QA: Organized literature synthesis with attribution. In Pushkar Mishra, Smaranda Muresan, and Tao Yu, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 513523, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-253-4. doi: 10.18653/v1/2025.acl-demo.49. https://aclanthology.org/2025.acldemo.49/. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Manya Wadhwa, Zayne Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, and Greg Durrett. Evalagent: Discovering implicit evaluation criteria from the web. arXiv preprint arXiv:2504.15219, 2025. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025a. Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, and Aviral Kumar. Rlac: Reinforcement learning with adversarial critic for free-form generation tasks. arXiv preprint arXiv:2511.01758, 2025b. Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. critical evaluation of evaluations for long-form question answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32253245, Toronto, Canada, July 2023. 23 Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.181. https://aclanthology.org/2023. acl-long.181/. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. https://aclanthology.org/D18-1259/. Li Yifei, Allen Chang, Chaitanya Malaviya, and Mark Yatskar. Researchqa: Evaluating scholarly question answering at scale across 75 fields with survey-mined questions and rubrics. arXiv preprint arXiv:2509.00496, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. https://arxiv.org/abs/2503.14476. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In International Conference on Learning Representations (ICLR), 2024. Zhiyuan Zeng, Hamish Ivison, Yiping Wang, Lifan Yuan, Shuyue Stella Li, Zhuorui Ye, Siting Li, Jacqueline He, Runlong Zhou, Tong Chen, Chenyang Zhao, Yulia Tsvetkov, Simon Shaolei Du, Natasha Jaques, Hao Peng, Pang Wei Koh, and Hannaneh Hajishirzi. Rlve: Scaling up reinforcement learning for language models with adaptive verifiable environments. arXiv preprint 2511.07317, 2025."
        },
        {
            "title": "A Additional Details of RLER",
            "content": "A.1 Evolving Rubric Generation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Rubric Reward Judge Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Citation, Search, and Format Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . RLER Analysis and Toy Case Study . B.1 Rubric Specificity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Toy Case Study on Evolving Rubrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Data Creation Details",
            "content": "C.1 SFT Data Construction . C.2 RL Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Onpolicy SFT Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Training Details",
            "content": "D.1 SFT Hyperparameters D.2 RL Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Prompt used for General Rubric Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Experimental Details",
            "content": "E.1 Prompts for DR Tulu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Evaluation Details of Baseline Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Score Calculation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Details of Pathogenic Gene Variants Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F More Results and Analysis",
            "content": "F.1 Full RL Training Curves . F.2 Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Effect of the tool-call budget at inference time. . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Evaluation Variances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Mismatch between RL Training and Downstream Evaluation . . . . . . . . . . . . . . . . . . . F.6 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 26 26 29 30 30 30 30 34 37 37 37 37 38 38 38 42 43 43 43 49 49"
        },
        {
            "title": "A Additional Details of RLER",
            "content": "A.1 Evolving Rubric Generation Prompt We show the instruction we used for evolving rubric generation in Figure 11 and Figure 12. A.2 Rubric Reward Judge Prompt We show the rubric-judge prompt in Figure 13. Note that we use scale of 2 and divide the models score by 2 before returning it as the reward score. We omitted this detail from the main paper for simplicity. We leave exploring different scoring scales to future work. A.3 Citation, Search, and Format Rewards In this section, we detail the implementations of citation, search, and format rewards that are used as auxiliary rewards in RLER. We refer to the code for detailed implementations and prompts. A.3.1 Citation Reward Design Citation Reward Given query and response πθ(x), we evaluate citations with respect to citation store = {(i, si)} mapping citation IDs to snippets si. We first extract set of claims from y, with an associated (possibly empty) set of cited IDs for each claim, = {c1, . . . , cC} = ExtractClaims(y), I(c) { }, C. Citation-format reward. We reward valid citations by the fraction that resolve in S: (cid:12) (cid:12) (cid:83) Rfmt = (cid:12) (cid:12) cC I(c) keys(S) (cid:12) (cid:12) (cid:83) cC I(c) (cid:12) (cid:12) , 0, (cid:12) > 0, I(c)(cid:12) (cid:12) (cid:12)(cid:83) otherwise. Per-claim recall and precision. For each claim c, we define the concatenated evidence and obtain two LLM-judge signals: E(c) = (cid:76) iI(c) si, Recall. If I(c) = , the judge rates support of by E(c) as Fully = 1, Partially = 0.5, No = 0. Denote this by r(c) {1, 0.5, 0}. If I(c) = , we ask whether needs citation given (x, y). Let NeedCite(c) {0, 1}. Then r(c) = 1 NeedCite(c). Precision. If I(c) = , the judge checks whether E(c) is relevant to c: Relevant = 1, Irrelevant = 0. Denote this by p(c) {1, 0}. If I(c) = , we set p(c) = 1. Per-claim F1. Average F1. (c) = 2 r(c) p(c) r(c)+p(c) , r(c) + p(c) > 0, 0, otherwise. F1 = 1 (cid:88) cC (c). Final reward. We combine faithfulness (via F1) and format validity (via Rfmt) with fixed weights: rcit(x, y) = 0.6 F1 + 0.4 Rfmt, rcit(x, y) [0, 1]. 26 Evolving Rubric Generation Prompt (Part 1) You are an expert evaluator generating adaptive rubrics to assess model responses. ## Task Identify the most discriminative criteria that distinguish high-quality from low-quality answers. Capture subtle quality differences that existing rubrics miss. ## Output Components - **Description**: Detailed, specific description of what makes response excellent/problematic - **Title**: Concise abstract label (general, not question-specific) ## Categories 1. **Positive Rubrics**: Excellence indicators distinguishing superior responses 2. **Negative Rubrics**: Critical flaws definitively degrading quality ## Core Guidelines ### 1. Discriminative Power - Focus ONLY on criteria meaningfully separating quality levels - Each rubric must distinguish between otherwise similar responses - Exclude generic criteria applying equally to all responses ### 2. Novelty & Non-Redundancy With existing/ground truth rubrics: - Never duplicate overlapping rubrics in meaning/scope - Identify uncovered quality dimensions - Add granular criteria if existing ones are broad - Return empty lists if existing rubrics are comprehensive ### 3. Avoid Mirror Rubrics Never create positive/negative versions of same criterion: - \"Provides clear explanations\" + \"Lacks clear explanations\" - Choose only the more discriminative direction ### 4. Conservative Negative Rubrics - Identify clear failure modes, not absence of excellence - Response penalized if it exhibits ANY negative rubric behavior - Focus on active mistakes vs missing features ## Selection Strategy ### Quantity: 1-5 total rubrics (fewer high-quality > many generic) ### Distribution Based on Response Patterns: - **More positive**: Responses lack sophistication but avoid major errors - **More negative**: Systematic failure patterns present - **Balanced**: Both excellence gaps and failure modes exist - **Empty lists**: Existing rubrics already comprehensive ## Analysis Process 1. Group responses by quality level 2. Find factors separating higher/lower clusters 3. Check if factors covered by existing rubrics 4. Select criteria with highest discriminative value Figure 11 System prompt for generating evolving rubrics. Note that this is the first-half of the prompt and the second-half is in Figure 12 27 Evolving Rubric Generation Prompt (Part 2) ## Output Format ```json { \"question\": \"<original question verbatim>\", \"positive_rubrics\": [ {\"description\": \"<detailed excellence description>\", \"title\": \"<abstract label>\"} ], \"negative_rubrics\": [ {\"description\": \"<detailed failure description>\", \"title\": \"<abstract label>\"} ] } ``` ## Examples **Positive:** ```json {\"description\": \"Anticipates and addresses potential edge cases or exceptions to the main solution, demonstrating thorough problem understanding\", \"title\": \"Edge Case Handling\"} ``` **Negative:** ```json {\"description\": \"Conflates correlation with causation when interpreting data or making recommendations\", \"title\": \"Causal Misattribution\"} ``` ## Inputs 1. **Question**: Original question being answered 2. **Responses**: Multiple model responses (Response 1, Response 2, etc.) 3. **Existing Rubrics** (optional): Previously generated/ground truth rubrics ## Critical Reminders - Each rubric must distinguish between actual provided responses - Exclude rubrics applying equally to all responses - Prefer empty lists over redundancy when existing rubrics are comprehensive - Focus on observable, objective, actionable criteria - Quality over quantity: 2 excellent rubrics > 5 mediocre ones Generate only the most impactful, non-redundant rubrics revealing meaningful quality differences. Figure 12 Continuation of the system prompt for generating evolving rubrics."
        },
        {
            "title": "Rubric Judge Prompt",
            "content": "You will be given question someone asked (in <question></question> tags) and the corresponding response (in <response></response> tags) given to them by an assistant. You will then be given specific criterion of the response to evaluate (in <criterion></criterion> tags). Return score on scale of 0 to 2 indicating how appropriate the response is based on the given criterion. Judge only the specified aspect(s), not any other qualities of the answer. Output JSON in the format: {{\"score\": x}}. <question>{question}</question> <response>{response}</response> <criterion>{rubric}</criterion> Figure 13 System prompt for rubric reward computation. A.3.2 Search Reward Design To encourage the model to engage in multi-turn information gathering, we introduce search reward that scores the number of search tool calls made during generation. Specifically, we extract all search queries issued by the model (identified by search protocol tokens in the generated text) and count the number of valid, non-empty queries. The reward is computed as the ratio of the number of searches performed to an upper bound (set to 3 in our experiments), capped at 1.0. This design incentivizes the model to conduct multiple searches to gather diverse information sources, while preventing unbounded reward accumulation. A.3.3 Format Reward Design Beyond rubric-based and citation-specific rewards, we introduce lightweight auxiliary rewards that encourage structural correctness of responses with respect to the expected output schema. Given response to query x, we check for the presence of three components: 1. Answer format. Whether encloses final answer between <answer></answer> tags, producing binary indicator a(y) {0, 1}. 2. Citation format. Whether contains at least one citation enclosed in <cite></cite> tags, producing c(y) {0, 1}. 3. Query format. Whether includes at least one valid search query enclosed in <query></query> tags (or parser-specific equivalents), producing q(y) {0, 1}."
        },
        {
            "title": "We then define a weighted format reward as",
            "content": "rfmt(x, y) = 0.5 a(y) + 0.3 c(y) + 0.2 q(y), rfmt(x, y) [0, 1]. This reward acts as low-cost signal that steers the model toward producing well-formed outputs aligned with the tool-augmented interface, even when semantic judgments (e.g., citation recall or rubric alignment) are unavailable."
        },
        {
            "title": "B RLER Analysis and Toy Case Study",
            "content": "In this section, we provide additional experimental details for the RLER analysis and toy case study discussed in Section 3.3. 29 B.1 Rubric Specificity Analysis To evaluate the specificity level of generated rubrics, we instruct an LM to first classify whether the rubric is assertive as defined in Section 3.3 and, if it is assertive, whether it is factual. As this task requires the LM to have knowledge that is enough to check the factuality, we apply search-based API modelgpt-4o-searchpreviewwhich has access to OpenAI internal search tool, which is not as competitive as Deep Research model but is helpful enough for simple fact check. We use the prompt presented in Figure 14 to obtain the assertive rubric fraction and factuality scores. B.2 Toy Case Study on Evolving Rubrics To study the impact of an evolving rubric in RL runs over more training epochs, we perform toy case study in closed-book LM setup (where the policy model is not instructed to use tools and must answer on its own). Specifically, we train on single query for an extended number of epochs. The query asks Write comprehensive survey paper about retrieval-augmented generation (RAG) and the latest progress in the field, e.g., Deep Research, reasoning-intensive retrieval, context engineering, etc.. We train from Qwen3-8B using the same set of hyper-parameters as our main RL training as described in Appendix D.2. We launch two runs for the toy study, one with evolving rubrics, and another with an initial rubric only. For both runs, we set the initial rubric to be The response should mention the first paper that proposed RAG. This is simple case that echoes the limitation of static rubrics often being under-specified. In Figure 15, we show one example output from the policy model that exhibits undesirable code reasoning behavior in our toy case training."
        },
        {
            "title": "C Data Creation Details",
            "content": "C.1 SFT Data Construction C.1.1 Prompt curation Long-form prompt curation. For long-form prompts, we curated high-quality prompts by using an LLM judge. An LM (gpt-5) scores each prompt from 15 (higher is better) based on whether it demands multi-step search, planning, and synthesis, and we retain prompts with scores> 3 for OpenScholar and prompts with scores >2 for SearchArena. Consequently, we retain 20% of OpenScholar queries and 10% of SearchArena queries. We further construct rubric sets via LM prompting and subsequently use them to assess trajectory quality. Figure 16 presents the system prompt used to select queries. Short-form prompt curation. For short-form, we derive initial questions from widely open-sourced data including MegaScience (Fan et al., 2025), HotpotQA (Yang et al., 2018), TaskCraft (Shi et al., 2025), WebWalkerSilver (Wu et al., 2025a), PopQA (Mallen et al., 2022), and TyDi QA (Clark et al., 2020). We also used GPT-4.1 to generate 916 BrowseComp (Wei et al., 2025) style questions. C.1.2 Trajectory Generation Given the set of initial prompts, we generate high-quality trajectories data using three different teacher models. Those trajectories include reasoning traces, tool calls, and final answers with citations. Trajectory generation with GPT-5. We generated trajectories using GPT-5 and our search inference pipeline, using google_search, web_browse, and paper_search. Figure 17 shows the exact prompt that was used. We set the maximum tool call to be 15, and discarded instances where the model does not return the final answers marked with answer tags under the maximum tool call step. After we collect the trajectory data, we conducted light-weight rejection sampling. Specifically, we first discard responses that do not match the expected search workflow (e.g., does not include the final answer tag, or the citation or tool calling formats are incorrect). Then, for short-form, verifiable QA only, we apply answer-matching based rejection sampling: we keep examples only if the final answers match the original"
        },
        {
            "title": "Prompt for Assertive Fraction and Factuality Analysis",
            "content": "You are careful evaluator who determines whether each criterion contains factual claim, and if so, whether that claim is factually correct and verifiable. Instructions: 1. Read the question and the list of criteria carefully. 2. For each criterion, decide first whether it *makes factual claim* that is, whether it asserts something that can be verified as true or false in the real world. **Distinguishing referential vs. assertive phrasing:** - Referential ( NA): Criteria that only ask to *mention*, *explain*, *describe*, *discuss*, or *include information about* something, without specifying what that information should be. These refer to factual topics but do not assert any particular fact. - Example: 'Explain the principle of masked diffusion models.' NA (requests explanation, not asserting the content). - Example: 'Mention information about A.' NA. - Assertive ( factual claim): Criteria that *state or imply specific fact*, relationship, or property that could be true or false. They assert content, not just reference it. - Example: 'Masked diffusion models use random masking during the denoising process.' factual claim. - Example: 'A is located in B.' factual claim. 3. If the criterion is about writing style, tone, clarity, structure, or formatting, or if it only requires mentioning or explaining topics without specifying factual assertions, return 'NA'. 4. For each factual claim, check whether it can be verified using reliable evidence or reasoning. - If evidence confirms it factual and correct. - If reliable evidence contradicts it factual but incorrect. - If no verifiable evidence is found (e.g., no data, no known sources) factual but *unverified*. 5. Compute the factuality score as: - 1 All verifiable factual claims are correct. - Between 0 and 1 Some verifiable factual claims are correct, others are incorrect (average them). - 0 All verifiable factual claims are incorrect. - 'NA' None of the criteria makes any factual claims. 6. Do *not* lower the score for claims that are unverified (i.e., lacking evidence) unless there is evidence showing they are *false*. 7. Also count how many criteria are assertive but unverified. Output Format: Return your result strictly in JSON format as follows: {{\"factual_score\": <float_or_\"NA\">, \"explanation\": \"<short explanation>\", \"num_non_na_criteria\": <number>, \"num_na_criteria\": <number>, \"num_unverified_assertive_criteria\": <number>}} Now evaluate the following: Question: {question} Criteria: {criteria} Figure 14 System prompt for accessing the assertive fraction and factuality for rubrics."
        },
        {
            "title": "Example of a Code Reasoning Response in the Toy Case Study",
            "content": "```python import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity ... [Some outputs are omitted due to space limit.] # Load the survey data survey_data = pd.read_csv('survey_raw.csv') # Remove duplicate entries survey_data.drop_duplicates(inplace=True) # Clean the text data survey_data['response'] = survey_data['response'].str.replace('[^ws]', '') # Create TF-IDF matrix vectorizer = TfidfVectorizer(stop_words='english') tfidf_matrix = vectorizer.fit_transform(survey_data['response']) # Calculate the cosine similarity matrix cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix) # Define function to get the top 5 most similar responses def get_similar_responses(query, cosine_sim=cosine_sim, survey_data=survey_data, n=5): # Get the index of the query query_index = survey_data[survey_data['question'] == query].index[0] # Get the similarity scores similarity_scores = list(enumerate(cosine_sim[query_index])) # Sort the similarity scores similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True) # Get the top 5 most similar responses most_similar_responses = similarity_scores[1:n] # Return the responses return [survey_data.iloc[i[0]]['response'] for in most_similar_responses] # Define function to get the insights ... [Some outputs are omitted due to space limit.] # Example usage print(get_similar_responses('What is the latest progress in the field of retrieval-augmented generation (RAG)?')) print(get_insights('What is the latest progress in the field of retrieval-augmented generation (RAG)?')) ``` This code creates survey about retrieval-augmented generation (RAG) and the latest progress in the field, e.g., Deep Research, reasoning-intensive retrieval, context engineering, etc. It uses TF-IDF and cosine similarity to find the most similar responses to given question and also provides insights about the latest progress in the field. Figure 15 Example of Qwen3-8Bs code reasoning behavior in our toy case study."
        },
        {
            "title": "Query Selection Prompt",
            "content": "You are query-quality grader for Search-Augmented LLMs. Your job: Given ONE user query (English only) from Search Arena, decide whether to SKIP or GRADE it for retrieval-oriented quality. Dataset facts you may rely on (if provided): - Each record contains chat histories `messages_a` and `messages_b`; the FIRST message in each is from the user and is the query we grade. - Records may also include `primary_intent`, `secondary_intent`, and `languages`. - If `primary_intent` is `creative_generation` or `others`, SKIP. - If the query is not English, SKIP. (If fields are missing, infer from the query text.) What counts as high-quality search query? 1) Requires external knowledge (factual/domain content from web/docs/papers/data). 2) Requires complex planning (multi-source search, comparisons, aggregation, synthesis). 3) Often expects long-form responses. 4) Cannot be answered well from parametric knowledge alone (up-to-date or niche). 5) Is evaluable by single answer or clear rubrics (metrics, dates, versions, counts). Safety: - Must be safe: no PII harvesting, disallowed instructions, or offensive content. Scoring (integers only): 1 = Trivial/chit-chat; no retrieval; not evaluable. 2 = Mostly reasoning/riddle/definitional; little retrieval; unclear target. 3 = Some retrieval and synthesis but scope/intent modest or underspecified. 4 = Clearly retrieval-heavy and planning-oriented; evaluable with evidence/rubrics. 5 = Strong retrieval + complex planning + clear, evaluable targets; likely long-form. Few-shot demonstrations (for guidance only; DO NOT copy or echo these in outputs): - Query: who is ion vlad-doru -> Score: 3 Rationale: Factual knowledge and retrieval helps, but simple entity lookup; limited planning. - Query: hello -> Score: 1 Rationale: Chit-chat; no external knowledge or evaluable target. - Query: Windows 11 build 27813 vs Windows 11 24H2 vs Windows 11 23H2, comparison for modern PCs? -> Score: 3 Rationale: Requires searching and synthesis across versions/builds; intent mostly clear but scope (what counts as \"modern PCs\") needs clarification. - Query: I'm an even, single-digit number. Once you write me, have no start or end. look like standing pair of glasses. Who am I? -> Score: 2 Rationale: Riddle; reasoning required but no external knowledge retrieval; not search task. - Query: best running watch -> Score: 3 Rationale: Retrieval and synthesis likely; intent clear but underspecified; could be rubricized (features, price, ecosystem). - Query: what is SWE-Bench state of the art at the moment? -> Score: 4 Rationale: Up-to-date SOTA requires extensive retrieval (papers/leaderboards); evaluable by metrics; planning needed. - Query: amount of remote jobs for Java jobs (exclude android and desktop) vs .Net vs GoLang vs NodeJS in EU? Please note UK is not in EU -> Score: 5 Rationale: Complex planning with constraints, aggregation across sources/regions, and clearly evaluable counts/methodology. Figure 16 System prompt for selecting high-quality prompts."
        },
        {
            "title": "Prompt Source",
            "content": "Output format Number Avg. Tool Calls Avg. Length OpenScholar Search Arena ScholarQA HotpotQA MegaScience TaskCraft WebWalkerSilver BrowseComp PopQA & TyDi QA Long-form Long-form Long-form Short-form Short-form Short-form Short-form Short-form Short-form 5704 3547 1000 1176 814 583 1438 916 874 3.5 3.1 5.4 2.4 2.3 2.8 2.5 8.6 3.7 3878.7 2745.9 5400.5 1488.8 1494.8 1518.1 1540.2 4083.5 1514. Table 8 SFT data stats. The output format specifies whether task requires long-form or short-form response, and the number denotes the number of instances. We also report the average number of tool calls and the average length (in words) of the teacher trajectories. gold answers, based on (1) if the F1 overlap between the predicted answer and the gold answer exceeds 0.9, or (2) if an LLM judge deems the two answers are semantically identical. Figures 3134 show examples of generated trajectories. We also use GPT-5 with google_search and web_browse to generate few hundred interleaved search and think trajectories for BrowseComp-style questions. Trajectory generations using Ai2 ScholarQA. We used the trajectory data from Ai2 ScholarQA to create SFT data. Ai2 ScholarQA collects all search results before generation and does not perform iterative searches. To create synthetic data with iterative searches, we transformed the data provided in the Ai2 ScholarQA traces. Each trace consists of retrieved results, CoT planning steps, and an answer with citations. We used GPT-4.1 to create sub-query for each section in the Ai2 ScholarQA. The sub-query was generated conditioned on the section text and retrieved papers cited in the section. We created the final iterative search data by interleaving sub-queries, associated retrieved papers, reasoning from the CoT plan. The iterative trace was combined with the final answer to create SFT data. Data stats. Table 8 shows the final statistics of the resulting SFT data. Example of SFT data. Figures 3134 show an example trajectory of OpenScholar in our SFT data. C.2 RL Data Construction Initial rubric constructions. For RL, we used the same query selection process and collected high-quality prompts that are not used during SFT, from SearchArena and OpenScholar. For each prompt, we generate initial set of rubrics, using external search systems. Specifically, for OpenScholar queries, we use paper search (S2 snippet search) and for SearchArena queries, we use google search (serper search) and web browsing (serper browse) to retrieve top 10 search results. Given the retrieved documents, we generate set of initial rubrics. The system prompts for this is in Figure 18. We used GPT-4.1-mini as the rubric generation model. C.3 Onpolicy SFT Data Construction We also generate on-policy SFT data by sampling trajectories from our SFT checkpoint and applying rejection sampling. While this improves the standalone performance of the SFT model, we initialize RL from the original SFT checkpoint instead, as our preliminary experiments suggest that starting from slightly weaker SFT model ultimately leads to higher overall performance after RL. Trajectory generation. After we train our initial SFT model (DR Tulu SFT), we use dr-agent-lib to generate responses to the randomly sampled prompts from our initial SFT dataset. For each prompt, we generate 2-4 trajectories, using the same inference pipeline as our evaluation time."
        },
        {
            "title": "Trajectory Generation Prompt",
            "content": "You are research assistant that answers questions through iterative reasoning and evidence-backed search using multiple external search systems. 1. Operating Principles, Process & Guidelines 1.1 Principles - Provide comprehensive, evidence-backed answers to scientific questions. - Ground every nontrivial claim in retrieved snippets; never fabricate content. Cite using <cite id=\"...\">...</cite> drawn only from returned snippets. - Prefer authoritative sources (peer-reviewed papers, reputable benchmarks/docs) and prioritize recent work for fast-moving areas. - Acknowledge uncertainty and conflicts; if evidence is thin or sources disagree, state it and explain what additional evidence would resolve it. - Structure with clear Markdown headers and coherent flow. In each section, write 2-5 sentence paragraphs with clear topic sentences and transitions; use lists sparingly only when they improve clarity. - Synthesize, don't enumerate: group findings across papers, explain relationships, and build coherent narrative that answers the question, supported by citations. - Do not invent snippets or citations. Snippets arrive only via tool calls (<query> -> <snippet>, see more details below); use them as the sole evidence base. 1.2 Process and Iteration loop (at least search four times) 1) **Initial plan** Begin with `<think>` that decomposes the question, lists assumptions, outlines concrete search plan (start broad -> ablations/benchmarks -> domain-specific; include venues/years), and defines the first query. 2) **Query -> Snippets -> Think** For each iteration: - Run `<call_tool>` and read the returned `<snippet>` results. - Then add `<think>` (natural prose) that: - Summarizes what the latest snippets show; marks which are relevant vs. irrelevant **and why**. - Extracts quantitative details (metrics, deltas), definitions, settings, and limitations. - States what is still missing and the **exact next query** you will run (refined terms, venues, years, paper IDs). - Prefer `snippet_search` for paragraph-level evidence. If you use `search_papers_by_relevance`, **immediately** follow with `snippet_search` over returned paper IDs to retrieve paragraphs. - Continue searching until you have enough evidence to answer the question or exhaust reasonable queries. 3) **Sufficiency check** When evidence is adequate for precise answer (including trade-offs), synthesize single `<answer>` with section headers and inline citations. Before generating the final answers, briefly reflect on the evidence and any remaining gaps in `<think>`. Carefully think about the structure of the responses, write it down inside <think>, and then generate the final answer in `<answer>`. ... (Guideline, few shot demonstrations, and tool call details) Figure 17 System prompt for generating trajectory data."
        },
        {
            "title": "Trajectory Generation Prompt",
            "content": "You will receive: (1) user Question that tests literature knowledge, and (2) list of Snippets (each with an id and text). Your task: design rubric compact set of elements (\"ingredients\") that high-quality final answer should satisfy, and map each element to the most relevant snippets. Important: You are specifying what *good answer must contain*, not grading any existing answer. Use ONLY the provided snippets for evidence. -------------------------------- INPUT FORMAT -------------------------------- - Question: single string. - Snippets: list of items. Each item has: - id: unique identifier (e.g., S_abcd123, DOI/CorpusID, or similar). - text: the snippet content (the ONLY citable text). -------------------------------- WHAT TO RETURN -------------------------------- Return single JSON object with EXACTLY these top-level keys: { \"Question\": <string>, \"Answer Critical\": [ { \"Ingredient\": <string>, \"Handle\": <string>, \"Specifics\": [ { \"Text\": <string>, \"Citation\": <id> } ... ] } ], \"Valuable\": [ { \"Ingredient\": <string>, \"Handle\": <string>, \"Specifics\": [ { \"Text\": <string>, \"Citation\": <id> } ... ] } ], \"Context\": [ { \"Ingredient\": <string>, \"Handle\": <string>, \"Specifics\": [ { \"Text\": <string>, \"Citation\": <id> } ... ] } ] } -------------------------------- INGREDIENT BUDGET & DIFFICULTY -------------------------------- - Include at least **5 \"Answer Critical\"** elements (ideally more); use \"Valuable\" and/or \"Context\" only if genuinely needed. - Make each element **detailed and challenging**: it should bundle multiple precise, testable requirements for the same capability (multi-criteria), not broad or vague checks. - Make each element **detailed and challenging**: write it as **multi-criteria** requirement (multiple precise, testable sub-checks for single capability). ... Figure 18 System prompt for generating initial-search based. Full system prompts are available in our repository. 36 Hyperparameter Cutoff length. Per device training batch size. Gradient accumulation step. learning rate. Number of training epochs. Learning rate scheduler. Warmup ratio. Data type. Temperature for sampling rollouts. Weight decay. Value 16384 1 16 0.00004 5 cosine 0.1 BF16 1.0 0.0 Table 9 Hyperparameters used for SFT training. Hyperparameter Unique prompts per batch. Number of rollouts for each prompt (group size). Number of minibatches per GRPO step. Inner epochs trained for each batch. Max number of tokens in the prompt. Max response length in tokens. Maximum number of tokens packed into single sequence. Maximum number of tool calls allowed during training. Temperature for sampling rollouts. Top-p for sampling rollouts KL penalty coefficient. Learning rate schedule. Learning rate. AdamW optimizer betas. Weight decay. Max number evolving rubrics retained per prompt (Kmax). Value 32 8 1 1 2048 16384 18500 10 1.0 1.0 0.001 constant 5 107 (0.9, 0.95) 0.0 5 Table 10 Hyperparameters used for GRPO training. Rejection sampling. After collecting trajectories, we apply rejection sampling. In addition to the lightweight procedure described in Appendix C.1, we further apply rubric-based and citation-based filters to trajectories from long-form prompts. Specifically, we compute rubric coverage and citation precision for each trajectory and retain only those with scores above 0.6 on both metrics. This offline filtering scheme mirrors our RL reward design, but is applied during data generation rather than online training."
        },
        {
            "title": "D Training Details",
            "content": "D.1 SFT Hyperparameters We provide the hyperparameters used during the SFT training in Table 9 D.2 RL Hyperparameters We provide the hyperparameters used during RL training in Table 10. D.3 Prompt used for General Rubric Training Figure 19 presents system prompts used for general rubric training."
        },
        {
            "title": "General Rubric Prompt",
            "content": "<system> You will be given question someone asked (in <question></question> tags) and the corresponding response (in <response></response> tags) given to them by an assistant. You will then be given specific criterion of the response to evaluate (in <criterion></criterion> tags). Return score on scale of 0 to 2 indicating how appropriate the response is based on the given criterion. Judge only the specified aspect(s), not any other qualities of the answer. Output JSON in the format: {{\"score\": x}}. <user> <question>{question}</question> <response>{response}</response> <criterion> (1) Overall Comprehensiveness: The report should cover content as comprehensively as possible (2) Thoroughness of Discussion: Each section should be discussed thoroughly, not just superficially (3) Factuality: There should be minimal factual errors (4) Coherence: The discussion should stay focused and relevant to the topic </criterion> Figure 19 General rubric (baseline). This prompt describes the general rubric from WebThinker (Li et al., 2025a) evaluation, which we used as baseline in Section 6.2."
        },
        {
            "title": "E Experimental Details",
            "content": "Category Tool Name Description General Search serper_google_webpage_search Web search using Google (via Serper.dev API) massive_serve_search Dense passage retrieval using massive-serve API semantic_scholar_search Search for paper information using Semantic Scholar API Scholar Search semantic_scholar_snippet_search Search for text snippets within academic papers pubmed_search Search for biomedical papers using PubMed API Browse Tools serper_google_scholar_search Academic paper search using Google Scholar serper_fetch_webpage_content Fetch webpage content using Serper.dev API crawl4ai_fetch_webpage_content Async webpage fetch using Crawl4AI jin_fetch_webpage_content Fetch webpage content using Jina.ai API Reranker Tools vllm_hosted_reranker Rerank documents using VLLM hosted reranker Table 11 The list of supported tools in our agent library. E.1 Prompts for DR Tulu Figures 20 and 21 show DR Tulu system prompt. E.2 Evaluation Details of Baseline Models Open deep research models. For WebExplorer and Tongyi Deep Research, we use their official codebase12 13 to generate trajectories for all tasks with their default settings, except that we replace their summary model 12https://github.com/hkust-nlp/WebExplorer 13https://github.com/Alibaba-NLP/DeepResearch"
        },
        {
            "title": "DR Tulu System Prompt Part I",
            "content": "You are research assistant who answers questions through iterative reasoning and research. ## Process - Use <think></think> tags to show your reasoning at any point. - Use <call_tool name=\"...\">query</call_tool> when you need information (see tools below). - You can alternate between thinking and searching multiple times. - Only provide <answer></answer> tags when you have enough information for complete response. If the problem asks for specific, short-form answer, you can also put the answer string in the boxed{} format. - Support every non-trivial claim with retrieved evidence. Wrap the exact claim span in <cite id=\"ID1,ID2\">...</cite>, where id are snippet IDs from searched results (comma-separated if multiple). Use only returned snippets; never invent IDs. Avoid citing filler text - cite just the factual claim. ## Calling Tools (<call_tool name=\"...\">query</call_tool>) - You can use the following tools: 1. google_search - Purpose: general web search. - Input via: <call_tool name=\"google_search\">your query</call_tool> - Output: web search snippets (see SEARCH RESULTS). - Optional parameters - gl: geolocation - hl: host language 2. browse_webpage - Purpose: open specific URL (typically one returned by google_search) and extract readable page text as snippets. - Input via: <call_tool name=\"browse_webpage\">https://example.com/article</call_tool> - Output: webpage (see SEARCH RESULTS). 3. snippet_search - Purpose: focused snippet retrieval from scientific papers - Input via: <call_tool name=\"snippet_search\">your query</call_tool> - Output: snippets from existing papers (see SEARCH RESULTS). - Examples: <call_tool name=\"snippet_search\" limit=\"8\" year=\"2021-2025\" fieldsOfStudy=\"Computer Science, Medicine\">large language model retrieval evaluation</call_tool> - Optional parameters - limit: number of snippets to retrieve - year: publication year; you can use single number (e.g., 2024) or range (e.g., 2022-2025) - fieldsOfStudy: One or comma-separated list from: Computer Science, Medicine, Chemistry, Biology, Materials Science, Physics, Geology, Psychology, Art, History, Geography, Sociology, Business, Political Science, Economics, Philosophy, Mathematics, Engineering, Environmental Science, Agricultural and Food Sciences, Education, Law, Linguistics. ## Tool Output - After you issue tool call, we will execute it and return results wrapped in <tool_output> tags. - For web search and snippet search, the results appear as: <tool_output><snippet id=UNIQUE_ID>content</snippet>...</tool_output> - For web browsing, the searched results are represented as <tool_output><webpage id=UNIQUE_ID>content</webpage></tool_output> Figure 20 DR Tulu System Prompts Part I."
        },
        {
            "title": "DR Tulu System Prompt Part II",
            "content": "## Answer and Citation Format - Once you collect all of the necessary information, generate the final answer, and mark your answer with answer tags: <answer></answer>. - If your answer is short (e.g., phrase or number), you can also put the answer string in the boxed{} format. - In your answer, wrap the supported text in <cite id=\"SNIPPET_ID\"> ... </cite>. You have to use the exact ID from returned <snippet id=...>...</snippet>. - If multiple sources support passage, use multiple <cite> tags around the relevant clauses/sentences. - Examples <cite id=\"S17\">LLMs often hallucinate on long-tail facts.</cite> <answer>Based on the search results, <cite id=\"S23\">the first Harry Potter movie was released on November 16, 2001.</cite>Therefore, the final answer is boxed{November 16, 2001}.</answer> ## WORKFLOW EXAMPLE Below is simple example that demonstrates the process and the correct use of tools and tags. In practice, you'll often need additional search iterations, and your final answer may be much longer (e.g., multi-paragraph report). Question: Give concise update on 2024 renewable energy market trends and current commercial solar efficiency benchmarks. <think>I need to understand the current market trends first</think> <call_tool name=\"google_search\">2024 renewable energy market trends</call_tool> <tool_output>[results provided as <snippet id=S_a1B9xQ2>...</snippet>, <snippet id=S_p0Zr41Q>...</snippet>]</tool_output> <think>Now need specific data on solar panel efficiency</think> <call_tool name=\"snippet_search\" limit=\"5\" year=\"2023-2025\" fieldsOfStudy=\"Engineering, Materials Science\">latest solar panel efficiency 2024</call_tool> <tool_output>[results provided as <snippet id=S_x4xU7dU>...</snippet>, <snippet id=S_GxA2ZLh>...</snippet>]</tool_output> <think>I have enough to answer succinctly</think> <answer> Global renewables expanded rapidly in 2024, <cite ids=\"S_p0Zr41Q,S_GxA2ZLh\">driven primarily by the growth of solar and wind energy.</cite> <cite ids=\"S_x4xU7dU\">State-of-the-art commercial solar modules report cell efficiencies of 26-27% and module efficiencies of 23-24%.</cite> boxed{Solar leads 2024 renewables; top commercial module efficiency 23-24%} </answer> Figure 21 DR Tulu System Prompts Part II. 40 GPT-5+ Search (baseline) Prompt You are research assistant who answers questions through reasoning and research. Requirements: - For the given question, please write comprehensive, evidence-backed answers to scientific questions. The report should be structure multi-paragraph report. - Think and search until you have sufficient information - Only provide the final answer when ready - Cite all claims from search results. You should ground every nontrivial claim in retrieved snippets. - Please prefer authoritative sources (peer-reviewed papers, reputable benchmarks/docs) and prioritize recent work for fast-moving areas. - You should acknowledge uncertainty and conflicts; if evidence is thin or sources disagree, state it and explain what additional evidence would resolve it. - It's important to structure with clear markdown headers and coherent flow. In each section, write 2-5 sentence paragraphs with clear topic sentences and transitions; use lists sparingly only when they improve clarity. Ideally, you should synthesize rather than enumerate content: it's helpful to group findings across papers, explain relationships, and build coherent narrative that answers the question, supported by citations. - Most importantly, DO NOT invent snippets or citations and never fabricate content. Question: Figure 22 GPT-5+ Search (baseline). This is the prompt used for the GPT-5 + Search baseline. with local Qwen3-8B server. For ASearcher, we use their official codebase14 to generate trajectories for all tasks. For WebThinker (Li et al., 2025a), we used their code base15 and evaluated both their default mode as well as report mode. Closed deep research systems. Figure 22 shows the prompt used to run the GPT-5 + Search and Gemini3 Pro + Search baseline. For Gemini3 Pro, we use Google Search and URL Content tools provided by Gemini API. Naive RAG. For the Naive RAG baselines, we retrieve the top 10 search snippets from google_search using the original question, and then prompt the LM with simple instruction: Can you try to answer the question given the retrieved documents? Specifically, you should reason step by step given the evidence retrieved from the web; when there is no evidence present, you should try to answer it based on your knowledge. Please provide final answer in the format of \"Final Answer: [your answer here]. Fixed pipeline deep research models on short-form questions. We do not evaluate models that use specialized inference pipeline on short-form questions because they are unable to follow instructions to only output the answer. For instance, here is an example answer from Ai2 ScholarQA for query from SimpleQA: Query: Which two scientists (first and last names) are credited with first isolating Azotobacter salinestris from saline soils? Answer in less than 10 words. Answer: Page and Shivprasad were the scientists who first isolated Azotobacter salinestris from saline soils <Paper corpusId=\"7646032\" paperTitle=\"(Robson et al., 2015)\" isShortName></Paper>. While Beijerinck isolated Azotobacter species in 1901, this was different species and predated the discovery of A. salinestris <Paper corpusId=\"87342753\" paperTitle=\"(Shin et al., 2016)\" isShortName></Paper> 14https://github.com/inclusionAI/ASearcher 15https://github.com/RUC-NLPIR/WebThinker 41 E.3 Score Calculation Details Asta-ScholarQA-CS2. We use the official code16 to evaluate our method and the baselines on Asta-ScholarQACS2. We compute rubric score, answer precision, citation precision and citation recall on the 100 test set questions using gemini-2.5-flash as the judge as detailed in Bragg et al. (2025). Asta-ScholarQA-CS2 requires evidence text from citations in order to judge citation precision and recall. For proprietary models (OpenAI Deep Research, GPT-5) that only provide URLs for citations, we use JINA API to scrape the URL and use the resulting text as citation evidence. HealthBench. We use an adapted version of the OpenAI simple-evals suite17 for the evaluation. For each multi-turn example, we concatenate the full conversation into single input and prepend an instruction directing the model to answer the question based on this doctorpatient conversation. For efficiency, we randomly sample subset of 1000 cases for evaluation. DeepResearchBench. We use the official code18 to evaluate our method and the baselines on DeepResearchBench. We compute the Comprehensiveness, Insight/Depth, Instruction-Following, and Readability of articles answering 50 English and 50 Chinese open-ended deep research questions. We report the Overall as the macro average of the component metrics. We use gemini-2.5-flash as the judge and Jina API to scrape the URL to acquire the evidence snippets when needed, as detailed in Du et al. (2025). For outputs from our system, we use the scraped URL content from the corresponding search or browsing tools. ResearchQA. We evaluate our method and baselines using the original ResearchQA evaluation suite19. We compute the averaged rubric scores with GPT-4.1-mini as the judge on the 776 official subset of questions used to evaluate deep research systems, following Yifei et al. (2025). E.4 Details of Pathogenic Gene Variants Evaluation Dataset The evaluation data for this task was derived from expert-curated information collected for 24 pathogenic gene variants published in the supplementary data of Cheerie et al. (2025), which was used to develop guidelines for the assessment of variant eligibility for various types of antisense oligonucleotide (ASO) gene therapy. Curations were done by members of the N=1 Collaborative Patient Identification Working Group, which consists of both medical professionals (MD, PhD, and masters level) and faculty with expertise in medical genetics. The selected variants were deemed feasible to assess with publicly available information, and each response was agreed upon by two members. This data reports characteristics of selected variants that are essential to determining therapeutic eligibility, including the variants pathomechanism, haploinsufficiency status of the gene it affects, inheritance pattern of associated diseases, splicing effects, and findings from prior therapeutic approaches explored. We manually reformatted this data into 47 questionanswer examples. Genetic variants are specified in HGVS notation. Questions were preceded with few sentences of context containing some details on what types of evidence are preferred and proper answer format. Setup To avoid the effects of contamination, we blocked all search results pointing to the paper and its supplementary files when evaluating models with our search tools. This was not possible for closed source DR systems, though the original paper did not appear in the output citations for any model. We also add the statement: \"You should try to find multiple pieces of evidence to support any claims you make, and acknowledge conflicting/supporting evidence among sources searched\" to the baseline prompt in Figure 22 for GPT+5, OpenAI DR, and Gemini 3 Pro to ensure they are calibrated to the preferences of this task Evaluation Criteria Given that the tasks ultimate goal is to aid medical decision-making, we designed criteria that capture not only the correctness of the final answer, but the usefulness of the generated report to clinician or researcher: 16https://github.com/allenai/asta-bench 17https://github.com/openai/simple-evals 18https://github.com/Ayanami0730/deep_research_bench 19https://github.com/realliyifei/ResearchQA 42 Final Answer indicates whether the expert-annotated fact was mentioned in the response. Each question has 1-3 key facts that should be present in an ideal response, and the per-example correctness is the average of these. Evidence Quality indicates if the type of evidence requested in the query is present within the cited statements (e.g. functional assays in patient-derived cells). This score aims to penalize (1) uncited evidence and (2) evidence that is cited, but irrelevant. To demonstrate this, we provide an example response from DR Tulu that met the Evidence Synthesis criteria (Figure 23) Evidence Support measures the proportion of cited statements in the response that are consistent with the original source text retrieved. This was calculated by evaluating each cited span with the original content of the cited source. Evidence Synthesis indicates whether or not there was at least one statement describing the relationship between multiple sources, e.g. how papers might build off each other or conflict. Similar to our prior experiments, we defined specific LLM judge instructions for each evaluation criteria per question and used GPT-4.1 to score each response. No additional training was performed for this task."
        },
        {
            "title": "F More Results and Analysis",
            "content": "F.1 Full RL Training Curves We show the reward, number of tool calls, and output sequence length through training in Figure 27. We observe that there appear to be two phases of training: in the first phase, sequence lengths and the average number of tool calls drop, and then they increase. Similar to prior work, we hypothesise this may be due to the model initially unlearning unsuccessful behaviors picked up during SFT training before stabilizing and exploring new strategies. similar drop-and-rise behaviour when combining RL training with SFT cold-start data has been observed in other domains such as mathematical reasoning (Chen et al., 2025a), and we leave further investigation of this phenomenon to future work. Surprisingly, we also find that our training is somewhat robust to tool errors, as we accidentally ran out of serper credits during training. While we eventualy refilled, the model did train for some number of steps wherein serper would continuously return errors, as seen by the drop in reward and sequence length. However, despite this, our overall model performance continued to improve, with step 1900 being significantly better than step 1000 (right before the serper credit issue). This suggests our training is somewhat robust to server-side tool errors, and exploring the degree of this robustness is an interesting avenue for future investigation. F.2 Cost Estimation In this section, we detail how we estimate the cost of deep research models. For proprietary models, we use the actual billed costs reported in their API consoles. Although Gemini 3 + Search waives the first 1,500 searches per day,20 we still include search costs in our estimates for fair comparison with other systems that charge for search. For open models (including our own), we compute the number of input and output tokens and use OpenRouters published pricing to estimate inference costs. Specifically, we use the published pricing for Tongyi Deep Research21; we use the published pricing for Qwen3-8B for DR Tulu, WebExplorer-8B, ASearcher22; We we use the published pricing for QwQ-32B for WebThinker models23. We treat system prompts and questions as input tokens, and all remaining tokens as output tokens in our calculations for open models. We also add tool-call costs based on the pricing of each tools API provider and the average number of tool calls performed by each model across our long-form evaluations. 20https://ai.google.dev/gemini-api/docs/pricing?hl=en 21Tongyi Deep Research costs USD 0.09 per input token and USD 0.4 per output token as of Nov 23 2025. 22Qwen3-8B costs USD 0.2 per input and output token as of Nov 23 2025. 23QwQ-32B costs USD 0.4 per input and output token as of Nov 23 2025. LLM judge prompt for Evidence Support evaluation (GeneticDiseasesQA) # Instructions You are claim validator. You will be given the text content from webpage, and list of claims from research report that cited the webpage. <<content>> <<prompt>> <<claims>> For each claim given you, you will determine if it is supported by the original source's content given. For source content with only the title available, judge them as `supporting` if the title indicates that the paper is likely relevant to the claim being considered. Return JSON object with single key `claims` which is list of `claim` objects, one for each phrase given from the research report. Each `claim` object contains the claim itself (`text`), boolean `is_supported` which indicates if the claim is fully supported by the citation source content, and `explanation`, which should be string briefly explaining why the response is or is not supported by the citation source content. If the provided source content is completely empty, entirely filler webpage text, or is an error message or content filter, output an empty list for the `claims` key. # Example USER: Content of cited source: \"US holidays include Veteran's Day (November 11th) and Halloween (October 31st).\" Claims from generated report: - \"Veteran's Day is national holiday in November.\" ASSISTANT: ```json \"claims\": [ \"text\": \"Veteran's Day is national holiday in November.\", \"is_supported\": true, \"explanation\": \"Sources states that Veteran's Day falls on November 11th.\" { }, ] ``` # Final instruction Return just the json object in markdown format. Do not include any other text in the response. 44 Example Response of DR Tulu-8B on GeneticDiseasesQA Part USER: Context: Your task is to determine the pathomechanism of given pathogenic gene variant. pathogenic variant can lead to different effects. The variant can lead to loss of function (LoF) of protein, toxic gain of function (GoF), or dominant-negative (DN) effect. Assessing the variant effect can be done by retrieving publications and reports of the variant [...] Question: What is the pathomechanism of variant NM_001127222.2(CACNA1A):c.4174G>A? Summary of the variant and its identity - The variant NM_001127222.2(CACNA1A):c.4174G>A corresponds to missense change p.Val1392Met in the CACNA1A alpha 1A subunit and is curated as pathogenic or likely pathogenic allele in clinical resources, notably ClinVar and the ClinGen allele registry, where it is associated with familial hemiplegic migraine and related neurologic phenotypes <cite id=\"00907dd4-0\">NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met) is cataloged as single-nucleotide variant in ClinVar with the canonical transcript NM_001127222.2</cite> <cite id=\"00907dd4-4\">The ClinGen Allele Registry lists NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met) as registered canonical allele</cite> <cite id=\"00907dd4-6\">MGeND maps NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met) to Migraine, familial hemiplegic, 1 (FHM1)</cite>. Gene-level context: CACNA1A encodes the P/Q-type calcium channel alpha 1A subunit and underlies an autosomal dominant channelopathy spectrum - CACNA1A encodes the pore-forming $alpha$1A subunit of the high-voltageactivated P/Q-type calcium channel and is dominant disease gene for three disorders: familial hemiplegic migraine (FHM), episodic ataxia type 2 (EA2), and spinocerebellar ataxia type 6 (SCA6) <cite id=\"00907dd4-1\">Pathogenic variants in CACNA1A are associated with episodic ataxia type 2, familial hemiplegic migraine, and spinocerebellar ataxia 6</cite> <cite id=\"8169b531-9\">The CACNA1A gene was identified for FHM; it encodes the $alpha$1A subunit of the P/Q-type calcium channel</cite>. - Clinically, affected individuals present on phenotypic spectrum from migraine with transient hemiplegia/ataxia to persistent cerebellar ataxia and atrophy, consistent with dominant inheritance and variable expressivity <cite id=\"7a079b1d-9\">Mutations in CACNA1A are responsible for at least three autosomal dominant disorders: FHM1, episodic ataxia, and spinocerebellar ataxia</cite> <cite id=\"7a079b1d-0\">CACNA1A-related hemiplegic migraine is associated with neurological variability and requires urgent treatment</cite> <cite id=\"7a079b1d-7\">Clinically, CACNA1A variants are linked to autosomal dominant FHM with cerebellar ataxia</cite>. - Across conditions, both gain-of-function (GoF) and loss-of-function (LoF)/haploinsufficient phenotypes have been described, with FHM commonly linked to GoF variants affecting channel gating/hyperexcitability and ataxia/SCA6 more often associated with LoF/hypofunction or channel dysfunction leading to neurodegeneration <cite id=\"8169b531-7\">Hemiplegic migraines are associated with particular types of genetic changes in CACNA1A called gain of function variants</cite> <cite id=\"7a079b1d-6\">FHM with cerebellar ataxia is only associated with specific CACNA1A mutations that lead to ataxia</cite>. - In broader neurodevelopmental disease, CACNA1A variants have also been reported in epileptic encephalopathy and cognitive/autism phenotypes, underscoring that some pathogenic variants may act through haploinsufficiency or dominant-negative effects in certain contexts <cite id=\"00907dd4-2\">ClinVar Miner notes CACNA1A missense variants in neurodevelopmental conditions</cite> <cite id=\"00907dd4-5\">Reports associate CACNA1A haploinsufficiency with cognitive impairment, autism, and epileptic encephalopathy with mild cerebellar symptoms</cite>. Figure 23 Example response of DR Tulu-8B on GeneticDiseasesQA Part 45 Example Response of DR Tulu-8B on GeneticDiseasesQA Part II Variant-specific evidence and phenotype triangulation for p.Val1392Met (c.4174G>A) - The p.Val1392Met change in CACNA1A is curated as pathogenic/likely pathogenic for FHM and, in some families, associated with persistent cerebellar ataxia and atrophy, aligning with dominant gain-of-toxic-function mechanism that perturbs neuronal excitability (migraine) and, in some cases, contributes to neurodegeneration <cite id=\"00907dd4-0\">ClinVar classifies NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met)</cite> <cite id=\"7a079b1d-9\">FHM and ataxia phenotypes are both linked to CACNA1A mutations in an autosomal dominant framework</cite> <cite id=\"7a079b1d-6\">FHM with cerebellar ataxia is only associated with certain CACNA1A mutations</cite>. - Population and registry compilations include c.4174G>A among pathogenic CACNA1A variants, reinforcing its role in the dominant channelopathy spectrum; however, direct electrophysiological characterization of this specific variant (e.g., activation voltage, Ca2+ influx, inactivation) is not provided in the retrieved summaries and would require targeted functional assays <cite id=\"00907dd4-9\">Variant catalogs list NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met) among pathogenic CACNA1A variants</cite> <cite id=\"8169b531-8\">Clinical-genetic characterization of CACNA1A-related disease summarizes dominant phenotypes across FHM, EA2, and SCA6</cite>. Applying the criteria to determine pathomechanism - Dominant inheritance and autosomal transmission across FHM, EA2, and SCA6 support dominant mechanism at the genephenotype level for CACNA1A variants <cite id=\"7a079b1d-9\">CACNA1A-related disorders are autosomal dominant</cite>. - For pathomechanism, CACNA1A variants can act as GoF (hyperexcitability/channel dysfunction causing migraine) or LoF/haploinsufficient (reduced channel function leading to ataxia/degeneration), with some variants producing mixed or variable phenotypes depending on domain impact and genetic background <cite id=\"8169b531-7\">FHM is associated with gain-of-function variants in CACNA1A</cite> <cite id=\"7a079b1d-6\">FHM with cerebellar ataxia is specifically associated with certain CACNA1A mutations that lead to ataxia</cite> <cite id=\"00907dd4-5\">Haploinsufficiency in CACNA1A has been linked to neurodevelopmental phenotypes</cite>. - When both GoF and LoF mechanisms are plausible at the gene level, the variants phenotype and inheritance refine the likely mechanism. In FHM, transient neurologic deficits and aura point to dominant gain-of-toxic-function mechanism in sensory-motor cortical circuits; in EA2/SCA6, persistent ataxia and cerebellar atrophy point to reduced channel function or progressive neurodegeneration, consistent with LoF/haploinsufficiency or dominant-negative effects in cerebellar pathways <cite id=\"8169b531-7\">Hemiplegic migraine variants are labeled as gain-of-function</cite> <cite id=\"7a079b1d-6\">Cerebellar ataxia in FHM is linked to specific CACNA1A mutations</cite>. - Given that p.Val1392Met is cataloged as pathogenic for FHM and associated with cerebellar ataxia/atrophy in some pedigrees, the most parsimonious pathomechanism is dominant gain-of-toxic-function effect on neuronal excitability that drives migraine, with possible additional contributions to cerebellar dysfunction via haploinsufficiency or dominant-negative effects in cerebellar circuits, depending on genetic background and penetrance <cite id=\"00907dd4-0\">ClinVar catalogs p.Val1392Met as pathogenic CACNA1A variant</cite> <cite id=\"7a079b1d-9\">Autosomal dominant inheritance and variable phenotypes across FHM, EA2, and SCA6 are established</cite> <cite id=\"00907dd4-5\">Haploinsufficiency in CACNA1A contributes to neurodevelopmental disease</cite>. Figure 24 Example response of DR Tulu-8B on GeneticDiseasesQA Part II 46 Example Response of DR Tulu-8B on GeneticDiseasesQA Part III Uncertainties and what functional studies would resolve them - The retrieved evidence establishes the variants identity, clinical classification, and the genes dominant channelopathy framework, but does not provide direct patch-clamp or Ca2+ imaging data for p.Val1392Met. Therefore, while dominant gain-of-toxic-function mechanism is most consistent with FHM presentation and gene-level GoF associations, we cannot exclude dominant-negative or partial haploinsufficient contribution without direct tests <cite id=\"8169b531-8\">Clinical and genetic characterization supports dominant mechanisms but lacks variant-specific electrophysiology</cite>. - To conclusively classify the mechanism for c.4174G>A (p.Val1392Met), the following functional evidence would be decisive: - Heterologous expression studies in HEK293 or Xenopus oocytes measuring Ca2+ current amplitude, activation/dactivation kinetics, voltage dependence, inactivation, and pharmacology, with rescue by wild-type co-expression to test for dominant-negative effects <cite id=\"00907dd4-9\">Variant catalogs emphasize the need for functional characterization for CACNA1A missense variants</cite>. - Neuronal action potential and neurotransmitter release assays in cerebellar Purkinje cells or cortical neurons to quantify excitability changes and test for GoF vs LoF phenotypes <cite id=\"8169b531-9\">CACNA1A encodes the P/Q-type channel critical for neurotransmitter release and neuronal excitability</cite>. - Human induced pluripotent stem cellderived neurons or patient-specific cerebellar organoids to assess developmental Ca2+ signaling and degeneration kinetics in LoF vs GoF contexts <cite id=\"00907dd4-5\">Reports link CACNA1A haploinsufficiency to neurodevelopmental phenotypes, supporting developmental functional assays</cite>. - Trans-heterozygosity testing in models (e.g., LoF knockouts) to probe dominant-negative interactions specific to p.Val1392Met <cite id=\"7a079b1d-6\">FHM with cerebellar ataxia is associated with specific CACNA1A mutations suggesting gene-dose interactions</cite>. Final assessment for NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met) - Pathomechanism: Dominant gain-of-toxic-function in the neuronal excitability pathway is the most consistent explanation for p.Val1392Met, given its classification as pathogenic FHM variant and the genes GoF-linked channelopathy basis for migraine; in some pedigrees with cerebellar ataxia/atrophy, additional haploinsufficient or dominant-negative contributions in cerebellar pathways may contribute, yielding mixed mechanism across brain regions and genetic backgrounds <cite id=\"00907dd4-0\">ClinVar classifies p.Val1392Met as pathogenic in CACNA1A</cite> <cite id=\"8169b531-7\">FHM is associated with gain-of-function CACNA1A variants</cite> <cite id=\"7a079b1d-6\">Cerebellar ataxia in FHM is associated with specific CACNA1A mutations, suggesting multi-domain effects</cite> <cite id=\"00907dd4-5\">Haploinsufficiency in CACNA1A is linked to neurodevelopmental phenotypes</cite>. - Practical implication: In clinical contexts where CACNA1A pathogenic variants are known to act dominantly and cause FHM with possible cerebellar involvement, c.4174G>A (p.Val1392Met) should be considered to confer dominant gain-of-function (toxic) effect on P/Q-type Ca2+ channelmediated neurotransmission, with possible additive/penetrant haploinsufficient effects in cerebellar pathways in affected families <cite id=\"7a079b1d-9\">Autosomal dominant inheritance and variable phenotypes across migraine and ataxia are established for CACNA1A</cite> <cite id=\"8169b531-7\">Gain-of-function mechanism is characteristic of FHM-associated CACNA1A variants</cite>. Figure 25 Example response of DR Tulu-8B on GeneticDiseasesQA Part III 47 Example Response of DR Tulu-8B on GeneticDiseasesQA Part IV Evidence gaps and recommendations - Direct functional characterization of p.Val1392Met remains to be reported in the retrieved literature; thus, further mechanistic studies (as outlined above) are warranted to exclude or confirm dominant-negative or LoF contributions and to delineate the variants phenotypemechanism continuum within the CACNA1A spectrum of disease <cite id=\"8169b531-8\">Clinical-genetic summaries highlight the need for functional follow-up of CACNA1A missense variants</cite> <cite id=\"00907dd4-9\">Variant catalogs emphasize the presence of pathogenic missense variants across neurological conditions and the need for functional characterization</cite>. Cited texts: [00907dd4-0]: Title: NM_001127222.2(CACNA1A):c.4174G>A (p.Val1392Met ... - NCBI [00907dd4-4]: Title: CA242642 - Allele Registry - ClinGen [00907dd4-6]: Title: Annotation Detail : MGeND [00907dd4-1]: Title: Clinical and genetic characterization of CACNA1A-related disease [8169b531-9]: Title: CACNA1A gene mutations in familial hemiplegic migraine [7a079b1d-9]: Title: Two distinct phenotypes, hemiplegic migraine and episodic Ataxia ... [7a079b1d-0]: Title: Hemiplegic Migraine Variant List - CACNA1A Foundation [7a079b1d-7]: Title: Next-generation sequencing identified novel CACNA1A I1379F ... [8169b531-7]: Title: CACNA1A-Related Disorders - Children's Hospital of Philadelphia [7a079b1d-6]: Title: [Familial hemiplegic migraine] - PubMed - PMID:15775664 [00907dd4-2]: Title: Submissions for variant NM_001127222.2(CACNA1A):c.4174G>T [00907dd4-5]: Title: LOC126862864 Gene - GeneCards - GeneCards [00907dd4-9]: Title: Total 90 pathogenic variants reported for Spinocerebellar ataxia type 6 [8169b531-8]: Title: Clinical and genetic characterization of CACNA1A-related disease Figure 26 Example response of DR Tulu-8B on GeneticDiseasesQA Part IV Figure 27 Overall reward, output length (in tokens, including tool outputs), and average number of tool calls during RL training. Note that during the run, we ran out of serper credits, causing rise tool errors until we refilled the account, causing drop in reward and output length, and rise in tool calls (as the model retried the failed calls). Figure 28 Ablations on maximum tool calls. We evaluate DR Tulu (SFT) and DR Tulu (RL) on four long-form datasets while varying the maximum number of allowed tool calls from 1 to 15, and report how performance changes under these caps. F.3 Effect of the tool-call budget at inference time. We studied how the inference-time tool-call budget affects performance by varying the maximum number of allowed tool calls to {1,3,5,10,15}. For both SFT and RL models, performance typically saturates around budget of five tool calls, although RL occasionally improves with an additional budget of up to ten tool calls; see Figure 28. This matches tool call behavior seen during RL training (Appendix F.1), in which the model uses 3-4 tool calls on average per sample. F.4 Evaluation Variances The inference and evaluation of deep research models often exhibit significant variance. When running models on the same questions or evaluating them on the same benchmarks, the results can vary substantially. Typically, there are three key factors that contribute to this variance and we will discuss them in the following sections. F.4.1 Variances Introduced by Tools Invoking tool with identical inputs at different times can yield inconsistent outputs, leading the model to produce divergent subsequent contexts. In this section, we focus on the output variance introduced by the search engine. We sampled 100 function-calling queries and reissued these queries to the serper google search, with approximately one week between the two invocations. We then computed the differences in the top-1, top-3, top-5, and top-10 retrieved snippets for each pair of calls. For comparison, we also measured the differences between two calls issued nearly the same time. We use the following metrics to evaluate the variance of search engines returns. 49 Top-K Snippets Top-K Snippets"
        },
        {
            "title": "Metric",
            "content": "1 0.77 set_same pos_match 0.77 3 2.04 1.71 5 3.34 2.22 10 6.01 2."
        },
        {
            "title": "Metric",
            "content": "1 0.88 set_same pos_match 0.88 3 2.53 2.35 5 4.20 3.48 10 7.67 5.64 (a) One-week apart (b) Within short interval Table 12 Search engine output variance across repeated queries. The left table shows results when two queries were issued one week apart, while the right table shows results when the two queries were issued within short interval. set_same: The number of shared items within the top-k results, regardless of order (i.e., the size of the intersection). pos_match: The number of positions in the top-k where both lists contain the same item at the same rank. As shown in Table 12, the search engines returns are unstable. Even when calling the same query within short interval, it still produces noticeably different results. The average overlap in the top-10 snippets is only about 7.67, with exact rank matches dropping to 5.64. When the same queries are reissued one week apart, the search engines returns diverge more significantly. We show examples of inconsistencies in Figure 29. F.4.2 Variances Introduced by Inference Generating particularly long trajectories can also introduce variance. Small differences early in the process can lead to substantially divergent final responses. To observe the impact of this variability, we re-ran one short-form benchmark and two long-form benchmarks, comparing the outputs from two separate generations using GPT-4.1 with our auto-search pipeline. As shown in Table 13, in 2Wiki, GPT-4.1 produces final answers that differ by 29.3% in the 300 cases and obtains high variances in long-form tasks like Healthbench and ResearchQA as well."
        },
        {
            "title": "Numbers\nDiff",
            "content": "2Wiki Healthbench ResearchQA 300 29.3 900 17.1 776 9.81 Table 13 Inference Variance. The Diff in 2Wiki refers to the difference in two final answers of the two trajectories under the same cases, while the Diff in Healthbench and ResearchQA represents the absolute difference in LLM judged scores. F.4.3 Variances Introduced by Judge Models When evaluating the same responses in different times, even if using the same model as judge, inconsistent judgments may occur. We use GPT-4.1 to evaluate the same trajectories twice and the results are shown in Table 14. The judgments show relatively high consistency and reliability on both short-form and long-form tasks. 2Wiki Healthbench ResearchQA 1 2 67.67 66.33 37.67 37. 66.18 66.43 Table 14 Judgement Variance. 50 Showcase of Search Engines Returns # Search Results (show top-5 snippets) Position: 1 Link: https://en.wikipedia.org/wiki/E._Howard_Hunt Snippet: Everette Howard Hunt Jr. (October 9, 1918 January 23, 2007) was an American intelligence officer and author. From 1949 to 1970, Hunt served as an officer ... Position: 2 Link: https://www.amazon.com/stores/author/B0034QAV74 Snippet: Top E. Howard Hunt titles American Spy: My Secret History in the CIA, Watergate and Beyond. American Spy: My Secret History in the CIA, Watergate and Beyond. Position: 3 Link: https://www.fantasticfiction.com/h/e-howard-hunt/ Snippet: Everette Howard Hunt, Jr. was an American author and spy. He worked for the Central Intelligence Agency (CIA) and later the White House under President ... Position: 4 Link: https://www.goodreads.com/author/list/118536.E_Howard_Hunt Snippet: E. Howard Hunt has 85 books on Goodreads with 2730 ratings. E. Howard Hunt's most popular book is House Dick. Position: 5 Snippet: E. Howard Hunt, spy's spy. Hunt carried on writing spy novels long after the Watergate scandal but the Peter Ward books are among his most popular series ... # Search Results II Position: 1 Link: https://en.wikipedia.org/wiki/E._Howard_Hunt Snippet: Everette Howard Hunt Jr. (October 9, 1918 January 23, 2007) was an American intelligence officer and author. From 1949 to 1970, Hunt served as an officer ... Position: 2 Link: https://www.goodreads.com/author/list/118536.E_Howard_Hunt Snippet: E. Howard Hunt has 85 books on Goodreads with 2730 ratings. E. Howard Hunt's most popular book is House Dick. Position: 3 Link: https://www.amazon.com/E-Howard-Hunt/e/B0034QAV74/ref=dp_byline_cont_ebooks_1 Snippet: Follow E. Howard Hunt and explore their bibliography from Amazon's E. Howard Hunt Author Page ... Howard Hunt. Most popular. American Spy: My Secret History ... Position: 4 Link: https://www.fantasticfiction.com/h/e-howard-hunt/ Snippet: Everette Howard Hunt, Jr. was an American author and spy. He worked for the Central Intelligence Agency (CIA) and later the White House under President ... Position: 5 Link: https://spyscape.com/article/cia-spy-howard-hunt-confessions-of-a-watergate-plumber Snippet: E. Howard Hunt, spy's spy. Hunt carried on writing spy novels long after the Watergate scandal but the Peter Ward books are among his most popular series ... Figure 29 Examples of Inconsistencies of Search Results. 51 Figure 30 Metrics during RL training when starting from two different models (with and without SFT). Pers. Rubric reward refers to the reward over the search-based rubrics only (not including the rubrics generated as part of the RLER process). Starting from model without cold start data (no SFT) achieves higher train reward, but underperforms on downstream evaluations. F.5 Mismatch between RL Training and Downstream Evaluation During development of DR Tulu, we found that our RL training setup had some mismatch with our downstream performance: models that achieved the highest training reward did not necessarily achieve the highest downstream evaluation performance. To highlight this, compare the training reward of the No SFT and Our SFT models in Figure 30 against their performance in Figure 7. While starting directly from Qwen 3 (No SFT) achieves highest train reward,it has dramatically lower downstream evaluation results (see Figure 7). It also displays significantly different behavior, using significantly more tool calls than the cold-started (Our SFT) model. This may be due to few factors: first, our evaluations use rubrics generated in different manners to our own training rubrics (e.g., Healthbench uses expert-annotated rubrics), potentially leading to cases where test-time rubrics evaluate features not commonly tested in our training rubrics. Second, there may be reward hacking behavior during RL training, due to our use of an LM judge different to the judges used in downstream evaluation. Our in-loop judge model uses GPT-4.1-mini, while downstream evaluations use varied different models (e.g., SQAv2 uses Gemini Flash 2.5, DRB uses mix of Gemini Pro 2.5 and Gemini Flash 2.5, and Healthbench uses GPT-4.1, all with varying prompts and evaluation harnesses). This may lead to our RL training optimizing for attributes preferred by GPT-4.1-mini, but not by downstream evaluation judges. We finally conjecture that another contributing factor may be the difference in model priors: different models may exploit rewards in different ways. For example, rollouts from weaker model may contain fewer highquality answers; when all answers are poor, the judge model may end up selecting based solely on spurious features rather than making meaningful quality comparisons. In contrast, when starting from the same initial model, we usually observe that reward improvements correlate well with downstream scores. We defer deeper investigation of this mismatch phenomenon to future work, as addressing it would help improve the effectiveness of rubrics for RL training. F.6 Qualitative Examples We present one trajectory of our DR Tulu on the long-form task ResearchQA in Figures 3537. The Response is truncated for brevity; see our full model responses examples at https://dr-tulu.github.io/."
        },
        {
            "title": "User",
            "content": "Please provide detailed overview of theories and models related to Artificial Intelligence (AI) in education. Include information on the foundational theories that underpin AI's role in education, such as constructivism, behaviorism, and connectivism, and how these theories inform AI-based learning tools. Additionally, explain prominent AI models and frameworks, such as Intelligent Tutoring Systems (ITS), Adaptive Learning Systems, Learning Analytics, and Natural Language Processing in education. Highlight their key features, applications, and contributions to personalized learning, teacher efficiency, and student engagement. Please reference recent scholarly works to support your response."
        },
        {
            "title": "Assistant",
            "content": "<think> Decompose the question: We need to (a) outline foundational learning theories (behaviorism, constructivism, connectivism) and explicitly connect them to AI-in-education designs, and (b) explain prominent AI models/frameworks (ITS, adaptive learning, learning analytics, NLP in education), including features, applications, and contributions (personalization, teacher efficiency, engagement), all supported by recent scholarly sources. Assumptions: There are review/survey papers in AI in Education (AIED), Learning Analytics (LAK), and Educational Data Mining (EDM) from 20202025 that synthesize how pedagogical theories inform AI tools, and evaluations of ITS, adaptive systems, analytics, and NLP tools. We should also pull specific examples and quantified impacts where possible (e.g., learning gains from ITS, prediction performance in LA, efficacy of automated feedback). Search plan: 1) Start with surveys/reviews linking AI in education to learning theories (20202025), ideally in AIED, BJET, Computers & Education, Review of Educational Research, LAK, EDM venues. 2) Retrieve authoritative sources on ITS and adaptive learning (classic + recent advances like knowledge tracing, deep KT, mastery learning, LLM tutoring). 3) Gather learning analytics frameworks and applications (early warning systems, dashboards, ethics). 4) Collect NLP-in-education applications (automated scoring, feedback, dialogue tutoring, LLMs) and evidence on teacher efficiency and student engagement. First query will target broad review tying theories and AI models together. </think> <call_tool name=\"snippet_search\" year=\"2020-2025\" limit=\"10\" fieldsOfStudy=\"Education,Computer Science\">AI in education review behaviorism constructivism connectivism intelligent tutoring systems adaptive learning learning analytics NLP</call_tool> Figure 31 Example trajectory for OpenScholar SFT data Part I."
        },
        {
            "title": "Functional Calling",
            "content": "<tool_output> ... <snippet id=\"S_4yWB0UY\">Title: The AI Revolution in Education: Will AI Replace or Assist Teachers in Higher Education? AI in education has evolved from computer-assisted instruction to intelligent tutoring systems (ITS) and adaptive learning technologies, with key themes including personalized feedback, natural language processing, and innovative tools like chatbots and virtual reality, enhancing student engagement and facilitating knowledge co-creation between 2020 and 2025.</snippet> <snippet id=\"S_KAymhlA\">Title: AI Language Models as Educational Allies: Enhancing Instructional Support in Higher Education AI in education enhances instructional techniques, assessment methods, and personalized learning through intelligent tutoring systems and adaptive learning, while also necessitating further knowledge on technology to avoid misinformation, indicating dynamic area of research from 2020 to 2025.</snippet> ... </tool_output>"
        },
        {
            "title": "Assistant",
            "content": "<think> Reflection on first query results: - The initial search returned broad reviews touching on AI branches (ML, DL, NLP) and applications (ITS, adaptive learning, automated assessment) with some claims about personalization and teacher support (S_c8C9s9m, S_NTanrK5, S_N8QWms7). These are relevant but high-level and light on explicit connections to learning theories and quantitative evidence. - S_tuhIPDv explicitly mentions ITS and NLP integrating behaviorism, constructivism, and connectivism, which is directly pertinent, though need stronger, peer-reviewed sources that articulate these theory-to-tool mappings. - For models/frameworks, also need authoritative, recent evidence on ITS effectiveness, adaptive learning via knowledge tracing, learning analytics frameworks (predictive models, dashboards), and NLP applications (AWE, ASR, chatbots, LLMs) with impacts on personalization, teacher efficiency, and engagement. Gaps: - Explicit theoretical grounding linking behaviorism, constructivism, and connectivism to AI tools with scholarly backing. - Quantitative or meta-analytic evidence for ITS/adaptive systems. - Up-to-date learning analytics frameworks and outcomes. - NLP in education evidence, including automated feedback and LLM-based tutoring. Next query: Search for papers that explicitly connect behaviorism, constructivism, and connectivism to AI/ITS/adaptive learning frameworks in education, 20202025, in education and CS venues. </think> <call_tool name=\"snippet_search\" year=\"2020-2025\" limit=\"10\" fieldsOfStudy=\"Education,Computer Science\">behaviorism constructivism connectivism artificial intelligence in education theoretical framework</call_tool>"
        },
        {
            "title": "Functional Calling",
            "content": "... Figure 32 Example Trajectory for OpenScholar SFT Data Part II."
        },
        {
            "title": "Assistant",
            "content": "<think> We have initial broad reviews but need stronger, specific evidence tying learning theories to AI designs and quantifying effects for ITS, adaptive learning, learning analytics, and NLP in education. Key gaps: (1) explicit mapping of behaviorism/constructivism/connectivism to AI paradigms with scholarly backing; (2) quantitative evidence for ITS/adaptive learning (e.g., learning gains, knowledge tracing); (3) learning analytics frameworks and teacher-facing outcomes; and (4) NLP applications (AWE/ASR/chatbots/LLM tutors) with impacts on personalization, efficiency, and engagement. Next, will search for meta-analyses or systematic reviews on ITS effectiveness and adaptive learning from 20202025, then for learning analytics frameworks and NLP-in-education evaluations. </think> <call_tool name=\"snippet_search\" year=\"2020-2025\" limit=\"10\" fieldsOfStudy=\"Education,Computer Science\">intelligent tutoring systems meta-analysis effectiveness learning gains 2020..2025 educational data mining knowledge tracing adaptive learning</call_tool>"
        },
        {
            "title": "Functional Calling",
            "content": "..."
        },
        {
            "title": "Assistant",
            "content": "<think> We have gathered broad reviews on AI in education and some mentions connecting pedagogical theories to AI paradigms. We also have meta-analytic evidence for ITS effectiveness summarized in recent reviews, but we still need: (a) clearer, recent sources on adaptive learning mechanisms like knowledge tracing; (b) specific learning analytics frameworks and teacher-facing applications; and (c) NLP-in-education evidence including automated feedback and LLM-based tutoring impacts. We have performed four searches; we can do up to four more if needed. However, current snippets include: systematic review describing AI branches and applications in education (S_c8C9s9m); papers situating AI paradigms within behaviorist, constructivist, and connectivist frames (S_YRwbcNE, S_JCPVjqk, S_6KKZmn3); reviews summarizing ITS/adaptivity and meta-analyses with effect sizes (S_lUcsvG5, S_sUEgQk4, S_xkpGPkk); and reviews of NLP applications in language learning (S_KVwSGwG) and integrated AI+learning analytics perspectives (S_RBEFb7h). While we could add more targeted evidence on knowledge tracing and dashboards, the current evidence suffices to construct theory-to-tool mapping and to detail features/applications and contributions across ITS, adaptive learning, learning analytics, and NLP with citations. will structure the answer with sections: (1) Foundations: behaviorism, constructivism, connectivism and their implications for AI tools; (2) Intelligent Tutoring Systems; (3) Adaptive Learning Systems; (4) Learning Analytics; (5) NLP in Education; (6) Contributions to personalized learning, teacher efficiency, and engagement; (7) Emerging trends and considerations. will ensure claims are supported by retrieved snippets. </think> Figure 33 Example trajectory for OpenScholar SFT data Part III."
        },
        {
            "title": "Example Trajectory for OpenScholar SFT Data Part IV",
            "content": "Assistant (Final Answer) <answer> # Overview Artificial Intelligence (AI) in education encompasses techniques such as machine learning, deep learning, and natural language processing that analyze learner data to personalize instruction, generate feedback, and support pedagogical decision-making across intelligent tutoring systems, adaptive platforms, and learning analytics tools <cite id=\"S_c8C9s9m\">AI has many branches and sub-branches, such as machine learning, deep learning, and natural language processing; in education, AI supports intelligent tutoring systems, intelligent agents, and intelligent collaborative learning systems, and supports teachers' decision making by reporting real-time class statuses and personalized learning platforms</cite>. Recent syntheses emphasize AIs rapid expansion into personalized feedback, chatbots, and virtual environments for engagement and co-creation, while underscoring the need for interdisciplinary integration with learning sciences <cite id=\"S_4yWB0UY\">AI in education has evolved from computer-assisted instruction to intelligent tutoring systems and adaptive learning technologies, with key themes including personalized feedback, natural language processing, and innovative tools like chatbots and virtual reality, enhancing student engagement and facilitating knowledge co-creation</cite><cite id=\"S_RBEFb7h\">The integration of AI, particularly through learning analytics and NLP, is revolutionizing education by enhancing personalized learning experiences and engagement, with advancements like large language models improving performance in examinations and emphasizing collaboration between AI developers and educators</cite>. # Foundational Theories and How They Inform AI in Education Behaviorism, cognitivism, constructivism, and connectivism offer complementary lenses for designing AI-enabled learning experiences. Behaviorism emphasizes observable behavior change through stimuli, repetition, and reinforcement, aligning with drill-and-practice, mastery learning, and immediate feedback mechanisms common in early computer-assisted instruction and some ITS components <cite id=\"S_6KKZmn3\">The earliest learning theory, behaviorism, focuses on human behaviors based on repetition and response to stimuli, enabling learning to be observed and quantified</cite>. Constructivism positions learners as active constructors of knowledge through authentic, collaborative, and reflective activity, motivating AI tools that scaffold inquiry, support problem-based learning, and adaptively guide students through complex tasks <cite id=\"S_6KKZmn3\">Constructivism is learner-centered, emphasizing collaborative, active, and reflective learning where students act autonomously and self-directed</cite><cite id=\"S_XrZdksg\">Constructivism encompasses situated cognition, activity theory, experiential learning, anchored instruction, and authentic learning as frameworks for educational technology, organized under constructivist umbrella</cite>. Connectivism reframes learning as networked, distributed across people and technologies, shaping AI designs that augment learner agency through networked resources, recommendation, and collaborative tools typical of MOOCs and social learning environments <cite id=\"S_JCPVjqk\">Connectivism, emerging as response to technology's role, emphasizes learning through external connections and technology performing cognitive tasks</cite><cite id=\"S_TzKH9RC\">Connectivism is grounded in interactions within networks, incorporating aspects of behaviorism, cognitivism, and social constructivism, and has been influential as an epistemological basis for MOOCs</cite>. ... (Response truncated for brevity; see our full dataset for complete prompts and examples.) Figure 34 Example trajectory for OpenScholar SFT data Part IV."
        },
        {
            "title": "User",
            "content": "How do the narrow and broad approaches to CSR reporting differ in terms of their scope and the stakeholders they address?"
        },
        {
            "title": "Assistant",
            "content": "<think> Decompose the question: We need to define and differentiate the \"narrow\" versus \"broad\" approaches to CSR reporting, specifically along two dimensions: (1) scope (what is reported) and (2) stakeholders addressed (who benefits/engages with the report). Assumptions: This terminology is used in CSR/ sustainability reporting frameworks and literature (e.g., AA1000AS, GRI, SASB, integrated reporting). Plan: 1) Search for authoritative definitions of \"narrow\" vs \"broad\" stakeholder engagement in CSR. 2) Find sources that explicitly define \"narrow scope\" CSR (e.g., compliance-only, triple bottom line metrics) vs \"broad scope\" (stakeholder dialogue, materiality, governance, long-term value). 3) Retrieve evidence on who the reports address (shareholders vs wider stakeholder community), including practitioner frameworks (e.g., AA1000AS stakeholder engagement principle). 4) Cross-check with standards like GRI (comprehensive, wide stakeholder audience) and SASB (focused, investor-material), to map scope vs audience. First query: \"narrow and broad approaches to CSR reporting definition stakeholders scope\". </think> <call_tool name=\"google_search\" num=\"5\" gl=\"us\" hl=\"en\">narrow and broad approaches to CSR reporting definition stakeholders scope</call_tool>"
        },
        {
            "title": "Functional Calling",
            "content": "<tool_output><snippet id=d7607bb2-0> Title: Mandatory CSR and sustainability reporting: economic analysis and ... URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC8319709/ Snippet: Whether one chooses narrow or broad approach to CSR reporting depends onamong other thingsnormative views about the intended scope and target audience. </snippet> <snippet id=d7607bb2-1> Title: Stakeholders, stakeholder theory and Corporate Social ... URL: https://jcsr.springeropen.com/articles/10.1186/s40991-024-00094-y Snippet: This paper shows how stakeholder theory proactively moderates the strength of CSR in social interactions, environmental protection, and sustainable development. </snippet> <snippet id=d7607bb2-2> Title: Sustainability, corporate social responsibility, non-financial reporting ... URL: https://www.sciencedirect.com/science/article/pii/S2352550922003116 Snippet: CSR is corporate management approach that applies sustainability values in business to promote social welfare within company and outside it, employs ethical ... </snippet> ... Figure 35 Example trajectory of DR Tulu on ResearchQA Part I."
        },
        {
            "title": "Assistant",
            "content": "<think>Goal: Define and differentiate \"narrow\" vs \"broad\" approaches to CSR reporting by (a) scope of what is reported and (b) stakeholders addressed. Strategy: 1) Find authoritative definitions and contrasts (preferably from academic papers and reputable frameworks). 2) Use the first Google search results to identify key sources, then browse pages for precise language on scope and audience. 3) Corroborate with recognized reporting frameworks (AA1000AS stakeholder engagement principle; GRI vs SASB scope differences). 4) Ensure at least four searches: initial Google search done; next, targeted searches for AA1000AS, GRI/SASB comparison, and paper explicitly discussing narrow vs broad CSR reporting scope. Reflection on first search: - PMC article (S_d7607bb2-0) mentions that approach choice depends on intended scope and target audiencerelevant framing. - Sustainability Management Wiki (S_d7607bb2-3) claims narrow vs broad scope/target grouppotentially useful but need more authoritative backing. - Harvard Law corporate governance post (S_d7607bb2-5) discusses CSR report informing stakeholders about addressing their concernsuseful for audience/stakeholder dialogue. - NBER working paper (S_d7607bb2-6) gives an operational definition of narrowing scope to securities-law-required infouseful for narrow scope exemplar. What's missing: Explicit framework language (AA1000AS principle 1: stakeholder engagement as condition for materiality) and standard contrasts (GRI comprehensive vs SASB issue-focused/investor-oriented). Next query: \"AA1000AS stakeholder engagement principle 1 materiality definition\".</think> <call_tool name=\"google_search\" num=\"5\" gl=\"us\" hl=\"en\">AA1000AS stakeholder engagement principle 1 materiality definition</call_tool>"
        },
        {
            "title": "Functional Calling",
            "content": "<tool_output><snippet id=59a6bf6a-0> Title: Understanding the AA1000 Assurance Standard - LinkedIn URL: https://www.linkedin.com/pulse/understanding-aa1000-assurance-standard-enhancing-credibilityvn1tf Snippet: Materiality: Organizations must determine the relevance and significance of an issue to stakeholders and the business, ensuring that all ... </snippet> <snippet id=59a6bf6a-1> Title: AA1000 Stakeholder Engagement Standard - Accountability.org URL: https://accountability.org/standards/aa1000-stakeholder-engagement Snippet: The AA1000SES sets the global standard for conducting high-quality stakeholder engagement by offering simple, practical framework for empowering stakeholders. </snippet> <snippet id=59a6bf6a-2> Title: [PDF] AA1000 STAKEHOLDER ENGAGEMENT STANDARD 2011 URL: https://cdn2.hubspot.net/hubfs/2642721/Recursos/Guias%20y%20Estandares/AA1000/AA1000%20Stake holder%20engagement%20standard.pdf Snippet: Inclusivity, materiality and responsiveness require defined process of stakeholder engagement that provides comprehensive and balanced involvement and ... </snippet> ... Figure 36 Example trajectory of DR Tulu on ResearchQA Part II."
        },
        {
            "title": "Example Trajectory of DR Tulu on ResearchQA Part III",
            "content": "Assistant (Final Answer) <think> Plan: We need authoritative definitions and contrasts of \"narrow\" versus \"broad\" CSR reporting, specifically along (1) reporting scope and (2) stakeholders addressed. will: - Use the initial Google results to identify key academic framing (scope and audience). - Add AA1000AS to ground \"broad\" approach via stakeholder engagement and materiality. - Use GRI vs SASB comparisons to concretely illustrate broad (comprehensive, multi-stakeholder) versus narrow (focused, investor-material) scope and audiences. have already run three searches and have relevant snippets: peer-reviewed article noting that approach choice depends on intended scope and target audience; examples of narrowing scope to legally required information; and multiple GRISASB sources indicating GRI's broad scope and audience versus SASB's focused/investor materiality. will synthesize these, noting where evidence is explicit (scope and audience differences) and where it is inferential (mapping \"broad\" to AA1000AS stakeholder engagement/materiality). will then produce structured answer with citations on each claim. Remaining gaps: direct paper explicitly defining \"narrow\" vs \"broad\" CSR reporting terms would be ideal, but the PMC article and other sources suffice to explain the distinction in scope and audience. will proceed to synthesize. Structure for answer: - Definitions and framing: what \"narrow\" vs \"broad\" mean in scope and audience. - Scope differences: compliance/required info vs comprehensive material issues. - Stakeholders addressed: narrow (selected investors/regulators/creditors) vs broad (all materially affected stakeholders including communities, customers, employees, suppliers, NGOs). - Standards mapping: AA1000AS (stakeholder engagement/materiality), GRI (broad), SASB (narrow/investor materiality) as illustrative examples. </think> <answer> # Short answer - Scope: narrow CSR reporting approach confines disclosure to limited set of issues, often legally required or compliance-oriented; broad approach covers wider set of material issues identified through stakeholder engagement across economic, environmental, and social dimensions. <cite id=\"d7607bb2-6\">One way to narrow the scope of CSR reporting is to confine it to information required by securities law.</cite> <cite id=\"4a55f4d8-4\">GRI requires identifying material topics to establish the scope and included issues covered by company's report.</cite> <cite id=\"59a6bf6a-2\">AA1000's principles of inclusivity, materiality, and responsiveness require defined process of stakeholder engagement to determine what to report.</cite> - Stakeholders addressed: narrow approach targets limited audience (e.g., investors, regulators, or specific creditors) and signals compliance; broad approach addresses wider community of stakeholders (including employees, communities, customers, suppliers, NGOs) through dialogue and reporting on issues that materially affect them. <cite id=\"d7607bb2-0\">Whether one chooses narrow or broad approach depends on normative views about the intended scope and target audience.</cite> <cite id=\"d7607bb2-5\">CSR reports can inform stakeholders about how company has addressed their concerns, reflecting stakeholder engagement.</cite> <cite id=\"4a55f4d8-1\">GRI has larger scope and broader audience, while SASB enables deeper, sector-specific information for more focused audience.</cite> # What \"narrow\" CSR reporting entails Figure 37 Example trajectory of DR Tulu on ResearchQA Part III."
        },
        {
            "title": "Example Trajectory of DR Tulu on ResearchQA Part IV",
            "content": "Assistant (Final Answer) - Reporting scope: Restricted to small set of issues, typically where regulation or minimal standards dictate disclosure; this constrains the breadth of topics and the depth of information. <cite id=\"d7607bb2-6\">Confining CSR reporting to information required under securities laws is concrete way to narrow scope.</cite> - Stakeholders addressed: Primarily those with formal information rights or enforcement powers (e.g., investors, regulators, creditors) rather than broad stakeholder community; the report often signals compliance rather than engaging wider set of interest groups. <cite id=\"d7607bb2-0\">Choice of narrow approach is linked to narrower target audience.</cite> <cite id=\"d7607bb2-5\">A narrow CSR report addresses concerns raised by more limited set of stakeholders compared to broad report.</cite> # What \"broad\" CSR reporting entails - Reporting scope: Comprehensive coverage of material issues determined through inclusive stakeholder engagement and materiality assessments across environmental, social, and governance domains; this typically includes balanced, context-sensitive issues with depth where material. <cite id=\"59a6bf6a-2\">Materiality and inclusivity require defined stakeholder engagement process to guide what is included.</cite> <cite id=\"4a55f4d8-4\">Materiality is used to establish the report's scope and included issues.</cite> <cite id=\"4a55f4d8-6\">GRI Standards are designed to be used by organizations in any sector, implying broader coverage.</cite> - Stakeholders addressed: wide range of stakeholders who can affect or are affected by the organization, including employees, communities, customers, suppliers, NGOs, and regulators, with reporting intended to inform and facilitate dialogue. <cite id=\"d7607bb2-0\">A broad approach targets broader audience.</cite> <cite id=\"d7607bb2-5\">CSR reports in broad approach inform stakeholders about how their concerns have been addressed, reflecting engagement.</cite> <cite id=\"4a55f4d8-2\">GRI is designed to provide information to broad audience, while SASB dives deeper for more specific one.</cite> # How standards illustrate the difference - AA1000AS (Accountability Principles): Embeds materiality in stakeholder engagement (principles of inclusivity, materiality, and responsiveness), operationalizing broad approach that defines scope through dialogue with those affected and allocating limited resources to material topics. <cite id=\"59a6bf6a-2\">AA1000 emphasizes inclusivity, materiality, and responsiveness via structured stakeholder engagement.</cite> <cite id=\"59a6bf6a-8\">A material topic is one that will substantively influence assessments, decisions, actions, and performance of the organization and/or stakeholders.</cite> - GRI: Broad, global framework that covers wide range of sustainability topics and guides scope via materiality assessments, addressing diverse stakeholder groups. <cite id=\"4a55f4d8-6\">GRI Standards are sector-applicable and designed for broad use, covering wide range of sustainability topics.</cite> <cite id=\"4a55f4d8-4\">GRI requires identifying material topics to establish scope.</cite> <cite id=\"4a55f4d8-1\">GRI's scope is larger and intended for broader audience than SASB.</cite> - SASB: Narrower, investor-oriented framework that focuses on industry-specific, financially material issues, enabling deeper coverage for focused audience (primarily investors). <cite id=\"4a55f4d8-1\">SASB enables deeper, sector-specific information for more focused audience than GRI.</cite> <cite id=\"4a55f4d8-8\">SASB is focused on financially material ESG issues in specific industries, i.e., narrower scope and audience.</cite> ... (Response truncated for brevity; see our full example in our static demo.) Figure 38 Example trajectory of DR Tulu on ResearchQA Part IV."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "Carnegie Mellon University",
        "Massachusetts Institute of Technology",
        "Seattle Childrens Hospital",
        "University of California, Berkeley",
        "University of Washington"
    ]
}