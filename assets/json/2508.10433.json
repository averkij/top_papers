{
    "paper_title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning",
    "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Peiqing Yang",
        "Yanzi Wang",
        "Xiaowan Wang",
        "Enhui Wan",
        "Sitong Zhou",
        "Guanting Dong",
        "Yuchen Zeng",
        "Yida Xu",
        "Jie Wang",
        "Chong Sun",
        "Chen Li",
        "Honggang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 3 4 0 1 . 8 0 5 2 : r WE-MATH 2.0: Versatile MathBook System for Incentivizing Visual Mathematical Reasoning Runqi Qiao1, Qiuna Tan1, Peiqing Yang1, Yanzi Wang3, Xiaowan Wang1, Enhui Wan1, Sitong Zhou1, Guanting Dong1, Yuchen Zeng1, Yida Xu1, Jie Wang1, Chong Sun2, Chen Li2, Honggang Zhang1 1BUPT qrq@bupt.edu.cn 2WeChat Vision, Tencent Inc. 3Tsinghua University qiunatan@bupt.edu.cn chaselli@tencent.com https://we-math2.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledgedriven design and model-centric data space modeling. In this paper, we introduce WE-MATH 2.0, unified system that integrates structured mathematical knowledge system, model-centric data space modeling, and reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, challenging dataset for robust training. (3) MathBook-RL: We propose two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging averagereward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning."
        },
        {
            "title": "Introduction",
            "content": "Large Language models (LLMs) have demonstrated remarkable capabilities across wide range of tasks [1, 9, 22, 62, 54, 60, 24, 56, 53, 63, 25, 49, 69]. Building on this foundation, Multimodal Large Language Models (MLLMs) have shown impressive performance in visual question answering (VQA) [3, 74, 18, 41, 44], optical character recognition (OCR) [68, 67, 30, 61], and object detection [31, 28, 45]. However, MLLMs still face difficulties with complex reasoning tasks, Equal contribution. Work done as interns at WeChat, Tencent Inc. Corresponding author. zhhg@bupt.edu.cn Preprint. Under review. Table 1: Comparison between MathBook and several multimodal mathematical datasets. To achieve comprehensive coverage of mathematical knowledge and ensure that each data sample is retrievable with precise knowledge labels, all data are manually constructed using GeoGebra. Dataset Usage Image Source Data Annotation Knowledge-level Annotation Principle-level Annotation Difficulty Levels Geometry3K [33] Training Data & Benchmark MathV360K [47] We-Math [43] GeoSense [64] Training Data Benchmark Benchmark Collection Collection Manually Created Collection Manual Collection Manual Manual MathBook (Ours) Training Data & Benchmark Manually Created Manual - - 67 148 491 - - - - 4 - - 8 particularly in visual mathematical problem-solving, where generalization remains fundamental challeng [32, 71, 57]. Recent efforts to enhance mathematical reasoning in MLLMs have primarily focused on three directions: dataset construction [33, 72, 47, 59], preference optimization [75, 35], and reinforcement learning (RL) [21, 36]. Foundation approaches aggregated datasets from diverse mathematical domains [48]. Subsequent efforts introduce structured supervision formats (e.g., Chain-of-Thought (CoT)) combined with preference optimization to guide step-by-step reasoning [76]. More recently, RL-based studies with curriculum-based training have been employed to further improve model performance on complex reasoning tasks [21, 5, 65, 55]. Despite this progress, several fundamental challenges remain (in Table 1): (1) Lack of comprehensive knowledge system: Existing MLLMs show uneven performance across different subfields of math reasoning. [32, 57] Unfortunately, current datasets suffer from limited coverage of knowledge points and domain diversity, underscoring the necessity of establishing more systematic knowledge system. (2) Lack of model-centric difficulty modeling: Existing multimodal training datasets primarily perform difficulty annotation based on human learning stages [36]. However, recent studies [26, 42, 32] reveal that MLLMs do not exhibit learning patterns that align well with these human-defined levels. This highlights the need for more model-centric approach to modeling data difficulty. (3) Lack of emphasis on reasoning generalization: MLLMs are capable of solving complex problems, but perform poorly on corresponding subproblems [42] as well as on similar, same-type tasks [77]. This underscores the current training methods focus on problem memorization rather than fostering reasoning generalization. To address these limitations, we introduce WE-MATH 2.0, versatile framework that combines structured mathematical knowledge system, model-centric data space modeling, and reinforcement learning-based training paradigm to comprehensively improve MLLMs reasoning capabilities. In detail, we begin by establishing the MathBook Knowledge System, five-level hierarchy comprising 491 knowledge points and 1,819 fundamental principles (see Figure 1). This structure is systematically derived from sources such as Wikipedia and open-source textbooks, refined through hierarchical clustering, and further revised by human experts. Building on this foundation, we introduce MathBook-Standard, dataset featuring comprehensive annotations at the level of 1,819 knowledge principles, along with carefully curated problems to ensure broad, balanced coverage, particularly in underrepresented mathematical domains. To foster deeper conceptual understanding, MathBook-Standard employs dual expansions: \"multi-images per question\" and \"multi-questions per image\", enabling diverse problem sets that achieve conceptual flexibility. Crucially, we propose pivotal three-dimensional difficulty modeling framework that redefines mathematical problem construction. By explicitly modeling \"step complexity\", \"visual complexity\" and \"contextual complexity\", each problem is systematically expanded into seven difficulty levels to form MathBook-Pro. This design enables structured, progressive learning for MLLMs, laying strong foundation for improved reasoning across difficulty levels. Notably, all images in our training data are meticulously handcrafted using GeoGebra software4. This approach ensures that our images are not only newly created and precise, but also demonstrate 4https://www.geogebra.org/ 2 level of rigor and complexity in image construction that surpasses commonly used Python-based rendering methods. To further improve the general mathematical reasoning capabilities of MLLMs, we propose MathBook-RL, two-stage reinforcement learning framework for progressive and robust mathematical reasoning: (1) Cold-Start Fine-tuning: We first adopt supervised fine-tuning that guides the MLLM to learn knowledge-oriented CoT reasoning, internalizing it to acquire conceptual understanding and structured problem-solving paradigms. (2) Progressive Alignment RL: We propose curriculumbased RL paradigm. Leveraging the \"one-question-multi-image\" and knowledge-point features in MathBook-Standard, we first align the models analogical reasoning by introducing an average reward mechanism. Building on this foundation, we progressively train the MLLM on MathBook-Pro and further introduce two dynamic scheduling strategies: i) Knowledge Increment Scheduling: When errors occur due to complex reasoning steps, the model is adaptively redirected to relevant incremental-step samples in MathBook-Standard. ii) Modality Increment Scheduling: When errors stem from increased modality complexity, the model is guided through single-modality incremental problems. This targeted curriculum enables effective knowledge transfer across difficulty levels. To comprehensively evaluate MLLMs reasoning capability, we introduce MathBookEval, benchmark covering all 491 knowledge points with diverse step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely used benchmarks, showing substantial improvements in generalization and robustness. In summary, our main contributions are as follows: We propose the MathBook Knowledge System, five-level hierarchical framework with 491 knowledge points and 1,819 fundamental principles, enabling systematic and comprehensive mathematical knowledge supervision. We develop MathBook-Standard and MathBook-Pro, two novel datasets that combine comprehensive step-wise annotation, dual expansions for conceptual flexibility, and principled three-dimensional difficulty modeling framework for structured and progressive learning. We introduce MathBook-RL, two-stage RL framework that integrates structured knowledge supervision and dynamic data scheduling, improving the reasoning capabilities of MLLMs. We present MathBookEval, benchmark designed to comprehensively evaluate model reasoning across diverse knowledge points and step distributions. Extensive experiments demonstrate that our approach achieves remarkable performance in both generalization and robustness. All images in our training set are manually and meticulously crafted using GeoGebra software, ensuring they are newly created, precise, and surpass common Python-based rendering methods in spatial geometric rigor and complexity."
        },
        {
            "title": "2 Related Work",
            "content": "Visual Mathematical Reasoning. Recently, research on visual mathematical reasoning has made notable progress [47, 72, 75, 19, 35, 13]. For benchmarks, MathVista [32] and MathVision [57] evaluate overall model performance, while MathVerse [71], We-Math [42], and Dynamath [77] focus on analyzing reasoning mechanisms and offer directions for optimization. Specifically, MathVerse finds that models benefit from richer textual information. Dynamath investigates model robustness by evaluating performance on near-duplicate questions. Methodologically, substantial efforts have been devoted to improving the alignment between visual and textual modalities, as demonstrated by Math-LLaVA [47], MAVIS [72] and MathCoder-VL [59]. In addition, Math-PUMA [75] and URSA [35] introduce step-wise reasoning processes to better mimic human problem solving. Furthermore, with the growing interest in RL-based approaches, series of recent works explore reward optimization as means to further enhance visual reasoning abilities [21, 70, 36, 5, 29, 2, 55, 73, 65, 50, 20, 52]. Notably, these RL-based methods have shown promising improvements by explicitly optimizing model behavior for complex reasoning tasks. Building upon these encouraging results, we contribute an alternative perspective by developing systematic, model-centric knowledge framework, combining it with RL-based alignment training, 3 and releasing new dataset. We hope these efforts will provide the community with new insights and resources to support ongoing advancements in visual mathematical reasoning."
        },
        {
            "title": "3 WE-MATH 2.0",
            "content": "Overview. In this section, we introduce WE-MATH 2.0, unified system designed to advance visual mathematical reasoning in MLLMs. WE-MATH 2.0 is developed from three key aspects: We construct five-level MathBook Knowledge System (3.1), systematically organizing 491 knowledge points and 1,819 fundamental principles for comprehensive mathematical supervision. We propose Multi-Dimensional data construction pipeline (3.2), incorporating seed problem construction, variant expansion, and principled three-dimensional difficulty modeling. We introduce MathBookEval (3.3), benchmark aligned with our knowledge system for systematic evaluation. 3.1 MathBook Knowledge System System Overview. We construct five-level hierarchical MathBook Knowledge System organized by the DefinitionTheoremApplication paradigm [15]. The core is set of knowledge points = {k1, k2, . . . , kN }, = 491, spanning primary to university mathematics. Each ki is associated with set of fundamental principles Pi = {pi1, . . . , pimi}, mi [1, 7], where = (cid:83)N i=1 Pi and = 1,819. These principles systematically cover definitions, theorems, and applications. (Concrete examples of knowledge points and fundamental principles can be found on our project page.) Hierarchical Construction via Human-AI Collaboration. We construct through hybrid process. Human experts first design an initial structure Khuman based on authoritative sources, including textbooks, Wikipedia, and national curriculum standards. In parallel, we sample 30K problems from the existing math dataset [34, 23, 16, 47, 39, 35, 75, 72], merging them into unified dataset. We then use GPT-4o [37] to assign multi-level topic tags = {t1, . . . , tn}, followed by hierarchical clustering on the semantic similarity matrix Rnn to obtain an AI-generated structure Kauto. The final knowledge point set is produced by expert-guided integration of Khuman and Kauto, with independent review for quality assurance. Fine-Grained Principle Annotation. Given the constructed K, we employ GPT-4o to annotate the step-level knowledge points for each problem qj = {q1, . . . , qM } by mapping each step in its chain-of-thought solution to the corresponding ki K. This yields mapping M1 : qj (cid:55) (ki1, ki2, . . . ), forming set of step-level solution paths for each knowledge point. Next, for each ki, GPT-4o summarizes the set of theorems and principles used across all associated solution paths, resulting in mapping M2 : ki (cid:55) {pi1, . . . , pimi}. Finally, these AI-extracted principles are consolidated and cross-checked with those written by human experts, with iterative refinement to ensure completeness and accuracy of P. Detailed guideline of our system are listed in Appendix. 3.2 Multi-Dimensional Data Construction In this section, we introduce the data construction pipeline of our versatile datasets: MathBookStandard and MathBook-Pro. 3.2.1 MathBook-Standard: Seed and Variant Problem Construction Seed Problem Construction. To ensure rich coverage and high-quality design, we construct problems based on the knowledge system following 3 guidelines: (1) All diagrams are rendered with GeoGebra for precise geometric representation; (2) Problems focus on math essence, avoiding reliance on superficial visual cues; (3) Each problem strictly corresponds to its designated principle set Pi. To achieve these, we adopt model-assisted, expert-led workflow. Given knowledge point ki and its associated principle set Pi, an LLM first generates draft problem, including the question, 4 Figure 1: Overview diagram of MathBook, including examples of knowledge points, fundamental principles, and sample problems. answer, and XML script. We then use GeoGebra, software that renders diagrams from XML-based scripts, to automate the generation of draft images: GLM(ki, pij) (qdraft ). The resulting visual drafts serve as references to guide human experts in constructing problems and diagrams via GeoGebra scripting. In practice, almost all drafts were revised or reworked by experts,5 in order to avoid reliance on superficial visual cues and ensure proper alignment with the underlying mathematical principles. The final seed problem set is Dseed = {(ki, pij, qi, ai, Ii, xxml )}, covering all knowledge points and principles. The detailed GeoGebra-based diagram generation guidelines can be found in the Appendix C.2.1. , xxml draft , adraft i Variant Problem Expansion. To further enhance the diversity and generalization ability of the dataset, we systematically construct two types of variants based on each seed problem: (1) One-Problem-Multi-Image Variants (DImgVar): Given seed problem (qi, ai, Ii) Dseed, we fix the problem statement qi and knowledge annotation (ki, pij), and generate set of images {I (1) } by varying the parameters in GeoGebra while maintaining the underlying geometric construction. Each image corresponds to different geometric instantiation (e.g., acute/obtuse/right triangle), resulting in distinct answers a(t) = 1, . . . , m}. This approach enriches the visual data diversity while preserving semantic consistency. ) DImgVar, : {(qi, a(t) , . . . , (m) , (2) , (t) (2) One-Image-Multi-Problem Variants (DQstVar): Given seed image Ii, we construct multiple new problems q(s) and principles p(s) ij , curated by experts with language model assistance: {((q(s) = 1, . . . , n} This strategy leverages the reusability of high-quality diagrams to generate diverse problem variants. targeting different knowledge points k(s) , Ii) DQstVar, , a(s) By systematically applying these variant construction methods to each seed problem, we build the MathBook-Standard dataset with rich semantic and visual diversity. 3.2.2 MathBook-Pro: Three-Dimensional Difficulty Modeling To systematically characterize problem complexity from model-centric perspective, we define three-dimensional difficulty space for each seed problem along three orthogonal axes: (1) Step Complexity (ϕs): Knowledge-oriented reasoning depth is quantified by the number of involved knowledge points, which are sourced from the MathBook knowledge system. Given 5Only 1.2% of the drafts were directly adopted by experts. 5 Figure 2: Overview of MathBook dataset and the corresponding training phase. seed problem with process-oriented knowledge points, we construct variants that require at least six, satisfying > l. (2) Visual Complexity (ϕv): We increase complexity by adding auxiliary elements (e.g., lines) to the original image via GeoGebra, while preserving the core structure. (3) Contextual Complexity (ϕc): Captures the contextualization of the problem statement. We vary the textual context from concise mathematical descriptions to complex linguistic scenarios. Each seed problem (q0, a0, I0) Dseed serves as the origin in structured difficulty space. To enable controlled and interpretable expansion, we generate derived problems by varying single dimension {ϕs, ϕv, ϕc} at time, yielding variants (q(d) , a(d) ). Through multiple rounds of such singlei axis transformations, we progressively construct more complex problems by composing changes across multiple dimensions. Formally, the most advanced variant takes the form: (q, a, ) = ϕs ϕv ϕc(q0, a0, I0). In MathBook-Pro, the expansion along each dimension is implemented as: , (d) (1) Along the ϕs dimension, we introduce intermediate conclusions as new conditions, enabling knowledge-driven, progressive deepening of reasoning, expressed as Ki+1 = Ki + 1, where Ki denotes the number of knowledge points involved at step i. In MathBook-Pro, the most complex step variants involve at least 6 knowledge points. (2) Along the ϕv, we increase visual complexity by adding auxiliary lines, altering geometric configurations or introducing new spatial constructs via GeoGebra, while preserving the core structure. (3) Along ϕc, we embed the mathematical core into real-world contexts or linguistically abstract scenarios, increasing the semantic and contextual demands of the problem statement. By expanding along the defined dimensions, we generate set of difficulty-controlled problem variants for each knowledge point, forming the difficulty modeling subset Ddifficulty of MathBook-Pro. Detailed dataset statistics are presented in Figure 2. 3.3 MathBookEval Design Principles. To ensure the quality and interpretability of annotations in visual math reasoning tasks, MathBookEval is designed based on the following principles: (1) Comprehensive Knowledge Coverage: Problems involve 491 knowledge points, spanning primary to university level, demonstrating broad coverage. (2) Multi-level Reasoning Depth: Each problem integrates 110 knowledge points, compared to 13 (Level 1) in existing process-oriented benchmarks, as illustrated in Figure 24. 6 Notably, our annotation adheres to three principles: (1) integrating public and newly constructed problems under unified guideline; (2) expert step-by-step annotation with explicit knowledge-point mapping; and (3) independent cross-validation, retaining only consistently annotated items. Data Statistics and Evaluation Protocol. MathBookEval contains 1,000 fully annotated problems, covering all 491 knowledge points in the unified knowledge system K, with 600 problems collected from existing benchmarks and 400 newly curated (Detailed statistics are presented in Table 6). We provide detailed statistics and splits along two key dimensions: (1) Reasoning Dimension: Problems are divided by reasoning steps into three levels: 1-3 (Level 1), 4-6 (Level 2), and 7-10 (Level 3), reflecting different reasoning depths. (2) Knowledge Dimension: The 491 knowledge points are grouped into 4 domains and 13 subdomains, covering primary to university level. Figure 24 demonstrates superior coverage of knowledge points and reasoning depth. All problems are in multiple-choice or fill-in-the-blank format."
        },
        {
            "title": "4 Methodology",
            "content": "In this section, we introduce MathBook-RL, two-stage framework that progressively guides MLLMs to develop reasoning capabilities from easy to hard. The first stage is cold-start fine-tuning phase that establishes knowledge-driven reasoning paradigm (4.1); the second is dynamic reinforcement learning phase that enhances the models generalization ability (4.2). 4.1 Cold-Start Fine-tuning The cold-start supervised fine-tuning (SFT) stage aims to instill explicit awareness of knowledge system and knowledge-driven reasoning paradigm, avoiding rote memorization. The initial training set Dinit is built from MathBook-Standard, which fully covers all 491 knowledge points. To improve rationale interpretability, we use GPT-4o [37] to rewrite each sample with natural language explanations that explicitly reference the relevant knowledge. The model is then trained using standard supervised fine-tuning: LSFT(θ) = E(x,y)Dinit [ log Pθ(y x)]. This stage enhances the models ability to internalize the knowledge system and follow knowledge-guided reasoning chains. 4.2 Progressive Alignment Reinforcement Learning (1) Pre-aligned RL. Prior to the dynamic scheduling stage, we perform initial RL training on MathBook-Standard dataset to ensure that the model develops genuine understanding of mathematical knowledge. Specifically, we utilize the DImgVar subset, where each group contains multiple variants of the same knowledge principle: (qi, a(t) ) DImgVar, = 1, . . . , m. To encourage consistent and robust performance across different formulations, we adopt mean-based reward function: = 1 t=1 r(t), where r(t) = 0.9 if the answer is correct, 0.1 if only the format is correct, and 0 otherwise. Specifically, for problems corresponding to the same knowledge principle, rollout rewards are first sorted within each problem. Next, the mean reward at each sorted position is calculated across these problems and subsequently employed in the calculation of Ai. Instead of focusing on individual problems, this design integrates rewards across all problems corresponding to the same knowledge principle, thereby providing more comprehensive critic. , (t) (cid:80)m (2) Dynamic Scheduling RL. In this section, we introduce dynamic RL algorithm based on MathBook-Pro. The training process is organized as curriculum along main trajectory of increasing difficulty, primarily centered on the knowledge dimension. For each base problem (q0, a0, I0), denoted as x0, we construct sequence of increasingly challenging variants as follows: x0 ϕs(x0) ϕs ϕv(x0) ϕs ϕc(x0) ϕs ϕv ϕc(x0) (1) where ϕs denotes increasing the number of knowledge points, ϕv and ϕc denotes increasing visual complexity and contextual abstraction. This forms progressive path from basic to advanced reasoning for each knowledge anchor. Incremental Learning Mechanism. At each curriculum transition ϕ(x), if the model fails on ϕ(x) after succeeding on x, we introduce an incremental learning step. Specifically, we define the incremental set (x, ϕ) as collection of samples that isolate the new knowledge or modality 7 introduced by ϕ. The model is first trained on (x, ϕ) to address the incremental challenge, then reattempts ϕ(x). Concretely: Knowledge Increment Scheduling: For x0 ϕs(x0), if the model fails on ϕs(x0), we construct (x0, ϕs), comprising auxiliary problems 0 that target the new knowledge point(s) from ϕs. Modality Increment Scheduling: For ϕs(x0) ϕs ϕv(x0) (or ϕs ϕc(x0)), if the model fails on the more complex sample, we construct (ϕs(x0), ϕv) (or (ϕs(x0), ϕc)), which contains samples isolating the new visual or contextual complexity. This incremental adaptation, denoted by (x, ϕ) at each step, ensures that the model can efficiently bridge the gap between curriculum stages. Notably, our overall RL objective is optimized using Group Relative Policy Optimization (GRPO) [46], which extends PPO by estimating the baseline from group scores instead of separate critic. The GRPO objective is: (θ) = E[q (Q), {oi}G i=1 πθold (Oq)] (cid:26) min (cid:20) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) ˆAi,t, clip oi (cid:88) t=1 (cid:88) 1 1 oi (cid:18) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) i= (cid:19) , 1 ϵ, 1 + ϵ (cid:21) ˆAi,t βDKL [πθπref ] (2) (cid:27) , where ϵ and β are hyperparameters, denotes the input, {oi}G corresponding reward. ˆAi,t is the normalized advantage value for the i-th trajectory in the group. This curriculum-driven RL process, augmented with explicit incremental adaptation at each stage, enables the MLLM to progressively master complex, multi-dimensional reasoning tasks while preserving stability and generalization across knowledge, visual, and contextual variations. i=1 are sampled outputs, and ri is the"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Datasets. All training data are sourced from WE-MATH 2.0 in compliance with copyright and licensing requirements, and all expert-constructed problems will be released under appropriate CC licenses. We use 1K, 5.8K and 4K samples for SFT, pre-aligned RL, and dynamic scheduling RL stages, respectively. Experiments are conducted on four standard mathematical reasoning benchmarks: MathVista [32], MathVision [58], MathVerse [71], and We-Math [43]. Baselines. We conduct our training based on Qwen2.5-VL-7B [3] and compare our method with three categories of baselines: (1) Closed-source models (e.g., GPT-4o [37]); (2) Open-source general models (e.g., InternVL2.5 series [6], Qwen2.5-VL series [3]); (3) Open-source reasoning models (e.g., R1-VL [70]). Our evaluation is based on VLMEvalKit [14], with some modifications to the API-calling components due to network constraints. Details of baselines and evaluation metrics are listed in supplementary. 5.2 Main Results Table 2 displays the performance of our MathBook-7B across various mathematical reasoning benchmarks. Overall, our method achieves remarkable performance in all benchmarks, clearly demonstrating its superiority. Further analysis reveals the following observations: (1) Overall superiority of MathBook. Compared to the backbone model Qwen2.5-VL-7B, MathBook-7B achieves over 5% improvement across all benchmarks, validating the effectiveness of our approach. (2) Effectiveness of progressive alignment reinforcement learning on knowledge generalization. Focusing on the We-Math benchmark, which requires models to correctly solve both complex multistep questions and their corresponding subproblems, MathBook-7B outperforms strong RL-based baselines. This demonstrates the effectiveness of progressive alignment reinforcement learning for improving knowledge generalization. (3) Less is More: Efficiency with limited training data. Notably, MathBook-7B achieves consistently strong performance using only 9.8K training samples. We attribute this to the high-quality, 8 Table 2: Performance comparison across four widely-used mathematical reasoning benchmarks. Each benchmark follows its standard evaluation metric: MathVista and MathVision use accuracy, We-Math reports the strict score, and MathVerse evaluates on the vision-only subset with accuracy. Data sizes used for SFT and RL are annotated in blue and red, respectively. Model #Data Avg. MathVista MathVision We-Math MathVerse GPT-4o-latest [37] Gemini-1.5-Pro [51] Qwen2.5-VL-7B [3] InternVL2.5-8B-BoN-8 [6] Math-PUMA-7B [76] URSA-8B [35] R1-OneVision-7B [66] R1-VL-7B [70] MM-Eureka-7B [36] WeThink-7B [65] VLAA-Thinker-7B[5] OpenVLThinker-7B [10] MathBook-7B (Ours) - - - - Closed-source 54.0 53.6 71.6 67.9 Open-source (General) 42.6 41.7 68.2 68. Open-source (Reasoning) 1.88M 2.96M 155K+10K 260K+10K 15K - 37.8 - - 45.2 120K+20K 47.5 46.0 - 48.7 25K 35K+15K 1K+9.8K 47.9 58.8 64.1 63.5 73.0 71.6 68.0 72.3 73.0 43.8 25.1 25.6 - 28.7 29.9 24.7 26.9 26.0 26.4 25.9 28.0 50.6 50.5 36.0 38.6 19.2 32.8 30.1 22.7 34.5 48.0 41.5 - 48.4 49.9 54. 41.1 34.5 26.0 31.0 - - 46.2 44.2 48.2 - 45.2 structured mathematical knowledge system we constructed, which enables efficient alignment and generalization even with limited data. 5.3 Results on MathBookEval To investigate the abilities of existing MLLMs in terms of reasoning depth and knowledge coverage breadth, we conduct MathBookEval evaluation and identify the following findings (see Table 5): Table 3: Results of the ablation study. MVt: MathVista; MVs: MathVision; WM: We-Math) Method SFT RL-Pre RL-Dyn MVt MVs WM M0 73.0 28.0 48.4 (1) MLLMs performance negatively correlates with the number of required knowledge points. As the \"reasoning step dimension\" increases, model accuracy steadily declines. In particular, problems requiring 710 knowledge points result in the lowest performance for most MLLMs (below 50%). These findings underscore the ongoing challenge of multi-step reasoning, further validating the number of knowledge points as robust indicator of problem difficulty. 72.4 27.0 47.2 72.0 26.3 43.3 71.5 26.3 46.7 65.8 25.7 38.3 M1 M2 M3 M4 - - - - - (2) MLLMs perform well on algebra but struggle with geometry. Along the knowledge dimension, most MLLMs demonstrate strong performance in algebra, achieving accuracies above 50%. However, their consistently poor performance in geometry highlights ongoing challenges in spatial reasoning. (3) Larger models yield more consistent improvements across all dimensions. Within the InternVL2.5 and Qwen2.5-VL families, increasing model size leads to consistent gains across all dimensions and in overall scores, emphasizing the role of scale in enhancing reasoning capabilities. 5.4 Quantitative Analysis Ablation Study. As shown in Table 3, we conduct ablation studies on the training stages. M0 denotes MathBook-7B, while M1M4 represent models at different training stages (RL-Pre: Prealigned RL; RL-Dyn: Dynamic Scheduling RL). We lead to following two key findings: (i) Both RL stages contribute significantly. Each RL stage (M0-M3) yields progressive improvements over M4. In particular, pre-aligned reinforcement learning (RL) in the first stage yields 9 Table 5: The performance of different MLLMs on MathBookEval for reasoning evaluation. Acc.: Accuracy; FS.: Foundational skills; PS.: Probability and statistics; Geo.: Geometry; Alg.: Algbra Models Acc. Reasoning Knowledge Level1 Level Level3 FS. PS. Geo. Alg. Closed-source MLLMs GPT-4o GPT-4V InternVL2.5-78B Qwen2.5-VL-72B LLaVA-OneVision-72B InternVL2.5-8B Qwen2.5-VL-7B Qwen2.5-VL-3B LLaVA-OneVision-7B GLM-4V-9B MathBook-7B 50.8 42.8 51.8 57.1 43.0 37.9 46.7 36.9 31.6 22.2 50.4 52.8 44.0 48.9 43. 41.7 31.9 33.8 36.8 57.6 56.6 44.2 33.5 67.2 59.4 Open-source MLLMs 52.5 58.3 44.8 40.7 50.1 38.7 34.3 23.7 52.0 51.8 56.4 42.0 34.5 43.0 34.2 28.0 20.5 48.2 45.8 50.0 31.9 27.8 33.3 33.3 23.6 16.7 45.8 50.0 52.9 38.2 33.8 44.1 35.3 36.8 26.5 57.4 64.2 58.5 52.8 46.2 58.5 49.1 41.5 23.6 67.9 42.6 52.1 37.0 31.4 38.8 29.1 24.9 18.4 40. 67.6 68.8 53.5 50.0 60.2 49.6 41.0 28.9 63.3 impressive results on MathVista and We-Math benchmarks, highlighting the crucial role of knowledge learning in enhancing mathematical reasoning abilities. (ii) SFT alone offers limited gains, but is crucial for unlocking RL potential. Comparing M0, M3, M4, we find that SFT alone yields marginal improvements over the Qwen2.5-VL backbone. However, when combined with RL, SFT version substantially boosts overall performance, highlighting its critical role in shifting the models reasoning paradigm and enabling more effective RL optimization. Analysis of SFT Data Paradigm and Scale. In this section, we explore the impact of both data paradigm and data scale during the SFT stage. Based on the M0 setting, we consider two variants: (1) Replacing the natural language CoT format with structured, step-wise CoT format [75] aligned with K; (2) Increasing the SFT data scale with larger datasets (from 1K to 10K). We identify following findings. Table 4: Results of the analysis experiments. SFT (Str.) refers to structured SFT training, while SFT (Lar.) indicates training with larger dataset. WM MVt Setting MVs M0 SFT(Str.) SFT(Lar.) 73.0 71.9 72.1 28.0 26.0 27. 48.4 46.7 47.8 (i) Natural language CoT outperforms the structured step-wise format in SFT. As evidenced in Table 4, the structured SFT format results in lower RL performance relative to natural language CoT. This highlights the advantage of natural prompts in cultivating flexible reasoning, which in turn strengthens visual mathematical reasoning skills. (ii) Minimal SFT suffices to unlock RL potential. Scaling up SFT data does not consistently lead to better performancemodels trained on minimal, well-curated SFT data perform comparably or even better than those trained on larger datasets. This suggests that small, high-quality SFT set is sufficient to establish the reasoning paradigm required for effective RL."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present WE-MATH 2.0, unified framework for multi-modal mathematical reasoning. It comprises: (1) MathBook Knowledge System, five-level hierarchy covering 491 knowledge points and 1,819 fundamental principles for comprehensive supervision; (2) MathBook-Standard and MathBook-Pro, two richly annotated datasets with conceptual expansions and principled difficulty modeling for structured learning; (3) MathBook-RL, two-stage reinforcement learning framework that leverages knowledge-guided supervision and dynamic data scheduling; and (4) MathBookEval, benchmark for evaluating reasoning across diverse knowledge and step distributions. Extensive experiments validate MathBooks effectiveness in enhancing generalization of MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Inclusion AI, Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, et al. M2-reasoning: Empowering mllms with unified general and spatial reasoning. arXiv preprint arXiv:2507.08306, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. CoRR, abs/2412.05271, 2024. [7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [9] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [10] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. [11] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025. [12] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. [13] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen. Progressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024. [14] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 11 [15] Richard Fitzpatrick. Euclids elements of geometry. 2008. [16] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [17] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [18] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [19] Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. arXiv preprint arXiv:2409.12568, 2024. [20] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. [21] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [22] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [23] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary In Proceedings of the IEEE conference on computer vision and pattern visual reasoning. recognition, pages 29012910, 2017. [24] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Stepdpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. [25] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Runqi Qiao, and Sirui Wang. Instructerc: Reforming emotion recognition in conversation with multi-task retrieval-augmented large language models. arXiv preprint arXiv:2309.11911, 2023. [26] Zhikai Lei, Tianyi Liang, Hanglei Hu, Jin Zhang, Yunhua Zhou, Yunfan Shao, Linyang Li, Chenchui Li, Changbo Wang, Hang Yan, et al. Gaokao-eval: Does high scores truly reflect strong capabilities in llms? arXiv preprint arXiv:2412.10056, 2024. [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [29] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [30] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024. 12 [31] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [32] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [33] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [34] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [35] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. [36] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [37] OpenAI. Hello gpt-4o, 2024. [38] OpenAI. Gpt-4v (ision) system card. Citekey: gptvision, 2023. [39] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. [40] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [41] Runqi Qiao, Qiuna Tan, Guanting Dong, MinhuiWu MinhuiWu, Jiapeng Wang, YiFan Zhang, Zhuoma GongQue, Chong Sun, Yida Xu, Yadong Xue, Ye Tian, Zhimin Bao, Lan Yang, Chen Li, and Honggang Zhang. V-oracle: Making progressive reasoning in deciphering oracle bones for you and me. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2012420150, Vienna, Austria, July 2025. Association for Computational Linguistics. [42] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [43] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. [44] Runqi Qiao, Lan Yang, Kaiyue Pang, and Honggang Zhang. Making visual sense of oracle bones for you and me. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1265612665, June 2024. [45] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. 13 [46] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [47] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-LLaVA: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 46634680, November 2024. [48] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [49] Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, et al. Cs-bench: comprehensive benchmark for large language models towards computer science mastery. arXiv preprint arXiv:2406.08587, 2024. [50] Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [52] Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. [53] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. [54] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [55] Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflectionaware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. [56] Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference on Machine Learning, 2024. [57] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [58] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. [59] Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, et al. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning. arXiv preprint arXiv:2505.10557, 2025. 14 [60] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. [61] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. 2024. [62] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [63] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. [64] Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, et al. Geosense: Evaluating identification and application of geometric principles in multimodal reasoning. arXiv preprint arXiv:2504.12597, 2025. [65] Jie Yang, Feipeng Ma, Zitian Wang, Dacheng Yin, Kang Rong, Fengyun Rao, and Ruimao Zhang. Wethink: Toward general-purpose vision-language reasoning via reinforcement learning. arXiv preprint arXiv:2506.07905, 2025. [66] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [67] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. [68] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. [69] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [70] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [71] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [72] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. [73] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 15 [74] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [75] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024. [76] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2618326191, 2025. [77] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 WE-MATH 2."
        },
        {
            "title": "3.3 MathBookEval",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Methodology 4.1 Cold-Start Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Progressive Alignment Reinforcement Learning . . . . . . . . . . . . . . . . . . . 5 Experiments 5.1 Experimental Setup . 5.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Results on MathBookEval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Quantitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion Broaden Impact Ethical Considerations Details of WE-MATH 2.0 C.1 MathBook Knowledge System . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.1 Hierarchical Structure of Knowledge Points . . . . . . . . . . . . . . . . . C.1.2 Knowledge Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 MathBook-Standard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 GeoGebra-based Diagram Generation. . . . . . . . . . . . . . . . . . . . . C.2.2 Dataset Diversity and Variant Construction. . . . . . . . . . . . . . . . . . C.3 MathBook-Pro . C.4 MathBookEval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4.1 Dataset Construction and Annotation Protocol. . . . . . . . . . . . . . . . C.4.2 Task Dimensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4.3 Dataset Statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4.4 Evaluation Protocol and Metrics. . . . . . . . . . . . . . . . . . . . . . . . C.4.5 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 3 4 4 4 5 6 7 7 7 8 8 9 9 10 18 19 19 19 24 26 26 34 35 35 35 36 37 C.4.6 Additional Results on MathBookEval . . . . . . . . . . . . . . . . . . . . More Details on MathBook-RL D.1 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Case Study . . . . D.3 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3.1 Details of the Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3.2 Details of the Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 38 38 39"
        },
        {
            "title": "A Broaden Impact",
            "content": "Towards principled and generalizable mathematical model training. WE-MATH 2.0 provides comprehensive and structured mathematical knowledge system, which fills the gap left by previous works that lack complete and systematic framework. By introducing five-level hierarchy with 491 knowledge points and 1,819 fundamental principles, MathBook enables more principled and interpretable mathematical learning for MLLMs. The dual expansion strategy (\"multi-images per question\" and \"multi-questions per image\") and the three-dimensional difficulty modeling not only enrich the diversity of training data but also facilitate robust and progressive learning. This systematic approach can inspire future research to adopt knowledge-driven and model-centric data space modeling, leading to more reliable and generalizable mathematical reasoning models. Furthermore, the fine-grained annotations and progressive difficulty levels provide valuable resource for benchmarking and analyzing the strengths and weaknesses of different MLLMs, promoting transparency and interpretability in model development. Bridging AI and Education: high-quality datasets for teaching and learning. WE-MATH 2.0s datasets are not only designed for model training but also have significant educational value. Each problem is accompanied by GeoGebra (GGB) file, which can serve as high-quality teaching material for educators and students. The hierarchical knowledge system and step-wise annotations make it easier to design personalized learning paths and targeted exercises, supporting adaptive learning and formative assessment. The multi-modal and multi-perspective problem sets encourage students to develop flexible thinking and deepen their conceptual understanding. By bridging the gap between AI research and educational practice, MathBook has the potential to enhance mathematics education, facilitate interactive and engaging learning experiences, and support the development of intelligent tutoring systems. Enhancing RL generalization through progressive and dynamic training. WE-MATH 2.0 introduces novel, model-centric curriculum for RL-based training, where problems are systematically organized from easy to hard based on explicit difficulty modeling. This approach provides new perspective for designing RL curricula, enabling more effective and efficient learning. The \"onequestion-multi-image\" and \"one-image-multi-question\" strategies, together with dynamic scheduling mechanisms, enhance the robustness and generalization of RL-trained models. These innovations can inspire the broader RL community to explore curriculum learning, dynamic data scheduling, and multi-modal data augmentation for complex reasoning tasks. Moreover, these hierarchical knowledge approaches also offers new solution for tool learning [40, 12, 11]. MathBook thus serves as valuable testbed for advancing RL methods in the context of mathematical reasoning and beyond."
        },
        {
            "title": "B Ethical Considerations",
            "content": "Licensing and Open Access. For all referenced or incorporated data in WE-MATH 2.0, we only use existing datasets with clear and appropriate licenses. All data curated by our team will be released under the CC BY 4.0 license, ensuring open access for the research community. The entire MathBook dataset, including both external and newly constructed components, will be made publicly available to facilitate further research and development. 18 Data Sources and Privacy. All data in WE-MATH 2.0 are either sourced from publicly available datasets or generated by our expert team, and do not contain any personal user information. Therefore, there are no privacy concerns related to our dataset. Expert Compensation. Experts involved in annotation are compensated on per-task basis, with payment issued only after cross-validation and quality assurance. All compensation meets or exceeds the local minimum wage standards. Details of WE-MATH 2.0 C.1 MathBook Knowledge System C.1.1 Hierarchical Structure of Knowledge Points As illustrated in Figures 36, we present partial example of the hierarchical structure of knowledge points in the MathBook Knowledge System. The system is organized as five-level hierarchical structure of knowledge points = {k1, k2, . . . , kN }, where = 491 denotes the total number of knowledge points at the lowest level of the hierarchy. The first level consists of four categories: Geometry, Fundamental Skills, Algebra, and Probability and Statistics. The construction of follows two-track, human-AI collaborative process. First, an initial version Khuman is constructed by collecting and merging knowledge point lists from authoritative sources, including Wikipedia, open-source mathematics textbooks, and national curriculum standards. This initial structure is deduplicated, reorganized, and refined for logical consistency and comprehensive coverage. In parallel, large-scale problem set is collected, including 30,000 sampled from existing math datasets. GPT-4o is used to assign multi-level topic tags = {t1, . . . , tn} to each problem, and semantic similarity matrix Rnn is computed. Hierarchical clustering is then applied to to generate an AI-assisted hierarchical structure of knowledge points Kauto. Finally, the AI-assisted structure Kauto is integrated with the initial structure Khuman through systematic comparison, merging, and revision. The Kauto serves as reference for revising and refining the manually constructed hierarchical structure of knowledge points, resulting in the final knowledge point set K. 19 Figure 3: Overview of the hierarchical structure of knowledge points in MathBook (1). 20 Figure 4: Overview of the hierarchical structure of knowledge points in MathBook (2). 21 Figure 5: Overview of the hierarchical structure of knowledge points in MathBook (3). 22 Figure 6: Overview of the hierarchical structure of knowledge points in MathBook (4). 23 C.1.2 Knowledge Principles As shown in Figures 710, we provide several examples of knowledge principles, which include definitions, theorems, and other foundational statements associated with each knowledge point. The annotation of principles = (cid:83)N i=1 Pi (P = 2,157) also follows two-track, human-AI collaborative approach. Based on the constructed knowledge hierarchy K, set of core principles for each knowledge point ki is first drafted, referencing authoritative sources such as Wikipedia, textbooks, and international curriculum standards. In parallel, for each ki, set of representative problems from is selected and their chain-of-thought (CoT) solutions are annotated. Each step in the CoT is mapped to the corresponding knowledge point using GPT-4o, and the relevant CoT steps for each ki are extracted. The CoT steps associated with each knowledge point are then aggregated and reviewed to supplement, refine, and validate the set of principles for ki. This process is repeated iteratively, consolidating both the expert-written and data-driven principles, cross-checking against original sources and annotated solution paths, until the set of principles Pi for each knowledge point is comprehensive and precise. Figure 7: Examples of knowledge principles corresponding to specific knowledge points in MathBook (1). 24 Figure 8: Examples of knowledge principles corresponding to specific knowledge points in MathBook (2). Figure 9: Examples of knowledge principles corresponding to specific knowledge points in MathBook (3). 25 Figure 10: Examples of knowledge principles corresponding to specific knowledge points in MathBook (4). C.2 MathBook-Standard C.2.1 GeoGebra-based Diagram Generation. As described in the main text, all diagrams in MathBook-Standard are rendered using GeoGebra, dynamic mathematics software that enables precise and reproducible geometric constructions. GeoGebra supports both interactive design and programmatic generation of diagrams, making it highly suitable for large-scale dataset creation. In our workflow, each problem is paired with high-quality diagram constructed in GeoGebra, ensuring mathematical rigor and visual clarity. For automated and scalable generation, we leverage GeoGebras ability to encode diagrams as scripts (e.g., XML, as shown in Figure 11), which allows for efficient parameter variation and reproducibility, but the core advantage lies in GeoGebras expressive power and accuracy for mathematical figures. Compared to general-purpose plotting libraries such as Pythons matplotlib, GeoGebra offers richer geometric primitives and more precise control over mathematical relationships, supporting broader range of problem types and visual styles. As shown in Figures 12, 13, and 14, diagrams generated by GeoGebra exhibit higher fidelity and better alignment with mathematical conventions, which is essential for both algorithmic evaluation and educational use. It is evident that the complexity and precision of these diagrams would be difficult to achieve using Python-based plotting tools alone. C.2.2 Dataset Diversity and Variant Construction. Building on the GeoGebra-based pipeline, we systematically construct diverse dataset as detailed in the main text. For each knowledge point and principle, seed problem is designed with corresponding diagram. To further enhance diversity, we introduce two types of variants: oneproblem-multi-image (generating multiple diagram instances for the same problem by varying 26 Figure 11: Overview of the GeoGebra interface and part of the corresponding XML script, showing core commands for defining geometric objects. parameters in GeoGebra) and one-image-multi-problem (curating multiple questions for single diagram, each derived from different knowledge points or mathematical principles). Representative examples of seed problems and their variants are shown in Figure 15 22, demonstrating the flexibility and extensibility of our approach. By leveraging GeoGebras capabilities, MathBook-Standard achieves both high-fidelity geometric representation and systematic dataset expansion, ensuring rich semantic and visual diversity for mathematical reasoning tasks. 27 Figure 12: Overview of group of GeoGebra-generated images in MathBook (1). Figure 13: Overview of group of GeoGebra-generated images in MathBook (2). 28 Figure 14: Overview of group of GeoGebra-generated images in MathBook (3). Figure 15: An example of MathBook-Standard data instance (1). 29 Figure 16: An example of MathBook-Standard data instance (2). Figure 17: An example of MathBook-Standard data instance (3). 30 Figure 18: An example of MathBook-Standard data instance (4). Figure 19: An example of MathBook-Standard data instance (5). 31 Figure 20: An example of MathBook-Standard data instance (6). Figure 21: An example of MathBook-Standard data instance (7). 32 Figure 22: An example of MathBook-Standard data instance (8). 33 C.3 MathBook-Pro As shown in Figure 23, we present concrete example from MathBook-Pro to illustrate the construction and expansion of problem variants within the three-dimensional difficulty space. The seed problem, positioned at the origin, focuses on the arc length formula in plane geometry, and involves knowledge points such as definition of angle, definition of circles, four arithmetic operations of integers and four arithmetic operations of fractions. We first demonstrate how the seed problem is expanded along each individual dimension: Step Complexity: The number of required knowledge points is increased by introducing new intermediate conclusions as conditions. In this example, the extended variant requires not only the arc length formula, but also incorporates circumference of circle and area of circle as additional knowledge points. The solution to the new problem depends on the answer to the seed problem, reflecting progressive deepening of reasoning. Visual Complexity: The original diagram is enhanced by adding shaded regions, which increases the visual and interpretive demands while keeping the core mathematical focus unchanged. Contextual Complexity: The problem statement is recontextualized from direct geometric description to real-world application scenario. Although the narrative becomes more complex, the essential assessment of the arc length formula remains at the core. By systematically combining these single-dimension expansions, we further generate multidimensional variants that integrate increased step, visual, and contextual complexity. In total, starting from the seed problem, we construct 7 variants corresponding to all possible combinations of the three dimensions. Each new variant is constructed through progressive modifications to both the problem statement and the accompanying image, resulting in diverse and interpretable set of difficulty-controlled problems. Figure 23: An example from MathBook-Pro in the difficulty space. 34 C.4 MathBookEval C.4.1 Dataset Construction and Annotation Protocol. To ensure both comprehensive knowledge coverage and rigorous, interpretable annotations for visual mathematical reasoning, MathBookEval is constructed through multi-stage, process-oriented pipeline. We begin by integrating representative samples collected from five open-source benchmarks: MathVista [32], MathVerse [71], MathVision [57], We-Math [43] and DynaMath [77], systematically filtering out redundant or highly similar items to maximize diversity in knowledge point combinations and reasoning patterns. All problems are re-annotated under unified guidelines, ensuring consistency in annotation style and granularity. For each problem, at least two human experts independently provide complete, step-by-step solution, where each step is explicitly decomposed according to the underlying knowledge point(s) from the unified knowledge system K. This knowledge-point-based decomposition is fundamental: it enables precise and systematic quantification of reasoning depth, as each reasoning step directly corresponds to specific knowledge point. Only those problems for which the set of annotated knowledge points at each step is exactly consistent across expert annotations are retained in the final dataset, ensuring high reliability and objectivity. To address knowledge points and reasoning depths insufficiently covered by existing benchmarks, additional samples are newly constructed by human experts following the same process-oriented, knowledgepoint-level annotation protocol. As result, MathBookEval comprises both collected samples from open-source benchmarks and newly constructed samples, achieving balanced and comprehensive coverage across mathematical domains and reasoning complexities, with every annotation step tightly aligned to the relevant knowledge points for robust reasoning depth modeling. C.4.2 Task Dimensions. MathBookEval is organized along two dimensions, capturing both the diversity of mathematical knowledge and the depth of reasoning required for problem solving. (1) Reasoning Dimension: Problems are categorized by the number of reasoning steps required to reach the solution. Each step is explicitly defined and directly mapped to specific knowledge point in the unified knowledge system K. We define three levels: Level 1 (13 steps, basic reasoning), Level 2 (46 steps, intermediate reasoning), and Level 3 (710 steps, complex reasoning). This step-by-step annotation based on knowledge points allows for clear and objective quantification of reasoning depth, enabling detailed analysis of model performance across different levels of reasoning complexity. (2) Knowledge Dimension: The 491 knowledge points in are distributed across 4 top-level domains and 13 subdomains, all of which are covered in the benchmark. Each problem is annotated with all the knowledge points involved in its solution, and every reasoning step is aligned with specific knowledge point. This enables fine-grained evaluation of model capabilities across various mathematical topics and educational stages. C.4.3 Dataset Statistics. Table 6 summarizes the key statistics of MathBookEval. The benchmark contains 1,000 fully annotated problems, covering all 491 knowledge points in the unified knowledge system. Of these, 600 are sourced from open-source benchmarks and 400 are newly constructed to address coverage gaps and increase diversity. Problems are distributed across multiple formats, including multiplechoice and fill-in-the-blank, and span wide range of reasoning depths and knowledge domains. As shown in Figure 24, we compare MathBookEval and existing benchmarks. In the right panel, the y-axis represents the percentage of problems at each reasoning level. Note that for some benchmarks, the total does not reach 100% because we exclude problems that experts annotate as belonging to other subjects such as physics, chemistry, or biology, in order to ensure rigorous comparison. It is evident from the figure that existing benchmarks contain less than 3% of problems at Level 2 (46 steps), and none at Level 3 (710 steps). In contrast, MathBookEval substantially supplements these two categories, providing more comprehensive evaluation of multi-step reasoning abilities. Table 6: Key statistics of MathBookEval. Statistics"
        },
        {
            "title": "Total Problems",
            "content": "- Open-source Benchmarks - MathVista - MathVerse - MathVision - We-Math - DynaMath - Newly Constructed"
        },
        {
            "title": "Knowledge Points Covered",
            "content": "- Domains - Subdomains"
        },
        {
            "title": "Reasoning Depth",
            "content": "- Level 1 (13 steps) - Level 2 (46 steps) - Level 3 (710 steps) Number 1,000 600 150 150 100 100 100 400 491 4 13 62.0% 30.2% 7.8% Figure 24: Comparison of MathBookEval and open-source benchmarks C.4.4 Evaluation Protocol and Metrics. MathBookEval incorporates existing evaluation protocols, including both LLM-as-a-judge and rulebased approaches. To ensure consistency, MathBookEval adopts the LLM-as-a-judge protocol as the evaluation rule. Specifically, we adopt the LLM-as-a-judge protocol, following MathVista and MathVerse by employing GPT-4o as the judge model and reporting overall accuracy as the evaluation metric. The detailed prompt used for evaluation is shown in Table 7. Table 7: Prompt templates for evaluation on MathBookEval. Type Prompt Template Evaluation Prompt Now, we require you to solve math question. Please briefly describe your thought process and provide the final answer. For multiple-choice questions, return the selected option and its content. For direct answer selection, return only the chosen result. For fill-in-the-blank questions, answer directly. Question: <Question> Regarding the format, please answer following the template below, and be sure to include two <> symbols: <Thought process>: <<your thought process>> <Answer>: <<your answer>> 36 Table 8: The release time and model source of MLLMs used in MathBookEval Model GPT-4o [37] GPT-4V [38] InternVL2.5-78B [7] InternVL2.5-8B [7] Qwen2.5-VL-72B [4] Qwen2.5-VL-7B [4] Qwen2.5-VL-3B [4] LLaVA-OneVision-72B [27] LLaVA-OneVision-7B [27] GLM-4V-9B [17] Release Time 2024-05 2024-04 20242024-12 2025-01 2025-01 2025-01 2024-08 20242024-06 Source https://gpt4o.ai/ https://openai.com/index/gpt-4v-system-card/ https://huggingface.co/OpenGVLab/InternVL2_5-78B https://huggingface.co/OpenGVLab/InternVL2_5-8B https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov-chat https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov https://huggingface.co/THUDM/glm-4v-9b Table 9: Model architecture of 10 MLLMs evaluated on MathBookEval. Models GPT-4o GPT-4V LLM - - Vision Encoder - - InternVL2.5-78B Qwen2.5-72B-Instruct InternViT-6B-448px-V2_5 InternVL2.5-8B internlm2_5-7b-chat InternViT-6B-448px-V2_5 Qwen2.5-VL-72B Qwen2.5-72B-Instruct CLIP ViT-bigG-P Qwen2.5-VL-7B Qwen2.5-7B-Instruct CLIP ViT-bigG-P14 Qwen2.5-VL-3B Qwen2.5-3B-Instruct CLIP ViT-bigG-P LLaVA-OneVision-72B Qwen2-72B SigLip-so400m-P14-384 LLaVA-OneVision-7B Qwen2-7B SigLip-so400m-P14-384 GLM-4V-9B GLM-9B EVA_02_CLIP-E-P14 C.4.5 Experiment Setup Details of the Evaluated Models. To evaluate the performance of various multimodal large models on mathematical tasks, we include diverse set of recent models in our benchmark. Table 8 lists the release dates and official sources of all evaluated models. Additionally, Table 9 provides an overview of their architectural designs to support comprehensive comparison. Details of the Model Hyperparameters. For all closed-source models accessed via API, we adopt the standard generation settings and perform inference on CPUs, with the process typically completing within day. For open-source models, inference is conducted on cluster equipped with 8 NVIDIA A800-SXM4-80GB GPUs, using the hyperparameter configurations provided in the official inference examples. If no specific instructions are available, default settings are applied. The detailed generation parameters are summarized in Table 10 and Table 11. C.4.6 Additional Results on MathBookEval As shown in Figure 28 to Figure 37, we present the performance of different models on various subdomains in MathBookEval, where subdomains belonging to the same domain are indicated with the same color. It can be observed that models generally perform worse on geometry-related problems, especially in subdomains such as solid geometry and analytic geometry, which involve higher visual complexity and require more advanced reasoning. In contrast, models tend to achieve better results on algebra and fundamental skills, particularly in subdomains related to computational methods. 37 Table 10: Generating parameters for Open-Source MLLMs. Model Generation Setup InternVL2.5-78B do_sample = False, max_new_tokens = 1024 InternVL2.5-8B do_sample = False, max_new_tokens = 1024 Qwen2.5-VL-72B do_sample = False, max_new_tokens = 1024 Qwen2.5-VL-7B do_sample = False, max_new_tokens = 1024 Qwen2.5-VL-3B do_sample = False, max_new_tokens = 1024 LLaVA-OneVision-72B do_sample = False, max_new_tokens = 1024 LLaVA-OneVision-7B do_sample = False, max_new_tokens = GLM-4V-9B do_sample = False Table 11: Generating parameters for Closed-Source MLLMs. Model Generation Setup GPT-4o \"model\" : \"gpt-4o\", \"temperature\" : 0, \"max_tokens\" : 1024 GPT-4V \"model\" : \"gpt-4-turbo\", \"temperature\" : 0, \"max_tokens\" : 1024 More Details on MathBook-RL D.1 Implementation details We use Qwen2.5-VL-7B-Instruct as the base model and conduct all experiments on 8A800 GPUs. The training process consists of two stages. In the first stage, we perform cold-start supervised fine-tuning (SFT) to help the model develop explicit awareness of knowledge system and knowledge-driven reasoning paradigm. The SFT stage uses learning rate of 1.0 105, is trained for 1 epoch, and adopts warmup ratio of 0.1. In the second stage, we apply dynamic reinforcement learning (RL) to further improve the models generalization ability. For RL, we set the rollout temperature to 1.0 and generate 8 rollouts per sample. The learning rate is set to 1 106, and the maximum completion length is 1024 tokens. The system prompt used in this stage is illustrated in Table 12. The reward function combines answer accuracy (weight 0.9) and response format compliance (weight 0.1). Specifically, we use MathVerify to extract and compare answers for accuracy, while format compliance ensures the output follows the required structure. Table 12: The system prompt template for response generation in the RL stage. Type Prompt Template System Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first outputs the thinking process in <think> </think> and then provides the final answer(number, option, phrase, or LaTeX expression as appropriate) in <answer> </answer> tags. User: {question} Assistant: D.2 Case Study To further illustrate the strengths of our approach, we present several representative case studies comparing our model with the Qwen2.5-VL-7B baseline across different benchmarks. Case 1: Conciseness and Reasoning Process. Figure 25 compares the response patterns of our model and the Qwen2.5-VL-7B baseline on We-Math. Our model produces more concise answers, 38 with reduced average response length, while still retaining all necessary formulas and reasoning steps. This demonstrates that our approach effectively mitigates the issue of overthinking, resulting in more focused and efficient solutions without sacrificing completeness or mathematical rigor. Case 2: Spatial Reasoning Enhancement. Figure 26 highlights the performance of our model on MathVision. Compared to the baseline, which often fails to correctly interpret or solve such problems, our model (MathBook-7B) shows clear improvements in spatial reasoning. This is particularly evident in questions requiring the understanding of geometric relationships or positional logic, indicating that our training strategy significantly enhances the models ability to handle spatially complex scenarios. Case 3: Knowledge-Oriented and Context-Aware Mathematical Reasoning. In Figure 27, we compare the responses of our model and the Qwen2.5-VL-7B baseline on MathVista. Our model not only applies the relevant mathematical concepts correctly but also demonstrates improved integration of mathematical knowledge with real-world problem contexts. This case exemplifies the models strengthened ability to bridge abstract mathematical reasoning and practical application, reflecting the benefits of our knowledge-oriented and context-aware training paradigm. Overall, these case studies provide qualitative evidence that our model achieves more concise, accurate, and contextually appropriate reasoning compared to strong baselines, particularly in scenarios demanding spatial understanding and knowledge-oriented problem solving. Figure 25: Comparison of MathBook-7B and Qwen2.5-VL-7B on We-Math. D.3 Experiment Setup D.3.1 Details of the Evaluation. We evaluate our model on four representative benchmarks: MathVista [32], MathVerse [71], MathVision [57], and We-Math [42]. During evaluation, we strictly follow the official scoring protocols provided in the respective benchmark GitHub repositories to ensure fair and consistent comparison. 39 Figure 26: Comparison of MathBook-7B and Qwen2.5-VL-7B on MathVision. Specifically, we report results on the testmini split for MathVista, MathVerse, MathVision, and WeMath. For We-Math, we adopt the main evaluation metric as defined in the original paper, reporting Score (Strict) as the primary metric. For the other benchmarks, we report the average accuracy as the main evaluation result. D.3.2 Details of the Baselines We compare our method with comprehensive set of baselines from three perspectives: closed-source models (e.g., GPT-4o [37], Gemini-1.5-Pro [51]), open-source general models (e.g., Qwen2.5VL-7B [4], InternVL2.5-8B [8]), and open-source reasoning models (e.g., Math-PUMA-7B [75], URSA-8B [35], R1-Onevision-7B [66], R1-VL-7B [70], MM-Eureka-7B [36]), which enables thorough and multi-faceted comparison to highlight the advantages of our approach. GPT-4o [37]: GPT-4o (o for omni) is OpenAIs 2024 flagship multilingual, multimodal large language model that accepts text, images, and audio as input for unified cross-modal understanding. It is designed for broad adaptability and seamless integration of visual and linguistic information, supporting complex reasoning across modalities and languages. Gemini-1.5-Pro [51]: Gemini 1.5 Pro, developed by Google DeepMind, is multimodal model capable of processing text, images, audio, and video inputs, with an extremely long context window (up to 12 million tokens). It is optimized for multi-task proficiency and structured tool integration, enabling analysis of lengthy and diverse content. Qwen2.5-VL-7B [4]: Qwen2.5-VL-7B is an open-source vision-language model with 7 billion parameters, designed to generate both free-form text and structured outputs such as bounding boxes 40 Figure 27: Comparison of MathBook-7B and Qwen2.5-VL-7B on MathVista. and JSON. It emphasizes fine-grained visual understanding and multi-modal alignment, supporting tasks like document analysis and event detection. InternVL2.5-8B [8]: InternVL2.5-8B is an 8B-parameter open-source multimodal model that employs progressive scaling and co-training strategies to align its vision and language components. It incorporates training optimizations and curated dataset to enhance cross-modal reasoning and reduce hallucinations. Math-PUMA-7B [75]: Math-PUMA-7B is vision-language model focused on mathematical reasoning with visual inputs, introducing three-stage curriculum for aligning textual and visual modalities. The model is optimized for visual math benchmarks and aims to ensure consistent problem-solving across formats such as text and diagrams. URSA-8B [35]: URSA-8B is an 8B-parameter multimodal model targeting chain-of-thought reasoning in visual mathematical problems, trained on large-scale multimodal CoT datasets. It employs reward model for stepwise verification, emphasizing both the generation and validation of reasoning chains for reliable solutions. R1-Onevision-7B [66]: R1-Onevision-7B is 7B-parameter multimodal reasoning model that converts images into structured textual representations for symbolic reasoning. It is trained on step-by-step multimodal reasoning annotations and refined with reinforcement learning, enabling precise multi-hop visual-textual inference. R1-VL-7B [70]: R1-VL-7B is an open-source 7B vision-language model designed for stepwise reasoning, applying Step-wise Group Relative Policy Optimization (StepGRPO) for dense intermedi41 ate rewards. This approach improves logical coherence and multi-step problem-solving, especially in mathematical and logical tasks. MM-Eureka-7B [36]: MM-Eureka-7B is vision-language model based on Qwen2.5-VL-7B, fine-tuned with the MMK12 dataset and rule-based reinforcement learning strategy. It is designed for multidisciplinary visual reasoning in math and science, using rule-based rewards to guide the learning of complex reasoning steps. Figure 28: Detailed performance of GPT-4o across 13 subdomains. Figure 29: Detailed performance of GPT-4V across 13 subdomains. Figure 30: Detailed performance of InternVL2.5-78B across 13 subdomains. 42 Figure 31: Detailed performance of Qwen2.5-VL-72B across 13 subdomains. Figure 32: Detailed performance of LLaVA-OneVision-72B across 13 subdomains. Figure 33: Detailed performance of InternVL2.5-8B across 13 subdomains. Figure 34: Detailed performance of Qwen2.5-VL-7B across 13 subdomains. 43 Figure 35: Detailed performance of LLaVA-OneVision-7B across 13 subdomains. Figure 36: Detailed performance of GLM-4V-9B across 13 subdomains. Figure 37: Detailed performance of Qwen2.5-VL-3B across 13 subdomains."
        }
    ],
    "affiliations": [
        "BUPT",
        "Tsinghua University",
        "WeChat Vision, Tencent Inc."
    ]
}