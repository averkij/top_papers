{
    "paper_title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?",
    "authors": [
        "Hyunjong Ok",
        "Suho Yoo",
        "Hyeonjun Kim",
        "Jaeho Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io."
        },
        {
            "title": "Start",
            "content": "AUDITORYBENCH++: CAN LANGUAGE MODELS UNDERSTAND AUDITORY KNOWLEDGE WITHOUT HEARING? Hyunjong Ok1,2 Suho Yoo2,3 Hyeonjun Kim1 Jaeho Lee1 1Pohang University of Science and Technology, South Korea 2HJ AILAB 3Korea Advanced Institute of Science and Technology, South Korea 5 2 0 S 2 2 ] . [ 1 1 4 6 7 1 . 9 0 5 2 : r ABSTRACT Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIRCoT, novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io. Index Terms Auditory Knowledge, Large Language Model, Reasoning Model, Benchmarks, Chain-of-Thought 1. INTRODUCTION Imagination is considered necessary ingredient of perception itself in Kantian perspectives [1, 2], as it allows us to reconstruct complete representation of the target object out of incomplete, raw sensations. For example, by reading text describing night with heavy rain and lightning, one can synthesize multimodal imagery of the scene by imagining the sounds of rain pounding like drumbeats and roaring thunder. Given the same text, people can imagine similar multimodal signals as they share similar experiences and understandings about objectsi.e., commonsense. Having such common sense enables efficient and effective communication between people, without having to describe every detail. Equal contribution Correspondence to: Jaeho Lee <jaeho.lee@postech.ac.kr>. Fig. 1. Overview of AuditoryBench++, which assesses auditory knowledge of language models without audio input. Do large language models (LLMs) share similar commonsense? Recent studies reveal that the answer is no; LLMs have poor understanding of both visual commonsense (e.g., colors of everyday objects) [35] and auditory commonsense (e.g., animal-sound associations) [6]. While visual knowledge in LLMs has received significant research focus [711], auditory knowledge remains critically underexplored, with existing benchmarks and methods being notably scarce. While recent advances in large audio-language models (LALMs) have shown promising results when processing audio inputs, their ability to engage in such auditory imagination in purely text-only settingswhere no audio signal is availableremains underexplored. We need to start with tasks that are trivial for humans yet require basic auditory imagination, providing an initial basis for systematically assessing such abilities in LLMs as well as LALMs. For this purpose, we introduce AuditoryBench++, comprehensive benchmark for evaluating auditory knowledge in LLMs. Unlike previous benchmarks [6] limited to shortanswer simple inference tasks across only two categories, our benchmark encompasses five tasks, including reasoning tasks, providing more thorough assessment of auditory reasoning capabilities. We refine and verify the dataset through rigorous processes, ensuring superior quality and reliability over previous benchmarks. Through this benchmark, we figure out that both current LLMs and LALMs (w/o audio input) perform close to random guessing in comparison tasks. We propose novel auditory reasoning method that enables LLMs to dynamically generate and process auditory embedding during inference, enabling LLMs to seemingly hear through explicit auditory imagination. We introduce an innovative training paradigm that equips LLMs with the ability to imagine sounds when encountering auditory reasoning tasks. Specifically, we introduce special token, [imagine], emitted by the model whenever auditory reasoning is required. When the model encounters contexts that require auditory reasoning, it learns to emit this token, signaling the system to pause text generation and invoke an imagination process to think contextually relevant sounds. The model then integrates this auditory information to continue reasoning with enhanced acoustic understanding. Unlike the cascade approach in previous methods such as Imagine to Hear [12], our end-to-end method empowers the model to reason, achieving superior performance. 2. RELATED WORK Multimodal commonsense benchmark. Multimodal commonsense benchmarks assess whether models can combine perceptual knowledge with language understanding. In the vision domain, various datasets evaluate visual commonsense, such as knowledge grounding or compositional reasoning [1317]. For the audio modality, there are large-scale resources for sound event classification, captioning, and reasoning [1822]. Taking this further, several studies have extended this evaluation to audio-visual tasks, requiring reasoning over audio and visual cues [2325]. However, these benchmarks generally assume perceptual inputs (image, audio, or both). In contrast, AuditoryBench++ targets text-only settings, requiring models to imagine auditory properties and isolating their internal commonsense without relying on perceptual inputs to provide cues. This design probes whether models can still reason about sounds when direct audio input is unavailable, reflecting real-world cases where only textual descriptions are accessible. Reasoning models via imagination. Reasoning models enhance the inference capabilities of LLMs, thereby expanding their overall capacity [26, 27]. While reinforcement learning tuning remains the dominant approach for enabling reasoning, recent methods have explored imagination-based alternatives [9,28]. These methods involve visual thought processes, such as generating subsequent scenes or editing given images [10, 11, 29], which leverage visual imagination as an intermediate reasoning stepdemonstrating improved performance on multi-step reasoning tasks. In contrast to existing visual imagination frameworks, our approach explores auditory imagination as novel modality for enhancing reasoning capabilities, introducing auditory thoughts as complementary tool for complex inference. Table 1. Task summary of AuditoryBench++ reconstructed from original resources with filtering and verification. Original Resource Question Type Task Type # QnA Task Comparison Pitch Duration Loudness AuditoryBench AudioTime AudioTime Binary Choice Recognition Animal Sound AuditoryBench Multiple Choice Reasoning Auditory Context MMAU Multiple Choice Total 3,625 1,645 445 75 6,732 3. AUDITORYBENCH++ Task Definition. AuditoryBench++ comprises 5 tasks evaluating spectrum of auditory knowledge, from fundamental comparisons to complex, contextually grounded reasoning: Pitch Comparison: The model selects which of two sounds has higher pitch, formulated as binary decision task. Duration Comparison: The model compares two described sounds and identifies the one with the longer duration. Loudness Comparison: This task asks the model to select the louder sound between two options based on prompts. Animal Sound Recognition: This task requires predicting the correct animal corresponding to given onomatopoeic expression (e.g., meow). Each sample is presented as multiple-choice question with four options. Auditory Context Reasoning: This component evaluates models ability to perform contextual auditory reasoning, focusing on interpreting nuanced auditory cues and situational contexts in multiple-choice format. Construction Pipeline. To construct AuditoryBench++, we carefully design multi-stage pipeline that integrates diverse existing resources and applies systematic filtering, statistical estimation, and human verification. This process ensures the resulting tasks are both reliable and unambiguous, providing robust benchmark for auditory reasoning. For pitch comparison, we use only the wiki set of AuditoryBench [6], since it consists of instrument-based pitch pairs that ensure objectivity and consistency in pairwise evaluation. For animal sound recognition, we draw from both the test and wiki sets of AuditoryBench, and apply human filtering to both choices and answers. Inappropriate or ambiguous samples (e.g., fictional animals or mislabeled cases) are removed to ensure the reliability of the evaluation. For duration and loudness comparison, we build new datasets from AudioTime [18], leveraging its segment-level annotations. In the loudness task, peak decibel levels are calculated to provide consistent measure of intensity across samples, ensuring clear distinctions between label pairs and minimizing ambiguity. We first retain only data classes with at least 30 samples to ensure statistical reliability. Outliers Fig. 2. Pipeline of the proposed AIR-CoT. (a) Data Preparation. Training data is augmented with [imagine] tokens to mark spans requiring auditory reasoning. (b) Stage 1: Span Detection. The model is fine-tuned to detect the spans by generating the special tokens during decoding. (c) Stage 2: Knowledge Injection. When encountering the [/imagine] token, the model pauses to generate the embedding using CLAP and injects it for auditory reasoning. are removed using the interquartile range (IQR) rule, and we compute pairwise differences to select label pairs with statistically significant contrasts (p < 0.01). The final dataset consists of comparison questions between label pairs that exhibit meaningful and measurable differences. To build the auditory context reasoning task, we start from the open set of MMAU [22] and adapt its audio-related reasoning questions into text-only format. Each audio clip is first described using Qwen2-Audio [30], generating detailed captions that capture the key auditory cues, such as sound sources, events, and acoustic properties. These captions are then paired with the original questions and rewritten by GPT-4o [31] to create text-only problems that retain the original reasoning objectives. Subsequently, human verification carefully removes unnatural or erroneous rewrites and revises them to ensure the questions are precise, coherent, and natural in purely text-based setting. At the end of each dataset construction pipeline, all task datasets underwent final round of human filtering and refinement to ensure correctness and remove residual noise. Omitted details. Certain details (e.g., details of the pipeline, dataset samples, etc.) are omitted here for brevity. These can be found on our project page1. 4. AIR-COT In this section, we introduce the auditory imagination reasoning Chain-of-Thought (AIR-CoT), novel method to equip language models with auditory capabilities and thereby enable reasoning grounded in auditory commonsense. 1https://auditorybenchpp.github.io AIR-CoT proceeds in two training stages. In the first stage, we apply SFT to train the model to detect spans requiring auditory knowledge during decoding via special tokens. In the second stage, we train an imagination module to inject relevant auditory knowledge into these identified spans, enabling the model to perform auditory reasoning. Data preparation. Before training, we curate data to train our model. Drawing from the training set of the sound pitch comparison task in AuditoryBench [6], which is independent from our pitch comparison data with no overlapping sound objects, we leverage an LLM to generate reasoning-style outputs. As shown in Fig. 2(a), these outputs encompass spans that require auditory knowledge within [imagine] tokens. Specifically, we employ the Qwen2.5-32B [32] model for this generation process, providing it with few-shot examples and the current datas information; context, span, and answer. Stage 1: Span detection via special token. To enable models to imagine and reason with auditory knowledge, we first train them to detect spans that require such knowledge. We introduce special [imagine] tokens, which the model generates to detect auditory knowledge demanded spans during reasoning (e.g., [imagine/] the sound of machinery [/imagine]). We perform SFT with the loss focusing solely on generating these tokens. Downstream answer tokens (e.g., higher in pitch comparison task) are excluded from the loss to prioritize accurate span detection without biasing final predictions. Stage 2: Knowledge injection via imagination. After span detection, we equip the model with the ability to imagine auditory knowledge by injecting suitable embeddings. During decoding, upon generating the [/imagine] token, the model Table 2. Experimental results of AIR-CoT across comparison, recognition, and reasoning tasks. Methods Majority Class LLaMA3.18B [33] Qwen2.57B [32] Phi4-mini4B [34] Qwen2-Audio7B [30] Phi4-MM6B [35] AudioBERT [6] Imagine to Hear [12] Pitch 52.14 52.39 42.46 55.75 46.59 56.08 59.34 75.64 Comparison Duration 56. Loudness 57.30 Off-the-Shelf LLMs 55.81 55.44 54.59 50.40 56.84 46.29 57.53 58.65 55.73 57.98 Augmented Methods 51.91 58.78 57.30 57.30 Recognition Animal Sound Auditory Context Reasoning 26.65 25. 56.26 62.21 60.72 31.42 60.93 22.40 24.73 69.33 70.67 68.00 37.33 65.33 30.67 29.33 AIR-CoT (Ours) 83.89 (+8.25) 54.59 (-4.19) 59.33 (+0.68) 71.55 (+9.34) 82.67 (+11.88) pauses to perform auditory imagination. We leverage audio models (e.g., CLAP [36]) to produce audio embeddings and inject them into the [/imagine] token. This enables AIRCoT; when the model needs auditory knowledge, it stops, imagines it, and then continues the CoT. In detail, we adapt the embeddings via 2-layer MLP to match the models hidden size. Training focuses solely on the MLP, with loss computed only on answer tokens. 5. EXPERIMENTS Baselines and metric. We establish comprehensive baselines for our AuditoryBench++, evaluating recent LLMs, LALMs, and previous methods for augmenting auditory knowledge. For LLMs, we test LLaMA3.18B [33], Qwen2.57B [32], and Phi4-mini4B [34]. For LALMs, we include Qwen2Audio7B [30] and Phi4-MM6B [35]. Additionally, we compare our approach with auditory knowledge injection methods such as AudioBERT [6] and Imagine to Hear [12]. All evaluations utilize accuracy as the metric. Implementation details. In our implementation, we base AIR-CoT on the Qwen2.57B model for both stages. The imagination process is built with CLAP text encoder and 2-layer MLP. For Stage 1, we perform fine-tuning with special tokens [imagine/] and [/imagine] added to the vocabulary. Training uses with 10 epochs, batch size of 4, gradient accumulation steps of 16, learning rate of 1 105, and the AdamW [37] optimizer. For Stage 2, we load the fine-tuned model from Stage 1 and integrate CLAP encoder with 2-layer MLP projector to align audio embeddings. We train only the projector for 10 epochs, with batch size of 4, learning rate of 1 104, weight decay of 0.01, and an AdamW optimizer. For fair comparison, AudioBERT [6] and Imagine to Hear [12] employ the same data. Experimental results. We report the performance of various baselines and our method in AuditoryBench++. As shown in Table 2, most language models perform poorly due to the absence of auditory knowledge. AudioBERT and Imagine to Hear provide some improvements but remain constrained. In contrast, AIR-CoT substantially outperforms these on pitch comparison, animal sound recognition, and auditory context reasoning, showcasing its ability to enable end-to-end auditory imagination within the reasoning chain. However, the improvements in duration and loudness remain limited. Current audio representations are primarily semantic, which makes them effective for animal sound recognition and auditory context reasoning. Pitch benefits from exposure to pitch-related training data, enabling relation reasoning. In contrast, duration and loudness depend on temporal and amplitude cues that these representations do not capture well, with duration being particularly challenging since current embeddings do not reflect the time axis [3840]. Future work should explore representations that encode such quantitative properties more directly. 6. CONCLUSIONS We presented AuditoryBench++, benchmark for assessing auditory knowledge in text-only settings, and AIR-CoT, reasoning method that equips LLMs with auditory imagination via span detection and knowledge injection. Experiments demonstrate that AIR-CoT outperforms off-the-shelf and augmented models. We believe our work provides strong foundation for building language models that can imagine auditory information without direct audio input, ultimately enabling more natural and human-like multimodal reasoning. 7. REFERENCES [1] P. F. Strawson, Imagination and perception, in Freedom and Resentment. London: Methuen, 1974. [2] D. Gregory, Imagery, the imagination and experience, Philosophical Quarterly, 2010. [3] C. Zhang, B. Van Durme, Z. Li, and E. Stengel-Eskin, Visual commonsense in pretrained unimodal and multimodal models, in Proc. NAACL, 2022. [4] X. Liu, D. Yin, Y. Feng, and D. Zhao, Things not written in text: Exploring spatial commonsense from visual signals, in Proc. ACL, 2022. [5] M. Alper, M. Fiman, and H. Averbuch-Elor, Is bert blind? exploring the effect of vision-and-language pretraining on visual language understanding, in Proc. CVPR, 2023. [6] H. Ok, S. Yoo, and J. Lee, Audiobert: Audio knowledge augmented language model, in Proc. ICASSP, 2025. [7] H. Tan and M. Bansal, Vokenization: Improving language understanding with contextualized, visualgrounded supervision, in Proc. EMNLP, 2020. [8] W. Wang, L. Dong, H. Cheng, H. Song, X. Liu, X. Yan, J. Gao, and F. Wei, Visually-augmented language modeling, in Proc. ICLR, 2023. [9] R. Liu, J. Wei, S. S. Gu, T.-Y. Wu, S. Vosoughi, C. Cui, D. Zhou, and A. M. Dai, Minds eye: Grounded language model reasoning through simulation, in Proc. ICLR, 2023. [10] W. Wu, S. Mao, Y. Zhang, Y. Xia, L. Dong, L. Cui, and F. Wei, Minds eye of llms: visualization-of-thought elicits spatial reasoning in large language models, in NeurIPS, 2024. [11] C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulic, and F. Wei, Imagine while reasoning in space: Multimodal visualization-of-thought, in Proc. ICML, 2025. [12] S. Yoo, H. Ok, and J. Lee, Imagine to hear: Auditory knowledge generation can be an effective assistant for language models, in Findings of ACL, 2025. [13] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, Ok-vqa: visual question answering benchmark requiring external knowledge, in Proc. CVPR, 2019. [14] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi, A-okvqa: benchmark for visual question answering using world knowledge, in Proc. ECCV, 2022. [15] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, From recognition to cognition: Visual commonsense reasoning, in Proc. CVPR, 2019. [16] X. Shen, F. Wang, S. Wu, and R. Xia, Vcd: dataset for visual commonsense discovery in images, in Proc. ACL, 2025. [17] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al., Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, in Proc. CVPR, 2024. [18] Z. Xie, X. Xu, Z. Wu, and M. Wu, Audiotime: temporally-aligned audio-text benchmark dataset, in Proc. ICASSP, 2025. [19] J. Huh, J. Chalk, E. Kazakos, D. Damen, and A. Zisserman, Epic-sounds: large-scale dataset of actions that sound, IEEE Trans. on Pattern Analysis and Machine Intelligence, 2025. [20] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, Audio set: An ontology and human-labeled dataset for audio events, in Proc. ICASSP, 2017. [21] C. D. Kim, B. Kim, H. Lee, and G. Kim, Audiocaps: Generating captions for audios in the wild, in Proc. NAACL, 2019. [22] S. Sakshi, U. Tyagi, S. Kumar, A. Seth, R. Selvakumar, O. Nieto, R. Duraiswami, S. Ghosh, and D. Manocha, MMAU: massive multi-task audio understanding and reasoning benchmark, in Proc. ICLR, 2025. [23] P. Yang, X. Wang, X. Duan, H. Chen, R. Hou, C. Jin, and W. Zhu, Avqa: dataset for audio-visual question answering on videos, in Proc. ACM MM, 2022. [24] G. Li, Y. Wei, Y. Tian, C. Xu, J.-R. Wen, and D. Hu, Learning to answer questions in dynamic audio-visual scenarios, in Proc. CVPR, 2022. [25] K. Gong, K. Feng, B. Li, Y. Wang, M. Cheng, S. Yang, J. Han, B. Wang, Y. Bai, Z. Yang et al., Av-odyssey bench: Can your multimodal llms really understand audio-visual information? arXiv preprint arXiv:2412.02611, 2024. [26] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [27] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang, Visual-rft: Visual reinforcement fine-tuning, arXiv preprint arXiv:2503.01785, 2025. [28] E. Zhou, Y. Qin, Z. Yin, Y. Huang, R. Zhang, L. Sheng, Y. Qiao, and J. Shao, Minedreamer: Learning to follow instructions via chain-of-imagination for simulatedworld control, arXiv preprint arXiv:2403.12037, 2024. [29] Y. Xu, C. Li, H. Zhou, X. Wan, C. Zhang, A. Korhonen, and I. Vulic, Visual planning: Lets think only with images, in Proc. CVPR Workshop, 2025. [30] Y. Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y. Leng, Y. Lv, J. He, J. Lin et al., Qwen2-audio technical report, arXiv preprint arXiv:2407.10759, 2024. [31] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [32] Q. Team, Qwen2.5 technical report, arXiv preprint arXiv:2412.15115, 2024. [33] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. AlDahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [34] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., Phi-4 technical report, arXiv preprint arXiv:2412.08905, 2024. [35] A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao, A. Benhaim, M. Cai, V. Chaudhary, C. Chen et al., Phi-4-mini technical report: Compact yet powerful multimodal language models via mixtureof-loras, arXiv preprint arXiv:2503.01743, 2025. [36] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, Clap learning audio concepts from natural language supervision, in Proc. ICASSP, 2023. [37] I. Loshchilov. and . Hutter, Decoupled weight decay regularization, in Proc. ICLR, 2019. [38] Y. Yuan, Z. Chen, X. Liu, H. Liu, X. Xu, D. Jia, Y. Chen, M. D. Plumbley, and W. Wang, T-clap: Temporalenhanced contrastive language-audio pretraining, arXiv preprint arXiv:2404.17806, 2024. [39] Z. Ma, J. Hong, M. O. Gul, M. Gandhi, I. Gao, and R. Krishna, Crepe: Can vision-language foundation models reason compositionally? in Proc. CVPR, 2023. [40] K. Sinha, R. Jia, D. Hupkes, J. Pineau, A. Williams, and D. Kiela, Masked language modeling and the distributional hypothesis: Order word matters pre-training for little, in Proc. EMNLP, 2021."
        }
    ],
    "affiliations": [
        "HJ AILAB",
        "Korea Advanced Institute of Science and Technology, South Korea",
        "Pohang University of Science and Technology, South Korea"
    ]
}