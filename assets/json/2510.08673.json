{
    "paper_title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation",
    "authors": [
        "Kang Liao",
        "Size Wu",
        "Zhonghua Wu",
        "Linyi Jin",
        "Chao Wang",
        "Yikai Wang",
        "Fei Wang",
        "Wei Li",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research."
        },
        {
            "title": "Start",
            "content": "THINKING WITH CAMERA: UNIFIED MULTIMODAL MODEL FOR CAMERA-CENTRIC UNDERSTANDING AND GENERATION Kang Liao1 Size Wu1 Zhonghua Wu2 Linyi Jin Chao Wang4 Yikai Wang1 Fei Wang2 1S-Lab, Nanyang Technological University Wei Li1 2SenseTime Research Chen Change Loy 3University of Michigan 4Max-Planck Institute for Informatics Website: https://kangliao929.github.io/projects/puffin 5 2 0 2 9 ] . [ 1 3 7 6 8 0 . 0 1 5 2 : r Figure 1: Illustration of the versatile capabilities of our Puffin model. It unifies camera-centric generation (a) and understanding (b), supports the thinking mode (c), and enables diverse applications (d)."
        },
        {
            "title": "ABSTRACT",
            "content": "Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffins superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research."
        },
        {
            "title": "INTRODUCTION",
            "content": "For machines, cameras serve as the primary interface to the physical world, enabling spatial intelligence that underlies applications such as robotics, AR/VR, and autonomous driving. In general, two principal camera-centric objectives work in tandem to enable machines to perceive and interact with their spatial context. On the one hand, understanding the camera geometry from images (Pollefeys et al., 1999; Hold-Geoffroy et al., 2018; Liao et al., 2023; Zhang et al., 2024; Veicht et al., 2024; Lin et al., 2025c), namely how the 3D world is projected onto the 2D image plane, lays the foundation for machines to recover spatial structure and navigate complex environments. On the other hand, by modulating intrinsic and extrinsic parameters, cameras encode spatial relationships and offer flexible control over spatial content generation (He et al., 2024; Wang et al., 2024b; Ren et al., 2025; Yuan et al., 2025; Bernal-Berdun et al., 2025; Ball et al., 2025), which simulates how the world appears from any viewpoint or orientation. To date, these two perspectives have been commonly treated as isolated problems and independently explored by the research community. In this work, we make the first attempt to unify camera-centric understanding and generation in cohesive framework. Motivated by recent progress in unified understanding and generation with large multimodal models (LMMs) (Team, 2024; Wu et al., 2025d; 2024; Pan et al., 2025; Wu et al., 2025b), we extend this paradigm to the spatial domain, where camera geometry plays central role. However, unlike language or images, camera parameters are abstract and non-intuitive: they describe FoV, orientation, or perspective in numerical form rather than semantic content. This discrepancy introduces modality gap when integrating cameras into LMMs. For instance, when users specify 20 roll or 35mm lens for controllable generation, existing models often ignore or misinterpret such cues, pursuing semantic alignment while neglecting precise spatial control. Similarly, current LMMs tend to collapse geometric details into coarse representations when understanding camera information, leading to spatially inconsistent outputs. As result, naïvely extending LMMs cannot resolve conflicts between modalities, producing suboptimal performance in both tasks. To address this challenge, we introduce Puffin, unified multimodal framework that interprets cameras as first-class modality. Puffin combines autoregressive and diffusion modeling to jointly perform camera-centric understanding and generation1. Instead of treating camera parameters as auxiliary labels, Puffin introduces the notion of thinking with camera, aligning spatially grounded visual cues with professional photographic terminology while reasoning over geometric context. This design provides shared chain-of-thought across multimodal tasks, enabling spatially consistent understanding and controllably aligned generation. To support this framework, we construct Puffin-4M, large-scale dataset of 4 million vision-languagecamera triplets. Puffin-4M includes single-view images with precise camera parameters, descriptive 1We mainly focus on single-view calibration and text-to-image controllable generation, but Puffin can be flexibly extended to cross-view understanding and generation via instruction tuning (see Figure 10)."
        },
        {
            "title": "Thinking with Camera",
            "content": "captions, pixel-wise camera maps, and spatial reasoning annotations across diverse indoor and outdoor scenarios. Beyond single views, it also incorporates cross-view and aesthetic images, making it versatile benchmark for both understanding and generation tasks. Experimental results show Puffin outperforms specialized models for camera-centric understanding or generation, and can be adapted to diverse downstream applications. We illustrate the versatile capabilities of our Puffin model in Figure 1. In each generated image (a), the target camera is marked at the bottom left, and the horizon lines are visualized from the estimated camera parameters (b). For world exploration (d), we visualize 3D reconstruction results derived from the initial and generated views. Our main contributions are threefold: We make the first attempt to seamlessly integrate camera geometry into unified multimodal model, introducing camera-centric framework to advance multimodal spatial intelligence. We propose thinking with camera, novel mechanism that guides the model to align spatially grounded visual cues with photographic terminology, bridging the modality gap between camera and vision-language and enabling effective spatial reasoning. We construct Puffin-4M, large-scale dataset of 4M vision-language-camera triplets spanning diverse indoor and outdoor scenes, and establish comprehensive benchmark for evaluating camera-centric multimodal models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Large Multimodal Models. Built upon visual encoder (Radford et al., 2021; Zhai et al., 2023; Tschannen et al., 2025) and large language model (LLM) (Touvron et al., 2023; Qwen et al., 2024; Cai et al., 2024; Liu et al., 2024), LMMs (Liu et al., 2023; Chen et al., 2024; Tong et al., 2024a; Bai et al., 2025; Zhu et al., 2025) process mixed visual and textual inputs and perform understanding and reasoning via language generation. Fueled by large-scale pre-training of the vision and language models and sophisticated instruction-tuning, LMMs excel at high-level understanding tasks, such as object localization, counting, and optical character recognition. However, these models, optimized for semantic alignment between vision and language, remain limited in capturing image intrinsics (e.g., depth and geometry), which constrains their ability in camera understanding and spatial reasoning. To bridge this gap, it is crucial to enrich LMMs with geometry-aware prior knowledge that preserves structural details beyond semantics. Moreover, aligning such geometric cues with linguistic tokens provides pathway to extend the reasoning capacity of LMMs from abstract semantics to physically grounded spatial understanding. Unified Multimodal Models. As an extension of standard LMMs, unified multimodal models (Team, 2024; Wang et al., 2024a; Tang et al., 2025; Wu et al., 2025d; Lin et al., 2025b; Wu et al., 2024; Tong et al., 2024b; Pan et al., 2025; Lin et al., 2025a; Wu et al., 2025c; Chen et al., 2025; Wu et al., 2025b; Xie et al., 2024; 2025a) jointly learn visual understanding and generation within single framework. Two main design philosophies are typically adopted. One line of work formulates visual generation as autoregression over either discrete (Team, 2024; Wu et al., 2024; Wang et al., 2024a; Wu et al., 2025b) or continuous (Fan et al., 2025) image tokens, sharing LLM parameters for both understanding and generation. Another line (Pan et al., 2025; Chen et al., 2025; Wu et al., 2025c; Lin et al., 2025a) aligns pre-trained LMMs with diffusion modules, enabling faster convergence and lower training cost. While both types of models advance general image understanding and generation, they are constrained to simplistic camera assumptions (e.g., fixed front-view, predefined FoVs), hindering their practical applicability to realistic and complex environments. To this end, we introduce camera-centric framework that jointly performs camera understanding and controllable generation. Camera Geometry from Vision. Tasks such as camera calibration and pose estimation have long been central topic in 3D vision (Pollefeys et al., 1999; Hartley & Zisserman, 2003; Liao et al., 2023; Veicht et al., 2024; Hold-Geoffroy et al., 2018; Jin et al., 2023; Zhang et al., 2024; Lin et al., 2025c). While earlier learning-based works attempted to directly regress camera parameters from input images (Hold-Geoffroy et al., 2018; Workman et al., 2015; Bogdan et al., 2018; Zhai et al., 2016; Kendall et al., 2015), recent advances increasingly explore the use of intermediate representations or geometry fields to bridge the prediction gap. Representative approaches (Lee et al., 2020; 2021; Song et al., 2024; Janampa & Pattichis, 2024; Yin et al., 2018) leverage geometric structures or semantic"
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure 2: Overview of the proposed Puffin. It jointly learns the camera-centric understanding and generation tasks in unified multimodal framework. The elements bounded with dotted boundaries represent the cross-view understanding and generation during instruction tuning, such as spatial imagination and world exploration. features to alleviate the inherent difficulty of inferring camera parameters from few views. Building on priors of the camera model and the perspective properties of captured images, growing body of methods proposes to learn dense geometry fields, such as distortion distribution maps (Liao et al., 2020; 2021), pixel displacement fields (Li et al., 2019; Liao et al., 2025; Xie et al., 2025b), camera rays (Zhang et al., 2024), perspective fields (Jin et al., 2023; Veicht et al., 2024; Tirado-Garín & Civera, 2025), or incidence fields (Zhu et al., 2023; He et al., 2025; Deng et al., 2024). However, such representations typically emphasize low-/mid-level patterns, limiting their ability to capture holistic and coherent spatial concept. Rather than pursuing better representations, this work explores an alternative perspective: interpreting the camera as language."
        },
        {
            "title": "3 CAMERA-CENTRIC UNIFIED MULTIMODAL MODEL",
            "content": "Puffin, as illustrated in Figure 2, unifies camera-centric understanding and generation within multimodal paradigm. For understanding, we introduce geometry-aligned vision encoder to large language model (LLM) to retain rich geometric features and enhance the models capacity for spatial analysis. For generation, connector module learns to map the hidden states of the LLM (via set of learnable queries) into conditioning signals that can be interpreted by the diffusion model. To facilitate the integration of camera geometry, apart from the discrete camera tokens derived from numerical camera parameters, we introduce continuous camera latent obtained from pixel-wise camera maps, allowing fine-grained spatial control in image generation."
        },
        {
            "title": "3.1 CAMERA UNDERSTANDING",
            "content": "Definition. In this work, camera understanding is formulated as question-answering task conditioned on image content. The generated text consists of concise description or spatial reasoning along with the estimated camera parameters (i.e., roll, pitch, FoV) of the input image. Unlike previous methods that directly estimate the parameters from images, our approach integrates camera geometry within the text and performs next-token prediction in multimodal sequence modeling paradigm. Motivation. As illustrated in Figure 3 (left), previous classical and learning-based methods focus on extracting or learning representations to predict the camera parameters, such as geometric structures (Pautrat et al., 2023) or semantic features with confidence estimates (Veicht et al., 2024). However, these representations often emphasize low-/mid-level patterns, limiting their ability to capture holistic and coherent spatial concept. As result, existing approaches tend to excel in scenarios with rich features but struggle to generalize across diverse visual environments. Thinking. Instead of focusing on how to learn representation, we propose to interpret the camera as language and introduce the notion of thinking with camera. It guides the LMMs to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. The details of each key element are elaborated below."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure 3: Methods for learning camera geometry. (Left) Previous classical and learning-based methods focused on extracting or learning representations such as geometric structures or semantic features (with confidence). (Right) We introduce the notion of thinking with camera through LMMs. It first decouples the camera parameters across geometric context, establishing connections between spatially grounded visual cues (highlighted in the masked regions) and professional photographic terms. The camera parameters are then predicted within the <answer></answer> tag through this spatial reasoning process <think></think>. Spatially Grounded Visual Cues. The 3D world is governed by physical laws, where gravity and human design shape stable spatial regularities that serve as strong perceptual priors. Texture-less regions such as sky, ceilings, floors, or ground surfaces lack local features but encode vertical regularities critical for pitch estimation. Similarly, FoV estimation relies on perceiving spatial composition, including the foregroundbackground ratio, object scale, and depth distribution. While such properties are difficult to infer from purely visual representations, they are implicitly captured by LMMs as knowledge priors. Thus, we embed these spatially grounded visual cues into our thinking captions, enabling the model to perform explicit spatial reasoning about camera geometry. Professional Photographic Terms. Existing LMMs typically acquire over-abstracted semantics, whereas the detailed numerical values of camera parameters are too fine-grained to estimate precisely. As practical alternative, professional photographic terms (e.g., close-up, tilt-up, Dutch angle) are widely used in annotations and well aligned with LMM knowledge (Liu et al., 2025; Wang et al., 2025b; Lin et al., 2025c). Thus, we leverage them as intermediate supervisory signals to naturally bridge low-/mid-level camera geometry and high-level multimodal reasoning. These terms, derived as quantized abstractions of camera parameters, are merged with textual scene descriptions, making global spatial arrangements linguistically accessible. The parameter-to-term mapping can be formulated as : (cid:55) t, in which the mapping is shown in Table A1. Geometric Context. As shown in Figure 3 (right), we decouple camera parameters across geometric context (roll, pitch, and FoV), which aligns specific spatially grounded visual cues such as sky, foreground composition, and object-level depth ordering with each professional photographic terminology. By anchoring numerical attributes to semantically meaningful descriptors, our framework bridges abstract visual features and physically interpretable geometry. The final parameters are predicted through this structured spatial reasoning. With the above designs, we interpret the camera as language by grounding its physical attributes in stable spatial regularities. Numerical parameters are abstracted into professional photographic terms, providing semantic vocabulary aligned with LMMs. Through this mapping, camera geometry becomes linguistically interpretable, allowing structured spatial reasoning for accurate camera parameter prediction. We visualize more reasoning results in Figure A2. Choosing Suitable Vision Encoder. straightforward approach to camera understanding is to fine-tune existing LMMs that couple vision encoder with an LLM, but this naïve strategy faces two major limitations: (i) vision encoders in LMMs are primarily designed for recognition tasks and thus yield condensed features lacking geometric fidelity, and (ii) language components contain little prior knowledge of spatial perception, reducing adaptability to camera-centric tasks. As result, such fine-tuning can lead to performance bottlenecks and even underperform pure visionbased methods (see Section 5.4). To overcome these issues, we introduce geometry-aligned vision encoder distilled from both semantic (e.g., CLIP, SigLIP) and vision-centric (e.g., DINO, SAM) teachers (Heinrich et al., 2025), offering versatile features that preserve geometric fidelity while"
        },
        {
            "title": "Thinking with Camera",
            "content": "maintaining strong semantic understanding. We then align this encoder with an LLM (Qwen et al., 2024) via progressive unfreezing and joint fine-tuning. This staged optimization stabilizes training and fosters spatial awareness that bridges low-/mid-level structural cues with high-level linguistic reasoning. The detailed training recipe is provided in Section 3.4."
        },
        {
            "title": "3.2 CAMERA-CONTROLLABLE GENERATION",
            "content": "Motivation. Unlike image understanding, image generation requires complex cross-modal alignment and the synthesis of fine-grained visual details. As discussed in Section 3.1, the detailed numerical values of camera parameters are too specific for current LMMs to interpret effectively, failing to faithfully capture the realistic spatial distribution required for camera-controllable generation. Thinking. To address this, we design step-by-step process that integrates visual detail analysis with reasoning. The model first infers the potential visual cues from vanilla captions, and then uses this textual reasoning as semantic planning stage to guide image generation. For instance, large pitch value may correspond to an expansive sky with clouds in outdoor scenes or to pendant lights and uncluttered ceilings indoors. Beyond textual reasoning, numerical camera parameters are translated into professional photographic terms more suitable for LMMs, naturally aligning with the reasoning process in camera understanding. We therefore adopt shared chain-of-thought mechanism between understanding and controllable generation. As shown in Figure 1 (c), given small pitch value and caption describing modern interior, our method translates the value into photographic term (e.g., small tilt-down), imagines salient cues such as windowsill, and produces more precise spatial simulation than the baseline. Flexible and Faithful Control. The pipeline of camera-controllable generation is shown in Figure 2 (right). The key design is to incorporate pixel-wise camera maps as continuous latent of camera geometry, apart from the discrete camera tokens derived from numerical parameters. Unlike tokens that capture only global attributes, these dense maps encode local geometric context at each pixel, including orientation and displacement cues (Jin et al., 2023). By converting maps into continuous latent, the diffusion model receives fine-grained spatial priors that preserve global camera settings while adapting to subtle geometric variations, thus offering flexible control of spatial layout and viewpoint. Additionally, we introduce connector module as an adaptive interface between the LLM and the diffusion model, where set of learnable queries together with text and camera tokens extract and restructure LLM hidden representations, which are then projected into conditioning signals for generation Pan et al. (2025); Wu et al. (2025c). This design enables semantic and geometric understanding from the LLM to faithfully guide the diffusion model. 3."
        },
        {
            "title": "INSTRUCTION TUNING",
            "content": "Although our Puffin focuses on single-view camera calibration and text-to-image controllable generation, it can be flexibly extended to cross-view settings with only minor modifications, such as appending additional tokens and switching prompts according to the target task. As shown in Figure 2, the dotted modules denote cross-view understanding and generation. We explore three tasks: (i) spatial imagination, where the model imagines the scene description of target view given its camera parameters and an initial view; (ii) world exploration, where the model generates the target view, incorporating an additional yaw parameter to represent cross-view deviations and conditioning on both the target-view camera map and the VAE-encoded initial view (with text descriptions randomly dropped to support both text-conditioned and text-free generation); and (iii) photographic guidance, where the model suggests camera parameter adjustments from an initial view to achieve images with higher photographic aesthetics. Visualization results are presented in Figure 10."
        },
        {
            "title": "3.4 TRAINING RECIPE",
            "content": "We conduct multi-stage training strategy, where the vision encoder, LLM, and the diffusion model are aligned in the first stage. Then, in the supervised fine-tuning (SFT) stage, the models are jointly optimized using both base and thinking datasets. Finally, an instruction-tuning stage is applied, involving various cross-view generation and understanding tasks. The details are listed in Table 1. We elaborate each training stage as follows."
        },
        {
            "title": "Thinking with Camera",
            "content": "Table 1: Training recipe of Puffin. For the data sampling ratio, we mark the data involving the spatial reasoning and instruction tuning in light blue and light red , respectively. For clarity, we abbreviate the generation and understanding as Gen. and Und.. Hyperparameters Learning rate LR Scheduler Weight Decay Betas Optimizer Batch Size Training Steps Warm-up Steps LLM Diffusion Model Vision Encoder Data Sampling Ratio Text-CameraImage (Gen.) ImageText-Camera (Und.) TextText Text-CameraImage (Gen.) ImageText-Camera (Und.) Image-CameraText (Cross-view Und.) Image-(Text)-CameraImage (Cross-view Gen.) ImageCamera (Photography Und.) Stage Stage II Stage III Stage IV 1 104 2 105 1 105 5 106 Cosine 0.05 (0.9, 0.95) AdamW 1024 30K 900 Trainable Trainable Trainable 512 60K 1.8K Trainable Trainable Trainable 256 20K 600 Trainable Trainable Frozen 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.33 0.33 0.33 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.4 0.4 0. 1024 10K 1K Frozen Frozen Frozen 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 Stage I-Alignment. In this stage, we align the vision encoder with the LLM by training only the MLP projector for the understanding task, where the framework learns to predict both text descriptions and camera parameters from the input image. For generation, the framework takes text descriptions, camera parameters, and the camera map as inputs, and learns to synthesize the target image with the corresponding description and configuration. Specifically, we train learnable queries and connector to bridge the LLM and the diffusion transformer, where the connector maps LLM hidden states into conditioning signals for the diffusion model. cross-entropy loss and diffusion loss supervise the understanding and generation, respectively, while parameters of the vision encoder, LLM, and diffusion model remain frozen. Stage II-SFT. After aligning different modalities, we unfreeze all modules except the VAE and fine-tune the entire framework, using the same inputs and outputs as in Stage I. To stabilize training, we apply gradient scaling of 0.1 to the vision encoder. Stage III-SFT w/ Thinking. To further bridge the modality gap between the camera and vision-language, we introduce thinking with camera in this stage. The implementation is the same as Stage-II, except that the training data contains spatial reasoning captions (the details of obtaining such captions are provided in Section 4). Beyond generation and understanding, this stage also learns the textual reasoning task, which enriches the vanilla captions with spatially grounded visual cues and translates specific camera parameter values into professional photographic terms. Stage IV-Instruction Tuning. Finally, we improve our models ability to adapt to diverse spatial configurations. In particular, three types of cross-view data are trained simultaneously, including the spatial imagination, world exploration, and photographic guidance. The KV cache mechanism is utilized in cross-view generation. The vision encoder is frozen while other modules are trainable. We release three model variants: Puffin-Base, Puffin-Thinking, and Puffin-Instruct, to accommodate different application needs. Puffin-Base provides foundation model for unified camera understanding and camera-controllable image generation; Puffin-Thinking enhances spatial reasoning and generation; and Puffin-Instruct is optimized by instruction tuning, supporting cross-view tasks and complex multimodal interactions."
        },
        {
            "title": "4 DATASET CONSTRUCTION",
            "content": "Datasets and benchmarks that span vision, language, and camera modalities remain scarce in the domain of spatial multimodal intelligence. To address this gap, we introduce Puffin-4M, large-"
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure 4: Overview of the proposed Puffin-4M dataset. It consists of 4 million vision-language-camera triplets under various scenarios and camera configurations. We mark the sample images with different colors, each denoting different variant of the camera configurations. scale, high-quality dataset comprising 4 million vision-language-camera triplets. The overview of our Puffin-4M is shown in Figure 4. The construction of this curated dataset consists of four stages: panoramic data collection and preprocessing, perspective image generation, scene and spatial reasoning captioning, and extensions for cross-view scenarios. Following previous works (Veicht et al., 2024; Jin et al., 2023; Bernal-Berdun et al., 2025; Hold-Geoffroy et al., 2018), we render the perspective images with various camera intrinsic and extrinsic parameters from 360 panoramic images using standard camera model. The pipeline of the dataset construction is illustrated in Figure 5 and the dataset comparison with previous works is listed in Table 2. More details are described as follows. Panoramic Data Collection and Preprocessing. We begin by collecting panoramic images from publicly available datasets (Bertel et al., 2020; Choi et al., 2023; Cao et al., 2025; Huang et al., 2024; Xu et al., 2022; Zhang et al., 2020; Bernal-Berdun et al., 2023; Cheng et al., 2018; Bolduc et al., 2023; Chang et al., 2018; Veicht et al., 2024; Armeni et al., 2017) as well as from online platforms (Fli; Str; Wik; HDR; Pol; Amb; Ble; You). In addition, we acquire large volume of outdoor panoramic data from Google Street View (Goo), spanning 12 cities across Asia, Europe, and North America. In total, our curated dataset comprises approximately 200K high-quality panoramic images with substantial diversity. significant portion of these images reaches 4K resolution or higher, up to 10K. However, due to variations in 360 camera calibration and acquisition stability, some panoramas exhibit geometric distortions and misalignment. To mitigate this, we apply geometric correction techniques based on line segmentation and vanishing point estimation (Jiang et al., 2022; Zou et al., 2018), aligning the panoramas with the gravity direction and improving structural consistency. Perspective Image Generation. We adopt the pinhole camera model with varying intrinsic parameters (vertical FoV) and extrinsic parameters (roll and pitch) to synthesize perspective images, following the protocol established in recent state-of-the-art camera calibration works (Veicht et al., 2024; Jin et al., 2023). For each panoramic image, we generate multiple perspective crops by uniformly sampling roll, pitch, and vertical FoV within the ranges [45, 45], [45, 45], and [20, 105], respectively. The number of crops is adaptively determined based on the resolution of the original panorama. While our current setup assumes an ideal pinhole model, incorporating radial distortion effects via an additional distortion parameter is left as future work. After generating the perspective images, we further convert the corresponding camera parameters into pixel-wise Perspective Field (Jin et al., 2023) as camera map, where each pixel is annotated with its up-vector ux and latitude angle φx to enable"
        },
        {
            "title": "Thinking with Camera",
            "content": "Table 2: Dataset Comparisons. The datasets proposed in previous individual models tailored for camera understanding (Lee et al., 2021; Bogdan et al., 2018; Veicht et al., 2024; Jin et al., 2023; Hold-Geoffroy et al., 2018) or camera-controllable image generation (Bernal-Berdun et al., 2025) vs. our Puffin-4M for the cameracentric unified multimodal model. In addition to its larger scale, our dataset also offers advantages in spatial reasoning captions, and cross-view image pairs. For the camera parameters, we denote the intrinsic parameters: focal length (f ), radial distortion coefficient (ξ); and the extrinsic parameters: roll (ϕ), pitch (θ), yaw (ψ). Dataset Task Type Intrinsics Extrinsics # Frames GeoCalib (Veicht et al., 2024) CTRL-C (Lee et al., 2021) Deepcalib (Bogdan et al., 2018) ParamNet (Jin et al., 2023) Perceptual (Hold-Geoffroy et al., 2018) PreciseCam (Bernal-Berdun et al., 2025) Puffin-4M (Ours) Understanding Understanding Understanding Understanding Understanding Generation Unified {f, ξ} {f, ξ} {f, ξ} {ϕ, θ} {ϕ, θ} - {ϕ, θ} {ϕ, θ} {ϕ, θ} {ϕ, θ, ψ} 37K 45K 67K 190K 390K 57K 4M Details Camera Text Reasoning Single-View Cross-View Figure 5: Pipeline of the dataset construction. P2T denotes the mapping from the numerical camera parameters to the professional photographic terms. For clarity, we omit the orientations clockwise and counterclockwise of the Dutch angle in photographic terms. fine-grained spatial encoding: ux = lim c0 P(X cg) P(X) (P(X) cg) P(X)2 , φx = arcsin (cid:18) (cid:19) , (1) where P(X) = denotes the mapping of 3D point to an image pixel x, and each pixel corresponds to light ray originating from X. Scene and Spatial Reasoning Captions. Captions are essential for multimodal understanding and generation. To construct high-quality descriptions, we first utilize Qwen2.5-VL-7B-Instruct (Bai et al., 2025) to generate semantically rich captions for each perspective image. These are subsequently distilled using Qwen2.5-7B-Instruct (Qwen et al., 2024) into shorter, vivid, and visually expressive sentences. To bridge the modality gap between camera geometry and vision-language representations, we introduce the notion of thinking with camera that explicitly guides multimodal tasks using spatially grounded visual cues and professional photographic terms. For captioning such spatial reasoning process, we propose two-step strategy: first, we map the numerical camera parameters to their corresponding semantic photographic terms (see Table A1); then, for each camera parameter, we use its corresponding photographic terms to retrieve and prompt out relevant visual concepts from the LMMs prior knowledge. For the trade-off between accuracy and efficacy, we employ Qwen2.5-VL32B-Instruct to generate the final spatial reasoning captions. Cross-View Extension. In addition to the single-view dataset construction described above, we further enrich our dataset with cross-view data involving various tasks. Specifically, during cross-view"
        },
        {
            "title": "Thinking with Camera",
            "content": "perspective image generation, we extend the cameras degrees of freedom by incorporating the yaw angle in addition to roll and pitch, sampling it uniformly from [0, 360). The initial view is initialized with standard camera pose (roll = pitch = yaw = 0), and the target view is rendered using random camera pose sampled within the aforementioned ranges. The target view in each cross-view pair is then captioned using Qwen2.5-VL-7B-Instruct. For the photographic guidance task, we first consulted photography experts and enthusiasts to identify four key aspects of photographic aesthetics: viewpoint creativity, subject emphasis, compositional balance, and spatial harmonization. These are then formulated into four criteria that serve as aesthetic rating prompts for LMMs. We focus on pitch and yaw as the key controllable camera parameters2. An initial view is generated with random pitch in [20, 20], and neighboring views are sampled by perturbing pitch and yaw within the same range. All rendered views are evaluated by Qwen2.5-VL32B-Instruct using the aesthetic prompts, and scores are assigned through voting. The pitch and yaw offsets between the initial view and the highest-scoring view are taken as labels for the photographic guidance task. Prompt Design. For the scene caption of each image, the prompt is formatted as: Describe this image in 3-4 sentences. Then, we refine the caption into more compact description using: Here is detailed image description: <caption>. Rewrite it into much shorter, vivid, and visually rich sentence (one or two sentences) that captures only the most essential elements and atmosphere of the scene. Ensure the description is concise, clear, and optimized for use with text-to-image generation model. For captioning the spatial reasoning and photographic aesthetics of each image, we show the corresponding prompts and example results in Figure A1."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Network Configuration. For the architecture of our Puffin, we use the pretrained C-RADIOv3H (Heinrich et al., 2025), Qwen2.5-1.5B-Instruct (Qwen et al., 2024), and SD3-Medium (Esser et al., 2024) to initialize our geometry-aligned vision encoder, LLM, and diffusion model, respectively. Learnable queries with the number of 64 and lightweight connector comprising six transformer layers are exploited to translate the LLM hidden states to conditioning signals for the diffusion model. The resolutions of the image and camera maps are set to 512 512 for all tasks. For tokenization, the camera parameter tokenizer follows the same procedure as the text tokenizer. For the camera map, we adopt Perspective Field (Jin et al., 2023). We first normalize its values to the range [1, 1], and then reuse the image tokenizer (i.e., the VAE encoder), as the camera map also has three channels. Pretraining specialized tokenizer for camera maps is left as future work. Training Setting. The whole training process includes four stages as listed in Table 1 and takes around 4 days with 64 NVIDIA A100 (80 GB) GPUs. In reference, we use greedy search for text generation in camera understanding and set the CFG weight as 4.5 for the camera-controllable image generation. Prompt template for each task is shown in Section A1.2."
        },
        {
            "title": "5.2 EVALUATIONS ON CAMERA UNDERSTANDING",
            "content": "Settings. Following prior works, we compare our method against range of learning-based camera calibration approaches, including DeepCalib (Lopez et al., 2019), Perceptual (Hold-Geoffroy et al., 2018), CTRL-C (Lee et al., 2021), MSCC (Song et al., 2024), ParamNet (Jin et al., 2023), and GeoCalib (Veicht et al., 2024), as well as traditional methods such as SVA (Lochman et al., 2021) and UVP (Pautrat et al., 2023). For each image, gravity estimation is evaluated using the angular errors in roll and pitch, while focal length is evaluated through the error in vertical FoV. For all metrics, we report both the median error and the Area Under the Recall Curve (AUC) at thresholds of 1, 5, and 10. We conduct evaluations on three common datasets, MegaDepth (Li & Snavely, 2Both professional and amateur users generally prefer near level-shot photography, as humans are highly sensitive to horizontal perturbations (Howard & Templeton, 1966; Dyde et al., 2006). Thus, we fix roll at 0 for this task."
        },
        {
            "title": "Thinking with Camera",
            "content": "Table 3: Evaluation results on camera understanding. The comparison methods are evaluated on the public datasets: MegaDepth (Li & Snavely, 2018), TartanAir (Wang et al., 2020), and LaMAR (Sarlin et al., 2022), and our constructed challenging benchmark Puffin-Und. We color the best and second best results. Approach DeepCalib (Lopez et al., 2019) Perceptual (Hold-Geoffroy et al., 2018) CTRL-C (Lee et al., 2021) MSCC (Song et al., 2024) ParamNet (Jin et al., 2023) SVA (Lochman et al., 2021) UVP (Pautrat et al., 2023) GeoCalib (Veicht et al., 2024) Puffin (Ours) DeepCalib (Lopez et al., 2019) Perceptual (Hold-Geoffroy et al., 2018) CTRL-C (Lee et al., 2021) MSCC (Song et al., 2024) ParamNet (Jin et al., 2023) SVA (Lochman et al., 2021) UVP (Pautrat et al., 2023) GeoCalib (Veicht et al., 2024) Puffin (Ours) DeepCalib (Lopez et al., 2019) Perceptual (Hold-Geoffroy et al., 2018) CTRL-C (Lee et al., 2021) MSCC (Song et al., 2024) ParamNet (Jin et al., 2023) SVA (Lochman et al., 2021) UVP (Pautrat et al., 2023) GeoCalib (Veicht et al., 2024) Puffin (Ours) DeepCalib (Lopez et al., 2019) CTRL-C (Lee et al., 2021) MSCC (Song et al., 2024) ParamNet (Jin et al., 2023) UVP (Pautrat et al., 2023) GeoCalib (Veicht et al., 2024) Puffin (Ours) p g i t R L - fi Roll [degrees] Pitch [degrees] FoV [degrees] error AUC 1/5/10 error AUC 1/5/10 error AUC 1/5/10 1.41 1.07 0.88 0.90 1.17 - 0.51 0.36 0.32 1.95 2.24 1.68 3.50 1.63 9.48 0.89 0.43 0.40 1.15 1.29 1.20 1.44 0.93 - 0.38 0.28 0.38 1.90 4.69 4.40 2.11 2.03 0.92 0.41 34.6 47.9 54.5 53.1 43.4 31.9 69.2 82.6 84. 24.7 23.2 32.8 15.0 34.5 32.4 52.1 71.3 71.7 44.1 40.0 43.5 39.6 51.7 8.6 72.7 86.4 80.6 29.3 20.3 17.4 24.9 32.7 53.6 78.3 65.4 72.4 75.0 72.8 70.7 35.0 81.6 90.6 93.4 55.4 48.6 59.1 37.2 59.2 39.6 64.8 83.8 86.2 73.9 68.9 70.9 60.7 77.0 9.2 81.8 92.5 89. 56.2 35.2 34.7 53.6 46.4 73.9 91.0 79.4 83.2 84.2 82.1 82.2 36.2 86.9 94.0 96.2 71.5 66.7 74.1 57.7 73.9 44.1 71.9 89.8 92.1 84.8 81.6 82.5 72.8 86.0 9.7 85.7 95.0 93.5 71.7 46.7 47.9 71.5 54.9 82.6 95.2 5.19 3.49 4.80 5.73 3.99 - 4.59 1.94 1. 3.27 2.86 2.39 3.48 3.05 18.46 2.48 1.49 0.95 4.68 2.83 1.94 3.02 2.15 - 1.34 0.87 0.71 3.71 8.43 6.87 3.40 9.04 2.18 0.74 11.9 19.8 16.6 19.0 15.4 13.6 21.6 32.4 47.6 16.3 23.5 24.6 18.8 19.4 21.2 36.2 38.2 51.0 10.8 21.2 27.6 20.9 27.0 3.4 42.3 55.0 61. 15.3 10.8 13.1 16.1 11.4 28.9 60.2 27.8 39.1 33.2 33.2 34.5 20.6 36.2 53.3 68.2 38.8 44.6 48.6 38.6 42.0 28.8 48.8 62.9 68.2 28.3 44.7 54.7 41.8 52.7 5.7 59.9 76.9 78.9 36.0 24.6 26.3 38.7 22.6 52.5 81.2 44.8 54.2 46.5 44.3 53.3 24.9 47.4 67.5 79. 58.5 61.5 65.2 54.3 60.3 34.5 58.6 76.6 79.3 49.8 62.6 70.2 55.7 70.2 7.0 69.4 86.2 86.4 54.9 36.1 38.9 58.6 32.5 69.6 90.0 11.14 13.40 18.65 10.80 11.01 - 10.92 4.46 2.42 8.07 15.06 5.64 11.18 8.21 43.01 9.15 4.90 7.48 10.93 17.78 5.64 14.78 14.71 - 5.57 3.03 3. 7.43 11.70 9.79 6.21 18.80 5.04 1.21 5.6 2.9 2.0 6.0 3.2 9.4 8.2 13.6 23.9 1.5 5.1 10.7 4.4 6.0 8.8 15.8 14.1 16.3 0.7 3.0 9.8 3.2 2.8 1.2 15.6 19.1 17.0 9.0 5.3 6.8 9.4 5.0 12.4 42.4 12.1 8.2 5.8 14.6 10.1 16.1 18.7 31.7 47. 8.8 8.9 25.4 11.8 16.8 16.1 25.8 30.4 28.5 13.0 5.3 24.6 8.3 6.8 2.7 30.6 41.5 37.3 19.4 12.7 16.3 22.3 12.1 28.0 70.5 22.9 16.8 12.8 26.2 21.3 21.1 29.8 48.2 64.1 27.2 17.1 43.5 23.0 31.6 21.6 35.7 47.6 39.0 24.0 10.7 43.2 16.8 14.3 4.1 43.5 60.0 53. 34.8 23.5 29.0 39.8 19.9 45.8 84.3 2018), TartanAir (Wang et al., 2020), and LaMAR (Sarlin et al., 2022). Notably, images from these datasets are primarily captured or simulated in well-structured environments, where buildings, rooms, or trees occupy substantial portion of the scene. Moreover, the camera parameters in some datasets are limited in distribution; for instance, TartanAir uses single FoV for all images. To complement these settings, we construct more challenging dataset, Puffin-Und, designed for comprehensive assessment of camera understanding. This dataset contains 1,000 images spanning diverse camera configurations and scenarios. Comparison Results. As shown in Table 3, our method outperforms the baselines on MegaDepth and Puffin-Und, and achieves comparable results on TartanAir and LaMAR. Due to the fixed image resolution in our training data (512 512), we adopt central cropping strategy followed by resizing to rectangular inputs for evaluating non-square images. The vertical FoV is then computed from the predicted value and scaled according to the crop ratio. Nevertheless, this procedure may discard semantically valid content and thereby degrade our camera understanding performance, particularly when the aspect ratio deviates substantially from unity, as in LaMAR, where Puffin slightly underperforms the state-of-the-art method (Veicht et al., 2024). While this limitation is orthogonal to our current exploration, it could be potentially mitigated in future work by constructing multi-scale training dataset. We present the horizon lines derived from the predicted camera parameters of different methods in Figure A3. Compared to prior approaches, our Puffin demonstrates strong performance not only in common scenarios such as architectural and indoor scenes, but also in challenging cases characterized by limited geometric features or significantly tilted camera poses. These results highlight the robustness of the proposed method. Discussion. From the quantitative evaluation of existing methods, we observe that estimating pitch and FoV is considerably more challenging than estimating roll. This difference arises from the nature of"
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure 6: Comparison results on controllable generation. We visualize the generated image along with its camera map (latitude or up vector, estimated by (Veicht et al., 2024)), error map to the GT camera map, and the median error. The caption and target camera map are presented at the bottom of each comparison. Table 4: Camera-controllable generation evaluation on Puffin-Gen. When evaluating multimodal models, we convert the camera parameters from radians to degrees* or express them using standard photographic terms. Approach Up Vector [degrees] Latitude [degrees] Gravity [degrees] mean error median error mean error median error mean error median error GPT-4o* (OpenAI, 2025) GPT-4o (OpenAI, 2025) Qwen-Image* (Wu et al., 2025a) Qwen-Image (Wu et al., 2025a) Nano Banana* (Google DeepMind, 2025) Nano Banana (Google DeepMind, 2025) PreciseCam (Bernal-Berdun et al., 2025) Puffin (Ours) 24.11 24.07 23.80 23.98 24.08 24.65 18.66 11.94 22.86 22.10 22.73 22.60 23.13 23.50 17.47 10.12 15.87 14.67 15.76 15.92 16.66 15.80 12.49 6.34 13.67 12.43 13.90 13.92 15.05 13.98 9.99 4. 28.08 27.19 27.75 27.86 28.78 28.22 18.39 6.79 27.39 26.32 27.22 26.45 28.22 26.73 15.34 3.43 the underlying visual cues. Roll estimation is supported by low-/mid-level geometric representations, such as edges and vanishing lines, which are directly embedded in the image structure and thus relatively straightforward to learn. In contrast, accurate estimation of pitch and FoV requires more extensive contextual priors. Unlike previous vision-based approaches, our method explicitly models the relationship between physical camera parameters and spatial context using large multimodal model. This integration allows the model to capture spatially contextual knowledge that cannot be sufficiently represented through visual features alone. We visualize additional camera understanding results (with camera maps converted from the predicted camera parameters) on diverse inputs, including AIGC images (OpenAI, 2025) and real-world photographs, in Figure A4."
        },
        {
            "title": "5.3 EVALUATIONS ON CAMERA-CONTROLLABLE GENERATION",
            "content": "Settings. We evaluate our generation performance against the state-of-the-art method PreciseCam (Bernal-Berdun et al., 2025). In addition, we compare our approach with recent powerful multimodal models, including GPT-4o (OpenAI, 2025), Qwen-Image (Wu et al., 2025a), and Nano Banana (Google DeepMind, 2025), using the same captions as our method. The prompt templates are"
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure 7: The predicted vs. ground truth camera parameters across all generated samples. Compared with previous methods, our generated results well align with the distribution of the ground truth camera parameters. shown in Section A1.2. To mitigate the data gap for the multimodal models, we convert the camera parameters in captions from radians to degrees or express them using professional photographic terms. For quantitative evaluation, we adopt an offline method (Veicht et al., 2024) to estimate pixel-wise camera maps. Using the ground truth maps, we then compute the mean and median errors of the up vector, latitude, and gravity, all measured in degrees. Since no benchmark dataset exists for text-toimage generation with precise camera parameters, we construct Puffin-Gen to fill this gap. The dataset consists of 650 captioncamera pairs spanning diverse scenarios and camera configurations. We will release Puffin-Gen and Puffin-Und to support standardized evaluation and facilitate subsequent works. Comparison Results. We report quantitative and qualitative results in Table 4 and Figure 6. Our method outperforms existing multimodal models by large margin across all metrics. While these models produce high-quality and aesthetically pleasing images, they fail to ensure spatially consistent layouts under specific camera configurations. PreciseCam (Bernal-Berdun et al., 2025) provides effective control but often generates monotonous stylized outputs (e.g., anime) with limited diversity, and struggles with challenging configurations such as significantly tilted poses. In contrast, our method generalizes well across diverse scenarios and camera settings, demonstrating strong practicality for real-world image generation. Additional generated results are shown in Figure A5 and parameterspecific controls in Figure A6. Discussion. We further conduct an in-depth analysis to understand why existing image generation models fail to achieve accurate spatial simulation. Specifically, we decouple the spatial distributions of the generated images with respect to three camera parameters: roll, pitch, and FoV. As illustrated in Figure 7, we visualize scatter plots of the predicted vs. ground truth camera parameters (with the reference line = x) across all generated samples. For fairness, the predicted camera parameters are obtained using the state-of-the-art vision-based camera calibration method (Veicht et al., 2024). Interestingly, we observe reversed role of camera parameters in controllable generation compared with camera understanding. Specifically, images generated by previous methods (OpenAI, 2025; Wu et al., 2025a; Google DeepMind, 2025; Bernal-Berdun et al., 2025) exhibit poor simulation accuracy on roll compared to pitch, where the predicted roll values fail to align with the target ground truth. In contrast, roll estimation in camera understanding is generally easier than pitch, due to its explicit link with geometric structures. We attribute this discrepancy to two main factors: (i) Most existing image generation models are trained on datasets curated for high visual aesthetics. Both professional and amateur photographers tend to prefer near-level shots, as humans are sensitive to horizontal perturbations (He et al., 2013; Howard & Templeton, 1966; Dyde et al., 2006). Consequently, variations in roll often conflict with"
        },
        {
            "title": "Thinking with Camera",
            "content": "Table 5: Ablation study on camera understanding. We evaluate our method with different architectures and the mode of thinking with camera. Approach Roll [degrees] Pitch [degrees] FoV [degrees] error AUC 1/5/10 error AUC 1/5/10 error AUC 1/5/10 InternVL3 (Zhu et al., 2025) Qwen2.5-VL (Bai et al., 2025) Vision Encoder (Heinrich et al., 2025) Ours Ours w/ Thinking 0.91 0.79 0.55 0.47 0.41 53.7 58.8 69.0 75.6 78.3 75.5 78.0 86.2 89.7 91.0 85.6 86.5 92.6 94.6 95.2 1.72 1.61 1.00 0.91 0.74 31.9 36.4 49.8 54.2 60. 59.7 62.4 74.1 77.5 81.2 76.3 78.0 85.9 87.9 90.0 2.96 2.91 1.87 1.48 1.21 19.7 19.4 28.3 38.0 42.4 43.1 42.5 57.9 66.2 70.5 63.5 62.5 75.9 81.5 84. Figure 8: Ablation study on the camera-controllable generation. We evaluate the effectiveness of the thinking mode (left) and the precise geometric encoding provided by camera map (right). aesthetic preferences, leading to skewed dataset distribution with far fewer roll variants compared to pitch or FoV. (ii) Roll directly alters the perceived gravity direction in an image, thereby reformulating the common sense of spatial layout. For instance, strong Dutch angle can make the sea surface appear above the horizon line, creating an inverted spatial illusion. Such cases are inherently more difficult to simulate, whereas pitch and FoV changes typically only affect the viewing scope without fundamentally disrupting the physical law."
        },
        {
            "title": "5.4 ABLATION STUDIES",
            "content": "Architecture. As discussed in Section 3.1, directly fine-tuning the existing VLMs yields significant performance bottleneck since their vision encoders learn overly condensed high-level features and language models have little prior knowledge of spatial perception. As listed in Table 5, directly finetuning the current VLMs (Bai et al., 2025; Zhu et al., 2025) even underperforms the vision-only network. To this end, we carefully pair an LLM (Qwen et al., 2024) with the vision encoder (Heinrich et al., 2025); both of them are first aligned and then fine-tuned on the camera understanding dataset. By jointly integrating the geometric perception and context understanding in staged optimization manner, our method (Ours) outperforms the above approaches on all evaluation metrics. Thinking with Camera. To mitigate the modality gap between camera and visionlanguage, we introduce thinking with camera. For camera understanding, we align spatially grounded visual cues with photographic terms across geometric context, enabling LMMs to predict camera parameters through structured spatial reasoning. As shown in Table 5, this design (Ours w/ Thinking) consistently improves performance, especially for pitch and FoV prediction that depend on broader contextual priors, demonstrating the frameworks ability to capture hierarchical spatial context beyond localized geometric cues. Thinking with camera also enhances camera-controllable generation: given prompt with scene descriptions and target parameters, the model infers potential spatial cues and uses them as semantic planning stage to guide synthesis. As illustrated in Figure 8 (left), it emphasizes visual cues such as ceilings under large tilt-up, yielding more accurate spatial simulation. Camera Parameters vs. Camera Map. Beyond discrete camera tokens derived from explicit numerical parameters, we further introduce continuous representation of camera geometry via pixel-wise camera maps for controllable image generation. We show the effectiveness of the precise geometric encoding provided by camera map in Figure 8 (right). Compared to numerical values of the camera parameters, the camera map encodes the local geometric context at each pixel, including orientation and spatial displacement clues, offering precise control over spatial layout and viewpoint. Without the camera map as conditions, generated images may exhibit severe geometric distortions and inverted spatial illusions under challenging camera configurations."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure 10: Applications of the proposed Puffin. Our method can help 3D object insertion into wild image by predicting its camera parameters. Additionally, it can flexibly extend to various cross-view tasks such as the spatial imagination, world exploration, and photographic guidance, by instruction tuning*. Single Task vs. Unified Training. In addition to performing multimodal tasks within unified framework, we aim to exploit the mutual benefits between understanding and generation through joint training. Unlike previous works (Pan et al., 2025; Wu et al., 2025c), we jointly optimize both the LLM and the diffusion model across understanding and generation tasks. This strategy avoids the representational bottleneck imposed by frozen modules and fosters bidirectional synergy between the two tasks. As illustrated in Figure 9(a)(c), training the camera understanding component in isolation underperforms compared to the unified framework, as the generation process contributes auxiliary diffusion loss at the low-level appearance, which implicitly enhances detailed geometric perception. While the performance gain for generation is less pronounced than for understanding in Figure 9(b)(d), notable improvements emerge in challenging scenarios such as FoV simulation, which requires prior knowledge regarding precise and holistic spatial understanding within an image. Figure 9: Ablation study on the mutual effect between camera understanding (a)(c) and cameracontrollable generation (b)(d) supervision."
        },
        {
            "title": "5.5 APPLICATIONS",
            "content": "We illustrate the versatile capabilities of our Puffin in Figure 10. Similar to previous methods (HoldGeoffroy et al., 2018; Jin et al., 2023), Puffin can support virtual 3D object insertion into in-the-wild images by accurately predicting camera parameters. Furthermore, it can be flexibly extended to range of cross-view tasks through instruction tuning, such as spatial imagination, world exploration, and photographic guidance. For both the initial and generated views in world exploration, we visualize 3D reconstruction results with VGGT (Wang et al., 2025a), showing proper spatial consistency across viewpoints. Additional results are presented in Figure A7, A8, A9."
        },
        {
            "title": "5.6 LIMITATION AND FUTURE WORK",
            "content": "Because our training dataset is constructed at fixed resolution of 512 512, Puffins image generation is currently restricted to single scale. For camera understanding, we applied central cropping followed by resizing (Section 5.2), an operation that may discard semantically valid content and degrade performance, particularly when the aspect ratio deviates significantly from unity. While"
        },
        {
            "title": "Thinking with Camera",
            "content": "these limitations are orthogonal to our main focus, they could be addressed in future work by building multi-scale training datasets. Beyond data design, our evaluation of camera-controllable generation relied on an offline vision-based calibration method (Veicht et al., 2024). Although this approach reflects the best available practice, the calibration errors it reports can be ambiguous, especially for generated images exhibiting only subtle spatial differences. Accurately evaluating spatial simulation thus remains an open challenge and is crucial for advancing camera-controllable generation. We plan to address this by incorporating stronger camera understanding models as evaluators and by designing benchmarks that more precisely capture geometric consistency. In addition, we aim to further enhance Puffins cross-view capability and extend it to camera-centric video generation and understanding, paving the way for broader applications in dynamic and immersive environments."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce Puffin, unified multimodal model that jointly performs camera-centric understanding and generation across arbitrary viewpoints. These two tasks have been commonly treated as isolated problems and independently explored by the research community. Yet, in essence, they represent two complementary sides: decoding the geometry of the world and encoding it back into controllable, perceptually consistent visual content. Unlike previous unified models restricted to oversimplified front-view assumptions, Puffin eliminates the modality gap by interpreting the camera as language and leverages the notion of thinking with camera. We argue that unifying camera-centric understanding and generation anchors perception and synthesis to shared representation of camera geometry, allowing machines to reason about space more holistically and interactively. Such unified cameracentric model underpins robust spatial intelligence and fosters more versatile applications."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This research is supported by cash and in-kind funding from NTU S-Lab and industry partner(s). We thank Zhouxia Wang, Zongsheng Yue, Haiwen Diao, Zhaoxi Chen, Zhijie Shen, Xiang Li, Qiqi Gong, Edurne Bernal-Berdun, and Koki Maeda for their insightful discussions. We thank Xu Song and Juncheng Zhou for their help with the comparison methods."
        },
        {
            "title": "REFERENCES",
            "content": "Ambientcg. URL https://ambientcg.com/list?type=hdri&sort=popular. Blenderkit."
        },
        {
            "title": "URL",
            "content": "https://www.blenderkit.com/asset-gallery?query= category_subtree:hdr. Flickr360. URL https://www.flickr.com/groups/360degrees/. Google street view. URL https://maps.google.com/streetview. Hdrmaps. URL https://hdrmaps.com/hdris. Poly haven. URL https://polyhaven.com/hdris. StreetView360AtoZ: 360 equirectangular street view dataset. URL https://huggingface. co/datasets/everettshen/StreetView360AtoZ. Wikimedia commons. URL https://commons.wikimedia.org. Youtube. URL https://www.youtube.com/@360swiss. Iro Armeni, Sasha Sax, Amir Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025."
        },
        {
            "title": "Thinking with Camera",
            "content": "Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aäron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 3: new frontier for world models. 2025. Edurne Bernal-Berdun, Daniel Martin, Sandra Malpica, Pedro Perez, Diego Gutierrez, Belen Masia, IEEE and Ana Serrano. D-sav360: dataset of gaze scanpaths on 360 ambisonic videos. Transactions on Visualization and Computer Graphics, 29(11):43504360, 2023. Edurne Bernal-Berdun, Ana Serrano, Belen Masia, Matheus Gadelha, Yannick Hold-Geoffroy, Xin Sun, and Diego Gutierrez. Precisecam: Precise camera control for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 27242733, 2025. Tobias Bertel, Mingze Yuan, Reuben Lindroos, and Christian Richardt. Omniphotos: casual 360 vr photography. ACM Transactions on Graphics (TOG), 39(6):112, 2020. Oleksandr Bogdan, Viktor Eckstein, Francois Rameau, and Jean-Charles Bazin. Deepcalib: deep learning approach for automatic intrinsic calibration of wide field-of-view cameras. In Proceedings of the 15th ACM SIGGRAPH European Conference on Visual Media Production, pp. 110, 2018. Christophe Bolduc, Justine Giroux, Marc Hébert, Claude Demers, and Jean-François Lalonde. Beyond the pixel: photometrically calibrated hdr dataset for luminance and color prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 80718081, 2023. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Zidong Cao, Jinjing Zhu, Weiming Zhang, Hao Ai, Haotian Bai, Hengshuang Zhao, and Lin Wang. Panda: Towards panoramic depth anything with unlabeled panoramas and mobius spatial augmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 982992, 2025. Shih-Hsiu Chang, Ching-Ya Chiu, Chia-Sheng Chang, Kuo-Wei Chen, Chih-Yuan Yao, Ruen-Rone Lee, and Hung-Kuo Chu. Generating 360 outdoor panorama dataset with reliable sun position estimation. In SIGGRAPH Asia 2018 Posters, pp. 12. 2018. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024. Dachuan Cheng, Jian Shi, Yanyun Chen, Xiaoming Deng, and Xiaopeng Zhang. Learning scene illumination by pairwise photos from rear and front mobile cameras. In Computer Graphics Forum, volume 37, pp. 213221, 2018. Changwoon Choi, Sang Min Kim, and Young Min Kim. Balanced spherical grid for egocentric view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1659016599, 2023."
        },
        {
            "title": "Thinking with Camera",
            "content": "Junyuan Deng, Wei Yin, Xiaoyang Guo, Qian Zhang, Xiaotao Hu, Weiqiang Ren, Ping Tan, et al. Boost 3d reconstruction using diffusion-based monocular camera calibration. arXiv preprint arXiv:2411.17240, 2024. Richard Dyde, Michael Jenkin, and Laurence Harris. The subjective visual vertical and the perceptual upright. Experimental Brain Research, 173(4):612622, 2006. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. Google DeepMind. Gemini 2.5 flash image, 2025. URL https://developers.googleblog. com/en/introducing-gemini-2-5-flash-image/. Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge University Press, 2003. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. Kaiming He, Huiwen Chang, and Jian Sun. Content-aware rotation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 553560, 2013. Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, and Dongyan Guo. Diffcalib: Reformulating monocular camera calibration as diffusion-based dense incident map generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 34283436, 2025. Greg Heinrich, Mike Ranzinger, Hongxu Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. Radiov2. 5: Improved baselines for agglomerative vision foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2248722497, 2025. Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisenmann, Matthew Fisher, Emiliano Gambaretto, Sunil Hadap, and Jean-François Lalonde. perceptual measure for deep single image camera calibration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 23542363, 2018. Ian P. Howard and William B. Templeton. Human Spatial Orientation. John Wiley & Sons, 1966. Huajian Huang, Changkun Liu, Yipeng Zhu, Hui Cheng, Tristan Braud, and Sai-Kit Yeung. 360loc: dataset and benchmark for omnidirectional visual localization with cross-device queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2231422324, 2024. Sebastian Janampa and Marios Pattichis. Sofi: Multi-scale deformable transformer for camera calibration with enhanced line queries. arXiv preprint arXiv:2409.15553, 2024. Zhigang Jiang, Zhongzheng Xiang, Jinhua Xu, and Ming Zhao. Lgt-net: Indoor panoramic room layout estimation with geometry-aware transformer network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, and David Fouhey. Perspective fields for single image camera calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1730717316, 2023. Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: convolutional network for real-time 6-dof camera relocalization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 29382946, 2015."
        },
        {
            "title": "Thinking with Camera",
            "content": "Jinwoo Lee, Minhyuk Sung, Hyunjoon Lee, and Junho Kim. Neural geometric parser for single image camera calibration. In European Conference on Computer Vision, pp. 541557. Springer, 2020. Jinwoo Lee, Hyunsung Go, Hyunjoon Lee, Sunghyun Cho, Minhyuk Sung, and Junho Kim. CtrlIn Proceedings of the IEEE/CVF c: Camera calibration transformer with line-classification. International Conference on Computer Vision, pp. 1622816237, 2021. Xiaoyu Li, Bo Zhang, Pedro Sander, and Jing Liao. Blind geometric distortion correction on images through deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 48554864, 2019. Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 20412050, 2018. Kang Liao, Chunyu Lin, Yao Zhao, and Mai Xu. Model-free distortion rectification framework bridged by distortion distribution map. IEEE Transactions on Image Processing, 29:37073718, 2020. Kang Liao, Chunyu Lin, and Yao Zhao. deep ordinal distortion estimation approach for distortion rectification. IEEE Transactions on Image Processing, 30:33623375, 2021. Kang Liao, Lang Nie, Shujuan Huang, Chunyu Lin, Jing Zhang, Yao Zhao, Moncef Gabbouj, and Dacheng Tao. Deep learning for camera calibration and beyond: survey. arXiv preprint arXiv:2303.10559, 2023. Kang Liao, Zongsheng Yue, Zhonghua Wu, and Chen Change Loy. Mowa: Multiple-in-one image warping model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025a. Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, and Ying Shan. Toklip: Marry visual tokens to clip for multimodal comprehension and generation. arXiv preprint arXiv:2505.05422, 2025b. Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, et al. Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376, 2025c. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36:3489234916, 2023. Hongbo Liu, Jingwen He, Yi Jin, Dian Zheng, Yuhao Dong, Fan Zhang, Ziqi Huang, Yinan He, Yangguang Li, Weichao Chen, et al. Shotbench: Expert-level cinematic understanding in visionlanguage models. arXiv preprint arXiv:2506.21356, 2025. Yaroslava Lochman, Oles Dobosevych, Rostyslav Hryniv, and James Pritts. Minimal solvers for In Proceedings of the IEEE/CVF Winter single-view lens-distorted camera auto-calibration. Conference on Applications of Computer Vision, pp. 28872896, 2021. Manuel Lopez, Roger Mari, Pau Gargallo, Yubin Kuang, Javier Gonzalez-Jimenez, and Gloria Haro. Deep single image camera calibration with radial distortion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1181711825, 2019. OpenAI. Introducing 4o image generation, 2025. URL https://openai.com/index/ introducing-4o-image-generation/."
        },
        {
            "title": "Thinking with Camera",
            "content": "Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Rémi Pautrat, Shaohui Liu, Petr Hruby, Marc Pollefeys, and Daniel Barath. Vanishing point estimation in uncalibrated images with prior gravity direction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1411814127, 2023. Marc Pollefeys, Reinhard Koch, and Luc Van Gool. Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters. International Journal of Computer Vision, 32(1):725, 1999. Yang Qwen, Baosong Yang, Zhang, Hui, Zheng, Yu, Chengpeng Li, Liu, Huang, Wei, et al. Qwen2.5 technical report. arXiv preprint, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PmLR, 2021. Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video In Proceedings of the Computer Vision and Pattern generation with precise camera control. Recognition Conference, pp. 61216132, 2025. Paul-Edouard Sarlin, Mihai Dusmanu, Johannes Schönberger, Pablo Speciale, Lukas Gruber, Viktor Larsson, Ondrej Miksik, and Marc Pollefeys. Lamar: Benchmarking localization and mapping for augmented reality. In European Conference on Computer Vision, pp. 686704. Springer, 2022. Xu Song, Hao Kang, Atsunori Moteki, Genta Suzuki, Yoshie Kobayashi, and Zhiming Tan. Mscc: Multi-scale transformers for camera calibration. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 32623271, 2024. Hongxuan Tang, Hao Liu, and Xinyan Xiao. Ugen: Unified autoregressive multimodal model with progressive vocabulary learning. arXiv preprint arXiv:2503.21193, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Javier Tirado-Garín and Javier Civera. Anycalib: On-manifold learning for model-agnostic singleview camera calibration. arXiv preprint arXiv:2503.12701, 2025. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024a. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, and Marc Pollefeys. Geocalib: Learning single-image calibration with geometric optimization. In European Conference on Computer Vision, pp. 120. Springer, 2024."
        },
        {
            "title": "Thinking with Camera",
            "content": "Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025a. Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 49094916. IEEE, 2020. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024a. Xinran Wang, Songyu Xu, Xiangxuan Shan, Yuxuan Zhang, Muxi Diao, Xueyan Duan, Yanhua Huang, Kongming Liang, and Zhanyu Ma. Cinetechbench: benchmark for cinematographic technique understanding and generation. arXiv preprint arXiv:2505.15145, 2025b. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024b. Scott Workman, Connor Greenwell, Menghua Zhai, Ryan Baltenberger, and Nathan Jacobs. Deepfocal: method for direct focal length estimation. In 2015 IEEE International Conference on Image Processing (ICIP), pp. 13691373. IEEE, 2015. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025b. Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025c. Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025d. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025a. Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo Jeni, and Zhiheng Jia. Aligndiff: Learning physically-grounded camera alignment via diffusion. arXiv preprint arXiv:2503.21581, 2025b. Hang Xu, Qiang Zhao, Yike Ma, Xiaodong Li, Peng Yuan, Bailan Feng, Chenggang Yan, and Feng Dai. Pandora: panoramic detection dataset for object with orientation. In European Conference on Computer Vision, pp. 237252. Springer, 2022. Xiaoqing Yin, Xinchao Wang, Jun Yu, Maojun Zhang, Pascal Fua, and Dacheng Tao. Fisheyerecnet: multi-context collaborative deep network for fisheye image rectification. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 469484, 2018."
        },
        {
            "title": "Thinking with Camera",
            "content": "Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, and Stanley Chan. Generative photography: Scene-consistent camera control for realistic text-to-image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 79207930, 2025. Menghua Zhai, Scott Workman, and Nathan Jacobs. Detecting vanishing points using global image context in non-manhattan world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 56575665, 2016. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023. Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. arXiv preprint arXiv:2402.14817, 2024. Yi Zhang, Lu Zhang, Wassim Hamidouche, and Olivier Deforges. fixation-based 360 {deg} benchmark dataset for salient object detection. arXiv preprint arXiv:2001.07960, 2020. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu. Tame wild camera: In-the-wild monocular camera calibration. Advances in Neural Information Processing Systems, 36:45137 45149, 2023. Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem. Layoutnet: Reconstructing the 3d room layout from single rgb image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 20512059, 2018."
        },
        {
            "title": "APPENDIX",
            "content": "Table A1: Camera parameter-to-term mapping. To align camera parameters (roll, pitch, and FoV) with the prior knowledge space of LMMs, their numerical ranges are mapped to professional photographic terms. Large counterclockwise Dutch angle Small counterclockwise Dutch angle Roll Near level shot Small clockwise Dutch angle Large clockwise Dutch angle Term (t) Example Parameter (p) [45, 20) [20, 5) [5, 5] (5, 20] (20, 45] Large tilt-down Small tilt-down Near straight-on shot Small tilt-up Large tilt-up Pitch Term (t) Example Parameter (p) [45, 20) [20, 5) [5, 5] (5, 20] (20, 45] Close-up Medium shot Wide-angle Ultra wide-angle FoV Term (t)"
        },
        {
            "title": "Example",
            "content": "Parameter (p) [20, 35) [35, 65) [65, 90) [90, 105] A"
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "A1.1 CAMERA PARAMETERS TO PHOTOGRAPHIC TERMS To bridge the gap between the detailed numerical values of camera parameters and the highly abstracted understanding capability learned by LMMs, we propose using professional photographic terms as intermediate supervision for our framework. Specifically, we quantize the range of each camera parameter and map them to the following photographic terms: (i) Roll: large counterclockwise Dutch angle, small counterclockwise Dutch angle, near level shot, small clockwise Dutch angle, large clockwise Dutch angle; (ii) Pitch: large tilt-down, small tilt-down, near straight-on shot, small tilt-up, large tilt-up; (iii) FoV: close-up, medium shot, wide-angle, ultra wide-angle. As quantized abstractions of camera parameters, these terms are combined with textual scene descriptions to express global spatial arrangements in linguistically accessible form. The detailed mapping relationship between the camera parameters and photographic terms is listed in Table A1. A1.2 PROMPT DESIGN FOR MULTIMODAL TASKS the basic user For text-to-image controllable generation, we use the following prompt template to format instructions: User: Generate an image: <caption>n Assistant:. The user <caption> includes both the image description and the numerical camera parameters (roll, pitch, FoV, all in radians). For camera understanding, we employ the following prompt template instruction: User: <image><question>n Assistant:. to format The <question> can be set as Describe the image in detail. Then reason its spatial distribution and estimate its camera parameters (roll, pitch, and field-of-view). For cross-view camera-controllable generation, the prompt template is formatted as: User: Generate target image given an initial view: <image><caption>n Assistant:. Here, <image> denotes the initial view token from the image tokenizer, while <caption> represents the target image description along with the target camera parameters (roll, pitch, yaw, and FoV, all in radians). During cross-view instruction tuning, we randomly set the <caption> to null with probability of 0.5, thereby enabling both text-free and text-conditioned image-to-image generation. When applying the spatial reasoning paradigm, we switch to new <question> for camera understanding: Reason the spatial distribution of this image in thinking mode, and then estimate its camera parameters (roll, pitch, and field-of-view). For generation, we first enrich the vanilla prompt using our model with the template: User: <caption><question>n Assistant:. Here, <caption> refers to"
        },
        {
            "title": "Thinking with Camera",
            "content": "(a) The prompt to generate the reasoning caption for thinking with camera. (b) The prompt to generate the photographic aesthetic score for the photographic guidance task. Figure A1: Examples of the designed prompts for captioning our dataset: (a) reasoning caption, (b) photographic aesthetic caption. For each sample, we visualize the input image, the prompt template for captioning, and the caption results from LMMs (Bai et al., 2025). the vanilla image description, and <question> is Given scene description and corresponding camera parameters, merge them into coherent prompt and generate an accurate visualization that highlights visual cues for spatial reasoning. For other instruction tuning tasks, <question> is set to Given the initial view and the camera parameters of the target view with the deviation yaw angle, how would you describe the target image to build replica of the scene? for spatial imagination, and Estimate the camera parameters (roll, pitch, and field-of-view) of this image. And then predict the deviation camera yaw angle and pitch angle of the target view with high photographic aesthetics. for photographic guidance. A2 ADDITIONAL EXPERIMENTS A2.1 CAMERA UNDERSTANDING Note that our training dataset consists of images rendered from the source panoramas in Stanford2D3D (Armeni et al., 2017). Although the sampled perspective images and camera parameters differ from those in the test set (Armeni et al., 2017), we exclude these results from the main evaluation to ensure rigor and report them in Table A2 only for reference."
        },
        {
            "title": "Thinking with Camera",
            "content": "Table A2: Additional evaluation results on camera understanding. The comparison methods are evaluated on the Stanford2D3D (Armeni et al., 2017) dataset. Approach Roll [degrees] Pitch [degrees] FoV [degrees] error AUC 1/5/10 error AUC 1/5/10 error AUC 1/5/10 3 2 f S DeepCalib (Lopez et al., 2019) Perceptual (Hold-Geoffroy et al., 2018) CTRL-C (Lee et al., 2021) MSCC (Song et al., 2024) ParamNet (Jin et al., 2023) SVA (Lochman et al., 2021) UVP (Pautrat et al., 2023) GeoCalib (Veicht et al., 2024) Puffin (Ours) 1.59 2.08 3.04 3.43 1.14 - 0.52 0.40 0.26 33.8 26.8 23.2 13.5 44.6 21.7 65.3 83.1 96.6 63.9 53.8 43.0 36.8 73.9 24.6 74.6 91.8 99.0 79.2 70.7 56.9 57.3 84.8 25.8 79.1 94.8 99.4 2.58 3.17 3.43 2.64 1.94 - 0.95 0.93 0. 21.6 21.5 18.3 22.6 29.2 15.4 51.2 52.3 82.0 46.9 41.8 38.6 45.0 56.7 19.9 63.0 74.8 93.6 65.7 57.8 53.8 60.5 73.1 22.4 69.2 84.6 96.7 6.67 13.84 8.50 5.81 9.01 - 3.65 3.21 2.30 8.1 2.8 7.7 9.6 5.8 6.2 22.2 17.4 23.4 20.6 7.7 18.2 23.8 14.3 11.5 39.5 40.0 51. 37.6 16.1 31.5 41.6 27.8 15.2 51.3 59.4 71.4 We show more visualization results on the proposed thinking with camera for camera understanding in Figure A2. Qualitative evaluations of the camera understanding methods with horizon line visualization are illustrated in Figure A3. We visualize additional camera understanding results (with camera maps converted from the predicted camera parameters) on diverse inputs, including AIGC images (OpenAI, 2025) and real-world photographs, in Figure A4. A2.2 CAMERA-CONTROLLABLE GENERATION Our camera-controllable generation results with various camera configurations are shown in Figure A5, and the text-to-image generation (single-view) results with specific controls for each camera parameter are presented in Figure A6. A2.3 DOWNSTREAM APPLICATIONS We visualize more downstream application results by instruction tuning. Specifically, image-to-image generation (cross-view) results with varying yaw angles are shown in Figure A7. World exploration results are provided in Figure A8. Examples of the spatial imagination and photographic guidance are shown in Figure A9."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure A2: Visualization on our spatial reasoning process for camera understanding. We highlight the reasoned spatially grounded visual cues regarding each camera parameter using different colors: roll, pitch, and FoV."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure A3: Qualitative evaluations on the camera understanding methods with horizon line visualization. We show the common cases (with architectures or indoor) and challenging cases (with few geometric features or significant tilted camera poses) at the top and bottom, respectively."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure A4: Our camera understanding on AIGC images (OpenAI, 2025) (left) and real-world photographs (right)."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure A5: Our camera-controllable generation results with various camera configurations."
        },
        {
            "title": "Thinking with Camera",
            "content": "(a) Text-to-image generation with varying roll angles. (b) Text-to-image generation with varying pitch angles. (c) Text-to-image generation with varying FoVs. Figure A6: Text-to-image generation (single-view) with specific controls for each camera parameter."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure A7: Image-to-image generation (cross-view) with varying yaw angles. The image with red box denotes the initial view, and the others are the generated views based on the yaw deviation from the previous view."
        },
        {
            "title": "Thinking with Camera",
            "content": "Figure A8: World exploration results. The 3D reconstruction results are obtained by VGGT."
        },
        {
            "title": "Thinking with Camera",
            "content": "(a) Spatial imagination. The plausible imagination results are highlighted. (b) Photographic guidance. The suggested deviations of the camera parameters (yaw/pitch) are highlighted. Figure A9: Examples of the spatial imagination and photographic guidance."
        }
    ],
    "affiliations": [
        "Max-Planck Institute for Informatics",
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "University of Michigan"
    ]
}