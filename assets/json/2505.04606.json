{
    "paper_title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution",
    "authors": [
        "Lianghong Guo",
        "Wei Tao",
        "Runhan Jiang",
        "Yanlin Wang",
        "Jiachi Chen",
        "Xilin Liu",
        "Yuchi Ma",
        "Mingzhi Mao",
        "Hongyu Zhang",
        "Zibin Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 6 0 6 4 0 . 5 0 5 2 : r OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution LIANGHONG GUO, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China WEI TAO, Independent Researcher, China RUNHAN JIANG, Sun Yat-sen University, China YANLIN WANG, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China JIACHI CHEN, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China XILIN LIU, Huawei Cloud Computing Technologies Co., Ltd., China YUCHI MA, Huawei Cloud Computing Technologies Co., Ltd., China MINGZHI MAO, Sun Yat-sen University, China HONGYU ZHANG, Chongqing University, China ZIBIN ZHENG, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China"
        },
        {
            "title": "GitHub Repo",
            "content": "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only Corresponding author. Authors Contact Information: Lianghong Guo, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, Zhuhai, China, guolh8@mail2.sysu.edu.cn; Wei Tao, Independent Researcher, Shenzhen, China, wtao@ieee.org; Runhan Jiang, Sun Yat-sen University, Zhuhai, China, guolh8@mail2.sysu.edu.cn; Yanlin Wang, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, Zhuhai, China, wangylin36@mail.sysu.edu.cn; Jiachi Chen, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, Zhuhai, China, chenjch86@mail.sysu.edu.cn; Xilin Liu, Huawei Cloud Computing Technologies Co., Ltd., Shenzhen, China, liuxilin3@huawei.com; Yuchi Ma, Huawei Cloud Computing Technologies Co., Ltd., Shenzhen, China, mayuchi1@huawei.com; Mingzhi Mao, Sun Yat-sen University, Zhuhai, China, mcsmmz@mail.sysu.edu.cn; Hongyu Zhang, Chongqing University, Chongqing, China, hyzhang@cqu. edu.cn; Zibin Zheng, Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, Zhuhai, China, zhzibin@mail.sysu.edu.cn. This work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 2025 Copyright held by the owner/author(s). ACM 2994-970X/2025/7-ARTISSTA002 https://doi.org/10.1145/3728871 Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002: L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs failure on OmniGIRL, providing insights for future improvements. CCS Concepts: Software and its engineering Software maintenance tools. Additional Key Words and Phrases: Github Issue Resolution, Benchmark, Large Language Models ACM Reference Format: Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, and Zibin Zheng. 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution. Proc. ACM Softw. Eng. 2, ISSTA, Article ISSTA002 (July 2025), 23 pages. https://doi.org/10.1145/"
        },
        {
            "title": "1 Introduction\nThe GitHub issue resolution task aims to automatically resolve a wide variety of issues proposed by\ndevelopers in code repositories. These issues include diverse tasks such as fixing bugs, adding new\nfeatures, refactoring code, writing documentation, etc [17, 46]. Effectively addressing these issues\nis crucial for maintaining and evolving real-world software systems. With the development of large\nlanguage models (LLMs), this task has gained increasing attention [13, 19, 31, 33, 47, 50, 52, 57, 63].\nSeveral benchmarks [10, 27, 38, 59] currently exist for the GitHub issue resolution task. SWE-\nbench [27] is the first benchmark in this area, consisting of 2,294 real-world issues from 12 Python\nrepositories. Subsequently, OpenAI proposes SWE-bench Verified [38], a subset of SWE-bench with\n500 instances verified by experienced developers to provide a more robust evaluation. Additionally,\nSWE-bench-java [59] is introduced to extend the task to the Java programming language, including\n93 task instances from 6 Java repositories. Although various benchmarks have been proposed, there\nare still some limitations that prevent them from fully capturing the diversity of real-world issue\nresolution tasks. The primary limitations are as follows:",
            "content": "L1: Focusing on Single Programming Language. Existing benchmarks typically focus on issues from repositories in single programming language, such as Python or Java, which limits their capacity to assess the LLMs ability to resolve issues across multiple programming languages. L2: Limited Repository Diversity. Current benchmarks, such as SWE-bench [27], largely rely on issues from limited range of domains, e.g., scientific computing, machine learning, and visualization. To better represent real-world issue resolution tasks, more repositories from wider variety of domains are needed. L3: Ignoring Multimodal Information. Previous benchmarks focus solely on textual information in issue descriptions, overlooking multimodal information. Users often use images, such as screenshots of error messages or debugging outputs, to help illustrate issues more clearly. Without incorporating this information, LLMs may have difficulty fully understanding the issue, leading to inaccurate evaluations. In this paper, we present OmniGIRL, benchmark for GitHub Issue ResoLution that incorporates multiple aspects of diversity in programming languages, repository domains and modality of input information. We first select four most popular programming languages (i.e., Python, TypeScript, JavaScript, and Java) as target languages,1 and collect candidate list of the most widely used repositories based on download counts from package management tools (i.e., pip [3], npm [2], maven [7]). From this candidate list, we select 15 repositories from various domains and collect real-world issues from these repositories to construct task instances. After validating the collected 1https://github.blog/news-insights/research/the-state-of-open-source-and-ai/ Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002:3 task instances, we obtain total of 959 task instances covering variety of domains in four programming languages (addressing L1 and L2). In addition to textual information in issue descriptions, we find some issues include other modalities. For instance, users use images, such as screenshots of error messages, to describe issues more clearly. We manually examine issue descriptions on OmniGIRL and identify 19 instances containing images providing crucial information for resolving the issues. Moreover, we find that users sometimes share website links to online code platforms, which typically contain code reproducing the issue. We annotate these links in the dataset, offering future researchers the opportunity to explore how to leverage such resources to enhance issue resolution performance (addressing L3). We evaluate state-of-the-art LLMs on OmniGIRL, and the results show that the best-performing method, GPT-4o with the Agentless-X approach,2 resolves only 8.6% of issues, highlighting the challenges of resolving issues from repositories in multiple languages. Additionally, we observe that the Agentless-X method performs worse on TypeScript and JavaScript tasks than Java and Python. Besides, we evaluate two advanced LLMs with visual abilities in the task instances with images as visual inputs, with Claude-3.5-Sonnet achieving best resolve rate of only 10.5% and GPT-4o achieving just 1.6%. The results show that both LLMs show limited performance on issues requiring understanding images. Finally, we analyze the reasons why LLMs fail to resolve issues on OmniGIRL. First, we observe that when using the Agentless method, Claude-3.5-Sonnet often struggles to generate outputs that follow the specified format outlined in the prompt. This formatting issue leads to intermediate results that cannot be parsed correctly, preventing the generation of patches in the next stage. As result, Claude-3.5-Sonnet achieves low issue resolve rate of only 1.9% on OmniGIRL. However, we find that the resolve rate increases to 7.4% by modifying the prompt simply. Second, we find that the current LLMs have significantly lower resolve rate on issues that require modifications across multiple files compared to those requiring single-file changes. Besides, for issues needing multi-file modifications, we find models tend to modify single file, highlighting limitation in the cross-file issue resolution capabilities of LLMs. In summary, our contributions are as follows: We introduce OmniGIRL, GitHub issue resolution benchmark with multi-aspect diversity in programming languages, repository domains and modality of input information. We evaluate LLMs issue resolving abilities on OmniGIRL, revealing that current models demonstrate limited overall performance across multiple programming languages. We evaluate LLMs performance on issues that require visual information, revealing their limited capability in resolving issues with multimodal inputs. We conduct an analysis to investigate the reasons why LLMs fail to resolve issues, providing insights for improving issue resolving performance of LLMs in the future."
        },
        {
            "title": "2.1 GitHub Issue Resolution\nThe GitHub issue resolution task aims to resolve issues reported in the GitHub repository auto-\nmatically [27, 59]. When evaluating the issue resolution ability of tools, we follow the pipeline as\nshown in Figure 1. During the inference stage, the tool receives the issue descriptions along with\nthe corresponding codebase and generates a patch that contains all necessary code changes to the\noriginal codebase. In the evaluation stage, the generated patch is applied to the codebase, and all\ntest cases related to this issue are run. After obtaining the test log, we check the statuses of the test",
            "content": "2Because the Agentless [52] method is designed for Python language, we extend this method to other programming languages without changing the key design of this method. The multilingual version of this method is called Agentless-X. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:4 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Table 1. Overview of existing GitHub issue resolution datasets. Dataset Year Programming Languages Python Java TypeScript JavaScript # Repositories # Instances 2023 SWE-bench [42] SWE-bench-Java [9] 2024 SWE-bench-Verified [30] OmniGIRL 2024 12 6 12 2,294 91 500 959 w/ Visual Input Fig. 1. Overview of the evaluation pipeline of GitHub issue resolution. cases. The issue is considered resolved if all related test cases pass successfully. Some key concepts are listed below: Issue Descriptions: descriptions about reported issues in the format of texts or images. Generated Patch: generated file including all code changes in the code repository to resolve reported issues. All code changes are formatted in the GitHub diff format. Codebase: the code repository containing the reported issue and all relevant code files necessary for resolving the issue. Test Log: log file containing all test results, which is used to verify the correctness of results."
        },
        {
            "title": "2.2 LLM-based Methods for GitHub Issue Resolution\nWith the development of LLMs, many researchers attempt to explore their potential for automating\nGitHub issue resolution. Jimenez et al. [27] are the first to use LLMs, such as GPT-4, to resolve\nissues in SWE-bench [27]. However, the performances of LLMs were limited at that time, with\nthe best-performing model, Claude-2, resolving only 1.96% of issues. Inspired by the success of\nagent frameworks in software engineering tasks [23, 26, 29], some researchers propose agent-\nbased methods [18, 19, 31, 43, 47, 50, 57, 61, 63] to enhance the performance of LLMs in issue\nresolution tasks. Tao et al. [47] propose the first multi-agent-based issue resolution framework,\nMAGIS, to improve the issue resolution ability of LLMs. Yang et al. [57] propose the SWE-agent\nframework to build an LLM-based agent that can autonomously utilize designed tools to resolve\nissues. Besides, some works also utilize existing software engineering techniques [28, 32, 51] to\nimprove the performance of LLMs. Zhang et al. propose the AutoCodeRover [63] method, which\nenhances the localization of edited code by analyzing the structure of the repository. Inspired by",
            "content": "Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002:5 existing LLM-based APR tools [18, 21, 25, 53, 54, 62], Xia et al. [52] propose the Agentless method, which uses hierarchical process to find the edit location. Compared with agent-based frameworks, software engineering oriented methods can achieve comparable performance and cost less. Following previous studies [11, 40, 41], we can divide current LLM-based issue resolution methods into three types: RAG-based method, LLM workflow-based method, and LLM agent-based method. The RAG-based methods use retriever to directly retrieve similar code files from the codebase to enhance issue resolution. However, these methods, such as the BM25 retrieval-based method, demonstrate limited performance on this task [27]. The second type, the LLM workflow-based method, breaks down the issue resolution process into predefined stages, where LLMs follow fixed sequence to complete tasks. For example, Agentless [52] employs structured workflow with localization stage to retrieve relevant code and patch generation stage to generate final patch. Finally, the LLM agent-based method, such as AutoCodeRover [63] and SWE-agent [57], allow LLMs to autonomously interact with the codebase, using tools to collect key information and resolve tasks dynamically."
        },
        {
            "title": "2.3 Benchmarks for GitHub Issue Resolution\nRecently, several benchmarks have been introduced to evaluate the issue resolution abilities of LLMs,\nas summarized in Table 1. Jimenez et al. [27] propose SWE-bench, the first GitHub issue resolution\nbenchmark, comprising 2,294 resolved issues from 12 Python repositories. Subsequently, OpenAI\nreleases SWE-bench Verified [38], a subset of SWE-bench containing 500 task instances verified by\nexperienced developers to ensure correctness. Zan et al. [59] further introduce SWE-bench-java,\nwith 91 task instances from 6 Java repositories.",
            "content": "However, existing benchmarks have some limitations. Unlike other software engineering tasks [67], where benchmarks evaluate LLMs coding abilities across multiple languages [14, 16, 20, 44, 45, 55, 56, 58, 60, 6466], current issue resolution benchmarks typically focus on single language, which restricts evaluation diversity. Additionally, benchmarks like SWE-bench [27] collect data from limited domains, such as scientific computing, machine learning, and visualization. Moreover, current benchmarks focus solely on text information in issue descriptions, overlooking multimodal data such as images. To address these limitations, we propose OmniGIRL, GitHub Issue ResoLution benchmark with Omni-aspect diversity in programming languages, repository domains and modality of input information."
        },
        {
            "title": "3.1 Language and Repository Selection",
            "content": "Language Selection. Before building our multilingual benchmark, we first select widely used 3.1.1 programming languages as target languages. Based on the latest GitHub report,3 JavaScript, Python, TypeScript, and Java are among the most popular languages, reflecting their extensive use in the developer community. Besides, these languages are widely used in different domains: Python is common in AI and data science, JavaScript and TypeScript are crucial for web development, and Java is widely applied in large-scale systems and backend development. Considering the popularity and significance of these languages, we choose them as our target languages. 3https://github.blog/news-insights/research/the-state-of-open-source-and-ai/ Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:6 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Fig. 2. Overview of benchmark construction."
        },
        {
            "title": "3.2.2 Attribute-Based Filtering. To make that each collected data contains issue descriptions and\ntests to verify the correctness of submitted solutions, following the approach of SWE-bench [27],\nwe use the attribute-based filtering method to filter the collected pull requests data further:",
            "content": "Keep PRs resolving at least one issue. In the GitHub repository, when submitting pull request that resolves some issues, the developer uses statements like fix #403 in the title or body to indicate which issues are resolved. Following the implementation code of SWE-bench,4 we first gather all text content from the title, body, and commit messages of each pull request. We then extract each issue number (e.g., #403) from this aggregated content using regular expressions. Finally, we filter out pull request data without any relevant issue number. Keep PRs with test files changed. When fixing bug or adding new feature, developers often submit pull request containing code changes in test files. These test files offer great solution to verify whether the bug is fixed or the feature is implemented successfully. Following SWE-bench,4 we first identify test files in the changed files of pull request by checking if their full paths contain keywords like test or testing. Then, we filter out pull request data without any test file changed. 4https://github.com/princeton-nlp/SWE-bench/blob/main/swebench/collect/utils.py Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002: Fig. 3. An example of task instance tailwindlabs__tailwindcss-10212."
        },
        {
            "title": "3.3 Task Instance Construction\nAfter collecting raw data of pull requests, we build task instances using GitHub Developer API [6]\nto obtain key information from each pull request. An example of a task instance is shown in Figure 3.\nHere, we introduce the attributes of this task instance:",
            "content": "Repo. This attribute refers to which repository the task instance belongs to. In Figure 3, this instance is from the repository tailwindlabs/tailwindcss. PR ID. This attribute refers to which pull request the task instance belongs to. Each pull request has unique ID, such as 10212. Instance ID. This attribute is unique identifier for the task instance, composed of Repo and PR ID. For example, the tailwindlabs__tailwindcss-10943 means this task instance is obtained from pull #10943 in the repository tailwindlabs/tailwindcss. Created At. This attribute refers to the date when the pull request was created. Issue IDs. This attribute refers to the ID numbers of the issues that are resolved by the pull request. For example, the issue number of instance tailwindlabs__tailwindcss-10943 is 10937, which means this pull request resolves the issue #10937. Issue Descriptions. This attribute refers to the text descriptions of issues related to the task instance. For example, as shown in Figure 3, the problem statement describes unexpected results of running programs. In the evaluation, this problem statement serves as the input of the task instance. Following the approach of SWE-bench, we extract and concatenate the issues title and body to form the content of the problem statement. Issue Images. This attribute refers to images in issue descriptions. As shown in Figure 3, this user attaches two images to describe the unexpected results of running programs. In the evaluation, these images can provide visual information for resolving issues. Since GitHub uses URLs to display images, we extract URLs containing png or jpg from the issue body of task instances. We then manually check whether these images are relevant to the content of the issue. Because not all task instances include images, this is an optional attribute. Website Links. This attribute refers to some website links users share to help describe reported issues. As shown in Figure 3, this user shares two links to an online code execution platform, Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:8 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng where the reproduced code and execution results are presented. These links provide execution environments for developers to understand issues by debugging. We manually check website links in each issue and keep website links that are crucial for resolving this issue. This is an optional attribute because not all task instances include such links. Version. This attribute refers to the version of the repository when the pull request was created. Since different versions of the code repository may require different environment setups, this information helps construct the appropriate code environment for each task instance. Considering that version information is typically updated in configuration files, we locate the configuration file corresponding to each task instance and extract the version information. For example, Python versions are usually stored in the setup.py file, JavaScript and TypeScript versions in package.json, and Java versions in pom.xml. Base Commit. This attribute is unique commit ID that the original pull request is based on. Using the base_commit, we can revert the GitHub repository to the state before the pull request was applied. This helps us construct the correct code repository environment for the target issues. We extract this information using the GitHub Developer API [6]. Test Patch. This attribute refers to the code changes in the pull request that are used to test the modified source code. In the evaluation, this content is used to run tests to verify the correctness of the submitted solution. Following the approach of SWE-bench [27], we check the file paths of each hunk in the code changes and retain only those where the file path contains keywords like test or testing. Additionally, for Java, we consider any Java file where the file name starts or ends with Test (e.g., TestClass.java or MyClassTest.java) as part of the test patch. Patch. This attribute refers to the code changes in the pull request that specifically address resolving the issue. This content provides the ground truth solution for the issue. To collect this data, we first examine each hunk in the code changes and filter out hunks related to testing. Then, we use the Python library Pygments [22] to check the file paths in each hunk and determine if they belong to the source files of the target programming language. For example, in Python repositories, files ending in .py are recognized as Python source files. This helps us exclude irrelevant files, such as documentation files ending in .md. Additionally, we collect certain configuration files that are crucial for the patchs correctness. For instance, for JavaScript, we gather changes in package.json, and for Java, we include changes in pom.xml. These specific changes are important for ensuring the patchs correctness. FAIL2PASS. This attribute refers to tests or test cases that are changed from fail status to pass status after applying the gold patch. These tests or test cases are used to verify whether the submitted solution can resolve issues in the task instance. PASS2PASS. This attribute refers to tests or test cases that maintain pass status both before and after applying the gold patch. This attribute can be used to verify whether the submitted solution does not change the function of the original source code."
        },
        {
            "title": "3.4.1 Environment Construction. Before conducting execution-based filtering, we create an isolated\nexecution environment for each task instance using Docker [5] to minimize potential conflicts",
            "content": "Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002:9 across environments. For each task instance, we define all setup commands in Dockerfile and bash scripts, which are then executed to set up the Docker environment for each instance. The execution environment includes the codebase before the issue was resolved and its required dependencies. For each task instance, we first use Git [49] to clone the repository. Then, with the task instances Base Commit attribute, we use Git to revert the repository to the state before the pull request addressing the issue was submitted. To set up the correct dependencies, we refer to development documentation in the codebase, such as README.md and CONTRIBUTING.md, which often provide guidance on how to build dependencies. Following these instructions, we install the necessary dependencies for the codebase. The process described above is written into the Dockerfile and bash scripts to automate the construction of the docker environment."
        },
        {
            "title": "4.1 Statistics of OmniGIRL\nAfter constructing the benchmark, OmniGIRL includes 959 instances collected from 15 repositories,\ncovering four programming languages: Python, JavaScript, TypeScript, and Java. The details of the\nrepositories included in the dataset are presented in Table 3, which lists the programming language\nused, the number of instances, and the license for each repository. The licenses ensure that our\ndataset can be freely accessed and used for research purposes.",
            "content": "Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:10 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Table 2. Results of filtering unnecessary images. Language All Python TypeScript JavaScript Java Before Filtering After Filtering # Instances # Avg Images # Repos # Instances # Avg Images # Repos 39 7 15 15 1.7 1.3 1.7 1.8 1.5 10 2 2 3 2 19 2 12 5 1.8 1.0 1.9 1.8 0.0 7 2 2 3 0 Table 3. Statistics of OmniGIRL repositories. Repository # Instances # Languages License python/mypy pyca/cryptography dateutil/dateutil tqdm/tqdm statsmodels/statsmodels redis/redis-py iamkun/dayjs prettier/prettier webpack/webpack jestjs/jest babel/babel tailwindlabs/tailwindcss netty/netty google/gson assertj/assertj 189 21 36 23 76 29 93 119 58 31 79 100 54 21 30 Python Python Python Python Python Python JavaScript JavaScript JavaScript TypeScript TypeScript TypeScript Java Java Java BSD 3-Clause BSD 3-Clause BSD 3-Clause MIT BSD 3-Clause MIT MIT MIT MIT MIT MIT MIT Apache-2.0 Apache-2.0 Apache-2.0 Table 4. Data analysis for our benchmark. Mean Max Issue Text Length (Words) 194 1,685 Codebase # Files (non-test) # Lines (non-test) 995 257K 3,016 1,010K Gold Patch Tests # Lines edited # Files edited # Func. edited # Fail to Pass # Total 46.3 1.2 2.2 3.3 135. 1,657 30 131 1,056 4,820 We also analyze the characteristics of task instances, as summarized in Table 4. We can find that task instances contain long issue text, including 194 words on average. The codebase includes 995 files and 257K lines of code on average, showing the complexity of the codebase. Besides, the gold patches require modifications across 46 lines, 2.2 functions, and 1.2 files on average. These characteristics highlight the complexity of GitHub issue resolution tasks. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002: Table 5. Categories of background knowledge in different programming languages contained in gold patch. Language of Repository Background Knowledge Contained in Gold Patch Python JavaScript TypeScript Java Decorator; Generator; Iterator; Reflection; Wraps Anonymous Functions; Arrow Functions; Event-driven; Interface; Anonymous Functions; Arrow Functions; Enum; Generics; Interface; Type Aliases; Abstract Classes; Annotations; Generics; Inheritance; Interface; Reflection;"
        },
        {
            "title": "4.2.2 Diversity of Repository Domains. In this section, we present the diverse range of domains\ncovered by the repositories included in our benchmark. We first collect a brief introduction from\nthe GitHub summary of each repository and label them with relevant domain tags. As shown in\nTable 6, our benchmark covers domains as follow:",
            "content": "Code quality (35.2%). Code quality tool is widely used to help developers improve code readability and maintainability. For instance, Prettier automatically formats code to ensure consistent style, making it more readable and maintainable. Web development (24.7%). Web development tools like tailwindcss are widely used for building modern, responsive user interfaces, facilitating efficient web development and ensuring compatibility with modern browsers and frameworks. Time tools (13.5%). Repositories such as dateutil provide crucial utilities for handling time and date operations. These libraries are widely used in various applications, particularly in scheduling and time-sensitive tasks across industries. Network tools (8.7%). Repositories like redis-py provide fundamental tools for managing databases and building scalable network applications. These libraries are key to developing high-performance, distributed systems. Statistical modeling (7.9%). The statsmodels library provides rich statistical and econometric tools essential for data-driven decision-making and financial trend modeling. Maintaining this repository requires rich knowledge of statistics and econometrics. Developer utility (4.4%). Developer Utility provides tools to enhance development productivity. For instance, gson simplifies JSON parsing and serialization in Java, while tqdm offers progress bars for Python, helping developers efficiently monitor and manage tasks. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:12 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Table 6. Repository summary and domain tags. Repository Summary Domain Tag python/mypy Optional static typing for Python prettier/prettier Opinionated code formatter for JavaScript assertj/assertj Library providing typed assertions for Java webpack/webpack bundler for javascript to pack modules Compiler for writing next generation JavaScript. babel/babel tailwindlabs/tailwindcss Utility-first CSS framework for UI development dateutil/dateutil iamkun/dayjs redis/redis-py netty/netty statsmodels/statsmodels tqdm/tqdm google/gson jestjs/jest pyca/cryptography Code quality Code quality Code quality Web development Web development Web development Time tool Extensions to the standard Python datetime features Time tool Lightweight immutable date-time library Network tool Redis python client Network tool Asynchronous network application framework Statitics modeling Statistical modeling and econometrics in Python Fast, Extensible Progress Bar for Python and CLI Developer utility Java library for JSON serialization and deserialization Developer utility Testing Framework for JavaScript Cryptographic recipes and primitives for Python Test tool Cryptography Test tool (3.2%). Test tools such as jest automate software testing, ensuring code correctness, reliability, and performance by detecting bugs and verifying expected functionality. Cryptography (2.4%). The cryptography repository offers essential cryptographic primitives and recipes, playing critical role in securing communications and data, especially in sensitive fields like cybersecurity and financial services."
        },
        {
            "title": "4.2.3 Diversity of Input Information Modalities. Previous benchmarks only focus on text infor-\nmation in issue descriptions, overlooking diverse multimodal information on issues. By manually\nreviewing issues in our benchmark, we identify three modalities of input: text information, image\ninformation, and website information.",
            "content": "Textual information. Text is the most common modality in GitHub issues, where users typically describe issues with details such as environment setup, reproduction code, error messages, etc. Image information. In addition to textual information, users often upload images to provide key details of reported issues. After analyzing images contained on OmniGIRL, we classify them into three types based on their purpose, as shown in Figure 4: Screenshots of reproduced code. Some users prefer to attach screenshots to demonstrate the code that reproduces an issue, as shown in Figure 4 (a). Error messages or logs. Some users prefer taking screenshots of error messages or logs instead of copying the text, especially when the error is long or has complex formatting. In Figure 4 (b), the user uploads screenshot to show the error messages. Unexpected output or behavior of code. These images are used to report unexpected results or behaviors after running the program. Unlike the previous two categories, these images often cannot be easily converted to text and require LLMs to understand the semantics of images with visual ability. For instance, in Figure 4 (c), after updating the code version, the user noticed that the colors of the elements were incorrect. They used images to show the unexpected changes in the element colors. Image information presents new challenges for current LLMs, requiring models with visual abilities to understand images, extract key issue details, and ultimately assist in resolving issues. Website information. Website information refers to some website links that provide important details related to an issue. For example, in the JavaScript and TypeScript GitHub communities, users often share links to online code execution platforms to help others reproduce reported issues. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002: Fig. 4. Examples of images contained in issue descriptions. Fig. 5. An example of website links within an issue. As shown in Figure 5, instead of providing the reproduced code directly, user shares link in the issue description that directs to Tailwind Play, website for debugging Tailwind CSS code online. These links allow developers to execute the reproduced code and observe errors in real-time. Unlike textual or image information, website information requires models to have web-browsing capabilities to interact with the website and obtain crucial information about issues. Distribution of different modality. We analyze the modalities present in each issue to calculate their distribution. Our analysis shows that all issues include text information, with 24.0% (230/959) also including website information, 4.1% (39/959) containing image information, This distribution highlights the diversity of multimodal data on OmniGIRL. 4https://github.com/tailwindlabs/tailwindcss/issues/12318 Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:14 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Table 7. Overview of evaluated LLMs. Model Company Size Context Window With Visual Ability Release Date DeepSeek-V2.5 GPT-4o-2024-08-06 Claude-3.5-Sonnet-2024-06-25 DeepSeek OpenAI Anthropic 236B Unknown Unknown 128K 128K 200K No Yes Yes Sep 2024 Aug 2024 Jun"
        },
        {
            "title": "5 Evaluation\nIn this section, we first evaluate the issue resolution ability of three advanced LLMs on OmniGIRL.\nWe then evaluate the performances of LLMs on issues that requires image understanding. Finally,\nwe investigate why LLMs fail to resolve issues on OmniGIRL. Note that we only evaluate LLM-based\nmethods here because, according to the SWE-bench leaderboard [15], the most advanced tools\nare primarily LLM-based. However, our benchmark is designed to evaluate the issue resolution\nability of any tools, including those that do not rely on LLMs. In summary, we answer the following\nresearch questions:",
            "content": "RQ1: How do the most advanced LLMs perform on OmniGIRL? RQ2: How do the most advanced LLMs perform in resolving issues with images? RQ3: What are the main reasons for LLM failures on OmniGIRL?"
        },
        {
            "title": "5.1.2 Evaluation Method. Due to the complexity of issue resolution tasks, it is challenging to\nobtain correct results by directly feeding issue descriptions and codebases into LLMs. Therefore,\nwe evaluate LLMs using some advanced GitHub issue resolution approaches. As mentioned in\nSection 2.2, we can divide current issue resolution approaches into three types following previous\nstudies [11, 40, 41]: retrieval augmentation-based method, LLM workflow-based method, and\nLLM agent-based method. To ensure a comprehensive evaluation, we select one representative\nmethod from each category. Based on the SWE-bench leaderboard [15], we choose baselines\nwith high performance and reasonable cost. Given the limited performance of current RAG-based\nmethods [27, 47], we choose the oracle retrieval method for evaluation. For the LLM workflow-based\nmethod, we select the Agentless [52] method, which is ranked 5th in the SWE-bench-Verified [15].\nAnd for the agent-based method, we choose the AutoCoderOver [63] method, which is ranked 2nd\nin the SWE-bench-Full [15]. These methods not only demonstrate advanced performance but also\nhave reasonable costs according to previous studies [52].",
            "content": "Finally, we use three methods: the oracle retrieval [27, 47], the Agentless method [52] and the AutoCodeRover method [63]. The descriptions of these methods are listed as follows: Oracle Retrieval. The oracle retrieval [27, 47] method provides both the issue description and the specific code files that require editing as inputs. These files are identified directly from the Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002: gold patch in each task instance. This approach enables an ideal-condition evaluation, simulating scenario where developers accurately locate the files necessary for resolving the issue. Agentless-X. Considering that current issue resolution frameworks are designed primarily for Python, we select the Agentless approach [52] as baseline and adapt it to support additional languages (e.g., JavaScript, TypeScript, and Java), enabling evaluation on OmniGIRL. We refer to this multilingual adaptation as Agentless-X. We choose Agentless as our baseline because it can achieve comparable performance while maintaining lower costs compared to other methods. The original Agentless approach is two-phase method for issue resolution. First, the localization phase uses hierarchical approach where the LLM successively identifies the relevant files, specific classes or methods, and finally, locates the exact code lines to be edited. Second, in the repair phase, the LLM generates patch based on the locations identified in the first phase. In Agentless-X, we retain Agentlesss original design and expand it to support multiple languages. The original Agentless approach uses AST tools to parse file structures during the localization stage. In Agentless-X, we use different AST tools for each language: Babel [4] for JavaScript/TypeScript and JParser [1] for Java. Additionally, we rewrite LLM prompts in multilingual versions. For parameter settings, we follow the default setting from the original Agentless implementation.5 AutoCodeRover-X.6 We choose the AutoCodeRover [63] method as our baseline. Similar to the Agentless method, the AutoCodeRover method only supports Python language. We adapt this method to support additional languages (e.g., JavaScript, TypeScript, and Java), enabling evaluation of our multilingual benchmark. We refer to this multilingual adaption as AutoCodeRover-X. The original AutoCodeRover method resolves issues using two LLM-based agents: the context retrieval agent and the patch generation agent. First, given issue descriptions and codebase as input, the context retrieval agent searches code information related to the given issue. In this stage, the agent searches relevant code information by calling code search tools iteratively until the agent thinks that enough code information is retrieved. Then, the retrieved code information is input to the patch generation agent. This agent will keep refining the generated patch until it can be applied successfully or until the maximum number of attempts is reached. In AutoCodeRover-X, we retain AutoCodeRovers original design and expand it to support multiple languages. The context retrieval agent uses AST tools to parse file structures. In AutoCodeRoverX, we use different AST tools for each language: Babel [4] for JavaScript/TypeScript and treesitter [8] for Java. Additionally, we rewrite LLM prompts in multilingual versions. For parameter settings, we follow the default setting from the original AutoCodeRover implementation [36]."
        },
        {
            "title": "5.1.3 Evaluation Metrics. Following previous studies [27, 47, 52], we use these metrics to evaluate\nthe performance of LLMs:",
            "content": "Resolve Rate: the percentage of task instances that are resolved. Apply Rate: the percentage of generated patches that are intergrated to codebase using Git [49] tool without any errors. Cost: the average cost of evaluating task instances."
        },
        {
            "title": "5.2 Performance of LLMs on OmniGIRL\nWe evaluate three advanced LLMs with three methods on OmniGIRL. The results are presented\nin Table 8. First, we can find that performance of advanced LLMs on OmniGIRL remains limited.\nAmong the evaluated models, GPT-4o with Agentless-X method achieves the highest resolve rate of\n8.6% and apply rate of 87.4% on OmniGIRL. And GPT-4o with AutoCodeRover-X method achieves\nthe second best performance with the resolve rate of 8.1% and apply rate of 85.8% on OmniGIRL.",
            "content": "5https://github.com/OpenAutoCoder/Agentless. We use the Agentless-v1 for evaluation. 6We use the AutoCodeRover(v20240620) for evaluation. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:16 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Moreover, Claude-3.5-Sonnet with oracle retrieval method achieves the third best performance with the resolve rate of 7.8% and apply rate of 61.8% on OmniGIRL. However, the performance of these LLMs across both methods remains limited, highlighting the need for further improvements in the issue resolution ability of LLMs. Second, the results show that the Agentless-X method and the AutoCodeRover-X outperform the oracle retrieval method for both GPT-4o and DeepSeek-V2.5. Specifically, for GPT-4o, Agentless-X increases the resolve rate from 2.5% to 8.6%, while AutoCodeRover-X improves it to 8.1%. Similarly, DeepSeek-V2.5s performance also improves, with Agentless-X raising the resolve rate from 2.7% to 3.9% and AutoCodeRover-X enhancing it further to 6.0%. These results demonstrate the effectiveness of the Agentless and the AutoCoderRover in improving the issue resolution ability of LLMs. Third, we find that for Claude-3.5-Sonnet, both the oracle retrieval method and the AutoCodeRoverX method perform better, achieving resolve rates of 7.8% and 7.6%, respectively, while its performance with the Agentless-X method is limited to only 1.9%. We believe this higher performance with oracle retrieval is due to Claude-3.5-Sonnets strong capability in understanding long code segments, which allows it to make accurate code changes when the correct file locations are provided. In contrast, with Agentless-X method, Claude-3.5-Sonnets performance is limited because it often fails to produce results in the expected format during the localization stage. This inconsistency makes it hard to parse the location output correctly, ultimately preventing patch generation. The detailed discussion of this issue is in Section 5.4. Compared to the Agentless-X method, AutoCodeRover-X achieves better performance on Claude-3.5-Sonnet, demonstrating its robustness in enhancing the performance of different LLMs. Last, we observe that both the Agentless-X method and the AutoCodeRover-X method perform worse on JavaScript and TypeScript than on Java and Python. This is likely due to the design of these methods, which both rely on AST tools to parse repository files and locate key structures, such as classes and methods. Specifically, they focus on extracting information from traditional object-oriented constructs like classes and methods but do not account for other critical structures. In Python and Java, essential information is typically organized within classes and methods, so this AST-based extraction is effective. However, in JavaScript and TypeScript repositories, code frequently relies on types and interfaces to define important structures that are not part of the traditional class or method constructs. This makes it challenging for the original extraction method to capture this information effectively. Additionally, JavaScript and TypeScript always use anonymous and arrow functions, which AST tools struggle to parse compared to standard functions. For these methods to handle JavaScript and TypeScript more effectively, it would need to account for these language-specific features, such as arrow functions, anonymous functions, and the use of types and interfaces. RQ1 Summary: The performances of current LLMs remain limited on OmniGIRL. Besides, the Agentless-X method and the AutoCodeRover-X can effectively enhance issue resolution preformance of GPT-4o and DeepSeek-V2.5. However, while Claude-3.5-Sonnet achieves good performance with the oracle retrieval method and the AutoCodeRover-X method, it struggles with Agentless-X. Lastly, Agentless-X and AutoCodeRover-X perform better on Java and Python compared to JavaScript and TypeScript."
        },
        {
            "title": "5.3 Performances of LLMs on Issues With Visual Inputs\nWe evaluate two advanced LLMs with visual understanding capabilities, GPT-4o and Claude-3.5-\nSonnet, on a set of 19 task instances that require understanding images in issues. This evaluation\nuses all three baselines. Additionally, given the relatively small size of the subset, we repeat each",
            "content": "Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002:17 Table 8. Performance comparison across different LLMs on OmniGIRL. In the table, DeepSeek-V2.5 is short for DeepSeek, GPT-4o-2024-08-06 is short for GPT-4o, and Claude-3.5-Sonnet-2024-06-25 is short for Claude-3.5. Model Oracle Retrieval Agentless-X AutoCodeRover-X Resolve Rate Apply Rate Cost($) Resolve Rate Apply Rate Cost($) Resolve Rate Apply Rate Cost($) DeepSeek (All) 2.7% (26/959) 49.0% (470/959) 0.005 3.9% (37/959) 52.0% (499/959) 0.010 6.0 % (58/959) 53.0% (508/959) 0.028 Python TypeScript JavaScript Java 1.3% (5/374) 1.9% (4/210) 2.2% (6/270) 10.5% (11/105) 49.7% (186/374) 45.2% (95/210) 48.5% (131/270) 55.2% (58/105) 0.008 0.003 0.003 0.038 6.1% (23/374) 1.9% (4/210) 2.6% (7/270) 2.9% (3/105) 70.9% (265/374) 44.3% (93/210) 37.0% (100/270) 39.0% (41/105) 0.014 0.006 0.009 0.007 7.2 % (27/374) 53.2% (199/374) 46.7% (98/210) 4.3 % (9/210) 3.7 % (10/270) 53.3% (144/270) 11.4 % (12/105) 63.8% (67/105) 0.026 0.033 0.029 0.021 GPT-4o (All) 2.7% (26/959) 42.6% (409/959) 0.069 8.6% (82/959) 87.4% (838/959) 0.098 8.1 % (78/959) 85.8% (823/959) 0. Python TypeScript JavaScript Java 1.9% (7/374) 3.3% (7/210) 1.9% (5/270) 6.7% (7/105) 42.5% (159/374) 44.3% (93/210) 40.0% (108/270) 46.7% (49/105) 0.112 0.035 0.049 0.003 8.8% (33/374) 6.2% (13/210) 6.3% (17/270) 18.1% (19/105) 89.3% (334/374) 85.2% (179/210) 87.8% (237/270) 83.8% (88/105) 0.090 0.095 0.113 0.053 9.9 % (37/374) 89.6% (335/374) 6.2 % (13/210) 78.6% (165/210) 3.7 % (10/270) 84.4% (228/270) 17.1 % (18/105) 90.5% (95/105) 0.156 0.179 0.193 0.119 Claude-3.5 (All) 7.8% (75/959) 61.8% (593/959) 0.110 1.9% (18/959) 14.6%(140/959) 0.272 7.6 % (73/959) 61.5% (590/959) 0. Python TypeScript JavaScript Java 5.1% (19/374) 6.7% (14/210) 8.5% (23/270) 18.1% (19/105) 52.9% (198/374) 70.0% (147/210) 63.3% (171/270) 73.3% (77/105) 0.179 0.047 0.073 0.063 1.6% (6/374) 1.0% (2/210) 2.2% (6/270) 3.8% (4/105) 1.6% (55/374) 10.5% (22/210) 18.5% (50/270) 12.4% (13/105) 0.335 0.170 0.178 0.342 8.8 % (33/374) 80.5% (301/374) 4.3 % (9/210) 33.8% (71/210) 4.1 % (11/270) 46.7% (126/270) 19.0 % (20/105) 87.6% (92/105) 0.260 0.353 0.392 0.327 experiment three times to minimize the effects of randomness. To investigate the effect of visual inputs, we conduct experiments under three different settings: Text Only. In this setting, we do not provide the actual images from issues to LLMs. Instead, we only include the image URLs as text, without any visual information. Text & Image. In this setting, both the text and images from issues are input to LLMs. The LLMs use its visual capabilities to understand the images and resolve the issue. Image-augmented Text. This is two-stage approach. In the first stage, we use prompt to instruct models to understand images and to rewrite the issue text with visual information. In the second stage, the rewritten text, augmented with image details, is used as input to the model. Compared to the second setting, this approach allows us to better understand how the model processes and interprets the visual information. The results are shown in Table 9, highlighting three key conclusions. First, current LLMs show limited performance on issues requiring image understanding. Using oracle retrieval method, which provides the correct files, Claude-3.5-Sonnet achieves the best performance, resolving only 10.5% of the issues in the image-augmented text setting. In contrast, GPT-4o resolves only 5.3% of issues with the AutoCodeRover-X method. Second, leveraging visual information is beneficial for resolving issues requiring image understanding. When using oracle retrieval method, compared to the Text only setting, Claude-3.5-Sonnets resolve rate increases from 3.5% to 10.5% in the Imageaugmented Text setting. Similarly, with the AutoCodeRover-X method, GPT-4o does not resolve any issue in the Text only setting. However, when image information is included, the resolve rate increases to 3.1% in the Text & Image setting and further improves to 5.3% in the Imageaugmented Text setting. Third, LLMs using the Agentless-X method and the AutoCodeRover-X method both struggle to resolve issues requiring image understanding. Despite its solid performance on OmniGIRL, Agentless-X failed to resolve any issues requiring image understanding on both models. Similarly, AutoCodeRover-X also shows limited performance in this area, with GPT-4o resolving at most 5.3% of issues, while Claude-3.5-Sonnet failed to resolve any. These results highlight that current methods need improvements to utilize visual information effectively. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002: L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Table 9. Performance comparison across different LLMs on OmniGIRL-V. In this table, Claude-3.5-Sonnet2024-06-25 is short for Claude-3.5 and the Image-augmented Text setting is short for IAG-Text. Model Setting Oracle Retrieval Agentless-X AutoCodeRover-X Resolve Rate Apply Rate Cost($) Resolve Rate Apply Rate Cost($) Resolve Rate Apply Rate Cost($) GPT-4o Text Only Text & Images IAG-Text 1.6% (1/57) 1.6% (1/57) 1.6% (1/57) 45.6% (26/57) 33.3% (19/57) 52.6% (30/57) Claude-3.5 Text Only Text & Images IAG-Text 50.9% (29/57) 3.5% (2/57) 3.5% (2/57) 59.6% (34/57) 10.5% (6/57) 59.6% (34/57) 0.036 0.038 0.046 0.058 0.062 0.074 0% (0/57) 0% (0/57) 0% (0/57) 0% (0/57) 0% (0/57) 0% (0/57) 42.1% (24/57) 26.3% (15/57) 38.6% (22/57) 57.9% (33/57) 63.2% (36/57) 52.6% (30/57) 0.096 0.110 0.109 0.210 0.224 0.233 0% (0/57) 3.5% (2/57) 5.3% (3/57) 0% (0/57) 0% (0/57) 0% (0/57) 84.2% (48/57) 77.2% (44/57) 78.9% (45/57) 84.2% (48/57) 61.4% (35/57) 68.4% (39/57) 0.159 0.220 0.269 0.271 0.309 0.339 RQ2 Summary: Our evaluation shows that current LLMs struggle with issues requiring image understanding. Besides, leveraging visual information is beneficial for resolving issues requiring image understanding. However, we find LLMs with Agentless-X and AutoCodeRover-X show limited performances on issues requiring image understanding, highlighting the need for these methods to utilize visual information effectively."
        },
        {
            "title": "5.4 Failure Analysis\nTo investigate why LLMs fail to resolve issues on OmniGIRL, we analyze their behavior at the\nlocalization and patch generation stages, identifying one type of failure at each stage.",
            "content": "Parsing failure of structural output. This type of failure occurs in the localization stage of the Agentless-X method. It happens when the model fails to follow the specified output format given in the prompt. In the example shown in Figure 6, the prompt instructs LLMs to provide the locations to be edited (e.g., class names, function names, method names, and line numbers) and to wrap these results with code block. Then, this method uses regex expressions to parse the localization results from responses of LLMs. We can find that both GPT-4o and DeepSeek-V2.5 follow the prompts instructions, successfully enclosing their output in code blocks, allowing the results to be parsed correctly and used in the following repair stage. However, Claude-3.5-Sonnet does not follow this format; instead of enclosing the location information in code block, it provides the output in plain text, which leads to parsing failure. As result, no location information is passed to the repair stage, and no patch can be generated from Claude-3.5-Sonnet in the following repair stage. To further explore the impact of this error, we compute the parsing success rate for each model in the localization stage of the Agentless-X framework, as shown in Table 10. GPT-4o achieves the highest parsing success rate at 97.2%, while Claude-3.5-Sonnet obtains much lower success rate of 11.8%. This parsing error significantly affects Claude-3.5-Sonnets ability to generate patches, leading to lower overall resolve rate (1.9%). However, we find that this failure can be mitigated effectively by adding format constraints in original prompt. In our experiments, we append the original prompt with You must wrap the results with ```. We find this prompt increases the Claude-3.5-Sonnets parsing success rate and hence increases the resolve rate from 1.9% to 7.4%. This results highlight the importance of building robust prompts in current methods. Insufficient ability of cross-file issue resolution. This type of failure occurs in the patch generation stage of all evaluated baselines. In the issue resolution task, resolving issues that require modifications across multiple files is particularly challenging. To investigate LLMs ability to resolve single-file and cross-file issues, we categorize issues on OmniGIRL into these two types. For each issue, we use its gold patch as reference and check how many files need to be modified to resolve the issue. If multiple files are modified in the gold patch of the issue, we classify this issue as crossfile issue; otherwise, it is categorized as single-file issue. And then we analyze performances of Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002: Fig. 6. Responses of different LLMs in the localization stage of Agentless-X. Table 10. Parsing success rate of LLMs with the Agentless-X method on OmniGIRL."
        },
        {
            "title": "Model",
            "content": "Agentless-X"
        },
        {
            "title": "Apply Rate",
            "content": "GPT-4o DeepSeek-V2.5 Claude-3.5-Sonnet Claude-3.5-Sonnet default default default new prompt 97.2% (932/959) 82.5% (791/959) 10.0% (96/959) 98.5% (945/959) 8.6% (82/959) 3.9% (37/959) 1.9% (18/959) 7.4% (71/959) 87.4% (838/959) 52.2% (501/959) 17.6% (169/959) 63.3% (607/959) LLMs in two types of issues. As shown in Table 11, we can observe that LLMs perform significantly worse on cross-file issues compared to single-file issues. We assume one reason LLMs perform poorly on cross-file issues is their tendency to modify only single file. To investigate this assumption, we first examine the number of files modified in the patches generated by LLMs for cross-file issues. Then, we calculate the proportion of patches that include modifications to multiple files versus those with changes to only single file. We find that, even with the oracle retrieval method, which provides the relevant files to the model, LLMs still primarily edit single file. Specifically, Claude-3.5-Sonnet modifies single file in 74.9% of issues, GPT-4o in 72.9%, and DeepSeek-V2.5 in 86.3%. Moreover, with the Agentless-X method, all models modify single file in 100% of issues. Additionally, with the AutoCodeRover-X method, Claude-3.5-Sonnet modifies single file in 93.3% of issues, GPT-4o in 82.4%, and DeepSeek-V2.5 in 90.0%. These results highlight the insufficient ability of current LLMs in resolving issues that require modifying multiple files. RQ3 Summary: Our analysis shows that Claude-3.5-Sonnet struggles to generate results in the expected format during the Agentless-X localization stage, leading to poor issue resolution performance. We find that adding specific format instructions to the prompt can improve Claude3.5-Sonnets results. Additionally, current LLMs perform poorly on cross-file issues and tend to modify only single file, even when multiple files require changes. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:20 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng Table 11. Performance comparison across different LLMs on OmniGIRL for cross-file and single-file issues."
        },
        {
            "title": "Method",
            "content": "Single-file Issues Cross-file Issues"
        },
        {
            "title": "Resolve Rate Apply Rate",
            "content": "GPT-4o DeepSeek-V2.5 3.5% (21/601) Oracle Retrieval Agentless-X 12.0% (72/601) AutoCodeRover-X 11.5% (69/601) 42.9% (258/601) 87.4% (525/601) 88.4% (531/601) 1.4% (5/358) 2.8% (10/358) 2.5% (9/358) 42.2% (151/358) 87.4% (313/358) 81.6% (292/358) 3.7% (22/601) Oracle Retrieval Agentless-X 5.2% (31/601) AutoCodeRover-X 8.5% (51/601) 50.6% (304/601) 52.4% (315/601) 55.9% (336/601) 1.1% (4/358) 1.7% (6/358) 1.9% (7/358) 46.4% (166/358) 51.4% (184/358) 48.0% (172/358) Claude-3.5-Sonnet 10.5% (63/601) Oracle Retrieval Agentless-X 2.5% (15/601) AutoCodeRover-X 10.8% (65/601) 66.1% (397/601) 13.6% (82/601) 63.4% (381/601) 3.4% (12/358) 0.8% (3/358) 2.2% (8/358) 54.7% (196/358) 16.2% (58/358) 58.3% (209/358)"
        },
        {
            "title": "6 Discussion",
            "content": "Potential Impacts. First, we propose GitHub issue resolution benchmark that captures diversity across programming languages, repository domains, and modalities of input information. This allows researchers to more effectively evaluate any method of resolving issues in different programming languages and with multimodal data. Second, our error analysis highlights specific limitations in current methods and LLMs, such as poor performance on TypeScript and JavaScript issues and challenges in resolving cross-file issues, etc. These findings provide insights for improving LLMs and issue resolution techniques. Third, we have made our data collection code and tutorial publicly available, allowing other researchers to collect GitHub issue resolution data to build new evaluation benchmark or training dataset. Limitations and future directions. First, following the SWE-bench [27], we collect data based on repository popularity, which may introduce bias because of overlooking less popular but potentially valuable repositories. To address this, we have open-sourced our data collection code and tutorial, enabling other researchers to collect data from repositories they consider valuable. Additionally, we plan to expand our benchmark in the future to include data from broader range of repositories to mitigate this bias. Second, following the SWE-bench [27], we use attribute filtering and execution-based filtering to remove invalid instances. However, this approach may filter out some instances. For example, using path keywords to identify test files could miss files that dont follow conventional naming patterns, and filtering out instances without FAIL2PASS tests excludes issues like performance improvements. We do this to ensure that the tests for each instance can reliably evaluate the correctness of the solution. In the future, we plan to use more advanced techniques, like machine learning-based file identification, to identify test files. Besides, we plan to expand the benchmark to include more issue types. Finally, we do not evaluate the effects of web information, as there are currently no methods to resolve issues using the website information contained in the issue description. In the future, we will evaluate potential approaches to incorporate web-based information into issue resolution. 7 Conclusion In this paper, we propose OmniGIRL, GitHub issue resolution benchmark with multi-aspect diversity in programming languages, repository domains and modality of input information. The evaluation results demonstrate that current LLMs show limited performances on OmniGIRL. Besides, we also find that current LLMs struggle to resolve issues that require understanding images. Furthermore, we analyze the reasons behind LLMs failure on OmniGIRL, to shed light on future performance improvements of LLMs on OmniGIRL. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002:"
        },
        {
            "title": "8 Data Availability\nOur code and data are available at https://github.com/DeepSoftwareAnalytics/OmniGIRL.",
            "content": "Acknowledgments This work is supported by CCF-Huawei Populus Grove Fund CCF-HuaweiSE202403, the National Natural Science Foundation of China (62032025) and the Guangdong Basic and Applied Basic Research Foundation (2023A1515012292). References [1] [n. d.]. JParser: JSON Stream Parser for Java. https://github.com/javadev/jparser. Accessed: 2023-10-26. [2] [n. d.]. npm - Node Package Manager. https://www.npmjs.com/. Accessed: 2023-10-25. [3] [n. d.]. pip - The Python Package Installer. https://pip.pypa.io/en/stable/. Accessed: 2023-10-25. [4] 2024. Babel. https://babeljs.io/. [5] 2024. docker. https://www.docker.com/. [6] 2024. github-rest-api. https://docs.github.com/en/rest. [7] 2024. maven. https://maven.apache.org/. [8] 2024. tree-sitter. https://tree-sitter.github.io/. [9] Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. 2021. Avatar: parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590 (2021). [10] Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. 2024. SWEBench+: Enhanced Coding Benchmark for LLMs. arXiv preprint arXiv:2410.06992 (2024). [11] Anthropic. 2024. Building Effective Agents. https://www.anthropic.com/research/building-effective-agents Accessed: 2024-12-31. [12] Anthropic. 2024. claude-3-5-sonnet. https://www.anthropic.com/news/claude-3-5-sonnet. [13] Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. 2024. MASAI: Modular Architecture for Software-engineering AI Agents. arXiv preprint arXiv:2406.11638 (2024). [14] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2023. Multi-lingual Evaluation of Code Generation Models. OpenReview (2023). https://arxiv.org/abs/2210.14868 ICLR 2023. [15] SWE bench Team. 2024. SWE-bench Leaderboard. https://www.swebench.com/ Accessed: 2024-12-30. [16] BigCode. 2023. MultiPL-E: Multi-programming Language Benchmark for Evaluating Code Generation. https: //nuprl.github.io/MultiPL-E/ [17] Tegawend F. Bissyand, David Lo, Lingxiao Jiang, Laurent Rveillre, Jacques Klein, and Yves Le Traon. 2013. Got issues? Who cares about it? large scale investigation of issue trackers from GitHub. In ISSRE. IEEE Computer Society, 188197. [18] Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. 2024. Repairagent: An autonomous, llm-based agent for program repair. arXiv preprint arXiv:2403.17134 (2024). [19] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. 2024. CodeR: Issue Resolving with Multi-Agent and Task Graphs. arXiv preprint arXiv:2406.01304 (2024). [20] Jiachi Chen, Qingyuan Zhong, Yanlin Wang, Kaiwen Ning, Yongkun Liu, Zenan Xu, Zhe Zhao, Ting Chen, and Zibin Zheng. 2024. RMCBench: Benchmarking Large Language Models Resistance to Malicious Code. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. 9951006. [21] Yang Chen. 2024. Flakiness Repair in the Era of Large Language Models. In Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings. 441443. [22] Pygments contributors. 2023. Pygments: Python syntax highlighter. https://pygments.org/ Version 2.15.1. [23] Robert Feldt, Sungmin Kang, Juyeon Yoon, and Shin Yoo. 2023. Towards autonomous testing agents via conversational large language models. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 16881693. [24] Python Software Foundation. 2024. Python Documentation. https://docs.python.org/3/contents.html Accessed: 2024-10-29. [25] Dvid Hidvgi, Khashayar Etemadi, Sofia Bobadilla, and Martin Monperrus. 2024. Cigar: Cost-efficient program repair with llms. arXiv preprint arXiv:2402.06598 (2024). Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. ISSTA002:22 L. Guo, W. Tao, R. Jang, Y. Wang, J. Chen, X. Liu, Y. Ma, M. Mao, H. Zhang, Z. Zheng [26] Yanxian Huang, Wanjun Zhong, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, Zibin Zheng, and Yanlin Wang. 2024. Agents in Software Engineering: Survey, Landscape, and Vision. arXiv preprint arXiv:2409.09030 (2024). [27] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770 (2023). [28] James Jones and Mary Jean Harrold. 2005. Empirical evaluation of the tarantula automatic fault-localization technique. In Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering. 273282. [29] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. 2024. Large language model-based agents for software engineering: survey. arXiv preprint arXiv:2409.02977 (2024). [30] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems 36 (2024). [31] Yizhou Liu, Pengfei Gao, Xinchen Wang, Chao Peng, and Zhao Zhang. 2024. MarsCode Agent: AI-native Automated Bug Fixing. arXiv preprint arXiv:2409.00899 (2024). [32] Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao, and Lu Zhang. 2020. Can automated program repair refine fault localization? unified debugging approach. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. 7587. [33] Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. 2024. How to Understand Whole Software Repository? arXiv preprint arXiv:2406.01422 (2024). [34] Mozilla Developer Network (MDN). 2024. JavaScript Documentation. https://developer.mozilla.org/en-US/docs/Web/ JavaScript Accessed: 2024-10-29. [35] Microsoft. 2024. TypeScript Handbook. https://www.typescriptlang.org/docs/handbook/intro.html Accessed: 2024-10-29. [36] NUS APR. 2024. AutoCodeRover Configuration Example. https://github.com/nus-apr/auto-code-rover/blob/main/ conf/example.conf Accessed: 2025-01-02. [37] Openai. 2024. gpt-4o. https://platform.openai.com/docs/models. [38] OpenAI. 2024. SWE-bench Verified: Human-Validated Subset for AI Model Evaluation. https://openai.com/index/ introducing-swe-bench-verified. Accessed: 2024-10-21. [39] Oracle. 2024. The Java Tutorials - Inheritance and Interfaces. https://docs.oracle.com/javase/tutorial/java/IandI/ Accessed: 2024-10-29. [40] Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. 2024. RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph. arXiv preprint arXiv:2410.14684 (2024). [41] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training Software Engineering Agents and Verifiers with SWE-Gym. arXiv preprint arXiv:2412.21139 (2024). [42] Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al. 2021. Codenet: large-scale ai for code dataset for learning diversity of coding tasks. arXiv preprint arXiv:2105.12655 (2021). [43] Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. 2024. SpecRover: Code Intent Extraction via LLMs. arXiv preprint arXiv:2408.02232 (2024). [44] Wei Tao, Yanlin Wang, Ensheng Shi, Lun Du, Shi Han, Hongyu Zhang, Dongmei Zhang, and Wenqiang Zhang. 2021. On the Evaluation of Commit Message Generation Models: An Experimental Study. In ICSME. IEEE, 126136. [45] Wei Tao, Yanlin Wang, Ensheng Shi, Lun Du, Shi Han, Hongyu Zhang, Dongmei Zhang, and Wenqiang Zhang. 2022. large-scale empirical study of commit message generation: models, datasets and evaluation. Empir. Softw. Eng. 27, 7 (2022), 198. [46] Wei Tao, Yucheng Zhou, Yanlin Wang, Hongyu Zhang, Haofen Wang, and Wenqiang Zhang. 2024. KADEL: KnowledgeAware Denoising Learning for Commit Message Generation. ACM Trans. Softw. Eng. Methodol. 33, 5 (2024), 133:1133:32. [47] Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, and Yu Cheng. 2024. MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution. arXiv preprint arXiv:2403.17927 (2024). [48] DeepSeek Team. 2024. DeepSeek-V2.5: Advanced Open-Source Large Language Model. https://www.deepseek.com/. Accessed: 2024-11-01. [49] Linus Torvalds et al. [n. d.]. Git. https://git-scm.com/ [50] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. 2024. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741 (2024). [51] Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. survey on software fault localization. IEEE Transactions on Software Engineering 42, 8 (2016), 707740. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025. OmniGIRL: Multilingual and Multimodal Benchmark for GitHub Issue Resolution ISSTA002:23 [52] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489 (2024). [53] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pre-trained language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 14821494. [54] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Going: Fixing 162 out of 337 bugs for $0. each using ChatGPT. arXiv preprint arXiv:2304.00385 (2023). [55] Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Shuiguang Deng, et al. 2023. Codescope: An execution-based multilingual multitask multidimensional benchmark for evaluating llms on code understanding and generation. arXiv preprint arXiv:2311.08588 (2023). [56] Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, and Wen Wang. 2023. Codetransocean: comprehensive multilingual benchmark for code translation. arXiv preprint arXiv:2310.04951 (2023). [57] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793 (2024). [58] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. 2024. Codereval: benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. 112. [59] Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, et al. 2024. SWE-bench-java: GitHub Issue Resolving Benchmark for Java. arXiv preprint arXiv:2408.14354 (2024). [60] Jiarui Zhang, Yicheng Luo, Yutao Xu, et al. 2023. HumanEval-X: Extending HumanEval to Evaluate Code Generation in Multilingual Contexts. arXiv preprint arXiv:2303.17568 (2023). https://arxiv.org/abs/2303.17568 [61] Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, et al. 2024. Diversity empowers intelligence: Integrating expertise of software engineering agents. arXiv preprint arXiv:2408.07060 (2024). [62] Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, and Yang Liu. 2024. Acfix: Guiding llms with mined common rbac practices for context-aware repair of access control vulnerabilities in smart contracts. arXiv preprint arXiv:2403.06838 (2024). [63] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. 15921604. [64] Dewu Zheng, Yanlin Wang, Ensheng Shi, Hongyu Zhang, and Zibin Zheng. 2024. How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation. arXiv preprint arXiv:2412.18573 (2024). [65] Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu Zhang, and Zibin Zheng. 2024. Towards more realistic evaluation of LLM-based code generation: an experimental study and beyond. arXiv preprint arXiv:2406.06918 (2024). [66] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 56735684. [67] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. 2023. Towards an understanding of large language models in software engineering tasks (2023). arXiv preprint arXiv:2308.11396 (2023). Received 2024-10-31; accepted 2025-03-31 Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA002. Publication date: July 2025."
        }
    ],
    "affiliations": [
        "Chongqing University, China",
        "Huawei Cloud Computing Technologies Co., Ltd., China",
        "Independent Researcher, China",
        "Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China"
    ]
}