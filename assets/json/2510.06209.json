{
    "paper_title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
    "authors": [
        "Jiahao Wang",
        "Zhenpei Yang",
        "Yijing Bai",
        "Yingwei Li",
        "Yuliang Zou",
        "Bo Sun",
        "Abhijit Kundu",
        "Jose Lezama",
        "Luna Yue Huang",
        "Zehao Zhu",
        "Jyh-Jing Hwang",
        "Dragomir Anguelov",
        "Mingxing Tan",
        "Chiyu Max Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts."
        },
        {
            "title": "Start",
            "content": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models Jiahao Wang1, Zhenpei Yang2, Yijing Bai2, Yingwei Li2, Yuliang Zou2, Bo Sun2, Abhijit Kundu3, Jose Lezama3, Luna Yue Huang2, Zehao Zhu2, Jyh-Jing Hwang2, Dragomir Anguelov2, Mingxing Tan2, Chiyu Max Jiang2 5 2 0 2 7 ] . [ 1 9 0 2 6 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Layout",
            "content": "gRealg"
        },
        {
            "title": "Augment",
            "content": "Fig. 1: By connecting driving video generation model with an end-to-end (E2E) planner, we can (1) Evaluate Synthetic Data Quality via Planner by controlling for the same traffic layout and scene conditions as the real videos to assess planner response discrepancies, (2) Assess End-to-end Planner Domain Gap via controlled experiments on operational conditions, and (3) Improve E2E Planner Performance on out-of-distribution domains via synthetic data from the video model. Planner Predictions () overlaid. Generated data in italics. Abstract Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing facilitating the expansion of Operational Design Domains, autonomous vehicle services into new operational contexts. *This work was supported by Waymo. 1Jiahao Wang is with Johns Hopkins University. Work done at Waymo. E-Mail: jiahaowg@gmail.com 2Authors are with Waymo. 3Authors are with Google DeepMind. I. INTRODUCTION Autonomous vehicles (AV) promise to revolutionize transportation, but ensuring their safety and reliability remains critical challenge. Typical AV development relies heavily on expensive and time-consuming real-world testing. Recently, two promising technologies have emerged with the potential to transform AV development: end-to-end (E2E) driving models [1], [2] and video generation models [3], [4], [5]. E2E models offer simplified approach to AV control by directly mapping sensor input to planning output, enabling the simplification of the AV stack and model scaling. On the other hand, video generation models can generate realistic sensor data for testing and training. Despite their potential, key questions remain for these technologies. While recent work on generating synthetic driving videos have shown increasingly impressive visual quality, it remains unclear if that correlates with the planners response. As shown in the adversarial literature [6], even the slightest perturbation in the image that is barely visible to the human eye can result in dramatically different output response of downstream deep learning model (e.g., predicting panda to be baboon). How planning models perceive the realism gap between real and synthetic data remains an open question. To our knowledge, we are among the first works to study the realism of such video generation model to facilitate the development and evaluation of an end-to-end planner. Meanwhile, E2E planner models present different set of challenges. While E2E models greatly simplify the model formulation by directly mapping sensor inputs to controls,"
        },
        {
            "title": "Change both",
            "content": "Fig. 2: Generated videos conditioned on various conditions. (1) The top row displays the input conditions, including road maps and bounding boxes, projected to the camera. (2) The second row shows the corresponding real-world video. The subsequent rows demonstrate the models ability to generate videos under different conditions: (3) identical conditions to the original video, (4) changing the weather from no-rain to rain, (5) changing the time of day to 00:00 (at midnight), (6) with both rain and nighttime conditions. it poses key challenge on how to evaluate such models, especially their performance on out-of-distribution domains. To address these questions, our key observation is that for certain driving scene, the expected driving behavior should largely be result of the underlying traffic scene layout (e.g., road map layout and agent features such as locations, types, and sizes) and mostly independent of other visual features, such as lighting conditions, weather conditions and the appearance of each agent (e.g., red vs blue car). This is the core underlying assumption in all behavioral simulation tasks [7]. video generation model, conditioned on both the scene layout and visual features such as weather and time-of-day, can generate the same underlying traffic scenario under different visual conditions. In this light, we present Drive&Gen, framework for co-evaluating E2E driving models and video generation models (see Fig. 1). First, by controlling for the same scene layout and visual conditions as the real videos, we can study the responses from the same end-to-end planner model based on each real scene and its synthetic counterpart to evaluate the sim-to-real domain gap of the video generation model. We introduce novel statistical measures utilizing E2E driver behavior within the generated environments to quantify the realism of these virtual worlds. Second, due to the ability of the controllable video generation models to generate traffic scenarios of the same layout and different operational design domains (ODD) such as varied weather and time-of-day, we are able to do controlled experiments to evaluate E2E planner performance under varied ODDs for model diagnostics and new ODD expansion readiness assessment. Finally, we demonstrate that synthetically generated data can be an effective mechanism to improve out-of-distribution generalization of E2E planner models. In summary, the main contributions of this work are: Introduces novel statistical measures for evaluating the realism of video generation models from the perspective of E2E driving models. Analyzes the performance differences of the E2E planner in in-distribution versus out-of-distribution contexts. Demonstrates the effectiveness of synthetic data generated by the video generation model for improving E2E model generalization to out of distribution scenarios. II. RELATED WORK World Models. World models [8] refer to learned representations of the environment and its dynamics. During early explorations, it has showcased remarkable success in various applications [9], [10], [11], [12], [13]. Constructing world models in real-world driving settings poses unique challenges because of the high sample complexity in driving worlds. Recently, with the development of diffusion-based video generation [3], [14], [15], [16], [17], [18], [4], [5], world models [19], [20], [21], [22], [23], [24], [25], [26], [27] are capable of generating photo-realistic videos, conditioning on user controls. GAIA-1 [19] and Vista [24] generate the future world with video diffusion models [3], [17] conditioning on text prompts and driving actions. DriveDreamer [21], DrivingDiffusion [25], MagicDrive [27], and Panacea [22] further generate controllable multi-view videos [28]. Concurrent work Delphi [20] also uses world models to improve E2E driving models, but our method more comprehensively covers all the stages of AV software development including evaluation, ODD-specific performance analysis, and synthetic data augmentation [29], [30], [31]. End-to-end Planning Models. For E2E driving, [32] introduces method to capture the temporal sequence of visual inputs, enabling direct learning from driving videos. Recently, end-to-end (E2E) driving has garnered increasing attention. Some approaches [33], [34], [35], [36], [37], [38] enable gradient back-propagation across modules, enhancing inter-task communication and mitigating error accumulation. Another line of research leverages pre-trained vision-language models (VLMs), which embed common-sense knowledge acquired from large-scale Internet data [2], [39], [1], [40]. Pioneer work DriveVLM [2] uses VLMs and chain-of-thought [41] prompting to describe critical objects and produce hierarchical planning signals, including high-level decisions and low-level waypoints. While these work mostly focus on image-only setting, Atlas [40] integrate 3D signals into VLM and showed strong results in both perception and planning. In this work, we also leverage the pre-trained vision-language model PaLI [42], to build our end-to-end planning model. Planner and World Model Evaluation. Evaluation of E2E planners in existing literature typically involves open-loop and closed-loop metrics. Open-loop evaluation measures how closely the planners predictions match ground-truth labels when the planner is not interacting with dynamic environment [43]. In closed-loop context, the E2E planner operates within either simulator or real-world environment. While certain benchmarks [44] provide closed-loop simulation capabilities, concerns persist regarding the validity of these simulators and the realism of their synthetically generated sensor data. Existing studies [20] generally do not directly evaluate the realism of synthetic data; instead, they use it as additional training material during fine-tuning and report the resulting performance gains. In this paper, we propose novel framework that directly measures simulation realism. over, isolating factors like traffic, weather, and time-of-day is costly in real-world data and demands precise control in synthetic environments. These issues motivate our proposed co-evaluation framework, which systematically measures both video generation quality and planning performance in diverse scenarios. To evaluate the video generation, we feed the generated videos into an E2E planner and compare how closely the planners responses match those observed in real scenes with an equivalent layout. By adjusting the conditions for the video generation model (e.g., weather or time-of-day), we can further analyze the planners behavior under different scenarios and track performance changes. We first introduce widely used metrics for E2E planning (Average Displacement Error) and video generation (Frechet Video Distance), then present our Behavior Permutation Test. Average Displacement Error (ADE). ADE measures the mean L2 distance between predicted and ground-truth trajectories, typically calculated at future horizons of 1s, 3s, and 5s. Although ADE offers straightforward comparison, it is not highly discriminative. For instance, two trajectories with the same ADE can deviate in opposite directions, yet exhibit fundamentally different errors. Frechet Video Distance (FVD). FVD [46] is widely used metric that correlates well with human perception of photo-realism. It measures the distributional distance between real and generated videos in latent feature space. In our evaluation, we randomly sample 5,000 videos from both the logged dataset and synthetic outputs to compute FVD. However, FVD alone does not fully capture whether the generated video adheres to the conditions. Behavior Permutation Test (BPT). We propose BPT, novel metric to assess whether generated videos can fool the planner, by measuring how similarly the planner responds behaviorally to the generated scene versus the real scene. For each driving scene, we conduct permutation test as follows. We feed the planner with real data and sample i=1, where τi Rq2 planned trajectories, denoted as {τ real is the way-point representation of trajectory and is the number of points. Similarly, we feed the planner with data generated by the video generation model under the same conditions, and sample planned trajectories {τ gen j=1. In the experiments, we use = = 10. }M }N III. METHOD"
        },
        {
            "title": "The null hypothesis posits that both sets of trajectories",
            "content": "originate from the same distribution. Formally, The primary focus of this work is introducing novel co-evaluation framework for video generation and E2E driving, which is introduced in Sec. III-A. In Sec. III-B, we describe how the diffusion-based video generative model is built especially how we encode various control modalities, including bounding boxes, road maps, ego-car pose, time-of-day, and weather. In Sec. III-C, we provide details on how we extend pre-trained vision-language model (VLM) into an E2E planner. H0 : {τ real }M i=1 d= {τ gen }N j=1 (1) The test statistic for the permutation test is generalized version of Chamfer distance between the two sets of trajectories, denoted D(). Formally, = (cid:0){τi}M i=1, {τj}N 1 2M τi τj2 + τi τj2 min i[1,M ] min j[1,N ] 1 2N (cid:88) (cid:88) (2) j= = (cid:1) i=1 j=1 A. Co-evaluation Traditional metrics for evaluating video generation cannot fully capture visual quality and controllability [45]. MoreFor = 1000 times, we randomly permute the trajectories and create two new trajectory sets {τi} of size and {τj} of size , and recalculate the test statistic using Eq. 2. for the bounding boxes. Segment types are encoded into one-hot vectors. We project the segment features into 256-dimensional space using an MLP, and reduce the number of tokens by using latent query attention [48], [47], which reduces computation and memory utilization. Ego-car pose. The pose of the ego-vehicle is flattened into 12-dimensional vector, comprising 33 rotation matrix and 3-dimensional translation. This vector is then projected into 256-dimensional space using an MLP. Time-of-Day. We enable precise control of time-of-day, allowing for specific time inputs such as 06:41 or 20:25. Since the same time-of-day can have very different lighting condition in different seasons or in different geographic locations, we propose to use sun angles instead. We use solar azimuth θ and elevation ϕ angles, which can be calculated from the local time of year ty, and geographic location lgeo (latitude, longitude). By manipulating the time-of-day td given certain ty and lgeo, we get different sun angles (θ, ϕ) and generate videos with diverse lighting scenarios based on (θ, ϕ). The sun angles are then encoded using sinusoidal functions of different frequency, and projected into latent space via MLP. Weather. Weather conditions, such as rain or no-rain, are encoded using one-hot vector and then also projected to latent space using an MLP. time-of-day td, We concatenate the embeddings of all these conditions into unified sequence z, and pass through transformer encoder to get feature fz. fz is then incorporated into the diffusion model through cross-attention, enabling effective conditioning. Additionally, similar to [5] we employ pooled representation of the feature fz, processed through an MLP, to modulate the multi-head attention and feed-forward layers in the diffusion models self-attention blocks. This mechanism allows for adaptive scaling and shifting of feature representations, leading to more precise control over the generated video content. C. End-to-end Driving Model adopt to right, frames, we We train the E2E driving model based on pretrained vision-language model PaLI [42] following EMMA [1]. To efficiently handle temporal collaged-image representation [49], arranging 33 grid of images from left top to bottom, with the earliest frame in the top-left corner. This collage is encoded into 1,536 tokens and concatenated with text tokens before being processed by the encoder-decoder model. The input text includes additional data such as the self-driving cars past states (e.g., position and velocity) and routing instructions (e.g., turn left, go straight). The model is trained to generate trajectories from temporal frames, where each trajectory is represented as sequence of waypoints encoded as float values in text format, framing the planning task as Visual Question Answering (VQA) problem. We initialize our model with pre-trained weights and fine-tune the language decoder, keeping the vision encoder fixed. The model is trained using cross-entropy loss. Fig. 3: Model architecture of our video generation model. We enable control of scene and traffic layout (bounding boxes, road map, and ego car pose) and operational conditions (time-of-day, weather), extending the latent video diffusion model W.A.L.T [5]. The conditions are encoded and interact with intermediate features in the diffusion transformer via combination of AdaLN and cross attention mechanisms. The model is fine-tuned on large corpus of driving videos. }M Denote T0 to be the distance when we have one set that consists of only real trajectory (i.e. {τ real i=1) and one set consists of only generated trajectory (i.e. {τ gen j=1). We compute probability P(T > T0) to be the p-value for each scene, representing the probability that the observed difference between trajectory sets is solely due to random chance. Specifically, < 0.05 indicates that the video generation model fails the Behavior Permutation Test, implying that the planner behaves significantly differently when fed real versus generated data. }N B. Video Generation Model We develop controllable video diffusion model based on the pre-trained W.A.L.T [5]. We extend it by enabling additional control modalities derived from real-world drivincluding bounding boxes, road map, ego car ing data, pose, time-of-day, and weather. This enables us to generate videos that are not only visually realistic but also adhere to specific driving scenarios, providing more controllable video generation framework. We introduces two novel designs: (1) Fine-grained time control: recise manipulation of time-of-day, enabling smooth transitions between various lighting conditions. (2) Efficient condition representation: sparse 3D representation for bounding boxes and road maps via learned tokenization, significantly reducing memory consumption. The overall architecture is shown in Fig. 3. Bounding box. Each bounding box is represented as 8-dimensional vector consisting of position (x, y, z), dimensions (width, height, length), yaw angle, and type. We encode the yaw angle using sinusoidal functions, and process the box types using one-hot embeddings. An MLP projects this vector into 256-dimensional space. To handle the varying numbers of bounding boxes, we set max number of 256 for each frame and apply padding or truncate as needed. We transform the bounding boxes from world coordinate into an ego-vehicle coordinate system. Road maps. Following [47], road maps are represented as line segments. We set the max number of line segments as 4,096. Each line segment has 3 attributes, i.e., starting point position, ending point position, and type. We transform the positions into an ego-vehicle coordinate system as"
        },
        {
            "title": "Real",
            "content": "Same Cond. w/o Box"
        },
        {
            "title": "Night",
            "content": "- FVD ADE 0.7548 BPT - 493.37 39.89 0.8594 0.8760 69.62% 55.28% 69.28% 67.66% 151.25 0.8736 38.97 1. Fig. 4: Evaluation of controllable video generation with FVD, ADE@5s, and BPT on 5000 random samples. FVD doesnt fully capture visual quality FVD for Rain/Night (relatively rare in our dataset) are much higher (because of distribution shifts) though the photo-realism of videos are visually similar. FVD cannot measure controllability removing the conditioning on bounding boxes greatly changes the car locations but has little effect on FVD. ADE and BPT dont suffer from such data distribution shifts, and can capture model controllability both metrics are notably worse when bounding boxes are removed. IV. EXPERIMENTS In this section, we first describe the model training and dataset details. Sec. IV-A evaluates the controllable video generation model and shows the models ability to generate videos that closely align with the specified conditions. We also assess the similarity between the real and generated videos using BPT. Subsequently, in Sec. IV-B, we leverage the versatility of the video generation model to create diverse driving scenarios and test the E2E planner. In Sec. IV-C, we demonstrate that our high-quality synthetically generated data improve the performance of the E2E planner. Since this work focuses on novel co-evaluation framework rather than state-of-the-art video generation, detailed FVD or resolution comparisons lie beyond our scope. Moreover, current methods lack the fine-grained control (e.g., minute-level time-of-day/sun angles) required for comprehensive planner evaluation. Finally, UniADs [34] deterministic trajectory thus prediction is incompatible with the proposed BPT, cannot be used in our framework. Model Training. To achieve video generation with conditions, we curated dataset with about ten million driving segments, among which we hold out 1% for testing and use the remaining for training. Each segment includes 17 frames in 10 Hz with resolution of 128 128 pixels, and comes with multiple features including agent bounding boxes, road map, ego-car trajectory, local time, geo-location and weather. We use maximum of 256 bounding boxes per frame. We train our video generation model on this for 700k steps with batch size of 64. During training, we randomly dropout each condition with the probability of 0.1. This improves generalization for the models and allows us to run inference without some of the conditions. We fine-tune the VLM-based E2E planner for 120k steps. A. Evaluation of Controllable Video Generation We evaluate the realism of the generated videos and consider few candidate metrics. commonly considered video realism metrics is the FVD score [46]. However, Fig. 5: Behavioral Permutation Test (BPT) visualizations. BPT performs set-to-set comparison of predicted trajectories from real and generated videos. In the top row, when the two sets of trajectories are similar, the distance between the two sets (red dash line) falls well within permuted distributions, resulting in failure to reject the null hypothesis. The bottom shows rejection of the null hypothesis, where the two sets of trajectories are significantly different from each other. as this is distributional matching metric, it measures distributional differences and not necessarily visual quality. In Fig. 4, we show that our FVD for night-time driving is disproportionately worse than the videos generated with the same driving conditions as the logged data though their visual realism is on par. An alternative for directly measuring video quality is to measure the resulting planner performance by ADE. However, though ADE is good measurement of planner quality, it doesnt indicate whether generated videos, conditioned on the same traffic layout, elicits similar planner prediction. That is because two vastly trajectory outputs (one leaning left, one leaning right) could end up with similar ADE compared to ground truth. higher ADE could also be due to worse planner performance in certain operational conditions (rain, night), and not necessarily unrealistic video inputs. In other words, the ADE metric doesnt allow us to easily disentangle the performance of the video generation model, versus the E2E planner itself. Finally, we consider the Behavior Permutation Test (BPT) metric, as introduced in Sec. III-A. In Fig. 5 we demonstrate pair of failure-to-reject and rejection examples. When the two sets of trajectories are similar, the distance between the original two sets (red dash line) falls well within permuted distributions while it significantly falls out-of-distribution when the two sets of trajectories are significantly different. For each scene, BPT emits signals for whether trajectory plans from real v.s. synthetic data are sufficiently similar. We measure the fail-to-reject rate of BPT over the entire validation set to obtain an average. Importantly, note that the expected ceiling for the BPT fail-to-reject rate is 95% (the nominal confidence level), since the hypothesis test rejects all cases with < 0.05. TABLE I: Comparison of video generation models. This table compares our model with baseline conditioned on local time instead of sun angles. The results highlight how sun angle encoding yields more realistic and controllable videos, reflected by improvements in FVD, ADE, and BPT. TABLE II: ADE scores on real and generated videos. Removing scene layout conditions (bounding box and road map) significantly increases the ADE, while removing operational conditions (weather and time-of-day) has less pronounced impact on ADE. Time-of-day encoding FVD ADE@5s BPT Input videos ADE@1s ADE@3s ADE@5s Local time Sun angles (ours) 45.54 39.89 0.8739 0.8594 68.46% 69.62% In this light, we evaluate the quality of our video generation model. Qualitative results can be found in Fig. 2 and more quantitative results in Fig. 4. Conditioned on the same conditions as real data, we obtain 69.62% BPT failure-to-reject rate (out of 95% expected ceiling), indicating broadly similar planner response when presented with real and synthetic data. We sanity check that altering the scene layout (removing bounding box constraints) leads to steep drop in BPT failure-to-reject rate, while modifying operational conditions (rain/night) result in small but not-insignificant planner behavior changes, due to varied performance of the planner under different operational conditions, which we further investigate in Sec. IV-B. We show an ablation study assessing the effectiveness of using sun angles for time-of-day encoding in Table I. By comparing our model to baseline that uses local time, we demonstrate better performance in FVD, ADE, and BPT. This suggests that sun angle encoding provides more informative representation of time-of-day variations, leading to more realistic and controllable video generation. B. Evaluation of End-to-End Planner By leveraging controllable video generation, we can systematically manipulate various conditions, such as weather and time-of-day, to create diverse and realistic driving scenarios. This enables us to isolate the impact of individual factors on planner behavior, leading to deeper understanding of the models strengths and weaknesses. For instance, by generating videos with varying levels of illumination, we can assess the planners performance under different lighting conditions, without the confounding effects of driving mix shifts, such as reduced traffic density at night (usually associated with better planner performance). Our model allows for precise control over individual conditions, enabling more granular analysis of the planners behavior. To evaluate the planners performance under these controlled conditions, we employ ADE, which directly measures the discrepancy between predicted and ground-truth trajectories. Table II presents the Average Displacement Error (ADE) when specific conditioning inputs are removed during video generation. Our model is trained with random dropout of conditions to promote robustness to missing inputs. The results show that removing scene layout information, specifically, bounding boxes and road maps, significantly increases ADE, highlighting the crucial role of spatial structure in guiding future trajectory predictions. In contrast, removing operational conditions such as weather and time-of-day has smaller impact on ADE, as long as the scene layout remains consistent. This aligns Real Gen Gen w/o bbox Gen w/o road map Gen w/o weather Gen w/o time-of-day 0.0288 0.0300 0.0437 0.0348 0.0299 0.0299 0.2606 0.2859 0.3814 0.3059 0.2857 0.2886 0.7548 0.8594 1.1216 0.9111 0.8593 0.8751 (a) Real video as input (b) Generated video as input Fig. 6: Comparison of predicted trajectories from planner given real and generated videos. Same scene layouts in two videos lead to highly similar trajectory predictions. with our intuition that the surrounding geometry is the primary factor influencing ego motion. This effect is further illustrated in Fig. 6, where similar scene layouts in real and generated videos result in comparable predicted trajectories. These findings emphasize the importance of layout-aware conditioning in trajectory-aware video generation. Table III presents the planners performance across different weather conditions. Performance slightly degrades in rainy conditions compared to no-rain conditions, as indicated by the higher ADE scores in all time horizons. This suggests that adverse weather introduces additional uncertainty that may slightly hinder accurate trajectory forecasting. Similarly, Table IV illustrates how the planners performance changes across different times of day. The planner achieves its best results at noon (12:00), while performance drops slightly at midnight (00:00), likely due to reduced visibility or lighting variance in nighttime scenes. These analyses offer valuable insights into the planners sensitivity to environmental factors, pointing to areas where targeted enhancements could strengthen model robustness under challenging conditions, as we will highlight in Sec. IV-C. TABLE III: ADE scores under varied weather. [Best, Worst]. TABLE VII: ADE on real-world validation data at nighttime (22:00 to 04:00), fine-tuned on different data mixtures. Weather No rain Rain ADE@1s ADE@3s ADE@5s 0.0299 0. 0.2853 0.2910 0.8580 0.8736 TABLE IV: ADE scores under varied time. [Best, Worst]. Time-of-day ADE@1s ADE@3s ADE@5s 00:00 06:00 12:00 18:00 0.0301 0.0301 0.0302 0.0298 0.2907 0.2907 0.2886 0.2893 0.8760 0.8744 0.8653 0.8747 Models ADE@1s ADE@3s ADE@5s Train on real fine-tune on real fine-tune on gen + real 0.0275 0.0284 0.0278 0.2470 0.2505 0. 0.7372 0.7328 0.7101 C. Improving Planner with Generated Videos (a) Case 1 (b) Case 2 Before FT After FT Before FT After FT We conduct experiments to evaluate the effectiveness of our generated data to fine-tune the planner. We compare two fine-tuning approaches: one is simply fine-tuning the planner on real-videos with 40K steps, and the other is to fine-tune on one million synthetic videos for 20K steps and then on real videos for 20K steps. The synthetic videos are generated with the same conditions as the real ones and the ground truth future trajectories are the same. We evaluate the planners performance on real-world data to demonstrate the effectiveness of synthetic data in improving real-world performance. Table presents the ADE of the different models. While fine-tuning solely on real data yields limited performance improvements, incorporating synthetic data from our generator effectively reduces the ADE at 5 seconds from 0.7548 to 0.7333. This demonstrates the potential of generating synthetic videos to enhance the performance of end-to-end planners. We further evaluate the planners performance in out-of-distribution scenarios, specifically rainy weather and nighttime (22:00 to 04:00). As shown in Table VI, fine-tuning the planner on both generated and real-world data significantly improves performance in rainy conditions compared to using real-world data alone. Similarly, Table VII demonstrates that combining generated and real-world data for fine-tuning yields improved performance at longer time horizons (3s and 5s) for nighttime scenarios. Notably, real-world nighttime data involves complex interplay of factors such as traffic density and illumination, which can affect planner TABLE V: ADE scores on real-world validation data, fine-tuned on different data mixtures. Here, gen refers to videos generated by our model. Models ADE@1s ADE@3s ADE@5s Train on real Fine-tune on real Fine-tune on gen + real 0.0288 0.0287 0.0282 0.2606 0.2591 0.2543 0.7548 0.7469 0.7333 TABLE VI: ADE scores on real-world validation data with rainy weather, fine-tuned on different data mixtures. Models ADE@1s ADE@3s ADE@5s Train on real Fine-tune on real Fine-tune on gen + real 0.0318 0.0328 0. 0.2893 0.2920 0.2891 0.8536 0.8482 0.8382 Fig. 7: Qualitative results illustrating the impact of synthetic data on planner performance (yellow arrows). FT means fine-tuning using synthetic and real data. Case 1: The ego vehicles response to green light (stopping vs. proceeding). Case 2: The ego vehicles interaction with stopped vehicle in the right lane (slow movement vs. safe bypass). performance. In some cases, reduced nighttime traffic can make planning simpler, resulting in lower ADE than those in Table V. These observations highlight the challenge of isolating individual factors when relying solely on real-world data and show the advantage of our co-evaluation framework with controllable video generation model. Qualitative results in Figure 7 show that fine-tuning the planner on generated and real-world data leads to improved performance in real-world driving scenarios. V. LIMITATIONS AND DISCUSSIONS The proposed BPT, by focusing on the distribution of planner outputs under the assumption of identical ground truth trajectories across varying environmental contexts, does not inherently assess the fidelity of physical realism or the implications for road safety. We leave deeper investigation of these aspects to future work. While neither model is flawless and no single metric can fully capture the complexity of real-world driving, our work provides meaningful methodology to systematically assess each component. VI. CONCLUSION In this work, we introduce novel framework for co-evaluating driving video generation and E2E planning. We propose new metric, the Behavior Permutation Test (BPT), to assess video realism by analyzing the distribution of outputs from planner. To the best of our knowledge, this is the first attempt to evaluate driving video generation using VLM-based driving model. In addition, we employ video generation model with precise control over scene layout and operating conditions (e.g., weather and time of day), enabling systematic evaluation of the E2E planner. Finally, we demonstrate that synthetic data generated by our model can improve E2E planner generalization in out-of-distribution scenarios. We hope our findings will move the field closer to robustly co-evaluating and improving both generative realism and planner performance in the future."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J.-J. Hwang, R. Xu, H. Lin, W.-C. Hung, J. Ji, K. Choi, D. Huang, T. He, P. Covington, B. Sapp, et al., Emma: End-to-end multimodal model for autonomous driving, arXiv preprint arXiv:2410.23262, 2024. [2] X. Tian, J. Gu, B. Li, Y. Liu, Y. Wang, Z. Zhao, K. Zhan, P. Jia, X. Lang, and H. Zhao, Drivevlm: The convergence of autonomous driving and large vision-language models, arXiv preprint arXiv:2402.12289, 2024. [3] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, Video diffusion models, in NeurIPS, 2022. [4] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, et al., Video generation models as world simulators, OpenAI Blog, vol. 1, p. 8, 2024. [5] A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, F.-F. Li, I. Essa, L. Jiang, and J. Lezama, Photorealistic video generation with diffusion models, in ECCV, 2024. [6] I. J. Goodfellow, J. Shlens, and C. Szegedy, Explaining and harnessing adversarial examples, arXiv preprint arXiv:1412.6572, 2014. [7] N. Montali, J. Lambert, P. Mougin, A. Kuefler, N. Rhinehart, M. Li, C. Gulino, T. Emrich, Z. Yang, S. Whiteson, B. White, and D. Anguelov, The waymo open sim agents challenge, in NeurIPS, 2023. [8] Y. LeCun, path towards autonomous machine intelligence version 0.9. 2, 2022-06-27, 2022. [9] D. Ha and J. Schmidhuber, Recurrent world models facilitate policy evolution, in NeurIPS, 2018. [10] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, Dream to conimagination, arXiv preprint trol: Learning behaviors by latent arXiv:1912.01603, 2019. [11] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, Mastering atari with discrete world models, in ICLR, 2021. [12] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, Mastering diverse domains through world models, arXiv preprint arXiv:2301.04104, 2023. [13] S. W. Kim, Y. Zhou, J. Philion, A. Torralba, and S. Fidler, Learning to Simulate Dynamic Environments with GameGAN, in CVPR, 2020. [14] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman, Make-a-video: Text-to-video generation without text-video data, in ICLR, 2023. [15] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou, Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation, in ICCV, 2023. [16] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, Align your latents: High-resolution video synthesis with latent diffusion models, in CVPR, 2023. [17] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, arXiv preprint arXiv:2311.15127, 2023. [18] W. Peebles and S. Xie, Scalable diffusion models with transformers, in ICCV, 2023. [19] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado, Gaia-1: generative world model for autonomous driving, arXiv preprint arXiv:2309.17080, 2023. [20] E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun, et al., Unleashing generalization of end-to-end autonomous driving with controllable long video generation, arXiv preprint arXiv:2406.01349, 2024. [21] X. Wang, Z. Zhu, G. Huang, X. Chen, J. Zhu, and J. Lu, Drivedreamer: Towards real-world-drive world models for autonomous driving, in ECCV, 2024. [22] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang, Panacea: Panoramic and controllable video generation for autonomous driving, in CVPR, 2024. [23] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang, Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving, in CVPR, 2024. [24] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li, Vista: generalizable driving world model with high fidelity and versatile controllability, in NeurIPS, 2024. [25] X. Li, Y. Zhang, and X. Ye, Drivingdiffusion: Layout-guided multi-view driving scenarios video generation with latent diffusion model, in ECCV, 2024. [26] J. Yang, S. Gao, Y. Qiu, L. Chen, T. Li, B. Dai, K. Chitta, P. Wu, J. Zeng, P. Luo, J. Zhang, A. Geiger, Y. Qiao, and H. Li, Generalized Predictive Model for Autonomous Driving, in CVPR, 2024. [27] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D.-Y. Yeung, and Q. Xu, Magicdrive: Street view generation with diverse 3d geometry control, in ICLR, 2024. [28] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuscenes: multimodal dataset for autonomous driving, in CVPR, 2020. [29] A. Mumuni, F. Mumuni, and N. K. Gerrar, survey of synthetic data augmentation methods in machine vision, Machine Intelligence Research, vol. 21, no. 5, p. 831869, Mar. 2024. [Online]. Available: http://dx.doi.org/10.1007/s11633-022-1411- [30] C. Bowles, L. Chen, R. Guerrero, P. Bentley, R. Gunn, A. Hammers, D. A. Dickie, M. V. Hernandez, J. Wardlaw, and D. Rueckert, Gan augmentation: Augmenting training data using generative adversarial networks, arXiv preprint arXiv:1810.10863, 2018. [31] W. Ma, Q. Liu, J. Wang, A. Wang, X. Yuan, Y. Zhang, Z. Xiao, G. Zhang, B. Lu, R. Duan, et al., Generating images with 3d annotations using diffusion models, in ICLR, 2024. [32] H. Xu, Y. Gao, F. Yu, and T. Darrell, End-to-end learning of driving models from large-scale video datasets, in CVPR, 2017. [33] S. Hu, L. Chen, P. Wu, H. Li, J. Yan, and D. Tao, St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning, in ECCV, 2022. [34] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang, et al., Planning-oriented autonomous driving, in CVPR, 2023. [35] S. Casas, A. Sadat, and R. Urtasun, Mp3: unified model to map, perceive, predict and plan, in CVPR, 2021. [36] J. Gu, C. Hu, T. Zhang, X. Chen, Y. Wang, Y. Wang, and H. Zhao, Vip3d: End-to-end visual trajectory prediction via 3d agent queries, in CVPR, 2023. [37] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu, C. Huang, and X. Wang, Vad: Vectorized scene representation for efficient autonomous driving, in ICCV, 2023. [38] A. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, and R. Urtasun, Perceive, predict, and plan: Safe motion planning through interpretable semantic representations, in ECCV, 2020. [39] S. Wang, Z. Yu, X. Jiang, S. Lan, M. Shi, N. Chang, J. Kautz, Y. Li, and J. M. Alvarez, Omnidrive: holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning, arXiv preprint arXiv:2405.01533, 2024. [40] Y. Bai, D. Wu, Y. Liu, F. Jia, W. Mao, Z. Zhang, Y. Zhao, J. Shen, X. Wei, T. Wang, et al., Is 3d-tokenized llm the key to reliable autonomous driving? arXiv preprint arXiv:2405.18361, 2024. [41] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large language models, NeurIPS, 2022. [42] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, X. Wang, Y. Tay, et al., Pali-x: On scaling up multilingual vision and language model, arXiv preprint arXiv:2305.18565, 2023. [43] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, End-to-end autonomous driving: Challenges and frontiers, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [44] X. Jia, Z. Yang, Q. Li, Z. Zhang, and J. Yan, Bench2drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving, arXiv preprint arXiv:2406.03877, 2024. [45] I. Skorokhodov, S. Tulyakov, and M. Elhoseiny, Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2, in CVPR, 2022. [46] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, Towards accurate generative models of video: new metric & challenges, arXiv preprint arXiv:1812.01717, 2018. [47] N. Nayakanti, R. Al-Rfou, A. Zhou, K. Goel, K. S. Refaat, and B. Sapp, Wayformer: Motion forecasting via simple & efficient attention networks, in ICRA, 2023. [48] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, et al., Perceiver io: general architecture for structured inputs & outputs, in ICLR, 2022. [49] R. Shi, H. Chen, Z. Zhang, M. Liu, C. Xu, X. Wei, L. Chen, C. Zeng, and H. Su, Zero123++: single image to consistent multi-view diffusion base model, arXiv preprint arXiv:2310.15110, 2023."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Johns Hopkins University",
        "Waymo"
    ]
}