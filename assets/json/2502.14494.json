{
    "paper_title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
    "authors": [
        "Jinnan Li",
        "Jinzhe Li",
        "Yue Wang",
        "Yi Chang",
        "Yuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at \\url{https://github.com/MLGroupJLU/StructFlowBench}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 4 9 4 4 1 . 2 0 5 2 : r StructFlowBench: Structured Flow Benchmark for Multi-turn Instruction Following Jinnan Li1, Jinzhe Li2 Yue Wang3 Yi Chang1,4,5* Yuan Wu1* 1School of Artificial Intelligence, Jilin University 2College of Computer Science and Technology, Jilin University 3School of Information and Library Science, University of North Carolina at Chapel Hill 4Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China 5International Center of Future Science, Jilin University {jnli23, lijz2121}@mails.jlu.edu.cn, wangyue@email.unc.edu, yichang@jlu.edu.cn, yuanwu@jlu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Multi-turn instruction following capability constitutes core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multiturn from single-turn interactions. This structural dependency not only reflects user intent but also establishes second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closedsource LLMs. Experimental results reveal significant deficiencies in current models comprehension of multi-turn dialogue structures. The code is available at https://github.com/ MLGroupJLU/StructFlowBench."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLMs) in multi-turn dialogue systems has elevated instruction-following capabilities to pivotal research frontier in human-AI interaction (Chang et al., 2024). Current evaluation methodologies bifurcate into two streams: multi-turn dialogue evaluations focusing on capability evaluation (Zheng et al., 2023b; Bai et al., 2024; Kwan et al., 2024) and instruction-following analyses emphasizing *Corresponding authors 1 fine-grained constraint compliance (Jiang et al., 2024; He et al., 2024a; Zhang et al., 2024). More recent research has started to model the composition of intra-turn constraints (Wen et al., 2024). However, current evaluation methodologies treat multi-turn dialogues as simple concatenations of single-turn interactions, neglecting users planning and intentionality in extended conversations. This leads to three critical limitations: (1) Failure to model complex scenarios: Multi-turn dialogue data constructed with simplistic linear thinking cannot accurately capture key characteristics of real-world complex conversations, such as logical coherence, user goal clarity, and natural transitions. (2) Methodological bias: Single-turn evaluation strategies fragment inter-turn structural connections, overlooking multi-turn structural constraints. (3) Analytical deficiency: Existing approaches overemphasize intra-turn-level constraint compliance while lacking systematic framework to characterize dialogue structural flow. To bridge these gaps, we introduce StructFlowBench, novel instruction-following benchmark integrating multi-turn structural flow framework. It consists of two key components: 1) Dualconstraint evaluation system, which combines 8 intra-turn instruction constraints with 5 newly proposed structural constraints, enabling more comprehensive assessment of multi-turn dialogue instruction following capabilities of LLMs. These structural constraints account for inter-turn dependencies, ensuring that models are evaluated not only on their ability to satisfy individual constraints but also on their capacity to maintain logical coherence across multiple turns. 2) Six-category structural flow taxonomy, encompassing six fundamental inter-turn relationships: Follow-up, Refinement, Recall, Summary, Expansion, Unrelatedness. The illustration of the structural flow taxonomy and an example of the structural flow are presented in Figure 1. This taxonomy serves tripartite function: six-category structured taxonomy for multiturn instruction-following evaluation, offering an interpretable framework for analyzing dialogue structural flow. StructFlowBench: We introduce StructFlowBench, structurally annotated multi-turn benchmark that leverages structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios. Comprehensive LLM evaluation: We systematically evaluate 13 state-of-the-art LLMs (3 closed-source and 10 open-source), unveiling disparities in structural processing capabilities and providing empirical insights for optimizing dialogue systems."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Benchmarks for Multi-Turn Dialogues The evolution of dialogue evaluation paradigms has progressed from single-turn assessments to sophisticated multi-turn interaction analysis (Wang et al., 2023; Sun et al., 2024; Duan et al., 2024). Among these, MT-Bench (Zheng et al., 2023b) pioneered this transition by providing methodologies specifically designed to assess models ability to handle multi-turn interactions. Building upon this, MT-Bench-101 (Bai et al., 2024) introduces more granular evaluation framework to assess finegrained capabilities. Multi-IF (He et al., 2024b) expands single-turn dialogues into multi-turn interactions by following simple, predefined linear paths. However, most existing work on multi-turn dialogue evaluation does not prioritize the assessment of instruction following and overlooks the influence of structural information on the evaluation of multi-turn dialogues. The four modes of userassistant interactions proposed by MT-Eval (Kwan et al., 2024) cover certain structural information within multi-turn dialogues. However, MT-Eval does not establish systematic structural framework and lacks integration of various structural aspects for comprehensive evaluation. 2.2 Benchmarks for Instruction Following Recent instruction following evaluation predominantly employs constraint-based frameworks (Jiang et al., 2024; Zhang et al., 2024; He et al., 2024a; Zhou et al., 2023). InfoBench (Qin et al., 2024) introduces the Decomposed Requirements Following Ratio (DRFR) metric, which provides more granular scoring system by breaking down Figure 1: The Structural Flow Taxonomy includes six fundamental structures, each used to describe the interturn relationships in multi-turn dialogues. It can be applied to analyze any dialogue and generate specific structural flows. (a) Diagnostic evaluation: It enables structured analysis of cross-turn structural rationality, helping to identify inconsistencies in dialogue flow and ensuring that model responses align with the expected discourse structure. (b) Intent inference: By analyzing structural patterns, this taxonomy facilitates the extraction of implicit user intent, offering deeper understanding of how instructions evolve over multiple turns. (c) Controlled generation: The taxonomy provides configurable structural parameters that guide task-specific dialogue simulation, allowing for the tailored generation of multi-turn conversations with predefined structural patterns. This not only enhances dataset diversity but also supports the development of more robust instruction-following models adaptable to varied real-world applications. We summarize our contributions as follows: Structural Flow Taxonomy: We propose 2 the evaluation of complex instructions into assessments of their individual simple constraints. Furthermore, ComplexBench (Wen et al., 2024) explores instruction-following capabilities in singleturn complex dialogues through empirical studies of constraint composition. However, prior work on instruction-following evaluation has primarily focused on single-turn interactions, which do not align with the more common multi-turn dialogue scenarios observed in real-world user interactions. While some studies have attempted to split complex single-turn instructions into multi-turn dialogues, these approaches do not fully capture the intentionality and goal-oriented nature of users in real-world contexts."
        },
        {
            "title": "3 StructFlowBench",
            "content": "In this section, we first introduce the structural flow framework and the constraint categories of our benchmark. Next, we detail the data construction pipeline and present an overview of the statistics for StructFlowBench. Finally, we outline the evaluation protocol. 3.1 Structural Flow Taxonomy By analyzing existing LLM and real human multiturn dialogue datasets (such as WILDCHAT (Zhao et al., 2024) and LMSYS-Chat-1M dataset (Zheng et al., 2023a)), we identified and categorized six structural patterns of multi-turn dialogues to enhance the understanding and analysis of conversational structural flow. Follow-up: An adjacent-turn structure where the users next prompt builds on the content of the previous turn, incorporating details from either the users previous prompt or the AIs previous response. This is the most common structure in multi-turn dialogues, typically reflecting the users intent to explore the topic more deeply. Refinement: An adjacent-turn structure in which the user modifies or clarifies their immediate previous prompt to improve the AIs response. This structure usually signals the users dissatisfaction with the prior response, prompting them to refine the prompt while clarifying and emphasizing their concerns to obtain more satisfactory response. Recall: long-range structure in which the user refers back to content from two or more turns ago to provide context for the current prompt (longrange follow-up) or referencing prior content for clarification (long-range refinement). Expansion: multi-turn fan-out structure where the user introduces main theme and explores related subtopics in subsequent turns. This structure suggests that the users following turns are focused on specific subtopics derived from particular point in the conversation. Summary: multi-turn fan-in structure in which the user requests consolidation of content from multiple previous turns into cohesive overview. This structure acts as the counterpart to expansion, reflecting the need to summarize and condense the information discussed in earlier turns. Unrelatedness: conversational structure in which the users prompt is entirely independent of the previous turn, with no reference to prior content or context. This structure often occurs in everyday use of LLMs by non-experts, where new topic is introduced within previously unrelated dialogue, rather than starting new conversation. After defining the six basic dialogue structures, we can use the Structural Flow Taxonomy to analyze multi-turn dialogue data and construct the corresponding structural flows. 3.2 Constraint Categories We categorize our constraints into intra-turn constraints and multi-turn structural constraints. Details related to constraints can be found in Appendix B. For intra-turn constraints, we synthesize and refine constraint classification systems from several works in this field (e.g., IF-Eval (Zhou et al., 2023), CFBench (Zhang et al., 2024), FollowBench (Jiang et al., 2024)). Based on this synthesis, we categorize constraints into eight types: Inverse Constraint, Style Constraint, Situation Constraint, Keyword/Element Constraint, Basic Format Constraint, Quantity Format Constraint, Template Format Constraint, and Content Constraint. For multi-turn structural constraints, we define five types of structural design constraints, excluding the unrelatedness structure. These constraints are specifically designed to maintain logical coherence and continuity across multiple turns in dialogue. They ensure that the structural relationships between turns are consistent and contextually relevant, enabling smooth flow of conversation. The five types of constraints are aimed at handling key aspects such as follow-ups, refinements, recalls, expansions, and summaries, ensuring that each turn in the dialogue properly connects to the previous ones while adhering to the intended conversational 3 Figure 2: The construction pipeline of STRUCTFLOWBENCH. First, tasks, topics, user types, and structural flow templates are defined. Then, dialogue data is generated in two steps: intermediate dialogue plans (i.e., the summarized prompts) are created from the structural flow, followed by generating complete dialogues from these plans. Finally, intra-turn constraints are extracted by GPT-4o, and structural constraints are added based on the structural flow information. structure. 3.3 Data Construction Pipeline The construction pipeline of StructFlowBench, as shown in Figure 2, comprises three main components: parameter setting, two-step dialogue generation, and constraint extraction and addition. All prompt templates used in the data construction process are included in Appendix D, and sample data instance is provided in Appendix E. Parameter Setting Before dialogue generation, we select parameters such as topic, task, user characteristics, and structural flow template, ensuring comprehensive coverage of the evaluation scope for multi-turn dialogue generation. For task types, we refer to the taxonomy of ComplexBench (Wen et al., 2024), adapting it to our evaluation framework and selecting eight task types. For topics, we draw from the MT-Bench-101 (Bai et al., 2024) framework, making necessary adjustments to suit our context, and ultimately select 22 topics. For user characteristics, we consider the significant differences in questioning styles and language between experts and non-experts. For the structural flow template, we designed multiple templates based on insights from real data and specific scenarios. Two-Step Dialogue Generation We employ two-step process to generate dialogue for parameter setting. The first step uses the structural flow template to generate an intermediate dialogue plan (i.e., summarized prompts) via GPT4o. Locally deployed mini-models perform initial screening and manual inspection of error data to ensure the dialogue plan aligns with the structural flow. In the second step, each intermediate dialogue plan is used to generate complete dialogue, including user prompts and LLM responses via GPT-4o. This approach ensures high-quality generation of both dialogue content and structure while minimizing manual effort. Constraint Extraction and Addition For the complete multi-turn dialogue data, we extract intra-turn constraints using the GPT-4o, followed by manual validation to ensure accuracy. Based on the structural flow information, we then assign the corresponding multi-turn structural constraints to each dialogue turn. 3.4 Benchmark Dataset Statistics Table 1 presents comparison of related benchmark datasets, evaluating them from three perspectives: fine-grained constraints, multi-turn dialogue assessment, and structural information. Our StructFlowBench encompasses 8 task types, 22 topics, and 13 constraint types. It ultimately includes 155 multiturn dialogues, comprising total of 643 turns and 1,775 constraints. Detailed statistics for tasks and topics are provided in the Appendix A. 3.5 Evaluation Evaluation Criteria Drawing on the methodology of MT-Bench101 (Bai et al., 2024), we implemented the Golden Context approach in our evaluation framework. Instead of relying on model-generated contexts, this 4 Benchmark #Dialogues Avg. #Turns #Constraint Types IFEval CELLO FollowBench InfoBench CFBench ComplexBench MT-Bench-101 Multi-if MT-Eval StructFlowBench 541 523 820 500 1000 1150 1388 4501 155 1 1 1 1 1 1 3.03 3 6.96 4.14 4 4 6 5 10 19 - 24 - 13 Fine-grained Constraint Multi-turn Assessment Structural Information Table 1: Comparisons between STRUCTFLOWBENCH and other related benchmark datasets. represents partially satisfied. method uses carefully curated datasets as dialogue histories. By providing accurate and consistent contexts for each dialogue turn, it minimizes biases and noise, improving the reliability, fairness, and comparability of response quality assessments across different models. To achieve fine-grained evaluation of multiturn user instructions, we integrate insights from prior studies (Qin et al., 2024; Wen et al., 2024; Zhang et al., 2024; He et al., 2024a) and propose an assessment method based on constraint decomposition and binary question formulation. Specifically, we decompose each user instruction into multiple independent constraints and design concise binary questions for each, answered with simple Yes or No to assess satisfaction. These binary questions are then aggregated into checklist that comprehensively covers all critical constraints of the instruction. Building on this foundation, we further adopt the approach of leveraging state-of-the-art LLMs for evaluation, as outlined in MT-Bench (Zheng et al., 2023b). In our implementation, we use the advanced GPT-4o as the LLM evaluator. By providing the evaluator with the golden context, response of the test model, the constraint checklist, and carefully crafted prompt template, we ensure high consistency and reliability in the evaluation process. The prompt template is designed to emphasize key evaluation points, effectively enhancing the accuracy and credibility of the results. Evaluation Metrics We adopted several existing metrics, including Constraint Satisfaction Rate (CSR) and Instruction Satisfaction Rate (ISR) (Zhang et al., 2024), as well as Decomposed Requirements Following Ratio (DRFR) (Qin et al., 2024). (cid:17) i= (cid:80)m (cid:80)ni (cid:16) 1 ni j=1 sj across all instructions, calculated as CSR = 1 , where represents the total number of instructions, ni denotes the number of constraints in the i-th instruction, and sj {0, 1} indicates whether the j-th constraint in the i-th instruction is satisfied. The Instruction Satisfaction Rate (ISR) measures the proportion of instructions where all constraints are fully satisfied, computed as ISR = 1 i=1 si, where si {0, 1} indicates whether all constraints in the i-th instruction are satisfied. (cid:80)m The Decomposed Requirements Following Ratio (DRFR) evaluates the overall satisfaction of requirements across all instructions, defined as , where mi is the number of scorDRFR = ing questions for the i-th instruction, and i,j denotes the result of the j-th scoring question in the i-th instruction. (cid:80) i,j i,j (cid:80) mi Despite their utility, these existing metrics have limitations. For instance, CSR treats all constraints equally without considering their relative importance, while ISR provides binary evaluation that may overlook partial fulfillment of constraints. To overcome these limitations, we introduce the Weighted Constraint Satisfaction Rate (WCSR), (cid:80)n j=1 wj sj defined as WCSR = , which incorpo- (cid:80)n j=1 wj rates weighted factors to account for the varying significance of different constraint types. Here, denotes the total number of constraints, wj represents the weight assigned to the j-th constraint, and sj {0, 1} indicates whether the j-th constraint is satisfied. In our framework, intra-turn constraints are assigned weight of wr = 1, whereas structural constraints, which play critical role in ensuring coherence and correctness, are given higher weight of ws = 2. The Constraint Satisfaction Rate (CSR) evaluates the average proportion of satisfied constraints The introduction of WCSR provides more nuanced evaluation by emphasizing important con5 straints through weighted assessments. This improves the precision and relevance of evaluations, enhancing the reliability of LLMs in meeting complex requirements."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We evaluate 13 popular LLMs on StructFlowBench, including 3 closed-source models (GPT4o (Hurst et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024) and Gemini-1.5-Pro (Team et al., 2024) ) and 10 open-source models: Llama3.1-Instruct-8B (Dubey et al., 2024), Mistral7B-Instruct-v0.3 (Jiang et al., 2023), Qwen2.57B-Instruct, Qwen2.5-14B-Instruct (Yang et al., 2024), Yi-6B-Chat (Young et al., 2024), Phi-3.5mini-instruct (Abdin et al., 2024), GLM-4-9BChat (GLM et al., 2024), Deepseek-R1-DistillLlama-8B, Deepseek-R1-Distill-Qwen-7B (Guo et al., 2025) and DeepSeek-v3 (Liu et al., 2024). More details on these evaluated models can be found in Appendix F. 4.2 Main Results Overall Results Table 2 presents comprehensive evaluation of 13 representative LLMs on StructFlowBench, covering four key metrics as well as assessments of structural constraints. The detailed results, categorized by intra-turn constraints and task types, are provided in the Appendix C. The recently released DeepSeek-v3 outperforms all other models across all metrics, demonstrating its exceptional capability in fine-grained constraint satisfaction and multi-turn dialogue structure understanding. Gemini-1.5-Pro and GPT-4o closely follow, achieving comparable performance in intraturn constraints but showing slightly weaker results in adhering to structural constraints for multi-turn dialogues. Claude-3.5-Sonnet, GLM-4-9B-Chat, Qwen2.5-14B-Instruct, and Qwen2.5-7B-Instruct also exhibit strong instruction-following capabilities, with CSR exceeding 94%. Notably, all seven of these models achieve high DRFR scores, indicating their strong ability to follow fine-grained instructions. In contrast, mid-tier models such as DeepseekR1-Distill-Llama-8B, Llama-3.1-8B-Instruct, Phi3.5-Mini-Instruct, and Yi-6B-Chat perform reasonably well but exhibit greater instability, particularly in ISR and WCSR. While they handle simpler constraints effectively, they struggle with maintaining consistency when processing complex instructions and multi-turn dialogue structures. The weakest performers in multi-turn instruction following are Deepseek-R1-Distill-Qwen-7B and Mistral-7BInstruct-v0.3, revealing significant deficiencies in natural interaction scenarios. particularly interesting observation is that Deepseek-R1-Distill-Llama-8B, distilled from Llama-3.1-8B, outperforms Llama-3.1-8B-Instruct across all metrics, demonstrating the effectiveness of the distillation process. However, Deepseek-R1Distill-Qwen-7B, distilled from Qwen2.5-Math7B, underperforms due to its origin from model optimized primarily for mathematical reasoning tasks, which inherently makes it weaker in multiturn dialogue instruction following compared to Qwen2.5-7B-Instruct. One particularly noteworthy outcome is that DeepSeek-v3, an open-source model, surpasses its closed-source counterparts in multi-turn instruction-following evaluations. This result is encouraging for both the research community and the open-source ecosystem, suggesting that the theoretical advancements and training methodologies behind DeepSeek-v3 could offer valuable insights for improving LLMs in multi-turn instructionfollowing tasks. Structural-Constraint-Categorized Performance The evaluated LLMs exhibit strong performance in follow-up structures, with nearly all models excelling in maintaining contextual continuity and generating coherent responses. Additionally, most models handle recall structures well, demonstrating their ability to reference prior conversational turns effectively. However, performance varies when dealing with more complex structures such as summary and expansion. DeepSeek-v3 and proprietary models outperform the others, indicating their superior capability in nuanced content condensation and elaboration. In contrast, refinement tasks pose significant challenge across all models. Even the strongest model, DeepSeek-v3, achieves only 0.8 in this category, highlighting the inherent difficulty of processing refinements accurately and maintaining coherence when adapting to modified user inputs. While LLMs exhibit strong instruction-following abilities in structured dialogue, refinement remains the most challenging task, requiring improvements in dynamic response adaptation. Future advance6 Model Name follow-up refinement expansion summary recall CSR ISR WCSR DRFR Deepseek-v3 Gemini-1.5-Pro GPT-4o Claude-3.5-Sonnet GLM-4-9B-Chat Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct Deepseek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Llama-3.1-Instruct-8B Phi-3.5-mini-instruct Yi-6B-Chat Mistral-7B-Instruct-v0. 0.99 0.97 0.98 0.98 0.95 0.97 0.95 0.91 0.94 0.96 0.94 0.98 0.97 0.8 0.78 0.78 0.8 0.75 0.73 0.76 0.62 0.73 0.71 0.68 0.62 0.59 0.92 0.91 0.88 0.88 0.84 0.87 0.9 0.85 0.82 0.84 0.87 0.87 0.87 1.0 1.0 0.97 1.0 0.97 0.97 0.94 0.86 0.89 0.79 0.94 0.84 0.71 1.0 0.94 0.91 0.91 0.94 0.97 0.97 0.78 0.84 0.94 0.94 0.94 0.97 0.98 0.97 0.97 0.95 0.95 0.94 0.94 0.81 0.87 0.85 0.88 0.86 0. 0.93 0.91 0.9 0.88 0.86 0.84 0.84 0.69 0.79 0.68 0.73 0.7 0.56 0.96 0.95 0.95 0.94 0.93 0.92 0.92 0.8 0.86 0.83 0.87 0.84 0.76 0.98 0.97 0.97 0.96 0.95 0.94 0.94 0.82 0.87 0.86 0.88 0.86 0.78 Table 2: STRUCTFLOWBENCH rated by GPT-4o. The left side displays the performance of various models on the five basic structural constraints, while the right side presents their performance on the four key metrics. ments should focus on enhancing models flexibility in refining responses based on iterative user feedback, ensuring more robust handling of complex multi-turn interactions. Intra-Turn-Constraint-Categorized Performance their The evaluation of LLMs across various constrengths straint dimensions highlights and weaknesses in following specific instructions. DeepSeek-v3, Gemini-1.5-Pro, and GPT-4o achieve near-perfect satisfaction rates, demonstrating strong capabilities in fine-grained instruction following. Most other models also perform well in rule-based constraints, such as Inverse Constraint, Keyword/Element Constraint, Style Constraint, and Situation Constraint. However, performance drops noticeably in format-related constraints, including Basic Format Constraint, Template Format Constraint, and Quantity Format Constraint, indicating that rigid format adherence remains significant challenge, even for top-performing models. Overall, while LLMs effectively handle intra-turn constraints, their ability to maintain format consistency remains key limitation. Addressing this challenge requires further advancements in structured output generation and adherence to strict formatting requirements. Task-Categorized Performance We evaluated various models across seven NLP tasks and mixed task. Unlike the constraintcategorized evaluation, where DeepSeek-v3 led across all metrics, the task-based analysis presents more nuanced picture. DeepSeek-v3 remains the overall best-performing model but leads only in Fact-based Questions, Professional Writing, Practical Writing, and Casual Chat. Gemini-1.5-Pro outperforms others in Open-ended Questions and Creative Writing, while Claude-3.5-Sonnet achieves the highest performance in Fact-based Questions and Task-oriented Role-playing. Meanwhile, GPT4o excels in the Mixture task type, reflecting its strength in handling diverse instructions across domains. These results highlight the varying strengths of these top-tier models across different tasks. Following the top four models, GLM-4-9B-Chat, Qwen2.5-14B-Instruct, and Qwen2.5-7B-Instruct maintain consistently strong performance across all tasks. Their stability, combined with their significantly smaller parameter sizes compared to the leading models, makes them highly cost-effective alternatives. In contrast, the remaining models all exhibit noticeable weaknesses in at least one task category, with Mistral-7B-Instruct-v0.3 underperforming across nearly all tasks, revealing clear performance gap. 4.3 Further Analysis 4.3.1 Complex Scenario Suitability Study This study aims to verify whether the multi-turn dialogue dataset we have constructed more closely aligns with real-world complex use cases. To achieve this, we designed an experiment to analyze three key properties of dialogue: logical coherence, goal clarity, and transition naturalness. The datasets used in this experiment include our StructFlowBench, three other multi-turn dialogue evaluation datasets (MT-Bench-101, Multi-if, and MTEval), and real-world dialogue dataset, WILDCHAT. Data Preparation: For each dataset, we randomly selected 50 English multi-turn dialogue samples, ensuring diverse representation of dialogue types. 7 Figure 3: The comprehensive complex scenario evaluation heatmap of five multi-turn dialogue datasets. Evaluation Protocol: To quantify how well the dialogues meet complex scenario requirements, we employed GPT-4o for automated scoring. Each dialogue was evaluated based on its performance in the following areas: Logical Coherence: Evaluates whether the dialogue is logically consistent and free of abrupt or unreasonable shifts. Goal Clarity: Assesses whether the dialogue clearly communicates the tasks goals and ensures both the users and systems intentions are transparent. Transition Naturalness: Judges whether transitions between dialogue turns are smooth and natural, without awkward or forced shifts. Each property was scored on scale from 1 to 5, where 1 indicates complete failure to meet the expected standard, and 5 represents perfect alignment with complex scenario requirements. Confusion Factor (CF): To further evaluate the datasets, we introduced the Confusion Factor (CF), which quantifies the proportion of dialogues in each dataset that scored 4 or higher, indicating they were mistakenly perceived as real-world interactions. The CF is calculated as follows: CF = Number of dialogues with average score 4 Total number of dialogues , By comparing the CF values of our StructFlowBench dataset with those of others, we can assess whether our dataset outperforms the others in terms of alignment with complex scenarios. Results and Discussion: The results are presented as heatmap, as shown in Figure 3. StructFlowBench achieves the highest scores across all three evaluation dimensions, leading with confusion factor of 0.83. MT-Bench-101, with its comprehensive dialogue generation process and rigorous human proofreading, also produces highquality dialogues and ranks closely behind with strong scores. In contrast, the WILDCHAT real multi-turn dialogue dataset, containing one million dialogues, exhibits generally low quality. Although we performed preliminary filtering on the WILDCHAT data, such as considering prompt length and dialogue content, the extracted dialogues still failed to meet the ideal quality standards As result, WILDCHAT performed the worst across the three evaluation dimensions for data-driven simulated scenarios. 4.4 Human Verification We extracted 30 dialogues from the output of Qwen2-7B-Instruct and invited domain experts to conduct comprehensive and detailed evaluation of the results. The experts rated the outputs using binary scoring system. The results showed that the Kappa coefficient between GPT-4os evaluations and those of the experts was approximately 0.75. This indicates that utilizing advanced LLMs, like GPT-4o, to assess the quality of outputs from other models is reliable approach, effectively reducing both subjective bias and the time costs associated with relying solely on human evaluation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we address key limitations in current multi-turn instruction-following research by introducing StructFlowBench, novel benchmark designed to capture the structural intricacies of complex dialogue scenarios. By incorporating dualconstraint evaluation system and six-category structural flow taxonomy, we provide more comprehensive framework for assessing the logical coherence, goal clarity, and transition naturalness of multi-turn dialogues. Our evaluations of 13 representative LLMs reveal critical insights into the structural processing capabilities of both closedsource and open-source models, offering valuable guidance for future advancements in instructionfollowing systems. Through StructFlowBench, we lay the foundation for more robust, realistic, and 8 contextually aware dialogue systems."
        },
        {
            "title": "Limitations",
            "content": "Currently, the structural flow in StructFlowBench is designed with single linear relationship to facilitate analysis and data generation. For instance, if the third turn dialogue serves as both recall structure to the first turn and follow-up structure to the second turn, the current approach retains only the recall relationship while disregarding other structural dependencies. This simplification may limit the comprehensive modeling of hierarchical dialogue structures. Future work should extend the structural flow framework to simultaneously capture multiple coexisting dialogue relationships, thereby providing more holistic representation of multi-turn dialogue complexity."
        },
        {
            "title": "Ethics Statement",
            "content": "This study utilizes GPT-4o to generate multi-turn dialogue data and annotate constraints, with manual review to filter out inappropriate content. However, unintended biases in GPT-4os generation process, as well as potential oversight during human review, may result in residual errors or biases in the dataset. While we have made every effort to ensure data quality and mitigate these issues, completely eliminating them remains challenging. Additionally, since this dataset is publicly available, there is risk of misuse for model training, which may compromise the validity of our benchmark. Therefore, we encourage the research community to exercise caution when using this dataset and to complement it with other evaluation methods to ensure comprehensive and fair model assessment."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. AI Anthropic. 2024. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3:18. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024. MT-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74217454, Bangkok, Thailand. Association for Computational Linguistics. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145. Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024. BotChat: Evaluating LLMs capabilities of having multi-turn dialogues. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 31843200, Mexico City, Mexico. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024a. Can large language models understand real-world complex instructions? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1818818196. Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. 2024b. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin 9 Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652. Tao Zhang, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, et al. 2024. Cfbench: comprehensive constraints-following benchmark for llms. arXiv preprint arXiv:2408.01122. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. 2023a. Lmsyschat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Instruction-following evaluand Le Hou. 2023. ation for large language models. arXiv preprint arXiv:2311.07911. Jiang, Qun Liu, and Wei Wang. 2024. FollowBench: multi-level fine-grained constraints following benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46674688, Bangkok, Thailand. Association for Computational Linguistics. Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. 2024. MT-eval: multiturn capabilities evaluation benchmark for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2015320177, Miami, Florida, USA. Association for Computational Linguistics. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601. Yuchong Sun, Che Liu, Kun Zhou, Jinwen Huang, Ruihua Song, Xin Zhao, Fuzheng Zhang, Di Zhang, and Kun Gai. 2024. Parrot: Enhancing multi-turn instruction following for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 97299750, Bangkok, Thailand. Association for Computational Linguistics. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, et al. 2024. Benchmarking complex instruction-following with mularXiv preprint tiple constraints composition. arXiv:2407.03978. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. 2024. Yi: 10 of characters, words, sentences, or paragraphs as specified. Template Format Constraint: The response must follow predefined template structure, such as starting with specific phrase, ending with certain statement, or using custom template provided by the user. Situation Constraint: The response must be tailored to given scenario or perspective, such as responding from specific identity or context. Inverse Constraint: The response must deliberately exclude or avoid certain constraints, such as not containing specific keyword, not involving particular element, or not using certain language style."
        },
        {
            "title": "C Detailed Results Categorized by",
            "content": "Intra-turn Constraints and Task Types Table 5 presents the intra-turn constraints performance of various models on StructFlowBench, while Table 6 illustrates the task-categorized performance. Additionally, Figure 4 provides radar chart comparing both perspectives."
        },
        {
            "title": "D Details of Prompts",
            "content": "Figure 5 to Figure 8 respectively illustrate the intermediate dialogue plan generation template, complete dialogue generation prompt template, constraint extraction prompt template, and GPT-4o evaluation prompt template used in our study."
        },
        {
            "title": "E Case of Data",
            "content": "Table 7 presents sample case from StructFlowBench."
        },
        {
            "title": "F Details of Models",
            "content": "All the details about the evaluated models are provided in Table 8."
        },
        {
            "title": "A Details of Topics and Tasks",
            "content": "Topic: Our dataset is generated across diverse range of 22 topics, including health, history, science, technology, digital media, automotive, astronomy, geography, lifestyle, literature, physics, finance, stocks, law, humanities, entertainment, music, fashion, art, environment, psychology, and mixed category that incorporates multiple topics. This broad coverage ensures that our data spans multiple domains, capturing wide array of fields and areas of interest. Task: StructFlowBench comprises seven NLP tasks and one mixed-category task, with their exact distribution detailed in Table 3. Category #Dialogues Fact-based Questions Open-ended Questions Practical Writing Creative Writing Professional Writing Casual Chat Task-oriented Role Play Mixture Total 25 20 26 21 21 15 17 10 155 Table 3: Task distribution of STRUCTFLOWBENCH dataset."
        },
        {
            "title": "B Details of Constraints",
            "content": "The distribution of all constraints is detailed in Table 4, with the definitions of intra-turn constraints as follows: Content Constraint: The response must strictly focus on the specified content scope and avoid any deviation from the topic. Keyword/Element Constraint: The response must include specific words or elements as required. Style Constraint: The response must be generated in specific writing style, such as formal, humorous, poetic, etc. Basic Format Constraint: The output must adhere to specified basic format, such as JSON, XML, CSV, Table, Markdown, etc. Quantity Format Constraint: The response must meet precise requirement for the number 11 Follow-up Refinement Expansion Summary Recall C1 C2 C3 C4 C5 C6 C7 C8 95 32 156 118 505 153 140 105 98 83 52 Table 4: The constraints distribution of STRUCTFLOWBENCH. Follow-up, Refinement, Expansion, Summary, Recall denote the structural constraints. The designations C1 - C8 denote the Constraint types of Content Constraint, Keyword/Element Constraint, Style Constraint, Basic Format Constraint, Quantity Format Constraint, Template Format Constraint, Situation Constraint, Inverse Constraint Model Name Deepseek-v3 Gemini-1.5-Pro GPT-4o Claude-3.5-Sonnet GLM-4-9B-Chat Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct Deepseek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Llama-3.1-Instruct-8B Phi-3.5-mini-instruct Yi-6B-Chat Mistral-7B-Instruct-v0. Inverse Constraint Keyword/Element Constraint Style Constraint Situation Constraint Basic Format Constraint Quantity Format Constraint Template Format Constraint Content Constraint 1.0 1.0 1.0 0.98 0.98 0.96 0.96 0.9 0.88 0.98 0.94 0.83 0.88 1.0 0.99 1.0 0.97 0.98 0.99 0.97 0.89 0.95 0.87 0.93 0.92 0.82 1.0 0.99 1.0 0.99 0.99 0.99 0.99 0.91 0.9 0.92 0.96 0.91 0. 1.0 1.0 1.0 1.0 0.96 0.95 0.99 0.84 0.9 0.94 0.96 0.9 0.9 0.99 0.99 0.99 0.95 0.97 0.9 0.95 0.82 0.9 0.73 0.82 0.87 0.65 1.0 0.99 0.98 0.99 0.95 0.93 0.91 0.7 0.84 0.79 0.81 0.65 0.59 0.99 0.99 0.99 0.94 0.95 0.92 0.88 0.8 0.84 0.7 0.8 0.91 0.56 1.0 0.99 1.0 0.97 0.99 0.97 0.96 0.83 0.88 0.88 0.9 0.9 0.8 Table 5: The intra-turn constraints performance of various models on STRUCTFLOWBENCH. Model Name Deepseek-v3 Gemini-1.5-Pro GPT-4o Claude-3.5-Sonnet GLM-4-9B-Chat Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct Deepseek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Llama-3.1-Instruct-8B Phi-3.5-mini-instruct Yi-6B-Chat Mistral-7B-Instruct-v0.3 Fact-based Questions Open-ended Questions Professional Writing Practical Writing Creative Writing Casual Chat Task-oriented Role-playing Mixture 0.93 0.91 0.92 0.93 0.89 0.9 0.9 0.77 0.79 0.81 0.86 0.84 0.71 0.96 0.97 0.96 0.95 0.93 0.94 0.92 0.85 0.9 0.88 0.88 0.9 0. 0.99 0.96 0.96 0.97 0.96 0.93 0.89 0.86 0.9 0.8 0.86 0.87 0.72 0.96 0.91 0.95 0.88 0.92 0.9 0.91 0.82 0.87 0.83 0.84 0.82 0.76 0.97 0.98 0.97 0.94 0.94 0.94 0.93 0.74 0.86 0.84 0.94 0.82 0.75 0.98 0.96 0.94 0.92 0.95 0.91 0.93 0.79 0.88 0.76 0.86 0.77 0.73 0.95 0.95 0.92 0.97 0.93 0.91 0.94 0.8 0.86 0.88 0.86 0.86 0.79 0.97 0.97 0.98 0.95 0.97 0.93 0.95 0.77 0.83 0.88 0.86 0.8 0. Table 6: Task-categorized performance of various models on STRUCTFLOWBENCH. Figure 4: The radar chart of intra-turn-constraint-categorized performance (a) and task-categorized performance (b). 12 Figure 5: Intermediate Dialogue Plan Generation Template 13 Figure 6: Complete Dialogue Generation Prompt Template 14 Figure 7: Constraint Extraction Prompt Template 15 Figure 8: GPT-4o Evaluation Prompt Template 16 User purpose Structure Summarized Prompts Complete Dialogue Check Lists The user aims to develop financial plan for fictional character by interacting with the assistant as financial advisor.The user wants to learn about different music genres and styles to enhance their personal music knowledge and broaden their music listening experience. \"source\": \"c1\",\"target\": \"c2\",\"relation\": \"follow-up\" \"source\": \"c1\",\"target\": \"c3\",\"relation\": \"recall\" \"source\": \"c3\",\"target\": \"c4\",\"relation\": \"unrelatedness\" \"source\": \"c4\",\"target\": \"c5\",\"relation\": \"refinement\" \"c1\" : \"The user asks the assistant, role-playing as financial advisor, to provide general strategy for young professional who wants to start saving for retirement.\" ... \"c5\": \"The user modify the detail level in last rounds prompt to request deeper dive into the unique instruments used in each genre for better understanding of their sounds.\" \"name\": \"c1\", \"user prompt\": \"Imagine am young professional entering the workforce. As my financial advisor, could you...\", \"assistant answer\": \"Certainly! Heres comprehensive strategy for...\" ... \"name\": \"c5\", \"user prompt\": \"In order to delve deeper into the musical intricacies ... Please format the response as table and ...\" \"assistant answer\": \"Certainly! Here is detailed examination of the unique instruments associated with each genre in table format:...\" \"name\":\"c1\" \"Situation Constraint\":\"Is the response given from the perspective of financial advisor?\" \"Keyword/Element Constraint\":\"Does the response include specific keywords such as... ?\" ... \"name\":\"c5\" \"Basic Format Constraint\":\"Is the response formatted as table?\" \"Refinement Constraint\":\"Is the c5 conversation refinement of c4 conversation?\" Table 7: An example of synthetic data. 17 Model GPT GPT-4o https://platform.openai.com/docs/models#gpt-4o Model Link Claude Claude-3.5-Sonnet Gemini Gemini-1.5-Pro https://docs.anthropic.com/en/docs/aboutclaude/models https://ai.google.dev/geminiapi/docs/models/gemini?hl=en#gemini-1.5-pro Deepseek https://huggingface.co/deepseek-ai/DeepSeek-V DeepSeek-v3 DeepSeek-R1-Distill-Qwen-7B https://huggingface.co/deepseek-ai/DeepSeek-R1Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B https://huggingface.co/deepseek-ai/DeepSeek-R1Distill-Llama-8B Qwen Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-14BInstruct https://huggingface.co/Qwen/Qwen2.5-7B-Instruct GLM GLM-4-9B-Chat https://huggingface.co/THUDM/glm-4-9b-chat Yi Yi-6B-Chat https://huggingface.co/01-ai/Yi-6B-Chat LLAMA Llama-3.1-8B-Instruct Mistral Mistral-7B-Instruct-v0.3 Phi Phi-3.5-mini-instruct https://huggingface.co/meta-llama/Llama-3.1-8BInstruct https://huggingface.co/mistralai/Mistral-7BInstruct-v0. https://huggingface.co/microsoft/Phi-3.5-miniinstruct Table 8: Model Links."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology, Jilin University",
        "Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China",
        "International Center of Future Science, Jilin University",
        "School of Artificial Intelligence, Jilin University",
        "School of Information and Library Science, University of North Carolina at Chapel Hill"
    ]
}