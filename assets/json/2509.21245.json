{
    "paper_title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets",
    "authors": [
        "Team Hunyuan3D",
        ":",
        "Bowen Zhang",
        "Chunchao Guo",
        "Haolin Liu",
        "Hongyu Yan",
        "Huiwen Shi",
        "Jingwei Huang",
        "Junlin Yu",
        "Kunhong Li",
        "Linus",
        "Penghao Wang",
        "Qingxiang Lin",
        "Sicong Liu",
        "Xianghui Yang",
        "Yixuan Tang",
        "Yunfei Zhao",
        "Zeqiang Lai",
        "Zhihao Liang",
        "Zibo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows."
        },
        {
            "title": "Start",
            "content": "Tencent Hunyuan Hunyuan3D-Omni: Unified Framework for Controllable Generation of 3D Assets Tencent Hunyuan3D https://3d.hunyuan.tencent.com https://huggingface.co/tencent/Hunyuan3D-Omni https://github.com/Tencent-Hunyuan/Hunyuan3D-Omni 5 2 0 2 5 ] . [ 1 5 4 2 1 2 . 9 0 5 2 : r Figure 1: Hunyuan3D-Omni is unified framework for supporting controllable generation based on point cloud, bounding box, voxel, and skeleton."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in single cross-modal architecture. We train with progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals ( e.g., skeletal pose) while downweighting easier ones ( e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows."
        },
        {
            "title": "Introduction",
            "content": "3D generation is fundamental task in computer vision and computational imaging, with applications spanning virtual reality, gaming, and film. As the volume of 3D data continues to grow, native 3D generation has emerged as mainstream approach, offering significant advantages in 1 both quality and speed. It is anticipated that, with the ongoing expansion of high-quality 3D datasets, 3D generation models will evolve into the next generation of automated modeling tools, facilitating faster workflows and dynamic interactions in digital content creation. Native 3D generation primarily involves two key components: the 3D Variational Autoencoder (VAE) and the 3D latent diffusion model (LDM). For example, method utilizing VecSet representation employs 3D VAE to compress point clouds into VecSet, from which decoder retrieves the Signed Distance Function (SDF) field for the 3D model. An iso-surface sampling technique is then applied to generate the visible 3D model from the SDF field. Similarly, the 3D LDM builds on the VecSet representation by stacking multiple layers of Diffusion Transformers (DiT) to facilitate the learning process from images to their corresponding 3D representations in VecSet form. Recent advancements, such as Hunyuan3D 2.1 Hunyuan3D et al. (2025), have showcased the powerful capabilities of native 3D generation for efficient and high-quality 3D modeling Zhao et al. (2024); Li et al. (2024). Despite these significant advancements, generating 3D assets from single image remains an ill-posed problem, complicating the accurate reconstruction of complete 3D structures. This often results in uncertainties and ambiguities in 3D geometry generation. To enhance geometric accuracy, it is crucial to incorporate additional information through controllable generation techniques. Such techniques not only improve geometric fidelity but also enable customized outputs by imposing specific conditions. For instance, the integration of depth information can alleviate geometric distortions and spatial misalignments caused by viewpoint variations and self-occlusion, while also enriching geometric details. Recent studies, including Clay Zhang et al. (2024b) and PoseMaster Yan et al. (2025b), have made notable progress in introducing additional conditions for downstream editing and pose control within the realm of 3D native generation models. Nevertheless, there remains need for further exploration in developing systematic and unified 3D controllable model. In this paper, we introduce Hunyuan3D-Omni, unified framework for fine-grained and controllable 3D asset generation. Building upon the foundational model Hunyuan3D 2.1 and following the workflow of 2D controllable generation models, Hunyuan3D-Omni enhances controllability and geometric accuracy by integrating various additional conditions, including point clouds, voxels, bounding boxes, and skeletons. To optimize training and model deployment costs, we consolidate these additional conditions into single generative model. Specifically, we utilize point clouds to represent these extra conditions and propose unified control encoder to differentiate between them and obtain corresponding embeddings. To preserve the structure and functionality of the base model, we concatenate the extracted embeddings with the DINO features of the input image. This approach allows us to achieve controllable 3D generation with minimal training steps. Experimental results demonstrate that Hunyuan3D-Omni effectively addresses common challenges in native 3D generation, such as distortions, flatness, missing details, and aspect ratio discrepancies, by providing additional control signals. Furthermore, it facilitates the standardization of character poses and the stylization of generated outputs, offering new perspectives and solutions for post-training applications in 3D generation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 3D Native Generation In recent years, the field of 3D generation has advanced rapidly, bringing significant impact to domains such as gaming, film, and animation. Early works include SDS and multi-view supervision approaches Poole et al. (2023); Liu et al. (2023b;a; 2024); Voleti et al. (2025) leverage pretrained image diffusion models to supervise the optimization of radiance fields Mildenhall et al. (2021) or NeuS Wang et al. (2021), thereby enabling 3D object generation. However, these methods suffer from multi-view consistency issues and slow generation speed, often producing noisy geometry. To address efficiency, LRM Hong et al. (2023) introduces feed-forward architecture that directly outputs tri-plane radiance field, enabling fast single-image-to-3D generation. Subsequent works Yang et al. (2024); Tang et al. (2024); Xu et al. (2024); Zhang et al. (2024a) adopt similar strategies, but their results remain limited in geometric detail and texture fidelity. 3DShape2VecSet Zhang et al. (2023a) introduces point cloudbased VAE to build native 3D generation framework combining VAE and LDM, offering higher quality and faster generation. Michelangelo Zhao et al. (2024) further aligns the VecSet latent space with semantics to enable 2 text-conditioned generation. Many later works follow the VecSet paradigm, scaling up models and datasets, replacing EDM with flow matching, and incorporating MoE architectures. These techniques have yielded high-quality 3D generation models such as CLAY Zhang et al. (2024b), Craftsman3D Li et al. (2024), TripoSG Li et al. (2025), FlashVDM Lai et al. (2025b), and Hunyuan3D 2.0/2.1/2.5 Zhao et al. (2025); Hunyuan3D et al. (2025); Lai et al. (2025a), which can rapidly generate high-quality 3D assets from modalities such as single images or text. Another line of research utilizes voxel-based VAE representations to achieve similarly high-quality 3D generation, including works such as XCube Ren et al. (2024), TRELLIS Xiang et al. (2024), and SparseFlex He et al. (2025a). Meanwhile, some works build upon these models, enabling low-poly generation Weng et al. (2024), part generation Yan et al. (2025c); Zhang et al. (2025), scene generation Yao et al. (2025), material generation Jiang et al. (2025); He et al. (2025b); Feng et al. (2025), and assets generation pipeline Lei et al. (2025), supporting many downstream applications. Although these methods enable fast generation of 3D objects from images or text, they generally lack support for more complex conditioning signals, such as points, bounding boxes, or voxels, which limits controllability and practical adoption. 2.2 3D Controllable Generation Recently, 2D controllable generation has made remarkable progress in image synthesis and editing. Methods such as ControlNet Zhang et al. (2023b), T2I-Adapter Mou et al. (2024), and IP-Adapter Ye et al. (2023) introduce various structured conditions (e.g., edges, depth, and pose) into diffusion models, enabling high-precision control over generation results and highlighting the great potential of multimodal conditions in enhancing controllability and practicality. In the area of 3D generation, similar approaches have emerged to support multiple types of control inputs. CLAY Zhang et al. (2024b) explores the integration of point cloud, bounding box, and voxel conditions into generative models via LoRA finetuning, thereby adapting to different input modalities. PoseMaster Yan et al. (2025b) incorporates pose control signals, enabling precise control over the pose of generated 3D characters. These works collectively demonstrate that accurate controllability is also achievable in 3D generation. However, most existing research focuses on single or limited conditions, and unified multicondition framework for 3D controllable generation is still lacking. How to flexibly integrate diverse control signals such as points, voxels, bounding boxes, and poses within single model to achieve fine-grained and cross-modal controllability of 3D contents remains an important yet insufficiently explored challenge."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries 3D VAE. Given an input point cloud RN(3+C) sampled from the mesh surface, where denotes surface normals, 3D VAE first extract point features and then obtain the corresponding latent vector set RLd via resampling from estimated distribution, where and indicate the length and dimension of latent VecSet, respectively. Subsequently, decoder is applied to reconstruct the signed distance function (SDF) field Fsd , in which we can leverage the iso-surface extraction to obtain explicit mesh output. The procedure of VAE can be formulated as follows: = (P), Fsd = D(Z) (1) 3D Diffusion. Given an image and its latent set representation of shape, the 3D diffusion model aims to model the denoising process, thereby achieving conditional generation from an arbitrary image. It first leverages an image encoder, such as DINO-v2 , to capture image embeddings ci and then exploits the multi layers of DiT to predict the added noise or velocity. For flow matching model used in Hunyuan3D 2.1 , its training objective is to transform simple noise distribution x0 (0, I) into complex data distribution x1 conditioned on image embeddings ci, which can be formulated as follows: Et,x0,x1,cvθ(x, t, c) (x1 x0)2 (2) 3 Figure 2: Overview of Hunyuan3D-Omni Architecture. This framework integrates four conditioning modalitiespoint clouds, bounding boxes, voxels, and skeletonsthrough unified control encoder. Hunyuan3D-Omni leverages Diffusion Transformer (DiT) and VAE-based decoder, following the Hunyuan3D 2.1 framework, to generate high-quality 3D assets with additional geometric control. 3.2 Hunyuan3D-Omni As shown in Figure 2, Hunyuan3D-Omni is unified framework for the controllable generation of 3D assets, which inherits the structure of Hunyuan3D 2.1 Hunyuan3D et al. (2025). In contrast, Hunyuan3D-Omni constructs unified control encoder to introduce additional control signals, including point cloud, voxel, skeleton, and bounding box. In this section, we begin by introducing the details of these controls, which significantly benefit in understanding the function of each additional control signal. Following this, we describe how to unify and interpolate these signals into the base model to achieve unified controllable model. 3.2.1 Skeleton Condition Unlike conventional object data, such as furniture, character data involves the definition of poses. In 3D animation production, modelers typically rig and skin characters in their rest pose to facilitate animation. On the other hand, in the design and printing of 3D figurines, we often create more dynamic and elaborate poses. Therefore, for an input image depicting an arbitrary pose, we may want to obtain 3D model in specified pose. To address this issue, we introduce the skeleton condition. We follow the PoseMaster Yan et al. (2025b) approach to define the representation of the skeleton. Specifically, we use the 3D coordinates of the starting points of the bones to represent each bone, selecting both body bones and hand bones. Consequently, we can define the input for the skeleton condition as Ppose RM6, where is the number of bones. We follow the PoseMasters training strategy to construct the training pair, which randomly samples one frame to obtain the image and extracts the skeleton and mesh from the other. 3.2.2 Bounding Box Condition In addition to specifying the pose of character, defining the aspect ratio of the generated object in canonical space not only helps address the issue of overly thin geometry caused by the lack of thickness information in images, such as 2D cartoon, but also allows for geometric editing by adjusting the proportions, such as modifying the length, width, and height of table. To achieve this, we propose the bounding box condition to control the aspect ratio. Specifically, we convert the length, width, and height ratios into the coordinates of the eight vertices in canonical space, which 4 facilitates the design of shared condition encoder. Consequently, we can define the input for the bounding box condition as Pbox R83. During training, we randomly perturb the rendered image or the underlying point cloud so that there is misalignment between the image and the point cloud, and then extract bounding box from the point clouds as training input. 3.2.3 Point Cloud Condition The lighting, occlusion, and perspective of an image significantly influence the current 3D generation models. Under certain special viewpoints, even humans may struggle to accurately assess the geometry, making it challenge to recover precise geometry from single image. Therefore, it is essential to leverage additional input information to enhance the accuracy of the generated output, such as depth maps and LiDAR points. Unlike images, these two representations provide accurate spatial structural information, which aids the model in perceiving the spatial structure of the target model and consequently generating more accurate results. To this end, we introduce the point cloud condition. Specifically, we consider that point clouds may originate from various sources, such as reconstruction models (e.g., VGGT Wang et al. (2025)), LiDAR scans, and RGBD sensors. Typically, point clouds captured by LiDAR and RGBD cameras are incomplete. To simulate this type of data, we inherit random drop sampling strategy from point cloud completion methods Yan et al. (2022); Yu et al. (2021); Yan et al. (2025a). Additionally, we also support the input of complete point clouds, which can be obtained from reconstruction models. We directly use the 3D coordinates of the point clouds in space as their representation, employing three resolutions: 512, 1024, or 2048. To simulate noisy point clouds from depth sensors and LiDAR scans, we further apply noise perturbation by generating noise and adding it to the condition point cloud. Therefore, we can represent the point cloud condition as Pc RNc3, where Nc is the number of points. 3.2.4 Voxel Condition Similar to point clouds, voxels also serve as strong conditioning signal, and we represent each voxel by the coordinates of its center. During Training, we construct voxel condition from the surface point cloud. We uniformly sample from the object surface to obtain the initial point cloud P, and then convert the point cloud into voxels. First, we normalize the coordinates of to [0, 16]3, quantize them into integers, and remove duplicate coordinates to obtain voxel coordinates at resolution of 16 16 16. Then, we map these coordinates back to the center coordinates of the voxels in the [1, 1]3 space. 3.2.5 Unified Control Encoder The unified control encoder is used to unify and distinguish these extra control signals. As all control signals can be represented as type of point cloud, we can design shared point encoder to extract the corresponding features easily. In particular, we leverage the bone points as the skeleton representation to control the pose of the humanoid input. To align the dimensions of all extra conditions, we repeat the feature channel of the condition of voxel, point cloud, and bounding box. As result, all controls can be represented as point cloud Pc RN6, where is the number of points or bones. We then apply position embedding followed by linear layer on the point cloud Pc to extract the condition features. Additionally, Different conditions correspond to distinct control objectives. For example, the point cloud condition aims to provide depth information from the image, reducing geometric distortions caused by artifacts, without altering the corresponding geometry of the image. In contrast, the skeleton condition is designed to control the pose of the input image, which does modify the corresponding geometry. Meanwhile, all these conditions are represented by point clouds, which tends to lead to control confusion. Therefore, to differentiate between the various conditions, we employ an embedding function to construct embeddings for the different control signals. Finally, we aggregate the extracted condition features and the distinguished embedding to obtain the final condition features. Thus, we can formulate our shared control encoder as follows: βi = [Linear(PosEmb(Pc where βi RNC is final control feature. Pc is the control condition. and are the embedding function and the linear projection, respectively. is repeat operation, and is the time of )), R(M(E(i)), r)], [0, 1, 2, 3] (3) repeating, which is used to enhance the signal for the condition type. operation. [] is concatenation Benefiting from the scalability of the attention structure, we can concatenate the image feature ci and control feature β to form joint feature that is fed into the DiT model of Hunyuan3D 2.1. As result, we can define the training objective of our Hunyuan3D-Omni as follows: t,x0,x1,c vθ(x, t, c) (x1 x0)2 (4) where = [c, βi] is the joint feature."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Implementation Details We follow the PoseMaster Yan et al. (2025b) approach to construct the training data for pose control and adopt the dataset of Hunyuan3D 2.1 Hunyuan3D et al. (2025) for other conditions. We follow the setting of Hunyuan3D 2.1 Hunyuan3D et al. (2025) to train our Hunyuan3D-Omni. Due to the varying lengths of different conditions, we set the batch size to 1. Additionally, we employ random sampling strategy to select the control conditions for the current batch. Notably, since the data for the pose condition is less abundant and more challenging to learn, we use higher sampling probability to prioritize tasks related to pose control. We directly train our diffusion model and unified control encoder using fixed learning rate of 1e-5 and the AdamW optimizer. we utilize DINO-v2-Large Oquab et al. (2023) to extract image features. 4.2 Qualitative Results In this section, we showcase the qualitative results for our Hunyuan3D-Omni. Specifically, we first use our Hunyuan3D-Omni to generate the controlled result and then leverage the Hunyuan3D 2.5 Lai et al. (2025a) to refine its geometry. Figure 3: Qualitative results of pose control, demonstrating that our Omni model generates accurately aligned geometry across diverse poses and input styles. Skeleton condition. In contrast to the point cloud condition and voxel condition that provide sparse 3D information to enhance geometry accuracy, the skeleton condition aims to adjust the pose of the input image, which is usually used in character generation. As shown in Figure 3, with the skeleton condition as additional input, our Omni model can generate high-quality character geometry which accurately corresponds to the target pose, including pose, sky pose, and handsup pose. In our experiments, we use character images of various styles as conditional inputs, including rendered images from 3D character data and synthetic images produced by generative models. Regardless of the input style, our Omni model consistently produces human body meshes with fine geometric details that remain strictly aligned with the input skeleton, without artifacts. 6 Figure 4: Qualitative results of bounding box control, our Omni model can generate objects at different scales, given different bounding boxes. Figure 5: Qualitative results of bounding box control, given the bounding box condition, our Omni model can avoid generating thin, sheet-like geometry. This strong capability for pose control demonstrates significant practical value for downstream applications such as 3D animation production and 3D figurine printing. Bounding box condition. Bounding box control signals allow flexible adjustment of the aspect ratio of the generated object. As shown in Figure 4, given the same image condition, different bounding boxes successfully regulate the size of the output mesh. Notably, this manipulation is not naive stretching: when the sofa is lengthened, extra supporting legs appear, and the Arc de Triomphe likewise acquires plausible shape. Moreover, as shown in Figure 5, the bounding box signal can inject an activation cue into the generation network when single-image conditioned generation fails, yielding valid mesh. Point cloud condition. As shown in Figure 6, we present the generation results under two settings: image only and image with point cloud control. For the latter, we further consider three types of 7 Figure 6: Qualitative results of point cloud control, showing improved alignment and detail recovery across complete, depth-projected, and scanned point cloud conditions compared to image-only results. Figure 7: Qualitative results of voxel control, demonstrating improved scale alignment and fine detail recovery with voxel conditions compared to image-only results. point cloud inputs: complete point clouds, point clouds from depth images, and point clouds from scans. For the first two cases in the figure, we observe that providing complete point cloud as control signal effectively resolves the ambiguity inherent in single-view inputs and allows the recovery of occluded internal structures. In the third and fifth cases, where surface point clouds are obtained via depth map, the additional input similarly mitigates single-view ambiguity, ensuring that the generated geometry is well-aligned in scale with the ground truth. In the fourth case, given noisy surface point cloud from scan, the generated geometry is also better aligned with the original object compared with the image-only baseline, addressing the issue where the image encoder tends to ignore the true object pose. In summary, once point cloud input is provided, our Omni model can effectively align the generated geometry with real-world geometry. This further demonstrates that even partial point clouds serve as strong cue for improving the quality of 3D geometry generation. Voxel Condition. Similar to the point cloud condition, the voxel condition provides sparse geometric cues that help resolve the ambiguities inherent in single image. As shown in Figure 7, in the first and fifth cases, the additional voxel control condition ensures that the generated objects are properly aligned in scale with the ground truth geometry. Cases 2, 3, and 4 further illustrate how the voxel condition contributes to recovering fine geometric details. For instance, restoring the flat surface of the shield, capturing the shape of the birds wing, and reproducing the low-poly style geometry of the cup. These examples clearly demonstrate that incorporating voxel conditions enables the model to faithfully recover both the proportions and the details of object geometry, thereby further improving generation quality."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose unified framework, called Hunyuan3D-Omni, for fine-grained and controllable 3D asset generation. We incorporate point cloud, voxel, bounding box, and skeleton to mitigate geometry distortion in image-only 3D generation and achieve style control. To unify these extra conditions in one diffusion model, we design lightweight unified control encoder. Building on the powerful existing 3D generation model, such as Hunyuan3D 2.1, Hunyuan3D-Omni just introduces lightweight encoder to achieve high-quality controllable generation. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows."
        },
        {
            "title": "6 Contributors",
            "content": "Authors are listed alphabetically by the first name."
        },
        {
            "title": "Penghao Wang\nQingxiang Lin\nSicong Liu\nXianghui Yang\nYixuan Tang\nYunfei Zhao\nZeqiang Lai\nZhihao Liang\nZibo Zhao",
            "content": ""
        },
        {
            "title": "References",
            "content": "Yifei Feng, Mingxin Yang, Shuhui Yang, Sheng Zhang, Jiaao Yu, Zibo Zhao, Yuhong Liu, Jie Jiang, and Chunchao Guo. Romantex: Decoupling 3d-aware rotary positional embedded multi-attention network for texture synthesis. arXiv preprint arXiv:2503.19011, 2025. Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025a. Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao Zhang, Guanying Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Materialmvp: Illumination-invariant material generation via multi-view pbr diffusion. arXiv preprint arXiv:2503.10289, 2025b. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material, 2025. URL https: //arxiv.org/abs/2506.15442. DaDong Jiang, Xianghui Yang, Zibo Zhao, Sheng Zhang, Jiaao Yu, Zeqiang Lai, Shaoxiong Yang, Chunchao Guo, Xiaobo Zhou, and Zhihui Ke. Flexitex: Enhancing texture generation via visual guidance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 39673975, 2025. Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025a. Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qinxiang Lin, Jinwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, and Xiangyu Yue. Unleashing vecset diffusion model for fast shape generation, 2025b. URL https://arxiv.org/abs/2503.16302. Biwen Lei, Yang Li, Xinhai Liu, Shuhui Yang, Lixin Xu, Jingwei Huang, Ruining Tang, Haohan Weng, Jian Liu, Jing Xu, et al. Hunyuan3d studio: End-to-end ai pipeline for game-ready 3d asset generation. arXiv preprint arXiv:2509.12815, 2025. 10 Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner, 2024. Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: Highfidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025. Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36: 2222622246, 2023a. Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1007210083, 2024. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 92989309, 2023b. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 42964304, 2024. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 42094219, 2024. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pp. 118. Springer, 2024. 11 Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pp. 439457. Springer, 2025. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 5294 5306, 2025. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multiview reconstruction. arXiv preprint arXiv:2106.10689, 2021. Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. Hongyu Yan, Zijun Li, Kunming Luo, Li Lu, and Ping Tan. Symmcompletion: High-fidelity and high-consistency point cloud completion with symmetry guidance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 90949102, 2025a. Hongyu Yan, Kunming Luo, Weiyu Li, Yixun Liang, Shengming Li, Jingwei Huang, Chunchao Guo, and Ping Tan. Posemaster: Generating 3d characters in arbitrary poses from single image. arXiv preprint arXiv:2506.21076, 2025b. Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025c. Xuejun Yan, Hongyu Yan, Jingjing Wang, Hang Du, Zhihong Wu, Di Xie, Shiliang Pu, and Li Lu. Fbnet: Feedback network for point cloud completion. In European Conference on Computer Vision, pp. 676693. Springer, 2022. Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, and Jingyi Yu. Cast: Component-aligned 3d scene reconstruction from an rgb image. ACM Transactions on Graphics (TOG), 44(4): 119, 2025. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse point cloud completion with geometry-aware transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1249812507, 2021. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023a. Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pp. 119. Springer, 2024a. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024b. Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu. Bang: Dividing 3d assets via generative exploded dynamics. ACM Transactions on Graphics (TOG), 44(4):121, 2025. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023b. Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025."
        }
    ],
    "affiliations": [
        "Tencent Hunyuan"
    ]
}