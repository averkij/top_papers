{
    "paper_title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
    "authors": [
        "Bhavik Agarwal",
        "Ishan Joshi",
        "Viktoria Rojkova"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 5 0 9 4 1 . 2 0 5 2 : r Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence Bhavik Agarwal, Ishan Joshi, Viktoria Rojkova MasterControl AI Research {bagarwal, ijoshi, vrojkova}@mastercontrol.com"
        },
        {
            "title": "Hugging Face Model",
            "content": "R1-Reasoning-Unstructured-To-Structured Dataset JSON-Unstructured-Structured Dataset"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of 1.5B parameter model through novel pipeline that combines synthetic reasoning data set construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on 20K sample unstructured to structured data set, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8H100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B) and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of resource-efficient framework for schema-constrained text generation."
        },
        {
            "title": "1 Introduction",
            "content": "In the highly regulated domain of bio-manufacturing quality, there is growing need to convert legacy production records into structured digital formats for compliance and analysis. Biomanufacturing has historically been steeped in paper culture, and even incremental moves toward electronic batch records are significant steps in industry digitalization [Lab25]. key prerequisite of this digital migration is schema adherence: AI systems, such as large language models (LLMs) used to transcribe or summarize production logs, must output data that fit predefined schema exactly. Any deviation (missing fields, incorrect format) could violate data integrity standards and render the generated records unusable for regulatory compliance [ea24c]. This introduces critical challenge: While modern LLMs are extraordinarily powerful in free-form text generation, ensuring that they produce strictly structured, schema-valid outputs is not trivial. LLMs by default generate text probabilistically, with no built-in guarantee of conforming to given format [ea25b]. This unpredictability poses risks when structured output is required for machine consumption or auditing. Empirical studies have found that even state-of-the-art models can fail to consistently follow format instructions success rates in producing correct JSON, for example, can vary widely from 0% to 100% depending on the task complexity and model used [ea24a]. Such inconsistency is problematic in any setting, but in regulated bio-manufacturing, an output that does not exactly match the schema (e.g., misformatted timestamp or an extra delimiter) might lead to compliance issues or require costly manual correction. Developers report that substantial effort is spent on prompt tuning and post-processing to coerce 1 LLMs into the desired format [Sou24]. From user perspective, unreliable formatting undermines trust constraints help prevent nonsense or hallucinated fields, thereby ensuring the output remains credible and useful [Sou24]. In short, structured output generation is both technical and governance challenge: the model must be reliable in content as well as form."
        },
        {
            "title": "2 Relevant Work",
            "content": "Researchers and practitioners are exploring several approaches to address these challenges and enforce schema adherence in LLM outputs. Key strategies include:"
        },
        {
            "title": "2.1 Supervised Fine-Tuning",
            "content": "An LLM can be fine-tuned on domain-specific data with the required output schema, so it learns to produce the correct structure. Fine-tuning on curated inputoutput pairs (e.g., historical records mapped to structured entries) can significantly improve format fidelity [ea24b]. However, this approach is resource-intensive training large models on specialized data is complex and costly, often requiring techniques like low-rank adaptation to be feasible [ea24b]. Fine-tuning also risks making the model too domain-specific or rigid outside the training distribution."
        },
        {
            "title": "2.2 Reinforcement Learning with Human Feedback (RLHF)",
            "content": "RLHF has proven effective in aligning LLMs with human instructions and preferences [ea25c]. By training model with feedback signals that reward correct adherence to the desired format, one can encourage structured outputs. Notably, the instruction-following abilities of models like ChatGPT/GPT-4 are largely attributed to such alignment techniques [ea25c], enabling them to obey fine-grained formatting requests (e.g. output as JSON). In regulated settings, RLHF could incorporate compliance-specific criteria into the reward model. The downside is that RLHF requires extensive high-quality feedback data and careful reward design; even then, smaller open-source models often still lag behind in format obedience despite alignment efforts [ea25c]."
        },
        {
            "title": "2.3 Constraint-Based Decoding",
            "content": "Rather than relying on the model to choose the right format, constraint-based methods force compliance by integrating schema rules into the generation process. Techniques like grammaror regex-guided decoding intercept the models token output, only allowing continuations that keep the output valid according to formal schema [ea25b], [ea24d]. This guarantees 100% schema adherence by construction. Recent frameworks implement fast, non-invasive constrained decoding that can guide LLMs to produce, for example, JSON that matches given schema exactly [ea24b]. Industry adoption of these ideas is rising; for instance, OpenAIs API now accepts developer-provided JSON schemas and assures that the models response will conform to them [ea25b]. The trade-off here is potential complexity in setup and slight inference latency overhead, as well as the challenge of designing schemas that are neither overnor under-constraining. Nonetheless, when correctness is paramount, constrained decoding is powerful approach."
        },
        {
            "title": "2.4 Prompt Engineering",
            "content": "The most accessible technique is to craft the input prompt in way that strongly cues the desired structure. This can involve giving the model explicit formatting instructions, examples of correctly formatted output, or even layout hints in the prompt. well-designed prompt can often induce model to produce nearly perfect structured output [ea24b]. Prompt engineering requires no model training and can be iteratively refined. However, it demands significant manual effort and expertise, and even then does not guarantee consistency [ea24b]. Models may still err on edge cases or as the prompt complexity grows, and maintaining long, complex prompts (especially across different models or updates) can be cumbersome. In practice, prompt-based solutions might be combined with lightweight validation or post-processing in high-stakes applications."
        },
        {
            "title": "2.5 Hybrid Constraint-Based Decoding and Prompt Engineering",
            "content": "By embedding knowledge of the schema at the prompt level and using specialized procedure to keep the generation on track (via tagging, iterative re-checks, or extra control tokens), hybrid systems achieve schema adherence more reliably than vanilla LLM approach [BTW23]. This structured, schema-first method is key to guaranteeing the outputs are valid, parseable, and aligned with downstream consumption requirements. Schema acts as blueprint for how the final text must be organized while controllable generation mechanism conditions the models decoding process on these schema constraints. Instead of free-form text generation, the model is guided to fill in required slots, adhere to the correct format, and avoid extraneous or malformed outputs [BTW23]. Each of these approaches comes with effectiveness trade-offs, especially under the stringent demands of regulated industries. Fine-tuning and RLHF can deeply instill format compliance into model but at high development cost and with less transparency. Prompt engineering is more flexible and avoid retraining, but it relies on the base models capacity to follow instructions. Constraint-based decoding offers hard guarantees on structure, appealing for compliance, though it requires integrating external constraint logic with the models output stream. The choice often depends on the specific use case and constraints for instance, biomanufacturers must consider not only technical accuracy but also validation, auditing, and data governance. Ensuring that LLM-generated records are both accurate in content and precise in format is vital to meet quality and regulatory standards. Recent work underlines that reliable structured generation remains an open challenge, calling for continued research into methods that can robustly align LLM outputs with predefined schemas [ea24a]."
        },
        {
            "title": "3 Method",
            "content": "Although the strategies outlined aboveranging from prompt engineering to constraint-based decodingcan In regulated improve structured output, they often require specialized tooling or large-scale fine-tuning. domains such as bio-manufacturing, these approaches must also be cost-effective and robust. In this section, we describe reasoning-driven methodology that leverages synthetic data construction and iterative LLM reasoning to ensure schema adherence with minimal overhead. Specifically, we demonstrate how to: Build RL reasoning dataset Create synthetic unstructured and structured data [ea23a],[ea22b] in tandem using controlled prompts and Qwen 14B/32B [Tea24], Reverse-engineer how unstructured text can map onto an empty JSON schema by engaging distilled DeepSeek R1 Qwen 32B [DA25] to explainstep by stephow each schema field is populated. 3 Figure 1: Think inside the JSON pipeline 4 Train reasoning model with RL and SFT."
        },
        {
            "title": "Develop custom reward mechanisms that directly evaluate how well the outputs adhere to a predefined",
            "content": "schema while balancing fluency, completeness, and correctness. Train R1-Zero reasoning model from Qwen 2.5 1.5B base model using RL [aa24],[ea23c] and synthetic unstructured-structured pair dataset, integrate custom rewards into GRPO [aa24] without altering the core policy optimization loop. The combined reward drives the training so that the model produces outputs that score highly on all relevant criteria. Fine tune R1-Zero model into R1 with supervised fine-tuning using reasoning dataset."
        },
        {
            "title": "3.1 Generating Structured and Unstructured Data",
            "content": "We begin by prompting language model (Qwen 14B and 32B) to produce diverse, fully populated JSON schemas (including nested and complex fields). These filled schemas emulate real-world documentation (e.g., QA checklists, batch records) while showcasing variations in schema hardness and domain."
        },
        {
            "title": "You are an expert in building a hierarchical JSON schema and object for the domain",
            "content": "{ DOMAIN }. Your task is to create : 1. multi - level JSON Schema describing : - ROOT ( level 0) , - SECTION ( level 1) , - SUBSECTION ( level 2) , - $ DETAIL _{ }$ ( level 3+) . Each level may contain tables (2 data layouts ) and checkbox elements ( MCQs , confirmations ) , with nested components reflecting complex structures . 2. JSON Object that strictly matches this schema , including : - \" id \" and \" title \" - \" level \" and \"$ level _{ type }$\" - An array of \" component \" objects ( paragraphs , tables , or checkboxes ) - recursive \" children \" array - Special \" properties \" ( . . , \" variables \" , \" content \") for data , logs , metrics , or formulas Formatting Requirements : - Escape all quotes (\") , replace newlines with - No trailing commas , single quotes , or extra data - Enclose the final output with no extra explanations : In parallel, we generate corresponding blank schemasretaining structural outlines but omitting values. This gives us before and after pair for each schema: an empty template and filled instance. Such pairs are crucial for teaching LLMs how unstructured text should be systematically transformed into the exact JSON schema. We then produce unstructured text reflecting the same content as the filled schemabut presented in varying layouts (e.g., sequential paragraphs, parallel sections, combined strategies) and table formats (ASCII art, XML/HTML-like snippets, simulated PDF extraction, etc.). These multi-format narratives mimic the real challenge of reading and interpreting inconsistent legacy documents."
        },
        {
            "title": "You are an expert in generating hierarchical text documents from JSON Object data",
            "content": "points . ** Task **: Convert the JSON Object into an unstructured , paragraph - based document . ** Given Data **: ** Domain **; ** JSON Schema **; ** JSON Object ** ** OUTPUT FORMAT ** ( enclosed strictly within < text >) : 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 1 2 3 5 6 7 5 8 10 11 12 13 14 16 17 18 19 20 22 1 2 3 4 6 7 8 9 10 12 13 14 15 < text > [ Insert formatted hierarchical text from JSON object here ] </ text > ** Layout References **: - Layout options for components / levels : { RANDOM _ LAYOUT } - Table styles : { RANDOM _ TABLE _ STYLE } - Checkbox styles : [ ] , YES , NO , /A , etc . ** RULES **: 1. Map every JSON level , component , and attribute to the correct layout / style . 2. Surround JSON data points with additional words / sentences to obscure parsing . 3. Include all data ( title , variables , metadata , content ) ; no extra sections . 4. End each data point with brief , unrelated remark . 5. Add filler paragraphs ( definitions , domain info , etc .) not directly tied to the JSON content . In doing so, we create synthetic corpus that covers broad range of domain contexts, from general manufacturing logs to specialized quality assurance frameworks. Each piece of unstructured text is logically equivalent to filled JSON schema, yet differs in structure, style, and formatting."
        },
        {
            "title": "3.2 Reasoning Dataset: Reverse-Engineering from Text to Schema",
            "content": "We employ Distilled DeepSeekR1 Qwen 32B with the following prompt:"
        },
        {
            "title": "You are an AI assistant tasked with extracting structured data from a piece of",
            "content": "text . Inputs : 1. Text ( source of information ) 2. Blank Schema ( unfilled JSON schema ) 3. Filled Schema ( final populated JSON ) Goals : 1. Compare Text + Blank Schema to the Filled Schema . 2. Explain step by step ( chain - of - thought ) how the text populates the blank schema . 3. Output only the reasoning steps ( thought process ) . 4. Cross - verify that this reasoning exactly produces the Filled Schema . Format your final response as : Chain of Thought Explanation : \"\"\" The LLM is instructed to output only its chain-of-thought reasoning, explicitly describing the mapping from text to schema. Such self-explaining prompts push the model to maintain strict schema fidelity while revealing the logic behind each structural decision. Because the prompt demands an explicit reasoning path, the LLM self-checks how each field is filled, minimizing random or malformed output. The chain-of-thought not only ensures correctness but also documents how the text was interpreted which is vital for regulated environments. By varying the domain (e.g., different types of QA reports) and text layout styles, we create dataset that fosters LLM resilience to formatting quirks."
        },
        {
            "title": "3.3 GRPO Training on a Small Foundation Model",
            "content": "Once we finalize the reasoning dataset, we proceed to train small foundation modelmirroring the minimalistic DeepSeek R1 Zero approachusing GRPO [DA25]. We employ 1.5B-parameter base model to develop reasoning capabilities without any supervised data, focusing on their self-evolution through pure reinforcement learning process [DA25]. By leveraging group-based advantage calculation and carefully designed reward signals (e.g., schema compliance, correctness), we efficiently instill structured reasoning capabilities within resource-constrained pipeline. By incorporating multiple reward functions [ea23b] into 6 the GRPO framework, we can simultaneously encourage format correctness (via format) and content/domain correctness (via equation). The combined reward drives training so that the model produces outputs that score highly on all relevant criteria. The entire process remains computationally light (e.g., 20 hours on an 8H100 cluster), demonstrating that strict schema adherence can be achieved even with compact, low-overhead foundation models."
        },
        {
            "title": "3.3.1 JSON-Based Reward",
            "content": "This reward algorithm balances two aspects: (1) schema faithfulness via the key-value matching fraction, and (2) structural completeness via JSON length similarity. high final reward indicates that the predicted JSON object closely matches the ground truth both in field contents and overall size."
        },
        {
            "title": "3.3.2 Format Verification Reward",
            "content": "The format check enforces correct usage of specialized tags, crucial for downstream tasks that rely on clearly separated reasoning (<think> block) and final answers (<answer> block). The binary reward (0 or 1) simplifies reinforcement signals, focusing exclusively on structural correctness rather than content fidelity. The optional logging step enables sampling small fraction of completions for qualitative inspection, aiding diagnostic or future training data curation. 7 Algorithm 1 JSON-Based Reward Computation 1: Given: list of completions = {c1, . . . , cn} from the model. list of ground-truth JSON objects = {g1, . . . , gn}. Each gi is valid JSON string. 2: procedure ComputeReward(C, G) 3: 4: 5: for each pair (ci, gi) in (C, G) do \"<think>\" ci ans substring(c if ans is empty then i, <answer>, </answer>) 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: ri 0 append ri to R; continue end if parse ans into answer json; parse gi into gt json if either parse fails then ri 0 append ri to R; continue end if Ka keys(answer json) Kg keys(gt json) total fields Ka Kg matching fields (cid:80) if total fields > 0 then else key match score 0 end if key match score matching fields total fields k(KaKg) 1[answer json[k] = gt json[k]] ℓa length(answer json) or 1 ℓg length(gt json) or 1 length ratio min(ℓa,ℓg) max(ℓa,ℓg) ri key match score+length ratio clamp ri to [0, 1]; round to 1 decimal place if ri 0.6 then with 60% probability end if append ri to log 31: 32: 33: 34: 35: 36: end procedure end for return Initialize rewards list Insert <think> prefix Parse as JSON Compute field overlap Compare JSON lengths Calculate final reward Algorithm 2 Format Verification Reward 1: Goal: Assign reward of 0 or 1 depending on whether generated completion follows an expected structure using <think>...</think> and <answer>...</answer>. 2: Inputs: list of completions = {c1, . . . , cn} (model-generated). list of ground-truth objects = {g1, . . . , gn} (not directly used here, but included for extensibility). small probability (e.g., 0.1) for selectively logging completions. 3: Output: list of scalar rewards = {r1, . . . , rn}, with ri {0, 1}. 4: Initialize an empty rewards list: [ ]. 5: for each pair (ci, gi) in (C, G) do Synthesize prompt format: 6: Probabilistic logging: Draw [0, 1]. if < then Log i to file for future analysis. \"<think>\" ci Prepend <think> end if Format check via regex: Define = \"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)</think>n<answer>([sS]*?)</answer>$\" Match against i. if match fails (no correct grouping) then 7: 8: 9: 10: 11: 12: 13: 14: 15: ri 0 else ri 1 end if Append ri to R. 16: 17: 18: end for 19: return Algorithm 3 GRPO with Multiple Reward Functions"
        },
        {
            "title": "Notation and Setup",
            "content": "Define combined reward: Rcomb(c) = (cid:0)r1(c), r2(c), . . . , rK(c)(cid:1), where can be weighted sum, mean, or any aggregator, πθ be the current policy (a language model parameterized by θ), {rk}K k=1 be reward functions (e.g., rformat, requation) Group-Based Relative Advantage Let = { c1, . . . , cM } be group of outputs sampled from πθ. For each ci, compute combined reward Ri = Rcomb(ci). Define the relative (rank-based) advantage: A(rel)(ci) ="
        },
        {
            "title": "1\nM − 1",
            "content": "(cid:88) j=i 1(cid:0)Ri > Rj (cid:1), which is the fraction of samples in that have lower reward than ci."
        },
        {
            "title": "GRPO Update",
            "content": "Update θ to favor completions with higher relative advantage. The GRPO loss for group is: LGRPO(θ) = (cid:88) ciG A(rel)(ci) log πθ(ci) + Reg(θ), where Reg(θ) includes regularization terms (e.g., entropy bonus, KL-divergence). Figure 2: GRPO Training Metrics"
        },
        {
            "title": "3.4 Supervised Fine-Tuning",
            "content": "While reinforcement learning confers advanced reasoning capacities, but supervised fine-tuning provides the final taskand schema-specific polish that ensures outputs are both logically grounded and robustly aligned with real-world standards. [DA25]. Reinforcement learning (RL) optimizes policy for broad correctness or format adherence but can overlook rare or domain-specific intricacies (e.g., specialized field naming conventions, unusual data types). SFT exposes the model to explicit examples that emphasize precisely how to handle real-world edge cases, ensuring no field or condition is left under-represented. Although RL fosters adaptability, the learned policy may still exhibit variability in ambiguous contexts or unrepresented task scenarios [DA25]. SFT, by contrast, anchors the final policy to concrete labeled examples, reducing output drift. By overlaying final SFT stage, ThinkJSON tightly aligns its already-developed reasoning to the strict output requirements (e.g., correct JSON keys, mandatory fields), producing outputs suitable for audit 10 or compliance. For SFT (and SFT+LoRA) we used the Unsloth training framework on an A100 GPU, completing the process in about 3 hours. ### Role : You are an expert data extractor mapping hierarchical text to given JSON Schema . ### DATA INPUT : Text ; Blank JSON Schema ### TASK REQUIREMENT : 1. Map all relevant text to the JSON Schema . 2. Output in two sections : - < think >: Reasoning - < answer >: Filled JSON ### STRICT RULES : 1. Provide both < think > and < answer >. - If minimal reasoning , say : \" Direct mapping from text to schema .\" 2. Map text exactly to the JSON Schema ( no omission / alteration ) . 3. Preserve hierarchy ( ROOT $ to $ SECTION $ to $ SUBSECTION $ to $ DETAIL _ ) 4. Correctly set attributes ( id , idc , idx , level _ type , component _ type , etc .) . 5. JSON Format : - Escape quotes as \" - Replace newlines with - No trailing commas - Only double quotes 6. Explain key decisions in < think >. ### IMPORTANT : If < think > or < answer > is missing , response is incomplete .\") , axis =1) 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 Figure 3: SFT Training Metrics"
        },
        {
            "title": "4 Evaluation",
            "content": "We evaluated five models: ThinkJSON, Original DeepSeek R1 (671B), Distilled DeepSeek R1 (Qwen-1.5B / Qwen-7B) and Gemini 2.0 Flash (70B) which specializes on structured output generation [tea25], on structured data extraction benchmark involving 6.5K rows. Each row was processed to produce or omit 11 valid JSON object, and we measured metrics including: Rows With No Output: Number of rows for which the model produced no structured output. Rows With Valid JSON: Number of rows resulting in syntactically valid JSON objects. Mean Match Percentage: Average proportion of fields correctly mapped. Mean Noise Percentage: Average proportion of extraneous or malformed tokens within the extracted JSON. Figure 4: Performance Comparison As illustrated, ThinkJSON yields strong results, with 62.41% mean match (highest of all five models) and the lowest 0.27% mean noise, indicating minimal extraneous output. The Original DeepSeek R1 also achieves relatively high valid-JSON coverage but shows lower mean match (41.43%) and higher noise (11.14%). The two distilled variants of DeepSeek R1Qwen 1.5B and Qwen 7Bexhibit weaker performance overall, with high rates of no extracted JSON or large amounts of noise. Meanwhile, Gemini 2.0 Flash achieves midrange mean match of 42.88% but suffers from significant noise at 10.86%. These findings underscore the effectiveness of our structured reasoning approach in producing concise, schema-valid outputs."
        },
        {
            "title": "5 Discussion and Future Direction",
            "content": "Our experimental findings confirm that the reasoning-driven, schema-constrained generation pipeline is both broadly applicablecapable of handling diverse reasoning tasks beyond purely mathematical or scientific domainsand budget-conscious, as it requires comparatively moderate GPU resources and modest dataset of reasoning examples. This balanced approach addresses critical need in bio-manufacturing compliance , where AI systems must deliver not only correct structure but also reliable, domain-specific reasoning to meet regulatory standards [ea22a], [ea25a]. 12 The hallmark of our framework is integrating compliance considerations at the core of the generation process. Rather than relying on prompt-based or post-hoc solutions, our pipeline combines schema adherence objectives with iterative reasoning loops, thus reducing the need for manual oversight. This focus on strict output validation resonates with bio-manufacturings regulatory requirementswhere precise field mappings and hierarchical consistency are crucial for electronic batch records and industry audits. While we have employed 1.5B-parameter foundation model, our method is readily scalable to bigger backbones (e.g., 7B parameters). Larger models could potentially yield richer context interpretation and more robust handling of rare or domain-specific phenomena. In future work, we plan to explore how increased capacity further expands the set of reasoning scenarios the model can tackle while maintaining resource efficiencya pivotal benefit in industrial adoption. Overall, this reinforcement + fine-tuning pipeline for structured text generation offers flexible, compliance-aware approach that applies universal reasoning principlesspanning regulated biomanufacturing tasks and broader domainswithout incurring prohibitive computational overhead. This synergy of versatility and cost-effectiveness positions our method as significant step forward in delivering reliable, schema-adherent AI-driven solutions."
        },
        {
            "title": "References",
            "content": "[aa24] Z. Shao at all. Pushing the Limits of Mathematical Reasoning in Open Language Models. https://arxiv.org/pdf/arXiv:2402.03300, 2024. [BTW23] Remi Louf Brandon T. Willard. Efficient Guided Generation for Large Language Models. https://arxiv.org/pdf/2307.09702, 2023. [DA25] DeepSeek-AI. Deepeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. https://arxiv.org/pdf/2501.12948, 2025. [ea22a] Nico Erdmann et all. AI Maturity Model for GxP Application: Foundation for AI Validation. https://ispe.org/pharmaceutical-engineering/march-april-2022/ai-maturity-model-gxpapplication-foundation-ai, 2022. [ea22b] Yizhong Wang et all. Self-Instruct: Aligning Language Models with Self-Generated Instructions. https://arxiv.org/abs/2212.10560, 2022. [ea23a] [ea23b] [ea23c] [ea24a] Aman Madaan et https://arxiv.org/abs/2303.17651, 2023. all. Self-Refine:"
        },
        {
            "title": "Iterative Refinement with",
            "content": "Self-Feedback. Christoph Dann et all. Reinforcement Learning Can Be More Efficient with Multiple Rewards. https://proceedings.mlr.press/v202/dann23a.html, 2023. P. Wang et all. Math-Shepherd: Labelfree Step-by-Step Verifier for LLMs in Mathematical Reasoning. https://arxiv.org/pdf/2312.08935, 2023. Connor Shorten et all. StructuredRAG: JSON Response Formatting with Large Language Models. https://arxiv.org/abs/2408.11061, 2024. [ea24b] Diya Li et all. Large Language Model-Driven Structured Output: Comprehensive Benchmark and Spatial Data Generation Framework. https://www.mdpi.com/2220-9964/13/11/405, 2024. [ea24c] Michael Liu et all. We Need Structured Output: Towards User-centered Constraints on Large Language Model Output. https://lxieyang.github.io/assets/files/pubs/llm-constraints-2024/llmconstraints-2024.pdf, 2024. [ea24d] Yixin Dong et all. Xgrammar: Flexible and efficient structured generation engine for large language models. https://arxiv.org/pdf/2411.15100, 2024. 13 [ea25a] [ea25b] [ea25c] Vaishali Aher et al. An Overview on Pharmaceutical Regulatory Affairs Using Artificial Intelligence. https: // www. ijpsjournal. com/ article/ An+ Overview+ on+ Pharmaceutical+ Regulatory+ Affairs+ Using+ Artificial+ Intelligence+ , 2025. Saibo Geng et all. Generating Structured Outputs from Language Models: Benchmark and Studies. https://arxiv.org/html/2501.10868, 2025. Zhaoyang Wang et all. Verifiable Format Control for Large Language Model Generations. https://arxiv.org/html/2502.04498, 2025. [Lab25] MaryAnn Labant. Smart Biomanufacturing:"
        },
        {
            "title": "From piecemeal",
            "content": "to all of piece. https://www.genengnews.com/topics/bioprocessing, 2025. [Sou24] Tharsis Souza. Taming LLMs. 2024. [Tea24] Qwen Qwen2.5: https://qwenlm.github.io/blog/qwen2.5, 2024. Qwen. Team. A"
        },
        {
            "title": "Party",
            "content": "of Foundation Models. [tea25] Google AI team. Generate Structured Output with the Gemini API. https://ai.google.dev/geminiapi/docs/structured-output?lang=python, 2025."
        }
    ],
    "affiliations": [
        "MasterControl AI Research"
    ]
}