{
    "paper_title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
    "authors": [
        "Qingyang Mao",
        "Qi Cai",
        "Yehao Li",
        "Yingwei Pan",
        "Mingyue Cheng",
        "Ting Yao",
        "Qi Liu",
        "Tao Mei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\\%+ higher GPT-Balance score. Moreover, it completes a $512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit."
        },
        {
            "title": "Start",
            "content": "Visual Autoregressive Modeling for Instruction-Guided Image Editing Qingyang Mao1*, Qi Cai2, Yehao Li2, Yingwei Pan2, Mingyue Cheng1, Ting Yao2, Qi Liu1, Tao Mei2 1 University of Science and Technology of China, Hefei, China 2 HiDream.ai Inc. maoqy0503@mail.ustc.edu.cn, {mycheng, qiliuql}@ustc.edu.cn {cqcaiqi, liyehao, pandy, tiyao, tmei}@hidream.ai 5 2 0 2 1 2 ] . [ 1 2 7 7 5 1 . 8 0 5 2 : r Abstract Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer distinct paradigm by formulating image synthesis as sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, visual autoregressive (VAR) framework that reframes image editing as next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusionbased methods by 30%+ higher GPT-Balance score. Moreover, it completes 512 512 editing in 1.2 seconds, making it 2.2 faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit. Introduction Instruction-guided image editing (Brooks et al. 2023; Zhang et al. 2023; Sheynin et al. 2024) represents significant advance in generative AI, shifting the paradigm from pure synthesis towards fine-grained interactive control. This progress has been propelled by two primary factors: the curation of large-scale, high-quality editing datasets (Ge et al. 2024; Ye et al. 2025) and the rise of diffusion models (Ho et al. 2020; Peebles et al. 2023; Labs 2024; Qian et al. 2025, 2024; Wan et al. 2025) as the mainstream architecture. In particular, the impressive visual fidelity is largely derived from the iterative denoising process in diffusion models. However, this core mechanism also imposes critical limitation. The global nature of the denoising process often leads to unintended spurious edits, causing modifications to bleed into regions *This work was performed at HiDream.ai. that should remain unchanged. Simultaneously, the multistep denoising process incurs substantial computational cost and limits real-time practical use. In contrast to diffusion models, autoregressive (AR) models (Esser et al. 2021; Sun et al. 2024; Pang et al. 2025) offer fundamentally different paradigm. Instead of holistic iterative refinement, they synthesize an image causally in sequential token-by-token manner. This compositional generation process is natural fit for image editing. Specifically, it provides flexible mechanism for preserving unchanged regions while precisely modifying edited regions, addressing the entanglement problem of diffusion models. More encouragingly, the pioneering visual autoregressive (VAR) modeling (Tian et al. 2024) breaks the spatial structural degradation through the scale-by-scale prediction strategy, leading to high-quality image generation. While the VAR paradigm has shown competitive performance in image synthesis (Kumbong et al. 2025; Han et al. 2025), its potential for instruction-guided editing remains largely unexplored. Early training-free attempts (Wang et al. 2025) repurpose text-to-image models but lack the task-specific knowledge required for robust editing, causing them to lag significantly behind diffusion-based methods (Hou et al. 2024; Zhao et al. 2024a; Zhang et al. 2025). We address this critical gap by introducing VAREdit, the first tuning-based VAR framework for instruction-guided editing. VAREdit reframes image editing as next-scale prediction problem, which generates multi-scale target features to achieve precise image modification. central challenge in this paradigm is how to effectively condition the model on the source image. straightforward approach is to present full-scale source conditions, yet it is computationally inefficient due to the lengthy input sequences. While the finestscale-only conditional setting provides sufficient information and eliminates the redundant input tokens, it creates severe scale mismatch since high-frequency details may disrupt the prediction of coarse target features. To resolve this issue, we perform systematic analysis of inter-scale dependencies and uncover key insight: the models sensitivity to scale-aware conditioning is concentrated in its first self-attention layer. Accordingly, we propose the ScaleAligned Reference (SAR) module, mechanism that injects the scale-matched source information only in the first selfattention layer, while keeping other layers still use the finestFigure 1: VAREdit achieves high-precision performance in instruction-guided image editing. It excels across diverse and challenging editing scenarios, including object-level modifications (addition, replacement, removal), attribute changes (material, text, posture, style, color) and complex compositional edits. scale condition. As demonstrated in Figure 1, this design enables VAREdit to achieve exceptionally precise editing. Our contributions can be summarized as follows: We introduce VAREdit, the first tuning-based visual autoregressive model for instruction-guided image editing. We identify scale mismatch issue in the finest-scale conditioned VAREdit and propose the SAR module as an effective solution. VAREdit sets new state-of-the-art on standard editing benchmarks, outperforming leading diffusion models in both editing adherence and generation efficiency. Related Work Instruction-Guided Image Editing Instruction-guided image editing (Brooks et al. 2023; Zhao et al. 2024a; Yu et al. 2025; Cai et al. 2025) seeks to modify an image according to users textual instruction, which differs from captionbased approaches (Hertz et al. 2022; Feng et al. 2025) that often resynthesize an entire image from global description. This field is predominantly driven by diffusion models and seminal work InstructPix2Pix (Brooks et al. 2023) established powerful paradigm by channel-wise concatenating source and target images. Subsequent research advanced this by curating larger-scale datasets with expert models (Zhang et al. 2023; Sheynin et al. 2024; Zhao et al. 2024a; Ge et al. 2024; Ye et al. 2025) or exploring alternative conditioning, like spatial concatenation for large-motion edits (Zhang et al. 2025; Cai et al. 2025). However, diffusionbased models often produce unintended modifications and suffer from slow iterative sampling. To address these limitations, autoregressive (AR) models have been explored. For instance, EditAR (Mu et al. 2025) presents vanilla AR editing framework, yet its performance is not on par with leading diffusion methods due to the spatial dependency limitation in base model. Our work builds on the pioneering visual autoregressive (VAR) (Tian et al. 2024) architecture, tuning-based approach that surpasses recent diffusion-based models in both edit quality and efficiency. Autoregressive Image Generation Autoregressive (AR) models, mainstay in language tasks, have been extended to image synthesis. They operate by tokenizing an image into discrete sequence and then training Transformer model to predict the next token. Early works use vector quantization (VQ) for tokenization (Van Den Oord et al. 2017; Esser et al. 2021; Chang et al. 2022; Yu et al. 2022), while recent advances such as lookup-free quantization (Yu et al. 2023; Zhao et al. 2024b) improve performance with large vocabularies. The AR modeling paradigm (Zheng et al. 2025; Yao et al. 2025) itself has also made significant progress. For instance, MAR (Li et al. 2024) uses masked multi-token prediction to improve quality and speed, while RandAR (Pang et al. 2025) enables flexible token ordering for new capabilities such as resolution extrapolation. To better capture spatial dependencies, models like VAR (Tian et al. 2024) and Infinity (Han et al. 2025) introduced coarse-to-fine prediction schemes. Compared to diffusion models, AR approaches generally offer superior prompt adherence and faster inference. However, their application to instruction-guided image editing remains underexplored. This challenging as it requires conditioning on both visual and text inputs, and our work aims to bridge this gap. Methodology We first review the visual autoregressive (VAR) modeling paradigm. Then, we introduce VAREdit, novel framework that reframes instruction-guided image editing as multiscale conditional generation task. Finally, we analyze the challenges of source image conditioning and present our Scale-Aligned Reference (SAR) module, targeted solution to the scale-mismatch problem in naive conditioning. Preliminary Visual autoregressive (VAR) model generally comprises multi-scale visual tokenizer and Transformer-based generation model. The process begins with an encoder that maps an image to continuous feature representation = E(I) Rhwd. quantizer then decomposes into hierarchy of discrete residual maps R1:K = Q(F). These maps follow coarse-to-fine structure, where the spatial resolution (hk, wk) of each residual Rk increases with its scale index k. Then, the Transformer model predicts the residuals in an autoregressive manner: p(R1:K) = (cid:89) k=1 p(RkR1:k1). (1) Concretely, to predict the residual map for the next scale Rk+1, the model first computes an intermediate feature representation Fk, by aggregating all previously generated residuals R1:k: Fk = (cid:88) i=1 Up(Lookup(Ri, C), (h, w)), (2) where Lookup(, C) represents retrieving vector embeddings from the learned codebook and Up(, ) denotes upsampling. This cumulative feature Fk is then downsampled to match the spatial dimensions of the next scale, (hk+1, wk+1), creating the input for the next prediction step: (cid:101)Fk = Down(Fk, (hk+1, wk+1)). The process is initiated with (cid:101)F0, start-of-sequence representation derived from conditional embedding (e.g., class labels). Once all residual maps ˆR1:K have been generated, they are used to compute the final feature map ˆF. Finally, decoder synthesizes the output image ˆI = D( ˆF). VAREdit reframes We introduce VAREdit, framework that instruction-guided image editing as conditional, multiscale prediction problem. As illustrated in Figure 2, VAREdit leverages pre-trained VAR model as its foundation and autoregressively generates the target residual maps R(tgt) conditioned on source image I(src) and textual instruction t: p(R(tgt) 1:K I(src), t) = (cid:89) k= p(R(tgt) R(tgt) 1:k1, I(src), t). (3) key design challenge is how to effectively and efficiently incorporate the source image I(src) to guide the multi-scale generation process. Vanilla Full-scale Condition straightforward approach is to condition the generation on the complete multi-scale features of the source image, F(src) 1:K . This is achieved by prepending the sequence of all source tokens to the target sequence, which allows the model to reference any scale source residual when generating the target residual. The conditional likelihood becomes: p(R(tgt) 1:K I(src), t) = p(R(tgt) 1:K F(src) 1:K , t) = (cid:89) k=1 p(R(tgt) R(tgt) 1:k1, F(src) 1:K , t). (4) While this method provides comprehensive, scale-by-scale reference for the editing task, it is computationally expensive. Doubling the sequence length results in quadratic increase in the cost of self-attention, rendering it impractical for high-resolution editing. Moreover, providing multiple source scale features may bring redundant or conflicting information for predicting single scale target features, potentially degrading editing quality. Finest-scale Condition To address the prohibitive cost of full-scale conditioning, we propose more efficient strategy to condition solely on the finest-scale source feature, F(src) . This approach is motivated by the hierarchical nature of the visual tokenizer: the finest scale encapsulates the most detailed, high-frequency information from the source image, which is often the most critical for guiding an edit. This simplification slims down the likelihood to: 1:K F(src) 1:K I(src), t) = p(R(tgt) p(R(tgt) , t) = (cid:89) k=1 p(R(tgt) R(tgt) 1:k1, F(src) , t). (5) In this way, only the tokens from F(src) are prepended to the target sequence. While this strategy dramatically reduces sequence length and thus alleviates the computational bottleneck of the vanilla Full setting, it introduces critical scale mismatch challenge. The model is tasked with predicting coarse-grained target image structure while only having access to the fine-grained, local details in the source condition, which is insufficient for precise editing. Scale Dependency Analysis The critical challenge of scale mismatch introduced by the efficient finest-scale approach raises fundamental question: which source scales are truly necessary for high-fidelity editing? To investigate Figure 2: The overall architecture of VAREdit for instruction-guided image editing. VAREdit first encodes and quantizes the images into multi-scale residuals and maps the instructions into textual token embeddings. These features are organized as the finest-scale source feature F(src) 1:K1, and then sent to the VAR Transformer. The source feature F(src) is further sent to the SAR module in the first self-attention layer to address the scale mismatch issue, while the textual token embeddings are also used for cross-attention calculations of key and value matrices. The ground truth residuals R1:K guide the training of the last output residuals ˆR1:K. During inference, the residuals ˆR1:K are predicted autoregressively, which are then cumulated and decoded to the edited image. , the pooled textual representation (cid:101)F(tgt) , the coarse-to-fine target features (cid:101)F(tgt) 0 such scale dependencies of target residuals and source residuals, we perform diagnostic analysis of the self-attention mechanisms in model trained on the full-scale source features. This full-scale setting allows the model to freely attend to all source scales. Our empirical investigation of representative example in Figure 3 reveals distinct attention patterns for different Transformer layers. In the first self-attention layer, when predicting tokens for given target scale, the attention mechanism distributes broadly, focusing heavily on the corresponding and all coarser source scales. This pattern indicates that the initial layer is responsible for establishing the global layout and long-range dependencies. This behavior shifts in deeper layers, where attention patterns become highly localized. They exhibit strong diagonal structures, suggesting that attention is primarily confined to tokens in the spatial neighborhood. This functional transition suggests shift from global structuring to local refinement, where the model copies and adjusts high-frequency details. For this latter task, the fine-grained information provided by F(src) is sufficient. This motivates us to design hybrid solution that provides the scale-aligned reference in the first layer and all subsequent layers only attend to the finest-scale source. Scale-Aligned Reference Based on our analysis, we introduce the Scale-Aligned Reference (SAR) module, designed specifically to resolve the scale mismatch problem within the first self-attention layer. The core idea is to dynamically generate coarse-scale reference features by downsampling the single, finest-scale source feature map F(src) . This creates set of reference features, each aligned to the spatial dimensions of target scale: F(ref ) = Down(F(src) , (hk, wk)). (6) During generation, when predicting the tokens for target scale k, the models first self-attention layer computes queries Q(tgt) and attends to combined set of keys and values, which are derived from two sources: (i) the newly generated, scale-aligned reference feature F(ref ) and (ii) the causal history of previously generated target tokens. Specifically, the attention output ˆO(tgt) is computed as: ˆO(tgt) = Softmax Q(tgt) (cid:104) K(ref ) , K(tgt) 1 , , K(tgt) (cid:105) (cid:104) V(ref ) , V(tgt) 1 , , V(tgt) (cid:105) , where (K(ref ) , V(ref ) source reference F(ref ) previously generated target tokens. (7) ) are projected from the scale-aligned , and (K(tgt) 1:k1) are from all 1:k1, V(tgt) Crucially, this SAR mechanism is applied only to the first self-attention layer. All subsequent layers operate using only finest-scale conditioning, attending to F(src) and the targets history residuals. This allows VAREdit to capture multi-scale dependencies while achieve the efficiency of the finest-scale approach. Figure 3: Self-attention score heatmaps of the full-scale-based transformer among different layers in one edit. The dependency patterns are different in the first self-attention layer and others, inspiring scale-aligned reference module as an improvement."
        },
        {
            "title": "Experimental Setup",
            "content": "Datasets VAREdit is trained on large-scale dataset of 3.92 million paired examples, which are aggregated from the SEED-Data-Edit (Ge et al. 2024) and ImgEdit (Ye et al. 2025) datasets. From the SEED-Data-Edit dataset, we first extracted all single-turn samples and proceeded to decompose multi-turn conversations into single-turn editing pairs. These generated pairs were then filtered using visionlanguage model (Team et al. 2025) to remove instances with poor instruction-following quality. To finalize the training data, all single-turn samples from ImgEdit were also incorporated. Additional details regarding this data processing pipeline are available in the Appendix. For evaluation, we comprehensively assess VAREdit on two established benchmarks: the EMU-Edit (Sheynin et al. 2024), which contains 3,589 examples across 8 distinct editing types, and the PIE-Bench (Ju et al. 2023), featuring 700 examples that cover 10 different editing types. Evaluation Metrics Standard benchmarks like EMU-Edit and PIE-Bench rely on CLIP-based scores (Radford et al. 2021). EMU-Edit employs caption-image similarity (CLIPOut.) and text-image directional similarity (CLIP-Dir.), whereas PIE-Bench measures similarity for the whole image (CLIP-Whole) and the edited region (CLIP-Edit). However, these metrics often fail to capture important editing quality aspects, such as spurious modifications or incomplete edits. To address these shortcomings, we also adopt the evaluation protocol from OmniEdit (Wei et al. 2025), which uses GPT4o (Hurst et al. 2024) as an automated judge to provide two key scores on scale of 0-10: GPT-Success (Suc.): Measures the adherence to the editing instruction where higher is better. GPT-Overedit (Over.): Assesses the preservation of unedited regions where higher is better. Since model could achieve perfect GPT-Over. score by simply disregarding the edit instruction and outputting the original image, we introduce GPT-Balance (Bal.), the harmonic mean of GPT-Suc. and GPT-Over. metrics. This balanced score serves as our primary metric for overall editing performance. Detailed prompts and computation methods for these metrics are provided in the Appendix. Compared Approaches To ensure comprehensive and rigorous evaluation, we assess VAREdit against several state-of-the-art tuning-based methods. Our comparative analysis includes broad spectrum of leading diffusionbased methods: InstructPix2Pix (Brooks et al. 2023), UltraEdit (Zhao et al. 2024a), OmniGen (Xiao et al. 2025), AnySD (Yu et al. 2025), ACE++ (Mao et al. 2025), and ICEdit (Zhang et al. 2025). Furthermore, we include EditAR (Mu et al. 2025), vanilla AR method for benchmarking the effectiveness of our VAR-based framework. More details of these baselines are provided in the Appendix. Implementation Details Our VAREdit models are initialized with weights from the pre-trained Infinity model (Han et al. 2025). To differentiate source image tokens from target image tokens, we introduce positional offset of = (64, 64) to the 2D Rotary Position Embeddings (2D-RoPE) for all source tokens. To investigate scaling properties, we develop two distinct model sizes: VAREdit2.2B and VAREdit-8.4B. The 2.2B model undergoes twostage training procedure: an initial 8k iterations trained at 256 256 resolution with batch size 1,536 and learning rate 6e 5, followed by 7k fine-tuning iterations at 512 512 with batch size 960 and learning rate 1.875e 5. The larger 8.4B model is trained directly at 512512 resolution for 26k iterations with batch size 1,536 and learning rate 6e5. During inference, we employ classifier-free guidance (CFG) strength of η = 4 and logits temperature of τ = 0.5. Please refer to the Appendix for more details. EMU-Edit PIE-Bench Method InstructPix2Pix UltraEdit OmniGen AnySD EditAR ACE++ ICEdit VAREdit (256px) VAREdit VAREdit Size CLIP Score Dir. Out. 1.1B 0.083 0.268 0.278 7.7B 0.095 3.8B 0.082 0.274 0.059 2.9B 0.268 0.064 0.273 0.8B 0.058 16.9B 0.258 0.095 17.0B 0.271 0.091 0.268 2.2B 0.099 0.271 2.2B 0.115 0.276 8.4B GPT Score Over. 6.299 7.704 8.829 8.590 7.260 5.979 7.591 7.058 7.055 7.519 Bal. Whole 0.236 2.923 0.247 4.541 0.240 4.674 0.227 3.129 0.242 3.305 0.234 2.076 0.234 4.785 0.256 5.565 0.260 5.662 6.773 0. CLIP Score Edit 0.217 0.220 0.212 0.206 0.216 0.205 0.215 0.224 0.225 0.224 Suc. 3.358 4.881 4.728 3.098 3.582 2.375 5.027 6.072 6.210 7.512 GPT Score Over. 6.534 8.350 9.014 7.806 8.116 8.093 7.593 7.804 8.083 7.490 Bal. 4.034 5.580 3.492 3.326 4.707 2.574 4.933 6.684 6.996 7.298 Suc. 4.794 5.831 3.444 3.456 5.070 2.743 5.321 7.234 7.530 8.156 Time 3.5s 2.6s 16.5s 3.4s 45.5s 5.7s 8.4s 0.5s 0.7s 1.2s Table 1: Quantitative results of all methods on EMU-Edit and PIE-Bench benchmarks. The best results are marked in bold. Figure 4: Fine-grained categorical GPT-Suc. and GPT-Bal. scores of all methods on EMU-Edit and PIE-Bench benchmarks. Quantitative Results The quantitative results in Table 1 demonstrate the superiority of VAREdit in both editing performance and efficiency. Editing Quality VAREdit consistently outperforms all diffusion-based and autoregressive baselines on our primary metric, GPT-Balance. Our 8.4B model achieves GPT-Bal. score of 6.77 on EMU-Edit and 7.30 on PIE-Bench, surpassing the strongest competitor (ICEdit on EMU, UltraEdit on PIE) by 41.5% and 30.8%, respectively. This underscores VAREdits ability to perform precise edits while preserving unchanged regions. Notably, some methods like OmniGen achieve higher GPT-Over. scores by being overly conservative and failing to execute the requested edit, resulting in low GPT-Suc. and consequently poor GPT-Bal. scores. In contrast, VAREdit strikes much better balance, demonstrating the strength of the VAR paradigm for precise generation. VAREdit also achieves competitive or leading scores on conventional CLIP-based metrics. Robustness Across Categories The radar charts in Figure 4 break down performance by editing types. VAREdit achieves state-of-the-art performance across the vast majority of categories. While the 2.2B model shows some limitations in challenging global style and text editing tasks, the 8.4B model substantially closes this performance gap. This illustrates the excellent scaling properties of our framework, suggesting that performance can be further enhanced by scaling to larger models and datasets. Detailed numerical results are presented in the Appendix. Inference Efficiency In addition to high editing quality, VAREdit offers substantial efficiency improvements. The 8.4B model completes 512 512 edit within 1.2 seconds, making it 2.2 faster than the similarly-sized UltraEdit (7.7B, 2.6s) and 7 faster than the larger ICEdit model (17.0B, 8.4s). This efficiency stems from the single-pass, multi-scale generative process. Moreover, the 2.2B model achieves 0.7s inference time while surpassing all baselines in editing quality."
        },
        {
            "title": "Qualitative Results",
            "content": "Figure 5 provides visual comparison that reveals the underlying reasons for VAREdits quantitative success. In the first example, diffusion-based methods tend to overedit the image and achieve lower GPT-Over. scores. For instance, InstructPix2Pix alters the color of the entire ground and ICEdit erroneously removes the pole. The vanilla AR-based EditAR fails to execute the instruction at all. While this leads to high GPT-Over. score, it receives very low GPT-Suc. score. VAREdit successfully completes the task while precisely preserving the unchanged regions, thus achieving the highest GPT-Bal. score. Similar observations hold for the subsequent examples, confirming the effectiveness of VAREdit. Figure 5: Qualitative results among the compared methods and VAREdit of three editing samples in EMU-Edit and PIE-Bench datasets. GPT scores are reported to evaluate adherence to the editing instructions. Dataset Setting EMU-Edit PIE-Bench Full Finest SAR Full Finest SAR Suc. 5.775 5.819 6.072 7.184 7.291 7.234 GPT Score Over. 5.892 6.480 7.058 7.170 7.501 7.804 Bal. 4.967 5.248 5.565 6.423 6.588 6.684 Table 2: Ablation quantitative results of VAREdit in fullscale, finest-scale and SAR-enhanced settings. Ablations and Analysis To isolate the contribution of the SAR module, we conduct an ablation study comparing three conditioning strategies: Full: Conditions on features from all source scales. Finest: Conditions on the finest-scale source features. SAR: Our proposed SAR-enhanced conditions. The results in Table 2 and Figure 6 confirm our hypothesis on 256 256 resolution 2.2B models. The Full setting yields the lowest GPT-Bal. score, which can be attributed to significantly lower GPT-Over. scores. Introducing all source scales to the conditioning creates distractions for predicting target features, thereby leading to over editing. Moreover, this setting is 60% slower than the other two variants due to longer token sequences. Compared with the Finest setting, the SAR-enhanced model achieves superior GPT-Over. scores, demonstrating the effectiveness of scale-matched information injection. The visual results further support this analysis. The Full and Finest settings introduce unintended textual detail changes and incomplete style references, flaw that our SAR-based variant avoids. Figure 6: Ablation qualitative results of VAREdit in fullscale, finest-scale and SAR-enhanced settings. Conclusion In this paper, we have proposed VAREdit, an instructionguided image editing framework following the novel nextscale prediction paradigm in visual autoregressive (VAR) modeling. VAREdit takes the instruction and quantized visual token features into VAR Transformer model to predict multi-scale residuals of the desired target image, which enjoys enhanced editing adherence and generation efficiency. We have analyzed the efficacy of different conditioning strategies and proposed novel SAR module to effectively inject scale-matched conditions into the first self-attention layer. Extensive experiments have clearly demonstrated the superior performance of VAREdit, achieving significantly higher editing precision scores and faster generation speed when compared against state-of-the-art methods. As an initial exploration, we hope this study provides valuable new insights into the future design of more effective and efficient AR-based image editing models. References Brooks, T.; et al. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1839218402. Cai, Q.; Chen, J.; Chen, Y.; Li, Y.; Long, F.; Pan, Y.; Qiu, Z.; Zhang, Y.; Gao, F.; Xu, P.; et al. 2025. HiDream-I1: HighEfficient Image Generative Foundation Model with Sparse Diffusion Transformer. arXiv preprint arXiv:2505.22705. Chang, H.; Zhang, H.; Jiang, L.; Liu, C.; and Freeman, W. T. In 2022. Maskgit: Masked generative image transformer. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1131511325. Esser, P.; et al. 2021. Taming transformers for highresolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1287312883. Feng, K.; Ma, Y.; Wang, B.; Qi, C.; Chen, H.; Chen, Q.; and Wang, Z. 2025. Dit4edit: Diffusion transformer for image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 29692977. Ge, Y.; Zhao, S.; Li, C.; Ge, Y.; and Shan, Y. 2024. Seeddata-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007. Han, J.; Liu, J.; Jiang, Y.; Yan, B.; Zhang, Y.; Yuan, Z.; Peng, B.; and Liu, X. 2025. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1573315744. Hertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch, Y.; and Cohen-Or, D. 2022. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626. Ho, J.; et al. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 68406851. Hou, C.; et al. 2024. High-fidelity diffusion-based image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 21842192. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Ju, X.; Zeng, A.; Bian, Y.; Liu, S.; and Xu, Q. 2023. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506. Kumbong, H.; Liu, X.; Lin, T.-Y.; Liu, M.-Y.; Liu, X.; Liu, Z.; Fu, D. Y.; Re, C.; and Romero, D. W. 2025. HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, 25352544. Labs, B. F. 2024. FLUX. https://github.com/black-forestlabs/flux. Li, T.; Tian, Y.; Li, H.; Deng, M.; and He, K. 2024. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445. Mao, C.; Zhang, J.; Pan, Y.; Jiang, Z.; Han, Z.; Liu, Y.; and Zhou, J. 2025. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487. Mu, J.; et al. 2025. Editar: Unified conditional generation with autoregressive models. In Proceedings of the Computer Vision and Pattern Recognition Conference, 78997909. Pang, Z.; Zhang, T.; Luan, F.; Man, Y.; Tan, H.; Zhang, K.; Freeman, W. T.; and Wang, Y.-X. 2025. Randar: Decoderonly autoregressive visual generation in random orders. In Proceedings of the Computer Vision and Pattern Recognition Conference, 4555. Peebles, W.; et al. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 41954205. Qian, Y.; Cai, Q.; Pan, Y.; Li, Y.; Yao, T.; Sun, Q.; and Mei, T. 2024. Boosting diffusion models with moving avIn Proceedings of erage sampling in frequency domain. the IEEE/CVF conference on computer vision and pattern recognition, 89118920. Qian, Y.; Cai, Q.; Pan, Y.; Yao, T.; and Mei, T. 2025. Creatively Upscaling Images with Global-Regional Priors. International Journal of Computer Vision, 119. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PmLR. Sheynin, S.; Polyak, A.; Singer, U.; Kirstain, Y.; Zohar, A.; Ashual, O.; Parikh, D.; and Taigman, Y. 2024. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 88718879. Sun, P.; Jiang, Y.; Chen, S.; Zhang, S.; Peng, B.; Luo, P.; and Yuan, Z. 2024. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525. Team, K.; Du, A.; Yin, B.; Xing, B.; Qu, B.; Wang, B.; Chen, C.; Zhang, C.; Du, C.; Wei, C.; et al. 2025. Kimi-vl technical report. arXiv preprint arXiv:2504.07491. Tian, K.; Jiang, Y.; Yuan, Z.; Peng, B.; and Wang, L. 2024. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37: 8483984865. Van Den Oord, A.; et al. 2017. Neural discrete representation learning. Advances in neural information processing systems, 30. Wan, S.; Chen, J.; Pan, Y.; Yao, T.; and Mei, T. 2025. Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On. In The Thirteenth International Conference on Learning Representations. Wang, Y.; Guo, L.; Li, Z.; Huang, J.; Wang, P.; Wen, B.; Training-Free Text-Guided Image and Wang, J. 2025. Editing with Visual Autoregressive Model. arXiv preprint arXiv:2503.23897. Wei, C.; Xiong, Z.; Ren, W.; Du, X.; Zhang, G.; and Chen, W. 2025. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations. Xiao, S.; Wang, Y.; Zhou, J.; Yuan, H.; Xing, X.; Yan, R.; Li, C.; Wang, S.; Huang, T.; and Liu, Z. 2025. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1329413304. Yao, T.; Li, Y.; Pan, Y.; Qiu, Z.; and Mei, T. 2025. Denoising token prediction in masked autoregressive models. In Proceedings of the IEEE/CVF international conference on computer vision. Ye, Y.; He, X.; Li, Z.; Lin, B.; Yuan, S.; Yan, Z.; Hou, B.; and Yuan, L. 2025. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275. Yu, J.; Xu, Y.; Koh, J. Y.; Luong, T.; Baid, G.; Wang, Z.; Vasudevan, V.; Ku, A.; Yang, Y.; Ayan, B. K.; et al. 2022. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3): 5. Yu, L.; Lezama, J.; Gundavarapu, N. B.; Versari, L.; Sohn, K.; Minnen, D.; Cheng, Y.; Birodkar, V.; Gupta, A.; Gu, X.; et al. 2023. Language Model Beats DiffusionTokenizer is Key to Visual Generation. arXiv preprint arXiv:2310.05737. Yu, Q.; Chow, W.; Yue, Z.; Pan, K.; Wu, Y.; Wan, X.; Li, J.; Tang, S.; Zhang, H.; and Zhuang, Y. 2025. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2612526135. Zhang, K.; Mo, L.; Chen, W.; Sun, H.; and Su, Y. 2023. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36: 3142831449. Zhang, Z.; Xie, J.; Lu, Y.; Yang, Z.; and Yang, Y. 2025. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690. Zhao, H.; Ma, X. S.; Chen, L.; Si, S.; Wu, R.; An, K.; Yu, P.; Zhang, M.; Li, Q.; and Chang, B. 2024a. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37: 30583093. Zhao, Y.; et al. 2024b. tion with binary spherical quantization. arXiv:2406.07548. Zheng, G.; Li, Y.; Pan, Y.; Deng, J.; Yao, T.; Zhang, Y.; and Mei, T. 2025. Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots. In Forty-second International Conference on Machine Learning. Image and video tokenizaarXiv preprint"
        },
        {
            "title": "Appendix",
            "content": "Training Dataset Processing Details Our training dataset for VAREdit is constructed from the SEED-Data-Edit (Ge et al. 2024) and ImgEdit (Ye et al. 2025) datasets, yielding final collection of 3.92 million paired samples. We first extract all single-turn samples of the two datasets and decompose multi-turn conversations in SEED-Data-Edit into single-turn editing pairs for the collection. We identify significant number of noisy samples within the initial SEED-Data-Edit, characterized by poor instruction alignment, visual blurring or abnormal distortion. To address this, we filter these low-quality samples using qualified open-source visual language model (VLM), KimiVL-A3B-Thinking (Team et al. 2025). The filtering prompt is shown in Figure 7. We discard approximately 1 million samples for which the VLM provided negative judgment, resulting in our final refined dataset. Figure 7: Prompt for filtering SEED-Edit-Data samples. Additional Details of Evaluation Metrics CLIP-based Evaluation For conventional CLIP-based metrics, we adhere to the established configurations of previous approaches. On the EMU-Edit (Sheynin et al. 2024) benchmark, we follow UltraEdit (Zhao et al. 2024a) to use openai/clip-vit-base-patch32 as the base model. On the PIE-Bench (Ju et al. 2023) benchmark, we use openai/clip-vit-large-patch14 as the base model, which is explicitly specified in the benchmark. GPT-based Evaluation Conventional metrics often fail to capture important editing quality aspects, such as spurious modifications or incomplete edits. To address this, we adopt the evaluation protocol from OmniEdit (Wei et al. 2025) to use GPT-4o (Hurst et al. 2024) as an automated judge to provide GPT-Success (Suc.) and GPT-Overedit (Over.) scores. The evaluation prompt is shown in Figure 8. We also introduce GPT-Balance (Bal.) as balanced evaluation metric for overall editing performance, which stands for the harmonic mean of GPT-Suc. and GPT-Over. metrics. The GPT-Bal. score of an editing sample is calculated as: GPT-Bal.(e) = 2 GPT-Suc.(e) GPT-Over.(e) GPT-Suc.(e) + GPT-Over.(e) , (8) and it takes zero value if either the edit is absolutely unsuccessful (GPT-Suc.(e) = 0) or severe overediting occurs (GPT-Over.(e) = 0). The final reported score for each GPT-based metric on dataset is the arithmetic mean of the scores across all samples. Figure 8: Prompt for evaluating the GPT-Suc. and GPTOver. scores of the edits. Details for Compared Approaches We assess VAREdit against several tuning-based methods: InstructPix2Pix (Brooks et al. 2023) is the seminal study that introduces the natural, humanized instructionguided image editing task. It constructs considerable paired editing samples to train an end-to-end editing model based on SD v1.5, which establishes powerful channel-wise conditional concatenation paradigm in instruction-guided image editing. UltraEdit (Zhao et al. 2024a) introduces large-scale dataset covering broader instruction types, real images and region editing. Canonical diffusion models trained on the UltraEdit set new benchmarking records. We adopt the published SD v3 variant for comparison. OmniGen (Xiao et al. 2025) is diffusion-based architecture that tokenizes texts and images into token embeddings and denoises the noise tokens to reconstruct final latent representations through Transformer-based rectified flow optimization. It supports unified image generation, including image editing, subject-driven generation and visual-conditional generation. AnySD (Yu et al. 2025) is another diffusion-based architecture integrating task-aware routing strategy with mixture-of-experts (MoE) blocks to meet task-specific editing requirements. This model is trained based on SD v1.5 with newly proposed comprehensive image editing dataset, AnyEdit. EditAR (Mu et al. 2025) is an AR-based architecture including VQ-autoencoder and token-by-token autoregressive model to predict target image tokens in latent space. To inject general visual knowledge, EditAR introduces distillation loss from vision foundation model feature encoder as regularization term, thereby improving the text-to-image alignment. ACE++ (Mao et al. 2025) is an instruction-based diffusion framework, which adapts the long-context condition unit module to scenarios of multiple conditions. This model first experiences zero-reference training stage and then an N-reference training stage to enable support for general instructions across various tasks. ICEdit (Zhang et al. 2025) leverages large-scale Diffusion Transformers to construct an in-context editing framework. It integrates LoRA-MoE hybrid tuning strategy to enhance flexible adaptation for specialized condition feature processing, while VLM-based early filtering module is utilized to identify good samples at the very first denoising steps. All the compared approaches are implemented using the officially released open-source codes with their published default parameter settings. Additional Implementation Details Our VAREdit models are built on foundational VAR-based model, Infinity (Han et al. 2025). We initialize our models with the released pre-trained 2.2B and 8.4B checkpoints for full fine-tuning. For visual tokenization, we use the pretrained multi-scale tokenizers integrating binary spherical quantization (Zhao et al. 2024b), and the codebook dimension is 32 and 56 for the 2.2B and 8.4B models, respectively. During the training stage of 256 256 resolution, all the source and target images will be tokenized into 7 scales with the following list of grid sizes (hk, wk): (1, 1), (2, 2), (4, 4), (6, 6), (8, 8), (12, 12), (16, 16). (9) As for the training of 512 512 resolution, there will be 10 scales, consisting of the above 7 listed scales and an additional 3 larger shapes: (20, 20), (24, 24), (32, 32). In particular, we only extract the finest-scale tokens of the source images in the finest-scale conditioning strategy. We employ 2D-RoPE to encode the spatial information of all input tokens. For target token at position (i, j) in scale k, its 2DRoPE coordinate (x, y) is calculated as: (x, y) = (i h/hk, w/wk) , (10) while the source token is treated as spatially separate one with an offset = (64, 64), indicating that its 2D-RoPE coordinate starts from (64, 64): (x, y) = (i h/hk + 64, w/wk + 64) . (11) Additionally, we also use the pre-trained scale positional embeddings to mitigate inter-scale confusion. Numerical Results for Categorical Evaluation Table 3, 4, 5, 6 present the detailed numerical results of the fine-grained categorical scores shown in radar charts of the main context. It can be observed that VAREdit-2.2B outperforms the compared approaches across the vast majority of editing types, such as addition, removal, coloring and material transfer. VAREdit-8.4B enjoys further enhanced performance in most categories. According to these tables, the 2.2B model shows some limitations in challenging scenarios, including global editing and text editing. Through scaling up the model size and training iterations, the 8.4B model substantially mitigates or even closes these gaps, especially in text editing that achieves 215%+ improvements over the 2.2B variant. These findings strongly demonstrate that our VAREdit is not only effective but also highly scalable. As result, our VAREdit models exhibit high robustness across various editing types to achieve precise editing results. Method InstructPix2Pix UltraEdit OmniGen AnySD EditAR ACE++ ICEdit VAREdit-2.2B VAREdit-8.4B Overall 3.358 4.881 4.728 3.098 3.582 2.375 5.027 6.210 7.512 Global 3.950 6.178 2.653 1.379 4.717 3.717 3.548 5.475 6.069 Add 4.034 6.084 4.355 3.752 3.797 2.694 5.931 6.865 8.625 Text 0.330 1.985 4.236 1.899 0.472 2.288 4.751 2.708 6. Back. 4.043 5.625 2.542 5.501 3.488 2.871 2.635 6.724 7.491 Color 5.270 5.728 6.318 4.289 5.678 2.832 6.935 7.854 8.788 Style 4.634 6.889 5.560 0.744 6.571 3.438 5.138 6.740 7.562 Remove 1.637 2.328 6.435 3.684 1.501 0.535 4.793 6.889 6.881 Local 3.897 5.753 3.922 2.789 3.697 1.650 4.955 6.314 7.608 Table 3: Numerical fine-grained categorical GPT-Suc. scores of all methods on EMU-Edit. Among column names, Back. stands for Background. The best group results are marked in bold, while the second-best group results are underlined. Method InstructPix2Pix UltraEdit OmniGen AnySD EditAR ACE++ ICEdit VAREdit-2.2B VAREdit-8.4B Overall 2.923 4.541 4.674 3.129 3.305 2.076 4.785 5.662 6.773 Global 3.934 6.020 2.877 1.630 4.753 3.689 3.603 4.279 4.766 Add 3.477 5.695 4.365 3.893 3.622 2.070 5.673 6.775 8.173 Text 0.326 2.071 4.334 2.124 0.472 2.021 4.732 2.887 6.234 Back. 2.607 5.256 2.420 5.072 3.321 1.660 2.651 4.882 5. Color 4.740 5.221 6.068 4.276 4.392 2.422 6.301 7.303 8.162 Style 4.514 6.346 5.594 1.012 6.648 3.899 4.850 5.661 6.626 Remove 1.469 2.159 6.147 3.497 1.381 0.510 4.427 6.716 6.499 Local 3.204 5.079 3.942 2.818 3.364 1.392 4.756 5.821 7.025 Table 4: Numerical fine-grained categorical GPT-Bal. scores of all methods on EMU-Edit. Among column names, Back. stands for Background. The best group results are marked in bold, while the second-best group results are underlined. Method InstructPix2Pix UltraEdit OmniGen AnySD EditAR ACE++ ICEdit VAREdit-2.2B VAREdit-8.4B Overall Random Modify Add Remove Attr. 3.425 4.794 5.200 5.831 2.850 3.444 1.725 3.456 4.300 5.070 1.650 2.743 3.400 5.321 6.225 7.530 6.300 8.156 5.163 7.200 3.163 4.388 4.000 3.875 6.613 7.463 8.838 5.913 6.588 3.613 5.438 5.263 0.888 5.150 8.725 8.950 2.313 2.275 2.875 2.988 0.900 0.150 4.388 7.300 8.000 4.707 6.336 3.836 4.114 5.836 2.514 5.900 7.743 8.093 Post Color Mater. Back. 5.275 2.950 6.200 3.300 3.275 2.550 3.113 1.850 6.363 3.225 4.225 1.400 4.325 2.650 7.950 4.450 8.400 6. 7.075 6.000 5.500 3.525 7.600 4.750 8.150 8.425 9.100 6.175 6.750 3.000 3.725 6.525 3.600 5.350 7.375 8.075 Style 5.238 7.050 3.550 1.700 6.800 4.763 5.988 7.663 8.050 Table 5: Numerical fine-grained categorical GPT-Suc. scores of all methods on PIE-Bench. Among column names, Attr., Mater., Back. stands for Attribute, Material, Background, respectively. The best group results are marked in bold, while the second-best group results are underlined. Method InstructPix2Pix UltraEdit OmniGen AnySD EditAR ACE++ ICEdit VAREdit-2.2B VAREdit-8.4B Overall Random Modify Add Remove Attr. 2.767 4.034 5.066 5.580 2.766 3.492 1.814 3.326 4.279 4.707 1.586 2.574 3.289 4.933 5.717 6.996 5.977 7. 1.861 2.136 2.631 2.703 0.908 0.151 4.056 6.942 7.050 4.711 6.775 3.627 4.183 3.966 3.192 5.756 7.112 8.065 4.251 6.056 3.981 4.022 5.312 2.458 5.539 7.330 7.455 4.328 6.369 3.593 4.923 4.540 0.853 4.961 7.761 7.637 Post Color Mater. Back. 4.174 2.016 5.807 3.256 3.259 2.739 2.892 1.805 5.725 3.193 3.972 1.341 3.814 2.645 6.950 4.291 7.044 6.176 6.068 5.898 5.407 3.695 6.527 4.067 7.866 8.032 8. 5.147 6.394 3.144 3.752 5.902 3.047 4.750 6.502 6.855 Style 4.781 6.830 3.807 1.832 6.800 5.032 5.611 7.348 7.191 Table 6: Numerical fine-grained categorical GPT-Bal. scores of all methods on PIE-Bench. Among column names, Attr., Mater., Back. stands for Attribute, Material, Background, respectively. The best group results are marked in bold, while the second-best group results are underlined."
        }
    ],
    "affiliations": [
        "HiDream.ai Inc.",
        "University of Science and Technology of China, Hefei, China"
    ]
}