{
    "paper_title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "authors": [
        "Guojun Xiong",
        "Zhiyang Deng",
        "Keyi Wang",
        "Yupeng Cao",
        "Haohang Li",
        "Yangyang Yu",
        "Xueqing Peng",
        "Mingquan Lin",
        "Kaleb E Smith",
        "Xiao-Yang Liu",
        "Jimin Huang",
        "Sophia Ananiadou",
        "Qianqian Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements."
        },
        {
            "title": "Start",
            "content": "FLAG-TRADER: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading Guojun Xiong1, Zhiyang Deng2, Keyi Wang3, Yupeng Cao2, Haohang Li2, Yangyang Yu2, Xueqing Peng7, Mingquan Lin4, Kaleb Smith5, Xiao-Yang Liu Yanglet3,6, Jimin Huang7, Sophia Ananiadou8, Qianqian Xie7,* 1Harvard University, 2Stevens Institute of Technology, 3Columbia University, 4University of Minnesota, 5NVIDIA, 6Rensselaer Polytechnic Institute, 7TheFinAI, 8University of Manchester *Correspondence: xqq.sincere@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-TRADER, unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which partially finetuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financialdomain tasks. We present extensive empirical evidence to validate these enhancements."
        },
        {
            "title": "Introduction",
            "content": "Algorithmic financial trading represents critically complex decision-making domain that perpetually grapples with the intertwined challenges of synthesizing heterogeneous market signals and dynamically refining strategies (Hambly et al., 2023; Yu et al., 2024b; Li et al., 2023). Traditional reinforcement learning (RL) approaches, despite their theoretical grounding in Markov Decision Processes (MDPs), confront three fundamental limitations when deployed in financial markets. Firstly, their inability to coherently model multimodal market statesspanning sequential price movements, quantitative technical indicators, and unstructured textual sentimentscompromises data integration (Zhang et al., 2019; Nassirtoussi et al., 2014). Secondly, non-stationary data distributions inherent to financial systems systematically erode strategy generalizability across market regimes (Zhang et al., 2019). Thirdly, the heavy reliance on manually crafted technical indicators (e.g., MACD, RSI) and complex feature engineering (Liang et al., 2018) introduces subjective biases, leads to information loss, and reduces the robustness of real-time decision-making, especially in volatile market conditions. The emergence of Large Language Models (LLMs) offer significant potential for financial decision-making by addressing key limitations of RL-based trading strategies. Leveraging their transformer architecture, they serve as multimodal feature extractors, integrating time-series and textual data, capturing long-range dependencies, and generalizing across market regimes, while also extracting nuanced sentiment signals without relying on manually crafted features (Chen et al., 2021; Yang et al., 2023a; Jin et al., 2023; Wood et al., 2021; Yu et al., 2024a; Deng et al., 2023). Nonetheless, adapting LLMs for trading presents key challenges. First, their deployment often relies on agentic frameworks (Li et al., 2024b, 2023; Yu et al., 2025), which incur high implementation and operational costs due to their complex architecture. Second, LLMs are primarily trained for static text generation, making them ill-suited for sequential decision-making in trading. This prompts us to the following question: Can we design framework that seamlessly integrates LLMs reasoning with RLs reward-driven optimization to tackle the challenges of financial sequential decisionmaking? To resolve these interconnected challenges, we FLAG-TRADER, unified architecture integrating linguistic processing (via LLMs) with gradientdriven RL policy optimization, as shown in Figure 1. This framework advances two synergistic innovations: parameter-efficient fine-tuning module that jointly encodes temporal market data and tex5 2 0 F 8 1 ] . [ 2 3 3 4 1 1 . 2 0 5 2 : r Figure 1: high-level overview of our LLM-based reinforcement learning setup for financial trading. The environment provides the current state st. prompt containing task details, the action space, and the current state is fed into the LLM, which outputs trading action at. The action is executed in the environment, yielding reward (atlang(st)) is then leveraged by policy gradient method r(st, at) and next state st+1. The log-likelihood logπθ (e.g., PPO), with experience tuples stored in replay buffer for iterative updates. tual streams into unified state representations and hybrid RL component that explicitly incorporates external environment reward gradients into policy updates, ensuring alignment with trading performance metrics. Our contributions are summarized as follows. First, we propose the FLAG-TRADER framework, where partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. The model processes market data using textual state representation, enabling it to interpret and respond to market conditions effectively. Rather than fine-tuning the entire LLM, only subset of its parameters is updated, balancing domain adaptation and knowledge retention. This design allows FLAG-TRADER to make informed trading decisions while remaining computationally efficient and preserving the LLMs general reasoning capabilities. Second, we conduct extensive experiments to evaluate FLAG-TRADER across multiple financial trading tasks. Our results demonstrate that FLAGTRADER consistently outperforms both the buyand-hold strategy and LLM-agentic baselines, particularly in terms of cumulative return and Sharpe ratio, which we prioritize for financial performance assessment. Notably, our approach enables smallscale (135M parameter) open-source LLM to surpass much larger proprietary models, highlighting the effectiveness of RL fine-tuning in optimizing LLM-driven trading strategies. These findings underscore the potential of integrating LLMs with RL to enhance financial decision-making while maintaining computational efficiency."
        },
        {
            "title": "2 Related Work",
            "content": "RL in Finance. RL has shown promise for financial decision-making, spanning Q-learning approaches for Sharpe ratio maximization (Gao and Chan, 2000), dynamic asset allocation (Jangmin et al., 2006), deep Q-learning (Jeong and Kim, 2019), tabular SARSA (de Oliveira et al., 2020), policy-based portfolio optimization (Shi et al., 2019), and actor-critic methods (Liu et al., 2018; Ye et al., 2020) enhanced by adversarial training (Liang et al., 2018) and transformer-based architectures (Huang et al., 2024). Recent research efforts in RL for financial applications have been greatly aided by open-source frameworks like FinRL (Liu et al., 2021, 2022), which standardize implementations and provide reproducible benchmarks. Comprehensive surveys (Hambly et al., 2023; Sun et al., 2023) further showcase advances in both methodological rigor and real-world deployment. Despite these advances, RL-based trading still requires large training data, struggles with non-stationary markets, and faces challenges incorporating multimodal information in real time. LLMs in Finance. growing trend is the integration of LLMs into financial decision-making. Hybrid systems like FinCon (Yu et al., 2025) and TradingGPT (Li et al., 2023) leverage language understanding to enhance trading agents, while domain-specific models such as FINBERT (Araci, 2019; Yang et al., 2020) and FLANG (Shah et al., 2022) have excelled at financial text tasks through specialized pre-training. Recent efforts include machine reading comprehension (Zhang and Zhang, 2023), open-source financial LLMs (Liu et al., 2023), BloombergGPT with domainadapted tokenization (Wu et al., 2023), and InvestLM (Yang et al., 2023b) featuring numerical reasoningachieving strong results in sentiment analysis (Huang et al., 2023), earnings call interpretation (Xie et al., 2023), and regulatory document processing. However, LLM-based methods often lack sequential decision-making mechanisms, are computationally expensive (especially with RL), and struggle with non-stationary market conditions. LLM Agents for Sequential Decision Making. The integration of LLMs with agentic frameworks has opened new avenues for financial decisionmaking. For instance, FINMEM (Yu et al., 2024a) introduced memory-augmented LLM agents for portfolio management, FINAGENT (Zhang et al., 2024) leveraged hierarchical structures in highfrequency trading, and multi-agent systems like FINROBOT (Yang et al., 2024) and FINCON (Yu et al., 2024b) emphasize contextual adaptation and collaboration. Meanwhile, fine-tuning LLMs and vision-language models (VLMs) with reinforcement learning has proven effective in complex tasks: LLaRP (Szot et al., 2023) positions LLMs as generalizable policies for embodied tasks, and RL-tuned VLMs (Zhai et al., 2024) enhance multistep decision-making. However, LLMs remain computationally expensive for real-time deployment, and risk-sensitive trading demands robustness to non-stationary markets, calling for careful model complexity and balanced explorationexploitation.Rwork onsadvancedhighlight"
        },
        {
            "title": "3 Problem Statement",
            "content": "We define the financial decision-making process as finite horizon partially observable Markov decision process (MDP) with time index {0, , }, represented by the tuple: = (S, A, , R, γ), where each component is described in detail below. State. The state space = consists of two components: market observations and trading account balance, i.e., st = (mt, bt) S. Specifically, mt = (Pt, Nt) represents the market observation process, includingstock price Pt at time t, and financial news sentiment or macroeconomic indicators Nt; bt = (Ct, Ht) represents the trading account balance, including available cash Ct at time t, and number of stock shares Ht. Action. The agent chooses from discrete set of trading actions = {Sell : 1, Hold : 0, Buy : 1}, where at = 1 denotes selling all holdings (liquidate the portfolio), at = 0 denotes holding (no trading action), and at = 1 represents buying with all available cash (convert all cash into stocks). State Transition. The state transition dynamics are governed by stochastic process st+1 (st, at). The trading account evolves according to the following equations: If Sell: Ct+1 = Ct + HtPt+1, Ht+1 = 0. If Hold: Ct+1 = Ct, Ht+1 = Ht. If Buy: Ct+1 = 0, Ht+1 = Ht + Ct Pt+1 . Reward. The agent receives reward based on the daily trading profit & loss (PnLs): R(st, at) = SRt SRt1, where SRt denotes the Sharpe ratio at day t, computed by using the historical PnL from time 0 to time t. Moreover, PnL at time is calculated as pnlt := (Ct Ct1) + (HtPt Ht1Pt1). Then, the Sharpe ratio SRt at time can be calculated as: SRt := E[pnl1, , pnlt] rf σ[pnl1, , pnlt] , (1) where E[pnl1, , pnlt] is the sample average of daily PnL up to time t, rf is the risk-free rate, and σ[pnl1, , pnlt] is the sample standard deviation of daily PnL up to time t. The goal is to find an admissible policy π to maximize the expected value of cumulative discounted reward, i.e., π(s) = max π s0=s,atπ(st) st+1T (st,at) (cid:35) γtRt , (2) (cid:34) (cid:88) t=0 where Rt is shortened version R(st, at) and γ (0, 1] is the discount factor controlling the importance of future rewards."
        },
        {
            "title": "Our goal is to train an LLM agent parameterized",
            "content": "by θ to find the optimized policy πθ for (2), i.e., at πθ(st) = LLM(lang(st); θ), (3) where lang(st) are the prompts generated by converting state st into structured text. The proposed pipeline is illustrated in Figure 1. Figure 2: The FLAG-TRADER pipeline for financial trading, utilizing an LLM-based actor-critic architecture. The LLM consists of frozen base layers θfrozen that retain pre-trained knowledge and trainable top layers θtrain for financial decision-making. Both the POLICY_NET and VALUE_NET share these trainable layers while maintaining separate policy head θP and value head θV , which are updated by policy gradient method."
        },
        {
            "title": "4 FLAG-TRADER",
            "content": "To tackle the challenge of directly fine-tuning an LLM for both alignment and decision-making, we introduce FLAG-TRADER, fused LLM-agent and RL framework for financial stock trading. In FLAG-TRADER, partially fine-tuned LLM serves as the policy network, leveraging its pretrained knowledge while adapting to the financial domain through parameter-efficient fine-tuning, as shown in Figure 2. The model processes financial information using textual state representation, allowing it to interpret and respond to market conditions effectively. Instead of fine-tuning the entire network, only subset of the LLMs parameters is trained, striking balance between adaptation and knowledge retention. In the following, we will present the prompt input design and the detailed architecture of FLAG-TRADER."
        },
        {
            "title": "4.1 Prompt Input Design",
            "content": "The first stage of the pipeline involves designing robust and informative prompt, denoted as lang(st), which is constructed based on the current state st to guide the LLM in making effective trading decisions. The prompt is carefully structured to encapsulate essential elements that provide context and ensure coherent, actionable outputs. It consists of four key components: task description, which defines the financial trading objective, outlining the problem domain and expected actions; legible action space, specifying the available trading decisions (Sell, Hold, Buy); current state representation, incorporating market indicators, historical price data, and portfolio status to contextualize the decision-making process; and an output action, which generates an executable trading decision. This structured prompt ensures that the LLM receives comprehensive input, enabling it to produce well-informed and actionable trading strategies, as illustrated in Figure 3. Figure 3: The format of input prompt. It contains the task description, the legible action set, the current state description, and the output action format."
        },
        {
            "title": "4.2 FLAG-TRADER Architecture",
            "content": "To incorporate parameter-efficient fine-tuning into the policy gradient framework, we partition the intrinsic parameters of the LLM into two distinct components: the frozen parameters inherited from pretraining, denoted as θforzen, and the trainable parameters, denoted as θtrain. This separation allows the model to retain general language understanding while adapting to financial decision-making with minimal computational overhead. Building upon this LLM structure, we introduce policy network and value network, both of which leverage the trainable top layers of the LLM for domain adaptation while sharing the frozen layers for knowledge retention. The overall architecture is illustrated in Figure 2."
        },
        {
            "title": "4.2.1 Policy Network Design\nThe policy network is responsible for generating an\noptimal action distribution over the trading decision\nspace A, conditioned on the observed market state.\nIt consists of three main components:",
            "content": "State Encoding. To effectively process financial data using the LLM, the numerical market state is first converted into structured text using predefined template1 lang(s) = \"Price: $p, Vol: v, RSI: r,...\". (4) This transformation enables the model to leverage the LLMs textual reasoning capabilities, allowing it to understand and infer trading decisions in structured, language-based manner. LLM Processing. The tokenized text representation of the state is then passed through the LLM backbone, which consists of: 1) Frozen layers (preserve general knowledge): Token embeddings = Embed(lang(s)) pass through LLM frozen layers, i.e., h(1) = LLM1:N (E; θfrozen). (5) These layers preserve general knowledge acquired from pretraining, ensuring that the model maintains strong foundational understanding of language and reasoning. 2) Trainable layers (domainspecific adaptation): The output from the frozen layers is then passed through the trainable layers, which are fine-tuned specifically for financial decision-making, i.e., h(2) = LLMN +1:N +M (h(1); θtrain). (6) This structure enables efficient adaptation to the financial domain without modifying the entire LLM, significantly reducing training cost while maintaining performance. Policy Head. Finally, the processed representation is fed into the policy head, which outputs probability distribution over the available trading actions according to logits = POLICY_NET(h(2), θP ) RA, (7) where θP is the parameter of POLICY_NET, with action masking for invalid trades: (cid:40) 0 π(as) = (cid:80) exp(logits(a)) aA exp(logits(a)) / A, otherwise. (8) 1To simplify notation, we use lang(st) to represent both the state encoding and the prompt, acknowledging this slight abuse of notation for convenience. This ensures that actions outside the valid set (e.g., selling when no stocks are held) have zero probability, preventing invalid execution."
        },
        {
            "title": "4.2.2 Value Network Design\nThe value network serves as the critic in the RL\nframework, estimating the expected return of a\ngiven state to guide the policy network’s optimiza-\ntion. To efficiently leverage the shared LLM repre-\nsentation, the value network shares the same back-\nbone as the policy network, processing the tex-\ntual state representation through the frozen and\ntrainable layers (4)–(6). This design ensures ef-\nficient parameter utilization while maintaining a\nstructured and informative state encoding. After\npassing through the LLM processing layers, the\noutput h(2) is fed into a separate value prediction\nhead, which maps the extracted features to a scalar\nvalue estimation:",
            "content": "V (s) = VALUE_NET(h(2), θV ) R1, (9) where θV is the parameter of VALUE_NET."
        },
        {
            "title": "4.3 Online Policy Gradient Learning",
            "content": "The policy and value networks in FLAG-TRADER are trained using an online policy gradient approach, ensuring that the model continuously refines its decision-making ability. The learning process follows an iterative cycle of state observation, action generation, reward evaluation, and policy optimization. The parameters of the model are updated using stochastic gradient descent (SGD), leveraging the computed policy and value losses to drive optimization. At each training step, we define two key loss functions, i.e., policy loss LP : measures how well the policy network aligns with the expected advantage-weighted log probability of actions; value loss LV : ensures that the value network accurately estimates the expected return. Remark 4.1. The definitions of policy loss and value loss may vary across different actor-critic (AC) algorithms. Here, we present general formulation for clarity and ease of expression. Notably, our framework is designed to be flexible and adaptable, making it compatible with wide range of AC algorithms. Based on these loss functions, the model updates the respective network parameters using backpropagation as follows. Update Policy Head. The policy network parameters θP are updated via SGD to minimize the policy loss LP Algorithm 1 FLAG-TRADER θP θP ηθP LP , (10) where η is the learning rate for updating policy head θP . Update Value Head. The value network parameters θV are optimized via SGD to minimize the temporal difference (TD) error over policy loss LV θV θV ηθV LV . (11) Update Trainable LLM Layers. The trainable LLM parameters θtrain are updated via SGD jointly based on both the policy and value losses, i.e., LP and LV , allowing the shared LLM representation to align with optimal decision-making: θtrain θtrain βθtrain(LP + LV ), (12) where β is the learning rate for LLM parameter θtrain. The updates in (10)(12) are performed iteratively until the stopping criteria are met, as outlined in Algorithm 1. This iterative learning process effectively balances exploration and exploitation, enhancing policy performance while maintaining stability. To mitigate overfitting and policy divergence, we employ Proximal Policy Optimization (PPO), which constrains updates by limiting the divergence from previous policies, ensuring more controlled and reliable learning. The detailed procedure of how to compute policy loss LP and value loss LP can be found in Appendix A."
        },
        {
            "title": "5 Experiments",
            "content": "This section describes the overall experimental design and environmental setup for comparing the performance of different trading agents under consistent conditions."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "For our single-asset trading tasks, we adopt two baselines: the buy-and-hold strategy and the LLMbased trading agent from INVESTORBENCH (Li et al., 2024a), which integrates 13 proprietary or open-source large language models. Our proposed model, FLAG-TRADER (built on 135Mparameter LLM), is then evaluated against these baselines for comprehensive performance comparison. 1: Require: Pre-trained LLM with parameter θ := (θfrozen, θtrain), environment dynamics , reward function R; 2: Initialize policy network θP and value network θV with shared LLM trainable layers θtrain; 3: Initialize experience replay buffer 4: for iteration = 1, 2, . . ., do 5: Fetch the current state st from the environment and construct an input prompt lang(st); Pass prompt lang(st) through LLM; POLICY_NET outputs at from action space {buy, sell, hold} based on (8); Execute action at in the environment and observe reward r(st, at) and transition to new state st+1; Store experience tuple (st, at, rt, st+1) in replay buffer B; if mod τ = 0 then Update policy head θP according to (10); Update value head θV according to (11); Update the trainable LLM layers θtrain according to (12). 6: 7: 8: 9: 10: 11: 12: 13: end if 14: 15: end for 16: Return: Fine-tuned POLICY_NET(θP ). (JNJ), UVV Corporation (UVV), Honeywell International Inc. (HON), Tesla, Inc. (TSLA) and Bitcoin (BTC). As summarized in Table 1, each agents performance is measured across these assets. All language models use temperature of 0.6 during inference to balance consistency and creativity in their responses. We report four metrics-Composite Return (CR), Sharpe Ratio (SR), Annualized Volatility (AV), and Maximum Drawdown (MDD) and select final results from the test trajectory corresponding to the median of these metrics. If the median values arise from different epochs, we prioritize the run producing the median SR. Due to varying data availability, warm-up and test periods may differ. For the trading tasks of five stocks, the warm-up period is July 1, 2020, to September 30, 2020, and the test period is October 1, 2020, to May 6, 2021. On the other hand, the warm-up period of BTC trading is from 2023-02-11 to 2023-04-04 and the test period is from 2023-04-05 to 2023-11-05. We focus on five stocks and one crypto: Microsoft Corporation (MSFT), Johnson & Johnson We deploy LLMs using the vllm framework, with configurations depending on model size. Table 1: Performance of stock trading with different LLMs as backbone model across seven stocks. Model MSFT JNJ UVV Buy & Hold 15.340 1.039 24.980 9.428 13. 1.343 17.500 9.847 36.583 2.112 29. 15.406 CR SR AV MDD CR SR AV MDD CR SR AV MDD Palmyra-Fin-70B 14. 0.897 27.518 Financial Domain Models 0.450 5.748 9.428 19. 9.367 37.875 2.039 31.200 15.967 GPT-o1-preview GPT-4 GPT-4o 17.184 16.654 12.461 0.962 0.932 0.924 30.000 30.022 22.653 9.428 9.428 6.647 13.561 13.712 9.099 1.086 1.103 0. 20.864 20.894 17.471 9.847 9.860 7.169 41.508 31.791 8.043 2.147 1.640 0.496 32.479 32.567 27.241 9.633 10.434 14. Proprietary Models Qwen2.5-72B-Instruct Llama-3.1-70B-Instruct DeepSeek-67B-Chat Yi-1.5-34B-Chat Qwen2.5-32B-Instruct DeepSeek-V2-Lite (15.7B) Yi-1.5-9B-Chat Llama-3.1-8B-Instruct Qwen-2.5-Instruct-7B 7.421 17.396 13.941 22.093 -0.557 11.904 19.333 22.703 -10.305 0.588 1.335 0.834 1.253 -0.041 0.694 1.094 1.322 -0.724 21.238 21.892 28.081 29.613 22.893 28.796 29.690 28.855 23.937 Open-Source Models 6.973 7.045 7.850 9.428 8.946 16.094 9.428 7.385 23. 14.353 13.868 14.426 14.004 2.905 -7.482 18.606 13.988 21.852 1.140 1.121 1.185 1.180 0.292 -0.670 1.611 1.486 0.980 20.995 20.779 20.450 19.938 16.725 18.773 19.409 20.460 37.425 9.812 9.825 9.825 9.847 7.169 17.806 10.986 9.969 9.573 37.178 35.981 29.940 20.889 -1.623 33.560 49.415 41.108 11.752 1.822 1.728 1.481 1.020 -0.097 1.703 2.410 1.981 0. 34.223 34.986 33.964 34.417 27.973 33.099 34.446 34.866 22.988 13.365 15.406 15.407 14.936 17.986 12.984 11.430 16.429 15.451 FLAG-TRADER SmolLM2-135M-Instruct 20.106 1. 24.932 9.428 33.724 3.344 17.174 9. 46.799 1.463 67.758 35.039 1 The Buy & Hold strategy is passive investment approach commonly used as baseline strategy, where an investor purchases stocks and holds onto them for an extended period regardless of market fluctuations. 2 An upward arrow () next to metric indicates that higher values signify better performance, while downward arrow () indicates that lower values are preferable. 3 The numbers highlighted in red indicate the best-performing outcomes for the corresponding metrics. Small-scale models (under 10B parameters) run on two RTX A6000 GPUs (48GB each), mid-scale models (10B65B parameters) require four RTX A6000 GPUs, and large-scale models (over 65B parameters) use eight A100 GPUs (80GB each). These setups provide sufficient resources for both inference and training, enabling fair comparison of trading performance across different assets. FLAG-TRADER is trained by using PPO algorithm, which is detailed in Algorithm 2 in Appendix A."
        },
        {
            "title": "5.2 Evaluation Metrics",
            "content": "We use four widely recognized financial metrics (Hull, 2007) to evaluate and compare the investment performance of various LLM backbones across different tasks: Cumulative Return (CR), Sharpe Ratio (SR), Annualized Volatility (AV), and Maximum Drawdown (MDD). As CR and SR focus more on long-term gains and risk-adjusted returns, they are typically considered more important than AV and MDD for assessing asset trading performance. Accordingly, we treat CR and SR as our primary metrics for the final evaluation. Cumulative Return (CR) % measures the total value change of an investment over time by summing logarithmic return calculated from daily PnL: CR = (cid:88) t=1 log (1 + pnlt Ct1 + Ht1Pt where pnlt is the PnL at time t, and Ct1 + Ht1Pt1 is the account balance at time 1. Notice that higher values indicate better strategy effectiveness. Sharpe Ratio (SR) assesses risk-adjusted returns by dividing the average excess return (Rp) over the risk-free rate (rf ) by its volatility (σp): SR = Rp rf σp (14) Note that higher ratios signify better performance. Annualized Volatility (AV) % and Daily Volatility (DV) % quantify return fluctuations; AV is derived by scaling DV (standard deviation of daily logarithmic returns) by the square root of the annual trading days (252): AV = DV 252. (15) This metric highlights potential return deviations across the year. Max Drawdown (MDD) % calculates the largest drop from peak to trough of the value of balance account: MDD = max( Ppeak Ptrough Ppeak ). (16) Notice that lower values indicate lesser risk and higher strategy robustness. ), (13)"
        },
        {
            "title": "5.3 Experimental Results",
            "content": "FLAG-Trader achieves superior stock trading performance. Unlike the baseline agent, which relies on an LLM-agentic framework, Table 2: Performance of stock trading with different LLMs as backbone model across seven stocks. Model HON TSLA BTC CR SR AV MDD CR SR AV MDD CR SR AV MDD Buy & Hold 33.256 2. 23.967 9.195 39.244 0.869 75.854 37. 21.821 0.683 37.426 20.796 Palmyra-Fin-70B 20. 1.464 22.974 Financial Domain Models 6.824 -0.222 -6.661 50.379 25.820 -20. -1.212 20.036 27.782 GPT-o1-preview GPT-4 GPT-4o 13.162 34.342 38.540 0.776 2.005 2. 28.511 28.779 26.782 Qwen2.5-72B-Instruct Llama-3.1-70B-Instruct DeepSeek-67B-Chat Yi-1.5-34B-Chat Qwen2.5-32B-Instruct DeepSeek-V2-Lite (15.7B) Yi-1.5-9B-Chat Llama-3.1-8B-Instruct Qwen-2.5-Instruct-7B 34.309 43.944 32.536 30.743 26.332 16.686 29.028 39.079 4.291 2.000 2.646 1.909 1.823 1.980 0.974 1.700 2.320 0.285 28.779 27.903 28.628 28.335 22.348 28.771 28.682 28.299 24.933 SmolLM2-135M-Instruct 34.342 2.429 23.913 Proprietary Models 34.499 45.246 45.946 0.796 1.190 1.348 11.558 9.195 8. Open-Source Models 39.112 37.545 35.647 35.364 21.336 31.458 31.350 35.622 41.203 9.292 8.993 10.782 9.195 5.261 16.806 12.588 10.341 14.156 1.075 0.891 0.885 0.808 0.729 0.744 0.703 0.832 0.925 FLAG-TRADER 50.394 1.362 10. 72.822 63.896 57.281 35.490 25.031 21.631 34.060 22.396 14.330 1.114 0.828 0.532 35.846 31.699 31.304 17.075 17.206 17. 61.136 70.815 67.660 73.561 49.157 68.524 74.895 71.936 74.862 26.985 29.813 33.359 35.490 20.704 35.404 37.975 36.383 37.975 0.549 20.440 28.307 13.620 11.566 4.804 7.953 20.521 19.477 0.325 0.758 0.891 0.434 0.869 0.153 0.253 0.646 0.612 1.979 31.604 37.219 36.778 15.608 36.846 36.799 37.240 37.289 0.897 17.813 17.944 22.790 7.984 20.562 26.545 21.104 20. 64.004 37.975 45.511 1.734 30.903 24. 1 The Buy & Hold strategy is passive investment approach commonly used as baseline strategy, where an investor purchases stocks and holds onto them for an extended period regardless of market fluctuations. 2 An upward arrow () next to metric indicates that higher values signify better performance, while downward arrow () indicates that lower values are preferable. 3 The numbers highlighted in red indicate the best-performing outcomes for the corresponding metrics. training strategy can bridge or even surpass the performance gap typically associated with model scale."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced FLAG-TRADER, novel framework that integrates LLMs with RL for financial trading. In particular, FLAGTRADER leverages LLMs as policy networks, language-driven decisionallowing for natural making while benefiting from reward-driven optimization through RL fine-tuning. Our framework enables small-scale LLMs to surpass larger proprietary models by efficiently adapting to market conditions via structured reinforcement learning approach. Through extensive experiments across multiple stock trading scenarios, we demonstrated that FLAG-TRADER consistently outperforms baseline methods, including LLM-agentic frameworks and conventional RL-based trading agents. These results highlight the potential of integrating LLMs with RL to achieve adaptability in financial decision-making. FLAG-TRADER undergoes an RL-based posttraining phase. As shown in Table 1 and Table 2, FLAG-TRADER consistently outperforms the baseline across multiple metrics, demonstrating its stronger adaptability and optimization in diverse market environments. Convergence to (stable) optimal policy. When using an LLM as the policy network in deep RL, the policy is jointly parameterized by the models intrinsic parameters and the prompt. Although the initial prompts strongly influence policy generation during the first few training epochs, this effect diminishes over time. Consequently, the system converges to relatively stable policy that becomes less sensitive to the initial prompts, indicating that RL training allows the LLM-based agent to refine its strategy and achieve robust trading policy. FLAG-TRADER enables small-scale models to surpass large-scale counterparts. While increasing model size generally enhances financial decision-making and robustness as seen with large proprietary models (e.g., GPT-o1-preview) in the baseline framework-FLAG-TRADER leverages an RLbased training pipeline to enable 135Mparameter open-source model to outperform significantly larger models in financial trading tasks. This demonstrates that well-designed"
        },
        {
            "title": "Limitations and Potential Risk",
            "content": "Despite its promising results, FLAG-TRADER has several limitations. First, while our approach significantly enhances the decision-making ability of LLMs, it remains computationally expensive, particularly when fine-tuning on large-scale market datasets. Reducing computational overhead while maintaining performance is an important direction for future research. Second, financial markets exhibit high volatility and non-stationarity, posing challenges for long-term generalization. Future work should explore techniques such as continual learning or meta-learning to enhance model adaptability in evolving market conditions. Third, while FLAG-TRADER effectively integrates textual and numerical data, its reliance on structured prompts could introduce biases in decision-making. Improving prompt design or exploring retrieval-augmented methods may further enhance robustness. Lastly, real-world trading requires stringent risk management, and FLAG-TRADER currently optimizes for financial returns without explicitly incorporating risk-sensitive constraints. Extending the framework to integrate risk-aware objectives and dynamic portfolio optimization could provide more robust and practical financial trading solutions."
        },
        {
            "title": "References",
            "content": "Dogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097. Renato Arantes de Oliveira, Heitor Ramos, Daniel Hasan Dalip, and Adriano César Machado Pereira. 2020. tabular sarsa-based stock market agent. In Proceedings of the First ACM International Conference on AI in Finance, pages 18. Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, and Michael Bendersky. 2023. What do llms know about financial markets? case study on reddit market sentiment analysis. In Companion Proceedings of the ACM Web Conference 2023, pages 107110. Xiu Gao and Laiwan Chan. 2000. An algorithm for trading and portfolio management using q-learning and sharpe ratio maximization. In Proceedings of the international conference on neural information processing, pages 832837. Citeseer. Ben Hambly, Renyuan Xu, and Huining Yang. 2023. Recent advances in reinforcement learning in finance. Mathematical Finance, 33(3):437503. Allen Huang, Hui Wang, and Yi Yang. 2023. Finbert: large language model for extracting information from financial text. Contemporary Accounting Research, 40(2):806841. Yuling Huang, Xiaoxiao Wan, Lin Zhang, and Xiaoping Lu. 2024. novel deep reinforcement learning framework with bilstm-attention networks for algorithmic trading. Expert Systems with Applications, 240:122581. John Hull. 2007. Risk Management and Financial Institutions. John Wiley & Sons. Jangmin, Jongwoo Lee, Jae Won Lee, and ByoungTak Zhang. 2006. Adaptive stock trading with dynamic asset allocation using reinforcement learning. Information Sciences, 176(15):21212147. Gyeeun Jeong and Ha Young Kim. 2019. Improving financial trading decisions using deep q-learning: Predicting the number of shares, action strategies, and transfer learning. Expert Systems with Applications, 117:125138. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and 1 others. 2023. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728. Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, and 1 others. 2024a. Investorbench: benchmark for financial decision-making tasks with llm-based agent. arXiv preprint arXiv:2412.18174. Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. 2023. Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. arXiv preprint arXiv:2309.03736. Yuan Li, Bingqiao Luo, Qian Wang, Nuo Chen, Xu Liu, and Bingsheng He. 2024b. Cryptotrade: reflective llm-based agent to guide zero-shot cryptocurrency trading. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 10941106. Zhipeng Liang, Hao Chen, Junhao Zhu, Kangkang Jiang, and Yanran Li. 2018. Adversarial deep reinforcement learning in portfolio management. arXiv preprint arXiv:1808.09940. Xiao-Yang Liu, Guoxuan Wang, and Daochen Zha. 2023. Fingpt: Democratizing internet-scale data for financial large language models. arXiv preprint arXiv:2307.10485. Xiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang, Ming Zhu, Christina Wang, Zhaoran Wang, and Jian Guo. 2022. Finrl-meta: Market environments and benchmarks for data-driven financial reinforcement learning. Advances in Neural Information Processing Systems, 35:18351849. Xiao-Yang Liu, Zhuoran Xiong, Shan Zhong, Hongyang Yang, and Anwar Walid. 2018. Practical deep reinforcement learning approach for stock trading. arXiv preprint arXiv:1811.07522. Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. 2021. Finrl: Deep reinforcement learning framework to automate trading in quantitative finance. In Proceedings of the second ACM international conference on AI in finance, pages 19. Arman Khadjeh Nassirtoussi, Saeed Aghabozorgi, Teh Ying Wah, and David Chek Ling Ngo. 2014. Text mining for market prediction: systematic review. Expert Systems with Applications, 41(16):7653 7670. Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022. When flue meets flang: Benchmarks and large pre-trained language model for financial domain. arXiv preprint arXiv:2211.00083. Si Shi, Jianjun Li, Guohui Li, and Peng Pan. 2019. multi-scale temporal feature aggregation convolutional neural network for portfolio management. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 16131622. Shuo Sun, Rundong Wang, and Bo An. 2023. Reinforcement learning for quantitative trading. ACM Transactions on Intelligent Systems and Technology, 14(3):129. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, Devon Hjelm, and Alexander Toshev. 2023. Large language models as generalizable policies for embodied tasks. In The Twelfth International Conference on Learning Representations. Kieran Wood, Sven Giegerich, Stephen Roberts, and Stefan Zohren. 2021. Trading with the momentum transformer: An intelligent and interpretable architecture. arXiv preprint arXiv:2112.08534. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. Pixiu: large language model, instruction data and evaluation benchmark for finance. arXiv preprint arXiv:2306.05443. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023a. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031. Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, and 1 others. 2024. Finrobot: An open-source ai agent platform for financial applications using large language models. arXiv preprint arXiv:2405.14767. Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023b. Investlm: large language model for investment using financial domain instruction tuning. arXiv preprint arXiv:2309.13064. Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: pretrained language model arXiv preprint 2020. for financial communications. arXiv:2006.08097. Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Ju Xiao, and Bo Li. 2020. Reinforcementlearning based portfolio management with augmented asset movement prediction states. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11121119. Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan Suchow, and Khaldoun Khashanah. 2024a. Finmem: performance-enhanced llm trading agent with layered memory and character design. In Proceedings of the AAAI Symposium Series, volume 3, pages 595597. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan Suchow, Rong Liu, Zhenyu Cui, Zhaozhuo Xu, and 1 others. 2024b. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. arXiv preprint arXiv:2407.06567. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan Suchow, Zhenyu Cui, Rong Liu, and 1 others. 2025. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Advances in Neural Information Processing Systems, 37:137010137045. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and 1 others. 2024. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292. Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, and 1 others. 2024. Finagent: multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. arXiv preprint arXiv:2402.18485. Yuzhe Zhang and Hong Zhang. 2023. Finbertmrc: financial named entity recognition using bert under the machine reading comprehension paradigm. Neural Processing Letters, 55(6):73937413. Zihao Zhang, Stefan Zohren, and Stephen J. Roberts. 2019. Deep reinforcement learning for trading. In The Journal of Financial Data Science. Additional Algorithmic Details: FLAG-TRADER with PPO In this section, we outline detailed procedure for training the FLAG-TRADER architecture via PPO, where the POLICY_NET (actor) and the VALUE_NET (critic) share subset of trainable parameters (cid:1). We define θpolicy = (cid:0)θtrain, θP ) and θvalue = (cid:0)θtrain, θV ) for from LLM, with θ = (cid:0)θtrain, θP , θV simplicity. Advantage Estimation. We use the Generalized Advantage Estimation (GAE) to compute the advantage function At: At = 1 (cid:88) (γλ)k(cid:2)rt+k + γVθvalue(st+k+1) Vθvalue(st+k)(cid:3), (17) k=0 where γ is the discount factor, and λ is the GAE parameter. Probability Ratio. Let θpolicy,old denote the parameters before the current update. The PPO probability ratio is rt(θpolicy) = πθpolicy (at st) πθpolicy,old(at st) . (18) PPO Clipped Objective. PPO clips this ratio to prevent overly large updates. The surrogate objective is LP (θpolicy) = Et (cid:104) min(cid:0)rt(θpolicy) At, clip(cid:0)rt(θpolicy), 1 ε, 1 + ε(cid:1) At (cid:1)(cid:105) , (19) where ε is hyperparameter. Value Function Loss. The critic (value network) is updated by minimizing the difference between the predicted value Vθvalue(st) and the target return Rt. common choice is: LV (θvalue) = Et (cid:104) (Vθvalue(st) Rt)2(cid:105) . (20) Combined Loss. We often add an entropy term to encourage exploration, yielding the overall objective: Ltotal(θ) = LP (θpolicy) + c1 LV (θvalue) c2 H(cid:0)πθpolicy (cid:1), (21) where c1 and c2 are weighting coefficients, and H(πθpolicy ) represents the policy entropy. Parameter Updates. At each iteration, we apply gradient descent on the total loss: θP θP η θP LP , θV θV η θV LV , θtrain θtrain β θtrain Ltotal, (22) (23) (24) where η and β are learning rates for the policy head, value head, and trainable LLM layers respectively. The algorithm is summerized in Algorithm 2. Algorithm 2 FLAG-TRADER with PPO 1: Input: Pre-trained LLM parameters (θfrozen, θtrain); actor parameters θP ; critic parameters θV ; environment E; discount factor γ; GAE parameter λ; PPO clip ε; learning rates η, β; 2: Initialize θtrain, θP , θV ; let θold θ 3: Initialize replay buffer 4: for iteration = 1 to max_iters do 5: // Collect Rollouts for = 1 to do Fetch the current state st from the environment and construct an input prompt lang(st); Pass prompt lang(st) through LLM; POLICY_NET outputs at from action space {buy, sell, hold} based on (8); Execute action at in the environment and observe reward r(st, at) and transition to new state st+1; Store experience tuple (st, at, rt, st+1) in replay buffer B; end for // Compute Advantage and Targets for each transition in do Compute Vθvalue(st) and advantage At (e.g., via GAE) end for // Perform PPO Updates for update_epoch = 1 to do Sample mini-batch from Compute probability ratio rt(θpolicy) = (cid:104) πθpolicy πθpolicy,old (atst) (atst) ; Compute PPO loss LP (θpolicy) = Et Compute Value loss LV (θvalue) = Et Compute total loss Ltotal(θ) = LP (θpolicy) + c1 LV (θvalue) c2 H(cid:0)πθpolicy Perform gradient descent on each parameter group: min(cid:0)rt(θpolicy) At, clip(cid:0)rt(θpolicy), 1 ε, 1 + ε(cid:1) At (Vθvalue(st) Rt)2(cid:105) (cid:104) ; (cid:1); (cid:1)(cid:105) ; θP θP η θP LP , θV θV η θV LV , θtrain θtrain β θtrain Ltotal; 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end for // Update old policy parameters Update θ = (cid:0)θtrain, θP , θV 27: 28: end for 29: Return: Fine-tuned POLICY_NET(θP ). (cid:1) by θold θ;"
        },
        {
            "title": "B Additional Experimental Details",
            "content": "Hyperparameters for Finetuening FLAG-TRADER with PPO in Algorithm 2 Table 3: FLAG-TRADER with PPO Finetuning Hyperparameters and Settings."
        },
        {
            "title": "Description",
            "content": "total_timesteps learning_rate num_envs num_steps anneal_lr gamma gae_lambda update_epochs norm_adv clip_coef clip_vloss ent_coef vf_coef kl_coef max_grad_norm target_kl dropout llm train_dtype gradient_accumulation_steps minibatch_size max_episode_steps Total number of timesteps Learning rate of optimizer Number of parallel environments Steps per policy rollout Enable learning rate annealing Discount factor γ Lambda for Generalized Advantage Estimation Number of update epochs per cycle Advantages whitening Surrogate clipping coefficient Clipped loss for value function Coefficient of entropy term Coefficient of value function KL divergence with reference model Maximum gradient clipping norm Target KL divergence threshold Dropout rate 13860 5 104 1 40 True 0.95 0.98 1 True 0.2 True 0.05 0.5 0.05 0.5 None 0.0 \"SmolLM2-135M-Instruct\" Model to fine-tune Training data type \"float16\" Number of gradient accumulation steps 8 Mini-batch size for fine-tuning 32 Maximum number of steps per episode"
        }
    ],
    "affiliations": [
        "Columbia University",
        "Harvard University",
        "NVIDIA",
        "Rensselaer Polytechnic Institute",
        "Stevens Institute of Technology",
        "TheFinAI",
        "University of Manchester",
        "University of Minnesota"
    ]
}