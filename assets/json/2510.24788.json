{
    "paper_title": "The Underappreciated Power of Vision Models for Graph Structural Understanding",
    "authors": [
        "Xinjian Zhao",
        "Wei Pang",
        "Zhongkai Xue",
        "Xiangru Jian",
        "Lei Zhang",
        "Yaoyao Xu",
        "Xiaozhuang Song",
        "Shu Wu",
        "Tianshu Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 8 7 4 2 . 0 1 5 2 : r a"
        },
        {
            "title": "The Underappreciated Power of Vision Models for\nGraph Structural Understanding",
            "content": "Xinjian Zhao *, Wei Pang *, Zhongkai Xue*, Xiangru Jian *, Lei Zhang, Yaoyao Xu, Xiaozhuang Song, Shu Wu, Tianshu Yu School of Data Science, The Chinese University of Hong Kong, Shenzhen Institute of Automation, Chinese Academy of Sciences Cheriton School of Computer Science, University of Waterloo {xinjianzhao1,weipang,zhongkaixue}@link.cuhk.edu.cn, xiangru.jian@uwaterloo.ca {leizhang1,yaoyaoxu,xiaozhuangsong1}@link.cuhk.edu.cn shu.wu@nlpr.ia.ac.cn, yutianshu@cuhk.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition. The code is available at https://github.com/LOGO-CUHKSZ/GraphAbstract"
        },
        {
            "title": "Introduction",
            "content": "Graphs are powerful abstractions to represent complex relationships across entities like social networks [62] and molecular structures [24, 30]. Learning effective representations of these graphs is crucial for node classification, graph classification, and link prediction tasks [51, 67, 42, 34, 54]. Over the past decade, Graph Neural Networks (GNNs) have become the dominant paradigm for graph representation learning, demonstrating impressive performance through their message-passing mechanism that iteratively aggregates local neighborhood information [37, 24, 27, 59, 71]. Despite its success, message passing faces several key limitations, such as limited expressiveness [71, 52, 78] and difficulty in capturing long-range dependencies [2]. Numerous efforts have been trying to address different aspects of the message passing mechanism limitations, graph transformer architectures have * Equal Contribution Corresponding authors 39th Conference on Neural Information Processing Systems (NeurIPS 2025). enriched this paradigm by incorporating long-range interactions across broader graph contexts [16, 39, 74, 68, 50], positional encodings [76, 17, 31] to inject structural priors to improve global topological awareness and graph rewiring techniques mitigate structural bottlenecks in graphs by modifying the topology to enhance information diffusion [58, 48, 5]. However, these advances remain fundamentally constrained by cognitive discrepancy: While humans intuitively perceive global structures through visual Gestalt principles - understanding the whole before analyzing individual parts [57], GNNs and their variants operate through bottom-up processing. Humans instantly recognize ring structures, symmetric structures, and critical bridges in graph visualizations, yet even advanced architectures struggle with such basic topological understanding [29, 79]. Therefore, although the existing graph learning architecture innovations have made significant progress from both theoretical and practical perspectives, the essence of these efforts is still to identify and repair different aspects of the inherent defects in the message passing mechanism, with limited exploration of vision-based approaches for graph learning despite their natural alignment with how humans perceive network structures. The flourishing field of computer vision has produced rich ecosystem of powerful models and techniques that excel at holistic pattern recognition and global structure understanding [45, 7, 36, 38]. This vibrant research landscape provides fertile ground for re-imagining graph learning through visual lens. Inspired by this potential, we explore fundamentally different approach: leveraging powerful vision models by translating graph topology into the visual domain. By simply rendering graphs as images using standard layout algorithms [22, 14], we apply vision encoders to graph-level tasks without any graph-specific architectural modifications. This vision-based approach mirrors how humans perceive graph structures visually rather than through explicit message-passing operations. Our evaluations demonstrate that pure vision encoders perform comparably to specialized GNNs on established graph benchmarks, despite having no graph-specific inductive biases or architectural design. This finding is remarkable considering that these vision models operate solely on graph layouts without access to node features or explicit connectivity information. However, these traditional benchmarks tightly couple domain knowledge with topology, making it difficult to quantitatively study their separate impacts. As shown by [4, 65], models using fixed-structure expander graphs rather than the true molecular topology can match or exceed performance on multiple molecular benchmarks, suggesting that node features often dominate over structural information in standard datasets, potentially masking differences in how models perceive graph topology. To rigorously evaluate topological understanding, we introduce GraphAbstract, benchmark designed to evaluate how well models perceive graph structures in ways that mirror human visual cognition. We present four carefully crafted tasks that challenge models to abstract critical global graph properties in ways humans naturally do: recognizing organizational archetypes, detecting symmetry patterns, sensing connectivity strength, and identifying critical structural elements. For each task, we meticulously design diverse graph families with well-controlled topological properties. Our evaluation protocol directly tests out-of-distribution (OOD) generalization by systematically increasing graph sizes from training to testing, evaluating whether models can recognize the same structural patterns regardless of scale, fundamental capability of human cognition. Our experiments yield several key findings: On tasks requiring the abstraction of global graph properties, vision models demonstrate significant advantages and superior generalization capabilities across different graph scales compared to GNNs. For GNNs, we find that positional encoding methods that inject structural priors achieve substantial improvements over architectural innovations in message passing. These complementary findings point to unified insight: successful graph understanding stems from accessing global topological information, either through structural priors or visual perception. This global-first approach aligns more closely with human cognitive processes for graph understanding, suggesting that future advances in graph representation learning and foundation models may benefit from prioritizing global structural perception over refining local message passing mechanisms. These results open new perspectives for developing graph learning systems that are more effective, interpretable, and capable of generalizing to complex topological structures. Our main contributions include: (1) demonstrating that vision-based models, which better align with human cognition, achieve strong performance without any graph-specific architectural modifications; (2) introducing rigorously designed benchmark with diverse graph families that evaluates models ability to understand graph structures and generalize across varying scales, which is fundamental capability of human cognition; and (3) revealing that global-first approaches significantly outperform 2 traditional message-passing innovations, suggesting promising directions for visual-centric graph understanding."
        },
        {
            "title": "2 Related Work",
            "content": "Graph visualization and graph learning. Graph visualization and graph learning have traditionally advanced along separate paths. Visualization focuses on creating human-interpretable spatial representations that produce graph layouts through techniques like spectral methods and force-directed algorithms, while graph learning focuses on feature extraction with GNNs. Recent studies indicate convergence between these domains [63, 82, 13, 40, 64]. For example, GITA [63], GraphTMI [13], and DPR [40] incorporate visual graph layouts into vision-language models to improve graph reasoning tasks, DEL [82] introduces probabilistic layout sampling to enhance GNN expressivity, and domain-specific approaches explore visual representations for molecular graphs [77, 69]. However, these works directly introduced graph layout into existing architectures such as VLMs and GNNs, treating graph layouts and vision models as black boxes without exploring their fundamental cognitive differences in graph understanding. Graph learning benchmarks. Graph representation learning has evolved through two complementary benchmark traditions. The first utilizes real-world datasets from citation networks [73], molecules [47], and comprehensive collections like OGB [30] to evaluate practical performance. The second employs synthetic expressivity tests to probe theoretical limitations through challenges like graph isomorphism [1, 3, 61], substructure counting [9]. Recent LLM-oriented graph benchmarks focus on structural analysis tasks such as node counting, cycle detection, and path finding [60, 6, 44]. However, these benchmarks typically rely on limited set of random graph generators (e.g., ErdosRényi, Watts-Strogatz) that inadequately capture topological diversity [60, 40, 19, 44, 8, 70]. The critical gap in existing benchmarks is their inability to assess human-like graph cognition, specifically, the intuitive ability to recognize and abstract connectivity patterns across varying scales and contexts. For brevity, we provide extended discussions of GNN architectures and graph positional encoding methods in Appendix B."
        },
        {
            "title": "3 Visual vs. Message-Passing: Distinct Cognitive Patterns",
            "content": "We analyze the differences between two model families (GNNs and vision models) in graph tasks from three perspectives: prediction overlap, interpretability case study, and training dynamics & prediction confidence. All models are trained on five graph classification benchmarks [47], with implementation details provided in Appendix J.1. Prediction Overlap Analysis. Our experiments across five datasets show that vision models achieve performance competitive with GNNs, as shown in Table 1. However, Figure 1 reveals that while GNN variants behave similarly to each other, GNNs and vision models show distinct prediction patterns: they not only differ in their correct predictions, but often succeed on samples where the other fails. This consistent pattern across all datasets suggests that these two model families develop fundamentally different strategies for processing graph information. The stark contrast in their prediction patterns indicates that GNNs and vision models might be capturing different aspects of graph structure, motivating deeper investigation into their respective strengths and limitations. Case Studies. We conduct case studies across multiple benchmark datasets, with detailed analysis presented here using the first three samples from the test set of PROTEINS dataset in Figure 3. We leverage GNN Explainer [75] for graph models and Grad-CAM [53] for vision models, generating visualizations across multiple network layers (2/3/4-layer for GNNs, and low/mid/high-level features for vision models). The three cases represent distinct structural scenarios commonly found in protein graphs: hierarchical local-to-global patterns, critical bridge structures, and chain-like configurations. Through these visualizations, we observe vision models remarkable adaptability: exhibiting progressive focus from local to regional features in hierarchical structures, maintaining consistent attention on critical bridges across all levels, and adopting global-centric strategy for chain-like patterns. This flexible processing strategy stands in sharp contrast to GNNs, where GNN Explainer shows relatively uniform attention patterns constrained by local message passing. Notably, our studies on ENZYMES dataset (see Appendix J.3) reveal that vision models attention regions align better with previously identified discriminative patterns [12] compared to GNN-based approaches. 3 Table 1: Performance comparison on different datasets. Results show the accuracy (%) of different models, reported as mean std over 5 runs. Model NCI1 IMDB-MULTI ENZYMES IMDB-BINARY PROTEINS GNN Models GAT GIN GCN GPS 67.0 0.5 70.4 1.3 65.4 1.1 76.3 3. 49.7 1.0 50.9 1.7 48.4 1.8 50.1 2.2 27.0 5.9 26.3 3.9 25.3 2.2 34.3 5.6 Vision Models RESNET VIT SWIN ConvNeXt 67.7 1.0 63.5 1.4 69.0 0.5 70.2 2.2 51.2 1.1 50.8 3.3 52.0 2.0 53.5 1. 36.3 1.6 27.3 3.9 40.3 3.9 41.7 3.2 69.4 2.2 70.0 3.0 71.0 1.3 69.8 2.1 72.2 2.2 71.8 2.8 68.8 3.5 73.8 1.6 74.1 1.5 75.0 1.1 74.8 1.1 76.0 1.2 79.6 2.5 83.1 0.9 81.8 1.8 80.7 2.6 Figure 1: Prediction overlap analysis across different datasets. Top row: correct prediction overlap; Bottom row: error pattern overlap. GNN variants show high internal consistency, suggesting homogeneous learning behavior, while GNN and Vision models exhibit distinct prediction patterns. Training dynamics & confidence. We report the training curves in Figure 3 and provide detailed results for all datasets in Appendix J.2. The training dynamics reveal striking differences in learning behaviors across architectures. Vision models, regardless of their specific architectures, demonstrate strong memorization capabilities but with different learning speeds. CNN-based models show aggressive training dynamics, rapidly achieving near-perfect training accuracy while their training loss approaches zero. Transformer-based models exhibit somewhat slower learning progression, but ultimately reach similar training performance levels. However, all vision models suffer from substantial generalization gaps, with validation accuracy significantly lower than their training performance. GNN variants display notably different learning patterns. Traditional GNNs show more modest training performance, with both training and validation metrics evolving gradually and plateauing at lower levels. The GPS model, featuring additional global processing capabilities, achieves higher training accuracy but still exhibits limited validation performance similar to other GNN variants. These empirical observations suggest that the key challenge in vision-based graph learning lies not in improving models pattern recognition capabilities, which are already remarkably strong, but in developing effective mechanisms to bridge the substantial memorization-generalization gap. This consistent pattern across all vision architectures points to fundamental challenge that may require solutions beyond architectural modifications alone, such as specialized pre-training strategies or graph-specific data augmentation techniques. 4 Figure 2: Case Studies for PROTEINS dataset. Why an intuitive topology benchmark is essential. Our analysis shows that different types of models process graphs differently, but existing benchmarks fail to measure how closely these approaches align with human visual perception of graph structures. Current evaluations mix structural understanding with domain-specific features, leading to cases where models perform well even with random graph topologies [4]. This gap between intuitive human understanding and current evaluation practices highlights the need for dedicated benchmark to specifically assess intuitive topological perception. Figure 3: Training dynamics across different architectures on NCI1 dataset. For each model, we plot the training loss (blue), training accuracy (red), and validation accuracy (green) over 100 epochs. The shaded areas represent the standard deviation across multiple runs."
        },
        {
            "title": "4 A New Benchmark: GraphAbstract",
            "content": "4.1 Motivation and Design Principles Building on our analysis of cognitive divergence between model families, we introduce GraphAbstract, benchmark specifically designed to evaluate fundamental graph understanding capabilities that align with human visual reasoning. Traditional benchmarks in domains like molecular prediction, citation networks, and protein interaction graphs inadvertently couple domain-specific node features with topology, often allowing models to succeed through feature-based shortcuts rather than genuine 5 structural understanding. This limitation becomes particularly evident in recent studies [4, 65], where models using fixed-structure expander graphs rather than true molecular topologies match or exceed performance on multiple benchmarks. The theoretical evaluation of graph neural networks has primarily centered on the Weisfeiler-Lehman (WL) test, which measures models ability to distinguish between pairs of non-isomorphic graphs of the same size. While valuable for theoretical analysis, this approach is inherently limited to binary discrimination between fixed-size graph pairs rather than evaluating models ability to recognize abstract structural patterns across varying scales and contexts. What remains missing is rigorous evaluation framework that targets explicitly models ability to perceive, abstract, and reason about fundamental graph properties in ways that mirror human visual cognition. GraphAbstract addresses these limitations through four carefully designed tasks that isolate pure structural comprehension from domain-specific attributes. Each task evaluates different aspect of how humans intuitively perceive graphs: recognizing organizational archetypes, detecting symmetry patterns, sensing connectivity strength, and identifying critical structural elements. By focusing on these fundamental perceptual capabilities and systematically varying graph scale between training and testing, we can rigorously assess whether graph learning approaches develop the scale-invariant structural understanding that characterizes human visual cognition. 4.2 Benchmark Details 4.2.1 High-level Topology Classification The first and core task of our benchmark is high-level topology classification, where models must identify the dominant topological pattern in graph = (V, E). We carefully design six fundamental topological patterns commonly observed in real-world networks, each representing distinct organizational principles that humans can readily identify through visual inspection. This task evaluates models ability to perceive global structural organization beyond local connectivity patterns, mirroring how humans naturally identify network archetypes across diverse domains. The following six graph types represent our core taxonomy of high-level topological patterns: The Cyclic Structure is generated as an annular random geometric graph where nodes are distributed within ring-shaped region and connections are established based on spatial proximity [23]. The Random Geometric Graph structure emerges from spatial constraints, where connections are determined by the proximity of nodes in an underlying metric space. This topology is ubiquitous in wireless sensor networks, urban infrastructure, and physical systems governed by geographical limitations [49]. Hierarchical Structures organize nodes into multiple levels where higher tiers have fewer, more densely connected nodes controlling numerous nodes in lower layers. Community Structures feature multiple densely connected subgroups with relatively sparse inter-group connections, representing common patterns in social networks and biological networks [25]. The Bottleneck Configuration contains critical narrow passages between larger substructures, similar to traffic networks, information flow channels, and metabolic networks. These structures are particularly important for testing models ability to identify crucial connecting components that often represent vulnerability points in real systems. Multicore-periphery Networks [72] exhibit multiple densely connected centers with their respective peripheral nodes, reflecting patterns found in distributed computing systems, multi-center urban structures, and neural networks. 4.2.2 Symmetry Classification Symmetry perception represents one of the most fundamental pattern recognition capabilities in human cognition. Humans can readily identify symmetric patterns through visual inspection, even without explicit mathematical analysis, when examining graph structures. This capacity has profound practical importance across domains: in chemical structures, symmetry determines molecular properties and reactivity; in network design, engineers leverage symmetry for resilience and load balancing; while cryptographers specifically construct asymmetric structures to enhance security. Our Symmetry Classification task challenges models to develop this same intuitive capability by determining whether graph possesses non-trivial symmetry based on its automorphism properties. To precisely characterize graph symmetry, we employ the concept of graph automorphism. Using the automorphism group, we can precisely categorize graphs based on their symmetry properties: 6 Definition 1 (Graph Automorphism). Given graph = (V, E), an automorphism is bijection ϕ : such that (u, v) if and only if (ϕ(u), ϕ(v)) E. The set of all automorphisms forms group Aut(G) under composition: Aut(G) = {ϕ : ϕ is bijective and (u, v) (ϕ(u), ϕ(v)) E} (1) Definition 2 (Symmetric and Asymmetric Graphs). graph is classified as symmetric if Aut(G) > 1, indicating the existence of at least one non-identity automorphism, and asymmetric if Aut(G) = 1, where the only automorphism is the identity mapping. To construct diverse symmetry classification dataset, we implement several carefully designed generation strategies for both symmetric and asymmetric graphs. As part of our approach, we also extract collection of base graphs from real-world datasets using multiple sampling strategies to enhance structural diversity, as detailed in Appendix D.1.2. For symmetric graphs, we employ four principled approaches based on group-theoretic constructions. Our first symmetric graph generation method utilizes Cayley graphs, which are constructed from algebraic groups and naturally exhibit rich symmetry properties: Definition 3 (Cayley Graph). Given group Γ and generating set Γ where = S1 (closed under inverses), the Cayley graph Cay(Γ, S) has vertices = Γ and edges = {(g, gs) Γ, S}. Using this definition, we construct Cayley graphs Cay(Zn, S) where Zn is the cyclic group of order and contains generators of the group (elements coprime to n). These graphs inherently possess rich symmetry patterns with automorphism groups containing at least elements. The second approach leverages bipartite double covers, which provide systematic way to construct symmetric graphs from arbitrary base graphs: Definition 4 (Bipartite Double Cover). For graph = (V, E), its bipartite double cover = ( , E) is defined as: = {0, 1} = {(v, i) V, {0, 1}} = {((u, 0), (v, 1)), ((u, 1), (v, 0)) (u, v) E} (2) (3) We generate bipartite double covers from various base graphs, including random graphs, community graphs, bottleneck graphs, and real-world data. Each cover naturally possesses non-trivial automorphism σ((v, i)) = (v, 1 i) that swaps the two layers, guaranteeing Aut( G) 2 and ensuring the graph is symmetric by definition. Appendix provides detailed proof of this property. Our third method creates symmetric structures through the Cartesian product: Definition 5 (Cartesian Product). For graphs G1 = (V1, E1) and G2 = (V2, E2), their Cartesian product G1G2 = (V, E) is defined as: = V1 V2 = {(u, v) V1, V2} = {((u1, v), (u2, v)) (u1, u2) E1, V2} {((u, v1), (u, v2)) V1, (v1, v2) E2} (4) (5) We create symmetric graphs through Cartesian products of known symmetric components (e.g., cycle graphs Cn, path graphs Pn, star graphs Sn), resulting in structures like prism graphs CnK2 and torus grids CmCn. For products involving real-world graphs (which we also use to generate asymmetric graphs), we rigorously verify and filter based on the actual automorphism group properties, as the Cartesian product preserves symmetry only when both factor graphs are symmetric. Additionally, we employ multi-layer cyclic covers using real-world data as base graphs. These covers possess natural cyclic symmetry where the automorphism τ ((v, i)) = (v, (i + 1) mod k) generates cyclic group isomorphic to Zk, ensuring Aut(Gk) k. The full mathematical definition and additional properties of these structures are provided in Appendix E. For asymmetric graphs, we employ two primary strategies. First, we create perturbed graphs using Double-Edge Swap perturbations [46], where we start with symmetric structures and systematically transform them through edge swaps. Specifically, we repeatedly select two edges (vi, vj) and (vk, vl) and replace them with (vi, vl) and (vk, vj) if these new edges dont already exist. After each swap, we 7 verify both connectivity preservation and symmetry breaking using automorphism group computation. This method maintains the degree distribution of the original graph while disrupting its symmetry structure. Second, we leverage real-world graph patterns through Cartesian products of real-world graphs, whose inherent irregularity typically leads to asymmetric structures. 4.2.3 Spectral Gap Regression While humans cannot directly see mathematical properties of networks, we intuitively perceive network conductance, bottleneck structures, and overall connectivity strength through visual inspection. These perceptual judgments closely align with what graph theory formalizes as the spectral gap λ2(G), the second-smallest eigenvalue of the normalized Laplacian matrix [10]. This fundamental parameter quantifies graphs global connectivity characteristics, determining how quickly random walks mix (tmix 1/λ2) and providing lower bounds on critical connectivity measures. Our regression task challenges models to develop representations that can infer this abstract property directly from topology, mirroring human ability to estimate network efficiency without explicit computation. To ensure diverse spectral properties, we generate graphs using stochastic block models with varying mixing parameters, geometric graphs with different connection radii, and configuration models with targeted degree distributions. This design forces models to develop structural intuitions equivalent to understanding that bottlenecked networks (low λ2) exhibit restricted information flow, while expander-like graphs (high λ2) enable rapid diffusion. This reflects precisely the type of reasoning humans employ when analyzing network resilience in domains ranging from transportation systems to communication infrastructure. 4.2.4 Bridge Counting Bridge counting evaluates models ability to identify critical edges whose removal would increase the number of connected components in graph. Formally, given graph = (V, E), we define the set of bridges B(G) as: B(G) = {e κ(G {e}) > κ(G)} (6) where κ(G) denotes the number of connected components in graph G. The objective is to predict B(G), the total number of bridges in the input graph. This regression task requires models to understand both local edge importance and global connectivity patterns. Bridges serve as critical connectors between biconnected components of graph, with each bridge = (u, v) satisfying the property that there exists no alternative path between and when is removed. The bridge identification challenge varies systematically with graph structure, requiring models to adapt their reasoning across different topological contexts. Models lacking this capability face limitations in practical applications requiring critical connectivity awareness, such as molecular stability analysis and retrosynthetic planning [11, 56]. These four tasks systematically probe models ability to perceive and reason about fundamental graph properties. While representing subset of human topological capabilities, they provide diagnostic tests for structural understanding that underlies many practical applications. Systematically measuring these capability gaps offers insights into current limitations and directions for developing more robust graph learning models. 4.3 Evaluation Protocol To rigorously assess the generalization capabilities of graph learning models, we introduce systematic evaluation framework incorporating progressively challenging distribution shifts based on graph scale. This framework enables us to quantify how well models can transfer topological understanding across varying graph sizes, capability that humans demonstrate naturally. Our evaluation includes three test settings of increasing difficulty: ID (In-Distribution) setting uses test graphs containing 20-50 nodes, matching the training distribution. Near-OOD (Near Out-of-Distribution) setting contains graphs with 40-100 nodes, representing moderate scale shift. Far-OOD (Far Out-of-Distribution) setting features graphs with 60-150 nodes, constituting significant scale shift. These examples challenge models to recognize the same topological patterns at dramatically larger scales, testing their ability to abstract core structural principles independent of 8 Table 2: Performance comparison across different tasks and models. First and second best performances are highlighted in each setting and model family. Far-OOD ID Model GCN+Degree GPS+Degree GIN+Degree GAT+Degree GCN+LapPE GPS+LapPE GIN+LapPE GAT+LapPE GCN+SignNet GPS+SignNet GIN+SignNet GAT+SignNet GCN+SPE GPS+SPE GIN+SPE GAT+SPE Swin ConvNeXtV2 ResNet ViT ID 80.67 0.60 81.40 1.81 79.87 1.05 81.87 1.67 86.83 1.64 93.07 1.14 93.37 0.62 84.90 2.77 94.47 1.54 81.80 14.38 94.20 1.80 94.00 1.21 93.20 2.16 84.80 13.75 94.53 1.76 93.53 4. 94.80 0.54 95.20 0.34 95.87 0.62 94.00 0.99 Topology Near-OOD 54.67 2.69 64.33 2.18 62.47 2.79 58.40 3.52 70.97 3.99 81.00 2.77 82.13 2.96 72.07 3.13 94.20 1.65 87.67 5.19 84.73 6.50 96.47 1.60 33.67 3.56 37.87 4.60 39.33 4.31 42.80 3. 55.00 3.08 47.40 2.16 51.13 4.34 54.07 7.26 77.93 2.77 75.20 6.53 61.00 8.72 85.27 6.83 90.60 4.77 84.07 16.04 87.80 9.89 92.60 6.95 72.33 7.90 72.20 14.51 70.33 12.09 85.33 9.94 97.73 0.57 97.20 1.48 96.27 1.02 95.20 1.20 89.13 3.26 90.33 4.60 87.40 3.33 86.40 1. Symmetry Near-OOD 66.87 1.08 70.87 1.52 69.13 1.54 67.40 0.76 66.19 0.88 69.89 1.58 68.83 1.17 66.46 1.28 67.07 0.83 69.47 1.24 68.60 2.14 67.60 1.47 66.80 1.40 70.67 1.23 68.80 1.12 66.87 0.98 90.77 0.81 89.13 0.57 88.83 0.64 91.03 0. Far-OOD 65.13 1.94 66.53 2.44 68.47 0.51 65.70 1.36 65.35 2.30 66.80 1.88 67.17 1.55 66.22 1.71 65.60 1.43 67.73 1.03 67.50 2.11 67.27 1.37 64.80 2.85 67.70 1.37 68.63 0.97 67.47 0.64 84.70 1.36 84.67 0.77 84.20 0.39 85.67 1. ID 0.1325 0.0022 0.0696 0.0020 0.1159 0.0025 0.1329 0.0004 0.0442 0.0211 0.0263 0.0084 0.0217 0.0057 0.0182 0.0026 0.0203 0.0018 0.0244 0.0060 0.0237 0.0039 0.0204 0.0047 0.0255 0.0025 0.0681 0.0298 0.0376 0.0028 0.0296 0.0029 0.0312 0.0037 0.0279 0.0047 0.0335 0.0021 0.0345 0. Spectral Near-OOD 0.2517 0.0036 0.1844 0.0136 0.2885 0.0474 0.2512 0.0133 0.1076 0.0526 0.0706 0.0313 0.0538 0.0145 0.0419 0.0039 0.0274 0.0034 0.0783 0.0134 0.0750 0.0195 0.0303 0.0030 0.0507 0.0039 0.1537 0.0839 0.1491 0.0382 0.0784 0.0044 0.0594 0.0024 0.0578 0.0056 0.0600 0.0063 0.0746 0. Far-OOD ID 0.3167 0.0046 0.4271 0.0494 0.6460 0.2373 0.3149 0.0158 0.1840 0.0807 0.1781 0.0881 0.1268 0.0415 0.0722 0.0048 0.0523 0.0147 0.3133 0.0529 0.2417 0.0904 0.0571 0.0154 0.1351 0.0284 0.6716 0.2709 0.8412 0.3343 0.2210 0. 0.0946 0.0094 0.1006 0.0047 0.1102 0.0100 0.1154 0.0080 1.3995 0.0294 1.5226 0.1512 1.2953 0.0373 1.3775 0.0472 0.9961 0.0762 1.1665 0.6545 0.8683 0.1112 0.9603 0.0701 0.6750 0.1104 0.9872 0.3033 0.6303 0.1828 0.5713 0.0973 0.5503 0.0777 0.6402 0.1753 0.6011 0.1649 0.4854 0.0622 0.6526 0.0547 0.6261 0.0702 0.7771 0.1095 0.7406 0. Bridge Near-OOD 3.1067 0.1171 3.3043 0.2873 3.3695 0.3631 3.1871 0.1867 2.8534 0.2058 2.4846 0.5410 2.4427 0.2064 2.4669 0.1743 2.4387 0.5131 1.9819 0.4649 2.4745 0.3013 1.7152 0.1943 1.4143 0.2405 1.4666 0.0713 2.4499 0.6701 1.5176 0.4072 1.6338 0.1675 1.8045 0.2007 1.6356 0.1643 1.8263 0. Far-OOD 5.6302 0.0898 5.9010 0.3702 7.0563 1.0770 5.6034 0.1339 5.7156 0.3173 5.3825 0.6990 5.1461 0.3576 5.2778 0.2455 6.3090 1.5907 4.5278 0.8672 7.1992 1.0679 4.1380 0.2873 3.8632 1.2460 3.8021 1.0492 7.8487 2.0425 4.1430 1.7660 3.7918 0.3361 4.1809 0.2742 3.6814 0.1217 4.3765 0. 69.73 0.87 72.73 0.87 71.57 1.54 69.47 0.89 68.63 1.02 71.52 1.50 71.37 1.19 69.15 1.09 69.03 0.93 70.73 1.46 70.43 0.69 69.90 0.98 68.90 0.79 71.97 1.65 70.87 1.11 68.07 1.07 92.50 0.43 92.83 0.53 93.47 0.66 94.03 1.04 scale. This evaluation framework serves as an analog to human cognitive flexibility, where people can seamlessly recognize familiar patterns at vastly different scales. For instance, humans can readily identify the same community structure whether it appears in small departmental network of dozens of people or large organizational chart with hundreds of employees. The ability to maintain consistent performance across these distribution shifts reflects the kind of scale-invariant understanding that advanced graph reasoning systems should aspire to develop. Baselines. We evaluate two primary model families: graph neural networks and vision-based models. For GNNs, we implement four architectures using one-hot degree encoding as node features: GCN [37], GIN [71], GAT [59], and GPS [50], combined with three positional encoding schemes: LapPE [17], SignNet [41], SPE [31]. For vision-based approaches, we evaluate four backbone architectures: ResNet-50 [28], Swin Transformer-Tiny [43], ViT-B/16 [15], and ConvNeXtV2Tiny [66], with three graph layout algorithms: Kamada-Kawai [35], Spectral layout [26], and ForceAtlas2 [33]. Benchmark statistics, implementation details, and examples of graphs from all four tasks are provided in Appendices C, D, and H, respectively. 4.4 Main Results Our benchmark isolates the challenge of topology understanding from feature-based learning prevalent in tasks like node classification and molecular property prediction. This design enables us to focus specifically on evaluating models capability to comprehend graph structural patterns. Our extensive experiments reveal several key findings. Vision Models Exhibit Superior Scale-Invariant Understanding. Tables 2 and 3 demonstrate that pure vision-based models show remarkable proficiency in abstracting global topological patterns across varying scales. Vision models maintain consistent performance across increasing distribution shifts, while GNNs exhibit severe degradation. On topology classification, vision models drop only 5-6% accuracy from ID to Far-OOD settings, while basic GNNs with one-hot degree feature experience dramatic declines of over 45%. This stark contrast highlights vision models human-like ability to recognize organizational patterns regardless of scale. The vision advantage is particularly pronounced in symmetry detection, where vision models with spectral layouts achieve 20% higher accuracy than even the best GNN variants. This task directly evaluates models capacity to perceive global structural properties that humans naturally identify through visual inspection. Layout Algorithms Critically Shape Visual Graph Understanding. Different layout algorithms significantly impact how easily humans and models perceive key structural properties. Spectral layouts, for instance, excel at symmetry detection (8-10% higher accuracy than force-directed approaches). Their use of Laplacian eigenvectors often leads to node overlap when visualizing graphs with symmetries or highly repetitive patterns. This simplification of complex structures into more recognizable forms makes global properties more apparent (detailed in Appendix I.2). Similarly, circular layouts [21], with detailed performance reported in Appendix I.3, arrange nodes in perfect circles. This allows for straightforward symmetry assessment through edge density: symmetric graphs show uniform connections, while in asymmetric graphs, humans can easily spot irregular densities or specific edges that break the symmetry. In contrast, force-directed methods like Kamada Kawai prioritize preserving the graphs intrinsic shape by optimizing for clear visual Table 3: Performance comparison between KK, ForceAtlas2, and Spectral layout algorithms across different tasks and vision models Model KK Topology ForceAtlas2 Spectral KK Symmetry ForceAtlas2 Spectral KK ID Spectral ForceAtlas2 Spectral KK Bridge ForceAtlas2 Spectral Swin ConvNeXtV2 ResNet ViT 93.80 1.31 95.20 0.34 95.87 0.62 94.00 0.99 94.80 0.54 94.93 0.13 94.93 1.08 92.93 0. 87.27 0.57 87.53 0.78 85.67 0.47 86.13 0.65 85.07 1.05 87.30 1.20 85.63 0.84 86.47 1.75 80.93 1.09 80.73 0.98 79.53 0.90 80.07 1.96 92.50 0.43 92.83 0.53 93.47 0.66 94.03 1.04 0.0324 0.0035 0.0284 0.0024 0.0335 0.0021 0.0367 0.0045 0.0312 0.0037 0.0279 0.0047 0.0376 0.0098 0.0345 0. 0.0333 0.0025 0.0403 0.0032 0.0463 0.0071 0.0441 0.0067 0.6526 0.0547 0.6261 0.0702 0.7771 0.1095 0.7406 0.1167 0.9867 0.1587 0.9226 0.0540 1.0602 0.0759 1.0883 0.0887 1.2524 0.0543 1.3579 0.0320 1.5106 0.1855 1.3944 0.0493 Near-OOD Swin ConvNeXtV2 ResNet ViT 97.73 0.57 97.20 1.48 96.27 1.02 95.20 1.20 92.20 0.65 92.40 0.83 93.67 1.15 92.53 0.88 93.27 1.77 93.20 1.89 94.60 1.00 92.67 1.01 81.80 1.31 81.83 1.51 82.60 1.05 82.47 1.89 79.87 0.51 80.70 0.49 79.07 0.67 79.87 1.31 90.77 0.81 89.13 0.57 88.83 0.64 91.03 0. 0.0819 0.0141 0.0728 0.0122 0.0936 0.0120 0.1058 0.0132 0.0594 0.0024 0.0578 0.0056 0.0600 0.0063 0.0746 0.0081 0.0690 0.0084 0.0750 0.0109 0.0914 0.0052 0.0828 0.0068 1.6338 0.1675 1.8045 0.2007 1.6356 0.1643 1.8263 0.0679 2.2916 0.1672 2.4521 0.1678 2.3661 0.2173 2.4940 0.1665 2.4495 0.2141 2.5207 0.1870 2.9001 0.3499 2.6900 0. Far-OOD Swin ConvNeXtV2 ResNet ViT 89.13 3.26 90.33 4.60 87.40 3.30 79.53 0.69 81.93 1.73 87.47 1.39 76.93 1.16 86.40 1.61 87.13 2.95 89.00 1.62 85.20 2.08 81.80 0.69 74.20 2.05 75.90 2.30 74.80 1.36 76.87 2. 77.23 1.15 77.03 1.10 74.93 1.10 74.17 1.55 84.70 1.36 84.67 0.77 84.20 0.39 85.67 1.06 0.1668 0.0142 0.1419 0.0149 0.1739 0.0120 0.1837 0.0101 0.1182 0.0050 0.1006 0.0047 0.1102 0.0100 0.1471 0.0143 0.0946 0.0094 0.1018 0.0112 0.1179 0.0075 0.1154 0.0080 3.7918 0.3361 4.1809 0.2742 3.6814 0.1217 4.3765 0. 4.9075 0.2818 5.3567 0.2295 4.9441 0.2304 5.1766 0.1582 4.9141 0.2744 5.0889 0.1948 5.5314 0.4061 5.3629 0.1370 separation of nodes and sensible edge routing. This makes individual elements and local relationships more distinguishable, often excelling at revealing community structures and general topological arrangements. These observations highlight the importance of task-aware layout selection in visual graph understanding. Different layout algorithms naturally emphasize different structural properties, suggesting opportunities for developing adaptive visualization strategies that match layout choices to specific reasoning objectives. The intuitions behind how layout algorithms enable vision models to access structural information are discussed in Appendix G. Global Structural Priors Help GNNs Bridge the Cognitive Gap. Our experiments reveal that incorporating positional encodings (PEs) that inject pre-computed global structural information significantly outperforms innovations in message passing architectures alone. This approach represents an alternative global-first strategy within the message-passing framework, providing GNNs with structural context before local propagation begins. All three PE schemes substantially improve performance and generalization capability, with some advanced PE-enhanced GNNs approaching vision model performance on topology tasks. This finding suggests unified insight: successful graph understanding fundamentally requires access to global topological information, whether through visual perception or explicitly injected structural priors. It also indicates promising directions for future development where vision models could potentially benefit from more explicit structural priors through specialized pre-training or augmentation strategies that emphasize key topological features. Computational Trade-offs and Model Capacity. Vision models require approximately 10 more computational time than GNN+PE approaches in our standard experimental setting (detailed in Table 5 of Appendix F). To better attribute the performance differences, we conducted parameter scaling experiments with GPS+SPE, expanding it to match or exceed the vision model capacity. Consistent with known challenges in scaling GNNs, performance degraded rather than improved (full results in Table 6 of Appendix F). This confirms that the observed advantages stem from architectural differences, not simply parameter count. The two paradigms understand graph structure through fundamentally different mechanisms: GNNs process topology through abstract message passing and structural priors, while vision models directly recognize patterns in visual representations. The superior out-of-distribution generalization of vision models on global structural understanding tasks aligns with this direct pattern recognition approach. When selecting approaches for practical applications, the computational overhead must be weighed against these advantages in scale generalization and structural reasoning."
        },
        {
            "title": "5 Conclusion",
            "content": "Our work reveals the underappreciated power of vision models for graph structural understanding, demonstrating that vision-based methods better align with human cognitive processes in capturing global topological properties. Through GraphAbstract, we systematically quantified these advantages on tasks requiring holistic understanding and scale-invariant reasoning, particularly their ability to maintain consistent performance across varying graph scales. These findings establish visual processing as complementary pathway to traditional graph learning. While GNNs excel through explicit structural priors and domain-specific inductive biases, vision models offer distinctive strengths in perceiving global patterns through direct pattern recognition. This suggests promising directions for graph foundation models that integrate visual perception with structural reasoning, combining the strengths of both paradigms for more robust and generalizable graph understanding."
        },
        {
            "title": "6 ACKNOWLEDGMENTS",
            "content": "This work is supported in part by the National Natural Science Foundation of China (Grant No. 92470113), the National Key Research and Development Program (2018YFB1402600), and the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001)."
        },
        {
            "title": "References",
            "content": "[1] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. arXiv preprint arXiv:2010.01179, 2020. [2] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. ICLR, 2021. [3] Muhammet Balcilar, Pierre Héroux, Benoit Gauzere, Pascal Vasseur, Sébastien Adam, and Paul Honeine. Breaking the limits of message passing graph neural networks. In International Conference on Machine Learning, pages 599608. PMLR, 2021. [4] Maya Bechler-Speicher, Ido Amos, Ran Gilad-Bachrach, and Amir Globerson. Graph neural networks use graphs when they shouldnt. In Forty-first International Conference on Machine Learning, 2024. [5] Mitchell Black, Zhengchao Wan, Amir Nayyeri, and Yusu Wang. Understanding oversquashing In International Conference on Machine in gnns through the lens of effective resistance. Learning, pages 25282547. PMLR, 2023. [6] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model. arXiv preprint arXiv:2310.05845, 2023. [7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019. [8] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model for graph computational problems. In Proceedings of KDD 2024, 2024. [9] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? Advances in neural information processing systems, 33:1038310395, 2020. [10] Fan RK Chung. Spectral graph theory, volume 92. American Mathematical Soc., 1997. [11] Connor Coley, Wengong Jin, Luke Rogers, Timothy Jamison, Tommi Jaakkola, William Green, Regina Barzilay, and Klavs Jensen. graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science, 10(2):370377, 2019. [12] Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, and Caihua Shan. How do large language models understand graph patterns? benchmark for graph pattern comprehension. arXiv preprint arXiv:2410.05298, 2024. [13] Debarati Das, Ishaan Gupta, Jaideep Srivastava, and Dongyeop Kang. Which modality should use - text, motif, or image? : Understanding graphs with large language models, 2024. arXiv:2311.09862v2. [14] Sara Di Bartolomeo, Tarik Crnovrsanin, David Saffo, Eduardo Puerta, Connor Wilson, and Cody Dunne. Evaluating graph layout algorithms: systematic review of methods and best practices. In Computer Graphics Forum, volume 43, page e15073. Wiley Online Library, 2024. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 11 [16] Vijay Prakash Dwivedi and Xavier Bresson. generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020. [17] Vijay Prakash Dwivedi, Chaitanya Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):148, 2023. [18] Peter Eades and Xuemin Lin. Spring algorithms and symmetry. Theoretical Computer Science, 240(2):379405, 2000. [19] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like graph: Encoding graphs for large language models. In Proceedings of ICLR, 2024. [20] Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. Advances in Neural Information Processing Systems, 35:3137631390, 2022. [21] Emden Gansner and Yehuda Koren. Improved circular layouts. In International Symposium on Graph Drawing, pages 386398. Springer, 2006. [22] Helen Gibson, Joe Faith, and Paul Vickers. survey of two-dimensional graph layout techniques for information visualisation. Information visualization, 12(3-4):324357, 2013. [23] Alexander Giles, Orestis Georgiou, and Carl Dettmann. Connectivity of soft random geometric graphs over annuli. Journal of Statistical Physics, 162:10681083, 2016. [24] Justin Gilmer, Samuel Schoenholz, Patrick Riley, Oriol Vinyals, and George Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 12631272. PMLR, 2017. [25] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks. Proceedings of the national academy of sciences, 99(12):78217826, 2002. [26] Kenneth Hall. An r-dimensional quadratic placement algorithm. Management science, 17(3):219229, 1970. [27] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017. [28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [29] Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and Karsten Borgwardt. Topological graph neural networks. International Conference on Learning Representations, 2022. [30] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:2211822133, 2020. [31] Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the stability of expressive positional encodings for graphs. International Conference on Learning Representations, 2024. [32] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with i2-gnns. arXiv preprint arXiv:2210.13978, 2022. [33] Mathieu Jacomy, Tommaso Venturini, Sebastien Heymann, and Mathieu Bastian. Forceatlas2, continuous graph layout algorithm for handy network visualization designed for the gephi software. PloS one, 9(6):e98679, 2014. [34] Xiangru Jian, Xinjian Zhao, Wei Pang, Chaolong Ying, Yimu Wang, Yaoyao Xu, and Tianshu Yu. Rethinking spectral augmentation for contrast-based graph self-supervised learning. arXiv preprint arXiv:2405.19600, 2024. [35] Tomihisa Kamada, Satoru Kawai, et al. An algorithm for drawing general undirected graphs. Information processing letters, 31(1):715, 1989. [36] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: survey. ACM computing surveys (CSUR), 54(10s):141, 2022. [37] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [39] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Létourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:2161821629, 2021. [40] Yunxin Li, Baotian Hu, Haoyuan Shi, Wei Wang, Longyue Wang, and Min Zhang. Visiongraph: Leveraging large multimodal models for graph theory problems in visual context. arXiv preprint arXiv:2405.04950, 2024. [41] Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. International Conference on Learning Representations, 2023. [42] Lu Lin, Jinghui Chen, and Hongning Wang. Spectral augmentation for self-supervised learning on graphs. arXiv preprint arXiv:2210.00643, 2022. [43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [44] Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, and Xing Xie. Graphinstruct: Empowering large language models with graph understanding and reasoning capability, 2024. arXiv:2403.04483v2. [45] Sébastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM international conference on Multimedia, pages 14851488, 2010. [46] Sergei Maslov and Kim Sneppen. Specificity and stability in topology of protein networks. Science, 296(5569):910913, 2002. [47] Christopher Morris, Nils Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020. [48] Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh In Nguyen. Revisiting over-smoothing and over-squashing using ollivier-ricci curvature. International Conference on Machine Learning, pages 2595625979. PMLR, 2023. [49] Mathew Penrose. Random geometric graphs, volume 5. OUP Oxford, 2003. [50] Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:1450114515, 2022. [51] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019. [52] Ryoma Sato. survey on the expressive power of graph neural networks. arXiv preprint arXiv:2003.04078, 2020. [53] Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017. [54] Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, and Jiliang Tang. Lpformer: An adaptive graph transformer for link prediction. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining, pages 26862698, 2024. [55] Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, and Jia Li. Grapharena: Evaluating and exploring large language models on graph computation. In The Thirteenth International Conference on Learning Representations, 2025. [56] Amol Thakkar, Alain Vaucher, Andrea Byekwaso, Philippe Schwaller, Alessandra Toniato, and Teodoro Laino. Unbiasing retrosynthesis language models with disconnection prompts. ACS Central Science, 9(7):14881498, 2023. [57] Dejan Todorovic. Gestalt principles. Scholarpedia, 3(12):5345, 2008. [58] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522, 2021. [59] Petar Veliˇckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [60] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? Advances in Neural Information Processing Systems, 36:3084030861, 2023. [61] Yanbo Wang and Muhan Zhang. An empirical study of realized gnn expressiveness. In Proceedings of the 41st International Conference on Machine Learning, pages 5213452155, 2024. [62] Stanley Wasserman and Katherine Faust. Social network analysis: Methods and applications. 1994. [63] Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James Kwok, and Yu Zhang. Gita: Graph to visual and textual integration for vision-language graph reasoning. Advances in Neural Information Processing Systems, 37:4472, 2024. [64] Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, and James Kwok. Open your eyes: Vision enhances message passing neural networks in link prediction. arXiv preprint arXiv:2505.08266, 2025. [65] JJ Wilson, Maya Bechler-Speicher, and Petar Veliˇckovic. Cayley graph propagation. arXiv preprint arXiv:2410.03424, 2024. [66] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1613316142, 2023. [67] Qitian Wu, Wentao Zhao, Zenan Li, David Wipf, and Junchi Yan. Nodeformer: scalable graph structure learning transformer for node classification. Advances in Neural Information Processing Systems, 35:2738727401, 2022. [68] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. Advances in neural information processing systems, 34:1326613279, 2021. [69] Hongxin Xiang, Shuting Jin, Jun Xia, Man Zhou, Jianmin Wang, Li Zeng, and Xiangxiang Zeng. An image-enhanced molecular graph representation learning framework. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pages 61076115, 2024. 14 [70] Hao Xu, Xiangru Jian, Xinjian Zhao, Wei Pang, Chao Zhang, Suyuchen Wang, Qixin Zhang, Zhengyuan Dong, Joao Monteiro, Bang Liu, et al. Graphomni: comprehensive and extendable benchmark framework for large language models on graph-theoretic tasks. arXiv preprint arXiv:2504.12764, 2025. [71] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [72] Bowen Yan and Jianxi Luo. Multicores-periphery structure in networks. Network Science, 7(1):7087, 2019. [73] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 4048. PMLR, 2016. [74] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34:2887728888, 2021. [75] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. Advances in neural information processing systems, 32, 2019. [76] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International conference on machine learning, pages 71347143. PMLR, 2019. [77] Xiangxiang Zeng, Hongxin Xiang, Linhui Yu, Jianmin Wang, Kenli Li, Ruth Nussinov, and Feixiong Cheng. Accurate prediction of molecular properties and drug targets using selfsupervised image representation learning framework. Nature Machine Intelligence, 4(11):1004 1016, 2022. [78] Bingxu Zhang, Changjun Fan, Shixuan Liu, Kuihua Huang, Xiang Zhao, Jincai Huang, and Zhong Liu. The expressive power of graph neural networks: survey. IEEE Transactions on Knowledge and Data Engineering, 2024. [79] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. arXiv preprint arXiv:2301.09505, 2023. [80] Muhan Zhang and Pan Li. Nested graph neural networks. Advances in Neural Information Processing Systems, 34:1573415747, 2021. [81] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. arXiv preprint arXiv:2110.03753, 2021. [82] Xinjian Zhao, Chaolong Ying, Yaoyao Xu, and Tianshu Yu. Graph learning with distributional edge layouts. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1, pages 20552066, 2025."
        },
        {
            "title": "Appendix",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "Contents",
            "content": "A Limitations and Future Work A.1 Limitations . A.2 Future Work . . . . . . . . . . . . . Extended Related Work Benchmark Statistics Implementation Details D.1 Dataset Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1.1 Topology Classification Dataset Generation . . . . . . . . . . . . . . . . . D.1.2 Symmetry Classification Dataset Generation . . . . . . . . . . . . . . . . D.1.3 Spectral Gap Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1.4 Bridge Counting Dataset Generation . . . . . . . . . . . . . . . . . . . . . D.2 Node Features and Positional Encodings . . . . . . . . . . . . . . . . . . . . . . . D.3 Model Architecture and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . D.4 Training Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Graph Image Generation for Vision Models . . . . . . . . . . . . . . . . . . . . . Proof of Symmetry in Graph Coverings E.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Symmetry of Bipartite Double Cover . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Symmetry of k-fold Cyclic Cover . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Algorithmic Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Extended Experimental Results Discussion: Visual vs. Message-Passing Paradigms Visualization Examples from GraphAbstract Analysis of Graph Layout Algorithms I.1 Layout Algorithm Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Visualizing Asymmetric Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . I.3 Characteristics and Performance of Circular Layout . . . . . . . . . . . . . . . . . Preliminary Experiments and Implementation J. Implementation Details of Preliminary Experiments . . . . . . . . . . . . . . . . . J.2 Training dynamics and confidence . . . . . . . . . . . . . . . . . . . . . . . . . . 16 18 18 18 18 19 19 19 21 22 22 22 22 23 23 23 24 25 28 29 31 31 31 33 33 33 J.3 Case Studies . J.4 Heatmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37"
        },
        {
            "title": "A Limitations and Future Work",
            "content": "A.1 Limitations Theoretical Foundations for Layout Algorithms. While our experiments demonstrate that layout algorithms critically shape performance, we lack complete theoretical characterizations of why certain layouts benefit particular reasoning tasks. The relationship between geometric properties of layouts and their learnability by neural networks remains an open theoretical question. Early work by Eades & Lin [18] connected spring algorithms to geometric automorphisms, but the graph visualization community has since focused primarily on human aesthetics rather than machine learning objectives. Establishing rigorous theoretical frameworks connecting layout properties to learning guarantees represents an essential direction for bridging graph visualization and machine learning research. Human Cognition Alignment: While tasks in GraphAbstract are designed to mirror human cognitive capabilities, we do not directly compare model performance with human behavior on these tasks. Future studies could incorporate human experiments (e.g., eye-tracking studies or timed reasoning tasks) to establish quantitative benchmarks for cognitive alignment. A.2 Future Work Vision-centric Graph Foundation Models. Our work establishes visual processing as viable pathway for graph understanding, demonstrating competitive performance and superior scale generalization on structural reasoning tasks. Realizing this potential requires developing comprehensive ecosystem for vision-based graph learning, ultimately enabling vision-centric graph foundation models. Key directions include curating large-scale pretraining datasets of graph visualizations across diverse domains, designing graph-specific augmentation strategies that preserve topological properties in the visual domain, developing visual encoding schemes for node and edge attributes (e.g., color mapping, size encoding, visual markers), creating specialized architectures optimized for processing graph images, and exploring new application scenarios such as interactive graph visualization and visual graph analytics. Such infrastructure could enable foundation models that leverage visual perception for robust and generalizable structural understanding while incorporating rich semantic information."
        },
        {
            "title": "B Extended Related Work",
            "content": "Graph Neural Networks and Positional Encodings. Graph Neural Networks have achieved remarkable success across diverse domains through learnable aggregation of neighborhood information [37, 24, 27, 59, 71]. The field has developed various architectural innovations to enhance structural understanding capabilities. Subgraph-based methods [81, 20, 80, 32] extract features from local structural patterns around nodes. Graph transformers [16, 39, 74, 50] enable broader context aggregation through global attention mechanisms. Particularly relevant to our work are positional encodings (PE) that augment graph models with pre-computed global information. Spectral approaches [17, 39] leverage Laplacian eigenvectors to encode global connectivity patterns. SPE [31] learns continuous soft-partition mappings that weight each eigenvector by its eigenvalue, ensuring both provable Lipschitz stability under graph perturbations and universal expressivity for basis-invariant functions. SignNet [41] addresses the ambiguity of eigenvectors by computing node-wise features through learned function, followed by MLPs to produce sign-invariant representations. These methods demonstrate that injecting pre-computed global structural information can substantially improve performance on tasks requiring holistic graph understanding."
        },
        {
            "title": "C Benchmark Statistics",
            "content": "Table 4 provides comprehensive overview of our benchmark statistics across all four tasks. Each task is carefully designed with appropriate sample sizes and node ranges to enable robust evaluation. To gain deeper insights into the distribution characteristics of our regression tasks, we visualized the distributions of bridge counts and spectral gaps across different graph types and dataset splits in Figures 4 and 5. 18 Table 4: Dataset statistics across our four benchmark tasks. Each cell shows the number of graphs followed by the node count range in parentheses. Topology Classification is 6-way classification task, Symmetry Classification is 2-way classification task, while Spectral Gap and Bridge Count are regression tasks. Split Train Val Test (ID) Test (Near-OOD) Test (Far-OOD) Topology Symmetry Spectral Gap Bridge Count 3000 (20-50) 300 (20-50) 300 (20-50) 300 (40-100) 300 (60-150) 2000 (30-60) 200 (30-60) 600 (30-60) 600 (50-100) 600 (70-150) 3000 (20-50) 300 (20-50) 300 (20-50) 300 (40-100) 300 (60-150) 2500 (20-50) 250 (20-50) 250 (20-50) 250 (40-100) 250 (60-150) Figure 4: Distribution of bridge counts across different graph types under various settings (Train, ID, Near-OOD, and Far-OOD). The plots reveal distinct bridge count patterns for each graph structure (Geometric, Community, Hierarchical, Bottleneck, and Multicore). Notably, the distributions exhibit shifts as graph sizes increase, particularly visible in the OOD scenarios."
        },
        {
            "title": "D Implementation Details",
            "content": "This appendix provides comprehensive implementation details for our benchmark tasks, including dataset generation methodology, model architectures, and training protocols. D.1 Dataset Generation Our benchmark consists of four distinct graph understanding tasks. For each task, we generate three test sets with varying difficulty: ID (same distribution as training), Near-OOD (moderate distribution shift), and Far-OOD (significant distribution shift). All datasets are implemented using PyTorch Geometric. D.1.1 Topology Classification Dataset Generation For the topology classification task, we implemented six distinct topology generators, each producing graphs with visually and structurally distinct patterns. Cyclic Structure. We generate annular random geometric graphs (ARGG) with inner radius randomly sampled from [0.7, 1.2], outer radius set as rinner + random(0.05, 0.3), and connection radius randomly sampled from [0.5, 0.8]. Random Geometric Graph. Nodes are distributed uniformly in unit square with connection radius randomly sampled from [0.15, 0.25]. Connections are established between nodes within this radius of each other. We ensure connectedness by adding edges between disconnected components if necessary. 19 Figure 5: Distribution of spectral gaps across different graph types under various settings (Train, ID, Near-OOD, and Far-OOD). The plots reveal distinct spectral gap patterns for each graph structure. Notably, the distributions exhibit shifts as graph sizes increase. Community Structure. We generate communities with 3 to 5 groups (based on graph size), intracommunity edge probability from [0.6, 0.8], and inter-community edge probability from [0.01, 0.05]. This creates densely connected communities with sparse connections between them, reflecting real-world community structures. Hierarchical Hub Structure. We implement multi-level hierarchical organization with 2 to 4 levels. Level size ratio decreases by factor of approximately 0.4 with higher levels. Intra-level connectivity decreases for lower levels, calculated as 0.7 num_levelslevel . Each node connects to 1-3 nodes in the level above it, creating graphs with clear hierarchical organization from top to bottom. num_levels Bottleneck Structure. We generate bottleneck topologies with 2 to 4 communities, intra-community edge probability from [0.4, 0.6], and bottleneck width of 1 to 3nodes connecting adjacent communities. This creates graphs with distinct modules connected by narrow pathways. Multi-core Periphery Structure. We generate multi-core networks with 2 to 3 cores occupying 50-60% of total nodes (minimum 4 nodes per core). Core internal probability ranges from [0.6, 0.8] with 1-2 bridging nodes between each core pair. Periphery nodes are randomly connected to 1-2 nodes in randomly selected core, creating structures with multiple highly connected centers surrounded by sparsely connected peripheral nodes. D.1.2 Symmetry Classification Dataset Generation For symmetry classification, we employ several theoretically grounded approaches to generate both symmetric and asymmetric graphs. Real-world Base Graph Extraction. To enhance diversity and realism in our generated graphs, we extracted collection of base graphs from real-world datasets. For MUTAG, we directly utilized the molecular graphs. For Cora, which is large citation network, we employed three different sampling techniques to obtain diverse set of subgraphs: (1) BFS sampling from random starting nodes. (2) Random walks with restart probability α = 0.2, which were explored by previous work [55]. (3) GraphSAGE-style neighborhood sampling with controlled layer expansion. For each target node size range (5-50 nodes), we ensured sufficient number of base graphs (approximately 30 graphs per size) to support subsequent operations like Cartesian products and graph covers. These real-world base graphs were cached and reused across different generation methods, providing consistent structural patterns while maintaining diversity. All base graphs were processed to ensure connectedness, and their node indices were normalized to ensure compatibility with our generation pipelines. We implemented five distinct methods to generate symmetric graphs: Cayley Cyclic Graphs. We construct Cayley graphs based on cyclic groups Zn. For given node count n, we identify valid generators (elements coprime to n), randomly select 1-3 generators, and create connections according to the Cayley graph definition. Each node Zn is connected to nodes (i + g) mod and (i g) mod for each generator g, ensuring that the graph exhibits the algebraic symmetry of the cyclic group. Bipartite Double Cover. We create bipartite double covers from various base graphs (random graphs, community structures, bottleneck structures, and real-world data). For base graph = (V, E), the bipartite double cover = ( , E) is constructed with = 0, 1 and edges = ((u, 0), (v, 1)), ((u, 1), (v, 0)) (u, v) E. Base graphs have approximately half the target node 20 count. For random base graphs, edge probability is from [0.15, 0.3]. For community-based graphs, intra-community probability is from [0.3, 0.7] and inter-community probability is from [0.05, 0.15]. For bottleneck base graphs, internal connectivity probability is from [0.3, 0.6] with bottleneck width of 1-3 nodes. Cartesian Product. We generate cartesian products of known symmetric components with combinations including cycle graphs CnPm (cycle path), CnCm (cycle cycle), and PnSm (path star). The cartesian product G1G2 contains vertices (G1) (G2) with edges between (u1, v1) and (u2, v2) if either u1 = u2 and (v1, v2) E(G2), or v1 = v2 and (u1, u2) E(G1). Factors are chosen to optimize the product size to be close to the target node count. Cartesian Product with Real Data. We compute Cartesian products using real-world graph data, selected from the MUTAG and Cora datasets. It is important to note that the Cartesian product preserves symmetry only when both base graphs are symmetric. Since real-world graphs typically lack perfect symmetry, we use the pynauty1 library to verify the symmetry of each generated graph and filter accordingly. The introduction of real-world graphs enables us to generate more diverse and structurally realistic graphs, including both symmetric and asymmetric variants with complex topological features that purely synthetic generators cannot easily produce. Real Data Cyclic Cover. We construct k-fold cyclic covers using real data as base graphs, with the number of layers (k) from 2 to 5 based on target size. For base graph = (V, E), the k-fold cyclic cover creates copies of with edges connecting corresponding vertices across consecutive layers in cyclic pattern. Formally, vertices in the cover are Zk, and for each edge (u, v) E, we add edges ((u, i), (v, (i + 1) mod k)) and ((v, i), (u, (i + 1) mod k)) for all Zk. We implemented two primary approaches to generate asymmetric graphs: Perturbed Asymmetric Graphs. We start with symmetric graphs and apply targeted edge perturbations via double-edge swap operations. We select two edges (a, b) and (c, d) and replace them with edges (a, d) and (c, b) if they dont already exist. This operation maintains the degree distribution while potentially breaking symmetry. We verify that connectivity is preserved and symmetry is broken, applying up to 20 swap attempts. Each generated graph is verified to ensure Aut(G) = 1, meaning the only automorphism is the identity mapping. Cartesian Products with Real Graphs. We leverage the inherent asymmetry of real-world data through Cartesian products involving the previously extracted real-world networks. Its important to note that while the Cartesian product operation naturally creates repeating structural patterns (as each vertex from one graph is combined with every vertex from the other graph), these repeating patterns do not necessarily translate into mathematical symmetry (automorphisms). The resulting structure may contain similar local neighborhoods but still lack the global permutation invariance required for true symmetry. This distinction is crucial - Cartesian products create structural regularity that can be challenging for models to analyze, but often without introducing the simplifying symmetry properties that might make the task easier. All generated graphs are rigorously verified using pynauty to confirm their asymmetric nature (Aut(G) = 1) before inclusion in the dataset. D.1.3 Spectral Gap Regression For spectral gap regression, we employed mixture of techniques to generate graphs with diverse spectral properties: SBM Evolution. Using Stochastic Block Models(SBM) with controlled mixing, including SBMdumbbell structure (two equal-sized communities) and SBM-multi communities structure (3-5 communities). Within-block probability ranges from [0.6, 0.8]. Between-block probability varies with mixing parameter µ [0, 1]: [0.001, 0.02] when µ < 0.1, [0.02, 0.1] when µ < 0.3, [0.1, 0.3] when µ < 0.7, and [0.3, µ] when µ 0.7. The spectral gap λ2 of these graphs is strongly influenced by the between-block connectivity. Geometric Evolution. Modified random geometric graphs with base radius calculated to ensure basic connectivity. Additional connections are added based on mixing parameter µ, with the number of extra connections approximately equal to µ 0.1 ln(n). Higher µ values create more small-world-like properties, affecting the spectral gap. 1https://github.com/pdobsan/pynauty 21 Configuration Model. Starting with an SBM or geometric base graph, we rewire edges using the configuration model while preserving the degree distribution. The randomization level controls the fraction of edges rewired, ranging from [0.3, 0.8]. This process disrupts the original structure while maintaining the degree sequence, often resulting in graphs with different spectral properties. To ensure comprehensive coverage of the spectral gap value range, we strategically sample the mixing parameter µ, with 40% of samples using low connectivity (µ < 0.2), 30% using medium connectivity (µ [0.2, 0.5]), and 30% using higher connectivity (µ [0.5, 0.8]). The spectral gap λ2 is computed as the second-smallest eigenvalue of the normalized Laplacian matrix = D1/2AD1/2, where is the degree matrix and is the adjacency matrix. D.1.4 Bridge Counting Dataset Generation For the bridge counting task, we generate diverse graph structures using five topology generators similar to those used in the topology classification task. These include Random Geometric Graphs with connection radius [0.15, 0.25], Community Structures with 3-5 communities and controlled density parameters, Hierarchical Structures with 2-4 levels, Bottleneck Configurations with singleconnection bottlenecks between adjacent communities, and Multi-core Structures with 2-3 cores and peripheral attachments. For each graph, we compute the exact number of bridges using NetworkXs bridge detection algorithm2, which identifies edges whose removal would increase the number of connected components in the graph. D.2 Node Features and Positional Encodings All positional encodings are incorporated into our GNN models with 16 dimensions. For node features, we use one-hot degree encoding with maximum degree of 100 across all graph datasets. D.3 Model Architecture and Hyperparameters For our graph neural network models, we experiment with varying numbers of layers ranging from 2 to 4, with consistent hidden dimension size of 128 across all architectures. Dropout with rate of 0.5 is applied throughout the networks to prevent overfitting. For vision-based models, we use standard architectures: ResNet-50, ViT-B/16, Swin Transformer-Tiny, and ConvNeXtV2-Tiny. All models resize graph images to 224224 resolution as input. D.4 Training Protocol All models are trained with batch size of 128 for maximum of 200 epochs, employing early stopping with patience of 30 epochs to prevent overfitting. We use the Adam optimizer with different learning rates: 1e 5 for vision backbone parameters, 1e 3 for GNN models and classifier heads. Weight decay is set to 1e 4 for vision models. For classification tasks (Topology, Symmetry), we use cross-entropy loss, while for regression tasks (Spectral Gap, Bridge Counting), we employ mean squared error loss. All experiments are conducted on 4 NVIDIA A800 GPUs. For consistent evaluation, we measure accuracy for classification tasks, while regression tasks use Mean Absolute Error (MAE). To ensure reproducibility, we set fixed random seeds [0, 1, 2, 3, 4] for all experiments, controlling the initialization of model parameters, data splitting. D.5 Graph Image Generation for Vision Models For vision-based models, we render graph visualizations with specific parameters to ensure visual consistency. Nodes are rendered as skyblue circles with white borders and size 50, while edges are rendered as white lines with width 1.5 and alpha 0.8. Each graph is rendered once for each dataset split, ensuring consistent visual representation across training and evaluation. 2https://networkx.org"
        },
        {
            "title": "E Proof of Symmetry in Graph Coverings",
            "content": "In this section, we provide formal proofs of the symmetry properties stated in the main text. For notational simplicity, we use to denote cover graph throughout these proofs. This corresponds to (bipartite double cover) and Gk (k-fold cyclic cover) in the main text. E.1 Definitions Definition 1 (Automorphism). An automorphism of graph is bijection ϕ : (G) (G) such that for any u, (G): (u, v) E(G) (ϕ(u), ϕ(v)) E(G) (7) Definition 2 (Automorphism Group). The set of all automorphisms of G, denoted Aut(G), forms group under function composition. Definition 3 (Symmetry). graph is symmetric if Aut(G) > 1, i.e., it admits at least one non-identity automorphism. Definition 4 (Bipartite Double Cover). For graph G, its bipartite double cover is constructed by: Creating two vertices (v, 0) and (v, 1) in for each vertex in Creating edges ((u, 0), (v, 1)) and ((u, 1), (v, 0)) in for each edge (u, v) in Definition 5 (k-fold Cyclic Cover). For graph (without self-loops), its k-fold cyclic cover is constructed by: Creating vertices (v, 0), (v, 1), . . . , (v, 1) in for each vertex in Creating edges ((u, i), (v, (i + 1) mod k)) and ((v, i), (u, (i + 1) mod k)) in for each edge (u, v) in and each {0, 1, . . . , 1} E.2 Symmetry of Bipartite Double Cover Theorem 1. For any graph G, its bipartite double cover satisfies Aut(H) > 1. Proof. We define mapping σ : (H) (H) as follows: σ((v, i)) = (v, 1 i) (G), {0, 1} (8) Step 1: We prove σ is bijection. Since for each (v, i) (H), σ maps to exactly one element (v, 1 i) (H), and since σ(σ((v, i))) = σ((v, 1 i)) = (v, i), σ is its own inverse. Therefore, σ is bijection. Step 2: We verify σ preserves edges. For any edge = ((u, i), (v, j)) E(H), by the definition of double cover: (u, v) E(G) and = 1 Applying σ to both endpoints: σ((u, i)) = (u, 1 i) and σ((v, j)) = σ((v, 1 i)) = (v, i) Since (u, v) E(G), by the definition of double cover: ((u, 1 i), (v, i)) E(H) Thus, (σ(u, i), σ(v, j)) E(H). Step 3: We verify σ preserves non-edges. For any non-edge ((u, i), (v, j)) / E(H), there are two cases: (9) (10) (11) 1. (u, v) / E(G): Then ((u, 1 i), (v, 1 j)) / E(H) by definition of double cover. 23 2. (u, v) E(G) but = 1 i: Then either = = 0 or = = 1. After applying σ, we have σ((u, i)) = (u, 1 i) and σ((v, j)) = (v, 1 j). Since 1 = 1 j, we have (σ(u, i), σ(v, j)) / E(H). Step 4: We show σ is not the identity mapping. For any vertex (v, 0) (H): σ((v, 0)) = (v, 1) = (v, 0) (12) Therefore, σ is non-identity automorphism of H, which proves Aut(H) > 1. Corollary 1. If Aut(G) = m, then Aut(H) 2m. Proof. Every automorphism ϕ Aut(G) induces two automorphisms on H: ϕ1((v, i)) = (ϕ(v), i) and ϕ2((v, i)) = (ϕ(v), 1 i) (13) These 2m automorphisms are all distinct, hence Aut(H) 2m. E.3 Symmetry of k-fold Cyclic Cover Lemma 1. For 2, the map τ : (H) (H) defined by τ ((v, i)) = (v, (i + 1) mod k) is an automorphism of the k-fold cyclic cover of any graph G. Proof. Step 1: We prove τ is bijection. For each (v, i) (H), τ maps to exactly one element (v, (i + 1) mod k). The inverse is defined by τ 1((v, i)) = (v, (i 1) mod k). Hence, τ is bijection. Step 2: We verify τ preserves edges. Let = ((u, i), (v, (i + 1) mod k)) E(H). By definition of k-fold cover, (u, v) E(G). Applying τ to both endpoints: τ ((u, i)) = (u, (i + 1) mod k) τ ((v, (i + 1) mod k)) = (v, (i + 2) mod k) Since (u, v) E(G), by definition of k-fold cover: ((u, (i + 1) mod k), (v, (i + 2) mod k)) E(H) Step 3: We verify τ preserves non-edges. For any non-edge ((u, i), (v, j)) / E(H), there are two cases: (14) (15) 1. (u, v) / E(G): Then ((u, (i + 1) mod k), (v, (j + 1) mod k)) / E(H) by definition. 2. (u, v) E(G) but = (i + 1) mod k: After applying τ , we have τ ((u, i)) = (u, (i + 1) mod k) τ ((v, j)) = (v, (j + 1) mod k) (16) Since (j +1) mod = (i+2) mod when = (i+1) mod k, we have (τ (u, i), τ (v, j)) / E(H). Step 4: For 2, τ is not the identity mapping. For any vertex (v, 0) (H): τ ((v, 0)) = (v, 1) = (v, 0) (17) Therefore, τ is non-identity automorphism of H. Theorem 2. For any graph and 2, the k-fold cyclic cover admits an automorphism group containing subgroup isomorphic to Zk, thus satisfying Aut(H) k. 24 Proof. Consider the mappings τ, τ 2, . . . , τ k1, τ k, where τ is the automorphism defined in the lemma above. We have: τ j((v, i)) = (v, (i + j) mod k) Step 1: The mappings τ 0 = id, τ 1, τ 2, . . . , τ k1 are all distinct. For 0 j1 < j2 < k, there exists (v, 0) (H) such that: τ j1 ((v, 0)) = (v, j1) = (v, j2) = τ j2((v, 0)) (18) (19) Step 2: These mappings form cyclic subgroup of order k. Since τ k((v, i)) = (v, (i + k) mod k) = (v, i), we have τ = id. This means τ generates cyclic group of order isomorphic to Zk. Therefore, Aut(H) contains subgroup isomorphic to Zk, implying Aut(H) k. E.4 Algorithmic Implementation The implementation of graph coverings in our code precisely follows the mathematical constructions in the above definitions: Algorithm 1 Generate Bipartite Double Cover Add vertices (v, 0) and (v, 1) to Require: Base graph G(V, E) Ensure: Double cover graph 1: Initialize as empty graph 2: for each do 3: 4: end for 5: for each (u, v) do 6: 7: end for 8: return Add edges ((u, 0), (v, 1)) and ((u, 1), (v, 0)) to Algorithm 2 Generate k-fold Cyclic Cover from Real-world Network end for for = 0 to 1 do Add vertex (v, i) to Require: Real-world base graph G(V, E), integer 2 Ensure: k-fold cyclic cover graph 1: Initialize as empty graph 2: for each do 3: 4: 5: 6: end for 7: for each (u, v) do for = 0 to 1 do 8: 9: 10: 11: end for 12: return end for Add edges ((u, i), (v, (i + 1) mod k)) and ((v, i), (u, (i + 1) mod k)) to"
        },
        {
            "title": "F Extended Experimental Results",
            "content": "This section provides detailed experimental results that support our main findings, including comprehensive computational cost analysis, model scaling experiments, advanced GNN comparisons, and resolution robustness studies. Computational Cost Analysis. Table 5 presents detailed breakdown of computational requirements across all four benchmark tasks. We report both time per epoch and total time to reach best validation accuracy. Vision models require approximately 10 more time per epoch than GNN+SPE models across all tasks, with the ratio ranging from 8.2 to 12.2 depending on the specific task. This computational overhead primarily stems from the larger model capacity and more complex feature extraction in vision backbones compared to compact GNN architectures. Model Scaling Analysis. To investigate whether performance differences stem from model capacity, we conducted systematic scaling experiments with GPS+SPE. Table 6 shows results for models with 53.2M and 212.2M parameters, substantially exceeding vision model sizes (ResNet-50: 25.6M). Consistent with known limitations of message-passing architectures, performance degraded rather than improved with increased capacity across all tasks. For topology classification, accuracy dropped from 84.80% (baseline) to 82.53% (212.2M model) on Near-OOD tasks. This confirms that architectural differences, rather than parameter count, drive the observed advantages of vision models. Comparison with Advanced GNNs. Table 7 compares our results with I²-GNN [32], an advanced structure-aware GNN. While I²-GNN achieves competitive in-distribution performance, it shows significantly worse generalization on out-of-distribution tasks, particularly on Far-OOD settings. This demonstrates that even advanced GNN architectures with enhanced expressiveness face challenges in scale generalization compared to vision-based approaches. Resolution Robustness. Table 8 examines how image resolution affects vision model performance. Performance remains relatively stable across different resolutions (6464 to 448448), with some tasks even performing better at lower resolutions. This suggests that the structural patterns captured by vision models are robust to resolution changes, and extremely high resolution may not be necessary for graph structural understanding tasks. Table 5: Computational cost analysis across all benchmark tasks, showing time per epoch and time to best validation accuracy. Topology Spectral Gap Symmetry Bridge Time/Epoch (s) Time to Best (s) Time/Epoch (s) Time to Best (s) Time/Epoch (s) Time to Best (s) Time/Epoch (s) Time to Best (s) Model ConvNeXt ResNet Swin ViT GAT+SPE GCN+SPE GIN+SPE GPS+SPE Avg. Vision Avg. GNN+SPE Ratio (V/G) 13.1 5.5 11.5 23.1 1.2 1.2 1.1 1.4 13.3 1. 10.9 329.5 120.7 368.9 509.9 43.2 45.1 47.0 26.9 332.2 40.6 8.2 9.4 4.2 8.2 15. 0.9 0.8 0.8 1.0 9.4 0.9 10.7 245.2 76.5 123.1 127.0 16.3 5.3 6.1 22.0 142.9 12. 11.5 11.2 4.8 9.8 19.5 1.1 1.0 1.1 1.2 11.3 1.1 10.3 573.6 127.5 227.1 858. 2.5 93.3 56.0 34.9 446.7 46.7 9.6 13.2 5.6 11.6 23.2 1.2 1.2 1.2 1.5 13.4 1. 10.5 717.7 312.0 348.2 1139.7 107.7 35.1 30.6 32.7 629.4 51.5 12.2 Table 6: Model scaling analysis across all benchmark tasks. Scaled GPS+SPE models show degraded performance compared to baseline, confirming that architectural constraints rather than parameter count limit GNN performance. Model Near-OOD Far-OOD Task ID Topology (%) Symmetry (%) Spectral Gap (MAE) Bridge Count (MAE) Baseline GPS+SPE GPS+SPE (53.2M) GPS+SPE (212.2M) ResNet (25.6M) Baseline GPS+SPE GPS+SPE (53.2M) GPS+SPE (212.2M) ResNet (25.6M) Baseline GPS+SPE GPS+SPE (53.2M) GPS+SPE (212.2M) ResNet (25.6M) Baseline GPS+SPE GPS+SPE (53.2M) GPS+SPE (212.2M) ResNet (25.6M) 84.80 13.75 87.73 6.87 82.53 5.58 95.87 0.62 71.97 1.65 65.83 2.21 56.10 4.47 93.47 0.66 84.07 16.04 70.13 13.03 66.20 11.42 96.27 1.02 70.67 1.23 65.83 2.83 55.67 5.75 88.83 0.64 72.20 14.51 40.53 11.06 34.80 8.16 87.40 3. 67.70 1.37 67.63 3.94 54.97 4.53 84.20 0.39 0.0681 0.0298 0.1483 0.0210 0.1214 0.0365 0.0335 0.0021 0.6402 0.1753 1.4502 0.2315 2.1581 0.8106 0.7771 0.1095 0.1537 0.0839 0.1901 0.0167 0.2125 0.0373 0.0600 0.0063 1.4666 0.0713 3.0053 0.5616 3.3334 0.7540 1.6356 0.1643 0.6716 0.2709 0.7497 0.4243 0.9101 0.5625 0.1102 0. 3.8021 1.0492 5.6101 0.8129 5.7994 1.1003 3.6814 0.1217 Table 7: Comparison with I²-GNN across all benchmark tasks. I²-GNN shows strong symmetry detection but poor scale generalization. Task Topology (%) Symmetry (%) Model I²-GNN Best Vision I²-GNN Best Vision ID Near-OOD Far-OOD 94.67 1.49 95.87 0. 90.80 3.49 94.03 1.04 87.87 4.90 97.73 0.57 84.00 2.53 91.03 0.56 63.33 5.83 90.33 4.60 83.40 3.56 85.67 1.06 Spectral (MAE) I²-GNN Best Vision 0.1044 0.0091 0.0279 0.0047 0.3315 0.0941 0.0578 0.0056 0.7893 0.2709 0.0946 0.0094 Bridge (MAE) I²-GNN Best Vision 0.4580 0.1131 0.6261 0.0702 1.0459 0.1523 1.6338 0.1675 3.7353 1.0961 3.6814 0.1217 Table 8: Impact of image resolution on vision model performance (ResNet on Symmetry classification). Resolution ID Near-OOD Far-OOD 448448 224224 128128 6464 448448 224224 128128 6464 Kamada-Kawai Layout 80.87 1.80 84.30 0.36 84.50 0.46 86.20 0. 76.27 2.99 81.07 0.56 82.73 0.90 81.03 1.50 72.60 4.15 75.60 2.19 74.77 1.60 72.73 2.22 Spectral Layout 93.97 0.96 93.17 1.15 93.53 0.69 91.97 1.27 91.53 0.71 89.47 0.46 88.30 0.40 87.17 1.48 85.93 0.23 83.20 0.97 81.23 0.88 80.93 1. 27 Discussion: Visual vs. Message-Passing Paradigms In this section, we provide an informal analysis of why vision models achieve strong performance on graph structural understanding despite lacking graph-specific inductive biases. While we lack formal theoretical proofs, several observations from our communitys own research practices offer intuitions about the complementary computational mechanisms underlying these approaches. Different Computational Paradigms. The two paradigms solve fundamentally different problems. GNNs take graph structure (adjacency matrices, edge lists) as input and build understanding through iterative local message passing. Layout algorithms, in contrast, perform global computations upfront: eigendecomposition for spectral layouts, energy minimization for force-directed methods. Once graphs are rendered as images, the task transforms from graph analysis to visual pattern recognition. Vision models then process geometric patterns where structural properties manifest as directly observable visual features: symmetric graphs produce symmetric layouts, clustered graphs show dense regional connections, bridges appear as narrow connectors between substructures. Evidence from Known GNN Limitations. This difference becomes evident when examining tasks where GNNs face theoretical limitations. When researchers construct counterexamples for the Weisfeiler-Lehman test and its k-dimensional variants, they invariably use graph visualizations to illustrate why graphs are non-isomorphic despite fooling the WL algorithm [61]. Visual representations make structural distinctions immediately apparent that iterative refinement procedures miss. Horn et al. [29] explicitly present datasets containing graphs they describe as easily distinguished by humans visually. Their NECKLACES dataset shows graphs with identical cycle counts but different connectivity patterns: two individual cycles versus merged one. While humans immediately see this difference, standard message-passing approaches fail to distinguish them, requiring sophisticated persistent homology calculations. Similarly, Zhang et al. [79] proved that standard GNNs cannot identify bridges, yet these structures manifest as obvious visual bottlenecks in graph layouts. These observations suggest that layout algorithms and vision models provide complementary pathway to graph understanding: layout algorithms convert abstract topological properties into spatial patterns through global computation, while vision models recognize these geometric features through hierarchical processing. This explains why vision models maintain strong performance despite lacking graph-specific inductive biases: they access structural information through fundamentally different computational mechanism than message passing."
        },
        {
            "title": "H Visualization Examples from GraphAbstract",
            "content": "In this section, we provide visualizations of representative graphs in our benchmark. Figures 68 illustrate the diverse topological patterns across the four tasks: topology classification, symmetry classification, and spectral gap regression. (a) Cyclic (b) Random geometric (c) Hierarchical hub (d) Community (e) Bottleneck (f) Multi-core Figure 6: Training examples of topology classification. For bridge counting task, similar graph structures are used except for the cyclic structure, as bridge counting focuses on identifying edges whose removal would disconnect the graph. (a) Cayley cyclic (b) Bipartite double cover (c) Cartesian product (d) Cartesian w/ real (S) (e) Real data cover (f) Cartesian w/ real (NS) (g) Perturbed graph Figure 7: Training examples of symmetric classification. (a)-(e) are symmetric graphs, while (f)-(g) are asymmetric graphs. S: symmetric, NS: non-symmetric. 29 (a) SBM-DB, λ2 = 0.04 (b) SBM-DB, λ2 = 0. (c) SBM-M, λ2 = 0.01 (d) SBM-M, λ2 = 0.25 (e) Geo, λ2 = 0.12 (f) Geo, λ2 = 0.16 (g) Cfg, λ2 = 0.62 (h) Cfg, λ2 = 0. Figure 8: Training examples of spectral gap regression, where λ2 is the second smallest eigenvalue of the normalized Laplacian. SBM-DB: SBM dumbbell, SBM-M: SBM multi-community, Geo: Geometric, Cfg: Configuration model."
        },
        {
            "title": "I Analysis of Graph Layout Algorithms",
            "content": "I.1 Layout Algorithm Details Graph layout algorithms aim to produce 2D (or 3D) representations of graphs that are interpretable and reveal underlying structures. Different algorithms employ distinct heuristics and optimization criteria, leading to varied visual outputs. In this work, we primarily consider four common layout types: Kamada-Kawai. This is force-directed algorithm that models the graph as system of springs. It aims to position nodes such that the geometric distance between them in the layout is proportional to their graph-theoretic distance (shortest path length). This often results in aesthetically pleasing layouts that emphasize the overall shape and connectivity. ForceAtlas2. Another popular force-directed algorithm, particularly well-suited for larger graphs. It simulates attraction forces between connected nodes and repulsion forces between all nodes, often effectively revealing clusters and community structures within the network. Spectral Layout. This method uses the eigenvectors of the graph Laplacian (or related matrix) as coordinates for the nodes. Typically, the eigenvectors corresponding to the smallest non-zero eigenvalues are used. Spectral layouts are mathematically principled and often highlight global symmetries and Cheeger-type cuts. characteristic feature is that structurally equivalent or highly similar nodes can overlap in the visualization. Circular Layout. One of the simplest layout algorithms, it places all nodes equidistant on the circumference of circle. The ordering of nodes around the circle can be arbitrary, based on node IDs, or determined by other properties like node degree. I.2 Visualizing Asymmetric Graphs To illustrate how different layout algorithms can affect the visual perception of graph properties, particularly symmetry and structural regularity, we present visualizations of non-symmetric graph in Figure 9. This graph is generated via the Cartesian product of two distinct real-world graphs, resulting in structure with high local regularity but no global symmetry. As detailed in the caption, force-directed layouts (Kamada-Kawai and ForceAtlas2) tend to faithfully represent the resulting complex structure. Consequently, to determine its asymmetry, one might need to mentally deconstruct the layout to infer the properties of the underlying, distinct base graphs and recognize that their product would not yield simple visual symmetry. In contrast, the spectral and circular layouts render the graph such that its lack of global symmetry is more immediately visually evident. Figure 9: Visualizations of non-symmetric graph generated by the Cartesian product of real-world base graphs. Despite its high structural regularity, this graph lacks symmetry. In Kamada-Kawai (first) and ForceAtlas2 (second) layouts, determining asymmetry requires mentally reconstructing the base graphs and analyzing their properties. In contrast, the Spectral layout (third) reveals the grid-like product structure. However, critical visual cues about its asymmetry are evident in the lack of perfect geometric regularity and the varying thickness/brightness of lines due to edge overlap. These imperfections prevent global geometric symmetry axis (like vertical line through the center) from mapping the graph onto itself, serving as clear visual signals of the underlying asymmetry. Circular layout (fourth) displays nodes on circle and visually emphasizes the lack of uniformity in edge distribution, also clearly indicating the absence of symmetry. Both Spectral and Circular layouts facilitate more direct visual assessment of asymmetry. 31 I.3 Characteristics and Performance of Circular Layout We present the performance of vision models using circular-layout generated graph visualization in Table 9. Despite circular layouts significantly disrupting many visual topological properties, evidenced by the sharp performance drop on Topology classification in Far-OOD settings, they prove remarkably effective for tasks like symmetry detection. This effectiveness stems from how symmetry (or its absence) becomes visually apparent: for symmetric graphs, edge densities are balanced across the circle, while for asymmetric graphs, humans can readily perceive uneven edge densities or specific edges that break the expected uniform pattern. This allows both humans and models to assess symmetry without needing to discern complex topological features that the circular layout inherently obscures. Table 9: Performance comparison of circular layout across different tasks and models Task/Model Swin ConvNeXtV2 ResNet ViT ID Near-OOD Far-OOD 92.00 0.56 81.13 1.09 58.93 2.17 ID Near-OOD Far-OOD 85.17 1.12 85.07 1.03 83.67 0.73 Topology 90.89 1.29 79.22 2.01 56.22 4. Symmetry 85.08 1.25 83.67 1.33 82.50 1.00 Spectral 92.27 1.00 75.40 2.35 47.87 5.46 91.47 0.98 80.33 2.80 59.13 3.31 86.07 0.95 82.17 1.05 76.00 2. 83.87 0.88 84.53 0.87 80.13 1.86 ID Near-OOD Far-OOD 0.0503 0.0030 0.1330 0.0111 0.2090 0.0141 0.0454 0.0017 0.1503 0.0149 0.2325 0.0127 0.0627 0.0057 0.1569 0.0078 0.2296 0.0083 0.0538 0.0020 0.1457 0.0139 0.2194 0. Bridge ID Near-OOD Far-OOD 1.1834 0.1239 2.5184 0.2428 6.1317 0.2053 1.1461 0.0842 2.8263 0.1644 6.7032 0.1245 1.2615 0.0825 2.8143 0.1015 6.8447 0.1495 1.2484 0.0609 2.3876 0.1618 5.8798 0."
        },
        {
            "title": "J Preliminary Experiments and Implementation",
            "content": "J.1 Implementation Details of Preliminary Experiments We implemented eight neural architectures adapted to graph data. For GNNs, we used GCN, GIN, GAT, and GPS, with 25 layers, 128 hidden units, ReLU activation, dropout with 0.5, and global mean pooling. GIN and GPS included MLPs in each block, GAT used multi-head attention, and GPS combined local GIN aggregation with global attention. For vision models (ResNet-50, ViT-B/16, Swin-Tiny, ConvNeXtV2-Tiny), graphs were converted to 2D image representations (e.g., adjacency layouts or distance matrices). Models were initialized with ImageNet-1K weights, classification heads replaced by MLPs, and inputs resized to 224224. All dataset splits were generated using seed 0, with experiments conducted across five random seeds [0, 1, 2, 3, 4] for robust evaluation. Models were trained using Adam optimizer (learning rate 5e 6, weight decay 1e 4) with batch size 64 and early stopping. For early stopping, the patience settings are set as: 30 epochs for all GNN models, and for vision models, 5 epochs on PROTEINS and 15 epochs on all other datasets. J.2 Training dynamics and confidence Figures 1013 present training dynamics across four datasets, showing model differences in convergence and generalization. Figures 14 and 15 display confidence distributions for two representative datasets, one from the biological domain (PROTEINS) and one from social networks (IMDBBINARY), highlighting the contrast between GNNs and vision models in prediction certainty. Figure 10: Training dynamics across different architectures on PROTEINS datasets. For each model, we plot the training loss (blue), training accuracy (red), and validation accuracy (green) over 100 epochs. The shaded areas represent the standard deviation across multiple runs. Figure 11: Training dynamics across different architectures on IMDB-BINARY datasets. For each model, we plot the training loss (blue), training accuracy (red), and validation accuracy (green) over 100 epochs. The shaded areas represent the standard deviation across multiple runs. Figure 12: Training dynamics across different architectures on IMDB-MULTI and enzymes datasets. For each model, we plot the training loss (blue), training accuracy (red), and validation accuracy (green) over 100 epochs. The shaded areas represent the standard deviation across multiple runs. Figure 13: Training dynamics across different architectures on ENZYMES dataset. For each model, we plot the training loss (blue), training accuracy (red), and validation accuracy (green) over 100 epochs. The shaded areas represent the standard deviation across multiple runs. 34 Figure 14: Confidence distribution across different model architectures on the PROTEINS dataset. The first two rows show results from the training set, while the last two rows present the test set. Vision models demonstrate strong tendency toward high-confidence predictions (0.8-1.0) in both splits, while traditional GNNs typically make lower-confidence predictions. The GPS model, featuring global message passing, uniquely exhibits high-confidence predictions among GNN variants. Figure 15: Confidence distribution across different model architectures on the IMDB-BINARY dataset. The first two rows show results from the training set, while the last two rows present the test set. 36 J.3 Case Studies To understand how different models make decisions in graph classification, we explore interpretability methods for both GCN and ConvNeXtV2 as examples. For GCN, we used the GNNExplainer to identify important substructures within input graphs. This approach optimizes masks over edges to highlight influential connections for classification decisions, with visualizations created using NetworkX to display node and edge importance. Results across different model depths were compared side by side. For ConvNeXtV2, we applied Grad-CAM-based approach by registering hooks on target layers to extract activation maps and gradients. The resulting class-specific heatmaps were overlaid on the input images and uniformly displayed to highlight attention differences between models. Figures 1619 illustrate additional explanation results for GNNs (via GNNExplainer) and vision models (via Grad-CAM) in the first three samples in the testing set of four datasets. To delve deeper into how these models utilize underlying graph structures for prediction, especially in the presence of known important features, we highlight specific case study on the ENZYMES dataset. Amongst the datasets considered in our study, ENZYMES offers unique opportunity for this detailed interpretability analysis, as prior work by [12] has already identified and characterized discriminative pattern for it. This pre-existing knowledge allows us to assess alignment with established important features. Figure 16 presents this analysis: Figure 16: Comparison of GNN explainer (left) and Vision Model Grad-CAM (right) on ENZYMES dataset. The discriminative pattern, defined in [12] as square with two diagonal connections that appears in >90% of graphs within one class but <10% in others, is are key feature for classification. In the top row, where two discriminative patterns exist (one at each end of the graph), while both approaches identify these patterns, the Vision models Grad-CAM shows particularly sharp focus on them at high-level features, compared to the GNNExplainers more uniform attribution. In the middle row, the Vision model effectively highlights multiple discriminative patterns near cut-vertices and cut-edges where the graph structure narrows. In contrast, the GNNExplainer shows scattered attention without emphasizing these critical patterns. The bottom row, containing no pre-defined discriminative patterns, demonstrates the different attention patterns of both approaches on noncharacteristic structures. These results suggest that Vision models have learned to effectively leverage these discriminative patterns as reliable shortcuts for classification, while GNNExplainers maintain relatively uniform attention distributions. 37 Figure 17: Case Studies for NCI1 dataset. Figure 18: Case Studies for IMDB-BINARY dataset. 38 Figure 19: Case Studies for IMDB-MULTI dataset. J.4 Heatmap In this section, we present detailed prediction overlap analysis for all evaluated models across five benchmark datasets. Figures 2024 illustrate the prediction overlap patterns between GNN models of varying depths (1-6 layers) and vision-based models. Consistent with our main findings, the results show high intra-family similarity among GNNs regardless of layer depth, while maintaining distinctly different prediction patterns compared to vision models. (a) ENZYMES (1 layers) - Correct (b) ENZYMES (2 layers) - Correct (c) ENZYMES (3 layers) - Correct (d) ENZYMES (4 layers) - Correct (e) ENZYMES (5 layers) - Correct (f) ENZYMES (6 layers) - Correct (g) ENZYMES (1 layers) - Error (h) ENZYMES (2 layers) - Error (i) ENZYMES (3 layers) - Error (j) ENZYMES (4 layers) - Error (k) ENZYMES (5 layers) - Error (l) ENZYMES (6 layers) - Error Figure 20: Prediction overlap patterns for ENZYMES dataset with varying GNN layer depths. Top row shows correct prediction overlap, while bottom row shows error overlap patterns across different layer configurations. 40 (a) NCI1 (1 layers) - Correct (b) NCI1 (2 layers) - Correct (c) NCI1 (3 layers) - Correct (d) NCI1 (4 layers) - Correct (e) NCI1 (5 layers) - Correct (f) NCI1 (6 layers) - Correct (g) NCI1 (1 layers) - Error (h) NCI1 (2 layers) - Error (i) NCI1 (3 layers) - Error (j) NCI1 (4 layers) - Error (k) NCI1 (5 layers) - Error (l) NCI1 (6 layers) - Error Figure 21: Prediction overlap patterns for NCI1 dataset with varying GNN layer depths. Top row shows correct prediction overlap, while bottom row shows error overlap patterns across different layer configurations. (a) PROTEINS (1 layers) - Correct (b) PROTEINS (2 layers) - Correct (c) PROTEINS (3 layers) - Correct (d) PROTEINS (4 layers) - Correct (e) PROTEINS (5 layers) - Correct (f) PROTEINS (6 layers) - Correct (g) PROTEINS (1 layers) - Error (h) PROTEINS (2 layers) - Error (i) PROTEINS (3 layers) - Error (j) PROTEINS (4 layers) - Error (k) PROTEINS (5 layers) - Error (l) PROTEINS (6 layers) - Error Figure 22: Prediction overlap patterns for PROTEINS dataset with varying GNN layer depths. Top row shows correct prediction overlap, while bottom row shows error overlap patterns across different layer configurations. 42 (a) IMDB-BINARY (1 layers) - Correct (b) IMDB-BINARY (2 layers) - Correct (c) IMDB-BINARY (3 layers) - Correct (d) IMDB-BINARY (4 layers) - Correct (e) IMDB-BINARY (5 layers) - Correct (f) IMDB-BINARY (6 layers) - Correct (g) IMDB-BINARY (1 layers) - Error (h) IMDB-BINARY (2 layers) - Error (i) IMDB-BINARY (3 layers) - Error (j) IMDB-BINARY (4 layers) - Error (k) IMDB-BINARY (5 layers) - Error (l) IMDB-BINARY (6 layers) - Error Figure 23: Prediction overlap patterns for IMDB-BINARY dataset with varying GNN layer depths. Top row shows correct prediction overlap, while bottom row shows error overlap patterns across different layer configurations. 43 (a) IMDB-MULTI (1 layers) - Correct (b) IMDB-MULTI (2 layers) - Correct (c) IMDB-MULTI (3 layers) - Correct (d) IMDB-MULTI (4 layers) - Correct (e) IMDB-MULTI (5 layers) - Correct (f) IMDB-MULTI (6 layers) - Correct (g) IMDB-MULTI (1 layers) - Error (h) IMDB-MULTI (2 layers) - Error (i) IMDB-MULTI (3 layers) - Error (j) IMDB-MULTI (4 layers) - Error (k) IMDB-MULTI (5 layers) - Error (l) IMDB-MULTI (6 layers) - Error Figure 24: Prediction overlap patterns for IMDB-MULTI dataset with varying GNN layer depths. Top row shows correct prediction overlap, while bottom row shows error overlap patterns across different layer configurations."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: In Section 3 and 4.4, we provide comprehensive experimental evidence and case studies to support our claims. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Appendix A, We discuss the limitations. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: In Section E, we provide the full set of assumptions and complete proof. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We present the implementation detail in Appendix and also submit the code in the supplemental material. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? 46 Answer: [Yes] Justification: The code for generating the datasets and running the experiments will be submitted via the supplementary material. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We state the experimental setup in Section 4.4 and Appendix D. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments were run multiple times using multiple random seeds, with the final mean and standard deviation reported. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). 47 It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide sufficient information on the computer resources in Appendix D. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We make sure that the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact on the work performed. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 48 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Properly credited. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 49 If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We will release the code. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage 50 Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: LLM is used only for writing. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Cheriton School of Computer Science, University of Waterloo",
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
    ]
}