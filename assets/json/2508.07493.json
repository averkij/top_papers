{
    "paper_title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding",
    "authors": [
        "Jian Chen",
        "Ming Li",
        "Jihyung Kil",
        "Chenguang Wang",
        "Tong Yu",
        "Ryan Rossi",
        "Tianyi Zhou",
        "Changyou Chen",
        "Ruiyi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 9 4 7 0 . 8 0 5 2 : r VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding Jian Chen1*, Ming Li2, Jihyung Kil3, Chenguang Wang, Tong Yu3 Ryan Rossi3, Tianyi Zhou2, Changyou Chen1, Ruiyi Zhang3 University at Buffalo1, University of Maryland2, Adobe Research3 ryzhang.cs@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Most organizational data in this world are stored as documents, and visual retrieval plays crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on single-page image. To bridge this gap, we introduce VisRBench1, multilingual benchmark designed for questiondriven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval. 1. Introduction Retrieval-Augmented Generation (RAG) systems powered by Multimodal Large Language Models (MLLMs) [1, 13, 44, 46] have recently gained widespread attention in visionand-language. In particular, retrieving accurate and relevant information from long documents poses unique challenges, as the systems should understand diverse structured content (e.g., tables, catalogs, figures) and capture complex document layouts [9, 26, 35, 40]. Several MLLM-based retrieval *work done when JC is at University at Buffalo. 1Code is available at https://github.com/puar-playground/VisR-Bench Figure 1. Given PDF document, we create VisR-Bench by extracting text into Markdown files and visual elements into separate images for each page. The extracted content is used to generate queries for retrieving the corresponding evidence page. During testing, retrieval model identifies relevant pages, and question answering model then uses these page images to answer the queries. models [6, 12, 17, 45] have been introduced to address the challenges, yet their effectiveness remains largely untested. Existing retrieval benchmarks [10, 23, 28, 36, 39] fall short in evaluation since they rely too heavily on text-image similarity rather than on Question-Answer (QA) relevance. An effective retrieval benchmark should require models to locate the information in the document that is relevant to the query and the answer, not simply retrieving the visual content with the highest similarity to the query. For example, given the query, When does the first train leave in the morning?, the relevant information should be retrieved from train schedule table, not from an image of train, despite 1 the latter having the highest visual similarity to the query. In other words, the ideal retrieval benchmarks should go beyond surface-level similarities and incorporate deeper semantic and layout understanding. Besides, most prior Visual Question Answering (VQA) datasets [21, 26, 28, 38] focus on QA tasks, assuming that the correct evidence image is provided. However, real-world retrieval scenarios are often more complex, not providing single evidence page but instead documents with hundreds of images. Another key challenge lies in the multilingual setting, especially in multimodal scenarios. For instance, most existing multilingual benchmarks focus on text-only document retrieval [9, 22, 35, 40], offering limited insights into multimodal retrieval performance. Likewise, most prior multimodal retrieval benchmarks rely on the English-only setting [8, 10, 23] without considering other languages. These limitations further hinder comprehensive evaluation of MLLMs retrieval capabilities. To address these challenges, we propose VisR-Bench, the first question-driven multilingual Visual Retrieval benchmark designed to evaluate MLLM retrieval performance in visually rich document images. VisR-Bench consists of 53K high-quality synthetic QA pairs and 1,286 documents (373 English and 913 multilingual), with an average length of about eighteen pages. By generating QA pairs for different evidence types, tables, figures, and visual text, our benchmark enables granular performance analysis in multimodal reasoning, OCR robustness, and table understanding. As illustrated in Figure 2, we collect multimodal documents from sixteen languages, such as English and Italian, allowing for the assessment of language-specific weaknesses in existing retrievers and an extensive evaluation of multilingual retrieval abilities. Additionally, English documents are categorized into ten document types (e.g., newsletter, magazine). We summarize our key contributions as follows: We introduce VisR-Bench, benchmark that systematically evaluates the retrieval capabilities of MLLMs in multilingual and multimodal settings. Our dataset spans sixteen languages and encompasses diverse document types. We evaluate wide range of retrieval models, including text-based methods, multimodal encoders, and large MLLMs, providing quantitative insights into retrieval performance across different evidence types and languages. We show that MLLMs significantly outperform text-based and vision-language encoder models but still struggle with structured documents and low-resource languages, revealing language-specific and layout-specific challenges and providing insights for improving MLLMs. 2. Related Work Text-based Retrieval Methods Traditional text-based retrieval methods extract text from images using OCR tools Figure 2. Distribution of language and document types in VisRBench. The blue colors represent the English multimodal split, which can further be categorized into ten document types. Other colors represent the multilingual multimodal split, containing documents in fifteen non-English languages. [11, 33] and apply text-based retrieval techniques. BM25 [31] is statistical algorithm based on text frequency. Deep learning models such as Sentence-BERT [30] and BGE Models [4, 41] enable semantically aware search. NV-Embed [20], built upon LLMs (e.g., Mistral 7B [16]), generates text embedding to enhance retrieval accuracy by capturing richer contextual information. However, these approaches struggle with complex layouts and cannot process visual elements, limiting their performance in real-world applications. Multimodal Retrieval Methods Multi-modal encoders like CLIP [29] and SigLIP [43] can be used for image retrieval by similarity in shared embedding space, but they are optimized for natural images rather than document pages. With the advent of MLLMs, recent approaches customize MLLMs as encoders, leveraging their pre-trained knowledge for improved accuracy. VLM2Vec [17] and GME [45] compute similarity using single-vector embeddings, while ColPali [12], ColPhi [6], and ColInternVL2 [6] utilize sequences of hidden states and apply sequence interaction scoring [18] for more effective relevance estimation. Comparison with Multi-page Datasets Existing multipage document datasets focus on domain-specific documents, such as SlideVQA [35], SciMMIR [40], and MMVQA [9]. Wiki-SS [22] emphasizes text-based evidence. DocMatix [19] contains noisy and ambiguous queries, and CVQA [32] is limited to single natural images, making it unsuitable for document retrieval. Additionally, MMLongBench-Doc [23], MMDocIR [10], and M-LongDoc [8] are English-only, limiting the multilingual applicability."
        },
        {
            "title": "Benchmark",
            "content": "Multi-page Multiligual QA Retrieval Text Table Figure Doc # Question # M-LongDoc [8] SlideVQA [35] CVQA [32] SciMMIR [40] MMVQA [9] DocMatix [? ] MMLongBench-Doc [23] MMDocIR [10] DocVQA [27] ChartVQA [24] InfographicVQA [25] ViDoRe [12] VisR-Bench 300 2,619 5,239 530,000 3,146 2,444,750 135 313 12,767 21,945 5,400 8,310 10,070 14,484 10,374 530,000 262,928 9,500,000 1,082 1,658 50,000 32,719 30,000 3,810 1,286 35,571 Table 1. Comparison with existing benchmarks. VisR-Bench is the first work on multi-page multilingual documents. Figure 3. Boxplot of document length distribution. Each box represents the inter-quartile range (IQR), covering the middle 50% of the data. The horizontal line inside each box indicates the median document length, while the whiskers extend to the minimum and maximum values within 1.5 times the IQR. The dashed vertical line separates English multimodal split (left) from multilingual multimodal documents (right). 3. VisR-Bench The VisR-Bench is divided into an English Multimodel Split (English only) curated from web-crawled data and Multilingual Multimodel Split (15 non-English languages) filtered from the CCpdf dataset [37]. As demonstrated in Table 1, VisR-Bench is comprehensive benchmark that integrates multiple essential features of multilingual visual retrieval tasks. Unlike existing benchmarks, VisR-Bench simultaneously supports multilingual documents while being suitable for evaluating retrieval functionalities across diverse content types, including text, tables, and figures. Its large scale, spanning 16 languages, 1,286 documents, and 35,571 questions, makes it robust resource for developing and evaluating models capable of handling complex, real-world document analysis challenges. 3.1. Data Sourcing We start from large-scale, diverse, multilingual corpus of PDF files from all over the Internet using Common Crawl [37]. All documents are extracted using document parser2. We excluded documents with PDF or quality issues and obtained 301,553 documents. The document parser outputs markdown files, including texts, tables, and figures for each document page in an interleaved manner. All figures are saved separately as images with path reference in mark2Adobe Document-Extract-API: https://developer.adobe.com/documentservices/apis/pdf-extract/ 3 down files. Document extraction examples are provided in Figure 4. We will release all these markdown files for future document research. categorize all questions into figure-related, text-related, and table-related questions, and use different prompting strategies using GPT-4o [14] to curate questions. 3.2. English Multimodel Split To construct the English multimodel split, we select about 4, 000 PDF documents from the filtered ccPDF documents, which are from 38 types of documents based on human annotations. To ensure focus on multimodal content, we select ten types of documents that are visually rich, such as product manuals and presentations. Further filtering is applied to retain only English documents that contain both Markdown files and informative figures while excluding single-page documents, as retrieval is unnecessary for them. The final dataset includes 210 table-rich documents, 310 text-rich documents, 125 figure-rich documents, and 913 multilingual documents, ensuring balanced evaluation across different retrieval types. Figure 2 and Figure 3 show the distribution of the types of human-annotated documents and their average lengths, highlighting greater diversity than previous benchmarks [15, 23, 35]. To select documents with informative figures, we apply figure classification on the extracted images using the CLIP model ViT-L/14-336 [29]. Each figure is classified into one of 19 predefined categories, and we retain 6 relevant types while discarding decorative figures such as logos and banners. After filtering, the multimodal evaluation split is refined to 373 unique documents. All documents have been validated by human reviewers to ensure the exclusion of harmful content and personally identifiable information (PII). Furthermore, we confirm that the license and usage terms of each document explicitly allow its use for research purposes. To build multimodal document retrieval benchmark, we Figure-related QA We combine the figures with their corresponding contexts and use GPT-4o (API version 202408-15) to generate QA pairs. For prompt construction, we provide two demonstrations and instruct GPT-4o to generate new QA pair. To ensure that the figures are necessary to answer the questions, we apply heuristic filtering step: we discard any question that GPT-4o can already answer using only the textual information extracted from the Markdown files, as shown in Figure 4. This process not only enforces reliance on visual content, but also serves as an additional validation step for the correctness of the generated answers. In contrast, although many existing benchmarks in Table 1 include figure-based questions, they do not isolate them or verify whether the figure is actually required to answer. Text-based QA To generate text-based QA pairs, we first filter pages that contain only text in the extracted Markdown files, excluding those with tables or figures to ensure sole focus on textual information. We then use GPT-4o to generate QA pairs over the given page. We design system prompt to enforce key constraints: (1) Questions must simulate realistic retrieval scenario where user queries multi-page document for relevant information. (2) Answers must be explicitly present in the text to prevent hallucination. (3) Questions should not be ambiguous or overly broad, such as asking for the page number or requiring document-level summarization. (4) If page lacks sufficient content for meaningful questions, the model returns an empty string instead of generating forced or unnatural queries. Figure 4. Overview of our QA data generation pipeline. Text and visual content are first extracted from documents, with regular text and tables saved as Markdowntables are preserved in structured text format using Markdown table syntaxwhile figures are saved as separate image files. For tableand text-based QA, we prompt GPT-4o using the extracted Markdown content. For figure-based QA, we first filter out decorative figures using CLIP-based classifier, then generate figure-centered questions by prompting GPT-4o with only the image. To ensure that figures are truly required for answering, we revise the QA pairs by incorporating surrounding text and apply heuristic filtering step: any question that GPT-4o can already answer using only the Markdown text is discarded. This ensures that the final figure-based QA pairs require both visual and textual information for accurate retrieval and comprehension. Table-related QA Similar to text-based QA, we extract pages that contain tables but no figures to ensure that the generated questions are not influenced by visual elements. This guarantees that the QA pairs focus solely on tabular data and its text context. In addition to the constraints applied to text-based QA, table-related questions are designed to require computation or logical inference rather than simple fact lookup. Instead of directly extracting single value, the questions encourage tasks such as analyzing trends, making comparisons, identifying rankings, or interpreting correlations within the table data. This ensures that retrieval models must engage in structured reasoning. 3.3. Multilingual Multimodel Split Our dataset includes multilingual queries over documents in 15 non-English languages, including Spanish, Italian, German, French, Dutch, Arabic, Croatian, Japanese, Swedish, Vietnamese, Portuguese, Finnish, Czech, Slovenian, and Danish, as shown in Figure 2. These fifteen languages are selected based on our filtered documents, with the number of associated documents more than 500. This subset is designed to assess the accuracy of the retriever in diverse linguistic landscape. The queries are general questions generated by GPT-4o, conditioned on text, tables, and figures. To simplify human inspection of the generated QA pairs, we curated prompt to generate questions in both English and another language. Detailed prompts are provided in the Appendix B. Multilingual Finetuning Data Previous retriever models [6, 12] were typically fine-tuned on data biased toward English, which may lead to reduced performance in other languages. To investigate whether multilingual training could improve performance, we scaled the data generation process described above to produce larger multilingual dataset for fine-tuning, containing 210k QA pairs and 39.5k documents. 4. Experimental Results 4.1. Evaluation Suite Top-k Retrieval Accuracy Since all QA samples in the VisR-Bench dataset require single evidence page, we evaluate document retrieval using top-1 and top-5 Accuracy. This binary metric assigns score of 1 if the ground-truth evidence page appears in the first and top five retrieved results and 0 otherwise. The final accuracy is the percentage of samples with score of 1, directly measuring retrieval effectiveness in this setting. PNLS We adopt PNLS [5] as metric to evaluate the similarity between the model-generated answer and the ground truth. PNLS is variant of normalized Levenshtein similarity [3] that identifies an optimally aligned substring in the ground truth using dynamic programming. It then measures the edit distance between this substring and the modelgenerated answer, normalizing by the length of the aligned substring (including matches and gaps). This normalization ensures that concise responses are not unfairly penalized, making PNLS particularly suitable for evaluating long-form answers and cases where partial correctness matters. GPT Evaluation For long or complex answers, stringmatching metrics fail to provide accurate evaluation. Instead, we use GPT-based evaluation, binary metric where GPT compares the models answer with the ground truth. If they convey the same information, the sample receives score of 1, otherwise 0. The average score across samples is reported as GPT accuracy (GAcc), offering more reliable assessment beyond exact string matching. 4.2. Retrieval Results on English Split The retrieval performance of 14 different methods on the English English split is shown in Table 2. Retrieval methods are categorized into (1) Text-based Methods, (2) Multimodel Encoders, and (3) Multimodel Large Language Models. These results highlight the key trends in multimodal document retrieval, revealing the strengths and limitations of different retrieval approaches across figures, tables, and text. Below are some findings from our experimental results. Even the best method does not perform perfectly in VisRBench, indicating the difficulty of our benchmark and the substantial room for improvement. By examining the top-1 performances across different methods, we observe that even the best method, ColQwen23, can only reach 75.23% of average accuracy, while the performances are even worse on figure and table content retrieval. This phenomenon not only shows the difficulty of our benchmark but also indicates the large space for future models. Retrieval of table content is still challenging for multimodal encoders and MLLMs. Multimodal encoders and MLLMs-based methods consistently perform poorer on table content retrieval compared with text content and figure content, probably due to the different information processing process. This phenomenon indicates that structured tabular data still poses unique perceiving, understanding, and retrieval challenges that are not fully addressed by existing models. The results suggest that tables require specialized retrieval mechanisms beyond standard embeddings, emphasizing the need for better table-aware perceiving and retrieval techniques. MLLM-based methods outperform other methods consistently. Our results show that MLLM-based retrieval methods consistently outperform all other methods with large 3huggingface: https://huggingface.co/vidore/colqwen2-v0."
        },
        {
            "title": "Accuracy",
            "content": "top1 top5 top1 top5 top1 top"
        },
        {
            "title": "Average",
            "content": "top1 top5 Text-based Methods BM25 [4] SBERT [30] BGE-large [41] BGE-M3 [4] NV-Embed-v2 [20] Multimodal Encoders CLIP [29] SigLip [43] Multimodal Large Language Models VisRAG [42] VLM2Vec [17] GME [45] Col-InternVL2 [6] Col-Phi [6] ColPali-v1.2 [12] ColQwen2-v0.1 [12] 24.27 25.24 31.55 31.07 35.44 33.90 38.98 31.96 40.44 68.04 68.28 68.77 68.77 74. 45.63 49.27 56.07 56.80 65.05 61.74 69.73 66.83 76.27 91.53 90.31 93.22 91.77 95.64 38.58 26.31 40.36 51.11 44.04 24.68 24.73 19.82 28.51 61.50 63.85 65.65 66.12 67. 66.43 52.68 70.14 78.51 73.34 47.59 53.22 48.53 57.77 86.38 86.36 88.51 88.26 88.98 64.72 49.96 57.00 67.70 61.38 39.47 39.06 31.00 39.90 76.34 79.19 81.67 82.63 83. 89.10 76.97 82.68 89.89 87.46 70.21 70.97 61.49 71.69 95.62 96.45 97.04 96.89 97.61 42.52 33.84 42.97 49.96 46.95 32.68 34.26 27.59 36.28 68.63 70.44 72.03 72.51 75. 67.05 59.64 69.63 73.95 75.28 59.85 64.64 58.95 68.58 91.18 91.04 92.92 92.31 94.08 Table 2. Retrieval accuracy results on VisR-Bench(English split). Bold font indicates the best overall performance for each language. margin, demonstrating their advantage in end-to-end document understanding and retrieval. Specifically, ColQwen2 achieves the highest retrieval accuracies across figures, tables, and text. Despite identical data and protocols, it surpasses ColPali by 2%, suggesting that base-model pretraining quality plays key role in this task. Among the remaining MLLMs, ColPali and ColPhi perform comparably, while ColInternVL2 and GME underperform slightly. VisRAG and VLM2Vec perform poorly, likely due to their optimization for natural images rather than document structures. Meanwhile, without surprise, text-based methods perform promisingly well on text retrieval but struggle with figures and tables, confirming the limitations of text-only approaches in multimodal retrieval. Contextualized late interaction outperforms single-vector similarity. Although trained on massive image datasets, multimodal encoders such as CLIP and SigLip fall well behind MLLMs, suggesting that visionlanguage pretraining alone is insufficient and that deeper contextual reasoning, as enabled by contextualized late interaction, is crucial for effective multimodal retrieval. This distinction is further illustrated by the gap between multi-vector and single-vector embedding models: ColQwen2, multi-vector model built on the smaller Qwen2-VL-2B, significantly outperforms GME, single-vector model based on the larger Qwen2-VL7B. These results underscore that capturing finer-grained, context-dependent representations through late interaction can outweigh even substantial differences in base model size. LLM Retrievers Excel at Figures and Tables. As shown in the table, NV-Embed-v2a recent 7B LLM-based retrieverdemonstrates clear advantages over BM25 in figureand table-based QA. Although it operates solely on Markdown input, its strong performance in these settings may be attributed to its language modeling capabilities and pretrained knowledge of figureand table-related concepts. This enables it to infer implicit information even without direct visual input. These results highlight the potential of LLMbased retrievers to reason over semi-structured content by leveraging contextual cues and prior knowledge, particularly in scenarios where information is weakly grounded in text. 4.3. Retrieval Results on Multilingual Split Table 3 shows the retrieval performance of 15 different methods, including fine-tuned ColQwen2-v0.1 model on the multilingual training set described in section 3.3, evaluated across 15 non-English languages in the Multilingual English split. For clarity, we only present the average accuracy for each language without splitting them into different content sources. The performance on this multilingual split for the first time shows how different methods perform under this challenging multilingual multimodal retrieval scenario. Below are some findings from our experimental results. Most Methods Struggle on Low-Resource Languages. The retrieval accuracy results show clear gap in performance between different languages, particularly when comparing well-resourced languages like Spanish, Italian, and German to low-resource languages such as Arabic, Finnish, and Vietnamese. Across all model categories, text-based methods, multimodal encoders, and MLLMs, the accuracy scores drop significantly for low-resource languages. This indicates that despite the advances in current retrieval methods, language resource availability continues to play critical role in performance, and models still struggle to generalize well to underrepresented languages. This phenomenon highlights the importance of our benchmark. MLLMs and Encoders still face multilingual challenges. Despite recent progress, both MLLM-based retrieval methods (e.g., ColQwen) and multimodal encoders (e.g., CLIP and SigLIP) exhibit clear limitations in low-resource language settings. MLLMs perform well in some languages. 6 Spanish Italian German French Dutch Arabic Croatian Japanese top5 top1 top top1 top5 top1 top5 top1 top top1 top5 top1 top5 top1 top top1 11.14 13.08 60.25 22.77 34.55 58.16 42.92 Accuracy Text-based Methods BM25 SBERT BGE-large BGE-M3 NV-Embed-v2 Multimodal Encoders CLIP SigLIP Multimodal Large Language Models 9.70 VisRAG 18.59 VLM2Vec 60.57 GME 58.26 ColInternVL2 65.42 ColPhi 71.44 ColPali-v1.2 75.04 ColQwen2-v0.1 Finetuned on Multilingual Data 67.25 ColQwen2 (E) 69.77 ColQwen2 (M) 82.50 41.83 60.41 83.13 72.71 29.32 32. 28.48 44.48 88.08 84.57 89.00 92.62 94.34 90.60 92.48 59.14 21.82 30.27 52.94 40.84 82.02 41.12 56.24 77.96 66.32 65.82 25.74 39.75 67.64 52.23 86.92 48.54 66.82 88.94 80. 54.07 27.43 41.34 60.68 49.41 77.79 51.33 67.42 82.10 76.13 59.83 27.99 39.14 63.62 47.12 84.88 52.25 67.53 87.73 78.74 7.43 4.02 6.15 10.55 5.47 21.49 17.29 19.53 26.26 21. 52.98 17.72 32.67 59.07 41.86 72.71 36.67 58.14 81.46 68.30 11.59 13.06 31.92 58.38 42.17 38.60 41.24 64.97 84.33 72.70 12.39 17.52 31.77 40. 19.53 25.69 45.69 51.69 19.52 24.85 44.44 53.15 16.22 22.70 42.71 50. 4.64 5.53 18.91 19.56 10.46 13.98 27.36 33.56 14.28 15.62 44.86 46. 10.69 19.42 52.96 51.89 56.06 62.02 65.18 33.09 43.84 79.08 77.96 81.43 85.81 88.24 14.48 26.07 65.97 60.35 65.02 72.96 78.63 40.22 56.10 89.61 86.32 88.96 92.48 95.77 16.37 29.53 66.78 64.06 67.83 72.62 77.81 42.55 60.50 89.55 87.17 89.65 92.09 93. 15.22 22.51 57.92 58.27 62.15 65.15 70.30 42.02 52.97 85.16 84.60 88.17 89.73 92.12 4.78 7.39 15.33 5.09 8.46 14.33 12.05 19.80 24.10 35.72 17.50 25.95 32.59 27.16 6.38 12.31 45.09 47.68 48.83 51.54 55.27 22.25 32.04 72.60 73.16 74.82 76.94 79. 21.04 19.19 61.11 39.65 25.28 43.85 65.81 52.37 50.02 89.37 71.57 56.49 77.53 89.41 57.10 59.77 82.29 85.39 71.99 72.71 93.18 92. 72.01 72.70 91.39 92.16 60.26 64.37 86.57 89.10 10.15 11.67 26.70 28. 44.50 50.44 71.12 76.83 61.54 63.91 87.70 89.98 Swedish Vietnamese Portuguese Finnish Czech Slovenian Danish Average 17.38 26.78 57.44 28.26 42.18 65.25 53.02 Text-based Methods BM25 SBERT BGE-large BGE-M3 NV-Embed-v2 Multimodal Encoders CLIP SigLIP Multimodal Large Language Models VisRAG 14.93 VLM2Vec 25.98 GME 59.09 ColInternVL2 61.16 ColPhi 64.19 ColPali-v1.2 65.37 70.16 ColQwen2-v0.1 Finetuned on Multilingual Data 62.30 ColQwen2 (E) 66.12 ColQwen2 (M) 83.68 60.99 74.69 89.33 81.40 48.84 61.66 49.30 62.17 89.79 90.51 93.08 92.11 95. 91.37 94.05 48.81 17.94 23.94 44.93 25.75 73.01 37.07 48.97 68.82 60.24 61.47 25.85 38.53 60.07 56.98 79.92 50.24 66.08 82.16 80.34 50.11 23.34 31.58 56.90 34. 71.24 47.29 57.97 77.19 61.63 66.11 26.28 33.97 65.87 41.99 89.34 50.00 61.94 90.22 70.59 56.45 22.31 35.30 65.05 43.91 81.81 48.03 63.89 88.53 73.21 54.38 29.56 35.58 64.42 52. 82.35 58.11 71.02 88.38 79.48 52.38 22.05 33.33 56.25 42.03 74.82 43.98 59.75 79.34 69.63 6.67 8.38 22.13 25.13 16.75 25. 42.17 51.03 12.13 17.24 36.84 45.84 11.86 20.67 34.78 47.92 13.35 17. 36.29 43.28 13.77 23.53 45.48 54.38 13.40 17.87 35.60 41.87 5.53 8.22 26.22 25.75 34.28 35.32 35. 18.56 25.39 51.81 54.19 65.25 66.60 64.51 12.68 23.73 65.29 62.32 64.99 76.03 76.32 38.29 53.46 90.72 86.95 88.59 92.96 93.53 9.76 16.17 38.83 46.22 49.73 42.11 49.72 34.78 44.47 68.80 72.85 75.82 73.07 75.82 9.46 21.07 51.52 55.29 58.65 62.34 65. 34.13 50.56 82.29 85.90 88.86 91.27 92.57 8.69 15.59 51.34 54.75 56.81 55.82 61.67 34.50 44.09 80.91 83.96 85.66 86.29 88.98 13.77 25.39 54.52 60.11 61.12 62.41 72.31 46.34 58.68 86.94 90.10 90.67 90.10 94.76 11.61 19.72 53.85 51.29 54.71 60.00 62. 34.61 46.57 80.26 77.04 79.84 83.65 84.35 24.50 27.10 53.29 55.05 49.44 71.86 80.19 92.09 40.88 40. 68.43 70.04 52.87 61.32 84.12 89.63 49.44 58.43 80.19 89.35 63.92 64. 92.22 93.22 53.84 56.07 79.32 82.45 Table 3. Retrieval accuracy results on VisR-Bench(multilingual split). Bold font indicates the best overall performance for each language. ColQwen2 achieves the best accuracy in several casesbut are inconsistent overall, often being outperformed by textbased methods like BM25 and BGE-M3, especially in Czech and Portuguese. Meanwhile, CLIP and SigLIP consistently underperform across nearly all low-resource languages, with significantly lower top-1 accuracy compared to both MLLMs and text-only methods. These results suggest that neither current MLLMs nor multimodal encoders are robustly optimized for multilingual scenarios, highlighting the need for improved multilingual training and evaluation across both unimodal and multimodal retrieval systems. Text-based methods Beating MLLMs in Multilingual. Although MLLMs have shown promising results, text-based retrieval methods, especially those tailored for multilingual settings, remain competitive and, in some cases, superior. BGE-M3, multilingual text-based method, achieves the best performance in several languages, such as Finnish and Czech, surpassing all MLLM by considerable margin. Similarly, BM25, traditional text retrieval method, performs surprisingly well in languages like Vietnamese, outperforming many large models. Challenges of Arabic for Retrieval Models. Arabic remains one of the most challenging languages, with retrieval accuracy far below that of others. Even top models like ColQwen2 and GME, as well as text-based methods such as BM25 and BGE-M3, perform poorly. This may stem from its rich morphology, complex script, distinct syntax, and rightto-left reading order, which may require dynamic designs in attention masks and position embeddings. Addressing these issues could involve language-specific pretraining, improved tokenization, or dedicated architectural adaptations."
        },
        {
            "title": "Average",
            "content": "GPT-4o (all page) GPT-4o Paligemma2-3B Phi-4-multimodal InternVL2-4B Accuracy GAcc 0.53 0.48 0.03 0.10 0.75 PNLS 0.75 0.59 0.02 0.02 0.90 GAcc 0.57 0.55 0.11 0.34 0.33 PNLS 0.65 0.64 0.18 0.35 0.53 GAcc 0.85 0.84 0.43 0.51 0. PNLS 0.83 0.82 0.43 0.47 0.70 GAcc 0.65 0.62 0.19 0.32 0.58 PNLS 0.74 0.68 0.21 0.28 0.71 Spanish Italian German French Dutch Arabic Croatian Japanese Accuracy GAcc PNLS GAcc PNLS GAcc PNLS GAcc PNLS GAcc PNLS GAcc PNLS GAcc PNLS GAcc PNLS GPT-4o (all page) GPT-4o Phi-4-multimodal Paligemma2-3B InternVL2-4B 0.88 0.77 0.54 0.27 0.53 0.82 0.81 0.51 0.34 0.67 0.73 0.70 0.47 0.30 0. 0.84 0.80 0.40 0.33 0.60 0.74 0.80 0.38 0.26 0.41 0.74 0.80 0.37 0.30 0.60 0.86 0.73 0.53 0.39 0.61 0.85 0.85 0.49 0.35 0.75 0.68 0.73 0.36 0.28 0. 0.70 0.74 0.35 0.31 0.60 0.75 0.39 0.14 0.11 0.04 0.60 0.58 0.12 0.00 0.20 0.92 0.71 0.49 0.31 0.29 0.78 0.67 0.43 0.35 0.42 0.50 0.50 0.24 0.15 0. 0.61 0.51 0.24 0.19 0.29 Swedish Vietnamese Portuguese Finnish Czech Slovenian Danish Average GPT-4o (all page) GPT-4o Phi-4-multimodal Paligemma2-3B InternVL2-4B 0.82 0.70 0.30 0.26 0.37 0.90 0.73 0.26 0.30 0. 0.80 0.70 0.20 0.15 0.20 0.75 0.63 0.19 0.11 0.41 0.80 0.85 0.62 0.42 0.46 0.94 0.82 0.39 0.31 0.71 0.88 0.71 0.29 0.14 0.10 0.90 0.80 0.25 0.19 0. 0.60 0.69 0.44 0.38 0.12 0.90 0.70 0.33 0.25 0.32 1.00 0.75 0.12 0.19 0.06 1.00 0.77 0.21 0.26 0.32 1.00 0.69 0.31 0.23 0.15 0.91 0.82 0.31 0.26 0. 0.77 0.70 0.39 0.26 0.36 0.79 0.74 0.36 0.28 0.53 Table 4. Vision question-answering results on VisR-BenchEnglish split (upper), multilingual split (bottom). Bold font indicates the best overall performance for each language. Multilingual vs. English-Only Training To evaluate the impact of multilingual data, we compared three models: the original ColQwen2-v0.1, and two ColQwen2 models trained from Qwen2-VL-2B using the training set from the original paper4. ColQwen2 (E) was trained only on this set, while ColQwen2 (M) was trained on the same set combined with our multilingual data. As shown in Table 3, including multilingual data improved performance across multiple languages compared to training with English data alone. 4.4. Vision Question-Answering Results We benchmark answer generation performance of three opensource MLLMs, Phi-4-multimodal [2], Paligemma2-3B [34], and InternVL2-4B [7]. The results are reported in Table 4. For open-source models, we use ColPaliv1.2 to retrieve the most relevant page as the evidence page and perform inference on single page. For GPT-4o, we include baseline where the model is given all pages as input, providing an upper bound for retrieval-dependent models. Below are some findings that can be inferred from the experimental results. GPT-4o (all page) achieves the best overall performance. Across almost all content types except figures, GPT-4o (all page) consistently achieves the highest PNLS and GAcc scores, especially for text-based questions. This suggests that providing the full page context significantly improves answer quality for GPT-4o, likely due to better cross-referencing of information across different elements in the document. Phi-4-multimodal and Paligemma2-3B perform poorly in most scenarios. Phi-4-multimodal and Paligemma24Train Set: https://huggingface.co/datasets/vidore/colpali_train_set 3B fail to provide reliable answers, with very low GAcc and PNLs scores across all content types and all languages. This suggests that these models are not well optimized for document VQA scenarios. Multilingual performance varies significantly. Although GPT-4o generally performs well across multiple languages, its performance drops in Arabic and low-resource languages (e.g., Croatian, Czech, Vietnamese, and Slovenian). This highlights the challenges of multilingual document retrieval, which further highlights the contribution of our VisR-Bench. 4.5. Qualitative Error Analysis Due to space limitations, we include qualitative examples of retrieval failures and comparisons between true and hard negative pages in Appendix Figure A.1. 5. Conclusion We introduce VisR-Bench, the first multilingual, questiondriven visual retrieval benchmark for long documents, designed to evaluate retrieval performance across diverse document types and languages. Our evaluations across text-based retrieval methods, multimodal encoders, and MLLMs reveal that while MLLMs outperform other approaches, they still struggle with structured content and low-resource languages, exposing critical gaps in multilingual multimodal retrieval. By establishing comprehensive evaluation framework, VisR-Benchpaves the way for future advancements in document-aware multimodal retrieval and RAG systems, advancing the way to more robust and linguistically diverse MLLM-based retrieval systems."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: comprehensive survey on multimodal retrieval-augmented generation, 2025. [2] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixtureof-loras. arXiv preprint arXiv:2503.01743, 2025. [3] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question In Proceedings of the IEEE/CVF interanswering. national conference on computer vision, pages 4291 4301, 2019. [4] Jian Chen, Ruiyi Zhang, Yufan Zhou, Ryan Rossi, Jiuxiang Gu, and Changyou Chen. Mmr: Evaluating reading ability of large multimodal models. arXiv preprint arXiv:2408.14594, 2024. [5] Jian Chen, Ruiyi Zhang, Yufan Zhou, Tong Yu, Franck Dernoncourt, Jiuxiang Gu, Ryan Rossi, Changyou Chen, and Tong Sun. Sv-rag: Lora-contextualizing adaptation of mllms for long document understanding. arXiv preprint arXiv:2411.01106, 2024. [6] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multilingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [8] Yew Ken Chia, Liying Cheng, Hou Pong Chan, Chaoqun Liu, Maojia Song, Sharifah Mahani Aljunied, Soujanya Poria, and Lidong Bing. M-longdoc: benchmark for multimodal super-long document understanding and retrieval-aware tuning framework. arXiv preprint arXiv:2411.06176, 2024. [9] Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and Soyeon Caren Han. Mmvqa: comprehensive dataset for investigating multipage multimodal information retrieval in pdf-based visual question answering. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI, pages 39, 2024. [10] Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, and Yong Liu. Mmdocir: Benchmarking multi-modal retrieval for long documents. arXiv preprint arXiv:2501.08828, 2025. [11] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Pp-ocr: practiYang, Qingqing Dang, et al. arXiv preprint cal ultra lightweight ocr system. arXiv:2009.09941, 2020. [12] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models, 2024. [13] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. Exploring the frontier of vision-language models: survey of current methodologies and future directions. arXiv preprint arXiv:2404.07214, 2024. [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [15] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. [16] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. [18] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948, 2020. [19] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637, 2024. [20] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. [21] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. 9 Chatqa: Building gpt-4 level conversational qa models. CoRR, 2024. [22] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. arXiv preprint arXiv:2406.11251, 2024. [23] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024. [24] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. [25] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [26] Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. Infographicvqa, 2021. [27] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images, 2021. [28] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. [30] Reimers. [31] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. [32] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. arXiv preprint arXiv:2406.05967, 2024. large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 88028812, 2021. [34] Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. [35] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on In Proceedings of the AAAI Conmultiple images. ference on Artificial Intelligence, volume 37, pages 1363613645, 2023. [36] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023. [37] Michał Turski, Tomasz Stanisławek, Karol Kaczmarek, Paweł Dyda, and Filip Gralinski. Ccpdf: Building high quality corpus for visually rich documents from web crawl data. In International Conference on Document Analysis and Recognition, pages 348365. Springer, 2023. [38] Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1952819540, 2023. [39] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. Needle in multimodal haystack. Advances in Neural Information Processing Systems, 37:2054020565, 2024. [40] Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, et al. Scimmir: Benchmarking scientific multi-modal information retrieval. arXiv preprint arXiv:2401.13478, 2024. [41] Shitao Xiao, Zheng Liu, Peitian Zhang, and Muennighof. C-pack: packaged resources to advance general chinese embedding. 2023. arXiv preprint arXiv:2309.07597, 2023. [42] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrievalaugmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594, 2024. [33] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards [43] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre10 training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [44] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [45] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms. arXiv preprint arXiv:2412.16855, 2024. [46] Shaolin Zhu, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi Xiong, et al. Multilingual large language models: systematic survey. arXiv preprint arXiv:2411.11072, 2024. 11 A. Demo figure-based retrieval Figure A.1. Qualitative error analysis for figure-based question answering. This figure presents two examples where the GME model fails to retrieve the correct evidence page, while ColPali-v1.2 successfully identifies it. The incorrect pages retrieved by GME are shown as hard negatives. Notably, these hard negatives are highly similar to the correct evidence: in the top example, both pages are architectural blueprints containing references to door types and numbers such as 5 and 6; in the bottom example, both figures depict grain yield advantage plots and contain the keyword hybrid. These visually and semantically similar distractors demonstrate typical failure cases for GME and highlight the improved discriminative ability of ColPALI-v1.2 in retrieving the truly relevant figure. 12 B. System Prompts B.1. Figure-related question You are an AI assistant designed to refine and improve question-answer pairs for document evidence retrieval. Your task is to enhance the given QA pair by making the question self-contained and explicitly identifying the relevant figure, table, or section within the document. Guidelines: 1. Clarify Ambiguous References If the question refers to figure, table, or section without specifying which one, revise it to include explicit identifiers (e.g., Figure 3 or the bar chart titled Sales Trends). Ensure the question contains enough context so that the audience can locate the evidence page without prior knowledge. 2. Extract Contextual Cues Use captions, labels, headings, or surrounding text to infer the most precise reference. If multiple figures or tables exist, distinguish them based on their title, description, or content. 3. Maintain Original Meaning Preserve the intent and focus of the original question while making it self-contained. Ensure clarity and specificity without adding unnecessary details. Examples: Input QA Pair (Ambiguous Question) Q: What is the value of Data 3 in the chart? A: The value of Data 3 is 45. Refined QA Pair (Self-Contained Question) Q: In Figure 5, which presents the monthly sales distribution, what is the value of Data 3 in the bar chart? A: The value of Data 3 is 45. Expected Output Format: Output Format (JSON Response) Generate valid JSON response structured as follows: { \"detected_language\": \"LANGUAGE\", \"question_in_document_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_document_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" } - Replace `LANGUAGE` with the detected language (e.g., \"French\", \"Spanish\"). - Replace `XXXXXX` with the generated questions. - Replace `'YYYYYY'` with the corresponding answers extracted from the text. Your response should include: 1. The revised question that clearly specifies the evidence page. 2. The original answer (unchanged, unless adjustments are necessary for clarity). Now, please revise the given question pair according to these guidelines. B.2. Text-related question You are an assistant specialized in multilingual document retrieval tasks. 13 The task is as follows: given the text content of document page, detect the language of the document and generate questions in the detected language as well as its English version. Each question should: 1. The question should be relevant to the page, and should not be too general. The question should be about the subject of the page, and the answer needs to be found in the page. 2. The question is asked by user to get information from multi-page document. Generate question that could be asked by provided infomation in the given page. 3. Generate as well the answer to the question, which should be found in the page. Please do not generate: 1. Questions that are too broad or global (e.g., summarization or conclusion-type questions that require information beyond the given page). 2. Questions that are not specific to the page (e.g., questions that apply equally to all pages, like \"What is the page number?\"). 3. Questions that require cross-page reasoning or involve multiple pages to answer. For each question: - Generate its corresponding answer, which must be found explicitly in the text content of the page. - The answer should be formatted as words or phrases extracted directly from the text. - Questions and answers must be provided in both the detected language and in English. Generate at most THREE pairs of questions and answers per page in dictionary format. If no relevant questions can be generated for the page, return an empty list. The output format should include only valid json string, such as: { \"detected_language\": \"LANGUAGE\", \"questions\": [ { }, { }, { \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" } ] } - Replace `LANGUAGE` with the detected language (e.g., \"French\", \"Spanish\"). - Replace `XXXXXX` with the generated questions. - Replace `'YYYYYY'` with the corresponding answers extracted from the text. - If no questions can be generated, return an empty list. Focus on crafting meaningful and diverse questions that represent realistic user queries about the document. Here is the text: B.3. Table-related question You are an intelligent assistant specialized in multilingual document analysis and table-based question generation. 14 Your goal is to generate computational or reasoning-based questions from given interleaved document page. Task Overview Given the content of document page (text and tables), you must: 1. Detect the language of the document. 2. Generate at most three pairs of questions and answers, where: - Questions should require reasoning, computation, or trend analysis (not direct lookups). - Each question must be generated in both the detected language and English. - Answers must be extracted from the table in the page and formatted as words or phrases. Question Requirements Require computation or logical inference rather than simple fact lookup. Analyze trends, comparisons, rankings, or correlations in the table data. Ensure relevance to the page while avoiding overly general or document-wide questions. The question should require information from the table to answer. Example Question Types Trend Analysis: \"How has changed over the last five years?\" Growth Rate: \"Which category experienced the fastest increase?\" Comparison: \"Which product had the highest price difference between regions?\" Correlations: \"Does an increase in correspond to decrease in Y?\" Restrictions: Do Not Generate Fact-based questions (e.g., \"What is the value of in row 3, column 2?\"). Broad summarization or conclusions beyond the given page. Questions requiring multi-page reasoning. Irrelevant metadata questions (e.g., \"What is the page number?\"). Output Format (JSON Response) Generate valid JSON response structured as follows: { \"detected_language\": \"LANGUAGE\", \"questions\": [ { }, { }, { } \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" ] } 15 - Replace `LANGUAGE` with the detected language (e.g., \"French\", \"Spanish\"). - Replace `XXXXXX` with the generated questions. - Replace `'YYYYYY'` with the corresponding answers extracted from the text. Focus on designing meaningful and diverse questions that reflect realistic user queries and require information from the table. Here is the text: B.4. General Multilingual question You are an assistant specialized in multilingual document retrieval tasks. The task is as follows: given the text content and image content of document page, detect the language of the document and generate questions in the detected language as well as its English version. Each question should: 1. The question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page. 2. The question is asked by user to get some information from large documentary corpus that contains multimodal data. Generate question that could be asked by user without knowing the existence and the content of the corpus. 3. Generate as well the answer to the question, which should be found in the page. And the format of the answer should be list of words answering the question. Please do not generate: 1. Questions that are too broad or global (e.g., summarization or conclusion-type questions that require information beyond the given page). 2. Questions that are not specific to the image (e.g., questions that apply equally to all pages, like \"What is the page number?\"). 3. Questions that require cross-page reasoning or involve multiple pages to answer. For each question: - Generate its corresponding answer, which must be found explicitly in the text content of the page. - The answer should be formatted as list of words or phrases extracted directly from the text. - Questions and answers must be provided in both the detected language and in English. Generate at most THREE pairs of questions and answers per page in dictionary format. If no relevant questions can be generated for the page, return an empty list. The output format should include only valid json string, such as: { \"detected_language\": \"LANGUAGE\", \"questions\": [ { }, { }, \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" 16 \"question_in_detected_language\": \"XXXXXX\", \"question_in_english\": \"XXXXXX\", \"answer_in_detected_language\": \"YYYYYY\", \"answer_in_english\": \"YYYYYY\" { } ] } - Replace `LANGUAGE` with the detected language (e.g., \"French\", \"Spanish\"). - Replace `XXXXXX` with the generated questions. - Replace `'YYYYYY'` with the corresponding answers extracted from the text. - If no questions can be generated, return an empty list. Focus on crafting meaningful and diverse questions that represent realistic user queries about the document. Here is the text:"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University at Buffalo",
        "University of Maryland"
    ]
}