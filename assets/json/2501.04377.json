{
    "paper_title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
    "authors": [
        "Yekun Ke",
        "Xiaoyu Li",
        "Yingyu Liang",
        "Zhizhou Sha",
        "Zhenmei Shi",
        "Zhao Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, Visual Autoregressive ($\\mathsf{VAR}$) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine \"next-scale prediction\" paradigm. However, the state-of-the-art algorithm of $\\mathsf{VAR}$ models in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^4)$ time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of $\\mathsf{VAR}$ Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which $\\mathsf{VAR}$ computations can achieve sub-quadratic time complexity. Specifically, we establish a critical threshold for the norm of input matrices used in $\\mathsf{VAR}$ attention mechanisms. Above this threshold, assuming the Strong Exponential Time Hypothesis ($\\mathsf{SETH}$) from fine-grained complexity theory, a sub-quartic time algorithm for $\\mathsf{VAR}$ models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria. This work initiates the study of the computational efficiency of the $\\mathsf{VAR}$ model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in $\\mathsf{VAR}$ frameworks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 7 7 3 4 0 . 1 0 5 2 : r On Computational Limits and Provably Eﬃcient Criteria of Visual Autoregressive Models: Fine-Grained Complexity Analysis Yekun Ke Xiaoyu Li Yingyu Liang Zhizhou Sha Zhenmei Shi Zhao Song Abstract Recently, Visual Autoregressive (VAR) Models introduced groundbreaking advancement in the ﬁeld of image generation, oﬀering scalable approach through coarse-to-ﬁne next-scale prediction paradigm. However, the state-of-the-art algorithm of VAR models in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes O(n4) time, which is computationally ineﬃcient. In this work, we analyze the computational limits and eﬃciency criteria of VAR Models through ﬁne-grained complexity lens. Our key contribution is identifying the conditions under which VAR computations can achieve sub-quadratic time complexity. Speciﬁcally, we establish critical threshold for the norm of input matrices used in VAR attention mechanisms. Above this threshold, assuming the Strong Exponential Time Hypothesis (SETH) from ﬁne-grained complexity theory, sub-quartic time algorithm for VAR models is impossible. To substantiate our theoretical ﬁndings, we present eﬃcient constructions leveraging low-rank approximations that align with the derived criteria. Formally, suppose that is the height and width of the last VQ code map in VAR models, is the hidden dimension, is the bound of the entries of the input matrices for attention calculations in VAR models. We present two results: On the positive side, we show that when = O(log n) and = o(log n), there is an O(n2+o(1))-time algorithm that approximates the output of VAR model up to 1/ poly(n) additive error. On the negative side, we show that when = O(log n) and = Θ(log n), assuming SETH, it is impossible to approximate the output of VAR model up to 1/ poly(n) additive error in truly sub-quartic time O(n4Ω(1)). This work initiates the study of the computational eﬃciency of the VAR model from theoretical perspective. Our technique will shed light on advancing scalable and eﬃcient image generation in VAR frameworks. keyekun0628@gmail.com. Independent Researcher. 7.xiaoyu.li@gmail.con. Independent Researcher. yingyul@hku.hk. The University of Hong Kong. shazz20@mails.tsinghua.edu.cn. Tsinghua University. zhmeishi@cs.wisc.edu. University of Wisconsin-Madison. magic.linuxkde@gmail.com. The Simons Institute for the Theory of Computing at UC Berkeley. yliang@cs.wisc.edu. University of Wisconsin-Madison."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Related Work 2.1 Visual Generation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Acceleration via Low-rank Approximation . . . . . . . . . . . . . . . . . . . . . . . . 3 Model Formulation 3.1 Notations and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Phase 1: Token Maps Generation Phase . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Phase 2: Feature Map Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Phase 3: VQ-VAE Decoder process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3 4 4 4 5 5 5 8 8 4 Computational Limits 9 9 4.1 Strong Exponential Time Hypothesis . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Hardness of Approximate Attention Computation . . . . . . . . . . . . . . . . . . . . 9 4.3 Computational Limits of Fast VAR Models . . . . . . . . . . . . . . . . . . . . . . . . 10 5 Provably Eﬃcient Criteria 10 5.1 Running Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.2 Error Propagation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.3 Existence of Almost Quadratic Time Algorithm . . . . . . . . . . . . . . . . . . . . . 14 6 Discussion 7 Conclusion Notations 14 20 Perturbation Error Analysis 20 B.1 Perturbation Error for Linear Interpolation . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Perturbation Error of Power Function . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 Perturbation Error of Production of Two Terms . . . . . . . . . . . . . . . . . . . . . 22 . . . . . . . . . . . . . . . . . . . . . . 23 B.4 Perturbation Error of Production of Terms Error Analysis of Visual Auto-Regressive Transformer 24 C.1 Lipschitz of Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2 Error Propagation of Inner Product C.3 Error Analysis of AAttC(X ) and AAttC(X) . . . . . . . . . . . . . . . . . . . . . . . 27 C.4 Error Analysis of AAttC(X ) and Attn(X) . . . . . . . . . . . . . . . . . . . . . . . . 28 C.5 Error Analysis of Up-Interpolation Layer . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.6 Error Analysis for VAR Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 C.7 Error Analysis for the Fast VAR Transformer . . . . . . . . . . . . . . . . . . . . . . 31 Error Analysis of VQVAE Decoder 32 1 Running Time 33 E.1 Phase 1: Running Time of Token Maps Generation . . . . . . . . . . . . . . . . . . . 33 E.2 Phase 2: Running Time of Feature Map Reconstruction . . . . . . . . . . . . . . . . 35 . . . . . . . . . . . . . . . . . . . . . . 37 E.3 Phase 3: Running Time of VQ-VAE Decoder"
        },
        {
            "title": "Introduction",
            "content": "Visual generation technologies now underpin broad array of applications, ranging from image enhancement [LHC+25, GLD+25] and augmented reality [AWT+24] to medical diagnostics [AKH+24, MHL+24, LLL+24] and creative pursuits like game development [RHR+20, CGX+25]. By translating text descriptions or other input into detailed and diverse visuals, these models are reshaping both how machines interpret images and how new visual content is created. Leading methods in the ﬁeld include Variational AutoEncoders (VAE) [Doe16], Generative Adversarial Networks (GAN) [GPAM+20], and Diﬀusion models [HJA20]. Their advancements in producing high-resolution, high-ﬁdelity, and varied imagery have signiﬁcantly broadened the scope of visual generation, driving improvements in realism, diversity, and overall quality. The emergence of the Visual AutoRegressive model (VAR) [TJY+24] marks notable paradigm shift in image generation. Rather than relying on conventional next-token prediction, the VAR model introduces coarse-to-ﬁne next-scale prediction approach, enabling autoregressive transformers to more eﬃciently learn visual distributions and outperform diﬀusion-based alternatives. Moreover, the VAR model demonstrates robust zero-shot capabilities in tasks like image inpainting and editing, underscoring its potential for advancing autoregressive models in visual generation. Despite its demonstrated strengths, there remains critical need to investigate the VAR models computational limits and to design eﬃcient algorithms. In [TJY+24], the authors report that the VAR model has computational cost of O(n4), improving upon the O(n6) complexity associated with earlier autoregressive (AR) methods, where is the height and width of the last (largest) VQ code map. In this work, we aim to investigate the computational limits and potential eﬃcient algorithms of VAR models. Speciﬁcally, we ask the following questions: Can we perform the computations of VAR models faster than O(n4) time? We answer this question aﬃrmatively and summarize our contributions as follows. Computational Limits: We analyze the computation of the VAR models under the Strong Exponential Time Hypothesis. Let represent the upper bound of the elements in the input matrices used for attention calculations in VAR models. We establish an upper bound criterion = Θ(log n). Crucially, only when is below this threshold, one can compute VAR models in O(n4Ω(1)) time (truly sub-quartic time). Provably Eﬃcient Criteria: We further show that when = o(log n), it becomes possible to design an algorithm that approximates the VAR model in almost quadratic time, speciﬁcally O(n2+o(1))."
        },
        {
            "title": "1.1 Our Results",
            "content": "Ω(log n), it is impossible to design truly sub-quartic time Our ﬁrst result shows that when algorithm. Our results for the lower bound make use of the Strong Exponential Time Hypothesis (SETH) [IP01] from the area of ﬁne-grained complexity regarding the time required to solve k-SAT. Theorem 1.1 (Computational Limits of VAR Models, informal version of Theorem 4.4). Suppose = O(log n) and = Θ(log n). Assuming SETH, there is no algorithm that approximates the VAR model up to 1/ poly(n) additive error in O(n4Ω(1)) time. Our second result shows that when is o(log n), an almost quadratic time algorithm exists: Theorem 1.2 (Existence of Almost Quadratic Time Algorithm, informal version of Theorem 5.8). Suppose = O(log n) and = o(log n). There is an algorithm that approximates the VAR model up to 1/ poly(n) additive error in O(n2+o(1)) time. 3 Roadmap. Section 2 oﬀers summary of related work. In Section 3, we outline the mathematical formulation of both the VAR model and its fast version and divide the model into three stages: the VAR Transformer, the feature map reconstruction block, and the VQVAE-Decoder. Section 4 delves into analyzing the computation limits of the VAR model. In Section 5, we examine the running time and error propagation for each block in the fast VAR model and establish the conditions under which the model can be accelerated with proven eﬃciency. In Section 6, we discuss the potential impacts and future directions. In Section 7, we conclude our contributions."
        },
        {
            "title": "2.1 Visual Generation Models",
            "content": "Recent years have witnessed remarkable advancements in visual generation models, driven by progress in several prominent architectures. AutoRegressive Models. AutoRegressive models for visual generation [DYH+21, DZHT22] transform 2D images into 1D token sequences for processing. Early works like PixelCNN [VdOKE+16] and PixelSNAIL [CMRA18] pioneered pixel-by-pixel generation using raster-scan approach. Subsequent studies [RVdOV19, ERO21, LKK+22] extended this concept by generating image tokens in similar raster order. For example, VQ-GAN [ERO21] employs GPT-2-style decoder-only transformer for image generation, while models such as VQVAE-2 [RVdOV19] and RQ-Transformer [LKK+22] enhance this method with additional hierarchical scales or stacked representations. More recently, Visual AutoRegressive (VAR) modeling [TJY+24] introduced novel coarse-to-ﬁne nextscale prediction approach. This method improves scalability, inference speed, and image quality, outperforming traditional autoregressive techniques and diﬀusion transformers. Diﬀusion Models. Diﬀusion models [HJA20, RBL+22] are known for their ability to generate high-resolution images by progressively reﬁning noise into coherent visuals. Models such as DiT [PX23] and U-ViT [BNX+23] exemplify this approach, leveraging probabilistic frameworks to capture underlying data distributions. Recent advancements in diﬀusion-based generation focus on improving sampling techniques and training eﬃciency [SE19, SME20, LZB+22, HWL+24], exploring latent-space learning [RBL+22, WSD+24, WXZ+24, LZW+24], enhancing model architectures [HSC+22, PX23, LSSS24, WCZ+23, XSG+24], and 3D generation [PJBM22, WLW+24, XLC+24]."
        },
        {
            "title": "2.2 Acceleration via Low-rank Approximation",
            "content": "Low-rank approximation has emerged as powerful technique for addressing the computational challenges associated with modern transformer architectures. By approximating key operations such as attention and gradient computations, these methods signiﬁcantly reduce the time and resource requirements of training and inference. Accelerating Attention Mechanisms. Due to its quadratic computational complexity with respect to context length, the attention mechanism faces increasing diﬃculty as the sequence length grows in modern large language models [Ope24, AI24, Ant24]. To tackle this problem, polynomial kernel approximation methods [AA22] have been proposed, leveraging low-rank approximations to construct an eﬃcient approximation of the attention matrix. These approaches lead to notable improvements in computation speed, enabling single attention layer to handle both training and inference tasks with near-linear time complexity [AS23, AS24b]. Additionally, these methods can 4 be extended to more sophisticated attention mechanisms, like tensor attention, while maintaining almost linear time complexity in both training and inference phases [AS24c]. Furthermore, there are works considering RoPE-based attention [AS24a, CHL+24], and diﬀerentially private cross attention [LSSZ24a]. Additionally, alternative approaches like the conv-basis method introduced in [LLS+24a] oﬀer further opportunities for accelerating attention computations, providing complementary solutions to this critical bottleneck. Furthermore, there are many other works that use pruning to accelerate attention mechanisms [LLS+24b, CLS+24, LLSS24, SSZ+25b, SSZ+25a, HYW+24, WHL+24, XHH+24]. Gradient Approximation. The low-rank approximation is widely used technique for optimizing transformer training by reducing computational complexity [LSS+24a, LSSZ24b, AS24b, HWSL24, CLS+24, LSS+24b]. Speciﬁcally, [AS24b] builds upon the low-rank approximation framework introduced in [AS23], which originally focused on forward attention computation, to approximate the gradient of attention mechanisms. This method eﬀectively reduces the computational cost associated with gradient calculations. In [LSS+24a], this low-rank gradient approximation approach is further extended to multi-layer transformers, demonstrating that backward computations in such architectures can be approximated in nearly linear time. Additionally, [LSSZ24b] generalizes the work of [AS24b] to tensor-based attention model, leveraging the forward computation results from [AS24c] to enable eﬃcient training of tensorized attention mechanisms. Finally, [HWSL24] utilizes low-rank approximation methods in the training process of Diﬀusion Transformers (DiTs)., highlighting the versatility of these methods in various transformer-based models."
        },
        {
            "title": "3 Model Formulation",
            "content": "In Section 3.1, we give some deﬁnitions which will be used later. In Section 3.2, we present the In Section 3.3, we provide the mathematical formulation for the token map generation phase. mathematical formulation for the feature map reconstruction phase. Finally, in Section 3.4, we detail the mathematical formulation of the VQ-VAE Decoder within the VAR model."
        },
        {
            "title": "3.1 Notations and Deﬁnitions",
            "content": "Z+ We give the following notations in our setting. Given an integer 1, 2, . . . , } { is represented by [n]. In our paper, nearly linear time is deﬁned as O(n poly log n), and almost linear time is deﬁned as O(n1+o(1)). Given vector c, the diagonal matrix formed from is denoted as diag(c), where ci is the i-th diagonal entry of this matrix. Given matrix , we use to denote the transpose of . Given two vectors and b, which have the same length. The elementwise multiplication of and is denoted as with i-th entry being cidi. Given matrix , i,j 2 i,j. we use to represent the maximum norm of . Speciﬁcally, we have to represent the Frobenius norm of . Speciﬁcally, we have , the set 0 } { := qP U Given matrix , we use := maxi,j Ui,j ."
        },
        {
            "title": "3.2 Phase 1: Token Maps Generation Phase",
            "content": "The VAR model uses the VAR Transformer to convert the initial tokens of the generated image into several pyramid-shaped token maps. And in the token maps generation phase, the token maps for the next scale, Mk+1, are generated based on the previous token maps M1, . . . , Mk. This phase has the main modules as the following: 5 Up Sample Blocks. Before inputting the token maps into the VAR Transformer, the VAR model upsamples the (i)-th token map to the size of the (i + 1)-th token map. Speciﬁcally, the VAR model employs an upsampling method using interpolation for the image. Here, we deﬁne the up-interpolation layers: Deﬁnition 3.1 (Bicubic Spline Kernel). bicubic spline kernel is piecewise cubic function : that satisﬁes (x) [0, 1] for all R. Deﬁnition 3.2 (Up-interpolation Layer). The Up-Interpolation layer is deﬁned as follows: Let N and respectively. Let and respectively. denote the height of the input feature map and output feature map, denote the width of the input feature map and the output feature map, denote the number of channels of the input feature map and the output feature map. Let Let Let Let s, Rhwc denote the input feature map. Rhwc denote the output feature map. { . 1, 0, 1, 2 } We use φup : Rhwc = φup(X). Speciﬁcally, for Let : be bicubic spline kernel as deﬁned in 3.1. Rhwc to denote the up-interpolation operation then we have [h], [w], 2 2 Yi,j,l := (s) s=1 t=1 [c], we have ih +s, jw +t,l (t) Transformer Blocks. After the up-sample process, the generated token maps above will be input into the Transformer to predict the next token map. Here, we deﬁne several blocks for the VAR Transformer. We ﬁrst deﬁne the attention matrix. Deﬁnition 3.3 (Attention Matrix). Deﬁne the model weights as WQ, WK Rnd represent the input of length n. The attention matrix i, [n], Rdd, and let Rnn is given by the following, for Ai,j := exp(Xi,WQW j,. To move on, we deﬁne the single attention layer in the following way: Rdd Deﬁnition 3.4 (Single Attention Layer). Let represent the weighted matrix of value. Similar to the standard attention mechanism, the goal is to Rnn. We deﬁne attention layer Attn as produce an the following: Rnd be the input matrix. Let WV output matrix, where := diag(A1n) Firstly, we give the deﬁnition of the VAR Transformer Layer. Attn(X) := 1AXWV . 6 Deﬁnition 3.5 (VAR Transformer Layer). Given that the following conditions are true: Deﬁne Rnd as the input data matrix. Let the up-interpolation layer φup as given in Deﬁnition 3.2. Let the Attn(X) as given in Deﬁnition 3.3. Then, we deﬁned one VAR Transformer Layer as Fvar(X) := Attn(φup(X)) Then, we can give the deﬁnition of the Fast VAR Transformer Layer. Deﬁnition 3.6 (Fast VAR Transformer Layer). Given that the following conditions are true: Let Rnd denote the input data matrix. Let the up-interpolation layer φup as given in Deﬁnition 3.2. Let the AAttC(X) as given in Deﬁnition 4.2. Then, we deﬁned one Fast VAR Transformer Layer as Ffvar(X) := AAttC(φup(X)) Deﬁnition 3.7 (VAR Model). Given that the following conditions are true: Let the initial input be deﬁned as x1 Let the VAR Transformer layer Fvar as given in Deﬁnition 3.5. Let represent the total iteration of VAR model. R1d. Then, for 0, 1, 2, { , , we deﬁned the intermediate variable Ti(X) as follows: } Ti(X) = x1, Fvar(Ti1(X)), ( = 0 [r] Then, the ﬁnal output of the VAR model is Tr(X). Deﬁnition 3.8 (Fast VAR Model). Given that the following conditions are true: R1d as the initial input. Deﬁne x1 Let the Fast VAR Transformer layer Ffvar be deﬁned as Deﬁnition 3.6. We take to be the upper bound on the entries of the input matrices involved in attention computations for VAR models. Let Then, for denote the total iteration of the Fast VAR model. 0, 1, 2, , , we deﬁned the intermediate variable } Ti(X) as follows: { Ti(X) = x1, Ffvar( ( Ti1(X)), Then, the ﬁnal output of the VAR model is Tr(X). 7 = 0 [r]"
        },
        {
            "title": "3.3 Phase 2: Feature Map Reconstruction",
            "content": "In phase 2, the VAR model will transform the generated token maps into feature maps. This phase has the following main modules: Up Sample Blocks. The VAR model performs up-sampling on token maps of diﬀerent sizes, scaling them to the size of the ﬁnal output feature map. In this process, the VAR model will use the up-interpolation blocks deﬁned in Deﬁnition 3.2. To mitigate information loss during token map up-scaling, the VAR model employs convolution blocks to post-process the up-scaled token maps. We deﬁne the convolution layers as the following: Deﬁnition 3.9 (Convolution Layer). The Convolution Layer is deﬁned as follows: Let denote the height of the input and output feature map. denote the width of the input and output feature map. denote the number of channels of the input feature map. denote the number of channels of the output feature map. Let Let cin Let cout Let Rhwcin represent the input feature map. For [cout], we use Let = 1 denote the padding of the convolution layer. Let = 1 denote the stride of the convolution kernel. Let Rhwcout represent the output feature map. R33cin to denote the l-th convolution kernel. We use φconv : Rhwcin φconv(X). Speciﬁcally, for Rhwcout to represent the convolution operation then we have = [h], [cout], we have [w], 3 3 cin Yi,j,l := Xi+m1,j+n1,c m=1 n=1 c=1 m,n,c + Remark 3.10. Assumptions of kernel size, padding of the convolution layer, and stride of the convolution kernel are based on the speciﬁc implementation of [TJY+24]."
        },
        {
            "title": "3.4 Phase 3: VQ-VAE Decoder process",
            "content": "VAR will use the VQ-VAE Decoder Module to reconstruct the feature map generated in Section 3.3 into new image. The Decoder of VQ-VAE has the following main modules: ResNet Blocks. In the VQVAE decoder, the ResNet block, which includes two (or more) convolution blocks, plays crucial role in improving the models ability to reconstruct high-quality outputs. The convolution blocks help capture spatial hierarchies and patterns in the data, while the residual connections facilitate better gradient ﬂow and allow the model to focus on learning the residuals (diﬀerences) between the input and output. The deﬁnition of convolution block is given in Deﬁnition 3.9. 8 Attention Blocks. The Attention block helps the Decoder fuse information from diﬀerent locations during the generation process, which can signiﬁcantly improve the clarity and detail of the generated images. When applied to feature map, the attention mechanism computes attention scores for all pairs of pixels, capturing their pairwise relationships and dependencies. The deﬁnitions of blocks in attention are given in Section 3.2. Up Sample Blocks. The VQ-VAE decoder uses Up-Sample Blocks to progressively increase the spatial resolution of the latent representation. The Up-Sample Blocks in VQVAE combine up-interpolation and convolution blocks to restore the spatial dimensions of the feature maps, facilitating the reconstruction of the high-resolution output image. The convolution block has already been deﬁned in Deﬁnition 3.9, and the up-interpolation block has already been deﬁned in Deﬁnition 3.2."
        },
        {
            "title": "4 Computational Limits",
            "content": "In this section, we delve into the computational limits of VAR Models, particularly in the context of solving key problems under the assumptions of the Strong Exponential Time Hypothesis (SETH). Section 4.1 introduces SETH as the basis for our complexity analysis. In Section 4.2, we discuss key result from [AS23] that establishes the hardness of Approximate Attention Computation. Finally, Section 4.3 presents the lower bound for VAR model eﬃciency, pinpointing the limitations for sub-quartic performance."
        },
        {
            "title": "4.1 Strong Exponential Time Hypothesis",
            "content": "We begin by presenting the foundational hypothesis (SETH) [IP01], which underpins much of our complexity analysis: Hypothesis 4.1 (Strong Exponential Time Hypothesis (SETH) [IP01]). For every ǫ > 0, there exists positive integer 3 such that no randomized algorithm can solve k-SAT on formulas with 2(1ǫ)n variables in time."
        },
        {
            "title": "4.2 Hardness of Approximate Attention Computation",
            "content": "(cid:0) (cid:1) We begin by introducing the deﬁnition of Approximate Attention Computation (AAttC). Deﬁnition 4.2 (Approximate Attention Computation AAttC(n, d, B, δ), Deﬁnition 1.2 in [AS23]). Rnd denote the input of the attention mechanism. Given three Let δ > 0. Let > 0. Let matrices Q, K, R, output Rnd that approximately represents Attn(X), meaning matrix Rnd, with the guarantees that and V R, Attn(X) δ Next, we state result for the Approximate Attention Computation (AAttC) from [AS23]. Lemma 4.3 (Theorem 4.6 in [AS23]). Suppose = O(log n) and = Θ(log n). Assuming SETH, there is no algorithm that solves the Approximate Attention Computation (AAttC) up to 1/ poly(n) additive error in O(n4Ω(1)) time."
        },
        {
            "title": "4.3 Computational Limits of Fast VAR Models",
            "content": "We now present main theorem detailing the lower bound for VAR model computation. Theorem 4.4 (Computational Limits of Fast VAR Models, formal version of Theorem 1.1). Suppose = O(log n) and = Θ(log n). Assuming SETH, there is no algorithm that approximates the VAR model up to 1/ poly(n) additive error in O(n4Ω(1)) time. Proof. By Lemma 4.3, in the K-th step (K = loga n), the VAR Transformer must compute attention with computational cost at least Ω(L2q d) = Ω(( (αi1)2)2q d) = Ω(( i=1 α2K 1 )2q α2 1 1)2q Ω((α2K Ω(n42q d). d) d) In the ﬁrst step above, we use the deﬁnition of LK. The second step applies the standard geometric series formula. The third step involves basic algebra, and the ﬁnal inequality is due to the fact = loga n."
        },
        {
            "title": "5 Provably Eﬃcient Criteria",
            "content": "Section 5.1 details the running time of the fast VAR Transformer, feature map reconstruction block, and Fast VQ-VAE Decoder. In Section 5.2, we analyze the error propagation in both the Fast VAR Transformer and the Fast VQ-VAE Decoder. Section 5.3 presents our ﬁndings regarding the existence of an almost quadratic time algorithm."
        },
        {
            "title": "5.1 Running Time",
            "content": "Here, we present an overview of the computational cost associated with the Fast VAR Transformer, feature map reconstruction block, and Fast VQ-VAE Decoder. Firstly, we show that the runtime of the VAR Transformer can be sped up to O(n2+o(1)). Lemma 5.1 (Running time of Fast VAR Transformer, informal version of Lemma E.3). Assuming the conditions below are satisﬁed: Let denote the total number of the generated token maps. [K]. Let Let r1 Let α > 1 denote the growth rate of the height and width of the token map at each level. Then Rαk1αk1d. R11d denote the ﬁrst scale token map. [K], the k-th token map rk for Rnnd denote the last scale token map, where = αK1. Let rK Let = O(log(n)). 10 Then, the total runtime of the VAR Transformer for generating token maps can be accelerated to O(n2+o(1)). Then, we proceed to show that the runtime of the feature map reconstruction layer is O(n2+o(1)). Lemma 5.2 (Running time of Feature Map Reconstruction Layer, informal version of Lemma E.4). Assuming the conditions below are satisﬁed: Let denote the total number of generated token maps. [K]. Let Let r1 Let α > 1 denote the growth rate of the height and width of the token map at each level. Then Rαk1αk1d. R11d denote the ﬁrst scale token map. [K], the k-th token map rk for Rnnd denote the last scale token map, where = αK1. Let rK Let = O(log(n)). Then, the total runtime of the Feature Map Reconstruction Layer is O(n2+o(1)). Finally, we show that the runtime of the VQVAE Decoder can be sped up to O(n2+o(1)). Lemma 5.3 (Running time of Fast VQ-VAE Decoder, informal version of Lemma E.6). Assuming the conditions below are satisﬁed: Deﬁne k1, k2, k3 Let as constant numbers. Rnnd represent the input feature map. We assume there are k1 up-interpolation layers φup deﬁned in Deﬁnition 3.2. Given feature map up(M ) output φi RO(h)O(w)d. Rhwd. For [k1], we assume i-th up-interpolation layers We assume there are k2 attention layers Attn deﬁned in Deﬁnition 3.4. Rhwd. For Given feature map [k1], the i-th attention layers output Attn(M ) Rhwd. We assume there are k3 convolution layers φconv deﬁned in Deﬁnition 3.9. Rhwd. For [k1], we assume i-th up-interpolation layers Given feature map up(M ) output φi RhwO(d). Let = O(log(n)). Then, the total runtime of the VQ-VAE decoder can be accelerated to O(n2+o(1))."
        },
        {
            "title": "5.2 Error Propagation Analysis",
            "content": "Here, we present the error analysis results of the fast Transformer layer and the fast VQ-VAE Decoder. We begin with analyzing the error propagation of AAttC (see Deﬁnition 4.2). Lemma 5.4 (Error analysis of AAttC(X ) and AAttC(X), informal version of Lemma C.3). Assuming the conditions below are satisﬁed: Deﬁne Deﬁne Rnd as the input matrix. Rnd as the approximation version of input matrix. (0, 0.1) denote the approximation error. Let ǫ Suppose we have X ǫ. Let > 1. Assume the value of each entry in matrices can be bounded by . Let the polynomial approximation of attention matrix AAttC(X) as given in Deﬁnition 4.2. Let U, Rnk represent low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X). Let be polynomial with degree g. Then, we can show that AAttC(X) Then, we move on to analyzing the approximation error between AAttC(X ) and the ground AAttC(X O(kM g+2d) ǫ ) truth Attn(X), where denotes the approximated version of the original input X. Lemma 5.5 (Error analysis of AAttC(X ) and Attn(X), informal version of Lemma C.4). Assuming the conditions below are satisﬁed: Deﬁne Deﬁne Rnd as the input matrix. Rnd as the approximation version of input matrix. (0, 0.1) denote the approximation error. Let ǫ Suppose we have ǫ. Let > 1. Assume the value of each entry in matrices can be bounded by . Let the attention matrix Attn(X) Rnn as given in Deﬁnition 3.3. Let the polynomial approximation of attention matrix AAttC(X) tion 4.2. Rnn as given in DeﬁniLet U, Rnk be low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X). 12 Let be polynomial with degree g. Then, we can show that AAttC(X ) Attn(X) O(kM g+1d) ǫ With the above foundations, we are ready to move to analyzing the approximation error of one fast VAR layer (see Deﬁnition 3.6). Lemma 5.6 (Error propagation analysis for one VAR Transformer Layer, informal version of Lemma C.7). Assuming the conditions below are satisﬁed: Deﬁne Deﬁne Rnd as the input data matrix. Rnd as the approximation version of X. (0, 0.1) denote the approximation error. Let ǫ Suppose we have Let the VAR Transformer Layer Fvar as given in Deﬁnition 3.5. X ǫ. Let the Fast VAR Transformer Layer Ffvar as given in Deﬁnition 3.6. Let > 1. Assume the value of each entry in matrices can be bounded by . Let the attention matrix Attn(X) Rnn be deﬁned as Deﬁnition 3.3. Let the polynomial approximation of attention matrix AAttC(X) nition 4.2. Rnn be deﬁned as DeﬁLet U, Rnk be low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X). Let be polynomial with degree g. Then, we can show that Fvar(X) Then, we show that the VAR Transformer will introduce additive error bounded by 1/ poly(n). Ffvar(X O(kM g+1d) ǫ ) Lemma 5.7 (Error analysis for the Fast VAR Transformer, informal version of Lemma C.8). Assuming the conditions below are satisﬁed: Deﬁne X0 Deﬁne For Deﬁnition 3.7. R1d as the initial input data matrix. as the total iteration of the Fast VAR model. [r], let Ti(X0) denote the intermediate variable of the VAR model, as deﬁned in For [r], let Deﬁnition 3.8. Ti(X0) denote the intermediate variable of the fast VAR model, as deﬁned in 13 Assume Tr(X0) Assume Tr(X0) RO(n2)d is ﬁnal output of the VAR Transformer. RO(n2)d is ﬁnal output of the Fast VAR Transformer. Assume each element of the matrices is encoded in O(log n) bits. Then, we can show that the error bound of the ﬁnal output Tr(X0) as"
        },
        {
            "title": "5.3 Existence of Almost Quadratic Time Algorithm",
            "content": "e Tr(X0) Tr(X0) 1/ poly(n) This section presents theorem proving the existence of quadratic-time algorithm that speeds up the VAR model and guarantees bounded additive error. Theorem 5.8 (Existence of Almost Quadratic Time Algorithm, formal version of Theorem 1.2). Suppose = O(log n) and = o(log n). There is an algorithm that approximates the VAR model up to 1/ poly(n) additive error in O(n2+o(1)) time. Proof. By combining the result of Lemma 5.1, Lemma 5.2, Lemma 5.3 and Lemma 5.7, we can easily derive the proof."
        },
        {
            "title": "6 Discussion",
            "content": "The ﬁne-grained analysis of Visual Autoregressive (VAR) models we provided in this paper uncovers the critical computational limitations and proposes criteria that ensure eﬃciency under the Strong Exponential Time Hypothesis (SETH). The insights from this analysis are not only important for VAR models but also carry broader implications for the deep learning and machine learning communities as whole. One of the key contributions of this work is that understanding the computational bottlenecks of VAR models allows us to more clearly delineate the theoretical boundaries of model performance, which in turn helps guide the design of future models. By exploring the speciﬁc conditions under which the VAR models hit computational limits, it is important to identify and address these bottlenecks early in the model development process. This understanding can prevent the misallocation of resources toward achieving computational feats that are not feasible, particularly in the context of autoregressive models used for visual generation tasks. In particular, demonstrating that sub-quartic time complexity is unattainable when input matrices exceed critical threshold provides crucial reference point for the deep learning community. This knowledge empowers researchers to set realistic expectations regarding model eﬃciency and to focus their eﬀorts on optimizations that are computationally viable. This work provides foundational framework for understanding and overcoming the computational bottlenecks in generative models. It will serve as key resource for researchers striving to design the next generation of eﬃcient autoregressive models. By addressing the limitations of current models and oﬀering clear guidance on how to optimize them, we hope to inspire more eﬃcient and scalable solutions for wide array of machine learning applications, extending far beyond visual generation."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper provides ﬁne-grained complexity analysis of Visual Autoregressive (VAR) models, identifying computational limits and eﬃcient criteria under the Strong Exponential Time Hypothesis (SETH). By rigorously analyzing computational trade-oﬀs and proposing provably eﬃcient 14 criteria, this work establishes foundational understanding that will guide the development of next-generation autoregressive models in visual generation. We demonstrate the infeasibility of achieving sub-quartic time complexity for VAR computations when the norm of input matrices exceeds critical threshold. In contrast, we establish that sub-quadratic time approximations become feasible under carefully designed conditions, leveraging low-rank approximations. In future works, we will explore the extension of these methods to other domains where autoregressive models play pivotal role, such as text-to-image synthesis and multi-modal generation tasks. Additionally, integrating hardware acceleration strategies could further optimize the computational pipeline, broadening the applicability of VAR models in resource-constrained environments."
        },
        {
            "title": "References",
            "content": "[AA22] Amol Aggarwal and Josh Alman. Optimal-degree polynomial approximations for exponentials and gaussian kernel density estimation. In Proceedings of the 37th Computational Complexity Conference, pages 123, 2022. [AI24] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2024. https://ai.meta.com/blog/meta-llama-3/. [AKH+24] Reza Azad, Amirhossein Kazerouni, Moein Heidari, Ehsan Khodapanah Aghdam, Amirali Molaei, Yiwei Jia, Abin Jose, Rijo Roy, and Dorit Merhof. Advances in medical image analysis with vision transformers: comprehensive review. Medical Image Analysis, 91:103000, 2024. [Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_C [AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. Advances in Neural Information Processing Systems, 36, 2023. [AS24a] Josh Alman and Zhao Song. Fast rope attention: Combining the polynomial method and fast fourier transform. manuscript, 2024. [AS24b] Josh Alman and Zhao Song. The ﬁne-grained complexity of gradient computation for training large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [AS24c] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. In The Twelfth International Conference on Learning Representations, 2024. [AWT+24] Tej Azad, Anmol Warman, Jovanna Tracz, Liam Hughes, Brendan Judy, and Timothy Witham. Augmented reality in spine surgerypast, present, and future. The Spine Journal, 24(1):113, 2024. [BNX+23] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diﬀusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22669 22679, 2023. 15 [CGX+25] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma : Weak-to-strong training of diﬀusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2025. [CHL+24] Yifang Chen, Jiayan Huo, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Fast gradient computation for rope attention in almost linear time. arXiv preprint arXiv:2412.17316, 2024. [CLS+24] Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Hsr-enhanced sparse attention acceleration. arXiv preprint arXiv:2410.10165, 2024. [CMRA18] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In International conference on machine learning, pages 864872. PMLR, 2018. [Doe16] Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016. [DYH+21] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [DZHT22] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. [ERO21] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for highresolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [GLD+25] Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, and Shu-Tao Xia. Mambair: simple baseline for image restoration with state-space model. In European Conference on Computer Vision, pages 222241. Springer, 2025. [GPAM+20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diﬀusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [HSC+22] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diﬀusion models for high ﬁdelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. [HWL+24] Jerry Yao-Chieh Hu, Weimin Wu, Yi-Chen Lee, Yu-Chao Huang, Minshuo Chen, and Han Liu. On statistical rates of conditional diﬀusion transformers: Approximation, estimation and minimax optimality. arXiv preprint arXiv:2411.17522, 2024. [HWSL24] Jerry Yao-Chieh Hu, Weimin Wu, Zhao Song, and Han Liu. On statistical rates and provably eﬃcient criteria of latent diﬀusion transformers (dits). arXiv preprint arXiv:2407.01079, 2024. [HYW+24] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopﬁeld model. Advances in Neural Information Processing Systems, 36, 2024. [IP01] Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of Computer and System Sciences, 62(2):367375, 2001. [LHC+25] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Yu Qiao, Wanli Ouyang, and Chao Dong. Diﬀbir: Toward blind image restoration with generative diﬀusion prior. In European Conference on Computer Vision, pages 430448. Springer, 2025. [LKK+22] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523 11532, 2022. [LLL+24] Chenxin Li, Xinyu Liu, Wuyang Li, Cheng Wang, Hengyu Liu, Yifan Liu, Zhen Chen, and Yixuan Yuan. U-kan makes strong backbone for medical image segmentation and generation. arXiv preprint arXiv:2406.02918, 2024. [LLS+24a] Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, and Junze Yin. Conv-basis: new paradigm for eﬃcient attention inference and gradient computation in transformers. arXiv preprint arXiv:2405.05219, 2024. [LLS+24b] Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, and Yufa Zhou. Beyond linear approximations: novel pruning approach for attention matrix. arXiv preprint arXiv:2410.11261, 2024. [LLSS24] Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. tighter complexity analysis of sparsegpt. arXiv preprint arXiv:2408.12151, 2024. [LSS+24a] Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Looped relu mlps may be all you need as practical programmable computers. arXiv preprint arXiv:2410.09375, 2024. [LSS+24b] Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Multi-layer transformers gradient can be approximated in almost linear time. arXiv preprint arXiv:2408.13233, 2024. [LSSS24] Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Diﬀerential privacy mechanisms in neural tangent kernel regression. arXiv preprint arXiv:2407.13621, 2024. [LSSZ24a] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Diﬀerential privacy of cross-attention with provable guarantee. arXiv preprint arXiv:2407.14717, 2024. [LSSZ24b] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention trainarXiv preprint ing: Provably eﬃcient learning of higher-order transformers. arXiv:2405.16411, 2024. 17 [LZB+22] Lu, Zhou, Bao, Chen, and Li. fast ode solver for diﬀusion probabilistic model sampling in around 10 steps. Proc. Adv. Neural Inf. Process. Syst., New Orleans, United States, pages 131, 2022. [LZW+24] Chengyi Liu, Jiahao Zhang, Shijie Wang, Wenqi Fan, and Qing Li. Score-based generative diﬀusion models for social recommendations. arXiv preprint arXiv:2412.15579, 2024. [MHL+24] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. [Ope24] OpenAI. Introducing openai o1-preview. https://openai.com/index/introducing-openai-o1-pre 2024. Accessed: September 12. [PJBM22] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Textto-3d using 2d diﬀusion. arXiv preprint arXiv:2209.14988, 2022. [PX23] William Peebles and Saining Xie. Scalable diﬀusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [RBL+22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diﬀusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [RHR+20] Janet Rafner, Arthur Hjorth, Sebastian Risi, Lotte Philipsen, Charles Dumas, Michael Mose Biskjær, Lior Noy, Kristian Tylen, Carsten Bergenholtz, Jesse Lynch, et al. Crea. blender: neural network-based image generation game to assess creativity. In Extended abstracts of the 2020 annual symposium on computer-human interaction in play, pages 340344, 2020. [RVdOV19] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [SME20] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diﬀusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [SSZ+25a] Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, Zhihao Shu, Wei Niu, Pu Zhao, Yanzhi Wang, and Jiuxiang Gu. Lazydit: Lazy learning for the acceleration of diﬀusion transformers. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2025. [SSZ+25b] Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Jing Liu, Ruiyi Zhang, Ryan A. Rossi, Hao Tan, Tong Yu, Xiang Chen, Yufan Zhou, Tong Sun, Pu Zhao, Yanzhi Wang, and Jiuxiang Gu. Numerical pruning for eﬃcient autoregressive models. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2025. 18 [TJY+24] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [VdOKE+16] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. [WCZ+23] Yilin Wang, Zeyuan Chen, Liangjun Zhong, Zheng Ding, Zhizhou Sha, and Zhuowen Tu. Dolﬁn: Diﬀusion layout transformers without autoencoder. arXiv preprint arXiv:2310.16305, 2023. [WHL+24] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tandem hopﬁeld model for memory-enhanced time series prediction. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [WLW+24] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Proliﬁcdreamer: High-ﬁdelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. [WSD+24] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-image diﬀusion with token-level supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8553 8564, 2024. [WXZ+24] Yilin Wang, Haiyang Xu, Xiang Zhang, Zeyuan Chen, Zhizhou Sha, Zirui Wang, and Zhuowen Tu. Omnicontrolnet: Dual-stage integration for conditional image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74367448, 2024. [XHH+24] Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, HsiSheng Goan, and Han Liu. Bishop: Bi-directional cellular learning for tabular data with generalized sparse modern hopﬁeld model. In Forty-ﬁrst International Conference on Machine Learning, 2024. [XLC+24] Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, and In ProceedZhuowen Tu. Bayesian diﬀusion models for 3d shape reconstruction. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1062810638, 2024. [XSG+24] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diﬀusion paths. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "Appendix",
            "content": "Roadmap. Section introduces key notations. Section provides an analysis of the perturbation error in diﬀerent VAR model modules. Section details the error analysis for the VAR Transformer. In Section D, we present the error analysis of the VQVAE decoder. In Section E, we evaluate the running time of VAR models and fast VAR models."
        },
        {
            "title": "A Notations",
            "content": "Z+ } 1, 2, . . . , { , the set 0 } { Given an integer is represented by [n]. In our paper, nearly linear time is deﬁned as O(n poly log n), and almost linear time is deﬁned as O(n1+o(1)). Given vector c, the diagonal matrix formed from is denoted as diag(c), where ci is the i-th diagonal entry of this matrix. Given matrix , we use to denote the transpose of . Given two vectors and b, which have the same length. The element-wise multiplication of and is denoted as to represent the Frobenius norm to represent the d with i-th entry being cidi. Given matrix , we use of . Speciﬁcally, we have i,j 2 i,j. Given matrix , we use := maxi,j maximum norm of . Speciﬁcally, we have := Ui,j qP k ."
        },
        {
            "title": "B Perturbation Error Analysis",
            "content": "Section B.1 focuses on the perturbation error of linear interpolation. In Section B.2, we explore the error incurred when raising an approximated value to power. Section B.3 analyzes the perturbation error of approximating the product of two values, while Section B.4 addresses the error in approximating the product of values. B.1 Perturbation Error for Linear Interpolation First, we state fact that demonstrates that the simple linear interpolation operation does not introduce additional error. Fact B.1. Assuming the conditions below are satisﬁed: Let a, R. Deﬁne a, as the approximated value of a, b. and and b R. R. Let Let Let δ Suppose δ and δ. b (0, 1) Then we have, for any (0, 0.1) denote the approximation error. (wa + (1 w)b ) (wa + (1 w)b) δ Proof. We have = (wa + (1 w)b) (wa w(a δ + ( + (1 w)b ) a) + (1 w)(b w) + (1 w) δ b) = δ We derive the ﬁrst step using basic algebra, the second step using the triangle inequality and the fact that (0, 1), the third step using the conditions of the lemma, and the last step using basic algebra. B.2 Perturbation Error of Power Function We then present several lemmas that will be utilized in the subsequent proof. Fact B.2. Assuming the conditions below are satisﬁed: Let Let R. denote the approximation value of a. (0, 0.1) as the approximation error. R. R. Let Let Deﬁne δ Suppose δ Suppose Let Let := (2m Then we have . N. δ. Rm1. 1) (a )m (a am am δ )m am (a)m (a + δ)m am amiδi i (cid:19) (cid:19) (cid:19) = i=0 (cid:18) i=1 (cid:18) (cid:18) i=1 amiδi amiδi Proof. Case 1: We ﬁrstly consider the case am (a + δ)m am . Then we have: The ﬁrst step above is based on the condition of this lemma, the second relies on binomial expansion, the third uses simple algebra, and the last step follows from the triangle inequality. Then, we keep on showing that: (cid:18) i=1 (cid:19) amiδi = i=1 (cid:18) i ami δi (cid:19) am1 Rm1 δ δ i=1 (cid:18) (2m δ = (cid:19) 1) The ﬁrst step above is based on δ > 0 and that δ . The third step is due to the fact that i=1 R. The ﬁnal step is trivially from the deﬁnition of C. Case 2: Then we consider the case am (a i > 0. The second step is derived from the condition 1 and the condition that = 2m P (cid:0) (cid:1) δ)m am . Then we have: )m (a am δ)m am ami( δ)i am (cid:1) (cid:0) (a)m (a m m (cid:19) (cid:19) (cid:19) (cid:19) = i=0 (cid:18) i=1 (cid:18) = (cid:18) i=1 (cid:18) i=1 ami( ami( δ)i δ)i amiδi The ﬁrst step relies on this lemmas condition, the second on binomial expansion, the third on basic algebra, the fourth on triangle inequality, and the last on basic algebra. Then, we can further show that: (cid:18) i=1 (cid:19) amiδi = i=1 (cid:18) i ami δi (cid:19) am1 Rm1 δ δ i=1 (cid:18) (2m δ = (cid:19) 1) The ﬁrst step above relies on the condition that δ > 0 and the condition that δ condition that . The third step derives from the fact that R, and the ﬁnal step is from the deﬁnition of C. (cid:0) (cid:1) > 0. And the second step uses = 2m 1 and the i= i (cid:0) (cid:1) B.3 Perturbation Error of Production of Two Terms In this section, we introduce the perturbation error of the production of two terms. 22 Fact B.3. Assuming the conditions below are satisﬁed: Let a, R. Deﬁne a, as the approximated values of a, b. R. R. Let Let Let b Let δ Let Suppose we have Suppose we have Then we have Proof. We have (0, 0.1) denote the approximation value. b δ. δ. ab 2R δ ab = = (b (a δ + b) + (a b) + b(a ab) a) δ 2R δ Starting with simple algebra for the ﬁrst and second steps, the third step uses the triangle inequality along with δ, and the ﬁnal step is based on and δ and R. a B.4 Perturbation Error of Production of Terms In this section, we introduce the perturbation error of production of terms. Fact B.4. Assuming the conditions below are satisﬁed: Let a1, a2, Deﬁne 1, 2, For every , ad R. , Let δ [d], we assume as the approximation values of a1, a2, i (0, 0.1) denote the approximation error. and ai R. , ad. For every [d], we assume ai i δ. Then we have ai i=1 i=1 i Rd δ 23 Proof. The proof is carried out by mathematical induction as follows: Consider the case = 2. We can show that 2 ai i=1 i=1 a = 1a 2 a1a2 δ 2R The reasoning above follows from Fact B.3. Assume the statement is true for = k: i=1 consequently, we can demonstrate that: ai i=1 i Rk1 δ (1) k+1 k+1 ai ai) i=1 ak+1 ai) (ak+1 i=1 ( i=1 ( i=1 = = ( i) k+1 i=1 k+1) + ( ( ai) i=1 Rk δ + = (k + 1) (ak+1 Rk1 δ Rk k+1) + δ k ai i=1 i=1 ai ( i=1 i=1 i) k+1 i) k+1 Starting with simple algebra for the ﬁrst and second steps, the third step follows from the triangle inequality, the fourth from the conditions of this lemma and Eq. (1), and the ﬁnal step involves simple algebra. Hence, we have i=1 where this comes from mathematical induction. i=1 ai d Rd1 δ Error Analysis of Visual Auto-Regressive Transformer This section focuses on the error analysis of the VAR model. In Section C.1, we introduce the In Section C.2, we analyze the error propagation Lipschitz property of polynomial function. In Section C.3, we analyze the of inner product operation by giving two approximated inputs. error propagation of the fast attention AAttC(X). In Section C.4, we analyze the error between AAttC(X ) and Attn(X) where is the approximated value of X. In Section C.5, we conduct an error analysis of the up-interpolation layer. In Section C.6, we conduct the error analysis of the VAR transformer. Finally, in Section C.7, we conduct the error analysis of the Fast VAR transformer. 24 C.1 Lipschitz of Polynomial In this section, we introduce the Lipschitz property of polynomials. Lemma C.1 (Lipschitz of polynomial). Assuming the conditions below are satisﬁed: Let Let R. denote the approximated version of x. Let > 1. Suppose we have Let be polynomial with degree g. . M, We can then demonstrate that: Proof. Firstly, we can show (x) (x ) O(M g1) (2) where for each [g], ai Thus, we can show (x) = agxg + + a1x + a0 R. (x) (x ) = i=1 ai(xi i) ai(xi i) (x ai ) = i=1 i=1 j=0 i1j xjx The ﬁrst step is derived from Eq. (2), the second from the triangle inequality, and the ﬁnal step from simple algebra. Then, we can move forward to show that i=1 i=1 (x ai ) j=0 i1j xjx (x ai ) i1 i1 ai O(M g1) i=1 = The ﬁrst step above is consequence of the condition that derives from basic algebra, and the ﬁnal step is due to > 1. and , And the second 25 C.2 Error Propagation of Inner Product In this section, we conduct the error analysis of the inner product operation given two approximated inputs and v. Lemma C.2 (Error propagation of inner product). Assuming the conditions below are satisﬁed: Let u, Rk denote two vectors. Deﬁne u, Let > 1. Rk as the approximation value of u, v. Assume the value of each entry in matrices can be bounded by . Let ǫ (0, 0.1) denote the initial approximation error. Suppose we have max {k , v } ǫ. Then, we can prove that , u, i 2kǫM Proof. Firstly, we can show that , u, = k 1v (u 1 u1v1) 1v 1 u1v1 i=1 i=1 i=1 i=1 1(v 1 v1) + v1(u 1 u1) ( 1 1 v1 + v1 1 ) u1 The ﬁrst step results from simple algebra, the second from triangle inequality, the third from algebraic manipulation, and the ﬁnal step again from triangle inequality. Then, we can move forward to show ( 1 1 v1 + v1 1 ) u1 i=1 ǫ 2 i=1 2kǫM The ﬁrst step is derived from the conditions that each entry is at most , ǫ. The second step follows directly from algebraic manipulation. k ǫ and 26 C.3 Error Analysis of AAttC(X ) and AAttC(X) This section presents the error analysis between AAttC(X ) and AAttC(X). Lemma C.3 (Error analysis of AAttC(X ) and AAttC(X), formal version of Lemma 5.4). Assuming the conditions below are satisﬁed: Let Rnd denote the input matrix. Deﬁne Rnd as the approximation version of input matrix. (0, 0.1) denote the approximation error. Let ǫ Suppose we have ǫ. Let > 1. Assume the value of each entry in matrices can be bounded by . Let the polynomial approximation of attention matrix AAttC(X) as given in Deﬁnition 4.2. Let U, Rnk represent low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X). Let be polynomial with degree g. Then, we can show that Proof. Firstly, given and , we need to compute Q, Q, K, . And we demonstrate that AAttC(X ) AAttC(X) O(kM g+2d) ǫ k = = (X WQ ) nd WQ dd WQ X {z } ǫ {z} WQ (3) The initial step is derived from the computation of matrix Q. The second step is consequence of basic algebra, the third step arises from standard matrix multiplication, and the ﬁnal step is result of the condition ǫ and the fact that each entry is bounded by . In the same way, we can have Then, we move forward to calculate Rnk and [k], we have Ui,j = (Qi,1, . . . , Qi,d) and Vi,j = (Ki,1, . . . , Ki,d). Then, we can show Rnk. Speciﬁcally, for every X . ǫ [n] and that O(M g1) O(M gd) ǫ The ﬁrst step above is derived from Lemma C.1 and the condition that each entry is bounded by , while the second step results from Eq. (3) and basic algebra. In the same way, we can have O(M gd) ǫ. Finally, we can move forward to calculate AAttC(X ) = and AAttC(X) = . Then, for each [n], [n], it can be demonstrated that AAttC(X )i,j AAttC(X)i,j = i,, O(M gd) 2k O(kM g+1d) ,ji ǫ ǫ Ui,, V,j The ﬁrst step above is result of basic algebra, the second step comes from Lemma C.2 and the lemmas condition, and the ﬁnal step is derived from basic algebra. Thus, using the deﬁnition of the ℓ norm of matrix, we can demonstrate that AAttC(X ) AAttC(X) O(kM g+1d) ǫ Thus, we complete the proof. C.4 Error Analysis of AAttC(X ) and Attn(X) In this section, we conduct the error analysis between AAttC(X ) and Attn(X). Lemma C.4 (Error analysis of AAttC(X ) and Attn(X), formal version of Lemma 5.5). Assuming the conditions below are satisﬁed: Let Rnd denote the input matrix. Deﬁne Rnd as the approximation version of input matrix. (0, 0.1) denote the approximation error. Let ǫ Suppose we have X ǫ. Let > 1. Assume the value of each entry in matrices can be bounded by . Let the attention matrix Attn(X) Rnn as given in Deﬁnition 3.3. Let the polynomial approximation of attention matrix AAttC(X) tion 4.2. Rnn as given in DeﬁniLet U, Rnk be low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X). Let be polynomial with degree g. We can demonstrate the following: AAttC(X ) Attn(X) O(kM g+1d) ǫ Proof. It can be shown that AAttC(X ) Attn(X) = (AAttC(X (AAttC(X ) ) AAttC(X)) + (AAttC(X) AAttC(X)) + (AAttC(X) Attn(X)) Attn(X)) O(kM g+1d) = O(kM g+1d) ǫ + ǫ ǫ The ﬁrst step is based on simple algebra, the second step is derived using the triangle inequality, the third step is obtained from Lemma C.3 and Lemma E.2, and the ﬁnal step results from basic algebra. C.5 Error Analysis of Up-Interpolation Layer Furthermore, we still need the error analysis of up-interpolation layers. Lemma C.5 (Error Analysis of Up-Interpolation Layer). Assuming the conditions below are satisﬁed: Rhwd denote the input feature map. Rhwd as the approximated input feature map. Let Deﬁne Let φup : Rhwd Rhwd represent the up-interpolation layer deﬁned in Deﬁnition 3.2. Let : be bicubic spline kernel as deﬁned in 3.1. Let ǫ Let (0, 0.1) denote the approximation error. ǫ. Then we have φup(X) φup(X ) 16ǫ Proof. For each [h], [w], [d], we have φup(X)i,j,l φup(X )i,j,l = 2 s=1 2 t=1 2 (s) (X ih +s, jw +t,l (s) (X ih +s, jw +t,l (s) (t) ǫ s=1 2 t=1 2 t=1 s=1 16ǫ +s, jw ih +t,l) (t) h +s, jw ih +t,l) (t) The ﬁrst step is based on Deﬁnition 3.2, the second step is derived using the triangle inequality, [0, 1] and the third step is consequence of basic algebra. ǫ, and the ﬁnal step follows from (x) Then, according to the deﬁnition of the norm, we obtain φup(X) φup(X ) 16ǫ 29 Recall we have deﬁned φup : Rhwc Rhwc in Deﬁnition 3.2. Since there is no non-linear operation in φup, φup is equivalent to matrix multiplication operation, where the dimension of the matrix is Rhwhw. For simplicity, we view φup as Rhwhw dimension matrix in the following proofs. Remark C.6 (Applying φup on input token maps, X1 := where = i=1 hiwi. We denote φup(X) iw i. i=1 Rh1w1d, . . . , Xr Rnd). The actual input of VAR Transformer Layer are Rnd, where [r], Rhrwrd. We denote them as Rnd as applying φup to each Xi Rhiwid for C.6 Error Analysis for VAR Transformer Then, we move forward to show the error propagation analysis for one VAR Transformer Layer. Lemma C.7 (Error propagation analysis for one VAR Transformer Layer, formal version of Lemma 5.6). Assuming the conditions below are satisﬁed: Let Rnd denote the input data matrix. Deﬁne Rnd as the approximation version of X. (0, 0.1) denote the approximation error. Let ǫ Suppose we have Let the VAR Transformer Layer Fvar as given in Deﬁnition 3.5. X ǫ. Let the Fast VAR Transformer Layer Ffvar as given in Deﬁnition 3.6. Let > 1. Assume the value of each entry in matrices can be bounded by . Let the attention matrix Attn(X) Rnn be deﬁned as Deﬁnition 3.3. Let the polynomial approximation of attention matrix AAttC(X) tion 4.2. Rnn as given in DeﬁniLet U, Rnk be low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X). Let be polynomial with degree g. It can be shown that Ffvar(X ) Fvar(X) O(kM g+1d) ǫ Proof. Firstly, we demonstrate that Ffvar(X ) Fvar(X) = AAttC(φup(X O(kM g+1d) O(kM g+1d) ǫ )) 16ǫ Attn(φup(X)) The ﬁrst step is derived from Deﬁnition 3.7 and Deﬁnition 3.6, the second step is obtained using Lemma C.5 and Lemma C.4, and the ﬁnal step follows from basic algebra. 30 C.7 Error Analysis for the Fast VAR Transformer we perform an error analysis of the Fast VAR Transformer in this section. Lemma C.8 (Error analysis of the Fast VAR Transformer, formal version of Lemma 5.7). Assuming the conditions below are satisﬁed: R1d denote the initial input data matrix. Let X0 Let the VAR Transformer Layer Fvar be deﬁned as Deﬁnition 3.5. Let the Fast VAR Transformer Layer Ffvar as given in Deﬁnition 3.6. Let represent the total iteration of the Fast VAR model. For [r], let Ti(X) denote the intermediate variable of the VAR model, as deﬁned in Deﬁnition 3.7. For [r], let Deﬁnition 3.8. Deﬁne Tr(X) Deﬁne Tr(X) Ti(X) denote the intermediate variable of the fast VAR model, as deﬁned in RO(n2)d the ﬁnal output of the VAR Transformer. RO(n2)d as the ﬁnal output of the Fast VAR Transformer. Assume each entry in the matrices can be represented using O(log n) bits. Rnk be low-rank matrices constructed for polynomial approximation of attention Let U, matrix AAttC(X). Let be polynomial with degree g. Then, we can show that the error bound of the ﬁnal output Tr(X) as Proof. We can conduct math induction as the following: Consider the ﬁrst iteration. We can show that. Tr(X0) Tr(X0) 1/ poly(n) T1(X0) = T1(X0) e Fvar(X0) Ffvar(X0) AAttC(φup(X0)) 1/ poly(n) Attn(φup(X0)) The ﬁrst step is based on Deﬁnition 3.8 and Deﬁnition 3.7, the second step is derived from Deﬁnition 3.6 and Deﬁnition 3.5, and the ﬁnal step follows from Lemma E.2. Assume that the following statement is true for the k-th iteration: Then we move forward to consider the + 1-th iteration as the following: Tk(X0) Tk(X0) 1/ poly(n) (4) Tk+1(X0) = Tk+1(X0) Ffvar( O(k Tk(X0)) (poly(n))g+1 1/ poly(n) Fvar(Tk(X0)) (1/ poly(n)) d) 31 The ﬁrst step is based on Deﬁnition 3.8 and Deﬁnition 3.7, the second step is derived from Lemma C.7, the fact that each entry in the matrices can be represented using O(log(n)) bits, and Eq. (4), while the ﬁnal step results from basic algebra. Finally, we can use math induction to show that Tr(X0) Tr(X0) 1/ poly(n). Thus, we complete the proof. e"
        },
        {
            "title": "D Error Analysis of VQVAE Decoder",
            "content": "In this section, we conduct the error analysis of the VQ-VAE Decoder. Firstly, the following lemma presents the error analysis of the Convolution Layer. Lemma D.1 (Error analysis of Convolution Layer). Assuming the conditions below are satisﬁed: Let Rhwcin denote the input feature map. Rhwcout denote the output feature map. Rhwcout denote the convolution layer deﬁned in Deﬁnition 3.9. (0, 0.1) denote the approximation error. Let Let φconv : Rhwcin Let ǫ Let Let > 1. ǫ. Assume the value of each entry in matrices can be bounded by . Let = 9cin denote constant. Then we have φconv(X) φconv(X ) CǫM Proof. For each [h], [w], [cout], we have φconv(X)i,j,l φconv(X )i,j,l = 3 3 cin m=1 3 n=1 3 c=1 cin (Xi+m1,j+n1,c i+m1,j+n1,c) m,n,c m,n,c i+m1,j+n1,c) (Xi+m1,j+n1,c m=1 3 n=1 3 c=1 cin ǫ c=1 n=1 cinǫM m=1 9 = CǫM The 1st step is consequence of Deﬁnition 3.9, the 2nd step is based on the triangle inequality, the 3rd is result of the lemmas conditions, the fourth arises from elementary algebra, and the ﬁnal step stems from the deﬁnition of C. 32 Then, we can show the lemma, which presents the error analysis of the Fast VQ-VAE Decoder. Lemma D.2 (Error analysis of Fast VQ-VAE Decoder ). In the case that the following conditions are satisﬁed: Let Rnd denote the input matrix. Let the up-interpolation Layer φup be deﬁned as Deﬁnition 3.2. Let the convolution layer φconv be deﬁned as Deﬁnition 3.9. Let the attention layer Attn be deﬁned as Deﬁnition 3.4 Let the fast attention layer AAttC be deﬁned as Deﬁnition 4.2. Let the VQ-VAE Decoder be the composition of constant number of up-interpolation layers, convolutions layers, and attention layers. Let the Fast VQ-VAE Decoder be deﬁned as substituting all Attn layers in VQ-VAE with AAttC layers. Then, we can show that the approximation error of the Fast VQ-VAE Decoder can be bounded by 1/ poly(n). Proof. By Lemma C.7, we have shown that fast attention computation will introduce an approximation error no more than 1/ poly(n). Then, by Lemma C.5 and Lemma D.1, we still can bounded have shown the approximation error by 1/ poly(n) after passing another up-interpolation layer, convolutions layer. Since VQ-VAE is composition of constant number of up-interpolation layers, convolution layers, and attention layers, the overall approximation error can still be bounded by 1/ poly(n)."
        },
        {
            "title": "E Running Time",
            "content": "In this section, we conduct the running time analysis of every component of the VAR model and the fast VAR model. In Section E.1, we conduct the running time analysis of the VAR transformer and the fast transformer. In Section E.2, we conduct the running time analysis of the feature map reconstruction block. In Section E.3, we conduct the running time analysis of the VQVAE Decoder and fast VQVAE Decoder. E.1 Phase 1: Running Time of Token Maps Generation In this section, we present lemma on the time complexity of the token maps generation phase of VAR and reference the results in [AS23] to derive faster variant of VAR. Lemma E.1 (Running time of VAR Transformer). Assuming the conditions below are satisﬁed: Let denote the total number of the token maps. Let [K]. Let denote the embedding size of each token. Let r1 R11d denote the ﬁrst scale token map. 33 Let α > 1 denote the growth rate of the height and width of the token map at each level. Then Rαk1αk1d. [K], the k-th token map rk for Let rK Rnnd denote the last scale token map, where = αK1. then the time complexity of VAR generation is O(n4d). Proof. Firstly, we consider the k-th autoregressive token map generation. We use Lk to denote the total number of the tokens input into the VAR transformer at k-th step and Lk can be calculated as the following: (αi1)2 Lk = = i=1 α2k α2 α2k 1 1 1 α2 α2k 0.5α2 = α2k2 In the ﬁrst step, we use the condition of this lemma. The second and third steps are consequence of basic algebra. The fourth step is due to α 2, and the last step is derived from elementary algebra. RLkd to denote the input matrix. The transformer Thus, at the k-th step, we use Xk computation cost at the k-th step is O(L2 kd). We then sum up the computation time across all steps: (5) = O((L 1 + L2 2 + + L2 K) d) loga n+1 O( (2 Xk=1 loga n+ α2k2)2 d) 4α4k4 d) = O( k=1 = O(n4d) In the ﬁrst step, the total time is obtained by summing the times for each k-th step, while the second step is consequence of Eq. (5), and the ﬁnal step is attributed to elementary algebra. Then, we show lemma that demonstrates fast way to compute attention in [AS23]. Lemma E.2 (Fast Attention Computation, Theorem 1.4 of [AS23]). Let AAttC be deﬁned as Deﬁnition 4.2. Then we have AAttC(n, = log n, = (log n), δ = 1/ poly(n)) can be solved in time Tmat(n, no(1), d) = n1+o(1). Now we can apply the result Lemma E.2 to the VAR Transformer. Lemma E.3 (Running time of Fast VAR Transformer, formal version of Lemma 5.1). Assuming the conditions below are satisﬁed: 34 Denote as the total number of generated token maps. [K]. Let Let r1 Let α > 1 denote the growth rate of the height and width of the token map at each level. Then Rαk1αk1d. R11d denote the ﬁrst scale token map. [K], the k-th token map rk for Let rK Rnnd denote the last scale token map, where = αK1. Let = O(log(n)) denote embedding size of each token. Then, the total running time of the VAR Transformer for generating token maps can be accelerated to O(n2+o(1)d). Proof. To generate the k-th token map, let the input of this step be Lk α2k2 RLkd. And we have (6) where this step is consequence of Eq. (5). So the transformer computation cost at the k-th step can be improved from O(L2 kd) by using the result of Lemma E.2. Then, we sum up the computation time across all steps: kd) to O(L1 = O((L1 + L2 + logα n+1 + LK) α2k2)1+o(1) d) d) (2 O( Xk=1 = O(n2+o(1)d) The computation time in the ﬁrst step is determined by summing all k-th step times, while the second step is consequence of Eq. (6), and the last step is attributed to elementary algebra. E.2 Phase 2: Running Time of Feature Map Reconstruction In this section, we analyze the running time of the feature map reconstruction layer. Lemma E.4 (Running time of Feature Map Reconstruction Layer, formal version of Lemma 5.2). Assuming the conditions below are satisﬁed: Deﬁne as the total number of the token maps. Let [K]. Let denote the embedding size of each token. Let r1 Let α > 1 denote the growth rate of the height and width of the token map at each level. Then Rαk1αk1d. R11d denote the ﬁrst scale token map. [K], the k-th token map rk for Let rK Rnnd denote the last scale token map, where = αK1. then the total runtime of the VAR models for feature map reconstruction is O(n2d2 log n). 35 [K], VAR Model will up-interpolate token map rk Proof. For each Rnnd by using bicubic interpolation deﬁned in Deﬁnition 3.2. Speciﬁcally, the computation of each entry of requires 32 multiplications and 16 additions (see more details in Deﬁnition 3.2). Thus, the computation cost for the up-interpolation of each token map is Rαk1αk1d to k up = 48 n2d = O(n2d) There are total O(log n) token maps needed to be up-interpolated, so the total time for upinterpolation is Tup = O(n2d) = O(n2d log n) O(log n) The computation time in the ﬁrst step is determined by summing the log(n) up-interpolation time, and the second step is consequence of elementary algebra. Furthermore, to address the information loss in the up-interpolation process, the VAR Model 1, . . . , r generated by up-interpolation. uses convolution operation φ( ) on the token map { d, and the convolution layer does not change the 3 We assume the convolution kernel size is 3 [K], φ(r dimension of each token map, i.e., for each i) i), it needs O(d) operations. Then, we can have the convolution computation time for one token map is Rnnd. Hence, for every entry in φ(r } conv = O(d) n2d = O(n2d2) In the ﬁrst step, the total computation time is obtained by adding the times for the entries, while the second step results from simple algebra. There are total O(log n) token maps needed to be passed to the convolution layer, so the total time for convolution operations is Tconv = O(log n) = O(n2d2 log n) O(n2d2) In the ﬁrst step, the total time is obtained by O(log(n)) convolution operations, while the second step results from simple algebra. Then, the VAR Model will sum up O(log n) token maps processed by convolution layers, where each token map has size of d. Thus, the computation cost of addition needs Tadd = O(log n) = O(n2d log n) (n2d) In the ﬁrst step, token maps are added element-wise, and there are O(log(n)) token maps in total, while the second step results from basic algebra. Hence, the running time of feature map reconstruction is as follows: Trc = Tup + Tconv + = O(n2d2 log n) Tadd The ﬁrst step is derived by summing the times for up-interpolation operations, convolution operations, and token map additions, while the second step is due to basic algebra. 36 E.3 Phase 3: Running Time of VQ-VAE Decoder In this section, we analyze the running time of the VQ-VAE Decoder and fast VQ-VAE Decoder. Lemma E.5 (Running time of VQ-VAE Decoder). Assuming the conditions below are satisﬁed: Let k1, k2, k3 Given be constant numbers. Rnnd as the input feature map. Assume that there are k1 up-interpolation layers φup deﬁned in Deﬁnition 3.2. Given feature map up(M ) Rhwd. For RO(h)O(w)d. output φi [k1], we assume i-th up-interpolation layers We assume there are k2 attention layers Attn deﬁned in Deﬁnition 3.4. Rhwd. For Given feature map [k1], the i-th attention layers output Attn(M ) Rhwd. We assume there are k3 convolution layers φconv deﬁned in Deﬁnition 3.9. Rhwd. For Given feature map up(M ) RhwO(d). output φi [k1], we assume i-th up-interpolation layers then the total running time of the VQ-VAE Decoder is O(n4d). Proof. By the condition, we can have that for each intermediate layer (up-interpolation layer, convolution layer, attention layer) is O(n) The running time can be computed as follows: [k1 +k2 +k3], the size of the output of any O(d). O(n) Part 1. Running time of Up-interpolation layers. For each RO(n)O(n)O(d) as the output feature map from the l-th up-interpolation layer. For every entry of l, it needs 32 multiplications and 16 additions (see more details in Deﬁnition 3.2). Thus, the computation cost for the feature map is [k1], we assume up = 48 = O(n2d) O(n) O(n) O(d) The ﬁrst step is derived by summing the computation costs for each entry of l, while the second step is due to basic algebra. Since there are k1 up-interpolation layers in total, the total time of the up-interpolation layers in the VQ-VAE decoder is O(n2d) Tup = k1 = O(n2d) (7) The ﬁrst step is derived by summing the computation costs for each up-interpolation layer, while the second step is due to the fact that k1 is constant number. Part 2. Running time of Attention layers. For each RO(n)O(n)O(d) as the feature map used as input for the l-th attention layer. We can consider the input of this attention layer as sequence with length O(n2) and embedding dimension O(d). Hence, the computation cost of this attention layer is [k2], we assume l attn = O(n4d) 37 Since there are k2 attention layers in total, the total time of the attention layers in the VQ-VAE decoder is Tattn = k2 O(n4d) = O(n4d) The ﬁrst step is derived by summing the computation costs for each up-interpolation layer, while the second step is due to the fact that k2 is constant. Part 3. Running time of Convolution layers. For each RO(n)O(n)O(d) as the output feature map of the l-th convolution layer. For every entry of l, it needs O(d) operations. Thus, the computation cost for the feature map is [k3], we assume l conv = O(d) O(n2d) = O(n2d2) The ﬁrst step is derived by summing the computation costs for each entry of l, while the second step stems from basic algebra. Since there are k3 convolution layers in total, the total time of the convolution layers in the VQ-VAE decoder is Tconv = k3 O(n2d2) = O(n2d2) (8) The ﬁrst step is derived by summing the computation costs for each convolution layer, while the second step is due to the fact that k3 is constant. Finally, the computation cost of the VQ-VAE decoder can be calculated as follows: Tdec = Tup + = O(n4d) Tattn + Tconv The ﬁrst step results from summing the computation costs of up-interpolation, attention, and convolution layers, while the second step is consequence of d. Then, we move forward to show the running time of the fast VQ-VAE decoder. Lemma E.6 (Running time of Fast VQ-VAE Decoder, formal version of Lemma 5.3). In the case that the following conditions are satisﬁed: Assume k1, k2, k3 We represent the input feature map by are constant numbers. Rnnd. We assume there are k1 up-interpolation layers φup deﬁned in Deﬁnition 3.2. Given feature map up(M ) output φi RO(h)O(w)d. Rhwd. For [k1], we assume i-th up-interpolation layers We assume there are k2 attention layers Attn deﬁned in Deﬁnition 3.4. Rhwd. For Given feature map [k1], the i-th attention layers output Attn(M ) Rhwd. 38 We assume there are k3 convolution layers φconv deﬁned in Deﬁnition 3.9. Rhwd. For [k1], we assume i-th up-interpolation layers Given feature map up(M ) output φi RhwO(d). then the total runtime of the VQ-VAE decoder can be accelerated to O(n2+o(1)d). Proof. As the same in Eq. (7) and Eq. (8), the computation cost for up-interpolation layers and convolution layers in VQ-VAE decoder still needs O(n2d) and O(n2d2), respectively. For each [k2], we assume l1 RO(n)O(n)O(d) as the input feature map for the l-th attention layer. We can consider the input of the attention layer as sequence with length O(n2) and embedding dimension O(d). By using the result of Lemma E.2, the computation cost of can be speed up to Since there are k2 attention layers in total, the total computation cost of the attention layers in the VQ-VAE decoder is attn = O(n2+o(1)d) Tattn = k2 O(n2+o(1)d) = O(n2+o(1)d) The computation cost in the ﬁrst step is obtained by adding the costs of the up-interpolation layers, attention layers, and convolution layers, while the second step stems from k2 is constant. Thus, the total runtime of the VQ-VAE decoder can be calculated as follows: Tdec = Tattn + Tup + = O(n2+o(1)d) Tconv The computation cost in the ﬁrst step is obtained by adding the costs of the up-interpolation layers, attention layers, and convolution layers, while the second step comes from d."
        }
    ],
    "affiliations": [
        "The Simons Institute for the Theory of Computing at UC Berkeley",
        "The University of Hong Kong",
        "Tsinghua University",
        "University of Wisconsin-Madison"
    ]
}