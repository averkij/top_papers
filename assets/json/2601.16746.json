{
    "paper_title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents",
    "authors": [
        "Yuhang Wang",
        "Yuling Shi",
        "Mo Yang",
        "Rongrui Zhang",
        "Shilin He",
        "Heng Lian",
        "Yuting Chen",
        "Siyu Ye",
        "Kai Cai",
        "Xiaodong Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact."
        },
        {
            "title": "Start",
            "content": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents Yuhang Wang1, Yuling Shi1, Mo Yang2, Rongrui Zhang1, Shilin He3 Heng Lian1, Yuting Chen1, Siyu Ye3, Kai Cai3, Xiaodong Gu1 1LLMSE Lab, Shanghai Jiao Tong University 2Sun Yat-sen University 3Douyin Group Abstract LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers selectively skim source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., focus on error handling) as hint to guide the pruning targets. lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruners effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84 compression on single-turn tasks like LongCodeQA with minimal performance impact. Project Page: https://github.com/Ayanami1314/swe-pruner 6 2 0 J 3 2 ] . [ 1 6 4 7 6 1 . 1 0 6 2 : r (a) Claude Sonnet 4.5 (b) GLM 4. Figure 1 Efficiency analysis on SWE-Bench Verified. SWE-Pruner (orange) achieves substantial reductions on Token Cost and Agent Rounds for the base Mini SWE Agent (gray) for both Claude Sonnet 4.5 and GLM 4.6. Equal Contribution Corresponding author: xiaodong.gu@sjtu.edu.cn 1 2 MOTIVATION"
        },
        {
            "title": "Introduction",
            "content": "2 Large Language Models (LLMs) have rapidly progressed in software engineering tasks, from simple code understanding (Chen et al., 2021; Li et al., 2023b; Yang et al., 2025; Hu et al., 2025) to interactive agents capable of navigating repositories, running tests, and submitting patches end-to-end (Chen et al., 2025; Li et al., 2025; Xiao et al., 2025). Recent coding agents like Claude Code (Anthropic, 2025) and Gemini CLI (Google, 2025) have augmented LLMs with sophisticated toolchains including terminals, editors, and file search, enabling multi-step reasoning over complex codebases. Despite this progress, critical bottleneck remains: the accumulation of context length within the constrained window of LLMs. Navigating real-world software repositories confronts agents with massive Context Wall. Although long-context models have emerged (Achiam et al., 2023; Team et al., 2024), blindly ingesting large volumes of code incurs prohibitive inference costs (Jiang et al., 2023) and introduces severe noise, leading to attention dilution and hallucinations (Liu et al., 2023a; Li et al., 2023a; Wang et al., 2025b). Although context compression techniques offer potential remedy, existing approaches face critical limitations when applied to coding agents. Prior work on context compression primarily targets natural language (Jiang et al., 2023; Li et al., 2023c; Jiang et al., 2024; Mu et al., 2023), leading to severe trade-offs when applied to code (Shi et al., 2025): token pruning methods compromise syntactic validity, while abstractive techniques discard character-level information critical for debugging (Jiang et al., 2023; Li et al., 2023c). Beyond structural concerns, these methods are fundamentally misaligned with coding agent requirementsthey operate with static compression ratios and task-agnostic criteria (e.g., perplexity), unable to dynamically prioritize context based on the evolving goals of multi-turn agent interactions (Zhang et al., 2024; Yang et al., 2024a). To address these limitations, we introduce SWE-Pruner, self-adaptive pruning framework tailored for coding agents. Our key insight stems from how both programmers and agents navigate large codebases: rather than reading code line by line, they employ goal-driven selective attention that programmers quickly scan to locate relevant sections based on their goals (e.g., find error handling logic), while agents use tools like grep or file search to coarsely retrieve targeted code regions including excessive surrounding context. SWE-Pruner refines this process by enabling agents to articulate explicit natural language goals alongside retrieval actions. To capture the dynamic, task-specific objectives of agent workflows, we train lightweight 0.6B pruning model on 61K synthetic data, enabling adaptive selection conditioned on given goals. By operating at line-level granularity, our approach preserves syntactic and structural integrity while achieving precise compression that distills coarse retrieval results into focused, task-relevant context. We evaluate SWE-Pruner across multi-turn agent tasks (SWE-Bench Verified (Jimenez et al., 2024) and SWEQA (Peng et al., 2025)) and single-turn long-context understanding tasks (Long Code Completion (Guo et al., 2023) and Long Code QA (Rando et al., 2025)). Across models and benchmarks, SWE-Pruner consistently delivers substantial efficiency gains with minimal performance degradation. Beyond token savings (e.g., 39% reduction on SWE-Bench Verified with Claude Sonnet 4.5, as illustrated in Figure 1), the focused context also improves agent decision quality, reducing interaction rounds by up to 26% through more decisive reasoning and fewer redundant exploratory actions. In summary, we make the following contributions: We propose novel pruning framework for coding agents that performs task-aware, line-level pruning to alleviate the context wall problem. We design novel architecture with lightweight backbone of only 0.6B parameters for efficient inference with adaptive, task-dependent pruning. Evaluations across various benchmarks demonstrate 2354% token reduction on agent tasks and up to 14.8 compression on single-turn tasks with minimal performance impact."
        },
        {
            "title": "2 Motivation",
            "content": "In practice, coding agents spend an excessive amount of their token budget on repeatedly exploring the codebase (Majgaonkar et al., 2025; Xiao et al., 2025). To quantify this, we perform preliminary analysis of 2 3 APPROACH 3.2 Figure 2 Token cost distribution over different tool calls for Mini-SWE-Agent on SWE-Bench Verified with Claude Sonnet 4.5. Read operations dominate token consumption at 76.1%, motivating the need for context pruning mechanisms. the trajectories by Mini SWE Agent (Yang et al., 2024b). The agent is instantiated with Claude Sonnet 4.5 on SWE-Bench Verified. We extract action types from the trajectories and categorize them into three types: 1) read, which denotes file and directory inspection operations such as cat, grep, and head; 2) execute, which means running programs or scripts for testing, and 3) edit, representing in-place modifications. We count the total tokens consumed in each agent round. As shown in Figure 2, read -type operations consume an overwhelming 76.1% of total tokens, significantly exceeding execute (12.1%) and edit (11.8%) operations combined. This issue is further exacerbated in multi-round agent interactions, where code retrieved in earlier rounds persists in the context and accumulates as the interaction progresses. Similar patterns are observed with other backbone models like GLM-4.6 (Section A). This empirical pattern reveals critical opportunity for context optimization. When navigating unfamiliar codebases, agents must extensively explore through coarse-grained file operations that stream entire files or blocks into the context. While this exploratory strategy is necessary for understanding the codebase structure, it causes substantial token costs and introduces redundant content that accumulates across rounds. Addressing this challenge requires context pruning mechanism that is context-aware (capable of identifying relevant information based on the agents current goal) and lightweight enough to avoid introducing additional latency. These observations strongly motivate SWE-Pruner, an lightweight pruning framework that enables more efficient token allocation throughout the problem-solving trajectory."
        },
        {
            "title": "3.1 Overview",
            "content": "We introduce SWE-Pruner, framework that prunes long code contexts for agents through task-aware, adaptive filtering. As illustrated in Figure 3, SWE-Pruner operates as middleware between coding agents and their environment. When an agent issues file-reading commands (e.g., grep, cat), the raw retrieved contextoften thousands of linesis captured and filtered before reaching the agent. To guide this pruning, the agent generates Goal Hint describing its current information need (e.g., Focus on MRO resolution logic). lightweight neural skimmer is trained to adaptively selects relevant lines from the raw context, while preserving code structure. Only this pruned context (relevant and low-noise) is returned to the agent. The framework comprises three core components as introduced in the following sections, respectively. First, the target agent is instructed to provide goal hint indicating the current information need (Section 3.2). Next, lightweight neural skimmer is designed to score context tokens and aggregate relevant lines (Section 3.3). Finally, we integrate the skimmer into real agentic systems with minimal modifications (Section 3.4). 3 3 APPROACH 3.3.0.0 Figure 3 Overview of SWE-Pruner. Left: The Interaction Workflow demonstrates how SWE-Pruner functions as middleware between the Coding Agent and the Environment. It intercepts the Raw Context from file operations and delivers Pruned Context to the agent. Right: The Pruning Pipeline details the internal mechanism. Based on specific goal hint from the coding agent, the neural skimmer processes the raw context through line-level scoring and adaptive selection to deliver the pruned context."
        },
        {
            "title": "3.2 Goal Hint Generation",
            "content": "Central to our framework is the Goal Hinta natural language description of the agents current information need. Rather than relying on keyword-based filtering, we instruct the agent to generate goal hints as complete, self-contained questions (e.g., How is authentication handled?) that capture the semantic intent of its current reasoning step (detailed prompt in Section J). To enable agents to communicate these goal hints to our pruning system, we augment standard file manipulation tools (e.g., cat, grep) with an optional context_focus_question parameter. When provided, this parameter passes the goal hint to the skimmer, which then filters large outputs for query-relevant content. When omitted, the skimmer is bypassed and full outputs are returned, preserving backward compatibility. This lightweight wrapper design (illustrated below) requires minimal modifications to existing agent infrastructures, enabling seamless integration without disrupting established workflows. (cid:7) (cid:4) # Original tool ( unchanged ) def grep ( file_path , pattern ) : # ... original grep implementation ... return matches # New tool with pruner def grep_with_pruner ( file_path , pattern , on tex t_ foc us_ qu est io = None ) : # Call original tool raw_output = grep ( file_path , pattern ) if on ex _f cu _ qu st ion : # Prune if hint provided return prune ( raw_output , co nt ext _f ocu s_q ue sti on ) return raw_output # Bypass pruner otherwise (cid:6) (cid:5)"
        },
        {
            "title": "3.3 Lightweight Neural Skimmer",
            "content": "Model Architecture. We formulate context pruning as reranking problem: Given the agent context = {x1, x2, ..., xn} where each xi represents token, and query representing the agents current goal, the 4 3 APPROACH 3.4 model computes relevance score si for each token through neural scoring function: si = F(q, xiC; θ) where represents the scoring function parameterized by θ that evaluates token xi in the context of both the query and the full code context C. To enable line-level pruning decisions, we aggregate token scores to the line level. Let = {l1, ..., lm} denote the set of lines obtained by splitting C, and let Tj denote the set of tokens in line lj. The line-level relevance score sj is computed as the average of its constituent token scores: (1) sj = 1 Tj (cid:88) tTj st (2) This averaging operation ensures that lines are evaluated based on their overall relevance rather than being dominated by few high-scoring tokens, maintaining the semantic coherence necessary for code comprehension. We adopt the Qwen3-Reranker-0.6B (Zhang et al., 2025) as our backbone due to its efficiency and pre-trained knowledge of code structures. During inference, line lj is retained if its aggregated score sj exceeds predefined threshold τ . The model processes retrieved chunks in parallel to minimize latency. Given its lightweight architecture of 0.6B parameters, pruning overhead is negligible compared to the token savings for downstream agent LLMs. See Algorithm 1 for detailed inference steps. Training Objective. We train the pruning model by minimizing the conditional random field negative log likelihood (CRF-NLL) (Zheng et al., 2015). Unlike mere binary cross entropy, CRF explicitly models transition probabilities between retain/prune states, enabling the model to learn line-level retention decisions while capturing sequential dependencies. Given the context representation = x1, . . . , xn and silver labels y, the pruning head minimizes the following loss function: Lcompress = 1 (cid:88) i=1 LCRF-NLL(xi, yi) Li (3) where xi denotes the learned feature representation for each token; Li is the sequence-length normalization, which prevents bias toward aggressive pruning in long contexts. We keep the original reranking head in Qwen Reranker (Bai et al., 2023) to preserve document-level relevance scoring capability. The reranking head minimizes Lrerank = MSE(spred, sref) between predicted and reference document-level relevance scores in [0, 1]. The final objective combines both heads with balancing weight λ: More details about the architecture, CRF formulations, and training configurations are provided in Section B. Ltotal = (1 λ) Lcompress + λ Lrerank (4) Constructing Training Data. Training the neural skimmer requires data with line-level supervision that preserves both relevance and code structure. Since such dataset is unavailable, we construct polyglot training corpus following teacher-student paradigm. We sample code snippets from GitHub1, curated collection of high-quality repositories, and utilize teacher LLM (Qwen3-Coder-30B-A3B-Instruct (Team, 2025)) to synthesize task-oriented queries that target specific functional subsets of each snippet. This process produces training quadruplets (q, C, M, S), where is binary line-level mask indicating which lines to retain and is document-level relevance score. To ensure generalization across diverse coding scenarios, we design queries based on taxonomy of nine distinct agentic tasks, such as Code Debugging, Feature Addition, and Code Refactoring, covering common information needs in real-world agentic workflows. We employ an LLM-as-a-Judge filtering mechanism (Wang et al., 2025a; He et al., 2025) with Qwen3-Next-80B-A3B-Thinking (Team, 2025) to ensure annotation quality. This rigorous filtering yields final training corpus of 61,184 high-quality samples with verified line-level annotations. Complete details on task taxonomy, data statistics, and generation configurations are provided in Section C. 1https://huggingface.co/datasets/nick007x/github-code5 5 RESULTS 5.1.0.0 3."
        },
        {
            "title": "Integration with Agentic Workflows",
            "content": "The trained skimmer is deployed in real coding agents to accomplish specific tasks. Our framework flexibly adapts to different task scenarios. For multi-turn agent tasks (e.g., SWE-Bench), agents dynamically generate Goal Hints at each round based on their evolving reasoning trace, enabling them to shift focus from high-level navigation to detailed debugging as needed. For single-turn tasks with inherent queries (e.g., code question answering), the task description serves as the initial Goal Hint, though agents can refine it in subsequent retrieval rounds. This flexibility allows agents to seamlessly transition between broad exploration (no pruning) and focused investigation (with pruning) as their information needs evolve."
        },
        {
            "title": "4.1 Benchmarks and Agents",
            "content": "We evaluate SWE-Pruner on four benchmarks spanning single-turn and multi-turn scenarios. For single-turn tasks, we use Long Code Completion (Guo et al., 2023) (500 Python examples with 5K+ token contexts) and Long Code QA (Rando et al., 2025) (question answering on long code contexts up to 1M tokens), evaluating under 4x and 8x compression constraints. For multi-turn agent tasks, we use SWE-Bench Verified (Jimenez et al., 2024) (500 real-world GitHub issues requiring patch generation) and SWE-QA (Peng et al., 2025) (repository-specific question answering across three repositories). Specifically, we integrate SWE-Pruner into Mini SWE Agent (Yang et al., 2024b) for SWE-Bench Verified and OpenHands (Team, 2024) for SWE-QA, evaluating with Claude Sonnet 4.5 and GLM-4.6 backbone models. Detailed benchmark descriptions, agent configurations, and experimental settings are provided in Section D."
        },
        {
            "title": "4.2 Baselines",
            "content": "We compare SWE-Pruner against representative methods for compressing code context and environment observations. Full Context and No Context establish upper and lower performance bounds. For compression baselines, we evaluate LLMLingua-2 (Pan et al., 2024) and Selective-Context (Li et al., 2023c) which perform token-level pruning based on perplexity and self-information respectively, RAG which retrieves code chunks via embedding similarity using UniXCoder (Guo et al., 2022), and LongCodeZip (Shi et al., 2025) which leverages program structure for compression. For multi-turn agent tasks, we additionally compare with LLM Summarize that generates abstractive summaries using the backbone model. All baselines are configured to match 4x and 8x compression constraints under identical experimental settings. We did not compare with agent history compression methods (Ye et al., 2025; Kang et al., 2025). While these methods effectively manage long-horizon agent interactions, they tackle different problem to ours. They aim at compressing agents prior interaction trajectories, while our method compresses agents observations like repository content, making them incomparable to our approach."
        },
        {
            "title": "4.3 Metrics",
            "content": "We evaluate both task performance and compression efficiency. Task performance is measured through Edit Similarity (ES) and Exact Match (EM) for code completion (Guo et al., 2023), Accuracy for question answering (Rando et al., 2025), Resolve Rate on SWE-Bench Verified (Jimenez et al., 2024), and Average LLM-as-a-Judge Score on SWE-QA (Peng et al., 2025). Compression efficiency is quantified via Compression Ratio (1/τ = Coriginal/Ccompressed), absolute Token Consumption, interaction Rounds, and API Cost ($)."
        },
        {
            "title": "5.1 Performance on Multi-Turn Tasks",
            "content": "Evaluation across Diverse Tasks. We integrate SWE-Pruner into the Mini SWE Agent (Yang et al., 2024b) framework for SWE-Bench Verified (Jimenez et al., 2024) and the OpenHands (Team, 2024) framework for SWE-QA (Peng et al., 2025), evaluating with different backbone models. On SWE-Bench Verified, SWE-Pruner achieves substantial token reductions of 2338% across models while maintaining nearly identical success rates (less than 1% degradation). Notably, interaction rounds decrease by 1826%. By filtering 6 5 RESULTS 5.1.0.0 Agent Rounds Solved Success (%) Tokens (M) Cost ($) Mini SWE Agent (Claude Sonnet 4.5) + SWE-Pruner Mini SWE Agent (GLM 4.6) + SWE-Pruner 51.0 41. 49.3 36.6 353/500 351/500 277/500 274/500 70.6 70.2 55.4 54.8 0. 0.504 0.70123.1% 0.36926.8% 0.791 0.055 0.48838.3% 0.03536.4% Table 1 Results on SWE-Bench Verified. SWE-Pruner reduces token consumption by 2338% while maintaining comparable success rates. Method Avg Score Avg Rounds Tokens (K)"
        },
        {
            "title": "Streamlink",
            "content": "Claude Sonnet 4.5 + SWE-Pruner GLM-4.6 + SWE-Pruner 8.36 8.59 0.23 8.56 8.560.00 Reflex Claude Sonnet 4.5 + SWE-Pruner GLM-4.6 + SWE-Pruner 8.68 8.85 0.17 8.37 8.230.14 Conan Claude Sonnet 4.5 + SWE-Pruner GLM-4.6 + SWE-Pruner 8.70 8.84 0.14 8.58 8.450.13 23.4 23. 18.2 25.0 33.2 32.4 26.1 36.7 23.9 23.5 21.4 27.7 611.2 557.18.9% 318.2 145.154.4% 1081.6 866.819.9% 142.3 101.228.9% 654.7 520.720.5% 175.9 116.633.7% Table 2 Results on SWE-QA across different repositories. SWE-Pruner achieves 2954% token reduction on Streamlink, Reflex, and Conan repositories with minimal impact on task performance. redundant information while preserving task-relevant code, SWE-Pruner enables agents to locate issues more precisely and make more decisive decisions, thereby reducing repeated exploratory file reads and completing tasks earlier. We conduct in-depth case studies in Section to illustrate these behavioral differences, showing how context pruning transforms both failure-to-success scenarios (with 83.3% token reduction) and successful trajectories (with 30.2% reduction in peak prompt length). On SWE-QA, similar efficiency gains emerge with 2954% token reduction across repositories. Interestingly, we observe that GLM-4.6 exhibits increased rounds (2941% more) while Claude Sonnet 4.5 maintains similar round counts with minor variations (within 3%). Through trajectory analysis, we find that after pruning, GLM tends to explore more files before formulating answers, suggesting more conservative reasoning strategy when presented with focused context. Nevertheless, the overall token consumption remains substantially lower, demonstrating that SWE-Pruner maintains efficiency advantages regardless of the agents exploration patterns. These model-agnostic efficiency gains directly translate to proportional cost savings. Detailed analysis is provided in Section E. Comparison with Alternative Context Management Strategies. To understand what design choices contribute to SWE-Pruners effectiveness, we compare against three categories of baselines: token-level compression (LLMLingua2), coarse-grained retrieval (RAG), and generative summarization (LLM Summarize). Due to computational cost considerations, we conduct this comparison on random subset with 50 samples of SWE-Bench Verified following Xia et al. (2025); Chen et al. (2024). As shown in Table 3, our method achieves the best success rate (64%), outperforming the vanilla agent baseline (62%) despite using 31% fewer tokens. In contrast, LLMLingua2 and RAG degrade performance substantially (to 54% and 50%), likely because token-level pruning disrupts code syntax while coarse-grained retrieval misses fine-grained implementation 5 RESULTS 5.3 Method Rounds Success (%) Tokens (M) Mini SWE Agent + LLMLingua2 + RAG + LLM Summarize + LongCodeZip + SWE-Pruner 52.3 42.1 40.2 41.3 44.3 41. 62.0 54.0 50.0 56.0 54.0 64.0 0.972 0.856 0.771 0.794 0.889 0.670 Table 3 Comparison of context compression strategies on SWE-Bench. SWE-Pruner achieves highest success rate with lowest token usage. Methods Full No Context Selective-Context LLMLingua2 RAG LongCodeZip SWE-Pruner Long Code Completion Long Code QA 4x constraint 8x constraint 4x constraint 8x constraint 1/τ ES 1.0 64.65 44.90 52.48 49.47 58.97 57.77 58.63 3.27 3.32 3.29 2.77 5.56 EM 40.5 13. 22.0 15.5 30.5 28.0 31.5 1/τ ES 1.0 64.65 44.90 48.67 44.74 55.82 56.08 57.58 7.49 7.89 6.60 7.85 10.92 EM 40.5 13.5 17.0 13.0 29.0 27.5 31.0 1/τ Acc 1/τ Acc 1.0 54.05 38.39 55.36 55.36 58.04 52.25 59.46 3.69 3.57 3.06 3.98 13.95 1.0 54.05 38.39 51.79 51.33 55.86 54.95 58.71 7.32 7.68 5.87 7.39 14.84 Table 4 Main results on Long Code Completion and Long Code QA tasks. The table compares the performance and compression ratio (1/τ ) under 4x and 8x constraints. SWE-Pruner demonstrates superior effectiveness in maintaining high performance while achieving significant context compression. details. LLM Summarize achieves moderate performance (56%) but incurs additional latency. These results validate our design choice of line-level granularity with task-aware filtering, striking an optimal balance between compression and information retention. The effectiveness of SWE-Pruner is further validated by consistent improvements on single-turn tasks  (Table 4)  , where we observe similar advantages across diverse compression constraints."
        },
        {
            "title": "5.2 Performance on Single-Turn Tasks",
            "content": "While SWE-Pruner is designed for coding agents, its query-aware, line-level pruning mechanism generalizes to single-turn tasks. We evaluate on Long Code Completion and Long Code QA under 4x and 8x compression constraints with Qwen2.5-Coder-7B-Instruct (Hui et al., 2024). These benchmarks require direct completion or question answering without iterative exploration. Notably, SWE-Pruner achieves substantially higher effective compression ratios than baselines configured at identical compression targets. We also validate with Seed-Coder-8B-Instruct (ByteDance Seed et al., 2025), obtaining similar results (see Section G). On Long Code Completion  (Table 4)  , SWE-Pruner achieves up to 10.92x effective compression under the 8x constraint while maintaining an Edit Similarity (ES) score of 57.58 and an Exact Match (EM) rate of 31.0. As compression constraints tighten, token-level methods (LLMLingua2, Selective-Context) experience rapid performance degradationSelective-Context drops to 48.67 ES under 8x constraintwhereas SWE-Pruner maintains stable performance through line-level granularity that preserves syntactic structure. Under the 4x constraint, SWE-Pruner achieves 5.56x compression with 58.63 ES and 31.5 EM, outperforming all baselines. On Long Code QA, the advantages become even more pronounced. SWE-Pruner achieves 14.84x compression under 8x constraint with 58.71% accuracy, substantially exceeding other baselines. Under the 4x constraint, our method achieves 13.95x compression while maintaining 59.46% accuracy, demonstrating that task-aware line-level pruning excels at identifying relevant code segments for question answering. This highlights the synergy between query-aware filtering and line-level granularity, validating that our approach generalizes 8 6 RELATED WORKS 6.0.0.0 Figure 4 First token latency comparison across different sequence lengths. SWE-Pruner maintains consistently low latency below 100 ms. effectively to single-turn code understanding tasks."
        },
        {
            "title": "5.3 Efficiency Impact",
            "content": "Beyond token reduction, critical consideration for practical deployment is the computational overhead introduced by the skimmer itself. As shown in Figure 4, our approach maintains remarkably low and stable first token latency across all sequence lengths, remaining below 100ms even at 8K tokens. In contrast, larger generative models exhibit exponential latency growth, with Qwen3-32B exceeding 1200ms and closed-source models incurring even more overhead. Since our skimmer employs only 0.6B encoder, its computational cost is negligible and readily amortized by the reduced decoding cost from context compression. This lightweight characteristic validates the practical viability of our approach for real-world applications. Detailed latency measurements are provided in Table 6."
        },
        {
            "title": "6 Related Works",
            "content": "Prompt Compression Prompt compression has been extensively studied for both natural language and code. Token-level pruning methods such as LLMLingua (Jiang et al., 2023, 2024), Selective Context (Li et al., 2023c), and AttentionRAG (Fang et al., 2025), along with embedding/retrieval-based approaches like RAG (Lewis et al., 2020), XRAG (Cheng et al., 2024), and repo-level retrieval for code completion (Zhang et al., 2023), can reduce prompt length on single-round tasks but often fail to preserve syntactic structures essential for code correctness (Shi et al., 2025). Code-specific methods including DietCode (Zhang et al., 2022), SlimCode (Wang et al., 2024b), and LongCodeZip (Shi et al., 2025) address structural concerns but are primarily evaluated on single-round proxy tasks (e.g., code search, completion) rather than multi-round agentic workflows. Moreover, these approaches typically apply static, content-only compression that lacks contextual awareness, leading to fixed behaviors such as indiscriminately removing comments (Yang et al., 2024a) regardless of task requirements. In contrast, SWE-Pruner performs line-level pruning with dynamic, queryconditioned thresholding that adapts to the agents current task stage, enabling context-aware compression validated on end-to-end benchmarks like SWE-Bench (Jimenez et al., 2024) without repository-specific retraining. Agent Context Management Modern coding LLMs support context windows of 128k tokens or more (Achiam et al., 2023; Team et al., 2024), yet they remain insufficient for large-scale codebases and suffer from documented performance degradation on long contexts (Liu et al., 2023a; Laban et al., 2025; Li et al., 2023a). Agentic systems (Yao et al., 2023; Wei et al., 2022) for software engineering (Yang et al., 2024b; Wang et al., 2024a; Xia et al., 2024; Bouzenia et al., 2024; Qin et al., 2024; Liu et al., 2024) face critical challenges in managing multi-turn interaction contexts. In NLP domains, trajectory management approaches employ LLM-based summarization when contexts overflow (Cursor, 2025; Anthropic, 2025), fixed-size truncation (Gao et al., 2025), or simple observation masking (Lindenbauer et al., 2025). For long-horizon scenarios, recent frameworks such as SUPO (Lu et al., 2025) and FoldGRPO (Sun et al., 2025) learn to manage history 9 REFERENCES 7 through reinforcement learning, while others like COMPASS (Wan et al., 2025), ACON (Kang et al., 2025), AgentDiet (Xiao et al., 2025), and AgentFold (Ye et al., 2025) introduce hierarchical oversight or proactive folding policies. These methods primarily optimize prior interaction histories, whereas SWE-Pruner serves as lightweight middleware at the agentenvironment boundary to prune the environments observation (i.e., file content); it is thus orthogonal to and can be seamlessly combined with such learned history managers."
        },
        {
            "title": "7 Conclusion",
            "content": "Pruning long code context for agents requires both fine-grained, structure-preserving compression and dynamic, task-aware filtering. Our approach, SWE-Pruner, addresses these challenges through lightweight binary classifiers trained on synthetically diversified code-question pairs and an adaptive, query-conditioned thresholding mechanism. By integrating seamlessly as middleware within agentic workflows, SWE-Pruner reduces token usage by 2338% on SWE-Bench and 2954% on SWE-QA, achieves up to 14.84 compression on single-turn benchmarks. These results demonstrate that line-level, context-aware pruning effectively addresses context-window constraints across both agentic workflows and general code understanding tasks while reducing costs and improving efficiency."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge several limitations. First, our implementation focuses on Python repositories, though our approach does not rely on Python-specific features and demonstrates effective generalization across different codebases. Comprehensive multilingual support remains future work. Second, we mitigate data leakage by selecting recently collected repositories from SWE-QA that postdate our training data, though continuous evaluation on newly released repositories remains important. Finally, while our lightweight neural skimmer significantly reduces token consumption, it introduces marginal latency overhead that could be further optimized through distillation or early-exit mechanisms."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Claude code: Built for developers, 2025. URL https://claude.com/product/claude-code. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. RepairAgent: An Autonomous, LLM-Based Agent for Program Repair. 2024. URL https://doi.org/10.48550/arXiv.2403.17134. ByteDance Seed, Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, Jinhua Zhu, Shulin Xin, Dong Huang, Yetao Bai, Lixin Dong, Chao Li, Jianchong Chen, Hanzhi Zhou, Yifan Huang, Guanghan Ning, Xierui Song, Jiaze Chen, Siyao Liu, Kai Shen, Liang Xiang, and Yonghui Wu. Seed-Coder: Let the code model curate data for itself, 2025. URL https://arxiv.org/abs/2506.03524. Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and task graphs. arXiv preprint arXiv:2406.01304, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. Swe-exp: Experience-driven software issue resolution. arXiv preprint arXiv:2507.23361, 2025. Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token, May 2024. 10 REFERENCES 7 Cursor. Cursor the ai code editor, 2025. URL https://cursor.com/. Yixiong Fang, Tianran Sun, Yuling Shi, and Xiaodong Gu. Attentionrag: Attention-guided context pruning in retrieval-augmented generation. arXiv preprint arXiv:2503.10720, 2025. Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, et al. Trae agent: An llm-based agent for software engineering with test-time scaling. arXiv preprint arXiv:2507.23370, 2025. Google. Gemini cli: Command-line ai assistant, 2025. URL https://geminicli.com/. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. Unixcoder: Unified cross-modal pre-training for code representation. arXiv preprint arXiv:2203.03850, 2022. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: long-range pre-trained language model for code completion. In International Conference on Machine Learning, pages 1209812107. PMLR, 2023. Junda He, Jieke Shi, Terry Yue Zhuo, Christoph Treude, Jiamou Sun, Zhenchang Xing, Xiaoning Du, and David Lo. From code to courtroom: Llms as the new software judges. arXiv preprint arXiv:2503.02246, 2025. Minghao Hu, Junzhe Wang, Weisen Zhao, Qiang Zeng, and Lannan Luo. Flowmaltrans: Unsupervised binary code translation for malware detection using flow-adapter architecture. arXiv preprint arXiv:2508.20212, 2025. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, and Saravan Rajmohan. Acon: Optimizing context compression for long-horizon llm agents. arXiv preprint arXiv:2510.00615, 2025. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. LLMs Get Lost In Multi-Turn Conversation. 2025. URL https://doi.org/10.48550/arXiv.2505.06120. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Han Li, Yuling Shi, Shaoxin Lin, Xiaodong Gu, Heng Lian, Xin Wang, Yantao Jia, Tao Huang, and Qianxiang Wang. Swe-debate: Competitive multi-agent debate for software issue resolution. arXiv preprint arXiv:2507.23348, 2025. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939, 2023a. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023c. Tobias Lindenbauer, Igor Slinko, Ludwig Felder, Egor Bogomolov, and Yaroslav Zharov. The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management. 2025. URL https://doi.org/10.48550/arXiv.2508.21433. Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large Language Model-Based Agents for Software Engineering: Survey. 2024. 11 REFERENCES Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023a. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, 2023b. Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, and Jiecao Chen. Scaling llm multi-turn rl with end-to-end summarization-based context management. arXiv preprint arXiv:2510.06727, 2025. Oorja Majgaonkar, Zhiwei Fei, Xiang Li, Federica Sarro, and He Ye. Understanding code agent behaviour: An empirical study of success and failure trajectories. arXiv preprint arXiv:2511.00197, 2025. Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36:1932719352, 2023. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Findings of the Association for Computational Linguistics ACL 2024, pages 963981, 2024. Weihan Peng, Yuling Shi, Yuhang Wang, Xinyun Zhang, Beijun Shen, and Xiaodong Gu. Swe-qa: Can language models answer repository-level code questions? arXiv preprint arXiv:2509.14635, 2025. Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. Agentfl: Scaling llm-based fault localization to project-level context. arXiv preprint arXiv:2403.16362, 2024. Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, and Tatsunori Hashimoto. Longcodebench: Evaluating coding llms at 1m context windows. arXiv preprint arXiv:2505.07897, 2025. Yuling Shi, Yichun Qian, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. Longcodezip: Compress long context for code language models. arXiv preprint arXiv:2510.00446, 2025. Haoyang Song, Haochen Liu, Lingfeng Sha, Hongyu Zhang, and Xiaodong Gu. Finecodeeval: Fine-grained code evaluation using llm as judge. arXiv preprint arXiv:2406.12170, 2024. Weiwei Sun, Miao Lu, Zhan Ling, Kang Liu, Xuesong Yao, Yiming Yang, and Jiecao Chen. Scaling long-horizon llm agent via context-folding. arXiv preprint arXiv:2510.11967, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. OpenHands Team. Openhands: An open platform for ai software developers as generalist agents. https://github.c om/All-Hands-AI/OpenHands, 2024. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Tree-sitter. Tree-sitter. https://tree-sitter.github.io/tree-sitter/, 2024. Guangya Wan, Mingyang Ling, Xiaoqi Ren, Rujun Han, Sheng Li, and Zizhao Zhang. Compass: Enhancing agent long-horizon reasoning with evolving context. arXiv preprint arXiv:2510.08790, 2025. Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, and Xin Xia. Can llms replace human evaluators? an empirical study of llm-as-a-judge in software engineering. Proceedings of the ACM on Software Engineering, 2(ISSTA):19551977, 2025a. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. 2024a. Yan Wang, Xiaoning Li, Tien Nguyen, Shaohua Wang, Chao Ni, and Ling Ding. Natural is the best: Model-agnostic code simplification for pre-trained large language models. Proceedings of the ACM on Software Engineering, 1(FSE): 586608, 2024b. Yifei Wang, Feng Xiong, Yong Wang, Linjing Li, Xiangxiang Chu, and Daniel Dajun Zeng. Position bias mitigates position bias: Mitigate position bias through inter-position knowledge distillation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 14951512, 2025b. 12 EMPIRICAL RESULTS ON GLM MODEL Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying LLM-based Software Engineering Agents. 2024. Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, and Lingming Zhang. Live-swe-agent: Can software engineering agents self-evolve on the fly? arXiv preprint arXiv:2511.13646, 2025. Yuan-An Xiao, Pengfei Gao, Chao Peng, and Yingfei Xiong. Improving the efficiency of llm agent systems through trajectory reduction. arXiv preprint arXiv:2509.23586, 2025. Guang Yang, Yu Zhou, Wei Cheng, Xiangyu Zhang, Xiang Chen, Terry Yue Zhuo, Ke Liu, Xin Zhou, David Lo, and Taolue Chen. Less is More: DocString Compression in Code Generation, 2024a. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. In The Thirty-eighth Annual SWE-agent: Agent-computer interfaces enable automated software engineering. Conference on Neural Information Processing Systems, 2024b. URL https://arxiv.org/abs/2405.15793. Xinwei Yang, Zhaofeng Liu, Chen Huang, Jiashuai Zhang, Tong Zhang, Yifan Zhang, and Wenqiang Lei. Elaboration: comprehensive benchmark on human-llm competitive programming. arXiv preprint arXiv:2505.16667, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, 2023. Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, and Yong Jiang. Agentfold: Long-horizon web agents with proactive context management. arXiv preprint arXiv:2510.24699, 2025. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 24712484, 2023. Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, and Min Yang. Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. Diet code is healthy: Simplifying programs for pre-trained models of code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 10731084, 2022. Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE international conference on computer vision, pages 15291537, 2015."
        },
        {
            "title": "A Empirical Results on GLM Model",
            "content": "In our main experiments, we demonstrated that Claude Sonnet 4.5 dedicates 76.1% of its token budget to read operations when solving software engineering tasks on SWE-Bench Verified. To verify whether this token consumption pattern generalizes across different model architectures, we extend our analysis to GLM-4.6, an open-source large language model with fundamentally different training methodology and architecture. We conduct parallel analysis on GLM-4.6 agent trajectories using the same experimental setup as in Section 3, categorizing token usage into read, edit, and execute operations. The results reveal consistent patterns that validate the model-agnostic nature of context pruning needs in coding agents. As illustrated in Figure 5, GLM-4.6 exhibits remarkably similar token consumption patterns to Claude Sonnet 4.5. Read-type operations account for 67.5% of total tokens (2.89M), demonstrating that the dominance of codebase exploration remains consistent across different model architectures. Edit and execute operations 13 MODEL ARCHITECTURE AND INFERENCE DETAILS B.1.0.0 Figure 5 Token cost distribution over different tool calls for Mini-SWE-Agent with GLM-4.6 on SWE-Bench Verified. Read operations dominate token consumption at 67.5%, further validating the necessity of context pruning mechanisms across different backbone models. Figure 6 SWE-Pruner Model Architecture. The model consists of lightweight reranker backbone with multi-layer feature fusion, followed by dual heads for pruning and reranking. The pruning head employs CRF layer to model structured retention decisions, while the reranking head produces document-level relevance scores. consume 18.5% (0.79M) and 14.0% (0.60M) respectively, with their relative proportions slightly different from Claude Sonnet 4.5 but still maintaining read operations as the overwhelming majority. This crossmodel consistency strongly reinforces our motivation for context pruning: regardless of the underlying model architecture, training methodology, or parameter scale, coding agents universally spend the majority of their token budget on codebase exploration rather than reasoning or editing. These findings establish that SWEPruner addresses fundamental inefficiency inherent to the agentic workflow itself, rather than model-specific behavior."
        },
        {
            "title": "B Model Architecture and Inference Details",
            "content": "B.1 Model Architecture The neural skimmer extends the Qwen3-Reranker-0.6B backbone with two specialized heads: CRF-based pruning head for line-level filtering and reranking head for document-level scoring. An overview of the architecture is shown in Figure 6. Multi-Layer Feature Fusion. To capture representations at different semantic levels, we extract and concatenate hidden states from three intermediate layers (layers 7, 14, and 28) of the backbone. These fused features are processed through self-attention block followed by multi-head attention layer (8 heads, hidden size 256) to refine contextual representations. 14 MODEL ARCHITECTURE AND INFERENCE DETAILS C.0.0. Algorithm 1 Token Scoring and Line-level Aggregation Require: Input text = {x1, x2, ..., xn}, token-level scoring model (), threshold τ Ensure: Set of kept lines Lkept 1: Lkept = 2: Compute token scores = {s1, s2, ..., sn} where si = (xi) 3: for each line li do 4: 5: 6: Let Ti be the set of tokens in line li Compute line score si = 1 st Ti if si > τ then tTi (cid:80) Lkept Lkept {li} 7: end if 8: 9: end for 10: return Lkept CRF-Based Pruning Head. The pruning head formulates line-level retention as structured sequence labeling problem using Conditional Random Field. Let denote the input token sequence and the corresponding label sequence in space = {retain, prune}. The CRF negative log-likelihood is: where the score function combines emission and transition potentials: LCRF-NLL(x, y) = log Z(x) score(x, y) score(x, y) = starty1 + (cid:88) t= emissionst,yt + (cid:88) t=2 transitionsyt,yt1 + endyT and the partition function normalizes over all possible label sequences: log Z(x) = log (cid:88) yY exp(score(x, y)) (5) (6) (7) Emissions Et = MLP(ht) R2 represent local confidence for each token, while transitions R22 capture dependencies between adjacent decisions. This structured formulation encourages coherent pruning patterns that respect syntactic boundaries. Reranking Head. The reranking head reuses the original language modeling head from Qwen3-Reranker, producing scalar relevance score for the entire document via mean squared error against teacher-provided scores. B.2 Inference Strategy During inference, the pruning head computes token-level scores through forward propagation, which are then aggregated to line-level scores via averaging (as detailed in Algorithm 1). The CRF layer applies Viterbi decoding to find the optimal label sequence, ensuring structurally coherent pruning decisions. Lines are retained if their average token score exceeds the threshold τ = 0.5. The reranking head simultaneously produces document-level scores, enabling the system to perform both granular pruning and coarse-grained relevance assessment in single forward pass. Algorithm 1 summarizes the complete scoring and aggregation pipeline. The algorithm first computes tokenlevel relevance scores for all input tokens (Step 2), then iterates through each line to aggregate token scores via averaging (Steps 4-7), and finally applies the threshold-based retention criterion (Step 8) to produce the final set of kept lines. EXPERIMENTAL DETAILS D."
        },
        {
            "title": "C Training Dataset for the Neural Skimmer",
            "content": "Code Source and Preprocessing. We construct our training dataset from code snippets sampled from the GitHub Code 2025 dataset2, meticulously curated collection comprising over 1.5 million repositories. This dataset employs dual-perspective design that balances proven quality with emerging innovation: it includes both high-quality repositories (above 2 stars) representing established patterns and practices, as well as newly-created 2025 repositories capturing contemporary development trends. The dataset undergoes extensive preprocessing to remove binary files, build artifacts, configuration noise, and minified code, ensuring that only clean, meaningful source code is retained. This rigorous curation provides us with diverse, polyglot code corpus spanning multiple programming languages and coding paradigms, which is essential for training pruning model that generalizes across varied agentic scenarios. Agentic Task Taxonomy. To ensure the skimmer generalizes across diverse real-world coding scenarios, we design comprehensive taxonomy of nine distinct agentic task types that reflect common information needs in software development workflows. These tasks encompass both exploratory activities and focused interventions. Code Summarization (code-summarize) targets high-level understanding by requesting concise summaries of code functionality for integration or review purposes. Code Refactoring (code-refactor) focuses on improving code quality through readability, modularity, or structural enhancements. Relevant Part Identification (find-relevant-part) and Code Location (code-locate) address navigation needs by asking to identify where specific features, bugs, or logic are implemented. Code Optimization (code-optimize) requests efficiency improvements in terms of performance, resource usage, or scalability. Code Explanation (code-explain) seeks understanding of particular algorithms or design choices without requiring full code walkthroughs. Code Debugging (code-debug) targets actionable assistance for resolving specific issues, exceptions, or edge cases. Feature Addition (feature-addition) involves extending existing code with new capabilities while maintaining integration with current logic. Finally, Code Completion (code-completion) represents unique scenario where the query itself is code snippet requiring contextual completion, simulating realistic code intelligence workflows. This taxonomy ensures broad coverage of task diversity while maintaining clear distinctions between query intents. Detailed generation instructions for each task type are provided in Table 5. Data Generation Pipeline. We synthesize queries and line-level labels for 200,000 sampled code snippets from 195,370 files across 5,945 repositories. For each code snippet C, we employ Qwen3-Coder-30B-A3BInstruct (Bai et al., 2023) as the teacher LLM to generate task-oriented queries and corresponding line-level retention masks. Query generation employs temperature of 0.7 and top-p sampling with = 0.9 to balance diversity and coherence. To ensure representativeness, we randomly sample across all nine task types, three snippet length levels (short, medium, long), and three relevance levels (low, medium, high). Following initial generation, we apply rigorous quality control mechanism using Qwen3-Next-80B-A3B-Thinking as an LLM-as-a-Judge filter (Wang et al., 2025a; Liu et al., 2023b; Song et al., 2024; He et al., 2025). This judge model evaluates the reasoning quality, annotation consistency, and task alignment of each sample, retaining approximately 1/6 of candidates that meet high-quality standards. The complete generation prompts and judge criteria are detailed in Section J. Dataset Statistics. Following filtering and quality enhancement procedures, we obtain 61,184 training samples with verified line-level annotations. The resulting dataset exhibits natural query length distributions with an average of 39.98 words and median of 24.00 words per query, reflecting realistic information needs in agentic workflows. The average character count per query is 291.69, with median of 169.00, indicating balanced mix of concise and detailed information requests. This final corpus provides diverse, high-quality supervision for training the neural skimmer to handle varied agentic coding scenarios."
        },
        {
            "title": "D Experimental Details",
            "content": "This section provides comprehensive details about our experimental setup, including training hyperparameters, benchmark specifications, agent frameworks, and baseline configurations. 2https://huggingface.co/datasets/nick007x/github-code-2025 16 EXPERIMENTAL DETAILS D.2 Table 5 Agentic Tasks Taxonomy used for Query Synthesis Task Type Instruction for Query Generation code-summarize code-refactor Summarize the main purpose or functionality of the code, but do not explain every line. Frame your query as developer seeking summary for integration or review. Suggest refactoring or improvement for the code. Your query should be practical, such as asking to improve readability, modularity, or performance. find-relevant-part Ask to locate or identify the part of the code that implements specific feature or logic. Your query should be about finding where something is handled in the code. code-optimize code-locate code-explain code-debug Request an optimization for the (core logic maybe) code, such as improving efficiency, reducing resource usage, or enhancing scalability. Ask to pinpoint the location of bug, feature, or important logic within the code. Request an explanation for particular logic, algorithm, or design choice in the code, but do not ask for full code walkthrough. Ask for help debugging specific issue, exception, or edge case in the code. Your query should be actionable and focused. feature-addition Request to add new feature or capability to the code, specifying what should be added and how it should interact with existing logic. code-completion This is special query format. In code completion, the query should be CODE instead of text, which means you should image yourself as developer write other code snippet(query) that can used the code given for completion. The completion will be the next line for query, but you should keep it in your mind and never write the completion in query. QUERY like PUZZLE. D.1 Training Configuration Training Hyperparameters. We fine-tune the Qwen3-Reranker-0.6B backbone with global batch size of 128, obtained from per-device batch size of 16 on 8 GPUs with tensor parallelism. We use the AdamW optimizer with learning rate of 3 105 and weight decay of 0.01, training for 3 epochs with dropout rate of 0.4. The CRF-based pruning head fuses hidden states from three intermediate transformer layers (layers 7, 14, and 28). Only the last two transformer layers of the backbone are fine-tuned, along with an additional feature fusion module consisting of self-attention block followed by multi-head attention (8 heads, hidden size 256) and CRF layer. The weight balancing parameter is set to λ = 0.05. Detailed architecture specifications are provided in Section B. Inference Configuration. The pruning threshold is set to τ = 0.5, tuned on held-out validation set. Agent tasks use maximum of 250 interaction rounds. All experiments employ Claude Sonnet 4.5 and GLM-4.6 APIs with temperature 0 for deterministic generation, averaged over three random seeds where applicable. D.2 Benchmark and Agent Descriptions We evaluate SWE-Pruner on both single-turn and multi-turn benchmarks spanning diverse code intelligence tasks. For single-turn evaluation, we use Long Code Completion (Guo et al., 2023), which evaluates code completion under long contexts, and Long Code QA (Rando et al., 2025), which tests code comprehension through question answering on contexts up to 1 million tokens drawn from real-world GitHub issues and documentation across multiple languages and project domains. For multi-turn agent benchmarks, we use SWE-Bench Verified (Jimenez et al., 2024), which contains 500 GitHub issues from 12 Python repositories where success is measured by automated test execution in Docker containers and patches must pass all tests without regressions, and SWE-QA (Peng et al., 2025), which evaluates repository-specific question 17 AGENT ROUNDS AND TOKEN CONSUMPTION ANALYSIS (a) Claude Sonnet 4.5 (b) GLM 4.6 Figure 7 Comprehensive efficiency analysis on SWE-Bench Verified. SWE-Pruner (orange) achieves substantial reductions compared to baseline (gray). (a) With Claude Sonnet 4.5: 38.7% in prompt tokens, 40.8% in completion tokens, 39.2% in total tokens, and 18.3% in agent rounds. (b) With GLM 4.6: 44.2% in prompt tokens, 44.0% in completion tokens, 43.6% in total tokens, and 34.6% in agent rounds. answering across three repositories (streamlink, reflex, conan) with answers scored via LLM-as-a-judge across five dimensions: correctness, completeness, relevance, clarity, and reasoning. For agent frameworks, we integrate SWE-Pruner with two representative systems. Mini SWE Agent (Yang et al., 2024b) is typical agent framework designed for SWE-Bench that operates with three tool categories: file reading (cat, grep), code editing (sed), and command execution. We integrate SWE-Pruner as middleware intercepting file reading operations to apply task-aware pruning. OpenHands (Team, 2024), which we use following the SWE-QA original paper, provides comprehensive tools including repository exploration, version control, and debugging. Our integration with both frameworks demonstrates that SWE-Pruner generalizes across agent architectures as modular component. D.3 Baseline Configurations All baseline methods are configured to match identical compression constraints (4x and 8x) under the same experimental conditions. For performance bounds, Full Context provides complete code context as an upper bound, while No Context provides only task instructions as lower bound. For token-level compression baselines, Selective-Context (Li et al., 2023c) computes self-information log (xiinstruction) and removes low-information tokens, and LLMLingua-2 (Pan et al., 2024) uses trained token classifier (XLM-RoBERTa) to predict binary retention decisions. Both operate at sub-word granularity without code-specific syntactic constraints. For retrieval-based methods, RAG employs function-level chunking with UniXCoder (Guo et al., 2022; Zhang et al., 2023) embeddings, retrieving top-k chunks via cosine similarity. LongCodeZip (Shi et al., 2025) represents code-specific compression that combines AST-based chunking with entropy-guided compression, retaining high-entropy regions. Finally, LLM Summarize (for multi-turn agent tasks only) generates abstractive summaries of code files using the backbone model, trading summarization cost for reduced downstream tokens."
        },
        {
            "title": "E Agent Rounds and Token Consumption Analysis",
            "content": "Section 5.1 shows the token consumption and interaction rounds for two different backbone models: Claude Sonnet 4.5 and GLM 4.6. For Claude Sonnet 4.5, SWE-Pruner reduces average tokens per instance from 0.911M to 0.701M (23.1% reduction), while reducing average rounds from 51.0 to 41.7 (18.2% reduction). The GLM 4.6 model exhibits even more substantial improvements, with token reduction from 0.791M to 0.488M (38.3% reduction) and rounds reduction from 49.3 to 36.6 (25.7% reduction). Figure 7 present the complete distribution shifts for both models, revealing consistent patterns across prompt and completion tokens. GLM 4.6 exhibits 44.2% and 44.0% reductions respectively, while Claude Sonnet 18 SINGLE-TURN TASKS WITH SEEDCODER 4.5 demonstrates 38.7% and 40.8% reductions. This symmetry indicates that SWE-Pruner not only reduces input context but also enables more focused agent responses, distinguishing it from naive retrieval filtering which would primarily affect prompt tokens. The variation in pruning effectiveness between models provides insights into architectural differences: GLM 4.6s more pronounced gains (34.6% round reduction vs. Claudes 18.3%) suggest greater susceptibility to context noise, while Claudes extensive context capabilities maintain reasonable focus even unpruned. The reduction in agent rounds represents qualitative improvementfor GLM 4.6, the shift from 49.3 to 36.6 rounds translates to faster task completion and reduced cumulative latency in production environments where each round incurs API overhead and rate-limiting delays."
        },
        {
            "title": "F Detailed Efficiency Analysis",
            "content": "Model Input Length Qwen3-0.6B Qwen3-4B Qwen3-14B Qwen3-32B SWE-Pruner 64 26.18 64.75 97.17 73.99 44.70 26.31 97.24 78.11 55.46 43.91 512 2048 8192 32.00 104.93 143.65 84.01 42.05 29.64 99.10 129.97 274.22 49. 76.73 241.97 529.45 1188.67 102.00 Table 6 Average TTFT (ms) for different models and input lengths. SWE-Pruner maintains consistently low latency across all sequence lengths. To complement the efficiency analysis in Section 5.3, we provide detailed first token latency measurements across different models and input lengths. Table 6 presents comprehensive comparison demonstrating how SWE-Pruner maintains stable and low latency across various sequence lengths compared to larger generative models. The latency analysis reveals critical insights for practical agent deployments. At 8192 tokens, SWE-Pruner achieves 102.00ms TTFTa 7.5 speedup compared to Qwen3-32B (1188.67ms) and 5.2 compared to Qwen3-14B (529.45ms)while exhibiting sublinear scaling (2.1 increase from 2048 to 8192 tokens vs. Qwen3-32Bs 14.1). Crucially, in real-world deployments with closed-source models like Claude Sonnet 4.5, API latency typically ranges from 500ms to several seconds per request, making our pruning overhead (4050ms) less than 10% of typical roundtrip times. This marginal cost is amortized many times over through 2354% token reductions (Section 5.1), yielding proportional savings in both inference time and API costs. Combined with 18.325.7% reductions in interaction rounds across models, the efficiency gains compound throughout multi-turn agent trajectories, delivering substantial end-to-end improvements despite the upfront pruning cost. Single-Turn Tasks with SeedCoder To validate the generalizability of our approach across different model architectures, we conduct additional evaluations using Seed-Coder-8B-Instruct on the Long Code Completion and Long Code QA benchmarks. Results are presented in Table 7. The results with Seed-Coder-8B-Instruct exhibit similar patterns to those observed with Qwen2.5-Coder-7BInstruct. On Long Code Completion, SWE-Pruner achieves 8.13x compression under 8x constraint with 56.73 ES and 28.5 EM, outperforming Selective-Context (7.49x/49.89 ES/17.5 EM) and LongCodeZip (6.53x/54.91 ES/23.0 EM). On Long Code QA, the advantages are particularly pronounced: under 8x constraint, our method achieves 14.68x compression with 55.75% accuracy, substantially exceeding all baselines including RAG (6.65x/53.57%) and LongCodeZip (7.49x/50.91%). This cross-model consistency validates that the effectiveness of task-aware, line-level pruning is not specific to particular model architecture but represents generalizable approach to context compression for code understanding tasks. 19 SYNTACTIC STRUCTURE PRESERVATION ANALYSIS Methods Full No Context Selective-Context LLMLingua2 RAG LongCodeZip SWE-Pruner Long Code Completion Long Code QA 4x constraint 8x constraint 4x constraint 8x constraint 1/τ ES 65.03 1.0 43.85 53.68 48.09 58.21 56.72 57.71 3.27 3.95 3.32 3.96 4.22 ES EM 1/τ 65.03 1.0 40.5 43.85 14.0 49.89 45.53 56.97 54.91 56.73 7.49 7.89 6.78 6.53 8.13 24.5 15.0 31.0 25.5 31.0 Acc EM 1/τ 49.11 1.0 40.5 37.50 14.0 51.33 43.36 53.98 51.33 56.25 2.71 3.70 3.44 3.29 9. 17.5 13.5 28.5 23.0 28.5 1/τ Acc 49.11 1.0 37.50 46.43 39.82 53.57 50.91 55.75 6.69 5.46 6.65 7.49 14.68 Table 7 Results on Long Code Completion and Long Code QA with Seed-Coder-8B-Instruct. SWE-Pruner demonstrates consistent effectiveness across different model architectures, achieving superior compression while maintaining competitive task performance. Method AST Correctness (%) Baseline (No Compression) Full Context No Context Token-Level Compression LLMLingua2 Selective Context Random Token Pruner Line-Level Compression Function RAG + Random Line Pruner + SWE-Pruner Structural Compression LongCodeZip + SWE-Pruner 98.5 98.5 0.29 12.4 49.6 92.3 78.2 87.3 89.3 76. Table 8 AST Correctness Rate after Compression. Line-level methods maintain substantially higher syntactic validity compared to token-level approaches."
        },
        {
            "title": "H Syntactic Structure Preservation Analysis",
            "content": "Line-level pruning preserves syntactic structure better than token-level compression. Our pruning strategy retains semantically relevant lines with minimal syntactic context for code integrity, while token-level methods disrupt AST structure through arbitrary token deletion. We evaluate AST correctness using tree-sitter (Treesitter) on Long Code Completion. Section presents results. Token-level methods (LLMLingua2, Selective Context) achieve near-zero AST correctness (0.29%, 12.4%), while line-level approaches maintain substantially higher validity. Our SWE-Pruner achieves 87.3% AST correctness on Function RAG, outperforming random token pruning (49.6%) and competitive with random line pruning (78.2%). This demonstrates that structure-aware labeling successfully identifies safe removals while preserving syntactic dependencies. Note that SWE-Pruner operates as second-stage pruner on top of Function RAGs output, meaning any additional compression inherently introduces some risk of removing syntactically critical lines. The slight reduction from baseline Function RAG (92.3%) to our method (87.3%) reflects this compression-validity trade-off, where the 5% decrease enables substantially higher compression ratios while maintaining practical code validity for downstream tasks. 20 CASE STUDY ON SWE BENCH I."
        },
        {
            "title": "I Case Study on SWE Bench",
            "content": "To complement the aggregate statistics from SWE-Bench experiments, we conduct an in-depth case study comparing agent behaviors under two configurations: Baseline agent operating with full interaction histories, and Pruner-augmented agent that applies context pruning to file observations. Both agents operate on the same set of software engineering tasks from SWE-Bench, and we analyze their trajectories through the lens of tool invocation patterns and token consumption. We select two representative instances that illustrate distinct benefits of context pruning: one where pruning enables task completion by preventing resource exhaustion, and another where pruning achieves structural efficiency gains even when both agents succeed. I.1 Case 1: High-Impact Scenario The first case examines task django__django-10554, which requires fixing missing deep copy in the Query.clone() method for the combined_queries attribute. As shown in Section I.1, the Baseline agent exhausts its resource limits after 164 steps, accumulating over 7 million tokens with maximum prompt length of 87,790 tokens. In contrast, the Pruner-augmented agent completes the task successfully in 56 steps with 1.17 million tokens and peak prompt length of 38,226 tokens. The token reduction reaches 83.3% in absolute terms, demonstrating that context pruning can transform resource-bound failures into successful completions. Setting Steps Read Search Exec Edit Tokens Baseline Pruner 164 56 59 20 39 10 1 25 11 7,001,934 1,170,160 Table 9 Comparison of agent behaviors on django__django-10554. Baseline exhausts limits while Pruner succeeds with 83.3% token reduction. Examining the trajectory reveals fundamental differences in exploration strategy. The Baseline agent engages in extensive breadth-first file reading, issuing commands such as find . -name \"*.py\" grep union followed by repeated segmented reads using sed -n x,y across numerous files. This results in accumulation of redundant context from tangentially related code. Meanwhile, the Pruner agent directly navigates to the core file django/db/models/sql/query.py, reads it with line numbers (cat -n), and identifies the relevant branch (if self.combinator: ...) for modification. By filtering out irrelevant sections during file reads, the pruner enables the agent to maintain focused working context, avoiding the context overflow that derails the Baseline. I.2 Case 2: Structural Efficiency Gains The second case analyzes task django__django-11740, which requires adding foreign key dependency tracking to the migration autodetectors AlterField operation. Unlike the previous case, both agents successfully complete this task, but the Pruner achieves substantial efficiency improvements as shown in Section I.2. Although the Pruner agent takes 6 additional steps (48 versus 42), its token consumption is 6.0% lower and its maximum prompt length is reduced by 30.2%. This illustrates that pruning benefits extend beyond preventing failures to improving resource efficiency in successful trajectories. Setting Steps Read Search Exec Edit Tokens Baseline Pruner 42 48 12 8 8 1 0 18 13 857,371 806,220 Table 10 Comparison on django__django-11740. Both succeed, but Pruner achieves 30.2% reduction in peak prompt length. Analysis of the trajectories shows that both agents converge on the same logical solution, modifying autodetector.py to adjust the _get_dependencies_for_foreign_key invocation. However, their reading strategies differ qualitatively. The Baseline agent performs multiple segmented reads of the target file 21 PROMPT TEMPLATES and creates temporary validation scripts (e.g., python /tmp/test_uuid_to_fk3.py) to verify understanding, accumulating historical noise from these exploratory steps. The Pruner agent reads the complete file with line numbers once, directly edits the relevant section, and avoids auxiliary validation artifacts. This behavioral shift reflects transition from defensive exploration to confident intervention, enabled by cleaner, more focused context at each decision point. Discussion These cases illustrate two distinct modes of benefit from context pruning. In the high-impact scenario, pruning prevents catastrophic context overflow that leads to task abandonment, effectively converting failures into successes. In the structural efficiency scenario, pruning does not change task outcomes but significantly reduces the computational burden and maximum context requirements, improving operational cost-effectiveness. Both patterns support the hypothesis that context pruning serves not merely as an engineering optimization but as cognitive strategy that enables more efficient problem-solving behaviors. By reducing the volume of irrelevant information presented to the agent at each step, pruning allows the underlying language model to allocate more attention to task-critical signals, thereby improving both decision quality and resource utilization."
        },
        {
            "title": "J Prompt Templates",
            "content": "This appendix presents the complete prompt templates used in our experiments. We provide four key prompt templates that form the core of our approach. First, the Silver Label Prompt is used to generate training data by asking models to answer queries using provided code context with explicit line citations. Second, the Mini SWE Agent with Pruner Template defines the agent system for solving software engineering tasks, which includes detailed instructions on response format, context focus questions, workflow recommendations, and command execution rules. Third, the SWE-QA Bash Tool Descriptions specify how the bash execution tool works within the agent framework, particularly highlighting the usage of context focus questions for filtering large outputs. Finally, the Quality Evaluation Prompt provides the criteria and procedure for assessing the quality of pruned code samples across three dimensions: query quality, deletion relevance, and semantic preservation. 22 PROMPT TEMPLATES Silver Label Prompt You are given: - natural-language(or code for code completion task) Query - code snippet split into numbered lines (1>, 2>, 3>, ...) Question: {query} Code Context: {code} Answer the Question, using ONLY information provided in the Code Context. If no useful information is provided, you MUST output \"No answer\". If some parts of the Context are used to answer, you MUST cite ALL the corresponding lines. Use the symbols [ ] to indicate when fact comes from line in the context, e.g [1] for fact from line 1. - For multi-line context, use [line1-line2], e.g. [12-25]. - For multi context, use [line1,line2,...], e.g. [1,3,5-7]. You should only answer the given question and should not provide any additional information HINT: - For code, context should be wider than the line just answer the question, for example, if the question is about variable in class method function, include the function definition, class definition and where it is used. - When you try to cite something, its better to cite the structure of the code. e.g. if you want to cite B1 in the code structure below: 1> if cond: 2> A1 3> A2 4> else: 5> B1 , best citation will be [1,4,5], which keeps the structure of the if-else block while removing the unrelated A1, A2. Now give your answer with citations: 23 PROMPT TEMPLATES Mini SWE Agent with pruner template agent: system_template: You are helpful assistant that can interact multiple times with computer shell to solve programming tasks. ## Response Format Your response must contain exactly ONE bash code block(with optinal context_focus_question shown below) with ONE command (or commands connected with && or ). Include THOUGHT section before your command where you explain your reasoning process. <format_example> THOUGHT: Your reasoning and analysis here. Explain why you want to perform the action. bash your_command_here <context_focus_question> Optional question to focus on relevant parts of the command output. </context_focus_question> </format_example> ## Context Focus Question (Optional) The context_focus_question is an optional field that helps filter large command outputs to show only relevant information. **Question requirements:** - question **MUST be complete, self-contained question** (not keywords or phrases) - question should be specific enough to filter effectively - questions should **NOT** contain file-level info (e.g., filenames, line numbers), as the filter model only processes direct command output. For such info, use tools like grep/sed and leave the question blank. - question should be place it immediately after the bash code block (after the closing ) ### Examples of good context_focus_question: - Find where the [some logic] is implemented in the [some class/function]? - Given [some background], [some problem]? - Locate the [some logic]? - How can the code implement [some feature]? - [some combination of the above, like background + origin problem + current purpose + specific attention area] ### Examples of bad context_focus_question: - load_raw function (too vague) - lines 50-100 of data_loader.py (contains file-level info) - fix the bug in rwkv6.py (too vague) Gives some context for the query is better for more effective filtering. Failure to follow these rules will cause your response to be rejected. instance_template: <pr_description> Consider the following PR description: {{task}} </pr_description> <instructions> # Task Instructions ## Overview Youre software engineer interacting continuously with computer by submitting commands. Youll be helping implement necessary changes to meet requirements in the PR description. Your task is specifically to make changes to non-test files in the current directory in order to fix the issue described in the PR description in way that is general and consistent with the codebase. IMPORTANT: This is an interactive process where you will think and issue ONE command, see its result, then think and issue your next command. For each response: 1. Include THOUGHT section explaining your reasoning and what youre trying to accomplish 2. Provide exactly ONE bash command to execute 3. Give an optional context_focus_question to filter the command output(or not give for output no need to be filtered) ## Important Boundaries - MODIFY: Regular source code files in /testbed (this is the working directory for all your subsequent commands) - DO NOT MODIFY: Tests, configuration files (pyproject.toml, setup.cfg, etc.) 24 PROMPT TEMPLATES Mini SWE Agent with pruner template (Part 2) ## Recommended Workflow 1. Analyze the codebase by finding and reading relevant files to figure out why this issue happenes(with context_focus_question for large outputs like read whole file) **HINT**: since we have pruner enabled, prefer reading files fully instead of using grep/find first **ATTENTION**: pruned code might miss some details, so its **highly recommend** to use cat -n or nl -ba command with context_focus_question, so you can see the line numbers. If you looked the filtered output and need more context, you can always use sed without context_focus_question to read more lines before/after the relevant lines. 2. When you have enough information, edit the source code to resolve the issue, you have only one chance, no tests, no retry, so read widely first(with context_focus_questions) before making changes. ## Command Execution Rules You are operating in an environment where 1. You write single command 2. The system executes that command in subshell 3. You see the result(if you set context_focus_question, the output will be filtered accordingly) 4. You write your next command Each response should include: 1. **THOUGHT** section where you explain your reasoning and plan 2. single bash code block with your command 3. An optional context_focus_question to filter the command output, actually you can put some thoughts directly as background in it. Format your responses like this: <format_example> THOUGHT: Here explain my reasoning process, analysis of the current situation, and what Im trying to accomplish with the command below. bash your_command_here <context_focus_question> Your question here. </context_focus_question> </format_example> Commands must be specified in single bash code block: bash your_command_here **CRITICAL REQUIREMENTS:** - Your response SHOULD include THOUGHT section explaining your reasoning - Your response MUST include EXACTLY ONE bash code block(with optinal context_focus_question) - This bash block MUST contain EXACTLY ONE command (or set of commands connected with && or ) - If you include zero or multiple bash blocks, or no command at all, YOUR RESPONSE WILL FAIL - Do NOT try to run multiple independent commands in separate blocks in one response - Directory or environment variable changes are not persistent. Every action is executed in new subshell. - However, you can prefix any action with MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ... or write/load environment variables from files Example of CORRECT response(no context_focus_question because ls will not output large content, and we want to see the full output): <example_response> THOUGHT: need to understand the structure of the repository first. Let me check what files are in the current directory to get better understanding of the codebase. bash ls -la </example_response> Example of an INCORRECT response: <example_response> THOUGHT: need to examine the codebase and then look at specific file. Ill run multiple commands to do this. bash ls -la PROMPT TEMPLATES Mini SWE Agent with pruner template (Part 3) Now Ill read the file: bash cat -n file.txt <context_focus_question> Some question about your purpose for reading the file. </context_focus_question> </example_response> If you need to run multiple commands, either: 1. Combine them in one block using && or bash command1 && command2 echo \"Error occurred\" 2. Wait for the first command to complete, see its output, then issue the next command in your following response. ## Environment Details - You have full Linux shell environment - Always use non-interactive flags (-y, -f) for commands - Avoid interactive tools like vi, nano, or any that require user input - If command isnt available, you can install it ## Useful Command Examples ### Create new file: bash cat <<EOF > newfile.py import numpy as np hello = \"world\" print(hello) EOF ### Edit files with sed: bash # Replace all occurrences sed -i s/old_string/new_string/g filename.py # Replace only first occurrence sed -i s/old_string/new_string/ filename.py # Replace first occurrence on line 1 sed -i 1s/old_string/new_string/ filename.py # Replace all occurrences in lines 1-10 sed -i 1,10s/old_string/new_string/g filename.py ### View file content: bash # View specific lines with numbers nl -ba filename.py sed -n 10,20p <context_focus_question> If you know what line you want to view and the range of sed if large, you can also set question here. </context_focus_question> ### Any other command you want to run bash anything ## Submission When youve completed your work (reading, editing, testing), and cannot make further progress issue exactly the following command: bash echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT && git add -A && git diff --cached 26 PROMPT TEMPLATES Mini SWE Agent with pruner template (Part 4) This command will submit your work. You cannot continue working (reading, editing, testing) in any way on this task after submitting. </instructions> action_observation_template: <returncode>{{output.returncode}}</returncode> {% if output.output length < 10000 -%} <output> {{ output.output -}} </output> {%- else -%} <warning> The output of your last command was too long. Please try different command that produces less output. If you have not used context_focus_question, use it. If you have used it but the output is still too long, do not try to change the question, just follow the instructions below while also use the context_focus_question. If youre looking at file you can try use head, tail or sed to view smaller number of lines selectively. If youre using grep or find and it produced too much output, you can use more selective search pattern. If you really need to see something from the full commands output, you can redirect output to file and then search in that file. </warning> {%- set elided_chars = output.output length - 10000 -%} <output_head> {{ output.output[:5000] }} </output_head> <elided_chars> {{ elided_chars }} characters elided </elided_chars> <output_tail> {{ output.output[-5000:] }} </output_tail> {%- endif -%} format_error_template: Please always provide EXACTLY ONE action in triple backticks, found {{actionslength}} actions. Please format your action in triple backticks as shown in <response_example>. <response_example> THOUGHT: Here are some thoughts about why you want to perform the action. bash <action> <context_focus_question> Please provide question about the context you want to focus on. This part is optional. If you dont need to filter the output, you can leave it blank or omit it. </context_focus_question> </response_example> If you have completed your assignment, please consult the first message about how to submit your solution (you will not be able to continue working on this task after that). Note: common trigger for this error is you put > 1 bash block(action) in your response. step_limit: 250 cost_limit: 3. pruner: url: http://localhost:8000/prune timeout: 120 retries: 3 min_chars: 500 chunk_overlap_tokens: 50 threshold: 0.5 environment: cwd: \"/testbed\" timeout: 60 env: PAGER: cat MANPAGER: cat LESS: -R PIP_PROGRESS_BAR: off TQDM_DISABLE: 1 environment_class: docker PROMPT TEMPLATES SWE-QA bash tool descriptions _BASH_DESCRIPTION = \"\"\"Execute bash commands in the workspace. * Use this tool to run shell commands, navigate directories, search files, etc. * Commands are executed in the workspace directory. * Output is returned as text. \"\"\" _BASH_DESCRIPTION_PRUNE = ( _BASH_DESCRIPTION + \"\"\" The meaning of argument context_focus_question (Optional): Use context_focus_question to filter large command outputs for relevant information. **Requirements:** - Must be complete, self-contained question (not keywords/phrases) - Be specific for effective filtering - Dont include file-level info (filenames, line numbers) - use grep/sed instead **Good examples:** - Where is [some logic] implemented in [some class/function]? - Given [background], whats the [problem]? - How does the code implement [feature]? **Bad examples:** - load_raw function (too vague) - lines 50-100 of data_loader.py (contains file info) - fix bug in rwkv6.py (too vague) **IMPORTANT:** With pruner enabled, prefer cat -n or nl -ba with context_focus_question to see line numbers. Then you can use sed without filtering for more detailed context since you have line number information. **IMPORTANT:** If the command output is small and important like ls, just leave context_focus_question blank. \"\"\" ) 28 PROMPT TEMPLATES Quality Evaluation Prompt You are code quality evaluator for code pruning dataset. Your task is to assess the quality of query-guided code deletion sample. You will be given: 1. **Query**: code snippet for means making completion below or natural language question/task 2. **Original Code**: Full code snippet with line numbers 3. **Diff**: Shows which lines were removed (- prefix) and kept (no change or + prefix) Evaluate THREE dimensions: ## 1. Query Quality - **Good**: Realistic, specific, actionable developer question related to partial feature/function - **Acceptable**: Valid but generic, or slightly unclear but answerable - **Poor**: Too vague, treats code as the subject (\"explain this code\") HINT: just focus on query itself, dont take query-code relevance into consideration in this part. ## 2. Deletion Relevance - **Appropriate**: Removes truly unrelated code while keeping necessary context - **Minimal**: Mostly removes whitespace/comments/trivial lines, little semantic pruning - **Excessive**: Removes too much, including code relevant to the query HINT: Both query-code high relevance and low relevance are ok, key point is the context preserved correctly. For high relevance, might more code; For low relevance, might less code. ## 3. Semantic Preservation - **Preserved**: Remaining code is syntactically valid and semantically coherent (can understand the query-relevant logic) - **Partially Preserved**: Minor issues (e.g., unmatched braces, missing imports that dont affect core logic understanding) - **Broken**: Code is syntactically invalid or key logic is incomprehensible ## Overall Quality Rating Based on the above three dimensions, assign: - **high**: All three dimensions are good/appropriate/preserved, or at most one acceptable/partially_preserved - **medium**: Two dimensions are good/acceptable/appropriate, one has issues; or all three are acceptable - **low**: Two or more dimensions are poor/minimal/broken, or query is fundamentally flawed --- ### Input Data: **Query:** {query} **Original Code (with line numbers):** {code_with_numbers} **Diff (deletions marked with -):** diff {diff} --- ### Your Task: 1. Provide concise reasoning for each dimension (1 sentences per dimension) 2. Assign ratings: query_quality (good/acceptable/poor), deletion_relevance (appropriate/minimal/excessive), semantic_preservation (preserved/partially_preserved/broken) 3. Determine overall_quality (low/medium/high) Output JSON format (no code fences, just JSON): {{ \"reasoning\": \"<Brief analysis covering all three dimensions>\", \"query_quality\": \"<goodacceptablepoor>\", \"deletion_relevance\": \"<appropriateminimalexcessive>\", \"semantic_preservation\": \"<preservedpartially_preservedbroken>\", \"overall_quality\": \"<lowmediumhigh>\" }}"
        }
    ],
    "affiliations": [
        "Douyin Group",
        "LLMSE Lab, Shanghai Jiao Tong University",
        "Sun Yat-sen University"
    ]
}