{
    "paper_title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
    "authors": [
        "Qing Jiang",
        "Gen luo",
        "Yuqin Yang",
        "Yuda Xiong",
        "Yihao Chen",
        "Zhaoyang Zeng",
        "Tianhe Ren",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Perception and understanding are two pillars of computer vision. While multimodal large language models (MLLM) have demonstrated remarkable visual understanding capabilities, they arguably lack accurate perception abilities, e.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on the COCO dataset, limiting many tasks requiring the combination of perception and understanding. In this work, we aim to bridge this perception gap from both model designing and data development perspectives. We first introduce ChatRex, an MLLM with a decoupled perception design. Instead of having the LLM directly predict box coordinates, we feed the output boxes from a universal proposal network into the LLM, allowing it to output the corresponding box indices to represent its detection results, turning the regression task into a retrieval-based task that LLM handles more proficiently. From the data perspective, we build a fully automated data engine and construct the Rexverse-2M dataset which possesses multiple granularities to support the joint training of perception and understanding. After standard two-stage training, ChatRex demonstrates strong perception capabilities while preserving multimodal understanding performance. The combination of these two capabilities simultaneously unlocks many attractive applications, demonstrating the complementary roles of both perception and understanding in MLLM. Code is available at \\url{https://github.com/IDEA-Research/ChatRex}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 3 6 3 8 1 . 1 1 4 2 : r ChatRex: Taming Multimodal LLM for Joint Perception and Understanding Qing Jiang Gen Luo Yuqin Yang Yuda Xiong Yihao Chen Zhaoyang Zeng Tianhe Ren Lei Zhang International Digital Economy Academy (IDEA) Abstract Perception and understanding are two pillars of computer vision. While multimodal large language models (MLLM) have demonstrated remarkable visual understanding capabilities, they arguably lack accurate perception abilities, e.g. the stage-of-the-art model Qwen2-VL only achieves 43.9 recall rate on the COCO dataset, limiting many tasks requiring the combination of perception and understanding. In this work, we aim to bridge this perception gap from both model designing and data development perspectives. We first introduce ChatRex, an MLLM with decoupled perception design. Instead of having the LLM directly predict box coordinates, we feed the output boxes from universal proposal network into the LLM, allowing it to output the corresponding box indices to represent its detection results, turning the regression task into retrievalbased task that LLM handles more proficiently. From the data perspective, we build fully automated data engine and construct the Rexverse-2M dataset which possesses multiple granularities to support the joint training of perception and understanding. After standard two-stage training, ChatRex demonstrates strong perception capabilities while preserving multimodal understanding performance. The combination of these two capabilities simultaneously unlocks many attractive applications, demonstrating the complementary roles of both perception and understanding in MLLM. Code is available at https://github.com/IDEA-Research/ChatRex."
        },
        {
            "title": "Introduction",
            "content": "Perception and understanding are two fundamental human faculties within behavioral science. Humans initially perceive objects, with vision signals transmitted to the brain for understanding, and can then locate back to the objects during conversation. In pursuit of AGI, Multimodal Large Language Models (MLLMs) OpenAI (2023); Chen et al. (2024b); Team et al. (2023); Bai et al. (2023b); Alayrac et al. (2022); Chen et al. (2022); Dai et al. (2023); Agrawal et al. (2024); Li et al. (2024b); Deitke et al. (2024); Wang et al. (2024); Lv et al. (2023) have exhibited remarkable capacities for visual understanding empowered by advancements in Large Language Models (LLMs) Touvron et al. (2023a,b); Chiang et al. (2023); Taori et al. (2023); Team (2023); Achiam et al. (2023); Bai et al. (2023a); Abdin et al. (2024); Dubey et al. (2024). Despite showing strong visual understanding, we find through experiments that these models generally lack fine-grained perception capabilities, particularly in object detection, as shown in Fig. 1. We evaluate the performance of several general-purpose Chen et al. (2024b); Wang et al. (2024) and detectionfocused You et al. (2023); Chen et al. (2023a); Ma et al. (2024) MLLMs on the COCO Lin et al. (2014) dataset by prompting them to detect objects within the image. The state-of-the-art model Qwen2-VL-7B Wang et al. (2024) only achieves 43.9 recall rate at an IoU threshold of 0.5. The results indicate that MLLMs still struggle with fundamental perception tasks despite their remarkable visual understanding capabilities. This shortfall Work done during Qing Jiang, Gen Luo, and Yuqin Yang were interns at IDEA. Corresponding author. 1 Figure 1: Visualized prediction results on the COCO dataset, from general-purpose MLLM including Qwen2VL Wang et al. (2024), InternVL2 Chen et al. (2024b), and detection-focused MLLMs including Ferret You et al. (2023), Shikra Chen et al. (2023a), and Groma Ma et al. (2024). These models generally suffer from low recall rate in multi-object scenes. in perception constrains them in numerous tasks requiring precise perception, such as autonomous driving and robotic navigation. Also, it hinders their interactivity by identifying objects during conversation. We argue that this performance gap between perception and understanding in MLLMs arises primarily from two factors: i) modeling conflicts between these two tasks, and ii) lack of data that seamlessly balances both perception and understanding. For object detection, common practice is to quantize Chen et al. (2021) box coordinates into tokens within the vocabulary of LLM to fit the auto-regressive framework. Although this ensures compatibility with understanding tasks through next-token prediction, we argue this method is in conflict with accurately modeling perception for three reasons: i) Error propagation: representing single box typically requires 9 tokens including digits, square brackets, and commas, where an error in any token can cause cascading errors, which become even worse in multi-object detection. In subsequent experiments, we find that this is one of the reasons for the low recall rate; ii) Ambiguity in prediction order: there is no inherent order among objects in object perception, yet the auto-regressive nature imposes sequential order that the LLM must decide which object to predict first and iii) Quantization range limitation: quantization error easily occurs when the image size is large. To address these inherent modeling conflicts, we adopt decoupled model design and introduce ChatRex. ChatRex follows standard design similar to LLaVA Liu et al. (2023b), which integrates language model with vision encoder. For multimodal understanding tasks like image caption and image QA, we retain the auto-regressive text prediction framework. However, for perception, particularly object detection, we transform the task as retrieval-based task inspired by Groma Ma et al. (2024). Specifically, instead of prompting the LLM to predict bounding box coordinates, the boxes are directly provided as inputs, each represented as an object token by combining its RoI feature with its positional embedding. When the LLM needs to reference an object, it outputs the index of the relevant box. This method represents each box as single token without quantization, with the sequence order determined by the input boxes, effectively addressing prior modeling conflicts. However, this retrieval-based approach presents two key challenges for achieving optimal performance: the 2 Figure 2: Overview of the perception capabilities from ChatRex. We utilize decoupled design for perception and understanding, allowing ChatRex to respond to questions while simultaneously grounding its answers to the referenced objects. need for high-resolution visual input and robust object proposal model. To address the first challenge, we adopt dual vision encoder design to incorporate additional vision encoder Liu et al. (2022) to provide high-resolution visual information for perception. For the second, we introduce Universal Proposal Network (UPN), which leverages granularity-based prompt learning on pre-trained open-set object detection model. This enables the generation of proposals that cover diverse granularities, categories, and domains, thereby ensuring robust box inputs for the LLM. From the data perspective, current MLLMs are also limited by the lack of data that effectively balances both perception and understanding. To address this limitation, we developed fully automated data engine to construct the Rexverse-2M dataset, which comprises image-region-text annotation triplets at varying levels of granularity. The data engine is composed of three primary modules. The first module generates image captions for input images Chen et al. (2024b), while the second aligns referenced objects or phrases using grounding model Ren et al. (2024). The third module Dubey et al. (2024) refines region descriptions at multiple granularities. Experiment results show that ChatRex delivers strong performance in both object detection on COCO Lin et al. (2014) and LVIS Gupta et al. (2019), and referring detection on RefCOCO Kazemzadeh et al. (2014); Yu et al. (2016); Mao et al. (2016), while maintaining competitive results on general MLLM benchmarks. We demonstrate that both perception and understanding are essential capabilities for multimodal models, and their integration can unlock wider range of applications as shown in Fig. 2. To summarize, our contributions 3 are threefold: We reveal the performance gap in the perception of MLLMs and introduce decoupled model ChatRex and universal proposal network (UPN) to address the modeling conflict between perception and understanding. We develop an automated data engine to create Rexverse-2M, comprehensive dataset supporting both perception and understanding tasks for model training. Experimental results demonstrate that ChatRex exhibits strong perception capabilities alongside robust multimodal understanding, highlighting that these two complementary abilities are both essential for MLLM."
        },
        {
            "title": "2.1 General MLLMs",
            "content": "Leveraging breakthroughs in large language models within natural language processing, Multimodal Large Language Models (MLLMs) OpenAI (2023); Chen et al. (2024b); Team et al. (2023); Bai et al. (2023b); Alayrac et al. (2022); Chen et al. (2022); Dai et al. (2023); Agrawal et al. (2024); Li et al. (2024b); Deitke et al. (2024); Wang et al. (2024); Lv et al. (2023) have demonstrated robust visual comprehension capabilities. LLaVA Liu et al. (2023b) pioneered the paradigm of visual instruction tuning, inspiring wave of subsequent work. Research on general-purpose MLLMs encompasses various directions, including: i) exploring the use of high-resolution image inputs to enhance model perceptual abilities, with models like LLaVA-Next Liu et al. (2024), SPHINX Lin et al. (2023c), Monkey Li et al. (2023c), InternLM-Xcomposer2 Dong et al. (2024), LLaVA-UHD Xu et al. (2024), NVLM Dai et al. (2024a) employing image slicing methods, and others like LLaVA-HR Luo et al. (2024), Mini-Gemini Li et al. (2024d), Eagle Shi et al. (2024), and MG-LLaVA Zhao et al. (2024) utilizing high-resolution vision encoders for additional vision encoding; ii) investigating diverse approaches for pre-training Lin et al. (2024a); McKinzie et al. (2024); Xue et al. (2024b) and fine-tuning data Li et al. (2024a); Tong et al. (2024), and iii) extending to multi image Jiang et al. (2024); Li et al. (2024c) or video tasks Xue et al. (2024a); Lin et al. (2023a)."
        },
        {
            "title": "2.2 Perception MLLMs",
            "content": "While generic multimodal models demonstrate strong image-level understanding, they still lack fine-grained perception capabilities. Inspired by Pix2seq Chen et al. (2021), several works such as Kosmos-2 Peng et al. (2023), Shikra Chen et al. (2023a), Ferret You et al. (2023); Zhang et al. (2024), CogVLM Wang et al. (2023b), Griffon Zhan et al. (2025, 2024) and other generalized MLLMs Wang et al. (2024); Chen et al. (2024b); McKinzie et al. (2024) have transformed box regression into quantized coordinate prediction task suited for LLM next-token prediction. Another research direction employs additional decoders for perception. For instance, LISA Lai et al. (2023), GLaMM Rasheed et al. (2024), LLaVA-Grounding Zhang et al. (2025), PerceptionGPT Pi et al. (2024), and VisionLLMv2 Wu et al. (2024) use auxiliary detection or segmentation models for perception tasks. Groma Ma et al. (2024) initially proposed re-framing detection as box retrieval task, and we follow this method in this work."
        },
        {
            "title": "3 ChatRex Architecture",
            "content": "ChatRex employs design that decouples perception from understanding. For perception, we train universal proposal network to detect arbitrary objects, supplying box inputs to the LLM. For understanding, we adopt the standard LLaVA Liu et al. (2023b) structure with dual vision encoder to facilitate high-resolution image encoding. We introduce each part in the following. 4 Figure 3: Model structure of the Universal Proposal Network (UPN). UPN is DETR-based model capable of detecting any object at two granularities."
        },
        {
            "title": "3.1 Universal Proposal Network (UPN)",
            "content": "To ensure that the LLM can accurately retrieve the correct box, it is essential that the input boxes comprehensively encompass all objects within an image. This requires proposal model with two key properties: i) robust generalization ability to generate proposal boxes for any object in any scenario, and ii) the proposed boxes should be comprehensive, including both instance-level and part-level objects. To meet these requirements, straightforward approach is to aggregate multiple detection datasets, merge their categories, and treat all object classes as single foreground category for training. However, this strategy is suboptimal due to inconsistencies in object definitions across different datasets. For example, while datasets such as COCO Lin et al. (2014) and O365 Shao et al. (2019) annotate objects at the instance level, SA-1B Kirillov et al. (2023) annotate objects at part-level. These discrepancies in labeling can introduce ambiguities that compromise training stability. To mitigate this issue, we adopt dual-granularity prompt tuning strategy. Specifically, we utilize T-Rex2 Jiang et al. (2025) as our base model. T-Rex2 is DETR-based Carion et al. (2020) model trained on vast data and exhibits strong generalization, making it suitable pre-trained model for detecting any objects in varied scenes. The model outputs object queries Qdec that pass through an MLP to predict bounding boxes. The classification of these bounding boxes is achieved via dot product between the queries and the prompt embeddings E: Scls = QT dec : RCD RDN RCN (1) Where is the number of classes, represents the number of detection queries and is the channel dimension of outputted queries. We extend T-Rex2 by introducing two additional learnable prompts, Pfine and Pcoarse, concatenated into Pconcat to classify boxes into fine-grained or coarse-grained categories: Scls = Pconcat QT dec : R2D RDN R2N (2) For training, we utilize SA-1B as the fine-grained dataset and other detection datasets (such as COCO and O365) as coarse-grained inputs. This dual-granularity prompt design effectively resolves labeling ambiguities between datasets, allowing the proposal model to accurately capture and characterize objects across varying levels of detail. To illustrate its effectiveness, we provide visualization results of the UPN in Fig. 4, highlighting its ability to handle diverse scenarios. Figure 4: Visualization of the fine-grained proposal outputs produced by the UPN."
        },
        {
            "title": "3.2 MLLM Architecture",
            "content": "Dual Vision Encoders. An accurate perception system typically requires high-resolution inputs. To equip ChatRex with sufficient perception capabilities, we adopt an additional high-resolution vision encoder for image encoding. As illustrated in Fig. 5, we use the ViT from CLIP Radford et al. (2021) for low-resolution image encoding and ConvNeXt from LAION Schuhmann et al. (2022) for high-resolution image encoding. To reduce the number of vision tokens fed into the LLM, we use the merging strategy from MG-LLaVA Zhao et al. (2024) to merge high-resolution tokens into low-resolution ones through gate convolution. Specifically, we adjust the input resolutions for both vision encoders to ensure they generate the same number of tokens at the last scale. Let the visual token output of the low-resolution encoder be fL RN Cl , and the visual token output of the high-resolution encoder at the final scale be fH RN Ch , where represents the number of tokens and Cl and Ch denote the output dimensions of the two vision encoders respectively, we employ gate convolution layer to fuse these two sets of tokens. This process yields the final visual token output, Fv: Fv = fL + σ (WG [W1fL; W2fH ]) W2fH W1 and W2 represent the 11 convolution layers applied to fL and fH , respectively, WG is the weight matrix of the linear layer, σ is an activation function (e.g., sigmoid), and denotes element-wise multiplication. Object Encoder. We encode each output box from the universal proposal network to object tokens and feed from the UPN, let FH denote the multi-scale visual features them to the LLM. Assume input boxes {Bi}K i=1 produced by the high-resolution encoder, for each box Bi, we extract its content feature Ci using multi-scale RoI Align He et al. (2017): Ci = RoIAlign(FH, Bi) (3) 6 Figure 5: Overview of the proposed ChatRex model architecture, and the workflow for modeling the detection output of the LLM from coordinates prediction task to input box indices retrieval task. Since the RoI feature does not contain positional information which is essential for referring tasks, we enhance each object feature with positional embedding to capture spatial context. We encode each box coordinate through sin-cos position embedding layer and add to the RoI feature: Vi = Ci + PE(Bi) (4) LLM. We use two separate MLP projectors to map visual and object tokens to the text space. We also add an index token to each object token to inform the LLM about the index of each object token, which will be described in Sec. 3.3. These tokens are then concatenated with the text token and fed into the LLM for the next-token prediction task. We use Vicuna-7B Chiang et al. (2023) as our default LLM."
        },
        {
            "title": "3.3 Task Formulations",
            "content": "We formulate the task of leveraging LLM for detection as an index selection process over input boxes. To do so, we first extend the vocabulary of LLM by adding specialized tokens, including object index tokens <obj0>, ..., <objN>, where denotes the maximum number of input boxes and is set to 100 in this work; grounding start token <g>; grounding end token </g>; object start token <o>; and object end token </o>. LLM Input Format. The input to LLM is constructed as follow: The Input Format of ChatRex <image>n<obj1><roi><obj2><roi>...<objN><roi>nUser: Question. where <image> represents the visual tokens from the vision encoder, and <roi> denotes the object feature associated with each corresponding bounding box. Each <roi> token is prefixed by its object index token. Detection, Grounding, Referring Task Formulations. For detection, grounding, and referring tasks, the input and output if formatted as follows: 7 Input and Output Formats for Detection Related Tasks. Single-Object Detection User: Can you help me detect dog in this image? Answer the question with object indices. Assistant: <g>dog</g><o><objm>...<objn></o>. Multi-Object Detection User: Can you help me detect yellow flower; man wearing blue shirt; dog at the left in this image? Answer the question with object indices. Assistant: <g>a yellow flower</g><o><objm>...<objn></o>n <g>a man wearing blue shirt</g><o><objm>...<objn></o>n <g>a dog at the left</g><o><objm>...<objn></o> where <objm> and <objn> refer to specific object index tokens, identifying the start (m) and end (n) of the sequence of detected objects linked to the noun phrases. This structured format enables precise mapping between noun phrases and their corresponding bounding box indices. Region Understanding Task Formulations. The Region Understanding task consists of describing regions at different levels of granularity (category names, short phrases, brief descriptions, and detailed descriptions), region QA, and analysis of relationships between regions. The format is shown below: Input and Output Formats for Region Understanding Related Tasks. Single-Object Description User: What is the category name of <objm>. Assistant: <g>cat</g><o><objm></o>. Multi-Object Description User: Can you provide brief description for <objm>; <objn>? Assistant: <g>a cat lying an the bed</g><o><objm></o>n <g>an opened tomato can</g><o><objn></o>n Region QA User: What might be the species of <objm>? Assistant: The species of <objm> appears to be dog, likely bread with black and white fur. Region Relationship Analysis User: Can you tell me about the relationship between <objm> and <objn>? Assistant: <objm> and <objn> are standing next to each other, which suggests they might be companions or partners Grounded Conversation Task Formulations. In Grounded conversation task, ChatRex will detect the corresponding object in the answer. Input and Output Formats for Grounded Conversation Task. User: Please briefly describe this image and detect all the mentioned objects. Answer with grounded object indexes. ChatRex: <g>man</g><o><objm></o> in <g>white tuxedo</g><o><objn></o> with <g>red bow tie</g><o><objm></o> is holding an <g>Oscar statuette</g><o><objn></o> and standing on stage with microphone, while large, ornate Oscar statue is visible in the background. 8 Figure 6: Overview of the Rexverse-2M data engine. There are three main components, including the image captioning module, the grounding module, and the region captioning module"
        },
        {
            "title": "4 Data and Training",
            "content": "To equip ChatRex with robust perception and understanding capabilities, we build Rexverse-2M dataset with two million annotated images, featuring multi-granularity annotations generated through fully automatic data engine. We then adopt standard two-stage training methodology following LLaVA Liu et al. (2023b), enabling the model to preserve its perception capabilities while progressively acquiring multimodal understanding and dialog skills."
        },
        {
            "title": "4.1 Rexverse-2M Data Engine",
            "content": "Our objective is to construct dataset that can be effectively utilized for both perception and understanding tasks. To achieve this, our data pipeline focuses on generating an annotation triplet comprising image descriptions, region descriptions, and bounding boxes. As shown in Fig. 6, the data engine is structured around three core modules: image captioning, object grounding, and region captioning. Image Collection. We started by collecting images from COYO700M Byeon et al. (2022) dataset through series of filtering processes including removing images with small resolution and NSFW tags. We also train an image classifier to filter out low-content web images with plain white backgrounds. Finally, we selected two million images as the dataset images. Image Caption. We use InternVL2-8B Chen et al. (2024b) to generate image caption for each image. This image caption will refer to the main objects in the image by their category name or descriptive phrases. Phrase Grounding. We then utilize SpaCy to extract noun phrases from generated image captions. Depending on the caption, SpaCy may identify category names, such as soldier or descriptive phrases (at least 3 words per region) like military-style uniform. We will also filter out some abstract nouns that might not be an object like image, background etc. Subsequently, we employ Grounding DINO 1.5 Ren et al. (2024) to ground the filtered noun phrases. This process ultimately produces boxes associated with their category https://spacy.io/ names or short phrase descriptions. Phrase-Conditioned Region Caption. To support the training for understanding tasks, it is essential to generate detailed descriptions for each region rather than relying solely on category names or short phrases, which often provide limited information. straightforward approach might involve cropping each region and feeding it into an MLLM model for image captioning. However, this method is prone to hallucinations when the cropped regions are too small or contain parts of other objects. To reduce such inaccuracies, we implemented phrase-conditioned image description strategy. Specifically, we leverage the InternVL2-8B model Chen et al. (2024b) to generate image captions that are conditioned on predefined phrases related to each region. By guiding the model with these phrases, we ensure that the generated descriptions are more accurate and context-relevant, significantly reducing the likelihood of hallucinations and enhancing the quality of the region-specific captions. Region Caption Filtering and Rewriting. Lastly, we employ LLaMA3-8B Dubey et al. (2024) to verify whether the generated captions accurately align with their original category names or short phrases, filtering out any remaining hallucinated outputs. Once validated, we then prompt it to refine these detailed captions into more concise referring expressions, thereby enhancing training for referring tasks. Rexverse-2M consists of 2.1 million images with captions, 10.2 million regions annotated with category labels, 2.5 million regions labeled with short phrases, 2.5 million regions with detailed descriptions, and 2.4 million regions with referring descriptions. Additionally, we use this data engine to annotate 776K grounded conversation data from the ALLaVA-4V-Instruct Chen et al. (2024a) dataset for instruction tuning. Specifically, the conversation responses are treated as image captions, which are then passed through the engine."
        },
        {
            "title": "4.2 Training",
            "content": "UPN Training. We utilize two types of datasets with bounding boxes to train our UPN: coarse-grained datasets including O365 Shao et al. (2019), OpenImages Kuznetsova et al. (2020), Bamboo Zhang et al. (2022b), COCO Lin et al. (2014), LVIS Gupta et al. (2019), HierText Long et al. (2022), CrowdHuman Shao et al. (2018), SROIE Huang et al. (2019) and EgoObjects Zhu et al. (2023); and fine-grained datasets SA-1B Kirillov et al. (2023). All dataset categories are defined as either coarse-grained or fine-grained, reducing the task to binary classification problem. Following T-Rex2, Hungarian matching is used to match predictions with ground truth. We employ L1 Loss and GIOU Loss for box predictions, along with sigmoid focal loss for classification. ChatRex Training Tasks. We adopt three main tasks to train ChatRex including i) Detection: where the model outputs the indices of corresponding objects based on given category name, phrase, or referring expression. ii) Region Captioning: where, given region indices, the model generates descriptions at varying levels of detail, including category names, short phrases, detailed descriptions, or referring descriptions, and iii) Grounded Image Captioning: The model needs to output indices of objects mentioned in its generated image caption. We mix the ground truth boxes of the current image with the proposal boxes from UPN, and keep at most 100 boxes as input. We adopt two-stage training process and the data for each stage are listed in Tab. 1 Stage-1: Alignment Training. In the first stage, the objective is to align visual features and object features with the text feature space. To achieve this, we train the image projection MLP, object projection MLP, as well as the input and output embeddings of the LLM, given that we have added special tokens to its vocabulary. Stage-2: Visual Instruction Tuning. In this stage, we focus on enhancing the models dialogue and multimodal understanding capabilities while maintaining its perception abilities. To achieve this integration, we train all model parameters on both perception and understanding tasks jointly. 10 Stage Task # Samples Stage1 Grounding & Detection Referring Region Description Image Caption Grounding & Detection Referring Stage2 Region Description Region QA & Relationships Grounded Counting Grounded Image Caption Grounded Conversation Conversation & QA 1.32M 1.32M 1.32M 976K 623K 623K 545K 266K 328K 600K 777K 538K Datasets COCO Lin et al. (2014), O365 Shao et al. (2019), LVIS Gupta et al. (2019), Rexverse-2M Rexverse-2M Rexverse-2M ALLAVA-4V-Caption Chen et al. (2024a) COCO, O365, LVIS, Rexverse-2M RefCOCO/+/g Kazemzadeh et al. (2014); Yu et al. (2016); Mao et al. (2016), Rexverse-2M LVIS, PACO Ramanathan et al. (2023), Rexverse-2M MVDP Lin et al. (2024b), Osprey Yuan et al. (2024), VCR Zellers et al. (2019) O365, CrowdHuman Shao et al. (2018) Rexverse-2M ALLAVA-4V-Instruct Chen et al. (2024a) LLAVA-1.5 Liu et al. (2023a) Table 1: Training data and tasks for each stage. Method Type Faster-RCNN Ren et al. (2015) DETR Carion et al. (2020) Pix2Seq Chen et al. (2021) DINO Zhang et al. (2022a) Florence2 Xiao et al. (2024) GLIP Li et al. (2022) T-Rex2 Jiang et al. (2025) Grounding DINO Liu et al. (2023c) Shikra-7B Chen et al. (2023a) Ferret-7B You et al. (2023) Groma-7B Ma et al. (2024) InternVL2-8B Chen et al. (2024b) Qwen2-VL-7B Wang et al. (2024) ChatRex-7B (Ours) Closed-set Detection Model Open-set Detection Model MLLM COCO-Val P@0.5 R@0.5 mAP 42.0 - 43.3 - 43.2 - 49.4 - - - - - - - - - 40.3 66.3 69.9 45.3 59.3 73.2 - - - - 21.5 33.5 28.9 24.5 43.9 73. 43.4 49.8 46.5 48.4 - - - - - 48.5 LVIS-Mini Val P@0.5 R@0.5 mAP AP-R AP-C AP-F - - - - - - - - - - - - - - - - - - - - - - - - - - - 52.8 72.9 76.3 51.6 77.0 80.6 - - - - - 14.5 25.2 10.9 13.1 34.7 59. - 37.3 47.6 33.0 - - - - - 43.1 - 28.2 45.4 22.2 - - - - - 47.3 - 34.3 46.0 30.7 - - - - - 48. - 41.5 49.5 38.8 - - - - - 38.1 Table 2: Comparison of different models on object detection tasks on the COCO and LVIS datasets. We report the R@0.5 and P@0.5 metrics for MLLMs, representing recall and precision at an IoU threshold of 0.5, respectively."
        },
        {
            "title": "5 Experiments",
            "content": "We assess the perception and understanding capabilities of ChatRex from multiple perspectives. In terms of perception, we evaluate its performance on the COCO Lin et al. (2014) and LVIS Gupta et al. (2019) datasets, as well as its referring detection capabilities on RefCOCO, RefCOCO+, and RefCOCOg. For understanding, we evaluate its performance on several general multimodal benchmarks."
        },
        {
            "title": "5.1 Perception Capability Evaluation",
            "content": "Evaluation Metrics. Mean Average Precision (mAP) Lin et al. (2014) is common metric for object detection, which measures the area under the precision-recall curve, reflecting both the precision and recall of the model. However, for MLLMs that predict coordinates as vocabulary tokens, computing AP can be challenging due to the lack of confidence scores for each predicted box. Therefore, we directly report recall and precision metrics instead. We provide all ground truth categories for the current test image and prompt the model to generate the corresponding coordinate boxes. The details of prompts used for each model are included in the Appendix. For ChatRex, we use fine-grained proposal boxes from UPN and their corresponding confidence scores as input, enabling us to compute precision, recall, and AP. 11 Method Grounding DINO-L Liu et al. (2023c) ONE-PEACE Wang et al. (2023a) UNINEXT-H Lin et al. (2023b) Shikra-7B Chen et al. (2023a) InternVL2-8B Chen et al. (2024b) Groma-7B Ma et al. (2024) Qwen2-VL-7B Wang et al. (2024) Ferret-v2-7B Zhang et al. (2024) ChatRex-7B (Ours) RefCOCO RefCOCO+ val testA testB val testA testB RefCOCOg test val 90.6 92.6 92.6 87.0 87.1 89.5 91.7 92.8 90.1 93.2 94.2 94.3 90.6 91.1 92.1 93.6 94.7 93.0 88.2 89.3 91.5 80.2 80.7 86.3 87.3 88.7 85. 82.8 88.8 85.2 81.6 79.8 83.9 85.8 87.4 85.2 89.0 92.2 89.6 87.4 87.9 88.9 90.5 92.8 89.6 75.9 83.2 79.8 72.1 71.4 78.1 79.5 79.3 79. 86.1 89.2 88.7 82.3 82.7 86.4 87.3 89.4 88.8 87.0 89.3 89.4 82.2 82.7 87.0 87.8 89.3 88.6 Table 3: Comparison with other models on the referring task across the RefCOCO, RefCOCO+, and RefCOCOg datasets, where prediction is considered correct if its overlap IoU with the ground truth is larger than 0.5. Proposal Model MMB MM-Vet SEEDI LLaVAW MMMU POPE Hallusion InstructBLIP-7B Dai et al. (2024b) Qwen-VL-Chat-7B Bai et al. (2023b) InternVL-Chat-7B Chen et al. (2023b) LLaVA-1.5-7B Liu et al. (2023b) Shikra-7B Chen et al. (2023a) Ferret-v2-7B Zhang et al. (2024) ChatRex (Ours) - 59.1 64.6 64.3 - - 65.8 26.2 - - 30.5 - 34.9 36. 58.8 65.4 - 66.1 - - 69.6 60.9 - - 63.4 - 67.7 65.9 - - - 35.7 - - 38.0 - 74.9 86.4 85.9 - 87.8 87.3 - 36.8 - 27.6 - - 45.8 Table 4: Comparison on multimodal benchmarks. Common Object Detection. As shown in Tab. 2, ChatRex achieves 48.5 mAP on the COCO dataset, which is comparable to conventional object detectors like DINO Zhang et al. (2022a), indicating that ChatRex possesses strong perception capabilities. In contrast, other MLLMs generally exhibit low recall rates. This discrepancy arises from the multi-object nature of COCO, where each image contains multiple categories with numerous instances. The low recall rate implies that current MLLMs face significant challenges in detecting multiple objects, which is common requirement in real-world scenarios. Furthermore, we identified specific issues with general MLLMs such as InternVL2 and Qwen2-VL, which have tendency to repeatedly generate the same coordinates until reaching the models maximum output length. detailed analysis of these problems is provided in the Appendix, highlighting areas for potential improvement in future work. Long-tailed Object Detection. We further evaluated ChatRex on the more challenging LVIS Gupta et al. (2019) dataset, which encompasses 1,203 object categories. ChatRex achieved 43.1 mAP, surpassing open-set detection models like Grounding DINO Liu et al. (2023c) and GLIP Li et al. (2022), and is on par with T-Rex2 Jiang et al. (2025). We attribute this performance to the strong semantic understanding capabilities of the LLM. Within the ChatRex model structure, the LLM primarily functions to classify bounding boxes generated by the proposal model. By aligning visual features with the textual space through comprehensive training and data optimization, the LLM is able to accurately classify broad spectrum of categories, thereby demonstrating its robustness in handling complex, long-tailed object detection scenarios. Referring Object Detection. Referring object detection involves identifying an object based on given descriptive statement. We evaluated ChatRex on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, which predominantly focus on single-object detection, where each expression generally corresponds to single object. As shown in Tab. 3, ChatRex achieves results comparable to specialized models and other MLLMs. This demonstrates that ChatRex possesses strong referring capabilities, which are crucial for tackling more complex perception tasks. 12 Method LLaVA-1.5 Liu et al. (2023b) Kosmos-2 Peng et al. (2023) Shikra-7B Chen et al. (2023a) GPT4RoI-7B Zhang et al. (2023) Ferret-7B You et al. (2023) Osprey-7B Yuan et al. (2024) VisionLLM v2-7B Wu et al. (2024) SPHINX-V-7B Lin et al. (2023c) ChatRex-7B (Ours) LVIS SS 49.0 39.0 49.7 51.3 63.8 65.2 68.9 87.1 89.7 S-IoU PACO SS S-IoU 19.8 8.7 19.8 12.0 36.6 38.2 46.3 62.9 82. 42.2 32.1 43.6 48.0 58.7 73.1 67.7 80.0 89.9 14.6 4.8 11.4 12.1 26.0 52.7 44.0 55.0 82.0 Table 5: Comparison on referring object classification task."
        },
        {
            "title": "With Separated\nGrained Training",
            "content": "COCO R@0."
        },
        {
            "title": "COCO\nmAP",
            "content": "LVIS R@0."
        },
        {
            "title": "No\nYes",
            "content": "87.3 91.2 47.4 48.5 67.9 77.1 35.3 43.1 Table 6: Ablation on the granularity-based training design of the universal proposal network. R@0.3 and P@0.3 represents recall and precision at score threshold at 0.3."
        },
        {
            "title": "5.2 Understanding Capability Evaluation",
            "content": "General Multimodal Benchmarks. We evaluated ChatRex on various academic benchmarks including MMBench Liu et al. (2023d), MM-Vet Yu et al. (2023), SEEDI Li et al. (2023a), LLaVAW Liu et al. (2023a), MMMU Yue et al. (2023), POPE Li et al. (2023b), and HallusionBench Guan et al. (2023). As shown in Tab. 4, ChatRex performs on par with other leading MLLMs, demonstrating that it maintains strong understanding and dialogue capabilities. Notably, ChatRex achieves higher scores on POPE and HallusionBench, which we attribute to the joint training of perception tasks. This incorporation of fine-grained object information significantly reduces hallucination, illustrating how perception and understanding can enhance one another. Region Caption Benchmarks. In addition to image-level understanding, ChatRex demonstrates strong region-level understanding capabilities. Following Osprey, we evaluate the referring object classification task on the LVIS Gupta et al. (2019) and PACO Ramanathan et al. (2023) datasets. In this task, given the object index, the model is prompted to output the category name of the specified region. The evaluation metrics include Semantic Similarity (SS) and Semantic Intersection over Union (S-IOU) Rezatofighi et al. (2019). As shown in Tab. 5, ChatRex achieves state-of-the-art results, highlighting its robust region classification capabilities."
        },
        {
            "title": "5.3 Ablation Experiment",
            "content": "Ablation on Universal Proposal Network. In ChatRex, we leverage Universal Proposal Network (UPN) to supply box inputs to the LLM. We conduct an ablation study to validate the effectiveness of the granularitybased training methods. Specifically, we compare results with and without the use of distinct granularity prompts. In the absence of dual granularity, all datasets are treated as single class during training. We evaluate the recall rates on the COCO and LVIS datasets at fixed confidence threshold of 0.3, as well as their final mAP. As shown in Tab. 6, the granularity-based approach achieves higher recall rate, indicating that proposal models trained with separate granularity prompts can detect larger number of objects at given confidence level, ultimately enhancing the overall detection performance."
        },
        {
            "title": "6 ChatRex Capabilities and Qualitative Examples",
            "content": "In this section, we showcase the capabilities of ChatRex visualization results."
        },
        {
            "title": "6.1 Common Object Detection",
            "content": "Results on Common Object Detection Task QA Example: User: Please detect person; cup in this image. Answer the question with object indexes. ChatRex: <g>person</g><o><obj1><obj5><obj16><obj21></o>n <g>cup</g><o><obj12><obj14><obj33></o> Figure 7: Visualization on Common Object Detection Task."
        },
        {
            "title": "6.2 Long-tailed Object Detection",
            "content": "Results on Long-tailed Object Detection Task QA Example: User: Please detect saluki; folding chair in this image. Answer the question with object indexes. ChatRex: <g>saluki</g><o><obj12></o>n <g>folding chair</g><o><obj19><obj23></o>n Visualization: Figure 8: Visualization on Long-tailed Object Detection Task."
        },
        {
            "title": "6.3 Short-Phrase Object Detection",
            "content": "Results on Short-Phrase Object Detection Task QA Example: User: Please detect bamboo cutting board; fresh green onions in this image. Answer the question with object indexes. ChatRex: <g>bamboo cutting board</g><o><obj1></o>n <g>fresh green onions</g><o><obj52><obj66></o>n Visualization: Figure 9: Visualization on Short-Phrase Object Detection Task."
        },
        {
            "title": "6.4 Referring Object Detection",
            "content": "Results on Referring Object Detection Task QA Example: User: Please detect man step on skateboard; man wearing hat and phone in his hand in this image. Answer the question with object indexes. ChatRex: <g>man step on skateboard</g><o><obj26></o>n <g>man wearing hat and phone in his hand</g><o><obj11></o>n Visualization: Figure 10: Visualization on Referring Object Detection Task."
        },
        {
            "title": "6.5 Region Caption",
            "content": "Results on Region Caption Tasks QA Example: User: Give short phrase that describes <obj0>;<obj1>. ChatRex: <g>Starbucks logo</g><o><obj0></o>n <g>a coffee cup</g><o><obj1></o>n Visualization: Figure 11: Visualization on Detailed Grounded Image Caption Task."
        },
        {
            "title": "6.6 Region QA",
            "content": "Results on Region QA Task QA Example: User: What is <obj0> wearing and what is he doing now? ChatRex: <obj0> is wearing black jacket and black beanie. He is standing in store, looking at something on the shelf. Visualization: Figure 12: Visualization on Region QA Task."
        },
        {
            "title": "6.7 Brief Grounded Image Caption",
            "content": "Results on Brief Grounded Image Caption Task QA Example: User: Please briefly describe this image and detect all the mentioned objects. Answer with grounded object indexes. ChatRex: <g>man</g><o><obj13></o> in <g>white tuxedo</g><o><obj17></o> with <g>red bow tie</g><o><obj16></o> is holding an <g>Oscar statuette</g><o><obj14></o> and standing on stage with microphone, while large, ornate Oscar statue is visible in the background. Visualization: Figure 13: Visualization on Brief Grounded Image Caption Task."
        },
        {
            "title": "6.8 Detailed Grounded Image Caption",
            "content": "Results on Detailed Grounded Image Caption Task QA Example: User: Please provide detailed description of the image and detect all the mentioned objects. Answer the question with grounded object indexes. ChatRex: The image is portrait-style photograph of <g>person</g><o><obj12></o> standing in front of backdrop. The individual appears to be young adult with <g>curly hair</g><o><obj3></o>, styled in way that it falls over the <g>forehead</g><o><obj13></o> and frames the <g>face</g><o><obj18></o>... Visualization: Figure 14: Visualization on Detailed Grounded Image Caption Task."
        },
        {
            "title": "6.9 Grounded Counting",
            "content": "Results on Grounded Counting Task QA Example: User: How many dogs are there in this image? Answer the question with the number of objects and locate them with object indexes. ChatRex: The are 20 <g>dog</g><o><obj12>...<obj62></o>s. Visualization: Figure 15: Visualization on Grounded Counting Task."
        },
        {
            "title": "6.10 Grounded Conversation",
            "content": "Results on Grounded Conversation Task QA Example: User: Answer the question in grounded format. How would you organize this space? ChatRex: To organize the space, would sort the items into categories such as stationery, electronics, personal items, and accessories. For stationery, would group the <g>pen</g><o><obj23></o>, <g>pencil</g><o><obj12></o>... Visualization: Figure 16: Visualization on Detailed Grounded Conversation Task. 23 Results on Grounded Conversation Task QA Example: User: Answer the question in grounded format. How would you organize this space? ChatRex: To organize the space, would sort the items into categories such as stationery, electronics, personal items, and accessories. For stationery, would group the <g>pen</g><o><obj23></o>, <g>pencil</g><o><obj12></o>... Visualization: Figure 17: Visualization on Detailed Grounded Conversation Task."
        },
        {
            "title": "7 Conclusions",
            "content": "In this work, we reveal the perception drawback in existing MLLMs due to their conflict modeling between perception and understanding, and the lack of data that effectively balances these two aspects. To address these issues, we introduce ChatRex, model with decoupled architecture, along with Rexverse-2M, multi-granularity dataset designed to balance these two aspects. ChatRex demonstrates strong perception abilities while also excelling in multimodal understanding and dialog capabilities. The synergy between perception and understanding allows ChatRex to be highly interactive by grounding mentioned objects within visual contexts during dialogue. We believe that perception and understanding are both critical for MLLMs, as their integration can significantly enhance model capabilities and unlock wide range of novel applications."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Feng Li and Hao Zhang for their early suggestions on this work."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 1 Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 1, 4 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35:2371623736, 2022. 1, 4 Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. 1 Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023b. 1, 4, Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. 9 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 5, 11 25 Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024a. 10, 11 Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023a. 1, 2, 4, 11, 12, 13, Ting Chen, Saurabh Saxena, Lala Li, David Fleet, and Geoffrey Hinton. Pix2seq: language modeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021. 2, 4, 11 Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. In ICLR, 2022. 1, 4 Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv: 2312.14238, 2023b. 12 Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv:2404.16821, 2024b. 1, 2, 3, 4, 9, 10, 11, 12, 32 Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. 1, Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In NeurIPS, 2023. 1, 4 Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024a. 4 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. NeurIPS, 36, 2024b. 12 Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 1, 4 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k HD. arXiv: 2404.06512, 2024. 4 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 3, 10 Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv preprint arXiv:2310.14566, 2023. 13 26 Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 3, 10, 11, 12, 13 Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, pages 29612969, 2017. 6 Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE, 2019. 10 Dongfu Jiang, Xuan He, Huaye Zeng, Con Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv:2405.01483, 2024. 4 Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic object detection via text-visual prompt synergy. In European Conference on Computer Vision, pages 3857. Springer, 2025. 5, 11, 12, 33 Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 3, Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. Segment anything. arXiv: 2304.02643, 2023. 5, 10 Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 10 Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023. 4 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. 4 Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. 13 Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024b. 1, 4 Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024c. 4 Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. 11, 12 Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv: 2403.18814, 2024d. 4 Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, EMNLP, pages 292305, 2023b. 27 Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023c. 4 Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a. 4 Fangjian Lin, Jianlong Yuan, Sitong Wu, Fan Wang, and Zhibin Wang. Uninext: Exploring unified architecture for vision recognition. In Proceedings of the 31st ACM International Conference on Multimedia, pages 32003208, 2023b. 12 Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024a. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, volume 8693, pages 740755, 2014. 1, 3, 5, 10, 11 Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024b. 11 Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023c. 4, 13 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv: 2310.03744, 2023a. 11, 13 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023b. 2, 4, 9, 12, 13 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. URL https://llava-vl.github.io/blog/ 2024-01-30-llava-next/. 4 Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023c. 11, 12 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv: 2307.06281, 2023d. 13 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 1001210022, 2021. 33 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, pages 1197611986, 2022. 3 Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis Raptis. In Proceedings of the IEEE/CVF Towards end-to-end unified scene text detection and layout analysis. Conference on Computer Vision and Pattern Recognition, pages 10491059, 2022. 10 Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixtureof-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. 4 28 Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: multimodal literate model. arXiv preprint arXiv:2309.11419, 2023. 1, Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. arXiv preprint arXiv:2404.13013, 2024. 1, 2, 4, 11, 12 Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 1120, 2016. 3, 11 Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. MM1: methods, analysis & insights from multimodal LLM pre-training. arXiv: 2403.09611, 2024. 4 OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023. 1, Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 4, 13 Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. Perceptiongpt: Effectively fusing visual perception into llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2712427133, 2024. 4 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, volume 139, pages 87488763, 2021. 6 Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. 11, 13 Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13009 13018, 2024. 4 Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In NIPS, pages 9199, 2015. 11 Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. 3, 9 Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: metric and loss for bounding box regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 658666, 2019. 13, 33 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:2527825294, 2022. 6 Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: benchmark for detecting human in crowd. arXiv preprint arXiv:1805.00123, 2018. 10, 29 Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In ICCV, pages 84308439, 2019. 5, 10, 11 Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 4 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/ tatsu-lab/stanford_alpaca, 2023. 1 Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv: 2312.11805, 2023. 1, 4 InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities. https: //github.com/InternLM/InternLM, 2023. 1 Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 4 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv: 2302.13971, 2023a. 1 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv: 2307.09288, 2023b. 1 Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. One-peace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172, 2023a. 12 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 4, 11, 12, Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023b. 4 Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. arXiv preprint arXiv:2406.08394, 2024. 4, 13 Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. 11 Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an LMM perceiving any aspect ratio and high-resolution images. arXiv: 2403.11703, 2024. 4 Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024a. 4 Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024b. 4 Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 1, 2, 4, 11, 13 Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, pages 6985, 2016. 3, 11 Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 13 Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820228211, 2024. 11, Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 13 Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 11 Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring. arXiv preprint arXiv:2403.09333, 2024. 4 Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out all object locations at any granularity with large language models. In European Conference on Computer Vision, pages 405422. Springer, 2025. 4 Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022a. 11, 12, Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Chunyuan Li, Jainwei Yang, et al. Llava-grounding: Grounded visual chat with large multimodal models. In European Conference on Computer Vision, pages 1935. Springer, 2025. 4 Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 4, 12 Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv: 2307.03601, 2023. 13 Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu. Bamboo: Building mega-scale vision dataset continually with human-machine synergy. arXiv preprint arXiv:2203.07845, 2022b. 10 Xiangyu Zhao, Xiangtai Li, Haodong Duan, Haian Huang, Yining Li, Kai Chen, and Hua Yang. Mg-llava: Towards multi-granularity visual instruction tuning. arXiv preprint arXiv:2406.17770, 2024. 4, 6 Chenchen Zhu, Fanyi Xiao, Andrés Alvarado, Yasmine Babaei, Jiabo Hu, Hichem El-Mohri, Sean Culatana, Roshan Sumbaly, and Zhicheng Yan. Egoobjects: large-scale egocentric dataset for fine-grained object understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20110 20120, 2023."
        },
        {
            "title": "A MLLM Evaluation Details on Detection Datasets",
            "content": "In this section, we explain our methodology for evaluating MLLMs on the object detection task, including the design of model-specific prompts, the visualization of output results, and comprehensive analysis of the challenges and limitations encountered throughout the evaluation process. A.1 Prompt for Each MLLM For each MLLM evaluated, we utilized either the prompts used in the original paper or manually crafted optimized prompts to maximize performance. The specific prompts used in our evaluation are detailed in Tab. 7, Method Prompt In this picture, you are required to finish object detection for every instance of the category we provide. To complete the above mission, you need to provide me with the answers in the format of Python list of dictionaries by the category provided above. Attention: No other category shall appear in the detection object attributes, except for the genre we offer. Bounding box format: [108(xmin), 210(ymin), 810(xmax), 640(ymax)], where xmin, ymin, xmax and ymax must be positive integers. If there is no object in the picture, please provide an empty list. Here is an example which you must follow in your responses. Example: If the question is as below: Category: [person, car]. If there is an object of the category, The Answer should be: [{{\"class\": \"person\", \"rect\": [0, 614, 220, 771]}}, {{\"class\": \"person\", \"rect\": [638, 468, 784, 941]}}, {{\"class\": \"car\", \"rect\": [110, 100, 500, 300]}}]. Else if no object of the category in the picture, the Answer should be:[]. Here is the question you shall answer: Category: {} What is the location of all instances of categories {} in the image? Please answer me respectively. Qwen2-VL-7B & InternVL2-8B Ferrt-7B Shikra-7B Help me locate {} in <image>and give its coordinates, please. Groma-7B [grounding] There are categories you need to describe with positions, only including <p>{}<p>. Give me short description of the image and include the coordinates [[x0,y0,x1,y1]] for each instance of categories. Table 7: Prompt of different MLLMs for detection on COCO. A.2 More Analysis on the Detection Results In previous sections, we showed that most MLLMs generally have low recall on object detection tasks such as COCO. Additionally, specific deficiencies are observed in certain models. For example, Shikra Chen et al. (2023a) exhibits notable issue with coordinate offset, where the predicted bounding boxes fail to accurately enclose the target objects. This misalignment is primarily attributed to quantization errors inherent in the model. Moreover, during the evaluation phase, most models, excluding general-purpose MLLMs such as InternVL2 Chen et al. (2024b) and Qwen2-VL Wang et al. (2024), frequently struggle to follow task-specific instructions. This observation highlights persistent challenges in the instruction-following capability within these models. We also identified distinct issue with general MLLMs during the evaluation process: pattern of repeated output coordinates in their predictions. This repetition is not random but follows systematic pattern. For example, in the case of Qwen2-VL-7B, the model consistently generates bounding box coordinates where each subsequent bounding box is derived by incrementally adjusting the preceding one. For example, the xmin and xmax values of the bounding boxes are increased by fixed value, such as 24, as illustrated in the below block. We argue this behavior stems from the auto-regressive coordinate prediction mechanism. The reliance on previously predicted coordinates introduces probabilistic bias, causing subsequent predictions to deviate systematically as regular modifications of earlier outputs. This limitation underscores the models inability to independently and accurately generate coordinates without cumulative error propagation. 32 Task Prompt Templates Grounding & Detection & Referring Brief Image Caption with Grounding Detailed Image Caption with Grounding Region Caption in Category Name Region Caption in Short Phrase Region Caption Briefly Region Caption in One Sentence Grounded Counting Grounded Conversation Please detect [OBJ] in this image. Answer the question with object indexes. Please briefly describe this image and detect all the mentioned objects. Answer with grounded object indexes. Please provide detailed description of the image and detect all the mentioned objects. Answer the question with grounded object indexes. What is the category name of [OBJ]? Answer the question with its category name in free format. Can you provide me with short phrase description of [OBJ]? Answer the question with short phrases. Can you provide me with brief description of [OBJ]? Answer the question with brief description. Can you provide one sentence description of [OBJ] in the image? Answer the question with one sentence description. How many [OBJ] are there in this image? Answer the question with the number of objects and locate them with object indexes. Answer the question in grounded format. Question: Table 8: Examples of prompts used for different perception tasks in ChatRex. Example of repeated coordinate prediction in Qwen2-VL-7B. Response: [ {class: car, rect: [234, 186, 370, 283]}, {class: car, rect: [568, 214, 622, 283]}, {class: car, rect: [743, 186, 822, 300]}, {class: car, rect: [110, 199, 128, 240]}, {class: car, rect: [134, 200, 152, 240]}, {class: car, rect: [158, 200, 176, 240]}, {class: car, rect: [182, 200, 200, 240]}, {class: car, rect: [206, 200, 224, 240]} ]... Repeat until exceeds the max output length."
        },
        {
            "title": "B Model Details",
            "content": "B."
        },
        {
            "title": "Implementation Details of UPN",
            "content": "In alignment with the methodology of T-Rex2 Jiang et al. (2025), we employ the Swin Transformer Large model Liu et al. (2021), pre-trained on ImageNet Deng et al. (2009), as the vision backbone. During the Hungarian matching process, the optimization incorporates three types of losses: classification loss, box L1 loss, and generalized intersection over union (GIOU) loss Rezatofighi et al. (2019), with respective weights of 2.0, 5.0, and 2.0. For the overall loss computation, we similarly utilize classification loss, box L1 loss, and GIOU loss, adjusting the corresponding weights to 1.0, 5.0, and 2.0. Consistent with the training strategy of DINO Zhang et al. (2022a), we adopt contrastive denoising training (CDN) to enhance training stability and accelerate convergence. The pre-trained weights of T-Rex2-L are used for initialization, followed by full-parameter optimization on the universal proposal task. B."
        },
        {
            "title": "Implementation Details for ChatRex",
            "content": "We utilize the CLIP pre-trained ViT-Large-14-336 model as the low-resolution visual encoder and the LAION pre-trained ConvNext-Large-320 model as the high-resolution visual encoder. The input resolution is set to 336x336 for the low-resolution encoder and 768x768 for the high-resolution encoder. During the pretraining stage, we employ batch size of 32 per device, resulting in an aggregate batch size of 256 across all devices. For the instruction-tuning stage, the batch size is reduced to 16 per device, with total batch size of 128. The learning rate is initialized at 1e-3 for the pretraining stage and adjusted to 2e-5 during the instruction-tuning stage. For perception and region-based question-answering tasks, we designed tailored prompts to effectively guide and instruct the models. Examples of these customized prompts are provided in Tab. 8 33 Figure 18: Visualization of the proposed phrase-conditioned region description method. Details for the Rexverse-2M Data Engine C.1 Visualization of Rexverse-2M dataset We visualize portion of the Rexverse-2M dataset in Fig. 19, including image-level annotations and regionlevel annotations. C.2 Prompt for Different Modules In the Rexverse-2M data engine, we leverage both state-of-the-art MLLMs and Large Language Models LLMs to construct the dataset. The prompts employed in each module are detailed in Tab. 9. Task Model Prompt Image Description InternVL2-8B Please provide one-sentence description for this image. Phrase-Conditioned Region Descrption InternVL2-8B Region Filtering & Rewriting LLAMA3-8B-Instruct will provide you with short phrase description of an object and its image. You need to rewrite this short phrase description to one sentence description by adding more details about this object based on the image. The rewritten description can only focus on this object according to the original description and should also be one-sentence description. The original short phrase description is: will provide you with one-sentence description of an object, and the category name of that object. Based on these two pieces of information, write referring description of the object. This description should capture the most important and distinguishing features of the object, and should not describe anything that doesnt exist in the description Ive provided. Note that the referring object should be the category name provided. The rewritten referring description should be more than 5 words but less than 10 words. The referring description should be as short and concise as possible, without commas. Directly output the answer. Table 9: Prompt for each module in Rexverse-2M data engine where MLLM or LLM is used. C.3 Effectiveness of Phrase-Conditioned Region Description In the data engine, to generate region-level descriptions, we adopt phrase-conditioned region captioning method. This approach utilizes short phrase or category name, specified for each region during the grounding phase, as an additional prompt to guide multimodal large language model (MLLM) in producing captions. This method differs from directly inputting regional images into the MLLM for captioning. As illustrated in Figure 18, the direct image captioning approach often suffers from hallucinations, particularly when regions contain distracting objects or are too small to be reliably recognized. In contrast, the phraseconditioned method mitigates these issues by incorporating contextual input, leading to more accurate captions. 34 Figure 19: Visualization of the Rexverse-2M dataset."
        }
    ],
    "affiliations": [
        "International Digital Economy Academy (IDEA)"
    ]
}