{
    "paper_title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
    "authors": [
        "Zachary Novack",
        "Zach Evans",
        "Zack Zukowski",
        "Josiah Taylor",
        "CJ Carr",
        "Julian Parker",
        "Adnan Al-Sinan",
        "Gian Marco Iodice",
        "Julian McAuley",
        "Taylor Berg-Kirkpatrick",
        "Jordi Pons"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating $\\approx$12s of 44.1kHz stereo audio in $\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest text-to-audio model to our knowledge."
        },
        {
            "title": "Start",
            "content": "Fast Text-to-Audio Generation with Adversarial Post-Training Zachary Novack1,2, Zach Evans2, Zack Zukowski2, Josiah Taylor2, CJ Carr2, Julian Parker2 Adnan Al-Sinan3, Gian Marco Iodice3, Julian McAuley1, Taylor Berg-Kirkpatrick1, Jordi Pons2 1UC San Diego, 2Stability AI, 3Arm, Shared authorship, Work done while an intern at Stability AI 5 2 0 2 3 1 ] . [ 1 5 7 1 8 0 . 5 0 5 2 : r AbstractText-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) posttraining, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is simple procedure that (1) extends recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with number optimizations to Stable Audio Open and build model capable of generating 12s of 44.1kHz stereo audio in 75ms on an H100, and 7s on mobile edge-device, the fastest text-to-audio model to our knowledge. 1. INTRODUCTION Despite the recent progress building generative text-to-audio models, such systems require seconds to minutes per generation [1][4], which limits their usability for most creative use cases. Our goal is to accelerate gaussian flow-based models, i.e., diffusion models [5] or the more recent rectified flows [6]. Since gaussian flow models can be costly at inference time due to their iterative (step-based) sampling [7], considerable effort has gone into accelerating them [8][11]. Most of these works rely on (step) distillation, where the teacher model provides direct supervision to train distilled few-step generator. As such, the few-step generator learns to map multiple inference steps into single step, or small number of steps, by distilling the teachers trajectories. However, most distillation approaches come with practical drawbacks: like online methods [9], [12][19], which are costly to train as they require 2-3 full models held in memory at the same time, or offline methods [20][22], which require significant resources to generate and store trajectory-output pairs offline to later train on. Furthermore, most distillation setups [12][15], [17], [18] distill the teacher with Classifier-Free Guidance (CFG), which is double-edged sword, as the generator inherits both the high quality/prompt adherence as well as the low diversity and over-saturation artifacts of CFG [23]. To avoid such drawbacks, some studied acceleration through posttraining (without distillation) [24], [25]. These works are primarily adversarial, as opposed to distillation methods that use adversarial auxiliary losses [9][11], [17], [18], and use real data rather than teacher-generated samples, thus freeing the costly requirement of using trajectory-output pairs. The motivation here is that the adversarial loss encourages realism, making each estimate better than the standard gaussian flow estimate. Such improved estimates enable post-trained models to use fewer (1-8) sampling steps [24], [25]. Recent image works, like UFOGen [24] and APT [25], explored adversarial posttraining, yet UFOGen reported limited gains [24] and APT required distillation-based initialization [25]. How to improve adversarial post-training and apply it to audio remains an open question. Within the audio-domain, only few works exist on accelerating gaussian flow models. Most rely on consistency models [26][29] and have no adversarial component, despite the presence of adversarial losses in their image-domain counterparts [17]. The only work to use any adversarial component is Presto [18], although it is mainly based Fig. 1: ARCs adversarial relativistic loss. Pairs of generated and real samples (with the same text prompts) are passed into the discriminator (with additive noise), where the generator and discriminator are trained to minimize and maximize (respectively) the difference between fake and real outputs. Fig. 2: ARCs contrastive loss. The discriminator is also trained to maximize the difference between audios with correct and incorrect (shuffled) prompts. on distillation [9]. Finally, most existing accelerated text-to-audio models focus on band-limited [26], mono [18], [27], [29] audio. We show that rectified flows can be accelerated with adversarial post-training, text-to-audio introducing the first fully-adversarial acceleration framework that requires neither distillation nor CFG. We propose Adversarial Relativistic-Contrastive post-training (or ARC) that (1) extends recently proposed relativistic adversarial formulation [30], [31] to text-conditional audio generation and (2) combines this with novel contrastive loss that encourages the discriminator to focus on prompt adherence. To our knowledge, we are the first to attempt such post-training recipe on audio generation, the first to use adversarial relativistic losses for gaussian flow acceleration across modalities, and the first to use contrastive discriminator formulation. We pair ARC with an array of architecture updates to Stable Audio Open (SAO) [4] to achieve accelerated performance, generating 12s of 44.1kHz stereo audio in 75ms on H100 GPU, being 100x faster than the original SAO model. Across experiments, we find that ARC is competitive with existing state-of-the-art acceleration methods, and notably preserves generative diversity far beyond previous methods. We further optimize it for on-device CPU-based inference, achieving text-to-audio model that can run locally on edge-devices (i.e., smartphones) with an inference time of 7s. To our knowledge, we are the first to operate at this inference speed, enabling efficient on-device execution, and facilitating the applicability of text-to-audio in creative domains. Code will be released upon acceptance, and demo website is available. 2. METHOD 2.1. Rectified flows pre-training Given prompt and (stereo audio) representation x0, the goal of (text-to-audio) rectified flows [6], [20] is to learn model that transfers between the data distribution p0 and some prior distribution p1 (e.g., isotropic gaussian noise) given c, thus allowing to generate samples from p0 by sampling from p1 given according to the learned model. The forward corruption process (from data to noise) is given by: xt = (1 t)x0 + tϵ, ϵ (0, I), (1) where is the scalar time parameter ranging from 0 (data) to 1 (noise) that is pgen(t) distributed. For brevity, we refer to the noising process in Eq. 1 as sampling from the forward probability distribution q(xt x0). To reverse this process and generate samples (from noise to data), we can solve the ordinary differential equation (ODE): dxt = vθ(xt, t, c)dt, (2) where vθ(xt, t, c) is model trained to predict the velocity of the flow = ϵ x0. This model can thus be trained by adding noise to real samples according to Eq. 1 and predicting the velocity: Ex0,cX ,ϵN (0,I),tpgen(t)[v vθ(xt, t, c)2 2], arg min (3) θ 2.2. Adversarial Relativistic-Contrastive post-training (ARC) Gaussian flow models require tens to hundreds [6] of steps to produce high-fidelity outputs, as the objective in Eq. 3 (ℓ2-based) is minimized by estimating the instantaneous velocity of the flow at any noise level, which is equivalent to fitting the noise-conditional mean of the denoised outputs. This results in sampling schedules that require number of small sampling steps to maintain stable sampling trajectories [32]. Thus, the goal of adversarial post-training is to turn our pre-trained flow model vθ into few-step generator Gϕ by supplanting the ℓ2-based conditional mean loss with an adversarial loss, defined by some discriminator Dψ. Such discriminator evaluates the realism of denoised samples, providing distribution-level feedback that goes beyond the broad conditional mean loss. As such, if the denoised output at any given step is sufficiently real and higher-quality, fewer steps are required. key advantage of adversarial post-training over (the costly) distillation methods is that it avoids the generation and storage of trajectory-output pairs for offline methods and storing 2-3 full generative models in memory for online methods. It also sidesteps reliance on the potentially poor performance of the pretrained (teacher) model. While adversarial post-training is broad class of algorithms, our method is based on combining relativistic adversarial loss and contrastive loss that are jointly optimized: min ϕ max ψ LARC(ϕ, ψ) = LR(ϕ, ψ) + λ LC(ψ), (4) where LR and LC are defined in Sec. 2.3 and 2.4, respectively. 2.3. Adversarial relativistic loss: the AR in ARC To start, both the few-step generator Gϕ(xt, t, c) and the discriminator Dψ(xt, t, c) are initialized from vθ(xt, t, c) that is and conditioned and already can process noisy data xt. This initialization improves training stability. Under this setup, adversarial relativistic post-training proceeds as in Fig. 1: given x0 (clean data) and (text prompt), we corrupt x0 with noise following Eq. 1 to obtain xt (noised data) for some sampled from pgen(t). This is then passed into the generator to produce ˆx0 = Gϕ(xt, t, c)1. Given the generated ˆx0 and 1In practice, for training stability we do not convert the model parameterization to predict the clean output directly, and instead keep the base velocity prediction and parameterize the generator as Gϕ(xt, t, c) = xt tvϕ(xt, t, c). the real x0, we again add noise following Eq. 12 given some timestep that is sampled from pdisc(s). Then, the noisy samples xs and ˆxs become inputs to the discriminator Dψ. Although past work uses the same distribution for pgen(t) and pdisc(s), we disentangle these two for added flexibility and performance [18]. We train both the discriminator and generator with this relativistic adversarial loss [30]: LR(ϕ, ψ) = (cid:2)f (cid:0)gen(x0, t, s, c) real(x0, s, c)(cid:1)(cid:3) x0,cX tpgen(t), spdisc(s) gen(x0, t, s, c) = Dψ(q(ˆxs Gϕ(q(xt x0), t, c)), s, c) real(x0, s, c) = Dψ(q(xs x0), s, c), (5) (6) (7) where (x) = log(1 + ex), real are the discriminator logits on noisy versions of real samples, and gen are the logits on noisy versions of generated samples, themselves generated from noisy versions of real samples. This relativistic adversarial loss is notably different from the standard GAN objective. The Gϕ of (standard) GANs aims to minimize the detection probability by Dψ, while the Dψ attempts to maximize the detection probability for generated samples and minimize detection probability for real sampleswhere each min/max happens independently. Instead, LR is calculated on pairs of real/generated data [30], [31], such that Gϕ minimizes its detection relative to its paired real sample in Dψ space (with same prompt), and Dψ maximizes detecting the generated sample relative to its real pair. Thus, Gϕ wants every generated sample to be more real than its paired real sample, while Dψ wants every real sample to be more real than its paired generated sample. Critically, while Huang et al. [30] do not specify how samples may be paired, our pairs are always highly related due to our text-conditional task, where pairs of real/generated samples share the same prompt c, thus providing stronger gradient signal than relying on random pairings. While we find relativistic losses more effective than standard GAN losses  (Table 1)  , we also find that adversarial losses alone are worse at prompt adherence than distillation. Without direct distillation from teacher, with strong prompt adherence through CFG [23], it is hard for the Gϕ to improve prompt adherence with realism-focused Dψ. 2.4. Contrastive loss: the in ARC The above relativistic objective contributes in improving output quality and sampling efficiency, see Sec. 2.2, but it alone does not fix the poor text-following caused by the adversarial loss, see Sec. 2.3. Instead of using CFG to increase consistency, for simplicity, we again rely on new relativistic loss using the same discriminator. Specifically, we find that prompt adherence can be improved by training the discriminator also as an audio-text contrastive model [33]. Since our discriminator is natively text-conditional due to its pre-trained text-to-audio backbone, we train it with similar relativistic loss that maximizes the difference between real samples with incorrect and correct prompts  (Fig. 2)  : LC(ψ) = (cid:104) x0,cX , spdisc(s) (cid:0)real(x0, s, P[c]) real(x0, s, c)(cid:1)(cid:105) , (8) where the discriminator Dψ maximizes the difference between the incorrect and correct prompts in Dψ logit space, and P[] denotes random intra-batch permutation of the text prompts c. In words, by maximizing this objective, Dψ attempts to make its outputs higher for incorrect audio-text pairs relative to the pairs with correct prompts. This loss can be viewed as contrastive loss [34], where the discriminator is mapping correct audio-text pairs closer than 2The ϵ noise is independently sampled for each element of the real/generated pairs, but the noise level (or in Eq. 1) is fixed per pair. mismatched pairs. Note that this is only loss for the discriminator, as this encourages it to understand the alignment between prompts and noisy inputs and focus on semantic features, and prevents the model from focusing on easier (e.g., high-frequency [18]) features. Thus, with semantically-aware Dψ, the relativistic adversarial loss is tailored towards improving prompt adherence. Interestingly, LC eliminates the need for CFG, which helps with prompt adherence but is documented to worsen diversity and over-saturate outputs [23]. 2.5. Ping-pong sampling With ARC post-training, our model is finetuned to directly estimate clean outputs x0 from different noise levels τ , instead of predicting the instantaneous velocity like rectified flow models do. To adjust for that, we employ ping-pong sampling [14] instead of the traditional ODE solvers that the rectified flow models use (e.g., Euler). Ping-pong sampling alternates between denoising and re-noising to iteratively refine samples. Given some starting noisy sample xτi , we denoise it using our few step generator ˆx0 = Gϕ(xτi , τi, c). Then, we re-noise the sample to some lower noise level xτi1 = (1 τi1)ˆx0 + τi1ϵ, where τi1 < τi and ϵ (0, I). This process repeats times, progressively improving the sample towards the final clean data ˆx0. Note that here we do not use CFG [23]. Provided that CFG is memory intensive, because two forward passes (conditional and unconditional runs) are required [23], this decision is critical for our on-device experiments, as our method uses only half the (V)RAM. 2.6. Acceleration as reward modeling: post-training perspective We can connect ARC to language models preference post-training (alignment) [35][37], wherein one trains reward model on human preferences (i.e., winning-losing pairs yw, yl given the same prompt c) to fit preference model p(yw yl c) [38], and then post-trains (aligns) language model to maximize these rewards. Specifically, because our relativistic objective is on pairs of real/generated data given the same text prompts c, then with (x) = log(1+ex) [30] LR is equivalent to maximizing the likelihood of pψ(xs ˆxs c) as in language models. This means that Dψ is trained implicitly as reward model, where the reward is defined as realism against an onlinecreated dataset of generated samples, and our generator is trained to produce samples that maximize the relative reward pψ(ˆxs xs c). 3.2. Training and sampling details Our training data follows SAO [4] but excludes the long-form FMA music, as our focus is on sound effects and loops. This results in 6,330h (472,618 audios) of Freesound samples with CC0, CC-BY, or CC-Sampling+ licenses. Our rectified flow model was trained for 670k iterations. For each acceleration algorithm, we finetune the model 100k iterations with batch size of 256 across 8 H100 GPUs. We use learning rate of 5107 for both the generator and discriminator, with AdamW, and set λ=1. pgen(t) is uniform distribution in log-SNR space from -6 to 2 [18], to align post-training with inference, while pdisc(s) is the shifted logit normal distribution [6] which puts more weight on mid-to-high SNR regions [18]. During ARC post-training, we alternate between updating Gϕ (LR) and updating Dψ (LR+LC ). 3.3. Objective Evalaution We use FDopenl3 [43], KLpasst [44], and CLAP score [33] metrics, which are established metrics that broadly assess audio quality, semantic alignment, and prompt adherence. We also assess diversity, as recent text-to-image acceleration works noted distinct diversity reduction in distilled models [22], [45]. Following previous audio works, we also report recall and coverage metrics [18], [46], Rpasst and passt, which measure the overall distributional diversity in PASST [44] space. Yet, there exists no metric for assessing conditional diversity, i.e., the diversity of generations with the same prompt. To solve this, we propose the CLAP Conditional Diversity Score (or CCDS), which is calculated as the average CLAP cosine distance between pairs of generations within some batch with the same prompt c, averaged across all batches. Intuitively, if the distance between generations is low CCDS indicates low diversity. Accordingly, higher CCDS results indicate more diverse outputs. Finally, to measure speed, we report Real-Time Factor (RTF), which is the amount of audio generated divided by the latency, and VRAM peak usage measured on an H100. We evaluate our models with AudioCaps [47] test set, that contains 979 audio segments, each with several captions (881 audios were available and it includes 4,875 captions). We generate an audio per caption, resulting in 4,875 generations for FDopenl3 [43], KLpasst [44], CLAP score [33], Rpasst, and passt. For CCDS we generate 24 audios per caption from subset of 203 randomly selected AudioCaps prompts. 3. EXPERIMENTS 3.4. Subjective Evaluation 3.1. Models Generative models (vθ, Gϕ). Our latent generative models synthesize variable-length (up to 11.89s, with timing control [4]) stereo audio at 44.1kHz from text. It consists of the pre-trained 156M parameter autoencoder from SAO [4] that compresses waveforms into 64-channel 21.5Hz latent space, 109M parameter T5 text embedder [39], and Diffusion Transformer (DiT) that operates in the latent space. Note that the DiT is first pre-trained as rectified flow. Our model is variant of SAO [4] with improved efficiency: we reduce DiTs dimension from 1536 to 1024, the number of layers from 24 to 16, we add QK-LayerNorm [40], and remove the seconds start embedding. These reduce the DiT from 1.06B to 0.34B parameters. During inference, we compile the base DiT with torch.compile. Discriminator (Dψ). Our discriminator is initialized using the weights of the pre-trained rectified flow [9], [18]. Specifically, our discriminator is initialized using the input embedding layers and 75% of the DiT blocks of the pre-trained rectified flow. Then, these intermediate diffusion features are passed into (randomly initialized) lightweight discriminator head, comprised of 4x blocks of 1D convolutions, interleaved with GroupNorm [41] and SiLU activations [42]. In total, the discriminator has 274M parameters. We also run listening test with webMUSHRA [48]. Participants were asked to rate diversity, audio quality, and prompt adherence. We report mean opinion scores (MOS) on 5-point scale. Audio quality and prompt adherence are rated from 1 (bad) to 5 (excellent), and diversity from 1 (identical) to 5 (diverse). All 14 test participants used good playback system. Given that in our objective evaluation we already assess broad sound synthesis capabilities, this qualitative evaluation is designed to explore specific use cases and challenges. We focus on prompts relevant to music production, such as latin funk drumset 115 BPM, as well as spatially complex scenes like sports car passing by. Additionally, to assess diversity, we include broader, more ambiguous prompts such as fire crackling and water. 3.5. Baselines Stable Audio Open (SAO) [4]: quality baseline and acceleration reference point, being both larger and not optimized for speed. Pre-trained RF is our base accelerated model, see Sec. 2.1. Presto [18] is state-of-the-art distillation method for accelerating audio diffusion, using the base model and an auxiliary score model to minimize reverse KL loss along with GAN loss. Ablations. We ablate ARC by omitting LC or replacing LR with the standard least-squares adversarial (LLS) loss [18], [49], [50]. Table 1: Qualitative and quantitative results ( the lower the better or the higher the better). For MOS scores we also present 95% confidence intervals. Method SAO [4] SAO ( steps) SAO ( steps) Pre-trained RF Pre-trained RF ( steps) +ARC (ours) +Presto [18] +LR (w/o LC ) +LLS+LC (w/o LR) RF + ARC (one-step) RF + ARC (few-step) Steps 100 50 8 50 8 8 8 8 8 1 4 FDopenl3 () KLpasst () CLAP () CCDS Rpasst () () Cpasst () Diversity () Quality () Prompt () adherence 78.24 82.17 143.21 88.13 91.97 84.43 93.05 90.92 98.13 100.17 90.45 2.14 2.22 4. 2.04 2.59 2.24 2.11 2.62 2.39 2.45 2.21 0.29 0.28 0.05 0.30 0.23 0.27 0.27 0.20 0.26 0.24 0.27 0.35 0.34 0. 0.34 0.38 0.41 0.26 0.57 0.36 0.33 0.40 0.26 0.26 0.17 0.34 0.25 0.28 0.27 0.29 0.17 0.11 0.26 0.41 0.42 0. 0.42 0.30 0.37 0.36 0.28 0.34 0.27 0.40 4.00.2 - - 3.10.2 - 4.40.3 2.70.4 - - - - 4.00.3 - - 3.70.3 - 3.50.4 4.00.3 - - - - 4.30.3 - - 4.20.2 - 3.80.5 4.20.3 - - - - RTF () 3.56 7.01 37.58 18.82 100.74 156.42 156.42 156.42 156.42 247.67 440.30 VRAM (GB, ) 5.51 5.51 5.51 4.20 4.20 4.06 4.06 4.06 4. 4.06 4.06 We naively speed up methods by reducing the number of steps. As, we aim to accelerate text-to-audio models without compromising quality. Hence, our qualitative evaluation includes the following baselines: Presto (8-step, distillation-based), SAO (100-steps, highestquality), and pre-trained RF (50-step, before ARC post-training). 3.6. Results and discussion Unsurprisingly, the best results are from the slow, much larger SAO [4]. The studied accelerated models obtain comparable metrics but are 100x faster than SAO (100-steps), and 10x faster than the pre-trained RF model (50-steps). We found significant differences in diversity MOS scores, but only minor, non-significant differences for quality/prompt adherence. One interesting outlier is Presto, which improves the quality of the base RF model (in terms of MOS score) at the expense of severely compromising diversity (red in Table 1) and worsening FDopenl3. Conversely, ARC post-training further improves the diversity of the generations and, while obtains among the best FDopenl3, the MOS quality results are slightly worse than the pre-trained RF model. While Presto delivers high quality, its outputs lack variety, making it less suitable for generative tasks and creative applications. Our results also show that rectified flow models (pre-trained RF, 50-steps) are competitive with diffusion models (SAO, 100-steps) despite using half the steps. However, simply reducing the number of steps is not viable solution, as it leads to significant quality degradation. Our first ablation shows that training with only LR leads to bad prompt adherence. Interestingly, diversity scores are higher when prompt adherence is low (blue in Table 1). This is because the generator becomes an unconditional model generating all types of (diverse) outputs, not following the prompt. In line with that, also note that ARC results show slightly lower prompt adherence (in MOS score) due to its improved diversity. Our second ablation shows that the relativistic loss outperforms the least-squares loss for adversarial plus contrastive post-training. We also find that our model performs best using 8 steps, which is in line with recent work that smaller accelerated models may need more steps than their larger counterparts [51]. Finally, we note that our proposed CCDS diversity metric is completely aligned with the results of the listening test, giving evidence that such metric is reasonable in automatically assessing diversity. 3.7. Edge-device optimizations We use Arms KleidiAI library that is integrated into LiteRT runtime via the XNNPACK library. We experiment with Vivo X200 pro phone with an Octa-core Arm CPU (1x Cortex-X925, 3x Cortex-X4, 4x Cortex-A720, 12GB RAM). To balance quality and deployment efficiency, we chose dynamic Int8 quantization, which (when applied selectively) typically does not require quantization-aware training. Weights are quantized ahead of time, while activations are quantized dynamically at runtime based on their statistical distribution. This allows the model to benefit from reduced memory usage and faster inference while maintaining acceptable output quality. Since the quantization happens post-training and is applied only to certain layers, it offers practical trade-off without significantly increasing development complexity. This decreases inference time from 15.3s (original F32) to 6.6s, and reducing peak runtime RAM usage from 6.5GB to 3.6GB. We can compare runtimes using high-end (H100) and consumer (3090) GPUs, which achieve speeds of 75ms and 187ms. 3.8. Creative applications Our primary goal is to accelerate text-to-audio models for practical use in creative workflows. To feel like compelling instrument, or have similar experience, text-to-audio models must be responsive and react quick. To that end, we reduced latency to below 200ms on consumer-grade GPUs. We informally experimented with this model to make music and found it inspiring for sound design, due to its speed, prompt versatility, and capacity to generate unconventional sounds. We were also captivated by its audio-to-audio capabilities for style transfer, which required no additional training. This is achieved by using any recording as the initial noisy sample xτi during ping-pong sampling (Sec. 2.5). This approach enables voice-to-audio control by initializing xτi with voice recording, as well as beat-aligned generations by initializing xτi with recording having strong beat. We provide examples on our demo page. However, key limitation of our model is its memory and storage requirements, occupying several GB of RAM and disk space (Sec. 3.7), which may pose challenges for integration into many applications and for its efficient distribution. 4. CONCLUSION ARC post-training is the first acceleration method for text-to-audio models that does not rely on distillation or CFG. By extending an adversarial relativistic loss to gaussian flow acceleration, combined with novel contrastive discriminator loss, we speed up gaussian flow models runtime to milliseconds, with consumer-grade GPUs, or to seconds, with edge-device CPUs. Such speedups are obtained without significantly compromising quality and increasing the diversity of the generations. Diversity is measured using our proposed CCDS metric, which we find to align with perceptual assessments of diversity. We hope that, with improved efficiency and diversity, text-to-audio models will soon be able to support broader range of creative applications. Recognizing the creative potential of such models, we also include small audio-to-audio experimentation, and future efforts may focus on fine-tuning with targeted datasets for more precise sound design. [29] Z. Novack, J. McAuley, T. Berg-Kirkpatrick, and N. J. Bryan, DITTO-2: Distilled diffusion inference-time t-optimization for music generation, in ISMIR, 2024. [30] N. Huang, A. Gokaslan, V. Kuleshov, and J. Tompkin, The gan is dead; long live the gan! modern baseline gan, in ICML Workshop on Structured Probabilistic Inference and Generative Modeling, 2024. [31] A. Jolicoeur-Martineau, The relativistic discriminator: key element missing from standard gan, arXiv:1807.00734, 2018. [32] K. Frans, D. Hafner, S. Levine, and P. Abbeel, One step diffusion via shortcut models, arXiv:2410.12557, 2024. [33] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation, in ICASSP, 2023. [34] M. Gutmann and A. Hyvarinen, Noise-contrastive estimation: new estimation principle for unnormalized statistical models, in AISTATS, 2010. [35] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving, Fine-tuning language models from human preferences, arXiv:1909.08593, 2019. [36] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, in NeurIPS, 2023. [37] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv:2402.03300, 2024. [38] R. A. Bradley and M. E. Terry, Rank analysis of incomplete block designs: I. the method of paired comparisons, Biometrika, vol. 39, no. 3/4, pp. 324345, 1952. [39] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of machine learning research, vol. 21, no. 140, pp. 167, 2020. [40] A. Henry, P. R. Dachapally, S. Pawar, and Y. Chen, Query-key normalization for transformers, arXiv:2010.04245, 2020. [41] Y. Wu and K. He, Group normalization, in ECCV, 2018. [42] S. Elfwing, E. Uchibe, and K. Doya, Sigmoid-weighted linear units for neural network function approximation in reinforcement learning, Neural networks, vol. 107, pp. 311, 2018. [43] A. L. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello, Look, listen, and learn more: Design choices for deep audio embeddings, in ICASSP, 2019. [44] K. Koutini, J. Schluter, H. Eghbal-Zadeh, and G. Widmer, Efficient training of audio transformers with patchout, arXiv:2110.05069, 2021. [45] R. Gandikota and D. Bau, Distilling diversity and control in diffusion models, arXiv:2503.10637, 2025. [46] J. Nistal, M. Pasini, C. Aouameur, M. Grachten, and S. Lattner, Diffa-riff: Musical accompaniment co-creation via latent diffusion models, arXiv:2406.08384, 2024. [47] C. D. Kim, B. Kim, H. Lee, and G. Kim, Audiocaps: Generating captions for audios in the wild, in NAACL, 2019. [48] M. Schoeffler, S. Bartoschek, F.-R. Stoter, M. Roess, S. Westphal, B. Edler, and J. Herre, webmushraa comprehensive framework for web-based listening tests, Journal of Open Research Software, vol. 6, no. 1, 2018. [49] E. Postolache, J. Pons, S. Pascual, and J. Serr`a, Adversarial permutation invariant training for universal sound separation, in ICASSP, 2023. [50] E. Guso, J. Pons, S. Pascual, and J. Serr`a, On loss functions and evaluation metrics for music source separation, in ICASSP, 2022. [51] Z. Geng, A. Pokle, W. Luo, J. Lin, and J. Z. Kolter, Consistency models made easy, arXiv:2406.14548, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Defossez, Simple and controllable music generation, in NeurIPS, 2023. [2] Z. Evans, C. Carr, J. Taylor, S. H. Hawley, and J. Pons, Fast timingconditioned latent audio diffusion, in ICML, 2024. [3] Z. Evans, J. Parker, C. Carr, Z. Zukowski, J. Taylor, and J. Pons, Longform music generation with latent diffusion, arXiv:2404.10301, 2024. [4] Z. Evans, J. D. Parker, C. Carr, Z. Zukowski, J. Taylor, and J. Pons, Stable audio open, arXiv:2407.14358, 2024. [5] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, in ICLR, 2020. [6] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Muller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in ICML, 2024. [7] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in NeurIPS, 2020. [8] T. Yin, M. Gharbi, R. Zhang, E. Shechtman, F. Durand, W. T. Freeman, and T. Park, One-step diffusion with distribution matching distillation, arXiv:2311.18828, 2023. [9] T. Yin, M. Gharbi, T. Park, R. Zhang, E. Shechtman, F. Durand, and W. T. Freeman, Improved distribution matching distillation for fast image synthesis, arXiv:2405.14867, 2024. [10] A. Sauer, F. Boesel, T. Dockhorn, A. Blattmann, P. Esser, and R. Rombach, Fast high-resolution image synthesis with latent adversarial diffusion distillation, arXiv:2403.12015, 2024. [11] A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, Adversarial diffusion distillation, arXiv:2311.17042, 2023. [12] Y. Ren, X. Xia, Y. Lu, J. Zhang, J. Wu, P. Xie, X. Wang, and X. Xiao, Hyper-SD: Trajectory segmented consistency model for efficient image synthesis, arXiv:2404.13686, 2024. [13] F.-Y. Wang, Z. Huang, A. W. Bergman, D. Shen, P. Gao, M. Lingelbach, K. Sun, W. Bian, G. Song, Y. Liu, H. Li, and X. Wang, Phased consistency model, arXiv:2405.18407, 2024. [14] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, Consistency models, in ICML, 2023. [15] C. Lu and Y. Song, Simplifying, stabilizing and scaling continuous-time consistency models, arXiv:2410.11081, 2024. [16] J. Chen, S. Xue, Y. Zhao, J. Yu, S. Paul, J. Chen, H. Cai, E. Xie, and S. Han, Sana-sprint: One-step diffusion with continuous-time consistency distillation, arXiv:2503.09641, 2025. [17] D. Kim, C.-H. Lai, W.-H. Liao, N. Murata, Y. Takida, T. Uesaka, Y. He, Y. Mitsufuji, and S. Ermon, Consistency trajectory models: Learning probability flow ODE trajectory of diffusion, in ICLR, 2023. [18] Z. Novack, G. Zhu, J. Casebeer, J. McAuley, T. Berg-Kirkpatrick, and N. J. Bryan, Presto! distilling steps and layers for accelerating music generation. in ICLR, 2025. [19] Y. Xu, W. Nie, and A. Vahdat, One-step diffusion models with - divergence distribution matching, arXiv:2502.15681, 2025. [20] X. Liu, C. Gong, and Q. Liu, Flow straight and fast: Learning to generate and transfer data with rectified flow, arXiv:2209.03003, 2022. [21] T. Salimans and J. Ho, Progressive distillation for fast sampling of diffusion models, arXiv:2202.00512, 2022. [22] M. Kang, R. Zhang, C. Barnes, S. Paris, S. Kwak, J. Park, E. Shechtman, J.-Y. Zhu, and T. Park, Distilling diffusion models into conditional gans, arXiv:2405.05967, 2024. [23] H. Chung, J. Kim, G. Y. Park, H. Nam, and J. C. Ye, CFG++: Manifold-constrained classifier free guidance for diffusion models, arXiv:2406.08070, 2024. [24] Y. Xu, Y. Zhao, Z. Xiao, and T. Hou, Ufogen: You forward once large scale text-to-image generation via diffusion gans, in CVPR, 2024. [25] S. Lin, X. Xia, Y. Ren, C. Yang, X. Xiao, and L. Jiang, Diffusion adversarial post-training for one-step video generation, arXiv:2501.08316, 2025. [26] Y. Bai, T. Dang, D. Tran, K. Koishida, and S. Sojoudi, Accelerating diffusion-based text-to-audio generation with consistency distillation, in Interspeech, 2024. [27] K. Saito, D. Kim, T. Shibuya, C.-H. Lai, Z.-W. Zhong, Y. Takida, and Y. Mitsufuji, Soundctm: Uniting score-based and consistency models for text-to-sound generation, arXiv:2405.18503, 2024. [28] J. Nistal, M. Pasini, and S. Lattner, Improving musical accompaniment co-creation via diffusion transformers, arXiv:2410.23005, 2024."
        }
    ],
    "affiliations": [
        "Arm",
        "Stability AI",
        "UC San Diego"
    ]
}