{
    "paper_title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
    "authors": [
        "Peng Cai",
        "Qiang Li",
        "Kaicheng Yang",
        "Dong Guo",
        "Jia Li",
        "Nan Zhou",
        "Xiang An",
        "Ninghua Yang",
        "Jiankang Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document image rectification aims to eliminate geometric deformation in photographed documents to facilitate text recognition. However, existing methods often neglect the significance of foreground elements, which provide essential geometric references and layout information for document image correction. In this paper, we introduce Foreground-Centric Network (ForCenNet) to eliminate geometric distortions in document images. Specifically, we initially propose a foreground-centric label generation method, which extracts detailed foreground elements from an undistorted image. Then we introduce a foreground-centric mask mechanism to enhance the distinction between readable and background regions. Furthermore, we design a curvature consistency loss to leverage the detailed foreground labels to help the model understand the distorted geometric distribution. Extensive experiments demonstrate that ForCenNet achieves new state-of-the-art on four real-world benchmarks, such as DocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the proposed method effectively undistorts layout elements, such as text lines and table borders. The resources for further comparison are provided at https://github.com/caipeng328/ForCenNet."
        },
        {
            "title": "Start",
            "content": "ForCenNet: Foreground-Centric Network for Document Image Rectification Peng Cai1, Qiang Li1, Kaicheng Yang2, Dong Guo1, Jia Li1, Nan Zhou1 Xiang An2, Ninghua Yang2, Jiankang Deng3* 1Qihoo Technology 2DeepGlint 3Imperial College London {caipeng1,liqiang-s}@360.cn 5 2 0 2 6 2 ] . [ 1 4 0 8 9 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Document image rectification aims to eliminate geometric deformation in photographed documents to facilitate text recognition. However, existing methods often neglect the significance of foreground elements, which provide essential geometric references and layout information for document image correction. In this paper, we introduce ForegroundCentric Network (ForCenNet) to eliminate geometric distortions in document images. Specifically, we initially propose foreground-centric label generation method, which extracts detailed foreground elements from an undistorted image. Then we introduce foreground-centric mask mechanism to enhance the distinction between readable and background regions. Furthermore, we design curvature consistency loss to leverage the detailed foreground labels to help the model understand the distorted geometric distribution. Extensive experiments demonstrate that ForCenNet achieves new state-of-the-art on four real-world benchmarks, such as DocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the proposed method effectively undistorts layout elements, such as text lines and table borders. The resources for further comparison are provided at https://github.com/caipeng328/ ForCenNet. 1. Introduction With the widespread adoption of mobile devices and advancements in camera technology, document digitization has become essential. Smartphones and portable cameras, offering greater flexibility than traditional scanners, often capture images that suffer from perspective distortion and geometric deformation due to variable shooting angles and document conditions. These distortions negatively impact visual quality and hinder downstream tasks such as OCR and document structure analysis. Therefore, accurate rectification of documents captured by camera sensors remains *Corresponding Author Figure 1. Visualization of Deformation Degree. We compute the deformation displacement for each pixel and visualize it using heatmap. Blue represents minor deformation, while red indicates significant displacement. significant challenge in the field of document analysis and recognition."
        },
        {
            "title": "With advances",
            "content": "in deep learning, data-driven approaches [7, 9, 43] have been developed for geometric correction of distorted document images. Unlike traditional computer vision tasks such as image classification [16] and object detection [14], acquiring fine-grained annotations for document dewarping poses significant challenges. The limited availability of training data restricts the models generalization capabilities. Certain methods [27, 30, 49] introduce random perturbations to distorted images and assess the consistency between the corrected images and synthetic deformation patterns. While these weakly-supervised methods achieve satisfactory results in terms of image similarity when compared to contemporary studies, they often fail to preserve document readability. This observation highlights the need to reassess the data scale and motivates the exploration of more diverse synthetic deformation fields to better approximate the true distribution. The accuracy of information extraction and content readability are fundamental objectives in document dewarping. As shown in Fig. 1, readable regions in the document, such as text and table lines, occupy only small fraction of the image pixels, while major distortions are primarily concentrated in the background. Current methods [2, 9, 43, 54, 55] uniformly predict warping flows across the entire image, leading to misalignment between the tasks primary objectives and the models optimization goals. This misalignment complicates the optimization process. Consequently, the precise definition of foreground regions in document dewarping has emerged as critical research focus. Recent approaches, such as DocGeoNet [11] and FTDR [22], emphasize text lines as foreground elements to guide model predictions. DocGeoNet [11] incorporates mask annotations for the Doc3D [7] dataset, while FTDR [22] utilizes frozen detection model to extract coarse text line information from distorted images. However, these methods exhibit two notable limitations: (1) conventional detection models [36, 56] struggle to accurately identify distorted text lines and linear structures in geometrically deformed document images, and (2) readable information in documents encompasses not only text lines but also additional foreground elements, such as table lines and graphical content. To address the aforementioned limitations, this paper introduces the Foreground-Centric Network (ForCenNet) for eliminating geometric distortions in document images. Specifically, we propose foreground-centric label generation method to extract detailed foreground elements, including text, straight lines, and drawings, from undistorted images. These foreground elements are represented using both masks and discrete points. The undistorted image, along with its corresponding mask and point annotations, undergoes forward mapping process, while the backward mapping serves as the prediction target for the model. Given that foreground regions encapsulate the most significant distortion magnitudes in dewarped images, we introduce foreground-centric mask mechanism to enhance the differentiation between readable and background regions. Additionally, curvature consistency loss is designed to exploit detailed foreground labels, allowing the model to better capture the geometric structure of distortions. Extensive experiments demonstrate that ForCenNet achieves state-of-theart performance on four real-world benchmarks: DocUNet, DIR300, WarpDoc, and DocReal. The main contributions of this paper are summarized as follows: We propose foreground-centric label generation method to address data scarcity in document rectification. This module generates and utilizes intrinsic foreground information, enabling efficient training with only distortionfree reference images. We introduce foreground-centric mask mechanism to improve the distinction between readable content and background regions, effectively capturing primary distortion patterns in dewarped images. We design curvature consistency loss to leverage detailed foreground labels, enhancing the models ability to capture the geometric distribution of distortions. We conduct extensive experiments and demonstrate that the proposed ForCenNet establishes new state-of-theart across four real-world benchmarks. 2. Related Work 2.1. Deformation Field Supervised for Rectification The advancement of deep learning techniques [1, 13, 42, 45, 50] has established deep networks [39] as an effective alternative to traditional methods to predict pixellevel deformation fields. Compared to conventional approaches [3, 4], deep learning-based document image dewarping methods [9, 10, 15, 24, 29, 43, 54] achieve superior feature representation. DocUNet [29] utilizes stacked U-Net [36] to estimate pixel-wise displacement fields for document distortion correction. PWUNet [8] introduces an end-to-end trainable piecewise dewarping framework that integrates local deformation prediction with global structural constraints. DocTr [9] employs transformer-based architectures [42] to enhance representation learning and correction accuracy. Marior [54] addresses large-margin document distortions by incorporating edge-removal modules and content-aware loss functions. CGU-Net [43] predicts 3D and 2D distorted grids using fully convolutional network, effectively modeling the transformation from 3D to 2D. To alleviate data scarcity in document dewarping, recent methods [27, 30, 49] adopt weak supervision strategies. PaperEdge [30] fine-tunes the contour network ENet using the weakly supervised mask-labeled dataset DIW [30], enabling self-supervised learning through its texture prediction network. FDRNet [49] leverages WarpDoc [49] to generate dual distortion modes and predicts control points with shared parameter weights for mutual correction. DRNet [27] introduces novel supervision approach using undistorted images as direct supervisory signals. In contrast to these methods, our approach facilitates rapid model iteration using only undistorted images, thereby supporting the development of an efficient and cost-effective training framework. 2.2. Foreground Constraints Based Rectification Several studies [11, 18, 20, 22, 23, 31, 41, 47, 48] focus on utilizing inherent foreground constraints in document images to optimize geometric rectification. Early works [31, 41, 47] employ polynomial approximation to model curved text lines and correct document images captured by single cameras, relying on priori layout information. Similarly, [20] utilizes cubic B-splines [35] and an active contour network, to improve the accuracy of straightening curved text lines. Recent methods, such as DDCP [48], predict fixed set of foreground control points and compute backward mapping based on their relationship to reference points. DocGeoNet [11] enhances correction performance by integrating segmentation loss to guide CNNbased text line extractor. RDGR [18] identifies text lines and boundaries, followed by grid regularization for backward mapping, ensuring structural preservation during dewarping. FTDR [22] employs global-local fusion and crossattention mechanisms to emphasize foreground and text line regions, improving readability and correction quality. LADocFlatten [23] introduces transformer-based segmentation module to capture foreground layout, combined with regression and merging modules for UV mapping prediction. In this work, we explicitly define foreground elements, including text, lines, and vector graphics, and design curvature consistency loss and mask-guided mechanisms to enhance the models geometric understanding of foreground elements. 3. Methods In this section, we propose ForCenNet, novel framework designed for efficient document rectification using only undistorted images. As illustrated in Fig. 2, foreground elements are first extracted from undistorted images (Section 3.1), then enhance the models focus on the foreground through mask-guided module (Section 3.2), and finally design curvature consistency loss (Sec. 3.3) to improve the models geometric perception of small elements, such as table lines. 3.1. Foreground-Centric Label Generation We introduce foreground-centric label generation method to alleviate data scarcity in document correction tasks. To address inaccuracies caused by image deformation and varying lighting conditions, our approach extracts precise foreground elements from undistorted images and applies randomly generated distortion field to generate accurate training samples. As shown in Fig. 2a, the proposed method operates in three stages: foreground-background segmentation, line element extraction, and distortion field generation. Character-Level Foreground-Background Segmentation. To obtain character-level foreground-background segmentation, we fine-tune Hi-SAM [51] for segmenting readable regions in document images. As shown in Fig. 2a, given an undistorted input image Iu, it generates the corresponding foreground mask Mu. The fine-tuned Hi-SAM unifies the segmentation of text regions, line elements, and graphics, which will serve as subsequent supervision and guidance signals. Extraction of Line Elements. We employ OCR engines [33] to extract text lines, utilizing the midlines of Algorithm 1 Line Segment Filtering Require: Undistorted image I, slope threshold ϵs, intercept threshold δ, slope ranges (0, α) for horizontal lines and (β, ) for vertical lines. 1: edges Canny(I) 2: line segments LSD(edges) 3: for line in line segments do (x0, y0), (x1, y1) line 4: (cid:12) y1y0 (cid:12) slope (cid:12) x1x0 if slope < α or slope > β then (cid:12) (cid:12) (cid:12) 5: 6: keep(line) 7: 8: for line1, line2 in line segments do 9: 10: (m1, c1) calculate slope and intercept(line1) (m2, c2) calculate slope and intercept(line2) if m1 m2 < ϵs and c1 c2 < δ then 11: remove(line2) 12: 13: Return line segments text bounding boxes as the text line representations. For document line elements such as table lines, we introduce document-specific detection method based on the Line Segment Detector (LSD) [44], which eliminates non-horizontal and non-vertical lines and suppresses duplicate detections. The implementation details are provided in Algorithm 1. Distortion Field Generation. We obtain the native backward mapping BM from DOC3D [7] to compute the corresponding forward mapping FM. To enrich the deformation field, we slightly crop and apply random pairwise overlapping to the BM. Subsequently, the FM is superimposed onto the undistorted image, foreground mask, and line elements to generate the distorted image Id, foreground mask Md, and line elements Ld, with BM serving as the training target. 3.2. Foreground-Centric Network Architecture Existing methods [43, 55] often treat text, line elements, and background uniformly, overlooking the significance of foreground regions. To address this, we employ distortion masks to guide the model in focusing on foreground information, prioritizing text readability and structural alignment. As shown in Fig. 2b, our proposed ForCenNet comprises four key components: Feature Extraction Module, Efficient Transformer Encoder, Foreground Segmentation Module, and Mask-Guided Transformer Decoder. Feature Extraction Module. Given distorted document image Id RHW C, we resize it to = = 288 and = 3. The resized image is passed through convolutional layers with large kernels and multiple residual layers, yielding the features Fu Efficient Transformer Encoder. To capture global dependencies, we utilize vanilla Transformer [42] with three 8 256. 8 (a) Label Generation (b) The Architecture of ForCenNet (c) Curvature Consistency Loss Figure 2. The overview architecture of the proposed ForCenNet. is the predicted foreground mask. is the curvature value calculated from line elements, with ˆki as the predicted value and ki as the ground truth. ˆBM is the predicted backward mapping field. layers. We adopt overlapping patch embeddings [40] and employ kernel size of 3 and stride of 2 to preserve feature information at text boundaries while reducing computational overhead. The Transformer encoder produces three feature maps: {E1, E2, E3}. To reduce the complexity of the attention mechanism, we implement Spatial Pooling Window (SPW) strategy [17] on the key and value features. Foreground Segmentation Model. Given the feature sequence E, the foreground segmentation model predicts binary mask using lightweight network. First, the feature channel dimensions are unified via 11 convolution, followed by upsampling to spatial resolution of . The merged features are then processed through multiple 11 convolutions to produce the foreground segmentation result RHW Cseg , where Cseg = 2 represents the foreground and background classes. The training process is supervised using an L1 loss with the ground truth provided by Md: Lseg = Md1. (1) To incorporate the foreground segmentation result into the decoder, we apply softmax operation with smoothing coefficient to , yielding pixel-wise class probabilities. The expected value of these probabilities is then computed to generate the predicted foreground mask : (cid:88) sof tmax(γ )i, (2) = i{0,1} where γ denotes the smoothing coefficient. The softmax operation normalizes , ensuring that its values sum to 1, which allows the expected value to be interpreted as probability density. Mask-Guided Transformer Decoder. We use the foreground mask to guide feature extraction in the Transformer decoder. The decoder takes the feature sequence as inputs, {E1, E2, E3} and the foreground mask utilizing learnable embeddings Qlearn to capture distortion information, and outputs the distorted deformation field. The encoder consists of three Transformer layers, with upsampling to connect them. Each transformer layer includes mask-guided self-attention and encoderdecoder cross-attention mechanisms. For mask-guided selfattention, we use decoder embeddings DI as input, incorporating the foreground mask to focus attention on foreground regions. In encoder-decoder cross-attention, the decoder embeddings serve as queries, and the encoder embeddings from the same layer act as keys and values. MSA(Q, K, ) = Softmax( QK + σSeq( )Seq( )T dhead )V, CA(Q, K, ) = CrossAttention(Di1, Ei, Ei), (3) where MSA denotes mask-guided self-attention, with σ as the scaling factor. Seq. refers to sequential expansion along the feature dimension. CA represents encoderdecoder cross-attention, where Di1 and Ei are the outputs of the (i 1)-th decoder layer and i-th encoder layer, respectively. To reduce attention complexity, spatial reduction [17] is applied to the key and value in the decoder. Finally, we use the upsampling method from DocTr [9] and DocGeoNet [11] to obtain high-resolution backward deformation field BM, which initializes grid coordinates and refines them through weighted optimization with learnable parameters. 3.3. Foreground-Centric Optimization Objectives The ForCenNet has three objectives: foreground mask loss Lseg, backward map regression Lmap, and curvature consistency loss Lk. Lseg supervises the generation of the foreground mask, which is defined as Eq. 1. Backward map regression loss is calculated by using the L1 distance between the predicted ˆBM and the ground truth BM, as defined by the formula: Lmap = ˆBM BM1. Curvature Consistency Loss. Compared to foreground elFigure 3. Qualitative Comparison with Prior Methods on DocUNet and DIR300 Benchmarks. Red arrows highlight the differences. Additional visualizations are available in the appendix. ements like text and images, table lines occupy fewer pixels, making it challenging for the network to capture line distortion. This imbalance weakens the supervisory effect of the L1 loss. Furthermore, although L1 loss governs pixel-level distortion offsets, it does not adequately capture the geometric structure of linear elements. To resolve these issues, we introduce curvature consistency loss based on control lines, utilizing line elements Lu from undistorted images. As illustrated Fig. 2a, every 4 pixels along each extracted line element Lu are sampled to generate point set = {pi pi Lu, = 1, 2, . . . , }. This set is projected ˆBM and the ground onto the predicted deformation field truth deformation field BM to obtain the predicted control points Cp and the ground truth control points Cpgt. To minimize errors and ensure differentiability, bilinear interpolation [19] is applied for point mapping and projection, formulated as: where w(p) is the bilinear interpolation weight determined by the relative position of pi on the grid. (pi) denotes the set of four neighboring points surrounding pi. For the mapped point set Cp = {(xi, yi) = 1, 2, . . . , }, the derivatives are calculated using the central difference method [5], while the forward and backward differences are applied at the boundary points. The curvature of the control line is given by: κi = y + y2 (x2 )3/2 + ε , (5) , i, i, and where represent the first and second discrete derivatives of the coordinates. To prevent gradient explosion or overflow, small positive value ε = 0.0001 is added. Finally, constraint is imposed to ensure that the curvature variation at local points aligns with the true curvature trend. The loss function Lk is defined as: Cp = { (cid:88) wp ˆBM (p) pi }, Cpgt = { pN (pi) (cid:88) pN (pi) wp BM (p) pi }, Lk = 1 1 1 (cid:88) ( ˆki ki), (6) (4) where ˆki and ki represent the curvature values at discrete points in the predicted and ground truth deformation Type Model MS-SSIM LD AD ED CER Type Model MS-SSIM LD AD ED CER WS. Other. FU. PaperEdge [30] FDRNet [49] DRNet [27] DewarpNet [7] PWUNet [8] DocTr [9] Marior [54] CGU-Net [43] DDCP [48] DocGeoNet [11] RDGR [18] FTDR [22] LA-DocFlatten [23] ForCenNet-DOC3D ForCenNet 0.470 0.542 0.510 0.473 0.491 0.510 0.478 0.557 0.472 0.504 0.495 0.497 0.526 0.579 0.582 8.49 8.21 7.42 8.39 8.64 7.75 7.43 6. 8.97 7.71 8.51 8.43 6.72 4.91 4.82 0.39 0.42 0.39 0.40 0.31 0.45 0.38 0.46 0.37 0.30 0.19 0.19 825.48 829.78 644.48 885.90 1069.28 724.84 823.80 513. 1411.38 713.94 729.52 697.52 592.37 571.40 0.211 0.206 0.164 0.237 0.267 0.183 0.205 0.178 0.357 0.182 0.171 0.170 0.158 0.136 Table 1. Result comparisons between our proposed with existing methods on the DocUNet Benchmark [29]. WS. refers to weakly supervised methods. FU. refers to methods that leverage foreground elements. Bolded values indicate the best, and underlined values indicate the second best. fields. 4. Experiments 4.1. Implementation Details We implement ForCenNet using the PyTorch framework [34]. We train our model on two distinct undistorted datasets. The first, referred to as ForCenNet, comprises 365 images from DocUNet [29] and DIR300 [11]. The second variant, ForCenNet-DOC3D, is trained on undistorted images from the DOC3D dataset [7]. The initial backward mapping field is generated from 100,000 samples of the Doc3D dataset [7]. To simulate real-world conditions, we overlay distorted document images onto random COCO backgrounds [25]. The training images are resized to 288 288 pixels. Optimization is performed using the AdamW optimizer [28] with batch size of 32, and the OneCycle learning rate scheduler [37] is employed, setting the maximum learning rate at 104. The warm-up phase [12] constitutes 10% of the total training cycles. We train for 30 epochs on two NVIDIA A100 GPUs until convergence. 4.2. Evaluation Metrics MS-SSIM, LD, and AD. MS-SSIM [46] is an image quality assessment method that measures structural similarity. It constructs Gaussian pyramid to compute weighted sum of SSIM across multiple scales, thereby mitigating the influence of sampling density. LD [52] quantifies distortion by evaluating the average local deformation at each pixel. This metric leverages SIFT Flow [26] to align pixel positions and computes the mean L2 distance between corresponding pixels. AD [30] serves as robust metric for document dewarping by aligning an undistorted image with reference image WS. PaperEdge [30] other. FU. DewarpNet [7] DocTr [9] MetaDoc [6] DocRes [55] CGU-Net [43] DDCP [48] DocGeoNet [11] FTDR [22] LA-DocFlatten [23] ForCenNet-DOC3D ForCenNet 0. 0.492 0.616 0.638 0.626 0.621 0.552 0.638 0.607 0.651 0.709 0.713 8.00 0.255 704.34 13.94 7.21 5.75 6.83 7. 10.97 6.40 7.68 5.70 4.73 4.65 0.331 0.254 0.178 0.241 0.217 0.357 0.242 0.244 0.195 0.136 0.123 1059.57 699.63 774.80 735.95 2130.01 664.96 652.80 449.12 390.61 0. 0.355 0.223 0.241 0.283 0.552 0.218 0.211 0.153 0.138 Table 2. Result comparisons between our proposed with existing methods on the DIR300 Benchmark [11]. through translation and scaling, followed by distortion error computation. ED and CER. The Edit Distance (ED) [21] measures the minimum number of operationsdeletion, insertion, and substitutionrequired to transform one string into another. The Character Error Rate (CER) [32] is derived from ED and is defined as CER = (d + + r)/N , where denotes the total number of characters in the reference string. 4.3. Baseline Comparison Evaluation on the DocUNet. As presented in Tab. 1, we categorize methods into three groups: weakly supervised, foreground-based, and foreground-independent. Our distortion correction method surpasses existing approaches across almost all evaluated metrics. Notably, ForCenNet reduces the LD metric to 4.823. Compared to weakly supervised methods such as FDRNet [49] and PaperEdge [30], our method demonstrates superior performance in distortion metrics (MS-SSIM, LD, and AD). For fair comparison, we conduct an additional experiment on undistorted DOC3D dataset. ForCenNet-DOC3D lowers the ED metric below 600 for the first time, surpassing foregroundsupervised methods such as FTDR [22], DocGeoNet [11], and RDGR [18]. These findings underscore the significance of leveraging foreground features for improved distortion correction. Evaluation on the DIR300. As shown in Tab. 2, our method outperforms state-of-the-art approaches, such as FTDR [22], on the DIR300 dataset. ForCenNet achieves an MS-SSIM score of 0.713, the highest reported to date, and reduces the LD metric to 4.653. In the OCR evaluation, following the protocols of DocGeoNet [11] and FTDR [22], we assess 90 images and reduce the ED metric to below 400, surpassing the previous leader, FTDR [22]. These results underscore the effectiveness of our method in enhancing both distortion removal and OCR performance. We visualize the dewarping results on the DocUNet Benchmark [29] and DIR300 dataset [11], comparing ForCenNet with existing methods  (Fig. 3)  . The initial examples demonstrate our methods ability to correct text regions with complex distor-"
        },
        {
            "title": "Model",
            "content": "Pub. DocTr [9] DocGeonet [11] FDRNet [49] CGU-Net [43] DocReal [53] DocRes [55]"
        },
        {
            "title": "ForCenNet",
            "content": "ACM MM 21 ECCV 22 CVPR 22 SIGGRAPH 23 WACV 23 CVPR 24 MS-SSIM WarpDoc LD AD ED MS-SSIM 0.39 0.40 0.46 0.35 0.5 0. 27.01 24.71 19.11 26.28 12.86 8.10 0.77 0.75 0.63 0.45 0.18 1796.11 1871.51 1760.84 1425.40 899. 0.550 0.553 0.549 0.555 0.550 0.595 DocReal LD AD 12.60 12.23 11.33 9.82 11.52 6.95 0.32 0.31 0.27 0.23 0. 0.17 ED 785.05 784.47 753.35 736.69 769.51 753.12 Table 3. Generalization Comparison. Columns 3-6 show the comparison results on WarpDoc [49], while columns 7-10 present the results on DocReal [53]. Sample Size MS-SSIM LD AD CER ID MGD CL Sample Size MS-SSIM LD CER 1 100 500 1000 2000 5000 0.449 0.530 0.566 0.571 0.567 0.569 10.745 5.348 4.892 4.950 4.965 4. 0.382 0.231 0.201 0.197 0.203 0.209 0.291 0.208 0.149 0.141 0.147 0.151 Table 4. ForCenNet Performance on Different Dataset Sizes. 1000 indicates the application of 1,000 randomly generated deformation fields to the same image. tions, while the subsequent examples showcase the success of our foreground-centric approach in reducing distortions in table lines and intricate graphics. Cross-domain Robustness. Document correction models typically suffer from performance degradation in unseen environments with varying lighting conditions, viewpoints, and textures. Ensuring cross-domain robustness is crucial for real-world applications. To assess this property, we evaluate our method on two additional datasets, WarpDoc [49] and DocReal [53], without incorporating undistorted reference images during continued training. The results in Tab. 3 indicate that our method achieves competitive performance on previously unseen document images. 4.4. Ablation Study We conduct ablation studies to evaluate the effectiveness of the proposed approach, focusing on label generation and core model architecture. All the ablation experiments are performed on the DocUNet [29] dataset. Dataset Scaling. We conduct an ablation study on dataset scaling using randomly generated sample sets. total of 65 undistorted images from DocUNet [29] are used to construct experimental groups of varying sizes. Multiple distortion fields are applied to each image through label preprocessing. All groups are trained with identical epochs and hyperparameters. The results, summarized in Tab. 4, show that increasing dataset size improves model performance, highlighting the efficiency of the label generation module in C 1000 1000 1000 1000 0.558 0.565 0.571 0.530 5.44 5.10 4.95 7. 0.173 0.169 0.141 0.198 Table 5. Ablation Study. MGD denotes the Mask-Guided Transformer Decoder, and CL represents the Curvature Consistency Loss. Figure 4. Visualization of Foreground Segmentation. From left to right: the distorted input image, segmentation results from the frozen model and the differentiable model, followed by the corresponding dewarped outputs generated by each model. iterative training with domain-specific undistorted images. However, performance gains plateau beyond certain scale, indicating scalability limitations. Structure Modifications. We subsequently examine various components of the model, with the results summarized in Tab. 5. The comparison between experiments and reveals that incorporating the mask-guided module improves MS-SSIM from 0.558 to 0.571, while Local Distortion (LD) decreases from 5.44 to 4.95. Similarly, experiments versus show that removing curvature loss slightly reduces MS-SSIM from 0.571 to 0.565 and increases the Character Error Rate (CER) from 0.141 to 0.169. These findings indicate that generating sufficient distorted samples and effectively utilizing foreground information significantly enhance document image correction. By refining foreground definitions and emphasizing readable regions, dewarping Figure 5. Visualization of Foreground Results on WarpDoc [49] and DocReal [53] benchmarks. Columns 1-3: detection results of line elements. Columns 4-6: detection results of text elements. Highlighted colors indicate detected regions, while red arrows mark differences. Figure 6. Quantitative evaluation of straight-line rectification. The first image displays results on the DocReal [53] dataset, while the second image presents results on the WarpDoc [49] dataset. performance is further improved. Ablation on Different Segmentation Models. To assess the necessity of differentiable foreground segmentation model, we replace it with separately trained model and freeze its weights during dewarping training. Compared to the differentiable model, this modification leads to rapid decline in MS-SSIM to 0.468 and an increase in Character Error Rate (CER) to 0.212. Fig. 4 presents two examples from DocUNet, illustrating that rare characters and complex distortions severely impact foreground segmentation performance, consequently degrading the final dewarping results. 4.5. Experimental Analysis Visualization of Foreground Results. We visualize foreground detection in distorted images. Fig. 5 compares the performance of DocRes [55] and our method using the Tesseract OCR engine (v5.0.1) [38] and the line detection results produced by Algorithm 1. The dewarping process significantly improves the recall of text and line elements. To quantify the correction capability of our model for straight line elements, we detect straight lines in the corrected images and calculate the total line length for each Figure 7. Visualization of intermediate layer results. Left to right: distorted image, foreground mask, foreground attention map, and rectification results of our model. sample. We compare our method with DocRes on the WarpDoc [49] and DocReal [53] datasets, as shown in Fig. 6. Our method outperforms DocRes on 65% of DocReal samples and 69% of WarpDoc samples. For challenging cases, unclear boundaries between foreground and background slightly reduce segmentation accuracy. Overall, guided by foreground masks and curvature supervision, our model significantly enhances geometric perception and image readability. Intermediate Results Visualization. To improve model interpretability, we visualize the foreground segmentation results and the attention maps from the decoders final layer. Fig. 7 demonstrates that the differentiable mask prediction module effectively handles complex distortions, such as folds, creases, and shadows. Concurrently, the heatmap distribution in the attention maps demonstrates the models ability to identify distorted regions. Overall, our model successfully captures the foreground mask, guiding subsequent modules to focus on the distorted areas within the foreground, thereby facilitating the reconstruction of an undistorted document image. 5. Conclusion This paper explores the use of naturally occurring foreground elements in documents for rectification. We propose ForCenNet, framework that enables efficient model Our training using only undistorted images. method extracts foreground labels directly from undistorted images, leveraging mask-guided module to enhance the models focus on foreground information. Furthermore, we introduce curvature consistency loss to improve the models geometric understanding of fine structures such as lines and text. Extensive experiments on four public benchmark datasets demonstrate In future work, as the effectiveness of our approach. ForCenNet can generate mask predictions, we will investigate its integration with image scanning and enhancement."
        },
        {
            "title": "References",
            "content": "[1] Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, and Jiankang Deng. Multi-label cluster discrimination for visual representation learning. In ECCV, pages 428444. Springer, 2024. 2 [2] Hmrishav Bandyopadhyay, Tanmoy Dasgupta, Nibaran Das, and Mita Nasipuri. gated and bifurcated stacked u-net module for document image dewarping. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 1054810554. IEEE, 2021. 2 [3] M.S. Brown and W.B. Seales. Document restoration using 3d shape: general deskewing algorithm for arbitrarily warped documents. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, 2002. 2 [4] Huaigu Cao, Xiaoqing Ding, and Changsong Liu. Rectifying the bound document image captured by the camera: model based approach. In Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings., page 7175, 2004. 2 [5] Steven Chapra, Raymond Canale, et al. Numerical methods for engineers. Mcgraw-hill New York, 2011. [6] Beiya Dai, Qunyi Xie, Yulin Li, Xiameng Qin, Chengquan Zhang, Kun Yao, Junyu Han, et al. Matadoc: margin and text aware document dewarping for arbitrary boundary. arXiv preprint arXiv:2307.12571, 2023. 6 [7] Sagnik Das, Ke Ma, Zhixin Shu, Dimitris Samaras, and Roy Shilkrot. Dewarpnet: Single-image document unwarping with stacked 3d and 2d regression networks. In ICCV, pages 131140, 2019. 1, 2, 3, 6 [8] Sagnik Das, Kunwar Yashraj Singh, Jon Wu, Erhan Bas, Vijay Mahadevan, Rahul Bhotika, and Dimitris Samaras. Endto-end piece-wise unwarping of document images. In ICCV, pages 42684277, 2021. 2, 6 [9] Hao Feng, Yuechen Wang, Wengang Zhou, Jiajun Deng, and Houqiang Li. Doctr: Document image transformer for geometric unwarping and illumination correction. arXiv preprint arXiv:2110.12942, 2021. 1, 2, 4, 6, 7 [10] Hao Feng, Wengang Zhou, Jiajun Deng, Qi Tian, and image arXiv preprint Houqiang Li. rectification with progressive learning. arXiv:2110.14968, 2021. Docscanner: Robust document [11] Hao Feng, Wengang Zhou, Jiajun Deng, Yuechen Wang, and Houqiang Li. Geometric representation learning for In ECCV, pages 475492. document image rectification. Springer, 2022. 2, 3, 4, 6, 7 [12] Priya Goyal, Piotr Dollar, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 6 [13] Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432, 2025. 2 [14] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 1 [15] Felix Hertlein and Alexander Naumann. Template-guided illumination correction for document images with imperfect geometric reconstruction. In ICCV, pages 904913, 2023. 2 [16] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 47004708, 2017. [17] Di Jia, Peng Cai, Qian Wang, and Ninghua Yang. transformer-based architecture for high-resolution stereo IEEE Transactions on Computational Imaging, matching. 2024. 4 [18] Xiangwei Jiang, Rujiao Long, Nan Xue, Zhibo Yang, Cong Yao, and Gui-Song Xia. Revisiting document image dewarping by grid regularization. In CVPR, pages 45434552, 2022. 2, 3, 6 [19] Earl Kirkland and Earl Kirkland. Bilinear interpolation. Advanced computing in electron microscopy, pages 261263, 2010. 5 [20] Olivier Lavialle, Molines, Franck Angella, and Pierre Baylou. Active contours network to straighten distorted text In Proceedings 2001 International Conference on lines. Image Processing (Cat. No. 01CH37205), pages 748751. IEEE, 2001. 2 [21] VI Lcvenshtcin. Binary coors capable or correcting deleIn Soviet physics-doklady, tions, insertions, and reversals. 1966. [22] Heng Li, Xiangping Wu, Qingcai Chen, and Qianjin Xiang. Foreground and text-lines aware document image rectification. In ICCV, pages 1957419583, 2023. 2, 3, 6 [23] Pu Li, Weize Quan, Jianwei Guo, and Dong-Ming Yan. ACM Layout-aware single-image document flattening. Transactions on Graphics, 43(1):117, 2023. 2, 3, 6 [24] Xiaoyu Li, Bo Zhang, Jing Liao, and Pedro V. Sander. Document rectification and illumination correction using patchbased cnn. ACM Transactions on Graphics, page 111, 2019. 2 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 6 [26] Ce Liu, Jenny Yuen, and Antonio Torralba. Sift flow: Dense IEEE correspondence across scenes and its applications. transactions on pattern analysis and machine intelligence, 33(5):978994, 2010. 6 [27] Shaokai Liu, Hao Feng, and Wengang Zhou. Rethinking supervision in document unwarping: self-consistent flowfree approach. IEEE Transactions on Circuits and Systems for Video Technology, 2023. 1, 2, [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. Cornell University - arXiv,Learning, 2017. 6 [29] Ke Ma, Zhixin Shu, Xue Bai, Jue Wang, and Dimitris Samaras. Docunet: Document image unwarping via stacked unet. In CVPR, pages 47004709, 2018. 2, 6, 7 [30] Ke Ma, Sagnik Das, Zhixin Shu, and Dimitris Samaras. Learning from documents in the wild to improve document unwarping. In SIGGRAPH, pages 19, 2022. 1, 2, 6 [31] Gaofeng Meng, Chunhong Pan, Shiming Xiang, Jiangyong Duan, and Nanning Zheng. Metric rectification of curved IEEE transactions on pattern analysis document images. and machine intelligence, 34(4):707722, 2011. 2 [32] Andrew Cameron Morris, Viktoria Maier, and Phil Green. From wer and ril to mer and wil: improved evaluation measures for connected speech recognition. In Interspeech 2004, 2021. 6 [33] PaddlePaddle. Paddleocr: practical and easy-to-use ocr tool for multilingual scenarios. https://github.com/ PaddlePaddle/PaddleOCR, 2020. 3 [34] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 6 [35] Hartmut Prautzsch, Wolfgang Boehm, and Marco Paluszny. Bezier and B-spline techniques. Springer, 2002. 2 [36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 2 [37] Leslie N. Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning In Artificial Intelligence and Machine Learning for rates. Multi-Domain Operations Applications, 2019. 6 [38] Ray Smith. An overview of the tesseract ocr engine. In Ninth international conference on document analysis and recognition (ICDAR 2007), pages 629633. IEEE, 2007. 8 [39] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow (extended abstract). In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021. 2 [40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through atIn International conference on machine learning, tention. pages 1034710357. PMLR, 2021. [41] Adrian Ulges, Christoph Lampert, and Thomas Breuel. Document image dewarping using robust estimation of curled text lines. In Eighth International Conference on Document Analysis and Recognition (ICDAR05), pages 1001 1005. IEEE, 2005. 2 [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanN. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems,Neural Information Processing Systems, 2017. 2, 3 [43] Floor Verhoeven, Tanguy Magne, and Olga SorkineHornung. Uvdoc: Neural grid-based document unwarping. In SIGGRAPH, pages 111, 2023. 1, 2, 3, 6, 7 [44] Rafael Grompone Von Gioi, Jeremie Jakubowicz, JeanMichel Morel, and Gregory Randall. Lsd: fast line segIEEE transment detector with false detection control. actions on pattern analysis and machine intelligence, 32(4): 722732, 2008. 3 [45] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: versatile backbone for dense In Proceedings of the prediction without convolutions. IEEE/CVF international conference on computer vision, pages 568578, 2021. [46] Z. Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: From error visibility to structural IEEE Transactions on Image Processing, page similarity. 600612, 2004. 6 [47] Changhua Wu and Gady Agam. Document image deIn Structural, Synwarping for text/graphics recognition. tactic, and Statistical Pattern Recognition: Joint IAPR International Workshops SSPR 2002 and SPR 2002 Windsor, Ontario, Canada, August 69, 2002 Proceedings, pages 348 357. Springer, 2002. 2 [48] Guo-Wang Xie, Fei Yin, Xu-Yao Zhang, and Cheng-Lin Liu. Document dewarping with control points. In ICDAR, pages 466480. Springer, 2021. 2, 3, 6 [49] Chuhui Xue, Zichen Tian, Fangneng Zhan, Shijian Lu, and Song Bai. Fourier document restoration for robust document In CVPR, pages 45734582, dewarping and recognition. 2022. 1, 2, 6, 7, 8 [50] Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. Clip-cid: Efficient clip distillation via cluster-instance discrimination. In AAAI, pages 2197421982, 2025. 2 [51] Maoyuan Ye, Jing Zhang, Juhua Liu, Chenyu Liu, Baocai Yin, Cong Liu, Bo Du, and Dacheng Tao. Hi-sam: Marrying segment anything model for hierarchical text segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(03):14311447, 2025. 3 [52] Shaodi You, Yasuyuki Matsushita, Sudipta Sinha, Yusuke Bou, and Katsushi Ikeuchi. Multiview rectification of folded documents. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 505511, 2018. [53] Fangchen Yu, Yina Xie, Lei Wu, Yafei Wen, Guozhi Wang, Shuai Ren, Xiaoxin Chen, Jianfeng Mao, and Wenye Li. Docreal: Robust document dewarping of real-life images via attention-enhanced control point prediction. In WACV, pages 665674, 2024. 7, 8 [54] Jiaxin Zhang, Canjie Luo, Lianwen Jin, Fengjun Guo, and Kai Ding. Marior: Margin removal and iterative content rectification for document dewarping in the wild. arXiv preprint arXiv:2207.11515, 2022. 2, 6 [55] Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, and Lianwen Jin. Docres: generalist model toward unifying document image restoration tasks. In CVPR, pages 15654 15664, 2024. 2, 3, 6, 7, 8 [56] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In 2019 International conference on document analysis and recognition (ICDAR), pages 10151022. IEEE, 2019. 2 ForCenNet: Foreground-Centric Network for Document Image Rectification"
        },
        {
            "title": "Supplementary Material",
            "content": "compute the correspondences of the deformation field before and after cropping. Fig. 8 shows the cropped distorted images along with their corresponding rectified document images. Additionally, we process the foreground lines and masks required by ForCenNet and highlight them in yellow. 8. Downstream tasks We conduct an exploratory investigation into significant downstream task, namely appearance enhancement, also known as illumination correction. This task aims to restore pristine appearance similar to that produced by scanner or digital PDF file, without being limited to specific degradation types. In our study, we leverage the model-predicted foreground mask to assign non-foreground regions to white, while preserving the original colors of the foreground regions, simulating document enhancement effects. The selected visualization results from the DocUNet dataset are shown in Fig. 9. Furthermore, we quantitatively evaluate the enhanced images against the ground-truth enhanced images from the DocUNet dataset, achieving an MS-SSIM score of 0.6712. These results highlight the potential of the ForCenNet architecture in synthetic enhancement tasks. 6. Experimental Details In the Tab.6, we present the detailed experimental setup and model hyperparameters. To investigate the impact of sampling density on geometric fidelity, we evaluate line sampling intervals of 2, 4, 8, 16, and 32 pixels on the DocUNet dataset. The corresponding MS-SSIM scores are 0.582, 0.582, 0.579, 0.575, and 0.569, respectively. These results indicate that finer sampling better captures local curvature, thereby enhancing perceptual quality. Considering both reconstruction performance and computational efficiency, we adopt sampling interval of 4 pixels as trade-off. To bridge the domain gap between synthetic and real-world data, we synthesize more realistic training samples by compositing geometrically distorted document images onto randomly selected COCO17 backgrounds. Rather than explicitly modeling illumination artifacts such as shadows, we leverage standard data augmentation strategies to implicitly simulate such effects, enhancing the models generalization to diverse visual contexts."
        },
        {
            "title": "Hyperparameter",
            "content": "Learning rate Scaling factor σ Positive value ε Smoothing coefficient γ Batch size Warm-up phase Training epochs GPU"
        },
        {
            "title": "Value",
            "content": "0.0001 0.005 0.0001 0.8 32 0.2 30 2 A100 Table 6. The model hyperparameters 7. Data augmentation Figure 8. Visualization of the cropping process To enhance the diversity of distortion relationships, we apply minor cropping to the generated distorted images and Figure 9. Exploration of enhancement tasks. 9. Bias Analysis In the label preprocessing module, the forward map (FM) is derived by proportionally sampling anchor points on the backward map (BM) and constructing an augmentation matrix. However, certain bias arises due to incomplete sampling. To quantify this bias, we apply two mappings (FM and then BM) sequentially to the original image, line elements, and foreground masks. For the original image Iu, we compute SSIM with the mapped image. For line elements Lu, we calculate the displacement of points after mapping. For foreground elements Mu, we compute the IoU of the masks. The bias is quantified using 100,000 backward maps from DOC3D [7]. The specific calculations are as follows: Bias(Iu) = SSIM (BM(FM(Iu)), Iu), Bias(Lu) = OF SET (BM(FM(Lu)), Lu), Bias(Mu) = IOU (BM(FM(Mu)), Mu). (7) Taking line elements as an example, we calculate the displacement before and after the two mappings for all 100,000 samples, then compute the minimum, maximum, and average displacements. Fig. 10 shows the displacement variation under different sampling ratios. As the sampling ratio increases, the displacement is effectively controlled. By balancing these metrics, we determine that 40% sampling ratio is optimal. Figure 10. Statistic analysis of bias. 10. More visualizations We present additional comparisons of the models dewarping results in Fig. 11 , 12 ,13 and 14, which effectively demonstrate the superiority of our approach. Red arrows highlight the differences. Figure 11. Visualization comparison on the DocUNet dataset. Figure 12. Visualization comparison on the DIR300 dataset. Figure 13. Visualization comparison on the DocReal dataset. Figure 14. Visualization comparison on the WarpDoc dataset."
        }
    ],
    "affiliations": [
        "DeepGlint",
        "Imperial College London",
        "Qihoo Technology"
    ]
}