{
    "paper_title": "Selective Training for Large Vision Language Models via Visual Information Gain",
    "authors": [
        "Seulbi Lee",
        "Sangheum Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 6 8 1 7 1 . 2 0 6 2 : r a"
        },
        {
            "title": "SELECTIVE TRAINING FOR LARGE VISION LANGUAGE MODELS\nVIA VISUAL INFORMATION GAIN",
            "content": "Seulbi Lee and Sangheum Hwang Department of Data Science, Seoul National University of Science and Technology seulbi@ds.seoultech.ac.kr, shwang@seoultech.ac.kr February 20,"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens."
        },
        {
            "title": "Introduction",
            "content": "Large Vision Language Models (LVLMs) [1, 2, 3, 4, 5, 6, 7, 8, 9] have demonstrated remarkable capabilities across wide spectrum of multimodal tasks, ranging from image captioning [10, 11] and visual question answering [2, 12, 13] to more complex instruction following [14, 15, 16] and reasoning [17, 18]. By combining powerful large language models [19, 20, 21, 22] with pre-trained vision encoders [23, 24], LVLMs can generate fluent and context-aware responses conditioned on both images and textual queries. However, despite this progress, it remains challenging to ensure that LVLMs are reliably grounded in the visual input rather than dominated by textual priors [25, 26, 27]. growing body of work has shown that LVLMs often exhibit language bias: an over-reliance on language even when relevant visual evidence is available [28]. This bias manifests as visual ignorance, where the model effectively behaves as text-only model and ignores salient image content [29, 30]. It also leads to hallucinations [31, 32, 33, 34, 35], in which the model confidently describes objects or attributes that are not present in the image. Such behaviors call into question the reliability of LVLMs: to what extent do these models actually use the image, as opposed to merely being conditioned on it? To mitigate language bias, prior work has mainly focused on model-level interventions. Training-free methods, such as contrastive decoding [25, 36], compare outputs with and without visual input at inference time, while other approaches boost image attention or modify attention mechanisms to encourage stronger visual grounding [27, 37]. In parallel, data-centric efforts construct higher-quality multimodal instruction datasets by leveraging stronger models or careful filtering [7, 33, 38]. However, these approaches share common limitation: they do not explicitly quantify, within given multimodal dataset, how much each sample or token actually depends on visual information. Corresponding author PREPRINT - FEBRUARY 20, 2026 (a) sample that can be answered from common sense (b) sample that requires fine-grained visual understanding Figure 1: Examples of LLaVA-1.5 instruction tuning data. The dataset includes both samples and tokens with very different levels of visual dependency: some questions can be answered without looking at the image, whereas others need fine-grained visual details (highlighted in green). In practice, multimodal instruction-tuning datasets contain heterogeneous mixture of examples: some can be answered from common sense or linguistic context alone (Fig. 1a), while others genuinely require fine-grained visual understanding (Fig. 1b). Although this heterogeneity appears not only at the sample level but also at the token level, visually grounded tokens (e.g., color, object attributes) are generally optimized under the same objective as non-visual or structural tokens (e.g., articles, auxiliaries, and discourse markers). When all such samples and tokens are treated with equal importance during training, the model receives no incentive to distinguish visually dependent signals from text-only patterns, and thus fails to fully acquire robust visual grounding, instead defaulting to easily exploitable linguistic shortcuts. In this work, we take data-centric perspective on language bias and visual grounding in LVLMs. We hypothesize that key driver of language bias is the prevalence of weakly grounded, text-dominant examples in multimodal training datasets, combined with the uniform treatment of all tokens during training. We therefore ask question: can we directly measure how much each training sample and token benefits from the visual input, and use this signal to focus learning on genuinely visual evidence? To this end, we introduce Visual Information Gain (VIG), perplexity-based metric that quantifies the contribution of visual information and supports both analysis and training of LVLMs. The contributions of this work are threefold: We introduce Visual Information Gain (VIG), perplexity-based metric that quantifies the contribution of visual input by measuring the reduction in model uncertainty. VIG provides model-agnostic and decomposable measure, enabling fine-grained analysis of visual dependency at both sample and token levels. We empirically demonstrate that VIG serves as reliable indicator of visual grounding. Our analysis shows that VIG aligns with benchmark-level modality dependencies and successfully identifies visually grounded tokens (such as colors, spatial relations, and attributes) while distinguishing them from tokens driven primarily by textual priors. We propose VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach enhances data efficiency by pruning weakly grounded samples and focusing optimization on visually informative tokens. Notably, this strategy improves visual grounding and mitigates language bias, achieving superior performance with highly sparse supervision compared to full-data training."
        },
        {
            "title": "2 Related Work",
            "content": "Despite the remarkable progress of LVLMs [4, 14, 16, 39, 40, 41], recent studies have identified persistent challenge known as language bias [25, 26, 27, 29, 30]. This refers to the tendency of LVLMs to produce visually ungrounded responses by over-relying on textual priors. Such bias often arises from language shortcuts [42, 43] earned from 2 PREPRINT - FEBRUARY 20, 2026 noisy multimodal datasets, which are frequently synthesized from text-only LLMs and contain visually irrelevant or misleading captions [7, 33, 38]. During training, LVLMs may find it statistically advantageous to exploit these textual patterns rather than attending to images. Empirical analyses further confirm that attention distributions within LVLMs tend to concentrate on textual tokens over visual features [44, 45], thereby limiting the models active reference to visual information. Efforts to generate visually grounded responses in LVLMs have primarily focused on reducing the dominance of textual priors or encouraging more effective visual grounding. Training-free approaches such as contrastive decoding [25, 36] attempt to compare model predictions with and without visual input, thereby suppressing language-driven responses during inference. However, these methods mainly circumvent rather than resolve the underlying issue and often incur additional inference overhead, as they do not modify how visual information is represented or utilized. Another line of work enhances visual grounding by boosting image attention [27, 37], though its effects are often overly broad, amplifying irrelevant regions and occasionally introducing noise [46]. Training-based strategies aim to address the issue more fundamentally. For instance, Zhao et al. [26] proposes dual attention and soft-image guidance to explicitly promote visual utilization, though such techniques require architectural modifications. Collectively, these approaches highlight the need for bias mitigation strategies that can enhance visual grounding effectively while maintaining architectural simplicity and avoiding additional inference overhead."
        },
        {
            "title": "3 Visual Information Gain",
            "content": "3.1 Preliminary The prevalent architecture of LVLMs [1, 2, 3, 4, 7] consists of three components: pre-trained vision encoder Ev, an adapter and pre-trained language model D. Training typically follows two-stage paradigm. In the pre-training stage, the adapter is trained on large-scale imagecaption pairs formatted as single-turn instructions. For each image and its associated caption, simple question (e.g., Describe this image) is randomly sampled to request brief description, and the original caption serves as the target answer A. This process aligns the visual feature space with the semantic space of the language model, while keeping both Ev and frozen. Subsequently, in the instruction tuning stage, the model is fine-tuned on complex multimodal instruction-following data (I, Q, A). In this stage, represents diverse, task-oriented question, and is the corresponding answer. This stage jointly optimizes and to enhance the capability of the model in multimodal reasoning and instruction following. For each sample, the visual feature and its projected embedding are obtained as fv = Ev(I), zv = P(fv). The models predictive distribution over answer tokens is denoted as qθ( a<t, Q, zv), parameterized by θ. The per-sample instruction tuning objective is thus defined as: L(A Q, I; θ) = 1 (cid:88) log qθ(at a<t, Q, zv) (1) t=1 where at denotes the t-th token in the answer and is the sequence length. For notational simplicity, we omit θ and denote the models predictive distributions under different conditioning as qQ() = qθ( Q) and qI,Q() = qθ( I, Q) which correspond to predictions without and with visual input, respectively. 3.2 Definition of VIG To measure the sample-level contribution of visual information, we introduce VIG, which quantifies how much the inclusion of image reduces the models uncertainty in predicting the answer given the question Q. Formally, we define VIG as the log-ratio between the models perplexities (PPL) on the same answer with and without visual conditioning: VIG = log (cid:18) PPL(A Q) PPL(A Q, I) (cid:19) (2) where PPL(A Q) and PPL(A Q, I) denote the perplexities evaluated under the predictive distributions of the model qQ and qI,Q, respectively. To simulate the absence of visual information within the LVLM architecture, we calculate PPL(AQ) using blurred image that removes visual cues, as proposed by Xing et al. [47] (see Appendix for details). higher VIG value indicates that the models uncertainty is substantially reduced when visual information is provided, implying that the image plays critical role in producing the correct answer. To establish theoretical foundation for VIG, we reformulate it in terms of cross-entropy loss and KL divergence. Using the relationship PPL = exp(L), where is the cross-entropy loss, Eq. 2 can be rewritten as: VIG = L(AQ) L(AQ, I). (3) 3 PREPRINT - FEBRUARY 20, 2026 Table 1: VIGs sensitivity to the degree of the visual grounding. Examples from the MS-COCO [48] validation set show that VIG quantitatively captures the strength of visual support: high positive for perfect match, moderate positive for partial grounding, and negative for conflicting image. Question What do you see on the floor near the red towel? Answer white cat sitting on the floor next to his bowl. Image VIG 0. 0.409 -0.520 This formulation shows that VIG represents the reduction in cross-entropy loss attributable to the inclusion of the visual input. The cross-entropy between ground-truth distribution and the models predictive distribution is L(p, q) = H(p) + DKL(pq) where H(p) is the intrinsic entropy of the target distribution and DKL denotes the KL divergence. Then, VIG can be rewritten as: VIG = [H(pAQ) H(pAI,Q)] + [DKL(pAQqQ) DKL(pAI,QqI,Q)] (4) where pAQ and pAI,Q represent the true conditional answer distributions given text-only and multimodal inputs, respectively, while qQ and qI,Q are the corresponding model predictive distributions. In general, incorporating reduces the intrinsic uncertainty of the true answer distribution, such that H(pAI,Q) < H(pAQ). However, under deterministic supervision (typical in VQA and captioning datasets), we adopt the empirical distribution determined by the single ground-truth answer as the target p. In this case, is Dirac delta distribution (one-hot), so the intrinsic entropy terms vanish: H(pAQ) = H(pAI,Q) = 0. Thus, Eq. 4 simplifies to: VIG = (cid:2)DKL(pAQqQ) DKL(pAI,QqI,Q)(cid:3) . Consequently, VIG quantifies empirically how much the visual information reduces the divergence between the models predictive distribution and the ground truth. (5) Expanding Eq. 3, VIG can be expressed as the average of token-wise loss differences over = (a1, . . . , aT ): VIG = 1 (cid:88) t=1 [ log qθ(at a<t, Q)] [ log qθ(at a<t, Q, zv)] (6) where zv = P(Ev(I)) denotes the visual embedding. Each term log qθ(at a<t, Q) and log qθ(at a<t, Q, zv) represents the token-level cross-entropy loss computed without and with visual conditioning. This decomposition reveals that although VIG is defined at the sample level, it inherently reflects the aggregate contribution of per-token loss reductions. Analyzing token-level loss differences therefore provides fine-grained view of which parts of response strongly depend on visual information. Throughout this work, we compute VIG using models after the pre-training stage, where the adapter is trained to align the visual feature space with the language semantic space while keeping the vision encoder Ev and the language model frozen. This ensures that the model has established basic visualtextual correspondence, allowing VIG to meaningfully reflect the contribution of visual information. 3.3 Analysis To empirically validate the effectiveness of VIG, we conduct analyses based on LLaVA-v1.5 7B after the pre-training stage, where the model has learned to establish correspondences between visual and textual modalities. VIG is fine-grained measure of visual grounding. To isolate the impact of the visual input, we vary only the image while keeping the questionanswer pair fixed. As shown in Tab. 1, VIG effectively quantifies the degree of visual support for given textual description. The first image, perfectly aligned with the text, produces high positive VIG of 4 PREPRINT - FEBRUARY 20, 2026 Figure 2: VIG distribution across benchmarks. Blue benchmarks (COCO, POPE) show stronger multimodal interaction, while red benchmarks (GQA, SQA) exhibit weaker visual dependency. Table 2: Tokens and their loss differences in LLaVA-1.5 instruction-tuning data. Red regions show large positive loss differences (strong visual grounding), while blue regions exhibit near-zero or negative differences (weak visual contribution). Red Blue white (3.59), black (6.08), lying (5.99), flying (5.73) sitting (4.22), standing (5.10), reading (5.974), crowd (3.30) (0.03), of (0.01), the (0.03), ize (0.00), which (0.00), are (-0.02), The (0.04), - (-1.10) 0.923. The second image depicts the correct subject (cat) but mismatches an attribute (white), yielding moderate positive VIG of 0.409. In contrast, the dog image contradicts the textual content, resulting in negative VIG of 0.520. These examples confirm that VIG serves as sensitive and reliable metric for measuring the extent to which visual information reduces model uncertainty. VIG aligns with benchmark-level modality dependency. Previous studies have suggested that LVLM benchmarks differ in the extent to which they rely on visual versus textual information [49]: COCO [48] and POPE [34] generally require substantial visual understanding, whereas benchmarks such as GQA [50] and SQA [51] are often considered to be more text-dominant. To examine whether VIG captures these tendencies, we compute the VIG score for every sample across benchmarks and visualize the distributions of sample-level VIG values in Fig. 2. We observe that COCO exhibits distribution shifted toward positive values, consistent with the expectation that image captioning relies heavily on visual inputs. POPE displays distribution centered near zero, suggesting balanced dependency where models may utilize both visual evidence and textual cues. In contrast, GQA and SQA show distributions skewed toward negative values. This aligns with prior findings that these benchmarks often exhibit more reliance on textual information, where introducing visual information can inadvertently increase prediction uncertainty compared to text-only inference. Overall, these results suggest that VIG can characterize the sample-level modality dependency across LVLM benchmarks. VIG captures token-level visual grounding. To empirically examine the token-level decomposition in Eq. 6, we visualize the relationship between token losses with and without visual conditioning on the LLaVA-1.5 instructiontuning dataset in Fig. 3. Points along the diagonal = correspond to tokens unaffected by visual input. Notably, the scatter reveals broad spectrum of visual dependency at the token level. Tokens with high VIG (visualized in red) appear above the diagonal, indicating that prediction losses are substantially reduced by the image. As detailed in Tab. 2, these tokens largely correspond to visually salient concepts such as colors, spatial relations, and physical states, i.e., elements that are often unpredictable from text alone. Conversely, tokens with low or negative VIG (blue region), such as articles and prepositions, primarily serve syntactic purposes. These findings confirm that sample-level visual dependency measured by VIG arises from the cumulative contribution of such visually grounded tokens. 5 PREPRINT - FEBRUARY 20, 2026 Figure 3: Visualizing the token-level VIGs. Each point shows tokens prediction loss with (x-axis) and without (y-axis) visual input. The color encodes the token-level loss difference (y x). 3.4 VIG-Guided Selective Training To demonstrate the practical utility of VIG, we adopt the principle of selective modeling, recently shown to be effective for LLMs [52]. For the i-th training sample (Ii, Qi, Ai) with answer length Ti, we denote its sample-level VIG by VIGi and its token-level visual gain by VIGi,t, where VIGi,t corresponds to the token-wise loss difference term in Eq. 6. Then Eq. 6 can be rewritten as VIGi = 1 Ti Ti(cid:88) t=1 VIGi,t. (7) We use these quantities to perform VIG-guided selective training. First, we rank all training samples by VIGi and select the top p%. Let τp denote the corresponding threshold, i.e., the minimum VIGi within this top-p% set, and define Sp = { VIGi τp } (8) as the index set of selected samples. This sample-level filtering allows the model to focus on examples that provide substantial visual gains and reduces the influence of weakly grounded, text-dominant data. Within this curated subset, we further perform token-level selection using the same threshold. For each Sp, we define the set of visually informative tokens as + = { VIGi,t τp }, (9) and compute the loss only on these tokens. Concretely, during instruction tuning, we feed the full answer sequence Ai and compute logits at every timestep, but the loss is computed exclusively over tokens in (cid:83) , ensuring that unselected tokens do not contribute to gradient updates. Reusing the same threshold τp at both the sample and token levels is an intentional design choice to avoid introducing additional hyperparameters, and it concentrates optimization on the most visually informative regions of the data, leading to more visually grounded and data-efficient learning. + iSp"
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Tasks and Benchmarks We evaluate the effectiveness of VIG-based selective training on two categories of benchmarks: vision understanding and hallucination evaluation. For vision understanding, which spans basic recognition to more complex multimodal reasoning, we use LLaVAW [1], MMVet [53], MMBench [54], and DocVQA [55]. To assess hallucination behavior, we adopt POPE [34], CHAIR [32], and MMHal [56]. Further details are provided in the Appendix A. 6 PREPRINT - FEBRUARY 20, 2026 Table 3: Quantitative comparison on vision understanding and hallucination benchmarks. We report results for three LVLMs: LLaVA-1.5 7B, LLaVA-1.5 13B, and ShareGPT4V 7B. VIG training denotes our VIG-guided selective training. # Sample Tokens represents the total number of answer tokens contained in the multimodal samples retained after sample-level selection. # Active Tokens refers to the effective number of tokens that contribute to the loss computation after applying token-level masking. indicates the percentage reduction in token count or the performance improvement compared to the vanilla baseline. For each metric, indicates higher is better and indicates lower is better. Bold indicates entries where VIG-guided selective training outperforms the vanilla model. Model # Sample Tokens # Active Tokens Vision Understanding LLaVAW MMVet MMBench DocVQA POPE Hallucination CHAIR MMHal Score Acc. F1 Acc. CS CI Score Hall. LLaVA-1.5 7B + VIG training LLaVA-1.5 13B + VIG training ShareGPT4V 7B + VIG training 58.61M 51.17M -13% 58.61M 28.94M -51% 60.33M 49.34M -18% 58.61M 38.45M -34% 58.61M 12.14M -79% 60.33M 39.20M -35% 59.02 61.22 +2.20 72.01 73.45 +1. 64.49 66.67 +2.18 28.62 32.71 +4.09 36.19 36.87 +0.68 33.16 35.51 +2.35 65.46 66.33 +0.87 67.52 68.67 +1. 65.89 67.81 +1.92 22.31 22.51 +0.20 24.08 25.27 +1.19 26.15 28.23 +2.08 85.90 85.93 +0.03 85.72 86.95 +1. 85.69 87.15 +1.46 87.08 87.47 +0.39 87.05 87.53 +0.48 86.98 87.24 +0.26 52.93 47.00 +5.93 51.96 48.19 +3. 28.12 25.66 +2.46 14.99 12.80 +2.19 13.22 13.19 +0.03 7.88 6.56 +1.32 1.71 2.23 +0.52 2.05 2.12 +0. 1.80 2.01 +0.21 71.25 62.78 +8.47 67.09 63.11 +3.98 70.99 66.12 +4.87 4.2 Overall Performance and Data Efficiency Setup. We evaluate VIG-guided selective training on LLaVA-1.5 7B, LLaVA-1.5 13B [2], and ShareGPT4V 7B [7]. Following Sec. 3, we first obtain aligned LVLMs by performing alignment training on imagecaption data (558K pairs for LLaVA-1.5 and 1.2M pairs for ShareGPT4V). The subsequent instruction-tuning datasets for both models contain roughly 665K instances each: about 40K of these are text-only, and the remaining 625K are multimodal (imageinstructionanswer) samples. We leave the text-only samples unchanged and compute VIGi only on the multimodal subset using the aligned models. For VIG-guided sample selection in the instruction-tuning stage, we set = 70 and use the corresponding threshold τ70 for all experiments. We rank all multimodal instruction samples by VIGi and retain the top 70%, resulting in 437K multimodal instruction samples for LLaVA-1.5 and 436K for ShareGPT4V, respectively. Within these selected samples, VIG-guided token selection adopts the same threshold: only tokens whose token-level gain VIGi,t τ70 are included in the loss computation during instruction tuning. Details on the training data and configurations are provided in the Appendix C. Results. As shown in Tab. 3, VIG-based selective training yields strong data efficiency: By training on only 70% of the samples and further pruning supervision via token-level masking, all models exceed their vanilla counterparts. For LLaVA-1.5 7B, VIG training optimizes on only 38.45M target tokens yet improves performance on all benchmarks. The effect is even more pronounced for LLaVA-1.5 13B: although it is optimized on only 12.14M tokens, it boosts performance across all benchmarks. It suggests that larger models can make more effective use of carefully selected, visually grounded data, even when trained on substantially fewer tokens. For ShareGPT4V 7B, the vanilla model already outperforms LLaVA-1.5 7B due to its stronger imagetext alignment. On top of this, VIG selection (only 39.20M tokens) further improves performance with particularly clear gains on vision-understanding benchmarks. Note that while we fix the sample selection ratio at = 70 for all models, the resulting # Active Tokens differ because the distribution of token-level VIGi,t varies across models: applying the same threshold τ70 thus retains different proportions of tokens. Overall, these results demonstrate that prioritizing visually important data at both sample and token levels can substantially reduce the amount of supervision required, while even improving performance on both vision understanding and hallucination benchmarks. We further provide qualitative comparisons in Appendix C.3. 4.3 Comparison with Existing Methods Baselines. We compare our approach on LLaVA-1.5 7B with four recent methods that aim to strengthen visual grounding. We include three training-free methods: VCD [25], which utilizes contrastive decoding to suppress language priors, PAI [27], which explicitly amplifies visual attention, and VAR [46], which redistributes attention to mitigate attention sink issues. Also, we compare against LACING [26], training-based method that fine-tunes the model to enforce the usage of visual information. 7 PREPRINT - FEBRUARY 20, 2026 Table 4: Quantitative comparison with existing methods on LLaVA-1.5 7B. We compare our approach against recent training-free (VCD [25], PAI [27], VAR [46]) and training-based approaches (LACING [26]), proposed to improve visual grounding. VIG training\" denotes VIG-guided selective training on LLaVA-1.5 7B. Values in parentheses indicate the performance improvement over the vanilla model. Bold indicates entries where VIG-guided selective training improves over the vanilla model. Model LLaVAW MMBench DocVQA POPE Vision Understanding Hallucination CHAIR MMVet Score 28.62 27.01 27.99 30.98 34.15 59.02 60.55 57.54 61.11 61.09 65.46 64.34 65.45 66.30 66.45 Acc. 22.31 22.98 21.98 23.11 21.45 F1 85.90 86.53 85.98 86.10 85.58 Acc. 87.08 86.61 85.84 87.12 86.68 CS 52.93 49.81 35.46 50.09 30.85 CI 14.99 13.69 12.11 14.77 11.73 MMHal Score Hall. 1.71 1.67 1.78 2.14 2.12 71.25 76.01 72.31 62.89 64. 61.22 (+2.20) 61.98 (+1.43) 59.99 (+2.45) 63.00 (+1.89) 62.99 (+1.90) 32.71 (+4.09) 32.98 (+5.97) 32.69 (+4.70) 34.91 (+3.93) 37.01 (+2.86) 66.33 (+0.87) 67.65 (+3.31) 67.34 (+1.89) 67.66 (+1.36) 67.89 (+1.45) 22.51 (+0.20) 23.15 (+0.17) 22.22 (+0.24) 23.22 (+0.11) 22.02 (+0.57) 85.93 (+0.03) 86.11 (-0.42) 86.12 (+0.14) 86.99 (+0.89) 86.19 (+0.61) 87.47 (+0.39) 87.12 (+0.51) 87.49 (+1.65) 87.50 (+0.38) 87.39 (+0.71) 47.00 (+5.93) 44.99 (+4.82) 32.12 (+3.34) 44.98 (+5.11) 28.11 (+2.74) 12.80 (+2.19) 12.00 (+1.69) 11.86 (+0.25) 11.87 (+2.90) 9.97 (+1.76) 2.23 (+0.52) 2.11 (+0.44) 2.43 (+0.65) 2.54 (+0.40) 2.71 (+0.59) 62.78 (+8.47) 62.77 (+13.24) 62.90 (+9.41) 58.90 (+3.99) 56.10 (+8.44) LLaVA-1.5 7B + VCD + PAI + VAR + LACING VIG training + VCD + PAI + VAR + LACING Results. As shown in Tab. 4, existing methods exhibit notable trade-offs. LACING achieves strong results on MMVet, MMBench, and CHAIR but degrades on fine-grained document understanding (DocVQA). Similarly, VCD and PAI improve hallucination metrics but often at the expense of general vision understanding capabilities. In contrast, VAR offers more balanced trade-off, serving as strong inference-time baseline. Our VIG-trained model achieves competitive or superior performance across all benchmarks without any architectural changes or inference-time overhead. Unlike LACING, which sacrifices performance in specific domains, VIG training strictly improves over the vanilla baseline on all vision-understanding tasks, including LLaVAW, MMVet, MMBench, and DocVQA. Notably, on MMHal, VIG substantially reduces hallucination while simultaneously boosting the overall score, demonstrating that training exclusively on visually informative tokens effectively strengthens visual grounding. Furthermore, VIG training exhibits strong orthogonality to existing approaches. As detailed in Tab. 4, integrating VIG with inference-time strategies (VCD, PAI, VAR) consistently yields additive gains in both vision understanding and hallucination robustness. It also combines naturally with the training-based method LACING, since VIG operates at the data level while LACING modifies the architecture. Their combination (VIG training + LACING) achieves the strongest overall performance on MMVet (37.01) and MMBench (67.89). These results confirm that VIG serves as fundamental, data-centric enhancement that complements diverse visual grounding strategies. 4.4 Analysis Increased attention to visual tokens. To better understand why VIG-guided training improves performance, we examine how the model allocates attention to visual tokens. Following the analysis protocol of Kaduri et al. [44], we evaluate the models on subset of the MS-COCO [48] validation set and measure the proportion of attention weights assigned to image tokens relative to the total attention at each layer. This ratio effectively summarizes the strength of visual reference across network depth. As shown in Fig. 4, the VIG-trained model consistently assigns larger fraction of attention to visual tokens than the vanilla LLaVA-1.5 7B model. The gap is especially pronounced in the middle layers, which have been identified as crucial for semantic feature extraction from visual inputs [37, 44]. These results indicate that VIG-guided selective training encourages the model to refer more strongly to visual evidence. Further analyses on LLaVA-1.5 13B and ShareGPT4V 7B are provided in the Appendix D.1. Reducing language bias via VIG-guided training. Recent work has identified form of text bias in LVLMs, termed blind faith in text, where models follow misleading textual cues instead of relying on the image [57]. To evaluate whether VIG-guided training mitigates this behavior, we adopt the evaluation protocol of Kembhavi et al. [57] on VQAv2 [31], using LLaVA-1.5 7B as the baseline. In this setup, the model is presented with the original image together with corrupted description, where spurious text is appended to encourage an incorrect answer. Fig. 5 reports the accuracy on clean inputs (Base), the accuracy under textual corruption (Corruption), and the normalized score (Norm=Corruption/Base). While both models achieve similar accuracy on clean questions, the VIG-trained model attains higher accuracy in the corruption setting and larger normalized score, indicating that the VIG-trained model effectively resists textual interference by grounding its predictions in the visual input, even when misleading text is present. These results suggest that VIG-guided selective training reduces the models reliance on spurious textual cues and encourages stronger use of visual evidence. Appendix D.2 provides further analyses on LLaVA-1.5 13B and ShareGPT4V 7B. 8 PREPRINT - FEBRUARY 20, 2026 Figure 4: Attention fraction allocated to visual tokens. Compared to LLaVA-1.5 7B, VIG training assigns significantly more attention to visual tokens across all layers. Figure 5: Evaluation of text reliance under textual corruption. Base: accuracy on clean inputs. Corruption: accuracy when the same image is paired with corrupted caption containing conflicting description. Norm: corruption accuracy normalized by the corresponding Base (Corruption/Base). 4.5 Ablation Study Effectiveness of VIG-based selection. To validate the effectiveness of our VIG-guided selection strategy, we conduct an ablation study on LLaVA-1.5 7B using three settings: (i) training on random 70% subset of the full data (Random), (ii) selecting the top 70% samples by VIG score without token-level filtering (SS), and (iii) applying both sampleand token-level selection (SS+TS). As shown in Tab. 5, the Random setting yields slightly lower scores compared to the vanilla model. This marginal degradation aligns with prior observations that LLaVA retains around 95% of its performance even when trained on half of the instruction data [2]. In contrast, selecting the same number of samples based on VIG (SS) surpasses the Random setting and even outperforms the vanilla model across all four benchmarks, confirming that VIG is effective at identifying visually informative samples. Finally, incorporating fine-grained token selection (SS+TS) leads to the best results on every metric, highlighting that token-level filtering is crucial for maximizing the benefits of VIG-guided training. Please refer to Appendix E.1 for the detailed results. Effect of selection ratio p. We investigate the sensitivity of VIG-guided training to the selection ratio p. Since determines the selection threshold τp, smaller implies higher threshold, resulting in fewer selected samples and more aggressive token-level selection. Fig. 6 reports normalized performance of LLaVA-1.5 7B for {30, 50, 70, 100}, where = 100 represents the vanilla model trained on the full instruction-tuning dataset. Note that at = 30, = 50, and = 70, the model is updated with approximately 5%, 17%, and 65% of the total tokens, respectively. Overall, the impact of varies across benchmarks. On LLaVAW, all reduced-ratio settings (p < 100) outperform the full-data baseline. Even the most aggressive setting (p = 30) yields strong results, suggesting that open-ended generation prioritizes data quality over 9 PREPRINT - FEBRUARY 20, 2026 Table 5: Ablation study of selection levels on LLaVA-1.5 7B. Random trains on random 70% subset of the data, SS selects the top 70% samples by VIG score (sample-level selection only), and SS+TS additionally applies token-level VIG selection. We report single metric per benchmark: LLaVAW score, MMBench score, CS for CHAIR, and Hall. for MMHal. Model LLaVAW MMBench CHAIR MMHal LLaVA1.5-7B Random SS SS+TS 59.02 56.91 58.12 61. 65.46 55.97 57.56 66.33 52.93 54.88 50.23 49.10 71.25 74.49 68.14 61. Figure 6: Ablation study of selection ratio p% on LLaVA-1.5 7B. We report single metric per benchmark: LLaVAW score, MMBench score, CS for CHAIR, and Hall for MMHal. = 100 corresponds to the vanilla model trained on the full instruction-tuning dataset (no VIG-based selection). All scores are normalized with respect to the = 100 setting. quantity. In contrast, on MMBench, aggressive filtering (p = 30, 50) results in slight performance drop, whereas = 70 matches or exceeds the baseline. This implies that multiple-choice reasoning requires broader data coverage to maintain robustness across diverse topics. For hallucination benchmarks (CHAIR and MMHal), all VIG-trained models (p < 100) consistently outperform the baseline. However, the performance gap between = 30, 50, and 70 is marginal. These results demonstrate that VIG-based selection can substantially reduce the supervision cost while maintaining and often improving performance across wide range of selection ratios. Detailed results are provided in the Appendix E.2."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Visual Information Gain (VIG), perplexity-based metric that quantifies how much each multimodal training sample and token benefits from visual input. Our analysis shows that VIG correlates well with benchmark-level modality dependency and highlights visually grounded tokens such as colors, spatial relations, and object attributes, while deemphasizing tokens that can be predicted from text alone. Building on this, we propose VIG-guided selective training scheme that prioritizes high-VIG samples and tokens, enabling LVLMs to achieve better vision understanding and hallucination robustness while utilizing only fraction of the original supervision. We further demonstrate that VIG-based data selection is complementary to existing visual grounding strategies, yielding additional gains when combined. Overall, our results suggest that explicitly quantifying the visual contribution of training data is promising direction for building LVLMs that more reliably use what they see. practical limitation of our approach is the computational overhead of computing VIG: for each multimodal instruction, we require additional forward passes using the aligned model. However, VIG scoring is one-time, forward-only, and fully parallelizable procedure, and the resulting scores can be reused across training runs and model variants. Thus, our primary focus in this work is on maximizing data efficiency, reducing the amount of multimodal supervision needed to train strong LVLM, rather than minimizing the overall computational cost. In addition, our empirical study is primarily demonstrated on the LLaVA-1.5 and ShareGPT4V families. Applying VIG-guided selection to other architectures and domains remains an important direction for future work. 10 PREPRINT - FEBRUARY 20,"
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the National Research Foundation of Korea (NRF) under Grant [RS2024-00352184] and [RS-2024-00354675] funded by the Ministry of Science and ICT (MSIT)."
        },
        {
            "title": "References",
            "content": "[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023. [2] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [3] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. [4] Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai, and Wenhai Wang. Mini-internvl: flexible-transfer pocket multi-modal model with 5 Visual Intelligence, 2024. [5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024. [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, 2025. [8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. [9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as unified interface for vision-language multi-task learning, 2023. [10] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl: Modularization empowers large language models with multimodality, 2024. [11] Zhiliang Peng, Wenhui Wang, Liliu2024llavanext Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world, 2023. [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, 2023. [13] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022. [14] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. [15] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. 11 PREPRINT - FEBRUARY 20, 2026 [16] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [17] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. [18] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. Technical report, Alibaba, 2025. [19] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [20] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. [21] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. Technical report, Alibaba, 2023. [22] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, 2021. [24] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [25] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [26] Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Baobao Chang, and Minjia Zhang. Looking beyond text: Reducing language bias in large vision-language models via multimodal dual-attention and soft-image guidance. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025. [27] Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms. In European Conference on Computer Vision, 2025. [28] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. MMICL: Empowering vision-language model with multi-modal in-context learning. In The Twelfth International Conference on Learning Representations, 2024. [29] Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang, Muyang He, Jian Li, Zheng Liu, Harry Yang, Sernam Lim, and Bo Zhao. Unveiling the ignorance of mllms: Seeing clearly, answering incorrectly. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [30] David Wan, Jaemin Cho, Elias Stengel-Eskin, and Mohit Bansal. Contrastive region guidance: Improving grounding in vision-language models without training. In European Conference on Computer Vision, 2025. [31] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 12 PREPRINT - FEBRUARY 20, [32] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018. [33] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The International Conference on Learning Representations, 2024. [34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2023. [35] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [36] YiFan Zhang, Yang Shi, Weichen Yu, Qingsong Wen, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, and Rong Jin. Debiasing multimodal large language models via penalization of language priors. In Proceedings of the ACM International Conference on Multimedia, 2025. [37] Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, and Xu Yang. Devils in middle layers of large vision-language models: Interpreting, detecting and mitigating object hallucinations via attention lens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [38] Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an eos decision perspective. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2024. [39] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. Mm1: Methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, 2025. [40] Team Gemini. Gemini: family of highly capable multimodal models. Technical report, Gemini Team Google, 2025. [41] OpenAI. Gpt-4 technical report. Technical report, OpenAI, 2024. [42] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. [43] An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, and Daeyoung Kim. Vision language models are biased, 2025. [44] Omri Kaduri, Shai Bagon, and Tali Dekel. Whats in the image? deep-dive into the vision of vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [45] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, 2025. [46] Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. See what you are told: Visual attention sink in large multimodal models. In The International Conference on Learning Representations, 2025. [47] Xiaoying Xing, Chia-Wen Kuo, Li Fuxin, Yulei Niu, Fan Chen, Ming Li, Ying Wu, Longyin Wen, and Sijie Zhu. Where do large vision-language models look at when answering questions?, 2025. [48] Tsung-Yi Lin, Michael Maire, Serge Belongie andJames Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, 2014. [49] Divyam Madaan, Varshan Muhunthan, Kyunghyun Cho, and Sumit Chopra. Multi-modal data spectrum: Multimodal datasets are multi-dimensional, 2025. [50] Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and In Proceedings of the IEEE/CVF Conference on Computer Vision and compositional question answering. Pattern Recognition, 2019. [51] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems, 2022. 13 PREPRINT - FEBRUARY 20, 2026 [52] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, yelong shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Not all tokens are what you need for pretraining. In Advances in Neural Information Processing Systems, 2024. [53] Yu Weihao, Yang Zhengyuan, Li Linjie, Wang Jianfeng, Lin Kevin, Liu Zicheng, Wang Xinchao, and Wang Lijuan. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning, 2024. [54] Liu Yuan, Duan Haodong, Zhang Yuanhan, Li Bo, Zhang Songyang, Zhao Wangbo, Yuan Yike, Wang Jiaqi, In European He Conghui, Liu Ziwei, et al. Mmbench: Is your multi-modal model an all-around player? Conference on Computer Vision, 2024. [55] Minesh Mathew, Dimosthenis Karatzas, and C.V. Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021. [56] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. In Findings of the Association for Computational Linguistics, 2024. [57] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. Words or vision: Do vision-language models have blind faith in text? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [58] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, 2022. 14 PREPRINT - FEBRUARY 20,"
        },
        {
            "title": "A Details of Benchmarks",
            "content": "Visual understanding task. To assess the models capabilities in general visual perception and reasoning, we employ the following benchmarks. LLaVAW [1]: LLaVA-Bench (In-the-Wild) comprises 24 images and 60 associated questions. It encompasses diverse array of visual domains such as indoor and outdoor environments, memes, paintings, and sketches. The dataset is designed to assess the LVLMs capability in handling complex tasks and generalizing to unfamiliar environments. It is evaluated by GPT-4 (gpt-4o-2024-11-20). MMVet [53]: MM-Vet evaluates the integrated capabilities of LVLMs in visual conversation across broad range of tasks. Comprising 200 images and 218 questions with ground-truth references, it employs GPT-4 (gpt-4-0613) evaluation framework to assess both the precision and utility of the models responses. MMBench [54]: MMBench is comprehensive benchmark containing roughly 3,000 multiple-choice questions that cover 20 skills. It uses GPT-3.5 (gpt-3.5-turbo-0613) to extract the final prediction label (A, B, C, D) from the models response. Our evaluation focuses specifically on the English subset of the dataset. We report the results using the development split of the dataset. DocVQA [55]: DocVQA targets the task of visual document understanding, challenging models to extract and reason about information embedded in document images such as forms, invoices, and reports. In this study, we perform all evaluations using the official validation split and report accuracy. Hallucination evaluation task. We evaluate the models robustness to hallucination across the following benchmarks. POPE [34]: Polling-based Object Probing Evaluation (POPE) serves as robust metric for assessing object hallucination. Constructed from MSCOCO [48], A-OKVQA [58], and GQA [50], it comprises 27,000 query-answer pairs derived from 500 images per dataset. The core mechanism involves querying LVLMs about the existence of specific objects, with balanced 50:50 ratio of existent to non-existent objects. To rigorously test the model, POPE employs three distinct negative sampling strategies: random (arbitrary missing objects), popular (high-frequency missing objects), and adversarial (co-occurring but absent objects). With six questions assigned per image, performance is evaluated using Accuracy, Precision, Recall, and F1 score. We report the average Accuracy and F1 score computed across all three negative sampling strategies. CHAIR [32]: To quantify object hallucination in generated captions, we employ the CHAIR (Captioning Hallucination Assessment with Image Relevance) metric. This method evaluates caption faithfulness by cross-referencing generated objects with those actually present in the image. The evaluation consists of two distinct metrics: CHAIRI (instance-level), which measures the proportion of hallucinated objects among all generated objects, and CHAIRS (sentence-level), which represents the percentage of captions containing at least one hallucination. The metrics are expressed by the following equations: CHAIRI = #hallucinated objects #generated objects , CHAIRS = #hallucinated captions #generated captions . (10) MMHal [56]: MMHal-Bench is specialized evaluation framework designed to assess hallucination in LVLMs using 96 challenging queries derived from OpenImages, employing GPT-4 (gpt-4-0613) to grade responses on scale of 0 to 5. We report both the average score and the hallucination rate (Hall.). Specifically, the hallucination rate is calculated by treating score of 3 as the threshold for factual correctness; responses scoring below 3 are considered to contain hallucinations."
        },
        {
            "title": "B Details of Visual Absence Simulation",
            "content": "To quantify VIG, we compare the models loss with and without visual information. Following Xing et al. [47], we simulate visual absence by applying Gaussian blur with hyperparameters scaled to the input image resolution, thereby eliminating semantic visual cues. Fig. B.1a and Fig. B.1b illustrate the original image and its blurred counterpart. Details of VIG-guided Selective Training In this section, we provide the implementation details of VIG-based training for Sec. 4 to ensure clarity and reproducibility. 15 PREPRINT - FEBRUARY 20, 2026 (a) Ground truth image. (b) Image-absent condition. Figure B.1: Examples of with and without image setting. Table C.1: Selection threshold τp at = 70 across the models. LLaVA-1.5 7B LLaVA-1.5 13B ShareGPT4V 7B Model Threshold τ70 0. 0.046 0.042 Table C.2: Hyperparameters for training, which are identical to the original models and training time. LR LR warmup ratio Optimizer Epoch Batch size LR schedule Model Training time Pretraining ShareGPT4V Instruction Tuning LLaVA-1.5 Family, ShareGPT4V 1 1 256 128 2e-5 cosine decay 2e-5 cosine decay 0.03 0.03 AdamW 12 hours AdamW 7 hours C.1 Instruction-tuning Datasets and the Threshold τp For the LLaVA-1.5 family, we employ the instruction-tuning dataset proposed by Liu et al. [2]. In the case of ShareGPT4V, we adhere to the original protocol by substituting the detailed description samples within the LLaVA dataset with the high-quality captions produced by ShareGPT4V [7]. The specific selection thresholds τp derived at = 70 across the models are as shown in Tab. C.1. C. Implementation Details Pretraining. We use the official pretrained checkpoints (not instruction-tuned) of the LLaVA-1.5 family released on Hugging Face2. For ShareGPT4V 7B, we pretrain the model following the training configuration proposed in Chen et al. [7]. The detailed training configuration for ShareGPT4V 7B is provided in Tab. C.2. Using these pretrained models, we compute the VIG score for each training sample and select samples to be used for subsequent instruction-tuning. VIG Calculation. We utilize the pretrained model to compute VIG scores of the training samples. On setup with 8 RTX 4090 (24GB) GPUs, this process takes approximately 6 hours without specific inference optimizations (e.g., vLLM). VIG-guided Selective Training. Based on the data selected via VIG, we perform instruction tuning with VIG training. During this stage, we adopt the same hyperparameter settings as those used in the original implementations of LLaVA-1.5 and ShareGPT4V. We use 8 A100 (80GB) GPUs. Training details for VIG training are summarized in Tab. C.2. C.3 Qualitative Results We provide qualitative results of VIG-training. We examine how VIG-guided selective training mitigates hallucinations and enhances visual grounding across different models, LLaVA-1.5 7B, 13B, and ShareGPT4V 7B. As shown in Tab. C.3, both the vanilla LLaVA-1.5 7B and 13B models suffer from severe object hallucinations. The 7B model fabricates dining table and two people, while the 13B model hallucinates three people in the background. These errors suggest that the models are retrieving generic descriptions associated with classical paintings or indoor 2LLaVA-1.5 7B: https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5 LLaVA-1.5 13B: https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5 16 Table C.3: Examples of models responses from LLaVAW. When applying VIG training, the model provides more visually-grounded responses for writing tasks with visual inputs than base LLaVA-1.5 and ShareGPT4V. PREPRINT - FEBRUARY 20, 2026 User Describe this photo in detail. LLaVA-1.5 7B Vanilla +VIG Training Vanilla LLaVA-1.5 13B +VIG Training Vanilla +VIG Training ShareGPT4V 7B The image features painting of dog dressed up as woman, wearing dress and hat. The dog is the main focus of the painting, and it appears to be the subject of the artwork. The painting is set in room with dining table visible in the background. There are also two people in the scene, one on the left side and another on the right side of the painting. The person on the left is partially visible, while the person on the right is more prominently featured in the scene. The image features painting of dog wearing Renaissance-style dress, complete with hood and cape. The dog is sitting in pose that resembles woman, giving the impression of human-like figure. The dogs attire and the way it is positioned in the painting create unique and intriguing scene. The image is painting featuring dog dressed up as Mona Lisa, wearing bandana on its head and holding its hands together, posing for the portrait. The background includes mountains and river, adding to the compositions artistic appeal. In addition to the dog, there are three people present in the scene. One person is positioned near the left edge, another person can be seen at the bottom right corner, and the third person is located on the right side of the painting. The image features painting of dog dressed as the famous painting of the Mona Lisa. The dog is wearing bandana on its head and is positioned in way that resembles the iconic pose of the Mona Lisa. The painting is quite detailed, capturing the essence of the original artwork while incorporating the unique twist of the dog as the subject. The dogs facial expression and posture are reminiscent of the Mona Lisas enigmatic smile and pose, making the painting creative and amusing take on the classic artwork. The image presents surrealistic portrait of dog, painted in the style of the Renaissance period. The dog, the central figure of the painting, is adorned with red hat and blue robe, adding touch of whimsy to the otherwise realistic style. The background is landscape, painted in realistic style, with river meandering through it, adding depth to the composition. The painting is executed in oil on canvas, medium that allows for rich colors and intricate details. The genre of the painting is surrealism, style that combines elements of the real and the imaginary, creating dreamlike quality. The painting is testament to the artists skill in blending realism with surrealism, creating unique and captivating piece of art. The image presents unique piece of art, reminiscent of the famous painting Mona Lisa by Leonardo da Vinci. The central figure is dog, adorned with black turban and long, flowing brown robe. The dogs gaze is directed straight at the viewer, creating sense of connection. The background is landscape, featuring river and mountains, adding depth to the composition. The art style is realistic, with meticulous attention to detail evident in the dogs features and the surrounding environment. The genre of the artwork is portraiture, focusing on the depiction of the dog as the main subject. The overall composition and style suggest fusion of classical art techniques with modern twist. 17 Table C.4: Examples of models responses from MMVet. When applying VIG training, the model provides more accurate responses for writing tasks with visual inputs than base LLaVA-1.5 7B. PREPRINT - FEBRUARY 20, 2026 User Is this airplane taking off or landing? LLaVA-1.5 7B Vanilla Based on the image, it is not possible to definitively determine whether the airplane is taking off or landing. However, we can observe that the airplane is in the process of either ascending or descending, as it is flying low over the runway. The presence of smoke coming from the airplanes wheels suggests that it could be either taking off or landing. To determine the exact phase of the flight, we would need more information or clearer view of the airplanes position relative to the ground. LLaVA-1.5 13B ShareGPT4V 7B +VIG Training Vanilla +VIG Training Vanilla +VIG Training The airplane is taking off from the runway. The airplane in the image is taking off from the runway. The airplane in the image is taking off from the runway. The airplane is taking off from the runway. The airplane is taking off from the runway. scenes from their trained knowledge rather than referring to the specific input image. In contrast, the VIG-trained models successfully suppress these text-driven fabrications. By filtering out low-VIG tokens during training, our method encourages the model to verify visual existence before generation, resulting in faithful descriptions that accurately capture the dogs solitary presence and specific pose. Beyond object existence, VIG training significantly improves the precision of attribute recognition. The vanilla ShareGPT4V 7B model, despite being stronger baseline than LLaVA-1.5 7B, is susceptible to linguistic shortcuts. Recognizing the visual similarity to the Mona Lisa, the model defaults to describing the original paintings attributeshallucinating red hat and blue robe. This indicates blind faith in the semantic concept over pixel-level evidence. The VIG-trained model effectively breaks this shortcut. By prioritizing tokens with high visual information gain, the model correctly grounds the attributes in the actual image, accurately identifying the black turban and brown robe. This demonstrates that VIG-guided training forces the model to override misleading textual priors with genuine visual evidence, leading to more robust and grounded multimodal generation. Furthermore, Tab. C.4 presents an illustrative example from the MMVet, suggesting that VIG training can help smaller models achieve response quality comparable to larger or stronger baselines in certain scenarios. In this case, when asked to determine the airplanes action, the vanilla LLaVA-1.5 7B model exhibits uncertainty, providing lengthy and ambiguous description. In contrast, the VIG-trained model delivers concise and accurate answer (taking off), which aligns with the outputs of the significantly larger LLaVA-1.5 13B and the more advanced ShareGPT4V 7B. This observation implies that VIG training has the potential to enhance the visual grounding capability of smaller models. 18 PREPRINT - FEBRUARY 20,"
        },
        {
            "title": "D Additional Analysis",
            "content": "In this section, we extend the analysis presented in Sec. 4.4 to LLaVA-1.5 13B and ShareGPT4v 7B. D.1 Visual Attention Ratio (a) LLaVA-1.5 13B (b) ShareGPT4V 7B Figure D.1: Attention fraction allocated to visual tokens. Compared to their respective baselines, VIG-trained models consistently assign significantly higher attention weights to visual tokens across all layers, demonstrating improved visual grounding regardless of model scale or architecture. Fig. D.1a and Fig. D.1b illustrate the proportion of attention weights allocated to visual tokens across all layers for each model. Regardless of the model scale or the baseline architecture, common trend is observed: the VIG-trained models consistently assign significantly higher attention scores to visual tokens compared to their original counterparts. D.2 Blind Faith in Text (a) LLaVA-1.5 13B (b) ShareGPT4V 7B Figure D.2: Evaluation of text reliance under textual corruption. Base: accuracy on clean inputs. Corruption: accuracy when the same image is paired with corrupted caption containing conflicting description. Norm: corruption accuracy normalized by the corresponding Base (Corruption/Base). Fig. D.2a presents the results for LLaVA-1.5 13B. While the clean accuracy (Base) remains comparable, the VIGtrained model shows significantly higher robustness under corruption (42.9%) compared to the vanilla model (36.8%), resulting in notable improvement in the normalized score (Norm) from 45.7 to 53.0. similar trend is observed with ShareGPT4V 7B in Fig. D.2b. The vanilla model suffers severe performance drop when facing corrupted text (30.5%). In contrast, VIG training boosts the corruption accuracy to 39.3% and improves the normalized score by over 10%. These results consistently demonstrate that VIG-guided training effectively mitigates language bias and encourages the model to ground its predictions in visual evidence. PREPRINT - FEBRUARY 20,"
        },
        {
            "title": "E Details of Ablation Study",
            "content": "E.1 Effectiveness of VIG-based Selection E.1.1 Comprehensive Results Tab. E.1 provides comprehensive results for the experimental results of the selection level. Notably, Random degrades performance across most benchmarks compared to the baseline, confirming that simply reducing data volume is detrimental. In contrast, the combination of sample and token-level selection (SS+TS) achieves the best results, significantly outperforming the baseline. Table E.1: Ablation study of selection levels on LLaVA-1.5 7B. Random trains on random 70% subset of the data, SS selects the top 70% samples by VIG score (sample-level selection only), and SS+TS additionally applies token-level VIG selection. For each metric, indicates higher is better and indicates lower is better. Bold indicates the best performance. Model Vision Understanding LLaVAW MMVet MMBench DocVQA POPE Hallucination CHAIR MMHal LLaVA-1.5 7B Random SS SS+TS 59.02 58.50 58.71 61.22 Score 28.62 27.27 32.29 32.71 Acc. F1 Acc. CS CI Score Hall. 65.46 55.97 57.56 66.33 22.31 21.38 22.06 22.51 85.90 87. 52.93 14.99 85.48 85.22 85.93 86.63 86.53 87.47 53.11 52.10 47.00 15.00 14.61 12. 1.71 1.79 1.84 2.23 71.25 71.33 63.54 62.78 E.2 Effect of Selection Ratio E.2.1 The threshold τp values To investigate the impact of the selection ratio, we conduct experiments with {30, 50, 70, 100} on LLaVA-1.5 7B. Note that = 100 corresponds to the vanilla model trained on the full instruction-tuning dataset without VIG-based selection. Selection thresholds τp derived at each are as shown in Tab. E.2. Table E.2: Selection threshold τp at each on LLaVA-1.5 7B. Ratio 30 50 Threshold τp 0.124 0.031 0.021 E.2.2 Comprehensive Results Tab. E.3 demonstrates that VIG-guided filtering (p < 100) consistently outperforms the full-data baseline on hallucination and open-ended benchmarks. While the aggressive setting (p = 30) offers extreme efficiency (using only 5% of tokens), it shows minor degradation on broad-coverage tasks like MMBench. In contrast, the = 70 configuration achieves the optimal balance, securing peak scores on POPE and MMHal by preserving sufficient diversity for complex reasoning while still significantly reducing computational cost. Table E.3: Ablation study of selection ratio p% on LLaVA-1.5 7B. # Sample Tokens represents the total number of answer tokens contained in the multimodal samples retained after sample-level selection. # Active Tokens refers to the effective number of tokens that contribute to the loss computation after applying token-level masking. For each metric, indicates higher is better and indicates lower is better. Bold indicates the best performance. Ratio # Sample Tokens # Active Tokens Vision Understanding LLaVAW MMVet MMBench DocVQA POPE CHAIR MMHal Hallucination 30% 50% 70% 100% 8.58M 26.32M 51.17M 58.61M 3.19M 10.00M 38.45M 58.61M 60.90 61.81 61.22 59.02 Score 34.03 31.19 32.71 28.62 Acc. F1 Acc. CS CI Score Hall. 56.87 56.44 66.33 65.46 22.66 22.49 22.51 22. 82.80 85.09 85.93 85.90 84.28 86.14 87.47 87.08 48.71 47.63 47.00 52.93 13.31 12.99 12.80 14.99 1.99 1.95 2.23 1.71 63.01 62.91 62.78 71."
        }
    ],
    "affiliations": [
        "Seoul National University of Science and Technology"
    ]
}