{
    "paper_title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
    "authors": [
        "Siyang He",
        "Qiqi Wang",
        "Xiaoran Liu",
        "Hongnan Ma",
        "Yiwei Shi",
        "Yuerong Song",
        "Ying Zhu",
        "Tianyi Liang",
        "Zengfeng Huang",
        "Ziwei He",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 3 ] . [ 1 2 8 1 3 2 . 1 0 6 2 : r FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation Siyang He1,2,3, Qiqi Wang1,2,3, Xiaoran Liu1,2,3 Hongnan Ma3 Yiwei Shi3 Yuerong Song1,2,3 Ying Zhu1,2,3 Tianyi Liang2,3 Zengfeng Huang1,2 Ziwei He1,2,3, Xipeng Qiu1,2,3, 1Fudan University 2Shanghai Innovation Institute 3OpenMOSS Team"
        },
        {
            "title": "Abstract",
            "content": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages frequency-domain sliding window mechanism to dynamically guide the model to achieve structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.18B-Instruct. hsy0406071@gmail.com, 22307140082@m.fudan.edu.cn, ziwei.he@sii.edu.cn, Correspondence: xpqiu@fudan.edu.cn Repository: https://github.com/ShirleYoung/FourierSampler"
        },
        {
            "title": "Introduction",
            "content": "Diffusion Large Language Models (dLLMs) [34, 38, 40] have emerged as notable non-autoregressive paradigm. By breaking the strict left-to-right decoding constraint of traditional autoregressive (AR) models [6, 12, 33], dLLMs utilize an arbitrary-order generation strategy. This flexibility allows the model to leverage global bidirectional context during the entire generation process, rather than being limited to prefix history. Consequently, this paradigm has demonstrated distinct advantages over AR models, including mitigating the reversal curse [4]\" inherent to unidirectional causal masking, facilitating controllable generation tasks such as text infilling, and enabling non-sequential planning for complex problem-solving. Equal contribution. 1 Despite these benefits, existing research indicates that dLLMs often exhibit strong positional bias. In fact, [20] have shown that constraining the model to left-to-right order can even surpass standard confidence-based decoding on several math tasks. Although empirically effective on specific benchmarks, such strict generation constraints the inherent flexibility of dLLMs, preventing them from fully leveraging global bidirectional context for holistic planning. To address this problem, recent works have adopted ad-hoc measures, including rule-based methods [20] or reward-guided strategies [16]. Notably, the effectiveness of these heuristic approaches indicates that optimizing the decoding schedule is crucial to further enhance dLLM capabilities. Figure 1 The average score in different tasks of FourierSampler compared with other decoding strategies in LLaDA [32, 55]. Different from the above works that calibrate the sampling distribution with external priors, in this work, we introduce principled approach to mitigate the dominance of positional bias and unlock the intrinsic non-autoregressive capabilities of dLLMs. Diverging from reliance on external intervention signals, we delve into the frequency-domain characteristics of the models internal representations. Drawing on insights from the Fourier Transformer[18], we observe that low-frequency components in textual representations typically encapsulate global structural information and long-range dependencies, whereas high-frequency components are responsible for characterizing local details. Based on this intuition, we propose FourierSampler, decoding strategy that dynamically focuses on different frequency bands at different decoding steps, which guides dLLMs to first decode structural content dominated by low-frequency signals and subsequently complement detailed content dominated by high-frequency signals. This produces beneficial long-range influence on the generation trajectory, thereby enhancing the overall quality and logical coherence of the output. As shown in Figure 1, our method can statistically outperform the original model and other work relying on external signal-guided decoding on various math and code tasks. Our contribution can be summarized as follows. We conduct the first frequency analysis in dLLMs that the low-frequency components of hidden states in the temporal dimension correspond to structural information in the output, while the high-frequency components correspond to detailed information, which provides internal guidance for dLLM decoding. We propose FourierSampler, our dLLM decoding sampling scheme, which guides the model to achieve structure-to-detail decoding with the Translated Fourier Score, and balances the guidance with the original confidence by an Adaptive Fourier Calibrator. We conduct validation experiments on two types of dLLMs, LLaDA and SDAR, and find that FourierSampler consistently achieves stable improvements in code and math tasks, specifically achieving relative improvements up to 20.4% and 16.0% on LLaDA1.5-8B and LLaDA-8B-Instruct, and up to 45.1% and 26.5% on SDAR-1.7B-Chat and SDAR-4B-Chat, respectively. These results surpass other inference enhancement strategies as well as autoregressive models with similar sizes, providing new insights for an in-depth understanding of dLLM decoding enhancement."
        },
        {
            "title": "2.1 Decoding Strategy for dLLMs",
            "content": "Diffusion Large Language Models (dLLMs) [34, 38, 40] have become hot topic in NLP. Models like LLaDA [32], Dream [51], Mercury [22], Gemini Diffusion [14] and SDAR [9] confirm the scalability of this paradigm [15, 30, 31] and fuel intensive follow-up work on long-context modeling [17, 27], inference efficiency [41, 45, 46], multimodal extensions [47, 53], and post-training strategies [44, 5456]. Although 2 Figure 2 Visualization of the correspondence between frequency-domain analysis and textual information. In the hidden states after forward pass, tokens dominated by low-frequency signals correspond to structural information like if and elif, while tokens dominated by high-frequency signals correspond to detailed information like gcd. dLLMs can decode in any order, their decoding trajectories still exhibit pronounced positional bias [16]. Consequently, recent work focuses on optimizing token unmasking orders to enhance generation planning. Mainstream models like LLaDA [32] and SDAR [9] adopt confidence-based unmasking, with variants prioritizing unmasking strategies based on maximum probability, entropy [3], or confidence gaps [23], alongside random sampling baselines [1]. To enhance generation performance, advanced methods introduce external interventions. For example, PC-Sampler [20] uses rules-based biases for specific positions, while RWS [16] employs reward models to enhance coherence. Alternatively, training-based approaches like DOT [50], DDPD [25], and DCoLT [21] optimize generation trajectories via post-training or reinforcement learning. However, these methods rely on complex external signals or costly training. They overlook the potential of mining dLLM internal representations for effective decoding guidance."
        },
        {
            "title": "2.2 Spectral Foundations of Transformers",
            "content": "The representation power of Transformers can be deeply understood through the lens of signal processing, particularly by analyzing their behavior in the frequency domain. While FNet [24] and its successors [39, 57] demonstrate the efficacy of spectral token-mixing, theoretical evidence suggests that multi-head self-attention inherently functions as low-pass filter [35, 43]. This is further corroborated by Fourier Transformer [18], which observes that power spectra concentrate in low-frequency bins within deeper layers. However, while these spectral properties have been integrated into autoregressive architectures, their potential to guide the decoding process in dLLMs remains unexplored."
        },
        {
            "title": "3.1 Spectral Semantic Analysis in dLLMs",
            "content": "Building upon the theoretical foundations of spectral analysis (Sec 2), we investigate how spectral properties manifest within the unique architecture of dLLMs. Given that low-frequency components typically carry global trends while high-frequency components encode fine-grained details, we hypothesize that the hidden states in dLLMs exhibit similar semantic stratification across their frequency spectrum. 3 To validate this, we select two text samples possessing distinct structural features for analysis, Python script for calculating the Greatest Common Divisor shown in Figure 2, and mathematical derivation of the difference of squares formula in Figure 6. After single forward in dLLMs, we extract the final-layer hidden states RLD where is the sequence length and is the hidden dimension, and compute the low-frequency component low by retaining only the lowest-frequency half of the spectrum via binary frequency mask {0, 1}W(W denotes the number of frequency components): low = 1 (Fr(H) ) , Mk = (cid:40) 0 < W/2, 1, 0, otherwise. (1) where Fr denotes the real-valued Fourier transform, since is real-valued [42]. We then define the low-frequency ratio rlow as: , rlow = low2 2 H2 2 denotes the Euclidean norm along the feature dimension D. Similarly, we also define the where 2 high-frequency ratio rhigh = 1 rlow, which accounts for the remaining spectral energy. Based on this metric, we highlight the Top-14 tokens with the highest low-frequency and high-frequency ratios in Figure 2 and Figure 6. Observations indicate that in the code task, reserved keywords constituting the logical skeleton of the program (if, elif, return) exhibit energy significantly concentrated in the low-frequency band. Conversely, specific function names and numerical values display distinct high-frequency characteristics. Similarly, in the math task, natural language text guiding the derivation logic is dominated by low-frequency signals, whereas specific mathematical formulas account for the majority of the high-frequency energy. (2) This analysis confirms, for the first time in dLLMs, semantic correspondence where low-frequency components capture the structural skeleton, while high-frequency components encode local details. This discovery implies that dLLM decoding can be optimized as hierarchical refinement process. Specifically, prioritizing the generation of low-frequency structural\" tokens provides stable global context that constrains the search space for subsequent high-frequency detail\" tokens. Conversely, attempting to determine finegrained details before the logical skeleton is established can lead to structural inconsistencies or error propagation, which is common failure mode in non-autoregressive generation. Notably, this structure-todetail generation paradigm has been extensively explored and validated in diffusion-based text-to-image generation [13] [5], where models are widely observed to first form low-frequency global layouts and semantic compositions before progressively refining high-frequency textures and fine details. Inspired by this hierarchical dependency, we implement dynamic spectral filtering method called FourierSampler. By employing frequency-domain sliding window that gradually transitions the passband from low to high frequencies, we enforce structure-to-detail generation order. This ensures that the model first converges on robust global plan before refining the specific nuances of the output."
        },
        {
            "title": "3.2 Translated Filtering Score",
            "content": "The Translated Filtering Score provides frequency-domain criterion for ranking token positions during decoding. At each decoding step, it retains fixed-width frequency band whose location shifts uniformly from the low-frequency end toward the high-frequency end of the spectrum, and scores tokens according to their energy within the retained band. Assume the decoding block size is and the number of decoding steps per block is S. Let H(s) RBD denote the hidden states of the current block at decoding step s. We apply real-valued Fourier transform along the sequence dimension, filter the spectrum using step-dependent binary mask g(s), and then 4 Figure 3 Overview of our FourierSampler. sliding window in the frequency domain, retaining the low frequency at the beginning and the high frequency at the end based on the decoding step s, guides the dLLM to decode structural content first, then detailed content via Translated Fourier Score and Adaptive Fourier Calibrator. transform back to the original space: (s) = (cid:16) (cid:0)H(s)(cid:1) g(s) Fr (cid:17) , (3) where denotes element-wise multiplication. The filtered hidden state (s) is the reconstructed timedomain representation after retaining only the frequency components selected at step s. Let denote the number of frequency components. We set the width of the frequency window as = max(1, ρW) , (4) where ρ (0, 1] controls the relative bandwidth. The window translates uniformly along the frequency axis over decoding steps. Accordingly, the starting position of the window at decoding step is defined as os = (cid:22) 1 (W w) (cid:23) . The frequency mask g(s) {0, 1}W is then defined as gp(s) = (cid:40) os < os + w, 1, 0, otherwise, (5) (6) for frequency index {0, . . . , 1}. As increases, the retained frequency band shifts monotonically from the beginning toward the end of the spectrum, resulting in step-dependent spectral preference during decoding, favoring low-frequencydominated tokens in the earlier stage and high-frequency-dominated tokens later. For token at position within the block (0 1), we compute its energy under the filtered representation and normalize it within the block to obtain the Translated Filtering Score ℓs,t, where ϵ is small constant added for numerical stability: 5 ℓs,t = DΣ (cid:32) DΣ d= d=1 (cid:16) [t,d](s) (cid:17)2 (cid:33) (cid:17)2 (cid:16) [t,d](s) . + ϵ max (7) The score ℓs,t measures the relative intensity of token under the frequency band selected at step s, and is used to guide the decoding priority within the block."
        },
        {
            "title": "3.3 Adaptive Fourier Calibrator\nTo dynamically adjust the guidance strength of the translated filtering score ℓs,t in the sampler, we introduce\nan adaptive weight βs based on the original decoding confidence. At decoding step s, let the output prediction\ndistribution be ps,t. For the set of masked positions, Ms, we take the maximum probability qs,t = maxv ps,t(v)\nin the prediction distribution at each position t ∈ Ms, and treat its variance over Ms as the ability of dLLM\nto distinguish between the writing priorities of different positions in the current decoding state.",
            "content": "(cid:16) = Var σ2 {qs,t}tMs (cid:17) . (8) We record the values of this method over the past 20 decoding steps, compute the percentile Ps of the current variance within the history, and normalize it into ws (0, 1) interval, the cumulative distribution function of the normal distribution. The process of normalization is detailed in Appendix A, and, finally, the adaptive weight is defined as follows, where βmin and βmax are the minimum and maximum values of the adaptive weight, respectively. βs = βmin + (1 ws)(cid:0)βmax βmin Based on the above translated filtering score and adaptive weight calculation, we add it to the original confidence cs,t as step for token to obtain the fusion score, shown as follows. (cid:1). (9) cs,t = cs,t + βsℓs,t. (10) This design ensures that when the confidence differences between different positions are large, the models own decoding intention is relatively clear, and the frequency guidance automatically weakens. Conversely, the frequential prior is strengthened, thereby achieving an adaptive decoding scheduler."
        },
        {
            "title": "4.1 Setup",
            "content": "We conduct experiments on widely used diffusion-based dLLMs, including LLaDA1.5-8B [55] and LLaDA-8BInstruct [32]. In addition, to demonstrate that our method applies to models with other dLLM architectures, namely dLLMs with block-wise causal attention, we also evaluate our FourierSampler on SDAR-4B-Chat and SDAR-1.7B-Chat [9]. The evaluation benchmarks include GSM8K (4-shot) [10], MATH (4-shot) [19], MBPP (3-shot) [2], HumanEval (0-shot) [8], and Countdown(0-shot) [49, 52]. During evaluation, we set the default block size to 64 for all dLLMs and adopt OpenCompass [11] as the evaluation framework. We use 512 generation steps for GSM8K, MATH, HumanEval, and MBPP, and 128 generation steps for Countdown. All experiments are conducted on NVIDIA H200 GPUs. For the LLaDA Series, we use PC-Sampler [20] and RWS [16] as main baselines. For the SDAR Series, since PC-Sampler does not provide the token frequency distribution for SDAR, we use RWS as the primary baseline. The relevant coefficients in PC-Sampler follow the settings in its original paper. For RWS, we adopt GRM-Llama3.2-3B [48] as its reward model, recommended by its paper. We also compare dLLMs enhanced via different decoding strategies with similarly sized autoregressive models from Llama and Qwen Series [12, 29, 36]. 6 Llama3.1-8B-Instruct Qwen2.5-7B-Instruct LLaDA1.5-8B + PC-Sampler + RWS + FourierSampler (ours) LLaDA-8B-Instruct + PC-Sampler + RWS + FourierSampler (ours) GSM8k 80.97 81.65 79.83 81.20 80.67 81.80 78.24 79.00 79.61 79.61 Math 41.60 49. MBPP 65.37 66.93 HE 54.27 52.44 CD 0.00 4. Avg. 48.44 50.90 30.47 41.40 42.02 +2.8% 39.63 -3.0% 30. 40.85 +1.7% 43.00 +3.9% 51.36 +22.2% 39.02 -4.5% 21.09 -30.8% 47.13 +0.5% +1.1% 42.00 +1.4% 43.19 -1.3% 47.11 +0.4% +2.5% 44.20 +6.8% 50.58 +20.4% 43.29 +6.0% 34.77 +14.1% 50.93 +8.6% 39.63 -4.3% 49.81 +20.8% 39.63 45.34 42.20 -4.6% 46.61 +2.8% +1.0% 40.40 +1.8% 42.80 +1.4% 42.02 +1.9% 39.02 -1.5% 30.08 +18.5% 46.71 +3.0% +1.8% 45.20 +6.8% 47.86 +16.0% 40.85 +3.1% 28.90 +13.8% 48.48 +7.1% 25.39 0.0% 24.22 41.25 46.91 Table 1 Results on LLaDA Series including LLaDA1.5-8B [55] and LLaDA-8B-Instruct [32] with best values in bold and relative improvement over vanilla decoding based on confidence colored in green (positive) and red (negative). Notably, FourierSampler achieves consistent positive improvements across all evaluated tasks, resulting in the best average performance. It also surpasses other competitive decoding strategies and similarly sized autoregressive models, including Llama3.1-8B-Instruct [12] and Qwen2.5-7B-Instruct [36]. Llama3.2-3B-Instruct Qwen2.5-3B-Instruct SDAR-4B-Chat + RWS + FourierSampler (ours) SDAR-1.7B-Chat + RWS + FourierSampler (ours) GSM8k 69.37 76.42 86.58 87.41 87. 72.93 75.59 73.84 Math 36.60 39.20 MBPP 50.97 47.47 HE 26.83 32.93 48.20 57.93 42.02 +1.0% 49.20 +2.1% 39.69 -5.5% 60.37 +1.2% 50.00 +3.7% 47.47 +13.0% 62.20 37.80 CD 0.00 10.94 13. Avg. 36.75 41.39 49.60 +4.2% 16.02 +20.6% 50.54 +1.9% +7.4% 16.80 +26.5% 52.82 +6.5% 39.60 35.41 +3.6% 41.00 +3.5% 35.80 +1.2% 40.00 +1.0% 36. 15.62 +1.1% 31.71 -16.1% 17.97 +15.0% 40.41 +0.3% +3.3% 43.29 +14.5% 22.66 +45.1% 43.27 +7.4% 40.27 Table 2 Results on SDAR Series including SDAR-4B-Chat and SDAR-1.7B-Chat [55] with best values in bold and relative improvement over vanilla decoding based on confidence colored in green (positive) and red (negative). Notably, FourierSampler achieves consistent positive improvements across all evaluated tasks, resulting in the best average performance. It also surpasses other competitive decoding strategies and similarly sized autoregressive models, including Llama3.2-3B-Instruct [29] and Qwen2.5-3B-Instruct [36]."
        },
        {
            "title": "4.2 Main Results",
            "content": "For LLaDA1.5-8B and LLaDA-8B-Instruct, which are dLLMs based on full bidirectional attention, the experimental results in Table 1 demonstrate that our method consistently and significantly outperforms the baseline across different math and code tasks. In particular, we observe substantial relative improvements of up to 7.2% on MATH, 20.4% on MBPP, and 14.1% on Countdown compared to the baseline. Moreover, our approach achieves the highest average performance across all benchmarks, surpassing other competitive methods. Notably, our FourierSampler further enables LLaDA1.5-8B to bridge and exceed the performance gap with similarly sized autoregressive models, such as Llama3.1-8B-Instruct and Qwen2.5-7B-Instruct, where LLaDA1.5-8B originally underperformed on average, and other decoding strategies fail to achieve it. This result highlights that our method can more effectively unlock the potential of non-autoregressive generation in dLLMs. For dLLMs with block-wise causal attention, including SDAR-4B-Chat and SDAR-1.7B-Chat, our method also consistently outperforms the baseline across all evaluated benchmarks in Table 2. Specifically, we achieve relative improvements of 3.7% on MATH, 14.5% on HumanEval, and 45.1% on Countdown. In addition, the average performance across tasks is superior to that of other competing approaches. These 7 GSM8k MBPP Avg. LLaDA1.5-8B + FourierSampler + Fixed β = 0.4 + Fixed β = 0.5 + Fixed β = 0.6 + ρ = 0.4 + ρ = 0.6 LLaDA-8B-Instruct + FourierSampler + Fixed β = 0.4 + Fixed β = 0.5 + Fixed β = 0.6 + ρ = 0.6 79.83 81.80 81.12 81.12 81.20 81.35 81.05 78.24 79. 78.85 79.38 79.23 80.36 42.02 50.58 49.81 47.86 50.19 42.80 43.00 41.25 47.86 44.75 46.69 47.08 60.97 66.19 65.47 64.49 65. 62.08 62.03 59.75 63.74 61.80 63.04 63.16 45.91 63.14 Table 3 Results on LLaDA Series [32, 55] for the ablation study of FourierSampler. GSM8K MBPP Vanilla Ours (%) Vanilla Ours (%) 80.82 48.25 -1.6% 51.36 80.06 +3.1% 50.58 +20.4% 79.83 44.75 +18.6% 68.16 81.05 +0.3% 49.03 80.36 +0.3% 49.81 81.80 +2.5% 42.02 72.55 +9.5% 37.74 = 16 = 32 = 64 = 128 Table 4 Results with different block sizes on LLaDA1.5. Figure 4 Lowand high-frequency features of different parts of speech in the frequency domain. The blue bars represent the proportion of tokens classified into the low group, while the orange bars represent the high group. Observations indicate that functional words responsible for syntactic structure, like conjunctions, exhibit dominant low-frequency features. In contrast, nouns, which typically serve as specific content fillers, show the strongest high-frequency tendency. This distribution corroborates our hypothesis that low-frequency components encode the structural skeleton, while high-frequency components correspond to detailed entities. results further demonstrate that our method generalizes well across different dLLM designs, including both full-bidirectional-attention and block-wise causal attention."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "To verify the rationality of the Adaptive Fourier Calibrator module design in our method, we conduct ablation studies by fixing the parameter β to the maximum, minimum, and mean values of the adaptive weights, respectively, and evaluate the performance on GSM8K and MBPP. The results in Table 3 show that different tasks on the same model may prefer different weight values. However, using fixed weights consistently underperforms the adaptive weighting strategy. These observations further validate the effectiveness of our method. We also conduct ablation studies regarding the choice of the sliding window size in the frequency domain, denoted as the window ratio ρ, for each model. Table 3 presents the experimental results for LLaDA-1.5B and LLaDA-8B-Instruct under different settings. Based on the overall performance across downstream tasks, we selected 0.2 and 0.4 as the window ratios for the two models, respectively."
        },
        {
            "title": "5.1 Analysis of Decoding Block Size",
            "content": "To verify the effectiveness of FourierSampler under different decoding block sizes for dLLMs, we further conduct an experiment shown in Table 4 and observe that as the block size increases, applying FourierSampler to downstream tasks leads to more pronounced performance gains. This is because larger blocks provide more complete and continuous signal for frequency-domain analysis, allowing the Fourier transform to 8 Figure 5 Visualization of step-wise generation trajectory for the prompt Write python function to compute Fibonacci sequence on LLaDA-8B-Instruct [32]. The heatmap displays the Translated Fourier Score ℓs,t at each step, with red stars marking the precise step where each token is decoded. We highlight blue boxes that correspond to structure words and are decoded in the early stages, and red boxes that correspond to detail words and are filled in later stages, which validates the structure-to-detail decoding pattern of FourierSampler. more accurately capture low-frequency components that correspond to global semantics and structural information. When the block size is too small, the sequence is frequently segmented, which can fragment frequency-domain representations across blocks and make low-frequency information difficult to localize reliably. In contrast, larger blocks allow frame-level information to be fully distilled and well modeled at early stages, and to consistently guide subsequent fine-grained generation. As result, the advantages of the FourierSampler become more evident in downstream task performance. It can be further observed that after applying FourierSampler, the scores on MBPP and GSM8K at block size = 64 are both higher than the baseline at block size 32 or 16, suggesting that our method effectively mitigates, and in some cases even avoids the severe performance degradation when block size increases."
        },
        {
            "title": "5.2 Analysis of Generation Order",
            "content": "To investigate whether the generation trajectory under FourierSampler aligns with the spectral characteristics observed in the static forward pass discussed in Section 3.1 and truly activates the non-autoregressive potential of dLLMs, we visualize the step-by-step decoding process of LLaDA-8B-Instruct [32] on code generation task. The heatmap visualizes, for two decoding blocks (Block 1 and Block 2), the Translated Fourier Score ℓs,t computed at each token position under the frequency band selected for each generation step. Red stars indicate the generation step at which each token is ultimately finalized. structure-to-detail generation pattern can be observed in Figure 5. From the spectrum of Block 1, it is evident that keywords representing the logical skeleton of the programsuch as if (position 9), and return (positions 17 and 28)achieve high scores and are determined at very early decoding stages (Steps 010). This indicates that the model prioritizes constructing the overall structural framework of the code. In contrast, specific variable names (e.g., fib, n) and numerical values (e.g., 0, 1) are generally generated at later decoding stages (Steps 1530). For instance, in Block 2, although else (position 33) 9 and return (position 37) appear relatively early, the subsequent concrete computation logic involving fib (position 45) and (position 47) does not emerge until around Step 20. This structure-to-detail generation trajectory provides intuitive evidence that our Translated Fourier Score successfully maps low-frequency energy in the frequency domain to structural information in text. As result, dLLM can plan global logic first and subsequently fill in local details."
        },
        {
            "title": "5.3 Analysis of Part-of-Speech",
            "content": "To investigate which words in natural language likely correspond to high-frequency components and which favor low-frequency ones, we conducted detailed statistical analysis on the WikiText-103 dataset [28]. For each paragraph, we extract the final-layer hidden state sequence RLD, apply the spectral filtering mechanism defined in Equation 1, and calculate the low-frequency ratio rlow as defined in Equation 2. Based on this, tokens with rlow > 0.5 (indicating low-frequency dominance) are classified into the low group, while others are assigned to the high group. Figure 4 illustrates the distribution ratios of different parts of speech across these two groups. Function words and connectives, which are primarily responsible for constructing sentence logic and structural scaffolding [7, 26, 37], occupy larger proportion of the low-frequency group. In particular, conjunctions (e.g., but, if, because), prepositions (e.g., in, for), and adverbs (e.g., firstly) exhibit the highest ratios of low-frequency dominance. In addition, verbs, as the core predicates of sentences, also show strong low-frequency tendency. This observation explains why control-flow tokens such as def and return are preferentially generated in the code-generation heatmap presented in the previous section. In contrast, nouns exhibit the strongest high-frequency characteristics among all part-of-speech categories. Nouns typically refer to concrete entities, variables, or values (e.g., fib, n, 0), serving as the specific content that fills the syntactic skeleton. These results are consistent with our distinction between framework words and detail words in natural language. FourierSampler leverages this property to transform implicit linguistic hierarchies into explicit generation planning."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we investigate the internal decoding mechanisms of dLLMs from the perspective of signal processing. We conduct the first frequency analysis in dLLMs, showing that low-frequency implies structure, and high-frequency implies detail. Then, we propose FourierSampler. By leveraging the Translated Fourier Score and Adaptive Fourier Calibrator, our method dynamically guides the dLLMs to achieve structure-to-detail generation. Extensive experiments across full-bidirectional-attention (LLaDA Series) and block-wise causal attention (SDAR Series) architectures demonstrate that FourierSampler achieves consistent performance improvements in different tasks, such as math and code. Furthermore, our analyses regarding different decoding block sizes, generation order, and part-of-speech distributions further corroborate the rationality of FourierSampler. This study not only experimentally surpasses similarly sized auto-regressive models but also paves an endogenous way for future research to unlock the arbitrary-order generation potential of dLLMs."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [3] Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. 10 [4] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on\" is b\" fail to learn\" is a\". arXiv preprint arXiv:2309.12288, 2023. [5] Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, and Matthieu Cord. Halton scheduler for masked generative image transformer. ArXiv, abs/2503.17076, 2025. URL https://api.semanticscholar.org/CorpusID: 277244147. [6] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aĳia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haĳun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruĳie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [7] Carnap. The logical syntax of language. 1937. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Shuang Cheng, Yihan Bian, Dawei Liu, Linfeng Zhang, Qian Yao, Zhongbo Tian, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, et al. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303, 2025. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https: //github.com/open-compass/opencompass, 2023. [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [13] Xiang Gao, Zhengbo Xu, Junhan Zhao, and Jiaying Liu. Frequency-controlled diffusion model for versatile text-guided image-to-image translation. Proceedings of the AAAI Conference on Artificial Intelligence, 38(3):18241832, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i3.27951. URL http://dx.doi.org/10.1609/aaai.v38i3.27951. [14] Gemini. Gemini diffusion, our state-of-the-art, experimental text diffusion model, 2025. URL https://deepmind. google/models/gemini-diffusion/. [15] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. [16] Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park, Junha Hyung, and Jaegul Choo. Rewardweighted sampling: Enhancing non-autoregressive characteristics in masked diffusion llms. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3456234582, 2025. [17] Guangxin He, Shen Nie, Fengqi Zhu, Yuankang Zhao, Tianyi Bai, Ran Yan, Jie Fu, Chongxuan Li, and Binhang Yuan. Ultrallada: Scaling the context length to 128k for diffusion large language models. arXiv preprint arXiv:2510.10481, 2025. [18] Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, and Zhouhan Lin. Fourier 11 transformer: Fast long range modeling by removing sequence redundancy with fft operator. arXiv preprint arXiv:2305.15099, 2023. [19] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [20] Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, and Tong Xiao. Pc-sampler: Position-aware calibration of decoding bias in masked diffusion models. arXiv preprint arXiv:2508.13021, 2025. [21] Zemin Huang, Zhiyang Chen, Zĳun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025. [22] Inception. Introducing mercury, the worlds first commercial-scale diffusion language model, 2025. URL https: //www.inceptionlabs.ai/introducing-mercury. [23] Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. [24] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with fourier transforms. In Proceedings of the 2022 Conference of the north American chapter of the Association for Computational Linguistics: human language technologies, pages 42964313, 2022. [25] Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi Jaakkola, and Rafael Gómez-Bombarelli. Think while you generate: Discrete diffusion with planned denoising. arXiv preprint arXiv:2410.06264, 2024. [26] Xiaoran Liu, Kai Lv, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, and Dahua Lin. Longwanjuan: Towards systematic measurement for long text quality. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 57095725, 2024. [27] Xiaoran Liu, Yuerong Song, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Longllada: Unlocking long context capabilities in diffusion llms. arXiv preprint arXiv:2506.14429, 2025. [28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [29] AI Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI., 2024. [30] Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, and Michael Qizhe Shieh. Training optimal large diffusion language models. arXiv preprint arXiv:2510.03280, 2025. [31] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. [32] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [33] OpenAI. Gpt-4 technical report. Technical report, 2023. [34] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. [35] Namuk Park and Songkuk Kim. How do vision transformers work? arXiv preprint arXiv:2202.06709, 2022. [36] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. [37] Dongyu Ru, Lin Qiu, Xipeng Qiu, Yue Zhang, and Zheng Zhang. Distributed marker representation for ambiguous discourse markers and entangled relations. arXiv preprint arXiv:2306.10658, 2023. [38] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. 12 [39] Carmelo Scribano, Giorgia Franchini, Marco Prato, and Marko Bertogna. Dct-former: Efficient self-attention with discrete cosine transform. Journal of Scientific Computing, 94(3):67, 2023. [40] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. [41] Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction. arXiv preprint arXiv:2508.02558, 2025. [42] Sorensen, Jones, Michael Heideman, and Burrus. Real-valued fast fourier transform algorithms. IEEE Transactions on acoustics, speech, and signal processing, 35(6):849863, 2003. [43] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. arXiv preprint arXiv:2203.05962, 2022. [44] Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025. [45] Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhĳian Liu, Pavlo Molchanov, Ping Luo, Song Han, and Enze Xie. Fast-dllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025. [46] Chengyue Wu, Hao Zhang, Shuchen Xue, Zhĳian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. [47] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. [48] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms. In Advances in Neural Information Processing Systems, 2024. [49] Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024. [50] Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, et al. Diffusion of thought: Chain-of-thought reasoning in diffusion language models. Advances in Neural Information Processing Systems, 37:105345105374, 2024. [51] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. [52] Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, and Danqi Chen. Longproc: Benchmarking long-context language models on long procedural generation. arXiv preprint arXiv:2501.05414, 2025. [53] Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. [54] Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. [55] Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. [56] Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, and Xipeng Qiu. Dirl: An efficient post-training framework for diffusion language models. arXiv preprint arXiv:2512.22234, 2025. [57] Yimeng Zhuang, Jing Zhang, and Mei Tu. Long-range sequence modeling with predictable sparse attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 234243, 2022."
        },
        {
            "title": "A Details of Method",
            "content": "The key hyperparameters of our FourierSampler for different dLLMs are shown in Table 5. ϵ ρ 1e-5 0.2 LLaDA1.5-8B LLaDA-8B-Instruct 1e-5 0.4 βmin βmax 0.6 0.4 0.6 0.4 Table 5 Hyper-parameters. As we have presented in Section 3, we introduce an adaptive weight βs based on the original decoding over the maximum probability confidence. At the decoding step s, it is calculated based on the variance σ2 qs,t in the prediction distribution at each masked position Ms. We record the σ2 over the past 20 decoding steps, compute the percentile ps of the current variance within the history, and linearly map it to the effective support interval [3, 3] of the standard normal distribution, then obtain smooth value ws (0, 1) through the cumulative distribution function of the normal distribution, F(x) = 1 . The pseudocode of the whole process is shown in Alg 1. 1 + erf (cid:16) 2 (cid:17)(cid:17) (cid:16) 2 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: Algorithm 1 Compute Adaptive Weight βs 1: procedure ComputeAdaptiveWeight 2: if not Ms.any() then return βmin (cid:16) {qs,t}tMs = Var σ2 v_list.append(cid:0)σ2(cid:1) if len(v_list) > 20 then (cid:17) v_list.pop(0) if len(v_list) == 0 then ps = 1 2 else v_list_ = [_ for _ in v_list if _ < σ2 )] ps = (cid:16) len (v_list_) len (v_list) ps 1 3 2 (cid:16) 1 + erf (cid:17) zs = (cid:16) zs ws = 1 βs = βmin + (1 ws) (βmax βmin) return βs, v_list 2 (cid:17)(cid:17) 2 14 Figure 6 Visualization of the correspondence between frequency-domain analysis and textual information."
        },
        {
            "title": "B Details of analysis",
            "content": "Beyond code generation tasks, we also conducted frequency-domain analysis on mathematical derivation passage concerning the difference of squares formula. As shown in Figure 6, narrative text appears as low-frequency components, while specific formulas and variables emerge as high frequency. This observation further corroborates our experimental findings in Section 3.1."
        }
    ],
    "affiliations": [
        "Fudan University",
        "OpenMOSS Team",
        "Shanghai Innovation Institute"
    ]
}