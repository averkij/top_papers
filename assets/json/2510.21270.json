{
    "paper_title": "Sparser Block-Sparse Attention via Token Permutation",
    "authors": [
        "Xinghao Wang",
        "Pengyu Wang",
        "Dong Zhang",
        "Chenkun Tan",
        "Shaojun Zhou",
        "Zhaoxiang Liu",
        "Shiguo Lian",
        "Fangxu Liu",
        "Kai Song",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (\\textbf{PBS-Attn}), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to $2.75\\times$ in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 7 2 1 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SPARSER BLOCK-SPARSE ATTENTION VIA TOKEN PERMUTATION Xinghao Wang1, Pengyu Wang1, Dong Zhang1, Chenkun Tan1, Shaojun Zhou1, Zhaoxiang Liu2, Shiguo Lian2, Fangxu Liu3, Kai Song3, Xipeng Qiu1,4 1Fudan University, 2China Unicom, 3ByteDance, 4Shanghai Innovation Institute"
        },
        {
            "title": "ABSTRACT",
            "content": "Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose O(N 2) complexity with respect to sequence length presents major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as promising solution that partitions sequences into blocks and skips computation for subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (PBS-Attn), plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permutedFlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to 2.75 in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn."
        },
        {
            "title": "INTRODUCTION",
            "content": "Modern Large Language Models (LLMs) have demonstrated remarkable proficiency in handling long-context tasks (OpenAI, 2025; Gemini Team, Google, 2025; Anthropic, 2025), capability fueled by advancements in infrastructure (Liu et al., 2023; Jin et al., 2024), training methodologies (Yang et al., 2025a), and novel positional embedding schemes (Su et al., 2023; Press et al., 2022; Peng et al., 2023). This progress enables models to process context windows spanning thousands or even millions of tokens, unlocking novel applications such as analyzing entire codebases, summarizing lengthy legal documents, and interpreting long-form video content. However, this extended capability is constrained by prohibitive memory and computational overheads. This bottleneck primarily stems from the self-attention mechanism within the Transformer architecture (Vaswani et al., 2023). The necessity for each token to attend to all other tokens results in computational complexity that scales quadratically with the input sequence length, posing fundamental challenge to scalable and accessible long-context processing. To address this challenge, researchers have proposed solutions from multiple perspectives. Architecturally, some approaches replace the standard quadratic attention with sub-quadratic alternatives, such as linear transformers (Katharopoulos et al., 2020; Yang et al., 2025d). Others substitute the attention mechanism entirely with alternatives like State Space Models (SSMs), which operate recurrently to process extremely long sequences with high efficiency (Gu & Dao, 2024; Dao & Gu, 2024; Yang et al., 2025c). Concurrently, hardware-aware optimizations, exemplified by FlashAttention (Dao et al., 2022), reduce memory overhead by tiling the sequence into blocks and performing an online softmax computation. This method avoids the materialization of the full attention ma-"
        },
        {
            "title": "Preprint",
            "content": "trix, thereby alleviating the memory overhead and efficiency constraints imposed by I/O limitations. Building directly upon this tiled approach, block-sparse attention further reduces computation by skipping the computation for certain blocks using pre-computed sparse block mask (Dao et al., 2022; Jiang et al., 2024; Lai et al., 2025; Xu et al., 2025; Zhang et al., 2025; Gao et al., 2025). This technique leverages the inherent sparsity of attention matrices, wherein most of the attention mass for given query is concentrated on small subset of key tokens. This property, particularly prominent in long sequences, allows for drastic reduction in computation without significantly compromising performance. While this block-level approach maximizes parallel efficiency, its rigidity can lead to sub-optimal sparsity pattern. This issue arises when the key tokens relevant to queries within single block are widely scattered, collectively spanning an unnecessarily large number of key blocks and thereby forcing redundant computation. Fortunately, the same token-wise computation that leads to quadratic complexity also presents an opportunity to mitigate it. The attention mechanism is permutation-invariant, meaning we can reorder the query and key sequences to achieve more favorable block-sparse structure and further improve block sparsity. Leveraging this insight, we propose Permuted Block-Sparse Attention (PBS-Attn), plug-and-play strategy that reorganizes query and key sequences to accelerate LLM prefilling. To accommodate causal attention for LLMs, we introduce novel segmented permutation strategy that preserves inter-segment causality while applying intra-segment permutation. Extensive experiments demonstrate that PBS-Attn increases block-level sparsity, yielding significant efficiency gains with minimal degradation in model performance. Specifically, powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to 2.75 in LLM prefilling, while maintaining performance close to the full attention baseline on real-world datasets like LongBench (Bai et al., 2024) and LongBenchv2 (Bai et al., 2025)."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Scaled Dot-Product Attention As the cornerstone of modern large language models, the attention mechanism facilitates dynamic synthesis of information by calculating weighted aggregation of value (V ) vectors. These weights, or attention scores, are determined by the dot-product similarity between given tokens query (Q) vector and the key (K) vectors of all other tokens in the sequence. This process allows the model to directly assess the relevance of every token relative to every other, enabling the effective capture of long-range dependencies, but at cost of quadratic complexity over the sequence length. Formally, the attention mechanism is given by: = softmax (cid:19) (cid:18) QKT Attention(Q, K, ) = AV (1) (2) where is the head dimension for multi-head attention and is the attention matrix. FlashAttention FlashAttention (Dao et al., 2022) employs tiled approach that partitions the input sequence into blocks and performs an online softmax computation. This strategy circumvents the materialization of the full attention matrix A, which significantly reduces memory overhead and improves efficiency for I/O-bound operations on GPUs. Formally, let the input query, key and value matrices be RN d, RM d, and RM and divide them into Tr = blocks with block size B(we use the same block size for and K/V for simple terminology), = [Q1, . . . , QTr ], = [K1, . . . , KTc], and = [V1, . . . , VTc]. For query block Qi, the computation for the corresponding output block Oi is defined by system of recursive equations over the key/value blocks = 1, . . . , Tc. The state at step is the triplet (O(j) = , and l(0) and local maximum = 0. For each step = 1, . . . , Tc, given the intermediate scores Sij = ). The state is initialized at = 0 with O(0) ij = row max(Sij), the state is updated from 1 to j: and Tc = = 0, m(0) QiKT , m(j) , l(j) m(j) em(j1) = max(m(j1) , ij) + row sum(exp(Sij m(j) m(j) + exp(Sij m(j) )Vj )) = l(j1) l(j) O(j) = O(j1) m(j) em(j1) (3) (4) (5)"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Illustration of causal attention without (Left) and with (Right) segmented permutation with = 1, = 4. Segmented permutation enhances block-level sparsity via intra-segment permutation while preserving inter-segment causality. By restricting computation of blocks within on-diagonal segments (green blocks), we can safely skip inter-segment blocks (yellow blocks) for block-sparse attention. After the final step, the output is normalized as Oi = diag (cid:16) (l(Tc) )1(cid:17) O(Tc) . Block-Sparse Attention Building upon the tiled computation of FlashAttention, block-sparse attention introduces further layer of optimization by selectively pruning block-wise interactions. This is achieved using predefined sparse block mask, {0, 1}TrTc. For any given query block Qi, the attention computation is only performed against key-value blocks Kj and Vj where the corresponding mask entry Mij = 1. If Mij = 0, the calculation of the score matrix Sij and the subsequent state update steps are entirely bypassed. Consequently, the state remains unchanged from the previous iteration; that is, (O(j) ) = (O(j1) , m(j1) , l(j1) , m(j) , l(j) ). i"
        },
        {
            "title": "3 PERMUTED BLOCK-SPARSE ATTENTION",
            "content": "3.1 PERMUTATION PROPERTIES OF ATTENTION The attention mechanism exhibits specific symmetries with respect to permutations of its inputs, which we formalize in the following lemmas. Lemma 3.1 (Key-Value Pair Permutation Invariance). The attention mechanism is invariant to the order of the source sequence, provided that the key-value pairings are maintained. Formally, let Pπ {0, 1}M be permutation matrix that reorders the rows of matrix according to permutation π on the index set {1, . . . , }. The following identity holds: Attention(Q, PπK, PπV ) = Attention(Q, K, ) Lemma 3.2 (Query Permutation Equivariance). The attention mechanism is equivariant with respect to permutations of the query sequence. Formally, let Pσ {0, 1}N be permutation matrix that reorders the rows of matrix according to permutation σ on the index set {1, . . . , }. The following relationship holds: (6) Attention(PσQ, K, ) = PσAttention(Q, K, ) (7) The proofs of Lemma 3.1 and 3.2 are provided in Appendix A.1 and A.2, respectively. Combining these properties, we arrive at general theorem for attention under simultaneous input permutations. detailed proof is provided in Appendix A.3."
        },
        {
            "title": "Preprint",
            "content": "(a) (b) Figure 2: Comparison of attention maps for Llama-3.1-8B (layer 0, head 30) on an 8K LongBench example, showing the pattern without (a) and with (b) segmented permutation. The red overlay indicates blocks selected for block-sparse attention, and the attention coverage is calculated as the total attention scores covered by the selected blocks. More visualizations are provided in Appendix D. Theorem 3.3 (Attention Permutation Invariance under Inverse Transformation). If the queries are permuted by Pσ and the key-value pairs are permuted by Pπ, the resulting output is permuted version of the original output. Applying the inverse of the query permutation recovers the original, unpermuted output. Formally: σ Attention(PσQ, PπK, PπV ) = Attention(Q, K, ) (8) Theorem 3.3 establishes that the query matrix and key matrix can be permuted by Pσ and Pπ σ to the output O. This respectively, provided that Pπ is also applied to the value matrix and property enables the rearrangement of the attention matrix A, without affecting the attention output. 3.2 SEGMENTED PERMUTATION FOR CAUSAL ATTENTION Motivated by Theorem 3.3, we explore whether its permutation properties can be leveraged to restructure the attention matrix. This rearrangement promises higher block sparsity and computational savings, particularly during the compute-bound prefill stage of inference. The primary objective is to co-locate the salient key tokens corresponding to queries from the same computational block, thereby enhancing block-level sparsity. However, critical challenge remains: maintaining causality post-permutation. Specifically, LLMs are trained with causal attention, which restricts queries to attending only to keys in preceding positions, resulting in lower-triangular attention matrix, A. During prefilling, blocks above the main diagonal are computationally redundant and can be skipped; consequently, the original block density for causal attention is Tc+1 . naive application of global permutation to the query and key se2Tc quences would dismantle this vital causal structure. Such permutation could scatter dependencies across the entire matrix, potentially transforming the sparse, lower-triangular structure into fully dense one (i.e., block density of 1). To address this challenge, we propose segmented permutation strategy that preserves inter-segment causality while applying intra-segment permutation, illustrated in Figure 1. Formally, we partition the initial N/S tokens of the input sequences Q, K, into = N/S non-overlapping, contiguous segments of size S. The remaining (mod S) tokens are left unpermuted. Let Qi, Ki, Vi RSd denote the i-th segment for {1, . . . , G}. For each segment i, we introduce local permutations, σi for queries and πi for keys, that reorder tokens within that segment. The global permutation operators, Pσ and Pπ, are then constructed as block-diagonal matrices from"
        },
        {
            "title": "Preprint",
            "content": "these respective local permutations. For the key permutation matrix Pπ: Pπ = diag(Pπ1 , . . . , PπG , IN (mod S)) = Pπ1 0 ..."
        },
        {
            "title": "0\nPπ2\n...\n0\n0",
            "content": "0 0 ... . . . PπG 0 0 0 ... 0 IN (mod S) (9) Here, each Pπi {0, 1}SS is the permutation matrix for the local key permutation πi, and IN (mod S) is the identity matrix corresponding to the last incomplete segment. The query permutation matrix Pσ is constructed analogously from its own set of local permutations, {σi}G i=1."
        },
        {
            "title": "3.3 QUERY-AWARE KEY PERMUTATION",
            "content": "The inherent sparsity of the attention mechanism implies that for any given query, small subset of key tokens accounts for most of the attention mass. prominent pattern within this distribution is that certain keys are consistently important across all queries, phenomenon widely recognized in the literature as Vertical Lines (Jiang et al., 2024; Lai et al., 2025; Xu et al., 2025). To maintain model accuracy, block-sparse attention must encompass all Vertical Lines. This constraint, however, can severely diminish block sparsity when these critical tokens are scattered throughout the sequence, spanning large number of blocks, as shown in Figure 2a. To leverage this sparse pattern, we introduce query-aware key permutation strategy. This method implements the permutation as an efficient, segment-wise sorting process. Within each segment, keys are sorted based on their estimated average attention scores using the last block of queries. Concretely, we first compute global importance score vector RN for all keys in the sequence using the last block of queries, Qlast block: (cid:18) = meanrows softmax (cid:18) Qlast blockKT (cid:19)(cid:19) (10) The local permutation πi for each segment is then obtained by sorting the keys within that segment based on in descending order: πi = argsort(s[(i1)S+1:iS]) (11) As shown in Figure 2b, this permutation strategy can effectively cluster the Vertical Lines thereby significantly improving block-level sparsity while maintaining attention coverage. 3.4 PERMUTED BLOCK-SPARSE ATTENTION The proposed permuted block-sparse attention (PBS-Attn) mechanism is detailed in Algorithm 1. The process commences by permuting the query, key, and value matrices. Subsequently, blocksparse mask, denoted as , is derived based on the permuted queries and keys. This mask, , governs the tiled attention computation by dictating which block-wise operations can be pruned. Finally, an inverse permutation is applied to restore the original ordering of the output, established by Theorem 3.3. For the block selection algorithm, we use simple strategy that utilizes mean pooling and block-wise attention to estimate the importance of each key block for each query block for the main method, where we detail in Appendix B.1. Crucially, we demonstrate that the sparsity improvements conferred by permutation are agnostic to the specific block selection algorithm, where we can combine the permutation with existing block selection algorithms to further improve block sparsity, as detailed in Appendix B.2."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETTINGS Models & Datasets We employ two state-of-the-art long-context LLMs, claiming support for available context lengths above 128K tokens: Llama-3.1-8B(128K) (Grattafiori et al., 2024) and"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Permuted Block-Sparse Attention Require: Q, K, RN d, permutation matrices Pσ, Pπ {0, 1}N , segment size S, block size Ensure: Permuted attention output RN and blocks Q PσQ, PπK, PπV into Tr = Divide K 1 , . . . , 1, . . . , Tc Tc BLOCK SELECTION(Q, K, B, S) Initialize 0; for = 1 to Tr do Load for = 1 to Tc do to SRAM; Initialize O(0) ; if Mi,j = 1 then Load j, Compute Compute l(j) Compute O(j) to SRAM; ij = / iK em(j1) = l(j1) = O(j1) , m(j) i else end if 1, . . . , Tr ; divide K, Apply permutation blocks into Tc = Select blocks, see Appendix 0, m(0) , l(0) 0; Compute attention only for selected blocks d, m(j) m(j) = max(m(j1) + row sum(exp(S m(j) + exp(S , row max(S ij m(j) ) ; ij m(j) ij)); )); em(j1) O(j) O(j1) m(j1) , l(j) l(j1) ; Skip computation diag((l(Tc) end for end for return σ )1) O(Tc) ; Write back to its rows in O; Reverse permutation Qwen-2.5-7B-1M(1M) (Yang et al., 2025a). We evaluate the sparse attention methods on two challenging real-world long-context datasets to validate their effectiveness in real-world scenarios: LongBench(Bai et al., 2024) and LongBenchv2(Bai et al., 2025). LongBench is collection of 21 long-context understanding tasks in 6 categories with mostly real-world data, with the average length of most tasks ranging from 5K to 15K. LongBenchv2 further scales the context length, ranging from 8K to 2M, covering various realistic scenarios. Baselines We evaluate PBS-Attn alongside set of strong baselines to validate its effectiveness. (1) Full Attention: The standard attention mechanism that computes the full attention matrix as the oracle method. Specifically, we use the FlashAttention (Dao et al., 2022) implementation. (2) Minference (Jiang et al., 2024): sparse attention method that performs offline attention pattern search, we utilize the official configuration for attention pattern setting. (3) FlexPrefill (Lai et al., 2025): block selection method for block-sparse attention that performs block selection based on the input and selects the attention pattern on the fly. We use γ = 0.95, τ = 0.1 as reported in the original paper. (4) XAttention (Xu et al., 2025): block selection method for block-sparse attention that selects blocks based on an antidiagonal scoring of blocks. We use threshold = 0.9, stride = 8 as reported in the original paper. (5) MeanPooling: This method uses mean pooling strategy on the unpermuted queries and keys to select blocks, which is the same selection method for PBSAttn(detailed in B.1). Our experiments shows that MeanPooling can serve as strong baseline when the first and the most recent key blocks are forcibly selected for each query block, due to the attention sink phenomenon (Xiao et al., 2024). We use selection threshold of 0.9 for MeanPooling. Implementation Details For PBS-Attn, we use block size of = 128 and segment size of = 256. The block selection threshold is set to 0.9 through all experiments. We implement custom permuted-FlashAttention kernel in Triton (Tillet et al., 2019) for efficient inference of PBSAttn. For model inference, we replace the prefilling process with PBS-Attn or baseline methods, while keeping the decoding process as in the original attention implementation. The experiments are conducted in computing environment with NVIDIA H100 80GB GPUs."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison of various sparse attention methods on LongBench. Bold and underlined scores indicate the best and second-best performing methods in each category, respectively, with the exception of the full attention baseline. Method Single-Doc QA Multi-Doc QA Summarization Few-shot Learning Code Synthetic Avg. 41.80 40.93 38.57 40.23 40.66 42.09 42.97 41.76 37.51 42.21 40.96 41. Llama-3.1-8B 17.79 17.72 17.78 17.86 17.85 17.72 Qwen-2.5-7B-1M 16.01 16.01 15.87 16.07 15.95 16.12 29.73 29.36 30.38 31.35 30.64 28.36 47.48 46.41 46.12 48.30 49.07 47.36 24.77 24.77 24.88 26.19 26.10 24.25 3.91 3.80 6.46 3.83 4.80 4.00 66.82 62.36 24.71 54.64 58.14 63. 67.50 66.50 26.67 63.33 40.83 66.33 38.28 37.06 30.56 36.42 36.67 37.37 37.01 36.21 28.51 36.26 31.83 36.37 Full MInference FlexPrefill XAttention MeanPooling PBS-Attn Full MInference FlexPrefill XAttention MeanPooling PBS-Attn 48.80 47.21 47.03 48.26 46.61 48. 44.21 42.82 38.44 43.82 39.39 43.01 4.2 MAIN RESULTS LongBench Table 1 presents performance comparison of various sparse attention methods on the LongBench benchmark, evaluated using the Llama-3.1-8B and Qwen-2.5-7B-1M models. As the results indicate, the unpermuted MeanPooling method already establishes strong baseline. Crucially, by incorporating our proposed permutation strategy, PBS-Attn significantly improves performance, surpassing other block-sparse attention methods and closely approaching the performance of the oracle full-attention baseline. PBS-Attn consistently achieves the best overall performance across both models, demonstrating its effectiveness and robustness. LongBenchv2 To rigorously evaluate the effectiveness of PBS-Attn in extreme long-context scenarios, we conducted experiments on the more challenging LongBenchv2 benchmark. The results, presented in Table 2, reveal that PBS-Attn exhibits minimal performance degradation compared to the full attention baseline while consistently surpassing other block-sparse attention methods. Notably, PBS-Attn consistently outperforms the unpermuted MeanPooling baseline. This advantage is particularly pronounced for the Qwen-2.5-7B-1M model, where permutation brings remarkable relative improvement of 31% in overall performance. Table 2: Performance comparison of various sparse attention methods on LongBenchv2. Bold and underlined scores indicate the best and second-best performing methods for each model, respectively, with the exception of the full attention baseline. Method Llama-3.1-8B Qwen2.5-7B-1M Full Minference FlexPrefill XAttention MeanPooling PBS-Attn 28.83 29.03 27.24 29.62 29.42 29.82 35.19 34.19 27.83 34.19 26.24 34.39 Efficiency Results To best evaluate the real-world practicality of the sparse attention methods, we measure the end-to-end time to first token (TTFT) on sequence lengths ranging from 8K to 512K. As shown in Figure 3, PBS-Attn achieves the highest speedup across all context lengths, whereas most competing methods only excel within limited range. For instance, Minference does not show speedup over FlashAttention until 128k, and the efficiency gains of XAttention stagnate after 128K. Although FlexPrefill matches the speedup of PBS-Attn in most cases, it suffers from significant quality drop as shown in Table 1 and 2. In contrast, PBS-Attn consistently delivers the best performance, reaching 2.75 end-to-end speedup at 256K, demonstrating its superior"
        },
        {
            "title": "Preprint",
            "content": "practicality and robustness. To analyze the permutation overhead in PBS-Attn, we further conduct detailed benchmarking study in Appendix C. Figure 3: Speedup of various methods relative to FlashAttention, measured by time to first token (TTFT) on LongBenchv2 across various sequence lengths. To accommodate longer sequences under memory constraints, we employ tensor parallelism with tp size of 2 and 8 for the 256K and 512K contexts, respectively. 4.3 ABLATION STUDIES AND ANALYSIS Effect of Permutation As illustrated in Figure 4, query-aware key permutation consistently increases block-level sparsity by noticeable margin. For instance, it achieves 7% absolute sparsity improvement at context length of 8K, and this gain continues to increase as the context length scales, highlighting the permutations effectiveness. Permutation Target Analysis To analyze the effect of permutation on queries, we propose keyaware query permutation approach. However, the attention distribution of queries over keys is often less structured than that of keys over queries. We therefore employ straightforward strategy that clusters queries which attend to similar keys within given segment. Specifically, we first compute set of centroids by calculating block-averaged keys, denoted as K. Each centroid is defined as Ki = MeanPool(K[(i1)B+1:iB]) for = 1, . . . , Tc. We then determine cluster assignments by computing the cosine similarity between each query and these centroids. Within each segment, queries are assigned greedily based on their similarity to the centroids. We evaluate the effect of the permutation target and order in Figure 5a. The results indicate that permuting both queries and keys brings no noticeable improvements, regardless of the order. Permuting queries offers marginal improvement over permuting keys in the performance-density trade-off, but it can be less efficient considering the overhead in models with Grouped-Query Attention (GQA) (Ainslie et al., 2023), which have multiple times more query heads than key heads. Accordingly, we exclusively adopt query-aware key permutation in our main method. Figure 4: Block-level density on various context lengths with and without permutation. relative sparsity improvement is calculated."
        },
        {
            "title": "Preprint",
            "content": "Effect of Segment Size Segment size plays crucial role in segmented permutation, where tokens are permuted within the corresponding segments to maintain inter-segment causality. Intuitively, larger segment size takes into account more tokens during sorting, thereby enhancing block-level sparsity; however, it would also include more blocks in the on-diagonal segments, which can not be skipped during computation to avoid breaking causality. Figure 5b illustrates how the segment size, S, affects the performance-density trade-off. larger flattens the trade-off curve, indicating that segmented permutation effectively clusters key tokens, allowing the model to maintain high performance even at high levels of block-level sparsity. However, this benefit diminishes at lower sparsity levels, as the wide on-diagonal segments contain large number of blocks that must be computed, limiting block-level sparsity. (a) Permutation Target (b) Segment Size Figure 5: LongBench score vs. average block-level density at context length of 32K."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Long-context LLMs As the capabilities of modern Large Language Models (LLMs) continue to advance, the expansion of their context length has become an inevitable and critical trend. longer context window enables models to process greater volume of information, which in turn facilitates more sophisticated context engineering (Mei et al., 2025) and agentic use (Wang et al., 2024). Extensive research has focused on extending the context length of LLMs through methods spanning data curation to post-training strategies (Liu et al., 2025). These advancements have culminated in models with context lengths up to millions of tokens (Yang et al., 2025a). Sparse Attention The quadratic growth in memory and computational requirements of the attention mechanism has been bottleneck for scaling LLM context lengths. Sparse attention has emerged as promising solution, leveraging the inherent sparsity in attention patterns to drastically reduce this overhead. These methods can accelerate different stages of inference, such as prefilling, decoding, or both. StreamingLLM (Xiao et al., 2024) first identifies the attention sink phenomenon in LLMs, proposing to capture majority of the attention mass with initial and recent tokens. NSA (Yuan et al., 2025) and MoBA (Lu et al., 2025) further incorporate sparse attention into the training stage, accelerating both prefilling and decoding. Methods like H2O (Zhang et al., 2023), can accelerate the decoding speed by exploiting the attention pattern after prefilling. Closely related to this work, various methods are proposed to accelerate the compute-bounded prefilling process. For example, Minference (Jiang et al., 2024) recognizes attention patterns in pre-computed manner. More recent works tend to perform attention pattern recognition on-the-fly. For instance, FlexPrefill (Lai et al., 2025) utilizes divergence to classify the attention pattern, XAttention (Xu et al., 2025) adopts an antidiagonal scoring metric to weight each block, and SpargeAttention (Zhang et al., 2025) accounts the intra-block similarity into the selection criterion. However, these methods primarily focus on developing better block selection algorithms, while our work is orthogonal: we focus on rearranging the attention matrix to create structure that inherently increases block-level sparsity. Attention with Token Permutation Concurrent with our work, methods like SVG2 (Yang et al., 2025b) and PAROAttention (Zhao et al., 2025) show promise in accelerating visual generation models like Diffusion Transformers (Peebles & Xie, 2023), but their reliance on bidirectional attention"
        },
        {
            "title": "Preprint",
            "content": "makes them incompatible with the causal constraints of auto-regressive LLMs. PBS-Attn accelerates this by introducing segmented permutation strategy, explicitly preserving inter-segment causality."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we formalize the permutation properties of the attention mechanism and leverage them to improve block-level sparsity. We introduce Permuted Block-Sparse Attention (PBS-Attn), plug-and-play method that employs novel segmented permutation strategy to preserve intersegment causality while reordering tokens within each segment. Our method achieves an end-to-end prefilling speedup of up to 2.75 with minimal performance degradation, demonstrating promising path toward more efficient long-context LLMs."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "Our work focuses on improving the computational efficiency of large language models. We believe this research carries positive ethical implications. By reducing the computational resources required for processing long sequences, our method contributes to lowering the energy consumption and carbon footprint associated with training and deploying large-scale AI models. This can also enhance the accessibility of advanced AI technologies, enabling researchers and developers with limited resources to contribute to the field and innovate responsibly."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our work, we commit to making our research as transparent and accessible as possible. Code All code used for our experiments, including the implementation of Permuted Block-Sparse Attention (PBS-Attn) and the custom permuted-FlashAttention kernel, will be made publicly available after the reviewing period. The repository will include scripts to run the evaluations and detailed instructions for setup. Models The experiments were conducted using publicly available state-of-the-art large language models: Llama-3.1-8B (128K) and Qwen-2.5-7B-1M (1M), available for downloading from platforms like HuggingFace. Datasets We used two publicly available and widely recognized benchmarks for long-context language understanding: LongBench and LongBenchv2, available for downloading from platforms like HuggingFace. Experimental Setup All experiments were performed on NVIDIA H100 80GB GPUs. Key implementation details are stated in Section 4.1."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https://arxiv.org/abs/2305.13245. Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4, May 2025. URL https:// www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47. pdf. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding, 2024. URL https://arxiv.org/abs/ 2308.14508. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks, 2025. URL https://arxiv.org/abs/ 2412.15204. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL https://arxiv.org/abs/2405.21060. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms, 2025. URL https://arxiv.org/abs/2410.13276. Gemini Team, Google. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities, 2025. URL https://storage. googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,"
        },
        {
            "title": "Preprint",
            "content": "Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo"
        },
        {
            "title": "Preprint",
            "content": "Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention, 2024. URL https://arxiv.org/abs/2407.02490. Yibo Jin, Tao Wang, Huimin Lin, Mingyang Song, Peiyang Li, Yipeng Ma, Yicheng Shan, Zhengfan Yuan, Cailong Li, Yajing Sun, Tiandeng Wu, Xing Chu, Ruizhi Huan, Li Ma, Xiao You, Wenting Zhou, Yunpeng Ye, Wen Liu, Xiangkun Xu, Yongsheng Zhang, Tiantian Dong, Jiawei Zhu, Zhe Wang, Xijian Ju, Jianxun Song, Haoliang Cheng, Xiaojing Li, Jiandong Ding, Hefei Guo, and Zhengyong Zhang. P/d-serve: Serving disaggregated large language model at scale, 2024. URL https://arxiv.org/abs/2408.08147. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. URL https://arxiv. org/abs/2006.16236. Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference, 2025. URL https://arxiv. org/abs/2502.20766. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context, 2023. URL https://arxiv.org/abs/2310.01889. Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, and Zhaoxiang Zhang. comprehensive survey on long context language modeling, 2025. URL https://arxiv.org/abs/2503.17407. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms, 2025. URL https://arxiv.org/abs/2502.13189. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, and Shenghua Liu. survey of context engineering for large language models, 2025. URL https://arxiv. org/abs/2507.13334. OpenAI. GPT-5 System Card, August 2025. URL https://cdn.openai.com/ gpt-5-system-card.pdf. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071."
        },
        {
            "title": "Preprint",
            "content": "Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. URL https://arxiv.org/abs/2108.12409. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Philippe Tillet, Hsiang-Tsung Kung, and David D. Cox. Triton: an intermediate language and compiler for tiled neural network computations. Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, 2019. URL https: //api.semanticscholar.org/CorpusID:184488182. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv. org/abs/1706.03762. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024. ISSN 2095-2236. doi: 10.1007/s11704-024-40231-1. URL http://dx.doi.org/ 10.1007/s11704-024-40231-1. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. URL https://arxiv.org/abs/2309.17453. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring, 2025. URL https://arxiv.org/abs/2503.16428. An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report, 2025a. URL https://arxiv.org/abs/2501.15383. Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, Jianfei Chen, Song Han, Kurt Keutzer, and Ion Stoica. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation, 2025b. URL https://arxiv.org/abs/2505.18875. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule, 2025c. URL https://arxiv.org/abs/2412.06464. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length, 2025d. URL https://arxiv.org/abs/ 2406.06484. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention, 2025. URL https://arxiv.org/abs/2502.11089. Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattention: Accurate and training-free sparse attention accelerating any model inference, 2025. URL https://arxiv.org/abs/2502.18137. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023. URL https: //arxiv.org/abs/2306.14048. Tianchen Zhao, Ke Hong, Xinhao Yang, Xuefeng Xiao, Huixia Li, Feng Ling, Ruiqi Xie, Siqi Chen, Hongyu Zhu, Yichong Zhang, and Yu Wang. Paroattention: Pattern-aware reordering for efficient sparse and quantized attention in visual generation models, 2025. URL https: //arxiv.org/abs/2506.16054."
        },
        {
            "title": "A PROOFS OF PERMUTATION PROPERTIES",
            "content": "A.1 PROOF OF LEMMA 3.1 Lemma A.1 (Key-Value Pair Permutation Invariance). The attention mechanism is invariant to the order of the source sequence, provided that the key-value pairings are maintained. Formally, let Pπ {0, 1}M be permutation matrix that reorders the rows of matrix according to permutation π on the index set {1, . . . , }. The following identity holds: Attention(Q, PπK, PπV ) = Attention(Q, K, ) (12) Proof. Let = Attention(Q, K, ) and = Attention(Q, PπK, PπV ). Our goal is to show that = O. We will prove this by showing that their corresponding row vectors, oi and i, are equal for any arbitrary row index {1, . . . , }. Let = QKT i-th row of the original output is given by: and = softmax(A)(we use instead of as in Eq.1 to avoid confusion) The oi = (cid:88) j=1 Wijvj Now, let = PπK and = PπV . The score matrix for is = Q(K)T AP = QKT π = π . Let = softmax(A). ij = (cid:80)M The (i, j)-th element of is softmax computation on the i-th row of is: l=1 Ail(P π )lj = Ai,π1(j). The denominator for the (cid:88) l=1 exp(A il) = (cid:88) l=1 exp(Ai,π1(l)) Since π1 is bijection on {1, . . . , }, this summation is reordering of the terms (cid:80)M which is the denominator for the i-th row of the original weights . Thus, the (i, j)-th element of the new weight matrix is: k=1 exp(Aik), ij = exp(A ij) l=1 exp(A il) (cid:80)M = exp(Ai,π1(j)) (cid:80)M k=1 exp(Aik) = Wi,π1(j) The i-th row of the new output is weighted sum of the rows of = PπV . The j-th row of is = vπ1(j). Therefore: (cid:88) = ijv = (cid:88) Wi,π1(j)vπ1(j) j=1 Let = π1(j). Since π1 is bijection, summing over all {1, . . . , } is equivalent to summing over all {1, . . . , }. By this change of variables, we have: j=1 = (cid:88) k=1 Wikvk = oi Since = oi for an arbitrary i, the matrices and are identical. A.2 PROOF OF LEMMA 3. Lemma A.2 (Query Permutation Equivariance). The attention mechanism is equivariant with respect to permutations of the query sequence. Formally, let Pσ {0, 1}N be permutation matrix that reorders the rows of matrix according to permutation σ on the index set {1, . . . , }. The following relationship holds: Attention(PσQ, K, ) = PσAttention(Q, K, ) (13)"
        },
        {
            "title": "Preprint",
            "content": "Proof. Let = Attention(Q, K, ) and = Attention(PσQ, K, ). We want to show that = PσO. Let = QKT (PσQ)KT = Pσ and = softmax(A), such that = . The score matrix for is = (cid:16) QKT = PσA. Let = softmax(A). (cid:17) The softmax function operates independently on each row. Let (X)i denote the i-th row of matrix X. Left-multiplication by Pσ permutes the rows of A, such that the i-th row of is the σ1(i)-th row of A: (A)i = (A)σ1(i). Applying the softmax function, the i-th row of is: (W )i = softmax((A)i) = softmax((A)σ1(i)) This resulting vector is identical to the σ1(i)-th row of the original weight matrix . Thus, (W )i = (W )σ1(i). This equality for all rows implies that the entire matrix is rowpermuted version of , i.e., = PσW . Now we can write the output as: = = (PσW )V By the associativity of matrix multiplication, we have: = Pσ(W ) = PσO This completes the proof. A.3 PROOF OF THEOREM 3.3 Theorem A.3 (Attention Permutation Invariance under Inverse Transformation). If the queries are permuted by Pσ and the key-value pairs are permuted by Pπ, the resulting output is permuted version of the original output. Applying the inverse of the query permutation recovers the original, unpermuted output. Formally: σ Attention(PσQ, PπK, PπV ) = Attention(Q, K, ) (14) Proof. We prove the theorem by showing that the left-hand side (LHS) of the equation simplifies to the right-hand side (RHS) through sequential application of the preceding lemmas. σ Attention(PσQ, PπK, PπV ) σ Attention(PσQ, K, ) σ (Pσ Attention(Q, K, )) σ Pσ) Attention(Q, K, ) LHS = = = = (P = Attention(Q, K, ) = Attention(Q, K, ) = RHS by Lemma 3.1 by Lemma 3.2 by associativity since Pσ is orthogonal The final expression is identical to the right-hand side, which concludes the proof."
        },
        {
            "title": "B BLOCK SELECTION",
            "content": "B.1 BLOCK SELECTION IN PBS-ATTN We use mean pooling strategy and block-wise attention to estimate the importance of each key block. This method is also used for unpermuted sequences, serving as strong baseline denoted as MeanPooling in the main paper. Here we detail the implementation of MeanPooling selection in Algorithm 2. Note that for the baseline MeanPooling, and remain unpermuted as = and = K. The causal mask is upper triangular matrix with entries set to . If segmented permutation is applied, this mask also includes the on-diagonal segments (as in Figure 1), to ensure valid intra-segment attention post-permutation."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 2 MeanPooling Block Selection Require: Query matrix RN d, Key matrix RN d, block size B, attention score threshj=1, where Tr = Tc = N/B. i=1, {K i) for = 1, . . . , Tr. j) for = 1, . . . , Tc. old τ , causal mask {0, }N/BN/B. Ensure: Block selection mask {0, 1}N/BN/B. j}Tc i}Tr 1: Divide Q, into blocks of size B: {Q 2: Compute pooled queries: Qi = MeanPool(Q 3: Compute pooled keys: Kj = MeanPool(K 4: Form pooled matrices RTrd and RTcd. 5: Compute block scores: Sblock = softmax( KT / 6: Initialize = 0. 7: for = 1 to Tr do 8: 9: 10: 11: 12: 13: 14: end for 15: return . Get scores for query block i: ai = Sblock[i, 1 : i]. Sort scores and get original indices: oi = argsort(ai). Compute cumulative sum on sorted scores: ci = cumsum(ai[oi]). Find number of blocks to select: = min({j ci[j] τ } {i}). Get indices of blocks to select: = oi[1 : k]. Set [i, j] = 1 for all . + C). B.2 PBS-ATTN WITH EXISTING BLOCK SELECTION ALGORITHMS In the main paper, we use simple mean pooling strategy for block selection in block-sparse attention, as detailed in Section B.1, and show that permutation can increase block-level sparsity under this naive mean pooling strategy (Section 4.3). In this section, we further demonstrate that advanced block selection algorithms (e.g. XAttention) can also benefit from permutation. Figure 6: Longbench score vs. average block-level density at context length of 32k of XAttention selection with and without permutation. As shown in Figure 6, XAttention selection can also benefit from the sparsity improvements of permutation, achieving better trade-off between performance and sparsity."
        },
        {
            "title": "C ANALYSIS ON THE PERMUTATION OVERHEAD",
            "content": "As shown in Figures 7a and 7b, the permutation overhead in PBS-Attn is negligible compared to the main attention computation time, especially at longer context lengths. For instance, at context length of 128K, permutation introduces an overhead of only 4% relative to the block attention computation time and just 1.3% compared to FlashAttention. While permuting queries introduces slightly higher overhead than permuting keys, this difference diminishes as the context length increases. However, query permutation can also result in lower block-level sparsity than key permutation under the same settings, leading to higher attention computation time."
        },
        {
            "title": "Preprint",
            "content": "(a) Query-aware Key Permutation. (b) Key-aware Query Permutation. Figure 7: Detailed benchmarking results of PBS-Attn vs. FlashAttention."
        },
        {
            "title": "D VISUALIZATION OF PERMUTATION",
            "content": "In this section, we provide more visualizations of the permutation effect on both Llama-3.1-8B (Figure 8) and Qwen-2.5-7B-1M (Figure 9)."
        },
        {
            "title": "E USE OF LARGE LANGUAGE MODELS",
            "content": "During the preparation of this work, we utilized large language models (LLMs) to assist with code development and manuscript writing. Specifically, their applications included improving the grammar and clarity of the text, as well as assisting in code completion."
        },
        {
            "title": "Preprint",
            "content": "(a) Layer 1, Head 13 (b) Layer 10, Head 26 (c) Layer 16, Head 9 (d) Layer 28, Head 28 Figure 8: Permutation visualizations of Llama-3.1-8B. (a) Layer 0, Head (b) Layer 7, Head 22 (c) Layer 22, Head 5 (d) Layer 26, Head 20 Figure 9: Permutation visualizations of Qwen-2.5-7B-1M."
        }
    ],
    "affiliations": [
        "ByteDance",
        "China Unicom",
        "Fudan University",
        "Shanghai Innovation Institute"
    ]
}