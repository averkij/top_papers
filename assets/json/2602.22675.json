{
    "paper_title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization",
    "authors": [
        "Qianben Chen",
        "Tianrui Qin",
        "King Zhu",
        "Qiexiang Wang",
        "Chengjun Yu",
        "Shu Xu",
        "Jiaqi Wu",
        "Jiayu Zhang",
        "Xinpeng Liu",
        "Xin Gui",
        "Jingyi Cao",
        "Piaohong Wang",
        "Dingfeng Shi",
        "He Zhu",
        "Tiannan Wang",
        "Yuqing Wang",
        "Maojia Song",
        "Tianyu Zheng",
        "Ge Zhang",
        "Jian Yang",
        "Jiaheng Liu",
        "Minghao Liu",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \\emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy."
        },
        {
            "title": "Start",
            "content": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization OPPO AI Agent Team"
        },
        {
            "title": "Abstract",
            "content": "Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose Search More, Think Less (SMTL), framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6%), GAIA (75.7%), Xbench (82.0%), and DeepResearch Bench (45.9%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7%, while improving accuracy. Date: February 27, 2026 Open Source: Code Correspondence: Wangchunshu Zhou at zhouwangchunshu@oppo.com Models ı Datasets 6 2 0 2 6 2 ] . [ 1 5 7 6 2 2 . 2 0 6 2 : r (a) Efficiency on BrowseComp: score vs. interaction steps. (b) Generalization across multiple benchmarks. Figure 1 Overview of SMTL Performance. (a) Efficiency on BrowseComp. All methods are evaluated with their default inference settings. (b) Generalization across benchmarks. Comprehensiveness, depth, instruction following, and readability are measured following DeepResearch Bench (Du et al., 2025a). We report our models performance for Deep xxxxx"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in deep research agents suggest that increasing reasoning depth and the number of tool calls can substantially improve task performance (Jin et al., 2025; Li et al., 2025d,e; Qiu et al., 2025; Roucher et al., 2025; Sun et al., 2025; Tang et al., 2025b; Wu et al., 2025b; Zhang et al., 2025; Zheng et al., 2025; Zhou et al., 2023, 2024; Zhu et al., 2025a,b). However, longer reasoning trajectories also lead to higher inference latency. Balancing long-horizon search performance and computational efficiency remains an open problem. (Chen et al., 2025; MiroMind Team, 2025; Tang et al., 2025a; Tongyi DeepResearch Team, 2025). To train agent models that can perform efficient long-horizon reasoning, key question is: How can we design tasks that require efficient long-horizon reasoning, and construct suitable training trajectories to teach such behavior? In addition, generalization across diverse task objectives and evaluation criteria remains challenging. Existing agentic search tasks can be roughly divided into two types. The first includes deterministic question-answering tasks with clear ground-truth answers, such as BrowseComp (Wei et al., 2025), GAIA (Mialon et al., 2023), and WebWalker (Wu et al., 2025c), where performance is mainly measured by accuracy. The second type focuses on open-ended research problems without single correct answer, such as DeepResearch Bench (Du et al., 2025a) and DeepResearchGym (Coelho et al., 2025), where evaluation emphasizes information coverage, coherence, and synthesis quality. These two settings have very different optimization objectives. As result, agents trained for one setting may struggle to generalize to the other, making it difficult to train single agent that performs well across both. In this work, we revisit long-horizon agentic search from the perspectives of efficiency and generalization. We argue that the main scalability bottleneck of existing deep research agents lies in their reliance on linear, sequential reasoning in search tasks. To address this, we propose an agentic framework that replaces sequential reasoning with parallel task decomposition and concurrent tool execution, together with structured context management for efficient long-horizon inference under constrained context budgets. At the data level, we introduce an automated pipeline that synthesizes representative multi-type search tasks, reducing redundant samples and improving generalization. Trained end-toend with supervised fine-tuning and reinforcement learning, our approach achieves state-of-the-art performance on BrowseComp (44.3), while substantially improving efficiency by reducing the number of reasoning steps by 78% and inference latency by up to 2.6. Our contributions are summarized as follows: Parallel agentic workflow. We propose unified agentic framework that replaces sequential reasoning with parallel evidence acquisition, using plan-driven context management to achieve efficient long-horizon search under constrained context budgets. Generalized data construction. We introduce an automated data pipeline that constructs representative multi-type search tasks across both deterministic and open-ended settings, supporting generalization in long-horizon agentic search. State-of-the-art performance. Our method achieves state-of-the-art results on multiple deep search and deep research benchmarks, while substantially improving efficiency in terms of reasoning steps and inference latency."
        },
        {
            "title": "2 Related Work",
            "content": "Agent Frameworks and Systems. Agentic workflows augment large language models (LLMs) with planning, multi-step tool use, and iterative environment interaction, and have become the dominant paradigm for search-intensive tasks. mainstream line of work relies on external orchestration, where controller decomposes queries into subgoals, schedules web search, browsing, and code-execution tools, and aggregates intermediate findings through predefined procedures. This paradigm underlies recent commercial deep research systems that combine strong proprietary backbones with multi-step web exploration, plan refinement, and long-context memory to maintain intermediate evidence (Anthropic, 2025; Google, 2024; OpenAI, 2025; Perplexity Team, 2025). In parallel, structured agentic frameworks such as WebWeaver (Li et al., 2025f) and OAgents (Zhu et al., 2025a) adopt plannerresearcher or plannerexecutor workflows to improve robustness in open-ended research and verification tasks. The open-source ecosystem further provides reusable orchestration templates, including plan-and-execute pipelines and hierarchical agent systems (LangChain, 2025; SkyworkAI, 2025; Together AI, 2025), with MiroFlow consolidating these patterns into benchmark-oriented research-agent framework (MiroMind AI Team, 2025). Multi-agent workflows additionally introduce division of labor 2 through specialized roles and coordination cycles (Chen et al., 2023; Fourney et al., 2024; Hong et al., 2024; Hu et al., 2025; Li et al., 2023; Qian et al., 2023; Roucher et al., 2025). Despite architectural diversity, most existing agentic workflows implicitly scale performance by deepening sequential reasoning and expanding interaction horizons. As consequence, this paradigm often leads to limited information efficiency, as substantial computation is devoted to prolonged model-side reasoning rather than effective external evidence acquisition. Synthetic Data Pipelines. High-quality training data is critical for search agents, yet collecting multi-step, toolinteractive trajectories at scale remains costly. Recent synthetic data pipelines can be broadly categorized into two approaches. The first is graph-based generation, exemplified by WebSailor (Li et al., 2025a), which constructs knowledge graphs from seed entities using web tools and samples complex questionanswer pairs or trajectories from subgraphs. The second approach follows an easy-to-hard expansion paradigm, where simple seed questions are progressively expanded into longer-horizon problems with predominantly tree-structured logic, as seen in WebShaper, ASearcher, and WebExplorer (Gao et al., 2025; Liu et al., 2025c; Tao et al., 2025a). Beyond web-centric pipelines, TaskCraft expands atomic tasks along both depth and width dimensions and applies incremental validation mechanisms, thereby improving data quality and controllability (Shi et al., 2025). Overall, existing synthetic pipelines highlight the effectiveness of tool-in-the-loop generation, while primarily emphasizing task difficulty or context length rather than explicitly shaping information-efficient search and verification behaviors. Besides, these pipelines are primarily designed around deterministic question answering or tightly constrained task structures, and provide limited support for synthesizing open-ended research tasks that require flexible information aggregation and cross-source validation."
        },
        {
            "title": "3 Parallel Agentic Workflow",
            "content": "Recent tool-augmented agents enable language models to interact with external tools for retrieval and reasoning (Li et al., 2025e; Xue et al., 2025; Yao et al., 2023). Building on this line of work, we design an efficient parallel agentic workflow inspired by Flash-Searcher (Qin et al., 2025), which explicitly supports concurrent execution and structured coordination across subtasks. As illustrated in Figure 2, the agent initializes task plan, executes multiple subtasks in parallel, and periodically synchronizes intermediate results through plan refinement before producing the final answer. Figure 2 Overview of our parallel agentic workflow design. Initial Plan Construction. Given composite search task, the agent first constructs an initial task plan G0 plan by decomposing the problem into set of interrelated yet partially independent subtasks. Each subtask corresponds to concrete information-seeking or verification objective, such as retrieving facts, validating relations, or gathering evidence. The plan is generated prior to any tool execution and is designed to expose parallelizable execution paths early, enabling concurrent evidence acquisition and higher information density. Rather than relying on single sequential reasoning chain, G0 plan provides structured starting point that supports parallel execution and subsequent refinement. 3 Parallel Execution and Tool Coordination. At each timestep, the system selects ready-to-execute subtasks from the pending set Pt based on their readiness, while completed subtasks are tracked separately. These pending subtasks are processed concurrently, leveraging available tools or agent actions to gather information and execute reasoning tasks. By executing multiple pending subtasks in parallel, the system accelerates task completion and reduces sequential bottlenecks. The system aggregates observations from each parallel execution into unified reasoning state: st+1 = F(st, {a(k) }m k=1, {o(k) }m k=1), (1) where a(k) and o(k) reasoning state at timestep t. represent the actions and observations of the k-th parallel execution, and st denotes the aggregated In practice, parallel execution is realized via limited set of reusable external tools (Appendix A), including web search and page crawling, which are repeatedly invoked across pending subtasks to facilitate concurrent information acquisition and verification. Dynamic Plan Refinement. To ensure the plan adapts to the ongoing execution, the task plan is periodically updated. Completed subtasks are removed, unresolved dependencies are rechecked, and new subtasks may be introduced. The task plan is refined based on the current execution state: Gt+ plan = R(Gt plan, Ct, Pt, st), where Ct represents the completed subtasks. This dynamic refinement ensures that the task adapts to progress and maintains efficiency. Algorithm 1 Parallel agentic workflow. plan, completed subtasks C0 5: plan D(T ) Require: Composite task 1: Construct initial task plan G0 2: Initialize reasoning state s0, pending subtasks P0 G0 3: while Pt do 4: Select executable subtasks Et Pt Execute subtasks in Et in parallel using available tools Update reasoning state st+1 = F(st, {a(k) 6: 7: Mark completed subtasks and update Ct+1 Remove completed subtasks from Pt+1 8: if mod = 0 then 9: 10: 11: 12: 13: end while 14: Return final reasoning state sT Refine task plan using current execution state Gt+1 plan, Ct+1, Pt+1, st+1) plan R(Gt }, {o(k) }) end if Algorithmic Outline. The workflow design is summarized in Algorithm 1. This method continuously refines the task plan while executing multiple subtasks in parallel, ensuring efficient task completion through dynamic updates and parallel execution."
        },
        {
            "title": "4 Data Construction",
            "content": "Existing agent data construction pipelines limit both generalization and efficiency. Many rely on static knowledge sources and focus on deterministic, entity-centric Deep Search tasks, providing weak coverage of open-ended Deep Research scenarios. Moreover, task difficulty is often scaled by increasing reasoning hops rather than improving information density, leading to redundant evidence and inefficient interaction traces. To address these issues, we propose high-diversity, high-density data synthesis pipeline. As illustrated in Fig. 3, our framework integrates raw corpus collection, graph construction, subgraph extraction, and QA generation with verification, enabling dense and correlated evidence acquisition for both deterministic and open-ended research tasks. 4 Figure 3 Overview of the data construction pipeline."
        },
        {
            "title": "4.1 Deep Search Data",
            "content": "Raw Corpus Collection. To support cross-domain generalization and multi-hop reasoning, we construct the raw corpus by leveraging trajectories from TaskCraft corpus (Shi et al., 2025). These trajectories contain rich collections of real-world URLs spanning diverse domains, including art, sports, history, government, economics, politics, music, geography, movies, computer science, physics, and chemistry. Crucially, URLs within each trajectory are not independent: they are connected through explicit information-seeking paths, where later queries and sources build upon evidence gathered from earlier ones. This structure naturally induces multi-hop relationships across documents, making the collected corpus well suited for graph-based task construction. We extract and normalize the content from these URLs using document parsing tool Jina (Jina, 2025), yielding large-scale, high-diversity raw corpus that preserves both domain coverage and inter-document relational structure. Graph Network Construction. Based on the initial corpus, we develop an efficient pipeline for generating sophisticated graph networks. First, we leverage LightRAG (Guo et al., 2024) to instantiate knowledge-driven graph network. This process involves partitioning the curated text crops into multiple chunks, from which an LLM extracts entities and their respective attributes. By integrating an embedding-plus-reranker retrieval mechanism, we recall pertinent chunks, enabling the LLM to synthesize detailed node descriptions and delineate complex inter-node relationships, ultimately culminating in highly intricate graph network. Subgraph Extraction. Given the constructed knowledge graph, we extract task-specific subgraphs using controlled random-walk strategy. For each task, we sample target entity as the ground-truth answer and perform breadth-first search (BFS) up to hops to collect its surrounding neighborhood. The resulting subgraph defines the supporting evidence structure required to infer the answer, where multi-hop nodes serve as question conditions with varying degrees of indirection. By adjusting the hop depth and branching factor, we flexibly control task difficulty while preserving semantic coherence and factual correctness. This construction yields compact, information-dense task skeletons that require integrating multiple related evidence sources rather than relying on single-hop retrieval. 5 To ensure high-quality and non-degenerate task structures, we adopt the following design principles: Rich topological structures. We prioritize subgraphs in which two (N +1)-hop nodes are interrelated while sharing common -hop parent, inducing cyclic dependencies that require cross-validation of multiple relationships. Controlled walk width and depth. We explicitly bound the depth and branching factor to keep task difficulty scalable and avoid trivial shortcuts or excessively long reasoning chains. Hierarchical Question Construction and Verification. Given task-specific subgraph with fixed target answer, we construct questions through hierarchical synthesis process. Starting from the outermost -hop frontier, we iteratively aggregate information from (i+1)-hop nodes to form sub-questions about i-hop entities. Each aggregation step yields valid intermediate question, and progressively merging all layers produces final question about the target entity that requires the maximum hop depth and reasoning difficulty. When multiple (i+1)-hop nodes exhibit semantic relationships, we explicitly encode these interdependencies as verifiable conditions, requiring agents to cross-validate parallel evidence paths rather than rely on linear reasoning. To prevent information leakage, we apply an LLM-based verification step after each synthesis iteration; if the answer can be inferred prematurely, the question is restructured or relevant information is obfuscated. This process repeats until the desired difficulty is achieved or maximum of five iterations is reached. The complete designs of the prompts for entity information extraction, entity evaluation, entity question generation and obfuscation, and verification are described in Section B.1."
        },
        {
            "title": "4.2 Deep Research Data",
            "content": "Research Question Construction. Deep research tasks are synthesized entirely within our unified data construction pipeline, without relying on externally curated queries. Given task-specific subgraph with fixed target entity and its multi-hop supporting structure, we formulate open-ended research questions that require integrating evidence across the entire subgraph. These questions are designed to elicit report-style answers involving explanation, comparison, and synthesis across multiple sources, rather than single factual outputs. This construction explicitly encourages long-horizon planning, information aggregation, and cross-source reasoning. Trajectory Generation and Quality Filtering. For each open-ended research question, we generate multiple candidate trajectories using our parallel agentic workflow and apply two-stage filtering process to ensure high-quality supervision. First, trajectories are subjected to rule-based hard rejection to remove structurally invalid or overly shallow solutions, enforcing basic requirements on completeness, context validity, reasoning depth, tool usage, and format consistency. Second, trajectories that pass these checks are then evaluated by an LLM-as-a-Judge, which assesses higher-level semantic quality, including comprehensiveness, insight/depth, instruction following, and readability. Only trajectories that satisfy both structural constraints and semantic criteria are retained as reference outputs, yielding scalable and reliable training data for open-ended deep research tasks. The complete designs of the prompts for open-ended question construction and quality filtering are illustrated in Section B.2."
        },
        {
            "title": "5.1 Post-training Supervised Fine-tuning.",
            "content": "We perform supervised fine-tuning (SFT) to initialize the agent with stable and efficient search behaviors before reinforcement learning. Task Composition. The SFT dataset consists of two task categories: Deep Search and Deep Research, which differ in supervision form while sharing the same underlying subgraph-based construction. Deep Search. Tasks are instantiated from task-specific subgraphs with hop depth ranging from 2 to 5. For each subgraph, all hierarchical question variants constructed during iterative aggregation are retained, yielding multiple questions that share the same target entity as the ground-truth answer. To prevent over-representation of frequent answers, we apply an answer frequency threshold and discard tasks whose target entities appear excessively often. 6 Deep Research. For each subgraph, we construct an open-ended research question centered on the target entity and its multi-hop supporting structure. Questions are formulated to encourage broad exploration and synthesis over the entire subgraph, rather than single-answer retrieval, ensuring sufficient topical richness and variability. Trajectory Construction and Curation. Training trajectories are generated using the agentic workflow described in Section 3. For Deep Search tasks, supervision is obtained by distilling trajectories generated by DeepSeek-V3.2 (Liu et al., 2025a), while Deep Research trajectories are distilled from GPT-5 (OpenAI, 2025), reflecting its stronger long-form synthesis capabilities. To ensure high-quality supervision, we apply the following curation criteria: Context length constraint. The total trajectory length is capped at 64K tokens to reduce redundant interactions and noisy supervision. Interaction efficiency constraint. The average number of tool calls per step must be no less than 3, encouraging active information acquisition. Trajectory efficiency optimization. For tasks with multiple successful trajectories, we retain only those that are correct and shortest in interaction length."
        },
        {
            "title": "5.2 Reinforcement Learning.",
            "content": "We adopt slightly modified version of the REINFORCE Leave-One-Out (RLOO) algorithm (Ahmadian et al., 2024) as our reinforcement learning method. Compared to GRPO, RLOO provides an unbiased advantage estimator. Our modifications are as follows. First, following the implementation in DAPO (Yu et al., 2025), we employ token-level loss function. Second, to mitigate the traininginference mismatch arising from discrepancies between the inference engine and the training framework in log-probability computation, we apply sequence-level importance sampling for rollout correction (Liu et al., 2025b). Third, to ensure trajectory quality, we filter out certain negative trajectories so that they do not participate in advantage estimation or gradient updates. These negative trajectories include (i) failures caused by environmental issues such as connection timeouts or server errors, and (ii) responses that are excessively long or reach the maximum number of turns. This filtering strategy prevents the model from learning spurious behaviors induced by environmental instability and effectively stabilizes training. During the RL stage, we optimize trajectories using an outcome-based reward. An LLM-as-a-judge evaluates whether the final answer is correct, assigning reward of 1 for correct answers and 0 otherwise. Notably, if tool call violates the required format, generation is immediately terminated and reward of 0 is assigned, thereby explicitly encouraging correct tool usage."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "Baselines. For comparison, we conducted comprehensive evaluation of our SMTL model against three categories of systems: (1) foundation models with tools, including closed-source models such as Claude-4.5-Sonnet Anthropic (2025), OpenAI-GPT-5 OpenAI (2025), Gemini-2.5-Pro Comanici et al. (2025), and open-source models GLM-4.5 Zeng et al. (2025), Minimax-M2 MiniMax (2025), DeepSeek-V3.2 Liu et al. (2025a), Kimi-K2-0905 Team et al. (2025); (2) deep research system, including OpenAI DeepResearch OpenAI (2025), Gemini DeepResearch Google (2025), Perplexity Deep Research Perplexity (2025), Kimi-Researcher Moonshot (2025), OAgents (Claude-3-7) Zhu et al. (2025a), MiroFlow (GPT-5) MiroMind AI Team (2025); and (3) open-source agentic models, including WebSailor32B Li et al. (2025b), WebDancer-QwQ Wu et al. (2025a), WebShaper-32B Tao et al. (2025b), DeepMiner-32B-RL Tang et al. (2025a), AFM-32B-RL Li et al. (2025c), Tongyi DeepResearch-30B Tongyi DeepResearch Team (2025), and MiroThinker-v1.0-30B MiroMind Team (2025). Evaluation Benchmarks. We evaluate all models performance on broad set of agentic benchmarks covering both deep search and deep research scenarios. Specifically, the deep search benchmarks include BrowseComp Wei et al. (2025), GAIA Mialon et al. (2023), XBench-DeepSearch Xbench-Team (2025), WebWalkerQA Wu et al. (2025c), FRAMES Krishna et al. (2025), and SEAL-0 Pham et al. (2025). For deep research evaluation, we use Deep Research 7 Bench RACE Du et al. (2025b), which evaluates long-form, open-ended research reports and reports both an overall score and four fine-grained criteria: Comprehensiveness, Insight, Instruction Following, and Readability. Metrics. We use LLM-as-judge approach for evaluation. For deep search tasks, we adopt the pass@1 metric with specific judge prompt. For Deep Research Bench RACE, which assesses report quality across four criteria: Comprehensiveness (coverage of key aspects), Insight/Depth (analytical novelty), Instruction-Following (adherence to query constraints), and Readability (clarity and coherence), we use another judge prompt. See the two judge prompts in D.3. Implementation Details. We conduct experiments using the backbone model: Qwen3-30B-A3B-Instruct-2507. During supervised fine-tuning, we train the models for 3.5 epochs with batch size of 128, using the AdamW optimizer and cosine decay learning rate schedule with an initial learning rate of 1.4 105. The maximum sequence length is set to 65,536 tokens to support long-horizon trajectories. All models are trained under the same agentic workflow and data settings described in earlier sections. In the RL stage, the learning rate is set to 1 106 with batch size of 32. For each question, 8 on-policy rollouts are generated, with maximum sequence length of 128k tokens, up to 120 interaction turns, and training is performed for 60 steps. During inference, we use vLLM, with context window of 128K tokens. Unless otherwise specified, all experiments are conducted with maximum of 100 interaction steps, plan refinement interval of N=5 interaction steps. Context Management during Inference Stage. Long-horizon tasks (e.g., BrowseComp) often exceed the effective context capacity of vanilla agent under 128K window, and this issue is amplified in SMTL because each interaction step produces more tool observations, reducing the number of steps that can be accommodated before hitting the context limit. To improve context efficiency, SMTL couples periodic plan refinement with an overflow-triggered compression scheme: the agent refines the task plan every =5 steps by default, and when the accumulated history reaches the 128K context budget without confirmed answer, it performs an additional forced plan refinement using the current history, then drops all pre-plan context and continues execution from the refreshed plan. This plan-centric reset preserves the latest execution state and subtask structure, keeping inference behavior aligned with training-time plan refinement. As result, SMTL supports longer effective trajectories under fixed context budget without sacrificing structured task context. To align with the interaction budgets commonly used by baselines, we further evaluate SMTL with maximum step limits of 100 and 300, referred to as SMTL-100 and SMTL-300, respectively. For Deep Research Bench, we report results under the 100-step setting, as open-ended research tasks typically converge within tens of interactions rather than exhausting the maximum interaction budget."
        },
        {
            "title": "6.2 Main Results",
            "content": "SMTL exhibits consistent Pareto dominance across heterogeneous benchmarks. As shown in Table 1, SMTL-30B demonstrates strong performance across Deep Search benchmarks under different interaction budgets. With moderate budget (SMTL-100), the model already achieves state-of-the-art performance among 30B-scale open-source agentic models on BrowseComp (43.6%), slightly surpassing Tongyi-DeepResearch-30B (43.4%) and clearly outperforming MiroThinker-v1.0-30B (41.2%). It also reaches 78.0% on XBench-DeepSearch and 74.9% on WebWalker-QA. When the budget is increased to 300 steps, performance further improves, most notably on the long-horizon benchmark BrowseComp, where accuracy rises from 43.6% to 48.6% (+5.0), substantially widening the gap over both Tongyi and MiroThinker. In contrast, gains on shorter-horizon tasks such as GAIA (74.8% 75.7%) and WebWalker (74.9% 76.5%) are comparatively modest, indicating that additional interaction budget primarily benefits deeper multi-step evidence aggregation. Moreover, as shown in Figure 1a, across interaction budgets from 50 to 300 steps on BrowseComp, SMTL consistently lies on the Pareto frontier of accuracy versus trajectory length, while mainstream baselines fall below this curve, confirming its superior efficiency-aware scaling on long-horizon search. Beyond deterministic deep search, SMTL further generalizes to open-ended deep research evaluation. On DeepResearch Bench RACE, SMTL-100 achieves an overall score of 45.9%, with strong and balanced performance across Comprehensiveness (42.1%), Insight/Depth (45.6%), Instruction Following (49.6%), and Readability (45.5%). This surpasses representative open-source agentic baselines such as WebSailor-32B (32.4%), WebDancer-QwQ (35.9%), WebShaper-32B (34.9%), and AFM-32B-RL (35.8%), and slightly outperforms Tongyi-DeepResearch-30B (45.7%) and Kimi-Researcher (44.6%), establishing strong competitiveness among 30B-scale systems. This demonstrates that the same parallel search framework transfers from accuracy-driven benchmarks to long-form research synthesis 8 Table 1 Main results on Deep Search and Deep Research benchmarks. For Deep Search benchmarks, BC: BrowseComp; Xbench-DS: Xbench-DeepSearch; WW: WebWalker-QA; DR-Gym: DeepResearch Gym; DR-Bench: DeepResearch Bench. For Deep Research Bench, Comp., Depth, Inst., and Read. denote Comprehensiveness, Insight/Depth, Instruction Following, and Readability, respectively. We report pass@1 for our models. Results marked with are obtained by our own evaluation, while results marked with + are taken from prior work (Yao et al., 2026). Deep Search Deep Research Bench RACE BC GAIA Xbench-DS WW FRAMES SEAL-0 Overall. Comp. Depth Inst. Read. Model GLM-4.5 Minimax-M2 DeepSeek-V3.2 Kimi-K2-0905 Claude-4.5-Sonnet GPT-5 Gemini-2.5-Pro OpenAI DeepResearch Gemini DeepResearch Perplexity Deep Research Kimi-Researcher OAgents (Claude-3-7) MiroFlow (GPT-5) WebSailor-32B WebDancer-QwQ WebShaper-32B DeepMiner-32B-RL AFM-32B-RL 26.4 44.0 40.1 14.1 19.6 54.9 - 51.5 59.2 22.0 26.9 22.2 33.2 10.5 3.8 9.0 33.5 11.1 31.6 NestBrowse-30B-A3B Tongyi-DeepResearch-30B 43.4 41.2 MiroThinker-v1.0-30B SMTL-100 SMTL-300 43.6 48.6 66.0 75.7 63.5 60.2 71.2 59.4 60. 67.4 66.7 82.4 53.2 51.5 52.4 58.7 55.3 75.7 70.9 73.5 74.8 75.7 Foundation Models with Tools 65.6 64.5 63.0 73.0 78.9 80.2 58.1 85.0 90.0 Deep Research System 67.0 53.0 52.6 83.0 78.8 36.0 38.5 25.2 53.4 51.4 19.8 38.7 36.0 Open-source Agentic Model 60.5 47.9 51.4 63.0 72.2 61.0 74.9 76.5 69.8 90.6 85. 84.3 85.1 16.2 20.7 46.8 50.5 51.4 70.0 72.0 71.0 61.0 66.0 56.0 26.6 50.0 69.0 54.5 72. 53.3 40.0 54.6 62.0 52.0 75.0 75.0 70.6 80.0 82.0 46.1 44.5 39.9 46.8 35.1 47.0+ 48.9+ 42.3+ 44.6 50.8+ 44.9 32.4 35.9 34.9 35.8 45.7+ 45.9 - 45.2 42.8 45.4 34.1 46.9+ 48.5+ 40.7+ 45.0 50.4+ 44.6 28.6 33.0 31.6 32.7 44.7+ 42.1 - 44.6 39.7 44.5 29.8 45.3+ 48.5+ 39.4+ 42.0 51.2+ 49.3 22.7 28.5 26.2 30.2 44.2+ 45.6 - 49.0 50.8 50.3 41.7 49.3+ 49.2+ 46.4+ 47.1 50.3+ 45.6 43.7 44.8 44.8 38.5 48.8+ 49.6 - 44.7 46.0 47.5 37. 47.1+ 49.4+ 44.3+ 45.6 49.4+ 46.1 37.5 40.8 40.4 40.9 44.2+ 45.5 without task-specific modification. Furthermore, as illustrated in Figure 1b, SMTL consistently achieves leading average performance across heterogeneous benchmarks, confirming that its gains are not confined to single task but reflect robust generalization across diverse objectives and evaluation criteria. Why SMTL is more efficient? We analyze the efficiency advantage of SMTL through qualitative case study comparing SMTL-30B with representative deep-search baseline, MiroThinker-v1.0-30B, on BrowseComp task. As illustrated in Figure 5, SMTL localizes the key entity within 8 assistant turns, whereas MiroThinker-v1.0 requires 16 turns to reach the same evidence. This difference arises from fundamentally different search organization strategies. SMTL decomposes the task into multiple hypothesized subtasks and explores them in parallel, allowing the agent to rapidly surface high-signal evidence and to periodically re-plan subtasks based on intermediate observations. As result, SMTL quickly converges on the correct search direction and allocates subsequent interactions to evidence verification. In contrast, MiroThinker-v1.0 follows strictly sequential interaction pattern, in which only single tool call is permitted per round. Information gathering therefore proceeds incrementally, requiring repeated query reformulation and delaying the discovery of key evidence. This case study demonstrates that SMTLs efficiency gains do not stem from deeper per-step reasoning, but from parallel subtask exploration and staged re-planning. By reorganizing search execution rather than extending reasoning depth, 9 SMTL substantially reduces the number of interaction rounds required to localize critical information and complete the task."
        },
        {
            "title": "7.1 Efficiency Evaluation",
            "content": "Table 2 Interaction efficiency on BrowseComp. Average number of assistant steps, average tool calls per step, and task accuracy. Model Steps Avg. Tool Calls Acc. Tongyi-DeepResearch-30B MiroThinker-v1.0-30B SMTL-100 SMTL-300 75.2 206. 60.4 150.7 1.0 1.0 3.5 3.7 43.4 41.2 44.6 48.6 We evaluate the efficiency of SMTL from the perspective of interaction complexity, measured by the number of assistant steps required to solve task. As shown in Table 2, SMTL achieves more favorable balance between interaction cost and task performance compared to representative baselines on BrowseComp. SMTL-100 attains 44.6% accuracy with an average of 60.4 assistant steps, slightly outperforming Tongyi-DeepResearch-30B (43.4%) while requiring fewer steps (75.2). The contrast with MiroThinker-v1.0-30B is even more pronounced: MiroThinker requires 206.0 steps to reach 41.2% accuracy, whereas SMTL-100 achieves substantially higher accuracy with less than one-third of the interaction cost. This efficiency is closely related to SMTLs parallel execution mechanism. Unlike sequential systems that invoke single tool per round, SMTL performs an average of 3.5 tool calls per step, enabling concurrent evidence acquisition across subtasks. By aggregating more information within each interaction round, SMTL increases information density per step and reduces redundant query reformulation, resulting in shorter yet more effective trajectories. Increasing the interaction budget to SMTL-300 further improves accuracy to 48.6%, with 150.7 average steps. While the trajectory length grows as expected, the additional steps translate into measurable performance gains rather than inefficient looping. Figure 1a further illustrates that across different interaction budgets, SMTL consistently lies on the Pareto frontier in the accuracystep plane, confirming that its performance improvements are achieved with well-controlled interaction complexity."
        },
        {
            "title": "7.2 Ablations on Maximum Interaction Steps.",
            "content": "We conduct ablation studies to analyze how the maximum interaction budget (max steps) affects task outcomes and trajectory behavior in long-horizon agentic search. Specifically, we vary the maximum number of interaction steps from 50 to 300 on BrowseComp and report four statistics in Figure 4a: overall average steps, overall median steps, the median steps of successful cases, and the median steps of failed cases. Several clear patterns emerge. First, the median step count of successful cases does not exhibit noticeable increasing trend as interaction steps grows. Most successful trajectories converge before reaching the interaction limit, suggesting that once correct reasoning path is identified, additional budget provides limited benefit for these cases. In contrast, the median step count of failed cases closely follows the = trend, indicating that the majority of failed trajectories terminate exactly at the maximum allowed step. This implies that many failures are due to exhausting the interaction budget rather than prematurely outputting incorrect answers. Consequently, the increase in overall average steps is primarily driven by the upward shift of failed cases, as more trajectories extend to the new budget ceiling before terminating. This observation suggests that the model is actively attempting to explore alternative reasoning paths when facing difficulty, rather than misunderstanding the task or exhibiting overconfidence through early answer generation. We further analyze why increasing max interaction steps leads to performance gains. Under smaller budgets, substantial portion of hard cases fail simply because SMTL cannot identify valid reasoning path within the limited number of tool interactions. When the interaction budget is enlarged, SMTL is afforded additional opportunities to explore different evidence chains. Combined with periodic plan refinement, this extended budget enables our model to correct suboptimal 10 (a) BC score under different max steps. (b) BC score under different web search topk results. Figure 4 Results of context management (CM) under different observation horizons with interaction budgets (IB) of 80 and 160 steps on the BC benchmark. search directions and progressively reorient toward promising subtasks. As result, increasing max steps improves success rates primarily by alleviating search budget constraints in genuinely difficult cases, rather than by compensating for systematic reasoning errors."
        },
        {
            "title": "7.3 Ablations on Retrieval Top-k.",
            "content": "We further investigate how the retrieval width of the web search tool affects performance by varying the parameter top-k, which controls the number of URLs returned for each query. We evaluate both SMTL-100 and SMTL-300 on BrowseComp under different top-k settings. As shown in Figure 4b, increasing top-k consistently improves task performance. When top-k increases from 4 to 8, both SMTL-100 and SMTL-300 exhibit substantial gains (e.g., SMTL-300 improves from 43.8 to 47.0, while SMTL-100 increases from 36.6 to above 41.8). This jump indicates that narrow retrieval window significantly constrains evidence coverage, limiting the SMTLs ability to identify relevant information within fixed interaction budget. When top-k further increases from 8 to 20, performance continues to improve, albeit at slower rate and gradually converges. This suggests diminishing returns: once the most informative candidates are included, additional results contribute marginal gains but still enhance robustness by reducing the risk of missing critical evidence. Overall, the results align with our design intuition that improving search breadth can be powerful scaling dimension for long-horizon agentic search. Under fixed number of interaction steps, increasing top-k effectively packs more candidate evidence into each search action, raising the information density per step. Rather than extending reasoning depth, SMTL benefits more from broader evidence acquisition within each interaction, demonstrating that expanding retrieval breadth is more efficient scaling axis for long-horizon search than merely increasing reasoning length."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we revisit long-horizon agentic search from the perspectives of efficiency and generalization. We propose Search More, Think Less (SMTL), unified agentic framework that replaces sequential reasoning with parallel evidence acquisition, enabling efficient long-horizon inference under constrained interaction budgets. SMTL integrates an efficient agentic workflow with structured context management and an automated data synthesis pipeline that supports both deterministic question answering and open-ended research tasks. Trained end-to-end with supervised fine-tuning and reinforcement learning, SMTL achieves state-of-the-art or competitive performance across wide range of deep 11 search and deep research benchmarks, while substantially reducing reasoning steps and inference latency. Our results demonstrate that prioritizing efficient, search-centric scaling over ever-deeper reasoning provides practical and generalizable foundation for future deep research agents, and we hope this work encourages further exploration of efficiency-oriented agentic designs."
        },
        {
            "title": "9 Contributions",
            "content": "Core Contributors"
        },
        {
            "title": "Jiaqi Wu",
            "content": "Contributors"
        },
        {
            "title": "He Zhu",
            "content": "Yuqing Wang Tianyu Zheng Jian Yang Minghao Liu Corresponding Author Wangchunshu Zhou"
        },
        {
            "title": "Tiannan Wang",
            "content": "Maojia Song Ge Zhang Jiaheng Liu Yuchen Eleanor Jiang"
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker, S. (2024). Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. Anthropic (2025). Claude takes research to new places. https://www.anthropic.com/news/research. Anthropic (2025). Introducing claude sonnet 4.5. https://www.anthropic.com/news/claude-sonnet-4-5. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. (2025). Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585. Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M., Qin, Y., Lu, Y., Xie, R., et al. (2023). Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6. Coelho, J., Ning, J., He, J., Mao, K., Paladugu, A., Setlur, P., Jin, J., Callan, J., Magalhães, J., Martins, B., et al. (2025). Deepresearchgym: free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253. Comanici, G., Bieber, E., Schaekermann, M., et al. (2025). Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Du, M., Xu, B., Zhu, C., Wang, X., and Mao, Z. (2025a). Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763. Du, M., Xu, B., Zhu, C., Wang, X., and Mao, Z. (2025b). Deepresearch bench: comprehensive benchmark for deep research agents. Fourney, A., Bansal, G., Mozannar, H., Tan, C., Salinas, E., Niedtner, F., Proebsting, G., Bassman, G., Gerrits, J., Alber, J., et al. (2024). Magentic-one: generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468. Gao, J., Fu, W., Xie, M., Xu, S., He, C., Mei, Z., Zhu, B., and Wu, Y. (2025). Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976. Google (2024). Gemini deep research: Your personal research assistant. https://gemini.google/overview/deep-research/. Google (2025). Gemini deep research. https://gemini.google/us/overview/deep-research/?hl=en. Guo, Z., Xia, L., Yu, Y., Ao, T., and Huang, C. (2024). Lightrag: Simple and fast retrieval-augmented generation. arXiv preprint arXiv:2410.05779. Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., Wang, J., Wang, Z., Yau, S. K. S., Lin, Z., et al. (2024). Metagpt: Meta programming for multi-agent collaborative framework. In International Conference on Learning Representations, ICLR. Hu, M., Zhou, Y., Fan, W., Nie, Y., Xia, B., Sun, T., Ye, Z., Jin, Z., Li, Y., Chen, Q., Zhang, Z., Wang, Y., Ye, Q., Ghanem, B., Luo, P., and Li, G. (2025). Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. (2025). Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Jina, I. (2025). Jina reader. Krishna, S., Krishna, K., Mohananey, A., Schwarcz, S., Stambler, A., Upadhyay, S., and Faruqui, M. (2025). Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. In Chiruzzo, L., Ritter, A., and Wang, L., editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47454759, Albuquerque, New Mexico. Association for Computational Linguistics. LangChain (2025). Open deep research. https://github.com/langchain-ai/open_deep_research. Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. (2023). Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., et al. (2025a). Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., Shen, W., Zhang, J., Zhang, D., Wu, X., Jiang, Y., Yan, M., Xie, P., Huang, F., and Zhou, J. (2025b). Websailor: Navigating super-human reasoning for web agent. Li, W., Lin, J., Jiang, Z., Cao, J., Liu, X., Zhang, J., Huang, Z., Chen, Q., Sun, W., Wang, Q., Lu, H., Qin, T., Zhu, C., Yao, Y., Fan, S., Li, X., Wang, T., Liu, P., Zhu, K., Zhu, H., Shi, D., Wang, P., Guan, Y., Tang, X., Liu, M., Jiang, Y. E., Yang, J., Liu, J., Zhang, G., and Zhou, W. (2025c). Chain-of-agents: End-to-end agent foundation models via multi-agent distillation and agentic rl. 14 Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. (2025d). Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366. Li, X., Jin, J., Dong, G., Qian, H., Zhu, Y., Wu, Y., Wen, J.-R., and Dou, Z. (2025e). Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Li, Z., Guan, X., Zhang, B., Huang, S., Zhou, H., Lai, S., Yan, M., Jiang, Y., Xie, P., Huang, F., et al. (2025f). Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. (2025a). Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Liu, J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Shen, Y. (2025b). When speed kills stability: Demystifying RL collapse from the training-inference mismatch. Liu, J., Li, Y., Zhang, C., Li, J., Chen, A., Ji, K., Cheng, W., Wu, Z., Du, C., Xu, Q., et al. (2025c). Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. (2023). Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. MiniMax (2025). Minimax m2 & agent: Ingenious in simplicity. https://www.minimax.io/news/minimax-m2. MiroMind AI Team (2025). Miroflow: high-performance open-source research agent framework. https://github.com/ MiroMindAI/MiroFlow. MiroMind Team (2025). Mirothinker: Pushing the performance boundaries of open-source research agents via model, context, and interactive scaling. arXiv preprint arXiv:2511.11793. Moonshot (2025). Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https://moonshotai.github.io/ Kimi-Researcher/. OpenAI (2025). Introducing deep research. https://openai.com/index/introducing-deep-research/. OpenAI (2025). Introducing gpt-5. https://openai.com/index/introducing-gpt-5/. Perplexity (2025). Introducing perplexity deep research. https://www.perplexity.ai/hub/blog/ introducing-perplexity-deep-research. Perplexity Team (2025). Perplexity deep research. introducing-perplexity-deep-research. https://www.perplexity.ai/hub/blog/ Pham, T., Nguyen, N., Zunjare, P., Chen, W., Tseng, Y.-M., and Vu, T. (2025). Sealqa: Raising the bar for reasoning in searchaugmented language models. Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., et al. (2023). Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924. Qin, T., Chen, Q., Wang, S., Xing, H., Zhu, K., Zhu, H., Shi, D., Liu, X., Zhang, G., Liu, J., et al. (2025). Flash-searcher: Fast and effective web agents via dag-based parallel execution. arXiv preprint arXiv:2509.25301. Qiu, J., Qi, X., Zhang, T., Juan, X., Guo, J., Lu, Y., Wang, Y., Yao, Z., Ren, Q., Jiang, X., et al. (2025). Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286. Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismäki, E. (2025). smolagents: smol library to build great agentic systems. https://github.com/huggingface/smolagents. Serper, I. (2025). Serper api. Shi, D., Cao, J., Chen, Q., Sun, W., Li, W., Lu, H., Dong, F., Qin, T., Zhu, K., Yang, M., et al. (2025). Taskcraft: Automated generation of agentic tasks. arXiv preprint arXiv:2506.10055. SkyworkAI (2025). Deepresearchagent: hierarchical multi-agent system for deep research. https://github.com/SkyworkAI/ DeepResearchAgent. Sun, S., Song, H., Wang, Y., Ren, R., Jiang, J., Zhang, J., Bai, F., Deng, J., Zhao, W. X., Liu, Z., et al. (2025). Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834. 15 Tang, Q., Xiang, H., Yu, L., Yu, B., Lu, Y., Han, X., Sun, L., Zhang, W., Wang, P., Liu, S., et al. (2025a). Beyond turn limits: Training deep search agents with dynamic context window. arXiv preprint arXiv:2510.08276. Tang, X., Qin, T., Peng, T., Zhou, Z., Shao, D., Du, T., Wei, X., Xia, P., Wu, F., Zhu, H., Zhang, G., Liu, J., Wang, X., Hong, S., Wu, C., Cheng, H., Wang, C., and Zhou, W. (2025b). Agent kb: Leveraging cross-domain experience for agentic problem solving. In ICML 2025 Workshop on Collaborative and Federated Agentic Workflows. Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025a). Webshaper: Agentically data synthesizing via information-seeking formalization. Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025b). Webshaper: Agentically data synthesizing via information-seeking formalization. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. (2025). Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Together AI (2025). Open deep research. https://www.together.ai/blog/open-deep-research. Tongyi DeepResearch Team (2025). Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. (2025). Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Fu, G., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025a). Webdancer: Towards autonomous information seeking agency. Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Jiang, Y., Xie, P., et al. (2025b). Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648. Wu, J., Yin, W., Jiang, Y., Wang, Z., Xi, Z., Fang, R., Zhang, L., He, Y., Zhou, D., Xie, P., et al. (2025c). Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572. Xbench-Team (2025). Xbench-deepsearch. Xue, Z., Zheng, L., Liu, Q., Li, Y., Ma, Z., and An, B. (2025). Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. https://simpletir.notion.site/report. Notion Blog. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Yao, Y., Zhu, H., Wang, P., Ren, J., Yang, X., Chen, Q., Li, X., Shi, D., Li, J., Wang, Q., et al. (2026). O-researcher: An open ended deep research model via multi-agent distillation and agentic rl. arXiv preprint arXiv:2601.03743. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., et al. (2025). Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Zhang, D., Zhao, Y., Wu, J., Li, B., Yin, W., Zhang, L., Jiang, Y., Li, Y., Tu, K., Xie, P., et al. (2025). Evolvesearch: An iterative self-evolving search agent. arXiv preprint arXiv:2505.22501. Zheng, Y., Fu, D., Hu, X., Cai, X., Ye, L., Lu, P., and Liu, P. (2025). Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Zhou, W., Jiang, Y. E., Li, L., Wu, J., Wang, T., Qiu, S., Zhang, J., Chen, J., Wu, R., Wang, S., et al. (2023). Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870. Zhou, W., Ou, Y., Ding, S., Li, L., Wu, J., Wang, T., Chen, J., Wang, S., Xu, X., Zhang, N., et al. (2024). Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532. Zhu, H., Qin, T., Zhu, K., Huang, H., Guan, Y., Xia, J., Yao, Y., Li, H., Wang, N., Liu, P., Peng, T., Gui, X., Li, X., Liu, Y., Jiang, Y. E., Wang, J., Zhang, C., Tang, X., Zhang, G., Yang, J., Liu, M., Gao, X., Liu, J., and Zhou, W. (2025a). Oagents: An empirical study of building effective agents. Zhu, K., Li, H., Wu, S., Xing, T., Ma, D., Tang, X., Liu, M., Yang, J., Liu, J., Jiang, Y. E., Zhang, C., Lin, C., Wang, J., Zhang, G., and Zhou, W. (2025b). Scaling test-time compute for llm agents."
        },
        {
            "title": "A Tools Setup",
            "content": "Our agent operates with minimal yet expressive toolset designed to support web-scale information acquisition and consolidation. Specifically, we employ two core tools: search interface for candidate retrieval and page-level crawler for content extraction and goal-directed summarization. web_search. This tool provides access to web search functionality through the Serper API, which interfaces with the Google Search engine (Serper, 2025). Given model-generated query string, the tool retrieves ranked list of search results, with the default setting returning the top five entries. Each result consists of page title, short snippet, and the corresponding URL. The search results serve as high-level signals for identifying potentially relevant sources and guiding subsequent crawling decisions. crawl_page. This tool is responsible for fine-grained content acquisition and structured summarization. It takes as input both target URL and an explicit goal describing the information need to be addressed. The URL is crawled using the Jina Reader API (Jina, 2025), after which the retrieved page content is summarized by the DeepSeek-V3.2 model (Liu et al., 2025a). Crucially, the goal specification provides semantic guidance for the summarization process, steering the model to extract and condense information that is directly relevant to the current subtask rather than producing generic page summary. This goal-conditioned summarization enables more targeted evidence collection and reduces irrelevant context propagation. The prompt template used for page summarization is provided in Appendix D.2."
        },
        {
            "title": "B Data Construction",
            "content": "B.1 Deep Search Data Entity-Centric Information Extraction Prompt Instruction: Given structured entity metadata and source content excerpt, extract entity-related information strictly based on explicit textual evidence. Input Fields: Entity Name: {entity_name} Entity Type: {entity_type} Entity Description: {description} Source Content Excerpt: {source_content[:2000]} Task: Extract entity-related information from the source content and return it in the following JSON format. 1 { 2 \" key_attributes \": { 3 5 6 7 8 9 11 12 // Extract attributes relevant to the entity type ({ template }) // Only include attributes that explicitly appear in the text }, \" surface_forms \": [ // Common surface mentions of the entity in the text // (e.g., abbreviations , short forms ) ], \" aliases \": [ // Fully equivalent alternative names // (e.g., full name , pen name , former name ) ] 13 14 } 17 Extraction Constraints: 1. Evidence-Only Extraction: Only extract information that explicitly appears in the provided source content. 2. No Inference or Fabrication: Do not infer, hallucinate, or supplement missing information. 3. Graceful Degradation: If field cannot be extracted, return an empty object ({}) or empty list ([]). Output Requirements: The output must be valid JSON object. Do not include any content outside the specified JSON fields. Entity Description Factuality Evaluation Prompt Instruction: Evaluate the factuality level of candidate entity descriptions and extract key factual information strictly based on the original text. Factuality Evaluation Criteria and Scoring: High Factuality (80100 points): The description contains concrete, objective facts such as numerical values, time, location, quantities, measurements, dates, or percentages. Example keywords: founded in 2010; 5,000 employees; headquartered in Beijing; market value of USD 10 billion; covers 500 square meters; November 2023. Medium Factuality (5079 points): The description includes some specific information but is predominantly qualitative. Example keywords: large company; located in China; multiple services; rapid growth; industry participant. Low Factuality (049 points): The description is mainly abstract, subjective, or generic, lacking concrete data. Example keywords: important organization; high-quality services; influential; widely followed; has potential. Candidate Entity List (Total: {len(candidates)}): {candidate_info} Evaluation Tasks (for each entity): 1. Assign factuality score between 0 and 100. 2. Extract key factual information (key_info) by directly copying the most factual phrases or words from the original description. Multiple fragments should be separated by commas. Critical Constraint: key_info must be copied verbatim from the original description. Do not paraphrase, rewrite, infer, or add new information. Output Format: Return the result as valid JSON object with the following structure: 1 { 2 \" entity_scores \": [ \" entity_name \": \" Entity Name \", \" score \": 85, \" level \": \" High \", \" key_info \": \" founded in 20 10 , 5, 00 0 employees , headquartered in Beijing \" \" entity_name \": \" Entity Name 2\" , \" score \": 45, \" level \": \" Low \", \" key_info \": \" important organization , provides services \" { }, { } ] 3 5 6 7 8 9 11 12 13 14 15 16 } Additional Requirements: 18 1. entity_scores must include all {len(candidates)} candidate entities. 2. Preserve the original order of entities; no sorting is required. 3. level must be one of: \"High\" (80100), \"Medium\" (5079), or \"Low\" (049). 4. Entity names must exactly match those in the candidate list. 5. key_info must be copied directly from the original description. 6. Scoring should be objective and unbiased; do not favor any entity. Hierarchical Fact-Centric Entity Description Generation Prompt Role Definition: You are an expert responsible for generating structured description of target entity based solely on objective, verifiable factual information. Core Principle: Use only objective, verifiable facts. Subjective, generic, or vague descriptions are strictly prohibited. Information to Be Extracted (Priority Order): 1. Precise Numbers: founding years, employee counts, revenue figures, rankings, IDs, award counts, etc. Examples: founded in 1959; more than 5,000 employees; annual revenue of 1 billion USD; won the Nobel Prize twice. 2. Specific Time: exact dates, historical periods, event times. Examples: independence on December 12, 1963; participated in the Olympics in 2000; founded in the 1990s. 3. Specific Locations: countries, cities, geographic positions, distances, elevations. Examples: located in East Africa; 5 km from the city center; 1,600 meters above sea level. 4. Specific Events: historical events, awards, participations, published works. Examples: won the Nobel Prize in Literature; published XXX; participated in the 2000 Olympics. 5. Measurable Attributes: area, population, scale, institutional size. Examples: area of 580,000 square kilometers; population of 53 million; has 12 colleges. Strictly Prohibited Information: Subjective descriptions (e.g., important, famous, influential) Generic descriptions (e.g., provides services, is an organization) Vague expressions (e.g., many people, certain region) Functional descriptions without concrete factual support Key Requirements: All facts must be directly extracted from the provided descriptions and relations. Numerical, temporal, and locational information must appear verbatim in the output. If piece of information contains no objective facts, it must be ignored. Special Emphasis: Hierarchical Abstraction Strategy Present all extracted information hierarchically. Do not omit any factual information, and do not invent content. Input Content: 1. Current Entity Information: Entity type: {entity_type} Number of related items: {len(children_descriptions)} 2. Related Information List ({len(children_descriptions)} items): {children_info_text} 3. Relationships Among Related Information: {peers_info_text} Output Content: Generate the following three items: 1. facts: list of independent, objective, and verifiable facts extracted from the input. 2. summary: one-sentence abstraction of relationship types (e.g., work, geography, education). 3. question: multi-hop search question constructed strictly from the extracted objective facts. Entity Name Usage Rules: The name of the current entity must never be mentioned. Names of related entities must also never be mentioned in the question. Use abstract references such as an organization, country, company, or relational descriptions. Fact Extraction Procedure: 1. Scan related information for objective facts containing numbers, time, locations, or events. 2. Categorize facts into numerical, temporal, locational, event-based, and measurement-based. 3. Construct fact descriptions that integrate all objective details without using specific names. 4. Ensure each fact contains at least one verifiable objective element. Question Construction Principles: The question must be built solely from extracted objective facts. All numerical, temporal, locational, event, and measurement information must be included. Completeness is more important than brevity. If insufficient objective facts exist, return an empty result. Output Format: 1 { 2 \" facts \": [ { }, { } \" fact \": \"[ single fact description without specific names ]\", \" source \": \" child :0\" \" fact \": \"[ single fact description without specific names ]\", \" source \": \" child_peer :0_1\" ], \" summary \": \" Abstract description of relationship types \" , \" question \": \" Which { entity_type } satisfies the following conditions : [ ... ] ?\" 3 4 6 7 8 9 10 12 13 14 } Search-Enforced Entity Description Obfuscation Prompt Role Definition: You are an expert responsible for obfuscating an entity description such that the correct answer cannot be inferred directly and must be determined through external search. Current Situation: Entity type: {entity_type} Entity name (ground truth): {entity_name} (This name must never appear in the obfuscated description.) Original description: {description} Conditions in the Description: {facts_detail_text} Direct Reasoning Basis (to be neutralized): {reasoning} Entity Details: Full description: {entity_description} Key attributes: 20 {key_attrs_text} Strictly Prohibited Content: 1. Do not use prompt-instruction language (e.g., related information 0). 2. Do not mention the entity name {entity_name} or any other specific entity name. 3. Do not use expressions such as this {entity_type} or the above {entity_type}. Required Natural Language Expressions: Use generic references (e.g., an organization, company, country, writer). Use characteristic descriptions (e.g., media company founded in year XX). Use relational descriptions (e.g., the employing company, the country of birth). Task: Select one single most critical condition to obfuscate such that the model can no longer directly infer the answer and must rely on search for verification. Obfuscation Strategies: 1. Time Obfuscation: Convert an exact time into vague period or search-dependent temporal description. Examples: founded in 1949 founded in the late 1940s. 2. Entity Obfuscation: Replace an entity name with search-required attributes. Examples: founded by Steve Jobs founded by Silicon Valley entrepreneur whose name starts with S. 3. Quantitative Obfuscation: Replace precise numbers with ranges or qualitative quantities. Examples: won the Nobel Prize twice won the Nobel Prize more than once. 4. Information Deletion: Remove redundant or overly revealing conditions that make the answer trivial. 5. Concept Obfuscation: Abstract specific concept into higher-level description supported by searchable facts. 6. Combined Obfuscation: Apply multiple lightweight obfuscations while preserving coherence. Selection Criteria: Prioritize obfuscating the key condition used in direct reasoning. Choose the condition that most reduces certainty. Keep the description understandable and logically coherent. Ensure the answer remains discoverable via search. Avoid subjective or vague phrasing. Output Format: 1 { 3 4 5 6 7 } \" target_fact_index \": 0, \" obfuscation_strategy \": \" Time obfuscation \", \" obfuscated_fact \": \" the obfuscated condition text \", \" obfuscated_description \": \" the full obfuscated description \", \" reasoning \": \" Explanation of why this condition was selected and how it was obfuscated \" Output Constraint: Output only the JSON object. Do not include any additional text. Entity Answer Verification Prompt Instruction: Answer the question strictly based on the provided description and identify the corresponding entity. Input Fields: Description: {description} Question: What is this {entity_type}? 21 Task: Infer the most specific entity name that satisfies the given description. Base your answer only on the provided description. Output Format: Return the result as valid JSON object with the following fields: 1 { 2 4 } \" answer \" : \" Entity name \", \" reasoning \": \" Brief explanation of which conditions were used to infer the answer \" Constraints: You must return specific and explicit entity name. Do not respond with concept, category, or abstract description. If the answer cannot be determined from the description, set \"answer\" to \"Cannot determine\". Output only the JSON object. Do not include any additional text. Entity Answer Equivalence Verification Prompt Instruction: Determine whether predicted answer refers to the same entity as the ground-truth answer. Input Fields: Ground-Truth Answer: {entity_name} Predicted Answer: {predicted_answer} Task: Evaluate whether the predicted answer correctly identifies the same entity as the ground-truth answer. Output Format: Return the result as valid JSON object with the following fields: 1 { 3 4 } \" is_correct \": true , \" explanation \": \" Brief explanation of why the answers match or do not match \" Judgment Criteria: If the two answers refer to the same entity (even if phrased differently), return true. If the predicted answer is \"Cannot determine\" or is clearly incorrect, return false. The predicted answer must be specific and explicit entity name; concepts, categories, or abstract descriptions are not acceptable. Consider valid aliases, abbreviations, and alternative names. Output Constraint: Output only the JSON object. Do not include any additional text. B.2 Deep Research Data Graph-Grounded Open-Ended Research Question Construction Prompt Instruction: Construct high-quality open-ended research question strictly grounded in the provided subgraph structure and factual evidence. Input Fields: Target Entity: {target_entity} Entity Type: {entity_type} Supporting Subgraph: {subgraph_description} Task: Generate one open-ended research question that requires synthesizing information from multiple nodes and relations in the supporting subgraph. The question should: Require integrating evidence across the subgraph rather than relying on single fact. Admit multiple valid answers supported by different reasoning paths. Be suitable for report-style response involving explanation, comparison, or synthesis. Construction Constraints: 1. Graph-Grounded: The question must be answerable only using information contained in the provided subgraph. 2. Non-Deterministic: Do not construct questions with single exact or short factual answer. 3. Multi-Evidence Requirement: The question must require reasoning over multiple entities, attributes, or relations. 4. No Trivial Reformulation: Avoid paraphrasing factual descriptions or directly listing conditions. Output Format: { \"research_question\": \"A single open-ended research question\" } Output Requirements: Output must be valid JSON object. Do not include explanations, rationales, or additional text."
        },
        {
            "title": "C Case Study",
            "content": "Task: Im looking for historical figure. They had blue eyes and never drank alcohol. They married after moving to different country from the one in which they had been born. They lost child and wrote letter asking for people to bring flowers to the childs resting place. What was this persons name and title upon their accession to rulership? SMTL-30B MiroThinker-v1.0-30B Stage 1: Parallel search over subtasks Subtask 1: Search clues: blue eyes; emigrated and married abroad; monarch Subtask 2: Search clues: lost child; letter; flowers at resting place Subtask 3: Search clues: blue eyes; emigrated and married abroad; never drank alcohol Assistant turns 14 Stage 2: Re-planned subtasks Subtask 1: Search clues: blue eyes; abstained from alcohol Subtask 2: Search clues: letter; flowers; child grave; royal context Subtask 3: Search clues: royal figure; emigrated to another country; married abroad Assistant turns 58 Assistant turns 14 Stage 1: Search clues: flowers; child; grave; letter; ruler Assistant turns 57 Stage 2 Search clues: emigrated to another country; married abroad; ruler Assistant turns 8 Stage 3 Search clues: never drank alcohol Assistant turns 1116 Stage 4: Query reformulation Search clues: baby, grave, flowers, queen Key Entity Identified: Queen Marie of Romania Continue evidence verification Final answer produced at assistant turn 36 Continue evidence verification Final answer produced at assistant turn Figure 5 Case study illustration comparing SMTL-30B and MiroThinker-v1.0-30B. SMTL performs parallel subtask execution with staged re-planning, enabling faster localization and verification of key evidence, while MiroThinker-v1.0-30B follows strictly sequential search process."
        },
        {
            "title": "D Prompts",
            "content": "D.1 System Prompts We employ two system prompts to support Deep Search and Deep Research tasks, respectively. While the two prompts differ in their output structure and interaction protocols, they operate under shared parallel agentic search framework as described in the main paper. Specifically, both system prompts follow unified design philosophy: tasks are represented over graph-structured evidence, decomposed into multiple goals or subtasks, and solved through parallel execution and coordinated tool use. In both settings, the agent performs explicit planning, iterative plan refinement based on tool observations, and structured progress tracking, enabling efficient long-horizon search under constrained interaction budgets. The primary distinction lies in the response format and organization. The Deep Search prompt adopts compact planplan-refineanswer structure optimized for deterministic question answering and efficient verification, whereas the Deep Research prompt enforces finer-grained subtask-oriented protocol and report-style synthesis, tailored to open-ended research problems with multi-dimensional evaluation criteria. Despite these differences in output formatting 24 and control flow granularity, both prompts instantiate the same underlying parallel agentic execution paradigm. System Prompt for Deep Search Tasks System Role: You are an expert assistant that solves complex search tasks through structured tool usage and explicit goal management. You must follow step-by-step execution process that combines planning, parallel execution, verification, and concise final answering. Core Objective: Decompose the task into clear, verifiable goals. Advance multiple goals in parallel whenever possible. Execute each goal through sequential fallback paths. Use tools purposefully and efficiently to gather and verify evidence. Produce final answer only after all goals are explicitly resolved. Execution Requirements: 1. Follow logical and structured order of tool usage. 2. Parallelize independent goals; execute paths within goal sequentially. 3. Each action step must include: brief reasoning explaining why the selected tool or path is chosen. One or more <tool_call> executions with valid parameters. Interpretation of observations returned by tools. Refinement of subsequent actions based on observations. 4. Avoid redundant tool calls and repeated queries. 5. Never assume task completion without explicit verification. Functional Interfaces: 1. <plan> Function Role: Decompose the task into parallelizable goals and execution paths. Constraints: 15 goals. Each goal has 15 sequential fallback paths. Each path must specify clear success criterion. Timing: Used only at the first step. 2. <plan_refine> Function Role: Summarize progress and decide next actions. Content: Plan recap. Execution status of each goal (Completed / In Progress / Blocked). Analysis of attempted paths. Next sub-paths to execute in parallel. Timing: Invoked periodically after multiple actions. 3. <tool_call> Function Available Tools: web_search(query): Broad information discovery. crawl_page(url, query): Deep verification guided by an information goal. Usage Rules: Use 15 tool calls per step, targeting distinct subtasks. Prioritize authoritative and official sources when precision is required. Always verify promising URLs via crawl_page. 4. <answer> Function Role: Output the final confirmed answer. Constraints: Only after all goals are resolved. Must consolidate evidence across all goals. Language must match the task language. Keep the answer concise and factual. Critical Execution Rules: Advance all goals in parallel whenever possible. Never terminate early without verification. Do not skip promising sources during verification. Use no more than 10 tool calls per step. Final Note: Do not produce an answer unless you are absolutely certain. Final answers should be concise and avoid unnecessary explanation. System Prompt for Deep Research Tasks System Role: You are an expert assistant who solves tasks through structured tool calls, following step-by-step process. Each step (action) involves analyzing needs, selecting tools, and executing calls to achieve the task. You are required to solve the task by formulating your thinking and reasoning process as described below. Allowed Functions (Tag-based Protocol). You can only use the following functions to answer given question: subtask_list, subtask, analysis, plan, tool_call, tool_response, subtask_answer, answer. 1. subtask_list: At the very beginning, break down the complex question into list of clear, independent subtasks. Start with <subtask_list> and end with </subtask_list>. 2. subtask: Marks the beginning of executing one specific subtask from subtask_list. Start with <subtask> and end with </subtask>. 3. analysis: Within subtask, before using plan or tool_call, provide reasoning, arguments, and next steps. Start with <analysis> and end with </analysis>. 4. plan: For the current subtask, break it down into fine-grained steps to be executed using tools. Start with <plan> and end with </plan>. 5. tool_call: Execute tools from the tool list below to gather information relevant to answering the question. 6. tool_response: The response returned after tool is executed. 7. subtask_answer: After completing subtask, provide an intermediate, definitive answer. Start with <subtask_answer> and end with </subtask_answer>. 8. answer: After all subtasks are completed, synthesize all subtask_answers into final comprehensive answer. Available Tools. 1. web_search: one parameter query. Example: {'name': 'web_search', 'arguments': {'query': 'xxx'}} 2. crawl_page: two parameters url and query. Example: {'name': 'crawl_page', 'arguments': {'url': 'xxx', 'query': 'xxx'}} Tool Usage Guide. 1. web_search: If retrieved information is irrelevant, re-search with new queries until sufficient relevant information is obtained and you are highly confident in the final answer. 2. crawl_page: Use crawl_page to obtain detailed information from URLs, and crawl additional URLs when needed. 3. Deeper search: Use web_search to discover URLs, then crawl_page to verify and extract details. Repeat web_search/crawl_page multiple times if deeper hints emerge. Trail Notes. 1. Overall workflow: Start with <subtask_list>. Then for each subtask, follow an analysis plan tool tool_response loop until the subtask is answerable, and output <subtask_answer>. After all subtasks, produce <answer>. 2. Information gathering: Based on the plan, you may use tools multiple times to collect sufficient external knowledge for the current subtask. 3. Tag restrictions: <subtask_list>, <subtask>, <analysis>, <plan>, <web_search>, <crawl_page>, <tool_response>, <subtask_answer>, <answer> are special tags and must not appear in free text, especially within the <analysis> function. Function Association Instructions. 1. The process must start with <subtask_list>. 2. After <subtask_list>, the first <subtask> must begin. 3. Inside each <subtask>, before using plan or tools, you must first use <analysis>. 4. After information gathering is complete for subtask, <subtask_answer> must be generated. 5. Following <subtask_answer>, either start the next <subtask> or, if all are complete, generate <answer>. Answering Tips (Report Formatting). The final answer must be detailed, well-structured report with traceable information and must strictly follow: 1. Structure: include an introduction, body paragraphs (organized by subtasks if appropriate), conclusion, and references section. 2. In-text citations: every key information point, data point, or quote must be supported by source. Use numbered square brackets with spaces, e.g., [1] , [2] , placed immediately after the supported claim. References Section. Include section titled References at the end of the report. It must be numbered list corresponding one-to-one with citation numbers. Each entry must follow: [Number]. URL - Webpage Title. D.2 Summary Prompt Goal-Conditioned Webpage Summarization Prompt Instruction: Please process the provided webpage content and extract information that is directly relevant to the specified information goal. Input Fields: Webpage URL: {url} Webpage Content: {content} Information Goal: {query} Task Guidelines: 1. Goal-Oriented Content Scanning: Identify the specific sections, passages, or data within the webpage that are directly relevant to the given information goal. Ignore unrelated or tangential content. 2. Evidence Extraction: Extract the most relevant information that supports the goal. Do not omit important details. Preserve the original wording and context as much as possible, and include extended excerpts when necessary (potentially spanning multiple paragraphs). 3. Structured Summarization: Organize the extracted information into coherent and concise summary with clear logical structure, prioritizing relevance, factual accuracy, and completeness with respect to the goal. Output Format: The output must be valid JSON object with the following fields: 1 { 2 3 4 \" rationale \": \" Explanation of why the extracted content is relevant to the information goal \", \" evidence \": \" Verbatim excerpts or minimally edited passages from the webpage that directly support the goal \", \" summary \": \"A concise , goal - focused summary synthesized from the extracted 27 evidence \" 5 } Additional Requirements: The output must be fully JSON-parsable. All special characters must be properly escaped. Do not include any content outside the specified JSON fields. D.3 LLM-as-a-Judge Prompts We design two distinct LLM-as-a-judge prompts for Deep Search and Deep Research evaluations, respectively. This separation is necessary because the two task settings produce fundamentally different types of outputs and therefore require different evaluation protocols to ensure fairness and reliability. For Deep Search tasks, model outputs are typically short, deterministic answers with well-defined ground truth. Accordingly, the judge prompt focuses on semantic equivalence between the predicted answer and the labeled answer, allowing for minor surface-form variations while enforcing strict correctness. In contrast, Deep Research tasks produce long-form, report-style responses that involve synthesis, analysis, and organization across multiple sources. Evaluating such outputs requires multi-dimensional rubric that assesses content coverage, analytical depth, instruction adherence, and readability rather than exact answer matching. By tailoring the judge prompts to the structural and semantic characteristics of each task type, we ensure that evaluation remains consistent, unbiased, and aligned with the intended objectives of the benchmark. Both judge prompts are applied within the same evaluation framework but are specialized to match the output format and reasoning demands of their respective scenarios. LLM-as-a-Judge Prompt for Deep Search Tasks Instruction: Please determine whether the Predicted Answer is semantically equivalent to the Labeled Answer, given the Question. Input Format: Question: {question} Labeled Answer: {gt_answer} Predicted Answer: {pred_answer} Evaluation Process: 1. Instruction Focus: Your judgment must be based solely on whether the meaning conveyed by the Predicted Answer aligns with the meaning of the Reference Answer. 2. Allowable Variations: For text answers: Differences in capitalization, punctuation, grammar (including prepositions, articles, and grammatical structures), word order, phrasing style, measurement units, or the inclusion/exclusion of non-essential descriptive phrases are acceptable if they do not alter the core meaning. For names and titles: Variations in name formats (e.g., inclusion of middle names, parentheses with additional information, honorifics) are acceptable if they refer to the same entity/person. For numerical answers: Minor acceptable margins of error are permissible when appropriate for the context. Synonyms and near-synonyms that convey the same meaning in context are acceptable. 3. Criteria for CORRECT Judgement: Judge as \"correct\" only if: a. The Predicted Answer directly addresses the Question, and b. Its core meaning is semantically equivalent to the Reference Answer, and c. It does not contradict any explicit requirements or constraints in the Question or Reference Answer. Important: If the Predicted Answer contains the Reference Answer within it (e.g., as an alternative name or with additional descriptive context) or expresses the same concept with different wording, it 28 should be considered correct. 4. Criteria for INCORRECT Judgement: Judge as \"incorrect\" if any of the following apply: a. The Predicted Answer misses essential information that changes the meaning of the Reference Answer (not just additional descriptive details), or b. The Predicted Answer adds contradictory or incompatible information that changes the intended meaning (mere additional non-contradictory details are acceptable), or c. The Predicted Answer is ambiguous, indirect, or evasive and fails to provide clear, direct response to the Question, or d. The Predicted Answer is semantically different from the Reference Answer in meaningful way that would lead to different understanding or action. Special Considerations: Preposition variations: Differences in prepositions (e.g., \"at\" vs \"in\", \"on\" vs \"upon\") are generally acceptable unless they fundamentally change the meaning in the specific context. Parenthetical information: Inclusion of additional information in parentheses (e.g., alternative names, explanations) is acceptable if the core entity/answer remains the same. Context sensitivity: Consider the specific context of the question. For factual questions, focus on whether the same fact is conveyed. Output Format: 1 { 2 3 4 } \" rationale \": \" your rationale for the judgement \", \" judgement \": \" correct or incorrect \" LLM-as-a-Judge Prompt for Deep Research Tasks System Role: You are strict, meticulous, and objective research article evaluation expert. You excel at using specific assessment criteria to deeply compare two articles on the same task, providing precise scores and clear justifications. Task Background: There is deep research task, and you need to evaluate two research articles written for this task. The evaluation is conducted across four dimensions: Comprehensiveness, Insight, Instruction Following, and Readability. Research Task: {task_prompt} Articles to Evaluate: Article 1: {article_1} Article 2: {article_2} Evaluation Criteria: The articles should be evaluated and compared based on the following evaluation criteria list. Each criterion includes detailed explanation and should be carefully considered. {criteria_list} Instruction: Your Task Please strictly evaluate and compare <article_1> and <article_2> based on each criterion in the <criteria_list>. You must: 1. Analyze Each Criterion: Assess how each article fulfills the requirements of the given criterion. 2. Comparative Evaluation: Compare the performance of the two articles for each criterion, explicitly referencing the article content and the criterion explanation. 3. Score Separately: Assign score to each article for each criterion on scale from 0 to 10. Scoring Rules: 02: Very poor performance; almost completely fails to meet the criterion. 29 24: Poor performance; minimally meets the criterion with major deficiencies. 46: Average performance; basically meets the criterion. 68: Good performance; largely meets the criterion with notable strengths. 810: Excellent performance; fully meets or exceeds the criterion. Output Format Requirements: The output must strictly follow the JSON format below. Do not include any introduction, summary, or unrelated content. Evaluation should start from Standard 1 and proceed sequentially through all criteria. 1 { 2 \" comprehensiveness \": [ \" criterion \": \" Text content of the first comprehensiveness evaluation criterion \", \" analysis \": \" Comparative analysis \", \" article_ 1 _score \": 0 -10, \" article_ 2 _score \": 0 -10 { }, ... ], \" insight \": [ { \" criterion \": \" Text content of the first insight evaluation criterion \" , \" analysis \": \" Comparative analysis \" , \" article_ 1 _score \": 0 -1 0, \" article_ 2 _score \": 0 -1 0 }, ... ], ... 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 21 } Additional Requirement: Ensure that the output JSON is fully parsable and that all characters that may cause JSON parsing errors are properly escaped."
        }
    ],
    "affiliations": [
        "OPPO"
    ]
}