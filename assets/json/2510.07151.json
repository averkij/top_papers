{
    "paper_title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
    "authors": [
        "Egor Cherepanov",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 5 1 7 0 . 0 1 5 2 : r ELMUR: EXTERNAL LAYER MEMORY WITH UPDATE/REWRITE FOR LONG-HORIZON RL Egor Cherepanov1,2 Alexey K. Kovalev1,2 Aleksandr I. Panov1,2 1 Cognitive AI Lab, Moscow, Russia 2 IAI MIPT, Moscow, Russia {cherepanov, kovalev, panov}@iaipht.ru elmur-paper.github.io ABSTRACT Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves 100% success rate on synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers simple and scalable approach to decision making under partial observability."
        },
        {
            "title": "INTRODUCTION",
            "content": "Imagine robot cooking pasta: it stirs once, adds salt, and later adds salt again, repeating until the dish is inedible. The issue is simple: the robot cannot remember if salt was already added, since it dissolves invisibly, nor how much is still in the container. This is case of partial observability the world rarely reveals all necessary information. Humans recall past actions effortlessly, but robots lack this ability. Though effective in controlled settings (Kim et al., 2024; Black et al., 2024), robots often fail under partial observability (Fang et al., 2025; Cherepanov et al., 2025). Standard recurrent (Ouyang et al., 2025) and transformer (Gao et al., 2025) models rely heavily on short observation windows, making them brittle under long-horizon dependencies and sparse signals. This motivates hybrid memory-augmented transformers that explicitly store and retrieve past information (Fang et al., 2025; Shi et al., 2025). Within the Reinforcement Learning (RL) paradigm (Sutton et al., 1998), long-horizon challenges are compounded by sample inefficiency and sparse rewards: real-world exploration is costly and unsafe, while simulation suffers from sim-to-real gap (Zhang et al., 2025a). Offline RL mitigates this with pre-collected datasets (Levine et al., 2020), but usually assumes dense feedback; reshaping sparse rewards demands domain knowledge and risks bias (Wu et al., 2021; Mu et al., 2024; Wang et al., 2025a). In robotics, delayed feedback makes long-term memory indispensable. complementary paradigm is Imitation Learning (IL) (Zare et al., 2024), whose simplest form, Behavior Cloning (BC), reduces control to supervised learning on demonstration pairs. Building on this idea, recent Vision-Language-Action (VLA) models (Brohan et al., 2022; Team et al., 2024; Kim et al., 2024) scale with large datasets, yet their fixed transformer windows (Fang et al., 2025; Shi et al., 2025) leave three challenges: (i) extending context without quadratic cost, (ii) mitigating truncationinduced forgetting, and (iii) retaining task-relevant information across long horizons. This motivates our central question: how can we equip IL policies with efficient long-term memory to solve long-horizon, partially observable tasks? 1 Figure 1: ELMUR overview. Each transformer layer is augmented with an external memory track that runs in parallel with the token track. Tokens attend to memory through mem2tok block, while memory embeddings are updated from tokens through tok2mem block. LRU block selectively rewrites memory via replacement or convex blending, ensuring bounded yet persistent storage. This design enables token-memory interaction and long-horizon recall beyond the attention window. To address these challenges, we introduce ELMUR (External Layer Memory with Update/Rewrite), transformer architecture in which every layer is augmented with structured external layer memory (Figure 1). ELMUR combines three ingredients: (i) layer-local memory embeddings that persist across segments, (ii) bidirectional tokenmemory read/write interaction via cross-attention (mem2tok, tok2mem), and (iii) Least Recently Used (LRU) update block that refreshes memory through replacement or convex blending, balancing stability and adaptability. This design enables efficient segment-level recurrence and extends retention of task-relevant information up to 100,000 beyond the native attention window, making long-horizon decision making feasible in robotics. the robotic MIKASAWe evaluate ELMUR on the synthetic T-Maze (Ni et al., 2023), Robo (Cherepanov et al., 2025) suite of sparse-reward manipulation tasks with visual observations, and the diverse POPGym benchmark (Morad et al., 2023a), all designed to test memory under partial observability. ELMUR achieves 100% success rate on T-Maze corridors up to one million steps, nearly doubles baseline performance on MIKASA-Robo, and obtains the top score on 24 of 48 POPGym tasks. These results demonstrate that ELMUR enables stable retention of task-relevant information, efficient long-term storage, and robust generalization under partial observability. Our contributions are twofold: We propose ELMUR, transformer with layer-local external memory, bidirectional tokenmemory cross-attention, and an LRU-based update rule rewriting memory via replacement or convex blending (Section 3). This design extends memory horizons far beyond the attention window. We empirically demonstrate that ELMUR achieves robust generalization under partial observability across synthetic, robotic, and puzzle/control tasks (Section 5). We provide theoretical analysis of LRU-based memory dynamics, establishing formal bounds on forgetting, retention horizons, and stability of memory embeddings (Section 4)."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Many real-world robotic and control tasks involve partial observability, where the agent cannot directly access the true system state (Lauri et al., 2022). This setting is modeled as partially observable Markov decision process (POMDP), defined as the tuple (S, A, O, T, Z, R, ρ0, γ), with latent state space S, action space A, and observation space O. The transition dynamics are : (S), where (s s, a) is the probability of reaching after taking in s. The observation function : (O) specifies Z(o s, a), the probability of observing after 2 Algorithm 1 ELMUR layer update for segment at layer ℓ. Inputs are token hidden states RBLd, memory (m, p) with RBM d, anchors ZBM , and absolute times t. Outputs are updated hidden states and memory (m, p). // Input embedding (before first layer) to encode observation 1: ObsEncoder(o) // Token track sequence processing and enrichment with information from memory 2: AddNorm(cid:0)h + SelfAttention(h; causal mask)(cid:1) 3: Brel RelativeBias(t, p) 4: AddNorm(cid:0)h + CrossAttention(Q=h, K=m, =m; noncausal mask, Bread)(cid:1) 5: AddNorm(cid:0)h + TokenFFN(h)(cid:1) 6: // bias for adding temporal dependence // Output decoding (after final layer) 7: ActionHead(h) // map to action distribution and compute loss // Memory track // reversed bias for write 8: Brel RelativeBias(p, t) 9: AddNorm(cid:0)m + CrossAttention(Q=m, K=h, =h; noncausal mask, Bwrite)(cid:1) 10: AddNorm(cid:0)u + MemoryFFN(u)(cid:1) 11: (m, p) LRU(m, p, u, t) 12: return h, (m, p) reaching under action a. The reward function is : R, the initial state distribution is ρ0 (S), and γ (0, 1) is the discount factor. In the special case of full observability, the observation equals the state (ot st), reducing the POMDP to Markov decision process (MDP). The optimal policy then depends only on the current state, π(at st). In the general POMDP case, however, the agent cannot access st directly and must rely on the full history ht = (o0, a0, o1, a1, . . . , ot), yielding π(at ht). practical alternative is to approximate history with learned memory state mt = fϕ(mt1, ot, at1), πθ : (A), πθ(at mt), where fϕ is a, for instance, recurrent (Hausknecht & Stone, 2015) or memoryaugmented (Parisotto et al., 2020) update rule."
        },
        {
            "title": "3 METHOD",
            "content": "is needed. Many real-world decision-making tasks involve long horizons and partial observability, where key information may appear thousands of steps before it Standard transformers are limited by fixed attention window: naive extensions of context length increase cost quadratically, while truncation causes forgetting. Efficient long-term reasoning thus requires mechanism to store and retrieve taskrelevant information across long trajectories. To this end, we propose ELMUR (External Layer Memory with Update/Rewrite), GPTstyle (Radford et al., 2019) transformer decoder augmented with structured external memory. Unlike architectures that simply cache hidden states (Dai et al., 2019), ELMUR equips each layer with its own memory track and explicit readwrite operations, enabling persistent storage and selective updating via the LRU memory management. Figure 2: LRU-based memory management in ELMUR. Each layer maintains memory slots, initialized with random vectors (green). As new segments arrive, tokens write updates into empty slots (purple) by full replacement. Once all slots are filled, the least recently used slot is refreshed via convex update with parameter λ that blends new content with the previous memory (grey). Anchors below each row indicate the timestep of the most recent update. This scheme ensures bounded capacity while preserving longhorizon information. ELMUR Overview. As shown in Figure 1, each ELMUR layer has two coupled tracks. The token track processes observations into actions, while the memory track persists across segments. Both 3 interact through cross-attention: memory shapes token representations, and tokens update memory. Interaction occurs via mem2tok (read) and tok2mem (write) blocks, modulated by relative biases from token timesteps and memory anchors. Trajectories are split into segments, processed sequentially for efficiency and recurrent memory updates. At each segments end, hidden states update memory, carried forward. LRU memory management fills empty slots first, then refreshes the least used slot by convexly blending old and new information. This bidirectional design provides temporally grounded memory for long-horizon decisions. Algorithm 1 summarizes the method. Segment-Level Recurrence. Feeding infinitely long sequences into transformer is infeasible, since self-attention scales quadratically. Splitting into shorter segments reduces cost but complicates information flow. Segment-level recurrence addresses this by treating the transformer as an RNN over segments, passing memory from one segment to the next (Dai et al., 2019; Bulatov et al., 2022). In ELMUR, this memory is realized as layer-local external memory instead of cached activations. Each layer maintains memory that is read within the current segment and updated before moving to the next. Formally, with context length L, trajectory of length is partitioned into = /L segments Si: h(i) = TokenTrack(cid:0)Si, sg(m i1)(cid:1), where h(i) RBLd denotes the hidden states of tokens in segment i, computed from the segment Si and the detached memory sg(m i1) carried from the previous segment. Token Track. Within each segment Si, observations are encoded into token embeddings RLd, where is the model dimension. The token track models local dependencies and augments them with information from memory RM d. Standard transformers rely on fixed-window selfattention, whereas ELMUR also retrieves information from its external memory via cross-attention, allowing predictions to depend not only on recent tokens but on distant past events stored in memory. Self-attention, equipped with relative positional encodings (Dai et al., 2019) and causal mask, models local dependencies within the segment: hsa = AddNorm(x + SelfAttention(x)) , (1) where AddNorm() denotes residual connection followed by normalization. Long-term context is handled by external memory. Tokens hidden states hsa then query memory via the mem2tok: hmem2tok = AddNorm(hsa + CrossAttention(Q = hsa, K, = m)) . (2) Here memory embeddings act as keys and values, with non-causal mask and relative bias reflecting token-memory temporal distance. Finally, representations are refined with feed-forward network (FFN). In contrast to popular Decision Transformer (DT) Chen et al. (2021) that employ standard MLP-based FFN, we adopt DeepSeek-MoE FFN (Dai et al., 2024), following the design of DeepSeek-V3 (Liu et al., 2024a). Mixture-of-Experts (MoE) improve parameter efficiency and specialization by routing tokens to sparse set of experts, scaling capacity without proportional compute. This design enables expressive updates while keeping inference efficient: = AddNorm(hmem2tok + FFN(hmem2tok)) . (3) The resulting hidden states are then passed to the action head, applied only after the final layer. Training is supervised, minimizing the error between predicted and demonstrated actions, using mean squared loss for continuous spaces and cross-entropy for discrete ones. The loss backpropagates through the entire network to update model parameters. Memory Track. Reading from memory is not enough for long-horizon reasoning; the model must also write new information. Without an explicit write path, past events would be forgotten or cached inefficiently. The memory track addresses this by allowing tokens to update persistent memory, retaining salient information while overwriting less useful content. Each layer maintains its own memory embeddings RM d. After processing segment, token states update memory through the tok2mem block: mtok2mem = AddNorm(m + CrossAttention(Q = m, K, = h)) . As in mem2tok, non-causal mask is applied, but the relative bias is reversed to favor temporally aligned memory embeddings. Updates are then refined by FFN with residual connection: (4) mnew = AddNorm(mtok2mem + FFN(mtok2mem)) , (5) 4 analogous to the token track, the FFN uses DeepSeek-MoE block instead of standard MLP. Finally, mnew is merged with existing slots via the LRU rule (Figure 2, Algorithm 2), filling empty slots first and otherwise refreshing the least recently used by convex blending. This keeps memory bounded yet consistently updated with relevant information. Relative Bias. When memory extends across multiple segments, absolute indices become ambiguous: the same token position may correspond to different points in the trajectory. To resolve this, the model requires signal that encodes relative distances between tokens and memory entries. ELMUR provides this signal through learned relative bias added to cross-attention logits: Attn(Q, K) = QK dh + Brel. (6) The bias Brel is derived from pairwise offsets = (tp) between token position and memory anchor (the last update time of slot). Offsets are clamped to [Dmax+1, Dmax1], where Dmax is the maximum relative distance supported by the bias table. These clamped values are shifted into [0, 2Dmax2] and used to index learnable embedding table R(2Dmax1)H , where is the number of attention heads. Each offset corresponds to per-head embedding E[] RH , and stacking these indices produces Brel = (cid:40)E[t p] RBHLM , mem2tok (read) E[p t] RBHM L, tok2mem (write). (7) In the read path (mem2tok), the bias prioritizes retrieval from temporally close memory embeddings while keeping disIn the write path tant ones accessible. (tok2mem), offsets are reversed, guiding updates toward memory embeddings aligned with the writing tokens. Both directions draw from the same embedding table but can learn distinct patterns. By relying on relative rather than absolute timestep, ELMUR ensures consistent and coherent memory interactions across long horizons. Algorithm 2 LRU update for layer memory. Inputs: current memory (m, p) (may be uninitialized), candidate updates u, newest segment time t, blend λ [0, 1], init scale σ. Output: updated memory (m, p). // Initialization (cold start) 1: if m, uninitialized then 2: (0, σ2I) 1 3: 4: end if // initial slots // sentinel anchors // Choose write index first(empty) α 5: empty (p < 0) 6: if any empty then 7: 8: 9: else 10: 11: 12: end if // use first empty slot // full replacement // Integrate // convex blend // least recently used arg minj pj α λ Memory Management with LRU. External memory must remain bounded: storing every token is infeasible, while naive truncation risks catastrophic forgetting. principled policy is needed to decide which slots to refresh or preserve as new content arrives. ELMUR employs Least Recently Used (LRU) block (Figure 2, Algorithm 2) that manages slots per layer, each holding vector and an anchor (its last update time). By always updating the least recently used slot, the block ensures bounded capacity while retaining context. At training start, initialization samples embeddings from (0, σ2I) and marks them empty. While empty slots remain, full replacement inserts new vectors directly. Once all slots are filled, the block switches to convex update, blending the oldest slot with new content: i+1 13: blend α uj + (1 α) mj 14: m; 15: p; 16: return (m, p) blend new + (1 λ) mi j, = λ mi+1 (8) where λ [0, 1] is tunable hyperparameter that controls the balance between overwriting and retention. By adjusting λ, one can choose whether memory favors fast plasticity (larger λ) or longterm stability (smaller λ). This policy uses memory capacity fully before overwriting and applies gradual blending thereafter, enabling bounded yet persistent long-horizon memory. 5 By combining token-level processing with an explicit memory system, ELMUR offers three core advantages: (i) relative-bias cross-attention provides temporally grounded read-write access, (ii) the LRU-based manager ensures bounded capacity while remaining adaptive, and (iii) segment-level recurrence enables scalable learning over long horizons."
        },
        {
            "title": "4 THEORETICAL ANALYSIS",
            "content": "Understanding the retention properties of ELMURs memory is crucial for characterizing its ability to handle long-horizon dependencies. In this section, we analyze how information is preserved or forgotten under the LRU update mechanism. We derive bounds on memory retention and effective horizons, and connect these results to the empirical behaviors observed in long-horizon tasks. At the core of ELMURs memory module is the convex update rule with blending factor λ [0, 1]. Let fix memory embedding at segment index i. If this memory embedding is selected for update new , the rule (Algorithm 2) is i+1 with new content i+1 , while all other memory embeddings = remain unchanged: i+1 If the memory embedding was empty, the update reduces to full replacement i+1 new + (1 λ) = λ i+1 = n. = i+1 new . Proposition 1 (Exponential Forgetting). After overwrites of memory embedding j, the content evolves as i+k = (1 λ)k j + (cid:88) u=1 λ(1 λ)ku i+u new , (9) where i+u the coefficient of the initial content write performed τ updates earlier is λ(1 λ)τ 1. new denotes the write at update i+u (see Appendix A.1 for full derivation). Consequently, after overwrites is (1 λ)k, and the contribution of the Corollary (Half-life). The number of overwrites k0.5 after which the contribution of k0.5 = ln(1/2) ln(1λ) = accelerates overwriting. halves is as λ 0. Thus, smaller λ extends retention, while larger λ ln(1λ) ln 2 λ , ln 2 Effective horizon in environment steps. Since only one memory embedding is updated per segment of length L, memory is overwritten once every segments in expectation. The effective retention horizon H(ϵ) thus quantifies how many environment steps stored contribution remains ln(ϵ) influential before its weight decays below negligible threshold ϵ, i.e., H(ϵ) = ln(1λ) . In ln(1λ) ln 2 as λ 0. λ , particular, the half-life in environment steps is H0.5 = ln 2 Unlike models where all memory is updated at every step (like RNNs), ELMURs LRU policy ensures (i) memory embeddings not selected for overwrite retain their content exactly until replacement, and (ii) once selected, their contributions decay exponentially with rate λ. This produces retention horizon that scales linearly with both the number of memory embeddings and the segment length L, providing conservative lower bound. In practice, effective horizons are often much longer (Figure 3). Proposition 2 (Memory Boundedness). natural question is whether repeated convex updates could cause memory values to grow without limit. We show that, under standard bounded-input assumptions, the norm of every memory embedding remains uniformly bounded throughout training and inference. Suppose that every new write is norm-bounded, new for some constant > 0, and the initial memory satisfies 0 C. Then for all segments and slots j, it holds that j C. Since each update is convex combination of the previous and bounded new values, the memory embedding always remains inside the closed ball of radius C. This guarantees stability of activations even across arbitrarily long trajectories. See Appendix A.2 for the detailed proof."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate ELMUR on synthetic (Ni et al., 2023) tasks, 48 POPGym puzzle/control tasks (Morad et al., 2023a), and robotic manipulation (Cherepanov et al., 2025), all designed to test memory under partial observability. Our study is guided by the following research questions (RQs): 1. RQ1: Does ELMUR retain information across horizons far beyond its attention window? 2. RQ2: How well does ELMUR generalize to shorter and longer sequences? 3. RQ3: Is ELMUR effective on manipulation tasks with visual observations? 4. RQ4: How consistent is ELMUR across puzzles, control, and robotics tasks? 5. RQ5: What is the impact of components of ELMUR on its memorization?"
        },
        {
            "title": "5.1 BENCHMARKS AND BASELINES",
            "content": "We evaluate ELMUR on three benchmarks designed to isolate memory (Appendix, Figure 7). The T-Maze requires recalling an early cue after traversing long corridor with sparse rewards. The MIKASA-Robo suite provides robotic tabletop tasks with RGB observations and continuous actions, including color-recall (RememberColor) and delayed reversal (TakeItBack). Finally, POPGym offers diverse collection of partially observable puzzles and control environments for evaluating general memory use. Detailed descriptions can be found in Appendix A.4. We compare against baselines spanning sequence models and offline RL for long-horizon tasks. We include transformers Decision Transformer (DT) (Chen et al., 2021) and Recurrent Action Transformer with Memory (RATE) (Cherepanov et al., 2023) as representative architectures for memory-augmented policy learning. We also evaluate DMamba (Ota, 2024), state-space model with efficient recurrence, as recent alternative to attention. For IL/offline RL, we use Behavior Cloning (BC) via MLP as the simplest supervised baseline, Conservative Q-Learning (CQL) (Kumar et al., 2020) as strong offline RL method, and Diffusion Policy (DP) (Chi et al., 2023) as stateof-the-art generative policy. Together, these span transformer, state-space, and offline/generative approaches, providing competitive reference set for evaluation. We do not compare with online RL baselines, since they assume interactive data collection with exploration, yielding incomparable training budgets. Likewise, we omit real-robot experiments to avoid confounds such as latency, resets, and safety constraints, focusing instead on controlled, reproducible studies. Experimental Setup. For RQ1, we test T-Maze cue retention by training with short contexts (L=10, S=3) and evaluating on corridors up to 106 steps. For RQ2, we train on 7 T-Maze lengths distributions (9900) and validate on 11 shorter/longer ones (99600) to assess interpolation and extrapolation. For RQ3, we use MIKASA-Robo tasks, training by imitation from expert demonstrations and evaluating zero-shot. For RQ4, we compare T-Maze, POPGym-48, and MIKASA-Robo to test robustness across synthetic puzzles, control, and robotics. For RQ5, we ablate RememberColor3-v0, varying , λ, σ, and (L, S), and remove relative bias, LRU, and per-layer memory to measure component contributions. Evaluation Protocol. Unless stated otherwise, each model is trained with three (four for T-Maze) independent runs (different initialization). For each run we evaluate on 100 episodes with distinct environment seeds and compute the run mean. We then report the grand mean standard error of the mean (SEM) across the three run means. For per-task leaderboards (e.g., POPGym-48) we apply this protocol per task and aggregate as specified in the benchmark. Training Details and Hardware. All models are trained from scratch under the same data budgets and preprocessing. We use segment-level recurrence with detached memory between segments; losses are applied on each processed segment. Optimizers, schedulers, and hyperparameters follow the task-specific configuration table in Appendix, Table 5. All experiments were run on single NVIDIA A100 (80 GB) per job. Training/evaluation code paths, seeds, and environment versions are fixed across methods for reproducibility. Figure 3: Success rate on the T-Maze task as function of inference corridor length. ELMUR achieves 100% success rate up to corridor lengths of one million steps. In this figure, the context length is = 10 with = 3 segments; thus ELMUR carries information across horizons 100,000 times longer than its context window. 5.2 RESULTS We evaluate ELMUR on T-Maze, MIKASARobo, and POPGym, addressing RQ1RQ5 on retention, generalization, manipulation, crossdomain robustness, and ablations. RQ1: Retention beyond attention. To test memory retention, we train on T-Maze corridors of length while restricting the context size to < , forcing the model to solve tasks where the cue must be preserved beyond the native attention span. At validation, we evaluate on much longer Figure 4: Generalization of ELMUR across T-Maze lengths. Each cell shows success rate (mean standard error) for training vs. validation lengths. ELMUR transfers perfectly: models trained on shorter sequences retain 100% success up to 9600 steps. Training lengths were split into three equal segments. corridors up to one million steps without increasing L, thereby probing memory retention far beyond the training horizon. ELMUR achieves 100% success even under this extreme extrapolation (Figure 3), implying retention horizons nearly 100,000 larger than the attention window (L=10 with only S=3 segments used during training). RQ2: Generalization across sequence lengths. We train ELMUR on T-Maze with short contexts (3 to 300 steps) and then evaluate across 11 validation lengths ranging from 9 to 9600 steps. The model transfers seamlessly in both directions: it solves tasks shorter than those seen during training without overfitting to fixed scale, and it also extrapolates to sequences orders of magnitude longer. As shown in Figure 4, ELMUR maintains 100% success across all train/test pairs, demonstrating robust generalization beyond the training horizon. RQ3: Manipulation with visual observations. Results in Table 1 indicate that ELIn MUR achieves higher success rates than other baselines on the MIKASA-Robo tasks. TakeItBack-v0, it obtains 0.780.03 compared to 0.420.24 for the next-best model, and in RememberColor[3,5,9]-v0 its performance remains stable as the number of distractors increases. Overall, ELMUR shows more reliable performance under visual interference in manipulation tasks with pixel inputs. Table 1: Success rates (mean standard error) on MIKASA-Robo tasks, averaged over 3 runs with 100 evaluation seeds. ELMUR outperforms baselines, showing stronger memory in manipulation. Task RATE DT BC-MLP CQL-MLP DP ELMUR (ours) RememberColor3-v0 RememberColor5-v0 RememberColor9-v0 TakeItBack-v0 0.650.04 0.130.03 0.090.02 0.420.24 0.010.01 0.070.05 0.010.01 0.080.04 0.270.03 0.120.01 0.120.02 0.330.10 0.290.01 0.150.02 0.150.01 0.040. 0.320.01 0.100.02 0.170.01 0.050.02 0.890.07 0.190.03 0.230.02 0.780.03 8 Table 2: Aggregated returns on 48 POPGym tasks. -6.8 -11.9 5.1 Robustness All (48) Puzzle (33) Reactive (15) RATE DT Rand. BC-MLP BC-LSTM ELMUR 9.5 5.8 -12.2 0.45 -3.5 -14.6 9.3 9.1 2.3 RQ4: across domains. Across synthetic (T-Maze), control/puzzle (48 POPGym), and robotic (MIKASARobo) benchmarks, ELMUR consistently outperforms baselines, generalizing across diverse modalities, actions, and rewards. On POPGym, it achieves the best overall score (10.4), with the largest gains on memory puzzles (1.2 vs. 0.45 for RATE; DT and BC-LSTM score below zero), showing the importance of explicit memory for long-term dependencies. On reactive tasks, ELMUR stays competitive without sacrificing puzzle performance, ranking first on 24 of 48 tasks (full results in Table 4). Figure 5 shows consistent per-task gains over DT, especially on memory-intensive puzzles. Improved retention comes with little overhead: on T-Maze, ELMUR has 2.1M parameters (vs. 1.7M for RATE, 1.8M for DT) yet runs faster per step (6.80.5 ms) than RATE (7.20.3 ms) and DT (10.70.1 ms). Efficiency stems from (i) short attention window with long-term context handled by bounded memory, so complexity depends on memory size not sequence length, and (ii) MoE feed-forward layers, which raise capacity without proportional compute. Thus, explicit memory is both effective and efficient for long-horizon RL. 10.4 1.2 9.2 9.0 -0.2 9. Table 3: Ablation study results. Score Setting RQ5: Ablation Study. We ablate ELMURs memory design on RememberColor3-v0 (Figure 6, Table 3). Unless noted, models use per-layer memory, relative-bias tokenmemory crossattention, and LRU-based updates; shared memory denotes embeddings shared across layers. In Figure 6 (bd) the LRU factor is fixed to λ = 0 to isolate other effects. Results average three runs of 20 episodes. Performance scales with memory size : when (the number of segments needed), success is nearperfect; when < , accuracy drops sharply, especially near (Figure 6, cd). Intermediate blending (λ 0.40.6) is unstable (Figure 6, a), while larger initialization σ mitigates collapse (Figure 6, b). Finer recurrence (shorter segments, larger ) stresses capacity unless scales accordingly. Component ablations confirm that capacity and LRU dominate. Removing LRU leaves stale entries, and removing both LRU and relative bias prevents effective retrieval. Relative bias gives modest gains, while shared memory degrades performance, underscoring the value of layer-local design. Finally, replacing MoE-FFN with MLP-FFN preserves accuracy while improving computational efficiency. Baseline ELMUR Shared memory No rel. bias No LRU No rel. bias; No LRU MoE MLP 1.00 0.00 0.45 0.03 0.95 0.05 0.43 0.22 0.22 0.11 1.00 0. To confirm that memory mechanisms do not harm performance on fully observable MDPs, we evaluated all models on the simple control task CartPole-v1 (Towers et al., 2024). ELMUR, RATE, RMT, TrXL, BC-MLP, BC-LSTM, and CQL all achieved the maximum return of 500 0, showing that adding memory does not break performance in standard MDP settings."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Manipulation. Transformer approaches to robotic manipulation can be broadly categorized by their underlying design principles. Perception-centric visuomotor transformers focus on multi-view or 3D perception to improve near fully observable control (Shridhar et al., 2023; Goyal et al., 2024). Figure 5: ELMUR compared to DT on all 48 POPGym tasks. Each model was trained with three independent runs, validated over 100 episodes each. Bars show the mean performance with 95% confidence intervals computed over these three means. ELMUR achieves consistent improvements over DT, with the largest gains on memory-intensive puzzles. 9 Figure 6: Ablations of ELMURs memory hyperparameters on RememberColor3-v0. (a) LRU blending factor (λ), (b) memory embeddings initialization (σ), (c) number of memory embeddings (M ), and (d) segment configuration (L S). Curves compare settings where the number of memory embeddings is smaller than the required segments to solve the task (M < ) versus larger (M > ). Results show that sufficient memory capacity (M ) yields stable success, while under-provisioned memory (M < ) is highly sensitive to λ, σ, and segmentation. Sequence/skill modeling distills demonstrations into reusable action chunks but remains bottlenecked by limited context (Huang et al., 2023; Kobayashi et al., 2025). Planning/value-augmented transformers integrate transformers with planning or value learning for closed-loop control under finite context (Zhang et al., 2025b; Hu et al., 2025). Alternative backbones adopt state-space models or diffusion for efficiency, but without persistent memory (Liu et al., 2024b; Chi et al., 2023). Scaling to VLA broadens task coverage with language but still suffers from fixed horizons, with some remedies via summarization, feature banks, or hierarchy (Zitkovich et al., 2023; Team et al., 2024; Kim et al., 2024; Fang et al., 2025; Shi et al., 2025). ELMUR differs by training as standard IL transformer while removing the context bottleneck through structured, layer-local external memory. Memory. Efforts to extend sequence models to long horizons take several forms. Implicit recurrence and state-space models compress history in hidden dynamics, offering efficiency but little control over forgetting (Beck et al., 2024; Gu & Dao, 2023). External memory with learned access provides addressable storage but complicates optimization (Graves et al., 2016; Santoro et al., 2016). Transformer context extension retains history via caches or auxiliary slots but keeps memory peripheral (Dai et al., 2019). In RL, memory is often implemented through episodic buffers for salient events (Lampinen et al., 2021) or sequence-model adaptations that retrofit transformers for recurrence (Parisotto et al., 2020; Cherepanov et al., 2024). Architectures vary in integration: RATE (Cherepanov et al., 2023) concatenates memory with tokens, Memformer (Wu et al., 2020) uses global slots, and Block-Recurrent Transformers (Hutchins et al., 2022) recycle hidden states. ELMUR instead gives each layer an external memory with dedicated mem2tok/tok2mem crossattention and LRU updates, yielding bounded memory for long-horizon tasks (Appendix A.5)."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced ELMUR, transformer architecture with layer-local external memory, bidirectional tokenmemory cross-attention, and an LRU-based update rule. Unlike prior methods, ELMUR integrates explicit memory into every layer, achieving retention horizons up to 100,000 beyond the native attention window. Our analysis establishes formal guarantees on half-life and boundedness under convex blending, and experiments on T-Maze, 48 POPGym tasks, and MIKASA-Robo demonstrate consistent improvements over strong baselines, underscoring reliable credit assignment under partial observability. We envision ELMUR as simple and extensible framework for longhorizon decision-making with scalable memory in sequential control."
        },
        {
            "title": "REFERENCES",
            "content": "Amir Hosein Khas Ahmadi. Memory-based graph networks. University of Toronto (Canada), 2020. Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. Advances in Neural Information Processing Systems, 37:107547 107603, 2024. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi 0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:1107911091, 2022. Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In Conference on Robot Learning, pp. 39093928. PMLR, 2023. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021. Egor Cherepanov, Alexey Staroverov, Dmitry Yudin, Alexey Kovalev, and Aleksandr Panov. Recurrent action transformer with memory. arXiv preprint arXiv:2306.09459, 2023. Egor Cherepanov, Nikita Kachaev, Artem Zholus, Alexey Kovalev, and Aleksandr Panov. Unraveling the complexity of memory in rl agents: an approach for classification and evaluation. arXiv preprint arXiv:2412.06531, 2024. Egor Cherepanov, Nikita Kachaev, Alexey Kovalev, and Aleksandr Panov. Memory, benchmark & robots: benchmark for solving complex tasks with reinforcement learning. arXiv preprint arXiv:2502.10550, 2025. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, pp. 02783649241273668, 2023. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, and DiImitating task and motion planning with visuomotor transformers. arXiv preprint eter Fox. arXiv:2305.16309, 2023. Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Erniedoc: retrospective long-document modeling transformer. arXiv preprint arXiv:2012.15688, 2020. Kevin Esslinger, Robert Platt, and Christopher Amato. Deep transformer q-networks for partially observable reinforcement learning. arXiv preprint arXiv:2206.01078, 2022. Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, and Jiafei Duan. Sam2act: Integrating visual foundation model with memory architecture for robotic manipulation. arXiv preprint arXiv:2501.18564, 2025. Niklas Funk, Julen Urain, Joao Carvalho, Vignesh Prasad, Georgia Chalvatzaki, and Jan Peters. Actionflow: Equivariant, accurate, and efficient policies with spatially symmetric flow matching. arXiv preprint arXiv:2409.04576, 2024. Kai Gao, Fan Wang, Erica Aduh, Dylan Randle, and Jane Shi. Must: Multi-head skill transformer for long-horizon dexterous manipulation with skill progress. arXiv preprint arXiv:2502.02753, 2025. 11 Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pp. 694710. PMLR, 2023. Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, and Dieter Fox. Rvt-2: Learning precise manipulation from few demonstrations. arXiv preprint arXiv:2406.08545, 2024. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adri`a Puigdom`enech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using neural network with dynamic external memory. Nature, 538:471476, 2016. URL https://api.semanticscholar.org/CorpusID:205251479. Jake Grigsby, Justin Sasek, Samyak Parajuli, Daniel Adebi, Amy Zhang, and Yuke Zhu. Amago-2: Breaking the multi-task barrier in meta-reinforcement learning with transformers. Advances in Neural Information Processing Systems, 37:8747387508, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. ByungOk Han, Jaehong Kim, and Jinhyeok Jang. dual process vla: Efficient robotic manipulation leveraging vlm. arXiv preprint arXiv:2410.15549, 2024. Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In AAAI fall symposia, volume 45, pp. 141, 2015. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735 1780, 1997. URL https://api.semanticscholar.org/CorpusID:1915014. Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Martın-Martın, Peter Stone, Kuo-Hao Zeng, and Kiana Ehsani. Flare: Achieving masterful and adaptive robot policies with large-scale reinforcement learning fine-tuning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 36173624. IEEE, 2025. Xiaoyu Huang, Dhruv Batra, Akshara Rai, and Andrew Szot. Skill transformer: monolithic policy for mobile manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1085210862, 2023. Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, Tan, Navonil Majumder, Soujanya Poria, et al. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. Advances in neural information processing systems, 35:3324833261, 2022. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi {0.5}: vision-languageaction model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li FeiFei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2(3):6, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. 12 Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Masato Kobayashi, Thanpimon Buamanee, Yuki Uranishi, and Haruo Takemura. Ilbit: Imitation learning for robot using position and torque information based on bilateral control with transformer. IEEJ Journal of Industry Applications, 14(2):161168, 2025. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in neural information processing systems, 33:11791191, 2020. Andrew Lampinen, Stephanie Chan, Andrea Banino, and Felix Hill. Towards mental time travel: hierarchical memory for reinforcement learning agents. Advances in Neural Information Processing Systems, 34:2818228195, 2021. Mikko Lauri, David Hsu, and Joni Pajarinen. Partially observable markov decision processes in robotics: survey. IEEE Transactions on Robotics, 39(1):2140, 2022. Hung Le, Kien Do, Dung Nguyen, Sunil Gupta, and Svetha Venkatesh. Stable hadamard memory: Revitalizing memory-augmented agents for reinforcement learning. arXiv preprint arXiv:2410.10132, 2024. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889, 2023. Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Efficient vision-languageaction model for robotic reasoning and manipulation. Advances in Neural Information Processing Systems, 37:4008540110, 2024b. Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, and Shanghang Zhang. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model, 2025. URL https://arxiv.org/abs/2503.10631. Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. POPGym: Benchmarking partially observable reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum? id=chDrutUTs0K. Steven Morad, Ryan Kortvelesy, Stephan Liwicki, and Amanda Prorok. Reinforcement learning with fast and forgetful memory. Advances in Neural Information Processing Systems, 36:72008 72029, 2023b. 13 Tongzhou Mu, Minghua Liu, and Hao Su. Drs: Learning reusable dense rewards for multistage tasks. ArXiv, abs/2404.16779, 2024. URL https://api.semanticscholar.org/ CorpusID:269362133. Tianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. When do transformers shine in rl? decoupling memory from credit assignment. Advances in Neural Information Processing Systems, 36:5042950452, 2023. Toshihiro Ota. Decision mamba: Reinforcement learning via sequence modeling with selective state spaces. arXiv preprint arXiv:2403.19925, 2024. Zikai Ouyang, Kaijun Wang, Junwei Liu, Haibo Lu, and Wei Zhang. Scil: Stage-conditioned imitation learning for multi-stage manipulation. IEEE Control Systems Letters, 2025. Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. arXiv preprint arXiv:1702.08360, 2017. Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers In International conference on machine learning, pp. 74877498. for reinforcement learning. PMLR, 2020. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Subhojeet Pramanik, Esraa Elelimy, Marlos Machado, and Adam White. Agalite: Approximate gated linear transformers for online reinforcement learning. arXiv preprint arXiv:2310.15719, 2023. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visuallanguage-action model. arXiv preprint arXiv:2501.15830, 2025. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Moritz Reuss, Hongyi Zhou, Marcel Ruhle, Omer Erdinc Yagmurlu, Fabian Otto, and Rudolf Lioutikov. FLOWER: Democratizing generalist robot policies with efficient vision-language-action In 7th Robot Learning Workshop: Towards Robots with Human-Level Abilities, flow policies. 2025. URL https://openreview.net/forum?id=ifo8oWSLSq. Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. Associative recurrent memory transformer. arXiv preprint arXiv:2407.04841, 2024. Adam Santoro, Sergey Bartunov, Matthew M. Botvinick, Daan Wierstra, and Timothy P. LillIn International Conference icrap. Meta-learning with memory-augmented neural networks. on Machine Learning, 2016. URL https://api.semanticscholar.org/CorpusID: 6466088. Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, and Gao Huang. Memoryvla: Perceptual-cognitive memory in vision-languageaction models for robotic manipulation. arXiv preprint arXiv:2508.19236, 2025. Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: multi-task transformer for robotic manipulation. In Conference on Robot Learning, pp. 785799. PMLR, 2023. Aleksei Staroverov, Andrey Gorodetsky, Andrei Krishtopik, Uliana Izmesteva, Dmitry Yudin, Alexey Kovalev, and Aleksandr Panov. Fine-tuning multimodal transformer models for generating actions in virtual and real environments. Ieee Access, 11:130548130559, 2023. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Denis Tarasov, Alexander Nikulin, Ilya Zisman, Albina Klepach, Nikita Lyubaykin, Andrei Polubarov, Alexander Derevyagin, and Vladislav Kurenkov. Nina: Normalizing flows in action. training vla models with normalizing flows, 2025. URL https://arxiv.org/abs/2508. 16845. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024. Linji Wang, Tong Xu, Yuanjie Lu, and Xuesu Xiao. Reward training wheels: Adaptive auxiliary rewards for robotics reinforcement learning. arXiv preprint arXiv:2503.15724, 2025a. Yongyi Wang, Lingfeng Li, Bozhou Chen, Ang Li, Hanyu Liu, Qirui Zheng, Xionghui Yang, and Wenxin Li. Synthetic pomdps to challenge memory-augmented rl: Memory demand structure modeling. arXiv preprint arXiv:2508.04282, 2025b. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025a. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025b. Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014. Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. Memformer: memory-augmented transformer for sequence modeling. arXiv preprint arXiv:2010.06891, 2020. Yuhuai Wu, Markus Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Tomizuka, and Stefan Schaal. Learning dense rewards for contact-rich manipulation tasks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 62146221. IEEE, 2021. William Yue, Bo Liu, and Peter Stone. Learning memory mechanisms for decision making through demonstrations. arXiv preprint arXiv:2411.07954, 2024. Maryam Zare, Parham Kebria, Abbas Khosravi, and Saeid Nahavandi. survey of imitation learning: Algorithms, recent developments, and challenges. IEEE Transactions on Cybernetics, 2024. Daniil Zelezetsky, Egor Cherepanov, Alexey Kovalev, and Aleksandr Panov. Re: Frame retrieving experience from associative memory. arXiv preprint arXiv:2508.19344, 2025. Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. Hirt: Enhancing robotic control with hierarchical robot transformers. arXiv preprint arXiv:2410.05273, 2024. 15 Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, et al. Generative artificial intelligence in robotic manipulation: survey. arXiv preprint arXiv:2503.03464, 2025a. Xinyu Zhang, Yuhan Liu, Haonan Chang, Liam Schramm, and Abdeslam Boularias. Autoregressive action sequence learning for robotic manipulation. IEEE Robotics and Automation Letters, 2025b. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17021713, 2025. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROOF OF EXPONENTIAL FORGETTING IN LRU UPDATES Proposition 1 (Exponential Forgetting). Fix slot and segment index i. Suppose this slot is updated times at segments + 1, . . . , + according to = λ new + (1 λ) t1 , = + 1, . . . , + k, with 0 λ 1. Then i+k = (1 λ)k + (cid:88) λ(1 λ)ku i+u new . (10) (11) u=1 after overwrites is (1 λ)k, and the coeffiConsequently, the coefficient of the initial content cient of the write performed τ updates ago (at segment + τ ) is λ(1 λ)τ 1. Proof. We prove Equation 11 by induction on k. Base case = 1. By the update rule, which matches Equation 11 with = 1. i+1 = λ i+1 new + (1 λ) , Induction step. Assume Equation 11 holds for some 1. For + 1, + (1 λ) i+k = λ i+k+1 i+k+1 . new (12) (13) Insert the induction hypothesis for i+k : i+k+ = λ i+k+1 new + (1 λ) (1 λ)k + (cid:34) λ(1 λ)ku i+u new . (14) (cid:35) (cid:88) u=1 Distribute (1 λ) and regroup terms: i+k+1 = (1 λ)k+1m + (cid:88) u=1 λ(1 λ)(k+1)u i+u new + λ i+k+1 new , (15) which is exactly Equation 11 with replaced by + 1. This completes the induction. Finally, the coefficients in Equation 11 form convex combination: (1 λ)k + (cid:88) u= λ(1 λ)ku = (1 λ)k + λ k1 (cid:88) (1 λ)r = 1, r=0 (16) so it is meaningful to call them fractions of contribution. Corollary (Half-life). The number of overwrites k0.5 after which the contribution of satisfies halves Equivalently, (1 λ)k0.5 = 1 2 = k0.5 = k0.5 = ln 2 ln(1 λ) . ln(1/2) ln(1 λ) . Using the Maclaurin series expansion ln(1 λ) λ as λ 0, we obtain Hence, lim λ0 k0.5 λ = ln 2. k0.5 ln 2 λ as λ 0, (17) (18) (19) (20) showing that smaller λ yields longer retention horizons, while larger λ overwrites past content more aggressively. 17 A.2 BOUNDEDNESS OF MEMORY EMBEDDINGS Fix slot and consider its update rule, i+1 = λ i+1 new + (1 λ) , 0 λ 1. (21) We prove the claim by induction on i. Base case. At = 0, the assumption gives 0 C. Induction step. Assume j for some 0. Using the update rule, i+1 = λ i+ new + (1 λ) . By the triangle inequality and the inductive hypothesis, new + (1 λ)m λm i+1 i+1 λC + (1 λ)C = C. (22) (23) Thus i+1 C, completing the induction. Therefore, for all i, the memory embedding satisfies i C. A.3 MEMORY-INTENSIVE ENVIRONMENTS Eventrecall pairs and correlation horizon. Following (Cherepanov et al., 2024), let αt te denote an event of duration starting at time te, and let βtr (αt ) be later decision point te that must recall information from that event. Define the correlation horizon as ξ = tr te + 1. (24) (a) MIKASA-Robo: RememberColor9-v0 (top) and TakeItBack-v0 (bottom). For an environment, collect all eventrecall horizons into the set Ξ = {ξn}n. Recent works have proposed alternative but complementary definitions of memoryintensive environments. For instance, Wang et al. (2025b) formalize memory demand structures that capture the minimal past sufficient to predict future transitions and rewards, while Yue et al. (2024) introduce memory dependency pairs to annotate which past observations must be recalled for correct decisions. Both perspectives emphasize controllable ways to scale memory difficulty, either by increasing order/span or by manipulating dependency graphs. In this paper, we adopt the eventrecall horizon framework for clarity and analytical tractability, but note that these alternative views are broadly consistent and provide useful tools for designing and benchmarking memory-intensive tasks. Figure 7: Environments used in experiments. (c) Some POPGym tasks. (b) T-Maze. Definition (memory-intensive environment). POMDP MP is memory-intensive if minn Ξ > 1; i.e., every relevant decision depends on information separated by at least one intervening step, making reactive (myopic) policies insufficient. This definition cleanly separates POMDPs that genuinely require memory from those that are effectively MDP-like. A.4 MEMORY-INTENSIVE ENVIRONMENTS The memory-intensive environments used in this work are presented in Figure 7. T-Maze. The T-Maze (Ni et al., 2023) features corridor ending in junction with two goals. At the start, one goal is randomly revealed, and the agent must recall this cue after traversing the corridor to choose the correct branch. Observations are vectors; actions are discrete. Rewards are sparse, provided only upon reaching the correct goal. The task tests whether the model can retain early cues across long delays. 18 further evaluate benchmark. We MIKASA-Robo benchmark (Cherepanov et al., 2025), which provides robotic tabletop manipulation tasks designed for memory evaluation. Each environment simulates 7-DoF arm with two-finger gripper. Observations are paired RGB images (3 128 128) from static and wrist camera; actions are continuous (7 joints + gripper). Rewards are binary, given only on task success. We study two (i) RememberColor[3,5,9]-v0, where the agent must families of MIKASA-Robo tasks: recall the color of hidden cube after delay with distractors, and (ii) TakeItBack-v0, where the agent first moves cube to goal, then must return it once the goal changes. the MIKASA-Robo on POPGym benchmark. POPGym benchmark (Morad et al., 2023a) is large suite of 48 partially observable environments designed to stress agent memory. The tasks span two categories: (i) diagnostic memory puzzles, which require agents to remember cues or solve algorithmic sequence problems (e.g., copy, reverse, n-back, and long-horizon T-Mazes), and (ii) partially observable control tasks, which adapt classic control benchmarks such as CartPole, MountainCar, and LunarLander to observation-limited settings. This diversity allows POPGym to test both symbolic memory skills and generalization in continuous control, providing broad and challenging benchmark for evaluating memory-augmented RL methods. Detailed results across all 48 POPGym tasks for each of considered baselines are presented in Table 4. A.5 EXTENDED RELATED WORK Transformers for manipulation. Work on transformer-based manipulation splits into characteristic families with complementary strengths and limits. Perception-centric visuomotor transformers emphasize 3D or multi-view geometry to map pixels to precise end-effector actions under (near) full observability, excelling at spatial alignment but assuming short temporal credit assignment: PerAct (Shridhar et al., 2023), RVT (Goyal et al., 2023), RVT-2 (Goyal et al., 2024), RozumFormer (Staroverov et al., 2023). Sequence/skill models distill demonstrations into reusable action chunks and long-horizon trajectories, improving sample efficiency but remaining bottlenecked by finite context windows: ACT (Zhao et al., 2023), Skill Transformer (Huang et al., 2023), ILBiT (Kobayashi et al., 2025). Planning/RL-driven transformers blend sequence modeling with value learning or planning for closed-loop control under uncertainty, yet still inherit fixed-context limitations: OPTIMUS (Dalal et al., 2023), ActionFlow (Funk et al., 2024), Q-Transformer (Chebotar et al., 2023), CCT/ARP (Zhang et al., 2025b), FLaRe (Hu et al., 2025). Alternative backbones (state-space, diffusion) target long continuous horizons and smooth control but lack explicit persistent state: RoboMamba (Liu et al., 2024b), Diffusion Policy (Chi et al., 2023). VLA models for manipulation. VLA models broaden task coverage by conditioning on language and scaling data, while keeping the transformer core unchanged and context-bounded. Instructionfollowing generalists demonstrate broad skill repertoires with language prompts but no explicit longterm memory: RT-1 (Brohan et al., 2022), RT-2 (Zitkovich et al., 2023), Octo (Team et al., 2024), OpenVLA (Kim et al., 2024), VIMA (Jiang et al., 2022). Efficiency/modularity variants freeze large encoders and train lightweight adapters or experts for practicality at scale: RoboFlamingo (Li et al., 2023), CogACT (Li et al., 2024), FLOWER (Reuss et al., 2025), NinA (Tarasov et al., 2025). Reasoning/hierarchy extensions inject geometric priors, step-wise reasoning, or multi-level control while still relying on finite windows: 3D-VLA (Zhen et al., 2024), CoT-VLA (Zhao et al., 2025), TraceVLA (Zheng et al., 2024), HiRT (Zhang et al., 2024), DP-VLA (Han et al., 2024). Specialized and hybrid designs tailor the backbone to domain constraints (dexterous hands, spatial priors) or mix diffusion with autoregression: HybridVLA (Liu et al., 2025), DexVLA (Wen et al., 2025a), SpatialVLA (Qu et al., 2025), OpenVLA-OFT (Kim et al., 2025). Generalist robot agents pursue openworld embodiment with growing breadth but inherit the same temporal limitations: TinyVLA (Wen et al., 2025b), π0 (Black et al., 2024), π0.5 (Intelligence et al., 2025), GR00T N1 (Bjorck et al., 2025), Gemini Robotics (Team et al., 2025), NORA (Hung et al., 2025). Memory in Deep Learning. Mechanisms for long-term information fall into three broad tracks. Implicit recurrence and long-range sequence models retain information in hidden dynamics but offer limited, indirect control over storage and forgetting: LSTM (Hochreiter & Schmidhuber, 1997), xLSTM (Beck et al., 2024), linear-attention Transformers (Katharopoulos et al., 2020), RWKV (Peng et al., 2023), S4 (Gu et al., 2021), Mamba (Gu & Dao, 2023). Explicit external memory with learned 19 readwrite provides addressable storage with content-based access, trading off simplicity for optimization/scaling complexity: NTM (Graves et al., 2014), DNC (Graves et al., 2016), Memory Networks (Weston et al., 2014), differentiable memory for meta-learning (Santoro et al., 2016), recent variants (Ahmadi, 2020). Transformer context extension pushes horizons via cached activations, compression, or auxiliary memory modules but typically keeps memory peripheral to the core token computation: Transformer-XL (Dai et al., 2019), Compressive Transformer (Rae et al., 2019), Memorizing Transformer (Wu et al., 2022), Ring Attention (Liu et al., 2023), Memformer (Wu et al., 2020), associative-memory Transformers (Rodkin et al., 2024), ERNIE-style memory (Ding et al., 2020), test-time memorization (TiTANS) (Behrouz et al., 2024). In partially observed decision processes, memory is not optional; Memory in RL. it is the state estimator. Spatial/episodic buffers externalize salient facts in readwrite maps or associative stores (navigational and episodic use-cases): Neural Map (Parisotto & Salakhutdinov, 2017), HCAM (Lampinen et al., 2021), Stable Hadamard Memory (Le et al., 2024). Sequence-model adaptations retrofit transformers for recurrence and stability in RL, improving long-horizon training but leaving persistence bounded by context: DTQN (Esslinger et al., 2022), GTrXL (Parisotto et al., 2020), AGaLiTe (Pramanik et al., 2023), AMAGO-2 (Grigsby et al., 2024). Evaluation frameworks formalize memory demands and failure modes under partial observability: (Cherepanov et al., 2024; Yue et al., 2024; Wang et al., 2025b). Transformers with external stores for RL insert explicit memory alongside the policy, but often as sequence-level attachment: RATE (Cherepanov et al., 2023), FFM (Morad et al., 2023b), Re:Frame (Zelezetsky et al., 2025). Memory in manipulation tasks. Long-horizon, partially observed manipulation has motivated three practical extensions. Trajectory summarization compresses the past into short token set, trading completeness for compactness: TraceVLA (Zheng et al., 2024). External feature banks cache recent visual features for retrieval, focusing attention but leaving distant history underrepresented: SAM2Act+ (Fang et al., 2025). Structured memory modules interface an explicit store with the policy to persist scene/task variables: MemoryVLA (Shi et al., 2025). Hierarchies shift temporal burden to slow planners or high-level controllers, which may mask but not remove the need for persistent state at the policy layer: HiRT (Zhang et al., 2024), DP-VLA (Han et al., 2024). For evaluation, targeted benchmarks isolate memory factors: MemoryBench (Fang et al., 2025); broader suites capture multiple memory types and delays: MIKASA-Robo (Cherepanov et al., 2025). A.6 TRAINING PROCESS When training ELMUR, we compute the loss on every segment processed recursively, detaching the memory state between segments (i.e., without backpropagation through time). The choice of loss depends on the task domain: for T-Maze, CartPole-v1, and subset of POPGym tasks with discrete actions, we apply cross-entropy loss; for MIKASA-Robo and the remaining POPGym tasks with continuous actions, we use mean-squared error (MSE) loss. The ELMUR hyperparameters used in the experiments are presented in The ELMUR hyperparameters used in the experiments are presented in Table 5. For data, we follow consistent offline imitation-learning setup. Each MIKASA-Robo environment provides dataset of 1000 expert demonstrations generated by PPO policy trained with oracle-level state access. For T-Maze, we collect 6000 successful oracle-level trajectories. For POPGym, we adopt the datasets of Morad et al. (2023a), consisting of 3000 trajectories per environment generated by PPO-GRU expert. Finally, for CartPole-v1, we use 1000 successful trajectories collected from pre-trained PPO policy. A.7 LIMITATIONS ELMUR uses simple LRU rule with fixed blending, which makes its memory mechanism transparent and easy to analyze, though future work could explore adaptive variants. Segment-level recurrence adds small cross-attention cost per layer, but this cost scales with the fixed number of memory slots rather than sequence length, making efficiency predictable even in long horizons. MoE-based FFNs already provide parameter efficiency at our scale, and larger architectures may further amplify this benefit. Finally, our study focuses on synthetic, POPGym, and simulated robotic 20 Table 4: POPGym benchmark results. Reported scores are mean standard error, averaged over 3 runs and evaluated with 100 random seeds. Across 21 partially observable tasks spanning puzzle and control domains, ELMUR achieves the best performance on 12 tasks, underscoring the effectiveness of its memory module for reasoning under partial observability. POPGym-48 Task RATE DT Random BC-MLP BC-LSTM ELMUR Expert PPO-GRU AutoencodeEasy-v0 AutoencodeMedium-v0 AutoencodeHard-v0 BattleshipEasy-v0 BattleshipMedium-v0 BattleshipHard-v ConcentrationEasy-v0 ConcentrationMedium-v0 ConcentrationHard-v0 CountRecallEasy-v0 CountRecallMedium-v0 CountRecallHard-v0 HigherLowerEasy-v0 HigherLowerMedium-v0 HigherLowerHard-v0 LabyrinthEscapeEasy-v0 LabyrinthEscapeMedium-v0 LabyrinthEscapeHard-v0 LabyrinthExploreEasy-v0 LabyrinthExploreMedium-v0 LabyrinthExploreHard-v0 MineSweeperEasy-v0 MineSweeperMedium-v0 MineSweeperHard-v MultiarmedBanditEasy-v0 MultiarmedBanditMedium-v0 MultiarmedBanditHard-v0 NoisyPositionOnlyCartPoleEasy-v0 NoisyPositionOnlyCartPoleMedium-v0 NoisyPositionOnlyCartPoleHard-v0 NoisyPositionOnlyPendulumEasy-v0 NoisyPositionOnlyPendulumMedium-v0 NoisyPositionOnlyPendulumHard-v0 PositionOnlyCartPoleEasy-v0 PositionOnlyCartPoleMedium-v0 PositionOnlyCartPoleHard-v0 PositionOnlyPendulumEasy-v0 PositionOnlyPendulumMedium-v0 PositionOnlyPendulumHard-v0 RepeatFirstEasy-v0 RepeatFirstMedium-v0 RepeatFirstHard-v RepeatPreviousEasy-v0 RepeatPreviousMedium-v0 RepeatPreviousHard-v0 VelocityOnlyCartpoleEasy-v0 VelocityOnlyCartpoleMedium-v0 VelocityOnlyCartpoleHard-v0 -0.290.00 -0.460.00 -0.470.00 -0.810.02 -0.920.01 -0.910.02 -0.060.02 -0.250.00 -0.840.00 0.070.01 -0.540.00 -0.470. 0.500.00 0.520.00 0.500.00 0.950.00 -0.560.01 -0.810.01 0.950.00 0.880.00 0.790.00 0.150.03 -0.200.00 -0.440.00 0.370.01 0.320.01 0.220.03 0.880.03 0.330.01 0.180. 0.870.00 0.680.00 0.600.01 0.930.03 0.070.00 0.050.01 0.540.02 0.490.01 0.470.01 1.000.00 0.990.01 0.100.02 1.000.00 -0.380.01 -0.460.00 1.000.00 1.000.00 1.000. -0.470.00 -0.490.00 -0.490.00 -0.930.03 -0.970.01 -0.910.03 -0.050.01 -0.250.01 -0.840.00 -0.460.01 -0.810.02 -0.750.03 0.500.00 0.510.00 0.500.00 0.800.01 -0.670.04 -0.820. 0.880.06 0.860.01 0.770.01 -0.330.04 -0.370.02 -0.400.01 0.270.01 0.350.01 0.270.01 0.870.02 0.340.00 0.170.01 0.840.01 0.630.01 0.560.01 1.000.00 0.340.08 0.030. 0.510.03 0.550.01 0.490.01 0.450.16 -0.210.18 0.420.14 1.000.00 -0.380.00 -0.470.00 1.000.00 1.000.00 0.960.02 -0.500.00 -0.500.01 -0.500.00 -0.460.01 -0.410.00 -0.390. -0.190.01 -0.190.00 -0.840.00 -0.930.00 -0.930.00 -0.880.00 0.000.01 0.010.01 -0.010.00 -0.390.00 -0.840.04 -0.940.01 -0.340.01 -0.610.00 -0.730.00 -0.260.03 -0.390.01 -0.430. 0.020.00 0.010.00 0.010.00 0.110.00 0.120.01 0.110.00 0.270.01 0.270.01 0.260.00 0.120.00 0.050.00 0.040.00 0.270.00 0.260.00 0.260.00 -0.490.01 -0.500.00 -0.500. -0.490.01 -0.500.01 -0.510.00 0.110.00 0.060.00 0.040.00 -0.470.00 -0.500.00 -0.490.00 -1.000.00 -1.000.00 -1.000.00 -0.920.00 -0.920.00 -0.880.00 -0.920.00 -0.920.00 -0.880. 0.470.00 0.500.00 0.490.00 0.720.05 -0.710.03 -0.890.01 0.870.01 0.450.01 0.260.01 -0.470.01 -0.480.00 -0.490.00 0.050.00 0.210.01 0.010.00 0.230.00 0.180.00 0.160. 0.310.00 0.300.00 0.280.00 0.150.00 0.090.00 0.050.00 0.290.00 0.300.00 0.280.00 -0.500.00 -0.500.00 -0.500.00 -0.520.00 -0.500.00 -0.480.00 0.990.00 0.830.01 0.630. -0.320.00 -0.440.00 -0.470.00 -0.490.01 -0.670.01 -0.810.02 -0.140.00 -0.190.01 -0.840.00 0.050.00 -0.560.00 -0.470.00 0.500.00 0.510.01 0.500.00 0.920.01 -0.690.02 -0.860. 0.930.00 0.820.01 0.710.01 0.200.00 -0.160.00 -0.350.01 0.170.02 0.140.00 0.170.01 0.440.01 0.250.01 0.220.01 0.880.00 0.720.00 0.660.00 0.170.00 0.120.00 0.060. 0.910.00 0.890.00 0.820.00 1.000.00 0.990.01 -0.500.00 1.000.00 -0.380.00 -0.450.00 1.000.00 1.000.00 1.000.00 -0.260.00 -0.440.00 -0.460.00 -0.790.02 -0.790.01 -0.800. -0.140.01 -0.240.00 -0.820.00 0.030.01 -0.560.01 -0.470.00 0.500.00 0.510.00 0.500.00 0.920.00 -0.540.01 -0.820.01 0.960.00 0.860.01 0.840.00 -0.170.02 -0.320.00 -0.390. 0.430.01 0.320.01 0.220.01 0.590.02 0.270.00 0.220.01 0.880.00 0.720.00 0.650.00 1.000.00 0.110.00 0.100.00 0.520.00 0.500.01 0.630.01 1.000.00 0.990.00 0.990. 1.000.00 -0.390.00 -0.460.00 1.000.00 1.000.00 1.000.00 Sum of returns 9.54 5.80 -12. -6.83 8.96 10.41 -0.26 -0.43 -0.48 -0.35 -0.40 -0.43 -0.12 -0.44 -0. 0.22 -0.55 -0.48 0.51 0.49 0.49 0.95 -0.49 -0.94 0.96 0.87 0.79 0.28 -0.10 -0.27 0.62 0.59 0. 0.98 0.57 0.36 0.90 0.73 0.67 1.00 1.00 1.00 0.92 0.88 0.82 1.00 1.00 0.99 1.00 -0.39 -0. 1.00 1.00 0.99 16.51 tasks under IL, giving controlled and reproducible insights; extending to online RL and real-robot deployments offers promising next steps. 21 Parameter RememberColor TakeItBack T-Maze POPGym-AutoencodeEasy CartPole-v dmodel Routed dff Shared dff Layers Heads Experts (MoE) Shared Experts Top-k routing Memory size Memory init std Memory dropout LRU blend α Dropout Dropatt Label smoothing Context length Batch size Learning rate Warmup steps Cosine decay LR end factor Weight decay Epochs Grad clip Beta1 Beta2 128 128 128 4 16 16 1 2 256 0.1 0.06 0.40 0.13 0.30 0.21 20 64 2.05e-4 30000 True 0.1 0.001 200 5 0.99 0.99 32 256 32 2 16 1 1 1 32 0.001 0.23 0.20 0.01 0.18 0.20 60 64 2.57e-4 30000 False 0.01 0.01 300 1 0.9 0.99 128 32 512 2 2 2 2 3 2 0.001 0.01 0.05 0.10 0.17 0.16 10 128 2.06e-4 10000 True 1.0 0.0001 1000 5 0.95 0.999 64 128 256 12 4 1 2 1 8 0 0.17 0.80 0.14 0.26 0.22 35 128 1.16e-4 50000 False 0.01 0.1 800 5 0.99 0.99 128 128 256 4 4 4 1 2 16 0.01 0.05 0.9 0.10 0.10 0.00 30 512 3.0e-4 1000 True 0.1 0.01 100 1.0 0.9 0. Table 5: Hyperparameters for ELMUR. We report all architecture and training parameters used across tasks."
        }
    ],
    "affiliations": [
        "Cognitive AI Lab, Moscow, Russia",
        "IAI MIPT, Moscow, Russia"
    ]
}