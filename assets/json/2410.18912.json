{
    "paper_title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
    "authors": [
        "Mingtong Zhang",
        "Kaifeng Zhang",
        "Yunzhu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io."
        },
        {
            "title": "Start",
            "content": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling Mingtong Zhang1, Kaifeng Zhang2, Yunzhu Li2 1University of Illinois Urbana-Champaign, 2Columbia University Abstract: Videos of robots interacting with objects encode rich information about the objects dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects 3D states, limiting their use in real-world robotic applications. In this work, we introduce framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robots action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our frameworks ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io. Keywords: Dynamics Model, 3D Gaussian Splatting, Action-Conditioned Video Prediction, Model-Based Planning 4 2 0 2 4 ] . [ 1 2 1 9 8 1 . 0 1 4 2 : r Figure 1: We propose novel approach for learning neural dynamics model from real-world data. Using videos captured from robot-object interactions, we obtain dense 3D tracking with dynamic 3D Gaussian Splatting framework. We train graph-based neural dynamics model on top of the 3D Gaussian particles for action-conditioned video prediction and model-based planning. *Equal contribution. 8th Conference on Robot Learning (CoRL 2024), Munich, Germany."
        },
        {
            "title": "Introduction",
            "content": "Humans naturally grasp the dynamics of objects through observation and interaction, allowing them to intuitively predict how objects will move in response to specific actions [1]. This predictive capability enables humans to plan their behavior to achieve specific goals. In robotics, developing predictive models of object motions is critical for model-based planning [24]. However, learning such models from real data is challenging due to the complex physics involved in robot-object interactions. Previous works have used simulation environments to learn dynamics models where ground truth 3D object states are available [57]. In contrast, real interaction data, such as videos, tend to be difficult to extract 3D information from, thus leading to inefficiencies in model learning. Recent advancements in 3D Gaussian Splatting have introduced novel approach to 3D reconstruction. 3D Gaussian Splatting uses collection of 3D Gaussians with optimizable parameters as object particles. direct extension of this framework is to optimize the temporal motions of 3D Gaussians, leading to dynamic scene fitting. Nevertheless, such reconstruction methods only fit given dynamic scenes but can not predict object motions into the future. In this work, we propose novel method that combines dynamic 3D reconstruction with dynamics modeling. Our method learns neural dynamics model from real videos for 3D action-conditioned video prediction and model-based planning. To achieve this, we first follow previous dynamic 3D reconstruction approaches [8] to obtain particle-based representation for dynamic scenes. We extract dense correspondence from long-horizon robot-object interaction videos, which serve as the training data for dynamics model based on Graph Neural Networks (GNNs). Operating on spatial graph of control particles, the model predicts object motions under external actions such as robot interactions. To enable dense motion prediction, we design an interpolation scheme to calculate the transformations of 3D Gaussians from sparse control particles, enabling action-conditioned video prediction. The dynamics model can also be incorporated into model-based planning pipeline, e.g. model predictive control, for object manipulation tasks. We perform experiments on various objects of deformable materials including ropes, cloths, and toy animals. Results show our method accurately reconstructs 3D Gaussians and maintains coherent correspondences across frames, even with occlusions. The learned dynamics model simulates the physical behaviors of the deformable materials truthfully and generalizes well to unseen actions. Our method outperforms other system identification approaches (such as parameter optimization in physics-based simulators) in motion prediction accuracy and video prediction quality. We also demonstrate the model-based planning performance using our learned model in range of object manipulation tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Dense Correspondence from Videos. Dense correspondence is usually extracted from videos using pixel-wise tracking [913]. Such tracking methods are usually formulated as 2D prediction problem and require large datasets to train. Our approach is more related to another line of work, which focuses on lifting RGB or RGB-D images to 3D and track in the 3D space [1416]. Recently, Dynamic 3D Gaussian Splatting approaches [8, 17] reconstruct objects as Gaussian particles and optimize per-sequence tracking using rendering loss. Building on this line of work, our method uses 3D point scans of objects as initialization and extracts correspondence for dynamics model training. primary incentive for examining point tracking is its potential application to robotics [18, 19]. For instance, RoboTAP [20] demonstrates that pre-trained point tracking models enhance the sample efficiency in visual imitation learning, and ATM [21] shows that predictions of point trajectories provide control guidance for robots. Our method, in contrast, learns neural dynamics on top of particle-based tracking, which generalizes to unseen robot actions and can be naturally integrated with model-based planning framework. Neural Dynamics Modeling of Real-World Objects. Modeling the dynamics of real-world objects is extremely challenging due to the high complexity of the state space and the variance of physical properties, especially for deformable materials. Using physics-based simulators, previ2 Figure 2: Overview of Our Framework: We first achieve dense 3D tracking of long-horizon robot-object interactions using multi-view videos and Dyn3DGS optimization. We then learn the object dynamics through graph-based neural network. This approach enables applications such as (i) action-conditioned video prediction using linear blend skinning for motion prediction, and (ii) model-based planning for robotics. ous works have manually specified physical parameters [22], or performed gradient-free parameter search [23] and gradient-based methods [2426] to optimize low-dimensional physical parameters in continuous domain. However, these methods require accurate perception models to transfer objects into simulatable assets and thus are limited to constrained, manually designed environments. Neural network-based simulators can also learn object dynamics without requiring the analytical model [2731]. Especially, graph-based networks can learn the dynamics of various kinds of models such as plasticine [32, 33], cloth [6, 34], fluid [5, 35], etc. Our method builds upon these previous works but reduces the requirement of large-scale offline training data, enabling direct learning from videos, thus potentially eliminating sim-to-real gaps. Video Prediction for Robotics. Autoregressive [3638] and diffusion models [3941] trained on Internet-scale data have demonstrated significant capabilities in video generation. Actionconditioned video prediction methods initially showed their capability in predicting future video frames conditioned on actions in simple scenarios, such as atari games [42, 43] and object push [44]. Recently, diffusion-based methods for video prediction [4549] have created realistic videos of robotic or human manipulations using large-scale data. These models can be applied to generating robot actions by solving robot poses from videos [50] or learning goal-conditioned policies [46, 51]. Compared to these approaches, our model learns action-conditioned video prediction in 3D, thus making it compatible with model-based planning frameworks for various manipulation tasks."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary: 3D Gaussian Splatting 3D Gaussian Splatting [52] optimizes dense set of 3D Gaussians as explicit scene representation. Each Gaussian is defined by 3D covariance matrix Σ centered at µ. The matrix Σ decomposes into rotation matrix and scale matrix by Σ = RSST RT . For image rendering, 3D Gaussians are projected to 2D using viewing transformation . The covariance matrix in camera coordinates Σ is computed as Σ = JW ΣW , where is the Jacobian of the affine approximation of the projective transformation. To optimize the 3D Gaussians, they are projected onto the image plane, where each pixels color is determined by weighted blending: 3 = (cid:88) iN ciαi i1 (cid:89) j=1 (1 αj), (1) where ci is the color, and αi is the opacity value of each 3D Gaussian. The loss function is defined between the rendering and the ground truth image, as the L1 error with D-SSIM term: = (1 λ)L1 + λLD-SSIM, where we take λ = 0.2 in our experiments. 3.2 Dynamic 3D Gaussian Splatting with Dense Tracking 3D Gaussians have been proven effective in modeling continuously changing dynamic scenes. For instance, Dynamic 3D Gaussians (Dyn3DGS) [8] optimizes the spatial transformation of fixed set of oriented Gaussian kernels over time to fit multiview video sequences. We build upon Dyn3DGS to extract dense correspondence of 3D Gaussians for deforming objects. Our key insight is that maintaining uniform Gaussian attributes improves modeling accuracy and physical consistency. Specifically, we keep the color, opacity, and scale of Gaussians constant during optimization, while allowing position and orientation change. We empirically found that enforcing color-consistent ellipsoids reduces optimization parameters and speeds up training, resulting in improved correspondence quality, especially when the video contains partial occlusion. Additionally, we only retain high-opacity Gaussians during training. In line with Dyn3DGS, we use physics-inspired optimization objectives to guide optimization, ensuring physical plausibility and accurate long-term dense correspondence. The non-rigid physical modeling principles capture natural scene dynamics, enhancing the fidelity and stability of tracking over time. To overcome the challenges of our 4-view configuration, we strengthen the local rigidity and isometry objectives, improving the ability to handle limited observations. To isolate the objects of interest, we use GroundingDINO [53] and Segment Anything [54] models to obtain masks for objects the robot interacts with. This allows us to filter out the interference of background objects, thus enhancing the optimization efficiency. 3.3 Graph-Based Neural Dynamics Learning The dense tracking of object particles facilitates action-conditioned dynamics learning. Consider the scenario of an external robotic action applied to an object. The action can be represented as sequence of end-effector positions = {at}0tT . From the video sequence of the action, we can fit 3D Gaussians to the video using our modified Dyn3DGS, resulting in collection of dynamic Gaussians: = {Xt}0tT = {µt i}1iN,0tT , where Xt denotes the set of Gaussians at time t, and µt denotes the position of single Gaussian indexed at time t. The underlying physical dynamics function could then be depicted as Xt+1 = (X0:t, at) (2) for any timestep t. We approximate this dynamics function with Graph Neural Network (GNN) architecture [55], parameterized by θ. To construct the graph, we apply the farthest point sampling algorithm on the Gaussians to extract sparse subset of control particles ˆX = { ˆXt}0tT = {ˆµt i}1in,0tT , with particles and pairwise distance threshold dv, and use them as graph vertices. The robot end-effector position at is also considered as graph vertex. bidirectional edge is connected if two vertices distance is below threshold de, resulting in the edge set ˆE. The vertex encoder takes as input the motion of each particle ˆµt in the previous time steps (k = 3 in our experiments) and one-hot vector indicating whether the vertex belongs to the object or the robot end-effector. To model the relationship between the object and the table surface, the distance of every vertex to the surface is also included in the input. The edge encoder takes as input the 3-dimensional position difference of the two vertices and one-hot vector indicating whether the edge is an object-object relation or an object-robot relation. ˆµt1 The graph encodes vertices and edges into latent features using shared vertex and edge encoders, performs message passing for timesteps (p = 3 in our experiments), and uses shared decoder to 4 map vertex latent features back to 3-dimensional output indicating the motion at time t. The entire process can be represented as ˆXt+1,pred = ˆXt + fθ( ˆXtk:t, ˆEt). Training. The main loss function to train our dynamics network is the MSE prediction error: Lpred = τ (cid:88) i=1 ˆXt+i,pred ˆXt+i2, (3) (4) where τ is the look-forward horizon size indicating how many steps we recurrently perform future prediction. We take τ = 5 in our experiments to get balance between prediction stability and computational cost. During training, we randomly sample time steps from the training sequences to serve as the starting frame. Empirically, we also find adding regularization terms to be helpful in some objects with complex shapes. These include an edge length regularization term penalizing the mean squared difference of edge lengths between subsequent frames, and rigidity regularization term, applied to objects that are rigid or close to rigid, penalizing the mean squared difference between the predicted motions and rigid transformation estimated from the predictions [56]. 3.4 Dense Motion Prediction on 3D Gaussians The flexibility of 3D Gaussians allows us to perform motion densification from the sparse subset of particle ˆX to the entire collection of Gaussians while maintaining the ability to render photorealistic videos. Taking an initial static collection of Gaussians = {X0} and the action sequence = {at}0tT as input, we first perform recurrent future prediction on the subsampled graph vertices, outputting ˆX0:T . Then, for each timestep t, we perform the following interpolation scheme to derive the center motions and rotations of each Gaussian: Graph Vertex Transformations. In the first step, we need to calculate the 6-DoF transformation for each graph vertex. The outputs of the GNN are per-vertex motions, which directly serve as the 3D translations. For the 3D rotation, for each vertex ˆµt according to the motion of its neighbors from time to + 1: i, we solve for the rotation Rt Rt = arg min RSO(3) (cid:88) jN (i) R(ˆµt ˆµt i) (ˆµt+ ˆµt+1 )2. (5) Gaussian Transformations. We adopt Linear Blend Skinning (LBS) [57, 58] to calculate the motions of Gaussian centers. Concretely, µt+1 = (cid:88) b=1 wt ib (cid:0)Rt b(µt ˆµt b) + ˆµt + (cid:1) , qt+1 = (cid:33) wt ibrt qt , (cid:32) (cid:88) b=1 (6) ib is the weight associated between Gaussian and graph vertex b; Rt b, rt where wt are the matrix and quaternion representations of vertex bs rotation at time t; is the translation of vertex b, which is directly predicted by our dynamics model. The blending weight between Gaussian and vertex is inversely proportional to their 3D distance: wt ib = ˆµt µt b=1 µt b1 ˆµt b1 , (cid:80)n (7) thus assigning larger weights to vertices that are spatially closer to the Gaussians. We start from the initial Gaussian centers and rotations at = 0 and repeatedly calculate the blending weights and updated Gaussian centers and rotations, resulting in dense and smooth Gaussian motions over the entire video sequence. 5 Figure 3: Qualitative Results of 3D Gaussian Tracking. We demonstrate point-level correspondence on the objects across various timesteps. Please check our website for more videos showcasing precise dense tracking even under different object deformations and occlusions."
        },
        {
            "title": "3.5 Model-Based Planning",
            "content": "The dynamics network can also be used to perform model-based planning in robotics tasks. Given the multi-view RGB-D image of the object, we apply segmentation and point fusing to get the complete point cloud of the object. We then construct graph vertices and edges with farthest point sampling. Given target configuration, we use Model Predictive Control (MPC) [59] framework to plan for robot actions. The optimization objective is defined as the mean squared difference between the target state and the predicted object state if an action is applied. In our experiments, we apply the Model-Predictive Path Integral (MPPI) [60] trajectory optimization algorithm."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment Setup We perform experiments on 3D tracking, action-conditioned video prediction, and model-based planning. To achieve this, we collect real-world dataset, with focus on acquiring multiview images synchronized with corresponding actions at each timestep. The dataset consists of 3 types of deformable objects: rope, cloth, and toy animals, totaling 8 object instances. For all objects, we capture RGBD images and record robot actions at 15 FPS. The depth is used to initialize the point cloud for Gaussian representation. Our data collection setup includes four cameras, strategically positioned at the corners of the workspace and oriented to provide comprehensive top-down perspective. This setup allows us to collect synchronized multiview data necessary for our experiments. 4.2 3D Tracking with Dynamic 3D Gaussians We initialize Gaussian tracking with point cloud from four views at the first timestep and optimize our Dyn3DGS for dense correspondence. To evaluate our method, we compare it with advanced 2D tracking approaches by projecting 4-view results into 3D using our depth data and evaluate the best results. The unmodified Dyn3DGS is considered as baseline to show our improvements within the 4-view real-world experiment setup. Following prior work [61], we evaluate median trajectory error (MTE) in millimeters, position accuracy (δ) at thresholds of 2, 4, 8, and 16 millimeters (reporting the average), and survival rate with threshold of 0.5 meters. For fair comparison, we also project our 3D tracking results into 2D pixels and evaluate the same metrics, allowing comprehensive assessment in both 2D and 3D contexts. The quantitative results in Tab. 1 demonstrate that our 3D tracking method outperforms all baselines, including SpatialTracker [15], which uses depth as 3D prior. The results highlight our methods capability across various scenes and objects. Additionally, the qualitative results in Fig. 3 show that our Dynamic 3D Gaussian tracking provides precise dense correspondence, even under challenging conditions such as occlusions from robot and object deformations. 6 Method Metric Rope Cloth Toy Animals Metric Rope Cloth Toy Animals 3D MTE [mm] 3D δavg 3D Survival CoTracker [12] PIPS++ [61] SpatialTracker [15] Dyn3DGS [8] Ours CoTracker [12] PIPS++ [61] SpatialTracker [15] Dyn3DGS [8] Ours CoTracker [12] PIPS++ [61] SpatialTracker [15] Dyn3DGS [8] Ours 46.96 49.37 38.72 62.41 6.90 79.28 69.83 86.56 60.32 89.26 94.41 92.61 100 84.29 100 55.84 58.10 53.85 69.20 13.14 75.79 66.92 83.85 56.28 89.13 95.19 91.39 98.46 79.04 51.84 58.45 42.82 66.19 12.83 75.46 68.95 79.42 61.97 82.71 92.26 85.73 96.26 74.82 98.83 2D MTE [mm] 2D δavg 2D Survival 42.73 46.17 32.29 57.39 4.92 83.71 76.30 92.36 67.20 93.27 97.20 96.74 100 87.83 100 49.82 50.28 46.26 61.28 11.72 79.28 72.48 90.52 62.28 92.18 100 94.28 100 82.14 46.30 49.96 37.49 60.17 10.94 78.44 76.96 89.62 67.96 94.19 96.06 92.82 100 79.32 100 Table 1: Quantitative Results on Dynamic 3D Gaussian Tracking. We labeled the ground truth for 200 frames per episode in 3D space for 1 or 2 object instances in each category, covering two episodes per object. Our Dyn3DGS-based tracking method outperforms all baselines, including the unmodified Dyn3DGS, in both 2D and 3D metrics. Figure 4: Qualitative Results of Action-Conditioned 3D Video Prediction. Our videos are generated by rendering predicted Gaussians on virtual backgrounds. Robot trajectories are visualized as curved lines (yellow: current end-effector positions, purple: history end-effector positions). Compared to the MPM baseline, our video prediction results align with the ground truth frames (GT) more accurately. 4.3 Action-Conditioned Video Prediction Using dynamic 3D Gaussian reconstruction with tracking, we train the graph-based neural dynamics models with downsampled control particles. These particle motions are used to interpolate dense Gaussian kernel motion and rotations. To evaluate the quality of our dynamics prediction and 3D Gaussian rendering, we assess action-conditioned video prediction performance. Baselines. Synthesizing action-conditioned object motions requires physics priors, and existing text-conditioned video prediction methods are not compatible with taking 3D robot actions as input. In this experiment, we consider 2 physics simulator-based baselines, MPM and FleX. MPM is based on the Material Point Method simulation framework [62, 63]. Our baseline MPM uses the same simulation setting as previous works [22, 26], while adding support for two types of robot end-effectors: cylindrical pusher and gripper. For each object instance, we set the friction coefficient µ and Youngs modulus (assumed uniform) to be two learnable parameters that are optimized from the training data using the CMA-ES algorithm. FleX is based on the NVIDIA FleX simulator. For all objects, we adopt the soft body simulator in FleX, reconstruct the initial object mesh from 3D Gaussians using alpha-shape mesh reconstruction. Similarly, we optimize the friction coefficient µ and stiffness coefficient (assumed uniform) of each object instance using CMA-ES. Metrics. To evaluate the 3D video prediction performance, we adopt 3D distance metrics (3D Chamfer Distance, 3D Earth Movers Distance), 2D segmentation similarity metrics (J &F score), 7 Figure 5: Quantitative Results of model-based planning. We perform each experiment 5 times and present the results as follows: (i) the median error curve relative to planning steps, with the area between 25 and 75 percentiles shaded, and (ii) the success rate curve relative to error thresholds. and 2D image-based metrics (LPIPS). The metrics are complementary and together give thorough comparison of future prediction accuracy and rendering quality. Results. The results are listed in Tab. 2. Across all metrics, our method outperforms both baselines significantly, proving that we learn more accurate dynamics model and give more realistic video prediction results. Our neural-based dynamics model effectively avoids the sim-to-real gap by learning from real-world data, thus improving the motion prediction accuracy under unseen robot actions and object configurations. This is also supported by the qualitative visualizations of predicted videos, shown in Fig. 4. Method Metric Cloth Toy Animals Rope Metric Cloth Toy Animals Rope MPM 3D Chamfer FleX Ours 3D EMD LPIPS MPM FleX Ours MPM FleX Ours 0.076 0.136 0.064 0.097 0.126 0. 0.057 N/A 0.044 0.052 0.069 0.030 0.053 0.066 0.035 0.027 N/A 0.021 score score 0.073 0.074 0.053 0.066 0.068 0.048 0.030 &F score N/A 0.025 0.421 0.268 0.538 0.296 0.182 0.446 0.359 0.225 0. 0.602 0.451 0.701 0.598 0.424 0.726 0.600 0.438 0.714 0.397 0.299 0.419 0.600 0.552 0.667 0.498 0.426 0. Table 2: Quantitative Results on Action-Conditioned 3D Video Prediction. We evaluate test sequence set for each object instance and present the results averaged by object category. Our method outperforms simulator baselines across all metrics. 4.4 Model-Based Planning The action-conditioned dynamics model enables deployment in real-world robot planning tasks, demonstrating its generalizability to unseen object configurations. We choose rope straightening and toy animal relocating tasks to show our models ability to manipulate highly deformable objects to target configurations. The quantitative results in Fig. 5 demonstrate that our approach effectively reduces error and achieves high success rates. The computational complexity and inefficiency of MPM and FleX baselines make them impractical to perform real-world planning tasks."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "In this paper, we introduce novel approach for graph-based neural dynamics modeling from realworld data for 3D action-conditioned video prediction and planning. Our method reconstructs 3D Gaussians with cross-frame correspondences and learns neural dynamics model that facilitates action-conditioned future prediction. The method outperforms baseline approaches in motion prediction and video prediction accuracy. We also showcase our planning performance for object manipulation tasks, highlighting our frameworks effectiveness in real-world robotic applications. Limitation Although our approach learns object dynamics directly from videos, collecting realworld data remains costly, and the perception module may fail when there are large occlusions or textureless objects. In future work, we aim to develop more efficient data collection pipeline and more robust perception system to reduce the learning cost and enhance the methods applicability to all kinds of real videos. 8 Acknowledgments The Toyota Research Institute (TRI) partially supported this work. This article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We would like to thank Haozhe Chen, Binghao Huang, Hanxiao Jiang, Yixuan Wang and Ruihai Wu for their thoughtful discussions. We thank Binghao Huang and Hanxiao Jiang for their help on the realworld data collection framework design, and also thank Binghao Huang and Haozhe Chen for their help with visualizations."
        },
        {
            "title": "References",
            "content": "[1] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [2] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 27862793. IEEE, 2017. [3] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar. Deep dynamics models for learning dexterous manipulation. In Conference on Robot Learning, pages 11011112. PMLR, 2020. [4] L. Manuelli, Y. Li, P. Florence, and R. Tedrake. Keypoints into the future: Self-supervised correspondence in model-based reinforcement learning. arXiv preprint arXiv:2009.05085, 2020. [5] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning In International conference on machine to simulate complex physics with graph networks. learning, pages 84598468. PMLR, 2020. [6] T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W. Battaglia. Learning mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020. [7] Y. Wang, Y. Li, K. Driggs-Campbell, L. Fei-Fei, and J. Wu. Dynamic-resolution model learning for object pile manipulation. arXiv preprint arXiv:2306.16700, 2023. [8] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. [9] C. Doersch, A. Gupta, L. Markeeva, A. Recasens, L. Smaira, Y. Aytar, J. Carreira, A. Zisserman, and Y. Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. [10] C. Doersch, Y. Yang, M. Vecerik, D. Gokay, A. Gupta, Y. Aytar, J. Carreira, and A. Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1006110072, 2023. [11] A. W. Harley, Z. Fang, and K. Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. [12] N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht. Cotracker: It is better to track together. arXiv preprint arXiv:2307.07635, 2023. [13] Q. Wang, Y.-Y. Chang, R. Cai, Z. Li, B. Hariharan, A. Holynski, and N. Snavely. Tracking everything everywhere all at once. In International Conference on Computer Vision, 2023. [14] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 343352, Los Alamitos, CA, USA, jun 2015. IEEE Computer Society. doi:10.1109/CVPR.2015.7298631. URL https://doi.ieeecomputersociety. org/10.1109/CVPR.2015.7298631. 9 [15] Y. Xiao, Q. Wang, S. Zhang, N. Xue, S. Peng, Y. Shen, and X. Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [16] C. Wang, B. Eckart, S. Lucey, and O. Gallo. Neural trajectory fields for dynamic novel view synthesis. arXiv preprint arXiv:2105.05994, 2021. [17] B. P. Duisterhof, Z. Mandi, Y. Yao, J.-W. Liu, M. Z. Shou, S. Song, and J. Ichnowski. Mdsplatting: Learning metric deformation from 4d gaussians in highly deformable scenes. arXiv preprint arXiv:2312.00583, 2023. [18] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani. Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation, 2024. [19] M. Xu, Z. Xu, Y. Xu, C. Chi, G. Wetzstein, M. Veloso, and S. Song. Flow as the cross-domain manipulation interface. arXiv preprint arXiv:2407.15208, 2024. [20] M. Vecerik, C. Doersch, Y. Yang, T. Davchev, Y. Aytar, G. Zhou, R. Hadsell, L. Agapito, and J. Scholz. Robotap: Tracking arbitrary points for few-shot visual imitation. arXiv, 2023. [21] C. Wen, X. Lin, J. So, K. Chen, Q. Dou, Y. Gao, and P. Abbeel. Any-point trajectory modeling for policy learning, 2023. [22] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang. Physgaussian: Physicsintegrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198, 2023. [23] B. Wang, L. Wu, K. Yin, U. M. Ascher, L. Liu, and H. Huang. Deformation capture and modeling of soft objects. ACM Trans. Graph., 34(4):941, 2015. [24] Y.-L. Qiao, J. Liang, V. Koltun, and M. C. Lin. Differentiable simulation of soft multi-body systems. In Conference on Neural Information Processing Systems (NeurIPS), 2021. [25] X. Li, Y.-L. Qiao, P. Y. Chen, K. M. Jatavallabhula, M. Lin, C. Jiang, and C. Gan. Pac-nerf: Physics augmented continuum neural radiance fields for geometry-agnostic system identification. arXiv preprint arXiv:2303.05512, 2023. [26] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, J. Wu, and W. T. Freeman. PhysDreamer: Physics-based interaction with 3d objects via video generation. arxiv, 2024. [27] Y. Wu, W. Yan, T. Kurutach, L. Pinto, and P. Abbeel. Learning to manipulate deformable objects without demonstrations. arXiv preprint arXiv:1910.13439, 2019. [28] P. Ma, P. Y. Chen, B. Deng, J. B. Tenenbaum, T. Du, C. Gan, and W. Matusik. Learning neural constitutive laws from motion observations for generalizable pde dynamics. In International Conference on Machine Learning, pages 2327923300. PMLR, 2023. [29] Z. Xu, J. Wu, A. Zeng, J. B. Tenenbaum, and S. Song. Densephysnet: Learning dense physical object representations via multi-step dynamic interactions. arXiv preprint arXiv:1906.03853, 2019. [30] B. Evans, A. Thankaraj, and L. Pinto. Context is everything: Implicit identification for dynamics adaptation. In 2022 International Conference on Robotics and Automation (ICRA), pages 26422648. IEEE, 2022. [31] Z. Chen, K. Yi, Y. Li, M. Ding, A. Torralba, J. B. Tenenbaum, and C. Gan. Comphy: Compositional physical reasoning of objects and events from videos. arXiv preprint arXiv:2205.01089, 2022. 10 [32] H. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu. Robocraft: Learning to see, simulate, and shape elasto-plastic objects in 3d with graph networks. The International Journal of Robotics Research, 0(0):02783649231219020, 0. doi:10.1177/02783649231219020. URL https: //doi.org/10.1177/02783649231219020. [33] H. Shi, H. Xu, S. Clarke, Y. Li, and J. Wu. Robocook: Long-horizon elasto-plastic object manipulation with diverse tools. arXiv preprint arXiv:2306.14447, 2023. [34] X. Lin, Y. Wang, Z. Huang, and D. Held. Learning visible connectivity dynamics for cloth smoothing. In Conference on Robot Learning, pages 256266. PMLR, 2022. [35] Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. In ICLR, 2019. [36] D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, R. Hornung, H. Adam, H. Akbari, Y. Alon, V. Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [37] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. [38] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In European conference on computer vision, pages 720736. Springer, 2022. [39] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [40] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, Imagen video: High definition video generation with diffuM. Norouzi, D. J. Fleet, et al. sion models. arXiv preprint arXiv:2210.02303, 2022. [41] T. Brooks, B. Peebles, C. Homes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, et al. Video generation models as world simulators, 2024. [42] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. Advances in neural information processing systems, 28, 2015. [43] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video prediction. Advances in neural information processing systems, 29, 2016. [44] H. Yuan, R. Wu, A. Zhao, H. Zhang, Z. Ding, and H. Dong. Dmotion: Robotic visuomotor control with unsupervised forward model learned from videos. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 71357142. IEEE, 2021. [45] M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023. [46] Y. Du, S. Yang, P. Florence, F. Xia, A. Wahid, P. Sermanet, T. Yu, P. Abbeel, J. B. Tenenbaum, L. P. Kaelbling, et al. Video language planning. In The Twelfth International Conference on Learning Representations, 2023. [47] S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. Schuurmans. Video as the new language for real-world decision making. ICML, 2024. [48] J. Liang, R. Liu, E. Ozguroglu, S. Sudhakar, A. Dave, P. Tokmakov, S. Song, and C. Vondrick. Dreamitate: Real-world visuomotor policy learning via video generation, 2024. 11 [49] B. Van Hoorick, R. Wu, E. Ozguroglu, K. Sargent, R. Liu, P. Tokmakov, A. Dave, C. Zheng, and C. Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. arXiv preprint arXiv:2405.14868, 2024. [50] P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum. Learning to Act from Actionless Video through Dense Correspondences. arXiv:2310.08576, 2023. [51] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36, 2024. [52] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https: //repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. [53] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [54] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. [55] Y. Li, J. Wu, J.-Y. Zhu, J. B. Tenenbaum, A. Torralba, and R. Tedrake. Propagation networks for model-based control under partial observation. In ICRA, 2019. [56] S. Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376380, 1991. [57] R. W. Sumner, J. Schmid, and M. Pauly. Embedded deformation for shape manipulation. ACM Trans. Graph., 26(3):80es, jul 2007. ISSN 0730-0301. doi:10.1145/1276377.1276478. URL https://doi.org/10.1145/1276377.1276478. [58] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. arXiv preprint arXiv:2312.14937, 2023. [59] E. Camacho and C. Alba. Model Predictive Control. Advanced Textbooks in Control and Signal Processing. Springer London, 2013. ISBN 9780857293985. URL https://books. google.com/books?id=tXZDAAAAQBAJ. [60] G. Williams, A. Aldrich, and E. A. Theodorou. Model predictive path integral control: From theory to parallel computation. Journal of Guidance, Control, and Dynamics, 40(2):344357, 2017. [61] Y. Zheng, A. W. Harley, B. Shen, G. Wetzstein, and L. J. Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In ICCV, 2023. [62] D. Sulsky, S.-J. Zhou, and H. L. Schreyer. Application of particle-in-cell method to solid mechanics. Computer physics communications, 87(1-2):236252, 1995. [63] Y. Hu, Y. Fang, Z. Ge, Z. Qu, Y. Zhu, A. Pradhana, and C. Jiang. moving least squares material point method with displacement discontinuity and two-way rigid body coupling. ACM Transactions on Graphics (TOG), 37(4):114, 2018."
        }
    ],
    "affiliations": [
        "Columbia University",
        "University of Illinois Urbana-Champaign"
    ]
}