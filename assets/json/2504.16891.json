{
    "paper_title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset",
    "authors": [
        "Ivan Moshkov",
        "Darragh Hanley",
        "Ivan Sorokin",
        "Shubham Toshniwal",
        "Christof Henkel",
        "Benedikt Schifferer",
        "Wei Du",
        "Igor Gitman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license."
        },
        {
            "title": "Start",
            "content": "2025-4-24 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, Igor Gitman Abstract: This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under commercially permissive license. 1. Introduction Recent advances in large language models (LLMs) have significantly improved their ability to solve complex reasoning tasks, including olympiad-level mathematics. key idea behind this progress has been to allow models to spend more tokens thinking about the solution before producing the final answer. Initially, models were trained to produce series of intermediate solution steps (chain-of-thought (CoT) [35]). More recently, long reasoning models [12, 10] have learned to reflect on their work, exploring and refining multiple strategies within single generation. This has led to further improvements across mathematics, 5 2 0 2 3 2 ] . [ 1 1 9 8 6 1 . 4 0 5 2 : r Figure 1: Accuracy on math problems from AIME and HMMT competitions. 2025 NVIDIA. All rights reserved. AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset coding, and scientific domains. To keep pace with this rapid development, the community has introduced increasingly challenging benchmarks and competitions that help to evaluate the progress. The AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) is an initiative designed to assess advancements in this domain by challenging participants to create models capable of solving 50 difficult, nationallevel mathematical problems within strict computational limits. These problems were never published online, ensuring more rigorous evaluation compared to traditional benchmarks. This report details our first-place submission to the competition, which correctly solved 34 out of 50 problems on the private test set. To develop the winning recipe, we focused on addressing several limitations of the publicly available reasoning models that we describe below. Large-scale long-reasoning dataset (2). To improve existing models we started by collecting an extensive set of mathematical problems from the internet. We developed an LLM-based problem extraction and refinement pipeline to construct dataset of 540K unique problems. Using this dataset, we then generated 3.2M long-reasoning CoT solutions by prompting DeepSeek-R1 [10] and QwQ-32B [29]. Training Qwen2.5-Base models [39] on this large-scale distillation data, we are able to surpass the accuracy of all other open-weight models of comparable size, except for QwQ-32B, which is slightly better than our 32B model. Tool-Integrated Reasoning (3). To improve the results further we developed method for integrating code execution into long-reasoning generations. Our initial attempts to elicit Tool-Integrated Reasoning (TIR) from DeepSeek-R1 and QwQ-32B through simple prompting proved unsuccessful. We hypothesize that these models struggle to deviate from their standard solution format due to extensive training on reasoning tasks and limited exposure to instructionfollowing. To overcome this challenge, we built pipeline that starts with small-scale reasoning finetuning of an instruction-following model [42]. By prompting this model to generate long-reasoning TIR solutions followed by aggressive quality filtering, we established an initial dataset suitable for training. Through multiple iterations of training, generation, and filtering, we constructed 1.7M TIR solution set that was crucial for improving the accuracy of our final models. To make TIR more efficient, we also developed method to accurately control the number of code executions the model is allowed to make for each generation. Generative Solution Selection (4). common approach to maximize model accuracy is to generate multiple candidate solutions and select the most promising one. While majority voting [34] serves as strong baseline, its performance falls significantly short of the theoretical maximum performance of pass@k. To address this limitation, we developed pipeline for training models to identify the most promising solution when presented with multiple candidates. We generated 566K selection examples to train our models. Although this approach showed considerable promise, we ultimately were unable to integrate it into our AIMO-2 Kaggle submission due to the competitions strict time constraints. Combining these three innovations, we developed series of state-of-the-art open-weight math reasoning models with 1.5B, 7B, 14B, and 32B parameters. Each model supports CoT, TIR and GenSelect inference modes when appropriately prompted. For our winning AIMO-2 submission we used an intermediate version of the 14B model and implemented various inference optimizations to accommodate the competitions time and compute constraints. We discuss model training process and evaluation results in Section 5 and list Kaggle-specific optimizations in Section 6. To accelerate progress in open-source mathematical reasoning, we are releasing our code, finetuned OpenMath-Nemotron models, and the complete OpenMathReasoning dataset under commercially permissive license.1 2. Data Preparation In this section, we outline our validation and training data curation pipeline. Section 2.1 presents our methodology for preparing large-scale problem set for training. Section 2.2 describes our validation set collection process. Finally, Section 2.3 details our approach to synthesizing long-reasoning Chainof-Thought (CoT) solutions. 2.1. Problems preparation We collect large set of mathematical problems from the Art of Problem Solving (AoPS) community forums. We include all forum discussions except Middle School Math, which we found to be too elementary and unhelpful for training in our preliminary experiments. After retrieving forum discussions, we implement systematic process to extract problems and their corresponding answers. Throughout our pipeline, we utilize Qwen2.5-32B-Instruct [39] for all processing steps unless specified otherwise. 1Data and at https://huggingface.co/collections/nvidia/openmathreasoning68072c0154a5099573d2e730, at code https://github.com/NVIDIA/NeMo-Skills avaliable available models our are is 2 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset 1. Problem Extraction: We prompt an LLM to identify and extract all problems from the initial forum posts (Appendix A.7). While most posts contain single problem, some include multiple problems or none at all. 2. Problem Classification: Each extracted problem is classified into the following categories. We use an LLM to perform the classification: Proof problem or not (Appendix A.4) Multiple choice question or not (Appendix A.3) Binary question (yes-or-no answer) or not (Appendix A.1) Valid problem or not (Appendix A.2)2 We remove all multiple-choice questions, binary questions, and invalid problems from the final dataset. 3. Question Transformation: For proof questions, we convert them into answer-based questions that require similar problem-solving techniques (Appendix A.5). 4. Answer Extraction: For non-proof questions, we attempt to extract the final answer from the forum discussions (Appendix A.6)3. 5. Benchmark Decontamination: Following [41] we use an LLM-based comparison to remove questions that closely resemble those in popular math benchmarks. All prompts and scripts necessary to run the above pipeline are available in NeMo-Skills. Table 1 has breakdown of the dataset size after each processing stage and Table 2 shows the final dataset composition. We provide comparison with other popular datasets sourced from AoPS forums in Table 3. Pipeline Stage Original forum discussions Extracted problems Removing bad problems Benchmark decontamination Data Size 620K 580K 550K 540K"
        },
        {
            "title": "Subset\nConverted proofs\nWith extracted answer\nNo extracted answer\nTotal problems",
            "content": "Size 260K 190K 90K 540K Table 2: Final dataset composition. Dataset OpenMathReasoning (ours) AoPS-Instruct [20] NuminaMath-1.5 (AoPS part) [14] # of Problems 540K 650K 68K Table 3: Comparison with other datasets sourced from AoPS forums. Our work was done concurrently with [20] and [14]. gathered from the Art of Problem Solving forums. We restricted our selection to 2024 and 2025 competitions to minimize potential data contamination. AIME and HMMT problems were selected for our validation set due to their strong alignment with AIMO-2 competition requirements. They covered similar mathematical topics, matched the difficulty level, and were predominantly non-proof-based questions requiring single numerical answers. We excluded proof-based questions and those awarding partial credit based on estimate accuracy, as these are generally incompatible with an exact match evaluation framework. The resulting dataset, which we call Comp-Math-24-25, consists of 256 problems, as detailed in Table 4. Problem source # of Problems AIME 2024 AIME 2025 HMMT Nov 2024 HMMT Feb 2024 HMMT Feb 2025 Total 30 30 62 68 66 256 Table 1: Dataset size after each processing stage. Table 4: Composition of our Comp-Math-24-25 validation dataset. 2.2. Comp-Math-24-25 Benchmark To create robust validation dataset for our evaluation, we combined problems from American Invitational Mathematics Examinations (AIME) and Harvard-MIT Mathematics Tournaments (HMMT) 2E.g. problems that are lacking context or referring to other problems are considered invalid. 3We do not try to extract the full solution, just the final answer. 2.3. Text-based Solution Synthesis To generate CoT solutions, we follow common pipeline of directly prompting an existing open-weight LLM to solve problems collected in Section 2.1. We utilize DeepSeek-R1 and QwQ-32B models and generate up to 32 solution candidates for each problem in our dataset. We use temperature 0.7, top-𝑝 = 0.95, and limit generations to 16384 tokens. We gener3 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset ate more solutions for harder problems with known answers, where the hardness was estimated by computing an average pass-rate across 32 generations from the Qwen2.5-72B-Math-Instruct model [40]. As the final filtering step we remove any solutions that do not reach the expected answer. Predicted and expected answers are compared by prompting Qwen2.5-32B-Instruct to judge whether they are equivalent in the context of the problem (we re-use judge prompt from [30]). For each problem where we werent able to extract the final answer (and for all converted proofs) we treat the most common answer across all available solution candidates as the groundtruth. Table 5 shows the final distribution of CoT solutions in our dataset."
        },
        {
            "title": "Model",
            "content": "QwQ-32B DeepSeek-R1 Total"
        },
        {
            "title": "CoT solutions",
            "content": "after filtering 0.5M 2.7M 3.2M all 1.0M 4.2M 5.2M Table 5: Final distribution of CoT solutions in our dataset. 3. Tool-Integrated Reasoning Allowing LLMs to integrate natural language reasoning with Python code execution is known way of improving accuracy on challenging math problems [31, 40]. However, the best open-weight reasoning models (most notably DeepSeek-R1 [10] and QwQ-32B [29]) are not able to directly produce such Tool-Integrated Reasoning (TIR) solutions. Our initial attempts to induce TIR generations by prompting these reasoning models with direct instructions or fewshot examples turned out to be unsuccessful. Unable to solve this via prompting, we had to develop more elaborate pipeline for building reasoning models capable of producing TIR solutions. In our early experiments, we noticed that when non-reasoning instruct LLMs are trained on limited quantity of reasoning data [42], they tend to retain their good instruction-following abilities. Building on this intuition, we were able to successfully prompt LIMO-Qwen-32B [42] model to produce TIR solutions, but found that they tend to be low-quality on average. The produced code was often irrelevant or was merely used to verify calculations of preceding CoT steps. To overcome this, we developed filtering step aimed at retaining only high-quality examples where code execution provides substantial reasoning benefits. Using this filtered dataset, we then fine-tuned our reasoning model, achieving significant performance improvements over the CoT-only predecessor. Finally, we employed an iterative model improvement approach by training more powerful TIR model in each iteration and using it to generate and filter additional TIR examples, further enhancing model performance. In the following subsections, we detail each stage of this pipeline. 3.1. Instruction-following reasoning model Prior work [21, 42] shows that fine-tuning on as few as 1000 samples is sufficient to make LLM produce longCoT solutions. We hypothesize that an instruct model fine-tuned on such small dataset can potentially preserve its instruction-following and long-reasoning capabilities. To test this, we prompted LIMO-Qwen-32B to solve the problem using Python code for the steps that require complex calculations. The zero-shot prompt we designed for this purpose is provided in Appendix B.1. For roughly half of the problems, the model produced solution that contained at least one Python code block. We then synthesized 1.2M solutions for OpenMathReasoning problems, using temperature=0.7, top-𝑝 = 0.95, allowing maximum sequence length of 16384 tokens and stopping generations if the solution contained more than 8 code executions. 3.2. Filtering TIR data Careful inspection of generated solutions revealed that code execution often does not benefit the solution and could easily be replaced with several simple CoT steps (see example in Appendix F.2). Instead, we want an ideal TIR solution to provide significant shortcuts by implementing otherwise infeasible brute-force approaches, e.g., using numeric solvers or conducting an exhaustive search of possible solutions. To filter unwanted code usages, we apply several filters. First, we utilize Qwen2.5-32B-Instruct to classify each code block by two criteria: novel calculation / verification. Whether the code execution leads to novel result or it simply verifies the previous steps (see the prompt in Appendix B.2). significant / moderate / trivial. Whether the code implements an important part of the solution or is easily substitutable with several CoT steps (see the prompt in Appendix B.3). We then only keep solutions that either have at least one novel and significant code block or more than half novel and moderate code blocks. Additionally, we 4 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset apply rule-based filtering and remove solutions with incorrect final answer and solutions without code execution. We also remove solutions with more than two code blocks, as we found it to be helpful in our preliminary experiments. As part of preprocessing, we also replace the tags marking the start and end of code blocks. During stage-0 generation, we instruct the model to place code between \"python\" and \"n\", following markdown-like style that models can easily produce; we then replace these with \"<tool_call>\" and \"</tool_call>\" tags, respectively, to make the code ending tag distinguishable from regular markdown and facilitate code extraction. All described filtering steps result in the TIR dataset, consisting of 15k samples, which we will refer to as stage-0 TIR data. 3.3. Iterative data generation For the next stage of TIR solution generation, we leverage QwQ-32B as it proved to be powerful yet lightweight synthetic reasoning data generator. For this purpose, we fine-tune it on the stage-0 data for 7 epochs with constant learning rate of 5e-6. We then synthesize solutions for OpenMathReasoning problems. We generate 700K samples and filter them down to 260K by removing incorrect solutions and solutions not using code. We find that novelty and significance filters degrade the performance at this stage, so we do not use them. To further improve results, we repeat this process one more time using an intermediate version of our 14B model, which was finetuned on the CoT-only subset of OpenMathReasoning data. We train this 14B model on QwQ-32B solutions and then execute final round of data generation and filtering, ultimately resulting in the final 1.7M TIR dataset. 3.4. Controlling the number of code blocks We developed simple, yet effective method to control the number of code blocks that the model can use. During all data generation stages, we format the code output as shown in Appendix F.1, appending additional notification warning about how many code executions are remaining. We find that model often refers to this message in its thinking process, refraining from further code usage when the limit is reached. Thus, for each problem we randomly select between 1 and 8 allowed code executions and provide this information in the prompt. We remove generations that try to use more code blocks than requested in order to reinforce the correct behavior in training. As result, model learns to follow specified code execution limit. An example of this behavior is provided in Appendix F.3. 4. Generative Solution Selection We observe considerable gap in the majority@k vs pass@k performance for our models, implying the models theoretical ability to solve far more problems than can be achieved with majority answer. To bridge this gap, we explore training model that, given set of candidate solution summaries, picks the most promising solution. In our early experiments, we found that comparing multiple solutions yields significantly better results than judging each solution in isolation. Following [45], we do not change the models architecture and instead let it reason in natural language before selecting one of the provided solutions. We detail the pipeline to prepare the training data for such selection generations (GenSelect) in the following sections. The data construction pipeline of is shown in Figure 3. 4.1. Creating New Summaries Solutions generated by reasoning models have thinking part and summary which follows it. We noticed that summaries generated by reasoning models, such as DeepSeek-R1, could be very succinct; in extreme cases, they could just be stating the final answer. Since we require representative summary for comparing different solutions during inference, we replace the native summary of the reasoning models by synthesizing new summaries with the Qwen2.5-32B-Instruct model. We synthesize four candidate summaries per solution with maximum length of 2048 tokens. To ensure the summary is faithful, we filter out summaries where the predicted answer is different from the original solutions predicted answer. If there are no valid summaries, we discard the sample4, otherwise we select the longest summary to replace the original summary. We regenerate summaries for the entire OpenMathReasoning dataset using this process, so that models trained on it can produce these summaries directly. See Appendix for comparison between one-word DeepSeek-R1 summary and new one generated by Qwen2.5-32B-Instruct. 4.2. Generating Selection Candidates We discover that modest accuracy gains over majority voting can be achieved by simply presenting new solution summaries to reasoning models and prompting them to compare and select one (see prompt in Appendix C.3). Building on this observation, we develop the following pipeline to generate training data for this GenSelect inference."
        },
        {
            "title": "For each problem in the OpenMathReasoning",
            "content": "4No more than 5% of all samples were discarded this way. 5 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset (a) 14B CoT (b) 14B TIR Figure 2: Comparison of majority, GenSelect and pass metrics for different number of generation samples. To construct the input for GenSelect, we use subsets of 16 solutions (or all if fewer samples were generated). For the final answer, we perform majority@8 over the answers selected by the GenSelect. OpenMath-Nemotron -14B model is used to perform CoT, TIR, and GenSelect inference. We find that GenSelect becomes unstable when using more than 32 generations as we can no longer show all solutions in single prompt. dataset, we randomly sample between 2 and 16 candidate solution summaries. We ensure that each sample group contains at least one correct and one incorrect solution. This process is repeated until we obtain 8 distinct comparison groups for each problem. Using the GenSelect prompt (Appendix C.3), we then task QwQ-32B with selecting the most promising solution from each group. This procedure generates 1M selections, which we subsequently filter down to 565K by eliminating any instances where incorrect solutions were chosen. 4.3. Reducing computational cost While this dataset is suitable for training, the comparison generations can be as long as the original solutions, making GenSelect inference computationally expensive. To address this challenge, we explored training models to directly generate the final comparison summary rather than learning the full reasoning trace. Consistent with our previous observations, the natural comparison summaries produced by QwQ-32B proved suboptimal. We therefore again used Qwen2.5-32B-Instruct to regenerate all comparison summaries (see the prompt in Appendix D.1) and trained our models using these summarized comparisons. Our early experiments revealed only small reduction in accuracy (12%) compared to models trained on the whole reasoning traces. This final setup makes GenSelect inference remarkably efficient compared to the original long-reasoning generations. With output tokens capped at 2048, most computation occurs in highly-parallelizable pre-filling phase. Since each solution summary is similarly limited to 2048 tokens, the total input context cannot exceed 32768 tokens when using the maximum of 16 solutions per problem. Although more than 16 solutions could theoretically be included in prompt, we generally observe diminishing returns as the context becomes too large. For scenarios requiring evaluation of more solution candidates, we propose sampling 16 solutions multiple times and then performing majority voting to determine the final answer. Nevertheless, our findings indicate that the most significant accuracy improvements occur when GenSelect is applied to smaller number of generations (Figure 2). 5. OpenMath-Nemotron models In this section we present the training and evaluation details of our OpenMath-Nemotron series of models. 5.1. Training To build our final models we perform supervisedfinetuning (SFT) on series of Qwen2.5-Base models (1.5B, 7B, 14B and 32B) [39]. For 1.5B and 7B models, we start from the special model versions finetuned for mathematical reasoning tasks [40]. Unlike general Qwen2.5 models, the math versions only support limited context window of 4096 tokens, which is inadequate for the long-reasoning generations. To overcome this, we follow [2] and change RoPE [27] base to 500K. 6 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset rate is set to be 1000 times smaller. We use batch size of 1024 samples and leverage sequence packing and context parallelization techniques from NeMoAligner [26] that significantly accelerate training on the long-reasoning data. Following [30] we save 4 equally spaced checkpoints during the training runs, which are averaged to create the final model. We show the accuracy on the Comp-Math-24-25 benchmark (Section 2.2) of intermediate 1.5B and 14B model checkpoints in Figure 4. After the first round of training, we perform another SFT on subset of harder problems. These problems are selected only from forums discussing Olympiad math and we discard any problems for which Qwen2.5-Math-72B-Instruct TIR model has pass-rate bigger than 0.3 out of 32 generations. Additionally, we filter any solutions that have fewer than 5000 tokens. The total SFT data size of this harder set is 2.2M samples. We follow the same setup as for the first round of SFT except we train for 4 epochs instead of 6. We do this second round of training for all models except 32B as we found some degradation in results. Models accuracy after the first and second round of training is presented in Table 6. We find that CoT results tend to significantly improve while TIR results stay stable or slightly degrade."
        },
        {
            "title": "Model First SFT Second SFT",
            "content": "1.5B CoT 1.5B TIR 7B CoT 7B TIR 14B CoT 14B TIR 55.1 64.1 61.3 71.1 62.9 74.6 58.2 64.5 62.5 70.7 65.2 73.4 Table 6: Accuracy with majority@64 on the Comp-Math-24-25 benchmark after the first and second SFT rounds. We see significant gains for CoT generations and comparable results for TIR generations. 5.2. Results Final evaluation results of our models are presented in Table 7. In addition to Comp-Math-24-25, introduced in Section 2.2, we use Humanitys Last Exam dataset [6]. We only evaluate on subset consisting of 975 text-only problems from Math category. We refer to it as HLE-Math. We notice that despite being superior in majority@𝑘 setting with TIR prompt, smaller models perform on par or even worse in pass@1, compared to CoT prompt. The results in Table 8 suggest that the 7 Figure 3: Data construction pipeline of GenSelect. The GenSelect input is constructed by sampling solution summaries of both correct and incorrect instances, ensuring that the input contains at least one correct and one incorrect solution. The input is then fed to QwQ-32B, which is tasked with selecting the best solution among the candidate solutions. The reasoning traces that select correct solutions are summarized with Qwen2.5-32B-Instruct, which forms the GenSelect output. All models are trained for six epochs on combination of three tasks: CoT solution generation, TIR solution generation, and GenSelect, where the task is to select one correct solution out of multiple candidates. Each task is defined by unique prompt that we can use at inference time to switch between different generation modes (see prompts in Appendix C). We found that training on mix of all tasks results in similar accuracy compared to training on each task sequentially (first CoT, then TIR, then GenSelect). The total SFT dataset size is 5.5M samples (3.2M CoT, 1.7M TIR, and 566K GenSelect). We train all models using AdamW optimizer [18] with weight decay of 0.01 and cosine learning rate decay schedule with 10% linear warmup. We use starting learning rate of 3e-4 for 1.5B, 2e-4 for 7B and 1e-4 for 14B and 32B models. The final learning AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset (a) 1.5B (b) 14B Figure 4: Accuracy improvement through the course of training. We observe that smaller models need to be trained for longer to achieve meaningful improvements. reason is that with the TIR prompt there are more unfinished solutions across all model sizes, with 1.5B clearly standing out. We hypothesize that the reason behind this is that smaller models are less consistent in using tools effectively. 6. Kaggle submission In this section, we present the details of our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) [7]. AIMO-2 is competition organized by the AIMO Prize team and hosted on Kaggle 5. The competition featured 110 challenging math problems: 10 were made publicly available as reference subset, while the remaining problems were split between the public and private leaderboards. Submitted solutions were evaluated under strict computational constraints: 5-hour time limit in an offline Jupyter notebook environment powered by four L4 GPUs. Our 1st-place submission correctly solved 34 out of 50 questions on the private leaderboard. 6.1. Training recipe For our winning Kaggle submission we used somewhat different training recipe that we detail in this section. We first trained Qwen2.5-14B-Base model for 8 epochs on 2.2M subset of CoT solutions, excluding any converted proof problems. We only used DeepSeek-R1 solutions for this training. This is followed by light-weight fine-tuning on 15k stage-0 TIR samples. The process for collecting these samples is detailed in section 3.2. We train TIR model for 5https://www.kaggle.com/competitions/ai-mathematicalolympiad-progress-prize-2 400 steps with constant learning rate of 1e-5 and use the last checkpoint without averaging. We then merge CoT and TIR checkpoints as it both improves accuracy and speeds up generation by reducing solution length and number of code executions. We did not use GenSelect training or inference for the Kaggle submission. 6.2. Model Merging In this competition, we explored various methods for merging two LLMs with CoT and TIR behaviors. Our primary goal was to effectively combine the distinct strengths of these two fine-tuning stages to enhance model performance. We experimented with several merging techniques from mergekit [9] package. Surprisingly, the most effective approach turned out to be simple linear combination of the two checkpoints: the CoT checkpoint used before TIR fine-tuning and the best TIR checkpoint attained thereafter. This strategy allowed us to control the extent to which each stage influenced the final models behavior. Table 10 provides the accuracy, as well as the generation length and code usage statistics of the models before and after the described merging procedure. 6.3. Inference Optimizations The strict time limits of the competition presented serious constraint. An extra requirement was that problems had to be answered one-at-a-time making it harder to parallelize computation and allocate time. To overcome these challenges we implemented several optimizations that maximize inference efficiency while maintaining output quality. 8 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset"
        },
        {
            "title": "Model",
            "content": "DeepSeek-R1-Distill-Qwen-1.5B OpenMath-Nemotron-1.5B CoT OpenMath-Nemotron-1.5B TIR + Self GenSelect + 32B GenSelect DeepSeek-R1-Distill-Qwen-7B OpenMath-Nemotron-7B CoT OpenMath-Nemotron-7B TIR + Self GenSelect + 32B GenSelect DeepSeek-R1-Distill-Qwen-14B OpenMath-Nemotron-14B-MIX (kaggle) OpenMath-Nemotron-14B CoT OpenMath-Nemotron-14B TIR + Self GenSelect + 32B GenSelect QwQ-32B DeepSeek-R1-Distill-Qwen-32B OpenMath-Nemotron-32B CoT OpenMath-Nemotron-32B TIR + Self GenSelect DeepSeek-R1 Comp-Math-24-25 AIME24 AIME25 HMMT-24-25 26.8 (60.0) 61.6 (80.0) 52.0 (83.3) 83.3 83.3 54.4 (80.0) 74.8 (80.0) 72.9 (83.3) 86.7 86.7 65.8 (80.0) 73.7 (86.7) 76.3 (83.3) 76.3 (86.7) 86.7 90.0 78.1 (86.7) 66.9 (83.3) 76.5 (86.7) 78.4 (93.3) 93.3 79.1 (86.7) 14.2 (26.5) 39.9 (53.6) 37.2 (60.7) 62.2 62.8 30.6 (42.9) 49.7 (57.7) 54.6 (66.3) 68.4 69.9 40.1 (52.0) 50.5 (64.8) 52.1 (60.7) 58.6 (70.9) 72.4 71.9 55.9 (63.3) 39.9 (51.0) 53.0 (59.2) 59.7 (70.9) 73.5 53.0 (59.2) 21.4 (36.7) 49.5 (66.7) 39.7 (70.0) 70.0 70.0 38.6 (53.3) 61.2 (76.7) 57.5 (76.7) 76.7 76.7 48.4 (60.0) 57.9 (73.3) 63.0 (76.7) 61.3 (76.7) 76.7 76.7 66.5 (76.7) 51.8 (73.3) 62.5 (73.3) 64.2 (76.7) 80.0 64.3 (73.3) HLE-Math 2.9 (5.0) 5.4 (5.4) 2.5 (6.2) 7.9 8.3 3.3 (5.2) 6.6 (6.6) 7.8 (10.8) 11.5 11.9 4.2 (4.8) 5.7 (6.5) 7.5 (7.6) 9.5 (11.5) 14.1 13.7 9.0 (9.5) 4.8 (6.0) 8.3 (8.3) 9.2 (12.5) 15.7 10.5 (11.4) Table 7: Evaluation results on mathematical benchmarks. All models are evaluated with maximum of 32768 output tokens, temperature of 0.6, and top-p 0.95. We present metrics as pass@1 (maj@64) where pass@1 is an average accuracy across 64 generations and maj@64 is the result of majority voting. The 14B model used in our kaggle submission is denoted as (kaggle). For HMMT and HLE-Math benchmarks we use LLM-judge setup of [30] to verify the answers. To construct the input for GenSelect, we use subsets of 16 solutions from the set of 64 solutions. We repeat this 64 times and perform majority voting over the answers selected by the GenSelect. 6.3.1. TensorRT-LLM Optimization size also freed up memory for larger key-value caches. Pretrained models were converted to TensorRT engines using TensorRT-LLM [23]. TensorRT-LLMs in-flight batching boosts throughput by dynamically grouping inference requests, releasing each sample as soon as it completesreducing latency and optimizing GPU utilization. Since samples are processed independently, batches can mix different prompts or inference parameters seamlessly. TensorRT-LLM includes number of other optimizations such as custom attention kernels and paged KV caching. Quantization involves speed-accuracy tradeoff, as outlined in TensorRT-LLMs best practices [24]. We prioritized int8 weight-only (W8A16) and FP8 quantization, which delivered faster inference than BF16 with minimal accuracy loss. The reduced weight 6.3.2. Speculative Decoding To accelerate inference, we employ ReDrafter [4], recurrent speculative decoding technique that uses an RNN-based drafter to propose and verify multiple tokens per decoding step. We trained drafter capable of proposing up to three tokens at each step, with all three tokens being accepted in approximately 65% of the steps. For training ReDrafter, we sampled random subset of 100k problems from the OpenMathReasoning dataset. With the target model, we generated one solution per problem, leveraging the resulting data to train the drafter. Table 9 presents an evaluation of various quantizaAIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Model Prompt Unfinished (in %)"
        },
        {
            "title": "Method",
            "content": "Speed (tok/s) AIME24 AIME25 1.5B 7B 14B 1.5B 7B 14B"
        },
        {
            "title": "TIR",
            "content": "2.23 0.98 1.13 40.31 6.45 4.06 BF16 W8A16 (int8) W4A16 (int4) FP8 FP8+ReDrafter 210 315 436 310 554 82.7 82.7 72.7 83.3 81.3 66.7 66.7 60.7 68.7 71.3 Table 8: Percentage of unfinished solutions on the Comp-Math-24-25 dataset. We generate 32k tokens and consider solution unfinished if it does not contain boxed. Table 9: Submission pipeline with different optimizations methods benchmarked on 4 L4 GPU. Reported scores are maj@12 on the merged model averaged over 5 runs each. Bold indicates configuration used in our winning submission. tion techniques and the speculative decoding method, analyzing their impact on both the inference speed and the accuracy. We experimented with various sampling parameters but observed minimal differences in the results. We thus based our winning submission on an almost greedy search strategy by setting temperature to 0 and enabling the redrafter_greedy_search parameter. Despite these settings TensorRT-LLM still produced varying outputs within single batch of identical prompts. We did not investigate this behavior in detail, but we suspect that it could be related to an accumulation of small numerical errors which cause few tokens to be different early on in the generation. This difference then accumulates over many tokens resulting in substantially diverse solution set at the end. Ultimately, we chose this approach because it provided more stable results at small batch sizes and offered small improvement in the speed of speculative decoding."
        },
        {
            "title": "Model",
            "content": "maj@16 pass@16 length code CoT TIR CoT*0.3 + TIR*0.7 62.9 66.8 69.1 76.2 80.1 81.3 11203 15834 12489 - 2.73 0. Table 10: Accuracy and generation statistics of merged models on Comp-Math-24-25 dataset. length indicates the average number of tokens per solution, while code refers to the average number of code executions per solution. 6.3.4. Time Management buffering strategy was implemented, allocating 350 seconds per question as the base time limit. If question completed early, the unused time was added to shared buffer. The next question could then draw up to 210 extra seconds from this buffer, allowing maximum of 560 seconds when combined with its base allocation. 6.3.3. Model Serving Models were served via FastAPI backend powered by Nemo-Skills [22], which supports time-constrained generation. This allowed us to dynamically limit response times per questionif an answer wasnt completed within the window, we returned early to check for extractable results. Nemo-Skills async generation enabled batched processing with early stopping. For example, in batch of 16, if the first 4-5 completions agreed on the final answer, we canceled the remaining generations and proceeded to the next question. We also mitigated stragglerssamples that ran significantly longer than othersby canceling the last 𝑛 pending requests once the rest finished. This early stopping increased response relevance as shorter answers tended to be higher quality. 6.3.5. Code Execution Integration For tool-calling capabilities, we used Nemo-Skillss code execution wrapper to enable tool-integrated reasoning. Flask-based sandbox environment handles parallelized Python execution for each inference thread, processing LLM-generated code blocks with strict safeguards: Maximum 6 code calls per generation cycle 2 second timeout for each code execution Only the first 200 characters of the code output were shown back to the LLM The system feeds back either execution results or error traces into the generation process, enabling iterative reasoning while maintaining computational efficiency. AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset 6.4. Discussion Our Kaggle submission is based on an early development version of the final OpenMath-Nemotron-14B model. This model was trained on smaller dataset, did not have GenSelect capability, and could not switch between CoT and TIR modes by changing the prompt. While we did have much better checkpoint towards the end of the competition, we were ultimately unable to make high-scoring submission with it. In this section, we explore several potential explanations for why this happened. High variance in scores. The competition rules allow only single submission per day. Since the public leaderboard consists of only 50 problems presented in random order, we observed substantial variance across our submissions. This made it hard to make quick decisions on which directions to prioritize, especially in cases when our local evaluations disagreed with the leaderboard scores. Focus on smaller models. As shown in Table 7 OpenMath-Nemotron-7B model performs comparably or better than the 14B model used in Kaggle. Observing this towards the end of the competition, we tried to prioritize submissions with the smaller model, allowing it more generations, and also increased the maximum generation length. Yet we were unable to see comparable leaderboard scores. This discrepancy suggests that either our local evaluation set differs substantially from what was used in Kaggle, or that the smaller models struggle with few particularly challenging problemsa limitation difficult to detect through aggregate benchmark scores alone. Longer average generations. Our local evaluations always had fixed token budget for each generation. However, the time management logic implemented in Kaggle (Section 6.3.4) heavily relied on solving easy problems quickly to allocate more time for challenging ones. Interestingly, we discovered that although our final models achieved superior scores within the same token budget, they produced around 10% more tokens on average. Not realizing this early enough, we were unable to fix this undesirable feature before the end of the competition. 7. Related Work 7.1. Tool Integration Reasoning Tool-augmented approaches to mathematical problem solving have advanced rapidly in recent years. seminal contribution by Chen et al. [3] introduced the Program of Thoughts (PoT) framework, which integrates natural language with executable code to support step-by-step reasoning through hybrid of textual and programmatic logic. Building on this foundation, subsequent research has focused on developing both datasets and models that facilitate tool-integrated reasoning. On the data side, OpenMathInstruct-1 [31] offers 1.8 million instruction-tuning examples derived from code interpreters across benchmarks such as GSM8K and MATH. Similarly, InfinityMATH [44] introduces 100K instances of programmatic reasoning, while MARIO [16] combines model reasoning with tool outputs, accompanied by dataset constructed from GSM8K [5] and MATH [11]. These resources have significantly enriched the training landscape for toolaugmented reasoning systems. On the modeling side, Qwen2.5 [40] introduced series of models with strong mathematical reasoning capabilities, supporting advanced techniques like Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). Gao et al. [8] proposed two-stage method: training large language models to generate reasoning chains, and then invoking domain-specific tools to execute each step by injecting the necessary knowledge. Xiong et al. [38] proposed multi-turn, online, iterative direct preference learning framework tailored to this unique context. By incorporating feedback from code interpreters during the training process, their approach achieves significant performance improvements on the MATH benchmark. Wu et al. [36] dynamically integrate web search, code execution, and structured reasoning with contextual memory to tackle complex problems that demand deep research and multistep logical deduction. Li et al. [15] developed ToolIntegrated Reinforcement Learning framework that autonomously utilizes computational tools by scaling reinforcement learning directly from base models, and demonstrate substantial improvements compared to RL without tools. 7.2. Generative Reward Models Conventional reward models and verifiers are often trained as discriminative binary classifiers, underutilizing the generative strengths of large language models (LLMs). To address this, Generative Reward Models (GenRM) [19], introduced by Mahan et al., reformulate verification as generation taskusing the log probabilities of tokens like Yes or No to represent correctness. This framing allows GenRM to better exploit LLMs natural language generation capabilities, leading to improved alignment with human judgments across both in-distribution and out-ofdistribution tasks. Concurrently, Zhang et al. [45] introduced Generative Verifiers, training CoT-GenRM with supervised fine-tuning (SFT) objective to serve 11 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset as verifier for mathematical reasoning. Building on similar motivation, Ankner et al. [1] combined Chainof-Thought (CoT) reasoning generation with BradleyTerry reward modeling, enabling reward models to explicitly reason about response quality before assigning scores. Extending this line of work, Wang et al. [33] proposed self-taught evaluators, jointly training generative models and LLM-as-a-Judge frameworks to produce both intermediate reasoning traces and final judgments. In related approaches, Wang et al. [32] trained large language models as generative judges by leveraging Direct Preference Optimization (DPO) on both positive and negative data, demonstrating improved evaluation performance across diverse tasks. Wu et al. [37] introduced Meta-Rewarding step in the self-improvement process, enabling the model to evaluate its own judgments and use the feedback to refine its evaluation capabilities. 7.3. Math Reasoning Datasets In the pursuit of improving mathematical reasoning in large language models, researchers have recently introduced several large-scale, high-quality datasets. Skywork-MathQA [43] stands out with its 2.5 million question-answer pairs, generated using trio of augmentation methods and built upon varied set of foundational problems. Complementing this, NuminaMath [13] offers 860K challenging competition-style math problems, each carefully annotated with step-bystep reasoning chains [34], enabling more interpretable and structured model outputs. More recent work has focused on advancing the complexity and depth of reasoning. New datasets have emerged that emphasize challenging questions paired with rich, multi-step reasoning traces, pushing models to handle more sophisticated mathematical thought processes. BackMATH was introduced in [46], novel dataset centered on backward reasoning. It contains approximately 14K problems specifically designed to support backward problem-solving, along with 100K detailed reasoning steps that trace the reverse logical flow from solution to problem. The OpenR1 team released OpenR1-Math-220K [25], large-scale dataset for mathematical reasoning comprising 220K math problems. Each problem includes two to four reasoning traces generated by DeepSeek R1, based on problems from NuminaMath 1.5 [14]. In addition, Zhao et al. [47] presented AM-DeepSeek-R1-Distilled, largescale dataset featuring 1.4 million question-response pairs with associated thinking traces for general reasoning tasks. This dataset is composed of high-quality, challenging problems aimed at advancing reasoning capabilities. Following similar direction, Liu et al. [17] introduced Chinese version of the DeepSeek-R1 distilled dataset, consisting of 110K question-solution pairs. The DolphinR1 team [28] released dataset of 800K samples, combining outputs from various reasoning models, including DeepSeek-R1, Gemini 2.0 Flash Thinking, and Dolphin Chat. 8. Conclusion In this paper, we present our winning submission to the AIMO-2 competition and pipeline for developing state-of-the-art mathematical reasoning models. Our contributions can be summarized as follows: We develop method to combine code execution with long chain-of-thought (CoT) generations to produce tool-integrated reasoning (TIR) solutions. We create pipeline for training models to generate samples that select the most promising solution from multiple candidates (GenSelect). We release large-scale OpenMathReasoning dataset. It contains 540K unique mathematical problems, 3.2M long chain-of-thought (CoT) solutions, 1.7M long tool-integrated reasoning (TIR) solutions, and 566K generative solution selection (GenSelect) traces. We release series of OpenMath-Nemotron models capable of operating in CoT, TIR, or GenSelect inference modes. With this release, we establish new state-of-the-art in mathematical reasoning among open-weight models."
        },
        {
            "title": "References",
            "content": "[1] Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-out-Loud Reward Models. arXiv preprint arXiv:2408.11791, 2024. [2] bloc97. Ntk-aware to without scaled rope extended any allows (8k+) fine-tuning degradation. have llama models size context and minimal https://www.reddit.com/r/LocalLLaMA/ comments/14lz7j5/ntkaware_scaled_rope_ allows_llama_models_to_have/, 2023. perplexity [3] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. TMLR, 2023. [4] Yunfei Cheng, Aonan Zhang, Xuanyu Zhang, Chong Wang, and Yi Wang. Recurrent Drafter 12 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset for Fast Speculative Decoding in Large Language Models. arXiv preprint arXiv:2403.09919, 2024. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word arXiv preprint arXiv:2110.14168, Problems. 2021. [6] Long Phan et al. Humanitys last exam, 2025. [7] Simon Frieder, Sam Bealing, Arsenii Nikolaiev, Geoff C. Smith, Kevin Buzzard, Timothy Gowers, Peter J. Liu, Po-Shen Loh, Lester Mackey, Leonardo de Moura, Dan Roberts, D. Sculley, Terence Tao, David Balduzzi, Simon Coyle, Alex Gerko, Ryan Holbrook, Addison Howard, and XTX Markets. Ai mathematical olympiad - progress prize 2. https://kaggle.com/competitions/ ai-mathematical-olympiad-progress-prize-2, 2024. Kaggle. [8] Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, and Tianlu Wang. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv preprint arXiv:2401.17464, 2024. [9] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees MergeKit: Toolkit for Merging Large Language Models, 2024. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. In NeurIPS Datasets and Benchmarks, 2021. [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [13] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. NuminaMath: The largest public dataset in AI4Maths with 860k pairs of competition math problems and solutions, 2024. [14] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. NuminaMath. [https://huggingface. co/AI-MO/NuminaMath-1.5](https://github. com/project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf), 2024. [15] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. [16] Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. MARIO: MAth Reasoning with code Interpreter OutputA Reproducible Pipeline. arXiv preprint arXiv:2401.08190, 2024. [17] Cong Liu, Zhong Wang, ShengYu Shen, Jialiang Peng, Xiaoli Zhang, ZhenDong Du, The chinese dataset and YaFang Wang. distilled from deepseek-r1-671b. https: //huggingface.co/datasets/Congliu/ Chinese-DeepSeek-R1-Distill-data-110k, 2025. [18] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019. [19] Dakota Mahan, Duy Van Phung, Rafael Rafailov, and Chase Blagden1 Nathan Lile1 Louis Castricato. Generative Reward Models. arXiv preprint arXiv:2410.12832, 2024. [20] Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, and Renjie Liao. Leveraging Online OlympiadLevel Math Problems for LLMs Training and arXiv Contamination-Resistant Evaluation. preprint arXiv:2501.14275, 2025. [21] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple testtime scaling. arXiv preprint arXiv:2501.19393, 2025. [22] NVIDIA. NeMo-Skills. https://github.com/ NVIDIA/NeMo-Skills. 13 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset [23] NVIDIA. TensorRT-LLM. https://github. com/NVIDIA/TensorRT-LLM. [24] NVIDIA. TensorRT-LLM Best Practices. https://nvidia.github.io/TensorRT-LLM/ blogs/quantization-in-TRT-LLM.html. [25] OpenR1 Team. OpenR1 Math 220k, February 2025. Dataset available on Hugging Face. [26] Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, et al. Nemo-aligner: Scalable toolkit for efficient model alignment. arXiv preprint arXiv:2405.01481, 2024. [27] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. [28] DolphinR1 Team. https://huggingface.co/datasets/ cognitivecomputations/dolphinr1, February 2025. Accessed April 2025."
        },
        {
            "title": "Dolphin",
            "content": "r1. [29] Qwen Team. QwQ-32B: Embracing the Power of Reinforcement Learning, March 2025. [30] Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data. In ICLR, 2025. [31] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. OpenMathInstruct-1: 1.8 Million Math Instruction Tuning Dataset. In NeurIPS Datasets and Benchmarks, 2024. [32] Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. Direct Judgement Preference Optimization. arXiv preprint 2409.14664, 2024. [33] Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-Taught Evaluators. arXiv preprint arXiv:2408.02666, 2024. [34] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. SelfConsistency Improves Chain of Thought Reasoning in Language Models. In ICLR, 2023. [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. [36] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research. arXiv preprint arXiv:2502.04644, 2025. [37] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. MetaRewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge. arXiv preprint arXiv:2407.19594, 2024. [38] Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint arXiv:2409.02392, 2024. [39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, 2025. [40] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5Math Technical Report: Toward Mathematical Expert Model via Self-Improvement, 2024. [41] Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking Benchmark and Contamination for Language Models with Rephrased Samples, 2023. [42] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: Less is More for Reasoning. arXiv preprint arXiv:2502.03387, 2025. [43] Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Hu, Yang Liu, Shuicheng Yan, Han Fang, and Yahui Zhou. Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models The Story Goes On, 2024. [44] Bo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu. InfinityMATH: Scalable Instruction Tuning Dataset in Programmatic Mathematical Reasoning. arXiv preprint arXiv:2408.07089, 2024. [45] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative Verifiers: Reward Modeling as Next-Token Prediction. arXiv preprint arXiv:2408.15240, 2024. [46] Shaowei Zhang and Deyi Xiong. BackMATH: Towards Backward Reasoning for Solving Math Problems Step by Step. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 466482, 2025. [47] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large arXiv preprint Language Model Training. arXiv:2503.19633, 2025. 15 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset A. Problem Preparation Prompts A.1. Binary Problem Classification Prompt: Binary Problem Classification l o e math problem , and you need e m whether s n . Respond y with a t w . t problem meets c e , and not a t b r problem l e a a u i f and y : 1 . The problem l t s a \" , n e u l twoc c s s . 2 . The problem h e a s n e n ( . . , . u ? \" o i y p e , \" Determine whether t n i \" such \" o no \" , \" e h a a l s b r e t n i u f e \" ) n p e a way , problem s not l t sk a a e n , even h d be s i s not a u i . i can be Here a few examples . Example 1 Problem : s e t $0 . 4 3 9 5 3 0 8 9 9 9 9 9 9 t = 0 . 4 3 9 5 3 0 9 $ ? Output : a Example 2 Problem : Write s v l t d and s u 3 6 . Output : not a Example 3 terms g e c g s i which d e c between terms 9 , and t between f h and r terms a Problem : v e l n u o : $ c {{ ( 6 0 ^ c+x )+ ( 6 0 ^ c ) }}{{2}} = c {{ tan }}{{(1+ tan ^2 ) ^2}}+ c {{ x }}{{(1+ ^2 ) ^2}} $ Output : not a Example 4 Problem : Given q r c r i ( ax ^2 + bx + ) with f i s ( , , ) such t ( > ) and ( neq 0 ) , ax ^2 + bx + = 0 ) a has two t t l i o ? e t e t ( Output : a 16 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Example 5 : Problem : Can v i o cube be o i red , l , and e such t r t h c r ? r l r t s t s Output : a Example 6 : Problem : Can numbers ( c {{14 + 5 } } { { 9 } } ) and ( c {{17 4 } } { { 1 2 } } ) both be e s r some e ( ) ? so , d t e . Output : not a Example 7 : Problem : Can d a s a t $1 , 1 , 2 , $ and $3$ ? from i on p e h r e a t s r be Output : a Now e h problem you need x c t answer from . Problem : { problem } Output : A.2. Valid Problem Classification Prompt: Valid Problem Classification l o e problem t n a i , whether s a math problem based on g n t . from math forum . Your k o e n Respond with not a i e problem meets o e l n n i : 1 . s l f d math s n , such computing an r i , r n e t . v an a n , d a minimum , 2 . o i enough o t t be v s s d m e i even t o i e r advanced c s ( . . , i , a h , h u , u o ) . 3 . 4 . s not t n t o f t a e o ( . . , \"What s n t n mean ? \" o not not l math problem ) . y on e l o e such images i n n t . Otherwise , problem annot be v . p with a , but y h i l and o e n why you u r n , a t not v d . Important Notes : 1 . The t o y (>99%) pr obl ms l be i math prob lems . 2 . Only r l r s r v d , such : Problems i n f t s . Vague i m t a e r i on e l t cannot be e e images h t l . Openended c u i s n t than problems i . 17 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset 3 . problem e s , i , o i s . l l even s i t u s advanced methods e 4 . Do not l e whether problem has l o not . 5 . Do not l t i c y t problem h methods u d o i . 6 . Only c whether e e . i e formed math problem t can be n f y Here a few examples . Example 1 Problem : v e a n ( ( 2 ) ( 2 3 ) = ( ^ 2 ) ) . Output : not a d Example 2 Problem : v e math problem found on Facebook ( image v d ) Output : a Example 3 Problem : v e l n u o : $ c {{ ( 6 0 ^ c+x )+ ( 6 0 ^ c ) }}{{2}} = c {{ tan }}{{(1+ tan ^2 ) ^2}}+ c {{ x }}{{(1+ ^2 ) ^2}} $ Output : not a Example 4 Problem : Find a o u T? Output : v d Example 5 : Problem : v a h example s l problem o n m d and a g number . Output : a Example 6 : Problem : What s e a n $ {{B}} $ mean h n t e r ? Output : a 18 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Example 7 : Problem : t e i way u p 59 and 61? so , l t method Output : a Example 8 : Problem : Nonenn ( Note : There n one problem h v forum t . ) Output : a Example 9 : Problem : $a+b=31$ and $ab=240$ , Output : not a d n e sum h c o s $a$ and $b$ . Example 1 0 : Problem : What h l f $35461 ^54593428 $ mod 11 $ ? Output : not a d Now e h problem you need x c e answer from . Problem : { problem } Output : A.3. Multiple Choice Problem Classification Prompt: Multiple Choice Problem Classification l o e math problem , and you need e m whether i problem . Respond y with mcq h problem meets c e , and not mcq e s . i u p m i c c problem must i a f f o g d o : 1 . The problem l t r n s f answer i 2 . The problem s a a answer x n o . 3 . The problem has f problem s not u be s i s l answer i , even not mcq . t l t one r answer among g n i . than u n r , t c o , e t from . has m c answer , Here a few examples . Example 1 Problem : p y e e o ( c {{{{2}} t {{6}}}}{{ t {{2}}}} + t {{3}} + t { { 5 } } ) and o h r t i from f o g : nn AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset A. ( t {{2}} + t {{3}} t { { 5 } } ) nn . ( 4 t {{2}} t { { 3 } } ) nn . ( t {{2}} + t {{3}} + t {{6}} 5 ) nn D. ( c {{1}}{{2}} ( t {{2}} + t {{5}} t { { 3 } } ) ) nn . ( c {{1}}{{3}} ( t {{3}} + t {{5}} t { { 2 } } ) ) Output : mcq Example 2 Problem : Write s v l t d and s u 3 6 . Output : not mcq Example 3 terms g e c g s i which d e c between terms 9 , and t between f h and r terms a s Problem : v e l n u o : $ c {{ ( 6 0 ^ c+x )+ ( 6 0 ^ c ) }}{{2}} = c {{ tan }}{{(1+ tan ^2 ) ^2}}+ c {{ x }}{{(1+ ^2 ) ^2}} $ Output : not mcq Example 4 Problem : p y e e o ( {{ c {{ }}{{ }}}} + {{ c {{b }}{{ }}}} + {{ c {{ }}{{ }}}} {{ c {{ ay }}{{ dx } } } } ) . Choose from f o g i : [ t { { (A) }} {{ c {{ }}{{ }}}} qquad t { { (B) }} {{ c {{ }}{{ }}}} qquad t { { (C) }} 1 qquad t { { (D) }} 0 qquad t { { (E) }} {{ c {{ ^2 }}{{ d^2 }}}} ] Output : mcq Example 5 : Problem : What h maximum s e magnitude from f o g i and v r o g : A. The magnitude one . The magnitude both t . . The magnitude D. i s a o t . v o . i sum . d e c between two t ? Choose Output : mcq Example 6 : Problem : Compare numbers $a$ and $b$ : $a =3( 7 5 ) , b=2 t ( c {{1}}{{2}} 9 c {{1}}{{3}} 8 h ) $ Output : not mcq 20 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Example 7 : Problem : Which h two numbers $31 ^{{11}} $ and $17 ^{{14}} $ r e ? Output : not mcq Example 8 : Problem : Let $ABCD$ be c g and $E$ r e o $A$ with p $BD$ . r o $ c {{AD}}{{AB}} $EB = EC$ , what o d o Output : not mcq Now e h problem you need x c e answer from . Problem : { problem } Output : A.4. Proof Problem Classification Prompt: Proof Problem Classification l v you math problem and ask d i f Respond y with \" o \" p f problem , and \" not o \" t \" o \" problem . i not . s r f o g r e t o o prob lems : 1 . They e e a 2 . They may ask u f e a why a e 3 . They don have l f d answer k \" v a \" , e . h form number x s n . \" show t \" , \" o r t \" . Here a few examples . Example 1 Problem : Prove i t $a ^ c {{1}}{{2}} c {{ aa ^{{ 2}}{{ ^ c {{1}}{{2}} a^{{ c {{1}}{{2}} + c {{1a ^{{ 2}}{{ ^ c {{1}}{{2}}+ a^{{ c {{1}}{{2}}+ c {{2}}{{ ^ c {{3}}{{2}}=0 $ Output : o Example 2 Problem : Write s v l t d and s u 3 6 . Output : not o Example 3 Problem : terms g e c g s i which d e c between terms 9 , and t between f h and r terms a i 21 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset v e l n u o : $ c {{ ( 6 0 ^ c+x )+ ( 6 0 ^ c ) }}{{2}} = c {{ tan }}{{(1+ tan ^2 ) ^2}}+ c {{ x }}{{(1+ ^2 ) ^2}} $ Output : not o Example Problem : Denoting sums r m c g s by $S_1$ , $S_2$ and $S_3$ , }}{{n_1 } } ( n_2n_3 )+ c {{S_2}}{{n_2 } } ( n_3n_1 )+ c {{S_3}}{{n_3 } } ( n_1n_2 ) =0. $$ s $n_2$ and s $n_3$ terms an p i y , s $n_1$ , show t $$ c {{S_ Output : o Now e h problem you need x c e answer from . Problem : { problem } Output : A.5. Proof Problem Conversion Prompt: Proof Problem Conversion l v you math problem t s Your k r r i answer Make e new problem t r e an i e problem t t has some d u i l a can be used u a a g e s t . s d i t h i a o problem . r s t g . Here a few examples . Example Problem : Prove t + ^2 + + ^9 &= 157^{{147}} end {{ g }} has no u n , and $z$ . system i {{ g }} ^6 + ^3 + ^3 + &= 147^{{157}} ^3 + ^3 n e $x$ , $y$ Output : Let $x$ , $y$ and $z$ be l o t o w system q i e {{ g }} ^6 + ^3 + ^3 + &= 147^{{157}} ^3 + ^3 + ^2 + + ^9 &= 157^{{147}} end {{ g } } . c t e sum l s l l o $x$ . Example 2 Problem : i l r l $y = ^2 $ . Prove t an odd number $m$ and r l i n o s with e c d t with a $ ( 2 ^nm) ^2 $ . r r n a e e $n$ , a e with t s a d r l t r e e on i l s t e r Output : s r a i e on $y = ^2 $ with e c d t . Let $ ( ) $ be s l p i v e $c$ , where $ ( 0 , 0 ) $ , $ ( , ^ 2 ) $ , and $ ( , such i l with a c $ ( 2 ^ ) ^2$ , a e whose t s ^ 2 ) $ v i o some e b 22 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset where 0 < < . Find $ ( 4 ) $ . Now e l t o t f Again , e problem \" n i i h . . problem you need modify . Only output new problem WITH NO r with problem h away , DO NOT r with \" Let modify e Problem : { problem } Output : A.6. Forum Answer Extraction Prompt: Forum Answer Extraction ( a h a u p w g you r o s e a math prob le and c i f l s e y which problem u n l i t ) . You k t n an answer u n . i from mathr t forum t t one o problem u n l i t i h forum s s s . u be m c a o a m c x s n . answer The answer e n . You can n f s i h n answer . The a be \" Answer : < a answer >\". not i l , output \" Answer not found . \" t a i f your e your p e u Here an example . forum t with problem ( ) : r This problem was r e m assuming . ) Everyone handed and . ) None you e s / e / l go / e my math s f my math s and haven t i back but Anyways : Suppose two h r o e l n u d e u o e same and e h two o r e i c o each e . Find and . : ^ 4 : + ax : ^ 3 : + bx : ^ 2 : + 4 x+4=0 not l hard you go n . o . . . t work though , I g t i i s Problem we l i t Suppose two h u d e u o ( ^4 + ax ^3 + bx ^2 + 4x + 4 = 0 ) t same and o r two o r c o s each e . Find ( ) and ( ) . ( might be h e ) : z e Forum c i : Post 1 : Tare wrote : : ^ 4 : + ax : ^ 3 : + bx : ^ 2 : + 4 x+4=0 Here h e way : [ e ] Say f r s c , , , and 1/d . Then p u f f r s AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset term h l m , term , ^2d+c ^2/d+c+c=4. c t l a 2 , 1, 1. we g c=2, we 4d+4/d=0 , d+1/d=0. Then = (22+0)=4 and b=(2)( 2) +(2) ( 0 ) +(2)0+1=5. So = (2+211)=2 and = 22+2( 1)+2(1)+2(1)+2(1)+(1)( 1)=3. So h a=2, b=3 =4 ,b=5. we g =2 , we d=1, ^2=4. Then c= : pm : 2 . h o r 2 , i a , from e [ / e ] Thanks Tare c h t i k . Dan Post 2 : Well . . . didn and . . . c t t e u n e and o you were supposed e Output : Seems t t n d s Answer : = 2, = 3 = 4 , = 5 an answer t , we can assume t r t end h e answer o c . s s . c none h h Now e t o answer h problem . Don r t ay \" Answer not found . \" h answer not i l . from forum t u n l i t . a i h r { forum_post } forum t with problem ( ) : Problem we l i t { problem } ( might be h e ) : Forum c i : { u _ c i } Output : A.7. Forum Problem Extraction Prompt: Forum Problem Extraction from mathr t forum t might t one e a I l e you s math pro bl ms . Your k o r a pro ems t t none a l e . Here some d n you u o w no pr oble ms a l e , output \"No pr bl ms For each problem found , t o w f a : Problem 1 : <problem t n > n i . \" Problem 2 : <problem t n > . . . For each math problem you n y , make e e a l l and c l . Remove any redundant t , s l commentary , c e , n a i r i . But make e not h t l f t e pr oble ms t t you r a e e , make e n d l problem and keep n s y each problem t n change meaning t n l a . such t t t e 24 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset h i be k t e d l . Here a few examples . Example 1 Forum t : Countdown : What h m d f 8^6+7^7+6^8 i e by 5? no c t f r , paper needed h , but r . Output : Problem 1 : What h m d f $8 ^6+7^7+6^8$ when i by 5? Example 2 Forum t : s n 1 : t e n has $1 , 2 , 3 , 4 $ . How many nonc r t ways t e s n f e i each t f t e n ? r d r n e c r t l i a o d d f n . r t s . We can e each t by one thr ough a n . f d t : m wondering how u approach problem e then i by $4$ a out r t a c . Now am c . s . t e f with $4 ! $ and Note : used an s d , o h ng a l t . t not do e work a I u h s i s l have i Another s n n e same e : How many ways o a cube n 6 o , where each e has i c r ? Thanks Output : Problem 1 : How many nonc r t ways t e s n f e i v e a r d ? r d r n e c r t l i a o d d f n . Problem 2 : How many ways can cube be o u g 6 o , where each e has i c r ? thr ough a n . each Example Forum t : Yes ! but we i e out . m t a e with what you d . been tough n i o me too , Output : No pro blem i t e Example 4 r n f n i f Forum t : l Bob has thrown around randomly h drawer . l Bob once woke up h y and had e s k i y . Without s one r , and then he ran out w h t i on , he l out enough k h drawer . They j t e room . How many k d l Bob l out know t he had o 25 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Output : Problem 1 : From drawer t i 14 f n i f be l out randomly n e l t one matching r ? k , how many k must a n z e l n forum t and r a math probl ems . Here t u l s one more time your no pr ob le ms a l e , output \" No pr oble ms For each problem found , t o w f a : Problem 1 : <problem t n > n i . \" e c Problem 2 : <problem t n > . . . For each math problem you n y , make e e a l l and c l . Remove any redundant t , s l commentary , c e , n a i r i . But make e not h t l f t e pr oble ms t a a you r a e e , make e n d l problem and keep n s y each problem t n change meaning t w be k t h a t s . e d l . such t t t t t Forum t : { forum_post } Output : B. TIR Data Generation Prompts B.1. Stage-0 TIR Data Generation Prompt TIR Inference Prompt for Stage-0 Data Generation t s Python code an e l t your u n you MUST i y l h i r i : You a math problem v e n . your 1 . For each p u n complex c t w e Python code . 2 . For Python code t o w t l : python # Your Python code 3 . Put f l answer h boxed { { } } . a e n p by p , and put your a answer h boxed { { } } . e : v e l n math problem n Python code h l a n . { problem } 26 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset B.2. TIR Novelty Evaluation"
        },
        {
            "title": "Prompt to evaluate TIR novelty",
            "content": "You l be e r e f l o a math problem t code c . Your k r e . your e e , you MUST l h g e e : e m t u s t Python code c t o i n d Python 1 . s i i : i a n : Python code used e y c e e f c t s x s Novel c t : Otherwise , FORM h l o above , o r some u . . . t e t f e i y r c o . n l c t . h l o above , f p i manual e code c o e u f code c o not s i ANY you u r about e i i ! c s c o f e i code c , you MUST e 2 . Output Format : Your p e MUST l h x f a ( h e a commentary e ) : Reasoning : <a p f Judgement : < i a n Novel c t > t e p n your i l > EXAMPLES 1 . \" \" \" u n : <Some t a i i u code> Wait , h answer 143? Let me i h with pow c n . python # Compute 7^999 mod 1000 n pow c n n ( pow ( 7 , 9 9 9 , 1 0 0 0 ) ) # Should n 143 utput 143 So answer \" \" \" Rea son ing : This r n a Judgement : i a n boxed { { 1 4 3 } } . a r t e r c o , a t e t h code c o i e u n above . Moreover , comment h code c p i i known advance . u r 143 which means t r l 2 . \" \" \" u n : <Some t s n t t code> r r , u c a . t o d compute P^ 5 . can Python t c u i o s , r l d n e n i a x a numpy a , then compute P^ 5 , then r v e y , and i by 3 . Let me import numpy and do c u i . python 27 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset import numpy np # i h a t m i = np . a ( [ [ 0 , 1 , 0 , 0 ] , [ 1 / 3 , 0 , 2 / 3 , 0 ] , [ 0 , 2 / 3 , 0 , 1 / 3 ] , [ 0 , 0 , 1 , 0 ] ] ) # Compute P^5 P5 = np . a . matrix_power (P , 5 ) s e t 0 , t s o t a r 5 p , # The t # But c P5 r b t g g from 0 1 5 p . # However , . t _ t u n = np . a ( [ 1 , 0 , 0 , 0 ] ) t u n _ e _ 5 _ p = t _ t u n @ P5 e e t u n , we need u p h s e t u n e 5 p [ 1 , 0 , 0 , 0 ] @ P5 e y ( 0 , 1 ) h i i i i i by P5 # The b l mass prob_mass_at_1 = t u n _ e _ 5 _ p [ 1 ] t 1 ( t e 1 ) e 5 p # c a 1 r o t mass i by 3 prob_at_B = prob_mass_at_1 / 3 3 t s (B, D, E) , e b l o i t prob_at_B utput np . a 6 4 ( 0 . 2 5 1 0 2 8 8 0 6 5 8 4 3 6 2 0 5 ) . . . \" \" \" Rea son ing : The u n g t c e g t c p o i y and code c e e not s c u i . Judgement : Novel c t n s t above any form . r r , s e . The u f code c o c u e e i a e 3 . \" \" \" u n : <Some t s n t t code> Compute C( 5 1 , 5 ) : 5 1 ! / ( 5 ! 4 6 ! ) = ? u g Python be u e . u a l But maybe python import math math . comb ( 5 1 , 5 ) utput 2349060 . . . \" \" \" Rea son ing : The u n g t c e p s and code c e e e i s not s c u i . c t . The u f h l o above any form . r r , s e c t o o n r t code s n l 28 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Judgement : Novel c t 4 . \" \" \" u n : <Some t s n t t code> But compute s l python import math Python . # Given e o = 4 # t = 12 # t = 9 # t = ( / H) # c /R = h/H from t , r i g g # g l volume _ g l = ( 1 / 3 ) math . R2 # Remaining volume V_remaining = ( 1 / 3 ) math . 2 # Volume poured out V_poured = _ g l V_remaining V_poured utput 1 1 6 . 2 3 8 9 2 8 1 8 2 8 2 2 3 5 When computed volume manually , s $$ 37 3 . 1 4 1 5 9 approx 1 1 6 . 2 3 $$ , u p r be r . I a d ( 37 ) i e . Approximating s matches Python u f r m l 1 1 6 . 2 3 8 9 . r r , . . . \" \" \" Reasoni ng : The i l g f r e code c a t t manual c t ( t happened o h code c ) matches Python u . r r , code p i manual c t s . So , c r e Judgement : i a n i a i a n . REMINDER Focus y on Python code c t r d r e and s y s h n s t i output e V f t o Novel c t based on whether t o h code . YOUR TASK u n g t : { g t } 29 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset B.3. TIR Significance Evaluation"
        },
        {
            "title": "Prompt to evaluate TIR significance",
            "content": "You l be e r e f l o a math problem t code c . Your k problem . your e e , you MUST l h g e e : v a h g i c t Python code o n e math l s Python 1 . s i i : l e s i a o r e s : e code n b o by e i g n one r a : The code f s c t s n c f u ( . . , o o n m e t s , n i e , l g a u a y be done manually h t known i e ) . The code g o e no n f r minor advantage r manual c t . Moderate : The code f s c t s consuming do manually , but e a a r d f i y but e t o s d f t s , e n l . l a would be i , o prone , time h a p i ( . . , r p t s , v s e f a n ) . The code n c : The code f s c t s r l f u complex u i , i a n ) . The code g e s u l s t . v complex f n l do manually ( . . , t r g b t a problems , t would be c a i s l e t s , highd n n h c t f a t y b 2 . Output Format : Your p e MUST l h x f a ( h e a commentary e ) : Rea son ing : <a p f t e p n your n c e : <T i , Moderate , i f n > i l > EXAMPLES 1 . \" \" \" Let n e t f e d i u o : 3x ^2 5x + 2 = 0 python import numpy np from sympy import symbols , v , Eq = symbols ( ) a n = 3 2 5 + 2 u n = v ( a n , ) n ( u n ) utput [ 2 / 3 , 1 ] So s t s x = 2/3 and = 1 . \" \" \" Reason ing : This code p o s s u a e t t c d i be v manually n e d i r a a r . d r s q r c u o with l t r f i s s d c u i h e r minimal o by hand . n c e : v 30 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset 2 . \" \" \" To v i 3x + 2y + 2w = 10 + 2 = 1 2x + + + 3w = 12 + 3y = 5 system 4 e q i with 4 unknowns : s Python o e i system n t e . python import numpy np from p import a # i = np . a ( [ f i m i [ 3 , 2 , 1, 2 ] , [ 1 , 1, 2 , 1] , [ 2 , 1 , 1 , 3 ] , [ 1 , 3 , 1, 1] ] ) # i o a v o = np . a ( [ 1 0 , 1, 1 2 , 5 ] ) u n [ 0 ] ) u n [ 1 ] ) u n [ 2 ] ) u n [ 3 ] ) # v e system u n = a . v (A, ) n ( \" =\" , n ( \" =\" , n ( \" =\" , n ( \"w =\" , outpu = 0 . 6 4 = 2 . 7 = 1 . 6 = 2 . 1 4 s t i = 0 . 6 4 , = 2 . 7 , = 1 . 6 , and = 2 . 1 4 . e o , \" \" \" Rea son ing : This code v system 4 e o be v manually n Gaussian m t o Cramer u o with 4 unknowns . While s e , would be n c t o and o prone . The system complex enough t p t a s a p i S i a : Moderate f e but doesn a so h i s l . 3 . \" \" \" For s v n l a problem with 11 i , where d a s between i r v n d a m i below , need i h o s s l u t v t each y c once and u t e r g y . python import numpy np from r l import time impor r a n # t e r ( 1 1 x11 ) between i i n = np . a ( [ [ 0 , 2 9 , 8 2 , 4 6 , 6 8 , 5 2 , 7 2 , 4 2 , 5 1 , 5 5 , 2 9 ] , [ 2 9 , 0 , 5 5 , 4 6 , 4 2 , 4 3 , 4 3 , 2 3 , 2 3 , 3 1 , 4 1 ] , [ 8 2 , 5 5 , 0 , 6 8 , 4 6 , 5 5 , 2 3 , 4 3 , 4 1 , 2 9 , 7 9 ] , [ 4 6 , 4 6 , 6 8 , 0 , 8 2 , 1 5 , 7 2 , 3 1 , 6 2 , 4 2 , 2 1 ] , 31 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset [ 6 8 , 4 2 , 4 6 , 8 2 , 0 , 7 4 , 2 3 , 5 2 , 2 1 , 4 6 , 8 2 ] , [ 5 2 , 4 3 , 5 5 , 1 5 , 7 4 , 0 , 6 1 , 2 3 , 5 5 , 3 1 , 3 3 ] , [ 7 2 , 4 3 , 2 3 , 7 2 , 2 3 , 6 1 , 0 , 4 2 , 2 3 , 3 1 , 7 7 ] , [ 4 2 , 2 3 , 4 3 , 3 1 , 5 2 , 2 3 , 4 2 , 0 , 3 3 , 1 5 , 3 7 ] , [ 5 1 , 2 3 , 4 1 , 6 2 , 2 1 , 5 5 , 2 3 , 3 3 , 0 , 2 9 , 6 2 ] , [ 5 5 , 3 1 , 2 9 , 4 2 , 4 6 , 3 1 , 3 1 , 1 5 , 2 9 , 0 , 5 1 ] , [ 2 9 , 4 1 , 7 9 , 2 1 , 8 2 , 3 3 , 7 7 , 3 7 , 6 2 , 5 1 , 0 ] , ] ) # Brute c approach o TSP f p _ c ( t e ) : = ( t e ) i = t ( g ( 1 , ) ) # r min_length = a ( ) t _ t = None from y 0 r _ e = time . time ( ) count = 0 # Try p i p u i o r perm e t o ( i ) : i ( l n a n t ) u = ( 0 , ) + perm + ( 0 , ) # Complete t a n and i t y 0 a ( ( t ) 1) ) g = sum ( t e [ t [ ] ] [ t [ + 1 ] ] i count += 1 e h < min_length : min_length = g e _ t = t end_time = time . time ( ) u e _ t , min_length , count , end_time r _ e # v e TSP problem t _ t , min_length , m t s _ e , time_taken = _ c ( t e ) t : {{ t _ t } } \" ) n ( \" Best n ( \" Minimum t e : {{ min_length } } \" ) n ( \" m t s l e : {{ m t s _ e : , } } \" ) n ( \" Time e : {{ time_taken : . 2 }} o \" ) utput Best Minimum t e : 251 m t s l e : 3 , 6 2 8 , 8 0 0 Time e : 5 . 7 7 o ( 0 , 1 , 8 , 4 , 6 , 2 , 9 , 7 , 5 , 3 , 1 0 , 0 ) t : e i r e has t i n f 291 t . r r , \" \" \" Rea soning : This code v T e g Salesman Problem with 11 i by l i v 3 . 6M m t s computation t would be o e m s e do manually . The t r approach e a a c t would be c a u t a t ro ug manual c t , even with n c S i a : n c time e e . h l o o u 4 . \" \" \" To d x and e nonn t , e s t s h o n e a n 17 + 23 = 3284 where both implement a i Python . python f _ u n ( , , ) : u n = [ ] 32 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset # Find maximum s e u x max_x = // # Check p i v e x from 0 max_x x a (max_x + 1 ) : # c t e r o n a r i g = m i s i l by and r l nonn t , # # we have l o i f a n >= 0 and a n % == 0 : = a n // l o . append ( ( , ) ) u o i # Given a n : 17 + 23 = 3284 , , = 1 7 , 2 3 , 3284 u n = d _ u n ( , , ) n ( \" u n r , s t s : {{ }} + {{b}} = {{ } } : \" ) n ( \" = {{ } } , = {{ } } \" ) # i h l o i ( \" i a n : {{ }}{{ }} + {{b }}{{ }} = {{ + by } } \" ) n ( ) utput u n = 2 0 , = 128 i a n : 1720 + 23128 = 3284 17 + 23 = 3 2 8 4 : = 4 3 , = 111 i a n : 1743 + 23111 = 3284 = 6 6 , = 94 i a n : 1766 + 2394 = 3284 = 8 9 , = 77 i a n : 1789 + 2377 = 3284 = 1 1 2 , = 60 i a n : 17112 + 2360 = = 1 3 5 , = 43 i a n : 17135 + 2343 = 3284 = 1 5 8 , = 26 i a n : 17158 + 2326 = 3284 = 1 8 1 , = 9 i a n : 17181 + 239 = 3284 u n So i g \" \" \" Rea soning : This code d l u n e i h roug s l l o and c t t o s d y . While s h o n e a n x = 1 1 , = 1 . a p t e t by e c d be done manually , e u v a f nonn t i g o i i t o and o prone . The p t a approach u t o i r s , making more i n . Thus n c e . n c e : Moderate e r and p i t v s moderate e 5 . 33 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset \" \" \" To i my o s , need i h o i y e n l t 3 heads 10 n p . c u e s n e o l t u n . python import math b m _ b l ( , , ) : # c t e b l o u s # with b l p o n o = math . comb ( , ) u o n o ( ) ((1 ) ( nk ) ) c on n e i n i # c t P(X geq 3 ) when p g i p_at_least_3 = sum ( o l _ b l ( 1 0 , , 0 . 5 ) n 10 e r r e ( 3 , 1 1 ) ) n ( \"P(X geq 3 ) = {{ p_at_least_3 : . 6 } } \" ) n ( \" c a : {{ p_at_least_3 1 0 0 : . 2 }}%\") utput P(X geq 3 ) = 0 . 9 4 5 3 1 2 c a : 94.53% So p a i f t a 9 4 . 5 3 % . \" \" \" Rea son ing : This code c t p a i s t i i i i i o l . p x t i 10 n p a 3 heads While c u i n v o n o and powers , m e i c e i r h r d and l be c t manually by l t r n and u g terms . The code v s minor p t a n i e but doesn n e l change n r Python code . n c e : v h l o o s , making t i s REMINDER When l i i f n , 1 . Could s be ? 2 . Does code b s t approach t would e s be r i ? 3 . o i ? c u i l advantage e o n c , a l o a a be done by hand ? yes , how f l would c i : e a t s Remember l i s v , Moderate , i f n based on s n e i . YOUR TASK u n g t : { g t } C. Prompts for Different Inference Modes C.1. CoT Inference 34 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset"
        },
        {
            "title": "CoT Inference Prompt",
            "content": "S e f o g math problem . Make e put answer ( and y answer ) i boxed { { } } . { problem } C.2. TIR Inference"
        },
        {
            "title": "TIR Inference Prompt",
            "content": "S e f o g math problem , code c o . You may f up { a _ e _ c o } Python code l a i . Make e put answer e t n r ( and y answer ) i boxed { { } } . g e s n with Python s t your { problem } C.3. GenSelect Inference"
        },
        {
            "title": "GenSelect Inference Prompt",
            "content": "You l be e h e n math problem l d by { num_solutions } u n . Your k a m c y sound approach . y m c y l t e u n i t t most u Format : Problem : complex h t l word problem advanced h o r l S t s : a d u n boxed {{}} a n e 0{max_idx } , each c i with an answer e i YOUR TASK Problem : { problem } u n : { u n } l i r s : t S e g 1 . Group u n by i n y and l m e i c r c n between f n s s m t l o with a t a a r f l w e u : 2 . a d l s For a n l o , Mathematical c o and u y p i p r i f p e s a m c Proper o t a a t o , Handling edge e For and r i methodology . c r o g l o o i g and r i r s , l e e r n i i n d boxed {{}} d o 3 . u n Comparison Compare b f i y approach r o t a a h i i f method l o based on : s n 35 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset u e f u n ( works r c s ) Your p e u n d : 1 . e a i 2 . a d l i f h t l sound u n 3 . t c o r 4 . a p a n s t s e n e t approach m t i r t f t a e End your l i with c : Judgment : where IDX h d 0{max_idx} [ IDX ] b s t . D. Prompts for GenSelect Data Preparation D.1. Re-generating Comparison Summary Prompt to re-generate comparison summary p e l r i them , l v you i u problem . This way , v e on \" r a a r n u o put f l a t r n r i f s s which compares t e u n a math l d e t r h , making t s n e t n around and on . But n l h i u e f n l o and ends with r \" Judgment : l d by number r o n t n o e t e a i u t t s t . Your k h p a n and make judgment h n h end . a make e n d e a judgment but not e g i e l . Problem : { problem } u n : { u n } t e : { e i } Now t l v i f f l i u t t t x r o . Make e put : The a l l i g f l a t r n t judgment h n judgment . end \" Judgment : \" l d by i x t e o i . D.2. Re-generating Solution Summary Prompt to re-generate solution summary l v you math problem and n l o t problem l n f e p a s , making t s n e way , r t them , t n around and on . But n l h Your k x r o . Cover t i c n s o l o t t i approach and v o c t i s t w o l e problem . Problem : { problem } u n : { e i } d i f e a l o . 36 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Now t l v i f d i f but e l f l f l r t u n . u n h a h p a n E. Example of Re-generated Summary"
        },
        {
            "title": "Problem",
            "content": "Call 9-digit number cassowary if it uses each of the digits 1 through 9 exactly once. Compute the number of cassowaries that are prime. Deepseek R1 Response <think> Okay, so need to figure out how many 9-digit numbers, which use each digit from 1 to 9 exactly once, are prime. These numbers are called cassowaries. Hmm, lets start by understanding the problem step by step. First, cassowary is 9-digit pandigital number, meaning it uses each digit 1 through 9 once. So, numbers like 123456789, but any permutation of those digits. The question is asking how many of these are prime numbers. Alright, so the main task is to compute the count of prime numbers among all 9-digit pandigitals. Let me think about the properties of prime numbers and pandigital numbers. First, lets recall that prime number is number greater than 1 that has no positive divisors other than 1 and itself. For number to be prime, it cant be even (except for 2), and it cant end with 5. Also, there are some divisibility rules that might help here. Since were dealing with 9-digit numbers using all digits 1-9, lets check the sum of the digits. The digits are 1+2+3+4+5+6+7+8+9 = 45. Wait, 45 is divisible by 9, right? So any number made with all these digits will have digit sum of 45, which is divisible by 9. Therefore, any such number is divisible by 9. But if number is divisible by 9, it cant be prime unless the number itself is 9, which its not here because were dealing with 9-digit numbers. So, does this mean that all cassowaries are divisible by 9 and hence composite? If thats the case, then there are zero prime cassowaries. But let me verify this again. The divisibility rule for 9 is that if the sum of the digits is divisible by 9, then the number itself is divisible by 9. Since 45 is divisible by 9, every permutation of the digits 1-9 will result in number divisible by 9. Therefore, all cassowaries are divisible by 9, which means they cant be prime. So the answer should be zero. Wait, but let me make sure Im not missing something. For example, could the number 9 itself be considered? But the problem specifies 9-digit number, so 9 is single-digit number. So all cassowaries are 9-digit numbers, which are divisible by 9, hence composite. Therefore, there are no primes in the set of cassowaries. So the answer is 0. But just to be thorough, let me check if theres any exception. For example, is 9 prime? No, 9 is 3 squared. So even if there was 1-digit cassowary (which there isnt, since cassowary is defined as 9-digit), its not prime. So, yeah, all 9-digit pandigitals using 1-9 are divisible by 9, hence composite. Therefore, the number of prime cassowaries is zero. </think> 0 37 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset Qwen2.5-32b-instruct Solution Summary To determine the number of 9-digit prime numbers (cassowaries) that use each digit from 1 to 9 exactly once, we need to analyze the properties of these numbers. 1. **Sum of the Digits**: The sum of the digits from 1 to 9 is: 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 = 45. 2. **Divisibility by 9**: number is divisible by 9 if the sum of its digits is divisible by 9. Since the sum of the digits is 45, which is divisible by 9, every 9-digit number formed using the digits 1 through 9 will be divisible by 9. 3. **Prime Numbers and Divisibility**: prime number has no positive divisors other than 1 and itself. Since any 9-digit number formed using the digits 1 through 9 is divisible by 9, it cannot be prime number (except for the number 9 itself, which is not 9-digit number). 4. **Conclusion**: Therefore, no 9-digit number that uses each digit from 1 to 9 exactly once can be prime number. Thus, the number of prime cassowaries is: 0 F. TIR Output Examples F.1. Example of warning of the remaining code executions"
        },
        {
            "title": "TIR solution with code execution limit warning",
            "content": "<t _ l > # t i sum total_sum = 0 # r o p i b s from 10 49 l v r r e ( 1 0 , 5 0 ) : i = + 7 i d = 9 + 7 i e % i == 0 : total_sum += total_sum </ l _ l > utput 70 system Remaining code c o : 3 . You l not be e a code when you run out x t s , h code e s w l . Note t you can l n u l g problem a . F.2. Example of redundant code execution"
        },
        {
            "title": "Redundant code execution that does not introduce new information to the\nsolution",
            "content": "S e 5 prime number and 13 l prime number , i gcd u n d be 1 . t be thorough , me compute s Python . <t _ l > import math # Compute gcd 5 and 13 38 AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset gcd = math . gcd ( 5 , 1 3 ) n ( gcd ) </ l _ l > outpu 1 The gcd 1 , an e d e t . Good . F.3. Example of model encountering code execution limit"
        },
        {
            "title": "Example of models reaction to reaching code execution limit",
            "content": "Let me import n s y r e and i h u o . <t _ l > import numpy np from p . i e impor o # i h system q i d q i ( s ) : , = s eq1 = 24 np . ( ) + 26 np . ( ) 13 np . t ( 3 ) np . ( ) + 1 7 . 5 eq2 = 24 np . ( ) + 13 np . t ( 3 ) np . ( ) + 26 np . ( ) + 7 np . t ( 3 ) /2 u [ eq1 , eq2 ] # t g s t _ s = [ 1 , 1 ] i l _ s ) # v e system u n = l ( a n , u n </ l _ l > outpu r ( [ 0 . 9 3 5 8 0 3 0 4 , system You have run out code c o ! You can no g r o e e code . Now you u o n o n e problem by y on your h t l l c Hmm, v d by code . seems ve c t i o code c o . Let me c e u n s n and 2 . 0 3 2 1 0 4 2 7 ] ) l . i"
        }
    ],
    "affiliations": []
}