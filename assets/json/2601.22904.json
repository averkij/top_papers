{
    "paper_title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
    "authors": [
        "Hun Chang",
        "Byunghee Cha",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs."
        },
        {
            "title": "Start",
            "content": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation Hun Chang * 1 Byunghee Cha * 1 Jong Chul Ye 1 6 2 0 2 0 3 ] . [ 1 4 0 9 2 2 . 1 0 6 2 : r Figure 1. Generated images from DiTDH model trained on DINO-SAE latents"
        },
        {
            "title": "Abstract",
            "content": "Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can *Equal contribution 1Graduate School of AI, KAIST. Correspondence to: Hun Chang, Byunghee Cha, Jong Chul Ye <{hun.mark.chang, paulcha1025, jong.ye}@kaist.ac.kr>. Preprint. February 2, 2026. 1 hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSLbased foundation model representations intrinsically lie on hypersphere, we employ Riemannian Flow Matching to train Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving gFID of 3.47 at 80 epochs. DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation 1. Introduction Diffusion models (Ho et al., 2020; Song et al., 2021; Peebles & Xie, 2023) have revolutionized image generation, with performance largely contingent on the quality of the latent space provided by the underlying autoencoder. Recently, leveraging pretrained Vision Foundation Models (VFMs) like DINOv2 (Oquab et al., 2023) as encodersexemplified by methods like RAE (Zheng et al., 2025b)has shown promise in capturing rich semantic information. However, critical trade-off remains: while these VFM-based approaches excel in semantic generation, they suffer from significant degradation in pixel-level reconstruction fidelity (e.g., low PSNR) compared to standard VAEs. We attribute this limitation to two primary factors: the aggressive downsampling in standard Vision Transformer (ViT) patch embeddings and the rigidity of feature alignment objectives. Existing methods typically employ Mean Squared Error (MSE) to align the student encoder with the VFM teacher. However, strictly enforcing both magnitude and directional alignment creates gradient conflict between semantic preservation and pixel reconstruction. This overconstrained optimization landscape prevents the encoder from learning the subtle high-frequency features necessary for fidelity. To address these challenges, we introduce the DINO Spherical Autoencoder (DINO-SAE), framework designed to reconcile semantic abstraction with high-fidelity reconstruction. Our approach incorporates Hierarchical Convolutional Patch Embedding to preserve local texture information and adopts Directional Feature Alignment strategy using cosine similarity. Unlike MSE, cosine similarity relaxes the magnitude constraint, smoothing the optimization landscape. This flexibility allows the encoder to prioritize semantic alignment via feature direction while utilizing the magnitude degree of freedom to minimize reconstruction error, effectively resolving the conflict between the two objectives. Furthermore, we extend this geometric perspective to the generative stage. We observe that representations from Contrastive Learning (e.g., DINO, CLIP) naturally converge to hyperspherical manifold rather than Euclidean space. Consequently, we argue that aligning the generative dynamics with the underlying hyperspherical manifold via Riemannian Flow Matching (RFM) (Chen & Lipman, 2023) provides more natural and efficient formulation. By constraining the generative process to the hypersphere and modeling geodesics, we eliminate redundant radial variations and focus solely on semantically meaningful directional dynamics. Our contributions can be summarized as follows: We propose Hierarchical Convolutional Stem that mitigates the information bottleneck of standard ViT patchfication, significantly enhancing local detail preservation. We introduce Directional Feature Alignment, which alleviates the optimization conflict inherent in MSEbased distillation. This enables the encoder to simultaneously achieve high semantic alignment and state-ofthe-art reconstruction fidelity (26.2 dB PSNR). We demonstrate that modeling the latent space as spherical manifold via Riemannian Flow Matching is geometrically well-suited for VFM-based latents. This formulation aligns with the intrinsic properties of SSL features and accelerates training convergence compared to Euclidean baselines. 2. Related Work Representation Alignment. Recent work has shown that leveraging semantic representations can significantly improve the training efficiency of diffusion-based generative models. Following the introduction of REPA (Yu et al., 2025), which aligns intermediate features of DiT with pretrained DINOv2 representations to achieve faster convergence, several extensions have been proposed. REG (Wu et al., 2025) further enhances this paradigm by introducing class token whose DINOv2 features are aligned with DiT during generation, leading to even faster convergence. Beyond external representation models, number of selfcontained approaches have demonstrated similar efficiency gains: SRA (Jiang et al., 2026) aligns intermediate features at higher noise levels with later-layer features at lower noise levels, while LSEP (Yun et al., 2025) shows that merely enforcing linear separability of intermediate features via classification probe can improve convergence. Dispersive Loss (Wang & He, 2025) similarly regularizes intermediate representations by encouraging feature dispersion, highlighting that improved internal representation geometry alone can accelerate diffusion training. REPA-E (Leng et al., 2025) extends representation alignment to the tokenizer itself by jointly optimizing VAE parameters under the REPA objective, further improving convergence speed. Semantic Latent Autoencoders. In parallel, substantial effort has been devoted to improving latent autoencoders by aligning their latent spaces with semantic representations. Methods such as VA-VAE (Yao et al., 2025) and MAETok (Chen et al., 2025b) explicitly align VAE latents with pretrained vision foundation models, achieving competitive or improved rFID while dramatically accelerating gFID convergence. RAE (Zheng et al., 2025b) takes more direct approach by replacing the VAE encoder entirely with pretrained vision foundation model, eliminating the need for explicit alignment losses and fully exploiting pretrained 2 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation representations. However, despite competitive generative performance, RAE and similar VFM-encoder-based tokenizers exhibit significantly degraded pixel-level reconstruction quality, as reflected by low PSNR. This limitation stems from the fact that pretrained vision foundation models prioritize semantic abstraction and invariance, failing to preserve fine-grained details necessary for exact reconstruction. Our work builds on this observation by identifying the precise architectural bottleneck responsible for this information loss and introducing minimal yet effective modification that substantially improves reconstruction fidelity while retaining the semantic advantages of VFM-based encoders. Riemannian Flow Matching. Recently, Chen & Lipman (2023) extended the idea of flow matching (FM) (Lipman et al., 2022) beyond Euclidean spaces and proposed Riemannian Flow Matching (RFM), enabling CNF training on general Riemannian manifolds M. In this setting, the model learns time-dependent tangent vector field vθ(x, t) TxM that transports samples from base distribution to the target data distribution q. Specifically, the RFM objective is defined as the regression loss Figure 2. DINO-SAE architecture and training loss. which can be expressed analytically using the exponential and logarithmic maps: ψt(x0, x1) = expx1(κ(t) logx1 (x0)). (5) This construction makes RFM simulation-free on simple manifolds, while remaining scalable to general geometries. LRFM(θ) = Et, xt (cid:104) vθ(xt, t) u(xt, t)2 (cid:105) , (1) 3. DINO-SAE where denotes the norm induced by the Riemannian metric, and u(xt, t) is the marginal target flow generating probability path between and q. Following the conditional flow matching, the target vector field can be expressed as the marginalization of conditional flow ut(x x1): u(xt, t) = (cid:90) ut(xt x1) pt(xt x1) q(x1) pt(xt) dvolx1. (2) key distinction from standard FM lies in the design of the conditional flow on manifolds. RFM introduces the notion of premetric d(x, x1) and constructs conditional trajectory ψt(x0 x1) that monotonically decreases this distance: d(ψt(x0 x1), x1) = κ(t) d(x0, x1), (3) where κ(t) is decreasing scheduler satisfying κ(0) = 1 and κ(1) = 0. Under this formulation, the conditional vector field admits the closed-form expression ut(x x1) = log κ(t) dt d(x, x1) d(x, x1) d(x, x1)2 . (4) In the special case where the premetric is chosen as the geodesic distance dg, the resulting conditional flow reduces to the constant-speed geodesic connecting the endpoints, Our DINO Spherical Autoencoder (DINO-SAE) is designed to equip pre-trained vision foundation models with highfidelity reconstruction capabilities. Our approach addresses the information bottleneck in standard Vision Transformers (ViTs) and introduces scale-decoupled alignment strategy to preserve high-frequency details without compromising semantic integrity. The overall architecture and its training loss is illustrated in Fig. 2, and more details follows. 3.1. Architecture Enhanced Patch Embedding. Standard ViT architectures, including DINOv3 (Simeoni et al., 2025), typically implement patch embedding using single convolutional layer. We hypothesize that this aggressive, non-overlapping downsampling operation acts as primary bottleneck, causing the irreversible loss of high-frequency spatial details essential for reconstruction tasks. To mitigate this, we replace the standard single-layer embedding with Hierarchical Convolutional Stem. Specifically, we design four-stage Convolutional Neural Network (CNN) that progressively downsamples the input image. This enhanced patch embedding layer allows the model to capture fine-grained local features (e.g., edges, textures) in the early stages, which are typically discarded by the largekernel convolution of standard ViTs. The output of this module matches the dimension of the Transformer blocks, serving as rich input token sequence z0. For the subse3 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation quent encoding stages, we utilize the pre-trained DINOv3 Transformer backbone, keeping all parameters frozen to preserve the original semantic representations. Decoder Architecture. For the decoding stage, we adopt the lightweight yet effective decoder architecture proposed in DC-AE (Chen et al., 2024). This decoder is designed to efficiently upsample the latent tokens back to the pixel space ˆx while minimizing computational overhead. 3.2. Training Training Loss. Replacing the patch embedding layer inevitably shifts the feature space relative to the pre-trained teacher. standard approach to realign the student encoder is to minimize the Mean Squared Error (MSE) between their feature representations. However, recent studies on foundation model alignment (Chen et al., 2025a; Tang et al., 2025) reveal that strictly enforcing MSE leads to degradation in reconstruction quality (higher rFID), despite improving semantic linear probing accuracy. We attribute this phenomenon to an over-constrained optimization landscape. The MSE objective enforces strict constraints on both the direction (semantic content) and the magnitude (signal strength) of feature vectors. Since the pre-trained teacher (DINOv3) is trained with contrastive objectives that encourage invariance to local textures, forcing the student to match the teachers exact feature magnitude restricts the students capacity to minimize pixel-level reconstruction error. This creates gradient conflict where the encoder struggles to satisfy both the rigid alignment constraint and the reconstruction objective simultaneously. To resolve this conflict, we propose directional feature alignment using the cosine similarity loss. Unlike MSE, this objective optimizes solely for the alignment of feature directions: Lalign = 1 zS zT zS2zT 2 (6) where zS and zT denote the student and teacher features, respectively. By adopting the cosine similarity, we effectively relax the optimization constraints. This relaxation decouples the semantic alignment from the vector magnitude, providing the student encoder with the necessary degrees of freedom to optimize its parameters for high-fidelity reconstruction. Crucially, this does not imply that semantic information is sacrificed; rather, it allows the model to converge to solution that preserves semantic direction while finding strictly better local minimum for reconstruction, avoiding the trade-off observed in prior works. This observation also motivates our use of spherical manifolds for generation in Section 3.4, as the essential information is primarily encoded in the angular component. Figure 3. Linear Probing result of DINOv3 and DINO-SAE encoder. We compare the linear probing of feature from DINOv3 and our DINO-SAE encoder. With our cosine similarity loss, DINO-SAE achieves high linear probing results, deteriorating only within 3% on Top-1 accuracy compared to DINOv3s linear probing results Progressive Training Strategy. We train our model in four progressive stages to balance semantic alignment and reconstruction fidelity. Stage 1: Semantic-Structural Alignment. In the initial phase, we train the CNN patch embedding and the decoder to establish stable latent space. We employ combination of the directional alignment loss, pixel-wise reconstruction loss, and perceptual loss: LStage1 = λcosLalign +λL1x ˆx1 +λlpipsLLPIPS(x, ˆx) (7) where LLPIPS denotes the LPIPS loss for perceptual quality. Stage 2: Adversarial Adaptation. To further enhance textural realism, we introduce adversarial training in the second stage. Following StyleGAN-T (Sauer et al., 2023) and RAE (Zheng et al., 2025a), we utilize DINO-Discriminator (DINO-Disc) which performs discrimination in the feature space of pre-trained DINO model. This allows the model to generate logically consistent textures. The objective becomes: LStage2 = LStage1 + λadvLGAN(x, ˆx) (8) where LGAN is the hinge adversarial loss. DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation τ = 0.8. This stochastic perturbation compels the decoder to reconstruct high-fidelity images even from slightly noisy latent signals, mimicking the generative imperfections of diffusion models. For optimization, we employ the same compound objective function as in Stage 3, utilizing the identical configuration of reconstruction loss, perceptual loss, and the adaptive adversarial objective Eq. (9). 3.3. Analysis of Semantic Preservation Figure 3 presents the quantitative evaluation of semantic preservation via linear probing on ImageNet-1K. Despite the architectural modifications to the patch embedding layer and the subsequent training phase, DINO-SAE retains robust classification performance. Specifically, compared to the original DINOv3 which achieves 89% Top-1 and 98% Top5 accuracy, our DINO-SAE maintains competitive 87% Top-1 and 97% Top-5 accuracy. This marginal degradation indicates that our model successfully preserves the highlevel semantic priors of the teacher model. To empirically validate that DINO-SAE effectively preserves the semantic structure of the teacher model, we visualize the latent feature spaces of both the Teacher (DINOv3) and the Student (DINO-SAE) encoder. We apply Principal Component Analysis (PCA) to the output feature maps RHW and visualize the top-3 principal components as RGB channels, following the methodology of DINO (Caron et al., 2021). As illustrated in Figure 4, the teacher (DINOv3) exhibits clear semantic segmentation, where object parts are distinctly clustered by color, demonstrating its robust semantic understanding. Crucially, while the student (DINO-SAE) trained with our cosine similarity objective exhibits slight noise compared to the teacher, the resulting PCA maps are nearly indistinguishable. The student faithfully reproduces the semantic layout, accurately capturing fine-grained details, such as the distinct separation among the dogs face, nose, paws, and the background. This result qualitatively proves that our directional alignment strategy successfully transfers the internal semantic geometry of the foundation model, achieving high-fidelity semantic preservation comparable to the teacher. 3.4. Image Generation via DINO-SAE Through empirical analysis, we observe that the reconstruction output of our autoencoder is largely invariant to the magnitude of latent features, while being highly sensitive to their direction. These observations suggest that semantic information in the latent space is primarily encoded in the direction rather than the norm. Motivated by this property, we propose to perform Riemannian Flow Matching (Chen & Lipman, 2023) Figure 4. PCA visualization of feature representations. We compare the principal components of feature maps from the Original Image, the Frozen Teacher (DINOv3), and the Student trained with our Cosine Similarity. Notably, our results demonstrate that the Cosine Similarity loss effectively preserves global semantic information, even as the patch embeddings are updated during training. This validates that directional alignment faithfully maintains the teachers semantic structure and object boundaries. Stage 3: Decoder Refinement. To maximize reconstruction fidelity and capture fine-grained details, we freeze the entire encoder (including the CNN patch embedding) to fix the semantic latent space. We then fine-tune only the decoder. This forces the decoder to extract maximal detail from the fixed semantic-detail decoupled latent space, effectively resolving the trade-off between alignment and reconstruction. LStage3 = λL1x ˆx1 + λlpipsLLPIPS + λadvLGAN (9) Stage 4: Noise Augmentation. In the final training stage, we adopt the latent noise augmentation strategy suggested by RAE (Zheng et al., 2025a) to enhance decoder robustness. While keeping the encoder frozen, we inject stochastic noise into the latent representations. Specifically, given the latent features z, the perturbed latent ˆz is computed as: ˆz = z+σϵ, where ϵ (0, I) and σ U(0, τ ). (10) Here, ϵ denotes standard Gaussian noise, and the noise scale σ is uniformly sampled for each input up to threshold 5 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation on patch-wise spherical manifold, explicitly removing the redundant radial degree of freedom. By constraining the generative process to directional variations, the model searches over reduced and more semantically meaningful space, leading to more efficient training dynamics. In our setting, the latent space is modeled as product manifold of hyperspheres: (cid:124) (11) SC = SC where SC SC , (cid:123)(cid:122) (cid:125) times = (cid:8)z RC (cid:12) (cid:12) z2 = R(cid:9) . Each patch-level latent x(i) RC is therefore constrained to lie on sphere of fixed radius R. Instead of linear interpolation in Euclidean space, we define the interpolation path between x0 and x1 along the geodesic of the spherical manifold. For each patch index i, the geodesic interpolation at time [0, 1] is given by sin(cid:0)(1 t)Ω(i)(cid:1) sin(cid:0)Ω(i)(cid:1) sin(cid:0)tΩ(i)(cid:1) sin(cid:0)Ω(i)(cid:1) x(i) 1 , x(i) = x(i) 0 + (12) where the angular distance Ω(i) is defined as Ω(i) = arccos (cid:68) 0 , x(i) x(i) R2 1 (cid:69) . (13) We train velocity field vθ(xt, t) to match the time derivative of the geodesic path using quadratic loss: L(θ) = Et, x0, x1 (cid:34) 1 (cid:88) i=1 (cid:13) (cid:13)vθ(xt, t)(i) u(i) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) , (14) where the target velocity u(i) the geodesic with respect to time: is obtained by differentiating u(i) = dt x(i) = Ω(i) sin(cid:0)Ω(i)(cid:1) (cid:16) cos(cid:0)tΩ(i)(cid:1) x(i) 1 cos(cid:0)(1 t)Ω(i)(cid:1) x(i) (cid:17) . (15) By performing Flow Matching directly on the spherical manifold, the model focuses on meaningful directional variations while remaining invariant to latent magnitude. This formulation aligns with the intrinsic geometry of the autoencoder latent space and leads to more efficient and stable training of flow-based generative models. 4. Experiments 4.1. Reconstruction Results Implementation We train our model on the ImageNet-1K dataset (Deng et al., 2009) at 256 256 resolution using 8 6 Table 1. Comparison of autoencoder models on ImageNet 256256. We report reconstruction quality for previous autoencoders and our method in terms of rFID and PSNR. Model SD-VAE VAVAE MAETok RAE DINO-SAE (Ours) rFID PSNR 0.62 0.28 0.48 0.59 0.37 26.04 27.96 23.61 18.94 26.20 NVIDIA A100 GPUs (40GB). The training proceeds in four progressive stages, optimized with AdamW (Loshchilov & Hutter, 2017) using global batch size of 256. In Stage 1, we establish structural and semantic alignment. Crucially, we freeze the pre-trained transformer blocks of the DINOv3 Large encoder and only optimize the hierarchical convolutional patch embedding and the decoder. This strategy prevents the degradation of pre-trained representations while adapting the input projection for high-fidelity reconstruction. The model is trained with learning rate of 1 105 using standard betas (β1 = 0.9, β2 = 0.999). The objective function combines L1 loss, LPIPS perceptual loss, and our proposed Cosine Similarity loss with weights λL1 = 1.0, λlpips = 1.0, and λcos = 0.5, respectively. In Stage 2, we introduce adversarial training to enhance textural realism. We employ projected DINO-Discriminator with hinge adversarial loss (λadv = 0.5). The learning rate is increased to 1 104, and the optimizer momentum is adjusted to β1 = 0.5, β2 = 0.9 to stabilize GAN training. In Stage 3, we freeze the entire encoder (including the patch embedding) to fix the learned semantic latent space and finetune only the decoder. This stage focuses on maximizing the decoders capability to reconstruct details from the aligned features, maintaining the learning rate at 1 104. Finally, in Stage 4, we apply latent noise augmentation to improve the decoders robustness against generative artifacts. The encoder remains frozen, and the decoder is fine-tuned with reduced learning rate of 5.4 105 to ensure stable convergence. We use BF16 mixed-precision training across all stages. Results. To qualitatively and quantitatively evaluate the reconstruction capability of our model, we compare DINOSAE with the baseline RAE and other state-of-the-art autoencoders. As shown in Table 1, the baseline RAE exhibits limited reconstruction performance with an rFID of 0.59 and PSNR of 18.94. This degradation suggests that directly utilizing pre-trained encoder features without proper structural alignment is insufficient for high-fidelity image reconstruction. In contrast, our proposed DINO-SAE achieves significant performance boost, recording an rFID of 0.37 and PSNR of 26.20. Notably, DINO-SAE outperforms the widely used SD-VAE (rFID 0.62) in perceptual quality and demonstrates DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation Table 2. Comparison of generative models on ImageNet 256256. We report gFID, IS, Precision, and Recall with and without classifier-free guidance. Lower gFID is better, while higher IS, Precision, and Recall indicate better performance. Method Epochs #Params Generation@256 w/o guidance Autoregressive VAR (Tian et al., 2024) MAR (Li et al., 2024) Latent Diffusion DiT-XL/2 (Peebles & Xie, 2023) SiT-XL/2 (Ma et al., 2024) REPA (Yu et al., 2025) DDT (Wang et al., 2025) VAVAE (Yao et al., 2025) MAETok (Chen et al., 2025b) REPA-E (Leng et al., 2025) RAE (Zheng et al., 2025b) +LightningDiT-XL/1 +DiTDH -XL/1 350 800 400 400 80 800 400 80 800 800 80 80 80 2B 943M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 676M 879M gFID IS Prec. Rec. 1.92 2.35 323.1 227.8 0.82 0.79 0.59 0.62 9.62 8.61 7.90 5.78 6.62 4.29 2.17 2.56 3.46 1. 121.5 131.7 122.6 158.3 135.2 - 205.6 224.5 159.8 217.3 4.28 2.16 - 214.8 0.67 0.68 0.70 0.70 0.69 - 0.77 - 0.77 0.77 - 0.82 0.67 0.67 0.65 0.68 0.67 - 0.65 - 0.63 0. - 0.59 Latent Diffusion with DINO-SAE (Ours) DINO-SAE+LightningDiT-XL/1 DINO-SAE+DiTDH -XL/1 80 80 676M 879M 3.47 3. 202.1 209.7 0.82 0.80 0.55 0.63 substantial improvement over RAE. These results validate the effectiveness of our progressive training strategy. Specifically, freezing the transformer blocks while optimizing the hierarchical convolutional patch embedding in Stage 1 ensures robust semantic alignment, while the adversarial training in Stage 2 further enhances textural realism. Figure 5 presents visual comparison of the reconstructed images. While RAE (Middle) fails to capture fine-grained details and produces blurrier outputs compared to the Ground Truth (Left), DINO-SAE (Right) successfully reconstructs high-fidelity images with preserved textures and sharp edges, corroborating the quantitative improvements observed in Table 1. 4.2. Generation Results Implementation. For image generation, we train two diffusion model architectures: LightningDiT-XL and DiTDH - XL, utilizing 8 NVIDIA H100 GPUs. For both models, we strictly follow the original training configurations described in their respective papers unless otherwise specified. For LightningDiT-XL, we train the model for 80 epochs using fixed learning rate of 2 104 and global batch size of 1024. We apply an exponential moving average (EMA) of model parameters with decay rate of 0.9999, and use the EMA weights for inference. No learning rate scheduler or weight decay is applied during training. For DiTDH -XL, we use the same learning rate (2 104) and batch size (1024), with an EMA decay rate of 0.9995. We adopt learning rate warmup for the first 40 epochs, following the original DiTDH training protocol. Gradient clipping is applied only for DiTDH -XL to stabilize training, and no weight decay is used. Both models are optimized using AdamW, with the second momentum parameter set to β2 = 0.95. All other optimizer hyperparameters follow the default settings of the original implementations. All evaluations are conducted on ImageNet at resolution of 256 256. For image generation, we generated 50,000 images using the Euler sampler with projection. All images were sampled for 50 steps using the EMA model. We report Frechet Inception Distance (FID), Inception Score (IS), as well as Precision and Recall. FID is computed against the ImageNet training set following standard evaluation protocols. Results. As shown in Table 2, the proposed DINO-SAE outperforms existing generative autoencoders in terms of generative quality. When paired with LightningDiT-XL, DINO-SAE achieves gFID of 3.80, outperforming VAEbased models such as VAVAE (4.29) and RAE (4.28) trained for the same number of epochs. This indicates that DINOSAE provides more suitable latent representation for downstream generative modeling. Figure 6 further demonstrates 7 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation Figure 6. FID Convergence of various Autoencoders and methods VFMs, semantic information is largely encoded in the direction of features, while enforcing magnitude matching can hinder the preservation of high-frequency details. Based on this insight, DINO-SAE combines (i) Hierarchical Convolutional Patch Embedding that alleviates the early information bottleneck of standard ViT patchification, and (ii) Directional Feature Alignment via cosine similarity, which maintains semantic consistency while allowing the encoder to use feature magnitude to retain fine details. Finally, motivated by the observed magnitude-invariance of reconstructions, we explored Riemannian Flow Matching on spherical latent manifold, explicitly removing the redundant radial degree of freedom and focusing generation on meaningful directional variations. We believe this geometric perspective offers promising path toward more efficient and stable generative training in VFM-aligned latent spaces. Across ImageNet-1K at 256256, DINO-SAE substantially improves reconstruction fidelity over VFM-based tokenizers, achieving 0.37 rFID and 26.2 dB PSNR while preserving strong semantic alignment to the frozen teacher representations. Moreover, DINO-SAE provides an effective latent space for downstream generative modeling: diffusion models trained on DINO-SAE latents achieve competitive or improved generative quality and exhibit faster convergence compared to baselines, indicating that improving reconstruction need not come at the cost of generative usefulness. Limitations and future work. An important next step is to evaluate DINO-SAE beyond ImageNet reconstruction and unconditional generation, including text-to-image settings and downstream tasks that require precise controllability (e.g., image-to-image translation and inverse problems)."
        },
        {
            "title": "Impact Statements",
            "content": "By leveraging DINO features, our method advances efficient generative modeling with faster convergence. Beyond the standard ethical considerations of synthetic media generaFigure 5. Visual comparison of reconstruction quality. Left: GT, Middle: RAE, Right: DINO-SAE. the training efficiency of our approach. In terms of gFID convergence, models trained on DINO-SAE latents converge approximately 6.67 faster than the SiT-XL trained on SD-VAE latents with REPA loss, and 1.6 faster than LightningDiT-XL trained on RAE latents. These results suggest that DINO-SAE not only improves reconstruction fidelity but also significantly accelerates generative model training. Finally, when trained with DiTDH -XL, the model using DINO-SAE latents achieves generation quality comparable to that obtained with RAE latents. This shows that DINOSAE maintains strong performance even under more expressive diffusion architectures, while still offering the benefit of faster convergence. Overall, these results demonstrate that DINO-SAE simultaneously improves reconstruction quality and generative efficiency. 5. Conclusion We presented the DINO Spherical Autoencoder (DINOSAE), simple yet effective framework that turns pretrained vision foundation model (VFM) encoders into highfidelity tokenizers without sacrificing their semantic advantages. Our key finding is that, for contrastive/self-supervised 8 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation tionsuch as potential misuse and inherited biases from the pre-trained modelwe do not foresee additional specific societal consequences."
        },
        {
            "title": "References",
            "content": "Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Chen, B., Bi, S., Tan, H., Zhang, H., Zhang, T., Li, Z., Xiong, Y., Zhang, J., and Zhang, K. Aligning visual foundation encoders to tokenizers for diffusion models. arXiv preprint arXiv:2509.25162, 2025a. Chen, H., Han, Y., Chen, F., Li, X., Wang, Y., Wang, J., Wang, Z., Liu, Z., Zou, D., and Raj, B. Masked autoencoders are effective tokenizers for diffusion models. In ICML, 2025b. Chen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H., Li, M., Lu, Y., and Han, S. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. Chen, R. T. Q. and Lipman, Y. Riemannian flow matching on general geodesics. 2023. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Ho, J., Jain, A., and Abbeel, P. Denoising Diffusion Probabilistic Models. In NeurIPS, 2020. Jiang, D., Wang, M., Li, L., Zhang, L., Wang, H., Wei, W., Dai, G., Zhang, Y., and Wang, J. No other representation component is needed: Diffusion transformers can provide representation guidance by themselves. In ICLR, 2026. Leng, X., Singh, J., Hou, Y., Xing, Z., Xie, S., and Zheng, L. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. In ICCV, 2025. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. In NIPS, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. 2024. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Sauer, A., Karras, T., Laine, S., Geiger, A., and Aila, T. Stylegan-t: Unlocking the power of gans for fast largescale text-to-image synthesis. In International conference on machine learning, pp. 3010530118. PMLR, 2023. Simeoni, O., Vo, H. V., Seitzer, M., Baldassarre, F., Oquab, M., Jose, C., Khalidov, V., Szafraniec, M., Yi, S., arXiv preprint Ramamonjisoa, M., et al. Dinov3. arXiv:2508.10104, 2025. Song, J., Meng, C., and Ermon, S. Denoising Diffusion Implicit Models. In ICLR, 2021. Tang, H., Xie, C., Bao, X., Weng, T., Li, P., Zheng, Y., and Wang, L. Unilip: Adapting clip for unified multimodal understanding, generation and editing. arXiv preprint arXiv:2507.23278, 2025. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NIPS, 2024. Wang, R. and He, K. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. Wang, S., Tian, Z., Huang, W., and Wang, L. Ddt: arXiv preprint Decoupled diffusion transformer. arXiv:2504.05741, 2025. Wu, G., Zhang, S., Shi, R., Gao, S., Chen, Z., Wang, L., Chen, Z., Gao, H., Tang, Y., Yang, J., et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. In Adv. Neural Inform. Process. Syst., 2025. Yao, J., Yang, B., and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. 9 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation In International Conference on Learning Representations, 2025. Yun, J., Alcalar, Y. U., and Akcakaya, M. No alignment needed for generation: Learning linearly separable representations in diffusion models. arXiv preprint arXiv:2509.21565, 2025. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025a. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025b. 10 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation A. Additional details Inference naive Euler sampler can degrade the generation quality of our model, since we adopt Riemannian Flow Matching on spherical manifold. In particular, the standard Euler update does not explicitly enforce samples to remain on the manifold, which leads to accumulated off-manifold drift during sampling. To address this issue, we evaluate two manifold-aware sampling strategies and compare their generation performance: Euler with Projection: At each Euler step, the updated sample is projected back onto the sphere. Rodrigues Rotation Sampler: The sample is updated via rotation-based exponential map using Rodrigues formula, ensuring movement along the great circle direction. Empirically, the Euler sampler with projection achieves better generation quality. Therefore, all reported metrics in this paper are obtained using Euler sampling with projection. The detailed mechanisms of two algorithms are described in Algorithm 1,2 Algorithm 1: Rodrigues Sampler on Spherical Flow Matching Input: Class label y, Number of steps , Radius Result: Generated sample X0 // 1. Sample (0, I) XT Define time steps tk from 1 to 0 (Time Shifted) for = 0 to 1 do (Normalize patch-wise) Initialize from Prior Distribution (Gaussian Sphere) Model Prediction tnext tk+1 tcur tk, dt tnext tcur // 2. vpred Model(Xtcur , tcur, y) // 3. for each patch do n(i) x(i) tcur /R pred v(i) tan v(i) v(i) pred, n(i) n(i) Project to Tangent Space (Safety Check) Update Step (Exponential Map Approximation) end // 4. for each patch do vnorm v(i) θ vnorm dt/R ; ddir v(i) tan/vnorm // Rotate along the great circle (Rodrigues rotation) tnext cos(θ)x(i) x(i) tcur + sin(θ)ddir tan2 // Angular distance end end return X0; 11 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation Algorithm 2: Euler Sampler with Projection on Spherical Flow Matching Input: Class label y, Number of steps , Radius Result: Generated sample X0 // 1. Sample (0, I) XT Define time steps tk from 1 to 0 (Time Shifted) for = 0 to 1 do (Normalize patch-wise) Initialize from Prior Distribution (Gaussian Sphere) Model Prediction tnext tk+1 tcur tk, dt tnext tcur // 2. vpred Model(Xtcur , tcur, y) // 3. for each patch do n(i) x(i) tcur /R tan v(i) v(i) pred v(i) pred, n(i) n(i) Project to Tangent Space (Safety Check) Update Step (Exponential Map Approximation) end // 4. for each patch do x(i) tnext x(i) tnext v(i) x(i) tan v(i) tan tcur + dt v(i) tan end end return X0; 12 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation A. Uncurated Generated Samples We provide additional qualitative results generated by our DINO-SAE model. All samples presented in this section are uncurated and generated without classifier-free guidance. (a) Class label = Cockatoo (89) (b) Class label = Balloon (417) Figure 7. Uncurated 256256 DiTDH -XL samples. Guidance Scale = 1.0. 13 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation (a) Class label = Daisy (985) (b) Class label = Monarch Butterfly (323) Figure 8. Uncurated 256256 DiTDH -XL samples. Guidance Scale = 1.0. 14 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation (a) Class label = Golden Retriever (207) (b) Class label = Volcano (980) Figure 9. Uncurated 256256 DiTDH -XL samples. Guidance Scale = 1.0. DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation (a) Class label = Goldfinch (11) (b) Class label = Jellyfish (107) Figure 10. Uncurated 256256 DiTDH -XL samples. Guidance Scale = 1.0. 16 DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation (a) Class label = Hamburger (933) (b) Class label = Ice Cream (928) Figure 11. Uncurated 256256 DiTDH -XL samples. Guidance Scale = 1.0."
        }
    ],
    "affiliations": [
        "Graduate School of AI, KAIST"
    ]
}