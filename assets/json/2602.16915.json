{
    "paper_title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
    "authors": [
        "Zeyu Ren",
        "Xiang Li",
        "Yiran Wang",
        "Zeyu Zhang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2."
        },
        {
            "title": "Start",
            "content": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation Zeyu Ren1 Xiang Li2 Yiran Wang3 Zeyu Zhang2 Hao Tang2 1The University of Melbourne 2Peking University 3Australian Centre for Robotics Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 6 2 0 F 8 1 ] . [ 1 5 1 9 6 1 . 2 0 6 2 : r AbstractStereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with novel ConvSS2D operator based on selective state space models. The proposed operator employs four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-ofthe-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvement on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https: //github.com/AIGeeksGroup/StereoAdapter-2. Website: https:// aigeeksgroup.github.io/StereoAdapter-2. I. INTRODUCTION Stereo depth estimation serves as cornerstone for robotic perception, providing metric 3D reconstruction from passive binocular cameras that underpins autonomous navigation [58], manipulation, and environmental mapping. In underwater domains, accurate depth sensing is indispensable for AUV/ROV operations spanning infrastructure inspection, ecological monitoring, and archaeological survey, where geometric fidelity directly governs mission safety and autonomy [1]. Nevertheless, underwater imaging introduces pronounced domain shifts stemming from wavelength-dependent attenuation, forward and backscattering, and refraction at waterglass interfaces, which severely violate the photometric consistency assumptions underlying terrestrial stereo pipelines [42, 84]. Recent advances have sought to bridge monocular vision foundation models (VFMs) [48] with stereo geometry for robust underwater adaptation. StereoAdapter [62] integrates LoRA-adapted encoder with GRU-based iterative refinement, achieving parameter-efficient domain transfer and demonstrating promising results on underwater benchmarks. However, Fig. 1: Conceptual comparison. The Gated Recurrent Unit (GRU) relies on multiple non-linear gates and candidate states ht to update the hidden state ht. Its complex gating mechanism introduces non-linear recursion that is difficult to analyze for long sequences. The Selective SSM streamlines this into linear recurrence. By dynamically generating parameters from the input xt, the Selective SSM maintains input-dependent selectivity to adaptively modulate information flow. We leveraged the characteristics of selective SSM to design ConvSS2D, enabling the adaptation iterative process. two key challenges remain for practical underwater deployment: (i) further improving the efficiency and accuracy of iterative disparity refinement, particularly in large-disparity and textureless regions prevalent in underwater scenes, and (ii) bridging the synthetic-to-real gap given the scarcity of diverse real-world underwater stereo data with accurate ground-truth annotations. Our motivation is to advance underwater stereo depth estimation along both the architectural and data dimensions while maintaining the parameter-efficient adaptation paradigm. Concretely, we seek to explore alternative update mechanisms that can capture long-range spatial dependencies more effectively, and to construct large-scale synthetic dataset that better covers the diversity of real underwater conditions including varying optical parameters and camera configurations. To this end, we propose StereoAdapter-2, framework that advances underwater stereo depth estimation through architectural innovation and data scaling. Architecturally, we introduce the ConvSS2D operator built upon selective state space models [17, 39], which employs four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation at linear computational complexity. On the data side, we construct UW-StereoDepth80K, large-scale synthetic underwater stereo dataset generated through two-stage pipeline combining semantic-aware style transfer via Atlantis [78] and geometry-consistent novel view synthesis via NVS-Solver [74], systematically varying baselines, attenuation coefficients, and scattering parameters to emulate diverse ROV configurations. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks, with 17% improvement on TartanAirUW and 7.2% on SQUID, while real-world deployment on the BlueROV2 platform validates practical applicability."
        },
        {
            "title": "The main contributions of this work are summarized as",
            "content": "follows: We introduce the ConvSS2D update operator built upon selective state space models, replacing ConvGRU with four-directional scanning strategy that captures both horizontal epipolar constraints and vertical structural consistency, enabling efficient long-range spatial propagation within single refinement step. We construct UW-StereoDepth-80K, large-scale synthetic underwater stereo dataset featuring diverse baselines and optical parameters through two-stage generative pipeline, providing rigorous foundation for training data-hungry stereo networks. We achieve state-of-the-art zero-shot performance on underwater benchmarks including TartanAir-UW and SQUID, with real-world validation on the BlueROV2 platform demonstrating robust generalization from synthetic training to real underwater scenes. II. RELATED WORK a) Deep Stereo Matching: Early deep stereo matching methods mainly relied on CNN-based cost volume aggregation [75, 76, 50, 54, 56], where stereo correspondence is modeled by constructing and processing cost volumes using 2D or 3D convolutional architectures [2, 53, 28, 21, 73, 70, 35, 8, 79, 77, 66, 72, 51, 43, 9, 52]. However, despite these advances, CNN-based cost aggregation remains fundamentally constrained by explicit cost volume construction, motivating iterative optimization-based stereo methods that bypass explicit aggregation and enable efficient refinement on highresolution representations [37, 34, 29, 26, 80, 53, 81, 63, 15, 10, 59, 24, 12, 65, 16, 62]. The ViT architecture transforms the stereo matching problem into sequence-to-sequence problem [41, 55, 60, 20, 27, 40, 33, 67]. It uses self-attention and crossattention [57] mechanisms with positional encoding to model global context and establish correspondences between stereo views, achieving competitive performance. b) Underwater depth estimation and datasets: Unlike terrestrial scenarios, obtaining accurate and dense ground-truth disparity annotations in underwater environments is extremely difficult, as active sensors such as LiDAR are unreliable underwater and large-scale data collection is costly [1]. Early underwater datasets, such as FLSea-Stereo [47], lacked accurate Stereo Disparity annotations. UWStereo proposed highquality synthetic underwater stereo matching dataset [42], but its scene complexity still falls short of real-world underwater scenarios. Beyond data limitations, underwater stereo matching itself remains highly challenging. Light scattering, absorption, and refraction significantly reduce photometric consistency between views [1], making reliable matching difficult. To address these challenges, UWStereo proposed an enhancement module to better perceive geometric structures [42], while UWNet and Fast-UWNet introduced attention mechanisms and 1D2D cross-search strategies to mitigate underwater image distortions [84]. However, these approaches rely on carefully designed, domain-specific modules for adaptation, which limits their generalization ability and scalability across diverse underwater conditions. c) State Space Model: State-space model (SSM) has become an efficient alternative to Transformers in sequence modeling [38]. SSM can efficiently model long-range dependencies, and their complexity is linear or near-linear with sequence length. Unlike gated recurrent architectures, SSM relies on structured state evolution, thus achieving stable and scalable sequence processing [32, 22, 19]. Early SSM-based methods, such as S4 [18], improved computational efficiency by parameterizing the state transition matrix and reconstructing sequence modeling into convolution operations. Mamba further improved SSM by introducing selective scanning with input correlation [17, 13]. Mamba is hardware-aware algorithm that parallelizes long sequences within recurrent computation paradigm [17], effectively alleviating the serial bottleneck of traditional RNNs. Mamba demonstrates stronger long sequence modeling capabilities while maintaining high computational efficiency. Following the successful application of SSM in sequence modeling, recent research has extended them to vision tasks [31, 25, 83, 39]. Vim adopts ViT-style architecture and addresses the problem of unidirectionality and lack of positional information in SSM by introducing bidirectional processing and positional embedding [83, 14]. Vmamba further points out that visual understanding requires modeling that considers spatial structure and global relevance [39], and proposes the SS2D module, which scans images along multiple spatial directions to capture spatial dependencies. Subsequent research continues to explore improved scanning strategies and scanning direction designs to better utilize the spatial context in visual data [46, 45, 69, 23]. The scarcity of underwater scene datasets and the inherent characteristics of SS2D [39] closely match the epipolar geometry in stereo matching tasks, inspiring our approach. We leverage the rich representations learned in the pre-trained model while utilizing LoRa for efficient parameter fine-tuning and domain adaptation. Replacing the traditional GRU-based update module with an SSM-based module enables effective underwater depth estimation. Fig. 2: Detailed architecture of the StereoAdapter-2: Our model iteratively refines disparity by integrating Mamba Adapter. The refinement step is powered by the ConvSS2D operator, which enables adaptive and long-range spatial information propagation through multi-directional selective scanning. III. PRELIMINARIES The SSM is type of continuous-time latent state model, which defines mapping from one-dimensional function or sequence u(t) to an output y(t) through an implicit latent state h(t) RN , as given in Eq. 1. h(t) = Ah(t) + Bu(t), y(t) = Ch(t) + Du(t), (1) where A, B, C, are the learned parameters, and for the sake of explanation, we omit parameter D. For SSM model training, we discretize the parameters of the continuous-time system. As shown in Eq. 2, the continuoustime parameters and are discretized using zero-order hold (ZOH), where denotes the discretization time step. = eA, = (A)1 (cid:0)eA I(cid:1) B. After discretization, the entire model can be computed using linear recurrence and global convolution. Global convolution computation can be efficiently parallelized, and efficient autoregressive inference can be performed through linear recurrence. (2) = (cid:0)C B, B, . . . , AL1 B(cid:1) , = K, (3) where is the length of the input sequence, and RL denotes the structured convolutional kernel. This formulation provides general view of state space models, which we later reinterpret as structured spatial state recursion for iterative disparity refinement. IV. THE PROPOSED METHOD A. Overview We propose StereoAdapter-2, which uses monocular depth to guide stereo disparity estimation, as foundation model shown in Fig. 2. Our framework adopts unified architecture that integrates Depth Anything 3 [36] as both the feature encoder and monocular depth estimator. To efficiently adapt the pretrained Depth Anything 3 to stereo matching in underwater scenes, we employ LoRA [62], which enables efficient parameter fine-tuning while maintaining the rich representations learned from large-scale pre-training. Monocular depth estimation is utilized for disparity initialization to accelerate convergence. To iterate disparity estimation, we replace the traditional GRU-based update module with Selective SSM module and enhance the learned gating mechanism. This design leverages the long-range spatial modeling capabilities of the SSM while retaining the adaptive memory control of the cyclic unit. B. Feature Extraction We first extract features FL and FR using the powerful depth foundation model Depth Anything 3 [36]. We extract multi-scale representations from four intermediate Transformer layers 1, 2, 3, 4 to capture details and semantic information at different levels. Meanwhile, for underwater scene domain adaptation, we fine-tune the encoder following the approach of StereoAdapter [62]. C. Correlation Pyramids Building We constructed correlation pyramid to encode the visual similarity between pairs of stereo images. Unlike the optical flow of 4D correlator that needs to cover all pixel pairs, stereo matching using calibrated images restricts the correspondences to the horizontal direction. , 1 Given the features 1 RHW extracted from FL and FR, we compute the correlation volume by calculating the inner product between features with the same coordinate, following Eq (4). Cijk = (cid:88) l,ijd 1 1 r,ikd, RHW , (4) where represents the row index of the left image, represents the column index of the left image, and represents the column index of the right image. To capture the correspondence between fine-grained and large displacements, we construct four-layer correlation pyramid (l)4 l=1 by repeatedly applying average pooling with kernel size of 2 along the last dimension, where the l-th layer has dimension of W/2l1, providing progressively larger receptive field while maintaining spatial resolution. In each refinement iteration, given the current disparity estimate d, we perform lookup operation using linear interpolation to retrieve the correlation values at integer offsets r, . . . , + from each pyramid layer, and concatenate the retrieved values from all layers to form the correlation features input to the update operator. D. Iterative Disparity Estimation Following RAFT-Stereo [37], we adopt an iterative refinement framework to progressively estimate disparity. Given an initial disparity estimate D0, we iteratively update it through iterations: D0, D1, ..., DL. However, instead of using ConvGRU, We propose ConvSS2D as the core operator. Firstly, the long-range dependency modeling of ConvSS2D is achieved through sequential state recursion [17, 13, 39], without requiring multiple layers of convolutions to expand the receptive field. Specifically, the state update at spatial location follows Eq (5). ht = Aht1 + Bxt, (5) where ht denotes the hidden state at spatial position along given scan direction, ht1 represents the propagated state from the previous position, and xt is the input feature at the current location. The discretized state transition matrix governs how information propagates sequentially across spatial positions, while controls how the input features are incorporated into the state update. As result, information can be propagated over long spatial extents through directional scans, allowing features at distant locations to influence each other within single refinement step. Owing to the inherent long-range propagation capability of ConvSS2D, we discard the traditional context encoder and directly project decoder features to initialize the hidden state a) Input-dependent Selectivity: key limitation of ConvGRU lies in its inductive bias for spatial information propagation. Although its gating functions are conditioned on the input, the update is implemented through local convolutional kernels, resulting in predominantly local and isotropic information aggregation within each refinement step. In contrast, ConvSS2D introduces input-dependent selectivity through dynamically computed parameters , B, and C. These parameters are generated from the input features via linear projections following Eq (6). = softplus(Wxt), = WBxt, = WCxt, (6) where W, WB, WC are learnable projection matrices. This mechanism enables the model to adaptively modulate: (1) the state update dynamics via , controlling the rate of state evolution; (2) input gating via B, selectively incorporating relevant features; and (3) output projection via C, emphasizing task-relevant information. Such content-aware processing allows the network to dynamically adjust its behavior based on local image characteristics, such as texture, edges, and occlusion boundaries. b) Scanning Strategy: We extend one-dimensional selective scanning to two dimensions using four-directional scanning strategy that handles features along both horizontal and vertical directions. This design is particularly suitable for stereo matching, because reliable matching still benefits from aggregating two-dimensional spatial context. The horizontal scan is directly aligned with the epipolar constraint, enabling efficient propagation of disparity information along the scan line. Simultaneously, the vertical scan contributes to consistency across the scan line, captures vertical structure, and normalizes disparity estimation in textureless regions. The outputs from all four scan directions are aggregated to form comprehensive feature representation that satisfies the inherent geometric constraints of stereo vision. E. Data Synthesis: UW-StereoDepth-80K To overcome the scarcity of diverse real-world underwater stereo data, we propose novel two-stage generative data synthesis pipeline. Our approach leverages diffusion models to synthesize high-fidelity underwater stereo pairs from terrestrial RGB-D data. As illustrated in Fig. 3, our pipeline sequentially applies semantic-aware style transfer and geometry-consistent novel view synthesis. a) Underwater Style Transfer: We utilize Atlantis [78], specialized framework for enabling underwater data synthesis via Stable Diffusion, to bridge the photometric domain gap. Given terrestrial source image Isrc and its corresponding source depth map Dsrc, Atlantis acts as style transfer module that hallucinates realistic underwater optical effects, such as wavelength-dependent attenuation, scattering, and turbidity, while preserving the semantic content and geometric structure of the original scene. By conditioning the diffusion process generated using NVS-solver [74] to synthesize virtual underwater data. For evaluation, we conduct experiments on two underwater datasets. The first is TartanAir-UW, subset from TartanAir [58] that only consists of 13,583 underwater stereo image pairs. The second is the SQUID dataset [6], which contains images from four distinct scenes. b) Evaluation Dataset and Metrics: We report standard depth estimation metrics, including Absolute Mean Relative Error (AbsRel), Squared Mean Relative Error (SqRel), Root Mean Square Error (RMSE), and logarithmic RMSE (Log RMSE). In addition, we report accuracy under threshold metrics δ1, δ2, and δ3. The accuracy threshold δk measures < 1.25k, the percentage of pixels for which max where di and ˆdi denote the ground-truth and predicted depth values, respectively, and {1, 2, 3}. (cid:16) ˆdi di , di ˆdi (cid:17) B. Implementation Details We trained StereoAdapter-2 on an H100 NVL and deployed it on ROV. Input image resolution is 480640 and normalized to [0, 1]. We initialize the feature encoder with Depth Anything 3 (ViT-B) [71] pretrained weights. We perform 22 iterations during training and 32 during inference. For LoRA settings, we follow the StereoAdapter [62] settings, LoRA rank = 16, sparsity threshold κmax = 0.005, and regularization weight λ = 1 104. The sparse phase activates at 50% of training. Our method uses the loss function Ldisparity and Lsparse, and the weight ratio is set to 1 : 1. The model is trained using the AdamW optimizer with learning rate of 1 104 and weight decay of 1 105. We employ the OneCycleLR scheduler for 100K iterations. Regarding data augmentation, we used strategies consistent with RAFT-Stereo [37], including saturation enhancement and random scaling. C. Main Results that Our trained experiments proposed the demonstrate the UW-StereoDepth-80K on StereoAdapter-2, dataset, achieves state-of-the-art zero-shot performance across both TartanAir Underwater and SQUID benchmarks. As summarized in Tables and II, our approach consistently outperforms existing stereo matching methods without any fine-tuning on the target domains. As shown in Table I, StereoAdapter-2 achieves superior zero-shot performance on the TartanAir Underwater subset, obtaining the lowest REL (0.0440) and RMSE (2.4038), along with the highest A1 accuracy (96.76%). Compared to our prior StereoAdapter trained on UW-StereoDepth-40K, StereoAdapter-2 reduces REL by 16.5% and RMSE by 17.0%, demonstrating both the effectiveness of our adapter architecture and the benefits of scaling the training dataset. Table II presents zero-shot evaluation on the real-world SQUID dataset. StereoAdapter-2 attains the best overall performance with an RMSE of 1.7481 and the lowest REL of 0.0705, reducing RMSE by 7.2% compared to the previous StereoAdapter while achieving leading accuracy across all δ thresholds (A1: 94.25%, A2: 97.65%, A3: 98.62%). These results highlight the strong zero-shot generalization capability Fig. 3: Data synthesis pipeline. Semantic-aware style transfer and geometry-consistent novel view synthesis rendering pipeline for UW-StereoDepth-80K dataset. on the source depth Dsrc, we ensure that the synthesized underwater imagery maintains structural fidelity to the input, into diverse effectively transforming terrestrial dataset underwater domain without losing ground truth geometric labels. b) Multi-Baseline Stereo Generation: To generate stereo correspondences from the stylized monocular images, we employ NVS-Solver [74], video diffusion model designed for zero-shot novel view synthesis. Standard diffusion-based image generation often lacks multi-view geometric consistency. NVS-Solver addresses this by treating the stereo generation task as view synthesis problem governed by explicit camera extrinsics. Taking the output from the Atlantis stage as the reference view, we synthesize the target right view by conditioning the solver on specific baseline displacements. As shown in the right panel of Fig. 3, we systematically generate stereo pairs across four distinct baselines: 20cm, 30cm, 40cm, and 50cm. This multi-baseline strategy simulates the diverse camera configurations found in real-world underwater robots, thereby enhancing the models robustness to scale variations and disparity ranges during training. c) Dataset Construction: By cascading Atlantis and NVS-Solver, we convert large-scale terrestrial RGB-D datasets into synthetic underwater stereo benchmark. Each stereo pair in our generated subset is synthesized at resolution of 640 480. The resulting dataset features physically plausible underwater appearance, consistent stereo geometry, and dense ground truth disparity, providing rigorous foundation for training data-hungry stereo matching networks. UW-StereoDepth-80K is constructed by merging our newly generated diffusion-based samples with the existing UWStereoDepth-40K dataset [62]. The final consolidated dataset comprises 80,000 high-quality stereo image pairs. V. EXPERIMENTS A. Datasets and Metrics a) Training Datasets: To cover various underwater scenarios, we used our training data based on the UWStereoDepth-80K dataset, which contains about 80K samples Fig. 4: Qualitative results of zero-shot stereo depth estimation TABLE I: Quantitative comparison of zero-shot stereo depth estimation on the TartanAir underwater subset. All methods are evaluated under the same protocol using standard depth metrics. Method Training Set Rel SqRel RMSE Log RMSE A1 A2 A3 LEAStereo [11] PSMNet [8] AANet [66] GwcNet [21] ACVNet [64] RAFT-Stereo [37] HSMNet [70] TiO-Depth [82] FoundationStereo [61] Stereo Anywhere [4] CREStereo [29] Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow KITTI2012 FoundationStereo dataset Scene Flow ETH3D StereoAdapter [62] StereoAdapter-2 (Ours) UW-StereoDepth-40K UW-StereoDepth-80K 0.1099 0.0884 0.6096 0.1013 0.0970 0.0814 0.9856 0.7194 0.0542 0.0592 2.5746 0.0527 0.0440 1.3898 0.8699 8.3687 1.2965 1.1335 0.7342 12.3768 8.6479 0.6701 0.5098 9. 0.5167 0.4312 4.5610 3.9721 13.0542 4.1829 3.9985 4.0423 15.2865 13.4635 2.9644 3.1572 8.4526 2.8947 2.4038 0.2063 0.1804 0.9903 0.1855 0.1803 0.1703 4.5961 1.6967 0.1358 0.1544 5.1297 0.1371 0.1198 0.8929 0.9122 0.2598 0.9085 0.9063 0.9030 0.0000 0.0053 0.9302 0.9442 0. 0.9467 0.9676 0.9512 0.9627 0.3451 0.9612 0.9612 0.9612 0.0000 0.0096 0.9701 0.9787 0.5732 0.9701 0.9704 0.9761 0.9804 0.3888 0.9801 0.9813 0.9832 0.0000 0.0550 0.9779 0.9889 0.7001 0.9753 0.9890 of StereoAdapter-2 from synthetic training data to real-world underwater scenes. As shown in Figure 4, StereoAdapter-2 generates substantially more accurate and visually coherent depth maps than baseline methods, with better scale estimation for far range details. In summary, these findings validate that our StereoAdapter-2 architecture, combined with the UW-StereoDepth-80K dataset, enables robust zero-shot stereo depth estimation in diverse underwater environments. D. Real-World Evaluation a) Hardware Configuration: As shown in Figure 6, We validate our approach using BlueROV2 platform equipped with an NVIDIA Jetson Orin NX (32GB) for onboard computation. Low-level motion control is delegated to an STM32 microcontroller. Visual input is captured by pair of fisheye cameras mounted in stereo arrangement; we apply offline rectification to transform the raw fisheye frames into standard pinhole geometry prior to network inference. b) Scene Setup and Data Collection: We conduct all trials in controlled indoor water tank environment. To emulate realistic underwater navigation scenarios, we arrange glass containers and irregularly shaped stones into 5 distinct spatial levels of clutter complexity. layouts representing different The robot is then teleoperated through 3 separate navigation routes per layout, yielding total of 15 time-aligned binocular recordings. All visual data and timestamps are logged directly on the Jetson platform. c) Ground-Truth Acquisition: Before experimentation, we construct geometrically calibrated 3D reference model of the tank interior. During each trial, camera poses are recovered by detecting AprilTags (family 16h5) and solving the corresponding pose estimation problem. These poses are subsequently aligned with the pre-scanned model, enabling us to project the geometry onto the left view and obtain per-pixel depth references. Regions without valid surface intersections are excluded from subsequent analysis. d) Evaluation Protocol: Every method under comparison receives the same rectified image pairs at uniform resoluFig. 5: Qualitative results of zero-shot underwater stereo depth estimation were obtained by deploying the model on robotic platform. TABLE II: Zero-shot evaluation on SQUID dataset. 5 Datasets refers to Scene Flow [44], Sintel [7], ETH3D [49], InStereo2K [3], and CREStereo [30]. Method Training Set Rel SqRel RMSE Log RMSE A1 A2 A3 LEAStereo [11] PSMNet [8] AANet [66] GwcNet [21] ACVNet [64] RAFT-Stereo [37] HSMNet [70] CREStereo [29] IGEV-Stereo [63] Selective IGEV [59] GMStereo [68] TiO-Depth [82] FoundationStereo [61] Stereo Anywhere [4] Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow ETH3D 5 Datasets + TartanAir 5 Datasets + TartanAir 5 Datasets + TartanAir KITTI2012 FoundationStereo dataset Scene Flow StereoAdapter [62] StereoAdapter-2 (Ours) UW-StereoDepth-40K UW-StereoDepth-80K 0.5574 0.5182 7.4801 0.2294 1.6030 0.0831 0.9772 2.5746 0.0932 0.0960 3.3442 1.3154 0.1095 0.0952 0.0806 0. 3.9434 7.1404 314.1577 1.2275 65.6518 0.6946 7.2766 9.8789 1.4685 0.9617 140.3211 11.6828 0.7012 1.1017 0.7082 0.6396 5.4659 4.9186 34.7612 3.0003 10.3828 1.9625 8.2301 8.4526 2.4741 1.9268 18.7829 7.0930 2.2510 2.4317 1.8843 1.7481 0.4335 0.5902 1.8994 0.3799 0.7293 0.1441 4.0887 5.1297 0.1523 0.1665 1.0219 0.8121 0.1584 0.1586 0.1469 0. 0.6512 0.7139 0.0602 0.7423 0.7019 0.9235 0.0000 0.4890 0.9346 0.9171 0.5300 0.1753 0.8995 0.9179 0.9413 0.9425 0.8042 0.7999 0.1087 0.8517 0.7925 0.9634 0.0000 0.5732 0.9712 0.9555 0.6076 0.3346 0.9433 0.9605 0.9748 0.9765 0.8869 0.8311 0.1570 0.9005 0.8321 0.9835 0.0000 0.7001 0.9820 0.9720 0.6578 0.5133 0.9501 0.9763 0.9852 0. TABLE III: Real-world evaluation on BlueROV2. Method REL SqRel RMSE Log RMSE A1 Stereo Anywhere [4] FoundationStereo [61] StereoAdapter [62] 0.1218 0.1304 0.1163 1.0623 0.6187 0.6794 2.4682 2.0893 1.9285 StereoAdapter-2 (Ours) 0.1023 0.5843 1.7164 0.1673 0.1635 0.1556 0.1354 0.8541 0.8812 0. 0.9256 Fig. 6: Hardware platform for real world experiments. tion with consistent pre-processing. When model produces disparity output, we recover absolute depth via the known stereo geometry parameters. Performance is quantified using established depth metrics: Absolute Relative Error (REL), Squared Relative Error (SQ REL), Root Mean Squared Error (RMSE), Logarithmic RMSE, and the threshold accuracy A1. All statistics are computed exclusively over valid pixels and aggregated across the full set of recordings. e) Results: The proposed method achieves better performance, as shown in table III, reaching REL of 0.1023, an RMSE of 1.7164, and A1 accuracy of 92.56%. Relative to other baselines, our proposed model exhibits consistent gains in both precision and stability across diverse underwater obstacle arrangements. E. Ablation Study Table IV shows ablation experiments for different components of our model, including the use of pre-trained model, monocular disparity initialization, ConvSS2D, and the context encoder. Table shows the ablation experiments performed on different hyperparameter settings during model training. Table VI further analyzes the impact of key SSM hyperparameters in ConvSS2D, including the state dimension dstate and the SSM expansion ratio. We observe that increasing dstate progressively improves model performance, with dstate = 16 achieving the best REL and RMSE scores. However, this comes at the cost of increased computational overhead and reduced throughput. In contrast, increasing the SSM expansion TABLE IV: Model ablation of StereoAdapter-2, evaluating the effects of different design components, including the Depth Anything 3 encoder, monocular disparity initialization, context encoder, and update module. TABLE VI: Ablation study on ConvSS2D SSM hyperparameters under FP32 precision, analyzing the effects of the state dimension dstate and SSM ratio on model accuracy and efficiency. DA3 Encoder Mono Disp. Init. Context Encoder Update Module REL RMSE ConvGRU 0.0516 ConvGRU 0.0482 ConvSS2D 0.0449 ConvSS2D 0.0463 ConvSS2D 0.0440 2.82 2.64 2.46 2.54 2.40 TABLE V: Ablation on training hyperparameters. Batch Size Learning Rate Train Iters REL RMSE state ssm ratio Params FLOPs 1 4 16 16 16 1.0 1.0 1.0 1.5 2.0 (M) (G) 20.40 20.42 20.47 20.60 20. 843.33 845.81 855.72 886.17 916.62 REL RMSE TP. (img/s) 5.26 5.22 4.71 4.62 4.26 0.0445 0.0440 0.0438 0.4430 0.4551 2.42 2.40 2.38 2.43 2. TABLE VII: Ablation on SS2D scanning patterns. 4 4 8 8 8 1 104 2 104 1 104 2 104 1 104 16 16 16 16 22 0.0461 0.0489 0.0453 0.0476 0.0440 2.53 2.68 2.47 2.59 2. Scanning Pattern Params FLOPs (M) (G) REL RMSE TP. (img/s) Unidi-Scan Bidi-Scan Cross-Scan 20.47 20.47 20.47 855.72 855.72 855.72 4.87 4.86 4.74 0.0459 0.0453 0.0440 2.46 2.42 2.40 ratio beyond 1.0 leads to significant performance degradation. Considering the trade-off between accuracy and efficiency, we choose dstate = 4 with an SSM ratio of 1.0 as the default configuration, which achieves competitive performance while maintaining high throughput. Table VII investigates the impact of different SS2D scanning modes. Compared to unidirectional and bidirectional scanning, the cross-scanning strategy consistently achieves better model performance while maintaining the number of similar parameters and FLOPs. This indicates that reliable matching still benefits from the aggregation of two-dimensional spatial context. VI. TEST-TIME EFFICIENCY We evaluate on an on-board Jetson Orin NX 32GB in MaxN mode with TensorRT, batch size 1, and input resolution 640320. With identical pre/post-processing for predictions. We report per-frame end-to-end latency in milliseconds (ms). As shown in table VIII FoundationStereo and Stereo Anywhere both adopt DepthAnythingV2-L as their encoder backbone, with FoundationStereo incurring additional overhead from its transformer-based feature refinement module. MGStereo, while using lighter encoder, involves multistage disparity fusion and iterative refinement, which contributes to its latency. In contrast, StereoAdapter-2 achieves the lowest latency of 1102 ms by employing LoRA-adapted DepthAnythingV3-B encoder and replacing conventional recurrent updates with ConvSS2D, which accelerates the disparity refinement process while maintaining accuracy. VII. LIMITATIONS AND FUTURE WORK Despite these advances, limitations remain. The syntheticto-real domain gap persists under extreme underwater conditions, such as severe turbidity, strong backscatter, or rapidly varying illumination, where the diversity of our training data may not fully capture real-world complexity. Furthermore, TABLE VIII: Average per-frame inference latency (ms) on Jetson Orin NX @ 640360, batch size=1. Method Params (M) On-board (ms) FoundationStereo [61] Stereo Anywhere [4] MGStereo [5] StereoAdapter [62] StereoAdapter-2 (Ours) 375 347 347 202 103 1933 1524 1631 1285 1102 while our method achieves strong per-frame accuracy, temporal consistency in continuous deployment remains challengingconsecutive depth predictions may exhibit flickering or instability. Future work will focus on incorporating temporal modeling to ensure prediction stability across consecutive frames, as well as exploring tighter integration with downstream robotic tasks, such as grasp point prediction for underwater manipulation. VIII. CONCLUSION We present StereoAdapter-2, novel framework for underwater stereo depth estimation by introducing the ConvSS2D operator, built upon selective state space models, our method enables efficient long-range spatial propagation through fourdirectional scanning strategy. To address the scarcity of diverse underwater stereo data, we constructed UW-StereoDepth-80K through two-stage generative pipeline, combining semanticaware style transfer and geometry-consistent novel view synthesis, enabling systematic variation for underwater images. Combined with dynamic LoRA adaptation, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks, with 17% improvement on TartanAir-UW and 7.2% on SQUID compared to prior methods. Real-world deployment on the BlueROV2 platform further validates the practical applicability of our approach."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Derya Akkaynak and Tali Treibitz. revised underwater In Proceedings of the IEEE image formation model. conference on computer vision and pattern recognition, pages 67236732, 2018. [2] Antyanta Bangunharcana, Jae Won Cho, Seokju Lee, In So Kweon, Kyung-Soo Kim, and Soohyun Kim. Correlate-and-excite: Real-time stereo matching via guided cost volume excitation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 35423548. IEEE, 2021. [3] Wei Bao, Wen Wang, Yuhua Xu, Yulan Guo, Siyu Hong, and Xiaohu Zhang. Instereo2k: large real dataset for stereo matching in indoor scenes. Science China Information Sciences, 63, 2020. URL https: //api.semanticscholar.org/CorpusID:221110870. [4] Luca Bartolomei, Fabio Tosi, Matteo Poggi, and Stefano Stereo anywhere: Robust zero-shot deep Mattoccia. stereo matching even where either stereo or mono fail. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10131027, 2025. [5] D. Berman, T. Treibitz, and S. Avidan. Diving into haze-lines: Color restoration of underwater images. In Proceedings of the British Machine Vision Conference. BMVA Press, 2017. [6] Dana Berman, Deborah Levy, Shai Avidan, and Tali Treibitz. Underwater single image color restoration using haze-lines and new quantitative dataset. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43 (8):28222837, 2020. [7] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for In European Conference on optical flow evaluation. Computer Vision (ECCV), pages 611625, 2012. [8] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 54105418, 2018. [9] Liyan Chen, Weihan Wang, and Philippos Mordohai. Learning the distribution of errors in stereo matching for joint disparity and uncertainty estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1723517244, 2023. [10] Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, and Jia Wu. Mocha-stereo: Motif channel attention network for stereo matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2776827777, 2024. [11] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo matching. Advances in Neural Information Processing Systems, 33, 2020. [12] Ziang Cheng, Jiayu Yang, and Hongdong Li. Stereo matching in time: 100+ fps video stereo matching for extended reality. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 87198728, January 2024. [13] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured In International Conference on state space duality. Machine Learning (ICML), 2024. [14] Alexey Dosovitskiy. An image is worth 16x16 words: arXiv Transformers for image recognition at scale. preprint arXiv:2010.11929, 2020. [15] Miaojie Feng, Junda Cheng, Hao Jia, Longliang Liu, Gangwei Xu, Qingyong Hu, and Xin Yang. Mc-stereo: Multi-peak lookup and cascade search range for stereo matching. arXiv preprint arXiv:2311.02340, 2023. [16] Rui Gong, Weide Liu, Zaiwang Gu, Xulei Yang, and Jun Cheng. Learning intra-view and cross-view geometric In Proceedings of the knowledge for stereo matching. IEEE/CVF conference on computer vision and pattern recognition, pages 2075220762, 2024. [17] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. [18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [19] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572585, 2021. [20] Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Russell Taylor, Mathias Unberath, Alan Yuille, and Yingwei Li. Context-enhanced stereo transformer. In European conference on computer vision, pages 263 279. Springer, 2022. [21] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo netIn Proceedings of the IEEE/CVF conference on work. computer vision and pattern recognition, pages 3273 3282, 2019. [22] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in neural information processing systems, 35: 2298222994, 2022. [23] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation In Proceedings of the IEEE conference on networks. computer vision and pattern recognition, pages 7132 7141, 2018. [24] Yaoyu Hu, Wenshan Wang, Huai Yu, Weikun Zhen, and Sebastian Scherer. Orstereo: Occlusion-aware recurrent In 2021 stereo matching for 4k-resolution images. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 56715678. IEEE, 2021. [25] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba: Visual state space arXiv preprint model with windowed selective scan. arXiv:2403.09338, 2024. [26] Junpeng Jing, Jiankun Li, Pengfei Xiong, Jiangyu Liu, Shuaicheng Liu, Yichen Guo, Xin Deng, Mai Xu, Lai Jiang, and Leonid Sigal. Uncertainty guided adaptive warping for robust and efficient stereo matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 33183327, 2023. [27] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. CVPR, 2023. [28] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE international conference on computer vision, pages 66 75, 2017. [29] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1626316272, 2022. [30] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1626316272, 2022. [31] Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-nd: Selective state space modeling for multidimensional data. In European Conference on Computer Vision, pages 7592. Springer, 2024. [32] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models arXiv preprint great on long sequence modeling? arXiv:2210.09298, 2022. [33] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis Creighton, Russell Taylor, and Mathias Unberath. Revisiting stereo depth estimation from sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 61976206, 2021. [34] Zhaohuai Liang and Changhe Li. Any-stereo: Arbitrary scale disparity estimation for iterative stereo matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 33333341, 2024. [35] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei Chen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learning for disparity estimation through feature conthe IEEE conference on stancy. computer vision and pattern recognition, pages 2811 2820, 2018. In Proceedings of [36] Haotong Lin, Sili Chen, Jun Hao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, and Bingyi Kang. Depth anything 3: Recovering the visual space from any views. arXiv preprint arXiv:2511.10647, 2025. [37] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: Multilevel recurrent field transforms for stereo matching. In 2021 International Conference on 3D Vision (3DV), pages 218227. IEEE, 2021. [38] Xiao Liu, Chenxu Zhang, Fuxiang Huang, Shuyin Xia, Guoyin Wang, and Lei Zhang. Vision mamba: comprehensive survey and taxonomy. IEEE Transactions on Neural Networks and Learning Systems, 2025. [39] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan Liu. Vmamba: Visual state space model. Advances in neural information processing systems, 37: 103031103063, 2024. [40] Zihua Liu, Yizhou Li, and Masatoshi Okutomi. Global occlusion-aware transformer for robust stereo matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 35353544, 2024. [41] Jieming Lou, Weide Liu, Zhuo Chen, Fayao Liu, and Jun Cheng. Elfnet: Evidential local-global fusion for stereo matching. arXiv preprint arXiv:2308.00728, 2023. [42] Qingxuan Lv, Junyu Dong, Yuezun Li, Sheng Chen, Hui Yu, Shu Zhang, and Wenhan Wang. Uwstereo: large synthetic dataset for underwater stereo matching. IEEE Transactions on Circuits and Systems for Video Technology, 2025. [43] Yamin Mao, Zhihua Liu, Weiming Li, Yuchao Dai, Qiang Wang, Yun-Tae Kim, and Hong-Seok Lee. Uasnet: Uncertainty adaptive sampling network for deep stereo matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 63116319, 2021. [44] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4040 4048, 2016. [45] Badri Patro and Vijay Agneeswaran. Simba: Simplified mamba-based architecture for vision and multivariate time series. arXiv preprint arXiv:2403.15360, 2024. [46] Xiaohuan Pei, Tao Huang, and Chang Xu. Efficientvmamba: Atrous selective scan for light weight visual mamba. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 64436451, 2025. [47] Yelena Randall. Flsea: Underwater visual-inertial and stereo-vision forward-looking datasets. Masters thesis, University of Haifa (Israel), 2023. [48] Zeyu Ren, Zeyu Zhang, Wukai Li, Qingxiang Liu, and Hao Tang. Anydepth: Depth estimation made easy. arXiv e-prints, pages arXiv2601, 2026. [49] Thomas Schops, Johannes L. Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multi-camera videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Orazio Gallo, and Stan Birchfield. Foundationstereo: Zero-shot stereo matching. CVPR, 2025. [50] Akihito Seki and Marc Pollefeys. Sgm-nets: SemiIn Proceedings global matching with neural networks. of the IEEE conference on computer vision and pattern recognition, pages 231240, 2017. [51] Zhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascade and fused cost volume for robust stereo matchIn Proceedings of the IEEE/CVF conference on ing. computer vision and pattern recognition, pages 13906 13915, 2021. [52] Zhelun Shen, Yuchao Dai, Xibin Song, Zhibo Rao, Dingfu Zhou, and Liangjun Zhang. Pcw-net: Pyramid combination and warping cost volume for stereo matching. In European conference on computer vision, pages 280297. Springer, 2022. [53] Xiao Song, Xu Zhao, Hanwen Hu, and Liangji Fang. Edgestereo: context integrated residual pyramid netIn Asian conference on work for stereo matching. computer vision, pages 2035. Springer, 2018. [54] Aristotle Spyropoulos, Nikos Komodakis, and Philippos Mordohai. Learning to detect ground control points In for improving the accuracy of stereo matching. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16211628, 2014. [55] Qing Su and Shihao Ji. Chitransformer: Towards reliable the IEEE/CVF stereo from cues. conference on computer vision and pattern recognition, pages 19391949, 2022. In Proceedings of [56] Fabio Tosi, Filippo Aleotti, Pierluigi Zama Ramirez, Matteo Poggi, Samuele Salti, Stefano Mattoccia, and IEEE Luigi Di Stefano. Neural disparity refinement. Transactions on Pattern Analysis and Machine Intelligence, 46(12):89008917, 2024. [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [58] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. in 2020 ieee. In RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. [59] Xianqi Wang, Gangwei Xu, Hao Jia, and Xin Yang. Selective-stereo: Adaptive frequency information sethe lection for stereo matching. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1970119710, 2024. In Proceedings of [60] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow. In ICCV, 2023. [61] Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, [62] Zhengri Wu, Yiran Wang, Yu Wen, Zeyu Zhang, Biao Wu, and Hao Tang. Stereoadapter: Adapting stereo depth estimation to underwater scenes. arXiv preprint arXiv:2509.16415, 2025. [63] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Iterative geometry encoding volume for stereo Yang. the IEEE/CVF confermatching. ence on computer vision and pattern recognition, pages 2191921928, 2023."
        },
        {
            "title": "In Proceedings of",
            "content": "[64] Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, and Xin Yang. Accurate and efficient stereo matching via IEEE Transactions on attention concatenation volume. Pattern Analysis and Machine Intelligence, 2023. [65] Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Junda Cheng, Chunyuan Liao, and Xin Yang. Igev++: Iterative multi-range geometry encoding volumes for stereo IEEE Transactions on Pattern Analysis and matching. Machine Intelligence, 2025. [66] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggregation network for efficient stereo matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19591968, 2020. [67] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via the IEEE/CVF global matching. conference on computer vision and pattern recognition, pages 81218130, 2022. In Proceedings of [68] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying IEEE Transactions flow, stereo and depth estimation. on Pattern Analysis and Machine Intelligence, 45(11): 1394113958, 2023. [69] Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, and Elliot Crowley. Plainmamba: Improving non-hierarchical mamba in visual recognition. arXiv preprint arXiv:2403.17695, 2024. [70] Gengshan Yang, Joshua Manela, Michael Happold, and Deva Ramanan. Hierarchical deep stereo matching on high-resolution images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55155524, 2019. [71] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [72] Menglong Yang, Fangrui Wu, and Wei Li. Waveletstereo: Learning wavelet coefficients of disparity map In Proceedings of the IEEE/CVF in stereo matching. conference on computer vision and pattern recognition, pages 1288512894, 2020. [73] Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6044 6053, 2019. [74] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvssolver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. [75] Jure Zbontar and Yann LeCun. Computing the stereo In matching cost with convolutional neural network. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15921599, 2015. [76] Jure ˇZbontar and Yann LeCun. Stereo matching by training convolutional neural network to compare image patches. Journal of Machine Learning Research, 17(65): 132, 2016. [77] Jiaxi Zeng, Chengtang Yao, Lidong Yu, Yuwei Wu, and Yunde Jia. Parameterized cost volume for stereo matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1834718357, 2023. [78] Fan Zhang, Shaodi You, Yu Li, and Ying Fu. Atlantis: Enabling underwater depth estimation with stable diffuIn Proceedings of the IEEE/CVF Conference on sion. Computer Vision and Pattern Recognition, pages 11852 11861, 2024. [79] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip HS Torr. Ga-net: Guided aggregation net for endto-end stereo matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 185194, 2019. [80] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Yong Zhao, Yitong Yang, and Ting Ouyang. Eai-stereo: Error aware iterative network for stereo matching. In Proceedings of the Asian conference on computer vision, pages 315332, 2022. [81] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen, Yitong Yang, and Yong Zhao. High-frequency the stereo matching network. IEEE/CVF conference on computer vision and pattern recognition, pages 13271336, 2023. In Proceedings of [82] Zhengming Zhou and Qiulei Dong. Two-in-one depth: Bridging the gap between monocular and binocuarXiv preprint lar self-supervised depth estimation. arXiv:2309.00933, 2023. [83] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional arXiv preprint arXiv:2401.09417, state space model. 2024. [84] Lvwei Zhu, Ying Gao, Jiankai Zhang, Yongqing Li, and Xueying Li. Reliable and effective stereo matching for underwater scenes. Remote Sensing, 16(23):4570, 2024."
        },
        {
            "title": "APPENDIX",
            "content": "Figure 7 shows qualitative comparisons of zero-shot stereo depth estimation on the SQUID dataset. Compared to existing methods, our approach produces more coherent disparity maps with clearer object boundaries and fewer artifacts in textureless and low-contrast underwater regions. Figure 8 shows qualitative results obtained on real-world robotic platform. The proposed method demonstrates stable and consistent depth predictions under real underwater conditions. Figure 9 shows qualitative zero-shot stereo depth estimation results on the TartanAir Ocean dataset. Our method preserves fine-grained structural details and large-disparity regions more effectively, indicating strong generalization under diverse underwater appearances. Figure 10 shows representative samples from the proposed UW-StereoDepth-80K dataset. The dataset covers diverse underwater scenes, baselines, providing rich supervision for large-scale underwater stereo adaptation. Fig. 7: Qualitative results of zero-shot stereo depth estimation for different models on the SQUID dataset. Fig. 8: Qualitative results of zero-shot stereo depth estimation for different models on the robot platform. Fig. 9: Qualitative results of zero-shot stereo depth estimation for different models on the Tartanair Ocean dataset Fig. 10: Visualization of UW-StereoDepth-80K"
        }
    ],
    "affiliations": [
        "Australian Centre for Robotics",
        "Peking University",
        "The University of Melbourne"
    ]
}