{
    "paper_title": "Generating Physically Stable and Buildable LEGO Designs from Text",
    "authors": [
        "Ava Pun",
        "Kangle Deng",
        "Ruixuan Liu",
        "Deva Ramanan",
        "Changliu Liu",
        "Jun-Yan Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/."
        },
        {
            "title": "Start",
            "content": "Generating Physically Stable and Buildable LEGO Designs from Text Ava Pun* Kangle Deng* Ruixuan Liu* Deva Ramanan Changliu Liu Jun-Yan Zhu"
        },
        {
            "title": "Carnegie Mellon University",
            "content": "5 2 0 2 8 ] . [ 1 9 6 4 5 0 . 5 0 5 2 : r Figure 1. Overview of LEGOGPT. (a) Our method generates physically stable LEGO structures from text descriptions through an end-to-end approach, showing intermediate brick-by-brick steps. (b) The generated designs are buildable both by hand and by automated robotic assembly. (c) We show example results with corresponding text prompts. Besides basic LEGO designs (top), our method can generate colored LEGO models (bottom right) and textured models (bottom left) with appearance descriptions. We highly recommend the reader to check our website for step-by-step videos."
        },
        {
            "title": "Abstract",
            "content": "We introduce LEGOGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly con- *Indicates equal contribution. 1 straints. Our experiments show that LEGOGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/. 1. Introduction 3D generative models have made remarkable progress, driven by advances in generative modeling [15, 65] and neural rendering [26, 52]. These models have enabled various applications in virtual reality, gaming, entertainment, and scientific computing. For example, several works have explored synthesizing 3D objects from text [58], adding texture to meshes [9, 61], and manipulating the shape and appearance of existing 3D objects and scenes [19, 40]. However, applying existing methods to create real-world objects remains challenging. Most approaches focus on generating diverse 3D objects with high-fidelity geometry and appearance [21, 85], but these digital designs often cannot be physically realized due to two key challenges [46]. First, the objects may be difficult to assemble or fabricate using standard components. Second, the resulting structure may be physically unstable even if assembly is possible. Without proper support, parts of the design can collapse, float, or remain disconnected. In this work, we address the challenge of generating physically realizable objects and study this problem in the context of LEGO design. LEGO is widely used in entertainment, education, artistic creation, and manufacturing prototyping. Additionally, it can serve as reproducible research benchmark, as all standard components are readily available. Due to the significant effort required for manual design, recent studies have developed automated algorithms to streamline the process and generate compelling results. However, existing approaches primarily create LEGO designs from given 3D object [45] or focus on single object category [12, 13]. Our goal is to develop method for generating LEGO designs directly from freeform text prompts while ensuring physical stability and buildability. Specifically, we aim to train generative model that produces designs that are: Physically stable: Built on LEGO baseplate with strong structural integrity, without floating or collapsing bricks. Buildable: Compatible with standard LEGO pieces and able to be assembled brick-by-brick by humans or robots. In this work, we introduce LEGOGPT with the key insight of repurposing autoregressive large language models, originally trained for next-token prediction, for next-brick prediction. We formulate the problem of LEGO design as an autoregressive text generation task, where the next-brick dimension and placement are specified with simple textual format. To ensure generated structures are both stable and buildable, we enforce physics-aware assembly constraints during both training and inference. During training, we construct large-scale dataset of physically stable LEGO designs paired with captions. During autoregressive inference, we enforce feasibility with an efficient validity check and physics-aware rollback to ensure that the final tokens adhere to physics laws and assembly constraints. Our experiments show that the generated designs are stable, diverse, and visually appealing while adhering to input text prompts. Our method outperforms pre-trained LLMs with and without in-context learning, and previous approaches based on mesh-based 3D generation. Finally, we explore applications such as text-driven LEGO texturing, as well as manual assembly and automated robotic assembly of our designs. Our dataset, code, and models are available at the project website: https://avalovelace1. github.io/LegoGPT/. 2. Related Work Text-to-3D Generation. Text-to-3D generation has seen remarkable progress in recent years, driven by advances in neural rendering and generative models. Dreamfusion [58] and Score Jacobian Chaining [76] pioneer zero-shot textto-3D generation by optimizing neural radiance fields [52] with pre-trained diffusion models [62]. Subsequent work has explored alternative 3D representations [3, 33, 35, 42, 48, 51, 66] and improved loss functions [25, 44, 47, 74, 77, 83]. Rather than relying on iterative optimization, promising alternative direction trains generative models directly on 3D asset datasets, with various backbones including diffusion models [20, 32, 34, 54, 60, 63, 85, 86, 89], large reconstruction models [21, 31, 73, 81], U-Nets [36, 68], and autoregressive models [46, 18, 55, 64, 67, 79]. However, these existing methods cannot be directly applied to LEGO generation because they do not account for the unique physical constraints and assembly requirements of real-world object designs [46]. Our work bridges this gap by introducing framework for generating physically stable and buildable LEGO designs directly from text prompts. Autoregressive 3D Modeling. Recent research has successfully used autoregressive models to generate 3D meshes [46, 18, 55, 64, 67, 79], often conditioned on input text or images. Most recently, LLaMA-Mesh [78] demonstrates that large language models (LLMs) can be fine-tuned to output 3D shapes in plain-text format, given text prompt. However, most existing autoregressive methods focus on mesh generation. In contrast, we focus on generating LEGO designs from text prompts, leveraging LLMs reasoning capabilities. LEGO Assembly and Generation. Creating LEGO designs given reference 3D shape has been widely studied [28]. Existing works [57, 69, 88] formulate the generation as an optimization problem guided by hand-crafted heuristic rules. Such heuristics can include ensuring that all bricks are interconnected, minimizing the number of bricks, and maximizing the number of brick orientation alternations. Luo et al. [45] leverages structural stability estimation to find weak structural parts and rearrange the local brick layout to generate physically stable designs. Kim et al. [27], Liu 2 Figure 2. StableText2Lego Dataset. (a) From ShapeNetCore [2] mesh, we generate LEGO design by voxelizing it onto 20 20 20 grid and applying legolization to determine the brick layout. (b) We augment each shape with multiple structural variations by randomizing the brick layout while preserving the overall shape. (c) Stability analysis [37] is performed on each variation to filter out physically unstable designs. (d) To obtain the corresponding captions for each shape, we render the LEGO design from 24 different viewpoints and use GPT-4o [1] to generate detailed geometric descriptions. (e) Data samples from 5 categories in our StableText2Lego dataset. et al. [39] formulate planning problem to fill the target 3D model sequentially. However, these methods only generate designs given an input 3D shape, assuming valid LEGO design exists, which is difficult to verify in practice. Few works have explored learning-based techniques to generate LEGO designs. Thompson et al. [70] use deep graph generative model in which the graph encodes brick connectivity. However, this method is limited to generating simple classes, such as walls and cuboids, using single brick type. More recently, Ge et al. [13] use diffusion model to predict semantic volume, which is then translated into high-quality micro building. Their method produces impressive results for single category. Zhou et al. [87] and Ge et al. [12] generate compelling figurine designs given an input portrait. They use machine learning to select from pre-made set of components that best match an input photo. Although effective for faces, extending this selection-based approach to arbitrary objects is challenging. Zhou et al. [90] formulate an optimization problem to create LEGO model from an input image. While their output is 2D LEGO mosaic, we focus on 3D structures in this work. Goldberg et al. [14] queries vision-language model (VLM) to generate diverse 3D assembly structures. However, they use regular building blocks instead of LEGO bricks, which do not have interlocking connections and thus have limited expressiveness. Our goal is closest to that of [30]. This work has three steps: (1) generating an image using text-to-image model, (2) converting the image into voxels, and (3) using heuristics to create physical LEGO build without considering physical constraints. In contrast, our method performs the text-to-LEGO task without requiring intermediate image or voxel representations. Physics-Aware Generation. Despite significant advances in 3D generation, incorporating physics constraints remains largely underexplored; most approaches focus primarily on geometry or appearance modeling. Physics-aware generation can be broadly categorized into two approaches: direct constraint enforcement and learned validation. Simple physical constraints, such as collision avoidance and contact requirements, can be incorporated directly through explicit penalty terms during optimization [16, 24, 41, 53, 75, 82, 84]. More complex physical properties, such as structural stability and dynamic behavior, typically require differentiable physics simulators [7, 50, 56, 80] or data-driven physics-aware assessment models [10, 49]. To our knowledge, our paper is the first attempt to incorporate physics-aware constraints into text-based LEGO generation. 3. Dataset Training modern autoregressive model requires largescale dataset. Therefore, we introduce StableText2Lego, new large-scale LEGO dataset that contains 47,000+ different LEGO structures, covering 28,000+ unique 3D objects from 21 common object categories of the ShapeNetCore dataset [2]. We select categories featuring diverse and distinctive 3D objects while excluding those resembling cuboids. Each structure is paired with group of text descriptions and stability score, which indicates its physical stability and buildability. Below, we describe the dataset construction, an overview of which is given in Figure 2. LEGO Representation. We consider LEGO structures built on LEGO baseplate. Each LEGO structure in StableText2Lego is represented as = [b1, b2, . . . , bN ] with bricks, and each element denotes bricks state as 3 Figure 3. Method. (a) Our system tokenizes LEGO design into sequence of text tokens, ordered in raster-scan manner from bottom to top. (b) We create an instruction dataset pairing brick sequences with descriptions to fine-tune LLaMA-3.2-Instruct-1B. (c) At inference time, LEGOGPT generates LEGO designs incrementally by predicting one brick at time given text prompt. For each generated brick, we perform validity checks to ensure it is well-formatted, exists in our brick library, and does not collide with existing bricks. After completing the design, we verify its physical stability. If the structure is unstable, we roll back to the stable state by removing all the unstable bricks and their subsequent ones and resume generation from that point. bi = [hi, wi, xi, yi, zi]. Here, hi and wi indicate the brick length in the and directions, respectively, and xi, yi, and zi denote the position of the stud closest to the origin. The position has xi [0, 1, . . . , 1], yi [0, 1, . . . , 1], zi [0, 1, . . . , 1], and H, , and represent the dimensions of the discretized grid world. Mesh-to-LEGO. We construct the dataset by converting 3D shapes from ShapeNetCore [2] into LEGO structures as shown in Figure 2(a). Given 3D mesh, we voxelize and downsample it to 20 20 20 grid world to ensure consistent scale, i.e., = = = 20. The LEGO brick layout is generated by split-and-remerge legolization algorithm. To improve data quality and diversity, we introduce randomness in legolization and generate multiple different LEGO structures for the same 3D object, as illustrated in Figure 2(b). This increases the chance of obtaining stable LEGO structure and more diverse LEGO brick layouts. More details can be found in Appendix A. We use eight commonly available standard bricks: 1 1, 1 2, 1 4, 1 6, 1 8, 2 2, 2 4, and 2 6. Stability Score. We assess the physical stability of each structure, as illustrated in Figure 2(c), using the analysis method [37]. For structure = [b1, b2, . . . , bN ], the stability score RN assigns each brick bi value si [0, 1] that quantifies the internal stress at its connections. Higher scores si indicate greater stability, while si = 0 indicates an unstable brick that will cause structural failure. Calculating the stability score requires solving nonlinear program to determine the forces acting on each brick to achieve static equilibrium that prevents structural collapse, as detailed in Section 4.2. For typically-sized (i.e., < 200 bricks) structures in Figure 2, it takes 0.35 seconds on average. We only include stable structures where all bricks have stability scores greater than 0. Caption Generation. To obtain captions for each shape, we render the LEGO from 24 different viewpoints and combine them into single multi-view image. We then prompt GPT-4o [1] to produce five descriptions for these renderings with various levels of detail. Importantly, we ask GPT-4o to omit color information and focus only on geometry. The complete GPT-4o prompt is provided in Appendix A. Figure 2(e) shows several data samples in StableText2Lego. The rich variations within each category and the comprehensive text-LEGO pairs make it possible to train large-scale generative models. More insights on our StableText2Lego are discussed in Appendix A. 4. Method Here, we introduce LEGOGPT, method for generating physically stable LEGO designs from text prompts. Leverag4 ing LLMs ability to model sequences and understand text, we fine-tune pre-trained LLM for the LEGO generation task (Section 4.1). To increase the stability and buildability of our designs, we use brick-by-brick rejection sampling and physics-aware rollback during inference (Section 4.2). Figure 3 illustrates an overview of our method. 4.1. Model Fine-tuning Pre-trained LLMs excel at modeling sequences and understanding natural language, making them promising candidates for our task. We further fine-tune pre-trained LLM on custom instruction dataset containing text prompts and their corresponding LEGO structures from StableText2Lego. Pre-trained Base Model. We use LLaMA-3.2-1BInstruct [11] as our base model. This model is fine-tuned to give coherent answers to instruction prompts, making it suitable for text-based LEGO design generation. As shown in Figure 5, the base model can generate LEGO-like designs through in-context learning, highlighting the promise of using pre-trained LLMs for our task. However, the generated designs often omit certain object parts and contain intersecting or disconnected bricks, making them physically unstable and unbuildable. To address these issues, we further fine-tune the pre-trained model using our dataset. Instruction Fine-tuning Dataset. For each stable design and its corresponding captions, we construct an instruction in the following format: (user) Create LEGO model of {caption}. (assistant) {lego-design}. To simplify training and reuse LLaMAs tokenizer, we represent LEGO designs in plain text. But what format should we use? The standard LEGO design format LDraw [29] has two main drawbacks. First, it does not directly include brick dimensions, which are crucial for assessing the structure and validating brick placements. Second, it contains unnecessary information, such as brick orientation and scale. This information is redundant, as each axis-aligned brick has only two valid orientations. Instead of using LDraw, we introduce custom format to represent each LEGO design. Each line of our format represents one LEGO brick as {h}{w} ({x},{y},{z}), where hw are brick dimensions and (x, y, z) are its coordinates. All bricks are 1-unit-tall, axis-aligned cuboids, and the order of and encodes the bricks orientation about the vertical axis. This format significantly reduces the number of tokens required to represent design while including brick dimension information essential for 3D reasoning. Bricks are ordered in raster-scan manner from bottom to top. With our fine-tuned model LEGOGPT θ, we predict the Figure 4. Force Model. (a) We consider all forces exerted on single brick, including gravity (black), vertical forces with the top brick (red/blue) and bottom brick (green/purple), and horizontal (shear) forces due to knob connections (cyan), and adjacent bricks (yellow). (b) The structural force model extends the individual force model to multiple bricks. Solving for static equilibrium in determines each bricks stability score. bricks b1, b2, ..., bN in an autoregressive manner: p(b1, b2, ..., bN θ) = (cid:89) i= p(bib1, ..., bi1, θ). (1) 4.2. Integrating Physical Stability Although trained on physically stable data, our model sometimes generates designs that violate physics and assembly constraints. To address this issue, we further incorporate physical stability verification into autoregressive inference. LEGO structure is considered physically stable and buildable if it does not collapse when built on baseplate. To this end, we assess physical structural stability using the stability analysis method [37]. Here, we briefly overview this method below. Figure 4(a) illustrates all possible forces exerted on single brick. We extend the single brick model and derive the structural force model F, which consists of set of candidate forces (e.g., pulling, pressing, supporting, dragging, normal, etc.), as demonstrated in Figure 4(b). For LEGO structure = [b1, b2, . . . , bN ], each brick bi has Mi candidate forces Fi, [1, Mi]. structure is stable if all bricks can reach static equilibrium, i.e., Mi(cid:88) F = 0, Mi(cid:88) τ = Mi(cid:88) j Lj = 0, (2) where Lj denotes the force lever corresponding to . The stability analysis is formulated into nonlinear program as arg min (cid:40) (cid:88) Mi(cid:88) Mi(cid:88) + τ +αDmax + β (cid:88) (cid:41) , Di (3) subject to three constraints: 1) all force candidates in should take non-negative values; 2) certain forces exerted on the same brick cannot coexist, e.g., the pulling (red arrow) and pressing (blue arrow), the dragging (green arrow) and supporting (purple arrow); 3) Newtons third law, e.g., at given connection point, the supporting force on the upper brick should be equal to the pressing force on the bottom brick. Di Fi is the set of candidate dragging forces (green arrow) on bi. α and β are hyperparameter weights. Solving the above nonlinear program in Eqn. 3 using Gurobi [17] finds force distribution that drives the structure to static equilibrium with the minimum required internal stress, suppressing the overall friction (i.e., (cid:80) Di) as well as avoiding extreme values (i.e., Dmax ). From the force distribution F, we obtain the per-brick stability score as ALGORITHM 1: LEGOGPT inference algorithm. Input: Text prompt c; Autoregressive model θ Output: LEGO design following the text prompt 1 empty LEGO structure; 2 loop /* Predict next brick w/ rejection sampling */ for = 1, . . . , max_rejections do context B.to_text_format(); θ.predict_tokens(context) (Eqn. 1); if is valid then break; end B.add_brick(b); if contains EOF then // Structure complete if is stable or max rollbacks exceeded then return B; while is unstable do // Rollback if unstable indices of unstable bricks in B; min I; // idx of 1st unstable brick [b1, . . . , bi1]; end 3 4 6 7 8 9 10 12 13 14 15 si = 0 FT Dmax FT (cid:80)Mi (cid:80)Mi Dmax otherwise, = 0 τ = 0 > FT , end 16 17 end (4) = 0 (cid:80)Mi where FT is measured constant friction capacity between brick connections. Higher scores si indicate greater stability, while si = 0 indicates an unstable brick that will cause structural failure: either cannot reach static equilibrium ((cid:80)Mi τ = 0) or the required friction exceeds the friction capacity of the material (Dmax > FT ). Due to the equality constraints imposed by Newtons third law, Eqn. 3 includes only the dragging forces and excludes pulling forces. For physically stable structure, we need si > 0, [1, ]. i When to apply stability analysis? Our model generates LEGO designs sequentially, one LEGO brick at time. straightforward approach to ensuring physical stability is to apply stability analysis to each step and resample brick that would cause collapse. However, this step-by-step validation, though efficient per check, could be time-consuming due to the large number of checks required. More importantly, many LEGO designs are unstable when partially constructed, yet become stable when fully assembled. Adding stability check after each brick generation could overly constrain the model exploration space. Instead, we propose brick-by-brick rejection sampling combined with physicsaware rollback to balance stability and diversity. Brick-by-Brick Rejection Sampling To improve inference speed and avoid overly constraining the model generation, we relax our constraints during inference. First, when the model generates LEGO brick and its position, the brick should be well-formatted (e.g., available in the inventory) and not lie outside the workspace. Second, we ensure that newly added bricks do not collide with the existing 6 structure. Formally, for each generated brick bt, we have Vt Vi = , [1, 1], where Vi denotes the voxels occupied by bi. These heuristics allow us to efficiently generate well-formatted LEGO structures without explicitly considering complex physical stability. To integrate these heuristics, we use rejection sampling: if brick violates the heuristics, we resample new brick from the model. Due to the relaxed constraints, most bricks are valid, and rejection sampling does not significantly affect inference time. Physics-Aware Rollback To ensure that the final design = [b1, b2, . . . , bN ] is physically stable, we calculate the stability score S. If the resulting design is unstable, i.e., si = 0, I, we roll back the design to the state before the first unstable brick was generated, i.e., = [b1, b2, . . . , bmin I1]. Here, is the set of the indices of all the unstable bricks. We repeat this process iteratively until we reach stable structure B, and continue generation from the partial structure B. Note that we can use the per-brick stability score to efficiently find the collapsing bricks and their corresponding indices in the sequence. We summarize our inference sampling in Algorithm 1. 4.3. LEGO Texturing and Coloring While our primary focus is on generating LEGO shapes, color and texture play critical role in creative LEGO designs. Therefore, we propose method that applies detailed UV textures or assigns uniform colors to individual bricks. UV Texture Generation. Given structure and its corresponding mesh M, we first identify the set of occluded bricks Bocc that have all six faces covered by adjacent bricks, and remove Bocc for efficiency. The remaining bricks Table 1. Quantitative Analysis. We evaluate our method against several baselines on validity (no out-of-library, out-of-bounds, or colliding bricks), stability, and CLIP-based text similarity. Stability and CLIP score are computed over valid designs only. For LLaMA-Mesh [78], validity requires well-formed OBJ file. Our method outperforms all baselines as well as the ablated setups on validity and stability using our proposed rejection sampling and rollback, while maintaining high text similarity."
        },
        {
            "title": "Method",
            "content": "% valid % stable mean brick stability min brick stability CLIP Pre-trained LLaMA (0-shot) In-context learning (5-shot) LLaMA-Mesh [78] LGM [68] XCube [60] Hunyuan3D-2 [86] Ours w/o rejection sampling or rollback Ours w/o rollback Ours (LEGOGPT) 0.0% 2.4% 94.8% 100% 100% 100% 37.2% 100% 100% 0.0% 1.2% 50.8% 25.2% 75.2% 75.2% 12.8% 24.0% 98.8% N/A 0.675 0.894 0.942 0.964 0.973 0.956 0. 0.996 N/A 0.479 0.499 0.231 0.686 0.704 0.325 0.228 0.915 N/A 0.284 0.311 0.299 0.322 0.324 0.329 0. 0.324 Figure 5. Result gallery and baseline comparisons. Our method can generate high-quality, diverse, and novel LEGO designs aligned with the given text prompts. Black bricks are colliding. For LLaMA-Mesh [78], LGM [68], XCube [60], and Hunyuan3D-2 [86], an inset of the generated mesh before legolization is shown. 7 Figure 6. Ablation study. Brick-by-brick rejection sampling and physics-informed rollback help to ensure that the generated LEGO design is both valid and stable. Black indicates colliding bricks. Bvis = Bocc are merged into single mesh with cleaned overlapping vertices using ImportLDraw [72]. We generate UV map UVM by cube projection. The texture map Itexture is then generated using FlashTex [9], fast textbased mesh texturing approach: Itexture = FlashTex(M, UVM, c), (5) where text prompt describes the visual appearance. This texture can be applied through UV printing or stickers. Uniform Brick Color Assignment. We can also assign each brick uniform color from the LEGO color library. Given structure B, we convert it to voxel grid and then to UV-unwrapped mesh MV . For every voxel V, let , = 1, . . . , Nv be its visible faces where 0 Nv 6. Each face is split into two triangles and mapped to UV region , creating mesh MV with UV map UVV . We apply FlashTex [9] to generate texture Itexture: Itexture = FlashTex(MV , UVV , c). (6) The color of each voxel C(v) R3 is computed as: C(v) = 1 Nv Nv(cid:88) i=1 C(f ), V, (7) (cid:80) ) = 1 where C(f Itexture(x, y) is the color of (x,y)S i each visible face , and i represents the number of pixels in region in the UV map. For each brick bt and its constituent voxels Vt, we compute the brick color C(bt) = 1 C(v). Finally, we find the closest color in the Vt color set. While UV texturing offers higher-fidelity details, uniform coloring allows us to use standard LEGO bricks. vVt (cid:80) Figure 7. Textured and Colored LEGO Generation. Our method can generate diverse textured (top two rows) and colored (bottom) LEGO models based on the same shape while using different appearance text prompts. 5. Experiments 5.1. Implementation Details Fine-tuning. Our fine-tuning dataset contains 240k distinct prompts and 47k+ distinct LEGO designs. We use 90% of the data for training and 10% for evaluation. For efficiency, we include samples only up to 4096 tokens in length. Training details are provided in Appendix A. Inference. To evaluate our method, we generate one LEGO design for each of 250 prompts randomly selected from the validation dataset. The nonlinear optimization in Eqn. 3 is solved using Gurobi [17]. We set FT = 0.98N with α = 103 and β = 106. We allow up to 100 physics-aware rollbacks before accepting the LEGO design. The median number of required rollbacks is 2, and the median time to generate one LEGO design is 40.8 seconds. 5.2. LEGO Generation Results Figure 5 shows gallery of diverse, high-quality LEGO designs that closely follow the input prompts. Baseline Comparisons. As baselines, we use LLaMAMesh [78], LGM [68], XCube [60], and Hunyuan3D-2 [86] to generate mesh from each prompt, and then convert the meshes to LEGO via legolization. Additionally, we compare our method with pre-trained models evaluated in both zero-shot and few-shot manner. For few-shot evaluation, we 8 Figure 8. Automated Assembly. Demonstrations of automated assembly of generated LEGO structures using robots. provide the model with 5 examples of stable LEGO designs and their captions. We cannot run comparisons with Lennon et al. [30] as the code has not been released. For each method, we compute the proportion of stable and valid structures among the generated designs. Additionally, for each valid structure, we compute its mean and minimum brick stability scores. To evaluate prompt alignment, we compute the CLIP score [59] between rendered image of each valid structure and the text LEGO model of {prompt}. As shown in Table 1, our method outperforms all baselines in these metrics. Figure 5 shows that our method generates LEGO structures of higher quality than the baselines. Ablation Study. We demonstrate the importance of rejection sampling and physics-aware rollback. As seen in Figure 6, rejection sampling eliminates invalid bricks, such as those with collisions, while rollback helps to ensure the final model is physically stable. The quantitative results in Table 1 show that our full method generates higher proportion of valid and stable LEGO designs, while closely following the text prompts. 5.3. Extensions and Applications Robotic Assembly of Generated LEGO. We demonstrate automated assembly using dual-robot-arm system in Figure 8. The robots use the manipulation policy [38] and the asynchronous multi-agent planner [23] to manipulate LEGO bricks and construct the structure. More details are included in Appendix B. Since the generated structures are physically stable, efficient and automated assembly can be performed. Manual Assembly. Our generated LEGO structures are physically valid and can be assembled by hand. See Appendix for details. Textured and Colored LEGO Model Generation. Figure 7 shows both UV texturing and uniform coloring results of LEGO models, demonstrating our methods ability to 9 generate diverse styles while preserving the underlying geometry. 6. Discussion and Limitations In this work, we have introduced LEGOGPT, an autoregressive model for generating LEGO designs from text prompts. Our method learns to predict the next brick sequentially while ensuring physical stability and buildability. We have shown that our method outperforms LLM backbone models and several recent text-to-3D generation methods. Limitations. Though our method outperforms existing methods, it still has several limitations. First, due to limited computational resources, we have not explored the largest 3D dataset. As result, our method is restricted to generating designs within 20 20 20 grid across 21 categories. Future work includes scaling up model training on larger, more diverse datasets, such as Objaverse-XL [8] at higher grid resolutions. Training on large-scale datasets can also improve generalization to out-of-distribution text prompts. Second, our method currently supports fixed set of commonly used LEGO bricks. In future work, we plan to expand the brick library to include broader range of dimensions and brick types, such as slopes and tiles, allowing for more diverse and intricate LEGO designs. Acknowledgments. We thank Minchen Li, Ken Goldberg, Nupur Kumari, Ruihan Gao, and Yihao Shi for their discussions and help. We also thank Jiaoyang Li, Philip Huang, and Shobhit Aggarwal for developing the bimanual robotic system. This work is partly supported by the Packard Foundation, Cisco Research Grant, and Amazon Faculty Award. This work is also in part supported by the Manufacturing Futures Institute, Carnegie Mellon University, through grant from the Richard King Mellon Foundation. KD is supported by the Microsoft Research PhD Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University Princeton University Toyota Technological Institute at Chicago, 2015. [3] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for highquality text-to-3D content creation. In IEEE International Conference on Computer Vision (ICCV), 2023. [4] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, Jingyi Yu, Gang Yu, Bin Fu, and Tao Chen. MeshXL: Neural coordinate field for generative 3D foundation models. arXiv preprint arXiv:2405.20853, 2024. [5] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang. MeshAnything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. [6] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. MeshAnything V2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. [7] Yunuo Chen, Tianyi Xie, Zeshun Zong, Xuan Li, Feng Gao, Yin Yang, Ying Nian Wu, and Chenfanfu Jiang. Atlas3D: Physically constrained self-supporting text-to-3D for simulation and fabrication. arXiv preprint arXiv:2405.18515, 2024. [8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-XL: universe of 10M+ 3D objects. In Advances in Neural Information Processing Systems (NeurIPS), pages 3579935813. Curran Associates, Inc., 2023. [9] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, and Maneesh Agrawala. FlashTex: Fast relightable mesh texturing with LightControlNet. In European Conference on Computer Vision (ECCV), 2024. [10] Yuan Dong, Qi Zuo, Xiaodong Gu, Weihao Yuan, Zhengyi Zhao, Zilong Dong, Liefeng Bo, and Qixing Huang. GPLD3D: Latent diffusion of 3D shape generative models by enforcing geometric and physical priors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Jiahao Ge, Mingjun Zhou, Wenrui Bao, Hao Xu, and ChiWing Fu. Creating LEGO figurines from single images. ACM Transactions on Graphics (TOG), 43(4):153:1153:16, 2024. [13] Jiahao Ge, Mingjun Zhou, and Chi-Wing Fu. Learn to create simple LEGO micro buildings. ACM Transactions on Graphics (TOG), 43(6):249:1249:13, 2024. [14] Andrew Goldberg, Kavish Kondap, Tianshuang Qiu, Zehan Ma, Letian Fu, Justin Kerr, Huang Huang, Kaiyuan Chen, Kuan Fang, and Ken Goldberg. Blox-Net: Generative designfor-robot-assembly using VLM supervision, physics, simulation, and robot with reset. In International Conference on Robotics and Automation (ICRA). IEEE, 2025. [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In Advances in Neural Information Processing Systems (NeurIPS), 2014. [16] Minghao Guo, Bohan Wang, Pingchuan Ma, Tianyuan Zhang, Crystal Elaine Owens, Chuang Gan, Joshua B. Tenenbaum, Kaiming He, and Wojciech Matusik. Physically compatible 3D object modeling from single image. arXiv preprint arXiv:2405.20510, 2024. [17] Gurobi Optimization, LLC. Gurobi Optimizer reference manual, 2023. [18] Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3D mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. [19] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Instruct-NeRF2NeRF: Holynski, and Angjoo Kanazawa. Editing 3D scenes with instructions. In IEEE International Conference on Computer Vision (ICCV), 2023. [20] Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3DTopia: Large text-to-3D generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234, 2024. [21] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. In International Conference on Learning Representations (ICLR), 2024. [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. [23] Philip Huang, Ruixuan Liu, Shobhit Aggarwal, Changliu Liu, and Jiaoyang Li. Apex-mr: Multi-robot asynchronous planning and execution for cooperative assembly. In Robotics: Science and Systems, 2025. [24] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusionbased generation, optimization, and planning in 3D scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [25] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. In International Conference on Learning Representations (ICLR), 2024. 10 [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3D Gaussian splatting for real-time radiance field rendering. In ACM Transactions on Graphics (TOG), 2023. [27] Jungtaek Kim, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, and Jaesik Park. Combinatorial 3D shape generation via sequential assembly. arXiv preprint arXiv:2004.07414, 2020. [28] Jae Woo Kim. Survey on automated LEGO assembly construction. 2014. [29] LDraw.org. Ldraw.org homepage, 2025. [30] Kyle Lennon, Katharina Fransen, Alexander OBrien, Yumeng Cao, Matthew Beveridge, Yamin Arefeen, Nikhil Singh, and Iddo Drori. Image2Lego: Customized LEGO set generation from images. arXiv preprint arXiv:2108.08477, 2021. [31] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3D: Fast text-to-3D with sparse-view generation and large reconstruction model. In International Conference on Learning Representations (ICLR), 2024. [32] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. DiffusionSDF: Text-to-shape via voxelized diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [33] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. SweetDreamer: Aligning geometric priors in 2D diffusion for consistent text-to-3D. arXiv preprint arXiv:2310.02596, 2023. [34] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. CraftsMan: High-fidelity mesh generation with 3D native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. [35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3D: High-resolution textto-3D content creation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [36] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. MeshFormer: High-quality mesh generation with 3D-guided reconstruction model. Advances in Neural Information Processing Systems (NeurIPS), 37: 5931459341, 2025. [37] Ruixuan Liu, Kangle Deng, Ziwei Wang, and Changliu Liu. StableLego: Stability analysis of block stacking assembly. IEEE Robotics and Automation Letters, 9(11):93839390, 2024. [38] Ruixuan Liu, Yifan Sun, and Changliu Liu. lightweight and transferable design for robust LEGO manipulation. International Symposium on Flexible Automation, page V001T07A004, 2024. [39] Ruixuan Liu, Alan Chen, Weiye Zhao, and Changliu Liu. Physics-aware combinatorial assembly sequence planning using data-free action masking. IEEE Robotics and Automation Letters, 10(5):48824889, 2025. [40] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance fields. In IEEE International Conference on Computer Vision (ICCV), pages 57735783, 2021. [41] Xueyi Liu, Bin Wang, He Wang, and Li Yi. Few-shot physically-aware articulated mesh generation via hierarchical deformation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [42] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3D: Single image to 3D using cross-domain diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. [44] Artem Lukoianov, Haitz Sáez de Ocáriz Borde, Kristjan Greenewald, Vitor Campagnolo Guizilini, Timur Bagautdinov, Vincent Sitzmann, and Justin Solomon. Score distillation via reparametrized DDIM. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [45] Sheng-Jie Luo, Yonghao Yue, Chun-Kai Huang, Yu-Huan Chung, Sei Imai, Tomoyuki Nishita, and Bing-Yu Chen. Legolization: optimizing LEGO designs. ACM Transactions on Graphics (TOG), 34(6), 2015. [46] Liane Makatura, Michael Foshey, Bohan Wang, Felix HähnLein, Pingchuan Ma, Bolei Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal Elaine Owens, Peter Yichen Chen, et al. How can large language models help humans in design and manufacturing? arXiv preprint arXiv:2307.14377, 2023. [47] David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Rethinking score distillation as bridge between image distributions. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [48] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-NeRF for shape-guided generation of 3d shapes and textures. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [49] Mariem Mezghanni, Malika Boulkenafed, Andre Lieutier, and Maks Ovsjanikov. Physically-aware generative network for 3D shape modeling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [50] Mariem Mezghanni, Théo Bodrito, Malika Boulkenafed, and Maks Ovsjanikov. Physical simulation layer for accurate 3D modeling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [51] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2Mesh: Text-driven neural stylization for meshes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. [53] Vihaan Misra, Peter Schaldenbrand, and Jean Oh. ShapeShift: synthesis with arXiv preprint Towards arrangement content-aware geometric constraints. arXiv:2503.14720, 2025. text-to-shape [54] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 3D-LDM: Neural 11 implicit 3D shape generation with latent diffusion models. arXiv preprint arXiv:2212.00842, 2022. regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. [55] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter Battaglia. PolyGen: An autoregressive generative model of 3D meshes. In International Conference on Machine Learning (ICML), pages 72207229. PMLR, 2020. [56] Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, and Siyuan Huang. PhyRecon: Physically plausible neural scene reconIn Advances in Neural Information Processing struction. Systems (NeurIPS), 2024. [57] Sumiaki Ono, Alexis Andre, Youngha Chang, and Masayuki Nakajima. LEGO builder: Automatic generation of LEGO assembly manual from 3D polygon model. ITE Transactions on Media Technology and Applications, 1:354360, 2013. [58] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In International Conference on Learning Representations (ICLR), 2023. [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. [60] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. XCube: Large-scale 3D generative modeling using sparse voxel hierarchies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 42094219, 2024. [61] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3D shapes. In ACM SIGGRAPH, 2023. [62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [63] Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3D neural field generation using triplane diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [64] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. MeshGPT: Generating triangle meshes In IEEE Conference on with decoder-only transformers. Computer Vision and Pattern Recognition (CVPR), pages 1961519625, 2024. [65] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathain, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. [66] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. DreamCraft3D: Hierarchical 3D generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. [68] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. LGM: Large multi-view gaussian model for high-resolution 3D content creation. In European Conference on Computer Vision (ECCV), pages 118, Cham, 2025. Springer Nature Switzerland. [69] Romain Pierre Testuz, Yuliy Schwartzburg, and Mark Pauly. Automatic generation of constructable brick sculptures. Technical report, École Polytechnique Fédérale de Lausanne, 2013. [70] Rylee Thompson, Elahe Ghalebi, Terrance DeVries, and Graham W. Taylor. Building LEGO using deep generative models of graphs. arXiv preprint arXiv:2012.11543, 2020. [71] Yunsheng Tian, Jie Xu, Yichen Li, Jieliang Luo, Shinjiro Sueda, Hui Li, Karl D.D. Willis, and Wojciech Matusik. Assemble them all: Physics-based planning for generalizable assembly by disassembly. ACM Transactions on Graphics (TOG), 41(6), 2022. [72] TobyLobster. ImportLDraw - blender add-on for imhttps : / / github . com / porting LDraw models. TobyLobster/ImportLDraw, 2025. Accessed: 202503-07. [73] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. TripoSR: Fast 3D object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. [74] Uy Dieu Tran, Minh Luu, Phong Ha Nguyen, Khoi Nguyen, and Binh-Son Hua. Diverse text-to-3D synthesis with augmented text embedding. In European Conference on Computer Vision (ECCV), 2024. [75] Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. CG3D: Compositional generation for text-to-3D via Gaussian splatting. arXiv preprint arXiv:2311.17907, 2023. [76] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score Jacobian chaining: Lifting pretrained 2D diffusion models for 3D generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [77] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [78] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. LLaMA-Mesh: Unifying 3D mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. [79] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. [80] Qingshan Xu, Jiao Liu, Melvin Wong, Caishun Chen, and Yew-Soon Ong. Precise-physics driven text-to-3D generation. arXiv preprint arXiv:2403.12438, 2024. [67] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. EdgeRunner: Auto- [81] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang 12 Xu, and Kai Zhang. DMV3D: Denoising multi-view diffusion using 3D large reconstruction model. In International Conference on Learning Representations (ICLR), 2024. [82] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. PhyScene: Physically interactable 3D scene synthesis for embodied AI. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [83] Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. DreamReward: Text-to-3D generation with human preference. In European Conference on Computer Vision (ECCV), 2024. [84] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. PhysDiff: Physics-guided human motion diffusion model. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [85] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. CLAY: controllable large-scale generative model for creating highquality 3D assets. In ACM Transactions on Graphics (TOG), 2024. [86] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3D 2.0: Scaling diffusion models for high resolution textured 3D assets generation. arXiv preprint arXiv:2501.12202, 2025. [87] Guyue Zhou, Liyi Luo, Hao Xu, Xinliang Zhang, Haole Guo, and Hao Zhao. Brick yourself within 3 minutes. In International Conference on Robotics and Automation (ICRA), pages 62616267, 2022. [88] J. Zhou, X. Chen, and Y. Xu. Automatic generation of vivid LEGO architectural sculptures. Computer Graphics Forum, 2019. [89] Linqi Zhou, Yilun Du, and Jiajun Wu. 3D shape generation and completion through point-voxel diffusion. In IEEE International Conference on Computer Vision (ICCV), 2021. [90] Mingjun Zhou, Jiahao Ge, Hao Xu, and Chi-Wing Fu. Computational design of LEGO sketch art. ACM Transactions on Graphics (TOG), 42(6), 2023."
        },
        {
            "title": "Appendix",
            "content": "A. LEGOGPT Implementation Details Captioning Details. The complete prompt template used for GPT-4o caption generation is as follows: This is rendering of 3D object built with LEGO bricks with 24 different views. The object belongs to the category of {CATEGORY_NAME}. You will generate five different captions for this {CATEGORY_NAME} that: 1. Describes the core object/subject and its key geometric features 2. Focuses on structure, geometry, and layout information 3. Uses confident, concrete, and declarative language 4. Omits color and texture information 5. Excludes medium-related terms (model, render, design) 6. Do not describe or reference each view individually. 7. Focus on form over function. Describe the physical appearance of components rather than their purpose. 8. Describe components in detail, including size, shape, and position relative to other components. 9. The five captions should be from coarse to fine, with the first one being the most coarse-grained (e.g., general description of the object, within 10 words) and the last one being the most fine-grained (e.g., detailed description of the object, within 50 words). The five captions should be different from each other. Do not include any ordering numbers (e.g., 1, a, etc.). 10. Describe the object using the category name \"{CATEGORY_NAME}\" or synonyms of the category name \"{CATEGORY_NAME}\". StableText2Lego Details. We generate StableText2Lego from the objects in ShapeNetCore [2]. While ShapeNetCore provides both voxel and mesh representations, we find that working directly with mesh data preserves better geometric details. We voxelize the 3D mesh into 20 20 20 grid representation and generate the LEGO structure using split-and-remerge legolization algorithm. Split-and-remerge legolization has been studied in prior work [45]. But instead of initializing the voxel with 1 1 unit bricks and randomly merging them into larger bricks [45], we initialize with heuristics by directly placing bricks to fill all the voxels, i.e., prioritizing placing 1) larger bricks and 2) bricks that connect multiple other bricks. After initializing the layout, we apply the new stability analysis [37] to identify weak parts in the structure and rearrange the local brick layout if necessary. In addition, to further improve data quality and diversity, we introduce randomness in the legolization process. Specifically, when initializing the LEGO structure, we randomly permute the order of brick placement within the same heuristic priority and generate two different structures for each object. With the mesh-to-LEGO and structure augmentation techniques, we generate 62,000+ LEGO structures Figure 9. Given prompt and several example LEGO designs in text format, pre-trained LLaMA model can generate LEGO designs with some structure. covering the 21 categories with 31,218 unique 3D objects from ShapeNetCore. Among the selected 3D objects, 92% (i.e., 28,822) of them have at least one stable LEGO design, which offer 47,000+ stable LEGO layouts. Our StableText2Lego dataset significantly expands upon the previous StableLego dataset [37] in several key aspects: it contains > 3 more stable unique 3D objects with > 5 more stable LEGO structures compared to 8,000+ in StableLego, spans diverse set of 21 object categories, and provides detailed geometric descriptions for each shape. Training details. The full prompt that we use to construct our instruction fine-tuning dataset is as follows: [SYSTEM]You are helpful assistant. [USER]Create LEGO model of the input. Format your response as list of bricks: <brick dimensions> <brick position>, where the brick position is (x,y,z). Allowed brick dimensions are 2x4, 4x2, 2x6, 6x2, 1x2, 2x1, 1x4, 4x1, 1x6, 6x1, 1x8, 8x1, 1x1, 2x2. All bricks are 1 unit tall. ### Input: [INSERT CAPTION] We fine-tune using low-rank adaptation (LoRA) [22] with rank of 32, alpha of 16, and dropout rate of 0.05. To pre14 Figure 10. Novelty analysis. For each structure generated by LEGOGPT, we find the closest structure in the training dataset as measured by Chamfer distance in voxel space. The generated structures are distinct from their nearest neighbors, indicating low memorization. vent catastrophic forgetting and training instability, we apply LoRA to only the query and value matrices for total of 3.4M tunable parameters. We train for three epochs on eight NVIDIA RTX A6000 GPUs. We use the AdamW optimizer [43] with learning rate of 0.002, using cosine scheduler with 100 warmup steps and global batch size of 64. The total training time is 12 hours. Inference Details. The sampling temperature is set to 0.6 for all experiments. During brick-by-brick rejection sampling, to mitigate repeated generation of rejected bricks, we increase the temperature by 0.01 each time the model generates brick that has already been rejected. To force each output brick to be in the format {h}{w} ({x},{y},{z}), we sample only from the set of valid tokens during each step. For example, the first token must be digit, the second an x, and so on. This has little to no effect on the output of LegoGPT, but helps force the baseline pre-trained models to output LEGO bricks in the specified format. Only 1.2% of the generated designs exceeded the maximum number of rollbacks and failed to produce stable final structure. Novelty Analysis. For each generated structure, we find its closest structure in the training dataset, measured by computing the Chamfer distance in voxel space. As seen in Figure 10, the generated structures are distinct from their nearest neighbors in the dataset, confirming that our model can create novel designs rather than simply memorizing the training data. use Learning. We In-context LLaMA-3.2-1BInstruct [11] as our base model, chosen for its computational efficiency. The in-context learning pipeline is shown in Figure 9, where the base model can generate LEGO-like designs through in-context learning, while suffering from collisions and disconnectivity. We did not use rejection sampling or rollback when evaluating zero-shot or few-shot generation, as doing so resulted in an excessive number of rejections and sharp increase in generation time. B. Robotic Assembly We demonstrate automated assembly using dual-robotarm system as shown in Figure 8. The system consists of two Yaskawa GP4 robots, each equipped with an ATI force-torque sensor. calibrated LEGO plate is placed between them, and the robots use the bricks initially placed on the plate to construct the LEGO structure. Given = [b1, b2, . . . , bN ], we employ the action mask in [39] with assembly-by-disassembly search [71] to generate physically executable assembly sequence for the robots, i.e., reordering the brick sequence so that 1) each intermediate structure is physically stable by itself, 2) each intermediate structure is stable under the robot operation, and 3) each assembly step is executable within the systems dexterity. With the executable assembly sequence, an asynchronous planner [23] distributes the assembly tasks to the robots, plans the robots movements, and coordinates the bimanual system to construct the LEGO structure. The robots use the end-ofarm tool and the manipulation policy presented in [23, 38] with closed-loop force control to robustly manipulate LEGO bricks and construct the structure. C. Manual Assembly As shown in Figure 11, human users can assemble our generated structures, demonstrating their physical stability and buildability. Notably, since our method outputs sequence of intermediate steps, it naturally serves as an intuitive assembly guide. 15 Figure 11. Manual Assembly. Demonstrations of manual assembly of generated LEGO structures."
        }
    ],
    "affiliations": []
}