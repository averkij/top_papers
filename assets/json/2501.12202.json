{
    "paper_title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "authors": [
        "Zibo Zhao",
        "Zeqiang Lai",
        "Qingxiang Lin",
        "Yunfei Zhao",
        "Haolin Liu",
        "Shuhui Yang",
        "Yifei Feng",
        "Mingxin Yang",
        "Sheng Zhang",
        "Xianghui Yang",
        "Huiwen Shi",
        "Sicong Liu",
        "Junta Wu",
        "Yihang Lian",
        "Fan Yang",
        "Ruining Tang",
        "Zebin He",
        "Xinzhou Wang",
        "Jian Liu",
        "Xuhui Zuo",
        "Zhuo Chen",
        "Biwen Lei",
        "Haohan Weng",
        "Jing Xu",
        "Yiling Zhu",
        "Xinhai Liu",
        "Lixin Xu",
        "Changrong Hu",
        "Tianyu Huang",
        "Lifu Wang",
        "Jihong Zhang",
        "Meng Chen",
        "Liang Dong",
        "Yiwen Jia",
        "Yulin Cai",
        "Jiaao Yu",
        "Yixuan Tang",
        "Hao Zhang",
        "Zheng Ye",
        "Peng He",
        "Runzhou Wu",
        "Chao Zhang",
        "Yonghao Tan",
        "Jie Xiao",
        "Yangyu Tao",
        "Jianchen Zhu",
        "Jinbao Xue",
        "Kai Liu",
        "Chongqing Zhao",
        "Xinming Wu",
        "Zhichao Hu",
        "Lei Qin",
        "Jianbing Peng",
        "Zhan Li",
        "Minghui Chen",
        "Xipeng Zhang",
        "Lin Niu",
        "Paige Wang",
        "Yingkai Wang",
        "Haozhao Kuang",
        "Zhongyi Fan",
        "Xu Zheng",
        "Weihao Zhuang",
        "YingPing He",
        "Tian Liu",
        "Yong Yang",
        "Di Wang",
        "Yuhong Liu",
        "Jie Jiang",
        "Jingwei Huang",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: https://github.com/Tencent/Hunyuan3D-2"
        },
        {
            "title": "Start",
            "content": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation Living out everyones imagination on creating and manipulating 3D assets. Hunyuan3D Team"
        },
        {
            "title": "Abstract",
            "content": "We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: large-scale shape generation model Hunyuan3D-DiT, and largescale texture synthesis model Hunyuan3D-Paint. The shape generative model, built on scalable flow-based diffusion transformer, aims to create geometry that properly aligns with given condition image, laying solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: https://github.com/Tencent/Hunyuan3D-2. 5 2 0 2 1 2 ] . [ 1 2 0 2 2 1 . 1 0 5 2 : r Hunyuan3D team contributors are listed in the end of report. Figure 1: An overall of Hunyuan3D 2.0 system."
        },
        {
            "title": "Introduction",
            "content": "Digital 3D assets have woven themselves into the very fabric of modern life and production. In the realms of gaming and film, these assets are vibrant expressions of creators imaginations, spreading joy and crafting immersive experiences for players and audiences alike. In the fields of physical simulation and embodied AI, 3D assets serve as essential building blocks, enabling machines and robots to mimic and comprehend the real world. Yet, the journey of creating 3D assets is anything but straightforward; it is often complex, time-consuming, and costly endeavor. typical production pipeline may involve stages like sketch design, digital modeling, and 3D texture mapping, each demanding high expertise and proficiency in digital content creation software. As result, the automated generation of high-resolution digital 3D assets has emerged as one of the most exciting and sought-after topics in recent years. Despite the importance of automated 3D generation and rapid development in image and video generation fueled by the rise of diffusion models [33, 72, 24, 47, 42], the field of 3D generation appears to be relatively stagnant in the era of large models and big data, with only handful of works making gradual progress [109, 116, 46]. Building on the 3DShape2Vectset [109] and Michelangelo [116], CLAY [111] is the first work to demonstrate the unprecedented potential of diffusion models in the 3D asset generation. Nevertheless, progress in the 3D domain remains limited. As evidenced in other fields [112, 4, 3], the prosperity of domain in the era of large models usually relies on strong open-source foundational model, such as Stable Diffusion [72, 67, 24] for image generation, LLaMA [88, 89, 22] for language models, and HunyuanVideo [42] for video generation. To this end, we present Hunyuan3D 2.0, 3D asset creation system with two strong open-sourced 3D foundation models: Hunyuan3D-DiT for generative shape creation and Hunyuan3D-Paint for generative texture synthesis. Hunyuan3D 2.0 features two-stage generation pipeline, starting with the creation of bare mesh, followed by the synthesis of texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and texture generation [104] and also provides flexibility for texturing either generated or handcrafted meshes. With this architecture, our shape creation model Hunyuan3DDiT, is designed as large-scale flow-based diffusion model. As prerequisite, we first train an autoencoder Hunyuan3D-ShapeVAE using advanced techniques such as mesh surface importance sampling and variational token length to capture fine-grained details on the meshes. Then, we build up dual-single stream transformer [44] on the latent space of our VAE with the flow-matching [50, 24] objective. Our texture generation model Hunyuan3D-Paint is made of novel mesh-conditioned multi-view generation pipeline and number of sophisticated techniques for preprocessing and baking multi-view images into high-resolution texture maps. We performed an in-depth comparison of Hunyuan3D 2.0 in relation to leading 3D generation models worldwide, including three commercial closed-source end-to-end products, an end-to-end opensourced model Trellis [98], and several separate models [9, 36, 96, 108, 52, 56] for shape and texture generation. We report visual and quantitative evaluation results across three dimensions: generated textured mesh, bare mesh, and texture map. We also provided user study results on 300 test cases involving 50 participants. The comparison shows the superiority of Hunyuan3D 2.0 in alignment between conditional images and generated meshes, generation of fine-grained details, and human preference ratings."
        },
        {
            "title": "2 Hunyuan3D 2.0 Architecture",
            "content": "In this section, we elaborate on the model architecture of Hunyuan3D 2.0, focusing on two main components: the shape generation model and the texture generation model. Fig. 2 illustrates the pipeline of Hunyuan3D 2.0 for creating high-resolution textured 3D asset. Given an input image, Hunyuan3D-DiT initially generates high-fidelity bare mesh via the shape generation model. This model comprises Hunyuan3D-ShapeVAE and Hunyuan3D-DiT, which will be discussed in Sec. 3. Subsequently, by leveraging strong geometric priors and the input image, we introduce Hunyuan3DPaint as our texture generation model in Sec. 4. This model produces self-consistent multi-view outputs, which are used for baking high-definition texture maps. 3 Figure 2: An overall of Hunyuan3D 2.0 architecture for 3D generation. It consists of two main components: Hunyuan3D-DiT for generating bare mesh from given input image and Hunyuan3DPaint for generating textured map for the generated bare mesh. Hunyuan3D-Paint takes geometry conditions normal maps and position maps of generated mesh as inputs and generates multi-view images for texture baking."
        },
        {
            "title": "3 Generative 3D Shape Generation",
            "content": "Hunyuan3D 2.0 employs the architecture of the latent diffusion model [72, 109, 116, 111] for shape generation. This design is driven by the successful applications of latent diffusion models in image and video generation. Specifically, our shape generation model consists of (1) an autoencoder Hunyuan3D-ShapeVAE (Sec. 3.1) that compresses the shape of 3D asset represented by polygon mesh into sequence of continuous tokens in the latent space; (2) flow-based diffusion model Hunyuan3D-DiT (Sec. 3.2), trained on the latent space of ShapeVAE for predicting object token sequences from user-provided image. The predicted tokens are further decoded into polygon mesh with VAE decoder. The details of these models are illustrated below."
        },
        {
            "title": "3.1 Hunyuan3D-ShapeVAE",
            "content": "Hunyuan3D-ShapeVAE employs vector sets [109, 11] as the compact neural representations of 3D shapes. Followed by Michelangelo [116], we use variational encoder-decoder transformer for shape compression and decoding. Besides, we choose 3D coordinates and the normal vector of point cloud sampled from the surface of 3D shapes as inputs for the encoder and instruct the decoder to predict the Signed Distance Function (SDF) of the 3D shape, which can be further decoded into triangle mesh via the marching cube algorithm. The overall network architecture is illustrated in Fig. 3. Importance Sampled Point-Query Encoder. The encoder Es aims to extract representative features to characterize 3D shapes. To achieve this, our first design utilizes an attention-based encoder to encode point clouds uniformly sampled from the surface of 3D shape. However, this design usually fails to reconstruct the details of complex objects. We attribute this difficulty to the variations in the complexity of regions on the shape surface. Therefore, in addition to uniformly sampled point clouds, we designed an importance sampling method that samples more points on the edges and corners of the mesh, which provides more complete information for describing complex regions. Concurrent work [11] also proposes similar importance sampling to improve the VAE reconstruction performance based on similar observations. 4 In detail, for an input mesh, we first collect uniformly sampled surface point clouds Pu RM 3, and importance sampled surface point clouds Pi RN 3. We use layer of cross attention, to compress the input point clouds into set of continuous tokens via set of point queries [109]. To obtain point queries, we apply Farthest Point Sampling (FPS) separately to Pu and Pi to obtain the uniform point query Qu RM 3 and the importance point query Qi RN 3. The final point cloud R(M +N )3 and point query R(M +N )3 for the cross attention are constructed by concatenating both sources. Then, we encode the point clouds and point queries with Fourier positional encoding followed by linear projection, resulting Xp R(M +N )d and Xq R(M +N )d, where is the width of the transformer. The encoded point cloud and point query are sent to the cross attention followed by number of self-attention layers, which helps improve the feature representation, to obtain the hidden shape representation Hs R(M +N )d. Since we adopt the design of variational autoencoder [40], an additional linear projection is applied on Hs to predict the mean E(Zs) R(M +N )d0 and variance Var(Zs) R(M +N )d0 of the final latent shape embedding in token sequence, where d0 is the dimension of latent shape embedding. Decoder. The decoder Ds reconstructs the 3D neural field from the latent shape embedding Zs from the encoder. Ds starts from projection layer to transform the latent embedding from dimension d0 back to the width of transformer d. Then, number of self-attention layers further process the hidden embeddings, after which is another point perceiver that takes 3D grid Qg R(HW D)3 as queries to obtain 3D neural field Fg R(FnW D)d from the hidden embeddings. We use another linear projection on the neural field to obtain the Sign Distance Function (SDF) Fsdf R(FoW D)1, which can be decoded into triangle mesh with marching cube algorithms. Training Strategy & Implementation. We employ multiple losses to supervise the model training, including (1) the reconstruction loss that computes MSE loss between predicted SDF Ds(xZs) and ground truth SDF(x), and (2) the KL-divergence loss LKL to make the latent space compact and continuous, which facilitates the training of diffusion models. Due to the dense computation required by complete SDF, the reconstruction loss is calculated as the expectation of losses on randomly sampled points in the space and shape surface. The overall training loss Lr can be written as, Lr = ExR3 [MSE(Ds(xZs), SDF(x))] + γLKL (1) where γ is the loss weight of KL loss. During training, we also utilize multi-resolution strategy to speed up model convergence, where the length of the latent token sequence is randomly sampled from predefined set. shorter sequence reduces the computation cost, and longer sequence facilitates the reconstruction quality. The longest sequence length is 3072 in our released version, which could support high-resolution shape generation with fine-grained and sharp details. Figure 3: The overall architecture of Hunyuan3D-ShapeVAE. Instead of only using uniform sampling on mesh surface, We have developed an importance sampling strategy to extract high-frequency detail information from the input mesh surface, such as edges and corners. This allows the model to better capture and represent the intricate details of 3D shapes. Note that during the point query construction, the Farthest Point Sampling (FPS) operation is performed separately for the uniform point cloud and the importance sampling point cloud."
        },
        {
            "title": "3.2 Hunyuan3D-DiT",
            "content": "Hunyuan3D-DiT is flow-based diffusion model aimed at producing high-fidelity and high-resolution 3D shapes according to given image prompts. Network Structure. Inspired by FLUX [44], we adopt dualand single-stream network structure as in Fig. 4. In dual-stream blocks, latent tokens and condition tokens are processed with separate QKV projections, MLP, and etc., but interact within an attention operation. In single-stream blocks, 5 Figure 4: Overview of Hunyuan3D-DiT. It adopts transformer architecture with both doubleand single-stream blocks. This design benefits the interaction between modalities of shape and image, helping our model to generate bare meshes with exceptional quality. (Note that the orange blocks have no learnable parameters, the blue blocks contain trainable parameters, and the gray blocks indicate module composed of more details.) latent tokens and condition tokens are concatenated and processed by spatial attention and channel attention in parallel. We only use the embedding of the timestep for the modulation modules. Besides, we omit the positional embedding of the latent sequence as the specific latent token of our ShapeVAE in the sequence does not correspond to fixed location in the 3D grid. Instead, the content of our 3D latent tokens themselves is responsible for figuring out the position/occupancy of the generated shape in the 3D grid, which is different from image/video generation where their tokens are responsible for predicting content in specific location in 2D/spatial-temporal grid. Condition Injection. We employ pre-trained image encoder to extract conditional image tokens of the patch sequence including the head token at the last layer. To capture the fine-grained details in the image, we utilize large image encoder DINOv2 Gaint [61] and large input image size 518 518. Besides, we also remove the background of the input image, resize the object to unified size, reposition the object to the center, and fill the background with white, which helps to remove the negative impact of the background and increase the effective resolution of the input image. Training & Inference. We utilize flow matching objective [50, 24] for training our model. Specifically, flow matching first defines probability density path between Gaussian distribution and data distribution, then, the model is trained to predict the velocity field ut = xt that drifting sample xt dt towards data x1. In our case, we adopt the affine path with the conditional optimal transport schedule specified in [51], where xt = (1 t) x0 + x1, ut = x1 x0. Therefore, the training loss is formulated as, = Et,x0,x1[ uθ(xt, c, t) ut 2 2], (2) where U(0, 1) and denotes model condition. During the inference phase, we first randomly sample start point x0 N(0, 1) and employ first-order Euler ordinary differential equation (ODE) solver to solve x1 with our diffusion model uθ(xt, c, t)."
        },
        {
            "title": "4 Generative Texture Map Synthesis",
            "content": "Given 3D mesh without texture and an image prompt, we aim to generate high-resolution and seamless texture map. The texture map should closely conform to the image prompt in the visible region, exhibit multi-view consistency, and maintain harmonious with the input mesh. 6 Figure 5: Overview of Hunyuan3D-Paint. We leverage an image delighting module to convert the input image to an unlit state to produce light-invariant texture maps. The system features doublestream image conditioning reference-net, which provides faithfully conditional image features to the model. Furthermore, it facilitates the production of texture maps that conform closely to the input image. The multi-task attention module ensures that the model synthesizes multi-view consistent images. This module maintains the coherence of all generated images while adhering to the input. To achieve these objectives, we employ three-stage framework, including pre-processing stage (Sec. 4.1), multi-view image synthesis stage (Sec. 4.2, Hunyuan3D-Paint), and texture baking stage based on dense multi-view inference (Sec. 4.3). Other details related to model training and textand image-to-texture pipeline are provided in Sec. 4.4. Fig. 5 illustrates the complete pipeline of our texture map synthesis method."
        },
        {
            "title": "4.1 Pre-processing",
            "content": "Image Delighting Module. The reference image typically exhibits pronounced and varied illumination and shadow, whether collected by the user or generated by T2I models. Directly inputting such images into the multi-view generation framework can cause illumination and shadows to be baked into the texture maps. To address this issue, we leverage delighting procedure on the input image via an image-to-image approach [6] before multi-view generation. Specifically, to train such an image delighting model, we collect large-scale 3D dataset and render it under the illumination of random HDRI environmental map and an even white light to form the corresponding pair-wise image data. Benefiting from this image delighting model, our multi-view generation model can be fully trained on white-light illuminated images, enabling an illumination-invariant texture synthesis. View Selection Strategy. In practical applications, to reduce the costs of texture generation (i.e., generate the largest area of texture with the minimum number of viewpoints), we employ geometryaware viewpoint selection strategy to support effective texture synthesis. By considering the coverage of the geometric surface, we heuristically select 8 to 12 viewpoints for inference. Initially, we fix 4 orthogonal viewpoints as basis since they cover most parts of the geometry. Subsequently, we iteratively add novel viewpoints using greedy search approach. The specific process is illustrated in Algorithm 1, and the coverage function in the algorithm is defined as: F(vi, Vs, M) = Aarea UV cover(vi, M) UV cover(vi, M) (cid:40) (cid:34) (cid:33)(cid:35)(cid:41) UV cover(vs, M) (3) (cid:32) (cid:91) sVs where UV cover(v, M) is function that returns the set of covering texels in UV space based on the input view and mesh geometry M, and Aarea( ) is function that calculates the coverage area according to the given set of covering texels. This approach encourages the multi-view generation 7 model to focus on viewpoints with more unseen regions, together with the dense-view inference, alleviating the burden of post-processing (i.e., texture inpainting). Algorithm 1 View Selection Algorithm 1: Input: 2: Initialize selected viewpoint set Vs, reference viewpoint set Vr, coverage function F, and mesh M. 3: Set Nmax = 12 as the maximum number of iterative searches, and Nf ixed = 4 as the initial Set cmax = 1 Set vmax = 1 for vi Vr do fixed number of viewpoints. 4: for = Nf ixed to Nmax 1 do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Set ccur = F(vi, Vs, M) if ccur > cmax then cmax = ccur vmax = vi end for Vr Vr {vmax} Vs Vs {vmax} end if"
        },
        {
            "title": "4.2 Hunyuan3D-Paint",
            "content": "Geometry-conditioned multi-view image generation is key component in the texture synthesis framework. In the context of image-guided texture synthesis, the design of the multi-view image generation must be meticulously crafted to achieve image alignment, geometric following, and multi-view consistency. These functionalities are realized in Hunyuan3D-Paint according to bunch of techniques containing double-stream image conditioning reference-net, multi-task attention mechanism, and strategy for geometry and view conditioning. Double-stream Image Conditioning Reference-Net. In the aim of the image following, Hunyuan3DPaint implements reference-net conditioning approach [113], upon which we develop series of alternative solutions. Specifically, rather than noisy feature synchronized with the generation branch, we directly feed the original VAE feature of the reference image into the reference branch to maintain image details as much as possible. Since the feature is noiseless, we set the timestep of the reference branch to 0 to maintain the input image information faithfully. On the other hand, to regularize potential style bias introduced by the 3D rendering dataset, we abandon the shared-weight reference-net as in [86, 76] and freeze the weights of the original SD2.1 weights (which serves as the base model for our multi-view generation). We have found that the fixed-weights reference-net serves as soft regularization that anchors the generated image distribution, preventing it from drifting away towards the rendered image distribution, significantly improving the performance on real-world image conditioning. Together, these two schemes form the double-stream image conditioning strategy. We leverage the zero-noised double-stream image conditioning reference-net by capturing feature cache prior to each self-attention module. This cache is then fed into the multi-view diffusion model via reference attention module. Multi-task Attention Mechanism. We introduce two additional attention modules alongside the original self-attention to enable the image diffusion model to generate multi-view images guided by reference image. The reference attention module integrates the reference image into the multiview diffusion process. In contrast, the multi-view attention module ensures consistency across the generated views. To mitigate potential conflicts arising from these multi-functionalities, we design the additional two distinct attention modules in parallel structure (illustrated in Fig. 5), which can be expressed as: ZM = ZSA + λref Softmax (cid:33) (cid:32) Qref ref Vref + λmv Softmax (cid:19) (cid:18) QmvK mv d"
        },
        {
            "title": "Vmv",
            "content": "(4) 8 where ZSA represents the feature calculated by the original frozen-weight self-attention, and Qref , Kref , Vref and Qmv, Kmv, Vmv are the Query, Key, and Value projected features of reference attention and multi-view attention, respectively. Geometry and View Conditioning. Following geometry is another unique feature in texture map synthesis. To enable effective training, we opt for an easy implementation of directly concatenating the geometry conditions with noise. Specifically, we first input the multi-view canonical normal maps and canonical coordinate maps (CCM)two view-invariant geometry conditions we utilizeinto pre-trained Variational Autoencoder (VAE) to obtain geometric features. These features are then concatenated with latent noise and fed into the channel-extended input convolution layer of the diffusion model. Other than geometry conditioning, we adopt learnable camera embedding in our pipeline to boost the viewpoint clue for the multi-view diffusion model. Specifically, we assign unique unsigned integer to each pre-defined viewpoint and set up learnable view embedding layer to map the integer to feature vector, which is then injected into the multi-view Diffusion model. We have found in our experiments that combining the geometry conditioning with learnable camera embedding yields the best performance."
        },
        {
            "title": "4.3 Texture Baking",
            "content": "Dense-view inference. Potential self-occlusion is significant challenge in the context of texture synthesis within multi-view image generation framework, particularly when handling irregular geometries produced by shape generative models. This issue necessitates modeling texture synthesis as two-stage framework. The first stage focuses on multi-view image generation, while the second stage involves non-trivial inpainting to fill the holes caused by self-occlusion. In our texture-map synthesis framework, we alleviate the burden on the second stage of inpainting by facilitating denseview inference during the multi-view generation stage. To enable effective dense-view inference, view dropout strategy is introduced as flexible training mechanism that allows the model to encounter all of the pre-set viewpoints, thereby enhancing its 3D perception capabilities and generalization. Specifically, we randomly select 6 viewpoints from total of 44 pre-set viewpoints to serve as batch input to the multi-view diffusion backbone network. During the inference phase, our framework is able to output the images of any specified viewpoints, supporting dense-view inference. Single Image Super-resolution. To enhance texture quality, we apply pre-trained single-image super-resolution model [93] to each generated image from different viewpoints. Experiments have demonstrated that this single-image super-resolution approach maintains consistency among multiviews, as it does not introduce significant variations to the images. Texture Inpainting. After unwrapping the synthesized dense multi-view images into texture map, small set of patches in the UV texture remain that are not fully covered. To address this issue, we employ an intuitive inpainting approach. First, we project the existing UV texture into vertex texture. Then, we query each UV texels texture by computing weighted sum of the textures from the connected, textured vertices. The weights are set to be the reciprocal of the geometric distances between the texels and the vertices. 4."
        },
        {
            "title": "Implementation Details",
            "content": "Model Training. For training the multi-view image generation framework, we start by inheriting the ZSNR checkpoint of the Stable Diffusion 2 v-model [49]. We train our multi-view diffusion model using self-collected large-scale 3D dataset. Multi-view images are rendered under the illumination of an even white light to accommodate our delighting model. Specifically, we render the reference image with random azimuth and fixed range of elevation from -20 to 20 degrees. This variation disrupts the consistency between the reference and generated images, thereby increasing the robustness of our texture generation framework. We directly train on 512 512 resolution with total of 80,000 steps, batch size of 48, and learning rate of 5 105. We use 1000 warm-up steps and the \"trailing\" scheduler proposed by ZSNR. General Textand Image-to-Texture. It is worth noting that Hunyuan3D-Paint not only generates high-quality texture maps for generated meshes but also supports arbitrary texture generation guided by any text or image input provided by the user for any geometric model. To achieve this, we leverage 9 3DShape2VecSet [109] Michelangelo [116] Direct3D [96] Hunyuan3D-ShapeVAE (Ours) V-IoU() S-IoU() 87.88% 80.66% 84.93% 76.27% 88.43% 81.55% 93.6% 89.16% Table 1: Numerical comparisons. We evaluate the reconstruction performance of Hunyuan3DShapeVAE and baselines based on volume IoU (V-IoU) and Surface (S-IoU). The results indicate Hunyuan3D-ShapeVAE overwhelms all baselines in the reconstruction performance. advanced T2I models and corresponding conditional generation modules, such as ControlNet [112] and IP-Adapter [105], to generate input images that align with geometric shapes based on userprovided text or image prompts. Benefitting from this paradigm, we are capable of texturing any specified geometry with arbitrary images, whether they are matched or mismatched. An application of using different images to texture the same geometry, dubbed as re-skinning, is illustrated in Fig. 9."
        },
        {
            "title": "5 Evaluations",
            "content": "To thoroughly evaluate the performance of Hunyuan3D 2.0, we conducted experiments from three perspectives: (1) 3D Shape Generation (including Shape Reconstruction and Shape Generation), (2) Texture map synthesis, and (3) Textured 3D assets generation. 5.1 3D Shape Generation Shape generation is crucial for 3D generation, as high-fidelity and high-resolution bare meshes form the foundation for downstream tasks. In this section, we compare and evaluate the capability of 3D shape generation in Hunyuan3D 2.0 from two perspectives: shape reconstruction and shape generation. Baselines. We compare the reconstruction performance of Hunyuan3D-ShapeVAE with 3DShape2VecSet [109], Michelangelo [116], and Direct3D [96]. The mentioned methods represent the state-of-the-art ShapeVAE architecture, and the core differences are neural representations, where 3DShape2VecSet uses downsampled vector set, point query; Michelangelo utilizes learnable vector set, learnable query; Direct3D leverages learnable triplane; Hunyuan3D-ShapeVAE employees point query with importance sampling. Note that, except Direct3D, which requires 3072 token length (suffering significant performance degeneration when reducing token length), all VAE models compare by 1024 token length. Hunyuan3D-DiT is compared with several state-of-the-art baselines. Open-source baselines are Michelangelo [116], Craftsman 1.5 [46], and Trellis [98]. Closed-source baselines are Shape Model 1, Shape Model 2, and Shape Model 3. Metrics. We employ the Intersection of Union (IoU) to measure the reconstruction performance. Specifically, we compute randomly sampled volume points IoU (V-IoU) and near-surface region IoU (S-IoU) to reflect the reconstruction performance comprehensively. To evaluate shape generative performance, we employ ULIP [102] and Uni3D [119] to compute the similarity between the generated mesh and input images (ULIP-I and Uni3D-I) and the similarity between the generated mesh and images prompt synthesizing by the vision language model [13] (ULIP-T and Uni3D-T). Shape Reconstruction Comparisons. The Numerical comparison of shape reconstruction is shown in Tab. 1. According to the table, Hunyuan3D-ShapeVAE overwhelms all baselines. Comparisons among Hunyuan3D-ShapeVAE, 3DShape2VecSet, and Michelangelo demonstrate the effectiveness of the importance sampling strategies. Fig. 6 illustrates the visual comparison of shape reconstruction, which shows that Hunyuan3D-ShapeVAE could faithfully recover the shape with fine-grained details and produce neat space without any floaters. Shape Generation Comparisons. Tab. 2 shows the numerical comparison between Hunyuan3D-DiT and competing methods, which indicates that Hunyuan3D-DiT produces the most condition following results. Furthermore, according to the visual comparison in Fig. 7, results from Hunyuan3D-DiT follow the image prompt most, including clear human faces, surface bumps, logo texts, and layouts. Meanwhile, the generated bare mesh is holeless, which supports solid basis for downstream tasks. 10 Figure 6: Visual comparisons. We illustrate the reconstructed mesh (blue paint aims to show more details) in the figure, which showcases that only Hunyuan3D-ShapeVAE reconstructs mesh with fine-grained surface details and neat space. (Better viewed by zooming in.) 11 ULIP-T() ULIP-I() Uni3D-T() Uni3D-I() Michelangelo [116] Craftsman 1.5 [46] Trellis [98] Shape Model 1 Shape Model 2 Shape Model 3 Hunyuan3D-DiT (Ours) 0.0752 0.0745 0.0769 0.0799 0.0741 0.0746 0.0771 0.1152 0.1296 0.1267 0.1181 0.1308 0.1284 0.1303 0.2133 0.2375 0.2496 0.2469 0.2464 0.2516 0.2519 0.2611 0.2987 0.3116 0.3064 0.3106 0.3131 0.3151 Table 2: Numerical comparisons. By evaluating the shape generation performance on ULIP-T/I, Uni3D-T/I, demostrating Hunyuan3D-DiT could produce the most condition followed results. Figure 7: Visual comparisons. We display the input image and the generated bare mesh (blue paint aims to show more details) from all methods in the figure. The human faces and piano keys show that Hunyuan3D-DiT could synthesize detailed surface bumps, maintaining completeness. Several scenes or logos demonstrate that Hunyuan3D-DiT could generate intricate details. (Better viewed by zooming in.) 12 TEXTure [71] Text2Tex [9] SyncMVD [56] Paint3D [108] TexPainter [110] Hunyuan3D-Paint (Ours) CMMD() 3.047 2.811 2.584 2.810 2.483 2.318 FIDCLIP () CLIP-score() 35.75 31.72 29.93 30.29 28.83 26. 0.8499 0.8680 0.8751 0.8724 0.8789 0.8893 LPIPS() 0.0076 0.0071 0.0063 0.0063 0.0062 0.0059 Table 3: Numerical comparisons. We compare Hunyuan3D-Paint with baselines on various metrics, and the results indicate that our model could produce the most condition-conforming texture maps."
        },
        {
            "title": "5.2 Texture Map Synthesis",
            "content": "As texture maps directly influence the visual appeal of textured 3D assets, we conduct comprehensive text-conditioned texture map synthesis experiments to validate the performance of Hunyuan3D-Paint. Baselines. We compare Hunyuan3D-Paint with the following texture generation methods, including TEXTure [71], Text2Tex [9], SyncMVD [56], Paint3D [108], and TexPainter [110]. All the baselines leverage geometric and diffusion priors to facilitate the overall generation quality of texture maps. Metrics. We apply several frequently used image-level metrics to enable fair comparison of texture map generation. Specifically, we leverage CLIP-version of Fréchet Inception Distance IDCLIP to compute the distance between the rendering of the generated textured map in semantic perspectives. We use the implementation of Clean-FID [64]. Besides, the recently introduced CLIP Maximum-Mean Discrepancy (CMMD) [37] is utilized to serve as another important criterion, which is more accurate measurement of images with rich details. In addition to these two metrics, we also use CLIP-score [69] to validate semantic alignment between renderings of the generated texture map and given prompt and LPIPS [114] to estimate the consistency between renderings of the generated texture map and ground-truth images. Comparisons. The numerical comparison of text-to-texturing is shown in Tab. 3, showcasing that Hunyuan3D-Paint achieves the best generative quality and semantic following. The visual comparison refers to Fig. 8. The fish and rabbit show that our model produces the most condition-following results. And the football demonstrates the ability of Hunyuan3D-Paint to produce clear texture maps. The texture map of the castle and bear contains rich texture patterns, showcasing that our model can produce intricate details. Applications. All generated texture maps are seamless and lighting-invariant. Moreover, Hunyuan3DPaint is flexible to produce various texture maps for bare mesh or hand-crafted mesh according to different prompts. As shown in the Fig. 9, our model produces different texture maps for mesh with seamless and intricate details."
        },
        {
            "title": "5.3 Textured 3D Assets Generation",
            "content": "In this section, we evaluate the generated textured 3D assets for reflecting the end-to-end generation capabilities of Hunyuan3D 2.0. Baselines. We compare Hunyuan3D 2.0 against leading models in the field, including open-source model Trellis [98] and closed-source models Model 1, Model 2, and Model 3. Metrics. We mainly measure the generative quality of textured 3D assets by their renderings. Similar to Sec. 5.2, we employ IDCLIP to compute the image content distance, CLIP-score to reflect semantic alignment, CMMD to measure the similarity in the image details, and LPIPS to evaluate the consistency between rendering from generated textured 3D assets and given image prompts. Comparisons. The numerical results reported in the Tab. 4 indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets and the condition following ability. The illustration in the Fig. 11 demonstrates that Hunyuan3D 2.0 produces the textured 3D assets with the highest quality. Even for the text in the image prompt, our model can produce the correct bumps on the shape surface and an accurate texture map according to the geometric conditions. The rest of the cases demonstrate the ability of our model to generate high-resolution and high-fidelity results with complex actions or scenes. 13 Figure 8: Visual comparisons. We demonstrate several generated texture maps on different bare meshes. The fish and rabbit texture map showcases that Hunyuan3D-Paint produces the most textconforming results. The football indicates that our model could synthesize seamless and clean texture maps. Moreover, Hunyuan3D-Paint could generate complex texture maps, like the castle and bear. (Better viewed by zooming in.) 14 Figure 9: Visual results. We generate different texture maps for two meshes, and the results validate the performance of Hunyuan3D-Paint on texture reskinning. (Better viewed by zooming in.) Trellis [98] Model 1 Model 2 Model 3 Hunyuan3D 2.0 (Ours) CMMD() 3.591 3.600 3.368 3.218 3.193 FIDCLIP () 54.639 55.866 49.744 51.574 49.165 FIDIncept() CLIP-score() 289.287 305.922 294.628 295.691 282.429 0.787 0.779 0.806 0.799 0.809 Table 4: Numerical comparison. According to the results, Hunyuan3D-Paint produces the most condition-following texture maps. User Study. In addition, we conducted user study by randomly inviting 50 volunteers to evaluate 300 unselected results generated by Hunyuan3D 2.0 subjectively. The evaluation criteria included 1) overall visual quality, 2) adherence to image conditions, and 3) overall satisfaction (dissatisfaction in either 1 or 2 results in overall dissatisfaction). The user study results in Fig. 10 indicate that Hunyuan3D 2.0 outperforms comparative methods, particularly in its ability to adhere to image conditions. Figure 10: The results of user study. Figure 11: Visual comparisons. The first case reflects that Hunyuan3D 2.0 could synthesize detailed surface bumps and correct texture maps. The second penguin showcases our models ability to handle complex actions. The last mountain demonstrates that Hunyuan3D-DiT could produce intricate structures, and Hunyuan3D-Paint can synthesize vivid texture maps. (Better viewed by zooming in.)"
        },
        {
            "title": "6 Hunyuan3D-Studio",
            "content": "We have developed Hunyuan3D-Studio. This platform includes comprehensive set of tools for the 3D production pipeline, as illustrated in Fig. 1. Hunyuan3D-Studio aims to provide experts and novices with no-frills way to engage in 3D generation production and research. In this section, we highlight several features of Hunyuan3D-Studio, including Sketch-to-3D, Low-polygon Stylization, and Autonomous Character Animator. These features aim to streamline the 3D creation process and make it accessible to broader audience."
        },
        {
            "title": "6.1 Sketch-to-3D",
            "content": "In game development and content creation, converting 2D sketches into 3D assets is crucial technology that significantly enhances digital artistry design efficiency and flexibility. Previous methods [1, 118, 115, 32] suffer from the lack of the generative foundation model. They tackle this by training small-scale generative or reconstruction model with sketch images as input directly on limited dataset, significantly limiting the models capabilities. Benefitting from Hunyuan3D 2.0, we could convert sketches to images with rich details as input to the foundation 3D generative model. Specifically, the Hunyuan3D-Studio has developed the Sketch-to-3D module, which first converts sketches to images with rich details, maintaining original contours. Then, synthesize high-resolution and high-fidelity textured 3D assets, significantly reducing the barrier for users to engage in content creation. As shown in Fig. 1, the Sketch-to-3D module can generate highly detailed and realistic 3D assets while maintaining close consistency with the original sketches. With this technology, users can synthesize 3D content with simple sketch, providing powerful tool for game developers and digital artists and low-barrier creation platform for ordinary users."
        },
        {
            "title": "6.2 Low-polygon Stylization",
            "content": "Low-polygon stylization is critical in many computer graphics (CG) pipelines, as the face count of mesh significantly impacts the application of 3D assets. Low-polygon stylization can significantly reduce computational costs, making it an essential process in 3D asset management. To address this, we have established low-polygon stylization module that efficiently converts the dense meshes generated by Hunyuan3D 2.0 into low-polygon meshes. This module operates in two steps: geometric editing and texture preserving. For geometric editing, we employ faster and more robust traditional method [28, 34], despite the recent auto-regressive transformer-based polygon-generation approaches [94, 101, 84, 12]. By setting an optimization criterion, we merge the vertices of the mesh to transform the dense mesh into low-polygon mesh. As shown at the top-right of Fig. 1, each 3D model can be represented by only dozens of triangles after geometric editing. The change in the face count of the mesh causes significant deviations in the vertices and faces of the low-polygon mesh compared to the dense mesh. Therefore, to preserve the texture patterns of the textured 3D assets, we construct KD-tree for the input dense mesh. We then use the nearest-neighbor search within the KD tree to query the texture colors for the vertices of the low-polygon mesh. Finally, we obtain the texture map for the low-polygon mesh by performing texture baking on the low-polygon mesh with vertex colors. This process ensures that the visual quality of the textures is maintained while optimizing the mesh structure for production-level textured 3D assets. 6.3 3D Character Animation Hunyuan3D 2.0 generates static 3D assets with high-resolution shapes and texture maps. However, drivable 3D models yet have broad requirements [80, 79, 82, 81, 54], such as game development and animation production. To extend the range of applications of Hunyuan3D 2.0, we develop 3D character animation function in Hunyuan3D-Studio. The animation algorithm inputs the generated character and extracts features from mesh vertices and edges. Then, we utilize the Graph Neural Network (GNN) to detect skeleton key points and assign skinning weights to the mesh surface. Finally, based on the predicted skeleton skinning and motion templates, the algorithm utilizes motion retargeting to drive the character. Some frames are displayed in Fig. 1. With 3D character animation, the generated results from Hunyuan3D 2.0 can come to life."
        },
        {
            "title": "7 Related Work",
            "content": "7.1 3D Shape Generation Representations. The field of shape generation has undergone significant advancements, driven by the unique challenges associated with the 3D modality. Unlike other modalities, 3D data lacks universal storage and representation format, leading to diverse approaches in shape generation research. The primary 3D representations include voxels, point clouds, polygon meshes, and implicit functions. With the advent of seminal works such as 3D ShapeNets [97], IF-Net [14], and 3D Gaussian Splatting [39], implicit functions, and 3D Gaussian Splitting have become prevalent in shape generation. However, even lightweight and flexible representations like implicit functions impose substantial modeling and computational burdens on deep neural networks. As result, neural representations of 3D shapes have emerged as new research focus, aiming to enhance efficiency. Pioneering methods such as DeepSDF [63], 3DShape2VecSet [109], Michelangelo [116], and Dora [11] represent 3D shapes using vector sets (one-dimensional latent token sequences), significantly improving representation efficiency. Another approach involves structured representations (e.g., triplane [66, 7, 26] or sparse volume [60, 117, 70]) to encode 3D shapes, which better preserve spatial priors but are less efficient than vector sets. Inspired by recent advances in Latent Diffusion Models, Hunyuan3D 2.0 employs vector sets to represent 3D shapes implicit functions, alleviating the compression and fitting demands on neural networks and achieving breakthrough in shape generation performance. Shape Generative models. The evolution of generative model paradigms has continually influenced shape generation research. Early works [95, 74, 103, 107] based on Variational Auto-encoder [40], generative adversarial networks (GANs) [29], normalizing flow [62], and auto-regressive modeling [31] demonstrated strong generative capabilities within several specific categories. The success of diffusion models [33] and their variants [55, 50] in text-conditioned image [72, 44] generation has spurred the popularity of diffusion-based shape generative models, with notable works [16, 116] achieving stable cross-category shape generation. Additionally, advancements in network architectures have propelled shape-generation research. The transition from early 3D Convolutional Neural Networks [30] to the now-common Transformer architectures [91, 65] has led to the development of classic shape generation networks, enhancing performance. Building on these advancements, Hunyuan3D 2.0 employs flow-based scalable transformer, further improving the models shape generation capabilities. Large-scale Dataset. Large-scale datasets are the cornerstone of scaling laws. However, the scale of 3D data is much smaller than that in large language models and image generation fields. From 3Dscanrep [18, 43, 90] to ShapeNet [8], the growth of 3D datasets has been gradual [83, 120, 25, 75, 100, 21, 17]. The release of objaverse [20] and objaverse-xl [19] has been significant driver in realizing the scaling law for shape generation. Leveraging these open-source 3D datasets, Hunyuan3D-2.0 can generate high-fidelity and high-resolution 3D assets. Benefiting from these open-source algorithms and 3D datasets, Hunyuan3D 2.0 is capable of generating high-fidelity and high-resolution 3D assets. Therefore, we have released Hunyuan3D 2.0 to contribute to the open-source 3D generation community and further advanced 3D generation algorithms."
        },
        {
            "title": "7.2 Texture Map Synthesis",
            "content": "High-quality texture-map synthesis has been long-standing topic within the computer graphics community. Its significance has only increased with the growing demand for end-to-end 3D generation techniques, where it plays crucial role for appearance modeling. Text to Texture. Given plain mesh, text/image to texture aims to generate high-quality texture that aligns well with the given geometry according to guided text and image. Early attempts tried to approach texture synthesis by harnessing the categorical information and train generative model on specified dataset [10, 5, 23, 78, 26, 27]. While achieving plausible texturing results, these methods failed to generalize to objects of other categories, limiting their applicability in production environments. 18 More recently, Stable Diffusion [72], owing to its impressive text-guided image generation capability and flexible structure, has spawned plethora of text-to-texture research. To take full advantage of pre-trained image diffusion models, most subsequent works have approached the texture synthesis problem as geometry-conditioned multi-view images generation problem. Initially, score distillation was adopted to harness the generation power of image diffusion models for 3D content (texture) synthesis [85, 48, 59, 68]. However, these methods are often limited by the over-saturated colors and misalignment with geometry. Subsequently, optimization-free approaches pioneered by TEXTure [71] have been introduced [99, 15, 53, 110, 108, 56, 9]. To ensure consistency across multi-view images, these methods either adopt an inpainting framework by specifying viewpoint-related masks or employ \"synchronizing\" operation during the denoising process. However, since Stable Diffusion is trained on dataset with noticeable forward-facing viewpoint bias [52], these training-free methods are limited and often suffer from severe performance issues, such as the Janus problem and multi-view inconsistency, which result in textures with significant artifacts. With the development of extensive 3D datasets, training multi-view diffusion models has become prevailing direction for texture generation [58, 2], exhibiting more powerful capabilities on texture consistency than the training-free approaches. Image to Texture. In related direction, image-guided texture generation has garnered attention in recent months, aligning closely with our research focus. This relatively unexplored area of image-guided texture synthesis demonstrates significant potential for further development since images provide more diverse information than text prompts, and text-to-texture generation can be fully replaced by text-to-image and image-to-texture pipeline. Unfortunately, most of the existing works focus on semantic alignment with the reference image rather than precise alignment. FlexiTex [38] and EASI-Tex [41] both utilize an IP-Adapter [105] for image prompt injection. While TextureDreamer [106] employs DreamBooth-like [73] approach to facilitate texture transfer across different objects. However, we argue that there are two explicit advantages to exactly following every detail of the reference image. First, as part of an end-to-end image-guided 3D generation process, the geometry generated in the first stage strives to align with the reference image, while the appearance details are left for texture synthesis stage. Thus, one of the main objectives of our texture generation framework is to enhance the geometry with more detailed appearance features from the well-aligned reference image. Second, with the rapid development of image diffusion techniques, more exquisite reference images are now available. Carefully adhering to these details can significantly improve the quality of the generated textures. Based on these advantages, Hunyuan3D-Paint is designed with detailed preserving image injection module according to the philosophy of aligning the reference image not only semantically but also following the details as closely as possible. Multi-view Images Generation. Due to the viewpoint bias and multi-view inconsistency inherent in training-free image diffusion models, multi-view image diffusion was developed to alleviate these issues by utilizing large-scale 3D datasets, such as objaverse and objaverse-xl [20, 19]. Most works force the multi-view generated latents to communicate with each other by manipulating the self-attention layers with 3D-aware masks [35, 45, 86, 57, 92, 77, 76, 52]. For example, Zero123++ [76] first treats the multi-view attention as self-attention on large image, which is the spatial concatenation of six multi-view images. MVDiffusion [87] applies correspondence-aware attention (CAA) to inform the model to focus only on the correlation among the spatially-close pixels. MVAdapter [35], following Era3D [45] implements simpler but effective row-wise and column-wise attention to alleviate the computational burden of CAA and achieves comparable performance. Inspired by these works, we propose multi-view generation framework equipped with multi-task attention mechanism to achieve both multi-view consistency and image alignment simultaneously. Benefiting from this careful design and being trained on large 3D rendering dataset, Hunyuan3DPaint is able to achieve high-quality, consistent textures with strong alignment to the reference image."
        },
        {
            "title": "8 Conclusion",
            "content": "In this report, we introduce an open-source 3D creation systemHunyuan3D 2.0 for generating textured meshes from images. We present Hunyuan3D-ShapeVAE, which is trained using novel importance sampling method. This approach compresses each 3D object into few latent tokens while minimizing reconstruction losses. Building on our VAE, we developed Hunyuan3D-DiT, an advanced diffusion transformer capable of generating visually appealing shapes that align precisely with input images. Besides, we introduce Hunyuan3D-Paint, another diffusion model designed to create textures for both our generated meshes and user-crafted meshes. With several innovative designs, our texture generation model, in conjunction with our shape generation model, can produce high-resolution, high-fidelity textured 3D assets from single image. As we continue to make progress, we hope that Hunyuan3D 2.0 will serve as robust baseline for large-scale 3D foundation models within the open-source community and facilitate future research endeavors."
        },
        {
            "title": "9 Contributors",
            "content": "Project Sponsors: Jie Jiang, Yuhong Liu, Di Wang, Yong Yang, Tian Liu Project Leaders: Chunchao Guo, Jingwei Huang Core Contributors: Data: Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan Shape Generation: Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Zibo Zhao Texture Synthesis: Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang Downstream Tasks: Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo Studio: Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Tianyu Huang Contributors: Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He"
        },
        {
            "title": "References",
            "content": "[1] Hmrishav Bandyopadhyay, Subhadeep Koley, Ayan Das, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Doodle your 3d: From abstract freehand sketches to precise 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97959805, 2024. [2] Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, and Oran Gafni. Meta 3d texturegen: Fast and consistent texture generation for 3d objects. arXiv preprint arXiv:2407.02430, 2024. [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [4] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [5] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. In IEEE International Conference on Computer Vision (ICCV), October 2023. [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. [7] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. [8] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: In Proceedings of the IEEE/CVF International Text-driven texture synthesis via diffusion models. Conference on Computer Vision, pages 1855818568, 2023. [10] Qimin Chen, Zhiqin Chen, Hang Zhou, and Hao Zhang. Shaddr: Interactive example-based geometry and texture generation via 3d shape detailization and differentiable rendering. In SIGGRAPH Asia 2023 Conference Papers, 2023. [11] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. arXiv preprint arXiv:2412.17808, 2024. [12] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [14] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59395948, 2019. [15] Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, et al. Mvpaint: Synchronized multi-view diffusion for painting anything 3d. arXiv preprint arXiv:2411.02336, 2024. [16] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44564465, 2023. [17] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022. 22 [18] Brian Curless and Marc Levoy. volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 303312, 1996. [19] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. [20] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. [21] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items, 2022. [22] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [23] Aysegul Dundar, Jun Gao, Andrew Tao, and Bryan Catanzaro. Fine detailed texture learning for 3d meshes with generative models. IEEE Trans. Pattern Anal. Mach. Intell., 2023. [24] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [25] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1093310942, 2021. [26] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems, 35:3184131854, 2022. [27] Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, and Hao Zhang. Tm-net: Deep generative networks for textured meshes. ACM Trans. Graph., 40(6):115, 2021. [28] Michael Garland and Paul Heckbert. Surface simplification using quadric error metrics. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, pages 209216, 1997. [29] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [30] Ben Graham. Sparse 3d convolutional neural networks. arXiv preprint arXiv:1505.02890, 2015. [31] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. In International Conference on Machine Learning, pages 12421250. PMLR, 2014. [32] Benoit Guillard, Edoardo Remelli, Pierre Yvernay, and Pascal Fua. Sketch2mesh: Reconstructing and editing 3d shapes from sketches. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1302313032, 2021. [33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [34] Hugues Hoppe. New quadric metric for simplifying meshes with appearance attributes. In Proceedings Visualization99 (Cat. No. 99CB37067), pages 59510. IEEE, 1999. [35] Zehuan Huang, Yuanchen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. [36] Ka-Hei Hui, Aditya Sanghi, Arianna Rampini, Kamal Rahimi Malekshan, Zhengzhe Liu, Hooman Shayani, and Chi-Wing Fu. Make-a-shape: ten-million-scale 3d shape model. In Forty-first International Conference on Machine Learning, 2024. 23 [37] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In IEEE Computer Vision and Pattern Recognition (CVPR), pages 93079315, 2024. [38] DaDong Jiang, Xianghui Yang, Zibo Zhao, Sheng Zhang, Jiaao Yu, Zeqiang Lai, Shaoxiong Yang, Chunchao Guo, Xiaobo Zhou, and Zhihui Ke. Flexitex: Enhancing texture generation with visual guidance. arXiv preprint arXiv:2409.12431, 2024. [39] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [40] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [41] Perla Sai Raj Kishore, Yizhi Wang, Ali Mahdavi-Amiri, and Hao Zhang. EASI-Tex: Edge-aware mesh texturing from single-image. ACM Transactions on Graphics (Special Issue of SIGGRAPH), 43(4), 2024. [42] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [43] Venkat Krishnamurthy and Marc Levoy. Fitting smooth surfaces to dense polygon meshes. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 313324, 1996. [44] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [45] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, et al. Era3d: High-resolution multiview diffusion using efficient row-wise attention. arXiv preprint arXiv:2405.11616, 2024. [46] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner, 2024. [47] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. [48] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE Computer Vision and Pattern Recognition (CVPR), 2023. [49] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 54045411, 2024. [50] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [51] Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code, 2024. [52] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023. [53] Shang Liu, Chaohui Yu, Chenjie Cao, Wen Qian, and Fan Wang. Vcd-texture: Variance alignment based 3d-2d co-denoising for text-guided texturing. In European Conference on Computer Vision, pages 373389. Springer, 2025. [54] Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid warping gan: unified framework for human motion imitation, appearance transfer and novel view synthesis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 59045913, 2019. [55] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 24 [56] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [57] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. [58] Jiawei Lu, Yingpeng Zhang, Zengjun Zhao, He Wang, Kun Zhou, and Tianjia Shao. Genesistex2: Stable, consistent and high-quality text-to-texture generation. arXiv preprint arXiv:2409.18401, 2024. [59] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shapeguided generation of 3d shapes and textures. In IEEE Computer Vision and Pattern Recognition (CVPR), pages 1266312673, 2023. [60] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d completion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 306315, 2022. [61] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. [62] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):164, 2021. [63] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019. [64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In IEEE Computer Vision and Pattern Recognition (CVPR), 2022. [65] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [66] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 523540. Springer, 2020. [67] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [68] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. [69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [70] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42094219, 2024. [71] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. [72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [73] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. 25 [74] Aditya Sanghi, Hang Chu, Joseph Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1860318613, 2022. [75] Pratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Maslioukova, Melinos Averkiou, Andreas Andreou, Siddhartha Chaudhuri, and Evangelos Kalogerakis. Buildingnet: Learning to label 3d buildings. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10397 10407, October 2021. [76] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. [77] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2023. [78] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In European Conference on Computer Vision (ECCV), pages 7288. Springer, 2022. [79] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase: Periodic autoencoders for learning motion phase manifolds. ACM Transactions on Graphics (TOG), 41(4):113, 2022. [80] Sebastian Starke, Paul Starke, Nicky He, Taku Komura, and Yuting Ye. Categorical codebook matching for embodied character controllers. ACM Transactions on Graphics (TOG), 43(4):114, 2024. [81] Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. Local motion phases for learning multi-contact character movements. ACM Transactions on Graphics (TOG), 39(4):541, 2020. [82] Sebastian Starke, Yiwei Zhao, Fabio Zinno, and Taku Komura. Neural animation layering for synthesizing martial arts movements. ACM Transactions on Graphics (TOG), 40(4):116, 2021. [83] Stefan Stojanov, Anh Thai, and James Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17981808, 2021. [84] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. [85] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [86] Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction. In European Conference on Computer Vision, pages 175191. Springer, 2025. [87] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. arXiv, 2023. [88] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [89] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [90] Greg Turk and Marc Levoy. Zippered polygon meshes from range images. In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, pages 311318, 1994. [91] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [92] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201, 2023. 26 [93] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In The European Conference on Computer Vision Workshops (ECCVW), September 2018. [94] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. [95] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems, 29, 2016. [96] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. [97] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19121920, 2015. [98] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [99] Xiaoyu Xiang, Liat Sless Gorelik, Yuchen Fan, Omri Armstrong, Forrest Iandola, Yilei Li, Ita Lifshitz, and Rakesh Ranjan. Make-a-texture: Fast shape-aware texture generation in 3 seconds. arXiv preprint arXiv:2412.07766, 2024. [100] Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, et al. Animal3d: comprehensive dataset of 3d animal pose and shape. arXiv preprint arXiv:2308.11737, 2023. [101] Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, and Shenghua Gao. Cad-mllm: Unifying multimodality-conditioned cad generation with mllm. arXiv preprint arXiv:2411.04954, 2024. [102] Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11791189, 2023. [103] Xingguang Yan, Liqiang Lin, Niloy Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Shapeformer: Transformer-based shape completion via sparse representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62396249, 2022. [104] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. [105] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models, 2023. [106] Yu-Ying Yeh, Jia-Bin Huang, Changil Kim, Lei Xiao, Thu Nguyen-Phuoc, Numair Khan, Cheng Zhang, Manmohan Chandraker, Carl Marshall, Zhao Dong, et al. Texturedreamer: Image-guided texture synthesis through geometry-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43044314, 2024. [107] Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Jiayuan Fan, Gang Yu, Taihao Li, and Tao Chen. Shapegpt: 3d shape generation with unified multi-modal language model. arXiv preprint arXiv:2311.17618, 2023. [108] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42524262, 2024. [109] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 27 [110] Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, and Xifeng Gao. Texpainter: Generative mesh texturing with multi-view consistency. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [111] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. [112] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [113] Lyumin Zhang. https://github.com/mikubill/sd-webui-controlnet/discussions/1236. https://github.com/Mikubill/sd-webui-controlnet/discussions/1236, 2023. [114] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Computer Vision and Pattern Recognition (CVPR), 2018. [115] Song-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu. Sketch2model: View-aware 3d modeling from single free-hand sketches. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60126021, 2021. [116] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. [117] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional sdf diffusion for controllable 3d shape generation. ACM Transactions on Graphics (ToG), 42(4):113, 2023. [118] Jie Zhou, Zhongjin Luo, Qian Yu, Xiaoguang Han, and Hongbo Fu. Ga-sketching: Shape modeling from multi-view sketching with geometry-aligned deep implicit functions. In Computer Graphics Forum. Wiley Online Library, 2023. [119] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. [120] Qingnan Zhou and Alec Jacobson. Thingi10k: dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016."
        }
    ],
    "affiliations": [
        "Tencent"
    ]
}