{
    "paper_title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models",
    "authors": [
        "Rui Wang",
        "Hongru Wang",
        "Boyang Xue",
        "Jianhui Pang",
        "Shudong Liu",
        "Yi Chen",
        "Jiahao Qiu",
        "Derek Fai Wong",
        "Heng Ji",
        "Kam-Fai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field."
        },
        {
            "title": "Start",
            "content": "Harnessing the Reasoning Economy Survey of Efficient Reasoning for Large Language Models Rui Wang1*, Hongru Wang1, Boyang Xue1, Jianhui Pang2, Shudong Liu2, Yi Chen3, Jiahao Qiu4, Derek Fai Wong2, Heng Ji5, Kam-Fai Wong1 1The Chinese University of Hong Kong 2University of Macau 3The University of Hong Kong 4Princeton University 5University of Illinois Urbana-Champaign {rwang, hrwang, byxue}@se.cuhk.edu.hk {nlp2ct.pangjh3, nlp2ct.shudong}@gmail.com https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as valuable resource for advancing research in this evolving area. We also provide public repository to continually track developments in this fast-evolving field. [Work in progress! Welcome to star and add your papers on github repo."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated exceptional performance across various language understanding and generation tasks, particularly with the advent of Chain-of-Thought (CoT) prompting (Wei et al., 2022), which encourages models to generate explicit, step-by-step reasoning to arrive at the final answers. While LLMs excel *Project Lead and Equal Contributions. Significant Contributions. in many scenarios, their reliance on fast, intuitive thinking often falls short when faced with complex reasoning challenges, such as advanced mathematics (AIME, 2025; Zhong et al., 2023) and coding tasks. Therefore, recent studies try to further boost the reasoning capabilities of LLMs, as exemplified by OpenAIs o1 (OpenAI, 2024), DeepSeeks R1 (DeepSeek-AI et al., 2025) and QwQ (QwQ, 2025), to thoroughly explore potential solutions and improve problem-solving accuracy with more slow and deep thinking (Wang et al., 2025a), leading to the blossoming of Large Reasoning Models (LRMs) and new scaling law during the inference (Snell et al., 2024). While this advancement is remarkable, it comes at considerable cost. Such LRMs require significantly more time to think before responding, resulting in much longer CoT reasoning and substantial inference overhead. However, not all tasks demand such extensive and deep thinking since tasks vary in complexity, and applying the one-fitall approach to all tasks results in wasted resources in terms of computation and time. Even worse, not all of the tokens in long thoughts contribute to the final answer. It is found that LRMs often waste excessive resources (Wu et al., 2025; Cuadron et al., 2025) on unnecessary thoughts (e.g., overthinking) while failing to allocate sufficient computation to truly challenging questions (e.g., underthinking) (Snell et al., 2024; Wang et al., 2025e). This imbalance between capability and efficiency presents significant challenge - achieving Reasoning Economy, global optimum that optimizes token usage (budgets) by emphasizing meaningful reasoning steps, reducing redundancy, and dynamically adjusting computational effort based on task complexity. Therefore, it not only ensures the efficiency of LRMs but also unlocks their full potential by \"intelligently\" stopping or diving deeper like humans (benefits) (Aggarwal et al., 2023; Kimi Team et al., 2025). With the grow5 2 0 M 1 3 ] . [ 1 7 7 3 4 2 . 3 0 5 2 : r Figure 1: We first provide review of the Foundation of LRMs, including post-training and test-time methods. Then, we analyze the Challenges of the Reasoning Economy and survey the efforts to achieve the reasoning economy by regulating inefficient model behaviors and improving test-time usage. ing importance of the reasoning economy, there is an urgent need to systematically understand and analyze different reasoning behaviors of LRMs, reveal potential challenges toward efficient LRMs, and clearly showcase corresponding solutions to achieve the reasoning economy. In this survey, we present the first comprehensive and systematic review of reasoning economy for LRMs. Specifically, we begin by establishing the Foundation of LRMs ( 2), quickly dissecting how post-training methods ( 2.1) (i.e, supervised finetuning and reinforcement learning) shape reasoning behaviors and how test-time strategies ( 2.2) (i.e, parallel and sequential) influence the model performance. Building on this foundation, we conduct rigorous analysis of Challenges towards Reasoning Economy ( 3), classifying them into inefficient behaviors from the model it self ( 3.1) or inefficient usage during test-time ( 3.2). Finally, we discuss potential solutions for optimizing reasoning economy in terms of two directions: i) behaviors regulations in post-training( 4), which targets undesirable reasoning behaviors at their source in terms of data, algorithm and even model architecture, and ii) usage improvement in test-time ( 5) to dynamically adapt computation. We also discuss several open challenges, and suggest future research directions ( 6). By offering clear and structured roadmap, our work aims to provide actionable insights to guide future research and foster the development of the reasoning economy for more sustainable LRMs."
        },
        {
            "title": "2 Foundations of LRMs",
            "content": "This section is about the preliminaries of the fundamental methods that underpin the advanced reasoning capabilities of LRMs, including post-training and test-time methods for LRMs. Starting from the pre-trained LLM, post-training methods will shape the capabilities and behaviors of LRMs; and testtime methods will decide in which way the LLMs are used and approach their upper bound by scaling test-time computation."
        },
        {
            "title": "2.1 Post-training",
            "content": "Post-training is crucial phase in the development of LLMs, aimed at enhancing their instructionfollowing capabilities (Wei et al., 2021) and aligning these models with human preferences (Ouyang et al., 2022). This alignment is typically achieved through techniques such as Supervised Fine-Tuning (SFT) (Wei et al., 2021; Ouyang et al., 2022) and Reinforcement Learning (RL) (Bai et al., 2022; Ouyang et al., 2022) methods, e.g., PPO (Schulman et al., 2017) and GRPO (Ramesh et al., 2024). Supervised Fine-tuning SFT plays crucial role in enhancing the zero-shot multi-task performance of LLMs (Achiam et al., 2023). By leveraging highquality task-specific data (Taori et al., 2023; Zhou et al., 2023), SFT refines the models ability to generalize across various domains, improving both accuracy and reliability in real-world applications, such as summarization (Fabbri et al., 2021), machine translation (Pang et al., 2025), and question answering (Kwiatkowski et al., 2019). Recent studies have proposed leveraging selfimprovement methodologies to enhance the reasoning capabilities of models (Zelikman et al., 2022; Huang et al., 2023a; Gulcehre et al., 2023; Khattab et al., 2023; Zelikman et al., 2024; Feng et al., 2024; Li et al., 2024a; Subramaniam et al., 2025). STaR (Zelikman et al., 2022) employs an iterative approach where the large LLM is prompted to generate multiple reasoning chains until the correct solution is obtained, after which the model is fine-tuned on the complete set of successful reasonFoundation of Reasoning LLMs Challenges towards Reasoning Economy Post-training Methods for Reasoning LLMs (2.1) Test-time Methods for Reasoning LLMs (2.2) Inefficient Model Behaviors from Post-training (2.1) Inefficient Model Usage in Test-time (2.2) Supervised Fine-tuning Li et al. (2025a),Muennighoff et al. (2025), Feng et al. (2024),Zelikman et al. (2022) Reinforcement Learning Ramesh et al. (2024),Schulman et al. (2017), Luo et al. (2024),Zhang et al. (2025d) Parallel Methods Wang et al. (2023),Brown et al. (2024), Ankner et al. (2024),Wang et al. (2024b) Sequential Methods Zhou et al. (2024),Liu et al. (2024a), Yao et al. (2023),Xie et al. (2023) Length Bias Overly Cautious Reasoning LLMs Singhal et al. (2024),Wu et al. (2025), Cuadron et al. (2025),Li et al. (2025b) Deceptive Behaviors Fake Thinking Reasoning LLMs Huang et al. (2024), Wang et al. (2025e) Unreasonable Algorithm Selection Parashar et al. (2025),Chen et al. (2024e), Setlur et al. (2025),Dhuliawala et al. (2024) Unreasonable Computation Allocation Yang et al. (2025),Wu et al. (2025), Jin et al. (2024a),Levy et al. (2024a) Data (4.1) High-Quality Data Construction Muennighoff et al. (2025),Li et al. (2025a) Long2short RL a. Quality-length Disentangle Chen et al. (2024d),Park et al. (2024) b. Length Penalty Kimi Team et al. (2025),Yeo et al. (2025) c. Procedure Reward Qu et al. (2025) Algorithm (4.2) Adaptive Budget-aware Tuning Han et al. (2025),Aggarwal and Welleck (2025) CoT Compression a. Explicit b. Implicit Xia et al. (2025),Cui et al. (2025) Deng et al. (2024),Hao et al. (2024) Behaviors Regulation in Post-training (4) n g o R Optimization for Reasoning Economy Architecture (4.3) Adaptive Activated Parameters a. Recurrent Layers Geiping et al. (2025) b. Dynamic Depth Varshney et al. (2024),Kim et al. (2024) Model Cooperation a. Single-Model Routing OpenAI (2024),Pan et al. (2024) b. Multi-Model Cooperation Xia et al. (2023) c. Knowledge Distillation DeepSeek-AI et al. (2025) Input-side (5.1) Adaptive Budget Allocation before Decoding Han et al. (2025),Wang et al. (2025b) Usage Improvement in Test-time (5) Adaptive Decoding Algorithm Snell et al. (2024),Dhuliawala et al. (2024) Output-side (5.2) Adaptive Budget Allocation during Decoding a. Early Stopping Li et al. (2024b),Manvi et al. (2024) b. Search with Pruning Xie et al. (2023),Liao et al. (2025) c. Constrained Decoding Figure 2: Taxonomy of achieving Reasoning Economy, from foundation, challenges, to solutions. Muennighoff et al. (2025),Wang et al. (2025e) ing trajectories. The latest work Self-Reasoning Language Models (SRLM) (Wang et al., 2025a) further extends to more general instruction-tuning datasets by mixing with few reasoning catalyst data, enabling SRLM to self-refine its own rationales iteratively. Though DeepSeek-AI et al. (2025) proves that the SFT stage is no longer must for LRMs by performing RL for pre-trained model, but they also found that SFT contributes to accelerate the training process and achieve better performance for LRMs. Reinforcement Learning In the training of LRMs, Reinforcement Learning (RL) plays key role in enhancing LLMs reasoning capabilities by providing rewards for both the underlying reasoning process and the final answer (OpenAI, 2024; DeepSeek-AI et al., 2025; Schulman et al., 2017; Ramesh et al., 2024), rather than relying exclusively on token-by-token supervision. Moreover, recent studies (Chu et al., 2025) further indicate that RL-based approaches significantly enhance the generalizability of LLMs, allowing them to perform better in wider range of tasks and scenarios. These findings underscore the importance of RL in refining model adaptability and improving reasoning capabilities. The core focus of reinforcement learning currently lies in the design of the reward signals. According to the different granularity of reward signals, there are two major reward models: the Process Reward Model (PRM) and the Outcome Reward Model (ORM). Specifically, the PRM (Lightman et al., 2023) assigns rewards based on intermediate steps within an action sequence rather than solely on the final outcome (Zhang et al., 2025d). PRM enables more fine-grained learning signals, guiding the LLMs toward optimal policies by rewarding beneficial intermediate behaviors. However, there are several limitations. One the one hand, the training data of PRM is challenging to obtain, as it either requires extensive human annotations (Lightman et al., 2023) or large amount of sampling (Wang et al., 2024b), which limits its application. On the other hand, they may also be overly stringent for the reasoning capabilities of LLMs (Chu et al., 2025). In contrast, the ORM (Cobbe et al., 2021) assigns rewards based on the final outcome of solutions (Ankner et al., 2024; Xin et al., 2024; Yang et al., 2024a; Luo et al., 2024). ORM is widely used in tasks like mathematical reasoning and decisionmaking, where the final result could be explicitly given and evaluated. Therefore, it is relatively easier to directly assign the reward based on some rulebased method such as format and answer matching, as used in DeepSeek-AI et al. (2025) and Luong et al. (2024). Despite ORM only provides supervision signal at the outcome level, it still brings exceptional reasoning capabilities since it allows language models to explore reasoning paths without restrictive constraints. For example, the R1 model exhibits the \"Aha\" moment (DeepSeek-AI et al., 2025), i.e., the emerged self-refine and critique abilities with only rule-based accuracy reward. In summary, both PRM and ORM offer distinct advantages and limitations in training LRMs. More effective reward modeling combining the strengths of PRM and ORM still remains an open question."
        },
        {
            "title": "2.2 Test-time Methods",
            "content": "Test-time methods aim to increase the compute for LLMs at test time to get more accurate and reliable results without post-training and often lift the LRMs to compare with further post-training (Snell et al., 2024). We classify the test-time methods into Parallel and Sequential methods, following Snell et al. (2024); Muennighoff et al. (2025). Parallel Methods Parallel methods will make LLMs generate several solutions simultaneously, then select the final answer by majority voting, i.e., Self-Consistency (Wang et al., 2023), or ORM (Ankner et al., 2024; Xin et al., 2024; Yang et al., 2024a), i.e., best-of-N and weighted majority voting (Brown et al., 2024; Wang et al., 2024b). These approaches leverage the collective wisdom of multiple solutions to improve accuracy and robustness. Sequential Methods Sequential methods, in contrast, involve LLMs iteratively refining their previous steps or answers. This category includes Chainof-Thought ((Wei et al., 2022)), self-refinement (Kumar et al., 2024), and search methods such as guided beam search (Yu et al., 2024a; Xie et al., 2023), tree-of-thought (Yao et al., 2023), and Monte-Carlo Tree Search (MCTS). The execution of these methods often requires PRM (Lightman et al., 2023; Wang et al., 2024b; Zhang et al., 2025e) to determine the most promising branches for exploration, thereby enhancing the efficiency and effectiveness of the solution search process. Summary Previous work found that the potential of LLMs is not fully reached, and the test-time methods aim to approach the upper bound of LLMs. Brown et al. (2024) found that LLaMA-3-8B-Instruct can achieve 98.44% accuracy with 10,000 times repeated sampling and selfconsistency, while only 82.9% with 100 samples. The state-of-the-art LRMs, e.g., o1 and R1, all exhibit natural test-time scaling abilities, such as selfrefinement, back-tracing, and thought-switching behaviors (Yeo et al., 2025) in their extensive intermediate steps. Moreover, repeated sampling could further improve the performances of R1 (DeepSeekAI et al., 2025) and R1-distilled LLMs. In particular, DeepSeek-R1-Distill-Qwen-14B (Yang et al., 2024a) achieves 80% of accuracy on AIME24 (AIME, 2025) by applying majority voting on 64 samples, with only 69.7% accuracy of pass@1. Moreover, Snell et al. (2024) found that the testtime methods are even more effective compared with additional training in easy and medium problems, while this is not the case on difficult problems. reasoning efficiency of LLMs: Length Bias and Deceptive Behaviors."
        },
        {
            "title": "3.1.1 Length Bias",
            "content": "One of the most prominent issues arising from Superficial Alignment is Length Bias, where LLMs tend to generate longer responses that contain much redundant content to maximize their reward scores. Previous research has shown that LLMs trained with RL tend to produce longer responses (Stiennon et al., 2020; Wu et al., 2023) compared to those trained through SFT. As result, several studies have aimed to answer two key research questions: RQ1: What are the reasons for longer responses? and RQ2: Does the increased length indicate bias or an enhancement of model capabilities? Singhal et al. (2024) discovered that in existing reward model training datasets, longer responses are often preferred (e.g., RLCD (Yang et al., 2024b): 63.1%), which leads to length preference in the RM (RQ1). Hence, the lengthbiased RM leads the LLMs to generate redundant content with little performance benefits, e.g., too many paraphrased or connection words. Furthermore, they found that using length as proxy for reward models can yield performance comparable to that of PPO with RMs (RQ2). Other studies have also indicated that RMs struggle to effectively disentangle length bias from response quality during RL training (Singhal et al., 2024; Chen et al., 2024d; Park et al., 2024; Lu et al., 2024a). Overly Cautious LRMs (Chen et al., 2024d) DeepSeek-AI et al. (2025) found that an accuracyand format-based reward function of RL process for LRMs is already effective, and can largely avoid reward hacking. Thus, the Deepseek R1 exhibits surprising improvement in complex reasoning tasks. Experiments (DeepSeek-AI et al., 2025; Kimi Team et al., 2025) suggest that such improvements from that the LRMs experience an \"Aha\" moment during the RL process: the reasoning path becomes longer and more complex, leading to behaviors such as self-refinement, recognizing and correcting mistakes, breaking down difficult steps, and iterating on alternative approaches(R1, R2). Though progress has been made, recent studies have also identified an \"overly cautious\" phenomenon (Li et al., 2025b; Wu et al., 2025; Cuadron et al., 2025; Jin et al., 2024a) in LRMs, characterized by excessive verification and redundant reasoning after giving the right answer (R2). Figure 3: An illustration of Overly Cautious and Fake Thinking Behaviors of LRMs. Circles: steps; Solid circles: answers. Overly cautious LLMs may repeatedly verify even after finding the correct answer. Fake thinking LLMs may show behaviors like self-refinement or back-tracing. Yet, these behaviors are often superficial, not driven by genuine awareness of needing to improve the answer."
        },
        {
            "title": "Economy",
            "content": "In this section, we primarily examine the challenges in LRMs that impair the reasoning economy. We first analyze the Inefficient Model Behaviors caused by post-training methods, which impair the LRMs performance and lead to computation waste. Then, we exhibit the importance of adaptive testtime setting to save computation while achieving higher performance. 3.1 Inefficient Model Behaviors from Post-training LRMs are post-trained to align with human preferences and improve reasoning abilities. RL methods play key role in the post-training stage for LRMs. However, RL optimization relies on reward models (RMs) that are inherently imperfect, primarily due to unreliable human preference annotations (Singhal et al., 2024; Gao et al., 2023). Consequently, over-optimizing based on these flawed RMs can negatively impact the overall capabilities of LLMs. This situation highlights the risk of reward hacking (Skalse et al., 2022), where models exploit the reward function to achieve high scores without genuinely aligning with human preferences. As result, LLMs may exhibit over-optimization (Gao et al., 2023) or Superficial Alignment, appearing to meet human expectations while lacking true understanding. Next, we will analyze previous research and summarize two notable Superficial Alignment behaviors that directly impact the This behavior stems from the assumption that longer outputs are more likely to contain the correct answer or appear more comprehensive, even when shorter, more concise responses would suffice. This overly cautious behavior not only results in inefficient token usage but also hampers LLM performance due to cumulated errors and the \"lost in the middle\" (Liu et al., 2024b; Levy et al., 2024b). self-refinement and thought-changing. Anderson et al. (2025) found that R1 lacks of confidence and tends to give up quickly on those problems they can explain why the answer is correct and get stuck \"thinking forever\". According to the reasoning boundary theory (Chen et al., 2024f), fake thinking can also be treated as the meaningless attempts of LLMs while solving problems beyond their capabilities."
        },
        {
            "title": "Fake Thinking LRMs",
            "content": "LRMs exhibit excessive unnecessary verification and redundant reasoning on easyto-handle questions or meaningless paraphrases and deviations, leading to inefficient token usage and increased computational costs."
        },
        {
            "title": "3.1.2 Deceptive Behaviors",
            "content": "Deceptive behavior refers to instances where LLMs appear to align with human preferences, but these behaviors either fail to produce tangible outcomes (Wang et al., 2024e) or conceal other underlying objectives (Greenblatt et al., 2024; Xu et al., 2024). For example, Greenblatt et al. (2024) has demonstrated that LLMs may display differential behaviors across various demographic groups, which could only be found by comparing LLMs responses to different people. Deceptive behavior is more challenging to detect than length bias or overly cautious behaviors, thus extensive and meticulous human observations (Greenblatt et al., 2024; Wang et al., 2025e) are needed to identify it. Fake Thinking LRMs In the context of LRMs, recent studies have uncovered Fake thinking behavior: They tend to generate plausible, sound reasoning steps that lack logical rigor or correctness. Some studies (Kamoi et al., 2024; Wang et al., 2025e; Huang et al., 2024) found that LRMs appear to engage in self-refinement or deliberate reasoning processes by analyzing the generated solutions. However, empirical evidence suggests that such behaviors are often merely superficial, with little substantial progress being made toward problem-solving. These deceptive behaviors can make LLMs seem like they are working, but in fact, they are wasting computational resources, and even hurting their performances (Gou et al.; Huang et al., 2024). For instance, Wang et al. (2025e) has found that LRMs tend to abandon high-quality reasoning steps early on and engage in ineffective LRMs appear to work towards problemsolving, but these actions are often superficial and do not lead to meaningful progress. 3.2 Inefficient Model Usage in Test-time Though the test-time methods could push the performance of LRMs further in training-free way, the application of test-time methods is often suboptimal (Snell et al., 2024; Levy et al., 2024a; Li et al., 2024b). Previous work found that two dimensions significantly influence the test-time performance of LLMs: the selection of inference algorithm (Snell et al., 2024) and test-time computation allocated to each question (Levy et al., 2024a; Li et al., 2024b)."
        },
        {
            "title": "3.2.1 Unreasonable Algorithm Selection\nTo employ a test-time method, there are two dimen-\nsions to be decided if we do not need to consider\ncomputation limitation: (1) which algorithm and\n(2) what are the hyper-parameters. The often case\nof employing the test-time method is to select one\n(e.g., sampling) and set its parameters (e.g., tem-\nperature, top-p). However, these are not sufficient\nto achieve efficient reasoning.",
            "content": "For the choice of inference algorithm, Parashar et al. (2025) found that there is not single inference algorithm that suits all of the tasks. To be more specific, Chen et al. (2024e) found that majority voting in LLMs improves accuracy on simple problems but degrades performance on complex ones as votes increase. Similarly, Snell et al. (2024) concluded that search-based methods outperform parallel methods on harder problems. Setlur et al. (2025) argue that the verifier is must for test-time scaling. As for the parameter setting of the inference algorithm, recent work (Dhuliawala et al., 2024) found that lower temperature of sampling is more suitable for reasoning tasks, while creative tasks need higher temperature. The above studies emphasize the need for algorithmic adaptability based on task complexity."
        },
        {
            "title": "3.2.2 Unreasonable Computation Allocation",
            "content": "As mentioned before, though scaling the computation could bring consistent performance benefits, scaling LLaMA-3-8B-Instruct from generating 100 samples to 10,000 samples is often unacceptable for simple question. However, more complex problem is worth high computation budget, while small computation budget could lead to suboptimal accuracy. Recent studies propose the Reasoning Boundary (Chen et al., 2024f) of LRMs, in which they found that middle-complexity problems need more computation. For the sequential inference algorithm, recent studies (Wu et al., 2025; Chen et al., 2025a; Jin et al., 2024a; Levy et al., 2024a) found that longer solutions with more selfrefinement are not necessarily better. Specifically, Wu et al. (2025) found that there is an optimal length for sequential refinement, and harder problems need longer optimal length. Snell et al. (2024) made an empirical study and found that there is an optimal computation allocation for problems with different complexities. Recently, Yang et al. (2025) found that there exists an optimal length distribution across different domains. Hence, inadequate computation will lead to the wrong answer, while more tokens impair the performance. Above all, previous work empirically found that there is not one-for-all computation setting for all tasks and samples varied in complexity. They emphasize the importance of adaptive computation allocation based on task complexity."
        },
        {
            "title": "4 Optimization for Reasoning Economy",
            "content": "part-1: Post-training In this section, we provide an overview of posttraining optimization methods aimed at enhancing reasoning economy in LLMs. These methods focus on regulating LLM behaviors to improve efficiency and reduce unnecessary computational overhead from Data, Algorithm, and Architecture perspectives."
        },
        {
            "title": "4.1 Data",
            "content": "High Quality Data Construction By explicitly encoding desired reasoning patterns and behaviors, researchers can guide LLMs toward more advanced and effective performance (Zhou et al., 2023; Li et al., 2025a; Qin et al., 2024; Muennighoff et al., 2025). Qin et al. (2024) utilize small-scale long-thought datasets sampled by testtime scaling (Zhou et al., 2024; Liu et al., 2024a) to enhance the reasoning performance of LLMs, and find that the obtained LLM exhibits clear long thought reasoning patterns. Similarly, Muennighoff et al. (2025) demonstrated that just 1,000 highquality and diverse samples for SFT could produce competitive LRMs comparable to advanced models like o1-preview. They found that the Quality, Diversity, and Difficulty of constructed data are important."
        },
        {
            "title": "4.2 Algorithm",
            "content": "Recent research has focused on three key strategies: optimizing reward structures to balance response length and quality (Long2short RL), enforcing explicit length constraints during training and inference (Adaptive Budget-aware Tuning), and developing compression techniques to eliminate redundant reasoning steps while preserving accuracy (CoT Compression). These approaches collectively aim to enhance inference efficiency by either directly guiding models to generate concise responses or restructuring reasoning processes for more compact representations."
        },
        {
            "title": "4.2.1 Long2short RL",
            "content": "To address the inefficiencies caused by length bias in RL-tuned LLMs, researchers have proposed various reward design improvements. For example, Singhal et al. (2024) explored intuitive approaches, such as increasing the KL coefficient, applying length penalties to reward model scores, and discarding overly long rollouts. Despite these efforts, RL-tuned models still tend to produce longer responses than SFT models. Recently, Kimi Team et al. (2025) proposed the long2short RL approach, which aims to transform lengthy and unnecessary reasoning processes into concise and accurate ones. They investigated strategies like model merging, shortest rejection sampling, and DPO optimization for shorter responses. They also introduced long2short RL, which uses normalized reward model across multiple responses, significantly reducing output length while maintaining reasoning quality. The following explores the long2short RL approach designed to improve the efficiency and effectiveness of RL-tuned LLMs. Quality-Length Reward Disentangle One approach to mitigate length bias is to develop more sophisticated reward models that can better distinguish between response quality and length (Park Figure 4: Post-training Methods Optimization for Reasoning Economy. et al., 2024; Chen et al., 2024d). Chen et al. (2024d) and Park et al. (2024) jointly train two reward heads on shared feature representations, one trained to correlate with length, and the other trained to focus on quality while ignoring the length. Length Penalty or Normalization Meng et al. (2024) proposed simple length normalization way for DPO (Rafailov et al., 2023) to eliminate the influence of length, which has been proven quite effective in alleviating the length bias (Chen et al., 2025a). Yeo et al. (2025) utilize cosine reward to incentivize different length scaling behaviors, eliminating length bias. Chen et al. (2025a) observed that LRMs often provide multiple correct answers within single solution. Leveraging this pattern, they split these solutions into multiple shorter ones and constructed preference pairs, with the first short and correct answer as the preferred solution. They optimized the LLMs using DPO and SimPO, finding SimPO (Meng et al., 2024) to be more effective, reducing response lengths by 30% to 40%. Reward Rule-based Procedure reward (DeepSeek-AI et al., 2025), i.e., accuracy-based reward, has been proved to be quite effective, and less likely to suffer from reward hacking (Skalse et al., 2022). However, recent study (Qu et al., 2025) highlights limitation of accuracy-based rewards in shaping LLM reasoning: They fail to ensure efficient utilization of scaled computation during inference. To address this, the authors propose the Meta Reinforcement Finetuning (MRT) by using cumulative regret as reinforcement learning objective, encouraging LLMs to make incremental progress with each token. This approach was shown to enable stable performance improvements as computational resources scale."
        },
        {
            "title": "4.2.2 Adaptive Budget-aware Tuning",
            "content": "Some studies have focused on explicitly guiding LLMs to adhere to token budgets by specifying desired response lengths in prompts. More importantly, they further post-trained LRMs to follow budget constraints better while achieving higher performance. Yuan et al. (2024) trained LLMs to follow length constraints by constructing SFT data with explicit length specifications. Aggarwal and Welleck (2025) extended this approach by using RL to optimize models for both accuracy and length control. Their results demonstrated superior performance across various token budget settings. Additionally, it has been observed that there is token elasticity phenomenon, meaning that overly strict constraints can lead to increased token costs (Han et al., 2025). To address this issue, budget prediction and allocation paradigm was implemented. Specifically, zero-shot or regression-based budget estimator was used to predict suitable budget, thereby avoiding excessive computation and overly strict constraints. This approach achieved 67% reduction in response length with only 3% loss in accuracy."
        },
        {
            "title": "4.2.3 CoT Compression",
            "content": "It is evident that longer CoT suffers from token redundancy and inference latency, which is not desired in practice. Thus, several studies try to identify important tokens and eliminate unnecessary tokens or reasoning steps, therefore enhancing the inference economy. These methods can be broadly categorized into two categories: 1) explicit compression that directly enforces the model to generate more concise reasoning by fine-tuning on carefully curated datasets or providing specific demonstrations.; and 2) implicit compression that maps multiple reasoning tokens or steps into continuous space to achieve more compact representation."
        },
        {
            "title": "4.3 Architecture",
            "content": "Explicit Compression The first step of compression is to identify key tokens (Lee et al., 2025) or step in the reasoning processing, while most methods rely on perplexity as primary metric, using it as proxy to determine the importance of individual tokens or steps. Some of the methods refine the supervised fine-tuning dataset, replacing lengthy reasoning with identified key reasoning steps as target outputs (Xia et al., 2025). Another approach uses key reasoning steps as demonstrations for incontext learning, guiding LLMs to generate only essential steps during inference while minimizing unimportant tokens (Cui et al., 2025). key limitation of these methods is the potential loss of coherence and comprehensive understanding of the full reasoning process. While the full CoT can be reconstructed using additional LLMs, the recovered version may not fully align with the original, leading to inconsistencies or missing nuances and additional inference costs. Implicit Compression Besides explicit compression, another line of work argues that the explicit language space (i.e., tokens) may not always be optimal for reasoning, as most word tokens are primarily for textual coherence and not essential for reasoning (Deng et al., 2023; Hao et al., 2024). The earlier work focuses on the implicit chain-ofthought approach, which compiles explicit CoT reasoning of the teacher model into student model that directly produces the final answer to develop more efficient reasoning (Deng et al., 2023, 2024). Another representative work is Coconut (Chain of Continuous Thought) (Hao et al., 2024), which feed hidden state instead of specific token back to the LLM as the subsequent input embedding, allowing the model to encode multiple alternative next reasoning steps in the continuous space and thus reducing the token cost, followed by Cheng and Durme (2024). Recent advances in memoryefficient architectures, such as Anchor-based LLMs (AnLLMs) (Pang et al., 2024), further highlight the inefficiency of token-based representations by demonstrating that compressing sequence information into anchor tokens can significantly reduce memory usage while maintaining accuracy, followed by Zhang et al. (2025c). To reduce the need to fully finetune the models, recent studies use an additional projection module to inject the compressed continuous states of the source model into the target model (Xu et al., 2025). One approach to enhancing efficiency is reducing active model parameters, requiring adjustments in model or system architecture. Two key strategies are integrating System-1 and System-2 thinking and adaptively utilizing model depth. System-1 and System-2 cooperation enables dynamic selection between fast, intuitive reasoning and slower, deliberate processing, optimizing efficiency. Meanwhile, adaptive parameter activation optimizes model depth and resource allocation during inference, balancing performance and computational cost."
        },
        {
            "title": "4.3.1 System-1 and System-2 Cooperation",
            "content": "Drawing inspiration from human cognition, several studies propose different methods to dynamically switch from slow, conscious, deliberate reasoning (System 2) to fast, automatic, intuitive thinking (System 1), therefore achieving better balance between fast and accurate thought. Specifically, there are different paths towards this goal. Single-Model Routing It is straightforward to empower one model with abilities to switch between fast (system-1) and slow (system-2) inference methods according to difficulty signals, thereby optimizing both efficiency and effectiveness. On the one hand, the choice can be left to the user to determine the extent of cognitive effort exerted by the model. For instance, OpenAI has equipped the O1 (OpenAI, 2024) with three distinct thinking modes: low, middle, and high. The high thinking mode delivers accurate yet lengthy responses tailored for extremely challenging problems, while the low mode provides swift responses that are satisfactory. However, this direction has not been sufficiently explored thus far. How to train models to possess diverse modes of thinking and how to evaluate the capability gap between different modes (Ballon et al., 2025) of thinking in models remain underexplored areas. On the other hand, the model can be designed to autonomously determine the appropriate level of cognitive effort based on the complexity of the task at hand (Wang et al., 2025c; Ong et al., 2025; Pan et al., 2024; Saha et al., 2024; Ding et al., 2024). This autonomous selection process can be facilitated by incorporating mechanism that assesses the difficulty of the input and dynamically adjusts the inference method accordingly. Ding et al. (2024) leverages binary router to assign queries to smaller or larger based on the difficulty and expected quality. Pan et al. (2024) use consistency verification and reasoning steps as complexity signals to determine which questions require slow, deliberate thinking and which do not. Similarly, Saha et al. (2024) curates dataset that trains models to decompose problems into sub-goals of varying difficulty levels and adaptively apply different strategies to solve them. Multi-Model Collaboration One common approach to accelerating inference is speculative decoding (Xia et al., 2023; Kim et al., 2023; Leviathan et al., 2023), which follows draft-thenverify paradigm. This method first generates multiple token candidates efficiently and then verifies them in parallel. Xia et al. (2023) formally analyze this method through formal study and extensive discussion of both drafting and verification phases, while Kim et al. (2023) propose the BiLD framework, where smaller model generates text at low cost, and larger model refines errors in parallel. Further research explores improved drafting and verification strategies using specific reward model (Bachmann et al., 2025; Liao et al., 2025), with broader overview in Xia et al. (2024). In addition, some methods take an agentic approach, dynamically determining when to engage larger models for refinement at different granularity (Lin et al., 2023; Chen et al., 2024c; Zheng et al., 2025). Specifically, SwiftSage (Lin et al., 2023) invokes system 2 thinking for subgoals during inference, while CITER (Zheng et al., 2025) focuses on tokenlevel refinement when needed. Knowledge Distillation Another approach to achieving balance between slow and fast reasoning is knowledge distillation, where knowledge from larger, more complex model (System 2) is transferred to smaller, more efficient model (System 1). The distilled smaller model will be capable of advanced reasoning abilities while costing less in computations. In detail, Yu et al. (2024b) leverages several self-supervised methods to distill higher-quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences. More recently, it has been proved that direct distillation from DeepSeekR1 outperforms applying RL on the base model such as Qwen2.5-32B, setting new record on the reasoning benchmarks (DeepSeek-AI et al., 2025). There are also emerging studies exploring distillation from Transformer to lower computation complexity architectures (Paliotta et al., 2025), such as Mamba, achieving much faster inference for large batches and long sequences with the Mamba-based model."
        },
        {
            "title": "4.3.2 Adaptive Activated Parameters",
            "content": "Performance improvements of deep learning models largely benefit from advancements in deep network design (He et al., 2016). Present LLMs also pay great attention to deep architecture designs. For models with comparable scales of parameters, increasing model depths often yields more significant benefits than broadening widths (Chen and Zou, 2024; Feng et al., 2023), especially in mathematical reasoning tasks (Ye et al., 2025). However, deepening layers will result in larger model sizes, which means that relatively small models with few layers are hard to resolve complex reasoning tasks. Additionally, increasing model layers leads to linear increase in inference computation costs. From the perspective of model depth, we can enhance the inference capabilities of relatively small-scale LLMs by recurrently utilizing intermediate layers or accelerate the inference of deep LLMs by skipping some intermediate layers. Recurrent Layers Recurrent mechanisms enable LLMs to perform arbitrarily many computations before emitting token, which is simple solution for test-time compute scaling for relatively small LLMs with few layers (Geiping et al., 2025). Recent work has described depth-recurrent, looped Transformers and studied their potential benefits with careful theoretical and small-scale analysis (Giannou et al., 2023; Gatmiry et al., 2024; Yang et al., 2024c). Specifically, Fan et al. (2025) established the superior length generalization capabilities of looped Transformers. Yu et al. (2025) proposes framework enhancing CoT reasoning by combining looped and auto-regressive Transformers. Recurrently utilizing layers in Transformerbased LLMs enhances reasoning ability on smallscale LLMs. Dynamic Depth Additionally, inspired by efficient reasoning using model pruning and sparse models, recent works validate that not all layers of LLMs are necessary during inference (Fan et al., 2024). In this way, two methods involving dynamic depths of LLMs are included: Early Exit (Varshney et al., 2024; Schuster et al., 2022) and Skip Layer (Yang et al., 2024d; Men et al., 2024; Kim et al., 2024). Since simpler tasks require fewer Figure 5: Test-time Methods Optimization for Reasoning Economy. The methods are divided into adding optimal computation constraint in the input-side, and selecting the best-performing decoding algorithm and controlling computation usage during decoding in the output-side. layers, while more complex tasks require more layers of inference, achieving efficient LLM inference then becomes when to stop the inference process adaptively based on the input instance (Fan et al., 2024)."
        },
        {
            "title": "5 Optimization for Reasoning Economy",
            "content": "part-2: Test-time Methods Test-time methods can enhance the performance of LRMs without the need for tuning. However, as previously noted ( 3.2), there is significant potential for optimizing test-time usage to achieve greater reasoning efficiency. In this section, we survey existing methods aiming for reasoning economy by optimizing test-time methods to more economically allocating inference computation as shown in Figure 5. 5.1 Input-side Optimization In this section, we introduce those works that attempt to optimize the usage of LLMs by controlling model behaviors through explicit constraints in the input."
        },
        {
            "title": "Decoding",
            "content": "The main idea of Adaptive Budget Allocation before Decoding is to perform Computation Budget Prediction & Allocation before Decoding, i.e., first decide computation budget for the question (Han et al., 2025; Wang et al., 2025b), and force the LLMs to follow the constraint (Nayab et al., 2025; Yuan et al., 2024; Han et al., 2025). Budget Prediction The basic idea of predicting the budget is to consider the difficulty of the problems to the reasoning LLM. According to previous works (Xue et al., 2024; Chen et al., 2024a; Liu et al., 2024c), the confidence of the model to solve the question can be estimated and predicted. As for the computation prediction methods for each question, Han et al. (2025) collected batch of questions with their optimal budgets and trained regression model to predict the computation needed for novel prompts. Meanwhile, Wang et al. (2025b) considered the difficulty of the questions and allocated budgets similar to those of questions with comparable difficulty. Budget Constrained Generation With the budget constraint, then we can indicate the length constraint in the prompt (Nayab et al., 2025; Yuan et al., 2024), thereby instructing the LLMs to give the response while satisfying the constraint. Previous work (Yuan et al., 2024) found that LLMs possess the capacities to follow the length constraint to some extent. To further improve token budget awareness of LLMs, Han et al. (2025) also posttrained LLMs to better follow length constraints."
        },
        {
            "title": "5.2 Output-side Optimization",
            "content": "In this section, we introduce the work of optimizing the usage of LLMs at the output end of the model. This includes Constrained Decoding, Adaptive Algorithm Selection, and Adaptive Computation Allocation during Decoding."
        },
        {
            "title": "5.2.1 Adaptive Algorithm Selection",
            "content": "The exploration of adaptive choice of test-time algorithms is quite limited, especially on efficient thinking. Here, we present several works that adaptively adjust the parameters of test-time algorithms, which could be potentially used to achieve algorithm determination. Snell et al. (2024) demonstrates the potential of adaptive method selection, showing that optimal settings can achieve up to 4x greater efficiency compared to the PRM best-of-N approach. This underscores the benefits of dynamically tailoring test-time strategies to specific tasks. Dhuliawala et al. (2024) introduces an Adaptive Decoding Layer combined with Latent Preference Optimization (LPO), enabling fine-grained temperature prediction at the token or sample level for more precise generation control. Chen et al. (2024g) propose using high initial temperatures to enhance response diversity, which expands the search space and increases the likelihood of discovering correct answers. These approaches collectively emphasize the importance of adaptive mechanisms in optimizing the choice of test-time algorithms, paving the way for more efficient and effective reasoning in LLMs."
        },
        {
            "title": "Decoding",
            "content": "Early Stopping The first line of adaptive budget allocation is early stopping (Manvi et al., 2024; Aggarwal et al., 2023; Li et al., 2024b; Wan et al., 2025; Huang et al., 2025a). Manvi et al. (2024) utilizes the self-evaluation abilities of LLMs to decide whether the current solution is good enough during the sequentially self-refinement procedure. Aggarwal et al. (2023) argue that constant sample number for self-consistency is not cost-efficient, and proposed to stop sampling when consistency rate is reached. Li et al. (2024b) splits the whole sampling window for self-consistency into several smaller ones and stops sampling if all of the answers within small window are the same, thereby greatly reducing the computation cost without performance sacrifice. Recent studies (Wan et al., 2025; Huang et al., 2025a) argue that not all solutions should be treated equally when determining when to perform early stopping. Wan et al. (2025) takes into account both the quality of the reasoning path and whether high level of consistency has been achieved. Huang et al. (2025a), on the other hand, utilizes the models confidence to select the preferred reasoning path and combines this with consistency judgment to decide whether to stop early. Search with Pruning The second line is pruning while searching (Wang et al., 2025d; Zhu et al., 2025; Sun et al., 2024; Qiu et al., 2024), no matter the parallel or sequential methods. The goal is to prune low-quality search branches early while retaining high-quality ones, thereby saving computational resources. For sequential test-time methods, RMs are often used to guide the search procedure (Xie et al., 2023; Yu et al., 2024a; Wang et al., 2024b) by pruning low-quality branches and rolling out high-quality ones. Guided search could eliminate unnecessary computation usage while improving the accuracy of LRMs. As for parallel test-time methods, Wang et al. (2025d) utilizes the self-evaluation abilities of LLMs to prune the low-quality samples during best-of-N sampling. To expand the search space of best-of-N, Sun et al. (2024) set very large sample window at the beginning of the sampling, large enough to consume all of the GPU memory, then gradually pruned those low-quality and completed the saved samples. Zhu et al. (2025) select the top high-quality steps and use these steps to guide and constrain the LLMs to complete these prefixes. Constrained Decoding Identifying suboptimal thinking patterns is often challenging, which makes mitigating them particularly difficult. Hence, previous works have to utilize human analysis to diagnose the deceptive behaviors of LLMs (Wang et al., 2025e; Xu et al., 2024; Kamoi et al., 2024) and more importantly, to utilize human observation to inspire specific solutions for them (Wang et al., 2024c). For example, for fake self-refine of LLMs, Huang et al. (2023b) and Gou et al. suggest using external feedback to assist the self-refine procedure of LLMs. Other works utilize the human-observed behaviors to design force decoding paradigm, aiming to solve the fake thinking and overly cautious behaviors of LRMs, e.g., enforcing LLMs to adhere to the previous thought (Wang et al., 2025e) or performing self-refine at proper time (Muennighoff et al., 2025). Wang et al. (2025e) found that the proposed Thought Switching Penalty approach can encourage LLMs to perform deeper reasoning and reduce unreasonable thought-switching."
        },
        {
            "title": "6 Discussion",
            "content": "Efficient Multi-modal Reasoning Multi-modal large language models (MLLMs) have demonstrated promising capabilities in various multimodal reasoning tasks (Wang et al., 2024d; Zhang et al., 2024), including but not limited to mathematical reasoning (Wang et al., 2024a; Lu et al., 2024b; Zhou et al., 2025c), visual question answering (VQA) (Goyal et al., 2017; Hudson and Manning, 2019), and multi-modal dialogue systems (Huang et al., 2023c). Recent advances, exemplified by o1-like and R1-like technologies, have further catalyzed progress in this field (Huang et al., 2025b; Meng et al., 2025; Peng et al., 2025; Zhang et al., 2025b; Feng et al., 2025; Yao et al., 2024). Current approaches on efficient multi-modal reasoning primarily focus on improvements within MLLMs themselves (Jin et al., 2024b), mainly including: i) model architecture optimization (e.g., lightweight vision encoders (Chen et al., 2024b), vision token compression (Guo et al., 2024), visionlanguage projector (Li et al., 2023), small language models (Chu et al., 2023; Zhu et al., 2024)), and efficient structures (Lin et al., 2024a; Qiao et al., 2024; Lin et al., 2024b); ii) efficient vision techniques adoption (e.g. Pruning (Yu et al., 2022), knowledge distillation (Touvron et al., 2021), and ViT quantization (Li et al., 2022)), among other methods. However, the evaluation and targeted optimization of efficiency in multi-modal (long-) reasoning remain relatively preliminary. Jiang et al. (2025) proposes two metrics for measuring the efficiency of multi-model CoT: relevance rate, which calculates the proportion of content contributing to answering, and reflection quality, which evaluates whether reflection steps can effectively correct previous mistakes or present new insights. Xiang et al. (2025) proposes self-structured COT paradigm that decomposes reasoning into minimal semantic atomic steps to achieve efficient multi-modal reasoning by generating dynamic structures and lengths based on the problem types. Moreover, most of the aforementioned efficient reasoning methods for LLMs can also be effectively applied to MLLMs. Efficient Agentic Reasoning The advent of LRMs has also brought substantial performance improvements to AI agents. Using RL training similar to o1, the OpenAIs Deep Research (OpenAI, 2025) utilizes advanced reasoning capabilities to synthesize large amounts of online information, attaining notable performance on the challenging Humanitys Last Exam (Phan et al., 2025). Recent research has increasingly focused on leveraging long reasoning capabilities in agent systems, where cutting-edge implementations integrate these with retrieval, tool augmentation, domain knowledge incorporation, and other auxiliary components to push performance boundaries (Jin et al., 2025; Alzubi et al., 2025; Gao et al., 2025). To analyze the limited effectiveness of LRMs in interactive environments, Cuadron et al. (2025) presents the first comprehensive empirical study in agentic tasks, analyzes the overthinking in LRMs, including its manifestations, quantification, and impact on different models, and also proposes potential mitigation approaches accordingly. Zhou et al. (2025b) proposes the LaRMA framework to explore the necessity of reasoning capabilities in LRMs for agent scenarios, analyzes the performance differences between LRMs and LLMs across various tasks, and provides insights into optimizing agent performance through hybrid LLMLRM configurations. Some studies also investigate efficient agent training methods (Chen et al., 2025b) and efficient interaction schemes for multiagent systems (Zhou et al., 2025a; Wang et al., 2025f; Zhang et al., 2025a). Evaluation Metrics and Benchmarks With the proliferation of long reasoning and the increasing severity of the over-reasoning problem, researchers have recently begun developing specialized benchmarks and metrics to quantitatively measure reasoning efficiency. Hashemi et al. (2025) proposes DNA Bench to expose the vulnerability of current LRMs and reveals that LRMs generate up to 70 more unnecessary tokens and fail at tasks that nonreasoning models handle efficiently. Chen et al. (2025a) introduces outcome-based and processbased efficiency metrics and concludes that LRMs overthink on simple problems, with later solutions contributing little to accuracy and diversity. Conversely, Wang et al. (2025e) introduces an underthinking metric to reveal the underthinking problem in o1-like LLMs, where models frequently switch wrong reasoning thoughts without fully exploring potentially correct initial thoughts, leading to insufficient reasoning depth and poor performance. Anderson et al. (2025) creates dataset from NPR Sunday Puzzle Challenges to evaluate LRMs reasoning with general knowledge (not PhD-level), also revealing behaviors similar to overthinking. Explainability of LRMs The study of the explainability of black-box LLMs has always been topic of interest (Ge et al., 2024; Gao et al., 2024). Particularly, LRMs have explored on their own through RL and have demonstrated reasoning abilities on par with those of human PhD students (DeepSeek-AI et al., 2025; OpenAI, 2024; Kimi Team et al., 2025; QwQ, 2025). However, the mechanisms underlying their achievement of such performance remain enigmatic. Current research on LRMs often focuses on their behavior analysis (Wu et al., 2025; Cuadron et al., 2025; Yeo et al., 2025), such as the observation of overly cautious or fake thinking behaviors, and then retraces the post-training algorithms or test-time methods. Nevertheless, it is essential to focus on how these models work on the inside, probing the internal mechanisms of LRMs (Anthropic, 2025b,a). Moreover, more user-friendly and powerful toolkits (Razzhigaev et al., 2025; Anthropic, 2025a) are also critical for research purposes and large-scale human analysis. This would help us understand the thought patterns of LRMs, identify their faults, and provide directions for further improvement."
        },
        {
            "title": "7 Conclusion",
            "content": "In this survey, we systematically examined the challenges and solutions involved in achieving an inference economy for large reasoning models, emphasizing the urgent need for efficient reasoning mechanisms that balance computational cost with performance. To the best of our knowledge, this is the first comprehensive review that analyzes the underlying causes, observed phenomena, key challenges, and emerging solutions for enabling efficient reasoning in LLMs, offering structured roadmap and actionable strategies for practical deployment. By anchoring progress in the principles of inference economy, this survey serves not only as synthesis of current knowledge but also as call to action for further research in this path, highlighting the importance of developing more sustainable and scalable models that reason not only effectively but also efficiently."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. 2023. Lets sample step by step: Adaptiveconsistency for efficient reasoning and coding with LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1237512396, Singapore. Association for Computational Linguistics. Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling How Long Reasoning Model Thinks With Reinforcement Learning. ArXiv:2503.04697 [cs]. arXiv preprint. AIME. 2025. Art of Problem Solving. Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, et al. 2025. Open deep search: Democratizing search with open-source reasoning agents. arXiv preprint arXiv:2503.20201. Carolyn Jane Anderson, Joydeep Biswas, Aleksander Boruch-Gruszecki, Federico Cassano, Molly Feldman, Arjun Guha, Francesca Lucchetti, and Zixuan Wu. 2025. Phd knowledge not required: reasoning challenge for large language models. Preprint, arXiv:2502.01584. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, and Prithviraj Ammanabrolu. 2024. Critique-out-loud reward models. Preprint, arXiv:2408.11791. Anthropic. 2025a. Circuit Tracing: Revealing Computational Graphs in Language Models. Anthropic. 2025b. On the Biology of Large Language Model. Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, and Jonas Kohler. 2025. Judge decoding: Faster speculative sampling requires going beyond model alignment. arXiv preprint arXiv:2501.19309. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. Preprint, arXiv:2204.05862. Marthe Ballon, Andres Algaba, and Vincent Ginis. 2025. The Relationship Between Reasoning and Performance in Large Language Models o3 (mini) Thinks Harder, Not Longer. arXiv preprint. ArXiv:2502.15631 [cs]. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. Preprint, arXiv:2407.21787. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024a. INSIDE: LLMs Internal States Retain the Power of Hallucination Detection. arXiv preprint. ArXiv:2402.03744. Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. 2024b. Vitamin: Designing scalable vision models in the vision-language era. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1295412966. Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. 2024c. Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning. Preprint, arXiv:2409.12147. Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. 2024d. ODIN: Disentangled reward mitigates hacking in RLHF. In Forty-first International Conference on Machine Learning. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024e. Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems. arXiv preprint. ArXiv:2403.02419. Qiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. 2024f. Unlocking the Capabilities of Thought: Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought. Weizhe Chen, Zhicheng Zhang, Guanlin Liu, Renjie Zheng, Wenlei Shi, Chen Dun, Zheng Wu, Xing Jin, and Lin Yan. 2024g. Flaming-hot Initiation with Regular Execution Sampling for Large Language Models. arXiv preprint. ArXiv:2410.21236. Xingwu Chen and Difan Zou. 2024. What can transformer learn with varying depth? case studies on In Proceedings of the sequence learning tasks. 41st International Conference on Machine Learning, ICML24. JMLR.org. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025a. Do not think that much for 2+3=? on the overthinking of o1-like llms. Preprint, arXiv:2412.21187. Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, and Tianyi Zhou. 2025b. Atlas: Agent tuning via learning critical steps. arXiv preprint arXiv:2503.02197. Jeffrey Cheng and Benjamin Van Durme. 2024. Compressed chain of thought: Efficient reasoning through dense representations. Preprint, arXiv:2412.13171. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training. arXiv preprint. ArXiv:2501.17161 [cs]. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. 2023. Mobilevlm: fast, strong and open vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. 2025. The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks. arXiv preprint. ArXiv:2502.08235 [cs]. Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Yue Xing, Jiliang Tang, and Qi He. 2025. Stepwise perplexity-guided refinement for efficient chain-of-thought reasoning in large language models. Preprint, arXiv:2502.13260. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint. ArXiv:2501.12948 [cs]. Yuntian Deng, Yejin Choi, and Stuart Shieber. 2024. From explicit cot to implicit cot: Learning to internalize cot step by step. Preprint, arXiv:2405.14838. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2023. Implicit chain of thought reasoning via knowledge distillation. Preprint, arXiv:2311.01460. Shehzaad Dhuliawala, Ilia Kulikov, Ping Yu, Asli Celikyilmaz, Jason Weston, Sainbayar Sukhbaatar, and Jack Lanchantin. 2024. Adaptive Decoding via arXiv preprint. Latent Preference Optimization. ArXiv:2411.09661 version: 1. Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks V. S. Lakshmanan, and Ahmed Hassan Awadallah. 2024. Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing. arXiv preprint. Alexander Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409. Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. 2024. Not all layers of Preprint, llms are necessary during inference. arXiv:2403.02181. Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. 2025. Looped transformers for length generalization. In The Thirteenth International Conference on Learning Representations. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2023. Towards revealing the mystery behind chain of thought: theoretical perspective. In Thirty-seventh Conference on Neural Information Processing Systems. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Yunlong Feng, Yang Xu, Libo Qin, Yasheng Wang, and Wanxiang Che. 2024. Improving language model reasoning with self-motivated learning. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 88408852, Torino, Italia. ELRA and ICCL. Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024. Scaling and arXiv preprint. evaluating sparse autoencoders. ArXiv:2406.04093 [cs]. Shanghua Gao, Richard Zhu, Zhenglun Kong, Ayush Noori, Xiaorui Su, Curtis Ginder, Theodoros Tsiligkaridis, and Marinka Zitnik. 2025. Txagent: An ai agent for therapeutic reasoning across universe of tools. arXiv preprint arXiv:2503.10970. Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, and Sanjiv Kumar. 2024. Can looped transformers learn to implement multi-step gradient descent for in-context learning? Preprint, arXiv:2410.08292. Xuyang Ge, Fukang Zhu, Wentao Shu, Junxuan Wang, Zhengfu He, and Xipeng Qiu. 2024. Automatically Identifying Local and Global Circuits with Linear Computation Graphs. arXiv preprint. ArXiv:2405.13868 [cs]. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025. Scaling up test-time compute with latent reasoning: recurrent depth approach. Preprint, arXiv:2502.05171. Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. 2023. Looped transformers as programmable computers. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1139811442. PMLR. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large Language Models Can Self-Correct with ToolInteractive Critiquing. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, Sören Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, and Evan Hubinger. 2024. Alignment faking in large language models. Preprint, arXiv:2412.14093. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced selftraining (rest) for language modeling. arXiv preprint arXiv:2308.08998. Zonghao Guo, Ruyi Xu, Yuan Yao, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. 2024. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. In European Conference on Computer Vision, pages 390406. Springer. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2025. TokenarXiv preprint. Budget-Aware LLM Reasoning. ArXiv:2412.18547 [cs]. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. Preprint, arXiv:2412.06769. Masoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan, Jishnu Sethumadhavan Nair, Aman Tiwari, and Vikas Yadav. 2025. DNA Bench: When Silence is Smarter Benchmarking OverarXiv preprint. Reasoning in Reasoning LLMs. ArXiv:2503.15793 [cs]. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025a. Efficient TestTime Scaling via Self-Calibration. arXiv preprint. ArXiv:2503.00031 [cs]. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023a. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10511068, Singapore. Association for Computational Linguistics. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023b. Large Language Models Cannot Self-Correct Reasoning Yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language In The models cannot self-correct reasoning yet. Twelfth International Conference on Learning Representations. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. 2025b. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. 2023c. Sparkles: Unlocking chats across multiple images for multimodal instruction-following models. arXiv preprint arXiv:2308.16463. Drew Hudson and Christopher Manning. 2019. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. 2025. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. 2024a. The impact of reasoning step In Findings of length on large language models. the Association for Computational Linguistics: ACL 2024, pages 18301842, Bangkok, Thailand. Association for Computational Linguistics. Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, et al. 2024b. Efficient multimodal large language models: survey. arXiv preprint arXiv:2405.10739. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When can LLMs actually correct their own mistakes? critical survey of selfcorrection of LLMs. Transactions of the Association for Computational Linguistics, 12:14171440. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714. Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and HyoungKyu Song. 2024. Shortened llama: Depth pruning for large language models with comparison of retraining methods. Preprint, arXiv:2402.02834. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir Gholami, and Kurt Keutzer. 2023. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36:3923639256. on the reasoning performance of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1533915353, Bangkok, Thailand. Association for Computational Linguistics. Angang Kimi Team, and Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025. Kimi k1.5: Scaling Reinforcement Learning with LLMs. arXiv preprint. ArXiv:2501.12599 [cs]. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training Language Models to Self-Correct via Reinforcement Learning. arXiv preprint. ArXiv:2409.12917. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Ayeong Lee, Ethan Che, and Tianyi Peng. 2025. How Well do LLMs Compress Their Own Chain-ofThought? Token Complexity Approach. arXiv preprint. ArXiv:2503.01141 [cs]. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via specIn International Conference on ulative decoding. Machine Learning, pages 1927419286. PMLR. Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024a. Same task, more tokens: the impact of input length Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024b. Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1533915353, Bangkok, Thailand. Association for Computational Linguistics. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. 2024a. Large language models can self-improve in long-context reasoning. arXiv preprint arXiv:2411.08147. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025a. Preprint, Limr: Less is more for rl scaling. arXiv:2502.11886. Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo. 2022. Q-vit: Accurate and fully quantized low-bit vision transformer. Advances in neural information processing systems, 35:3445134463. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. 2024b. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. Preprint, arXiv:2401.10480. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, and Cheng-Lin Liu. 2025b. From System 1 to System 2: Survey of Reasoning Large Language Models. arXiv preprint. ArXiv:2502.17419 [cs]. Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. 2025. Reward-guided speculative decoding for efficient llm reasoning. Preprint, arXiv:2501.19324. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets Verify Step by Step. arXiv preprint. ArXiv:2305.20050. Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. 2023. Swiftsage: generative agent with fast and slow thinking for complex interactive tasks. Advances in Neural Information Processing Systems, 36:23813 23825. Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, et al. 2024a. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947. Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. 2024b. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. arXiv preprint arXiv:2405.05803. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. 2024a. Dont throw away your value model! generating more preferable text with valueguided monte-carlo tree search decoding. Preprint, arXiv:2309.15028. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12:157173. Place: Cambridge, MA Publisher: MIT Press. Shudong Liu, Zhaocong Li, Xuebo Liu, Runzhe Zhan, Derek F. Wong, Lidia S. Chao, and Min Zhang. 2024c. Can LLMs learn uncertainty on their own? expressing uncertainty effectively in self-training manner. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2163521645, Miami, Florida, USA. Association for Computational Linguistics. Junru Lu, Jiazheng Li, Siyu An, Meng Zhao, Yulan He, Di Yin, and Xing Sun. 2024a. Eliminating biased length reliance of direct preference optimization via In Proceedings of down-sampled KL divergence. the 2024 Conference on Empirical Methods in Natural Language Processing, pages 10471067, Miami, Florida, USA. Association for Computational Linguistics. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024b. Mathvista: Evaluating mathematical reasoning of In Interfoundation models in visual contexts. national Conference on Learning Representations (ICLR). Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024. Reft: Reasoning with reinforced fine-tuning. Preprint, arXiv:2401.08967. Rohin Manvi, Anikait Singh, and Stefano Ermon. 2024. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. Preprint, arXiv:2410.02725. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language models are more redundant than you expect. Preprint, arXiv:2403.03853. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. 2025. Mm-eureka: Exploring visual aha moment with rulearXiv based large-scale reinforcement learning. preprint arXiv:2503.07365. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. SimPO: Simple Preference Optimization with arXiv preprint. Reference-Free Reward. ArXiv:2405.14734 [cs]. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. 2025. Concise thoughts: Impact of output length on llm reasoning and cost. Preprint, arXiv:2407.19825. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, Waleed Kadous, and Ion Stoica. 2025. Routellm: Learning to route llms with preference data. Preprint, arXiv:2406.18665. OpenAI. 2024. OpenAI o1. OpenAI. 2025. Deep research. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Li, Aviv Bick, Zico Kolter, Albert Gu, François Fleuret, and Tri Dao. 2025. Thinking slow, fast: Scaling inference compute with distilled reasoners. arXiv preprint arXiv:2502.20339. Jiabao Pan, Yan Zhang, Chen Zhang, Zuozhu Liu, Hongwei Wang, and Haizhou Li. 2024. DynaThink: Fast or slow? dynamic decision-making framework for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1468614695, Miami, Florida, USA. Association for Computational Linguistics. Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. 2025. Optimizing Test-Time Compute via Meta Reinforcement FineTuning. arXiv preprint. ArXiv:2503.07572. Jianhui Pang, Fanghua Ye, Derek Wong, Xin He, Wanshun Chen, and Longyue Wang. 2024. Anchor-based In Findings of the Assolarge language models. ciation for Computational Linguistics: ACL 2024, pages 49584976, Bangkok, Thailand. Association for Computational Linguistics. Jianhui Pang, Fanghua Ye, Derek Fai Wong, Dian Yu, Shuming Shi, Zhaopeng Tu, and Longyue Wang. 2025. Salute the classic: Revisiting challenges of machine translation in the age of large language models. Transactions of the Association for Computational Linguistics, 13:7395. Shubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, and Shuiwang Ji. 2025. Inference-time computations for llm reasoning and planning: benchmark and insights. Preprint, arXiv:2502.12521. Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. 2024. Disentangling length from quality in direct preference optimization. In Findings of the Association for Computational Linguistics: ACL 2024, pages 49985017, Bangkok, Thailand. Association for Computational Linguistics. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. 2025. Lmmr1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. arXiv preprint 2025. Humanitys last exam. arXiv:2501.14249. Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. 2024. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 Replication Journey: Strategic Progress Report Part 1. arXiv preprint. ArXiv:2410.18982 [cs]. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. 2024. Treebon: Enhancing inference-time alignment with speculative treearXiv preprint search and best-of-n sampling. arXiv:2410.16033. QwQ. 2025. QwQ-32B: Embracing the Power of Reinforcement Learning Qwen. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Curran Associates, Inc. Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou Ammar, and Ilija Bogunovic. 2024. Group robust preference optimization in reward-free RLHF. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Anton Razzhigaev, Matvey Mikhalchuk, Temurbek Rahmatullaev, Elizaveta Goncharova, Polina Druzhinina, Ivan Oseledets, and Andrey Kuznetsov. 2025. LLMMicroscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers. arXiv preprint. ArXiv:2502.15007 [cs]. Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal. 2024. System-1. x: Learning to balance fast and slow planning with language models. arXiv preprint arXiv:2407.14414. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxPreprint, Alec Radford, and Oleg Klimov. 2017. imal policy optimization algorithms. arXiv:1707.06347. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. 2022. Confident adaptive language modeling. In Advances in Neural Information Processing Systems, volume 35, pages 1745617472. Curran Associates, Inc. Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. 2025. Scaling Test-Time Compute Without Verification or RL is Suboptimal. arXiv preprint. ArXiv:2502.12118 [cs]. Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. 2024. long way to go: Investigating length correlations in RLHF. In First Conference on Language Modeling. Joar Skalse, Nikolaus H. R. Howe, Dmitrii KrasheninDefining Preprint, nikov, and David Krueger. 2022. and characterizing reward hacking. arXiv:2209.13085. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv preprint. ArXiv:2408.03314 [cs]. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning In Proceedto summarize from human feedback. ings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, pages 30083021, Red Hook, NY, USA. Curran Associates Inc. Vighnesh Subramaniam, Yilun Du, Joshua Tenenbaum, Antonio Torralba, Shuang Li, and Igor Mordatch. 2025. Multiagent finetuning: Self improvement with diverse reasoning chains. arXiv preprint arXiv:2501.05707. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast Best-of-N Decoding via Speculative Rejection. arXiv preprint. ArXiv:2410.20290 [cs]. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR. Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. 2024. Investigating acceleration of LLaMA inference by enabling intermediate layer decoding via instruction tuning with LITE. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 36563677, Mexico City, Mexico. Association for Computational Linguistics. Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. 2025. Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling. arXiv preprint. ArXiv:2408.17017 [cs]. Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Pan, Zeming Liu, and Kam-Fai Wong. 2025a. Self-reasoning language models: Unfold hidden reasoning chains with few reasoning catalyst. In Workshop on Reasoning and Planning for Large Language Models. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. 2024a. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2024b. Math-Shepherd: Verify and Reinforce LLMs Stepby-step without Human Annotations. arXiv preprint. ArXiv:2312.08935. Rui Wang, Hongru Wang, Fei Mi, Boyang Xue, Yi Chen, Kam-Fai Wong, and Ruifeng Xu. 2024c. Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 53455363, Mexico City, Mexico. Association for Computational Linguistics. Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. 2025b. Make every penny count: Difficultyadaptive self-consistency for cost-efficient reasoning. Preprint, arXiv:2408.13457. Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, and Haifeng Chen. 2025c. Mixllm: Dynamic routing in mixed large language models. Preprint, arXiv:2502.18482. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. 2025d. Sampling-Efficient Test-Time Scaling: SelfEstimating the Best-of-N Sampling in Early Decoding. arXiv preprint. ArXiv:2503.01422 [cs]. Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. 2024d. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning. arXiv preprint arXiv:2401.06805. Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, YuGang Jiang, Yu Qiao, and Yingchun Wang. 2024e. Fake alignment: Are LLMs really aligned well? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46964712, Mexico City, Mexico. Association for Computational Linguistics. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025e. Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs. arXiv preprint. ArXiv:2501.18585 [cs]. Zhexuan Wang, Yutong Wang, Xuebo Liu, Liang Ding, Miao Zhang, Jie Liu, and Min Zhang. 2025f. Agentdropout: Dynamic agent elimination for tokenefficient and high-performance llm-based multi-agent collaboration. arXiv preprint arXiv:2503.18891. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, pages 2482424837, Red Hook, NY, USA. Curran Associates Inc. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025. When More is Less: Understanding Chain-of-Thought Length in LLMs. arXiv preprint. ArXiv:2502.07266 [cs]. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023. Finegrained human feedback gives better rewards for language model training. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, pages 5900859033, Red Hook, NY, USA. Curran Associates Inc. Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. 2023. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 39093925, Singapore. Association for Computational Linguistics. Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. Preprint, arXiv:2502.12067. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking efficiency in large language model inference: comprehensive survey In Findings of the Assoof speculative decoding. ciation for Computational Linguistics: ACL 2024, pages 76557671, Bangkok, Thailand. Association for Computational Linguistics. Self-evaluation guided beam search for reasoning. Preprint, arXiv:2305.00633. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. Preprint, arXiv:2405.14333. Ruoxi Xu, Hongyu Lin, Xianpei Han, Jia Zheng, Weixiang Zhou, Le Sun, and Yingfei Sun. 2024. Large Language Models Often Say One Thing and Do Another. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025. Softcot: Soft chain-of-thought for efficient reasoning with llms. Preprint, arXiv:2502.12134. Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu, and Kam-Fai Wong. 2024. UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models. arXiv preprint. ArXiv:2412.11803 [cs]. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024a. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. Preprint, arXiv:2409.12122. Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2024b. RLCD: Reinforcement learning from contrastive distillation for LM alignment. In The Twelfth International Conference on Learning Representations. Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. 2024c. Looped transformers are better at learning learning algorithms. In The Twelfth International Conference on Learning Representations. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. 2025. Towards thinking-optimal scaling of Preprint, test-time compute for llm reasoning. arXiv:2502.18080. Yifei Yang, Zouying Cao, and Hai Zhao. 2024d. LaCo: Large language model pruning via layer collapse. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 64016417, Miami, Florida, USA. Association for Computational Linguistics. Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, et al. 2025. Can atomic step decomposition enhance the selfstructured reasoning of multimodal large models? arXiv preprint arXiv:2503.06252. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. 2024. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv preprint. ArXiv:2305.10601 [cs]. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan AllenZhu. 2025. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. In The Thirteenth International Conference on Learning Representations. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying Long Chainof-Thought Reasoning in LLMs. arXiv preprint. ArXiv:2502.03373 [cs]. Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui. 2022. Width & depth pruning for vision transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 31433151. Fei Yu, Anningzhe Gao, and Benyou Wang. 2024a. OVM, outcome-supervised value models for planning in mathematical reasoning. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 858875, Mexico City, Mexico. Association for Computational Linguistics. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024b. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023. Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, and Di He. 2025. Enhancing autoregressive chain-of-thought through loop-aligned reasoning. Preprint, arXiv:2502.08482. Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu. 2024. Following length constraints in instructions. Preprint, arXiv:2406.17744. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. 2024. Quiet-star: Language models can teach themselves In First Conference on to think before speaking. Language Modeling. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. 2025a. Cut the crap: An economical communication pipeline for LLM-based multi-agent systems. In The Thirteenth International Conference on Learning Representations. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. 2025b. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937. Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025c. Lightthinker: Thinking stepby-step compression. Preprint, arXiv:2502.15589. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025d. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025e. The lessons of developing process reward models in mathematical reasoning. Preprint, arXiv:2501.07301. Zhuosheng Zhang, Aston Zhang, Mu Li, George Karypis, Alex Smola, et al. 2024. Multimodal chainof-thought reasoning in language models. Transactions on Machine Learning Research. Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric Xing, Hongyi Wang, and Huaxiu Yao. 2025. Citer: Collaborative inference for efficient large language model decoding with token-level routing. arXiv preprint arXiv:2502.01976. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: humancentric benchmark for evaluating foundation models. Preprint, arXiv:2304.06364. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2024. Language agent tree search unifies reasoning acting and planning in language models. Preprint, arXiv:2310.04406. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less Is More for Alignment. arXiv preprint. ArXiv:2305.11206 [cs]. Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, and Lei Bai. 2025a. Reso: reward-driven selforganizing llm-based multi-agent system for reasoning tasks. arXiv preprint arXiv:2503.02390. Xueyang Zhou, Guiyao Tie, Guowen Zhang, Weidong Wang, Zhigang Zuo, Di Wu, Duanfeng Chu, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. 2025b. Large reasoning models in agent scenarios: Exploring the necessity of reasoning capabilities. arXiv preprint arXiv:2503.11074. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F. Wong, Xiaowei Huang, QiIs your ufeng Wang, and Kaizhu Huang. 2025c. model really good math reasoner? evaluating mathematical reasoning with checklist. In The Thirteenth International Conference on Learning Representations. Jiace Zhu, Yingtao Shen, Jie Zhao, and An Zou. Path-Consistency: Prefix Enhancement arXiv preprint. 2025. for Efficient Inference in LLM. ArXiv:2409.01281 [cs]. Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. 2024. Llava-phi: Efficient multi-modal In Proceedassistant with small language model. ings of the 1st International Workshop on Efficient Multimedia Computing under Limited, pages 1822."
        }
    ],
    "affiliations": [
        "Princeton University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong",
        "University of Illinois Urbana-Champaign",
        "University of Macau"
    ]
}