{
    "paper_title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens",
    "authors": [
        "Armel Zebaze",
        "Rachel Bawden",
        "Benoît Sagot"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation (MT) remains underexplored. In this work, we explore the benefits of the generation of intermediate tokens when performing MT across multiple language pairs of different levels of resourcedness and multiple setups. We find that \"thinking tokens\" do not help LRMs better perform MT. This result generalizes to models fine-tuned to reason before translating using distilled chain of thought (CoT) inspired by human translators' practices. Specifically, fine-tuning a model with synthetic CoT explanations detailing how to translate step-by-step does not outperform standard input-output fine-tuning. However, constructing the intermediate tokens by combining the outputs of modular translation-specific prompting strategies results in improvements. Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them. More broadly, our results suggest that using a teacher to refine target translations or to expand parallel corpora is more impactful than distilling their CoT explanations into \"thinking\" MT models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 9 1 9 1 1 . 0 1 5 2 : r Preprint. Under review. LLM REASONING FOR MACHINE TRANSLATION: SYNTHETIC DATA GENERATION OVER THINKING TOKENS Armel Zebaze, Rachel Bawden & Benoît Sagot Inria Paris, France armel.zebaze@inria.fr"
        },
        {
            "title": "ABSTRACT",
            "content": "Large reasoning models (LRMs) have led to new possibilities in terms of problemsolving, through the devising of natural language thought process prior to answering query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation (MT) remains underexplored. In this work, we explore the benefits of the generation of intermediate tokens when performing MT across multiple language pairs of different levels of resourcedness and multiple setups. We find that thinking tokens do not help LRMs better perform MT. This result generalizes to models fine-tuned to reason before translating using distilled chain of thought (CoT) inspired by human translators practices. Specifically, fine-tuning model with synthetic CoT explanations detailing how to translate step-by-step does not outperform standard input-output fine-tuning. However, constructing the intermediate tokens by combining the outputs of modular translation-specific prompting strategies results in improvements. Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them. More broadly, our results suggest that using teacher to refine target translations or to expand parallel corpora is more impactful than distilling their CoT explanations into thinking MT models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) are general-purpose problem solvers (Touvron et al., 2023; OpenAI et al., 2024; Dubey et al., 2024; Kimi Team et al., 2025). Their instruction-following capabilities help them carry out wide variety of requests provided by users via text. Research on alignment, typically through Reinforcement Learning from Human Feedback (RLHF) (Askell et al., 2021; Bai et al., 2022; Ouyang et al., 2022; Rafailov et al., 2023; Lambert et al., 2025) has greatly contributed to improving the quality of LLMs outputs. Recently, new paradigm has emerged: to train LLMs to think in natural language before answering query. OpenAI o1 and o3 (OpenAI, 2024), DeepSeek-R1 (DeepSeek-AI et al., 2025), Qwen3 (Yang et al., 2025), Claude 4 (Anthropic, 2025) and Gemini 2.5 (Gemini Team et al., 2025) inter alia are instances of these Reasoning Models (RM) or Thinking Models (TM). They capitalize on RL to generalize the success of Chain-ofThought (CoT) prompting (Wei et al., 2022) during training for improved safety robustness and performance. They particularly excel in reasoning-intensive tasks such as olympiad-level mathematics (AIME 2024/2025, HMMT etc.) and competition-level coding (Shi et al., 2024; Quan et al., 2025). When it comes to Machine Translation (MT), they also perform well (Chen et al., 2025) notably for stylized translation and document-level MT (Liu et al., 2025). Labeling o1-like models as thinking presupposes that the intermediate tokens they produce before answering meaningfully reflect their reasoning process. Many works challenge this view. For instance, Bhambri et al. (2025) find little correlation between the correctness of final answers and the accuracy of intermediate traces, echoing earlier results by Turpin et al. (2023) showing that CoT explanations can be unfaithful. Ma et al. (2025) further demonstrate that not thinking can outper1https://github.com/ArmelRandy/llm-reasoning-mt 1 Preprint. Under review. Figure 1: COT FINE-TUNING (left): Given source-target pair, teacher is prompted to get thought process on how to obtain the target given the source based on given strategy (right). The obtained trace is used as intermediate information to fine-tune student to think before translating. form explicit reasoning on certain challenging tasks, motivating exploration of not thinking in other settings. In this work, we examine the value of intermediate tokens in MT with LLMs and ask what forms of intermediate information are actually beneficial. MT is particularly interesting task in this context as CoT prompting has been shown not to result in better translation than vanilla few-shot prompting (Nguyen & Xu, 2025). We show that: The thinking mode of LRMs does not results in better MT outputs. We carry out extensive experiments across ten language directions and find no significant benefit from prior thinking. Our experiments cover zero-shot and few-shot settings, three benchmarks, highand low-resource language pairs (X-to-English and English-to-X directions) and different temperatures of generation, and they all point to the same conclusion. CoT distillation does not outperform standard fine-tuning. Many works report stark improvement of reasoning abilities when fine-tuning small model to think before answering, using the CoT outputs of teacher (Zelikman et al., 2022; Huang et al., 2024; Li et al., 2025; Guha et al., 2025). We apply this setup to fine-tune LMs to think before translating and compare it to standard input-output fine-tuning. Our experiments with gemma-3-4b-pt on English to Xhosa suggest the consistent superiority of standard finetuning across six different MT-specific CoT templates. Using traces obtained by translating the source with modular prompting strategies specifically designed for MT outperforms CoT distillation and standard input-output fine-tuning, but ultimately data matters most. Instead of vanilla CoT distillation, we propose to use the traces obtained when the teacher attempts to translate the source using modular prompting strategy for MT. Such strategies typically involve an analysis of the source, the proposal of intermediate candidates, and the derivation of the final translation. These steps can be concatenated into single text, which we then use as intermediate information for fine-tuning the student model. This approach outperforms input-output fine-tuning by up to 3.5 BLEU and 2 MetricX points. Analysis indicates that the gains stem mainly from translation attempts embedded in the traces. We further show that using the teacher to improve the fine-tuning dataset instead, by either enhancing the quality of its target translations or generating additional parallel pairs has greater benefits than relying on thinking tokens, without incurring extra inference cost after fine-tuning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reasoning with LLMs. CoT prompting (Wei et al., 2022) has revolutionized the approach to reasoning with LLMs. Following In-Context Learning (ICL;2 Brown et al., 2020), CoT prompting 2Also referred to as few-shot learning, which is the ability through which LLMs can carry out wide variety of tasks at inference based on few demonstrations 2 Preprint. Under review. drives the LLM to explain with natural language the thought process before deriving the solution to problem. It was shown to be particularly useful for mathematical tasks that require the LLM to think through set of reasoning steps (Cobbe et al., 2021; Hendrycks et al., 2021a;b; Suzgun et al., 2023). The intuition behind CoT prompting and its success has powered countless related prompting strategies (Kojima et al., 2022; Zhang et al., 2023; Yasunaga et al., 2024). Other developments involve using CoT as building block to solve sequential problems (Zhou et al., 2023; Zebaze et al., 2024), using CoT in combination with an external tool such as programming language interpreter (Chen et al., 2023; Gao et al., 2023) or to reason on diverse reasoning trajectories (Wang et al., 2023; Yao et al., 2023; Besta et al., 2024; Bi et al., 2025). CoT-based techniques have also been used to create datasets for supervised fine-tuning (Zelikman et al., 2022; Shao et al., 2024; Yue et al., 2024), which is often subsequent to prior continual pretraining on mathematics and code data (Lewkowycz et al., 2022; Azerbayev et al., 2024). We now have Thinking Models which are trained to think and produce long chain of thought before responding to user query, and these models have set new state-of-the-art for multiple benchmarks (GPQA (Rein et al., 2024); SWE-Bench (Jimenez et al., 2024; Chowdhury et al., 2024)) and keep motivating the creation of more challenging ones (Phan et al., 2025; Chollet et al., 2025). Despite their success, Ma et al. (2025) suggest that these thinking tokens can bring limited gains compared to not thinking3 on some reasoning tasks under thinking budget constraints. Others question the informativeness of CoT traces, showing that even incorrect traces can yield correct outcomes (Turpin et al., 2023; Bhambri et al., 2025) and that fine-tuning on incorrect traces can be as effective as on correct ones (Stechly et al., 2025). We focus this debate on the specific task of MT and ask whether general-purpose RMs translate better with thinking tokens or if they can be removed entirely. Machine Translation with LLMs. MT is one of the many tasks that LLMs can perform via ICL. Historically, encoder-decoder models have been the go-to architecture (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2016; Johnson et al., 2017; Vaswani et al., 2017). Decoder-based LLMs perform on par with or better than supervised MT models such as NLLB (Costa-jussà et al., 2022) when dealing with the so-called high-resource languages (HRLs), largely thanks to the availability of large quantities of high quality data on the internet, which facilitates their incorporation in the ever growing pretraining corpora of LLMs. LLMs still struggle with translating from and into low-resource languages (LRLs), but they offer more flexibility when prompting. ICL and the use of few-shot examples (including their selection and order, their number and quality) greatly impact the quality of MT outputs (Agrawal et al., 2023; Moslem et al., 2023; Hendy et al., 2023; Bawden & Yvon, 2023; Mu et al., 2023; Zhu et al., 2024; Bouthors et al., 2024; Zebaze et al., 2025c), and various prompting strategies have also been developed for MT such as Multi-Aspect Prompting and Selection (MAPS; He et al., 2024), Translating Step-by-Step (SBYS; Briakou et al., 2024), Translate, Estimate, and Refine (TEaR; Feng et al., 2025b), and Compositional Translation (CompTra; Zebaze et al., 2025a). This includes strategies that iteratively guide LLMs to refine translations, with or without external feedback (Chen et al., 2024; Xu et al., 2024b; Ki & Carpuat, 2024), inspired by the success of similar approaches in reasoning tasks (Madaan et al., 2023; Shinn et al., 2024). However, standard CoT prompting (e.g., Lets think step by step) has had little to no success in LLM-based MT, with most works reporting worse results than standard input-output prompting (Peng et al., 2023; Zebaze et al., 2025a; Nguyen & Xu, 2025). Several works have explored building RMs for MT (Wang et al., 2025a;b;c; He et al., 2025) by closely following what is done for reasoning tasks. They typically prompt large model (e.g., DeepSeek-R1) with curated CoT prompt that guides it from the source sentence to the target translation, then use the generated CoT for supervised fine-tuning (SFT) followed by RL fine-tuning. However, Zheng et al. (2025) suggests that thinking does do not help MT performance when applying GRPO (Shao et al., 2024) on rewards that only evaluate the final translation. In this work, we focus on SFT and study the pertinence of thinking tokens via CoT distillation, which is already successful on reasoning tasks (Huang et al., 2024; Li et al., 2025; Guha et al., 2025). Moreover, given source, we propose to use modular prompting strategies for MT that have been shown to outperform zeroand/or few-shot MT and ask teacher to translate the source with the strategies. These prompting strategies resemble reasoning pipeline, decomposed into multiple steps, each guided by distinct prompt serving specific purpose (e.g. identification of idiomatic expressions, generation of similar sentence, quality estimation, or drafting). For instance, MAPS (He et al., 2024) prompts the LLM to analyze 3They induce the not thinking mode by stopping the thinking process before it even occurs, for example by writing sentence such as Okay, think have finished thinking. 3 Preprint. Under review. the source and extract three translation-related aspectskeywords, topics, and relevant demonstrationseach used to generate candidate translation, with the final output selected from these and the zero-shot attempt. SBYS (Briakou et al., 2024) begins with pre-drafting research step, where the LLM identifies idiomatic or otherwise challenging expressions in the source. Based on this analysis, it is then prompted to produce an initial draft translation, followed by refinement stage. In subsequent conversation, the LLM is instructed to proofread the refined translationreflecting on both the source and the previously generated draft. In TEaR (Feng et al., 2025b), the LLM first produces draft translation in few-shot setting, then generates MQM-style error annotations and refines the draft accordingly. Self-Refine (Chen et al., 2024) involves drafting an initial translation and iteratively improving it through self-feedback. In CompTra (Zebaze et al., 2025a), the LLM decomposes the source into smaller phrases, translates them independently in few-shot manner, and uses these synthetic pairs as additional demonstrations to improve the final translation. The modularity of these methods lies in their multi-step structure. The outputs (or traces) of these individual steps can be concatenated into single text and used as CoT (intermediate tokens) to fine-tune student, thereby building thinking MT model, i.e. model that thinks before translating."
        },
        {
            "title": "3 BENCHMARKING LRMS AT SCALE: TO THINK, OR NOT TO THINK?",
            "content": "We first investigate the influence of prior reasoning on the translation quality in general-purpose thinking models. We compare two conditions: (i) the model is allowed to generate reasoning tokens prior to producing the translation (up to 3500 tokens), and (ii) reasoning is explicitly suppressed by appending <think>nn</think> to the prompt (Non-Thinking Mode; Yang et al., 2025). We carry out experiments with the Qwen3 model family (Yang et al., 2025), ranging in size from 0.6B to 32B parameters, in zero-shot English-to-X setting for ten FLORES-200 languages: Czech, Finnish, French, German, Japanese, Kazakh, Lithuanian, Portuguese, Spanish, and Turkish. The results, summarized in Table 1 show that the performance with and without prior thinking is similar. Non-thinking is slightly better, in particular in terms of MetricX but the difference is usually less than 0.5 MetricX point. We provide additional results with more models and directions in Appendix B, and with two other benchmarks, NTREX 128 (Federmann et al., 2022; Barrault et al., 2019) and TICO-19 (Anastasopoulos et al., 2020), in Appendix B.2. Figure 2: Impact of the Temperature on the translation quality with and without thinking tokens. We evaluate Qwen3-32B when sampling with temperatures 0.2, 0.4, 0.6, 0.8 and 1.0 and plot the results in Figure 2. As opposed to default usage recommendations for TMs, sampling degrades the performance, confirming the results obtained by Chen et al. (2025) with DeepSeek-R1-670B. In addition, at each temperature, outputting thinking tokens or not gives approximately the same level of performance. We observe slight performance gap in favor of thinking tokens with respect to BLEU ( 1 BLEU point), but they are behind with respect to MetricX. Low temperatures correlate with higher performance and in particular = 0.0 works best in general. The results of these experiments suggest that MT does not significantly benefit from the presence of thinking tokens. 4 Preprint. Under review. Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Czech Finnish French German Japanese BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 4.63 3.99 14.73 15.51 23.82 24.43 30.11 30.27 34.07 33. 34.62 33.27 22.86 13.60 15.58 15.09 9.05 9.27 6.38 6.61 4.99 5. 4.70 4.64 2.23 2.80 5.83 7.08 13.47 13.59 19.29 19.21 22.73 23. 24.33 24.14 23.64 21.16 20.29 19.38 14.22 14.30 9.95 10.20 7.74 7. 7.25 7.05 24.65 23.33 37.98 38.08 45.40 44.69 48.72 49.03 51.21 51. 51.80 50.94 9.03 8.52 4.80 4.44 3.19 3.14 2.59 2.49 2.26 2. 2.01 2.00 16.00 15.27 27.86 28.03 35.06 34.64 39.29 39.05 42.39 41. 42.78 42.08 9.95 8.21 4.50 3.97 2.58 2.48 1.62 1.56 1.29 1. 1.09 1.07 7.65 5.71 15.49 15.08 19.87 20.42 23.45 24.43 26.25 27. 26.44 27.33 8.67 7.56 5.84 5.87 5.06 4.82 4.31 4.08 3.80 3. 3.88 3.62 Kazakh Lithuanian Portuguese Spanish Turkish BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 0.41 0. 0.92 1.36 8.02 8.15 13.76 12.98 17.81 17.42 17.33 18.13 23.61 22. 23.41 23.54 16.02 16.37 11.68 11.76 8.87 8.84 9.54 8.39 1.28 1. 5.03 5.83 12.83 13.05 17.49 18.03 22.06 22.82 23.70 24.10 24.50 23. 21.70 21.56 15.69 15.45 11.51 11.30 8.78 8.45 8.41 7.58 25.58 23. 39.02 39.01 46.08 45.80 48.93 48.76 50.47 51.42 51.01 51.59 9.11 8. 4.46 4.35 2.97 3.09 2.53 2.41 2.25 2.09 1.96 1.96 17.80 17. 25.43 25.45 28.46 28.70 30.77 30.60 31.77 32.02 31.84 32.18 7.67 7. 3.87 3.75 2.69 2.69 2.15 2.03 1.95 1.86 1.66 1.71 5.88 6. 13.20 14.35 21.49 21.74 27.09 26.90 30.72 29.85 31.68 30.48 21.66 20. 15.48 13.94 9.75 9.55 7.28 6.76 5.93 5.70 5.80 5.59 Table 1: BLEU and MetricX scores for ten English directions from FLORES 200, with thinking (first line) and without thinking (second line). Best results are highlighted in bold."
        },
        {
            "title": "4 APPROACHES TO IMPROVING MT WITH INTERMEDIATE REASONING",
            "content": "Given that general-purpose RMs do not seem to benefit from outputting thinking tokens prior to translation, we investigate how to build successful thinking MT model, i.e., one that first produces intermediate reasoning before translation and outperforms models trained without intermediate steps. To this end, we apply COT FINE-TUNING (COTFT), in which student model is trained to first produce intermediate tokens (that we call CoT or thoughts) before generating the final target translation, as shown in Figure 1. Using parallel dataset and teacher model, we explore what types of intermediate information can be generated by the teacher to train thinking MT model (student). There are multiple ways of generating intermediate information with teacher (see the right side of Figure 1), which we categorize into two types. CoT prompting. This corresponds to the standard CoT distillation approach inherited from reasoning tasks (Figure 1, right, first box). For each sourcetarget pair, the teacher is fed with curated CoT prompt inspired by human translation strategies. It produces reasoning trace explaining how to obtain the target from the source, or justifying why the given target is correct translation of the source. In doing so, the model emulates the strategies used by human translators. It produces first-person thought process in which it explains how it analyzes the sentenceidentifying elements such as the subject, verb, and objectand how it arrives at the target translation by reasoning about linguistic aspects (syntactic rules, word order) and the broader context. Further details are provided in Appendix A.3. Modular translation-specific prompting strategies. Instead of adopting the classical road, we propose using as intermediate information the traces obtained after applying modular translation-specific prompting strategies to translate the source. As mentioned in Section 2, they generally involve multiple steps, (see the five other boxes of Figure 1): 5 Preprint. Under review. MAPS: modular process comprising source analysis (extraction of keywords, topics, and relevant demonstrations) and corresponding translation attempts (each inspired by the extracted information), complemented by zero-shot translation. SBYS: four-step process comprising pre-drafting research (identification of expressions that may pose challenge for translation), drafting, refinement, and proofreading (for terminology, fluency, etc.). TEaR: three-step process comprising translation (in few-shot setting), annotation (of potential translation errors), and refinement (based on these annotations). Self-Refine: an iterative process comprising the initial translation (in zero-shot setting) and successive rounds of self-refinement (to improve accuracy and fluency). CompTra: three-step process comprising decomposition of the source into simpler phrases, translation of each phrase (in few-shot setting), and recombination4 (into final translation). Given source sentence, we apply all the steps of each selected prompting strategy for MT and concatenate the outputs to form text, which serves as intermediate information for COTFT. We aim to determine whether this approach results in improved MT models, analogous to how RMs generalize the superiority of CoT prompting over direct inputoutput prompting with standard LLMs. We compare COTFT to INPUT-OUTPUT FINE-TUNING (IOFT), the baseline approach where the student is trained to directly predict the target translation given source sentence. In both cases, the source and target are the same and the difference only lies in the presence of intermediate reasoning during training. In summary, we first examine whether prompting teacher to emulate human translator and produce CoT traces helps to produce thinking MT model (student) that are more effective than standard IOFT model."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Evaluation Datasets. Our main evaluation dataset is FLORES-200 (Goyal et al., 2022; Costajussà et al., 2022) devtest set (1012 examples). For fine-tuning (distillation experiments), we focus on two languages: Xhosa, an LRL, in the main paper, and Lithuanian, HRL, in the appendix. Fine-tuning Datasets. For Xhosa, we use Llama-4-Scout-17B-16E-Instruct (AI at Meta, 2025) and synthetic, multi-domain sentence-level data generated using the TOPXGEN pipeline (Zebaze et al., 2025b).5, 6 For Lithuanian, we use the WMT19 dataset (Barrault et al., 2019) for training and run the same experiments as for Xhosa, as detailed in Appendices B.3 to B.5. Models. For Xhosa, we use Llama-4-Scout-17B-16E-Instruct (AI at Meta, 2025) as the teacher (to generate reasoning traces for COTFT) and gemma-3-4b-pt as the student. Ablation studies additionally consider gemma-3-27b-it (Gemma Team et al., 2025) and DeepSeek-R1-Distill-Llama-70B (DeepSeek-AI et al., 2025) as alternative teachers. For Lithuanian, we pair gemma-3-27b-it as the teacher with gemma-3-1b-pt as the student. Evaluation Metrics. Our main evaluation metric is MetricX-24 (Juraska et al., 2024). We use the reference-based version MetricX-24-Hybrid-XXL, which supports the same 101 languages as mT5 (Xue et al., 2021). MetricX assigns score ranging from 0 to 25, with higher scores indicating more errors in the translation. We also evaluate using BLEU7 (Papineni et al., 2002) as implemented in sacreBLEU (Post, 2018). 4We do not use the output of the recombination step when building the intermediate tokens. 5The pipeline enables the generation of EnglishLRL parallel data (in this cases the LRL being Xhosa) from LLM-generated LRL texts, backtranslated into English as way of alleviating the scarcity of diverse, highquality datasets for LRLs). 6https://hf.co/datasets/almanach/topxgen-llama-4-scout-and-llama-4-scout 7nrefs:1case:mixedeff:notok:flores200smooth:expversion:2.4.2 Preprint. Under review. Implementation Details We fine-tune all our models for 5k steps on one H100 80G with learning rate of 1e-5, constant scheduler with 500 warm-up steps (from 1e-6) and batch size of 4. For IOFT we use 4 gradient accumulation steps and maximum sequence length equal to 512, whereas for COTFT we use 16 gradient accumulation steps and maximum sequence length of 2048. All models are evaluated in zero-shot fashion with greedy decoding unless stated otherwise. See Appendix A.1 and A.2 for additional details."
        },
        {
            "title": "5.2 DISTILLED CHAIN-OF-THOUGHT AS INTERMEDIATE TOKENS",
            "content": "We compare IOFT and COTFT in the CoT distillation setup when the teacher is Llama-4-Scout-17B-16E-Instruct. We evaluate each of the six CoT instance construction prompt templates reflecting human translators reasoning proposed by Feng et al. (2025a) for generating cold-start data for their R1-T1 model: Hierarchical translation, Triangulating translation, Backtranslation, Context-aware translation, translation explanation and structural transformation (see Appendix A.3). We fine-tune gemma-3-4b-pt and compare the performance of all six COTFT variants against IOFT. It is important to recall that in all scenarios, the source and target are the same, only presence or absence of traces and their template change. We report the BLEU and MetricX scores on FLORES-200 every 200 steps in Figure 3. We observe that COTFT consistently fails to improve over IOFT (in black) across all templates. The variability of performance across templates is negligible; they fall short compared to IOFT by about 0.5 BLEU and 0.5 MetricX points. We ran the same experiment with DeepSeek-R1-Distill-Llama-70B as the teacher and reached the same conclusions (see Appendix B.6). Figure 3: Comparison between IOFT and COTFT with six different CoT templates. Across all figures, each unit on the x-axis represents 200 steps."
        },
        {
            "title": "5.3 MT TRACES GENERATED BY PROMPTING STRATEGIES AS INTERMEDIATE TOKENS",
            "content": "RMs were built based on the premise that thinking process formalized with natural language could help achieve better results. Asking an LLM to translate sentence step-by-step does not improve over CoT-free zero-shot MT. However multistep prompting strategies that mimic translation reasoning exist. Given teacher and prompting strategy, can the traces generated during translation, when used as intermediate information, help produce better outputs? We consider five modular prompting strategies: MAPS, SBYS, TEaR, Self-Refine and CompTra. As shown in Figure 4, COTFT on traces based on MT prompting strategies outperforms IOFT. For instance, we reach up to 3.5 BLEU and 2.0 MetricX gains with MAPS. For the other prompting strategies, improvements remain around +2 BLEU and -1.5 MetricX. Using CoT traces derived from these strategies appears beneficialbut why? The key difference is that, unlike pure CoT prompting, most of these strategies (except CompTra) include one or multiple drafting phases. The success of COTFT may therefore stem from drafts that surpass the ground truth. We test this hypothesis as follows. For each strategy, we use the quality estimation score BLASER 2.0-QE (Duquenne et al., 2023; Dale & Costa-jussà, 2024) to obtain the best translation between the ground truth and the attempts embedded in the teachers traces. We consider 2 scenarios. IOFT-MAX(STRATEGY) which is IOFT where the target is replaced by the best one between the ground truth and those potentially generated by 7 Preprint. Under review. Figure 4: Comparison between IOFT and COTFT with five different prompting strategies. the prompting strategy. COTFT-MAX(STRATEGY) which is analogous to IOFT-MAX(STRATEGY) but with the intermediate tokens. In addition to the above scenarios, we consider IOFT-BOA (best of all) which is IOFT where the target is the best between the ground truth and translations embedded into the traces obtained across all the prompting strategies considered (MAPS, SBYS, TEaR, Self-Refine and CompTra). Figure 5: Comparison between IOFT and COTFT with six different prompting strategies. First scenario. For MAPS, SBYS, TEaR and Self-Refine, IOFT-MAX (in red) works better than COTFT (i.e. with the traces) and IOFT (Figure 5). This indicates that the quality of the target is an important factor for downstream performance. Using better ground truths (IOFT-BOA, in green) can make IOFT go from 14 BLEU to 18 BLEU (8 MetricX to 5.6 MetricX) with the same number of parallel pairs and the same training recipe. Interestingly, CompTra behaves differently. As matter of fact, the traces of CompTra only contain translations of small sentences built by splitting the source, not of the source itself. The translations of these small phrases are unlikely to be better than the ground truth. This explains the performance similarity between standard IOFT and IOFT-MAX(COMPTRA). COTFT with CompTra outperforms IOFT-MAX(COMPTRA) and IOFT indicating that COTFT can be successful without including better translation attempts than the ground truth; partial translations are enough. Second Scenario. For MAPS, SBYS, TEaR and Self-Refine, IOFT-MAX generally works better than COTFT-MAX (Figure 6). This confirms the previous conclusion on these strategies, i.e. when the traces provided by the teacher do not contain translation attempts better than the ground truth, they do not help improve the MT performance. COTFT-MAX(COMPTRA) works slightly better than IOFT-MAX(COMPTRA), but both underperform COTFT with CompTra. This reinforces the idea that sentence-translation pairs (related to the sentence considered but smaller and different) can serve as valuable intermediate information for COTFT. 8 Preprint. Under review. Figure 6: Comparison between IOFT and COTFT with five different prompting strategies. IOFT-BOA (in green) being consistently above all the curves suggest that the quality of the target translations matters more than traces, and IOFT with better ground truths outperforms COTFT while being cheaper and faster to train. We obtained the exact same results when we use gemma-3-27b-it as the teacher in Appendix B.7."
        },
        {
            "title": "6.1 DOWN THE RABBIT HOLE OF SENTENCE DECOMPOSITION",
            "content": "We further investigate the generation of sentence-translation pairs as intermediate tokens. With CompTra, the pairs are obtained by decomposing the source into multiple phrases (Zebaze et al., 2025a). We consider three other decomposition strategies: Paraphrases (P), Syntactic Paraphrases (SP) and Hard Expressions (H). asks the teacher to generate five paraphrases of the source. SP generates five sentences with the same syntax as the source (grammatical roles, syntactic dependencies etc.) but using different words. Finally, asks the teacher to extract words or expressions it deems difficult to translate. In all cases, the teacher translates the expressions generated after decomposition. For each decomposition strategy (P, SP, H, and CompTra), we compare COTFT (which uses the teachers sentencetranslation pairs as reasoning traces) with IOFT. We also evaluate IOFT-EXT(strategy), which applies IOFT on the original dataset augmented with the generated pairs as additional training samples. Figure 7: Comparison between IOFT and COTFT with four sentence decomposition strategies (left) and with GRPO (right). As shown in Figure 7 (8 leftmost panels), COTFT consistently outperforms IOFT across all decomposition strategies. CompTra and SP are the best approaches with COTFT. IOFT-EXT(P) and IOFT-EXT(SP) result in significant gains over the IOFT baseline (+4 BLEU, -2 MetricX). IOFTEXT also outperforms COTFT with and SP. However, it has less success with and CompTra. We attribute this to the fact that the pairs generated by and CompTra are shorter and largely over9 Preprint. Under review. lap with the original training samples, giving fewer gains as additional data compared to entirely new sentences. However, these short phrases (generated via CompTra or H) are valuable intermediate information, as COTFT outperforms IOFT and IOFT-EXT in these scenarios. IOFT-EXT(SP) and IOFT-EXT(P) are the best overall, showing the large impact of the amount of parallel data."
        },
        {
            "title": "6.2 REINFORCEMENT LEARNING AFTER IOFT AND COTFT",
            "content": "Finally, we investigate whether CoTFT improves performance during RL fine-tuning. We consider three setups: IOFT, COTFT with CompTra and COTFT with T3 (like in Section 5.2). The final checkpoints (checkpoint-5000) are further fine-tuned with GRPO (Shao et al., 2024) on second parallel dataset8 (on 3 GPUs for 5000 steps, more details in Appendix A.5). We consider three reward functions: one based on the BLEU and chrF++ scores with the ground truth, second using COMET-22 (wmt22-cometkiwi-da; Rei et al., 2022) and last one based on the BLASER2.0 QE scores between the sources and hypotheses. For COTFT, we consider an additional format reward to ensure that the models preserve their prior thinking before translating. We compare all three RL fine-tunings with IOFT on the second dataset (on 1 GPU for 5000 steps) and report the results in Figure 7 (two rightmost panels). The ordering after SFT (COTFT with T3 IOFT COTFT with CompTra) remains unchanged after RL, with gains of about +1.3 BLEU and -1.0 MetricX in all setups. Notably, COTFT still does not outperform IOFT, even with RL. This is consistent with Zheng et al.s (2025) findings, namely that CoT signals fail to induce meaningful reasoning when the reward is applied only to the final translation. Moreover, unlike mathematics where step-by-step explanations are widely present in pre-training corpora (proofs), it is not the case for translation data. This scarcity of reasoning-like data may explain CoTs limited effectiveness in MT. Finally, we find that continuing SFT (IOFT) on the IOFT checkpoint gives much larger gains (+6 BLEU, -3 MetricX) than GRPO which quickly stagnates, reinforcing that standard IOFT alone can achieve superior MT performance. We report the results of applying GRPO on checkpoints obtained after COTFT with MAPS, SBYS, TEaR and Self-Refine in Appendix B.8."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We have explored fine-tuning LLMs to generate intermediate tokens as method to improve their MT capabilities. Through broad spectrum of experiments, we find that outputting reasoning traces does not help models to produce better translations (for thinking models and during CoT distillation). We also investigated how traces produced by alternative MT prompting strategies could help and found that parallel pairs can serve as valuable intermediate information. However, ultimately two factors considerably affect the success of MT fine-tuning: the quality of the target translation and the quantity of parallel data. When these factors are on point, standard IOFT goes long way. These findings generalize two important results in MT: (i) The inability of CoT prompting to improve over standard IO prompting in zero-shot with standard LLMs (Peng et al., 2023; Zebaze et al., 2025a; Nguyen & Xu, 2025), and (ii) the success of approaches using external resources such as grammars comes from the presence of parallel sentences in the grammar (Aycock et al., 2025; Marmonier et al., 2025). CoT (intermediate tokens) provided no benefit when translation attempts (full or partial) were absent, but accounted for all improvements when they were present. These findings suggest that parallel data is crucial for improving MT, both in the presence and absence of intermediate tokens."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was partly funded by Rachel Bawden and Benoît Sagots chairs in the PRAIRIE institute, now PRAIRIE-PSAI, funded by the French national agency ANR, respectively as part of the Investissements davenir programme under the reference ANR-19-P3IA-0001 and as part of the France 2030 strategy under the reference ANR-23-IACL-0008. It was also partly funded by the French Agence Nationale de la Recherche (ANR) under the project TraLaLaM (ANR-23IAS1-0006).This work was granted access to the HPC resources of IDRIS under the allocation 2025-AD011015933 made by GENCI. 8https://hf.co/datasets/almanach/Xhosa 10 Preprint. Under review."
        },
        {
            "title": "REFERENCES",
            "content": "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. Incontext examples selection for machine translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 88578873, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.564. URL https://aclanthology.org/2023. findings-acl.564. AI at Meta. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Franscisco Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, and Sylwia Tur. TICO-19: the Translation Initiative for COvid-19. In Karin Verspoor, Kevin Bretonnel Cohen, Michael Conway, Berry de Bruijn, Mark Dredze, Rada Mihalcea, and Byron Wallace (eds.), Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online, December 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. nlpcovid19-2.5. URL https://aclanthology.org/2020.nlpcovid19-2.5. Anthropic. Introducing claude 4, 2025. https://www.anthropic.com/news/claude-4. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. General Language Assistant as Laboratory for Alignment, 2021. URL https://arxiv.org/abs/2112.00861. Seth Aycock, David Stap, Di Wu, Christof Monz, and Khalil Simaan. Can LLMs really learn to translate low-resource language from one grammar book? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=aMBSY2ebPw. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An Open Language Model for Mathematics. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4WnqRR915j. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate, 2016. URL https://arxiv.org/abs/1409.0473. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862. Loïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2019 Conference In Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, on Machine Translation (WMT19). Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 161, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5301. URL https://aclanthology.org/W19-5301. 11 Preprint. Under review. Rachel Bawden and François Yvon. Investigating the Translation Performance of Large Multilingual Language Model: the Case of BLOOM. In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz (eds.), Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pp. 157170, Tampere, Finland, June 2023. European Association for Machine Translation. URL https: //aclanthology.org/2023.eamt-1.16. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michał Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690, Mar 2024. doi: 10. 1609/aaai.v38i16.29720. URL https://ojs.aaai.org/index.php/AAAI/article/ view/29720. Siddhant Bhambri, Upasana Biswas, and Subbarao Kambhampati. Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation, 2025. URL https://arxiv.org/abs/2505.13792. Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=BMJ3pyYxu2. Maxime Bouthors, Josep Crego, and François Yvon. Retrieving examples from memory for retrieval augmented neural machine translation: systematic comparison. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 30223039, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.190. URL https://aclanthology. org/2024.findings-naacl.190. Eleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. Translating step-bystep: Decomposing the translation process forimproved translation quality of long-form texts. In Proceedings of the Ninth Conference on Machine Translation, pp. 13011317, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.wmt-1.123. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, In H. Larochelle, M. Ranzato, and Dario Amodei. Language models are few-shot learners. R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, and Min zhang. Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis, 2025. URL https://arxiv.org/abs/2502.11544. Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. Iterative translation refinement with large language models. In Carolina Scarton, Charlotte Prescott, Chris Bayliss, Chris Oakley, Joanna Wright, Stuart Wrigley, Xingyi Song, Edward Gow-Smith, Rachel Bawden, Víctor Sánchez-Cartagena, Patrick Cadwell, Ekaterina Lapshinova-Koltunski, Vera Cabarrão, Konstantinos Chatzitheodorou, Mary Nurminen, Diptesh Kanojia, and Helena Moniz (eds.), Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 1), pp. 181190, Sheffield, UK, June 2024. European Association for Machine Translation (EAMT). URL https://aclanthology.org/2024.eamt-1.17/. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on 12 Preprint. Under review. Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=YfZ4ZPt8zd. Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 17241734, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://aclanthology.org/D14-1179/. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems, 2025. URL https://arxiv.org/abs/ 2505.11831. Neil Chowdhury, James Aung, Chan Jun Shern, Oliver Jaffe, Dane Sherburn, Giulio Starace, Evan Mays, Rachel Dias, Marwan Aljubeh, Mia Glaese, Carlos Jimenez, John Yang, Kevin Liu, and Aleksander Madry. Introducing SWE-bench Verified. OpenAI, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No Language Left Behind: Scaling Human-Centered Machine Translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/arxiv.2207.04672. URL https://doi.org/10.48550/arXiv.2207.04672. David Dale and Marta R. Costa-jussà. BLASER 2.0: metric for evaluation and quality estimation of massively multilingual speech and text translation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1607516085, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.943. URL https://aclanthology. org/2024.findings-emnlp.943/. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong 13 Preprint. Under review. Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. URL https://arxiv.org/abs/2501.12948. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa 14 Preprint. Under review. Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The Llama 3 Herd of Models, 2024. URL https://arxiv.org/abs/2407.21783. Paul-Ambroise Duquenne, Holger Schwenk, and Benoît Sagot. SONAR: Sentence-Level Multimodal and Language-Agnostic Representations, 2023. URL https://arxiv.org/abs/ 2308.11466. Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 News Test References for MT In Proceedings of the First Workshop on Scaling Up MultilinEvaluation of 128 Languages. gual Evaluation, pp. 2124, Online, nov 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4. Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. MT-R1-Zero: Advancing LLM-based Machine Translation via R1Zero-like Reinforcement Learning, 2025a. URL https://arxiv.org/abs/2504.10160. Zhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu Liao, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, and Zuozhu Liu. TEaR: Improving LLM-based machine translation with systemIn Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Asatic self-refinement. sociation for Computational Linguistics: NAACL 2025, pp. 39223938, Albuquerque, New Mexico, April 2025b. Association for Computational Linguistics. ISBN 979-8-89176-195-7. doi: 10.18653/v1/2025.findings-naacl.218. URL https://aclanthology.org/2025. findings-naacl.218/. 15 Preprint. Under review. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1076410799. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/gao23f.html. PAL: Program-aided language models. Gemini Team et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/ 2507.06261. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation. Transactions of the Association for Computational Linguistics, 10:522538, 2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.tacl-1.30. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995, 2024. doi: 10.1162/ tacl_a_00683. URL https://aclanthology.org/2024.tacl-1.54. Preprint. Under review. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. OpenThoughts: Data Recipes for Reasoning Models, 2025. URL https://arxiv.org/abs/2506.04178. Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, and Osamu Yoshie. R1T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning, 2025. URL https://arxiv.org/abs/2502.19735. Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Exploring human-like translation strategy with large language models. Transactions of the Association for Computational Linguistics, 11:229246, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob In International ConferSteinhardt. Measuring Massive Multitask Language Understanding. ence on Learning Representations, 2021a. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH the Neural Information Dataset. Processing Systems Track on Datasets and Benchmarks, volume 1, 2021b. URL https: //datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf. In J. Vanschoren and S. Yeung (eds.), Proceedings of Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How Good Are GPT Models at Machine Translation? Comprehensive Evaluation, 2023. URL https://arxiv. org/abs/2302.09210. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 Replication Journey Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?, 2024. URL https://arxiv. org/abs/2411.16489. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Googles multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339351, 2017. doi: 10.1162/tacl_a_00065. URL https://aclanthology.org/Q17-1024/. 17 Preprint. Under review. Juraj Juraska, Daniel Deutsch, Mara Finkelstein, and Markus Freitag. MetricX-24: The Google submission to the WMT 2024 metrics shared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), Proceedings of the Ninth Conference on Machine Translation, pp. 492 504, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.wmt-1.35. Dayeon Ki and Marine Carpuat. Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 42534273, Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology. org/2024.findings-naacl.265. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi K2: Open Agentic Intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Philipp Koehn. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu (eds.), Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388395, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-3250. Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2219922213. Curran Associates, Inc., 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing Frontiers in Open Language Model Post-Training, 2025. URL https://arxiv.org/abs/2411.15124. Preprint. Under review. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Imanol Schlag, Theo Gutman-Solo, Solving QuanIn S. Koyejo, S. Mohamed, InInc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ Vinay Ramasesh, Ambrose Slone, Cem Anil, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. titative Reasoning Problems with Language Models. A. Agarwal, D. Belgrave, K. Cho, formation Processing Systems, volume 35, pp. 38433857. Curran Associates, 2022. file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. and A. Oh (eds.), Advances in Neural Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!, 2025. URL https://arxiv.org/abs/2502.07374. Sinuo Liu, Chenyang Lyu, Minghao Wu, Longyue Wang, Weihua Luo, Kaifu Zhang, and Zifu Shang. New Trends for Modern Machine Translation with Large Reasoning Models, 2025. URL https://arxiv.org/abs/2503.10351. Xing Han Lù. BM25S: Orders of magnitude faster lexical search via eager sparse scoring, 2024. URL https://arxiv.org/abs/2407.03618. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning Models Can Be Effective Without Thinking, 2025. URL https://arxiv.org/abs/2504. 09858. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Selfrefine: Iterative refinement with self-feedback. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4653446594. Curran Associates, Inc., 2023. Malik Marmonier, Rachel Bawden, and Benoît Sagot. Explicit learning and the llm in machine translation. In Proceedings of The 2025 Conference on Empirical Methods in Natural Language Processing, Suzhou, China, 2025. URL https://arxiv.org/abs/2503.09454. Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. Adaptive machine translation with large language models. In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz (eds.), Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pp. 227237, Tampere, Finland, June 2023. European Association for Machine Translation. URL https://aclanthology.org/ 2023.eamt-1.22. Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. Augmenting large language model translators via translation In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the memories. Association for Computational Linguistics: ACL 2023, pp. 1028710299, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.653. URL https://aclanthology.org/2023.findings-acl.653. Lam Nguyen and Yang Xu. Reasoning for translation: Comparative analysis of chain-of-thought and tree-of-thought prompting for LLM translation. In Jin Zhao, Mingyang Wang, and Zhu Liu (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 259275, Vienna, Austria, July 2025. Association ISBN 979-8-89176-254-1. doi: 10.18653/v1/2025.acl-srw.17. for Computational Linguistics. URL https://aclanthology.org/2025.acl-srw.17/. OpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Preprint. Under review. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 Technical Report, 2024. URL https://arxiv.org/abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for automatic In Proceedings of the 40th Annual Meeting on Association evaluation of machine translation. Preprint. Under review. for Computational Linguistics, ACL 02, pp. 311318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/ 1073083.1073135. Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. Towards making the most of ChatGPT for machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 56225633, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.373. URL https://aclanthology.org/ 2023.findings-emnlp.373. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, Gözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres Bran, Andres Algaba, Kelsey Van den Houte, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Joseph Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan, Sergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra RodriguezRomero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, Václav Rozhoˇn, Vincent Ginis, Christian Stump, Niv Cohen, Rafał Poswiata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison Wang, Kalyan Ramakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, 21 Preprint. Under review. Yuzhou Nie, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, Jérémy Andréoletti, Raghav Singhal, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Jiaqi Wang, Maria Inês S. Nunes, Jeremiah Milbauer, Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl Fossum, Niklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâca, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yigit Yalın, Gbenga Daniel Obikoya, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle Häggström, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt, Pablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja 22 Preprint. Under review. Somrak, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran Ðuc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti, Abdallah Galal, Ng ZeAn, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub Łucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca Arnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran V, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubic, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał Perełkiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del Rio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena, Xing Han Lù, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Brianski, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi Wang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen, Sara Vera Marjanovic, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun Wang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan, Jun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gonçalves, Wei Hu, Kaushik Bar, Ondrej Bo23 Preprint. Under review. hdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline Geirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. In Ondˇrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina (eds.), Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 392395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology.org/W15-3049. Maja Popovic. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers, pp. 612618, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. URL http://www.aclweb. org/anthology/W17-4770. Matt Post. call for clarity in reporting BLEU scores. In Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin. CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings, 2025. URL https://arxiv.org/ abs/2501.01257. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5372853741. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation 24 Preprint. Under review. In Philipp Koehn, Loïc Barrault, Ondˇrej Bojar, Fethi Bougares, Rajen ChatterShared Task. jee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (eds.), Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 634645, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.60/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=Ti67584b98. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024. URL https://arxiv.org/abs/ 2402.03300. Ben Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. Can Language Models Solve In First Conference on Language Modeling, 2024. URL https: Olympiad Programming? //openreview.net/forum?id=kGa4fMtP9l. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: In Proceedings of the 37th International language agents with verbal reinforcement learning. Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2024. Curran Associates Inc. Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, and Subbarao Kambhampati. Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens, 2025. URL https://arxiv.org/abs/2505.13775. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to Sequence Learning with Neural In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger Networks. (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2014/ 2014. file/5a18e133cbf9f257297f410bb7eca942-Paper.pdf. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023. findings-acl.824/. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. URL https://arxiv.org/abs/2307.09288. Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language Models Don't AlIn ways Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. 25 Preprint. Under review. A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7495274965. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/ed3fea9033a80fea1376299fa7863f4a-Paper-Conference.pdf. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. TRL: Transformer Reinforcement Learning. https://github.com/huggingface/trl, 2020. Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. DRT: Deep reasoning translation via long chain-of-thought. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 6770 ISBN 979-86782, Vienna, Austria, July 2025a. Association for Computational Linguistics. 89176-256-5. doi: 10.18653/v1/2025.findings-acl.351. URL https://aclanthology. org/2025.findings-acl.351/. Jiaan Wang, Fandong Meng, and Jie Zhou. Deep reasoning translation via reinforcement learning. arXiv preprint arXiv:2504.10187, 2025b. Jiaan Wang, Fandong Meng, and Jie Zhou. Extrans: Multilingual deep reasoning translation via exemplar-enhanced reinforcement learning. arXiv preprint arXiv:2505.12996, 2025c. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in LanIn The Eleventh International Conference on Learning Representations, 2023. guage Models. URL https://openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. HuggingFaces Transformers: Stateof-the-art Natural Language Processing, 2020. URL https://arxiv.org/abs/1910. 03771. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview. net/forum?id=farT6XXntP. Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, and Markus Freitag. LLMRefine: Pinpointing and refining large In Kevin Duh, Helena Gomez, and language models via fine-grained actionable feedback. Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 14291445, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.92. URL https://aclanthology.org/ 2024.findings-naacl.92. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: Massively Multilingual Pre-trained Text-to-Text Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483498, 2021. 26 Preprint. Under review. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1180911822. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=AgDICX1h50. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=yLClGs770I. Armel Zebaze, Benoît Sagot, and Rachel Bawden. Compositional Translation: Novel LLM-based Approach for Low-resource Machine Translation, 2025a. URL https://arxiv.org/abs/ 2503.04554. Armel Zebaze, Benoît Sagot, and Rachel Bawden. TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation, 2025b. URL https://arxiv.org/abs/ 2508.08680. Armel Randy Zebaze, Benoît Sagot, and Rachel Bawden. Tree of problems: Improving structured problem solving with compositionality. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1802818047, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1001. URL https://aclanthology. org/2024.emnlp-main.1001/. Armel Randy Zebaze, Benoît Sagot, and Rachel Bawden. In-context example selection via similarity search improves low-resource machine translation. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, pp. 12221252, Albuquerque, New Mexico, April 2025c. Association for Computational Linguistics. ISBN 9798-89176-195-7. doi: 10.18653/v1/2025.findings-naacl.68. URL https://aclanthology. org/2025.findings-naacl.68/. Jesse Mu, Eric Zelikman, Yuhuai Wu, Reasoning With Reasoning. grave, K. Cho, Systems, https://proceedings.neurips.cc/paper_files/paper/2022/file/ 639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf. STaR: Bootstrapping In S. Koyejo, S. Mohamed, A. Agarwal, D. BelInformation Processing URL Inc., in Neural pp. 1547615488. Curran Associates, and A. Oh (eds.), Advances and Noah Goodman. volume 35, 2022. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic Chain of Thought Prompting in Large Language Models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5NTt8GFjUHkr. Mao Zheng, Zheng Li, Bingxin Qu, Mingyang Song, Yang Du, Mingrui Sun, and Di Wang. Hunyuan-MT Technical Report, 2025. URL https://arxiv.org/abs/2509.05209. 27 Preprint. Under review. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting In The Eleventh International Conferenables complex reasoning in large language models. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= WZH7099tgfM. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 27652781, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl. 176. URL https://aclanthology.org/2024.findings-naacl.176."
        },
        {
            "title": "A REPRODUCIBILITY DETAILS",
            "content": "A.1 MODELS, DATASETS AND TOOLS In Table 2, we list the links to the relevant resources used for the experiments. FLORES-200 NTREX HF TICO-19 ToPXGen llama-4-scout & llama-4-scout https://huggingface.co/datasets/facebook/flores hhttps://huggingface.co/datasets/mteb/NTREX https://huggingface.co/datasets/gmnlp/tico19 https://hf.co/datasets/almanach/topxgen-llama-4-scout-and-llama-4-scout Datasets Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-70B Gemma-3-27B-It Gemma-3-4B-Pt Gemma-3-1B-Pt Llama-4-Scout-17B-16E-Instruct MetricX24-Hybrid-XXL XCOMET-XXL FastText vLLM (Kwon et al., 2023) Models evaluated https://huggingface.co/Qwen/Qwen3-0.6B https://huggingface.co/Qwen/Qwen3-1.7B https://huggingface.co/Qwen/Qwen3-4B https://huggingface.co/Qwen/Qwen3-8B https://huggingface.co/Qwen/Qwen3-14B https://huggingface.co/Qwen/Qwen3-32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B https://huggingface.co/google/gemma-3-27b-it https://huggingface.co/google/gemma-3-4b-pt https://huggingface.co/google/gemma-3-1b-pt https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct Other resources https://huggingface.co/google/metricx-24-hybrid-xxl-v2p6 https://huggingface.co/Unbabel/XCOMET-XXL https://huggingface.co/facebook/fasttext-language-identification https://github.com/vllm-project/vllm Table 2: Links to datasets, benchmarks and models. A."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We use HuggingFaces Transformers library (Wolf et al., 2020; Gugger et al., 2022). We adopt the prompt template introduced by Xu et al. (2024a), and compute the loss only on the target (translation or intermediate tokens followed by the translation). We use the same prompt when evaluating the checkpoints. During COTFT the target is formatted as <think>n{Intermediate Tokens}n</think>nnFinal Translationn{Target translation}. Translate this from English to Hausa: English: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added. Hausa: For intruction-following models and thinking models we use the following evaluation prompt. Please write high-quality Xhosa translation of the following English sentence 28 Preprint. Under review. \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added. Please provide only the translation, nothing more. We calculate statistical significance using bootstrap resampling (Koehn, 2004) with 300 samples of 500 sentences and p-value threshold of 0.05. A.3 COT CONSTRUCTION TEMPLATES We use the six CoT construction templates proposed by He et al. (2025). They mimic reasoning strategies for translation commonly adopted by human translators. T1 is Hierarchical Translation <think> 1. Analyze the sentence structure and identify the core elements (subject , verb, object). 2. Translate the sentence from the origin language to the target language , focusing on the core elements. 3. Review the translation for basic accuracy and grammatical structure. 4. Identify areas that need further refinement (e.g., word choice, tense, or word order). 5. Modify the translation to improve fluency and coherence, considering the context. 6. Finalize the translation by ensuring it retains the original meaning while improving readability. </think> T2 is Triangulating Translation <think> 1. Identify basic elements: Break down the sentence into its main components and identify the key subject, verb, and object. 2. Translate to intermediate language: Convert these elements into an intermediate language structure (e.g., simple syntactic rules or function names). 3. Refine back to target language: Translate from the intermediate language back to the target language, adjusting for syntactic norms and idiomatic expressions. 4. Check for accuracy: Ensure that the meaning is preserved in the translated sentence by checking noun-verb agreement and connectors. 5. Adjust word order: Modify word order to ensure that it aligns with the target languages grammatical structure. 6. Final refinement: Review the translation for naturalness, idiomatic use, and overall flow. </think> T3 is Back Translation <think> 1. Analyze the provided context in the source language. 2. Translate the source text to the target language. 3. Perform back translation from the target language to the source language. 4. Compare the back translation with the original source context. 5. Evaluate whether the meaning of the back translation aligns with the original. 6. If discrepancies are identified, adjust the target language translation to enhance consistency with the original meaning. 7. Finalize the translation by ensuring both forward and back translations accurately align across all languages involved. </think> 29 Preprint. Under review. T4 is Context-aware Translation <think> 1. Analyze the current sentence, along with the previous sentences, to understand the overall conversation context. 2. Identify key elements like tone, formality, or subject matter based on the ongoing conversation. 3. Translate the sentence while ensuring that the translation is aligned with the tone, style, and subject of the preceding dialogue. 4. If any ambiguity exists in the translation due to context, refine the translation to better fit the conversation flow. 5. Verify that the translation maintains coherence with the larger conversation, ensuring consistency in language and tone. 6. Finalize the translation by cross-checking it with the conversations context to ensure it feels natural and appropriately aligned. </think> T5 is Translation Explanation <think> 1. Analyze the source sentence and identify the key elements (verbs, subjects, objects, etc.). 2. Based on these elements, determine the most suitable translation strategy (literal vs. idiomatic). 3. Select the best translation for each word or phrase, considering context and languagespecific structures. 4. Explain the rationale behind choosing specific words or phrases. 5. After completing the initial translation, review each translation decision and explain any adjustments made for fluency or accuracy. 6. Provide final explanation for the translation choices, discussing any trade-offs made between literal meaning and contextual appropriateness. </think> T6 is Structural Transformation <think> 1. Analyze the sentences syntactic structure in the source language (e.g ., identify whether its active or passive). 2. Determine the most appropriate syntactic structure in the target language (e.g., whether it needs to be rephrased from active to passive or vice versa). 3. Adjust the word order and grammatical structure in the target language to match the sentences meaning, while maintaining clarity. 4. Translate the sentence, ensuring that subject-verb-object relationships and other syntactic elements align with target language norms. 5. After the translation, check the sentences grammar and overall flow in the target language, making sure it is clear and fluid. 6. If the sentence feels awkward or unnatural, refine the structure by adjusting word choice or reordering components. </think> Given CoT template, we use the following prompt to obtain CoT produced by teacher explaining how to obtain the provided translation of given source sentence following the strategy corresponding to the template. The CoT produced is in the first-person and can later be used for COTFT. Assume that you are student engaged in translating sentence from {src } to {tgt}. Now you have both the source sentence and the target sentence, and need to analyze how to translate from the source sentence to the given target sentence based on the provided Thinking Chain Guide. And 30 Preprint. Under review. output the chain-of-thought trajectory from source to target sentence. The {src} statement is as follows: <Source Sentence> {sentence} </Source Sentence> The {tgt} statement is as follows: <Target Sentence> {translation} </Target Sentence>"
        },
        {
            "title": "You continuously reflect on how to translate the source sentence to the",
            "content": "given target sentence based on the thinking guidance provided. The given Thinking Chain Guide is as follows: <Thinking Chain Guide> {chain_of_thought_template} </Thinking Chain Guide> Please refine the entire analysis process into complete self-reflective description (in the present tense). For self-reflection, you can refer to the following thinking steps: directly output the self-reflective description in the <think></think> tags, without any additional descriptions or explanations."
        },
        {
            "title": "Each line in the reflective description can be viewed as a reasoning step",
            "content": "in the translation process. A.4 PROMPTING STRATEGIES Step-by-Step Translation (SBYS): {predrafting research} {draft translation} Now lets move to the next stage: Post-editing with local refinement. In this stage, the primary aim is to refine the draft translation by making micro-level improvements that improve the drafts fluency. Here is refined version of the translation {refinement} Now, we will proofread the refined text for grammar spelling, punctuation , terminology and overall fluency.\" Here is the translation after proofreading {proofreading} We will further improve it to obtain the final, polished translation. Multi-Aspect Prompting and Selection (MAPS):"
        },
        {
            "title": "Here is a draft translation",
            "content": "1. {zero-shot translation} Lets write an English sentence related to but different from the input English sentence and translate it into {language} {demonstrations} 31 Preprint. Under review. Given this knowledge, we can draft another translation 2. {demonstrations-inspired translation} Lets extract the keywords in the provided English sentence, and then translate these keywords into {language} {keywords} Given this knowledge, we can draft another translation 3. {keywords-inspired translation} Lets use few words to describe the topics of the provided English sentence {topics} Given this knowledge, we can draft another translation 4. {topics-inspired translation}"
        },
        {
            "title": "We will choose the best of these translations and further improve it to",
            "content": "obtain the final, polished translation. Self-Refine"
        },
        {
            "title": "Here is a draft translation",
            "content": "1. {draft translation} Lets improve it and write better translation 2. {refinement 1} Lets further improve it and write better translation 3. {refinement 2} Lets improve it one last time and write better translation 4. {refinement 3}"
        },
        {
            "title": "We will choose the best of these translations and further improve it to",
            "content": "obtain the final, polished translation. Translate, Estimate and Refine (TEaR)"
        },
        {
            "title": "Here is a draft translation",
            "content": "1. {draft translation} Lets identify errors and assess the quality of the draft translation. The categories of errors are accuracy (addition, mistranslation, omission , untranslated text), fluency (character encoding, grammar, inconsistency, punctuation, register, spelling), locale convention ( currency, date, name, telephone, or time format) style (awkward), terminology (inappropriate for context, inconsistent use), non translation, other, or no-error. Each error is classified as one of three categories: critical, major, and minor. Critical errors inhibit comprehension of the text. Major errors disrupt the flow, but what the text is trying to say is still understandable. Minor errors are technical errors but do not disrupt the flow or hinder comprehension. Preprint. Under review. Here are the MQM annotations of the draft: {MQM annotations} Upon reviewing the translation and error information, we can refine the draft and obtain better translation 2. {refinement} We will further improve it to obtain the final, polished translation.\" Compositional Translation (CompTra) 1. English Sentence {} Xhosa Translation {} 2. English Sentence {} Xhosa Translation {} 3. English Sentence {} Xhosa Translation {} A.5 RL TRAINING HYPERPARAMETERS For GRPO, we use the Hugging Face TRL library (von Werra et al., 2020). Training is conducted on four H100 GPUs, with one dedicated to model deployment for reward computation. We set per-device batch size of 4 with 4 gradient accumulation steps, for total of 5000 steps including 100 warmup steps. Hyperparameters include beta value of 0.02, maximum gradient norm of 1.0, and temperature of 1.0. For generation, we sample 12 outputs per prompt with an effective batch size of 48. We apply LoRA (Hu et al., 2022), fine-tuning the q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj modules with rank = 32, scaling factor α = 64, and dropout rate 0.05."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTS",
            "content": "B.1 BENCHMARKING LRMS AT SCALE: TO THINK, OR NOT TO THINK? In this section we further investigate the impact of thinking tokens when benchmarking LRMs. In Table 3 we report the chrF++9 (Popovic, 2015; Popovic, 2017) and XCOMET-XXL (Guerreiro et al., 2024) scores in the same setup as Table 1. They tell the same story as BLEU and MetricX. Outputting thinking tokens only marginally helps; the gains are not consistent and when they occur they are small. This questions the necessity of an LRM to think before doing MT, all the more so that thinking is considerably more expensive than straight up answering. It is worth noting that small models (Qwen-0.6B, Qwen-1.7B and Qwen-4B) often generate answers in English or Chinese when they struggle with the target language (e.g., Czech, Finnish, Kazakh, Lithuanian etc.) resulting in artificially better neural scores. The thinking mode particularly helps in such scenarios because it allows the model to remember that it should write an answer in different language than what it is used to generating in. When the models are big enough (typically 8B), thereby solving this incorrect language issue, thinking does not result in any gains. Moreover, we run additional experiments with more DeepSeek-R1-Distill models (see Table 4) and again we observe similar pattern, no thinking consistently outperforms thinking. We run additional experiments when translating in 9nrefs:1case:mixedeff:yesnc:6nw:2space:noversion:2.4.2 33 Preprint. Under review. Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Czech Finnish French German Japanese chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET 19.43 18.53 33.89 35.27 43.43 43.92 48.49 48.70 51.57 51. 51.45 50.85 16.52 55.22 39.91 38.62 68.22 65.45 80.32 77.64 86.91 84. 86.59 86.48 16.36 18.12 26.45 29.54 36.99 38.11 43.15 43.44 46.26 46. 46.47 47.40 13.56 23.62 20.00 22.68 45.99 42.54 65.78 62.60 76.27 74. 77.18 77.13 47.19 45.94 57.40 57.68 62.49 62.26 64.68 65.16 66.41 66. 65.99 65.98 53.77 54.82 80.58 80.67 89.14 88.23 92.19 91.89 92.99 93. 92.79 93.81 39.34 37.75 49.52 49.89 55.13 55.07 58.30 58.21 60.52 60. 60.35 60.38 71.05 74.96 88.42 89.58 93.48 94.25 95.94 96.38 96.89 97. 96.27 97.60 14.11 11.35 21.39 20.81 24.68 26.53 27.24 28.50 29.88 30. 29.10 30.89 44.66 53.73 69.82 67.83 80.26 79.23 86.18 86.60 89.88 89. 89.38 89.97 Kazakh Lithuanian Portuguese Spanish Turkish chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET 4.14 6. 7.51 10.84 26.54 26.51 34.64 33.86 39.53 38.95 38.34 39.22 15.77 20. 15.09 15.62 25.20 23.78 40.48 36.52 53.44 50.77 53.66 52.05 10.73 12. 24.83 26.11 36.26 36.60 41.54 42.02 45.08 45.82 45.92 46.68 14.19 16. 18.80 19.52 43.79 42.88 61.88 60.84 73.81 73.72 75.60 76.89 48.05 44. 58.97 58.99 63.93 63.70 65.66 65.77 66.70 67.55 66.71 67.64 67.21 68. 87.43 87.07 92.69 92.17 94.16 94.60 95.12 95.31 94.93 95.77 41.70 40. 48.25 48.61 50.66 51.07 52.45 52.55 53.29 53.60 53.19 53.57 70.45 72. 87.84 87.55 92.23 92.20 94.13 94.62 94.64 95.25 95.07 95.32 22.64 23. 34.08 35.89 42.97 43.52 47.86 47.90 50.97 50.42 51.29 50.60 18.39 22. 39.47 43.02 66.46 65.43 77.71 77.68 83.68 83.35 84.03 83.08 Table 3: chrF++ and XCOMET scores for 10 English directions from FLORES 200. Best results are highlighted in bold. Models Czech Finnish French German Japanese BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX DEEPSEEK-R1-DISTILL-QWEN-14B w/o Thinking DEEPSEEK-R1-DISTILL-QWEN-32B w/o Thinking DEEPSEEK-R1-DISTILL-LLAMA-70B w/o Thinking 22.78 23. 28.49 29.61 37.31 38.47 9.91 9.50 7.12 6.22 4.19 3.52 10.55 10. 15.61 16.12 29.90 30.96 17.40 17.13 13.06 12.52 5.35 4.38 46.13 45. 48.68 49.35 52.34 52.34 2.87 2.54 2.78 2.16 2.22 1.88 34.54 30. 38.25 38.95 43.39 44.78 2.49 2.40 1.82 1.44 1.16 0.91 21.04 22. 24.11 26.57 25.61 27.94 4.85 4.45 4.56 3.89 4.08 3.62 Models Kazakh Lithuanian Portuguese Spanish Turkish BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX DEEPSEEK-R1-DISTILL-QWEN-14B w/o Thinking DEEPSEEK-R1-DISTILL-QWEN-32B w/o Thinking DEEPSEEK-R1-DISTILL-LLAMA-70B w/o Thinking 2.38 2.50 5.66 4. 21.39 21.56 13.25 6.49 18.37 16.19 8.14 6.76 7.87 7.21 13.24 14. 25.63 27.01 20.02 18.79 16.09 14.92 8.22 7.33 46.28 47.11 49.17 50. 52.27 52.64 2.86 2.52 2.55 2.02 2.09 1.75 29.48 30.52 30.62 31. 32.68 33.54 2.56 2.13 2.26 1.74 1.78 1.55 17.99 17.78 24.52 24. 33.42 34.77 12.79 11.70 9.36 7.88 5.46 5.00 Table 4: BLEU and MetricX scores for 10 English directions from FLORES 200. Best results are highlighted in bold. the 5-shot setting, retrieving demonstrations from the FLORES 200 dev test with bm25s (Lù, 2024) following Zebaze et al. (2025c). As shown in Table 5, providing demonstrations does not help the thinking mode take performance to an upper level. Similarly, it does not help when translating into English, as reported in Table 6. B.2 RESULTS ON NTREX-128 AND TICO-19 In this section, we evaluate the models on 2 additional benchmarks: NTREX 128 (Barrault et al., 2019; Federmann et al., 2022) is an MT benchmark derived from WMT19 news data translated by professional human translators. It contains 1997 parallel sen34 Preprint. Under review. Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Czech Finnish French German Japanese BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 5.65 5.31 15.92 16.26 24.43 25.77 30.11 30.36 34.44 34. 15.69 34.34 22.53 22.49 14.68 14.48 9.18 8.74 6.65 6.59 5.11 4. 15.01 4.64 3.21 2.69 7.86 8.50 13.96 15.33 19.37 19.70 23.51 23. 11.19 24.91 23.13 23.40 19.18 18.71 13.88 13.39 10.21 9.89 7.59 7. 16.04 6.72 24.49 21.48 38.28 38.52 44.47 44.99 48.89 49.18 51.23 51. 29.14 50.35 9.51 10.28 4.69 4.54 3.66 3.25 2.89 2.58 2.37 2. 11.67 1.99 16.11 14.51 28.41 28.19 34.20 34.79 38.88 39.16 41.83 41. 22.99 42.58 10.15 10.43 4.30 4.05 3.14 2.54 1.92 1.59 1.39 1. 11.00 1.09 8.08 6.09 16.46 15.29 20.68 21.36 24.88 24.67 27.93 28. 16.14 27.86 9.29 11.84 5.85 5.89 5.22 4.79 4.28 4.10 3.86 3. 12.00 3.53 Kazakh Lithuanian Portuguese Spanish Turkish BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 0.93 1. 2.03 3.24 10.1 11.0 15.64 15.77 20.35 19.40 10.85 20.63 23.89 23. 22.03 21.58 15.26 14.50 11.07 10.80 8.30 7.97 15.88 7.61 2.21 2. 6.80 7.69 13.64 14.62 19.01 19.51 23.68 24.59 11.89 25.28 24.04 24. 20.51 20.65 15.10 14.44 10.97 10.67 8.41 7.86 16.26 7.12 26.00 21. 39.27 39.22 45.61 45.93 49.03 49.44 50.93 51.37 29.30 50.89 9.44 11. 4.41 4.23 3.44 3.16 2.64 2.47 2.28 2.10 11.61 1.95 17.51 15. 25.76 25.63 28.43 29.12 30.87 31.19 32.38 32.67 17.69 33.05 8.38 9. 3.85 3.76 3.14 2.69 2.30 2.09 1.94 1.72 11.53 1.58 6.46 5. 14.10 14.71 21.32 21.95 27.01 26.98 30.99 29.97 16.08 30.97 21.24 21. 14.36 13.41 10.28 9.63 7.34 6.82 6.08 5.66 14.69 5.26 Table 5: 5-shot BLEU and MetricX scores for 10 English directions from FLORES 200. Best results are highlighted in bold. tences and is recommended for the evaluation of from-English translation directions. We use the first 1000 sentence pairs for evaluation, and the last 997 sentence pairs as the selection pool. TICO-19 (Anastasopoulos et al., 2020) is an MT benchmark comprising texts on the COVID-19 pandemic covering 35 languages. Its validation and test sets consist of 971 (used as selection pool) and 2100 samples respectively. We focus on translating from English. We report the results obtained on NTREX 128 in Table 7 and those obtained on TICO-19 in Table 8. We reach the same conclusions on NTREX 128 than with FLORES 200. On TICO-19 we consider different languages, mostly from Asia. Despite some of them being low-resource languages (Khmer, Marathi, Nepali), thinking offers little to no advantage. 35 Preprint. Under review. Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking"
        },
        {
            "title": "Models",
            "content": "QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Czech Finnish French German Japanese BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 23.33 22.05 34.90 34.23 38.76 39.06 40.02 40. 41.91 43.28 43.19 44.23 7.29 7.42 3.33 3.54 2.15 2.26 1.80 1. 1.60 1.57 1.47 1.42 11.40 9.30 23.42 22.62 30.57 30.70 33.28 33. 35.45 36.31 37.01 37.84 13.01 11.40 6.25 6.25 3.68 3.47 2.52 2. 2.12 2.00 1.84 1.77 35.95 35.02 41.40 41.89 44.54 45.59 45.80 46. 46.70 48.64 47.51 48.72 3.32 3.31 1.99 2.03 1.59 1.61 1.42 1. 1.37 1.32 1.27 1.26 33.10 31.26 40.50 40.94 44.43 44.83 45.14 45. 46.28 47.49 46.77 47.52 4.12 4.24 2.29 2.31 1.59 1.66 1.50 1. 1.36 1.32 1.29 1.26 15.47 14.88 23.67 22.66 27.05 27.32 27.98 28. 29.20 30.43 29.99 30.88 5.96 6.38 3.03 3.41 2.26 2.40 1.92 1. 1.75 1.77 1.68 1."
        },
        {
            "title": "BLEU MetricX",
            "content": "6.94 5.92 17.90 16.20 25.64 24.20 29.31 28.91 32.01 32.17 33.33 34. 15.07 15.67 8.48 9.17 5.02 5.31 3.80 3.98 3.06 3.08 3.27 2. 10.21 4.36 23.61 22.23 29.64 29.37 33.01 33.28 34.31 35.15 36.10 37. 13.71 7.41 6.26 6.54 3.75 3.83 2.76 2.81 2.48 2.43 2.06 2. 39.22 37.72 45.63 46.32 49.12 50.16 50.04 50.89 51.22 53.28 51.69 53. 3.59 3.70 2.15 2.31 1.79 1.84 1.54 1.56 1.48 1.49 1.44 1. 25.92 25.64 29.79 30.49 32.59 33.55 32.89 33.80 34.06 35.50 34.49 35. 3.92 3.99 2.43 2.46 1.95 2.02 1.75 1.78 1.63 1.63 1.57 1. 16.12 13.83 29.31 27.65 34.96 34.69 37.88 37.77 39.52 40.18 40.91 41. 9.21 9.34 4.39 4.53 2.77 2.88 2.15 2.24 1.88 1.86 1.81 1. Table 6: BLEU and MetricX scores for 10 English directions from FLORES 200. Best results are highlighted in bold. 36 Preprint. Under review. Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Czech Finnish French German Japanese BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 4.62 4. 13.64 14.11 22.23 22.06 28.40 27.54 31.04 29.92 32.63 30.48 22.82 15. 16.44 16.13 9.92 10.16 7.19 7.92 5.68 6.18 5.73 5.73 3.03 3. 6.93 6.95 12.70 11.83 17.02 16.63 20.22 19.86 21.32 21.71 23.26 21. 19.37 19.19 14.04 14.56 10.28 10.82 7.84 8.25 7.47 7.38 16.80 16. 26.14 25.61 31.07 30.82 33.10 33.44 34.61 34.80 34.76 34.55 10.38 9. 5.86 6.13 4.09 4.43 3.51 3.51 3.07 3.17 2.95 2.84 13.93 13. 24.33 23.43 30.41 30.21 34.19 34.27 36.85 36.82 37.09 37.13 10.38 9. 4.97 4.98 2.97 3.16 1.98 2.20 1.68 1.69 1.53 1.46 5.66 4. 12.93 12.41 18.11 16.77 19.55 19.99 22.02 21.88 21.19 20.59 10.06 8. 6.94 7.03 5.64 5.91 5.09 5.15 4.65 4.61 4.97 4.51 Kazakh Lithuanian Portuguese Spanish Turkish BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 0.26 0.66 0.52 1.13 5.95 5. 9.70 8.93 13.18 12.65 13.53 13.05 23.60 22.05 23.71 23.68 17.29 17. 13.39 14.07 10.57 11.04 10.93 10.40 0.90 1.46 4.62 5.06 10.59 10. 14.80 14.96 18.89 19.42 19.93 20.77 24.26 23.90 21.42 21.47 15.76 15. 11.97 11.92 9.58 9.16 9.01 8.62 19.57 18.37 29.23 28.92 34.60 33. 36.71 36.89 38.49 38.39 39.21 39.23 10.00 9.28 5.49 5.60 3.76 4. 3.10 3.35 2.83 2.91 2.74 2.76 22.01 20.12 30.94 30.81 36.44 35. 38.91 38.85 40.24 40.18 40.70 41.30 8.77 8.21 4.83 4.96 3.32 3. 2.78 2.95 2.53 2.59 2.41 2.35 6.76 7.04 12.58 12.73 19.26 19. 23.07 23.29 26.35 26.31 27.62 26.68 21.87 20.30 16.33 15.25 11.29 11. 8.67 8.84 7.38 7.36 6.93 6.97 Table 7: BLEU and MetricX scores for 10 English directions from NTREX 128. Best results are highlighted in bold. 37 Preprint. Under review. Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Models QWEN3-0.6B w/o Thinking QWEN3-1.7B w/o Thinking QWEN3-4B w/o Thinking QWEN3-8B w/o Thinking QWEN3-14B w/o Thinking QWEN3-32B w/o Thinking Bengali Farsi Hindi Indonesian Khmer BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 0.07 0.32 2.85 4.69 14.50 13.90 18.97 17. 23.68 22.87 17.94 24.40 22.31 21.61 15.49 13.13 6.31 6.16 4.13 4. 3.10 3.28 7.76 2.89 1.86 3.17 10.07 10.46 19.11 18.59 25.13 24. 29.01 29.33 27.39 29.52 20.03 3.46 13.50 13.56 7.62 7.72 4.70 4. 3.55 3.38 4.41 3.16 0.32 0.58 6.32 9.03 24.40 23.40 30.22 29. 35.81 35.65 31.15 37.37 21.64 21.79 14.70 13.15 6.23 6.34 4.82 4. 4.12 4.26 6.83 3.98 22.66 21.18 37.04 38.33 45.14 46.60 48.83 50. 51.21 52.07 51.95 52.47 7.43 6.51 3.47 3.45 2.82 2.52 1.89 1. 1.66 1.71 1.62 1.59 0.63 1.02 1.04 1.54 9.20 8.27 16.66 16. 22.09 22.30 16.52 21.22 22.87 23.80 22.10 22.21 15.05 15.54 10.65 10. 8.21 8.41 11.42 7.13 Marathi Malay Nepali Tagalog Urdu BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 0.03 0.06 1.27 2.49 7.78 7.75 12.01 11.93 15.05 15.11 13.26 16. 23.58 24.04 19.50 18.36 9.80 10.61 6.92 7.03 5.35 5.49 8.61 4. 13.57 10.49 22.81 23.17 31.93 32.87 37.30 39.25 41.91 42.91 43.30 44. 9.11 4.98 4.71 4.54 3.99 4.07 2.95 3.12 2.63 2.67 2.57 2. 0.24 0.61 2.45 9.83 9.83 8.13 15.25 14.42 18.23 17.93 17.40 20. 22.75 21.58 17.30 10.04 10.04 11.42 6.93 7.53 5.89 6.26 8.31 5. 5.66 7.72 10.18 10.65 21.62 24.30 27.44 30.00 32.38 34.29 31.44 36. 21.11 17.76 18.67 18.64 11.54 10.52 7.60 7.18 5.35 5.33 6.04 4. 0.41 0.85 2.74 3.08 11.37 10.55 16.35 16.56 20.97 21.04 20.21 21. 22.99 22.18 18.42 18.26 10.82 10.97 7.06 6.88 5.06 5.24 6.36 4. Table 8: BLEU and MetricX scores for 10 English directions from TICO-19. Best results are highlighted in bold. 38 Preprint. Under review. B.3 DOES DISTILLED CHAIN-OF-THOUGHT AS INTERMEDIATE TOKENS IMPROVE PERFORMANCE? Figure 8: Comparison between COTFT and IOFT with six different CoT templates. Following Section 5.2, we compare COTFT with IOFT across all six templates using gemma-3-1b-pt as the student and gemma-3-27b-it as the intermediate teacher. We focus on translating from English to Lithuanian. As shown in Figure 8, COTFT consistently lags behind IOFT. The gap can be as large as 5 BLEU and 4 MetricX. Despite T3 being the best template, it is still largely behind IOFT in terms of performance. B.4 WHAT HAPPENS WHEN WE USE TRACES FROM MT PROMPTING STRATEGIES AS INTERMEDIATE TOKENS? In Figure 9, we observe that COTFT with reasoning traces based on alternative prompting strategies outperforms IOFT. SBYS is an exception, for which COTFT is behind IOFT. Across prompting strategies, IOFT-MAX outperforms IOFT and COTFT with the only exception of CompTra. This is exactly what happened with our experiments with Llama-4-Scout-17B-16E-Instruct and gemma-3-4b-pt in Xhosa. Figure 9: Comparison between IOFT and COTFT with five different prompting strategies. Finally, as shown in Figure 10, COTFT-MAX fails to improve over IOFT-MAX, confirming our previous conclusions with Xhosa. In this case, CompTra is not an exception. For all the strategies, COTFT-MAX and IOFT-MAX are very close in performance, with IOFT-BOA topping them all. This again suggests that reasoning traces do not help, even when they are based on MT prompting strategies whose drafting attempts do not outperform the ground truth in terms of quality. Having target translations of high-quality (IOFT-BOA) has the highest impact, outperforming the standard IOFT by 3 BLEU and 1.3 MetricX with the same number of parallel pairs and the same training recipe. 39 Preprint. Under review. Figure 10: Comparison between IOFT and COTFT with five different prompting strategies. B.5 DOWN THE RABBIT HOLE OF SENTENCE DECOMPOSITION Following Section 6.1, we evaluate multiple sentence decomposition approaches and compare COTFT against standard IOFT and IOFT-EXT. As shown in Figure 11, COTFT consistently outperforms IOFT across all decomposition strategies. Again, SP and CompTra works better than and H. Using the generated pairs as additional training samples (i.e. IOFT-EXT) is particularly helpful with and SP because they correspond to fully-fledged sentences as explained earlier. COTFT with and COTFT with CompTra outperform the corresponding IOFT-EXT suggesting that short phrases and their translations are relevant intermediate information for COTFT. COTFT with CompTra works just as well as IOFT-EXT(P) which is impressive as the latter required multiplying the size of by six. Figure 11: Comparison between IOFT and COTFT with four different sentence decomposition strategies. B.6 COT DISTILLATION: WHAT HAPPENS WHEN WE CHANGE THE TEACHER? In this section, we run the same CoT distillation experiment as in Section 5.2 but we use DeepSeek-R1-Distill-Llama-70B instead as the teacher. As seen in Figure 12, COTFT behaves similarly to IOFT across templates. The performance of COTFT is better than what we observed with Llama-4-Scout-17B-16E-Instruct despite LLAMA being better at translating into Xhosa. We attribute this to the thinking abilities of DEEPSEEK-R1 which, despite not being good at generating Xhosa, can generate better explanation compared to LLAMA as to why hypothesis is an accurate translation of source. However, in both cases, COTFT does not improve over IOFT, which is also faster to train in comparison. 40 Preprint. Under review. Figure 12: Comparison between IOFT and COTFT with six different CoT templates. B.7 MT TRACES GENERATED BY PROMPTING STRATEGIES AS INTERMEDIATE TOKENS: WHAT HAPPENS WHEN WE CHANGE THE TEACHER? on the run and"
        },
        {
            "title": "Xhosa",
            "content": "change teacher additional information We from Llama-4-Scout-17B-16E-Instruct to gemma-3-27b-it. GEMMAs performance zero-shot MT performance (on FLORES 200, BLEU=12.82, MetricX=7.62) is worse than to investigate how this impacts our LLAMAs (BLEU=16.90, MetricX=6.53) and we want findings. First of all, COTFT outperforms IOFT across prompting strategies with the exception of SBYS. IOFT-MAX outperforms IOFT and we observe the same behaviour between IOFT-MAX(COMPTRA) and IOFT as we did with LLAMA. Despite COTFT with SBYS underperforming IOFT, IOFT-MAX(SBYS) outperforms IOFT, meaning that translation attempts embedded in SBYS-inspired CoT are helpful, but they are drowned out by other useless tokens, which impacts how well COTFT performs. Ultimately, IOFT-BOA works best; although IOFTMAX(TEAR) achieves higher BLEU scores it lags behind in terms of MetricX. It even outperforms the teacher gemma-3-27b-it despite being six times smaller. Figure 13: Comparison between IOFT and COTFT with five different prompting strategies. When we use gemma-3-4b-it as the teacher (gemma-3-4b-pt being the student), the traces obtained using prompting strategies do not help COTFT to outperform IOFT. In Figure 14, we observe degradation of performance that confirms our intuition suggesting that these traces are helpful only if they contain translation attempts that are better than the ground truth. B.8 REINFORCEMENT LEARNING AFTER IOFT AND COFT Building on the experiments presented in Section 6.2, we apply GRPO to the final checkpoints (checkpoint-5000) under three additional configurations: COTFT with MAPS, SBYS, TEaR, and Self-Refine. The results, shown in Figure 15, indicate that GRPO results in consistent improvements of approximately +1 BLEU and -0.7 MetricX points across all setups, mirroring the trends observed 41 Preprint. Under review. Figure 14: Comparison between IOFT and COTFT with five different prompting strategies. with CompTra. Notably, GRPO maintains the relative performance ordering between IOFT and COTFT prior to fine-tuning. However, COTFT models do not gain more from GRPO than IOFT models, and in practice, performing IOFT alone (rather than GRPO) can achieve comparable or greater gains at lower computational cost. Figure 15: Comparison between IOFT and COTFT with GRPO."
        }
    ],
    "affiliations": [
        "Inria Paris, France"
    ]
}