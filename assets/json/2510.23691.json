{
    "paper_title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents",
    "authors": [
        "Zihao Wang",
        "Xujing Li",
        "Yining Ye",
        "Junjie Fang",
        "Haoming Wang",
        "Longxiang Liu",
        "Shihao Liang",
        "Junting Lu",
        "Zhiyong Wu",
        "Jiazhan Feng",
        "Wanjun Zhong",
        "Zili Li",
        "Yu Wang",
        "Yu Miao",
        "Bo Zhou",
        "Yuanfan Li",
        "Hao Wang",
        "Zhongkai Zhao",
        "Faming Wu",
        "Zhengxuan Jiang",
        "Weihao Tan",
        "Heyuan Yao",
        "Shi Yan",
        "Xiangyang Li",
        "Yitao Liang",
        "Yujia Qin",
        "Guang Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities."
        },
        {
            "title": "Start",
            "content": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents"
        },
        {
            "title": "Bytedance Seed",
            "content": "See Contributions section for full author list."
        },
        {
            "title": "Abstract",
            "content": "We present Game-TARS, generalist game agent trained with unified, scalable action space anchored to human-aligned native keyboardmouse inputs. Unlike APIor GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide promising path toward generalist agents with broad problem-solving abilities. Date: October 27, 2025 Correspondence: yujia.qin@bytedance.com; shiguang.sg@bytedance.com aProject: https://seed-tars.com/game-tars 5 2 0 2 7 2 ] A . [ 1 1 9 6 3 2 . 0 1 5 2 : r Figure 1 Game-TARS achieves higher level of performance compared to humans, domain experts, and general VLMs in unseen 3D virtual environments, including open-world [19], FPS games [78], web games, and simulators [11]."
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2 Continual Pre-Training for Generalist Game Agent",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . 2.1 Scalable and Generalist Action Space for Computer-Use Agents . . . . . . . . . . . . . . . 2.2 Native Sparse ReAct Pretraining via Thinking Aloud . . . . . . . . . . . . . . . . . . . . . . . 2.3 Continual Pre-training with Decaying Loss Function . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.1 Experimental Setups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.2 Evaluation on Open-World Minecraft\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.3 Evaluation on Unseen Benchmarks and Simulators . . . . . . . . . . . . . . . . . . . .\n4.3 Analysis and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . .\n4.3.1\n4.3.2 Ablation on sparse thinking\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3.3 Validation on Continual Pre-Training Loss . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "Scaling Experiments on Training Datasets and Inference Steps 5 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Game Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Generalist Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Broader AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Rollout Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Rollouts in Minecraft . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Rollouts in Unseen Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Prompts of Game-TARS on Minecraft . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Prompts of Game-TARS on Unseen Games 5 5 6 8 8 8 10 11 11 12 13 13 14 14 15 16 18 18 18 19 20 20 20 21 21 27 27 31 31 32"
        },
        {
            "title": "Introduction",
            "content": "Building Generalist Artificial Agents capable of seamlessly interacting with complex and dynamic digital environments has emerged as key research path toward achieving Artificial General Intelligence (AGI) [10, 41, 46]. Video games, with their diverse task objectives, intricate interaction logic, and rich visual information, provide an ideal platform for both training and evaluating such agents [7, 58, 65]. Despite substantial progress, existing approaches still face significant challenges in creating truly scalable agents with broad generalization capabilities [51, 61, 67]. One of the core bottlenecks is the definition of the action space: traditional agents often rely on highly customized, pre-defined action sets tailored for specific environments [1, 47, 48]. This tight coupling between the agent and the environment severely limits the agents generalization, making it difficult to adapt to new tasks, particularly in open-world settings [4, 19]. To overcome these limitations, we propose novel framework for pre-training general game agents. Our central contribution introduces fundamental shift in paradigm: we move away from application-specific high-level action instructions [66, 70] and instead define scalable, unified action space directly tied to low-level computer input devices, such as the keyboard and mouse. This native human interaction paradigm ensures that the agent can operate within any graphical user interface (GUI) environment, laying the foundation for large-scale, cross-domain pre-training. Utilizing this unified action space, we have developed comprehensive agent training process that includes continual pre-training and post-training stages on vision-language model [3, 12, 21, 44]. In the continual pre-training phase, we employ an efficient, native ReAct paradigm [82] that incorporates Sparse Thinkinginterleaving reasoning and action only at critical decision points to mirror human cognitive patterns. This is achieved by combining offline chain-of-thought synthesis [75] with an online think-aloud protocol [14] to collect and refine high-quality, causally consistent multimodal trajectory data. In the post-training phase, we implement series of targeted fine-tuning strategies to systematically enhance the agents core capabilities. These strategies include: enforcing strict adherence to instructions through automated action space augmentation; fostering dynamic context learning via multimodal prompts and long-range trajectory learning; refining the agents decision-making by enforcing efficient Sparse Thinking through rejection fine-tuning; and developing long-term memory abilities via dual-layer memory architecture. To extend the agents capabilities beyond gaming, we also incorporate cross-domain agent trajectories from fields such as code generation, GUI automation, and research tasks into the post-training data. This diverse mix of data aims to transform the model from specialized game-player into versatile general computer-user with broader problem-solving capabilities. The work presented in this paper offers comprehensive, end-to-end solution for developing the next generation of generalist agents. We first validated the effectiveness of the unified action space and large-scale continual pretraining by testing the agents capabilities in the open-world game Minecraft [19]. Unlike grounding-based [55] or API-based [66] actions, we found that the unified action space, when trained with less than 10B data, did not initially outperform GUI-based action agents in terms of success rate. However, the key advantage of the unified action space lies in its scalabilitylarge-scale data can be efficiently collected, and the agent can be pretrained using consistent format across all games. Our scaling experiments demonstrate that this unified action space provides an excellent foundation for the realization of generalist agents. When trained on large corpus of general computer-use data, Game-TARS, based on the unified action space, achieved approximately 2x performance improvement in Minecraft compared to previous state-of-the-art expert models [74]. Furthermore, we conducted extensive evaluations on wide range of untapped benchmarks, including FPSgame Vizdoom [78], 3D Simulator Miniworld [11], and online web games [49]. The results highlight the exceptional generalization ability of our generalist agent. Our mini version of Game-TARS outperformed renowned pre-trained models such as Gemini-2.5-Pro [12], GPT-5 [46], and Claude-4-Sonnet [2] on various tasks, showcasing superior performance across diverse domains. These results provide compelling evidence of the broad applicability and robustness of the unified action space and continual pretraining approach, establishing Game-TARS as formidable agent in wide variety of environments. 3 Figure 2 Generalist Game Agent Game-TARS. Game-TARS can interpret and respond to various human instructions across diverse environments using single neural network with consistent set of weights. It was pre-trained on wide range of multimodal datasets, including vision-language question-answering, captioning, over 20k hours of game trajectories, GUI agent trajectories, and more. 4 Table 1 Taxonomy of computer-use agent action spaces, contrasting high-level, environment-specific APIs with the low-level, general-purpose device controls. Action Hierarchy Agent Action Level Action Definition MCP Application MCP Application MCP Application Code Application Code Application System-Level(Desktop) System-Level(Desktop) System-Level(Desktop) System-Level(Desktop) System-Level(Mobile) System-Level(Mobile) GUI GUI GUI GUI GUI GUI - - - Retrieve the relevant URLs for the query. Get the content in URL. Execute script file or program. Compile source code into an executable. Search(query) ReadUrl(url) CreateFile(filePath) Create new file at the specified file path. Run(script) Compile(code) Drag(x1, y1, x2, y2) Drag from (x1, y1) to (x2, y2) scroll(200) Hotkey(key) LeftDouble(x, y) LongPress(x, y) PressHome() Scrolls up by 200 units. Presses the specified hotkey. Double-clicks at (x, y). Long presses at (x, y). Presses the home button. Device-Level(Mouse) Device-Level(Mouse) Device-Level(Keyboard) mouseMove(dx, dy) mouseClick(buttons) keyPress(keys) Move the mouse in the dx and dy directions. Click the mouse buttons. Press buttons in keyboard."
        },
        {
            "title": "2 Continual Pre-Training for Generalist Game Agent",
            "content": "Our objective is to model an inherently multi-task generalist agent. We formulate this agent as autoregressive, parameterized policy, πθ. The policys role is to map the agents history and current perception to decision. At any given timestep t, the agent receives an Observation (ot) from the environment (specifically, the current visual image frame). It also maintains an internal memory state (ht), which serves as compressed summary of all past interactions. Following the ReAct paradigm [82], the agents decision-making process is modeled as two-stage sequence: Reasoning (rt): An internal cognitive process that encompasses context analysis, memory retrieval, planning, and self-reflection. Action (at): An external interaction with the environment. The policy πθ is responsible for generating this entire reasoning and action chain. The core policy function at timestep is thus defined as: (rt, at) πθ( ot, ht) (1) i=0). We define complete where the memory state ht is function of all preceding steps, i.e., ht = ((ri, ai, oi)t1 ReAct cyclecomprising the agents reasoning, its action, and the environments subsequent observationas single \"step\": (rt, at, ot). Therefore, complete trajectory (τ ) of length , representing the full log of the agents interaction, is formalized as sequence of these steps: τ = (r0, a0, o0), (r1, a1, o1), . . . , (rT , aT , oT ) (2) Our training objective is to optimize the parameters θ of our policy πθ using large dataset of these trajectories."
        },
        {
            "title": "2.1 A Scalable and Generalist Action Space for Computer-Use Agents",
            "content": "A critical challenge in constructing generalist agent is the definition of an action space that is both broadly applicable and highly scalable. Traditional agents, such as those trained in environments like Minecraft [4, 22] or StarCraft [65], typically operate on predefined, task-specific action space. These action spaces, whether encapsulated via game-internal protocols (e.g., Minecraft Protocol) or code-level APIs [66], necessitate the design and implementation of unique instruction set for each new environment or task. While effective for specialized domains, this approach suffers from fundamental limitation: the tight coupling of the action space to the environment and task, which severely constrains the models generality and scalability. Adapting the agent to novel game or application often requires substantial manual effort in redefining action spaces, engineering API wrappers, and annotating data, thereby impeding generalization to open-world scenarios. 5 To partially mitigate this issue, prior work has introduced agents that operate on Graphical User Interfaces (GUIs), such as UI-TARS [49], by identifying and manipulating GUI elements. Although such methods improve generality, we observe that their action spaces remain constrained by the underlying operating system (OS) or specific UI frameworks. Significant discrepancies in GUI element structures and interaction modalities across Windows, macOS, and Linux, or even between different software applications, prevent the realization of truly universal action space. To transcend these limitations, we propose more fundamental solution: abstracting the agents action space to lower level and grounding it directly in the most universal input devices for human-computer interactionthe keyboard and mouse. By doing so, the agents action instruction set is decoupled from any high-level application or OS-specific API, achieving direct alignment with human users physical operations. We term this the Human-Native Interaction paradigm. This paradigm ensures an unprecedented level of universality for the action space across all computer graphical environments. Specifically, we define the agents action space as composition of three primitive operations: mouseMove(dx, dy): This action simulates the two-dimensional movement of the mouse cursor on the screen. The parameters dx and dy represent the relative displacement along the horizontal and vertical axes, respectively. mouseClick(button): This action simulates mouse button click event. The button parameter specifies the target button, such as left, right, or middle. keyPress(keys): This action simulates the press and release of one or more keyboard keys. The keys parameter can be single key (e.g., keyPress(w)) or combination of keys (e.g., keyPress(ctrl, c)). We augment this core set with supplementary actions such as Think, LongThinking, and No-Op to facilitate more complex reasoning and behavior. The complete action space and its descriptions are provided in Table 1. The principal advantage of grounding the action space in these input primitives is its exceptional scalability, enabling our generalist agent to operate in any standard computing environment without modification, much like human user. More importantly, in contrast to traditional discrete action spaces centered on highlevel maneuvers, our Human-Native Interaction paradigm is inherently time-aligned. Each action primitive corresponds directly to physical input at specific point in time. This design obviates the need for special modeling of action durations or complex temporal dependencies, as the agent can autonomously learn the timing and rhythm of its operations through experience. Consequently, whether interacting with 2D pixel-based games, complex 3D open-world environments, or standard desktop applications, the agent employs the same unified and stable action space. This consistency establishes robust foundation for large-scale, cross-domain Continual Pre-Training."
        },
        {
            "title": "2.2 Native Sparse ReAct Pretraining via Thinking Aloud",
            "content": "Prior research has demonstrated that co-modeling reasoning and action as unified output for an agent significantly enhances model generalization and performance [49, 59, 82]. Recent methods usually employ pre-trained vision language model [21] to inject reasoning steps into the raw trajectory data τ = {(ot, at)}T t=0 from human gameplaying, translating the observation-action pairs into the ReAct format. However, previous studies have shown that retrospectively adding reasoning to recorded action sequences is exceptionally difficult and inefficient [49, 67], as accurately reconstructing the annotators real-time thought process is nearly impossible. On the other hand, our time-aligned, high-frequency action modeling (with action intervals of only 50-100ms) results in action sequences with numerous repetitive and continuous operations, during which human players cognitive state does not significantly change. Consequently, forcing the generation of reasoning step at every action step is both impractical and unnecessary. Building upon this, we have designed an online think-aloud data collection and processing pipeline to generate high-quality, native Sparse ReAct trajectories for pre-training. Unified Trajectory Data Paradigm. We define the pre-training trajectories in sparse-thinking format, formalized as: τ = (r0, a0, o0), . . . , (an, on), . . . , (rm, am, om), . . . , (rT , aT , oT ). (3) 6 Figure 3 The pipeline of Think-Aloud data collection and post-processing. This process captures and synchronizes three types of original inputs (screen, keyboard, and mouse, audio), refines sparse-thinking through the ASR-LLM pipeline, and uses timestamp aligner to synthesize the final (Instruction, Observation, Thinking, Action) datasets. Here, some timesteps contain the complete tuple of reasoning, action, and observation (rm, am, om), while other timesteps only contain action-observation pairs (an, on). To ensure the structural integrity of each trajectory, we mandate that reasoning steps must be included at the beginning (t = 0) and end (t = ) of the trajectory, corresponding to planning and summarization, respectively. Inspired by Deitke et al. [14], we instead adopt think-aloud protocol, Online Think-Aloud Data Collection. requiring annotators to verbalize their thoughts in real-time via audio while performing tasks, as demonstrated in Figure 3. This method allows us to collect ReAct sequences containing native, in-situ reasoning. To ensure the density and quality of the reasoning data, we instruct annotators to provide thoughts for at least three key moments per minute. Data Recording and Preliminary Processing. Our data acquisition system utilizes parallel recording program to synchronously capture players screen frames (as observations ot), low-level mouse and keyboard input signals (as actions at), and the audio stream from the \"think-aloud\" process (as raw thoughts rt). After collection, we use an ASR model [52] to transcribe the audio into text, which is then refined using Large Language Model (LLM) to remove noise and enhance logical consistency. Causal Alignment via Visual Anchors. The precise alignment of multi-modal data, particularly ensuring the causality of observation-action pairs, is critical step. In practice, we found that due to minor latencies in the recording system, relying solely on timestamps causes action signals to lag behind their corresponding screen frames. This temporal misalignment severely disrupts the datas causal relationship. If used directly for training, it can lead the agent model to degenerate into an Inverse Dynamics Model (IDM) [4]where the model learns to infer actions from state differences rather than deciding based on the current statewhich would significantly impair its performance in practical applications. We developed realignment method based on visual anchors to overcome this issue. We use the on-screen mouse cursor as visual anchor and employ visual grounding model [67] to precisely track its pixel position in each frame. By comparing the tracked changes in cursor position with the recorded mouse movements (dx, dy), we can accurately realign each action signal to the exact observation frame in which it was executed, thereby completely correcting the timestamp bias and restoring the causal chain. Trajectory Construction and Refinement. After achieving causal alignment, we construct training sequences centered on observation frames. For the time window between any two consecutive frames, all recorded keyboard and mouse operations are merged into single action event; lack of operations is marked as no-op. Finally, to reinforce the semantic association between reasoning and action, we implement thought timestamp relocation strategy. This involves intelligently adjusting the timestamp of thought to align with the subsequent action to which it is most semantically relevant, ultimately producing highly synchronized and causally consistent (observation ot, reasoning rt, action at) training sequences."
        },
        {
            "title": "2.3 Continual Pre-training with Decaying Loss Function\nIn our auto-regressive training framework, the agent predicts an action a at each timestep t based on the\ncomprehensive history of all preceding observations and actions, denoted by Ht = (o1, a1, o2, a2, . . . , at−1, ot).\nA critical challenge we identified in our preliminary experiments is the severe imbalance in the action\ndistribution within non-Markovian agent trajectories [87], and this trend intensifies as the trajectory length\nincreases. Specifically, temporally persistent actions, where at = at−1, constitute the vast majority (over\n90%) of the action sequence1. This high frequency of repetitive, low-entropy actions causes the standard\ncross entropy loss to be dominated by these easy samples and leads to the causal confusion in imitation\nlearning [13], thereby masking the learning signal from the more challenging and crucial action transitions\n(where at ̸= at−1). Previous methods, such as VPT [4], remove all no-ops from the trajectory; however, this\ndisrupts the complete action distribution, as waiting is necessary in some scenarios. To mitigate this issue, we\nintroduce a history-aware loss re-weighting scheme that dynamically attenuates the loss contribution from\nconsecutive identical actions. Formally, we redefine the training loss for a trajectory τ = (o1, a1, . . . , oT , aT )\nas:",
            "content": "L = (cid:88) t=1 ωt log (at Ht) (4) The weight ωt for the action at timestep is determined by its consistency with the immediate past. Let kt be the number of consecutive timesteps including where the action has remained unchanged (i.e., at = at1 = . . . = atkt and atkt = atkt+1. The weight ωt is then defined as an exponential decay: ωt = γkt1 (5) Here, γ (0, 1) is decay factor hyperparameter (in our implementation, γ = 1/2). This formulation assigns full weight of 1 (since γ0 = 1) to any action that is new or different from the previous one, while exponentially reducing the weight for each subsequent repetition. By systematically down-weighting the loss from monotonous action sequences, this method compels the model to focus more of its capacity on learning the high-entropy decision boundaries associated with action changes, leading to more robust and capable policy."
        },
        {
            "title": "3 Post-Training for Generalist Game Agent",
            "content": "After large-scale continual pre-training on over 20k game hours (about 500B tokens), we introduce post-training stage to further enhance the agents execution capabilities and interactive intelligence in specific tasks. This stage focuses on strengthening three core abilities: instruction following, sparse thinking, and long-term memory. 3."
        },
        {
            "title": "Instruction Following",
            "content": "To build generalist agent that can interact seamlessly across different games and application environments, the primary task is to ensure it can strictly follow the instructions for given task, particularly in understanding and using the action space of specific environment. Although continual pre-training endows the model with powerful general interaction abilities, it also leads to the learning of prior distribution over actions based on massive dataset. This distributional bias presents two major challenges during the post-training phase: 1The detailed value is different on different games. 8 Figure 4 Distribution of different game types in the Game-TARS training dataset. Action Space Violation: The model tends to output the actions defined in the pre-training data, but they are invalid in the current task. For example, even if the System Prompt specifies the UpArrow as the jump key, the model might still output Space to perform jump out of habit. Behavioral Inertia: The model may ignore current observations and instructions, instead relying on the frequency distribution of actions to minimize the loss function. This manifests as the model frequently outputting common actions from the pre-training data (e.g., the key) rather than making decisions based on the current context. Automatic Action Space Augmentation. To address these issues, we propose fine-tuning strategy called Automatic Action Space Augmentation. The core idea of this strategy is to decouple the semantic function of an action from its specific key binding during training. We preserve the semantics of actions in the original trajectory data but randomly replace their corresponding key bindings. For instance, in trajectory, we might randomly replace the original key used for \"move forward\" with the key, while simultaneously defining the function of as \"move forward\" in the System Prompt. Through this random substitution, we compel the model to rely on the System Prompt to understand the currently valid action space and its semantics, rather than merely memorizing and reproducing high-frequency action patterns from the pre-training data. Furthermore, this method effectively mitigates the problem of highly imbalanced action distributions in the training data, preventing the model from gaming the loss function by exploiting this prior distribution. Inverse-Dynamics Prediction. To further reinforce the models understanding of action semantics, we introduce Inverse-Dynamics Prediction as an auxiliary training task. This task requires the model to infer the action at that caused state transition, given the current observation ot and the next observation ot+1. This is formally represented as: at ( A, [oi, ai]t1 i=1, ot, ot+1), (6) where is the predefined action space described in natural language. This task compels the model to learn the causal relationship between actions and changes in the environmental state, thereby fostering deeper understanding of the actual effects of actions. Finally, throughout the entire post-training phase, we provide explicit task instructions (Instruction, I) in the System Prompt for each video trajectory. This shifts the models prediction target from simple imitation learning to conditional instruction following, with the final training objective expressed as: (at I, A, ot). 9 Figure 5 Game-TARS is trained on wide range of games, including adventure, shooting, role-playing, and racing."
        },
        {
            "title": "3.2 Multimodal Prompts Elicit In-Context Learning",
            "content": "Another core insight of the post-training stage is that the agent should be able to learn from historical interactions and dynamically adjust its behavioral strategies. This is crucial for enhancing its generalization capabilities in unseen games. We define this ability as the agents In-Context Learning (ICL) [16, 26]: the agent dynamically adjusts the distribution of its future actions, including the selection of discrete actions and the scaling of continuous actions, by observing the effects of previous actions on the environment. Contextual Understanding of Discrete Actions via Multimodal Prompts. We found that relying solely on language-based System Prompts is often insufficient to accurately and comprehensively express the semantics of all actions. In complex action games, the function of certain keys (e.g., special skills or combos) is highly context-dependent and can even vary between characters. Such complex semantics are difficult to describe exhaustively with text. To address this, we introduce multimodal prompting mechanism. For each game, we select video clips demonstrating key \"action-effect\" pairs to provide as in-context examples for the agent. By observing these examples, the agent can intuitively understand the actual effect of specific actions in that environment, rather than relying on textual system prompts. During training, we even randomly mask the textual action descriptions, A, in the System Prompt, forcing the agent to infer the action space from the multimodal context. For continuous actions, such as mouse movements, the practical Adaptive Calibration of Continuous Actions. In-game mouse significance of their numerical values varies dramatically across different environments. sensitivity settings, differences in user hardware, and personalized key bindings can all cause the same (dx,dy) values to produce vastly different in-game responses. To enable the agent to adapt to this dynamism, we adopt the following strategies: Long-Horizon, Non-Markovian Trajectory Training: We use non-Markovian trajectory data for autoregressive training and extend the length of single trajectory to at least 80 consecutive frames. This ensures that the agent has sufficiently long history of interactions to reference for calibration when making decisions. Initial Action Loss Masking: To prevent the gradient direction from being distorted by uncalibrated actions at the beginning of trajectory (due to lack of historical context), we mask the action loss for the first mask steps, excluding them from gradient propagation. Continuous Action Data Augmentation: We apply random scaling to the mouse movements in the trajectories. For each training segment, we multiply all (dx,dy) values by random scaling factor k, while ensuring their absolute values do not exceed cap of 180. This forces the agent to learn to infer the correct scaling factor from the feedback of historical actions, rather than relying on fixed numerical range. 10 Finally, to further enhance the models ability to learn long-term dependencies, our training data also includes subset of ultra-long sequences with trajectories exceeding 240 steps."
        },
        {
            "title": "3.3 Sparse Thinking via Reinforcement Fine-Tuning",
            "content": "During the post-training phase, we observed that both the reasoning data synthesized offline by LLMs and that recorded online by humans exhibit degree of abstraction, preventing their content from precisely guiding subsequent action generation. Furthermore, the \"think-aloud\" data collection mechanism led some players to pause game actions while verbalizing their thoughts, resulting in large volume of reasoning text being associated with \"no-op\" actions. This decoupling of reasoning and action weakens the models ability to learn the causal relationship between them. Locating the sparse thinking Steps. Not all steps within trajectory require explicit reasoning for the agent to act correctly. To optimize the fine-tuning process, we first identify the timestamps where reasoning is most critical. We employ system prompt to let Game-TARS generate the actions only, πaction, which is required to predict the next action based solely on the history of observations and actions. We then evaluate Game-TARS on the offline trajectories. timestep is identified as requiring reasoning augmentation if the action-only model fails to predict the ground-truth action agt . This suggests that simple reactive policy is insufficient at this stage, and more deliberative reasoning process is required. This procedure yields set of timestamps Sr targeted for reasoning augmentation: Sr = 1, . . . , I(πaction(a ot, (oi, ai)t1 i=0) = agt ) = 0 (7) Where I() is the indicator function used to identify unmatched generated actions. The subsequent Rejection Fine-Tuning is then applied exclusively to the timestamps within this set Sr. Refining Thought-Action Chains via Rejection Sampling. To strengthen the causal link between reasoning and action, we adopted Rejection Fine-Tuning (RFT) approach to refine and enhance the offline trajectory data [25]. We use the actions from the original trajectories as critic to filter and generate high-quality reasoning-action pairs through Rejection Sampling. based on the observation ot, and then predict an action apred Specifically, for any given timestep in trajectory τ , we have the agent model π generate candidate thought based on this thought. We compare the rcand predicted action with the ground-truth action from the human demonstration, agt . The candidate thought is considered valid and retained for subsequent fine-tuning only if the predicted action matches the rcand ground-truth action. This process can be formally described as: Accept rcand iff I(π(at ot, rcand ) = agt ) = 1 (8) where I() is the indicator function. Through this method, we filter out significant amount of abstract reasoning that is irrelevant to the actual behavior, retaining only those thought chains that directly lead to correct actions. After obtaining high-quality reasoning data through rejection sampling, we perform two post-processing steps: Thought Consolidation: We found that in segments where the same action is performed consecutively, the effective reasoning content also shows high degree of repetitiveness. Therefore, we merge these consecutive, identical thoughts, retaining only the instance at the first timestep to reduce data redundancy. LLM Rewriting and Density Control: We further utilize an LLM to rewrite the filtered reasoning texts, making their expression more concise and their logic clearer. Concurrently, we actively control the density of reasoning in the trajectories (i.e., the proportion of steps with reasoning to the total number of steps) to better align with natural human cognitive rhythm."
        },
        {
            "title": "3.4 Long Episodic Memory Compression with Sparse Thought",
            "content": "To succeed in complex tasks requiring long-range planning and memory, an agent must be able to integrate historical information that far exceeds the standard context window length. To this end, we have designed 11 Figure 6 Memory Mechanism in Game-TARS Agent is divided into two areas: context and summary, corresponding to long-term and short-term memory, respectively. The context memory retains complete record of observations, thoughts, and actions for steps. In contrast, the summary memory only retains the thinking content for steps. Older information (beyond ) is popped using sliding window approach. two-tiered memory mechanism that remains consistent during both training and inference, effectively balancing high-fidelity recent memory with highly compressed long-term memory. Tier 1: Short-Term Contextual Memory. This tier serves as the agents \"working memory\" and is stored directly within the models context window. It maintains an interleaved sequence in the form of queue, containing the most recent observations and their corresponding ReAct pairs, refined through the \"sparse thinking\" process. This sequence is updated using sliding window, with standard context length of 32k tokens. However, because encoded visual images consume substantial number of tokens, the temporal span of interaction history that this window can accommodate is limited. Tier 2: Long-Term Summary Memory. When historical information is moved out of the short-term context window, it is compressed and stored in the second memory tier. To overcome the storage bottleneck imposed by visual tokens, we leverage the output of the \"sparse\" mechanismsparse and highly refined thought textsas summaries of historical events. Each step that is evicted from the context no longer retains its large visual observation data; instead, only the corresponding thought text is preserved. Thanks to the sparsity of thoughts and the high compression ratio of textual representation, this method allows the agent to retain long-term task context of over 2000 interaction steps at an extremely low token cost. Ultra-Long Context Training. To further enhance the agents ability to process and utilize long-range dependencies, we have included ultra-long sequences in our training data, with context window lengths reaching up to 128k tokens. By training on data of this scale, the agent learns to reason and plan over much broader time horizons, thereby improving its performance on tasks that require long-term memory."
        },
        {
            "title": "3.5 Post-Training with Cross-Source Agentic Trajectories",
            "content": "Our ultimate goal is to build generalist agent whose capabilities extend beyond the domain of gaming. Previous research has shown that incorporating diverse agentic trajectory data from various domains into the training process can significantly improve models general cognitive abilities, such as reflection, planning, and decision-making [67]. Therefore, in the post-training stage, we not only use massive amounts of game data but also introduce range of cross-source agentic trajectories, with the aim of cultivating more comprehensive generalist agent. We posit that the visual understanding and rapid-response capabilities learned through continual pre-training in game environments provide the agent with solid \"sensory-motor\" foundation. Building on this, we introduce several types of non-gaming agent data to strengthen its higher-level cognitive functions: Code Agent Trajectories: This data comes from agents performing programming tasks and includes the 12 Table 2 Training recipes in different stages of GameTARS agent. Stage Pre-Training Post-Training Image Sizes Image Tokens Text Tokens Total Tokens Game Number Game Hours Freeze ViT Maximum Learning Rate Minimum Learning Rate Batch Size Maximum Token Sequence Length Maximum Image Sequence Length Training Steps Warmup Steps (640,360)(1920,1080) 208B 326B 526B 100 20k 2e-5 2e-6 2048 32,000 80 16,000 1000 (640,360)(1280,720) 10B 29B 40B 500 2k 1.4e-5 2e-6 512 128,000 300 10,000 750 entire process from problem comprehension and logical decomposition to code implementation. By learning from these trajectories, the agent can enhance its structured thinking and long-range logical reasoning abilities [45, 50, 81]. GUI Agent Trajectories: This data covers the process of completing specific tasks in desktop or web applications (e.g., booking flight, managing files). It strengthens the agents ability to understand and execute multi-step instructions in non-gaming environments [48, 55]. MCP Agent Trajectories: This data records an agents process of information retrieval, document reading, content synthesis, and summarization to complete complex research tasks. Learning from this data can significantly enhance the agents ability to plan, reflect, and integrate knowledge within the ReAct loop [8, 20, 47, 54, 68]. By co-training on these diverse data sources alongside game data, we aim to achieve synergistic learning effect: game data solidifies the foundational interaction skills, while cross-domain agentic data builds more abstract and general problem-solving capabilities on top of this foundation. This process ultimately facilitates the models transformation from generalist game agent into general computer-use agent."
        },
        {
            "title": "4.1 Training Details",
            "content": "The training process for our agent comprises two main phases: continual pre-training and post-training (see Table 2 for detailed training parameters). During the early stages of this project, we explored multi-stage training strategy, consisting of the following steps: 1) training on pure action trajectories; 2) continuing training on offline-synthesized ReAct trajectories; 3) further training on online-collected human \"think-aloud\" ReAct trajectories; and 4) fine-tuning with general multimodal data to enable capability annealing. However, our experimental results indicated that this staged approach had detrimental effect on the models final performance. As result, we adopted unified, single-stage continual pre-training method, blending all data sources together, and trained the model on over 500 billion tokens. Subsequently, during the post-training phase, we fine-tuned the model on approximately 20 billion carefully curated high-quality tokens. Model Details. We trained three versions of Game-TARS, namely Game-TARS-MoE-mini, Game-TARSMoE-Large, and Game-TARS-Dense. Game-TARS-MoE-mini is initialized from the pre-trained checkpoint of Seed-VL-1.5 [21], and leverages all of its continual-training data. The architecture includes 632M-parameter vision encoder and Mixture-of-Experts (MoE) LLM with 2.5B active parameters. Game-TARS-MoE-large is larger version of MoE. To ensure fair comparison with other open-source models, we trained GameTARS-Dense using the Qwen2.5-VL-7B-Instruct checkpoint [3] and the VeOmni training strategies [38]. Due to training and data limitations, we only used game data for continual pretraining of the dense Game-TARS. 13 Dataset Details. Our training dataset represents an unprecedented scale, both in terms of breadth and depth. The game data includes over 500 popular games covering wide range of genres. For detailed statistics on the types, durations, and diversity of the game data, please refer to Figure 4. The overall token distribution across different data sources is as follows: 1. Pure Action Game Trajectories: 61 billion (61B) tokens across 50 open-world games. 2. Offline-Synthesized ReAct Trajectories: 45 billion (45B) tokens across 50 games. These data and the pure-action section use initial game data from different sources. 3. Online Human Think-Aloud ReAct Trajectories: 127 billion (127B) tokens across 500 games. We selected games with focus on reasoning in this part of the data and enhanced the diversity of the games. 4. Cross-Domain Agent Data: over 20 billion (20B) tokens including deep research, codex, and GUI trajectories [45, 76, 79]. 5. General Multimodal Data: 200 billion (200B) tokens, including question-answering, image captioning, visual question answering (VQA), and video captioning [21, 56]."
        },
        {
            "title": "4.2 Main Results",
            "content": "4.2.1 Experimental Setups To comprehensively evaluate the generalization capabilities of our agent model, we focused on testing its performance in games and environments that were not encountered during the training phase. We carefully selected set of representative benchmarks to assess the agents adaptability and performance across range of tasks and environments. The selected environments include: 1. Minecraft Although the model was exposed to large amount of Minecraft data during training, the open-ended nature and task diversity of this environment make it an ideal platform for testing the models ability to perform new tasks in familiar setting. We utilized variety of tasks from Minedojo [19] and MCU [35] to ensure the model was tested on broad spectrum of challenges, including task completion efficiency, flexibility, and reasoning capabilities in this open-world environment. 2. Web Games To ensure the novelty of the evaluation environment, we selected multiple games from the online platform Poki for testing. These games span across genres such as racing, puzzle, and other action types, providing an effective means of testing the models zero-shot generalization ability in entirely new visual and interaction paradigms. This evaluation serves to measure the models reasoning and decision-making performance in previously unseen games and environments. 3. Vizdoom Vizdoom is semi-realistic 3D environment based on the classic game Doom, commonly used to evaluate an agents decision-making ability in fast-paced FPS scenarios that require rapid responses. By testing in this dynamic environment, we can assess the agents ability to react quickly to rapidly changing situations and evaluate its decision-making speed and accuracy under competitive, high-stakes conditions [78]. 4. MiniWorld MiniWorld is minimalist 3D environment simulator designed to test an agents navigation, object interaction, and basic physical reasoning skills. In this environment, the agent is required to perform basic tasks such as finding target objects, avoiding obstacles, and handling physical simulations. It serves as an effective testbed for evaluating the models ability to manage simple physical and interactive tasks [11]. These benchmarks encompass variety of game types, task complexities, and interaction modalities, ensuring comprehensive evaluation of the agents generalization ability in new, unseen environments. By testing across these diverse setups, we aim to verify the models performance in unfamiliar settings, providing insights into its potential for general-purpose intelligence. 14 Table 3 Evaluation results in open-world Minecraft. The dense model of GameTARS is trained exclusively on game datasets, without incorporating datasets from other agents trajectories or general visual question-answering. Agents Embodied Tasks GUI Tasks Combat Tasks Steps ASR Steps ASR Steps ASR Policy-based Agents in Minecraft VPT [4] STEVE-1 [34] Dreamer V3 [22] .00 .10 .00 .00 .25 .00 377 384 6.011.4 8.017.0 3.010.2 VLM-based Agents in Minecraft JARVIS-VLA [31] OmniJARVIS [72] OpenHA [74] UI-TARS-1.5 [55] .25 .00 .10 .30 .80 .25 .45 .60 Our Generalist Game Agent GameTARS-Dense GameTARS-MoE-mini .35 .70 .85 .85 305 363 287 290 373 188 30.035.4 24.431.1 30.113.9 42.120. 50.420.7 72.021.5 .00 .00 - .35 .00 .25 .30 .40 .65 .20 .30 - .45 .00 .30 . .55 .55 398 391 - 339 393 314 320 406 288 0.83.3 3.28.4 - 25.123.9 3.07.5 32.59.2 36.717. 39.127.5 55.417.6 .00 .00 .00 .00 .00 .00 .00 .10 .40 .15 .20 .05 .20 .20 .30 . .70 .75 396 395 370 352 371 316 346 372 271 3.67.7 3.912.0 3.111.4 18.522.7 8.517.9 31.913.7 31.016. 38.124.6 66.125.2 4.2.2 Evaluation on Open-World Minecraft We first conducted systematic evaluation of the agents instruction-following capability on unseen tasks within the Minecraft environment. For this purpose, we adopted the MCU benchmark [35], which contains more than 800 diverse and easy-to-verify tasks. By evaluating on this large and heterogeneous task set, we can quantitatively assess the agents ability to understand novel instructions and generalize them into concrete action sequences. To ensure fairness, we manually curated 800+ distinct initial environments such that each task could be completed within 600 steps. We further controlled the random seed to guarantee that the agent had not been exposed to these environments during training. Each task was paired with unique natural language instruction that guided the agents behavior. Performance was measured using Average Success Rate and Average Completion Steps for evaluating instruction-following ability. As baselines, we compared against agents trained exclusively on Minecraft, including both policy-based and VLM-based approaches. To provide more fine-grained analysis, we grouped the tasks into three categories: (1) Embodied tasks, which evaluate exploration and object interaction in 3D space; (2) GUI tasks, which assess crafting and manipulation in 2D interfaces; and (3) Combat tasks, which focus on survival and combat capabilities. Each task was evaluated at least three times to reduce randomness in results. As shown in Table 3, our generalist agent Game-TARS substantially outperforms all expert baselines trained solely on Minecraft. In particular, the Game-TARS-MoE-mini variant achieved the highest success rates across all task categories, with significant improvements in both instruction-following accuracy and sample efficiency. These results confirm that large-scale continual pretraining with unified action space enables strong generalization even in complex, open-world environments. closer inspection of the results highlights several key findings. First, Game-TARS exhibits particularly strong performance in GUI tasks, where the success rate reached 55.0%, surpassing all prior baselines by large margin. This demonstrates the effectiveness of the unified action space in handling tasks that require reasoning and execution across both 2D graphical interfaces and 3D embodied environments. Second, in embodied tasks, Game-TARS achieved remarkable 72.0% success rate while completing tasks with substantially fewer steps, underscoring its ability to generalize complex exploration and interaction behaviors. Finally, in combat tasks, the model achieved 66.0% success, more than doubling the performance of the strongest VLM-based baseline. Taken together, these results validate our central hypothesis: large-scale continual pretraining with unified, device-level action space not only boosts overall task success but also enhances the agents adaptability to diverse instruction modalities. Game-TARS therefore establishes new state-of-the-art in open-world Minecraft benchmarks and provides compelling evidence of the viability of generalist game agents."
        },
        {
            "title": "4.2.3 Evaluation on Unseen Benchmarks and Simulators",
            "content": "Figure 7 Evaluation results in Web 3D games on Poki. Evaluation results in Web 3D games on Poki. Metrics include checkpoint time (Race), levels completed (Jump-Only), short-horizon score (Temple Run), and survival score (Airplane Flying). Figure 8 Evaluation results in FPS Vizdoom benchmark across four maps [78]. The reported metric is the average episode reward. Game-TARS-mini significantly outperforms all baselines. Figure 9 Evaluation results in Miniworld Simulator [11]. The goal of Game-TARS is not to achieve higher score on trained games, but to provide good foundation model for generalist game agent. Therefore, further evaluation is conducted on unseen games, including web games, FPS Doom, and 3D simulators. We first evaluated our model in browser-based 3D web games to test its zero-shot generalization ability in entirely novel visual and interaction paradigms. We selected four representative games from the Poki platform2: Race (racing), Jump-Only (action), Temple Run (endless running), and Airplane Flying (3D navigation). Evaluation metrics were task-specific: in Race, we measured the time to reach the first checkpoint (lower is better); in Jump-Only, the number of levels completed within 20 seconds; in Temple Run, the score achieved within 20 seconds; and in Airplane Flying, the cumulative score before failure. 2https://poki.com/ Figure 10 (Left) Inference-time Scaling Experiments. Game-TARS can improve the task success rates through exploration in inference-time steps. (Right) Training-time Scaling Experiments. The performance of Game-TARS improves as the training data increases, including in-domain, out-of-domain, and other agentic trajectory tokens. All games were wrapped into browser sandbox environment to ensure controlled and reproducible evaluation3. We used GPT-5 and Human players as baselines. For the human baseline, we recruited at least four individuals with no prior experience in the selected games and reported their best scores. For GPT-5 and our agent, we conducted at least five runs per game and reported the best-of-5 (Bo5) results. Both models were provided with identical system prompts, including basic game rules and action space definitions. As illustrated in Figure 7, Game-TARS-mini demonstrated competitive generalization compared to human players, achieving near-human or superior performance across most games. Notably, the model outperformed fresh human players in Jump-Only and Airplane Flying, highlighting its ability to adapt to novel tasks and mechanics without task-specific training. We further evaluated our agent in Vizdoom [78], classic FPS environment commonly used for benchmarking decision-making under fast-paced and adversarial conditions. We selected four standard maps: Battle-1, Battle-2, Defend the Line, and Defend the Center. The evaluation metric was the cumulative reward achieved per episode. Baselines included leading general-purpose models such as GPT-5 [46], Claude-4-Sonnet [2], and Gemini-2.5-Pro [12]. Results in Figure 8 show that Game-TARS-mini consistently outperformed all baselines across almost all maps, achieving substantial gains in both survival and combat effectiveness. Beyond raw scores, qualitative analysis revealed that the agent exhibited advanced FPS behaviors, including effective corner-peeking, evasive maneuvers to dodge projectiles, and adaptive targeting strategies. These results underscore the strong transferability of behaviors learned from diverse game data to previously unseen FPS environments. To further assess the agents ability to generalize to tasks requiring navigation, memory, and exploration, we evaluated it in the MiniWorld 3D simulator. This environment presents simplified but diverse tasks involving spatial reasoning, obstacle avoidance, and object interaction. Across nearly all tested maps, Game-TARS demonstrated robust performance, successfully navigating new environments and completing tasks despite having no prior exposure to MiniWorld during training. Together, these results confirm the broad generalization capability of Game-TARS across unseen environments, ranging from browser-based games and FPS simulators to navigation-focused 3D environments. The findings provide strong evidence that large-scale continual pretraining with unified action space enables the emergence of transferable skills applicable far beyond the training distribution. 3Due to the inference latency of the model, we packaged these web games into sandbox and used time-pause to ensure causal interaction. 17 Table 4 Ablation experiments on different thinking patterns in GameTARS across different evaluation benchmarks. Method No-thinking Greedy Thinking Efficient Thinking Minecraft Doom Race 13.2 55% 7.2 45% 11.5 63% 12.4s 11.3s 9.3s 2048 Avg. Tokens 445 986 924 22"
        },
        {
            "title": "4.3 Analysis and Discussions",
            "content": "In this section, we present set of analyses to provide deeper insights into the design and performance of Game-TARS. Our discussion covers three main aspects: sparse thinking, loss decay across training paradigms, and scaling experiments. These analyses aim to ensure fair and systematic evaluation of the models capabilities and limitations."
        },
        {
            "title": "4.3.1 Scaling Experiments on Training Datasets and Inference Steps",
            "content": "We first conducted scaling experiments on the continual pre-training datasets to examine how performance improves with increasing amounts and diversity of training data. The results are summarized in Figure 10, and several key insights emerge: 1. Specified action space performs well with small datasets but is hard to scale. When using visual groundingbased actions (e.g., GUI action spaces), agents achieve relatively strong performance with limited training data. However, such action spaces require extensive preprocessing of raw trajectories and do not naturally transfer across heterogeneous games. As result, their scalability and ceiling performance are inherently constrained (blue curve). 2. Unified action space requires more data initially but scales better. By contrast, the proposed unified device-level action space is highly scalable but not natively supported by pre-trained VLMs. As result, its performance lags behind GUI-based approaches when trained on small datasets. However, after pre-training on large-scale game data, its upper-bound performance approaches that of GUI-based action spaces (orange curve). 3. Cross-game scaling brings substantial gains. The unified action space enables consistent representation across diverse games, allowing large-scale aggregation of heterogeneous data. Incorporating 200B additional tokens from other games led to significant improvement in Minecraft performance, boosting the success rate from 42% to 65%. This observation echoes the Bitter Lesson in AI: general and simple representations, when combined with scaling in data and compute, yield stable and long-term improvements [60]. 4. Multimodal and cross-domain agentics data further enhance generalization. Extending pre-training to include multimodal data (e.g., captioning, VQA, video understanding) and agent trajectories from other domains led to an additional improvement, pushing performance to 72%. This finding demonstrates that generalist agent models can benefit from heterogeneous sources of actiontrajectory data, acquiring broadly transferable decision-making capabilities. Together, these results confirm the scalability advantages of the unified action space: while GUI-based representations may provide short-term advantage in low-data regimes, only the unified paradigm enables sustained performance gains through cross-game and cross-domain scaling. 4.3.2 Ablation on sparse thinking To better understand the role of reasoning in decision-making, we conducted ablation experiments on different thinking patterns, including No-thinking, Greedy Thinking, and sparse thinking. The results are reported in Table 4. The necessity of explicit reasoning is strongly task-dependent. In reasoning-intensive games such as 2048, where planning and long-horizon inference are critical, incorporating thinking at every step significantly 18 Table 5 Ablation experiments on the validation of decaying loss. The ASR and N-Acc represents average success rates on online evaluations and non-repetitive accuracy on offline datasets, respectively. Model Online Embodied Tasks Online Combat Tasks Online GUI Tasks Offline Evaluation ASR Diversity Steps ASR Diversity Steps ASR Diversity Steps Accuracy N-Acc w/o Decaying Loss w/ Decaying Loss 44.90% 60% 46.15% 90% 168 172 14.54% 50% 22.22% 60% 305 247 37.75% 80% 42.26% 100% 244 164 59% 47% 12% 39% improves performance (986 average tokens consumed per step). By contrast, in fast-paced environments such as Doom, excessive reasoning (Greedy Thinking) introduces latency and even degrades performance. One primary source of degradation is hallucinated reasoning loops, where the agent incorrectly assumes that certain goals have already been achieved and prematurely terminates actions. The No-thinking baseline performs relatively well in reaction-heavy games like Doom but fails to generalize to tasks that require compositional reasoning (e.g., crafting tasks in Minecraft or the strategic planning required in 2048). On the other hand, Greedy Thinkingforcing the agent to reason at every stepincurs substantial computational overhead, increases inference cost, and amplifies the risk of cascading reasoning errors. Our proposed sparse thinking strategy achieves the best trade-off between performance and inference efficiency. The agent learns to selectively engage in reasoning at key decision points (e.g., planning in Minecraft crafting tasks or assessing critical survival situations in Doom), while re-using or following previously established reasoning in routine steps. This reduces redundant computation, lowers average token consumption (37 tokens/step vs. 68 tokens/step in Greedy Thinking), and achieves state-of-the-art performance across most benchmarks (63% success rate in Minecraft, 924 points in 2048). Interestingly, qualitative trajectory analysis shows that the agent develops an implicit meta-cognition mechanism: it chooses when to reason and when to act reflexively. For instance, in Minecraft crafting sequences, the agent reasons carefully at the start of task (to identify needed resources and steps) but executes subsequent mining or crafting actions without redundant re-planning. Similarly, in 2048, it reasons at every move due to the inherent combinatorial structure of the game, while in VizDoom it minimizes reasoning to maintain responsiveness. These findings highlight that reasoning is not universally beneficial; rather, its effectiveness depends on the interaction between task structure and agent inference cost. Sparse thinking emerges as general strategy that balances adaptability, efficiency, and robustness, enabling the agent to approximate human-like decision-making where deliberation is applied selectively at critical junctures. 4.3.3 Validation on Continual Pre-Training Loss We further investigated the impact of applying decaying continual pre-training loss. The analysis was conducted from both training dynamics and downstream evaluation perspectives  (Table 5)  . From purely optimization perspective, introducing decaying loss initially leads to slightly higher absolute training loss compared to the non-decaying baseline. Nevertheless, the overall decreasing trend remains stable, and both approaches converge to comparable final loss values. This observation indicates that the decaying strategy does not compromise convergence stability, while potentially reshaping the optimization trajectory. We then conducted ablation studies on downstream online environments, measuring task-level Average Success Rate (ASR), trajectory diversity, and average completion steps. The results reveal several advantages of the decaying loss: 1) Improved task success rates across embodied, combat, and GUI tasks, with particularly large gains in combat scenarios (from 14.5% to 22.2%). 2) Substantially shorter trajectories, indicating that agents are less likely to become stuck or generate redundant action loops. 3) Enhanced behavioral diversity, especially in GUI and embodied tasks, where diversity increased from 6080% to nearly 100%. These improvements suggest that decaying loss helps mitigate the tendency of agents to fall into repetitive behaviors, thereby producing more robust and efficient action sequences. 19 To further probe this effect, we performed offline action prediction experiments across 1,000 trajectories containing more than 100k steps. We report two metrics: 1) Average action prediction accuracy across all steps. 2) Non-repetitive accuracy, i.e., accuracy on steps where the predicted action differs from the previous one. Interestingly, while the non-decaying baseline achieves higher overall accuracy (59% vs. 47%), the decaying loss yields dramatic improvement in non-repetitive accuracy (39% vs. 12%, +28% absolute gain). This indicates that models trained without decaying loss tend to exploit dataset bias by repeatedly copying the previous action, hacking the accuracy metric without genuinely improving decision quality. In contrast, decaying loss discourages such degenerate strategies, forcing the agent to learn meaningful state-dependent action prediction. The ablation results demonstrate that although decaying loss sacrifices some global prediction accuracy, it provides substantial benefits in downstream performance by reducing repetition, increasing action diversity, and improving robustness across online tasks. This aligns with the broader goal of continual pre-training: to shape learning signals in way that promotes adaptive, generalizable behaviors rather than exploiting dataset artifacts."
        },
        {
            "title": "5 Related Works",
            "content": "Our work builds upon and extends several key research areas in artificial intelligence: specialized game-playing agents, the emerging paradigm of generalist agents, and agents designed for broader computer interaction tasks. We situate Game-TARS by drawing critical distinction based on the agents action space, arguing that unified, human-friendly paradigm is fundamental to achieving true scalability and generality."
        },
        {
            "title": "5.1 Game Agents",
            "content": "The history of AI is deeply intertwined with games as benchmarks for intelligence. Foundational work in closed-world games demonstrated superhuman performance but relied on highly specialized action spaces. Deep Q-Networks (DQN) mastered numerous Atari games from raw pixels using low-dimensional, discrete action space [40]. AlphaGo and its successors achieved world-champion level Go play via deep neural networks and Monte Carlo Tree Search, with actions tied to the games formal rules [58]. Similarly, AlphaStar excelled in StarCraft II using custom, game-specific API, limiting transferability [65]. While highly successful, these agents tight coupling of action representation to specific environments remains bottleneck for generalization. The open-world environment of Minecraft has become key testbed for more generalizable agents. VPT pioneered learning behavioral priors from large-scale video data using the native human interface (keyboard- /mouse), aligning with our approach, though training was confined to Minecraft [4]. Other methods like DreamerV3 learn world models via RL, solving hard-exploration tasks without human data but lacking demonstrated cross-domain generalization [22]. The rise of pre-trained vision-language models (VLMs) and large language models (LLMs) has enabled new possibilities for multi-task game agents in open worlds. Voyager [66] leverages LLMs for long-horizon planning but executes actions via high-level APIs (e.g., JavaScript code), creating semantic gap by learning to sequence functions rather than acquiring sensorimotor skills [33, 70, 71, 83, 86]. Cradle demonstrated strong instruction-following across multiple games using unified agentic workflow [61]. Other approaches achieve strong performance via VLM-based fine-tuning (SFT) or RL within specific games [18, 29, 55, 72, 84]. Furthermore, general AI models like Gemini-2.5 [12], GPT-5 [46], and Claude 4 [2] are increasingly evaluated on games (e.g., Chess, Pokémon), highlighting gaming as crucial benchmark for foundational model capabilities [57, 85]."
        },
        {
            "title": "5.2 Generalist Agents",
            "content": "The pursuit of single agent capable of mastering multiple domains has led to the development of generalist architectures. Early DQN research showed that unified RL algorithm could achieve superhuman performance across many Atari games, albeit requiring separate training runs for each [40]. Gato was seminal work 20 demonstrating multi-modal, multi-task, multi-embodiment agent trained with single set of weights by serializing all data into token sequence for Transformer [53]. However, its heterogeneous, discretized action space still required domain-specific tokenization, falling short of truly universal interface. More recently, the SIMA project shares core philosophy with Game-TARS: training an agent to follow natural language instructions across diverse 3D environments using generic, human-like interface (pixels in, keyboard/mouse out) [51]. This marks critical shift toward unified action paradigm. However, initial SIMA agents were evaluated primarily on short-horizon tasks and limited in-domain games, underscoring the learning challenge. VPT built foundational model for reinforcement learning in an open world by pre-training on IDM and video data from the internet. However, this foundational model can only be used for Minecraft and is difficult to generalize to other game scenarios [4]. Recently, with the rise of pre-trained models such as LLM [25, 39, 64] and VLM [2, 12, 46], developing pre-trained foundation model for agents has become mainstream, such as embodied foundation models [9, 17, 24, 42, 62] and computer-use foundation models [48, 63, 67, 69]. Game-TARS builds on this shared vision, demonstrating enhanced performance and scalability through large-scale continual pre-training on vision language models."
        },
        {
            "title": "5.3 Broader AI Agents",
            "content": "From broader perspective, there is significant potential for AI agents based on VLM or LLM to be implemented in various fields, including computer use and embodied agents [5, 6, 17, 28, 43]. Research into GUI and Web Agents (e.g., using Mind2Web [15] or UI-TARS [67]) often focuses on grounding high-level plans to specific UI elements [23, 30, 36, 37, 68, 77, 80]. By operating directly on low-level keyboard/mouse outputs, Game-TARS learns an end-to-end policy that implicitly addresses grounding. Similarly, agents for software engineering like Claude [2] and SWE-agent [27, 45, 81] tackle complex, logic-driven tasks. Protocols such as MCP [1] have introduced standardized mechanisms for flexible tool integration, enabling agents to seamlessly interact with search engines, file parsers, or external APIs [20, 32, 59, 73]. Incorporating data from such domains in post-training enhances Game-TARS high-level reasoning, complementing its foundational sensorimotor skills."
        },
        {
            "title": "6 Conclusions",
            "content": "We present Game-TARS, generalist game agent utilizing unified, scalable action space based on native keyboard-mouse inputs. This human-native paradigm bypasses the limitations of API/GUI approaches, enabling large-scale continual pre-training (>500B tokens) across diverse domains (game trajectories, ai agents trajectories, multimodal data). Game-TARS demonstrates strong instruction-following capabilities and employs an efficient Sparse Thinking strategy, balancing reasoning depth with inference cost. Experiments show Game-TARS achieves 2x SOTA success rate in Minecraft, nears human-level generalization in unseen web games, and outperforms leading models (GPT-5, Gemini-2.5-Pro, Claude-4-Sonnet) on FPS benchmarks. Scaling results confirm that simple, scalable action representations paired with massive pre-training offer promising path toward generalist computer-use agents."
        },
        {
            "title": "Contributions",
            "content": "Authors Zihao Wang1,2, Xujing Li1, Yining Ye1, Junjie Fang1, Haoming Wang1, Longxiang Liu1, Shihao Liang1, Junting Lu1, Zhiyong Wu1, Jiazhan Feng1, Wanjun Zhong1, Zili Li1, Yu Wang1, Yu Miao1, Bo Zhou1, Yuanfan Li1, Hao Wang1, Zhongkai Zhao1, Faming Wu1, Zhengxuan Jiang3, Weihao Tan1, Heyuan Yao1, Shi Yan1, Xiangyang Li1, Yitao Liang2, Yujia Qin1, Guang Shi1 Afflications 1ByteDance Seed, 2Peking University, 3M-A-P Acknowledgment We greatly thank Muyao Li for discussions. And we appreciate Cheng Yuan and Hao Chens efforts on the Game-TARS website."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing the model context protocol, 2024. URL https://www.anthropic.com/news/ model-context-protocol. [2] anthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. [5] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. arXiv preprint arXiv:2403.01823, 2024. [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [7] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [8] ByteDance. Seed-thinking-1.6, 2025. URL https://seed.bytedance.com/zh/seed1_6. [9] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. Gr-3 technical report. arXiv preprint arXiv:2507.15493, 2025. [10] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. Exploring large language model based intelligent agents: Definitions, methods, and prospects. arXiv preprint arXiv:2401.03428, 2024. [11] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023. [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Advances in neural information processing systems, 32, 2019. [14] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. [15] Xiang Deng, Kelvin Guu, Panupong Pasupat, Afra Akyürek, Sheng Zhuang, Wenlong Chen, Tatsunori Hashimoto, Kelvin Guu, and Percy Liang. Mind2web: Towards generalist agent for the web. In NeurIPS Datasets and Benchmarks, 2023. URL https://arxiv.org/abs/2306.06070. [16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [18] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. 22 [19] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362, 2022. [20] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. [21] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [22] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [23] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [24] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. URL https://arxiv.org/abs/2504.16054. [25] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [26] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2(3):6, 2022. [27] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [28] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Proceedings of The 8th Conference on Robot Learning, volume 270 of Proceedings of Machine Learning Research, pages 26792713. PMLR, 0609 Nov 2025. URL https://proceedings.mlr.press/v270/kim25c.html. [29] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language models for minecraft. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1642616435, June 2024. [30] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. [31] Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, and Yitao Liang. Jarvis-vla: Post-training large-scale vision language models to play visual games with keyboards and mouse. arXiv preprint arXiv:2503.16365, 2025. [32] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl, 2025. URL https://arxiv.org/abs/ 2503.23383. [33] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. Advances in neural information processing systems, 37: 4988149913, 2024. [34] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: generative model for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 36:6990069929, 2023. 23 [35] Haowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: task-centric framework for open-ended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023. [36] Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy optimization for gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025. [37] Run Luo, Lu Wang, Wanwei He, Longze Chen, Jiaming Li, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [38] Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, et al. Veomni: Scaling any modality model training with model-centric distributed recipe zoo. arXiv preprint arXiv:2508.02317, 2025. [39] AI Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. [40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. [41] Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi for operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462, 2023. [42] NVIDIA, :, Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv: 2503.14734, 2025. [43] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. [44] OpenAI. Gpt-4v(ision) system card. https://openai.com/index/gpt-4v-system-card/, 2023. Accessed: 202504-23. [45] openai. Introducing swe-bench verified, 2024. URL https://openai.com/index/ introducing-swe-bench-verified/. [46] OpenAI. Introducing gpt 5, 2025. URL https://openai.com/index/introducing-gpt-5/. [47] OpenAI. Introducing deep research - openai. https://openai.com/index/introducing-deep-research/, 2025. [48] OpenAI. Computer-using agent (cua). https://openai.com/index/computer-using-agent/, 2025. Research preview / blog. [49] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [50] Qwen. Qwen-3-coder. https://qwenlm.github.io/blog/qwen3-coder, 2025. [51] Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. Scaling instructable agents across many simulated worlds. arXiv preprint arXiv:2404.10179, 2024. [52] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [53] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. 24 [54] Bytedance Seed. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. Technical report, Technical report, ByteDance, 2025. URL https://github. com/ByteDance-Seed . . . , 2025. [55] ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. [56] ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [57] Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, et al. Korgym: dynamic game platform for llm reasoning evaluation. arXiv preprint arXiv:2505.14552, 2025. [58] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354359, 2017. [59] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [60] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. [61] Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. [62] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [63] Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. [64] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [65] Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350354, 2019. [66] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [67] Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025. [68] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Acting less is reasoning more! teaching model to act efficiently, 2025. URL https://arxiv.org/abs/2504.14870. [69] Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025. [70] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. Advances in Neural Information Processing Systems, 36:3415334189, 2023. [71] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [72] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. Advances in Neural Information Processing Systems, 37:7327873308, 2024. 25 [73] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint arXiv:2403.05313, 2024. [74] Zihao Wang, Muyao Li, Kaichen He, Xiangyu Wang, Zhancun Mu, Anji Liu, and Yitao Liang. Openha: series of open-source hierarchical agentic models in minecraft. arXiv preprint arXiv:2509.13347, 2025. [75] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Neural Information Processing Systems, 2022. [76] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [77] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [78] Marek Wydmuch, Michał Kempka, and Wojciech Jaśkowski. Vizdoom competitions: Playing doom from pixels. IEEE Transactions on Games, 2018. IEEE Transactions on Games outstanding paper award 2022. [79] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [80] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [81] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. [82] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [83] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint arXiv:2303.16563, 2023. [84] Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2024. [85] Alex Zhang, Thomas Griffiths, Karthik Narasimhan, and Ofir Press. Videogamebench: Can vision-language models complete popular video games? arXiv preprint arXiv:2505.18134, 2025. [86] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et al. Proagent: Building proactive cooperative ai with large language models. arXiv preprint arXiv:2308.11339, 2023. [87] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023."
        },
        {
            "title": "A Rollout Trajectories",
            "content": "A.1 Rollouts in Minecraft Figure 11 Game-TARS has mastered advanced combat skills. In battles with creepers, he avoids damage from creeper explosions by constantly retreating and also uses jumping strikes to increase his damage. Figure 12 Game-TARS can use tools and interact with objects in the environment. Figure 13 Game-TARS have good long-term memory, allowing them to remember the movement trajectory of foxes during the process of killing them, and to track the foxes over long period through navigation and exploration skills. Figure 14 Game-TARS has great action consistency and can break obsidian blocks that require 10-second long press to obtain. Figure 15 Game-TARS can use unified actions to freely switch and interact between the embodied 3D interface and the 2D GUI interface. A.2 Rollouts in Unseen Games 27 Table 6 Rollout trajectory of Game-TARS on Poki Plane game. We skip some timestamps. Observation Model Response Thought: Just entered new scene, and there are obstacles ahead. need to quickly adjust the direction to avoid them; cant let the plane crash, or else the game will be over. Action: no_op Action: keyPress(d) Thought: The position of the obstacle ahead has changed, need to adjust direction to the right, maintain distance from the obstacle, and continue flying forward. Action: keyPress(d) Thought: There are new obstacles appearing again, need to adjust the direction to the left to avoid this, cannot let the plane hit any obstacles. Action: keyPress(a) Thought: The obstacles in front are arranged quite densely, so need to adjust the direction to the right to avoid them; cant let the plane hit them, otherwise the game will be over. Action: keyPress (d) ... 6 ... 14 ... ... 35 ... 28 1 ... 12 ... 24 ... 37 ... 53 ... 67 ... 82 ... 109 ... Table 7 Rollout trajectory of Game-TARS on Temple Run game. We skip some timestamps. Observation Model Response Thought: diamond-shaped obstacle appeared ahead, need to jump up quickly to avoid it, otherwise will crash and lose health. Action: keyPress(arrowup) Action: no_op Thought: There was stone wall ahead, you had to crouch down to pass through, otherwise you would be blocked, so quickly press the crouch key. Action: keyPress (arrowdown) Action: keyPress(arrowleft) Thought: There are diamonds and stone pillars ahead; be careful to avoid the stone pillars while jumping over the diamonds, and stay focused. Action: no_op Thought: There is gap ahead, you need to avoid it, otherwise you will fall. Press the right button to ensure safe passage. Action: keyPress(arrowright) Action: keyPress(arrowleft) Action: keyPress(arrowup) Action: keyPress(arrowup) 29 0 ... 6 7 ... ... 56 ... 82 83 ... 100 101 ... 130 ... 150 ... 180 ... 220 ... Table 8 Rollout trajectory of Game-TARS on Poki Race game. We skip some timestamps. Observation Model Response Thought: The game interface has now loaded, and need to start driving the vehicle. First, have to hold down the acceleration key to get the car moving; only then can start the timer. must ensure that maintain acceleration from the very beginning and not let my speed drop so that can reach the finish line as quickly as possible. Action: keyPress(arrowup) Thought: see the \"GO!\" signal now, its time to accelerate with all my might and pick up speed. The road ahead looks straight, so Ill stay in the middle lane to avoid hitting the trees or other cars on either side. need to pay attention to the vehicles ahead, but for now, Ill focus on accelerating and aim to get my speed up right from the start. Action: keyPress(arrowup) Action: keyPress(arrowup) Thought: The speed has already reached over 100, and the road ahead is still very straight, so continue to accelerate. Now need to pay attention to the vehicles in front, but currently there are no other cars, so can keep accelerating. At the same time, need to ensure that Im in the center of the lane; this way its both safe and helps maintain speed, avoiding veering off the road which could lead to slowdown. Action: keyPress(arrowup) Thought: The car on the right has passed, now return to the middle lane and continue to accelerate. The speed is already 200; maintain this speed. The road ahead is still straight, as long as dont deviate from the road, can keep going at high speed. need to pay attention and look further ahead to anticipate any potential situations. Action: keyPress(arrowup) Action: keyPress(arrowup) Thought: The car in front has already passed, now return to the middle lane and continue to accelerate. The speed is still 200, the road remains straight, and there are no other vehicles around, so can maintain this speed. need to pay attention to the mini-map to ensure Im in the center of the lane; this way its both safe and allows me to keep my speed, avoiding deceleration due to drifting off the road. Action: keyPress(arrowup) Action: keyPress(arrowup) Thought: The road ahead is starting to turn, so need to adjust my direction in advance and stay in the center of the road. When turning, cant reduce speed too much, so need to steer early, turn slowly while continuing to accelerate, and try to maintain my speed. This way is both safe and allows me to keep the fastest speed. Action: keyPress(arrowup, arrowright) Action: keyPress(arrowup) Action: keyPress(arrowup, arrowright) Action: keyPress(arrowup, arrowright) Thought: The runway is still very straight, and the speed is maintained at 200. There are still no other vehicles around, so can continue to accelerate. Pay attention to the mini-map to ensure Im in the center of the road, while also looking further ahead to anticipate any potential situations. This way, can maintain the fastest speed. Action: keyPress(arrowup) Action: keyPress(arrowup, arrowleft)"
        },
        {
            "title": "B Prompts",
            "content": "B.1 Prompts of Game-TARS on Minecraft System Prompt of Game-TARS on Unseen Poki Plane Game You are an AI agent performing tasks in Minecraft based on given instructions, action history, and visual observations (screenshots). Your goal is to take the next optimal action to complete the task. ## Output Format Thought: ... Action: ... ## Action Space * mouseMove(dx, dy) # Move the mouse position; dx and dy represent horizontal and vertical movement, respectively. * mouseClick(left or right) # left click or right click the mouse - left # Attack; In GUI, pick up the stack of items or place the stack of items in GUI cell; when used as double click (attack - no attack - attack sequence), collect all items of the same kind present in inventory as single stack. - right # Place the item currently held or use the block the player is looking at. In GUI, pick up the stack of items or place single item from stack held by mouse. * keyPress(keys) # press the keyboard buttons - # Move forward. - # Move backward. - # Strafe left. - # Strafe right. - # Open or close inventory and the 2x2 crafting grid. - space # Jump. - # Drop single item from the stack of items the player is currently holding. If the player presses ctrl-Q then it drops the entire stack. In the GUI, the same thing happens except to the item the mouse is hovering over. - 1-9 # Switch active item to the one in given hotbar cell. - left.ctrl # Move fast in the current direction of motion. - left.shift # Move carefully in current direction of motion. In the GUI it acts as modifier key: when used with attack it moves item from/to the inventory to/from the hotbar, and when used with craft it crafts the maximum number of items possible instead of just 1. * no_op # wait and do not interact with the world If multiple actions are activated, use and connect. Use Chinese in Thought part. Provide brief plan in the Thought section, specifying your next move and objective. If this step does not require thought, you can only output Action. Your history thoughts will accumulate continuously in history conversations. ## User Instruction {instruction} 31 B.2 Prompts of Game-TARS on Unseen Games System Prompt of Game-TARS on Unseen Poki Plane Game You are an AI agent playing \"Plane Avoid Barrier\" game, based on given instructions, action history, and visual observations (screenshots). Your goal is to take the next optimal action to achieve higher scores. ## Output Format Thought: ... Action: ... (if thought is necessary in this frame, you need to judge) or Action: ... (low level action is enough) ## Action Space * keyPress(keys) # press the keyboard buttons - # Strafe left. - # Strafe right. * no_op # wait and do not interact with the world. ## User Instruction Avoid the obstacle ahead. System Prompt of Game-TARS on Unseen Poki Race Game You are an AI agent playing \"Drive Freedom\" game, based on given instructions, action history, and visual observations (screenshots). Your goal is to take the next optimal action to achieve higher scores. ## Output Format Thought: ... Action: ... (if thought is necessary in this frame, you need to judge) or Action: ... (low level action is enough) ## Action Space * keyPress(keys) # press the keyboard buttons - arrowup # Accelerate and start the vehicle. - arrowdown # Operate vehicle brakes - arrowleft # Turn left - arrowright # Turn right * no_op # wait and do not interact with the world ## Game Tutorial Press the arrowup key to start the vehicle. Hold down arrowup to maintain high speed. At the appropriate time, press arrowleft and arrowright to control steering. **Do not hit the vehicles on the road**, as this will affect your speed. **Avoid hitting the trees and billboards on the roadside**. **Do not leave the road**, as this will slow down your speed. ## User Instruction Drive at the fastest speed to reach the destination. 32 System Prompt of Game-TARS on Unseen Poki TempleRun Game You are an AI agent playing \"Temple Run\" game, based on given instructions, action history, and visual observations (screenshots). Your goal is to take the next optimal action to achieve higher scores. ## Output Format Thought: ... Action: ... (if thought is necessary in this frame, you need to judge) or Action: ... (low level action is enough) ## Action Space * keyPress(keys) # press the keyboard buttons - arrowup # Make the character jump upward (to avoid ground obstacles such as traps and tree roots) - arrowdown # Make the character slide downward (to avoid low-hanging obstacles such as short stone gates and hanging vines) - arrowleft # Make the character move to the left (switch to the left track to avoid obstacles on the current track) - arrowright # Make the character move to the right (switch to the right track to avoid obstacles on the current track) * no_op # wait and do not interact with the world ## Game Tutorials 1. Survival First: Continuously avoid various obstacles in the game (such as cliffs, traps, flames, moving stone gates, and wild animals) to prevent the character from dying and extend the running duration. 2. Resource Collection: Actively collect coins (used to unlock new characters and upgrade power-ups) and power-ups (magnets, shields, speed boosts, revives, etc.) in the scene to increase the amount of resources obtained in single run. 3. Score Improvement: Maximize the score of single game through \"continuous survival time,\" \"number of coins collected,\" and \"score-boosting effects triggered by power-ups\" to compete for higher ranking. 4. Scene Exploration: On the premise of ensuring survival, try to explore hidden resources on different tracks (such as hidden coin piles and rare power-up spawn points) and familiarize yourself with the obstacle distribution rules of different scenes (jungles, mines, ancient tombs, etc.). Hint: At the beginning of the game, red arrow will be used on the screen to tell you which key to press, and when prompt appears on the screen, press the key while holding the screen prompt. ## User Instruction Play the temple run game. 33 System Prompt of Game-TARS on Unseen Poki Plane Game You are an AI agent playing \"Jump Only\" game, based on given instructions, action history, and visual observations (screenshots). Your goal is to take the next optimal action to achieve higher scores. ## Output Format Thought: ... Action: ... (if thought is necessary in this frame, you need to judge) or Action: ... (low level action is enough) ## Action Space * keyPress(keys) # press the keyboard buttons - arrowleft # Jump to the upper left - arrowright # Jump to the upper right * no_op # wait and do not interact with the world. You will fall freely Each step will activate only one key. ## Game Tutorial Jump! This is your only way to escape. \"Jump Only\" is high-skill platform game where you can showcase your amazing reaction time and jumping skills. On your way to the finish line, you need to jump over dangerous obstacles, avoid spikes, and saw blades. There are 49 different levels for you to conquer. The theme changes every few levels, bringing you different challenges. Hint: You need to master the most suitable timing for jumping. Holding down the left or right key will make you jump upwards. You cannot keep pressing left or right continuously, and you need to use the no_op key to wait until you land in suitable position before jumping again. ## User Instruction Avoid obstacles and jump to the finish line."
        }
    ],
    "affiliations": []
}