{
    "paper_title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding",
    "authors": [
        "Lin Long",
        "Changdae Oh",
        "Seongheon Park",
        "Yixuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP) -- memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on input-output probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. Our analysis reveals a universal phenomenon: each model exhibits a Visual Integration Point (VIP), a critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding. Building on this observation, we introduce the Total Visual Integration (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 model-dataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers a principled toolkit for diagnosing and understanding language prior in LVLMs."
        },
        {
            "title": "Start",
            "content": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding UNDERSTANDING LANGUAGE PRIOR OF LVLMS BY CONTRASTING CHAIN-OF-EMBEDDING Lin Long* Changdae Oh* University of WisconsinMadison {llong,changdae,seongheon park,sharonli}@cs.wisc.edu *Equal contribution. Seongheon Park Yixuan Li Corresponding author. 5 2 0 S 7 2 ] . [ 1 0 5 0 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP)memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on inputoutput probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. Our analysis reveals universal phenomenon: each model exhibits Visual Integration Point (VIP), critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding. Building on this observation, we introduce the Total Visual Integration (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 modeldataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers principled toolkit for diagnosing and Code: (cid:135) understanding language prior in LVLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Modern large vision-language models (LVLMs) (OpenAI, 2025; Comanici et al., 2025; Bai et al., 2025; Zhu et al., 2025) have extended the boundaries of AI applications at an unprecedented rate. Their remarkable capability in solving highly complex vision-language tasks originated from the internalized rich unimodal knowledge during the pre-training (Radford et al., 2021; Oquab et al., 2024; Brown et al., 2020) and also from the strong multimodal alignment (Liu et al., 2023; Dai et al., 2023; Zhu et al., 2024). Despite their successes, central challenge remains: LVLMs are prone to over-relying on their language prior (LP)the statistical patterns memorized during large-scale language model pretrainingwhile under-utilizing the actual visual evidence (Fu et al., 2024; Lee et al., 2025; Luo et al., 2025). This imbalance often results in hallucinations, shortcut reasoning, and brittle generalization when tasks truly demand visual grounding. For example, when asked What color is the banana?, an LVLM may confidently answer yellow even if the banana in the image is green, demonstrating that the model defaults to its LP. Recent studies (Yin et al., 2024; Liu et al., 2024d; Lee et al., 2025) further show that such LP reliance persists across diverse tasks. Understanding and quantifying LP in LVLMs is thus critical, both for diagnosing their limitations and for guiding the design of more reliable multimodal systems. However, current approaches to analyzing LP primarily rely on inputoutput probing. For instance, Lee et al. (2025) and Luo et al. (2025) constructed datasets with counterfactual visual input to measure models performance under challenging visual grounding scenarios, while Deng et al. (2025) evaluate models on modalityconflicting queries to assess modality preference. While useful, such coarse input-output analyses have fundamental limitation to investigate LP of LVLMs in-depth, because: (1) they ignore the rich latent representations inside the model, which may inform how textual and visual signals are integrated, and (2) they cannot disentangle where in the model the LP begins to interfere with effective visual integration, leaving per-sample mechanistic interpretation (Bereska & Gavves, 2024) elusive. 1 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Figure 1: Framework Overview. For data from two distributions PVT (vision-dependent) and Pt (vision-independent), we extract chain-of-embedding for two queries w/ and w/o visual input, and use the expected representation distance to spot visual integration point l. Then, estimating total visual integration based on allows us to quantify LP of an LVLM per sample. Motivated by this, we propose new framework for understanding and quantifying language prior, which leverages the chain-of-embeddingthe sequence of hidden representations across LVLM layers. Making use of these latent representations is essential, because they provide direct insight into the inner mechanisms of LP, beyond surface-level outputs. Specifically, our framework contrasts embeddings from visiontext inputs (Z blind), at each layer l. Based on the contrastive chain-of-embedding, we reveal striking phenomenon: LVLMs exhibit Visual Integration Point (VIP), layer at which visual information begins to meaningfully influence the LVLMs decoding process. At and beyond VIP, the distance between blind increases substantially for vision-dependent tasks, signaling that the model has begun to actively integrate visual evidence to solve the task. In contrast, vision-independent tasks show smaller such shift. Thus, VIP captures critical point where visual input begins to exert meaningful influence on inference, revealing the extent to which the model relies on vision or falls back on language priors. vis) with those from vision-removed inputs (Z vis and Inspired by observations from VIP, we propose quantifying LP through Total Visual Integration (TVI), which measures the effective amount of visual integration that affects the answer decoding of vis and LVLM. Specifically, TVI aggregates distance between contrastive embeddings blind across all post-VIP layers to measure the cumulative strength of visual integration. Intuitively, TVI is inversely related to the magnitude of LP: models with strong reliance on language priors exhibit low TVI, while those that leverage vision more deeply exhibit high TVI. Through extensive experiments covering 9 contemporary LVLMs and 6 datasets (54 settings combined), we show the universality of the existence of VIP, and that TVI can be reliable indicator of LP. Moreover, we demonstrate that TVI strongly correlates with performance on benchmarks requiring visual reasoning, outperforming other proxies such as visual attention weights or output divergence. Then, we provide theoretical interpretation of our VIP measurement as well as analytic bounds of it for broader use in practice. We illustrate the overall framework in Figure 1, and summarize our contribution as follows: 1. We present novel framework that contrasts the chain-of-embedding of an LVLM for finegrained analysis of the visual integration and language prior of LVLMs. 2. Based on this framework, we show that there is specific layer, VIP, where an LVLMs behavior undergoes dramatic change, and observe that post-VIP layers representations play key role in quantifying the amount of language prior of an LVLM. 3. Across 9 representative LVLMs and 6 datasets, we consistently demonstrate the existence of VIP, show how we can use it to predict the strength of language prior of an LVLM on certain sample through TVI, and further present theoretical analyses on our framework."
        },
        {
            "title": "2 PROBLEM STATEMENT",
            "content": "Basic notations. Let = {(xv, xt)i}N i=1 denote dataset of image-text queries (xv, xt), sampled from population distribution P. Each tuple (xv, xt) consists of visual input xv, and natural 2 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding language query xt, expressed in prompt form. We distinguish Xv from xv to denote random variable and its observation (similarly for Xt). Then, we define the data structure as follows. Definition 2.1. We define PVT as the vision-dependent distribution, consisting of examples where resolving the textual query requires access to the associated visual input. In contrast, PT is the vision-independent distribution, containing examples where the textual query can be answered correctly without visual information (i.e., the text alone suffices). sample dataset is constructed with DVT and DT, each containing at least one element from populations PVT and PT, respectively: = {DVT DT : min(DVT, DT) 1}, (1) where denotes the cardinality of set. See examples of DVT and DT in Figure 1. Meanwhile, we have an LVLM, Fθ = fh fL fL1 f1 f0, parameterized with θ. Here, f0 denotes the composition of the visual encoder, modality connector, and text embedding layer; (f1, . . . , fL) corresponds to the stacked decoder layers of the LLM; and fh is an output head. The LVLM maps the multimodal input query (xv, xt) to Vdimension probability distribution over the vocabulary space V, from which the most likely answer ˆy is obtained via the argmax operator. Language prior (LP). An LVLM has vast amount of knowledge in its parameters obtained during unimodal pretraining and visual instruction tuning of entire model components. Since the pre-training of LLM backbone is far more extensive in quantity and diversity of data, and total computing budget, LVLMs are prone to over-reliance on memorized statistical textual patterns without integrating visual information during inference. Given an input and an LVLM Fθ, we define the models reliance on statistical textual patterns as the language prior, LP(x, Fθ). Note that LP is more like latent property that lacks gold-standard measurement. Therefore, previous work typically approximates how robust an LVLM is against LP through its performance on carefully constructed In contrast, we propose novel approach that (1) does not require any annotations or datasets. careful data curation, (2) tries to quantify LP in more direct manner, which enables flexible and fine-grained, sample-wise diagnosis for LP of LVLMs. Refer to Appendix for additional context. Our position. Although there have been recent attempts in analyzing LP in LVLMs, they primarily focus on evaluating model predictions over curated datasets (Lee et al., 2025; Luo et al., 2025; Vo et al., 2025), without offering well-defined or generalizable formulation. We argue that such coarse inputoutput analysis is insufficient: it cannot reveal how LP manifests within the model nor how it can be rigorously quantified. In particular, prior approaches overlook the rich latent information encoded inside the LLM component of an LVLMintermediate representations that may inform how visual and textual signals are integrated and how biases emerge. Making use of these latent representations is essential because they provide direct insight into the inner mechanisms of LP, beyond surface-level outputs. With this motivation, we pose the following research question: Can we derive formal framework to understand and quantify the language prior of LVLMs through the lens of their internal states?"
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 CHAIN-OF-EMBEDDING AND REPRESENTATION DISTANCE In contrast to previous approaches that focus on LVLM output (Rahmanzadehgervi et al., 2024; Vo et al., 2025; Lee et al., 2025; Luo et al., 2025), we leverage the chain-of-embedding for fine-grained analysis of LVLM, which is defined as sequence of hidden states across layers, i.e., (Z 1, , L), where = fl(Xv, Xt) Rdz 1 denotes dz-dimensional output token embedding at layer {1, ..., L}. Importantly, we contrast embeddings from two different input constructions as below. vis := fl(Xv, Xt) blind := fl(, Xt) (embedding from both visual and textual inputs) (embedding from textual input only) 1Although = fl(...f2(f1(Xv, Xt))) is more precise, we slightly abuse the notation for clarity. 3 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Now, given distance metric d, and P, we analyze the difference between these two embeddings per layer by defining an expected representation distance and its finite-sample estimator, Dl(P, Fθ) := E(Xv,Xt)P [d(Z vis, blind)], Dl(D, Fθ) := 1 (cid:88) (xv,xt)iD d(zl,i vis, zl,i blind). (2) We adopt the cosine distance by default, though other distance functions, including non-metric distances (Deza & Deza, 2009), can also be valid. An ablation study with alternative metrics is provided in Section 4. Intuitively, vis should encode distinctive visual semantics that cannot be inferred from text alone, whereas blind primarily reflects the models default linguistic expectations. However, the degree of this discrimination can depend on how visual information contributes differently to different data, and across different layers of the model. We elaborate on this in the next section."
        },
        {
            "title": "3.2 VISUAL INTEGRATION POINT HYPOTHESIS",
            "content": "Deep neural networks are known to develop hierarchical representations across layers (Chen et al., 2023; Fan et al., 2024; Jin et al., 2025), where each layer has different types and resolutions of information (Joseph & Nanda, 2024; Skean et al., 2025; Artzy & Schwartz, 2024; Jiang et al., 2025). In this paper, we hypothesize that an LVLM has Visual Integration Point (VIP) l, critical layer where the model begins to actively leverage visual information to perform task-specific reasoning. Prior to this point, the model primarily engages in general-purpose processing of visual and textual inputsvisual features may be seen, but not yet used to guide inference, and the interactions between modalities remain shallow. This behavioral shift can be reflected in the representation distances: at and beyond VIP, the distance between blind increases markedly for visiondependent tasks (PTV), signaling that the model has started to utilize visual information to solve the task, while vision-independent tasks (PT) show smaller such shift. Thus, the notion of VIP captures key behavior transition inside LVLMs. If such specific point exists, identifying it allows us to localize where the differences between language-prior-dominated and visually grounded inference start to manifest within the models internal processing. We formalize this hypothesis below. Hypothesis 3.1 (Existence of the visual integration point). Given distance metric d(, ) : R, distributions PTV and PT (Eq. 1), and an LVLM Fθ with layers which produces chainof-embedding (Z 1, ..., L) given input, let Dl be an expected representation distance defined as Eq. 2. Then, there exists visual integration point that discerns Dl between PVT and PT, that is, vis and l {1, ..., 1} s.t. (cid:26)Dl(PVT, Fθ) Dl(PT, Fθ) > τ, Dl(PVT, Fθ) Dl(PT, Fθ) 0, < , (3) where τ R+ denote model-dependent constant threshold for each data distribution. Figure 2: Visual Integration Point. We consistently observe that there is specific layer that clearly distinguish the distance between blind across two groups DVT and DT. vis and In Figure 2, we plot the representation distance estimated for two groups: DVT (vision-dependent) in red and DT (vision-independent) in blue, across all layers in Qwen2.5-VL-7B (Bai et al., 2025) 4 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding and Gemma-3-4B (Team et al., 2025). We evaluate on three representative datasets: MME (Yin et al., 2024), MMBench (Liu et al., 2024d), and VLind-Bench (Lee et al., 2025). Since these datasets do not explicitly annotate the degree of visual dependency for each instance (PVT vs. PT), we partition each dataset into two auxiliary groups: DVT = {(xv, xt) : Fθ(xv, xt) = Fθ(, xt)} and DT = {(xv, xt) : Fθ(xv, xt) = Fθ(, xt)}. This split leverages the prediction agreement between multimodal and text-only inputs as proxy for task type: if two predictions differ, the sample must have demanded visual information to the model, suggesting membership in PVT likely. From Figure 2, we make four key observations: (1) Existence of VIP. Representation divergence between DV and DT does not show from the beginning. Instead, for both models, we observe clear visual integration point (l), where the representation distance for the DVT group rises more sharply compared to DT group, marking the onset of genuine multimodal integration; (2) Behavioral shift across VIP. We observe notable increase in the standard deviation of representation distances across VIP. Specifically, before VIP, the model exhibits relatively uniform representation distances across samples, suggesting general-purpose information processing. After VIP, the models usage of visual information becomes more diverse and instance-dependent to solve specified task for each query; (3) VIP is dataset-agnostic. Within each model, the location of the VIP is relatively consistent across all datasets. For Qwen2.5-VL-7B, the transition consistently occurs around layers 1820, and for Gemma-3-4B, the transition is around layers 2022. This stability suggests that the VIP is primarily the LVLMs intrinsic property, not one driven by dataset-specific biases; and (4) Model-specific patterns. Despite the shared existence of the VIP, the shape of distance across layers differs across models. In Qwen2.5-VL-7B, representation distance grows relatively smoothly before peaking near the middle-to-late layers and then declines. In contrast, Gemma-3-4B exhibits flat trajectories for many early layers, followed by steep and monotonic rise after VIP. This suggests that each model has distinctive hierarchical representation derived from its unique designs. Overall, these findings highlight not only the universality of the VIP existence, which distinguishes vision-centric decoding (post-l) from general information-gathering behavior (pre-l), but also the variability in how different LVLMs distinctively integrate visual information across depth. 3.3 QUANTIFYING LANGUAGE PRIOR OF LVLMS THROUGH TOTAL VISUAL INTEGRATION Although the visual integration point detects the birth of LP(x, Fθ), we are also (or even more) interested in how strong LP(x, Fθ) is. To quantify this, we define total visual integration (TVI) estimator in Def. 3.2, which measures the total amount of visual integration that effectively affects the answer decoding of LVLM, and thus is inversely related to LP in nature. Definition 3.2 (Total visual integration estimator). For an observed input = (xv, xt), define xvis := (xv, xt) and xblind := (, xt). Given an LVLM Fθ with decoder layers which produces two sets of chain-of-embedding (z1 vis) and (z1 blind), we define the empirical estimator for the per-sample total visual integration as follows, blind, ..., zL vis, ..., zL TVI(l; x, Fθ) = 1 + 1 (cid:88) l=l (cid:2)d(zl vis, zl blind))(cid:3), (4) where zl vis = fl(xvis), zl blind = fl(xblind), and d(, ) denotes distance metric. Here marks the VIP layer, where visual information begins to meaningfully influence the models internal states. The TVI score then measures the cumulative contribution of visual information by averaging representation distances across all subsequent layers (l l). The idea behind TVI is that once the model passes the visual integration point, its internal representations increasingly reflect meaningful visual grounding, rather than shallow alignment or language-driven statistical patterns. higher TVI indicates that visual information exerts stronger and more persistent influence throughout the later layers, while lower TVI suggests that the model is more likely to remain text-dominated even after l. In this sense, TVI provides holistic measure of how much the model truly uses vision for actual problem solving: strong LP corresponds to weak or shallow visual integration (low TVI), while effective multimodal reasoning corresponds to high TVI. To investigate the distinction between pre-l and post-l phases in visual integration, we analyze Spearmans rank correlation between them and answer correctness on VLind-Bench (Lee et al., 2025), which requires visual reasoning. The results in Table 1 show that correlations are weak 5 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Figure 3: VIPs of different models observed across different datasets. Our novel framework, fueled by contrasting chain-of-embedding, allows us to consistently observe VIP across multiple models and datasets, and further enables us to estimate TVI to measure language prior. and statistically less-significant when TVI is computed over pre-l layers. In contrast, the post-l aggregation yields remarkable correlations with the prediction correctness, indicating that only after the VIP, the representation distance becomes strongly associated with task performance, thereby serving as reliable indicator of effective visual integration. In this paper, we stick with post-l aggregation in Definition 3.2. Taken together, results from Figure 2 and Table 1 highlight two key insights: (1) the existence of the visual integration point l, where effective problem-solving starts to happen by integrating visual information, and (2) the strong relationship between post-l TVI and downstream performance on vision-dependent tasks. These findings demonstrate that VIP and TVI provide principled toolkit for analyzing visual integration and language prior in LVLMs. We summarize our findings below. Model Table 1: Spearmans rank correlation between prediction correctness and TVI aggregated from different layers. Qwen2.5-VL-7B Gemma3-4B pre-l post-l 0.1489 (p = 0.002) 0.4659 (p < 0.001) 0.7241 (p < 0.001) 0.7174 (p < 0.001) Summary of preliminary findings 1. The layer-wise expected representation distance between DVT and DT, i.e., Dl(DVT, Fθ) Dl(DT, Fθ), shows sudden bump up after specific layer l, while marginal before l. (cid:2)d(zl able indicator of language prior, particularly for datasets requiring visual reasoning. blind))(cid:3) over post-l layers serves as reli2. The aggregated distance 1 Ll+1 vis, zl (cid:80)L l=l"
        },
        {
            "title": "4 EXTENDED EXPERIMENTS",
            "content": "Building on the visual integration measurement introduced in the previous section, we conduct additional experiments to assess its empirical validity. Furthermore, we designed set of in-depth analyses to explore the relationship between visual integration and the language priors in LVLMs. VIP consistently emerges across different datasets and models. We extend the experimental setups described in Section 3 to broader range of 6 datasets and 9 LVLMs, including Qwen2.5-VL-7B (Bai et al., 2025), InternVL3-8B (Zhu et al., 2025), 6 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Gemma-3-4B (Team et al., 2025), LLaVA-v1.5-7B (Liu et al., 2024a), Eagle2.5-8B (Chen et al., 2025a), Llama-3.2-11B-Vision2, LLaVA-NeXT-Vicuna-7B (Liu et al., 2024b), LLaVA-OV-Qwen2-7B (Li et al., 2025) and SmolVLM (Marafioti et al., 2025). We use the instruction-tuned version for all models. For the dataset, we consider general VQA benchmarks including MME (Chaoyou et al., 2023), MMBench (Liu et al., 2024d), MMStar (Chen et al., 2024), and MMMU (Yue et al., 2024). We also incorporate two benchmarks specifically designed for language prior evaluation, which are VLind-Bench (Lee et al., 2025) and ViLP (Luo et al., 2024). This results in combination of 54 experimental settings. Implementation details, including data statistics, generation configuration, strategy for VIP selection, etc., are provided in Appendix C. As illustrated in Figure 3, the emergence of VIP is remarkably consistent across all settings: for each model, there exits clear transition layer where the distance between embeddings blind increases more significantly for vision-dependent group (DTV), compared to the vision-independent group (DT). These results highlight the universality of the VIP existence. The complete experimental results are provided in Appendix D. vis and Figure 4: TVI under language priors of different strengths. We see that TVI effectively discerns the differences in strength of LP, thereby standing for reliable measure for LP. TVI reliably differentiates strong vs. weak language prior. To examine whether TVI (c.f. Definition 3.2) reliably reflects the strength of the language prior, we contrast results on two complementary datasets: ViLP and MMBench. ViLP is intentionally constructed to induce strong LP on the data side by designing queries where plausible answers can often be inferred from textual patterns or statistical correlations without the need for visual grounding. In contrast, MMBench represents weak LP setting, with less misleading questions that elicit more visual grounding. As shown in Figure 4, our analysis reveals that datasets with stronger language priors (e.g., ViLP) yield lower TVI values, indicating weaker visual integration in the model, whereas less biased datasets (e.g., MMBench) produce higher TVI values, reflecting stronger use of visual information. This confirms that TVI serves as reliable quantitative indicator of LP. VLind Metric Qwen2.5-VL-7B InternVL-3-8B Table 2: Spearmans rank correlation between different metrics and answer prediction correctness. Comparison to existing proxy for language prior. There are alternative approaches to explain LP proposed in previous works, which rely on output-based or attention-based heuristics by assuming (1) LP manifests as high similarity between output tokens generated with and without visual input (Chen et al., 2025b; Xie et al., 2024), or (2) LP arises due to insufficient attention being allocated to visual tokens (Liu et al., 2025). In Table 2, we compare our TVI with two existing approaches (see Appendix for detailed formulation), average visual attention and output divergence, by conducting the Spearmans rank correlation analysis between these measures and the correctness of model predictions on two datasets, which all require integrating visual information to produce correct answers. Our TVI consistently exhibits stronger correlation with output correctness across all datasets and models, suggesting that TVI stands for 0.6727 (p < 0.001) 0.6335 (p < 0.001) 0.7155 (p < 0.001) 0.5709 (p < 0.001) -0.0364 (p = 0.530) 0.4967 (p < 0.001) 0.0871 (p = 0.075) 0.0746 (p = 0.197) 0.1627 (p < 0.001) 0.2978 (p < 0.001) 0.5084 (p < 0.001) 0.5615 (p < 0.001) Output Divergence Visual Attention VLind ViLP ViLP TVI 2https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct 7 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding reliable indicator of effective visual integration of LVLMs. In contrast, the other approaches show weak and inconsistent correlations in different scenarios. We argue that both existing approaches fail to directly capture the true impact of visual integration on the models generation. In the case of visual attention, the model may assign high weights to irrelevant regions of the image rather than the areas required for correct reasoning, and ultimately fall back on its language prior to generate the answerresulting in inflated attention scores but weak correlation with language prior. Meanwhile, solely measuring output-level discrepancy does not fully capture fine-grained behavior exhibited in internal representation dynamicsdifferences that are more fundamental in nature than what can be observed from final outputs. It shows the significance of procedural aggregation in TVI. We provide additional visualization analysis in Appendix E. Table 3: Spearmans rank correlation between correctness and TVI under different distance metrics. Results are based on evaluations using Qwen2.5-VL-7B. All p-values are < 0.001. Ablations on distance metrics. To investigate how different choices of distance metric affect our ability to capture model behavior, we conduct ablation studies with alternative formulations of TVI. As shown in Table 3, TVI remains strong indicator of model correctness when computed using the L2 distance between latent embeddings. However, when we apply the logit-lens technique (nostalgebraist, 2020)projecting hidden states at each layer into the output token space and computing divergence between the resulting distributionsthe effectiveness of TVI drops significantly. This degradation suggests that such projection distorts or suppresses the intermediate behavioral differences that occur during decoding. The output space, shaped by the language modeling head, inherently filters latent representations through decoding-biased lens, which may obscure subtle but meaningful cross-modal integration patterns. These observations reinforce our central contribution: to faithfully capture the behavioral dynamics of vision-language models, it is essential to examine the internal processing trajectory within the latent representation space, rather than relying on surface-level discrete outputs or their immediate projections. Additional visualization analysis is provided in Appendix E. Cosine Distance L2 Distance KL Divergence JS Divergence Output-based (w/ logit-lens) Embedding-based -0.1693 -0.2261 0.7155 0.7123 0.6335 0.6578 0.2901 0.2942 Metric VLind ViLP Ablations on model scales. We further examine whether our findings generalize across models of different scales. As shown in Figure 5, clearly defined VIP consistently emerges across models of varying sizes (4B, 12B, and 27B), underscoring the robustness and generality of our proposed behavioral analysis framework. Interestingly, we also find that the VIP tends to appear at similar relative depth, which is approximately 60% of the total number of layers, regardless of model size. In addition, after normalizing by the dimensionality of hidden states, we observe that the average normalized TVI is consistently higher in larger models on both DVT and DT. This suggests that larger models are more effective at leveraging visual information in uniform manner across diverse input types, thereby exhibiting greater robustness to misleading language priors. These observations collectively reinforce the broader applicability of our framework in analyzing visual integration behavior across model scales. Figure 5: Ablations on model scales. VIP and the dimension-normalized TVI analysis results for three variants of Gemma-3 model family. 8 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding"
        },
        {
            "title": "5 THEORETICAL ANALYSIS",
            "content": "We provide new interpretation for our metric, Dl(PVT, Fθ) Dl(PT, Fθ), that locates VIP (Theorem 5.1) and how we can practically employ the expected representation distance (Theorem 5.2) through theoretical analyses. All the proofs and an additional theorem that justifies the use of our empirical representation distance (Lemma F.1) are given in Appendix F. Information-theoretic interpretation on representation divergence. By recasting the represenblind) log ˆpT(Z l) tation distance measurement as density estimation problem, i.e., d(Z (please see Lemma F.2), we show that the difference in expected representation distances, Dl(PVT, Fθ) Dl(PT, Fθ), which we call representation divergence here, can be interpreted as relative distributional discrepancy that measures how far our density estimator ˆpT(Z l), defined by blind), from population distribution pVT(Z l) compared to pT(Z l) in Theorem 5.1. d(Z Theorem 5.1. Let = (Xv, Xt) be random variable from PVT or PT, and fl : be layer stack from an LVLM Fθ. For PT, define density estimator ˆpT(Z l) := (fl(Xt), I), and denote pVT(Z l) (resp. pT(Z l)) as the population distribution on = fl(X) derived from PVT (resp. PT). Then, given d(Z1, Z2) := 1 2 Z1 Z22 2, the difference in the expected representation distances between PVT and PT, i.e., Dl(PVT, Fθ) Dl(PT, Fθ), can be expressed as follows, vis, vis, KL(cid:0)pVT(Z l)ˆpT(Z l)(cid:1) KL(cid:0)pT(Z l)ˆpT(Z l)(cid:1) + H, (5) where is constant H(cid:0)pVT(Z l)(cid:1) H(cid:0)pT(Z l)(cid:1), and KL() denotes the KL divergence. Implication. Lemma F.2 shows that d(Zvis, Zblind) becomes density estimator on PT with strictly proper scoring. With that, Theorem 5.1 tells us that Dl(PVT, Fθ) Dl(PT, Fθ) can be interpreted as relative proximity of our density estimate ˆpT to each distributions pVT and pT with an additive constant H, defined by pVT and pT. More intuitively, the first term, KL(pVTˆpT), can be understood how the ground truth representation distribution on PVT far from ˆpT (distance-based estimate of PT), while the second term, KL(pTˆpT), can be regarded as quality of our density estimation with ˆpT to approximate the distribution pT. Analytic bounds on representation divergence for practical use. We have assumed fixed model Fθ so far. If one is willing to adapt the model to improve its effective visual integration, the analytic bounds in Theorem 5.2 described with H-divergence (see Def. F.4) can be useful. Theorem 5.2. Let = (Xv, Xt) be random variable of multimodal input query. Given stack of LVLM layers fl : and distance metric : [0, 1], define hypothesis = d(fl(Xv, Xt), fl(Xt)) : [0, 1] and set of these hypotheses that has pseudodimension c. Then, for Dl(P, Fθ) := EXP [h(X)] with any PVT, PT, and PM := PVT+PT , and the empirical distributions DVT PVT and DT PT of samples for each, we have the following bounds w.p. at least 1 δ for 0 < δ < 1, 2 i) 1 Dl(DT, Fθ) 1 H(DVT, DT) Oδ Dl(PVT, Fθ), ii) 1 2 1 4 H(DVT, DT) Oδ Dl(PM, Fθ) 1 2 + 1 4 H(DVT, DT) + Oδ (6) (7) where := {Ih(X)h(X)>t : h, H, 0 1} and Oδ := O( (cid:113) 1 (log 1 δ + log )). Implication. The first inequality (Ineq. 6) reveals relationship between two expected representation distances across PVT and DT PT with H(DVT, DT) as bridge. This tells us that if we want to increase visual integration after VIP for an unknown data distribution that require visual reasoning (PVT), we can pursue greater lower bound of it by decreasing Dl(DT, Fθ) and (DVT, DT) with empirical samples we have. Meanwhile, in case where we encountered an unknown mixture distribution PM, the second inequality (Ineq. 7) tells us we can broaden the effective range of Dl on PM by pursuing greater value of H(DVT, DT). 9 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding"
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present formal framework for understanding and quantifying the language prior in LVLMs by contrasting the chain-of-embedding between visual and blind contexts. Through this framework, we identify the consistent existence of the Visual Integration Point (VIP), specific layer at which the model begins to meaningfully incorporate visual context for task-solving beyond the shallow information gathering. Building on this observation, we propose new metric named Total Visual Integration (TVI), which estimates the degree of effective visual integration and therefore language prior. We conduct comprehensive experiments across 9 LVLMs and 6 datasets, and the results demonstrate that our framework robustly works across models and tasks, providing consistent and interpretable signals about the presence and strength of language prior. Finally, we present some theorems for better understanding and justification of our framework. We hope that this work sheds light on the internal mechanisms of multimodal models and provides foundation for diagnosing the language prior, ultimately guiding the development of reliable and responsible LVLMs."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "The authors would like to thank Boyi Li, Haobo Wang, Junbo Zhao, Min-Hsuan Yeh, and Jiatong Li for their insightful feedback and helpful discussions throughout the development of this work. Their suggestions greatly contributed to improving the quality of the draft. Changdae Oh, Seongheon Park, and Yixuan Li are supported in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation under awards IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, Schmidt Sciences Foundation, Open Philanthropy, Alfred P. Sloan Fellowship, and gifts from Google and Amazon."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work conducts empirical and analytical studies on the internal behavior of LVLMs, with the goal of understanding and quantifying their reliance on language priors and the extent of visual information integration during inference. To pursue high standards of scientific excellence, we propose formal framework with clear definitions of all used terms and conduct validation at scale, e.g., 54 combinations of models and datasets, and we further provide theoretical analyses on our framework. Our study does not involve any human subjects, personally identifiable information, or sensitive data. All experiments are conducted using publicly available models and benchmark datasets that are widely adopted in the multimodal learning community. Our proposed metrics and analyses are intended for research and diagnostic purposes. By providing tools to diagnose when LVLMs rely on text versus vision, we aim to support more accountable model development and contribute positively to the responsible advancement of AI. We encourage future work to further validate these findings under more diverse real-world conditions."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All of the models and datasets we used in this work are publicly available. To further ensure the reproducibility of our findings, we provide comprehensive descriptions of all experimental settings, including dataset preprocessing, model configurations, metric definitions, and evaluation protocols, in Section 4 and Appendix C. Our framework does not require model re-training or fine-tuning, and all evaluations are conducted in zero-shot setting using publicly available model checkpoints, which minimizes computational and hardware requirements. We release the complete codebase for our analysis framework, including tools for data preparation, TVI computation, and visualization, at the following repository: (cid:135)."
        },
        {
            "title": "REFERENCES",
            "content": "Amit Artzy and Roy Schwartz. Attend first, consolidate later: On the importance of attention in different llm layers. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 177184, 2024. 10 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Saketh Bachu, Erfan Shayegani, Rohit Lal, Trishna Chakraborty, Arindam Dutta, Chengyu Song, Yue Dong, Nael B. Abu-Ghazaleh, and Amit Roy-Chowdhury. Layer-wise alignment: Examining safety alignment across image encoder layers in vision language models. In Forty-second International Conference on Machine Learning, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19, 2006. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. theory of learning from different domains. Machine learning, 79(1):151175, 2010. Leonard Bereska and Stratis Gavves. Mechanistic interpretability for AI safety - review. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Bingjie Wang, and Chenliang Xu. Unveiling visual perception in language models: An attention head analysis approach. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 41354144, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Fu Chaoyou, Chen Peixian, Shen Yunhang, Qin Yulei, Zhang Mengdan, Lin Xu, Yang Jinrui, Zheng Xiawu, Li Ke, Sun Xing, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 3, 2023. Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, et al. Eagle 2.5: Boosting long-context post-training for frontier vision-language models. arXiv preprint arXiv:2504.15271, 2025a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. Yangyi Chen, Hao Peng, Tong Zhang, and Heng Ji. Prioritizing image-related tokens enhances vision-language pre-training. arXiv preprint arXiv:2505.08971, 2025b. Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, and Mikel Artetxe. Improving language plasticity via pretraining with active forgetting. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 3154331557, 2023. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. Ailin Deng, Tri Cao, Zhirui Chen, and Bryan Hooi. Words or vision: Do vision-language modIn Proceedings of the Computer Vision and Pattern Recognition els have blind faith in text? Conference, pp. 38673876, 2025. Michel Marie Deza and Elena Deza. Encyclopedia of distances. In Encyclopedia of distances, pp. 1583. Springer, 2009. 11 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Brian Everitt and Anders Skrondal. The Cambridge dictionary of statistics, volume 4. Cambridge university press Cambridge, UK, 2010. Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. Not all layers of llms are necessary during inference. arXiv preprint arXiv:2403.02181, 2024. Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1430314312, 2024. Stephanie Fu, Tyler Bonnen, Devin Guillory, and Trevor Darrell. Hidden in plain sight: Vlms overlook their visual representations. arXiv preprint arXiv:2506.08008, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):135, 2016. Tilmann Gneiting and Adrian Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359378, 2007. Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, and Xu Yang. Devils in middle layers of large vision-language models: Interpreting, detecting and mitigating object hallucinations via attention lens. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2500425014, 2025. Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, et al. Exploring concept depth: How large language models acquire knowledge and concept at different layers? In Proceedings of the 31st International Conference on Computational Linguistics, pp. 558573, 2025. Sonia Joseph and Neel Nanda. Laying the foundations for vision and multimodal mechanistic interpretability & open problems. In AI Alignment Forum, volume 2, 2024. Kang-il Lee, Minbeom Kim, Seunghyun Yoon, Minsung Kim, Dongryeol Lee, Hyukhun Koh, and Kyomin Jung. Vlind-bench: Measuring language priors in large vision-language models. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 41294144, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. Trans. Mach. Learn. Res., 2025, 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740755. Springer, 2014. Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, and Sheng Liu. More thinking, less seeing? assessing amplified hallucination in multimodal reasoning models. arXiv preprint arXiv:2505.21523, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, tuning. Seattle, WA, USA, June 16-22, 2024, pp. 2628626296. IEEE, 2024a. 12 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms. In European Conference on Computer Vision, pp. 125140. Springer, 2024c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024d. Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, and Honglak Lee. Probing visual language priors in vlms. arXiv preprint arXiv:2501.00569, 2024. Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, and Honglak Lee. Probing visual language priors in VLMs. In Forty-second International Conference on Machine Learning, 2025. Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, and Fazl Barez. Towards interpreting visual information processing in vision-language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. nostalgebraist. Interpreting gpt: The logit lens. https://www.lesswrong.com/posts/ AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens, 2020. LessWrong. Changdae Oh, Jiatong Li, Shawn Im, and Yixuan Li. Visual instruction bottleneck tuning. arXiv preprint arXiv:2505.13946, 2025. OpenAI. Gpt-5 system card. Technical report, OpenAI, August 2025. URL https://cdn. openai.com/gpt-5-system-card.pdf. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Featured Certification. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision, pp. 18 34, 2024. Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Nikul Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. In Forty-second International Conference on Machine Learning, 2025. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. 13 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024b. Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, and Neel Nanda. How visual representations map to language feature space in multimodal llms. arXiv preprint arXiv:2506.11976, 2025. An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, and Daeyoung Kim. Vision language models are biased. arXiv preprint arXiv:2505.23941, 2025. Yuxi Xie, Guanzhen Li, Xiao Xu, and Min-Yen Kan. V-DPO: mitigating hallucination in large vision language models via vision-guided direct preference optimization. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 1325813273. Association for Computational Linguistics, 2024. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao Costeira, and Geoffrey Gordon. Adversarial multiple source domain adaptation. Advances in neural information processing systems, 31, 2018. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing In The Twelfth Internavision-language understanding with advanced large language models. tional Conference on Learning Representations, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 14 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding"
        },
        {
            "title": "CONTENTS",
            "content": "A Related Work Discussion on Visual Integration Point B.1 Configuration and Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Algorithmic Estimation on Visual Integration Point . . . . . . . . . . . . . . . . . Implementation Details Additional Experimental Results Further Analysis Details on Theoretical Analysis and Proofs F.1 Justification and Interpretation on Representation Divergence . . . . . . . . . . . . F.2 Notations and Problem Setup for Theorem F.6 . . . . . . . . . . . . . . . . . . . . F.3 Proof for Theorem F.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Impact Statement Limitations Disclosure of LLM Usage 19 19 20 20 22 23 23 25 25 26 27 15 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Figure 6: Complete experimental results. (Part 1) 16 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Figure 7: Complete experimental results. (Part 2) 17 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Figure 8: Complete experimental results. (Part 3) 18 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding"
        },
        {
            "title": "A RELATED WORK",
            "content": "Visual perception in LVLMs. Most mainstream LVLMs (Liu et al., 2023; 2024a;b; Bai et al., 2025; Dai et al., 2023) adopt late-fusion architecture comprising three key components: vision backbone, language model that processes both image and text tokens, and modality adapter that aligns visual representations with the language space. The visual understanding capabilities of these models largely depend on the perception quality of pre-trained vision encoders (e.g., CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023)) and the reasoning ability of large-scale language models. Despite promising results on some multimodal benchmarks, this paradigm has been recently challenged due to its poor performance on vision-centric tasks, suggesting that these models often fail to truly see the image (Tong et al., 2024b;a; Vo et al., 2025). growing body of work has sought to uncover how visual perception operates internally within LVLMs. For example, Bi et al. (2025); Jiang et al. (2025); Neo et al. (2025) examine the layer-wise attention patterns and identify distinct phases of visual integration, typically emerging in the mid-to-late layers. Venhoff et al. (2025) utilize pretrained sparse autoencoders (SAEs) as analytical tools to show that visual representations gradually align with language representations over depth, converging in the later layers. Complementarily, Fu et al. (2025) apply probing techniques and argue that language decoders in existing LVLMs struggle to effectively leverage the visual features produced by their vision backbones. Their findings suggest that the bottleneck lies not in the availability of visual information but in feature misalignment. Language prior in LVLMs. One of the most prominent limitations of current LVLMs is their tendency to overly rely on language priors, often generating plausible outputs without grounding in the visual context. This behaviorcommonly referred to as the language prior problemhas drawn increasing attention. Recent studies attempt to evaluate this phenomenon by designing datasets that stress-test visual grounding. For example, Lee et al. (2025) and Luo et al. (2025) construct datasets with counterfactual visual inputs to assess whether models can disentangle visual signals from misleading linguistic cues. Similarly, Deng et al. (2025) test modality conflict scenarios to evaluate the models preference between text and image inputs. While these works help reveal the presence of language priors, they offer limited insight into the underlying causes. Most current understandings of language prior are based on two widely adopted assumptions: (1) it manifests as high similarity between outputs with and without visual input (Chen et al., 2025b; Xie et al., 2024), and (2) it arises due to insufficient attention allocated to visual tokens (Liu et al., 2025). Building on these assumptions, several works propose methods to mitigate language priorssuch as contrastive decoding (Favero et al., 2024) or inference-time attention reallocation (Liu et al., 2024c). Others, like Chen et al. (2025b) and Xie et al. (2024), explore training-time interventions that penalize outputs overly aligned with the models default language predictions."
        },
        {
            "title": "B DISCUSSION ON VISUAL INTEGRATION POINT",
            "content": "B.1 CONFIGURATION AND PRACTICE In Eq. 8, we chracterized the VIP based on the pre-l and post-l representation divergences Dl(PVT, Fθ) Dl(PT, Fθ) where the pre-l chain-of-embeddings shows nearly zero representation divergence whereas the post-l chain-of-embeddings exhibits an effectively large representation divergence defined by positive τ . In practice, we can not access the population distribution PVT and PT, therefore we can not compute the truth expected representation distance Dl(P, Fθ). What we do in practice is estimate that expected representation distance with finite samples DVT and DT. We observed that this empirical estimator (Eq. 2) is actually well aligned with our hypothesis, yet there can be some cases where the fitness of the estimator is bad, e.g., Figure 9. However, even in that case, the point l, where the empirical representation divergence exceeds positive constant for all subsequent layers, consistently emerges, and the TVI (Eq. 4) is calculated based on that becomes strong indicator of LP. 19 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding B.2 ALGORITHMIC ESTIMATION ON VISUAL INTEGRATION POINT As discussed in 3.2, identifying the VIP provides valuable insights into when visual information begins to influence answer decoding. In most cases, we rely on empirical observation of the representation distance curves to determine l, which proves to be straightforward and interpretable across wide range of models. However, in situations where manual inspection is impractical e.g., for large-scale model comparisons or automated pipelines it may be desirable to estimate the VIP programmatically. To this end, we introduce an optional empirical estimator that can automatically approximate the VIP given model and dataset. While not required for our core analysis, this estimation method serves as practical tool in settings where manual identification of is infeasible. Definition B.1 (Visual integration point estimator). For dataset sampled from mixture distribution = (1 λ)PVT + λPT for λ (0, 1) and an LVLM Fθ with decoder layers which produces chain-of-embedding (Z 1, ..., L) given input, let be the index set of LVLM decoder layers and Dl be an expected representation distance defined as Eq. 2. An empirical visual integration point estimator is as follows, VIP(D, Fθ) = arg min lLL (cid:2)Dl(DVT, Fθ) Dl(DT, Fθ) α σl Dl(D, ) (cid:3), (8) where α is pre-defined coefficient and σl denotes the sample standard deviation of the observed representation distance over D. VIP(D, Fθ) can be interpreted as the layer index where the average d(, ) on DVT first exceeds that on DT up to an empirical coefficient of variation (Everitt & Skrondal, 2010) of d(, ) on l-th layer. Besides, one can also devise another VIP estimator based on the test statistic we discuss in Eq 10 of Lemma F.1 by specifying arbitrary significance levels that the user prefers."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "Models. We evaluate our framework on 9 publicly available LVLMs, covering diverse range of architectures and training paradigms. For all models, we use the official instruction-tuned checkpoints available on Hugging Face3. To ensure consistent comparison across models, we set the generation temperature to 0. Datasets. Our evaluation spans 6 benchmark datasets, each consisting of either binary (Yes/No) or multiple-choice questions. This design ensures that the hidden state of the final tokenused for representation distance calculationsis closely tied to the models reasoning process. For MMMU, we use the validation set and filter out samples that involve more than one image or open-ended output to ensure consistency in the evaluation setting. Also, when constructing the prompt, we do not use the provided explanations for fair comparison. For ViLP, we consistently select image3 and answer3 to curate our (counterfactual) VQA pair. For VLind-Bench, we convert each annotated counterfactual statement into binary Yes/No question. To perform this transformation, we use the advanced language model GPT-4o with the following prompt: 3https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct https://huggingface.co/llava-hf/llava-1.5-7b-hf https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-ov-hf https://huggingface.co/OpenGVLab/InternVL3-8B-hf https://huggingface.co/nvidia/Eagle2.5-8B https://huggingface.co/google/gemma-3-4b-it https://huggingface.co/google/gemma-3-12b-it https://huggingface.co/google/gemma-3-27b-it https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct 20 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Generate question based on the counterfactual information in the given statement. The question should be answered by yes. Here are some examples: Statement: The Statue of Liberty is holding sword instead of torch. Question: Is the Statue of Liberty holding sword? Statement: The Sydney Opera House is illustrated as an underwater aquarium, with fish swimming around its structures. Question: Is the Sydney Opera House underwater? Statement: The Leaning Tower of Pisa is perfectly vertical in the image, without any tilt. Question: Is the Leaning Tower of Pisa perfectly vertical? Now generate question for the following statement: {statement} Figure 9: Experimental results under controlled settings. We use synthetic baseline constructed from CommonsenseQA questions paired with irrelevant images as DT (vision-independent), while standard VQA benchmarks (MME, MMBench, and VLind-Bench) are used as DVT (visiondependent). The models are instructed to directly generate the answer under zero-shot setting, without involving any reasoning steps. Additional statistics of each dataset and corresponding splits are provided in Table 4. To further validate the reliability of the agreement-based separation introduced in 3.2, and to demonstrate that our analysis can generalize to scenarios with known visual dependencies, we construct controlled baseline dataset. Specifically, we take questions from CommonsenseQA (Talmor et al., 2018), which are inherently language-only, and pair each question with randomly selected, irrelevant image from COCO 2017-val (Lin et al., 2014), forming synthetic VQA setting that does not require visual input. We treat this as our vision-independent group DT . For the vision-dependent group DV , we use samples from standard VQA benchmarks such as MME, MMBench, and VLindBench, which typically require more grounding in visual content. As shown in Figure 9, the average TVI is significantly lower for the baseline DT compared to the general VQA datasets, confirming that the model does not extract meaningful information from irrelevant visual input. In contrast, even in the presence of strong language priors, the model still benefits from image content in DV . It is also worth noting that there is reverse trend before VIP for Qwen2.5-VL-7B, indicating representation distance before VIP is not associated with the actual meaningful visual integration during decoding. These findings are consistent with our previous results and further support the effectiveness of our proposed separation framework. Nonetheless, to ensure better control over data attributes such as format and context length, and to reveal clearer trends, we continue to use the agreement-based separation strategy in the main text. Metrics. All TVI values reported in our experiments are computed based on empirically determined Visual Integration Points (VIPs) specific to each model. The following VIPs are used: Qwen2.5-VL-7B (18), InternVL3-8B (16), Gemma-3-4B (20), Gemma-3-12B (26), Gemma-3-27B (35), LLaVA-v1.5-7B (9), Eagle2.5-8B (15), Llama-3.2-11B-Vision (12), LLaVA-NeXT-Vicuna-7B (12), LLaVA-OV-Qwen2-7B (15) and SmolVLM (15). We Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Table 4: Dataset statistics. stands for multiple-choice and stands for binary-choice (Yes/No). Qwen2.5-VL-7B is used as an example here. Statistics MME MMBench MMStar MMMU VLind-Bench ViLP Question Type DV DT 2374 546 4377 2782 1595 1500 1057 443 805 446 359 418 144 274 300 177 123 also provide an automatic method for estimating VIP in Appendix B.2 for potential practical usage. Representation distances are computed using the hidden states corresponding to the last generated token. The metrics introduced in 4 are computed as follows: Visual Attention ="
        },
        {
            "title": "1\nLH",
            "content": "L (cid:88) (cid:88) l=1 h=1 α(l,h), Output Divergence = d(Z vis, blind) (9) where α(l,h) denotes the total attention from the final generated token to all preceding visual tokens in head at layer l, and represents the hidden state at the final layer."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "We provide the complete experimental results on 9 LVLMs and 6 datasets in Figure 6, 7, 8. Across all models and datasets, consistent existence of the VIP can be observed. However, for some models, such as SmolVLM, the divergence between DV and DT is less pronounced, likely due to the models limited capacity and thus less promising visual integration."
        },
        {
            "title": "E FURTHER ANALYSIS",
            "content": "In this section, we elaborate on the limitations of attention-based and output-based analysis for language prior in LVLMs. First, for attention-based methods, we argue that higher visual attention weight does not necessarily imply better visual grounding. As shown in Figure 10, under weak language priors, the model is able to correctly attend to the key areas that are semantically related to the given instruction. However, under strong language priors, we observe pathological attention pattern in which the models attention becomes abnormally concentrated in limited region of the image. We refer to this phenomenon as an attention sink. In such cases, although the aggregated visual attention weight appears high, it does not reflect meaningful visual processing. Instead, the model is effectively bypassing genuine visual understanding by fixating on irrelevant or static regions, thereby undermining the utility of attention-based metrics as reliable indicators of visual integration. To better understand why output-based representations are less effective in capturing language prior behavior, we visualize how representation distances vary across the latent and output spaces. As shown in Figure 11, the projection from the latent space to the output token space tends to obscure semantic distinctions that are otherwise indicative of the models underlying behaviorsuch as whether it is performing effective visual grounding or defaulting to language priors. This observation aligns with our earlier argument: surface-level outputs alone may not faithfully reflect the internal Instead, meaningful behavioral signals often reside in the decision-making process of LVLMs. deeper latent representations, emphasizing the importance of analyzing internal dynamics rather than relying solely on output-level comparisons. 22 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Figure 10: Visualization of visual attention maps under weak and strong language priors. Figure 11: Layer-wise representation distances in latent space vs. output space. We apply the logit-lens to project hidden states at each layer into the output space. In (a), distances are computed over the entire output vector, while in (b), they are restricted to the top-k token positions corresponding to candidate answer options."
        },
        {
            "title": "F DETAILS ON THEORETICAL ANALYSIS AND PROOFS",
            "content": "F.1 JUSTIFICATION AND INTERPRETATION ON REPRESENTATION DIVERGENCE We first show that our empirical estimate for the difference in the expected representation distances (Eq. 3) can be viewed as two-sample test statistic with asymptotic normality in Lemma F.1. Lemma F.1. Let = (Xv, Xt) be random variable sampled from PVT or PT, and denote DVT PVT and DT PT as empirical distributions with and i.i.d. samples, respectively. : from Fθ and distance metric : Given stack of LVLM layers fl with finite second moment, the difference in the expected representation distance estimates Dl(, ) between DVT and DT is two-sample test statistic with asymptotic normality, that is, Dl(DTV, Fθ) Dl(DT, Fθ) approx (µT µVT, σ2 + σ2 VT ) as N, , (10) where µVT = Dl(PVT, Fθ), µT = Dl(PT, Fθ) and σ2 σ2 = VarPT [d(fl(Xv, Xt), fl(Xt))] < . VT = VarPVT[d(fl(Xv, Xt), fl(Xt))] < , 23 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding Proof. Let IVT and JT be the index sets of and i.i.d. samples from PVT and PT, respectively. If samples from PVT and PT are independent, with finite second moments, we have, = (cid:80) iIVT Dl(DVT, Fθ) Dl(DT, Fθ) v, xi [d(fl(xi σ2 (µT µVT, σ2 VT + t), fl(xi t))] (cid:80) jJT v, xj [d(fl(xj t ), fl(xj ))] (11) (12) ) (by Central Limit Theorem as N, ) Note that, even though the samples from PVT and PT are not independent, the asymptotic normality still holds by considering covariance terms between the two distributions. Now, in Lemma F.2, we provide new interpretation of our representation distance measure between CoE by casting the distance measurement as density estimation problem. Lemma F.2. Let = (Xv, Xt) be random variable sampled from PVT or PT, and fl : be stack of LVLM layers. For PT, define density estimator ˆpT(Z l) := (fl(Xt), I). Given squared l2 distance d(Z1, Z2) := 1 2, the representation distance d(fl(Xv, Xt), fl(Xt)) is the negative log-likelihood estimate of from PT, denoted as ˆpT(Z l), up to an additive constant, and is strictly proper scoring rule. That is, 2 Z1 Z22 where = (2π) dz 2 is normalizing constant for the dz-dimensional unit-variance Gaussian. d(fl(Xv, Xt), fl(Xt)) log ˆpT(Z l) + log C, Proof. It is easy to check, ˆpT(Z l) = (Z blind, I) ˆpT(Z l) = exp( blind)2 2 l 2 blind2 l ) log ˆpT(Z l) = log 2 log ˆpT(Z l) = log d(Z l, blind) (17) where Zblind = fl(Xt). In addition, the logarithm of the likelihood is strictly proper scoring rule (Gneiting & Raftery, 2007), and an offset translation of it is also strictly proper scoring. On top of this new framing, i.e., distance measurement as density estimation problem, we further provide an information-theoretic interpretation of the difference in expected representation distance in Theorem F.3. Theorem F.3 (Restatement of Theorem 5.1). Let = (Xv, Xt) be random variable from PVT or PT, and fl : be layer stack from an LVLM Fθ. For PT, define density estimator ˆpT(Z l) := (fl(Xt), I), and denote pVT(Z l) (resp. pT(Z l)) as the population distribution on = fl(X) derived from PVT (resp. PT). Then, given d(Z1, Z2) := 1 2, the difference in the expected representation distances between PVT and PT, i.e., Dl(PVT, Fθ) Dl(PT, Fθ), can be expressed as follows, 2 Z1 Z2 KL(cid:0)pVT(Z l)ˆpT(Z l)(cid:1) KL(cid:0)pT(Z l)ˆpT(Z l)(cid:1) + H, where is constant H(cid:0)pVT(Z l)(cid:1) H(cid:0)pT(Z l)(cid:1), and KL() denotes the KL divergence. Proof. Dl(PVT, Fθ) Dl(PT, Fθ) = EPVT [d(fl(Xv, Xt), fl(Xt))] EPT [d(fl(Xv, Xt), fl(Xt))] = EpVT(Zl)[ log ˆpT(Z l) + log C] EpT(Zl)[ log ˆpT(Z l) + log C] = EpVT(Zl)[ log ˆpT(Z l)] EpT(Zl)[ log ˆpT(Z l)] = H(cid:0)pVT(Z l), ˆpT(Z l)(cid:1) H(cid:0)pT(Z l), ˆpT(Z l)(cid:1) = (cid:2)KL(cid:0)pVT(Z l)ˆpT(Z l)(cid:1) + H(cid:0)pVT(Z l)(cid:1)(cid:3) (cid:2)KL(cid:0)pT(Z l)ˆpT(Z l)](cid:1) + H(cid:0)pT(Z l)(cid:1)(cid:3) (18) (19) (20) (21) (22) (23) 24 (13) (14) (15) (16) Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding where H() and H(, ) denote entropy and cross-entropy, respectively. Here, Eq. 20 holds by Lemma F.2, and the remaining equality is trivial by the definitions of d() and information theoretic measures. F.2 NOTATIONS AND PROBLEM SETUP FOR THEOREM F. We recast the problem of measuring the representation distance d(Zvis, Zblind) as binary classification task, where we want to classify the sample (Zvis, Zblind) into 1 if it originates from the distribution PVT while 0 for the samples from PT. To be specific, let , Z, and denote input, LVLM representation, and output, respectively. We have multimodal input query = (Xv, Xt) , stack of LVLM layers fl : Z, and distance metric : [0, 1]. With that, we define hypothesis = d(fl(Xv, Xt), fl(Xt)) : [0, 1] as real value function to measure the relative likelihood that the input is sampled from PVT rather than PT, and we also define the labeling function : {0, 1} that maps the input into its ground-truth membership, i.e., 1 if its from PVT and 0 if its from PT. Then, we formulate an expected error (a.k.a. risk) of hypothesis w.r.t. the labeling function on distribution as follows: εP (h, h) := EXP [h(X) h(X)]. Besides, in Def. F.4, we introduce measure of discrepancy between two distributions, Hdivergence, which has been widely adopted in domain adaptation literature (Ben-David et al., 2006; 2010; Ganin et al., 2016; Zhao et al., 2018). Definition F.4 (H-divergence, (Ben-David et al., 2006; 2010)). Let and be probability distributions on the input domain , and be hypothesis class for . Denote AH := {h1(1)h H} as collection of subsets of that are the support of some hypotheses in H. Then, the distance between and based on is defined as follows: dH(P, ) = 2 sup AAH PP [A] PP [A]. (24) Now, we are ready to present the proof for Proposition 5.2 in the next subsection. F.3 PROOF FOR THEOREM F.6 Theorem F.5 (Restatement of Theorem 5.2). Let = (Xv, Xt) be random variable of : and distance metric multimodal input query. Given stack of LVLM layers fl : [0, 1], define hypothesis = d(fl(Xv, Xt), fl(Xt)) : [0, 1] and set of these hypotheses that has pseudo-dimension c. Then, for Dl(P, Fθ) := EXP [h(X)] with any PVT, PT, and PM := PVT+PT , and the empirical distributions DVT PVT and DT PT of samples for each, we have the following bounds w.p. at least 1 δ for 0 < δ < 1, 2 i) 1 Dl(DT, Fθ) 1 2 H(DVT, DT) Oδ Dl(PVT, Fθ), ii) 1 2 1 4 H(DVT, DT) Oδ Dl(PM, Fθ) 1 + 1 4 H(DVT, DT) + Oδ (25) (26) where := {Ih(X)h(X)>t : h, H, 0 1} and Oδ := O( (cid:113) 1 (log 1 δ + log )). Proof. Note the lemma below that provides connection between the difference in the expected errors across two distributions and their distributional discrepancy. Lemma F.6 (Zhao et al. (2018)). For h, := {h : [0, 1]} assume that has finite pseudo dimension d. For any distribution and over , εP (h, h) εP (h, h) 1 2 H(P, ), (27) where := {Ih(x)h(x)>t : h, H, 0 1}. Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding See Lemma 1 of Zhao et al. (2018) for the proof. We start our derivation of Proposition 5.2 from the ineq. F.6 as below, 1 2 H(PVT, PT) εPVT(h, h) εPT (h, h) (cid:12)Dl(PVT, Fθ) 1 Dl(PT, Fθ) 0(cid:12) (cid:12) = (cid:12) = 1 Dl(PVT, Fθ) Dl(PT, Fθ) 1 Dl(PVT, Fθ) Dl(PT, Fθ) = 1 Dl(PVT, Fθ) Dl(PT, Fθ), (28) (29) (30) (31) (32) where the first equality holds by definition, the first inequality holds by the reverse triangular inequality, and the second and fourth equality hold given 0 Dl(P, Fθ) 1. In the meantime, for the empirical distributions DVT PVT and DT PT of samples for each, given 0 < δ < 1, we have the following approximation error bounds with probability at least 1 δ for any (See Lemma 5 and Lemma 6 of Zhao et al. (2018)), εP (h, h) εD (h, h) + O( H(PVT, PT) H(DVT, DT) + O( (cid:114) (cid:114)"
        },
        {
            "title": "1\nN\n1\nN",
            "content": "(log (log 1 δ 1 δ + log + log"
        },
        {
            "title": "N\nc\nN\nc",
            "content": ")), )), (33) (34) where dim(H) = c. Then, by plugging the above inequality (Ineq. 34) into the Ineq. 30, we have, 1 1 + 1 2 1 2 H(DVT, DT) O( H(DVT, DT) + O( (cid:114) (cid:114) 1 1 (log (log 1 δ 1 δ + log + log c )) Dl(PVT, Fθ) + Dl(PT, Fθ), (35) )) Dl(PVT, Fθ) + Dl(PT, Fθ), (36) where we derive the first statement of Proposition F.5 from the Ineq. 35, and the second statement of that by combining both Ineq. 35 and Ineq. 36, that complete the proof."
        },
        {
            "title": "G IMPACT STATEMENT",
            "content": "Language prior represents pathological behavioral pattern in LVLMs, where the model overly relies on its linguistic knowledge and fails to properly ground its predictions in the visual input. This phenomenon underlies critical issues such as hallucination, modality misalignment, and failure cases in vision-centric reasoning. It also suggests that current LVLMs may not be operating in the modality-aware manner we expecteven when their outputs appear plausible (as the result of the vanilla next-token-prediction training paradigm). One of the main challenges in mitigating language prior lies in its vague and subjective nature: there exists no clear definition or quantitative measure of language prior in dataset or task. Consequently, efforts to balance visual and textual information during training or fine-tuning often rely on heuristics or manual annotations. Our work sheds light on this issue by proposing formal framework to characterize and quantify the language prior through the models own behavior. This makes the problem not only more visible, but also more measurable. If the degree of language prior can be reliably estimated from within the model, we can begin to incorporate this signal directly into training objectives or inference strategies in principled way. In this way, our framework is not only valuable for foundational understanding, but also offers actionable tools for improving real-world multimodal systems. In addition to the ultimate goal, i.e., understanding and quantifying LP of LVLM, our novel method, contrastive chain-of-embeddings, on the path to pursue that goal can also create rich inspiration for line of works on layer-wise representation analysis (Skean et al., 2025) as well as layer-specific adaptive training approach for LVLMs (Bachu et al., 2025; Oh et al., 2025), which ultimately contribute to building trustworthy multimodal AI system. 26 Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding"
        },
        {
            "title": "H LIMITATIONS",
            "content": "Our method requires white-box access to the models internal hidden states and attention patterns. This restricts its applicability to open-weight models and excludes commercial APIs or closedsource systems. Rather than serving as deployment tool, our framework is primarily designed for model analysis and interpretability research, aiming to shed light on how and when visual information is integrated during inference. We hope these insights can inform future directions in model design and training strategies."
        },
        {
            "title": "I DISCLOSURE OF LLM USAGE",
            "content": "Some portions of this paper were polished and refined with the assistance of large language model (LLM) tools (e.g., ChatGPT) to improve clarity, fluency, and consistency in writing. We also harnessed coding agent (e.g., Cursor) to write some simple utility functions after double-checking. All technical content, experimental results, and analytical conclusions were independently developed by the authors without the use of LLMs."
        }
    ],
    "affiliations": [
        "University of Wisconsin-Madison"
    ]
}