{
    "paper_title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
    "authors": [
        "Quyu Kong",
        "Xu Zhang",
        "Zhenyu Yang",
        "Nolan Gao",
        "Chen Liu",
        "Panrong Tong",
        "Chenglin Cai",
        "Hanzhang Zhou",
        "Jianan Zhang",
        "Liangyu Chen",
        "Zhidan Liu",
        "Steven Hoi",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence."
        },
        {
            "title": "Start",
            "content": "2025-12-23 MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments Quyu Kong,1, Xu Zhang,1, Zhenyu Yang2, Nolan Gao3, Chen Liu1, Panrong Tong1, Chenglin Cai1, Hanzhang Zhou1, Jianan Zhang1, Liangyu Chen1, Zhidan Liu2 Steven HOI1, Yue Wang1 (cid:0) 1Tongyi Lab , Alibaba Group Equal contribution (cid:0) yue.w@alibaba-inc.com 2HKUST (GZ) 3University of Florida 5 2 0 2 2 2 ] . [ 1 2 3 4 9 1 . 2 1 5 2 : r https://github.com/Tongyi-MAI/MobileWorld"
        },
        {
            "title": "Abstract",
            "content": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same rigorous level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agentuser interaction and Model Context Protocol (MCP)-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively, highlighting ample headroom for future research. Our analysis further shows that current models struggle significantly with user interaction and MCP calls. By identifying these core research gaps, we offer strategic roadmap toward more robust, next-generation mobile intelligence."
        },
        {
            "title": "Introduction",
            "content": "GUI agents on smartphones have emerged as transformative technology for automating mobile tasks, enabling users to delegate complex operations through natural language instructions. With the rapid advancement of Vision-Language Models (VLMs), numerous VLM-based mobile agents have been developed to navigate mobile interfaces automatically (Rawles et al., 2023; Deng et al., 2024; Chen et al., 1 Figure 1: Compared to AndroidWorld, MobileWorld exhibits lower SOTA success rates, longer task horizons, more cross-application tasks, and sharp performance drops for recent models. 2024; Yang et al., 2025). Reliable benchmarks are therefore essential for measuring their progress. Despite the existence of multiple benchmarks, AndroidWorld (Rawles et al., 2024) remains the go-to benchmark for mobile GUI agent evaluation, owing to its reproducible emulator environment and deterministic evaluation. However, AndroidWorld is approaching saturation state-of-the-art agentic frameworks now achieve success rates exceeding 90% on its leaderboard, fundamentally limiting our ability to distinguish between incremental improvements and genuine breakthroughs in mobile agent capabilities. Beyond saturation, current mobile agent benchmarks exhibit several fundamental limitations. First, existing tasks lack the complexity reflecting real-world mobile usage. Prior benchmarks typically feature short-horizon tasks confined within single applications, whereas practical mobile assistance frequently demands long-horizon planning and cross-application workflows. Second, existing benchmarks often assume fully-specified instructions, neglecting realistic scenarios where users provide vague or incomplete requests. In real-world deployments, agents must proactively engage in clarification dialogues to acquire missing information. Third, existing mobile-use benchmarks overlook the growing importance of external tool invocation. The Model Context Protocol (MCP) (Anthropic, 2024) has rapidly emerged as standardized interface exposing agents to external tools beyond traditional GUI operations. The community is actively debating the integration of GUI operations alongside MCP tools (Wang et al., 2025), recognizing that next-generation mobile agents must seamlessly combine interface manipulation with external tool invocations to solve tasks that extend beyond traditional smartphone capabilities. However, no existing benchmark evaluates this hybrid execution paradigm. To address these limitations, we introduce MobileWorld, substantially more challenging mobile-use online benchmark designed to better reflect real-world mobile usage. Our first contribution is substantial increase in task complexity. As illustrated in Figure 1, MobileWorld features higher difficulty than AndroidWorld across multiple dimensions: under the agentic framework equipped with GPT-5, MobileWorld tasks require on average 27.8 completion steps, nearly twice as many as 14.3 steps required in AndroidWorld. Moreover, 62.2% of tasks involve cross-application workflows compared to only 9.5% in AndroidWorld. These tasks demand challenging capabilities including long-horizon multi-step planning across subtasks, memory retention of past interactions, and precise instruction following. The experimental results clearly reflect this increased difficulty: whereas the best agents achieve success rates exceeding 90% on AndroidWorld, the top-performing agentic framework reaches only 51.7% on MobileWorld, highlighting substantial room for future improvement. Our second contribution is the introduction of agent-user interaction tasks that evaluate an agents ability to handle ambiguous instructions through collaborative dialogue. As shown in the left side of Figure 2, consider the user request: Send an email to Kevin with the message Hello. However, Kevins email address is not present in the devices Email or Contacts app. In such cases, the agent must recognize the missing information and proactively trigger an ask_user action to obtain the necessary details, rather than hallucinating or failing silently. Inspired by Yao et al. (2024), we employ user agent role-played by 2 Figure 2: Beyond traditional GUI-only tasks, MobileWorld includes agentuser interaction tasks and MCP-augmented tasks, each with distinct deterministic evaluation strategies. Left: An example of an agentuser interaction task, in which the agent must proactively request clarification from simulated user when encountering incomplete information. GPT-4.1based simulated user agent is then triggered to provide the requested information, which is embedded in its system prompt. Task completion is verified through the applications callback cache. Right: An example of an MCP-augmented task, where the agent is initialized with list of GitHub MCP tools and selects the appropriate tool to retrieve README content from GitHub repository before completing the task via GUI operations. Task completion is verified through backend database inspection. LLM (GPT-4.1 in the example), which holds the key information in its context, and interacts with the GUI agent. This task type tests whether agents can identify the ambiguity from the instruction and engage in clarification dialogues. 22.4% of all tasks involve agent-user interaction in MobileWorld. Our third contribution is the incorporation of MCP-augmented tasks that require hybrid-usage of the GUI navigation and external tool invocations. As illustrated in the right side of Figure 2, an agent is tasked with fetching GitHub repositorys README and sending its summarization via team collaboration software (Mattermost). Instead of navigating the repository through web browser via sequence of GUI actions, the agent can invoke an MCP tool to directly retrieve the content. After obtaining the relevant information from the GitHub MCP tool, the agent continues the task through GUI interactions to compose and send the message. With the rapid adoption of MCP as standardized protocol for tool integration, evaluating an agents strategic choice between GUI operations and API-based tools has become essential. MCP-augmented tasks constitute 19.9% of all tasks in MobileWorld. Our fourth contribution is reproducible, end-to-end evaluation infrastructure featuring deterministic verification. As summarized in Table 1, prior benchmarks for commercial applications such as Gmail or 3 Table 1: Comparison of online Mobile GUI Agent Benchmarks. MobileWorld uniquely enables deterministic evaluation even for applications requiring backends (e.g., messaging) by utilizing self-hosted environments. We also introduce novel task paradigms: agent-user interaction and Model Context Protocol (MCP) augmentation. Backend-Req. Apps: Environment includes third-party apps requiring external backend authentication. ( = Supported, = Not Supported). Benchmark #Apps #Tasks AndroidArena A3 Pro-Bench AndroidDaily SPA-Bench MobileAgentBench AndroidLab AndroidWorld MobileWorld (Ours) 16 20 34 48 66 10 9 20 221 201 200 235 340 100 138 116 201 Agent-User Int. Tasks MCP-Aug. Tasks Backend-Req. Apps Deterministic Evaluation YouTube suffer from inherent limitations, including mandatory authentication protocols and opaque internal states (Chai et al., 2025; Chen et al., 2024; Yang et al., 2025; Yan et al., 2025a). These restrictions frequently necessitate \"MLLM-as-a-judge\" evaluation, which introduces stochasticity and noise into the assessment process. We address these challenges by substituting proprietary services with productiongrade, self-hosted open-source alternatives (e.g., Mattermost as Slack alternative). By modifying source code and gaining direct access to backend databases, we establish fully observable and controlled environment. Our framework employs multi-faceted validation suite comprising textual response verification, backend database inspection, local storage analysis, and application-specific callbacks to ensure absolute determinism. Finally, by leveraging Docker-based Android Virtual Devices (AVDs) and snapshot-based state management, we provide standardized, \"push-button\" evaluation protocol designed for rigorous community benchmarking. Our fifth contribution is systematic empirical study on MobileWorld that characterizes the performance frontiers of contemporary mobile agents. To address the unique requirements of our benchmark, we propose planner-executor agentic framework as competitive baseline. We extend the primitive action space by introducing ask_user for disambiguation dialogues and mcp_call for structured tool invocation. This architecture enables the seamless unification of GUI manipulation, multi-turn user interaction, and external tool use within single closed-loop decision process. As demonstrated in Table 6, our framework achieves state-of-the-art (SOTA) success rate of 51.7% on MobileWorld. Crucially, our evaluation of existing end-to-end GUI agents reveals stark capability collapse on novel task categories: most baseline models score below 10% on agent-user interaction and near 0% on MCP-augmented tasks. These findings underscore fundamental deficiency in current architectures regarding active user interaction and tool-use. Finally, rigorous error analysis identifies five open research challenges for the community: (i) user ambiguity detection and clarification engagement, (ii) MCP context management, (iii) long-term memory and state checking, (iv) complex logic reasoning and (v) spatial-temporal context awareness. Taken together, MobileWorld aims to serve the community by providing rigorous and reproducible environment for advancing next-generation mobile agents systems that must excel at long-horizon reasoning, active user interaction, and MCP tool use. By characterizing the performance frontiers of contemporary models, we reveal significant capability gaps and identify core research challenges, offering the community clear roadmap toward more autonomous and user-aware mobile intelligence."
        },
        {
            "title": "2 Related Works",
            "content": "We review existing benchmarks for GUI agents and recent work on agent-user interaction and MCP augmentation, highlighting the gaps that MobileWorld addresses. GUI Agent Benchmarks The development of autonomous agents capable of controlling computers has spurred the creation of various benchmarks across different platforms. For desktop environments, benchmarks such as WindowsAgentArena (Bonatti et al., 2024), OSWorld (Xie et al., 2024), and WebArena (Zhou et al., 2023) evaluate agents on operating system tasks and web browsing scenarios. In the mobile domain, several benchmarks have emerged to evaluate Android agents. AndroidWorld (Rawles et al., 2024) introduces fully functional Android environment with 116 programmatic tasks across 20 real-world apps, featuring dynamic task construction with parameterized natural language instructions. AndroidLab (Xu et al., 2025) provides systematic framework supporting both LLMs and multimodal models. More recently, Android Agent Arena (A3) (Chai et al., 2025) addresses some limitations by incorporating 20 widely used third-party commercial apps and 201 tasks with real-time online information retrieval. Other notable efforts include SPA-Bench (Chen et al., 2024), ProBench (Yang et al., 2025), and AndroidDaily (Yan et al., 2025a), which further expand task coverage and evaluation methodologies. While these benchmarks have significantly advanced mobile agent research, they face critical limitations summarized in Table 1: performance saturation on simpler tasks, the assumption of fully-specified instructions, lack of external tool integration, and trade-off between realism and deterministic verifiability when using commercial applications. Agent-User Interaction and MCP-Augmented Benchmarks Beyond standard GUI operations, recent work has recognized the importance of evaluating agents on their interaction capabilities and tool usage. τ-bench (Yao et al., 2024) introduces dynamic conversations between simulated users and agents equipped with domain-specific API tools and policy guidelines, revealing that even state-of-the-art models like GPT-4o succeed on fewer than 50% of tasks and exhibit low consistency. This highlights the challenge of building agents that can reliably follow rules and handle ambiguous requests. Building on this, τ2-bench (Barres et al., 2025) models dual-control environment where both agent and user actively modify shared world states, demonstrating significant performance drops when agents must coordinate with and guide users rather than operate autonomously. Meanwhile, MCP has emerged as mechanism for agents to invoke external tools alongside GUI operations. OSWorld-MCP (Jia et al., 2025) presents the first benchmark assessing tool invocation, GUI operation, and decision-making in real-world environments, with 158 high-quality tools across seven applications. Their evaluation shows that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3), though tool invocation rates remain relatively low at 36.3%. Similarly, MCPWorld (Yan et al., 2025b) is benchmark for API, GUI, and hybrid agents using \"white-box apps\" with source code availability, enabling programmatic verification through dynamic code instrumentation and achieving 75.12% task completion accuracy. While these benchmarks advance evaluation of agent-user interaction and tool usage, they operate in domains separate from mobile GUI agents. MobileWorld is the first mobile benchmark to integrate both agent-user interaction tasks and MCP-augmented tasks within unified evaluation framework for practical mobile agent deployment."
        },
        {
            "title": "3 MobileWorld",
            "content": "In this section, we formalize the task definition, describe the environment architecture and detail the benchmark construction methodology. 5 Table 2: Complete action space supported by MobileWorld. Action Parameters Description GUI Operations click double_tap long_press drag input_text scroll x, x, x, start_x, start_y, end_x, end_y Drag from start to end coordinates text direction Tap at the specified coordinates Double-tap at the specified coordinates Long-press at the specified coordinates Type text into the focused field Scroll in the specified direction (up/down/left/right) Navigation navigate_home navigate_back keyboard_enter wait Return to the home screen Navigate to the previous screen Press the enter key Wait for screen content to update Task Control answer status Extended Actions ask_user mcp_call text goal_status Provide textual response to the user (for IR tasks) Mark task as complete or infeasible text tool_name, params Request clarification from the user Invoke an MCP tool with specified parameters"
        },
        {
            "title": "3.1 Task Definition",
            "content": "A standard Mobile GUI agent task can be formalized as Partially Observable Markov Decision Process (POMDP) (S, O, A, , R), where is the state space representing the mobile environment; is the observation space, including natural language instruction and interface representations such as screenshots; is the action space of standard mobile UI operations (see Table 2); : is the transition function; and : {0, 1} is binary reward indicating task completion. GUI-Only Tasks Following AndroidWorld (Rawles et al., 2024), GUI-Only tasks comprise task completion and information retrieval, requiring the agent to accomplish user objectives through standard mobile UI operations. Task Completion involves executing sequence of GUI actions to drive the system into specified target state (e.g., composing and sending an email), while Information Retrieval requires locating and gathering relevant information from internal mobile data or external sources (e.g., answering questions about todays scheduled events). Agent-user Interaction Tasks Previous mobile GUI agent benchmarks assume that user instructions are clear and unambiguous. However, this assumption rarely holds in real-world scenarios, where users often provide vague or ambiguous instructions, necessitating additional information for GUI agent to narrow down the solution space. To reflect this practical challenge, we introduce dedicated set of tasks in which critical information is deliberately omitted from the task goal, rendering successful completion impossible without actively seeking clarification. MCP-augmented Tasks MCP provides more straightforward way to access wider range of resources and functionalities beyond standard GUI operations, enabling agents to obtain critical information and complete tasks more effectively (Jia et al., 2025). However, in real-world scenarios, not all applications are equipped with well-designed MCP interfaces, and the GUI remains the predominant mode of interaction. Given this co-existence of GUI-based interaction and MCP-enabled tool access, we specifically design set of tasks augmented with MCP tools. These tasks require agents to integrate MCP tool invocations with standard GUI operations to succeed. 6 Figure 3: The system architecture of MobileWorld consists of two main components. Left: the host machine is where GUI agents receive task instructions and optionally interact with users for clarification, then choose between GUI actions or MCP tool calls to complete tasks. Right: the docker environment contains an isolated Android ecosystem with emulators, self-hosted app backends, and an evaluator that verifies task completion through text matching, backend database, local storage, and app callbacks."
        },
        {
            "title": "3.2 Environment Development",
            "content": "As illustrated in Figure 3, MobileWorld comprises two main components: the host machine running GUI agents and the containerized environment. We detail the key design elements below. Action Space MobileWorld supports hybrid action space that combines standard GUI operations with agent-user interaction and MCP tools invocation. Table 2 shows the complete action specification. GUI Operations. The core action space includes standard mobile interface manipulations: click, double_tap, long_press, drag, input_text, scroll, and navigation commands (navigate_home, navigate_back). Additionally, agents can use answer to respond to information retrieval queries, wait to allow screen updates, and status to signal task completion or infeasibility. Agent-User Interaction. We augment the action space with an ask_user action that enables agents to proactively request clarification. When invoked, queries are routed to user agent, implemented with standard LLM (e.g., GPT-4.1). It is conditioned on the context information omitted from task instructions. This design enables systematic evaluation of an agents ability to recognize knowledge boundaries and engage in collaborative dialogue. MCP Tool Integration. Agents can invoke external tools via the mcp_call action. We curate collection of popular MCP servers from the Bailian platform1, spanning diverse domains including geospatial navigation (Amap), code repository analysis (GitHub), document processing (Jina), financial data retrieval (StockStar), and scholarly literature fetching (arXiv). These 61 tools enable functionalities that significantly exceed the capabilities of standard GUI operations, such as retrieving commit histories or querying real1https://bailian.console.aliyun.com/?tab=mcp#/mcp-market 7 Table 3: MCP tools integrated in MobileWorld. These tools provide advanced capabilities beyond standard GUI operations, enabling agents to complete complex cross-domain tasks. MCP Server # Tools Description Amap Maps GitHub Jina AI Stockstar arXiv 26 16 4 Provides comprehensive geospatial services including geocoding, reverse geocoding, IP location, weather queries, and multi-modal route planning (cycling, walking, driving, public transit). Supports distance measurement and location-based search (keyword, nearby, and detail queries). Official GitHub integration offering advanced automation and interaction capabilities for developers. Enables repository querying, commit history retrieval, issue tracking, and code analysis. Document processing and search capabilities powered by Jina AI, supporting multi-modal content analysis and retrieval tasks. Financial intelligence service providing comprehensive data for A-share and Hong Kong stocks. Includes fundamental data, derived metrics, financial analysis, and business model evaluation. Academic paper search and retrieval from the arXiv repository, enabling agents to access and process scholarly literature. time stock data. The complete tool catalog is provided in Table 3. Stable and Reproducible Environment MobileWorld achieves stability and reproducibility through comprehensive containerization and careful engineering. Containerized Architecture. The entire evaluation environment is encapsulated in Docker-in-Docker containers (right side in Figure 3), including rooted Android Virtual Device (AVD), self-hosted application backends, and an API server for orchestration. This design eliminates external dependencies and enables consistent deployment across different host systems. Open-Source Applications. We construct stable, reproducible application environments based on popular open-source projects, including Mattermost (enterprise communication, an open-source alternative to Slack), Mastodon (social media, an open-source alternative to X/Twitter), and Mall4Uni (e-commerce platform). We fork these projects and apply moderate modifications to ensure compatibility within the Docker environment. In addition, self-hosting these applications provides full backend access, enabling precise control over task initialization states and deterministic verification of task outcomes. More details about the open-source applications can be referred in Section B.2. Snapshot-Based State Management. To ensure consistent initial conditions across evaluation runs, we employ AVD snapshots that capture the complete device state. Each task execution begins from predetermined snapshot, guaranteeing reproducible starting conditions. Task Evaluation Deterministic evaluation is essential for reliable benchmarking. We implement multiple complementary verification methods to assess task success: Textual Answer Verification. For information retrieval tasks where the expected output is specific textual response (e.g., string or numerical value), we employ pattern matching with regular expressions or exact string comparison against ground-truth answers. Backend Database Verification. For tasks involving self-hosted applications such as Mattermost or Mastodon, we directly query the backend database to verify that expected state changes (e.g., sent messages, created posts) have occurred. This provides ground-truth validation independent of UI state. Local Storage Inspection. Leveraging rooted emulator access, we use Android Device Bridge (ADB) to inspect application-specific local storage and verify internal state. For example, calendar events can be validated by directly examining the Fossify Calendar database, while email drafts can be retrieved from the Mail applications local storage. 8 Table 4: The example tasks with their evaluation modes and corresponding verification logics. Task Eval Mode Eval Logic want to drive to Tianjin. Please check the driving distance in kilometers. Response only one integer number. No other text. Reply to the toot of gourmet user about Greek food Moussaka, and the reply content should be Nice sharing, love it. Textual Answering Verification Compare the agent answer text with the ground-truth distance. Backend Database Verification Fetch the reply content from the Mastodon backend database and compare with the ground-truth content. Set weekend alarm for 8:25 a.m. with the ringtone beebeep and vibration off. Local Storage Inspection Leverage adb command to query the local storage of Alarm app. want to remove some electronic products in the shopping cart of the TaoDian app. Application Callbacks Implement callback APIs that capture the cart item changes in Taodiao app and persist in local file for evaluation. Application Callbacks. For tasks involving lightweight customized applications, we implement callback APIs that capture intermediate states during execution. These states are persisted and subsequently queried by the evaluator to determine task success. Table 4 lists some examples of tasks and demonstrate how conduct deterministic evaluation on them. This verification system ensures that task outcomes are evaluated deterministically, eliminating the evaluation noise inherent in MLLM-as-a-judge approaches used by prior work."
        },
        {
            "title": "3.3 Benchmark Construction",
            "content": "Task Instructions and Scenarios To ensure our benchmark reflects diverse and realistic mobile usage scenarios, we curate tasks from broad spectrum of real-world domains, including e-commerce, social platform, productivity collaboration, on-device system management, and information retrieval. All tasks are constructed based on the MobileWorld initial system state, which is preloaded by contacts, SMS messages, emails, calendar events, and files. The preloaded materials are either synthesized by large language models (LLMs) or derived from the internet. To increase task complexity, we instruct annotators to design long-horizon tasks that integrate multiple challenging dimensions. These include: (1) combining subgoals into single objective spanning multiple apps; (2) requiring fine-grained visual recognition (e.g., extracting data from complex PDF layouts); (3) demanding memory retention across steps (e.g., using prior search results in follow-up email); (4) involving numerical or logical reasoning (e.g., computing total price of cart items); (5) relying on implicit temporal or spatial context (e.g., inferring tomorrow from the system clock); and (6) enforcing precise instruction following (e.g., responding with only number). Together, these dimensions help expose capability gaps of GUI agents in realistic mobile scenarios. Agent-User Interaction Task Construction To construct agent-user interaction tasks, we first ask human annotators to write clear and achievable task goal (e.g., Send an email to kevin_zhang@example.com with the message Hello.). Next, they remove critical information from the instruction such as replacing the full email address with only name like Kevin making the task ambiguous or incomplete. This forces GUI agents to actively request clarification to uncover the missing details. To prevent unintended information leakage, annotators also verify that the device environment does not contain the omitted information (e.g., ensuring Kevin is not already saved in Contacts app with linked email). To respond to GUI agents clarification requests, we deploy LLM-simulated user agent, separate LLM configured with the full task goal and relevant background context. For those user-interaction tasks, the user agent is injected with the exact information intentionally omitted from the original instruction (e.g., the Kevins email address in the above example). In this way, the user agent is able to respond accurately to GUI agents requests. In addition, the user agent is instructed not to assist beyond this scope, i.e., if 9 Table 5: Key statistics in MobileWorld. Statistic Total tasks Category Breakdown GUI-Only Tasks Agent-User Int. Tasks MCP-Aug. Tasks App Complexity Single App Tasks Two Apps Tasks Three Apps Tasks Evaluation Mode App Callbacks Textual Matching Storage Inspection DB Verification Total MCP Servers Total MCP Tools Number 201 (100%) 116 (57.7%) 45 (22.4%) 40 (19.9%) 76 (37.8%) 100 (49.8%) 25 (12.4%) 10 (5.0%) 22 (10.9%) 74 (36.8%) 95 (47.3%) 5 Figure 4: Scenario Distribution. The benchmark predominantly features third-party applications (95%), with system apps comprising the remaining 5% of tasks. GUI agents ask irrelevant or off-task questions, the user agent should refuse to help. Full prompt for the user agent is provided in the following block. User Agent System Prompt # System Prompt: You are acting as mobile phone user. An mobile GUI agent is executing task on your phone. The task goal is: {self.goal}. You need to answer questions from the mobile GUI agent. The relevant information for the task is: {self.relevant_information}. If the question is not related to the task or no more task-related information is available, you need to refuse to answer in polite manner. DO NOT make up any information. You can ONLY give the answer based on the relevant information and the task goal. Today is 2025-10-16, Thursday. If the question is about the date, you need to answer the correct date based on the current date. MCP-Augmented Task Construction Given the selected MCP servers  (Table 3)  , annotators are instructed to read each tools description and sample output. They then create standalone tasks that can be completed using only MCP tools (e.g., List all commits in the main branch of the mastodon/mastodon repository from the past week via the GitHub MCP). These tasks are non-trivial or error-prone via pure-GUI operations. Subsequently, annotators extend these into hybrid workflows by appending one or more GUI-based actions that consume the MCP output. For example, after retrieving commit history via GitHub MCP tool calling, the agent is instructed to compose and send an email summarizing the changes using the native Email app. Human-in-the-Loop Task Validation Once tasks are constructed, annotators manually validate them and ensure that all tasks can be successfully completed by GUI agent. Specifically, annotators initialize the task environment and receive the task goal. They then interact directly with the MobileWorld environment, executing actions until they believe the task has been fulfilled. Each completed attempt is subsequently evaluated by the evaluator (see Section 3.2). score of 1.0 confirms that the task is solvable under the given conditions. If the score is 0.0, the validator retries the task, up to maximum of five attempts. If the task remains unsolved after five trials, this indicates potential issue in the task initialization, instruction clarity, or evaluation criteria. Such tasks are flagged and returned to the design phase for revision and reimplementation."
        },
        {
            "title": "3.4 Data Statistics",
            "content": "We summarize key statistics of MobileWorld in Figure 4 and Table 5, highlighting four core dimensions. (1) Tasks span diverse domains, including communication, productivity, and navigation. Approximately 95% of tasks involve third-party applications, aligning with authentic mobile usage patterns and ensuring the benchmarks real-world relevance. (2) The benchmark emphasizes cross-app complexity: 62.2% of tasks require coordination between multiple apps, and 12.4% involve three or more. (3) Agentuser interaction tasks and MCP-augmented tasks account for 42.3% of the dataset. These tasks go beyond pure GUI navigation by requiring dynamic user engagement and external tool use. (4) Multiple verification methods are used to assess task success: 47.3% of tasks rely on self-hosted database verification, and 36.8% use local storage inspection, highlighting the importance of apps with fully controlled backends."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct comprehensive experiments to evaluate state-of-the-art models on MobileWorld and analyze their capabilities across different task categories."
        },
        {
            "title": "4.1 Agentic Framework for MobileWorld",
            "content": "To accommodate with the new features in MobileWorld, we develop planner-executor agentic framework. It consists of two core components: planner and grounding executor (China Mobile Jiutian Team, 2025). The planner is responsible for interpreting the task instruction, analyzing the current screenshot along with the interaction history, and deciding the next action, such as click, swipe, wait or type. When the planned action involves clicking (e.g., click or long_press), the planner does not output coordinates directly; instead, it generates natural language description of the target UI element (e.g., the Send button at the bottom right corner or the email thread titled Lunch Meeting.). This description is then passed to the grounding executor, which takes the current screenshot as input and predicts the precise pixel coordinates to perform the click or long press. In this way, any general-purpose VLMs can be integrated into our framework, without requiring them to possess pixel-level grounding capabilities. To support the unique requirements of MobileWorld, we extend the action space with two special operations: ask_user and mcp_call. When the planner issues an ask_user action, the system forwards the query to the llm-simulated user agent; the user agents textual response along with the current screenshot is appended to the interaction history. For MCP-augmented tasks, we preload the specifications of the involved MCP tools into the system prompt. When the planner triggers mcp_call, it specifies the MCP tool name and fills in the required parameters. The MCP invocation request is then sent to the MCP server, which returns structured output. This result, along with the current screenshot, is injected into the interaction history as the systems observation for that step. This design enables seamless integration of external tool invocation, user clarification, and GUI operation within unified decision loop. The details of our agentic framework can be referred in Section A."
        },
        {
            "title": "4.2 Metrics",
            "content": "We define the following metrics to measure the effectiveness and efficiency of GUI agents on MobileWorld. Success Rate (SR) measures the proportion of tasks successfully completed by the agent. For each task in the MobileWorld, binary score si {0, 1} is assigned, where si = 1 if the task objective is fully achieved and si = 0 otherwise. The success rate is computed as: SR = 1 i=1 si. (1) We also report success rates for specific task categories. The GUI-Only SR is computed over GUI-Only tasks, the Interaction SR is computed over agent-user interaction tasks in Iinteract, and the MCP SR is 11 computed over MCP-augmented tasks in IMCP: SRGUI-Only = 1 IGUI-Only iIGUI-Only si, SRinteract = 1 Iinteract iIinteract si, SRMCP = 1 IMCP iIMCP si. (2) Average Completion Steps (Ave. Steps) denotes the average number of action steps taken across all execution trajectories. Let ti be the number of steps in the trajectory for task (including both successful and failed episodes). The average completion steps is defined as: Ave. Steps = 1 i=1 ti. (3) Under comparable success rates, lower value reflects greater execution efficiency. Average User Queries (Ave. Queries) measures the average number of ask_user actions invoked by the agent across agent-user interaction tasks. Let ci denote the number of ask_user actions for task Iinteract. This metric is defined as: Ave. Queries = 1 Iinteract iIinteract ci. (4) This metric reflects how frequently agents seek user clarification, with optimal agents achieving task success while minimizing unnecessary queries. User Interaction Quality (UIQ) evaluates both the effectiveness and efficiency of the agents ask_user invocations. Let Iinteract denote the set of agent-user interaction task indices, and let Itriggered denote the set of non-interaction task indices where the agent invoked at least one ask_user action. For each task i, let ci denote the number of ask_user actions invoked and si {0, 1} the task success indicator. The quality score for task Iinteract is computed as: (5) qi = si ci 0 if ci > 0, if ci = 0. The overall UIQ score is then defined as: UIQ = iIinteract Iinteract + Itriggered qi . (6) This metric rewards agents that successfully complete interaction tasks with fewer queries, while penalizing agents that fail to seek necessary clarification or unnecessarily invoke ask_user on non-interaction tasks. Higher UIQ indicates the agent appropriately recognizes when user clarification is needed and efficiently resolves ambiguities. Average MCP Tool Calls (Ave. MCP Calls) measures the average number of MCP tool invocations by the agent across MCP-augmented tasks. Let mi denote the number of MCP tool calls for task IMCP. This metric is defined as: Ave. MCP Calls = mi. (7) 1 IMCP iIMCP This metric indicates the agents propensity to leverage external tools, with effective agents appropriately integrating MCP tool access into their task execution workflows."
        },
        {
            "title": "4.3 Experimental Setup",
            "content": "We evaluate suite of state-of-the-art Large Multimodal Models (LMMs), including Qwen3-VL-235BA22B-Instruct (Qwen Team, 2025), Qwen3-VL-32B-Instruct (Qwen Team, 2025), GUI-Owl-32B (Ye et al., 2025), UI-Venus-72B (Gu et al., 2025), Doubao-1.5-UI-TARS (Seed, 2025), GELab-Zero (Team, 2025), OpenAI GPT-5 (OpenAI, 2025), Claude-4.5-Sonnet (Anthropic, 2025), and Gemini-3-Pro (Comanici et al., 12 Table 6: Success rate (%) comparison of state-of-the-art mobile GUI agent models on MobileWorld under maximum 50 steps. We report overall SR and breakdown by task category: GUI-Only tasks, agent-user interaction tasks, and MCP-augmented tasks. The number of tasks is indicated in the corresponding column. Bold indicates the best result and underline indicates the second best. Category Model Overall GUI-Only (116) User-Int. (45) MCP (40) Agentic Framework Claude-4.5-Sonnet + UI-Ins-7B Gemini-3-Pro + UI-Ins-7B GPT-5 + UI-Ins-7B End-to-End Model GUI-Owl-7B GUI-Owl-32B UI-Venus-7B UI-Venus-72B Qwen3-VL-8B Qwen3-VL-32B Qwen3-VL-235B-A22B GELab-Zero-4B Doubao-1.5-UI-TARS 43.8 46.3 51.7 4.5 5.5 5.5 10.4 5.5 9.0 9.5 10.9 20. 47.8 55.6 54.0 7.7 8.5 8.5 16.4 9.4 11.9 12.8 16.1 26.3 37.8 24.4 62.2 - - - - 0.0 6.7 4.4 6.7 32.4 50.0 48.6 51.6 - - - - 0.0 2.7 5.4 - - 2025). To enable consistent comparison of performance across models, we integrate general-purpose LMMs, such as GPT-5 and Claude-4.5-Sonnet, into the planner-executor agentic framework introduced in Section 4.1. In this setup, LMMs serve as the planner, while UI-Ins-7B (Chen et al., 2025), state-ofthe-art grounding model, acts as the executor to carry out coordinate-based actions. For end-to-end GUI agent models, inlcuding Doubao-1.5-UI-TARS, GUI-Owl, UI-Venus, GELab-Zero and Qwen3-VL, we employ their official implementations and adapt them to the MobileWorld environment. All models are evaluated with maximum of 50 steps using metrics defined in Section 4.2. Agentic frameworks and Qwen3-VL models are evaluated on all task categories. MCP tools are integrated into the system prompt following the tool call format for agentic frameworks, while Qwen3-VL models adopt the OSWorld-MCP format (Jia et al., 2025). Additionally, we augment the Qwen3-VL action space with ask_user action to enable evaluation on agent-user interaction tasks. Other end-to-end models are evaluated only on compatible tasks: MCP-augmented tasks are excluded due to lack of tool invocation support, and agent-user interaction tasks are excluded for models without user query actions (e.g., GUI-Owl, UI-Venus). Open-source models are deployed using vLLM (Kwon et al., 2023) on server with 8NVIDIA H20 GPUs. Proprietary models including GPT-5, Claude-4.5-Sonnet, Gemini-3-Pro, and Doubao-1.5-UI-TARS are accessed through their official API. The temperature is set to 0.0. The user agent for agent-user interaction tasks is implemented using GPT-4.1."
        },
        {
            "title": "4.4 Main Results",
            "content": "Table 6 presents the success rate comparison across all evaluated models under different task categories. Several key observations emerge from our results. First, MobileWorld presents significant challenge even for state-of-the-art models. The best-performing model, GPT-5 + UI-Ins-7B, achieves only 51.7% overall success rate, substantially lower than the 90%+ success rates reported on AndroidWorld (Rawles et al., 2024). This confirms that our benchmark effectively addresses the saturation problem in existing mobile agent evaluation. Second, there exists substantial performance gap between agentic frameworks and end-to-end GUIspecialized models. Agentic frameworks leveraging frontier LLMs (e.g., GPT-5, Claude-4.5-Sonnet and Gemini-3-Pro) consistently outperform end-to-end models by large margin. The best agentic model achieves an overall success rate of 51.7%, compared to only 20.9% for the best end-to-end model (Doubao1.5-UI-TARS). This gap stems from two key limitations of current end-to-end approaches: (1) insufficient 13 Table 7: Detailed metrics comparison on MobileWorld under maximum 50 steps. We report Success Rate (SR), Average Completion Steps (Steps), Average User Queries (Queries), User Interaction Quality (UIQ), and Average MCP Calls (MCP). Bold indicates the best result and underline indicates the second best. Category Model SR (%) Steps Queries UIQ MCP Agentic Framework Claude-4.5-Sonnet + UI-Ins-7B Gemini-3-Pro + UI-Ins-7B GPT-5 + UI-Ins-7B End-to-End Model GUI-Owl-7B GUI-Owl-32B UI-Venus-7B UI-Venus-72B Qwen3-VL-8B Qwen3-VL-32B Qwen3-VL-235B-A22B GELab-Zero-4B Doubao-1.5-UI-TARS 43.8 46.3 51.7 4.5 5.5 5.5 10.4 5.5 9.0 9.5 10.9 20.9 26.6 24.2 27.8 20.6 24.0 26.7 34.2 24.8 27.1 26.9 29.9 20.9 0.76 0.36 1. - - - - 0.04 0.00 0.00 0.37 1.22 0.25 0.19 0.40 - - - - 0.00 0.00 0.00 0.02 0.13 1.91 2.63 2.23 - - - - 2.32 3.84 2.38 - - capability in handling the complex reasoning and cross-app coordination required by MobileWorld, and (2) lack of support for agentuser interaction and MCP tool invocation. Third, agent-user interaction and MCP-augmented tasks prove particularly challenging and reveal critical capability gaps. On agentuser interaction tasks, Qwen3-VL and GELab-Zero achieve scores below 10%, while Doubao-1.5-UI-TARS achieves only 32.4%. This highlights the difficulties in recognizing when user clarification is needed. In contrast, GPT-5 demonstrates strong performance (62.2%) on these tasks, suggesting that advanced reasoning capabilities are essential for effective human-agent collaboration. Similarly, MCP-augmented tasks expose fundamental limitations in tool integration. Qwen3-VL is unable to properly utilize MCP tools when required to coordinate their invocation with GUI actions. On the other hand, among agentic frameworks, the best performance reaches only 51.6% (GPT-5). This reveals that current models struggle to effectively orchestrate between GUI actions and external tool invocations. 4.5 In-depth Analysis Table 7 presents comprehensive analysis of agent behavior beyond task success rates. Several notable patterns emerge from examining execution efficiency and interaction quality. Execution Efficiency Among all evaluated models, Gemini-3-Pro + UI-Ins-7B achieves the lowest average completion steps (24.2), followed by Claude-4.5-Sonnet + UI-Ins-7B (26.6) and GPT-5 + UI-Ins-7B (27.8). Interestingly, Doubao-1.5-UI-TARS and GUI-Owl-7B also exhibit relatively low step counts (20.9 and 20.6 respectively), but this is largely attributable to their lower success ratesthese models often terminate early due to failure rather than efficient task completion. In contrast, UI-Venus-72B requires the highest average steps (34.2) while achieving only 10.4% success rate, indicating inefficient behavior. User Interaction Quality The Average Queries and UIQ metrics jointly reveal how models manage agentuser interaction tasks. GPT-5 demonstrates the most effective clarification behavior, with an average of 1.11 queries per interaction task corresponding to its highest user-interaction success rate (62.2%) and UIQ (0.40). Claude-4.5-Sonnet follows, averaging 0.76 queries and achieving UIQ of 0.25. In contrast, Gemini-3-Pro got lower UIQ (0.19) and asks very few queries on average (0.36), even though it performs well on GUI-only tasks (55.6%). This suggests it under-utilizes user clarification even when beneficial. Notably, UIQ scores are consistently lower than User-Interaction SR across all models, indicating the presence of unnecessary ask_user operations that inflate query counts without contributing to task success. In particular for Doubao-1.5-UI-TARS, User-Interaction SR of 32.4% was achieved with 1.22 average queries, but the model obtains lower UIQ of only 0.13. This gap suggests that Doubao-1.5-UI-TARS frequently issues redundant or ineffective clarification requests. 14 Figure 6: Hallucination without user clarification: representative failure case showing how the mobile GUI agent hallucinates actions when faced with ambiguous scenarios that require user clarification. MCP Tool Integration For MCP-augmented tasks, Gemini-3-Pro leads in average tool invocations (2.63 calls per task), followed by GPT-5 (2.23) and Claude-4.5-Sonnet (1.91). This higher tool utilization correlates with stronger MCP SR performance, indicating that successful MCP task completion requires agents to actively and appropriately leverage external tools. The average MCP Tool calls of Qwen3VL (2.32 3.84) is comparable to that of the aforementioned models. However, due to inaccuracies in generated tool names and arguments, large proportion of tool invocation failed, leading to significantly lower MCP success rate (0.0 5.4). Other end-to-end models universally fail to invoke MCP tools, as they lack the architectural support for tool integration. These findings highlight that effective mobile agents require not only strong task completion capabilities but also appropriate utilization of interactive clarification and external tool integration, where current end-to-end approaches show significant room for improvement. Completion Step Comparsion with AndroidWorld We compared the task completion steps with 50 max steps under the GPT-5 + UI-Ins-7B agentic framework on MobileWorld and AndroidWorld, respectively. As shown in Figure 5, MobileWorld exhibits significantly higher complexity compared to AndroidWorld in terms of task length and step distribution. While AndroidWorld tasks are predominantly completed within 15 steps (with an average of 14.3 steps), MobileWorld tasks require substantially more actions, with 27.8 average steps. The distribution of MobileWorld tasks is notably rightskewed, with large proportion requiring over 20 steps, reflecting its focus on long-horizon workflows involving GUI interactions, user clarification, and MCP tool calls."
        },
        {
            "title": "4.6 Research Challenge Analysis",
            "content": "To better understand the limitations of current GUI agents, we manually inspect failed task trajectories Figure 5: Comparison of completion steps between AndroidWorld and MobileWorld. 15 Figure 7: Context overflow from MCP tool responses: failure case demonstrating ineffective MCP tool integration due to context management issues, where tool responses exceed the context window capacity. across evaluated models and categorize representative cases. This analysis reveals several challenges that current agents must overcome to achieve reliable real-world performance. Challenge 1: Ambiguity Detection and User Engagement When the model operates without the ability to query the user for clarification, it often generates plausible-sounding but factually incorrect or unsupported responses. This hallucination arises because the model attempts to infer missing or ambiguous information on its own, rather than seeking confirmation. In the example shown in Figure 6, the task instruction is: want to drive to Tianjin, China from my hometown. Please check the driving distance. The GUI agent opens Google Maps and correctly inputs the destination (Tianjin), but it hallucinates Shanghai as the hometown without asking the user to specify the departure city. This incorrect assumption leads to wrong distance calculation and ultimately causes task failure. Allowing interactive clarification significantly reduces these risks by grounding the models actions in verified user intent. Challenge 2: MCP Tool Descriptions and Output Management External tools invoked via MCP may return excessively long outputs that overwhelm the agents context window. As illustrated in Figure 7, when extracting specific benchmark scores from an academic PDF to email comparison summary, the MCP tool returns the entire 20k-token document as raw text. This floods the context with irrelevant content, preventing the agent from locating target information and causing incorrect extractions or downstream failures. Effective MCP integration requires content-aware retrieval strategies and context management mechanisms (Jones & Kelly, 2024). Challenge 3: Long-Term Memory and State Tracking The GUI agent struggles to maintain awareness of actions it has already performed without long-term memory tracking mechanism. As illustrated in Figure 8, the agent is instructed to rename all files in the Downloads folder with the prefix bid_, ordering them by creation date and renaming them uniformly as bid_index.extension. However, after successfully renaming several files, the agent forgot the processed files. In subsequent steps, it attempts to rename these renamed files again. This leads to repeated, conflicting modifications and ultimately results in an incorrect final state. The absence of reliable memory mechanism to track completed subtasks causes the agent to fall into destructive loop, highlighting critical gap in its ability to handle multi-step operations in real-world environments. Challenge 4: Complex Logic Reasoning The GUI agent exhibits limited capability in tasks requiring multi-step logical reasoning and accurate numerical calculation. For instance, in the task Find the three most expensive items in the TaoDian app shopping cart and calculate their total price, the agent must 16 Figure 8: Lack of long-term memory: representative case illustrating insufficient memory mechanisms for tracking multi-step operations, leading to failure in maintaining state across sequential actions. first scan all items in the cart, maintain running record of the top three highest-priced products, and then sum their prices precisely. While the model attempts this process and outputs final number, the result is incorrect, either because it fails to correctly identify the most expensive items (e.g., due to misreading prices or overlooking items) or because it makes arithmetic errors during summation. This reflects broader weakness in handling structured reasoning and exact calculation, which are essential for reliable performance in real-world e-commerce or financial tasks. Challenge 5: Temporal-Spatial Context Awareness The model generally lacks awareness of real-world time and location unless these are explicitly stated in the prompt. In the task Ive received lunch invitation via text message; please reply OK and schedule lunch event tomorrow the agent is expected to determine todays date by observing system UI elements such as the desktop clock and create calendar entry for an appropriate upcoming lunchtime. Most tested agents either hallucinate an arbitrary date or fail to consult time. This inability to ground actions in the actual temporal context leads to incorrectly scheduled events and highlights critical gap in the agents situational awareness. Reliable access to the real-time information on the device is essential for performing scheduling tasks accurately."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced MobileWorld, challenging benchmark for evaluating mobile GUI agents that addresses the saturation of existing benchmarks. MobileWorld advances mobile agent evaluation through complex workflows, agent-user interaction tasks that assess ambiguity recognition, and MCP-augmented tasks requiring orchestration between GUI operations and external tools. Built upon production-grade opensource applications in fully containerized environment, our benchmark enables deterministic evaluation through multiple verification methods. Extensive experiments reveal that state-of-the-art models achieve substantially lower success rates on MobileWorld compared to existing benchmarks, with the best agentic framework achieving only 51.7% success rate and the best end-to-end models reaching just 20.9%. Performance further degrades on agent-user interaction and MCP-augmented tasks, highlighting critical gaps in ambiguity detection, collaborative dialogue, and hybrid execution planning. Future Research Directions Our failure analysis points to several research directions. We identify two possiblilities here. First, foundation model improvements through reinforcement learning can enhance core reasoning capabilities, numerical computation accuracy, and generalization to long-horizon tasks. Second, agentic framework innovations addressing context length limitation for long-term memories, MCP tool management can bridge the gap between model capabilities and practical deployment requirements. We hope MobileWorld will serve as rigorous testbed to drive progress toward mobile agents capable of handling real-world automation complexity."
        },
        {
            "title": "References",
            "content": "Anthropic. What is the model context protocol (mcp)? https://modelcontextprotocol.io/docs/get ting-started/intro, 2024. Anthropic. Claude Sonnet 4. https://www.anthropic.com/claude/sonnet, 2025. [Accessed 31-08-2025]. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. Yuxiang Chai, Hanhao Li, Jiayu Zhang, Liang Liu, Guangyi Liu, Guozhi Wang, Shuai Ren, Siyuan Huang, and Hongsheng Li. A3: Android agent arena for mobile gui agents. arXiv preprint arXiv:2501.01149, 2025. Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents, 2024. Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, et al. Ui-ins: Enhancing gui grounding with multi-perspective instruction-as-reasoning. arXiv preprint arXiv:2510.20286, 2025. China Mobile Jiutian Team. JT-GUIAgent: An advanced gui agent framework with planner-grounder architecture, 2025. URL https://github.com/JT-GUIAgent/JT-GUIAgent. GitHub repository. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 18 Shihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Liujianfeng Liujianfeng, Ang Li, Jian Luan, Bin Wang, Rui Yan, et al. Mobile-bench: An evaluation benchmark for llm-based mobile agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 88138831, 2024. Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, et al. Ui-venus technical report: Building high-performance ui agents with rft. arXiv preprint arXiv:2508.10833, 2025. Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, and Fei Huang. Osworld-mcp: Benchmarking mcp tool invocation in computer-use agents. arXiv preprint arXiv:2510.24563, 2025. Adam Jones and Conor Kelly. Code execution with MCP: Building more efficient agents. Anthropic Engineering Blog, 2024. URL https://www.anthropic.com/engineering/code-execution-with-m cp. Accessed: 2024-12-XX. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. OpenAI. Introducing gpt-5. Technical report, OpenAI, 2025. URL https://openai.com/zh-Hans-CN/in dex/introducing-gpt-5/. Qwen Team. Qwen3-vl: The multimodal large language model series, 2025. URL https://github.com /QwenLM/Qwen3-VL. GitHub repository. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36: 5970859728, 2023. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. GELab Team. Gelab-zero: An advanced mobile agent inference system, 2025. URL https://github.com /stepfun-ai/gelab-zero. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37: 5204052094, 2024. Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. Androidlab: Training and systematic benchmarking of android autonomous agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 21442166, 2025. 19 Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, Shiliang Yang, Zhirui Wang, Brian Li, Kang An, Chenyang Li, Lei Lei, Mengmeng Duan, Danxun Liang, Guodong Liu, Hang Cheng, Hao Wu, Jie Dong, Junhao Huang, Mei Chen, Renjie Yu, Shunshan Li, Xu Zhou, Yiting Dai, Yineng Deng, Yingdan Liang, Zelin Chen, Wen Sun, Chengxu Yan, Chunqin Xu, Dong Li, Fengqiong Xiao, Guanghao Fan, Guopeng Li, Guozhen Peng, Hongbing Li, Hang Li, Hongming Chen, Jingjing Xie, Jianyong Li, Jingyang Zhang, Jiaju Ren, Jiayu Yuan, Jianpeng Yin, Kai Cao, Liang Zhao, Liguo Tan, Liying Shi, Mengqiang Ren, Min Xu, Manjiao Liu, Mao Luo, Mingxin Wan, Na Wang, Nan Wu, Ning Wang, Peiyao Ma, Qingzhou Zhang, Qiao Wang, Qinlin Zeng, Qiong Gao, Qiongyao Li, Shangwu Zhong, Shuli Gao, Shaofan Liu, Shisi Gao, Shuang Luo, Xingbin Liu, Xiaojia Liu, Xiaojie Hou, Xin Liu, Xuanti Feng, Xuedan Cai, Xuan Wen, Xianwei Zhu, Xin Liang, Xin Liu, Xin Zhou, Yingxiu Zhao, Yukang Shi, Yunfang Xu, Yuqing Zeng, Yixun Zhang, Zejia Weng, Zhonghao Yan, Zhiguo Huang, Zhuoyu Wang, Zheng Ge, Jing Li, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, and Daxin Jiang. Step-gui technical report, 2025a. URL https://arxiv.org/abs/2512.15431. Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, et al. Mcpworld: unified benchmarking testbed for api, gui, and hybrid computer use agents. arXiv preprint arXiv:2506.07672, 2025b. Leyang Yang, Ziwei Wang, Xiaoxuan Tang, Sheng Zhou, Dajun Chen, Wei Jiang, and Yong Li. Probench: Benchmarking gui agents with accurate process information. arXiv preprint arXiv:2511.09157, 2025. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: benchmark for toolagent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 20 Planner-Executor Agentic Framework Details A.1 Planner The planner prompt is shown in the code block below. We define an action space that aligns with the one implemented in MobileWorld. In addition, we integrate the ask_user action to enable clarification of ambiguous instructions. Furthermore, when task is tagged as MCP-augmented, we dynamically inject the specifications of the relevant MCP tools into the prompt. System Prompt ## Role: Android Phone Operator AI You are an AI that controls an Android phone to complete user requests. Your responsibilities: Answer questions by retrieving information from the phone. Perform tasks by executing precise actions. ## Action Framework. Respond with EXACT JSON format for one of these actions: Action click double_tap long_press drag input_text answer Description JSON Format Example Tap visible element (describe clearly) Double-tap visible element (describe clearly) Long-press visible element (describe clearly) Drag from visible element to another visible element (describe both clearly) Type into field (includes clicking field, typing, and pressing enter) Respond to user { \"action_type\": \"click\", \"target\": \"blue circle button at top-right\" } { \"action_type\": button at top-right\" } \"double_tap\", \"target\": \"blue circle { \"action_type\": John\" } \"long_press\", \"target\": \"message from { \"action_type\": point\", \"target_end\": \"the end point\" } \"drag\", \"target_start\": \"the start { \"action_type\":\"input_text\", \"text\":\"Hello\" } { \"action_type\":\"answer\", \"text\":\"Its 25 degrees today.\" } navigate_home navigate_back Return to home screen Navigate back { \"action_type\": \"navigate_home\" } { \"action_type\": \"navigate_back\" } scroll status wait ask_user Scroll direction (up/down/left/right) { \"action_type\":\"scroll\", \"direction\":\"down\" } Mark task as complete or infeasible { \"action_type\":\"status\", \"goal_status\":\"complete\" } Wait for screen to update Ask user for information { \"action_type\":\"wait\" } { \"action_type\":\"ask_user\", \"text\":\"what is the exact requirements?\" } keyboard_enter Press enter key { \"action_type\":\"keyboard_enter\" } ## Execution Principles. 1. Communication Rule: ALWAYS use answer action to reply to users - never assume on-screen text is sufficient Please follow the user instruction strictly to answer the question, e.g., only return single number, only return True/False, only return items separated by comma. NEVER use answer action to indicate waiting or loading - use wait action instead Note that answer will terminate the task immediately. 2. Efficiency First: Choose simplest path to complete tasks If action fails twice, try alternatives (e.g., long_press instead of click) 3. Smart Navigation: Gather information when needed (e.g., open Calendar to check schedule) For scrolling: Scroll direction is INVERSE to swipe (scroll down to see lower content) If scroll fails, try opposite direction 4. Text Operations: You MUST first click the input box to activate it before typing the text. For text manipulation: (a) Long-press to select (b) Use selection bar options (Copy/Paste/Select All) (c) Delete by selecting then cutting 5. Ask User: If you think you have no enough information to complete the task, you should use ask_user action to ask the user to get more information. ## Decision Process. 1. Analyze goal, history, and current screen 2. Determine if task is already complete (use status if true) 3. If not, choose the most appropriate action to complete the task. 4. Output in exact format below, and ensure the Action is valid JSON string: 5. The action output format is different for GUI actions and MCP tool actions. Note only one tool call is allowed in one action. ## Expected Output Format (Thought: and Action: are required): Thought: [Analysis including reference to key steps/points when applicable] Action: [Single JSON action] ## Output Format Example for GUI actions: Thought: need to ... to complete the task. Action: { \"action_type\": \"type\", \"text\": \"What is weather like in San Francisco today?\" } for MCP tools: Thought: need to use the provided mcp tool to get the information... Action: { \"action_type\": \"mcp\", \"action_json\": tool_args_obj, \"action_name\": \"mcp_tool_name\" } ## Available MCP Tools {{ tools }} ## User Goal {{ goal }} A.2 Grounding Executor For the grounding executor in our agentic framework, we design it to take click-related instructions generated by the upstream planner and output precise screen coordinates for clicking. The prompt used for this module is shown in the code snippet below. System Prompt You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. ## Output Format Return json object with function name and arguments within <tool_call></tool_call> XML tags: <tool_call> {\"name\": \"grounding\", \"arguments\": <args-json-object>} </tool_call> <args-json-object> represents the following item of the action space: ## Action Space {\"action\": \"click\", \"coordinate\": [x, y]}"
        },
        {
            "title": "B APP Information",
            "content": "B.1 APP List Table 8 lists the GUI applications included in MobileWorld, along with the number of tasks associated with each. These apps cover broad range of everyday mobile usage scenarios, including communication (e.g., Mail, Messages), productivity (e.g., Calendar, DocsReader), navigation (Google Map), social interaction (Mastodon, Mattermost), and e-commerce (Taodian). To ensure reproducibility and enable deterministic evaluation, we prioritize open-source or publicly available implementations that closely mimic the functionality of popular commercial applications, such as Taobao (via Taodian), Gmail (via Mail), and Slack (via Mattermost). This design choice allows us to maintain full control over the environment while still providing realistic, real-world-aligned tasks. By using these open-source equivalents, we strike balance between ecological validity and rigorous, verifiable assessment, enabling fair and repeatable benchmarking of mobile agents. B.2 Open-source APP Environment Construction To ensure reproducibility and enable deterministic evaluation, we construct self-hosted backend environments for key applications. This section details the implementation strategies for our four primary open-source applications. 22 Table 8: List of MobileWorld GUI apps and number of tasks for each one. App Description Comparable Commercial App #Tasks Manage events and schedules Take photos and videos Web browser for internet browsing Alarms, timers, and world clock Manage contact information View and read documents File manager for storage View and manage photos Email client for messaging Calendar Camera Chrome Clock Contacts Docreader Files Gallery Mail Google Map Navigation and location services Mastodon Mattermost Messages Settings Taodian Decentralized social network Team collaboration and messaging SMS and chat messaging System configuration E-commerce shopping platform Google Calendar - - - - Adobe Reader - - Gmail - Twitter/X Slack - - Taobao 30 3 15 7 11 10 32 11 61 1 41 18 41 7 16 Mattermost We build the Mattermost environment based on the official Docker deployment repository2, which provides Docker Compose-based setup for the Mattermost service. Initial chat histories are manually generated following the official import guide using the Mattermost CLI tool. All backend data, including PostgreSQL database contents and file storage, are consolidated into single directory. To ensure consistent initialization across task executions, we snapshot this directory and restore it at the beginning of each evaluation by copying the contents to the designated location before launching the Docker Compose stack. This approach guarantees that each task starts from an identical initial state. Additionally, we develop auxiliary tools that leverage the CLI to dynamically generate new chat messages during initialization when required by specific tasks. For evaluation, we directly query the PostgreSQL database to verify task outcomes, such as the presence of target messages or the creation of chat groups. Mastodon The Mastodon environment is constructed using the official Docker setup from the Mastodon repository3. We manually create initial posts and user accounts within the platform, then capture snapshot of the complete backend state, including the PostgreSQL database and media storage. Similar to Mattermost, we employ snapshot-and-restore strategy: the backend data directory is preserved and restored before each task execution to ensure reproducibility. Since the original Mastodon Android client enforces HTTPS connections, we apply minimal modifications to the client application to enable communication with the locally hosted HTTP backend service. Task verification is performed by directly querying the PostgreSQL database to validate outcomes such as new post creation or user interactions. Mail App We develop the Mail application based on pure-frontend React Native Gmail clone4. We resolve multiple compilation errors that arose with recent Android SDK versions and extend the application with critical functionalities, including attachment selection, email search capabilities, and email composition via the Android Share interface. To enable programmatic evaluation, we further augment the app with callback mechanism that persists email sending events to local file. When an email is sent, relevant metadata (recipient, subject, body, attachments) is recorded and subsequently retrieved by the evaluation function to verify task completion. Taodian App The Taodian e-commerce application is adapted from the Mall4Uni full-stack platform5. To simplify deployment and eliminate external dependencies, we replace the original backend service with lightweight file-based mock server. Product catalogs are manually curated from publicly available 2https://github.com/mattermost/docker 3https://github.com/mastodon/mastodon 4https://github.com/PrincewillIroka/gmail_clone 5https://gitee.com/gz-yami/mall4uni 23 Figure 9: Task Completion Task Example online sources. We enhance the user interface with improved theming and implement additional features such as SMS-based login and refined shopping cart management interface. Similar to the Mail app, we instrument the application with callback hooks at critical interaction points (e.g., checkout button clicks) to capture shopping cart contents and shipping details. This information is transmitted to callback server and subsequently used for automated task validation. This comprehensive construction approach ensures that all applications are fully self-contained, reproducible, and amenable to automated evaluation while closely approximating real-world mobile application functionality."
        },
        {
            "title": "C Case Study",
            "content": "In this section, we demonstrate various types of examples to illustrate the tasks in MobileWorld. 24 C.1 GUI-Only Task Example: Task Completion Figure 9 demonstrates editing Mastodon poll by removing the USA option and replacing Brazil with Canada, guided by precise click and input actions. The process involves navigating to the post, accessing the edit menu, deleting the unwanted option, and typing the new one. C.2 GUI-Only Task Example: Information Retrieval Figure 10 presents real-time mobile task in which the user retrieves Beijings highest temperature for today using the Chrome browser. Starting from the home screen, the agent should launch Chrome, inputs the query Beijing highest temperature today, and immediately extracts the current forecast from Googles dynamic weather widget. Both the execution and validation of the task are inherently real-time, ensuring that the final output (11C) reflects the actual temperature condition on the day of execution. Figure 10: Information Retrieval Task Example 25 C.3 Agent-User Interaction Task Example Figure 11 highlights task involving calendar event management with an ambiguous user instruction: My schedule on 10/20 is bit full, please remove few events. The directive lacks specificity regarding which events to delete, making it inherently ambiguous. To resolve this uncertainty, the agent must engage in interactive clarification. In this scenario, the system identifies multiple events on October 20 and explicitly asks whether to delete only the occurrence of Meet with Sam or the entire recurring series. This interaction underscores the sensitivity and importance of user confirmation in critical operations such as event deletion, where incorrect actions could disrupt scheduling. Figure 11: Agent-User Interaction Task Example C.4 MCP-Augmented Task Example The example shown in Figure 12 demonstrates how mobile GUI agent completes complex task with the help of MCP tool invocation: the user requests the three most recent commits from the googleresearch/android_world repository, formatted as author: commit message, and sent via email to specified address. The agent first invokes an MCP tool to fetch the commit data, then opens the Mail app, taps Compose, and fills in the recipient, subject, and body with the retrieved information before successfully sending the email. The example highlights the agents ability to seamlessly integrate external API calls with precise UI interactions for end-to-end task execution in real-world mobile environments. Figure 12: MCP-Augmented Task Example"
        }
    ],
    "affiliations": [
        "HKUST (GZ)",
        "Tongyi Lab, Alibaba Group",
        "University of Florida"
    ]
}