{
    "paper_title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
    "authors": [
        "Wang Qun",
        "Liu Yang",
        "Lin Qingquan",
        "Jiang Ling"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University's School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelLM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 3 8 0 0 1 . 1 1 4 2 : r Xmodel-1.5: An 1B-scale Multilingual LLM Wang Qun Liu Yang Lin Qingquan Jiang Ling XiaoduoAI {wangqun,liuyangfoam}@xiaoduotech.com"
        },
        {
            "title": "Abstract",
            "content": "We introduce Xmodel-1.5, novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn Universitys School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/ XmodelLM-1.5."
        },
        {
            "title": "Introduction",
            "content": "The rapid globalization of communication has created an urgent need for advanced multilingual natural language processing (NLP) models that can bridge linguistic divides across regions. Traditional NLP models often struggle with less-represented languages, limiting their effectiveness in global applications. As cross-cultural interactions increase, there is growing demand for AI systems that can understand and generate multiple languages with high accuracy and relevance. In response to this challenge, Xiaoduo Technologys AI Lab has developed 1-billion-parameter multilingual large model. Our model excels not only in widely spoken languages such as Chinese and English, but also in languages like Thai, Arabic, and French, demonstrating top-tier performance among models of similar scale. This work addresses the critical need for more inclusive AI systems capable of serving wider array of linguistic and cultural contexts. Beyond the model itself, we also contribute to the research community by open-sourcing Thai evaluation dataset. This dataset, consisting of hundreds of questions annotated by students from Chulalongkorn Universitys School of Integrated Innovation, offers valuable resource for future research in Thai language processing. These efforts highlight our commitment to advancing multilingual AI and improving the tools available for global communication and research."
        },
        {
            "title": "2 Related Work on Multilingual Large Language Models",
            "content": "Multilingual large language models (LLMs) have gained significant attention in recent years, addressing the challenges of natural language processing across diverse languages. These models aim to generalize well across both high-resource and low-resource languages, offering pathway for improved cross-lingual understanding and generation. Several notable models have contributed to this area, including XLM-R, mT5, and PolyLM, which provide important benchmarks for multilingual AI development. XLM-R (XLM-RoBERTa) [Conneau et al., 2020] is widely-used pre-trained language model supporting over 100 languages. With parameter sizes ranging from 270M to 3.5B, XLM-R has set high standard for natural language understanding tasks such as classification and question answering. Its robust generalization to low-resource languages has been key factor in its success. The 1B parameter variant of XLM-R serves as an important benchmark for comparing models of similar scale, as it balances strong performance with computational efficiency. mT5 (Multilingual T5) [Xue et al., 2021], developed by Google, is designed for both understanding and generation tasks across more than 100 languages. Ranging from 300M to 13B parameters, mT5 has shown impressive results, particularly in low-resource settings. The 1B parameter version is effective at handling complex cross-lingual tasks like machine translation and question answering, making it suitable model for comparing performance on languages such as Thai, Arabic, and French. PolyLM [Wei et al., 2023], developed by Alibaba DAMO Academy, is more recent open-source multilingual model available in two sizes: 1.7B and 13B parameters. PolyLM incorporates bilingual data and utilizes curriculum learning strategy to progressively introduce more non-English data during training. This approach improves its performance on lower-resource languages such as Thai and Indonesian. PolyLM has shown strong results across wide range of multilingual tasks, often outperforming models like LLaMA and BLOOM on non-English languages."
        },
        {
            "title": "3 Pretraining",
            "content": "This chapter details the pretraining process of Xmodel-1.5. We begin by introducing the sources and composition of our corpus, followed by an explanation of our preprocessing methods. Next, we describe the construction of our customized tokenizer. Finally, we outline the model architecture and training parameter configurations. 3.1 Training Data Data Sourcing: Building upon the Chinese and English data from the first generation of Xmodel [Wang et al., 2024a], we focused on enhancing the models performance in low-resource languages. To achieve this, we expanded the dataset by incorporating diverse multilingual data, specifically sourced from Multilang Wiki and CulturaX [Nguyen et al., 2023], as shown in Figure 1 The Wiki data was directly downloaded in 30 languages and preprocessed [CyberZHG, 2023] to remove unnecessary markers such as URLs. Similarly, CulturaX [Nguyen et al., 2023] provided data in 27 languages, with an emphasis on oversampling low-resource languages such as Mongolian (mn), Burmese (my), Nepali (ne), Khmer (km), Serbian (sr), and Tamil (ta), despite their smaller data volumes. To ensure data quality, we applied SimHash-based deduplication to the Wiki data, while leaving the CulturaX [Nguyen et al., 2023] data unchanged. To further improve the models proficiency in Chinese, we incorporated data from PTD [Wang et al., 2024b] and WanJuan [He et al., 2023], similar to the previous Xmodel. However, during the annealing phase, we transitioned the WanJuan dataset to curated version that focuses on e-commerce domain data. Additionally, during the annealing phase, we introduced high-quality instructional data[Phatthiyaphaibun et al., 2023, AIRESEARCH, 2023, Phatthiyaphaibun, 2024, Lowphansirikul et al., 2020, Viriyayudhakorn and Polpanumas, 2021, Sawatphol, 2019] , collected with feedback from Thai colleagues. The proportions of this instructional data are provided in Table 1. 3.2 Tokenizer For our multilingual large model, we used unigram tokenizer [Kudo, 2018a] trained with SentencePiece [Kudo and Richardson, 2018], resulting in vocabulary size of 65,280 tokens. This size strikes balance between performance and efficiency for 1 billion parameter model, enabling effective handling of diverse languages. We chose the unigram model [Kudo, 2018b] over the commonly used byte pair encoding (BPE) method [Bostrom and Durrett, 2020] because of its greater flexibility in handling rare and lowfrequency tokens, as well as its faster training process. The unigram approach allows more adaptable word segmentation, capturing linguistic nuances and morphological variations, especially for low2 Figure 1: Total multilingual data tokens during the pretraining phase sourced from MultiWiki and CulturaX. Figure 2: Data distribution during pretraining between 44,000 and 190,000 steps. Figure 3: Data distribution during the decay phase. resource languages with diverse word forms. While BPE is more memory-efficient, it requires much longer training times. For instance, in our experiments, BPE model with 128,000-token vocabulary took 60 hours and 300GB of memory for 51.2GB dataset with 270 million lines, whereas the unigram model completed the same task in under 12 hours, though it required 1TB of memory due to its sampling process. After several iterations of tokenizer design, we finalized version with several key improvements: Training Data Size and Distribution: The Xmodel-1.5 tokenizer was trained on 50GB subset of the Xmodel-1.5 pre-training corpus, with additional industry-specific data to prepare the model for commercial applications. The data distribution was 50% English, 25% Chinese, 10% industry-specific, and 15% low-resource languages. No additional text normalization was applied. Vocabulary Size: The vocabulary size was increased from 32,000 to 65,280 tokens to improve the models ability to represent diverse languages and specialized terms. Numeric data was encoded by splitting numbers into individual digits. Character coverage was set to 0.9999, with rare characters represented by UTF-8 bytes. To address out-of-vocabulary (OOV) issues, we enabled byte fallback mode and set maximum token length of 16 to better handle Chinese phrases. 3 instruct pretrain Data Format Dataset Name klongklon thai_usembassy_th2en_prompt han-instruct-dataset-v4 scb_mt_2020_th2en_prompt thai-wiktionary-prompt prd_news_30112023 th_iapp_wiki_qa_squad thai-wiki-dataset-v3_processed_thaiwikibooks oasst2_thai_top1_chat_format WangchanThaiInstruct_processed_dataset_thai Token Count Category 12,166,780 Literature 2,000,250 News, Opus 2,711,072 Wiki QA, Legal, Opus, Web 277,208,596 Opus 3,989,318 Wiktionary 527,628,430 News 6,373,522 Wiki QA 8,040,228 Textbook 93,256 Multi-task 29,366,662 Medical, Finance, Retail, Legal vajirayana_filtered_tlc_content wiki_th tnhc thai_beginner_content combined_tlc_poems thai-oldbooks thai-it-books goethe-website thailand-policy-statements thai-constitution-corpus tlcv2.0_oa_processed_raw thai-financial-dataset 103,112,882 News 617,827,100 Wiki Docs 3,023,992 Arts 140,652 Opus 18,955,824 Literature 38,930,572 Literature 579,196 Tech 92,162 Culture 1,152,378 Legal 1,551,736 Legal 12,687,966 Literature 800,000,000 Financial Table 1: Detailed Composition of Thai Decay Data. Whitespace Handling: Building on techniques from the LLaMA3 and InternLM2 tokenizers, we manually edited the vocabulary to include tokens for multiple consecutive spaces, improving compression rates for code data. Inspired by MAP-NEO [Zhang et al., 2024a], we disabled SentencePieces default option to remove extra spaces by setting remove_extra_whitespaces=false, which solved formatting issue observed in Xmodel-LMs code generation [Wang et al., 2024a]. This adjustment has been incorporated into Xmodel-1.5. These improvements make the Xmodel-1.5 tokenizer particularly well-suited for multilingual tasks, including those involving low-resource languages and code generation, by balancing language coverage, representational capacity, and data processing efficiency. comparison of the Xmodel-1.5 tokenizer with other widely used tokenizers is shown in Table 2, where our tokenizer demonstrates impressive compression rates despite its relatively small size. Tokenizer LLaMA 3 LLaMA 2 InternLM 2 Baichuan 2 Xmodel-1.5 Vocab Size Compression Rate 128,000 32,000 103,168 125,696 65, 0.3823 0.7524 0.4124 0.4103 0.3800 Table 2: Comparison of vocabulary size and text compression rate of Xmodel-1.5s tokenizer with other models. Lower values indicate better compression. 3.3 Model architecture We built upon the architecture of the first generation of the Xmodel [Wang et al., 2024a] with the following details: Rotary Positional Embedding. We integrate rotary positional embeddings (RoPE) [Su et al., 2023] at each layer of the network. Additionally, to improve the models long-context understanding capability, we adopted the approach from [Xiong et al., 2023] and adjusted the RoPE base from 10,000 to 500,000. Hidden size Intermediate size Attention heads KV heads Layers Context Len 2048 5632 32 24 4096 Table 3: Detailed settings of Xmodel-1.5. RMSNorm. To enhance training stability, we utilize the RMSNorm [Zhang and Sennrich, 2019] function to normalize the input of each transformer sub-layer, without normalizing the output. Linear layers do not incorporate bias, and word embeddings are not tied. SwiGLU. We replace the conventional ReLU non-linearity with the SwiGLU [Shazeer, 2020] activation function to optimize performance. Grouped-query attention. For efficient training and inference, we employ grouped-query attention (GQA) [Ainslie et al., 2023], which incorporates 32 attention heads and 4 KV heads. 3.4 Training Training is performed on single node using 7 out of the 8 available H800 GPUs. To enhance efficiency and maximize Model FLOPS Utilization (MFU), we utilize Distributed Data Parallel (DDP) and FlashAttention-V2. We apply cumulative gradient updating, setting mini-batch size of 4 and using 30 gradient accumulation steps per GPU, resulting in global batch size of 840 with sequence length of 4096. This configuration produces total of 3,440,640 tokens per iteration. Training is conducted over 600,000 iterations, yielding total token count of 2,064,384,000,000. We optimize using the AdamW optimizer with peak learning rate of 6e-4. The learning rate linearly ramps up from zero to the peak over the first 2000 updates, then gradually decreases to 2e-5 following cosine schedule. At 478K iterations, we introduce an exponential decay to further reduce the learning rate. Throughout training, the effective batch size is approximately 3.5 million tokens, with weight decay rate of 0.1. We also apply gradient clipping with threshold of 1.0 to control gradient values. Data allocation evolves throughout training, with the proportion of multilingual data increasing from 5% to 10%, with the initial data ratio as illustrated in Figure 2 Refer to Figure 4 for training logs, which include trend graphs depicting the progression of training and validation losses as the total token count increases. We use OpenWebText2 [Gokaslan et al., 2019], dataset excluded from training, as the validation set to measure validation loss. Figure 4: The trend of training and validation loss during pretraining."
        },
        {
            "title": "4 Post-training",
            "content": "Instruction fine-tuning has become pivotal approach for improving large pre-trained language models on instruction-based tasks. To enable our model to perform well on e-commerce RetrievalAugmented Generation (RAG) tasks, we trained an instruction-tuned model using our foundational architecture. Instead of restricting loss computation and backpropagation to just the answer outputs, we utilized the full instruction dataset, resulting in more effective training [Shi et al., 2024]. Our fine-tuning setup included learning rate of 6e-5, weight decay of 0.1, warmup ratio of 0.03, context length of 4096 tokens, and batch size of 120, with cosine annealing scheduler. We constructed the dataset progressively, starting from simple tasks and advancing to more complex ones, integrating the RAG and RAFT datasets [Zhang et al., 2024c] toward the end. The dataset comprised Belle (56.04%) [Ji et al., 2023], infinity-instruct-subject (34.25%), infinity-commonsense (5.97%), RAG_mixed (2.97%), and RAFT_mixed (0.77%). The models RAG performance in the e-commerce domain achieved satisfaction rate of 92.47%, as evaluated by GPT-4o mini on our e-commerce evaluation dataset."
        },
        {
            "title": "5 Evaluation",
            "content": "5.1 Evaluating the Pretrained Model Baseline Models To ensure fair comparison, we selected several popular decoder-only architecture models, each with approximately 1 billion parameters. Specifically, we compare Xmodel-1.5 with the following models: OPT [Zhang et al., 2022], Pythia [Biderman et al., 2023], TinyLLaMA [Zhang et al., 2024b], MobileLLaMA [Thawakar et al., 2024], H2O-danube [Singer et al., 2024], InternLM2 [Cai et al., 2024] and Qwen2.5 [Hui et al., 2024]. Commonsense reasoning tasks We evaluate our models using the Language Model Evaluation Harness [Gao et al., 2023], which includes tasks such as ARC-Challenge [Clark et al., 2018b], ARCEasy [Clark et al., 2018b], Boolq [Clark et al., 2019], HellaSwag [Zellers et al., 2019], OpenBookQA [Mihaylov et al., 2018], PiQA [Bisk et al., 2019], SciQ [Welbl et al., 2017], and Winogrande [Sakaguchi et al., 2021]. To ensure fairness and reproducibility, all evaluation metrics were computed in the same environment. Notably, we report raw accuracy metrics, rather than normalized ones. The evaluation results, presented in Table 4, show that our model, Xmodel-1.5, outperforms several baseline models, especially surpassing TinyLlama across multiple evaluation metrics. Model ARC-c ARC-e Boolq HS. OB. PiQA SciQ Wino. Avg 23.29 OPT-1.3B Pythia-1.4B 25.60 TinyLLaMA-3T-1.1B 27.82 MobileLLaMA-1.4B 26.28 37.54 InternLM2-1.8B 40.36 Qwen2.5-1.5B 57.03 57.58 60.31 61.32 70.20 74.83 57.80 41.52 23.20 71.71 84.30 59.59 52.32 60.34 39.81 20.20 71.06 85.20 56.20 53.38 57.83 44.98 21.80 73.34 88.90 59.12 54.26 57.92 42.87 23.60 71.33 87.40 58.25 53.60 69.48 46.52 24.40 75.57 93.90 65.67 60.41 73.27 50.09 31.40 75.95 94.90 63.06 62.98 Xmodel-1.5-1B 28.92 64.31 62.78 45.94 22.20 72.20 89.10 60.62 55. Table 4: Performance on commonsense reasoning tasks. Models marked in green perform worse than Xmodel-1.5, while models marked in red perform better than Xmodel-1.5. Multilingual ability In addition to evaluating the models proficiency in English, we also assessed its multilingual capabilities. Specifically, our evaluation included the following tasks: ARC [Clark et al., 2018a]: This dataset consists of 7,787 science exam questions from various sources, including questions provided by research partner affiliated with AI2. We used the Chinese-translated version of this dataset for our evaluation. XCOPA [Ponti et al., 2020]: Designed to test how well machine learning models transfer commonsense reasoning across different languages, XCOPA is translated and reannotated version of the English COPA [Gordon et al., 2011] and includes 11 languages from different language families and regions worldwide. 6 PIQA_AR [Almazrouei et al., 2023]: This is the Arabic version of the PIQA dataset, designed to evaluate physical commonsense reasoning in models, translated by AlGhafa. Belebele_tha_thai [Bandarkar et al., 2023]: Thai subset of the Belebele Benchmark, which assesses multilingual models reading comprehension through multiple-choice questions based on FLORES-200 passages. mMMLU[Hendrycks et al., 2021]: This benchmark evaluates models across 57 tasks, including math, history, and law, highlighting knowledge gaps, particularly in areas like morality and law. mHellaswag[Hendrycks et al., 2021]: machine-translated version of the HellaSwag [Zellers et al., 2019] dataset, which includes multiple-choice questions to test commonsense reasoning. The evaluation results are shown in Figure 5, with further details on the models performance evolution provided in the case study section 6.2. Figure 5: Comparison of performance in multilingual tasks between PolyLM 1.7B and Xmodel-1.5 1B 5.2 Evaluation of the Instruction Model To evaluate the performance of our instruction model, we conducted series of standard assessments, including ifeval [Zhou et al., 2023], which tests various aspects of language understanding and instruction-following abilities, as well as MT-Bench [Bai et al., 2024], fine-grained benchmark designed to evaluate large language models in multi-turn dialogues. These evaluations offer valuable insights into the models strengths and weaknesses across different languages and tasks. In addition to these standard evaluations, we collaborated with Chulalongkorn University to develop Thai-specific evaluation set consisting of 359 samples, which was manually annotated by native Thai speakers to ensure cultural and linguistic accuracy. This custom Thai evaluation allows for more precise measurement of the models performance in low-resource languages. The ratio of task types is summarized in Figure 6, and for each task, we collected three incorrect or impolite responses to generate four-option evaluation focusing on accuracy. 7 Model IFEval MT-Bench TinyLlama-1.1B-Chat-v1.0 Qwen2.5-1.5B-Instruct StableLM-2-zephyr-1.6B H2O-Danube-1.8B-Chat InternLM2-Chat-1.8B Gemma 2B Phi-2 2.7B PolyLM-Chat-13B 5.96 42.5 32.79 15.16 23.87 20.38 27.39 16.27 3.46 N/A 5.42 5.52 4.94 5.19 4.29 N/A Xmodel-1.5-Instruct-1B Table 5: Performance on instruction following and multi-turns chat tasks. 5.06 3. Figure 6: The distribution of task types in our evaluation set. Task types with fewer than 5 occurrences were removed."
        },
        {
            "title": "6 Case Study",
            "content": "6.1 Cases in Human Evaluation We developed user-based evaluation and annotation interface for students at Chulalongkorn University, as shown in Figure 10. Through this interface, annotators can perform ad hoc tests and rate model outputs. Feedback, including suggested corrections, is stored in the backend database. brief overview of both good and poor cases is provided in Appendix 9.1, which also discusses challenges such as the models handling of Thai slang, gender differentiation, and formal vs. informal tone distinctions. These issues can lead to responses that sound unnatural. 8 6.2 Evolution of the Models Performance We monitored and recorded the models performance on the multilingual benchmark throughout the pretraining process, as shown in Figure 7, 8, 9. As training progressed, it became evident that the performance of Xmodel-1.5 consistently improved, ultimately surpassing PolyLM-1.7B [Wei et al., 2023] across multiple tasks. Figure 7: Evolution of performance in Arabic bench during pre-training Figure 8: Evolution of performance in Thai bench during pre-training"
        },
        {
            "title": "7 Conclusions",
            "content": "In summary, our 1 billion parameter multilingual large model represents meaningful step in natural language processing. It shows promising performance in Thai, Arabic, French, and other languages, highlighting its potential for enhancing cross-cultural communication. While we are encouraged by these results, we recognize that there is still room for improvement and further exploration. We hope this work will contribute to ongoing advancements in multilingual AI and facilitate greater understanding across languages. 9 Figure 9: Evolution of performance in French bench during pre-training"
        },
        {
            "title": "8 Acknowledgments",
            "content": "We would like to express our sincere gratitude to the faculty and students from the School of Integrated Innovation at Chulalongkorn University in Thailand for their invaluable contributions to the Thai large language model collaboration project. Their enthusiasm and dedication, especially in the annotation of the Thai evaluation dataset, were critical to the success of this work. We would also like to extend our thanks to our colleagues at Xiaoduo Technology, including Xu Zongliang, Shi Yu, Chen Nanxi, and Chen Xinjie, for their efforts in organizing and supporting this project. Their commitment and teamwork made this achievement possible. References Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. AIRESEARCH. Wangchanthaiinstruct: instruction-following dataset. thai //huggingface.co/datasets/airesearch/ WangchanThaiInstruct, https://huggingface.co/datasets/ airesearch/WangchanThaiInstruct. cessed: 2024-11-15. 2023. URL AcA https: Ebtesam Almazrouei, Ruxandra Cojocaru, Michele Baldo, Quentin Malartic, Hamza Alobeidli, Daniele Mazzotta, Guilherme Penedo, Giulia Campesan, Mugariya Farooq, Maitha Alhammadi, Julien Launay, and Badreddine Noune. AlGhafa evaluation benchmark for Arabic language models. In Hassan Sawaf, Samhaa El-Beltagy, Wajdi Zaghouani, Walid Magdy, Ahmed Abdelali, Nadi Tomeh, Ibrahim Abu Farha, Nizar Habash, Salam Khalifa, Amr Keleg, Hatem Haddad, Imed Zitouni, Khalil Mrini, and Rawan Almatham, editors, Proceedings of ArabicNLP 2023, pages 244275, Singapore (Hybrid), December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.arabicnlp-1. 21. URL https://aclanthology.org/2023. arabicnlp-1.21. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. Mtbench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 74217454. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.acl-long.401. URL http://dx. doi.org/10.18653/v1/2024.acl-long.401. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The belebele benchmark: parallel reading comprehension dataset in 122 language variants, 2023. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: suite for analyzing large language models across training and scaling, 2023. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https://api.semanticscholar. org/CorpusID:208290939. Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining, 2020. URL https://arxiv.org/abs/2004.03720. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, 10 Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018a. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018b. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised crosslingual representation learning at scale, 2020. URL https://arxiv.org/abs/1911.02116. CyberZHG. wiki-dump-reader. https://github. com/CyberZHG/wiki-dump-reader, 2023. Accessed: 2024-10-23. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https: //zenodo.org/records/10256836. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http:// Skylion007.github.io/OpenWebTextCorpus, 2019. Andrew S. Gordon, Zornitsa Kozareva, and Melissa Roemmele. Choice of plausible alternatives: An evaluation of commonsense causal reasonLogiIn AAAI Spring Symposium: ing. cal Formalizations of Commonsense Reasoning, 2011. URL https://api.semanticscholar. org/CorpusID:434646. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/ 2009.03300. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409. 12186. Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. Exploring the impact of instruction data scaling on large language models: An empirical study on realworld use cases. arXiv preprint arXiv:2303.14742, 2023. Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates, 2018a. Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates, 2018b. URL https://arxiv.org/ abs/1804.10959. Taku Kudo and John Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing, 2018. Lalita Lowphansirikul, Charin Polpanumas, Attapol Rutherford, and Sarana Nutanong. scb-mt-en-th2020: large english-thai parallel corpus. arXiv preprint arXiv:2007.03541, 2020. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023. Wannaphong Phatthiyaphaibun. Thai tnhc2 books, URL https://doi.org/10.5281/ 2024. zenodo.10783421. Wannaphong Phatthiyaphaibun, Korakot Chaovavanich, Charin Polpanumas, Arthit Suriyawongkul, Lalita Lowphansirikul, Pattarawat Chormai, Peerat Limkonchotiwat, Thanathip Suntorntip, and Can Udomcharoenchaikit. PyThaiNLP: Thai natural language processing in Python. In Liling Tan, Dmitrijs Milajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors, Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 2536, Singapore, Singapore, December 2023. Empirical Methods in Natural Language Processing. URL https: //aclanthology.org/2023.nlposs-1.4. Edoardo M. Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https: //ducdauge.github.io/files/xcopa.pdf. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/ 10.1145/3474381. Jitkapat Sawatphol. Thai literature corpora. urlhttps://attapol.github.io/tlc.html, 2019. Noam Shazeer. Glu variants improve transformer, 2020. 11 Zhengyan Shi, Adam X. Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. Instruction tuning with loss over instructions, 2024. URL https://arxiv.org/abs/2405.14394. Philipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick, Nischay Dhankhar, Gabor Fodor, and Sri Satish Ambati. H2o-danube-1.8b technical report, 2024. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt, 2024. Kobkrit Viriyayudhakorn and Charin Polpanumas. iapp_wiki_qa_squad, February 2021. URL https: //doi.org/10.5281/zenodo.4539916. Yichuan Wang, Yang Liu, Yu Yan, Qun Wang, Xucheng Huang, and Ling Jiang. Xmodel-lm technical report, 2024a. URL https://arxiv.org/ abs/2406.02856. Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, and Shuangyong Song. Telechat technical report, 2024b. Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, and Jun Xie. Polylm: An open source polyglot large language model, 2023. URL https://arxiv.org/abs/2307.06018. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions, 2017. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023. URL https: //arxiv.org/abs/2309.16039. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: massively multilingual pre-trained text-to-text transformer, 2021. URL https://arxiv.org/abs/2010.11934. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Curran Associates Inc., Red Hook, NY, USA, 2019. Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. Map-neo: Highly capable and transparent bilingual large language model series, 2024a. URL https://arxiv.org/abs/2405.19327. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024b. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. Raft: Adapting language model to domain specific rag, 2024c. URL https://arxiv.org/ abs/2403.10131. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv. org/abs/2311.07911."
        },
        {
            "title": "9 Appendix",
            "content": "9.1 Project Collaboration with Chulalongkorn University This section presents the results and observations from our project collaboration with Chulalongkorn University.This section presents the results and observations from our project collaboration with Chulalongkorn University. Figure 10: Annotation interface for students at Chulalongkorn University We found that Xmodel-1.5 performs effectively in the e-commerce QA context. Feedback from students at Chulalongkorn University described it as \"concise and easy to understand.\" The Thai text in the Figure 11 translates as follows: User: To which account will the refund be issued? Assistant: The refund will be issued to the account used for the purchase. User: What is the procedure if want to exchange the product? Assistant: To exchange product, first contact the seller or their representative to inform them of your intention. You will then receive reference number. After that, reach out again to the seller or representative, provide details of the product you wish to exchange, and await the new product based on the reference number. However, as shown in Figure 12, the model struggled with gendered language in Thai, particularly with gendered particles. This issue was also observed in other state-of-the-art models, highlighting common challenge in handling Thai gender distinctions. Additionally, the LLM revealed that the unique ways of expressing time and numerical data in Thai pose challenges during translation, often resulting in inaccuracies and misrepresentation across languages. These subtleties make it difficult for models to handle temporal and numerical data with precision. Figure 13 illustrates how the model misrepresented time and numerical data. In summary, while our model performed well in e-commerce tasks, feedback highlighted its limitations in handling gender, politeness, and time-related expressions in Thai. These areas require further refinement. 13 Figure 11: The model performs well in e-commerce Q&A, providing concise and clear answers. Figure 12: The model struggles with distinguishing gendered language in Thai, particularly with gendered particles. Figure 13: The model faces challenges in handling time and numerical data in Thai, leading to translation errors. 15 9.2 Detailed Evaluation In this section, we provide detailed evaluation of the models performance across multilingual tasks, including Thai, Arabic, French, and Chinese.The results presented below are based on standard accuracy (acc) metrics for each task. Table 6: Performance on multilingual tasks (Thai, Arabic, French, Chinese). Model belebele_tha_Thai xcopa_th PolyLM-1.7B PolyLM-13B Xmodel-1.5-1B 0.2267 0.2367 0. 0.56 0.586 0.59 (a) Performance on Thai language tasks. Model PolyLM-1.7B PolyLM-13B Xmodel-1.5-1B arc_ar 0.2173 0.2284 0.2344 hellaswag_ar m_mmlu_ar piqa_ar 0.2818 0.3296 0.3279 0.2288 0.2434 0.2454 0.5381 0.5653 0. (b) Performance on Arabic language tasks. Model hellaswag_fr m_mmlu_fr paws_fr piqa_fr PolyLM-1.7B PolyLM-13B Xmodel-1.5-1B 0.3085 0.4064 0.37 0.2458 0.2602 0. 0.548 0.539 0.5325 0.5381 0.5653 0.5789 (c) Performance on French language tasks. Model PolyLM-1.7B PolyLM-13B arc_zh 0.1957 0.2803 Xmodel-1.5-1B 0.259 xcopa_zh 0.5381 0.5653 0.5789 (d) Performance on Chinese language tasks."
        }
    ],
    "affiliations": []
}