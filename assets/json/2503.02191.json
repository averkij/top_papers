{
    "paper_title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
    "authors": [
        "Mia Mohammad Imran",
        "Robert Zita",
        "Rebekah Copeland",
        "Preetha Chatterjee",
        "Rahat Rizvi Rahman",
        "Kostadin Damevski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub. To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants. Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 9 1 2 0 . 3 0 5 2 : r"
        },
        {
            "title": "Understanding and Predicting Derailment in\nToxic Conversations on GitHub",
            "content": "Mia Mohammad Imran, Robert Zita, Rebekah Copeland, Preetha Chatterjee, Rahat Rizvi Rahman, Kostadin Damevski Missouri University of Science and Technology, Rolla, MO, USA Email: imranm@mst.edu Elmhurst University, Elmhurst, IL, USA Email: rzita8729@365.elmhurst.edu Eastern Mennonite University, Harrisonburg, VA, USA Email: rebekah.copeland@emu.edu Drexel University, Philadelphia, PA, USA Email: preetha.chatterjee@drexel.edu Virginia Commonwealth University, Richmond, VA, USA Email: {rahmanr12,kdamevski}@vcu.edu AbstractSoftware projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub. To facilitate this research, we curate novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants. Leveraging these empirical observations, we propose proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over set of baseline approaches."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Toxicity is bad for the health of online communities, including those centered around software projects. Research demonstrates that toxic language significantly impedes the onboarding of newcomers into Open Source Software (OSS) projects [1], [2]. 2017 GitHub survey revealed that 50% of developers encountered negative interactions, with 21% reporting that such experiences caused them to cease contributing [3]. Consequently, it is imperative for projects to safeguard and promote the engagement of all participants, both newcomers and experienced contributors. Despite the increasing recognition of the negative impact of toxic interactions, existing toxicity detection methods are predominantly post-hoc [4], [5], [6], [2]. These approaches identify and address toxic comments and behaviors only after they have occurred, often relying on manual moderation or automated tools that flag inappropriate content retrospectively. While post-hoc detection can mitigate some of the damage caused by toxic interactions, it fails to prevent the initial harm and allows negative behaviors to persist unchecked for extended periods. This reactive approach not only delays intervention but also burdens community moderators and risks alienating contributors who might have otherwise remained engaged. Consequently, there is pressing need for proactive solutions that can anticipate and preemptively address potential toxicity. Proactive moderation in OSS projects involves moderators engaging in discussions to encourage positive behavior among developers. This approach contrasts with reactive moderation strategies, which include removing comments or locking threads post-incident. Effective proactive moderation can preempt toxicity; however, it is impractical for moderators to continuously monitor the multitude of communication channels (e.g., issues, chats, discussion boards) within an OSS project [7]. On the other hand, automatic proactive moderation necessitates profound understanding of the specific context and community dynamics. Unlike platforms such as (formerly Twitter) or Reddit, GitHub projects often exhibit more subtle inappropriate behaviors, such as entitlement, miscommunication, or resistance to new practices, rather than overt aggression [8]. The advent of foundational LLMs (e.g., GPT, LLaMa), capable of comprehending human text, offers unique opportunity to integrate advanced NLP techniques into OSS environments to proactively detect and mitigate potential communication issues. This paper aims to understand the characteristics of toxic conversations on GitHub and how conversations derail into toxicity. We use this knowledge to provide method for automatically detecting if conversations will derail. More specifically, the paper makes the following key contributions: We curate dataset of 202 toxic conversations on GitHub with annotated derailment points, as well as 696 non-toxic conversations as baseline. We examine the characteristics of toxic conversations and derailment points, identifying specific unique characteristics of each. We present an automated approach to OSS moderation that predicts whether conversation will derail into toxicity. It is based on novel prompt methodology that generates Summaries of Conversation Dynamics (SCD) for GitHub conversations. Our approach is able to effectively detect early signs of conversational derailment, achieving an F1-score of 0.69. Our studys datasets, scripts, and output logs are publicly available online at URL: https://anonymous.4open. science/r/derailment-oss-replication-C8B1."
        },
        {
            "title": "2 EXAMPLE OF CONVERSATIONAL DERAILMENT\nON GITHUB",
            "content": "When conversation on GitHub channels, like issues or pull requests, turns toxic, the toxicity often occurs with identifiable signs in the previous comments. In this research, we focus on understanding the early signs that conversation will turn toxic on GitHub. These preceding comments, where it becomes clear that the conversation has moved away from being productive and taken turn towards negativity, are called derailment points [9]. Overall, toxic conversations often contain the following identifiable elements: 1) conversation-initiating comment, 2) derailment point comment, 3) first toxic comment, and 4) (zero or more) subsequent toxic (or non-toxic) comments. Figure 1 shows an example of toxic conversation, highlighting these different structures. In this conversation between an OSS project contributor and an external participant (i.e., someone who has never made commit to the repository), the contributor derails the conversation by making mocking comment. The external participant responds with frustration and then makes toxic, insulting remark. This is followed by another toxic comment, this time made by the contributor."
        },
        {
            "title": "3 CHARACTERISTICS OF TOXIC CONVERSATIONS\nON GITHUB",
            "content": "Understanding the characteristics of toxic conversations is crucial for developing effective intervention strategies. This section aims to identify and analyze the key properties of 2 Fig. 1. Example of toxic conversation on GitHub. toxic conversations. We identify these properties by examining various aspects such as participants roles, comment patterns, thread initiation, linguistic features, and comment timing. Our analysis focuses on understanding the conversational patterns that lead to toxicity and the behaviors associated with it. 3.1 Datasets We curated two datasets of conversations, one toxic and one non-toxic, sourced from GitHub issues and pull requests. We describe our annotation process for each dataset, where our primary goal was to include high-quality annotated and representative examples of GitHub conversations. 3.1.1 Toxic Conversations Dataset We leverage dataset recently released by Ehsani et al. [10], which focuses on incivility in GitHub conversations. This dataset is based on 404 locked conversations (issues and PRs) on GitHub where the reason they were locked is listed as too heated, spam, or off-topic. These 404 conversation threads contain 5961 comments annotated with various categories of uncivil TBDFs (Tone Bearing Discussion Features). The definitions and examples of the incivility-related TBDFs are shown in Table 1. We use LLM-aided model-in-the-loop annotation approach to identify the uncivil comments that are also toxic [11]. Recent research shows that such model-in-theloop annotation methodology works well for this type of TABLE 1 Definitions and examples of uncivil tone-bearing discussion features (TBDF). TBDF Bitter Frustration Impatience Mocking Irony Vulgarity Threat Definition Expressing strong frustration, displeasure, or annoyance Expressing dissatisfaction due to delays Ridiculing or making fun of someone in disrespectful way Using language to imply meaning that is opposite to the literal meaning, often sarcastically Using offensive or inappropriate language Issuing warning that implies negative consequence Entitlement Expecting special treatment or privileges Insulting Identity attacks/ Namecalling Making derogatory remarks towards another person or project Making derogatory comments based on race, religion, gender, sexual orientation, or nationality Example No answer, no reaction, what kind of support is that. Issue not fixed in 30 days? Must be gone! Legend says this issue will still exist even on the end of mankind. Maybe you should actually write that down somewhere. You know, like in the documentation. Who cares, same sh*t. Any further responses will result in you being blocked from the repo entirely. [...] thats how good we are. dont want your contribution. [...] This looks like it was done by 5 year old. would not be surprised if this database is maintained by the [nationality]. data, including hate and violent speech detection tasks [12], [13], [14], [15], [16], [17], [18]. For each uncivil comment (as annotated by Ehsani et al.), we asked GPT-4o if each comment is toxic or not. We provided the complete conversation until the current comment for context and the following definition to the LLM, Toxicity is defined as rude, disrespectful, or unreasonable language that is likely to make someone leave discussion. GPT-4o identified 832 toxic comments belonging to 273 threads. Following GPT-4os annotation, two of the papers authors of the paper manually checked each toxic comment to ensure whether they were actually toxic or not. The two human annotators reached an initial agreement of 0.78 (Cohens Kappa). The comments where the annotators disagreed were resolved through an in-person discussion. After this annotation, we further excluded conversations where the first comment in the conversation was annotated as toxic. These are cases that are inappropriate for our analysis of conversational derailment in software engineering. Following this step, we retained 175 toxic GitHub threads. 3.1.2 Non-Toxic Conversations Dataset To analyze the properties of toxic conversations compared to ordinary, non-toxic GitHub issue/PR conversations, we collected random sample of GitHub threads from the same repositories as the toxic threads. To reflect realistic conversational data distribution, we intentionally sampled more data in this dataset by collecting four random threads for each toxic thread in the Toxic Dataset. More specifically, we examined 15 threads before and 15 threads after each of the toxic threads. We included their Fig. 2. Participants in different types of GitHub conversations. data if they had at least two comments, were not marked as too heated, were not locked, or if locked, were marked as resolved. We then randomly selected four threads from the group. However, we were unable to collect all the data as some repositories had been removed since Ehsani et al. [10] collected their dataset, and some repositories did not have enough issues/PRs. Some of the conversations were also non-english, which we excluded. The resulting collection consisted of 723 GitHub issues and PR conversational threads. To ensure these conversations are non-toxic, we used similar model-in-the-loop approach as in curating the Toxic Conversations Dataset. Specifically, two authors of this paper manually reviewed the toxic comments identified by GPT-4o to determine whether each comment was truly toxic. They achieved high agreement of 0.712 (Cohens Kappa), with the remaining disagreements resolved through inperson discussions. Ultimately, out of the initial 723 threads, 27 were identified as toxic, and the remaining 696 were marked as non-toxic, forming our Non-Toxic Conversations Dataset. We added the 27 toxic conversations identified through this process to the set of 175 Toxic Conversations Dataset, resulting in total of 202 conversations containing 483 toxic comments. To ensure consistency with the existing conversations, two authors of this paper annotated TBDF for these 27 new toxic conversations following Ehsani et al.s annotation guidelines [10]. They initially achieved Cohens Kappa of 0.70 and subsequently resolved their differences through discussion to reach complete agreement. 3.2 Findings 3.2.1 Participants of Toxic Conversations We divide participants in the GitHub repositories into two categories: Project Contributors and External Participants. GitHub assigns specific roles such as Owner, Collaborator, Member, Contributor, or None 1. Owners create the repository, Collaborators have administrative access, Members belong to the organization that owns the repository, and Contributors have made commits. The None role applies 1. https://docs.github.com/en/graphql/reference/enums# commentauthorassociation 4 engagement ( 60% of comments) in conversations appears to correlate with reduction in thread toxicity. However, this relationship is not necessarily causal as other factors, such as the nature of the issue/PR, the tone set by the initial post, and the general community culture, may also influence the toxicity level of thread. Observation # External participants initiated 76.0% of toxic conversations on GitHub and contributed 52.79% of the comments in toxic threads, playing significant role in driving toxicity. Conversations with higher project contributors engagement tended to be less toxic. 3.2.2 Length of Toxic Conversations Toxic conversations on GitHub tend to be relatively lengthy. The median number of comments in Toxic Conversations Dataset is 11, compared to the Non-Toxic Conversations Dataset median of 6 comments. The median occurrence of the first toxic comment is 6 comments after the conversation begins. The distribution of the first toxic comments occurrence is as follows: 24.75% (50/202) of the time within the first 3 comments, 42.07% (85/202) of the time within the first 5 comments, 56.93% (115/202) of the time within the first 7 comments, and 70.79% (143/202) of the time within the first 10 comments. The median number of comments after the first toxic comment is 3. Note that the moderators locked majority of these threads as heated, possibly preventing even more offensive comments from being posted. We observe that 53.47% (108/202) of the toxic conversations have more than one toxic comment with median of 3 toxic comments per conversation. This indicates that toxic comments are likely to elicit more toxic replies. Observation #2 Toxic conversations on GitHub tended to be longer, with median of 11 comments versus 6 in the non-toxic conversations. Toxic comments usually appeared later on, at median of 6 comments after the initial post. Over half of these conversations (53.47%) had multiple toxic comments, indicating that toxicity often escalates. 3.2.3 Timing of Toxic Comments The timing of toxic comments within thread is important for understanding the dynamics of how toxicity emerges and escalates in conversations, as well as how it can be best mitigated [19]. We extracted the timestamp of each comment and calculated the time difference between consecutive comments. Table 2 illustrates the distribution of the timing for the first toxic comment across 202 threads in the Toxic Conversations Dataset. We made number of observations: Rapid Escalation: The first toxic comment appears within an hour of the previous comment 51.98% (105/202) of the time. Of these comments, 80.0% (84/105) occur within shorter timeframe than the median time between comments in each conversation thread, indicating that there is often rapid response leading to toxicity. Fig. 3. Percentage of project participants comments in GitHub conversation threads (NToxic = 202; NNonToxic = 696). to authors without specific association. GitHub offers few additional roles that we did not encounter in either of our datasets. We classify the first four categories (Owner, Collaborator, Member, and Contributor) as Project Contributors of the project, and take None to represent External Participants. In the Toxic Conversations Dataset, there are 479 toxic comments made by 271 commenters in the None category, 96 in the Member category, 82 as Contributors, 22 as Collaborators, and 8 as Owners. Therefore, there are 271 external participants and 208 project contributors. It is important to note, however, that these are not necessarily unique commenters. As multiple conversations were extracted from the same repositories, it is possible that individuals may appear in more than one conversation. We exclude two toxic conversations containing 4 toxic comments from this analysis because they were deleted from GitHub since Ehsani et al. published their dataset, and were therefore unavailable for the collection of author-related information. In these toxic conversations, we observe that there are slightly more comments made by external participants (52.79% of comments) than by project contributors (47.21% of comments). However, in the non-toxic conversations, the percentage of project contributors comments is higher (66.62%). This suggests slight shift in participation rates when toxicity is present, with external participants being more vocal in toxic conversations. We observe that external participants initiate more (76.0%, 152/200) of toxic conversations. This is much higher than the non-toxic conversations, where they initiated 52.44% (365/696) of the conversations, as shown in Figure 2. Despite project contributors initiating only 24% (48/200) of toxic conversations, they made higher percentage of the first toxic comments (42.5%, 85/200). This indicates that while developers are less likely to start toxic conversations, they are disproportionately responsible for the toxicity after the thread derails. Figure 3 shows the project contributors (vs. external participants) comment percentage in conversations from the Toxic Conversations Dataset vs Non-Toxic Conversations Dataset. Each data point in the figure is an individual conversational thread. The median percentage of developer comments in 200 toxic conversations is 44.0%, whereas for the 696 threads in the Non-Toxic Conversations Dataset it is 66.0%. Figure 3 also suggests that high developer Decreasing Likelihood Over Time: The likelihood of toxic comment occurring diminishes as more time passes after comment is posted. For instance, only 9.90% (20/202) of toxic comments appear within 1-3 hours and the proportion continues to decrease for longer intervals (see also Table 2). Delayed Toxicity Still Exists: Despite the general trend of rapid escalation, notable 9.41% (19/202) of toxic comments occur after week, highlighting that toxic interactions can resurface even after period of inactivity. Observation #3 Toxic comments on GitHub often emerged quickly, with 51.98% appearing within an hour of the previous comment and 80.0% occurring earlier than the median response time in these threads. Despite the decreased likelihood of toxicity over time, 9.41% of the first toxic comments appeared more than week after the previous comment. 3.2.4 References to Other Participants in Toxic Comments To identify patterns of how GitHub participants refer to each other in toxic comments, we examined two aspects: 1) mentioning someone using @username, and 2) quoting someones previous comment. Our analysis revealed that the first toxic comment in conversation thread mentions someone using @username 25.74% of the time, which is comparable to the average mentioning percentage of 25.0% in the threads from the Toxic Conversations Dataset. However, more notable difference emerges when examining the prevalence of quoting in toxic threads. The data indicates that 27.23% of comments in toxic threads quote someone, which is substantially higher than the quoting rate of 11.72% observed in the representative threads. This finding suggests that quoting may serve as an indicator of contentious or confrontational interactions, potentially signifying higher level of engagement with specific individuals or arguments. The examination of common trigrams in toxic comments provides additional insights into the nature of these interactions. Phrases such as you want to and if you want frequently appear in toxic comments, indicating tendency towards instructive, assumptive, or confrontational language directed at specific individuals. Zhang et al. highlighted that the use of second person pronouns is strongly associated with nonconstructive disagreements [9]. Our analysis of the first toxic comments on GitHub further supports this connection, revealing substantially higher frequency of second person pronoun use (71.29%, 144/202) in toxic comments than in all representative comments (35.99%, 1913/5316). This stark contrast underscores the prevalence of direct address and potentially confrontational language in toxic exchanges. On the conversational level, we can observe similar properties as well: 202 toxic conversations have median of 6 comments with second person pronouns while the 696 non-toxic conversations have median of 2 comments. We observe that only 3.46% (7/202) of toxic conversations did not have any second person pronoun use, compared to 11.93% (83/696) in the Non-Toxic Conversations Dataset. The role of first person pronouns in non-constructive conversations, as noted by De Kock et al. [20] in the context TABLE 2 Timeframe between Toxic comment and previous comment in the thread. 5 Passed time since previous comment < 1 hour 1-3 hours 3-6 hours 6-12 hours 12-24 hours 1-7 days > 1 week Count (%) 105 (51.98%) 20 (9.90%) 13 (6.44%) 12 (5.94%) 9 (4.55%) 24 (11.88%) 19 (9.41%) Shorter than median timeframe in conversation 84/105 (80.0%) 10/20 (50.0%) 5/13 (38.46%) 4/12 (33.33%) 1/9 (11.11%) 6/24 (25.0%) 0/19 (0%) of Wikipedia, is also supported by GitHub interactions. The first toxic comments exhibit slightly higher usage of first person pronouns (70.79%, 143/202) compared to representative comments 60.68%, 3226/5316). We observe that 55.94% (113/202) of toxic comments use both first and second pronouns, compared to 24.66% (1311/5316) of comments in the Non-Toxic Conversations Dataset. This suggests that toxic exchanges may involve both direct address and self-referential language, potentially contributing to more subjective and personal tone. Observation #4 Toxic comments frequently quoted other commenters (27.23% vs. 11.72% in non-toxic threads) and used second person pronouns (71.29% vs 35.99%). Both first and second person pronouns occurred in 55.94% of toxic comments compared to 24.66% of regular comments. 3.2.5 Incivility TBDFs in Toxic Comments Out of the 1025 comments containing incivility TBDFs in the 202 toxic conversation threads, substantial 47.12% (483/1025) are annotated as toxic. This high proportion demonstrates the close relationship between uncivil discourse features and toxicity, as observed by previous research [21]. Notably, the TBDFs that exhibit particularly strong association with toxicity are: Identity Attacks/Name-Calling: 84.62% (22/26) toxic Vulgarity: 78.33% (47/60) toxic Insulting: 77.44% (127/164) toxic Entitlement: 64.86% (48/74) toxic TBDFs that involve Identity Attacks, Vulgarity, or Entitlement are highly indicative of toxic interactions. Notably, Entitlement is especially prevalent in software engineering text, as shown by Miller et al. [22]. The prevalence of these TBDFs in toxic comments highlights the need to pay close attention to these specific features when identifying and addressing toxicity. On the other hand, conversely, some TBDFs show lower correlation with toxic comments: Impatience: 18.35% (29/158) toxic Bitter Frustration: 31.17% (115/368) toxic Irony: 46.41% (22/47) toxic This suggests that expressions of frustration and impatience, although potentially detrimental to the conversation, may not always cross the threshold into overt toxicity. Mocking and Irony sometimes can be friendly banter or community jokes. In 75 cases where toxicity occurs suddenly, i.e., there is no derailment point, we observe that nearly half of the time (37/75) the reason for toxicity is the sudden occurrence of Vulgarity or Insulting TBDFs. This aligns with Miller et al.s findings, where they noted over half of their sample of toxic comments (55%) contained insulting, curse words or intentionally offensive language [22]. Observation #5 We observed that 47.12% of comments with uncivil TBDFs were toxic. TBDFs like Identity Attacks (84.62%), Vulgarity (78.33%), and Insults (77.44%) strongly indicated toxicity, while Bitter Frustration (31.17%) and Impatience (18.35%) were less likely to be toxic. 3.2.6 Toxic Issue/Pull Request Labels GitHub allows project maintainers to assign customized labels to conversations to provide quick insight (e.g., bug or needs triage). Out of the 202 toxic issues/PRs, 107 (52.97%) have labels assigned by participants. Using the GitHub API, we find that the largest label category is bug or similar (25.23%, 27/107). This seems to indicate that conversations related to bugs are most prone to turn toxic. The second largest category is labels such as feature, enhancement, and suggestion (17.76%, 19/107). This is mostly because of disagreements over new ideas or changes that lead to heated discussions. Following this, help wanted appeared in 13 cases (12.15%, 13/107), where one of the commenters assigned label that indicated they needed assistance with some aspect of the project. Often, this type of conversation becomes toxic as result of frustration from commenters. Labels such as wontfix and rejected occurred occasionally (12.15%, 13/107), suggesting that hostility may be caused when developers PR or feature request is not accepted by the community. However, positive status update labels, such as approved and completed were still present in toxic issues (5.61%, 6/107), demonstrating that although rejecting feature request or PR often leads to toxicity, accepting it does not completely eliminate negative response. 3.3 Implications This analysis of toxic conversations on GitHub has several implications. Firstly, it highlights that external participants are more likely to participate in and initiate toxic conversations than project contributors, in line with the findings of Miller et al. [22]. This difference in participation rates indicates need for more focus on managing human interactions to curb toxicity. Additionally, this analysis shows that higher developer engagement correlates with lower toxicity levels, suggesting that developers active participation can help maintain constructive tone in conversations. Toxic conversations also tend to be longer, as seen in Xia et al. [19], and toxic comments are often followed by more toxic comments [23], emphasizing the need for early detection and intervention to prevent further escalation. 6 Communication patterns such as higher rates of quoting and the use of second-person pronouns [24], are indicative of more direct and confrontational interactions, which can be flagged for early intervention. The rapid escalation of toxic comments, with responses often occurring within the hour, further highlights the need for real-time monitoring and quick responses. This occurrence of delayed toxicity suggests that unresolved issues may resurface, necessitating periodic reviews of inactive threads. Lastly, the strong association between toxicity and uncivil features, such as identity attacks and vulgarity, highlights the importance of focusing on these markers for accurate detection, similar to the work done by Ferreira et al. [25]. Once toxicity occurs in conversation, the damage to the participants and the community has already taken place [19], [26]. In this study, our focus is derailment, i.e., understanding when conversation derails and is likely to turn toxic. By detecting derailment, we could potentially avoid toxicity altogether. In the next section, we aim to understand the properties of derailment on GitHub."
        },
        {
            "title": "4 CHARACTERISTICS\nGITHUB",
            "content": "OF"
        },
        {
            "title": "DERAILMENT",
            "content": "ON Of the 202 toxic threads, 127 included preceding uncivil comment (i.e., derailment point) before the first toxic comment, which we refer to as Derailed Toxic Dataset. The remaining 75 exhibited sudden toxic comments midconversation without any observable derailment point. To analyze derailed conversations, we focus on the 127 threads in Toxic Conversations Dataset with derailment points, referred to as Derailed Toxic Dataset. In contrast, the remaining 75 threads represent instances of abrupt toxicity. 4.1 Timing and Distance to Derailment Points We observe that the median number of comments from the derailment point to the toxic comment is 2. The close proximity between derailment and toxic comments suggests that once thread derails, it is likely to rapidly devolve into toxicity. This aligns with Cheng et al.s findings, which indicate that negative context and mood increase the likelihood of trolling behavior [23]. The timing of the first toxic comment relative to the derailment point provides additional insights. Considering, 8-hour workday, we observe more than half of the time (58.26%, 74/127) of toxic comments occur within 8 hours of the derailment comment [27], emphasizing the importance of timely intervention. 4.2 TBDFs in Derailment Points The TBDFs at derailment points (10%) are: Bitter Frustration: 44.88% (57/127), Impatience: 18.11% (23/127), and Mocking: 10.23% (13/127). This and the analysis in Section 3.2.5 indicate that while Bitter Frustration and Impatience are not often toxic themselves, they often serve as precursors to toxicity. Identifying and addressing these TBDFs early on could prevent uncivil exchanges from escalating into full-blown toxicity. TABLE 3 Lexical cues in derailment point comments. Linguistic features Second person WH Question Negation terms Reasoning terms Emphasis terms Communication verbs Derailment point (127) All (8873) 40.83% 66.14% 40.24% 55.18% 39.79% 62.20% 27.16% 41.73% 26.73% 40.94% 21.89% 34.65% TBDF (1025) 66.15% 55.32% 58.44% 39.80% 42.54% 36.78% 4.3 Linguistic Features We analyze language indicative of conversation derailment. Using the 127 derailment point comments, we identified the 200 most frequent unigrams (i.e., words), excluding articles, particles, and common prepositions. Two authors collaboratively categorized (see Table 3) the remaining 104 unigrams into linguistic features using the card sorting method [28], where they met in person, discussed, and resolved differences, consulting dictionary as needed. Based on these categorizations, we automatically counted the frequency of each unigram in the derailment point comments after applying basic preprocessing steps (e.g., tokenize and lemmatize). Of the 127 derailment point comments, 66.14% (84/127) used second person pronouns [24] and 63.78% (81/127) used first person pronouns. Additionally, 50.34% (64/127) used both pronouns. Interestingly, these percentages are slightly lower than those found in toxic comments and higher than in the representative GitHub comments, as noted in 3.2.4. We also found that in derailment points the use of negation why, no, etc.), WH questions (what, terms (not, how, since, where, etc.), reasoning terms (because, etc.), communication verbs (say, comment, tell, etc.), and emphasis terms (actually, really, etc.) are comparatively higher than in general comments. These elements also occur more frequently in comments marked with incivility TBDFs. Table 3 shows the percentages in derailment points along with TBDFs. Observation #6 Derailment points in GitHub conversations frequently featured second person pronouns (64.17%), first person pronouns (65.83%), or both first and second person pronouns (51.67%). The use of negation terms, WH questions, reasoning terms, communication verbs, and emphasis terms was also notably higher in these derailment points than in the NonToxic Conversations Dataset. 4.4 Trigger Types Eshani et al. [10] annotated incivility triggers, i.e., what initiated the incivility in the conversation. Following their methodology, two authors of this paper independently annotated triggers at derailment points, achieving Cohens Kappa score of 0.78. Disagreements were resolved through in-person discussions for complete agreement. The most prevalent trigger was Failed Use of Tool/Code or Error Messages followed at 30.71% (39/127), where tool difficulties or bug troubleshooting led to derailment. For 7 example: [CODE SNIPPET] ... What more proof do you need? That is everything. Frustrated tones in this comment about replicating an error led to toxic comments. Technical Disagreement made up 25.20% (32/127) of cases, involving disputes over project changes. For instance, [CODE SNIPPET] Ask yourself what **intention** it expresses. This is some kind of esoteric gibberish without reference to the subject area. The code is too low-level and [...]. In this case, disagreements about method name changes escalated to toxicity. Communication Breakdown accounted for 22.83% (29/127) of cases. This included misunderstandings, misinterpretations, typos, or language barriers causing perceived hostility. For example, It is impolite to assume that each user opening an issue is stupid and lazy. Of course, search the issue tracker. assume used the wrong keywords. [...] Here, misunderstanding between the commenters triggered incivility, which resulted in toxicity. Finally, Politics/Ideology accounted for 9.45% (12/127) of cases, with off-topic political or ideological debates causing derailment. For example: Good intentions, but doubt theres any relation of the origin of the terms blacklist/whitelist to race. There are many idioms and phrases in the English language that make use of colors without any racial backstories. [...]. Observation #7 of Tool/ Code The primary triggers at derailment points were Failed Use or Error Messages (30.71%), Technical Disagreement (25.20%), Communication Breakdown (22.83%), and Politics/Ideology (9.45%)."
        },
        {
            "title": "5 CONVERSATION DERAILMENT PREDICTION",
            "content": "Automatically detecting conversational derailment on GitHub is essential for managing the scale and frequency of OSS communication channels. This section describes our method of predicting conversational derailment on GitHub using LLMs. Hua et al. demonstrated that automated derailment forecasting systems perform best when they make their predictions based on Summaries of Conversation Dynamics (SCD) they have previously generated [29]. SCDs provide succinct understanding of conversations trajectory, detailing the types of interactions that led to its current state and predicting their likely development. Hua et al. developed few-shot procedural prompt for SCD generation where the LLM was provided with manually written SCD examples to increase accuracy. Drawing inspiration from their methodology, we integrate our findings from Section 4 into the design of SCD generation prompts. We start with Hua et al.s SCD prompt, customize it for GitHub conversations, and then develop new prompts based on the conversation characteristics observed on GitHub. 5.1 Baseline Models We compare our SCD-based technique to two baselines: 1) CRAFT and 2) Hua et al.s approach. CRAFT is one of the earliest and best-known models for predicting conversational derailment [27]. Since its inception in 2019, various other strategies for predicting conversational derailment have been explored [30], [31], [32], [27], [33], [29]. Despite this, the CRAFT model is still widely used baseline. Since our approach adapts Hua et al.s recent SCD-based technique for predicting derailment on GitHub [29], we also compare to their technique as baseline. 5.2 LLM Prompt Design We explore several strategies to design the LLM prompt to predict conversational derailment on GitHub. Firstly, we adapted Hua et al. [29] few-shot procedural SCD prompt for GitHub by specifically mentioning GitHub conversation and by providing examples (i.e., few-shot prompting) based on GitHub conversations. An example of the SCD summary we provided in the prompt is as follows: The conversation involves six users discussing an issue that was encountered in their code. User1 posts the issue asking for guidance, causing User2 to ask for clarifying detail. User2 provides solution and User1 responds saying that the solution did not work. User3 then joins the conversation and asks if solution was ever found. User4 seconds this, causing User5 to join the conversation and comment on how User3 and User4 are not project contributors and therefore have done nothing to try to fix the problem. User4 then responds expressing frustration that solution has not been posted in the last few years. While this few-shot prompt appeared to be more effective than Hua et al.s original prompt at generating SCD summaries for GitHub conversations, we hypothesize that it may not yet be optimal. Hua et al. developed SCD prompts targeting general-purpose conversations, which may not be most effective for the highly technical discussions found on GitHub. We explored whether decomposing the problem [34] and integrating the properties of GitHub derailed conversations, uncovered in Section 4, could yield better SCDs for predicting derailment on GitHub. Previous research shows that decomposing the prompts into incremental steps enhances the LLMs accuracy [34], [35]. Therefore, we devise prompt that is based on the Least-to-Most (LtM) prompting strategy [35], which allows us to integrate our insights from Section 4. For the language features (e.g., questioning, rhetoric), we used the categories defined by Hua et. al. [29] (e.g., rhetorical questions, hedging, questioning logic, etc), which is consistent with our finding as described in section 4.3. Additionally, to better capture the tone of the conversation, we incorporate social orientation tags (e.g., Assured-Dominant, Gregarious-Extraverted, WarmAgreeable, Unassuming-Ingenuous, Unassured-Submissive, Aloof-Introverted, Cold, and Arrogant-Calculating), developed using circumplex theory [36]. These tags capture the levels of power and benevolence expressed by each comment and analyzing their interaction provides insight into the conversations dynamics. Circumplex theory suggests that social interactions can be described by two dimensions: power and benevolence. Power reflects the extent to which an individual seeks to control, lead, or assert themselves in relationships, while benevolence captures the warmth, friendliness, and positivity of interactions. recent study 8 by Morrill et al. showed that GPT-4 generated social orientation tags are effective for predicting conversation derailment [37]. In our final prompt, we integrate triggers, social orientation tags, TBDFs, and linguistic features in step-by-step manner by decomposing the problem into smaller parts. We used the social orientation tags definitions provided by Morrill et al. [37], TBDFs and trigger definitions as provided by Ehsani et al. [10]. Finally, we used the following least-tomost prompt for SCD generation: Least-to-most SCD Generator Prompt Here is step-by-step guideline to write an GitHub conversation trajectory summary: Step 1: Identify the main elements of the conversation. Step 2: Find any triggers of tension in the conversation. The common triggers are: Failed use of tool/code or error messages: trouble with code/tool. Communication breakdown: being misinterpreted by people or being unable to follow. Politics/ideology: arising over politics or ideology differences (specific beliefs). Technical disagreement: having differing views on some technical component of the project. ... Step 3: If there are triggers, identify the social orientation. Social orientation from circumplex theory is social theory that characterizes interactions between speakers. [...] Definitions are: Assured-Dominant: Demands attention, is firm, self-confident, assertive, persistent, and not selfconscious. Warm-Agreeable: Interested in people, polite, cooperative, accommodating, gentle. Arrogant-Calculating: Boastful, manipulative, cunning, cocky. ... Step 4: Describe the sentiments and tones expressed by each participant. Indicators include: Bitter frustration: strong frustration. Impatience: feeling that resolution is taking too long. Insulting: directed insults. ... Step 5: Note conversation strategies to find tones. Rhetorical Questions: posed for point, not an answer. Posing Challenges/Clarifications: asking for elaboration. Hedging: softening statements. ... Step 6: Combine information from steps 1-5 to write short summary. Step 7: Refine the summary using the following points: Do not include specific technical details. Keep it concise, capturing key moments and tonal shifts. Focus on moments where conversation dynamics change. Example: Several contributors discuss an unresolved code issue. User1 requests guidance, [...] responds with frustration over the lack of solution, and the conversation ends on tense note. Task: Using the guideline, write the final trajectory provided GitHub conversation summary for transcript. Conversation Transcript: < insert conversation > Write only the trajectory summary in double quotes. Here is an example SCD summary generated using this prompt: The conversation starts with @USER1 seeking documentation for brewfile syntax, which @USER2 provides partially by pointing to the DSL definition. However, @USER1 expresses frustration over the lack of comprehensive documentation and @USER3 responds defensively, stating that issues should not be left open without actionable work. The tone shifts as @USER1 criticizes the lack of documentation, leading @USER4 to accuse him of violating the Code of Conduct. Despite @USER1s attempts to clarify his intentions, the conversation ends with tense exchange and @USER1 deciding to leave. We observe that this SCD summary was more effective at capturing the emotional aspects and tone of the GitHub conversations, compared to our initial approach. We employ follow-up predictor prompt based on the generated SCD summary asking the model to assess the likelihood of the conversation derailing into toxicity on scale from 0 to 1. This second prompt is structured as follows: Predictor Prompt Trajectory You will be provided with summary of GitHub conversation trajectory. Conversation captures the evolution of discussion by identifying key elements, triggers of tension, conversational strategies, interaction patterns, shifts among participants. It provides concise overview of significant dynamics and tone changes without detailing the technical aspects or specific arguments. sentiments, and tonal Summary Now based on the given GitHub conversation trajectory summary, what is the probability that the conversation may derail and become toxic on scale of 0 to 1 (Toxicity is defined by any rude, disrespectful, or unreasonable comment that is likely to make one leave discussion.)? Conversation trajectory summary: < insert summary > Write exactly one word: the probability rounded to two decimal places. Do not write reasoning. This two-step prompting process allows us to first generate comprehensive summary of the conversation and then use that summary to make more informed prediction about the potential for toxicity. TABLE 4 Derailment prediction results for different models on Derailed Dataset and Representative Dataset. (SCD = Summaries of Conversation Dynamics, = Threshold). Model CRAFT [27] Hua et al. SCD [29] Few-shot SCD Least-tomost SCD () 0.4 0.5 0.6 0.4 0.5 0.6 0.4 0.5 0.6 0.4 0.5 0. Precision 0.20 0.27 0.33 0.64 0.80 0.85 0.45 0.60 0.60 0.58 0.76 0.79 Recall 0.76 0.58 0.47 0.54 0.35 0.32 0.83 0.68 0.66 0.81 0.65 0.61 F1 0.32 0.37 0.39 0.59 0.48 0.47 0.58 0.63 0.62 0.68 0.70 0.68 5.3 Experiment Setup and Metrics We conduct all of our experiments using the LLaMA-3.1-70B model, as it is one of the best open-source state-of-the-art LLMs at the time of writing. We set the model temperature to 0 to minimize output variance and context window size of 8192. We use popular metrics to evaluate classification: Precision, Recall and F1-score. Precision refers to the proportion of true positive observations among all the predicted positive observations. Precision = True Positive True Positive + False Positive Recall represents the proportion of true positive observations out of all actual positive observations in the true class. Recall = True Positive True Positive + False Negative The F1-score is the harmonic mean of Precision and Recall, providing balanced measure of both metrics. F1 score = 2 Recall Precision Recall + Precision For each toxic conversation in our dataset, we provide all the comments up to, but excluding, the first toxic comment. 5.4 Results and Discussion We conduct experiments over using the dataset consisting of the 127 derailed toxic threads and 696 non-toxic threads, total of 823 data points. The results of this experiment are shown in Table 4. As previous research shows that the decision threshold can vary widely for different datasets in conversational derailment prediction [27], we include results from different thresholds: 0.4, 0.5, and 0.6. The CRAFT model consistently underperformed across all thresholds, while the three prompt-based techniques demonstrated superior results. Among these, the GitHub-specific prompts significantly outperformed the generic approaches at all thresholds. The Least-to-Most SCD prompt consistently achieved the highest F1-scores at each threshold, with particularly strong performance in precision compared to the Few-shot SCD prompt. This precision advantage is critical, as higher precision ensures fewer false positives, which is particularly valuable in real-world scenarios where non-toxic conversations vastly outnumber toxic ones [2]. We envision threshold-based intervention strategy to mitigate toxicity: higher thresholds could alert moderators to review flagged content, while lower thresholds could trigger automated bots to issue reminders promoting civil discourse. 5.5 Error Analysis We limited the error analysis to the least-to-most SCD prompt using the threshold of 0.5, which has the highest F1-score. To better understand the performance and limitations of the model and datasets, we conduct an error analysis that focuses on two types of errors: 1) 44 cases where the model predicted toxic conversations as non-toxic; and 2) 26 cases where the model predicted non-toxic conversations as derailing to toxic. Two authors of the paper reviewed the conversations, examined the generated SCD, and determined the most likely reason for the error. They finalized the error categories using card sorting [28], with some cases belonging to more than one category. For instances where the model incorrectly predicted nontoxic conversations as derailing into toxicity, the primary error categories were: acknowledges tensions but overestimates effect (9/26 cases), where the model correctly identified tensions in comment but overestimated their impact, often in cases of technical disagreements; misinterpretation of tones in comments (5/26 cases), where the model misinterpreted positive or neutral comments as negative, such as perceiving constructive feedback as dismissive; and the issue/PR was locked/closed (5/26 cases), where the last comment in the conversation, often from automated bots or users, announced the locking or closure of the issue or pull request. This caused the model often to predict the conversation is going to be toxic. In cases where the model failed to predict toxicity in conversations that derailed, the key error categories were: underestimating or overlooking the seriousness of tone or comment (20/44 cases), where subtle toxic signals were missed or misinterpreted, such as expressions of frustration; conversation context being too extensive for effective analysis (8/44 cases), where the length and complexity of discussions surpassed the models ability to capture relevant nuances, even with large context window; and the derailed uncivil comment is followed by civil comment (4/44 cases), where toxic comments appeared after civil comment. In the later category, it is likely that the juxtaposition of civility and incivility can introduce ambiguity, making it difficult for the model to predict accurately. 5.6 Implications and Recommendations Our study indicates that developing domain-specific approaches to address toxicity in software engineering contexts can offer strong improvements over generic methods. The error analysis reveals that the moderators sometimes failed to lock toxic issues and pull requests. This oversight can be mitigated by implementing an automated proactive 10 moderation system, which can identify and flag potentially toxic interactions early, prompting moderators to take timely action. To operationalize our findings, we propose creating real-time monitoring tools for proactive moderation based on thresholds, such as interactive dashboards and automated bots that can flag at-risk conversations and gently remind users to follow codes of conduct [38], [39]. For example, threshold-based intervention strategy can be adopted to mitigate toxicity. For instance, at higher thresholds, moderators could be prompted to manually review flagged content requiring immediate attention. Conversely, at lower thresholds, automated bots could issue reminders to encourage civil discourse. The generated SCDs also offer an opportunity for more transparent and explainable moderation practices, providing clear justifications for interventions. To address root causes of derailment, we suggest developing targeted interventions based on common triggers and linguistic features associated with toxic interactions. Community education efforts should include resources explaining patterns of conversational derailment and strategies for maintaining constructive dialogue."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Our work builds upon and extends previous research in two main areas: toxicity analysis in software engineering and conversational derailment prediction. 6.1 Toxicity analysis in SE artifacts Researchers have investigated negative interactions, such as offensive language, sentiments, emotions, incivility, tones, and toxicity, and developed automated tools to detect them across different OSS channels, including pull requests, issues, code reviews, Stack Overflow, chat forums and GitHub discussions [4], [5], [40], [41], [42], [43], [44], [45], [25], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [10], [62], [63], [64], [65], [66]. Early software engineering research predominantly focused on sentiment and emotion analysis within software engineering communities [67], [49], [50], [68], [47], [48], [51], [69]. In recent years, researchers focus has shifted towards incivility and toxicity [70], [52], [5], [10], [46]. Raman et al. examined OSS contributors stress and burnout, linking negative interactions to increased dropout rates among developers [2]. Sarker et al. conducted experiment on automatically detecting toxicity in code reviews [5], [41], [40]. Jamieson et al. studied the role of value-related interactions in contributor turnover [71]. Miller et al. explored the dynamics of toxic interactions in OSS projects and emphasized the importance of active moderation and the potential benefits of automated tools for early detection of toxic behavior [22]. Heish et al. examined various moderation strategies and assessed how bots can be helpful [8]. Ferreira et al. and Ehsani et al. [25], [10] analyzed locked GitHub issues to understand the causes and patterns of incivility and provided comprehensive dataset of uncivil conversations. 11 6.2 Conversation derailment 7.3 External validity Online conversations often derail into toxicity, leading to negative user experiences and increased moderation challenges. In recent years, studies have explored methods for predicting and mitigating derailment and toxicity [9], [24], [20], [72], [27], [19], [33], [31], [32], [30], [29]. Chang et al. [27] developed the CRAFT tool to detect conversational derailment by analyzing the flow of past comments. They tested the effectiveness of the tool on Wikipedia and Reddit datasets. Kementchedjhieva et al. [33] used BERT-based model on the same datasets, while Leung et. al. [73] predicted whether Twitter conversations would become unhealthy. ConvoWizard provided users on Reddits r/ChangeMyView with toxicity forecasts, which most users found very useful [74]. Multimodal approaches have also been explored [31]. study examining the structure of toxic conversations on Twitter revealed that toxicity begets toxicity, and that toxic exchanges have larger reply tries but sparse follow graphs [75]. Considering conversation context reduces false positives and negatives in toxicity prediction [76]. Xia et al. identified user propensity, previous toxicity, engagement volume, and community norms as key antecedents of toxicity [19]. In another case, early controversy and conversational deterioration forecasts have been enabled by features from the start of conversation [77], edit tokens in comments [20], and patterns of consecutive user behaviors [78]. Despite advancements, preemptive toxicity detection datasets and models still show limitations [7]. Recent work has explored hierarchical transformers with multitask learning [32], GNN [30], social orientation features [37], and conversation summarization using the latest GPT models [29]."
        },
        {
            "title": "7 THREATS TO VALIDITY",
            "content": "We note potential threats to the validity of our study in the following categories: construct validity, internal validity, and external validity. 7.1 Construct validity This concerns whether our study accurately measures its intended concepts. Toxicity and conversational derailment are subjective, and open to varied interpretations. Our use of GPT-4o for annotation, and LLaMA-3.1:70B for summarizing and predicting toxicity may introduce biases from its training data. We performed rigorous checking during annotation and conducted error analyses to identify these biases. 7.2 Internal validity This relates to the accuracy of our findings, free from external influences. Biases and inconsistencies in our annotation process are potential threats. Despite using multiple annotators and cross-checking, human error and subjective judgment may affect our results. We address this in various ways: through rigorous guidelines for annotators, achieving high inter-annotator agreement, and utilizing model-inthe-loop approach. This addresses the generalizability of our findings. Using only GitHub data may limit applicability to other OSS platforms. Thus, our findings may not directly transfer to platforms like JIRA or non-OSS forums. However, our prompt design methodology is adaptable to any domain. Including diverse datasets from various platforms in future research will help validate and extend our conclusions. Another threat the external validity is the limited dataset that we curated. We note that our empirical observations of toxicity and derailment on GitHub need to be further investigated on larger scale as well as in specific GitHub sub-communities, as our findings may be limited to this specific curated dataset."
        },
        {
            "title": "8 CONCLUSION\nIn this study, we aim to understand and predict conver-\nsational derailment and toxicity on GitHub. Our analysis\nreveals that users are more likely to initiate toxic conver-\nsations, while higher developer engagement correlates with\nlower toxicity levels. We also find that toxic conversations\ntend to be longer and escalate quickly after derailment.\nGenerating conversation trajectory summaries using LLMs,\nwe propose a proactive moderation approach, achieving F1-\nscore of 0.69 with 0.76 precision in predicting conversational\nderailment.",
            "content": "Future work should focus on enhancing our understanding of conversational derailment and toxicity on GitHub and similar platforms. Expanding the dataset to include conversations from additional OSS platforms like JIRA, Gitter and other discussion boards will help validate our findings across different environments and improve their generalizability. Developing and deploying real-time intervention tools that provide immediate feedback during conversations can prevent the escalation of toxic interactions. Understanding the most relevant social orientation tags in the OSS context can be another line of future work."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. S. Qiu, Y. L. Li, S. Padala, A. Sarma, and B. Vasilescu, The signals that potential contributors look for when choosing opensource projects, Proceedings of the ACM on Human-Computer Interaction, no. CSCW, 2019. [2] N. Raman, M. Cao, Y. Tsvetkov, C. Kastner, and B. Vasilescu, Stress and burnout in open source: Toward finding, understanding, and mitigating unhealthy interactions, in Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results, 2020. [3] F. Zlotnick, Github open source survey 2017, http:// [4] [5] [6] opensourcesurvey.org/2017/, Jun. 2017. J. Sarker, A. K. Turzo, and A. Bosu, benchmark study of the contemporary toxicity detectors on software engineering interactions, in 2020 27th Asia-Pacific Software Engineering Conference. IEEE, 2020. J. Sarker, A. K. Turzo, M. Dong, and A. Bosu, Automated identification of toxic code reviews using toxicr, ACM Transactions on Software Engineering and Methodology, 2023. S. Mishra and P. Chatterjee, Exploring chatgpt for toxicity detection in github, in Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results, 2024. [7] C. Schluger, J. P. Chang, C. Danescu-Niculescu-Mizil, and K. Levy, Proactive moderation of online discussions: Existing practices and the potential for algorithmic support, Proceedings of the ACM on Human-Computer Interaction, 2022. [9] [8] J. Hsieh, J. Kim, L. Dabbish, and H. Zhu, nip it in the bud: Moderation strategies in open source software projects and the role of bots, Proceedings of the ACM on Human-Computer Interaction, no. CSCW2, 2023. J. Zhang, J. Chang, C. Danescu-Niculescu-Mizil, L. Dixon, Y. Hua, D. Taraborelli, and N. Thain, Conversations gone awry: Detecting early signs of conversational failure, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2018. [10] R. Ehsani, M. M. Imran, R. Zita, K. Damevski, and P. Chatterjee, Incivility in open source projects: comprehensive annotated dataset of locked github issue threads, in 2024 IEEE/ACM 21st International Conference on Mining Software Repositories. IEEE, 2024. [11] M. Bartolo, A. Roberts, J. Welbl, S. Riedel, and P. Stenetorp, Beat the ai: Investigating adversarial human annotation for reading comprehension, Transactions of the Association for Computational Linguistics, 2020. [12] S. Sanyal, T. Xiao, J. Liu, W. Wang, and X. Ren, Are machines better at complex reasoning? unveiling human-machine inference gaps in entailment verification, in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 10 36110 386. [13] O. Zendel, J. S. Culpepper, F. Scholer, and P. Thomas, Enhancing human annotation: Leveraging large language models and efficient batch processing, in Proceedings of the 2024 Conference on Human Information Interaction and Retrieval, 2024, pp. 340345. [14] F. Gilardi, M. Alizadeh, and M. Kubli, Chatgpt outperforms crowd workers for text-annotation tasks, Proceedings of the National Academy of Sciences, 2023. [15] T. H. Nguyen and K. Rudra, Human vs chatgpt: Effect of data annotation in interpretable crisis-related microblog classification, in Proceedings of the ACM on Web Conference 2024, 2024, pp. 4534 4543. [16] X. Wang, H. Kim, S. Rahman, K. Mitra, and Z. Miao, Humanllm collaborative annotation through effective verification of llm labels, in Proceedings of the CHI Conference on Human Factors in Computing Systems, 2024, pp. 121. [17] Y. Zhu, Z. Yin, G. Tyson, E.-U. Haq, L.-H. Lee, and P. Hui, Apt-pipe: prompt-tuning tool for social data annotation using chatgpt, in Proceedings of the ACM on Web Conference 2024, 2024, pp. 245255. [18] M. Jahan, H. Wang, T. Thebaud, Y. Sun, G. H. Le, Z. Fagyal, O. Scharenborg, M. Hasegawa-Johnson, L. M. Velazquez, and N. Dehak, Finding spoken identifications: Using gpt-4 annotation for an efficient and fast dataset creation pipeline, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 72967306. [19] Y. Xia, H. Zhu, T. Lu, P. Zhang, and N. Gu, Exploring antecedents and consequences of toxicity in online discussions: case study on reddit, Proceedings of the ACM on Human-computer Interaction, vol. 4, no. CSCW2, pp. 123, 2020. [20] C. De Kock and A. Vlachos, beg to differ: study of constructive disagreement in online conversations, in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, 2021. [21] F. Sadeque, S. Rains, Y. Shmargad, K. Kenski, K. Coe, and S. Bethard, Incivility detection in online comments, in Proceedings of the eighth joint conference on lexical and computational semantics, 2019. [22] C. Miller, S. Cohen, D. Klug, B. Vasilescu, and C. Kastner, did you miss my comment or what? understanding toxicity in open source discussions, in 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE), 2022. [23] J. Cheng, M. Bernstein, C. Danescu-Niculescu-Mizil, and J. Leskovec, Anyone can become troll: Causes of trolling behavior in online discussions, in Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing, 2017. [24] S. Levy, R. E. Kraut, J. A. Yu, K. M. Altenburger, and Y.-C. Wang, Understanding conflicts in online conversations, in Proceedings of the ACM Web Conference 2022, 2022. [25] I. Ferreira, B. Adams, and J. Cheng, How heated is it? understanding github locked issues, in 19th International Conference on Mining Software Repositories, 2022. [26] E. Wulczyn, N. Thain, and L. Dixon, Ex machina: Personal attacks seen at scale, in Proceedings of the 26th international conference on world wide web, 2017. [27] J. P. Chang and C. Danescu-Niculescu-Mizil, Trouble on the horizon: Forecasting the derailment of online conversations as 12 they develop, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, 2019. [28] M. Schreier, Qualitative content analysis in practice, 2012. [29] Y. Hua, N. Chernogor, Y. Gu, S. Jeong, M. Luo, and C. DanescuNiculescu-Mizil, How did we get here? summarizing conversation dynamics, in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024. [30] E. Altarawneh, A. Agrawal, M. Jenkin, and M. Papagelis, Conversation derailment forecasting with graph convolutional networks, in The 7th Workshop on Online Abuse and Harms, 2023. [31] Z. Li, M. Rei, and L. Specia, Multimodal conversation modelling for topic derailment detection, in Findings of the Association for Computational Linguistics: EMNLP 2022, 2022. [32] J. Yuan and M. P. Singh, Conversation modeling to predict derailment, in Proceedings of the International AAAI Conference on Web and Social Media, 2023. [33] Y. Kementchedjhieva and A. Sgaard, Dynamic forecasting of conversation derailment, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. [34] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, Decomposed prompting: modular approach for solving complex tasks, in The Eleventh International Conference on Learning Representations. [35] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V. Le et al., Least-to-most prompting enables complex reasoning in large language models, in The Eleventh International Conference on Learning Representations. [36] W. Strus and J. Cieciuch, Towards synthesis of personality, temperament, motivation, emotion and mental health models within the circumplex of personality metatraits, Journal of Research in Personality, 2017. [37] T. Morrill, Z. Deng, Y. Chen, A. Ananthram, C. W. Leach, and K. Mckeown, Social orientation: new feature for dialogue analysis, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, 2024. [38] H. S. Qiu, A. Lieb, J. Chou, M. Carneal, J. Mok, E. Amspoker, B. Vasilescu, and L. Dabbish, Climate coach: dashboard for open-source maintainers to overview community dynamics, in Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023. [39] R. Li, P. Pandurangan, H. Frluckaj, and L. Dabbish, Code of conduct conversations in open source software projects on github, Proceedings of the ACM on Human-computer Interaction, 2021. [40] J. Sarker, S. Sultana, S. R. Wilson, and A. Bosu, Toxispanse: An explainable toxicity detection in code review comments, in 2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). IEEE, 2023. [41] J. Sarker, Identification and mitigation of toxic communications among open source software developers, in Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 15. [42] F. Ebert, F. Castor, N. Novielli, and A. Serebrenik, Confusion in code reviews: Reasons, impacts, and coping strategies, in 2019 IEEE 26th international conference on software analysis, evolution and reengineering. IEEE, 2019. [43] N. Novielli, F. Calefato, D. Dongiovanni, D. Girardi, and F. Lanubile, Can we use se-specific sentiment analysis tools in crossplatform setting? in Proceedings of the 17th International Conference on Mining Software Repositories, 2020. [44] J. Cheriyan, B. T. R. Savarimuthu, and S. Cranefield, Towards offensive language detection and reduction in four software engineering communities, in Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering, 2021. [45] I. Ferreira, J. Cheng, and B. Adams, The shut the f** up phenomenon: Characterizing incivility in open source code review discussions, Proceedings of the ACM on Human-Computer Interaction, no. CSCW2, 2021. [46] I. Ferreira, A. Rafiq, and J. Cheng, Incivility detection in open source code review and issue discussions, Journal of Systems and Software, 2024. [47] M. R. Islam and M. F. Zibran, Sentistrength-se: Exploiting domain specificity for improved sentiment analysis in software engineering text, Journal of Systems and Software, 2018. 13 [67] D. Garcia, M. S. Zanetti, and F. Schweitzer, The role of emotions in contributors activity: case study on the gentoo community, in 2013 International conference on cloud and green computing. IEEE, 2013. [68] D. Gachechiladze, F. Lanubile, N. Novielli, and A. Serebrenik, Anger and its direction in collaborative software development, in 2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track. IEEE, 2017. [69] C. D. Egelman, E. Murphy-Hill, E. Kammer, M. M. Hodges, C. Green, C. Jaspan, and J. Lin, Predicting developers negative feelings about code review, in Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 2020. [70] G. A. A. Prana, D. Ford, A. Rastogi, D. Lo, R. Purandare, and N. Nagappan, Including everyone, everywhere: Understanding opportunities and challenges of geographic gender-inclusion in oss, IEEE Transactions on Software Engineering, 2021. [71] J. Jamieson, N. Yamashita, and E. Foong, Predicting open source contributor turnover from value-related discussions: An analysis of github issues, in Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, 2024. [72] J. Bao, J. Wu, Y. Zhang, E. Chandrasekharan, and D. Jurgens, Conversations gone alright: Quantifying and predicting prosocial outcomes in online conversations, in Proceedings of the Web Conference 2021, 2021. [73] S. Leung and F. Papapolyzos, Hashing it out: Predicting unhealthy conversations on twitter, arXiv preprint arXiv:2311.10596, 2023. [74] J. P. Chang, C. Schluger, and C. Danescu-Niculescu-Mizil, Thread with caution: Proactively helping users assess and deescalate tension in their online discussions, Proceedings of the ACM on Human-Computer Interaction, 2022. [75] M. Saveski, B. Roy, and D. Roy, The structure of toxic conversations on twitter, in Proceedings of the Web Conference 2021, 2021. [76] A. Anuchitanukul, J. Ive, and L. Specia, Revisiting contextual toxicity detection in conversations, ACM Journal of Data and Information Quality, 2022. [77] J. Hessel and L. Lee, Somethings brewing! early prediction of controversy-causing posts from discussion features, in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. [78] J. M. Tshimula, B. Chikhaoui, and S. Wang, On predicting behavioral deterioration in online discussion forums, in 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, 2020. [48] F. Calefato, F. Lanubile, F. Maiorano, and N. Novielli, Sentiment polarity detection for software development, in Proceedings of the 40th International Conference on Software Engineering, 2018. [49] N. Novielli, F. Calefato, and F. Lanubile, Towards discovering the role of emotions in stack overflow, in Proceedings of the 6th international workshop on social software engineering, 2014. [50] M. Ortu, B. Adams, G. Destefanis, P. Tourani, M. Marchesi, and R. Tonelli, Are bullies more productive? empirical study of affectiveness vs. issue fixing time, in 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories. IEEE, 2015. [51] Z. Chen, Y. Cao, X. Lu, Q. Mei, and X. Liu, Sentimoji: an emojipowered learning approach for sentiment analysis in software engineering, in Proceedings of the 2019 27th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering, 2019. [52] S. Cohen, Contextualizing toxicity in open source: qualitative study, in Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2021. [53] M. M. Imran, Y. Jain, P. Chatterjee, and K. Damevski, Data augmentation for improving emotion recognition in software engineering communication, in Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022. [54] M. M. Imran, P. Chatterjee, and K. Damevski, Shedding light on software engineering-specific metaphors and idioms, in Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, 2024. [55] , Uncovering the causes of emotions in software developer communication using zero-shot llms, in Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ser. ICSE 24. New York, NY, USA: Association for Computing Machinery, 2024. [56] A. Serebrenik, Social software engineering, 2022. [57] H. S. Qiu, B. Vasilescu, C. Kastner, C. Egelman, C. Jaspan, and E. Murphy-Hill, Detecting interpersonal conflict in issues and code review: cross pollinating open-and closed-source approaches, in Proceedings of the 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society, 2022. [58] S. D. Gunawardena, P. Devine, I. Beaumont, L. P. Garden, E. Murphy-Hill, and K. Blincoe, Destructive criticism in software code review impacts inclusion, Proceedings of the ACM on HumanComputer Interaction, 2022. [59] H. Hata, N. Novielli, S. Baltes, R. G. Kula, and C. Treude, Github discussions: An exploratory study of early adoption, Empirical Software Engineering, 2022. [60] N. Almarimi, A. Ouni, M. Chouchen, and M. W. Mkaouer, Improving the detection of community smells through sociotechnical and sentiment analysis, Journal of Software: Evolution and Process, 2023. [61] R. Ehsani, R. Rezapour, and P. Chatterjee, Exploring moral principles exhibited in oss: case study on github heated issues, in Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2023. [62] K. Madampe, R. Hoda, and J. Grundy, framework for emotionoriented requirements change handling in agile software engineering, IEEE Transactions on Software Engineering, 2023. [63] B. Wang, X. Zhang, K. Du, C. Gao, and L. Li, Multimodal sentiment analysis under modality deficiency with prototypeaugmentation in software engineering, in 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering. IEEE, 2023. [64] D. Coutinho, L. Cito, M. V. Lima, B. Arantes, J. Alves Pereira, J. Arriel, J. Godinho, V. Martins, P. V. C. Lib orio, L. Leite et al., looks good to me;-): Assessing sentiment analysis tools for pull request discussions, in Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, 2024. [65] J. Tian, L. Bao, S. Pan, and X. Hu, Analyzing and detecting toxicities in developer online chatrooms: fine-grained taxonomy and automated detection approach, in Proceedings of the 31st AsiaPacific Software Engineering Conference (APSEC 2024), 2024. [66] M. S. Rahman, Z. Codabux, and C. K. Roy, Do words have power? understanding and fostering civility in code review discussion, Proceedings of the ACM on Software Engineering, vol. 1, no. FSE, pp. 16321655, 2024."
        }
    ],
    "affiliations": [
        "Drexel University, Philadelphia, PA, USA",
        "Eastern Mennonite University, Harrisonburg, VA, USA",
        "Elmhurst University, Elmhurst, IL, USA",
        "Missouri University of Science and Technology, Rolla, MO, USA",
        "Virginia Commonwealth University, Richmond, VA, USA"
    ]
}