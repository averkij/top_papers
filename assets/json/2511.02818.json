{
    "paper_title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
    "authors": [
        "Mohamed Bouadi",
        "Pratinav Seth",
        "Aditya Tanna",
        "Vinay Kumar Sankarapu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP ."
        },
        {
            "title": "Start",
            "content": "5 2 0 8 1 8 2 0 . 1 1 5 2 : r Mohamed Bouadi, Pratinav Seth Aditya Tanna, Vinay Kumar Sankarapu Lexsi Labs, India & France"
        },
        {
            "title": "Abstract",
            "content": "Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP. Keywords: Tabular Foundation Models, In-Context Learning, AutoML, Benchmarking."
        },
        {
            "title": "1 Introduction",
            "content": "Tabular data remain the most prevalent form of data in real-world applications, spanning critical systems across healthcare, finance, and scientific research. Despite the remarkable progress of deep learning in natural language processing [27, 42] and computer vision [11], gradient boosted trees (GBTs) remain the predominant state-of-the-art (SOTA) for tabular prediction tasks. In other data modalities, foundation modelsparticularly Large Language Models (LLMs) [46, 26]have significantly advanced the ability to tackle new tasks and few-shot learning. This is largely due to their remarkable in-context learning (ICL) capabilities [45, 4], which enable them to capture patterns directly from prompts without updating their parameters. This success combined with the pervasiveness of tables have spurred interest in tabular foundation models [38]. Although LLMs are primarily designed to process natural language, recent efforts have explored fine-tuning them for tabular data tasks [14, 8]. These approaches typically rely on table serialization, which is the process of converting table rows into text or sentences suitable for tokenization. For instance, [9] fine-tuned Llama 3-8B model on large corpus of serialized tables and demonstrated that this strategy can outperform traditional tree-based models in few-shot scenarios. However, such language modelbased approaches face inherent challenges. Their limited context windows Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning restrict the number of serialized examples that can be processed simultaneously (e.g., up to 32 or 64 shots in [9]), and it remains uncertain whether LLMs can reliably interpret and reason over numerical values [37]. Adopting fundamentally different strategy, the authors of [16] introduced TabPFN, transformer-based tabular foundation model designed for classification tasks and pretrained exclusively on synthetic tabular data. key feature of TabPFN is its ability to perform in-context learning directly on tables, removing the need for tokenization and allowing efficient processing of relatively small datasetsup to 1K samples and 100 features. Building on this foundation, TabICL [32] introduced simplified three-component architecture comprising: (1) column-wise embeddings via Set Transformers to capture distribution-aware feature semantics, (2) row-wise interactions with rotary positional encodings to model inter-feature dependencies, and (3) dataset-level ICL prediction through split attention, ensuring clear separation between training and test samples. These developments position tabular foundation models as compelling alternative to traditional approaches, particularly for zero-shot prediction tasks where dataset-specific training is infeasible. However, current table-native ICL architectures face several fundamental limitations that hinder their practical deployment and scalability. First, existing tabular ICL architectures, including TabICL, process features uniformly at single scale, missing hierarchical interaction patterns that naturally occur in real-world tabular data. Just as computer vision benefits from multi-scale processingcapturing edges at fine scales and objects at coarse scalestabular data exhibits structure at multiple granularities: individual features interact locally (e.g., age and income), feature clusters form semantic groups (e.g., demographic attributes), and high-level blocks represent major data divisions (e.g., personal attributes versus behavioral patterns). Processing all features uniformly fails to capture these hierarchical relationships, limiting the models ability to learn robust and interpretable representations. Second, the dense attention mechanisms scale quadratically with feature count (O(m2)), where denotes the number of features. While TabICL addresses sample scalability through its column-then-row architecture, the quadratic feature complexity becomes computationally prohibitive for high-dimensional tables with more than 100 features common in genomics, finance, and sensor applications. For tables with = 100 features, dense attention requires 10, 000 attention operations per layer, with memory requirements growing quadratically. This fundamental scalability barrier limits the practical deployment of tabular foundation models on wide real-world datasets. Third, the strictly sequential processing pipeline in TabICL (column embedding row interaction ICL prediction) prevents iterative refinement and bidirectional information flow between architectural components. While each component produces rich representations, the unidirectional nature of the pipeline means that downstream insights (e.g., dataset-level patterns discovered during ICL) cannot inform upstream representations (e.g., refining feature embeddings based on dataset context). This limitation constrains the models ability to leverage holistic dataset understanding for improved predictions, and prevents the kind of iterative refinement that has proven beneficial in multimodal architectures. To address these limitations, we introduce Orion-MSP, novel tabular foundation model that extends TabICL with three synergistic architectural innovations. First, we propose multi-scale hierarchical feature processing that simultaneously captures interactions at multiple granularities (individual features, groups of 4, and groups of 16), enabling the model to learn representations at different levels of abstraction analogous to hierarchical processing in computer vision. Second, we design structured block-sparse attention patterns combining windowed local attention, global tokens for long-range dependencies, and random connectivity for universal approximation, reducing computational complexity from O(H 2) to O(H.logH) while maintaining expressiveness. Third, we introduce Perceiver-style cross-component memory that enables bidirectional information flow between architectural stages while provably maintaining in-context learning safety constraintsensuring test data never influences training representations through formal ICL safety analysis. The column-wise embedding component of Orion-MSP follows TabICLs approach [32], using Set Transformers with Induced Set Attention Blocks (ISAB) [25] to create distribution-aware feature embeddings in permutation-invariant manner. The multi-scale row interaction component processes these embeddings at multiple resolutions, with each scale using sparse attention patterns tailored to its granularity. The resulting multi-scale representations are aggregated into unified row embeddings, which then interact with the Perceiver memory before proceeding to the final ICL prediction stage. This ICL component employs split attention with label injection, ensuring proper train-test separation. Through extensive experiments across diverse tabular benchmarks, we demonstrate that Orion-MSP achieves competitive accuracy with state-of-the-art tabular ICL methods while enabling scalability to tables with more than 100 features where existing methods fail due to memory constraints. Our work establishes that hierarchical multi-scale processing, structured sparsity, and cross-component memory can simultaneously improve both effectiveness and efficiency in tabular foundation models, opening new application domains previously inaccessible to tabular in-context learning methods. 2 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning"
        },
        {
            "title": "2 Related Work",
            "content": "Tabular In-Context Learning: The application of in-context learning (ICL) to tabular data has recently attracted significant attention. TabPFN [16] pioneered this direction by meta-training transformer on synthetically generated datasets using structural causal models. Its encoderdecoder design allows test samples to attend to training examples, enabling zero-shot predictions without gradient-based fine-tuning. While TabPFN demonstrated strong performance on small datasets, its alternating columnand row-wise attention mechanisms make scaling to larger tables computationally prohibitive. TabDPT [28] showed that comparable performance can be achieved on real-world datasets by using similarity-based retrieval to construct contextual examplesan idea first explored in TabR [12]. The authors extended this paradigm by integrating diffusion-based representation learning, improving robustness to missing values and distributional shifts. However, the diffusion process introduces substantial computational overhead and retains dense attention, limiting scalability. Similarly, TabPFN-v2 [17] introduced cell-based in-context learning, extending row-wise encoding to datasets exceeding 10,000 samples, but it still inherits quadratic attention costs in high-dimensional tables. Building on these foundations, TabICL [32] proposed table-native transformer architecture with three components: column embedding via Set Transformers, row-wise interaction with rotary positional encodings, and an in-context learning prediction module. This design achieved state-of-the-art results across diverse benchmarks while maintaining architectural simplicity and training efficiency. Nonetheless, dense attention in row interactions and the strictly sequential pipeline limit iterative refinement, cross-component communication, and scalability to tables with more than 100 features. ContextTab [35] further enhanced tabular in-context learning by incorporating contextualized feature embeddings and attention mechanisms tailored for heterogeneous tabular data. While improving performance in complex datasets, it still processes features at single scale and relies on dense attention, limiting computational efficiency on high-dimensional tables. Collectively, existing tabular in-context learning models demonstrate strong performance yet share core limitations: dense quadratic attention, uniform single-scale processing, and lack of cross-component feedback. Sparse Attention Mechanisms: Sparse attention techniques from natural language processing offer promising route to improve computational efficiency in tabular in-context learning. BigBird [43] and Longformer [1] demonstrated that block-sparse attention patterns can approximate dense attention with linear complexity while maintaining strong theoretical guarantees. Similarly, Sparse Transformers [5, 20] employ structured sparsity for generative modeling, reducing computation without substantial performance degradation. Despite their success in sequential data, these methods have yet to be systematically adapted for tabular in-context learning, where the primary challenge lies in feature dimension rather than sequence length. Hierarchical and Multi-Scale Architectures: Hierarchical architectures have proven effective in other domains. Funnel Transformers [6] and Swin Transformers [30] use multi-scale processing and pooling to capture information at different resolutions, while Set Transformers [34, 10] leverage pooling by multihead attention for permutation-invariant set processing. Although TabICL [32] employs Set Transformers for column embeddings, it does not incorporate hierarchical multi-scale processing or iterative pooling across feature groups, limiting its ability to model complex interactions in high-dimensional tables. Cross-Component Communication: Cross-component memory and iterative refinement have shown success in multimodal learning. Perceiver [19] and Perceiver IO [18] introduce latent bottlenecks to compress and share information across modalities, and vision-language models [39] leverage iterative cross-attention for refinement. However, these approaches do not address the causal constraints of in-context learning, where test examples must never influence the representation of training data, leaving gap for tabular in-context learning."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "Consider tabular dataset = {(xi, yi)}n where each column cj Rn (j {1, . . . , m}) represents the values of the j-th feature across all samples. i=1 with samples and features. Let Rnm denote the feature matrix, 3 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning In the in-context learning setting, we are given context set of ntrain labeled examples: and set of ntest query samples: = {(xi, yi)}ntrain i=1 = {xi}ntest i=1 Our goal is to predict the conditional distribution of the target for each query sample given the context set:"
        },
        {
            "title": "3.2 High-level Structure: From Data to ICL",
            "content": "p(yx, C), Orion-MSP consists of four core components that collectively enable efficient and generalizable tabular in-context learning: (1) Column Embedding: transforms raw tabular features into dense, semantically meaningful representations; (2) Multi-Scale Sparse Row Interaction: captures dependencies at multiple granularities via an hierarchy of attention scales, combining CLS and GLOBAL tokens for local and long-range connectivity; (3) Cross-Component Perceiver Memory: introduces latent memory bottleneck that enables safe bidirectional communication between modules, promoting iterative refinement without information leakage; (4) Dataset-wise In-Context Learning Predictor: leverages the enriched representations to perform zero-shot prediction across new tasks without gradient updates. An overview of the complete architecture is shown in Figure 1. Orion-MSP extends the original TabICL [32] architecture with three complementary innovations designed to address the fundamental challenges of tabular data processing: computational inefficiency, limited feature interaction modeling, and the need for hierarchical pattern recognition. Our approach maintains the core in-context learning paradigm while introducing architectural enhancements that significantly improve both efficiency and performance."
        },
        {
            "title": "3.3 Column-wise Embedding",
            "content": "Tabular data exhibits unique characteristics compared to other modalities: each column represents distinct feature with its own distribution, scale, and statistical properties (e.g., mean, variance, skewness, kurtosis). To capture these distributional characteristics, we adopt the original TabICL [32] column-wise embedder to map each scalar cell in column cj Rn to d-dimensional representation using shareable Set Transformer, Fcol, that treats the column as permutation-invariant set of values. Our goal is to transform each cell value Xij into d-dimensional embedding Eij Rd that encodes both: 1. The value of the cell (Xij) 2. The distributional context of the column (cj) This differs fundamentally from standard embedding approaches (e.g., word embeddings) where each discrete token has fixed embedding regardless of context. In tabular data, the meaning of value depends heavily on the columns distribution: value of 50 may be typical in one feature but an outlier in another. Concretely, Fcol predicts per-cell affine map, assigning each cell its own weight and bias. The process consists of three main steps: 3.3.1 Initial Projection: Project the column values into d-dimensional embedding space: Uj = Linearproj(cj) Rnd (1) where Linearproj : Rd is learned linear transformation. This creates initial token embeddings for each cell in the column. 3.3.2 Induced Set Attention Blocks (ISAB): To efficiently capture global distributional information while maintaining computational tractability, we employ ISAB [25] with learnable inducing points. It consists of two sequential Multi-Head Attention Blocks (MAB1, MAB2): ) Rkd Mj = MAB1(I, Utrain Vj = MAB2(Uj, Mj, Mj) Rnd , Utrain (2) (3) 4 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Figure 1 An overview of Orion-MSP architecture. First, column-wise embedding transforms input table into embedding vectors E. Next, multi-scale sparse row interaction prepends learnable [CLS] and [GLOBAL] tokens to E, processes features at multiple granularities (scales 1, 4, and 16) with sparse attention transformers, and aggregates [CLS] outputs across scales to yield row embeddings H. Cross-component Perceiver memory enables bidirectional communication: training rows write to latent memory, which all rows read for enhanced representations R. Finally, ICL predicts test labels from in single forward pass. 5 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning where Rkd denote trainable inducing point embeddings (k n), which serve as compressed representation of the column distribution. We define Multi-Head Attention Block as: MAB(Q, K, V) = LayerNorm(H + MultiHead(Q, K, V)) where is residual connection (set to if dimensions match, otherwise passed through projection), and: MultiHead(Q, K, V) = Concat(head1, . . . , headh)WO with each head defined as: headi = Attention(QWi Q, KWi K, VWi ) (4) (5) (6) Following TabICL [32], we use d=128, k=128, 4 heads, and 3 ISAB blocks. Crucially, in Equation 2, we use only Rntraind as keys and values. This ensures that the inducing points Mj capture the distribution training samples Utrain of the training data only, preventing information leakage from test samples during embedding. This is crucial for maintaining the in-context learning paradigm. In Equation 3, all samples (training and test) query the inducing points to obtain their contextualized embeddings. The inducing points act as distributional summary: they encode statistical properties (e.g., mean, variance, skewness) of the training column values, and each cell embedding is adjusted based on where it lies within this learned distribution. 3.3.3 Weight and Bias Generation: The ISAB output Vj is passed through feedforward network to generate cell-specific weights and biases: where: Wj, Bj = FFN(Vj), Wj, Bj Rnd FFN(Vj) = Linearout(GELU(Linearhidden(Vj))) The final embeddings are then computed as: E:,j,: = Wj cj + Bj Rnd (7) (8) (9) where denotes element-wise (Hadamard) product, and cj is broadcasted to shape (n, d). This formulation allows each cells embedding to be function of both its raw value (cj) and the columns learned distributional properties (Wj, Bj). Note that, in our architecture, row-wise interaction requires prepending special tokens (e.g., [CLS], [GLOBAL]) to each row. To accommodate these, the column embedding reserves positions at the beginning of each column: For the reserved positions (indices 1 to C), we use skippable linear layer that outputs zeros or small random values: Rn(m+C)d (10) E:,j,: = (cid:26)SkipLinear(cj) Wj cj + Bj if (reserved) if > (features) (11) where SkipLinear is linear layer with very small initialization, allowing the model to learn appropriate embeddings for reserved positions during training. The Set Transformer architecture ensures that TFcol is permutation-invariant with respect to the order of samples within column. Formally, let π : [T ] [T ] be any permutation, and let = Pπcj where Pπ is the corresponding permutation matrix. Then: TFcol(c j) = Pπ TFcol(cj) (12) 6 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning This property is inherited from the attention mechanism in ISAB, where the softmax normalization and weighted aggregation are invariant to input order. The inducing points Mj Rkd learned by the first MAB serve as distributional summary of column j. Empirically, we observe that: Columns with similar statistical moments (mean, variance, skewness, kurtosis) have similar inducing point representations (measured by cosine similarity). The inducing points capture multi-modal distributions: for categorical features encoded numerically, different modes correspond to different cluster centers in the inducing point space. Outliers in cj receive distinct embeddings, as their attention weights to Mj differ significantly from typical values."
        },
        {
            "title": "3.4 Multi-Scale Sparse Row-Wise Interaction",
            "content": "While column-wise embedding captures distributional properties of individual features, row-wise interaction must model complex dependencies across features to extract meaningful sample representations. However, directly applying dense self-attention to all feature tokens incurs quadratic complexity O(m2) and may overfit when the number of features varies significantly across datasets. To address these challenges, we introduce hierarchical multi-scale sparse attention mechanism that processes features at multiple granularities with efficient block-sparse patterns. 3.4.1 Motivation and Design Principles Tabular datasets exhibit several unique characteristics that complicate feature interaction modeling: 1. Variable feature counts: The number of features varies dramatically across datasets, making fixed-scale architectures suboptimal. 2. Heterogeneous feature relationships: Some features interact locally (e.g., age and age-related health metrics), while others have global dependencies (e.g., categorical indicators). 3. Computational constraints: Dense attention over features has complexity O(m2), becoming prohibitive for wide tables or long context windows. 4. Overfitting risks: Full attention can memorize training-specific feature correlations that do not generalize to new datasets. Inspired by hierarchical representations in vision [41] and multi-resolution modeling in speech [44], Orion-MSP decomposes feature interactions into multiple resolution levels: Fine scale (s = 1): Captures detailed pairwise dependencies between individual features. Coarse scales (s > 1): Aggregates semantically related features into groups, reducing sequence length and enabling broader contextual reasoning. Scale aggregation: Combines representations across scales to balance local precision and global context. Figure 2 Building blocks of the attention mechanism used in Orion-MSP. White color indicates absence of attention. (a) special attention includes CLS = 4 and global attention with GB = 4, (b) sliding window attention with = 8, (c) random attention with = 2, (d) the combined row representation of Orion-MSP model. To further improve efficiency and generalization, we adopt block-sparse attention pattern inspired by Longformer [1] and BigBird [43], as depicted in Figure 2: Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Sliding window attention: Local connectivity within fixed radius w, preserving fine-grained structure. Global tokens: Specialized tokens with full connectivity, ensuring stable long-range information flow. Random links: Optional sparse stochastic connections that enhance expressivity and global reachability.. This design reduces attention complexity from O(m2) to O(m (w + + r)) where w, and are the window size, the number of global tokens, and the number of random links respectively. Formally, RBn(m+C)d to generate row-wise embeddings RBn(N clsd): the multi-scale sparse row-wise transformer, TFMS row, processes column-embedded features = TFMS row(E, dvalid) RBn(Nclsd) (13) where is the number of datasets, the number of samples per dataset, the number of features, and the embedding dimension. The constant = Ncls + Nglobal accounts for special token slots, anddvalid RB optionally indicates the number of valid features per dataset for handling variable-length inputs. The transformation proceeds through the following steps:"
        },
        {
            "title": "3.4.2 Multi-Scale Feature Grouping:",
            "content": "First, for each scale = {s1, s2, . . . , sM } (e.g., = {1, 4, 16}), we group the feature tokens into Ks = m/s groups of size s. The default grouping strategy uses learnable soft grouping via Pooling by Multihead Attention (PMA) [24] to adaptively attend to features: Qs = Seeds + PE(Ks) RKsd Ks = LinearK(E:,:,C:,:) RBnmd Vs = LinearV (E:,:,C:,:) RBnmd (cid:18) QsK As = softmax RKsn (cid:19) Fs = AsVs RBnKsd where Seeds RKsd is learnable seed embedding, and PE(Ks) adds sinusoidal positional encodings. PMA allows the model to learn which features to group together, adapting to dataset-specific correlation structures. 3.4.3 Special Tokens Injection: For each row at each scale, we prepend special tokens: 1. CLS RNclsd (learnable, per-row summary) 2. GLOBAL RNglobald (learnable, long-range connectivity) The full sequence at scale becomes: Xs = [CLS, GLOBAL, G(s)] RBn(Nspecial+Ks)d (14) where Nspecial = Ncls + Nglobal. 3.4.4 Block-Sparse Attention Mask: As depicted in Figure 2, for each scale, we construct sparse attention mask Ms RLsLs where Ls = Nspecial + Ks. The mask follows these rules: 1. Fully Connected Special Tokens: The first Nspecial tokens (CLS and GLOBAL) are fully connected to all other tokens and to each other (Figure 2.a): Ms[i, j] = 0 [1, Nspecial] or [1, Nspecial] (15) 8 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning 2. Sliding Window Attention: Feature tokens (indices > Nspecial) attend to neighbors within window of radius = 8 (Figure 2.b): Ms[i, j] = (cid:26)0 if j and i, > Nspecial otherwise (16) 3. Random Links (Optional): For each feature token > Nspecial, we randomly select additional tokens to attend to (Figure 2.c): Ms[i, jk] = 0 for [1, r], jk Uniform({Nspecial + 1, . . . , Ls} {i}) The final mask is (Figure 2.d): Ms[i, i] = 0 [1, Ls] (self-attention always allowed)"
        },
        {
            "title": "3.4.5 Transformer Encoder per Scale:",
            "content": "For each scale S, we apply dedicated Transformer encoder: where Encoders consists of row blocks/S stacked Transformer blocks with: Zs = Encoders(Xs, Ms) RBnLsd Multi-head self-attention: MHA(Q, K, V, Ms) with sparse mask Ms Rotary positional encoding (RoPE): Applied to queries and keys before attention [36] Feed-forward network: Two-layer MLP with GELU activation Pre-norm architecture: Layer normalization before each sub-layer The multi-head attention with sparse masking is computed as: headi = Attention(Qi, Ki, Vi, Ms) (cid:19) = softmax + Ms Vi (cid:18) QiK dk where Ms contains 0 for allowed positions and for disallowed positions (additive masking). After processing through Encoders, we extract the CLS token representations: Hs = Zs[:, :, 1 : Ncls, :] RBnNclsd (17) (18) (19) (20) (21) (22) These represent the row-wise features at scale s. We then aggregate these representations across all scales by averaging: Hagg = 1 (cid:88) sS Hs RBnNclsd (23) This simple averaging strategy ensures that each scale contributes equally, balancing fine-grained and coarse-grained information. Next, the CLS tokens are flattened and normalized to produce the final row embeddings: = LayerNorm(Flatten(Hagg)) RBn(Nclsd) (24) where Flatten concatenates the Ncls token embeddings. Algorithm 1 summarizes the complete multi-scale sparse row-wise interaction process. 3.4.6 Computational Complexity For given scale with Ks = m/s grouped feature tokens, the per-layer computational complexity of the sparse attention mechanism is: O(B Ls (w + Nglobal + r) d) (25) Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning // Feature Grouping Algorithm 1 Multi-Scale Sparse Row-Wise Interaction (TFMS row) Require: Embeddings RBn(m+C)d, valid features dvalid RB Require: Scales = {s1, s2, . . . , sM }, window w, random links Ensure: Row embeddings Rnm(Nclsd) 1: Initialize learnable tokens CLS RNclsd, GLOBAL RNglobald 2: Hall [] {Store CLS outputs from all scales} 3: for each scale do 4: Ks m/s {Number of groups at scale s} 5: 6: G(s) PMA(E[:, :, :, :], Ks) // Construct Sequence 7: 8: Xs [CLS, GLOBAL, G(s)] {Shape: (B, n, Ls, d) where Ls = Nspecial + Ks} 9: 10: Ms BuildBlockSparseMask(Ls, Nspecial, w, r) // Process Through Transformer 11: Zs Encoders(Xs, Ms) {Transformer with RoPE and sparse attention} 12: // Extract CLS Tokens 13: 14: Hs Zs[:, :, 1 : Ncls, :] 15: Hall.append(Hs) 16: end for 17: // Aggregate Across Scales (cid:80)S 18: Hagg 1 19: // Flatten and Normalize 20: LayerNorm(Flatten(Hagg)) 21: return // Build Sparse Mask s=1 Hall[s] where is the batch size, the number of samples per dataset, the number of features, the embedding dimension, and w, Nglobal, and denote the sliding-window size, number of global tokens, and number of random links, respectively. For scales and total of row blocks Transformer layers distributed evenly across scales, the overall complexity becomes: Ototal = (cid:88) sS O(B Ks (w + Nglobal + r) row blocks ) Since (cid:80) sS Ks (cid:16) 1 + 1 s2 + . . . + 1 sM (cid:17) and typically w, Nglobal, m, this simplifies to: Ototal O(B (w + Nglobal + r) row blocks) (26) (27) compared to the dense attention cost of O(B m2 row blocks). For typical hyperparameters (m [10, 100], = 8, Nglobal = 4, = 2), this results in reduction from quadratic O(m2) to near-linear O(m 14) complexityachieving linear scaling while preserving both local and global feature dependencies."
        },
        {
            "title": "3.5 Cross-Component Memory with Perceiver Architecture",
            "content": "While the column-wise embedding and row-wise interaction components of tabular transformers independently model featureand sample-level dependencies, richer contextual understanding can emerge if information is shared across these components. However, direct cross-component communication poses major risk to the in-context learning (ICL) paradigm: naive attention between components can leak test-set information, violating the principle that predictions for test samples must depend solely on training examples and the test input itself. To overcome this limitation, we introduce Perceiver-style latent memory module [19] that enables safe, leak-free communication between architectural components. This latent memory acts as shared representation space that can be written to by training samples and read from by both training and test samples, ensuring compliance with ICL constraints while promoting global knowledge sharing. 10 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning In standard transformer-based tabular architectures such as TabICL [32], model components operate in strictly sequential and isolated fashion: 1. Column Embedding (TFcol): Encodes feature-wise statistics across samples to capture column-level distributions. 2. Row Interaction (TFrow): Models dependencies across features within each sample. 3. ICL Prediction (TFicl): Performs in-context learning to infer test labels from training examples. This separation simplifies optimization and ensures ICL safety, but also introduces significant limitations: No backward adaptation: Column embeddings cannot adjust based on row-level feature interactions. Limited contextual refinement: Row-level interactions lack access to global, dataset-level statistics beyond static column embeddings. Dataset isolation: Each dataset is processed independently, preventing cross-dataset generalization within batch. fundamental ICL constraint is that test samples must not influence the models internal state in way that affects training representations. Formally, letting Dtrain = {(xi, yi)}ntrain i=1 , Dtest = {xj}n j=ntrain+1 the prediction for test sample xj must satisfy: P(ˆyj Dtrain, Dtest) = P(ˆyj Dtrain, xj) (28) (29) That is, the prediction depends only on the training set and the test features, never on other test representations or their labels. Inspired by the Perceiver architecture [19], we introduce learnable latent memory RP with memory slots. The key idea is: 1. Write Phase (train-only): Memory attends to training representations to extract relevant global patterns. 2. Read Phase (all samples): Both training and test samples attend to the memory to retrieve learned context, but cannot modify it. This asymmetry guarantees ICL safety, since only training data influence the memorys contents. The memory serves as compressed, permutation-invariant summary of the training context that enables consistent feature refinement across samples. The memory module is incorporated inside the ICL transformer (TFicl), refining the row embeddings before label injection and prediction. Given row embeddings RBndh (where is the batch size and the number of samples per dataset), the Perceiver memory transformation produces refined representations: = PerceiverMemory(H, ntrain) RBndH (30) with: dH = Ncls the hidden dimension after multi-head projection, the number of latent memory slots (a hyperparameter), and ntrain the number of labeled training examples. The Perceiver memory consists of three key stages, each composed of multiple cross-attention layers with residual connections and feed-forward transformations. 1. Latent Memory Initialization: We initialize set of learnable latent vectors: L0 HP dH drawn from truncated normal distribution (0, 0.022). These latents act as universal memory bank, shared across all datasets in the batch and reused across forward passes, providing stable foundation for information aggregation. (31) 11 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning 2. Cross-Attention Block: At the core of the memory is cross-attention mechanism allowing one representation to attend to another. Given query set and keyvalue set KV, we define: CrossAttn(Q, KV) = softmax (cid:18) QWQ(KVWK) dk (cid:19) (KVWV ) (32) where WQ, WK, WV RdRdk are projection matrices and dk = dR/h is the per-head dimension. Each cross-attention block is followed by layer normalization, residual connections, and feed-forward layer: = LayerNorm(Q) KV = LayerNorm(KV) = + MultiHeadCrossAttn(Q, KV) = + FFN(LayerNorm(Z)). (33) (34) (35) (36) This block structure ensures stable training and supports multi-head feature integration. 3. Write Phase: Memory Encoding: In the write phase, the memory attends to training samples only to extract and store relevant patterns. For each dataset in the batch: H(b) train = H(b)[: Ttrain, :] RntraindH and initialize the dataset-specific memory as L(b) 0 = L0. We then apply Nwrite cross-attention blocks where memory latents query the training representations: i+1 = CrossAttnBlock(Q = L(b) L(b) , KV = H(b) train) for = 0, . . . , Nwrite 1 The final encoded memory is: L(b) = L(b) Nwrite RP dH Importantly, L(b) depends only on training representations, ensuring no test leakage. (37) (38) (39) 4. Read Phase: Sample Refinement: In the read phase, all samples (training and test) attend to the memory to retrieve stored context. For dataset b: We apply Nread cross-attention blocks where sample queries attend to the memory: H(b) 0 = H(b) RndH R(b) The final refined embeddings are: i+1 = CrossAttnBlock(Q = R(b) , KV = L(b)) for = 0, . . . , Nread 1 This asymmetric readwrite design preserves the integrity of in-context learning: R(b) = R(b) Nread RndR (40) (41) (42) Only training samples write to the memory. Both training and test samples read from it. The memory functions as shared, compressed abstraction of the training data that can be safely leveraged for inference. The complete ICL forward pass with Perceiver memory is described in Algorithm 2:"
        },
        {
            "title": "3.6 Dataset-wise In-Context Learning",
            "content": "After column-wise embedding, multi-scale sparse row-wise interaction, and optional cross-component memory refinement, each sample is represented by fixed-dimensional row embedding: RBndR 12 (43) Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning train) for each dataset = 1 to do L(b) CrossAttnBlock(L(b), H(b) train H(b)[: Ttrain, :] {Extract training samples} H(b) L(b) L0 {Initialize memory} // Write: Memory attends to training samples for = 1 to Nwrite do Algorithm 2 ICL with Perceiver Memory Require: Row embeddings RBndH , training labels ytrain RBntrain Ensure: Predictions ˆy RB(nntrain)C for classes 1: // Perceiver Memory (optional) 2: if > 0 then 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: {Use refined embeddings} 17: end if 18: // Label Injection (training samples only) 19: R[:, : ntrain, :] H[:, : ntrain, :] + OneHot(ytrain)Wlabel 20: // ICL Transformer with Split Mask 21: TFicl(H, attn_mask = ntrain) {Prevent test-to-train leakage} 22: // Prediction Head 23: LayerNorm(R) 24: logits FFNdecoder(R[:, ntrain :, :]) {Predict test labels only} 25: return logits end for // Read: All samples attend to memory R(b) H(b) for = 1 to Nread do R(b) CrossAttnBlock(R(b), L(b)) end for end for where is the number of datasets in the batch, the total number of samples per dataset, and dR the embedding dimension. The final component, dataset-wise in-context learning (TFicl), leverages these embeddings to predict test labels by conditioning on labeled training examplesall within single forward pass and without any gradient-based parameter updates.. Formally, for each dataset in the batch: D(b) train = {(R(b) test = {R(b) D(b) , y(b) }n )}ntrain i=1 j=ntrain+ The objective is to predict test labels ˆy(b) for > ntrain using in-context reasoning from training examples only. The ICL module consists of three main stages: ˆytest = TFicl(R, ytrain) (44) (45) (46) 1. Label Encoding and Injection: To ensure consistency across datasets with potentially different label spaces, training labels ytrain RBntrain are first normalized to contiguous indices: mapping any label set {2, 5, 9} {0, 1, 2}. yi = argsort(unique(ytrain))[ytrain[i]] Normalized labels are embedded using one-hot encoding followed by linear projection: ey = OneHot(y, Cmax) Wy RdR 13 (47) (48) Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning where Cmax is the maximum number of classes (e.g., Cmax = 10), and Wy RCmaxdR is learned projection matrix. Label embeddings are injected only into training samples via additive combination: R[:, : ntrain, :] R[:, : ntrain, :] + ey(ytrain) ensuring test samples remain unaffected and ICL constraints are preserved. (49) 2. Split-Masked Transformer: The augmented embeddings are processed by split-masked Transformer, enforcing ICL-safe attention between training and test samples. The attention mask Msplit is defined as: Msplit[i, j] = if ntrain and ntrain 0 0 if > ntrain and ntrain if ntrain and > ntrain if > ntrain and > ntrain 0 (train-to-train) (test-to-train) (train-to-test: blocked) (test-to-test) No leakage from test to train samples. Training samples attend only to other training samples (learn from labeled context). Test samples attend to training samples and other test samples (contextual reasoning). No leakage from test to train samples. The Transformer applies Nicl blocks of multi-head self-attention and feed-forward layers: H(0) = H(ℓ+1) = TransformerBlock(H(ℓ), Msplit) for ℓ = 0, . . . , Nicl 1 with the final output normalized via: = LayerNorm(H(Nicl)) (50) (51) (52) (53) 3. Prediction head: Test sample representations H[:, ntrain :, :] are passed through two-layer MLP decoder: = GELU(H[:, ntrain :, :]W1 + b1) RBntest2dR logits = zW2 + b2 RBntestCmax Predictions are obtained via softmax with temperature τ : ˆytest = softmax(logits[:, :, : K]/τ ) (54) (55) (56) where is the number of classes in the current dataset (inferred from training labels), and τ = 0.9 by default. When the number of classes > Cmax (e.g., > 10), we employ hierarchical classification strategy: Grouping: Partition (a) Grouping: Partition classes into = K/Cmax balanced groups. (b) First-level prediction: Predict which group test sample belongs to. (c) Second-level prediction: For each group, train classifier on the subset of classes within that group. (d) Combination: Multiply group probability with intra-group probability to obtain final prediction. This hierarchical mechanism preserves the ICL paradigm while scaling to hundreds of classes. During pretraining, the model is trained with cross-entropy loss on test samples: = 1 ntest (cid:88) (cid:88) b=1 j=ntrain+1 log p(y(b) R(b), y(b) train) (57) Critically, gradients flow through the entire architecture (column embedding, row interaction, memory, ICL transformer, decoder) in an end-to-end manner, enabling the model to learn representations optimized for in-context learning. 14 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Algorithm 3 Dataset-wise In-Context Learning Require: Row embeddings RBndR , training labels ytrain RBntrain Ensure: Predictions ˆytest RB(nntrain)K 1: // Optional: Perceiver Memory 2: if memory enabled then 3: PerceiverMemory(R, ntrain) 4: end if 5: // Label Encoding and Injection 6: ytrain NormalizeLabels(ytrain) {Map to {0, 1, . . . , 1}} 7: ey OneHotLinear(ytrain) {Shape: (B, ntrain, dR)} 8: R[:, : ntrain, :] R[:, : ntrain, :] + ey 9: // Split-Masked Transformer 10: Msplit BuildSplitMask(n, ntrain) 11: TFicl(R, Msplit) 12: LayerNorm(H) 13: // Prediction Head 14: Htest H[:, ntrain :, :] {Extract test representations} 15: logits MLPdecoder(Htest) {Shape: (B, Ttest, Cmax)} 16: logits logits[:, :, : K] {Select active classes} 17: ˆytest softmax(logits/τ ) 18: return ˆytest"
        },
        {
            "title": "4 Experimental Evaluation",
            "content": "We conduct comprehensive evaluation of Orion-MSP. Below, we describe our experimental setup and present detailed results."
        },
        {
            "title": "4.1 Experimental Setting",
            "content": "4.1.1 Benchmark Suites and Datasets. Our experimental evaluation spans three widely recognized benchmark suites: TALENT [40] (181 automatically discovered classification datasets), OPENML-CC18 [2] (72 curated datasets), and TABZILLA [29] (36 heterogeneous tasks). Together, these benchmarks enable comprehensive assessment across diverse tabular learning scenarios. In addition, we perform domain-specific evaluations in high-impact application areas such as healthcare and finance to examine the real-world relevance of our method. All experiments strictly follow the official dataset splits provided by each benchmark to ensure reproducibility and fairness. For consistency across model families, results are reported only on the intersection of datasets available to all evaluated models within each benchmark suite. This unified evaluation protocol ensures that observed performance differences arise from methodological advances rather than variations in dataset coverage. After filtering, our evaluation encompasses 154 of 181 datasets from TALENT, 63 of 72 from OpenML-CC18, and 27 of 36 from TabZilla. small number of datasets were excluded due to out-of-memory (OOM) errors or CUDA-related issues, primarily affecting TabPFN-based architectures even on H200 GPUs. Finally, we emphasize that models with higher mean ranks may not always achieve the highest absolute accuracy or F1-scores on every dataset. Rankings based on accuracy are computed per dataset and then averaged across all datasets, providing normalized indicator of overall consistency rather than peak task-specific performance. In contrast, absolute metrics highlight maximum achievable performance on individual tasks. Comprehensive dataset statistics are presented in Appendix C. 4.1.2 Models and Baselines. We compare our model with six state-of-the-art tabular foundation models: TABPFN [16], TABICL [32], ORIONBIX, MITRA, CONTEXTTAB [35], and TABDPT [28]. In addition, we include established traditional baselines using autogloun [7] such as XGBOOST, LIGHTGBM, CATBOOST, and RANDOM FOREST as strong reference models for comparison. 15 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning"
        },
        {
            "title": "4.1.3 Hardware Configuration.",
            "content": "Experiments are executed on NVIDIA L40S GPUs, with H200 GPUs used for memory-intensive cases. This infrastructure ensures consistent execution across all experiments while handling the computational demands of large transformer-based models."
        },
        {
            "title": "4.1.4 Evaluation Metrics.",
            "content": "Our evaluation considers two complementary aspects: Performance. We measure predictive capability using standard classification metricsAccuracy (ACC), AUC-ROC, and weighted F1-score (F1)computed across the benchmark suites TALENT, OpenML-CC18, and TabZilla. These benchmarks encompass datasets with diverse characteristics, including varying sample sizes, feature dimensionalities, and class balance, allowing comprehensive assessment of model generalization. It is important to clarify how MEAN RANK values are derived. Within each benchmark suite, models are ranked by accuracy on every dataset (lower rank = better performance), and these per-dataset ranks are averaged to obtain the overall mean rank. Thus, lower mean rank indicates stronger and more consistent performance across datasets, rather than the highest score on any single task. While absolute metrics (ACCURACY, F1) reflect peak task-level performance, mean rank provides normalized measure of cross-dataset generalization consistency. Scalability. We further analyze model robustness as dataset complexity increases by examining performance trends with respect to sample size, feature dimensionality, and class imbalance. This analysis uses the same benchmark datasets, aggregated along these axes to reveal systematic scalability behaviors and guide practical model selection."
        },
        {
            "title": "4.2 Results",
            "content": "Table 1 Performance comparison across three benchmark suitesTALENT, OpenML-CC18, and TabZilla. Ranks denote the mean rank based on accuracy per benchmark suite (lower is better). Metrics: ACC = Accuracy, F1 = Weighted F1. The All column reports the aggregated rank across all suites. Formatting: 1st place; 2nd place. Models XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN Mitra ContextTab TabDPT All TALENT OpenML-CC Rank Rank ACC F1 6.70 6.43 7.38 6.78 4.96 5.37 3.58 4.61 11.77 9.70 5.42 6.02 5.57 6.15 6.11 4.09 4.59 3.26 3.72 10.38 9.84 5. 0.8403 0.8336 0.8285 0.8331 0.8471 0.8346 0.8461 0.8514 0.3921 0.5474 0.8408 0.8360 0.8259 0.8209 0.8245 0.8379 0.8260 0.8360 0.8412 0.2868 0.4596 0.8318 Rank 5.89 6.25 6.36 6.18 4.69 4.98 4.12 4.76 10.52 6.28 4.64 ACC 0.8558 0.8588 0.8547 0.8581 0.8667 0.8653 0.8722 0.8714 0.3614 0.8639 0.8672 0.8537 0.8520 0.8497 0.8493 0.8623 0.8596 0.8676 0.8663 0.2522 0.8581 0.8625 Rank 6.07 7.13 8.42 5.25 5.89 4.89 3.84 4.86 11.21 7.13 3.94 TabZilla ACC F1 0.8612 0.8579 0.8358 0.8618 0.8734 0.8728 0.8821 0.8752 0.3152 0.8389 0.8814 0.8326 0.8384 0.8399 0.8211 0.8698 0.8628 0.8786 0.8716 0.1830 0.8334 0.8775 Table 1 summarizes results across the TALENT, OpenML-CC18, and TabZilla benchmark suites, reporting mean rank, classification accuracy (ACC), and weighted F1-score (F1) for all evaluated models. Our experiments confirm that classical machine learning methods remain strong baselines, achieving mean accuracies between 0.833 and 0.861 with aggregated ranks around 6.0. In contrast, pretrained tabular foundation models (TFMs) demonstrate superior generalization, even without task-specific fine-tuning. Notably, our model, ORION-MSP, achieves the best overall zero-shot rank of 3.58, with ACC/F1 scores of 0.8461/0.8360 on TALENT, 0.8722/0.8676 on OpenMLCC18, and 0.8821/0.8786 on TabZilla. TABPFN follows closely, attaining an overall rank of 4.61 and scores of 0.8514/0.8412 on TALENT and up to 0.8752/0.8716 on TabZilla. TABDPT ranks 5.42, achieving 0.8408/0.8318 on TALENT and 0.8814/0.8775 on TabZilla. By contrast, Mitra (rank 11.77, ACC < 0.40) and ContextTab (rank 9.70) perform substantially worse, highlighting the advantages of hierarchical multi-scale processing and efficient attention in Orion-MSP. Overall, TABPFN and ORION-MSP emerge as the strongest models, with ACC ranging from 0.85 to 0.88 and ranks between 3.26 and 4.61. ORION-MSP peaks on OpenML-CC18 (rank 4.12, ACC 0.8722) and TabZilla (rank 3.84, ACC 16 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Table 2 Performance variation by dataset size across all benchmark suites. Rank denotes the accuracy-based ranking per size category (lower is better). ACC = Accuracy; F1 = Weighted F1-score, averaged across datasets within each size category: Small (<1K samples), Medium (1K10K), and Large (>10K). Models XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN Mitra ContextTab TabDPT Small (<1K) Medium (1K-10K) Large (>10K) Rank 7.70 7.88 8.55 7.80 6.04 6.32 5.93 6.50 13.88 9.60 5.48 ACC F1 0.8168 0.8124 0.7988 0.8143 0.8301 0.8330 0.8232 0.8325 0.4334 0.7578 0.8333 0.7964 0.7935 0.8187 0.7789 0.8338 0.8150 0.8194 0.8131 0.3236 0.7363 0.8271 Rank 6.88 6.47 7.16 6.94 4.77 5.48 3.70 3.81 11.59 9.52 5.40 ACC F1 0.8363 0.8340 0.8285 0.8314 0.8486 0.8348 0.8494 0.8557 0.3600 0.6210 0.8424 0.8314 0.8264 0.8221 0.8226 0.8398 0.8260 0.8402 0.8462 0.2553 0.5566 0.8339 Rank 5.41 5.48 7.30 5.63 4.61 4.42 3.04 5.73 11.11 10.22 5.26 ACC F1 0.8969 0.8797 0.8694 0.8827 0.8802 0.8729 0.8843 0.8783 0.3837 0.6388 0.8831 0.8920 0.8733 0.8628 0.8764 0.8743 0.8670 0.8768 0.8713 0.2754 0.5638 0.8765 Table 3 Performance variation by feature dimensionality (dataset width) across all benchmark suites. Rank denotes the accuracy-based ranking averaged within each width category (lower is better). ACC = Accuracy; F1 = Weighted F1-score. Values are on 01 scale (higher is better). Formatting: 1st place ; 2nd place within each group. Models XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN Mitra ContextTab TabDPT Narrow (<10) Medium (10-100) Wide (>100) Rank 6.77 5.63 7.15 6.15 5.14 4.64 3.76 5.30 11.25 9.52 4.66 ACC F1 0.8222 0.8145 0.8005 0.8128 0.8208 0.8112 0.8394 0.8187 0.3737 0.6391 0.8262 0.8159 0.8067 0.7044 0.7907 0.8119 0.8043 0.8314 0.8092 0.2683 0.5719 0.8189 Rank 6.90 6.88 7.44 6.92 4.61 5.46 4.09 4.07 11.84 9.59 5.45 ACC F1 0.8482 0.8441 0.8410 0.8458 0.8627 0.8510 0.8572 0.8676 0.3886 0.6480 0.8566 0.8410 0.8344 0.8235 0.8326 0.8549 0.8417 0.8478 0.8589 0.2781 0.5843 0.8483 Rank 4.79 5.50 7.52 7.47 6.46 6.73 5.69 6.141 13.03 10.97 7.23 ACC F1 0.9140 0.9157 0.9034 0.8999 0.9101 0.8859 0.8860 0.9129 0.2521 0.6017 0.8845 0.9039 0.9084 0.8936 0.8908 0.8936 0.8849 0.8837 0.9111 0.1497 0.5651 0.8820 0.8821), while TABPFN leads on TALENT (ACC 0.8514) and maintains stable performance across all benchmark suites. To further investigate the sources of Orion-MSPs performance gains, we analyze results across key dataset characteristics. All analyses partition datasets based on inherent properties rather than performance outcomes. Dataset Size. Table 2 reports model performance aggregated by dataset size: Small (< 1K samples), Medium (1K-10K), and Large (> 10). Performance trends reveal that Orion-MSP consistently performs well across small, medium, and large datasets. Classical ML models such as XGBoost excel on large datasets due to abundant training examples, achieving the highest ACC/F1 in the > 10K sample category. Orion-MSP, however, maintains competitive performance across all size categories, outperforming most baselines on small and medium datasets. This demonstrates the ability of multi-scale sparse attention to generalize effectively in low-data regimes while scaling gracefully to larger datasets. TabPFN also performs strongly, particularly on medium-sized datasets, but Orion-MSPs consistent performance across size scales highlights the robustness of its hierarchical and sparse design. Feature Dimensionality. Table 3 presents performance trends across narrow (< 10 features), medium (10 - 100) and wide ( > 100) datasets. When evaluating dataset width, Orion-MSP shows the highest accuracy on narrow datasets (<10 features) and strong performance on medium and wide datasets (10100 and >100 features). This suggests that sparse multi-scale attention enables effective learning even in high-dimensional feature spaces, where dense models such as TabICL exhibit diminished scalability to high-dimensional feature spaces. Based on Class Imbalance. Partitioning datasets based on class balance reveals that Orion-MSP achieves its strongest gains on imbalanced datasets. The model ranks second in this category, achieving ACC = 0.8840 and F1 = 0.8731. This highlights that multi-scale sparse attention amplifies signals from underrepresented classes while avoiding overfitting to dominant classes. On balanced datasets, performance gains are smaller, suggesting that the architectural complexity of Orion-MSP is most advantageous when datasets exhibit skewed distributions. In comparison, TabPFN maintains strong performance on both balanced and imbalanced datasets, but Orion-MSPs design more effectively addresses minority-class patterns due to hierarchical attention and cross-scale reasoning. 17 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Table 4 Performance variation by class imbalance across all benchmark suites. ACC = Accuracy; F1 = Weighted F1-score, averaged within each imbalance category. Rank denotes the mean rank within each category (lower is better). Formatting: 1st place ; 2nd place within each group. Models XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN Mitra ContextTab TabDPT Balanced ( 0.6) Imbalanced (<0.6) Rank 7.00 7.15 7.92 7.32 4.72 5.65 4.22 3.85 12.26 9.66 5.16 ACC F1 0.8175 0.8076 0.7983 0.8071 0.8279 0.8096 0.8265 0.8367 0.2763 0.5079 0.8233 0.8110 0.8020 0.7955 0.7977 0.8233 0.8040 0.8202 0.8309 0.1540 0.4487 0.8189 Rank 6.23 5.65 6.77 6.19 5.08 5.04 3.38 5.37 11.24 9.72 5.65 ACC F1 0.8859 0.8785 0.8741 0.8775 0.8806 0.8787 0.8840 0.8808 0.4794 0.7850 0.8798 0.8785 0.8665 0.8646 0.8633 0.8698 0.8683 0.8731 0.8697 0.3858 0.7192 0.8690 Domain-specific Analysis. Domain-wise evaluation provides deeper insight into Orion-MSPs strengths  (Table 5)  : Medical datasets: Orion-MSP achieves the highest ACC = 0.8045 and F1 = 0.7916, ranking second overall behind Orion-BiX. These datasets often involve hierarchical biological structures and complex interdependencies among features, which align naturally with Orion-MSPs multi-scale representation. Fine-grained scales capture local dependencies, while coarser scales aggregate contextual information, leading to improved predictive accuracy. Finance datasets: Orion-MSP ranks first in mean rank (4.60), achieving ACC = 0.8158 and F1 = 0.8047. Financial datasets frequently involve layered dependencies between assets, instruments, and market indicators. Orion-MSPs cross-component memory allows information to propagate across scales, capturing global dependencies that standard dense transformers or classical ML models fail to exploit. Overall, domain-specific results highlight that Orion-MSP excels in high-dimensional, context-rich datasets, where hierarchical patterns and feature correlations are prevalent. Table 5 Domain-specific performance for Medical and Finance datasets from the benchmark suites. Rank denotes the mean rank within each domain (lower is better). ACC = Accuracy; F1 = Weighted F1-score (01 scale, higher is better). Formatting: 1st place; 2nd place within each group. Models XGBoost RandomForest CatBoost LightGBM TabICL OrionBiX OrionMSP TabPFN Mitra ContextTab TabDPT Rank 6.32 6.38 6.36 5.32 5.54 4.10 4.50 5.04 10.77 8.66 6. Medical ACC F1 0.7834 0.7779 0.7784 0.7949 0.7819 0.7893 0.8045 0.7984 0.3935 0.6681 0.7764 0.7669 0.7752 0.7594 0.7614 0.7696 0.7759 0.7916 0.7857 0.2863 0.6129 0.7641 Rank 6.62 7.32 5.82 6.17 6.60 5.39 4.60 7.17 13.67 11.25 8.00 Finance ACC F1 0.7958 0.8052 0.8117 0.8095 0.8125 0.8206 0.8158 0.8094 0.5340 0.7430 0.8080 0.7885 0.8001 0.8015 0.7974 0.7942 0.8125 0.8047 0.7919 0.4250 0.6834 0. Deep Analysis and Interpretation detailed examination by dataset characteristics demonstrates why Orion-MSPs design is most effective under certain conditions: Class imbalance: Multi-scale sparse attention amplifies underrepresented patterns without overfitting to majority classes. Minority-class recognition improves substantially on datasets where the minority class constitutes less than 30% of the data. Balanced datasets show smaller gains, indicating that the hierarchical complexity is most beneficial in skewed settings. Hierarchical structure and cross-component memory: In domains such as healthcare and finance, datasets involve natural hierarchies and complex inter-feature relationships. Orion-MSPs multi-scale design allows 18 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning it to reason at both fine-grained and coarse-grained levels. Sparse attention reduces computational cost and provides implicit regularization, mitigating overfitting in high-dimensional or correlated-feature settings. Cross-component memory further enables information exchange across scales without violating ICL safety, enhancing performance on context-dependent tasks. Computational efficiency: Linear attention complexity with respect to feature number and attention window size allows Orion-MSP to scale to high-dimensional tables. Memory usage grows proportionally with input dimensions, making the model practical for large real-world datasets, unlike dense attention alternatives with quadratic scaling. In short, fine-grained scales capture subtle minority-class patterns, while coarser scales aggregate global context, yielding balanced representations of local and global dependencies. Sparse attention improves efficiency and regularization, reducing overfitting in high-dimensional or correlated-feature settings. The Perceiver memory enhances the models capacity to store and retrieve non-local patterns, enabling cross-scale reasoningparticularly valuable in contextdependent domains. However, the added architectural complexity offers limited benefit for simpler, low-dimensional datasets, suggesting future directions in adaptive designs with data-driven scale selection and dynamic sparsity control."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced Orion-MSP, novel tabular in-context learning model that leverages multi-scale sparse attention and cross-component memory to capture both fine-grained and coarse-grained dependencies in tabular data. Through extensive experiments across diverse benchmark suitesincluding TALENT, OpenML-CC18, and TabZillaas well as domain-specific datasets in healthcare and finance, we demonstrated that Orion-MSP consistently achieves state-of-the-art zero-shot performance, particularly on imbalanced, high-dimensional, and context-rich datasets. Our detailed analyses highlight that the hierarchical design, sparse attention, and cross-component memory collectively contribute to robust generalization, efficient computation, and improved representation of complex interdependencies. These architectural choices enable Orion-MSP to outperform existing tabular foundation models in challenging realworld scenarios while maintaining practical scalability. Nonetheless, we observe that the benefits of multi-scale sparse attention are less pronounced on simple, low-dimensional datasets, where the additional architectural complexity may not be fully leveraged. This limitation motivates future work on adaptive scale selection and data-aware sparsity scheduling, allowing model complexity to adjust dynamically to dataset characteristics. Such extensions could further enhance both efficiency and generality, enabling Orion-MSP to provide strong performance across the full spectrum of tabular learning tasks. In summary, Orion-MSP represents promising step toward scalable, adaptive, and context-aware tabular in-context learning, with significant potential for real-world applications and future improvements in dynamic model adaptation."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. [2] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang, Rafael Mantovani, Jan van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv preprint arXiv:1708.03731, 2017. [3] Stephan Bongers, Patrick Forré, Jonas Peters, and Joris Mooij. Foundations of structural causal models with cycles and latent variables. The Annals of Statistics, 49(5):28852915, 2021. [4] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In Michael D. Bailey and Rachel Greenstadt, editors, 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pages 26332650. USENIX Association, 2021. [5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. [6] DongHyun Choi, Lucas Spangher, Chris Hidey, Peter Grabowski, and Ramy Eskander. Revisiting funnel transformers for modern LLM architectures with comprehensive ablations in training and inference configurations. CoRR, abs/2504.02877, 2025. [7] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505, 2020. 19 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning [8] Xi Fang, Weijie Xu, Fiona Anting Tan, Ziqing Hu, Jiani Zhang, Yanjun Qi, Srinivasan H. Sengamedu, and Christos Faloutsos. Large language models (llms) on tabular data: Prediction, generation, and understanding - survey. Trans. Mach. Learn. Res., 2024, 2024. [9] Josh Gardner, Juan C. Perdomo, and Ludwig Schmidt. Large scale transfer learning for tabular data via language modeling. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [10] Roger Girgis, Florian Golemo, Felipe Codevilla, Martin Weiss, Jim Aldon DSouza, Samira Ebrahimi Kahou, Felix Heide, and Christopher Pal. Latent variable sequential set transformers for joint multi-agent motion prediction. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [11] Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, et al. Battle of the backbones: large-scale comparison of pretrained models across computer vision tasks. Advances in Neural Information Processing Systems, 36:29343 29371, 2023. [12] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, and Artem Babenko. Tabr: Tabular deep learning meets nearest neighbors. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [13] Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems (NeurIPS), 35:507520, 2022. [14] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David A. Sontag. Tabllm: Few-shot classification of tabular data with large language models. In Francisco J. R. Ruiz, Jennifer G. Dy, and Jan-Willem van de Meent, editors, International Conference on Artificial Intelligence and Statistics, 25-27 April 2023, Palau de Congressos, Valencia, Spain, volume 206 of Proceedings of Machine Learning Research, pages 55495581. PMLR, 2023. [15] Hendrycks. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [16] Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. Tabpfn: transformer that solves In The Eleventh International Conference on Learning small tabular classification problems in second. Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [17] Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with tabular foundation model. Nat., 637(8044):319326, 2025. [18] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and João Carreira. Perceiver IO: general architecture for structured inputs & outputs. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [19] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. [20] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 98959907, 2021. [21] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [22] Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017. [23] Alex Krizhevsky, Geoff Hinton, et al. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 40(7):19, 2010. [24] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: framework for attention-based permutation-invariant neural networks. In International conference on machine learning, pages 37443753. PMLR, 2019. [25] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: framework for attention-based permutation-invariant neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 20 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 37443753. PMLR, 2019. [26] Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, and Philip S. Yu. When llms meet cunning texts: fallacy understanding benchmark for large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [27] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM computing surveys, 55(9):135, 2023. [28] Junwei Ma, Valentin Thomas, Rasa Hosseinzadeh, Hamidreza Kamkari, Alex Labach, Jesse C. Cresswell, Keyvan Golestan, Guangwei Yu, Maksims Volkovs, and Anthony L. Caterini. Tabdpt: Scaling tabular foundation models. CoRR, abs/2410.18164, 2024. [29] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan, Vishak Prasad, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In Advances in Neural Information Processing Systems, 2023. [30] Li Meng, Morten Goodwin, Anis Yazidi, and Paal Engelstad. Deep reinforcement learning with swin transformers. In Proceedings of the 8th International Conference on Digital Signal Processing, ICDSP 2024, Hangzhou, China, February 23-25, 2024, pages 205211. ACM, 2024. [31] Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In International Conference on Learning Representations (ICLR), 2022. [32] Jingang Qu, David Holzmüller, Gaël Varoquaux, and Marine Le Morvan. Tabicl: tabular foundation model for in-context learning on large data. CoRR, abs/2502.05564, 2025. [33] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007. [34] Kira A. Selby, Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, and Pascal Poupart. Learning functions on multiple sets using multi-set transformers. In James Cussens and Kun Zhang, editors, Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands, volume 180 of Proceedings of Machine Learning Research, pages 17601770. PMLR, 2022. [35] Marco Spinaci, Marek Polewczyk, Maximilian Schambach, and Sam Thelin. Contexttab: semantics-aware tabular in-context learner. CoRR, abs/2506.10707, 2025. [36] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [37] Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro A. Szekely. Representing numbers in NLP: survey and vision. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 644656. Association for Computational Linguistics, 2021. [38] Boris van Breugel and Mihaela van der Schaar. Position: Why tabular foundation models should be research priority. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [39] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and In The Huaxiu Yao. Mmed-rag: Versatile multimodal RAG system for medical vision language models. Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [40] Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. closer look at deep learning methods on tabular datasets. arXiv preprint arXiv:2407.00956, 2024. [41] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems, 31, 2018. [42] Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. Natural language reasoning, survey. ACM Computing Surveys, 56(12):139, 2024. [43] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, 21 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [44] Neil Zeghidour and David Grangier. Wavesplit: End-to-end speech separation by speaker clustering. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:28402849, 2021. [45] Jingyang Zhang, Jingwei Sun, Eric C. Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao (Frank) Yang, and Hai Li. Min-k%++: Improved baseline for pre-training data detection from large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [46] Yujia Zhou, Zheng Liu, and Zhicheng Dou. Boosting the potential of large language models with an intelligent information assistant. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024."
        },
        {
            "title": "A Pretraining and Implementation Details",
            "content": "A.1 Pretraining Data Generation Following the pretraining paradigm of TabICL [32], we train our model on synthetically generated datasets to learn generalizable representations for in-context learning on tabular data. Unlike natural language or vision domains where large-scale real data is available, tabular datasets exhibit extreme heterogeneity in schemas, distributions, and task objectives. Synthetic data generation via structural causal models (SCMs) enables us to control dataset diversity while ensuring coverage of diverse statistical patterns. A.1.1 Structural Causal Model (SCM) Prior We generate synthetic datasets using SCM-based priors [3, 31], where features are related through nonlinear causal relationships. For dataset with features, we define directed acyclic graph (DAG) = (V, E) where each node represents feature, and edges encode causal dependencies. Each feature Xj is computed as: where Pa(Xj) are the parent features of Xj in G, fj is nonlinear activation function, and ϵj (0, σ2 noise. ) is Gaussian Xj = fj(Pa(Xj), ϵj) (58) Activation Function Diversity To ensure broad coverage of feature transformations observed in real-world tabular data, we used the following activation functions: Identity, tanh, LeakyReLU, ELU Standard nonlinearities: ReLU, ReLU6 [23], SELU [22], SiLU (Swish) [15], Softplus Bounded functions: Hardtanh(x) = max(1, min(1, x)), Signum function sgn(x) Periodic functions: sin(x) (captures cyclic patterns, e.g., time-of-day) Radial basis function: RBF(x) = exp(x2) (models local interactions) Exponential growth/decay: exp(x) (models compounding effects, e.g., financial data) Power functions: (x) = (cid:112)x, (x) = x2, (x) = (models scaling relationships) Indicator function: (x) = x1 (models threshold effects) Random Fourier features: (x) = ϕ(x)z where (0, I) and the feature map ϕ : RN is defined as: ϕi(x) := wi w2 sin(aix + bi), {1, . . . , } (59) with = 256, bi U[0, 2π], ai U[0, ], and wi := exp(u) function approximates complex, non-parametric relationships [33]. where U[0.7, 3.0]. This random 22 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning For each feature Xj, an activation function fj is sampled uniformly from this extended set, ensuring diverse nonlinear transformations across the dataset. A.1.2 Tree-Based SCM Prior To complement the continuous SCM prior, we also generate datasets using tree-based SCM prior [13], where the causal mechanism fj is decision tree or random forest. This prior is particularly important for modeling categorical interactions and hierarchical decision boundaries commonly observed in real-world tabular data (e.g., credit scoring, medical diagnosis). For each feature Xj, we construct random decision tree Tj with: Splitting criteria: Random thresholds on parent features Pa(Xj) Leaf values: Sampled from Gaussian or uniform distribution Tree depth: Sampled uniformly from {1, 2, 3, 4} to vary complexity The tree-based prior generates datasets with piecewise-constant relationships, contrasting with the smooth transformations of the MLP-based SCM prior. A.2 Pretraining Details We employ three-stage curriculum learning strategy that progressively increases dataset size (number of samples per dataset) and refines different architectural components. 1. Stage 1 (25K steps, 2,048 datasets) trains all components end-to-end with NB = 8 micro-batches for gradient accumulation, where each dataset contains fixed size of 1,024 samples. This stage establishes foundational representations across column embedding, multi-scale sparse row interaction, Perceiver memory, and ICL prediction. 2. Stage 2 (2K steps, 512 datasets) reduces micro-batch size to NB = 1 and samples dataset sizes from log-uniform distribution Ulog[1024, 40000], exposing the model to variable context lengths while maintaining architectural diversity. Within each micro-batch, all datasets share the same sample count, but this count varies across micro-batches. 3. Stage 3 (50 steps, 512 datasets) focuses exclusively on long-context ICL by freezing all components except TFicl and sampling dataset sizes uniformly from U[40000, 60000], ensuring robust in-context learning on large datasets. Across all stages, we use the Adam optimizer [21] with gradient norm clipping at 1.0 and learning rate schedule shown in Figure 3. This curriculum, progressing from small, uniform datasets to large, variable datasets with selective fine-tuning, enables the model to generalize effectively across diverse dataset scales while preventing overfitting to specific sample counts. (a) Cosine decay for stage 1 (b) Polynomial decay for stage (c) Constant lr for stage 3 Figure 3 Learning rate schedules for pretraining stages. A.3 Implementation Details This section provides comprehensive hyperparameters and training configurations for all architectural components of ORION-MSP. Our implementation is built on PyTorch and trained on NVIDIA H200 GPUs. 23 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning A.3.1 Column-wise Embedding (TFcol)"
        },
        {
            "title": "Hyperparameters",
            "content": "Embedding dimension: = 128 Number of inducing points: = 128 Attention heads: = 4 (head dimension dk = 32) ISAB blocks: = 3 Feedforward dimension: dff = 256 Dropout rate: = 0.0 Activation: GELU Layer norm: Pre-norm"
        },
        {
            "title": "Initialization and Training Considerations",
            "content": "Inducing points initialized from (0, 0.022) Linear and attention weights: Xavier/Glorot uniform Column dropout: pcol = 0.1 Gradient clipping: max-norm = 1.0 A.3.2 Multi-Scale Sparse Row Interaction Hyperparameters blocks = 6 (2 per scale) Embedding dimension: = 128 Transformer blocks: row Scales: = {1, 4, 16} Attention heads: = 8 Window size: = 8, Global tokens: Nglobal = 4, Random links: = 2 Dropout: = 0.0 Positional encoding: RoPE (θ = 100000) Training Details Sparse attention via PyTorch SDPA Cosine learning rate schedule with 5% warmup Mixed precision: FP16 (forward), FP32 (softmax) A.3.3 Cross-Component Perceiver Memory Hyperparameters Memory slots: = 32 Write layers: Nwrite = 2, Read layers: Nread = 2 Attention heads: = 4 Feedforward dimension: dff = 2dR Dropout: = 0.0 Activation: GELU, Layer norm: Pre-norm Training Considerations Random initialization of memory L0 (0, 0.022) Gradient clipping (max-norm = 1.0) Memory disabled (P = 0) for ablation 24 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning A.3.4 Dataset-wise In-Context Learning"
        },
        {
            "title": "Hyperparameters",
            "content": "Transformer blocks: Nicl = 12 Embedding dimension: dR = 512 Feedforward dimension: dff = 1024 Max classes: Cmax = 10 Temperature: τ = 0.9 Dropout: = 0.0 Activation: GELU Layer norm: Pre-norm"
        },
        {
            "title": "Initialization",
            "content": "Label encoder, Transformer, and decoder weights: Xavier/Glorot uniform Layer norm: γ = 1, β ="
        },
        {
            "title": "B Further Experiments",
            "content": "Figure 4 presents the relative accuracy improvement (%) of each method over XGBoost, evaluated per dataset across the three benchmarks: TALENT (Figure 4a), TabZilla (Figure 4b), and OpenML-cc18 (Figure 4c). Each point corresponds to per-dataset delta, while boxplots summarize the distribution (median, interquartile range, and whiskers). The dashed vertical line denotes parity with XGBoost (0%). Across all three benchmarks, Orion-MSP consistently improves upon the strong XGBoost baseline. On TabZilla and OpenML-cc18, Orion-MSP achieves positive median relative accuracy with compact interquartile range and few negative outliers, indicating both higher accuracy and greater reliability. On TALENT, Orion-MSP reaches parity in median performance but exhibits lower variance than most neural baselines. Among classical boosted trees, LightGBM, CatBoost, and Random Forest cluster tightly around parity, showing comparable behavior. In contrast, tabular foundation models (TFMs), notably Mitra and ContextTab, exhibit pronounced negative shifts and high variance. TabPFN and TabICL perform competitively and occasionally outperform Orion-MSP on specific datasets, yet their broader variance and heavier left tails reveal less consistent behavior. Overall, Orion-MSP matches or surpasses their central performance and achieves the best meanvariance trade-off, confirming the benefits of our model design for tabular generalization. To further quantify cross-dataset performance, we computed per-dataset ranks across all 11 methods (lower is better) for each benchmark, averaged them over datasets, and conducted one-way Friedman test to assess overall differences. When significant, Nemenyi post-hoc tests were applied, and the resulting critical difference (CD) diagrams were plotted at α = 0.05; methods connected by the CD bar are not significantly different. As shown in Figure 5, Orion-MSP attains the best average rank on all three benchmarks3.09 on TALENT, 3.32 on TabZilla, and 3.35 on OpenML-cc18appearing as the leftmost method in each CD diagram. TabICL, TabPFN, and TabDPT follow closely, typically lagging by 0.51.5 rank points. Tree ensembles (XGBoost, LightGBM, CatBoost, Random Forest) occupy middle tier with average ranks of 4.66.2, showing parity among themselves but consistent gap to Orion-MSP and the stronger TFMs. Finally, ContextTab and Mitra form the rightmost group (ranks 910), confirming the underperformance seen in the improvement plots. In summary, the CD diagrams corroborate our main finding: Orion-MSP is the top performer across diverse tabular benchmarks, outperforming tree ensembles on average and matching or exceeding pretrained tabular foundation models while maintaining favorable significance profile."
        },
        {
            "title": "C Datasets",
            "content": "Full details of all datasets and benchmarks are summarized in below, with their row and column distributions visualized in Figure 6. OpenML-CC18 Benchmark Datasets Table 6 lists all datasets from the OpenML-CC18 benchmark suite used in our evaluation. 25 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning (a) Relative accuracy improvement over XGBoost on TALENT Benchmark (b) Relative accuracy improvement over XGBoost on TabZilla Benchmark (c) Relative accuracy improvement over XGBoost on OPENML-CC18 Benchmark Figure 4 Relative accuracy improvement over XGBoost on three benchmarks. 26 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning (a) Relative accuracy improvement over XGBoost on TALENT Benchmark (b) Relative accuracy improvement over XGBoost on TabZilla Benchmark (c) Relative accuracy improvement over XGBoost on OPENML-CC18 Benchmark Figure 5 Figure 6 Column and row distribution of the evaluated datasets. 27 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning"
        },
        {
            "title": "TALENT Benchmark Datasets",
            "content": "Table 7 lists all datasets from the TALENT benchmark suite used in our evaluation."
        },
        {
            "title": "TabZilla Benchmark Datasets",
            "content": "Table 8 lists all datasets from the TabZilla benchmark suite. TabZilla uses OpenML dataset IDs, and these datasets are specifically selected for evaluating neural network performance on tabular data. Table 6 OpenML-CC18 benchmark datasets (72 datasets). Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML-ID-3 OpenML-ID-6 OpenML-ID-11 OpenML-ID-12 OpenML-ID-14 OpenML-ID-15 OpenML-ID-16 OpenML-ID-18 OpenML-ID-22 OpenML-ID-23 OpenML-ID-28 OpenML-ID-29 OpenML-ID-31 OpenML-ID-32 OpenML-ID-37 OpenML-ID-38 OpenML-ID-44 OpenML-ID-46 OpenML-ID-50 OpenML-ID-54 OpenML-ID-151 OpenML-ID-182 OpenML-ID-188 OpenML-ID-300 OpenML-ID-307 OpenML-ID-458 OpenML-ID-469 OpenML-ID-554 OpenML-ID-1049 OpenML-ID-1050 OpenML-ID-1053 OpenML-ID-1063 OpenML-ID-1067 OpenML-ID-1068 OpenML-ID-1461 OpenML-ID-1462 OpenML-ID-1464 OpenML-ID-1468 OpenML-ID-1475 OpenML-ID-1478 OpenML-ID-1480 OpenML-ID-1485 OpenML-ID-1486 OpenML-ID-1487 OpenML-ID-1489 OpenML-ID-1494 OpenML-ID-1497 OpenML-ID-1501 OpenML-ID-1510 OpenML-ID-1590 37 17 5 217 77 10 65 7 48 10 65 16 21 17 9 30 58 61 10 19 9 37 20 618 13 71 5 785 38 38 22 22 22 22 17 5 5 857 52 562 11 501 119 73 6 42 25 257 31 15 3196 20000 625 2000 2000 699 2000 2000 2000 1473 5620 690 1000 10992 768 3772 4601 3190 958 846 45312 6430 736 7797 990 841 797 70000 1458 1563 10885 522 2109 1109 45211 1372 748 1080 6118 10299 583 2600 34465 2534 5404 1055 5456 1593 569 48842 Other Handwriting Other Other Other Healthcare Other Other Other Healthcare Handwriting Finance Finance Handwriting Healthcare Healthcare Other Other Other Other Other Other Other Other Other Other Healthcare Other Other Other Other Other Other Other Finance Finance Healthcare Other Other Other Healthcare Other Other Other Other Other Other Other Other Other 28 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass 2 Yes 26 multiclass No 3 multiclass Yes 10 multiclass Yes 10 multiclass Yes Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 3 multiclass Yes 10 multiclass Yes Yes 2 2 Yes 10 multiclass Yes Yes 2 Yes 2 2 Yes 3 multiclass Yes Yes 2 4 multiclass Yes 2 Yes 6 multiclass Yes 5 multiclass Yes 26 multiclass No 11 multiclass No 4 multiclass Yes 6 multiclass Yes 10 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 2 Yes 9 multiclass Yes 6 multiclass Yes 6 multiclass Yes Yes 2 Yes 2 No 2 Yes 2 Yes 2 2 Yes 4 multiclass Yes 10 multiclass Yes Yes 2 Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass Continued on next page Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation Table 6 Details of OpenML-CC18 benchmark datasets. OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML OpenML-ID-4134 OpenML-ID-4534 OpenML-ID-4538 OpenML-ID-6332 OpenML-ID-23381 OpenML-ID-23517 OpenML-ID-40499 OpenML-ID-40668 OpenML-ID-40670 OpenML-ID-40701 OpenML-ID-40923 OpenML-ID-40927 OpenML-ID-40966 OpenML-ID-40975 OpenML-ID-40978 OpenML-ID-40979 OpenML-ID-40982 OpenML-ID-40983 OpenML-ID-40984 OpenML-ID-40994 OpenML-ID-40996 OpenML-ID-41027 Other Other Other Other Retail Other Other Games Other Other Other Handwriting Other Other Other Other Other Other Other Other Other Games 3751 11055 9873 540 500 96320 5500 67557 3186 5000 92000 60000 1080 1728 3279 2000 1941 4839 2310 540 70000 1777 31 33 40 13 22 41 43 181 21 1025 3073 82 7 1559 241 28 6 20 21 785 7 binclass binclass binclass binclass binclass binclass No 2 2 Yes 5 multiclass Yes Yes 2 Yes 2 2 No 11 multiclass No 3 multiclass No 3 multiclass Yes 2 Yes 46 multiclass No 10 multiclass No 8 multiclass No 4 multiclass Yes 2 Yes 10 multiclass Yes 7 multiclass Yes Yes 2 7 multiclass Yes 2 Yes 10 multiclass No 3 multiclass Yes binclass binclass binclass Table 7 TALENT benchmark datasets (auto-discovered, multiple domains). Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT ASP-POTASSCO-class Amazon_employee_access BLE_RSSI__Indoor_localization BNG(breast-w) BNG(cmc) BNG(tic-tac-toe) Bank_Customer_Churn Basketball_c CDC_Diabetes_Health California-Housing-Class Cardiovascular-Disease Click_prediction_small Credit_c Customer_Personality_Analysis DataScience_Kiva_Crowdfunding Diabetic_Retinopathy_Debrecen E-CommereShippingData Employee FICO-HELOC-cleaned FOREX_audcad-day-High FOREX_audcad-hour-High FOREX_audchf-day-High FOREX_audjpy-day-High FOREX_audjpy-hour-High FOREX_audsgd-hour-High FOREX_audusd-hour-High FOREX_cadjpy-day-High FOREX_cadjpy-hour-High Firm-Teacher_Clave-Direction Fitness_Club_c GAMETES_Epistasis_2-Way 141 7 3 9 9 9 10 11 21 8 11 3 22 24 11 19 10 8 23 10 10 10 10 10 10 10 10 10 16 6 1294 32769 9984 39366 55296 39366 10000 1340 253680 20640 70000 39948 100000 2240 671205 1151 10999 4653 9871 1834 43825 1833 1832 43825 43825 43825 1834 43825 10800 1500 1600 Other Other Other Healthcare Other Other Finance Retail Healthcare Other Healthcare Other Finance Retail Other Healthcare Other Other Other Finance Finance Finance Finance Finance Finance Finance Finance Finance Other Other Games 29 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass 11 multiclass Yes 2 Yes 3 multiclass Yes 2 Yes 3 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 No 2 Yes 2 2 Yes 3 multiclass Yes 2 Yes 4 multiclass No Yes 2 Yes 2 Yes 2 Yes 2 No 2 No 2 No 2 No 2 No 2 No 2 No 2 No 2 2 No 4 multiclass Yes Yes 2 Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass Continued on next page Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Table 7 Details of TALENT benchmark datasets. Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT GAMETES_Heterogeneity Gender_Gap_in_Spanish GesturePhaseSegmentation HR_Analytics_Job_Change IBM_HR_Analytics INNHotelsGroup Indian_pines JapaneseVowels KDDCup09_upselling MIC MagicTelescope Marketing_Campaign Mobile_Price_Classification National_Health_and_Nutrition PhishingWebsites PieChart3 Pima_Indians_Diabetes PizzaCutter3 Pumpkin_Seeds QSAR_biodegradation Rain_in_Australia SDSS17 Satellite Smoking_and_Drinking Telecom_Churn_Dataset UJI_Pen_Characters Water_Quality_and_Potability Wilt abalone accelerometer ada ada_agnostic ada_prior airlines_seed_0_nrows allbp allrep analcatdata_authorship artificial-characters autoUniv-au4-2500 autoUniv-au7-1100 bank banknote_authentication baseball car-evaluation churn cmc company_bankruptcy_prediction compass contraceptive_method_choice credit customer_satisfaction_in_airline dabetes_130-us_hospitals default_of_credit_card_clients delta_ailerons dis dna drug_consumption dry_bean_dataset eeg-eye-state 20 13 32 13 31 17 220 14 49 104 9 27 20 7 30 37 8 37 12 41 18 12 36 23 17 80 8 5 8 4 48 48 14 7 29 29 69 7 100 12 16 4 16 21 20 9 95 17 9 10 21 20 23 5 29 180 12 16 14 1600 4746 9873 19158 1470 36275 9144 9961 5128 1649 19020 2240 2000 2278 11055 1077 768 1043 2500 1054 145460 100000 5100 991346 3333 1364 3276 4821 4177 153004 4147 4562 4562 2000 3772 3772 841 10218 2500 1100 45211 1372 1340 1728 5000 1473 6819 16644 1473 16714 129880 101766 30000 7129 3772 3186 1884 13611 14980 Games Other Other Other Other Other Other Other Other Other Other Finance Telcom Healthcare Other Other Healthcare Other Other Healthcare Other Other Other Other Telcom Other Manufacturing Other Other Other Other Other Other Telcom Other Other Other Other Other Other Finance Finance Other Other Finance Other Finance Other Other Finance Retail Healthcare Finance Other Other Healthcare Healthcare Other Other 30 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass 2 Yes 3 multiclass Yes 5 multiclass Yes Yes 2 Yes 2 2 Yes 8 multiclass Yes 9 multiclass Yes Yes 2 Yes 2 Yes 2 2 Yes 4 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 2 Yes 3 multiclass No 3 multiclass Yes Yes 2 No 2 Yes 2 35 multiclass Yes Yes 2 Yes 2 3 multiclass Yes 4 multiclass No Yes 2 Yes 2 Yes 2 2 Yes 3 multiclass Yes 4 multiclass Yes 4 multiclass Yes 10 multiclass Yes 3 multiclass Yes 5 multiclass Yes Yes 2 2 Yes 3 multiclass Yes 4 multiclass Yes 2 Yes 3 multiclass Yes Yes 2 Yes 2 3 multiclass Yes Yes 2 No 2 Yes 2 Yes 2 Yes 2 2 Yes 3 multiclass Yes 7 multiclass Yes 7 multiclass Yes Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass Continued on next page Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Table 7 Details of TALENT benchmark datasets. Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT electricity estimation_of_obesity_levels eye_movements eye_movements_bin first-order-theorem-proving gas-drift gina_agnostic golf_play_dataset_extended heloc hill-valley house_16H htru ibm-employee-performance in_vehicle_coupon_recos internet_firewall internet_usage jm1 jungle_chess_2pcs_raw kc1 kdd_ipums_la_97-small kr-vs-k kropt led24 led7 letter madeline mammography maternal_health_risk mfeat-factors mfeat-fourier mfeat-karhunen mfeat-morphological mfeat-pixel mfeat-zernike mice_protein_expression microaggregation2 mobile_c36_oversampling mozilla4 naticusdroid+android national-longitudinal-surveybinary okcupid_stem one-hundred-plants-margin one-hundred-plants-shape one-hundred-plants-texture online_shoppers optdigits ozone-level-8hr page-blocks pc1 pc3 pc4 pendigits phoneme pol predict_students_dropout qsar rice_cammeo_and_osmancik ringnorm 8 16 27 20 51 128 970 9 22 100 16 8 30 21 7 70 21 6 21 20 6 6 24 7 15 259 6 6 216 76 64 6 240 47 75 20 6 4 86 16 13 64 64 64 14 64 72 10 21 37 37 16 5 26 34 40 7 20 45312 2111 10936 7608 6118 13910 3468 1095 10000 1212 13488 17898 1470 12684 65532 10108 10885 44819 2109 5188 28056 28056 3200 3200 20000 3140 11183 1014 2000 2000 2000 2000 2000 2000 1080 20000 51760 15545 29332 4908 26677 1600 1600 1599 12330 5620 2534 5473 1109 1563 1458 10992 5404 10082 4424 1055 3810 Manufacturing Other Healthcare Healthcare Other Other Other Other Other Other Other Other Other Retail Other Other Other Other Other Other Other Other Other Other Other Other Other Healthcare Other Other Other Other Other Other Other Other Telcom Other Healthcare Other Other Other Other Other Retail Other Other Other Other Other Other Other Other Other Other Other Other Other 31 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass 2 Yes 7 multiclass Yes 3 multiclass Yes 2 Yes 6 multiclass Yes 6 multiclass Yes Yes 2 Yes 2 Yes 2 No 2 Yes 2 Yes 2 Yes 2 2 Yes 4 multiclass Yes 46 multiclass Yes 2 No 3 multiclass Yes No 2 2 Yes 18 multiclass Yes 18 multiclass Yes 10 multiclass Yes 10 multiclass Yes 26 multiclass Yes Yes 2 2 Yes 3 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 8 multiclass Yes 5 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 binclass binclass binclass binclass binclass binclass binclass 3 multiclass Yes 100 multiclass Yes 100 multiclass Yes 100 multiclass Yes 2 No binclass 10 multiclass Yes Yes 2 5 multiclass Yes No 2 No 2 2 No 10 multiclass Yes Yes 2 2 Yes 3 multiclass Yes Yes 2 Yes 2 Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass Continued on next page Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Table 7 Details of TALENT benchmark datasets. Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT TALENT rl satimage segment seismic+bumps semeion shuttle spambase splice sports_articles_for_objectivity statlog steel_plates_faults sylvine taiwanese_bankruptcy telco-customer-churn texture thyroid thyroid-ann thyroid-dis turiye_student_evaluation twonorm vehicle volkert walking-activity wall-robot-navigation water_quality waveform-5000 waveform_database_generator waveform_database_generator-v2 website_phishing wine wine-quality-red wine-quality-white yeast Other Other Other Other Other Other Other Other Other Other Other Other Finance Telcom Other Healthcare Healthcare Healthcare Other Other Other Other Other Other Manufacturing Other Other Other Other Manufacturing Manufacturing Manufacturing Other 4970 6430 2310 2584 1593 58000 4601 3190 1000 1000 1941 5124 6819 7043 5500 7200 3772 2800 5820 7400 846 58310 149332 5456 7996 5000 4999 5000 1353 2554 1599 4898 1484 12 36 17 18 256 9 57 60 59 20 27 20 95 18 40 21 21 26 32 20 18 180 4 24 20 40 21 21 9 4 4 11 binclass binclass binclass binclass binclass binclass binclass binclass 2 No 6 multiclass Yes 7 multiclass Yes Yes 2 10 multiclass No 7 multiclass Yes 2 Yes 3 multiclass Yes Yes 2 2 Yes 7 multiclass Yes Yes 2 Yes 2 Yes 2 11 multiclass Yes 3 multiclass Yes 3 multiclass Yes 5 multiclass Yes 5 multiclass Yes 2 Yes 4 multiclass Yes 10 multiclass Yes 22 multiclass No 4 multiclass Yes 2 Yes 3 multiclass Yes 3 multiclass Yes 3 multiclass Yes 3 multiclass Yes 2 No 6 multiclass Yes 7 multiclass Yes 10 multiclass Yes binclass binclass binclass Table 8 TabZilla benchmark datasets (36 datasets via OpenML). Benchmark Dataset Name TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla OpenML-ID-999 OpenML-ID-10 OpenML-ID-11 OpenML-ID-14 OpenML-ID-22 OpenML-ID-29 OpenML-ID-27 OpenML-ID-31 OpenML-ID-46 OpenML-ID-54 OpenML-ID-333 OpenML-ID-1067 OpenML-ID-1468 OpenML-ID-1494 OpenML-ID-43973 OpenML-ID-1043 OpenML-ID-43945 OpenML-ID-1486 OpenML-ID-42825 OpenML-ID-4538 Domain Health Health Other Other Other Finance Health Finance Other Other Other Other Other Other Other Other Other Other Other Other Samples Features Classes Task Type Used In Experimentation 226 148 625 2000 2000 690 368 1000 3190 846 556 2109 1080 1055 3172 4562 38474 34465 8378 9873 70 19 5 77 48 16 23 21 61 19 7 22 857 42 6 49 9 119 123 33 binclass binclass binclass binclass 2 Yes 4 multiclass Yes 3 multiclass Yes 10 multiclass Yes 10 multiclass Yes Yes 2 Yes 2 2 Yes 3 multiclass Yes 4 multiclass Yes Yes 2 2 Yes 9 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 No 2 No 5 multiclass Yes binclass binclass binclass binclass binclass binclass binclass Continued on next page 32 Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning Table 8 Details of TabZilla benchmark datasets. Benchmark Dataset Name Domain Samples Features Classes Task Type Used In Experimentation 2 2 2 binclass binclass No binclass Yes binclass No binclass 100 multiclass Yes 10 multiclass Yes 3 multiclass Yes Yes 2 Yes 2 5 multiclass Yes No 2 No 2 Yes 2 No 2 Yes 2 Yes 2 10 multiclass No binclass binclass binclass binclass binclass binclass TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla TabZilla OpenML-ID-23512 OpenML-ID-4134 OpenML-ID-470 OpenML-ID-1493 OpenML-ID-1459 OpenML-ID-41027 OpenML-ID-40981 OpenML-ID-934 OpenML-ID-1565 OpenML-ID-41150 OpenML-ID-41159 OpenML-ID-846 OpenML-ID-1169 OpenML-ID-41147 OpenML-ID-41143 OpenML-IDOther Other Other Other Other Games Other Other Health Other Other Other Other Other Other Other 98050 3751 672 1599 10218 44819 690 1156 294 130064 20000 16599 539383 425240 2984 1025009 29 1777 10 65 8 7 15 6 14 51 4297 19 8 79 145"
        }
    ],
    "affiliations": [
        "Lexsi Labs, India & France"
    ]
}