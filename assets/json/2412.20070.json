{
    "paper_title": "On the Compositional Generalization of Multimodal LLMs for Medical Imaging",
    "authors": [
        "Zhenyang Cai",
        "Junying Chen",
        "Rongsheng Wang",
        "Weihong Wang",
        "Yonglin Deng",
        "Dingjie Song",
        "Yize Chen",
        "Zixu Zhang",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks, providing limited guidance on selecting datasets to enhance specific tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG)-the ability of models to understand novel combinations by recombining learned elements-as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG. Therefore, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and delivers consistent performance across different backbones, highlighting its versatility and broad applicability. Med-MAT is publicly available at https://github.com/FreedomIntelligence/Med-MAT."
        },
        {
            "title": "Start",
            "content": "Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, Benyou Wang The Chinese University of Hong Kong, Shenzhen wangbenyou@cuhk.edu.cn 4 2 0 2 8 2 ] . [ 1 0 7 0 0 2 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research suggests that multitask training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks, providing limited guidance on selecting datasets to enhance specific tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG)the ability of models to understand novel combinations by recombining learned elementsas guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG. Therefore, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and delivers consistent performance across different backbones, highlighting its versatility and broad applicability. Med-MAT is publicly available at github.com/FreedomIntelligence/Med-MAT."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) (Liu et al., 2023; Li et al., 2024; Chen et al., 2024b) are showing great promise for the medical community, facilitating efficient consultations for doctors and providing patients with anytime access to their medical conditions. However, limited data on rare or privacy-restricted diseases often restricts Corresponding author. Equal Contribution. Figure 1: Examples of Compositional Generalization: The model is required to understand unseen images by recombining the fundamental elements it has learned. MLLMs performance, making it crucial to explore what kinds of images can be used by MLLMs to generalize these diseases or learn them efficiently. Current research on medical image generalization (Mo and Liang, 2024; Ren et al., 2024) has demonstrated that models trained on multiple tasks outperform those trained on single task as they can leverage potential knowledge from other tasks. However, these studies have not conducted detailed analysis of which data within multitask datasets can mutually complement each other. Consequently, we aim to investigate the phenomenon of mutual improvement in MLLMs understanding of medical images from the perspective of composition generalization. Compositional generalization (CG) (Xu et al., 2022; Li et al., 2019) refers to models ability to learn fundamental elements and recombine them in novel ways to understand unseen combinations (Figure 1). In medical imaging, each image can be categorized by Modality, Anatomical area, and medical Task, presenting numerous natural opportunities for CG. We defined these three elements as the MAT-Triplet and collected 106 medical datasets, subsequently merging those that share the Figure 2: The process of integrating vast amount of labeled medical image data to create Med-MAT. same MAT-Triplet to create the Med-MAT dataset. Ultimately, Med-MAT comprises 53 subsets, encompassing 11 modalities, 14 anatomical regions, and 13 medical tasks, providing foundation for investigating CG and other generalization methods. To verify the existence of CG, we designated certain datasets as Target data and selected all the Related data from Med-MAT that shared the same MAT-Triplet with the Target data for generalization. In addition, we conducted larger-scale experiments and observed the changes in model generalization performance after deliberately disrupting CG. Ultimately, the experiments demonstrated that MLLMs can leverage CG to understand unseen medical images and CG is one of the key factors driving the generalization observed in multi-task training. Subsequently, the practical applications of CG and its effects on different backbones were further explored. We found that increasing the volume of CG combinations continuously enhanced the models understanding of the Target data. While CGs direct impact on unseen data was not always apparent, CG could assist the model in efficient fitting with limited data by being incorporated into the training data of the Target. Furthermore, we discovered that CG exists in different MLLM backbones, underscoring CGs broad applicability. Here are the key contributions of our work: 1) VQA dataset, Med-MAT, has been constructed, providing platform to explore the generalization of MLLMs on medical images. 2) Through this dataset, we observed that MLLMs can utilize compositional generalization to understand unseen images, demonstrating that this is one of the main forms of generalization for medical MLLMs. 3) Finally, the contribution of CG to understanding medical domains with limited data has been further explored, with its presence across different backbones demonstrating its broad applicability."
        },
        {
            "title": "2 Med-MAT",
            "content": "Most existing datasets for MLLMs (Zhang et al., 2023c; Li et al., 2024; Chen et al., 2024b), primarily VQA datasets, provide broad coverage but lack attribute annotations for individual samples, which are not suitable for CG exploration. To address this gap, we curated large collection of imagetext pairs to develop Med-MAT, ensuring that each sample is explicitly defined by MAT-Triplet. Section 2.1 provides data processing methods; and Section 2.2 presents pilot experiment."
        },
        {
            "title": "2.1 Data Processing",
            "content": "Data Construction: Med-MAT contains total of 106 image-label pair medical datasets, sourced from various medical public challenges or highquality annotated datasets. All datasets are categorized according to their MAT-Triplet, with data having identical elements grouped into single subset (Figure 2). Labels are manually clustered to ensure that annotations with the same meaning are not repeatedly used. In total, Med-MAT covers 11 medical modalities, 14 anatomical areas, and 13 medical tasks, hoping that it can spread across various medical tasks like mat. (Data lists are shown in Appendix B) Data Distribution: All subsets are divided into training and test sets following their original distributions or using 9:1 ratio. To ensure fair comparison, each training set is limited to 3,000 samples, with label balance maintained as much as possible. Any subset that cannot meet this requirement is treated as an OOD (out-of-distribution) dataset. For the test sets, we strictly balance the number of samples per label to ensure that the accuracy metric reliably reflects model performance. QA Pairs Construction: To enable MLLMs to directly train and test on Med-MAT, all image-label Table 1: Accuracy of different models on In-Distribution Dataset. Within each segment, bold highlights the best scores, and underlines indicate the second-best. Model 02 03 07 08 09 11 13 14 15 16 18 19 21 22 23 25 26 28 30 31 32 33 35 36 37 Baseline 22 47 40 25 26 27 28 24 22 24 25 23 49 26 25 24 49 30 49 21 49 20 25 23 19 Single-task Training 24 49 50 68 65 76 83 53 61 32 29 26 57 53 28 24 57 64 89 60 97 54 29 51 49 Multi-task Training 96 89 80 80 79 97 92 88 76 57 88 74 87 86 93 52 98 72 94 61 100 72 75 60 Table 2: Accuracy of different models on Out-Of-Distribution Dataset. Bold highlights the best scores. Model 01 04 05 06 10 12 17 20 24 27 29 34 Baseline 32 25 33 33 48 27 33 13 34 37 31 20 Multi-task Training 39 26 70 31 58 38 61 40 35 41 55 50 paired data were converted into visual questionanswering (VQA) format (Figure 3). Specifically, each subset was manually assigned 6 instructions to guide the MLLM in answering the subset task. For convenience, all samples were converted into single-choice questions with up to four options, and the remaining distractor options were randomly drawn from other labels within the subset. To mitigate potential evaluation biases arising from varying option counts, the ImageWikiQA dataset (Zhang et al., 2024b), non-medical dataset consisting of single-answer, four-option questions, was incorporated during the training. Figure 3: The QA formatting process of Med-MAT."
        },
        {
            "title": "2.2 A Pilot Study of Data Composition",
            "content": "Experiments Setup: Experiments in this paper mainly focused on classification datasets to explore the models combinatorial generalization capabilities across various image types. The base model LLaVA-v1.5-7B-Vicuna (Liu et al., 2023) features transparent pretraining process and uses very little medical data during training, thereby minimizing the risk of knowledge leakage. By leveraging the flexibility of MLLM, we achieved task switching and generalization simply by modifying the corresponding prompts, significantly simplifying generalization studies. Each experiment was conducted over 5 epochs using 8 A800 (80GB) GPUs, with batch size of 32 and learning rate of 5e-6. Generalization performance was assessed by comparing the models accuracy on the Target data. Pilot Study: To explore the benefits of data composition for downstream tasks, all in-distribution (ID) datasets were combined for multi-task training, with single-task training on individual ID datasets serving as the control. The results (Table 1 and 2) showed that multi-task training outperformed single-task training on specific tasks, and also surpassed the baseline in predicting unseen OOD datasets. This suggests that certain data combinations can enhance the models classification performance, highlighting the potential for CG. Exploring which combinations provide valuable insights for downstream medical tasks in promising avenue for further research. Take-away 1: Certain data combinations can assist MLLMs in medical classification tasks."
        },
        {
            "title": "3 Proof of Concept on CG",
            "content": "To explore whether MLLMs can leverage CG to understand unseen images, controlled variable studies were first conducted using all possible CG combinations to determine if CG exists among the MATTriplet (Section 3.1). Subsequently, the number of combinations was increased to explore more generalizable insights, aiming to assess the performance gains of multi-task training (Section 3.2)."
        },
        {
            "title": "3.1.1 Experiment Setup",
            "content": "In this section, the existence of CG was explored from finer perspective, focusing on CG with only Table 3: Generalization results on classification datasets: \"Related Combination\" is the training set, \"Target Subset\" is the goal. Baseline, Baseline+, and Trained represent the models accuracy without training, trained on randomly sampled unrelated data, and trained on related data, respectively. Green section indicates successful generalization, while red section denotes failure. The 4 segmented areas represent different Direction Types: fixed modality, fixed area, fixed task, and modality-area paired combinations. Related Combination Target Subset Baseline Baseline+ Trained Lung, COVID Lung, Cancer Brain, Cancer Bones, Level Bones, Level Bones, Level Bones, Level Bones, Level Bones, State Bones, State Bones, State Lung, COVID Lung, COVID Lung, COVID CT, Cancer CT, COVID CT, State CT, State CT, Brain(State) CT, Brain CT, Brain(Cancer) CT, Brain X-ray, Brain X-ray, Lung X-ray, Lung X-ray, Lung CT, Lung (State) CT, Lung (State) CT, Lung (Cancer) CT, Lung (Cancer) Brain, Cancer Brain, State Lung, State Lung, State Brain, State Breast, Diseases Lung, Diseases Chest, Diseases Breast, Diseases Lung, Diseases Chest, Diseases Breast, Diseases Bones, Diseases Chest, Diseases X-ray, COVID X-ray, Diseases X-ray, Diseases X-ray, Cancer Lung, Cancer Lung, State Brain, State Bones, State Bones, State Bones, Diseases Bones, Diseases Bones, Diseases Bones, Diseases Bones, Diseases Bones, Diseases Lung, Diseases Lung, Diseases Lung, Diseases CT, COVID X-ray, COVID X-ray, State CT, Cancer X-ray, Bones X-ray, Lung X-ray, Bones X-ray, Lung CT, Lung(State) CT, Brain CT, Brain(State) CT, Brain(Cancer) X-ray, Bones X-ray, Brain X-ray, Bones X-ray, Brain X-ray, Brain X-ray, Brain X-ray, Brain X-ray, Brain CT, Brain(State) CT, Lung(Cancer) CT, Lung CT, Lung X-ray, Lung X-ray, Lung X-ray, Lung X-ray, Lung FP, Fundus, Diseases Der, Skin, Diseases Der, Skin, Cancer OCT, Retine, Diseases Der, Skin, Diseases Der, Skin, Cancer Der, Skin, Cancer DP, Mouth, Cancer Der, Skin, Diseases Der, Skin, Cancer Mic, Cell, Cancer Der, Skin, Diseases DP, Mouth, Cancer Der, Skin, Cancer DP, Mouth, State DP, Mouth, Cancer Mic, Cell, Cancer DP, Mouth, State FP, Fundus, Level Mic, Cell, Level FP, Fundus, Diseases Mic, Cell, Level Mic, Cell, Cell Identification FP, Fundus, Level Mic, Cell, Cell identification Der, Skin, Cancer Mic, Cell, Cancer Mic, Cell, Cell identification DP, Mouth, Cancer Mic, Cell, Cancer Mic, Cell, Cancer Mic, Cell, Level Der, Skin, Cancer DP, Mouth, Cancer Mic, Cell, Cancer Mic, Cell, Level FP, Fundus, Level Mic, Cell, Cancer Mic, Cell, Level 25 47 33 49 49 37 37 37 37 37 37 49 49 49 47 30 30 49 49 25 49 33 25 47 47 30 30 30 30 25 25 40 40 48 48 33 23 49 49 49 49 23 25 46 50 53 53 33 33 31 37 37 37 48 48 48 46 21 21 28 49 50 51 52 50 25 50 50 32 32 32 32 29 29 33 33 50 50 36 33 50 51 51 51 27 50 57 51 72 39 43 43 43 43 41 51 52 51 72 49 46 28 91 81 74 52 60 36 81 71 28 35 41 42 33 33 63 63 52 55 42 32 50 62 52 58 27 two elements varying while the third remained constant. For example, we fixed the Modality and examined the CG between Areas and Tasks. Additionally, we identified specific Modality-Area pairs, such as dermoscopy paired consistently with skin, which were treated as special category. These 4 different fixed formats were classified into distinct Direction Types. Trained refers to the model trained solely on Related data. To ensure that our conclusions are not influenced by the amount of training data, we randomly sampled an equal number of data from the Unrelated subsets, and this configuration is referred to as Baseline+."
        },
        {
            "title": "3.1.2 Analysis of CG in MLLMs",
            "content": "Following the setup in Section 2.2, we evaluated the models performance on Target data. Baseline refers to the model without any training, while Results are shown in Table 3 and it can be observed that almost all CG combinations are able to generalize to downstream tasks, highlighting that MLLMs Figure 4: Accuracy results on the Target dataset for various models. All Related/Unrelated models are trained on all the related or unrelated datasets of the Target Data. w/o Modality/Area/Task are trained on All Related datasets but omit those sharing the same element as the Target Data, to intentionally disrupt CG. All Data uses all available training sets. (Note: The Target Data is excluded from training to observe generalization.) can leverage CG to generalize Target data across all Direction Types. Besides that, since this experiment focused solely on two-element tuples, we further investigated three-element tuples in Section 4.3, where we also observed similarly strong generalizations when obtaining MAT-Triplet elements from three different datasets. Take-away 2: MLLMs can leverage CG to understand unseen medical images. In the Baseline+ setting, we removed all datasets sharing any MAT-Triplet element with the Target data. Consequently, Baseline+ models perform at near-random levels on the test set, indicating they failed to acquire target-relevant knowledge. This suggests that only datasets related through the MAT-Triplet can help the model learn and generalize to new target tasks. Take-away 3: Generalization only arises from MAT-Triplet related medical datasets."
        },
        {
            "title": "3.2 Scaling the Combination Number of CG",
            "content": "To ensure the universality of CG, we expanded the number of combination datasets to evaluate its effects on larger scale. This section explores the following questions: (Q1) While Meta CG experiments indicate that Unrelated combinations provide no benefit to Target data, can generalization arise when training incorporates more Unrelated combinations, simulating multi-task scenario? (Q2) Previous studies suggest that multi-task training generally promotes better generalization than single-task training. To what extent does this generalization originate from CG?"
        },
        {
            "title": "3.2.1 Experiment Setup",
            "content": "Selection Strategy: To ensure balanced evaluation of Related and Unrelated combinations, Subset 03 and Subset 28 were chosen as Target datasets because they exhibit the most balanced ratios of Related to Unrelated subsets (13:11 for Subset 03 and 11:13 for Subset 28), making them ideal for providing diverse range of compositions in the scale-up experiments. The baseline was trained on all subsets excluding the Target data to evaluate the claim that mixing multi-task data enhances generalization ( All Data ). To construct multiple comparative experiments, models were further trained on either Related or Unrelated subsets ( All Related / All Unrelated ) to address Q1. For Q2, individual MAT-Triplet elements were systematically removed from the Related subsets ( Related w/o Modality / Area / Task ), disrupting CG and assessing the ability to maintain generalization. To ensure consistency, the total data volume in all experiments was limited to 15,000 samples, aligning with the number of ID subsets available after excluding related tasks from Subset 03."
        },
        {
            "title": "3.2.2 Analysis of Scaling Experiment",
            "content": "Figure 4 illustrates the results. It can be observed that even when we expanded the Unrelated combination volumes and increased task diversity, the performance of All Unrelated remains close to Figure 5: (RQ 1) The curves show how performance on the Target Data improves as the volume of composition datasets increases. The red and purple lines represent training with Related and Unrelated Data , respectively. Figure 6: (RQ 2) The curves show how accuracy on Target Data improves as its volume increases with fixed size of composition data. The red and purple lines represent training with Related and Unrelated Data , respectively. the Baseline , indicating that these datasets can not support MLLMs to understand the Target data. Take-away 4: Datasets without MAT-Triplet overlap offer limited benefit for generalization even in the multi-task training scenario (Q1). Besides, w/o Modality / Area / Task showed to significant All Related , training data volume constant. Notably, All Related performs at level comparable to All Data , where all datasets are included in training. This suggests that CG provides significant help for the generalization brought by multi-task training. accuracy despite drops holding compared the Take-away 5: CG is one of the primary forms of generalization for MLLMs trained on multi-task dataset in medical imaging (Q2). 4 In-depth Analysis of CG To further explore the application and applicability of CG, several research questions were proposed in this section. In order to present rich results with minimal consumption of computational resources, specific data Selection Strategies were applied for each question. Here are the research questions: (RQ 1): How does the quantity of Related data affect generalization, and should it be maximized to ensure CG quality? (RQ 2): Can CG combinations help the model efficiently fit Target data when only small amount of Target data is available? (RQ 3): Previous studies demonstrated that CG exists between two MAT-Triplet elements, does CG still exist if the three MAT-Triplet elements come from three different datasets? (RQ 4): Does CG exist between images from Detection tasks and Classification tasks? (RQ 5): Can different MLLM backbones leverage CG to interpret unseen medical images?"
        },
        {
            "title": "4.1 The Relationship Between CG\nPerformance and Data Volume",
            "content": "Selection Strategy: This section focuses on examining how the data volume of Related combinations influences generalization. To highlight the generalization trends, the combinations with strong generalization results were selected from the main experiments. For fairness, we chose the combinations across four types where Trained results exceed both Baseline and Baseline+ by at least 10. If multiple combinations meet the criteria, random seed of 42 was used to determine the selection. In this experiment, the amount of combination data was gradually increased (from 0 to 750, 1500, 2250, and 3000), and the trend of the models accuracy on Target data after training was observed. The experimental results are shown in Figure 5, where the red line represents the accuracy curve gained from Related combinations, and the purple line represents the gain from Unrelated combinations. Models trained on Related combinations demonstrate strong performance across all four Target data sets. As the data volume of Related combinations increases, the models understanding of Target data improves consistently. Take-away 6: Generalization effect improves with increasing CG combinations volume. (RQ 1) Table 4: Generalization results from 3 datasets providing different elements of MAT-Triplet (RQ 3). \"Related Combination\" is the training set, \"Target Subset\" is the goal. Baseline, and Trained represent the models accuracy without training and trained on Related data, respectively. Green section indicates successful generalization, while red section denotes failure. Related Combination Target Subset Baseline Trained Brain - Subset22 Cancer - Subset07 CT, Brain, Cancer Brain - Subset22 Cancer - Subset21 CT, Brain, Cancer Brain - Subset22 State - Subset09 Brain - Subset22 State - Subset26 CT - Subset02 CT - Subset03 CT - Subset02 CT - Subset03 X-ray - Subset25 Lung - Subset03 Diseases - Subset02 X-ray, Lung, Diseases X-ray - Subset26 Lung - Subset03 Diseases - Subset02 X-ray, Lung, Diseases X-ray - Subset26 Lung - Subset03 Diseases - Subset08 X-ray, Lung, Diseases X-ray - Subset26 Breast - Subset24 Diseases - Subset02 X-ray, Breast, Diseases X-ray - Subset28 Breast - Subset24 Diseases - Subset08 X-ray, Breast, Diseases CT, Brain, State CT, Brain, State 28 28 33 33 30 30 30 31 31 26 25 64 70 45 38 44 32 Figure 7: The accuracy of different backbones: The blue line represents the untrained model, and the green line represents the CG-trained model. All data are scaled based on the accuracy of CG combinations, which is displayed at each corner (details in Appendix A.1 and A.2)."
        },
        {
            "title": "4.2 Achieving Data-Efficient Training",
            "content": "a small amount of Target data. through CG Selection Strategy: This experiment evaluates how CG aids training and dataset fitting with limited Target data. To highlight performance trends, four combinations with poor generalization (marked in red in Table 3) were selected across the four Direction Types, ensuring both models in the control group started with similar baseline accuracies. When multiple poor-performing combinations existed within Direction Type, the more challenging datasets were chosen based on Table 1 to better observe potential improvements. In this experiment, the amount of Target data was progressively increased (from 0 to 500, 1000, 1500, and 2000), and fixed number of combination data were incorporated during training. Figure 6 displays the results, with red lines showing accuracy gains from Related combination and purple lines from Unrelated ones. It can be observed that training with Related combination data reaches peak performance more quickly, demonstrating another form of CG. Therefore, although CG performs poorly on these data due to task characteristics, it still helps the model quickly adapt with only Take-away 7: Although CG does not always provide direct generalization gains, it can help Target data achieve data-efficient training. (RQ 2)"
        },
        {
            "title": "Different Sources",
            "content": "In previous controlled experiments (Section 3.1), one element of the MAT-Triplet was kept constant while CG was explored in the remaining two elements. To ensure that all the 3 MAT-Triplet elements of the target data originated from three distinct datasets, additional experiments were conducted to further validate the effectiveness of CG. For these experiments, all possible combinations meeting the criteria in Med-MAT were selected (Selection Strategy). The results presented in Table 4 demonstrate that most combinations can effectively generalize to the Target data. Take-away 8: CG remains effective when all elements come from different datasets (RQ 3)."
        },
        {
            "title": "Detection Data for CG",
            "content": "Previous studies (Ren et al., 2024; Wang et al., 2025) have demonstrated that jointly training classification and detection tasks can mutually enhance their performance. Building on this, we explored CG within this context. To ensure comprehensive conclusions, we conducted experiments using NextChat and MiniGPT-v2, which represent the two mainstream approaches for MLLMs to perform segmentation tasks. The former treats bounding boxes as embeddings and decodes them into coordinates using an additional visual decoder, while the latter processes coordinate points as special text tokens and generates bounding box coordinates directly as output text. The final results (Figure 7) show that all CG combinations demonstrated the models successful utilization of detection data for CG to the Target data. Take-away 9: MLLMs can combine knowledge from detection tasks with classification tasks to improve classification accuracy (RQ 4)."
        },
        {
            "title": "Backbones",
            "content": "LLaVA was selected as the baseline because its training data and processes are publicly available, ensuring minimal exposure to medical images and preventing bias in the integration of medical image knowledge into the MLLM. To verify that CG is not limited to single framework, we also selected subsets for experiments on Qwen2-VL-7B (Wang et al., 2024a) and Llama3.2-11B-Vision (Meta AI, 2024). The results in Figure 7 demonstrate CG effects similar to those observed in the LLaVA. Take-away 10: CG persists across different MLLM backbones (RQ 5)."
        },
        {
            "title": "5 Related Work",
            "content": "Generalization on Medical Imaging: Generalization in medical imaging (Matta et al., 2024) has been extensively studied. Early methods utilized data manipulation techniques, such as data augmentation (Li et al., 2022; Zhang et al., 2022), to enhance model generalization on unseen medical data by adapting to varying distributions. Later approaches focused on representation learning (LeKhac et al., 2020), preserving essential image information to enable models to handle more complex scenarios. Additionally, some studies (Ren et al., 2024) explore multiple aspects of medical image processing, examining how classification and segmentation tasks can mutually benefit each other. Detection with MLLMs: MiniGPT-4 (Zhu et al., 2023) and LLaVA (Liu et al., 2023) enhance LLMs by fine-tuning models on synthetic multimodal instruction data. However, these models are limited to image and text inputs and cannot generate detection bounding boxes or segmentation masks. Recent works have addressed these limitations using various strategies, such as encoding regions as features to allow models to accept regions as input (Zhang et al., 2023b), representing object bounding box coordinates with text tokens (Wang et al., 2024c; Peng et al., 2023; Chen et al., 2023b), and employing unique identifiers for task instructions to improve learning efficiency. Additionally, some approaches introduce special tokens to represent images and use their hidden states to decode position information (Zhang et al., 2023a, 2024a)."
        },
        {
            "title": "6 Conclusion",
            "content": "Medical MLLMs: Recently, adapting MLLMs to medical tasks has gained prominence due to their success in capturing complex visual features. Current MLLMs typically pair visual encoder with text-only LLM, aligning image data with language understanding. Such as Med-Flamingo (Moor et al., 2023) and Med-PaLM (Tu et al., 2024), fine-tuned general multimodal models and achieved notable results. Med-Flamingo enhanced OpenFlamingo9B (Chen et al., 2024a) with medical data, while Med-PaLM adapted PaLM-E (Driess et al., 2023) using 1 million data points. Similarly, LLaVAMed (Li et al., 2024), Med-Gemini (Saab et al., 2024), and HuatuoGPT-Vision (Chen et al., 2024b) utilized specialized datasets and instruction tuning to refine medical VQA tasks. To explore whether MLLMs can leverage CG to generalize unseen medical data, we constructed the Med-MAT dataset as the research platform for generalization experiments. The results confirmed the existence of CG and identified it as one of the primary factors behind the generalization capability of MLLMs in multi-task learning. Subsequent experiments demonstrated that increasing the volume of CG combinations consistently improved their effectiveness. Furthermore, CG enables medical tasks to efficiently fit with limited data, reducing dependence on the volume of training data. Importantly, CG is present across various MLLM backbones and can even facilitate generalization using data from tasks such as detection, underscoring its broad applicability."
        },
        {
            "title": "Limitations",
            "content": "The experiment confirms that MLLMs can use CG to understand unseen medical images and achieve data-efficient training. However, as shown in the results in 3.2, even after disrupting CG, the models generalization performance declines but still maintains some level of effectiveness. Thus, CG represents only one form of generalization for MLLMs in medical imaging. To support the exploration of other generalization mechanisms, we will make datasets publicly available according to the license. This study is focused exclusively on medical scenarios, but we believe that similar generalization effects may also apply in other multimodal tasks. Additionally, more granular medical generalization strategies, such as leveraging existing pneumonia data to generalize the detection of newly emerging types of pneumonia, are also promising areas for further investigation."
        },
        {
            "title": "Potential Risks",
            "content": "Our research focuses on the compositional generalization of MLLMs on medical images, using data sourced from medical challenges and open-source datasets. However, further experiments are needed to mitigate potential risks when deploying this concept in real-world medical settings."
        },
        {
            "title": "References",
            "content": "Andrea Acevedo, Anna Merino, Santiago Alférez, Ángel Molina, Laura Boldú, and José Rodellar. 2020. dataset of microscopic peripheral blood cell images for development of automatic recognition systems. Data in brief, 30:105474. Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. 2020. Dataset of breast ultrasound images. Data in brief, 28:104863. Shams Nafisa Ali, Md. Tazuddin Ahmed, Joydip Paul, Tasnim Jahan, S. M. Sakeef Sani, Nawshaba Noor, and Taufiq Hasan. 2022. Monkeypox skin lesion detection using deep learning models: preliminary feasibility study. arXiv preprint arXiv:2207.03342. Sharib Ali, Barbara Braden, Dominique Lamarque, Stefano Realdon, Adam Bailey, Renato Cannizzaro, Noha Ghatwary, Jens Rittscher, Christian Daul, and James East. 2020. Endoscopy disease detection and segmentation (edd2020). MD Anouk Stein, Carol Wu, Chris Carr, George Shih, Jamie Dulkowski, kalpathy, Leon Chen, Luciano Prevedello, MD Marc Kohli, Mark McDonald, Peter, Phil Culliton, Safwan Halabi MD, and Tian Xia. 2018. Rsna pneumonia detection challenge. https://kaggle.com/competitions/ rsna-pneumonia-detection-challenge. Kaggle. Will Arevalo. 2020. Chexpert v1.0 small. https: //www.kaggle.com/datasets/willarevalo/ chexpert-v10-small. Kaggle. Asraf and Islam. 2021. Covid19, pneumonia and normal chest x-ray pa dataset. mendeley data v1 (2021). Francisco José Fumero Batista, Tinguaro Diaz-Aleman, Jose Sigut, Silvia Alayon, Rafael Arnay, and Denisse Angel-Pereira. 2020. Rim-one dl: unified retinal image database for assessing glaucoma using deep learning. Image Analysis & Stereology, 39(3):161 167. Dev Batra. 2024. Fracture detection https://www. x-ray images. using kaggle.com/datasets/devbatrax/ fracture-detection-using-x-ray-images. Kaggle. Veronica Elisa Castillo Benítez, Ingrid Castro Matto, Julio César Mello Román, José Luis Vázquez Noguera, Miguel García-Torres, Jordan Ayala, Diego Pinto-Roa, Pedro Gardel-Sotomayor, Jacques Facon, and Sebastian Alberto Grillo. 2021. Dataset from fundus images for the study of diabetic retinopathy. Data in brief, 36:107068. BenO, jljones, Kumar H, Meg Risdal, MRao, Vadim Sherman, Vipul, Wendy Kan, and Yau Ben-Or. 2017. Intel & mobileodt cervical cancer screening. https://kaggle.com/competitions/ intel-mobileodt-cervical-cancer-screening. Kaggle. Jorge Bernal, Javier Sánchez, Gloria FernándezEsparrach, Debora Gil, Cristina Rodríguez, and Fernando Vilariño. 2015. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics, 43:99111. Bukun. 2019. Breast cancer histopathological database (breakhis). https://www.kaggle.com/datasets/ ambarish/breakhis. Kaggle. Olivia Cardozo, Verena Ojeda, Rodrigo Parra, Julio César Mello-Román, José Luis Vázquez Noguera, Miguel García-Torres, Federico Divina, Sebastian A. Grillo, Cynthia Villalba, Jacques Facon, Veronica Elisa Castillo Benítez, Ingrid Castro Matto, and Diego Aquino-Brítez. 2023. Dataset of fundus images for the diagnosis of ocular toxoplasmosis. Data in Brief, 48:109056. Ling-Ping Cen, Jie Ji, Jian-Wei Lin, Si-Tong Ju, HongJie Lin, Tai-Ping Li, Yun Wang, Jian-Feng Yang, Yu-Fen Liu, Shaoying Tan, et al. 2021. Automatic detection of 39 fundus diseases and conditions in retinal photographs using deep neural networks. Nature communications, 12(1):4828. Delong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan Wang. 2024a. Visual instruction tuning with polite flamingo. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17745 17753. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478. Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. 2024b. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023b. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195. Pingjun Chen. 2018. Knee osteoarthritis severity grading dataset. Mendeley Data, 1(10.17632). Muhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al Emadi, Mamun Bin Ibne Reaz, and Mohammad Tariqul Islam. 2020. Can ai help in screening viral and covid-19 pneumonia? IEEE Access, 8:132665132676. Noel Codella, Veronica Rotemberg, Philipp Tschandl, Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. 2019. Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368. Noel CF Codella, David Gutman, Emre Celebi, Brian Helba, Michael Marchetti, Stephen Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. 2018. Skin lesion analysis toward melanoma detection: challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboraIn 2018 IEEE 15th international symtion (isic). posium on biomedical imaging (ISBI 2018), pages 168172. IEEE. Marc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan Halpern, Susana Puig, et al. 2019. Bcn20000: Dermoscopic lesions in the wild. arXiv preprint arXiv:1908.02288. Will Cukierski. 2018. Histopathologic cancer dehttps://kaggle.com/competitions/ tection. histopathologic-cancer-detection. Kaggle. Training Data. 2023. Computed tomoghttps://www. of the brain. raphy kaggle.com/datasets/trainingdatapro/ computed-tomography-ct-of-the-brain. Kaggle. Coen de Vente, Koenraad A. Vermeer, Nicolas Jaccard, He Wang, Hongyi Sun, Firas Khader, Daniel Truhn, Temirgali Aimyshev, Yerkebulan Zhanibekuly, Tien-Dung Le, Adrian Galdran, Miguel Ángel González Ballester, Gustavo Carneiro, Devika G, Hrishikesh S, Densen Puthussery, Hong Liu, Zekang Yang, Satoshi Kondo, Satoshi Kasai, Edward Wang, Ashritha Durvasula, Jónathan Heras, Miguel Ángel Zapata, Teresa Araújo, Guilherme Aresta, Hrvoje Bogunovic, Mustafa Arikan, Yeong Chan Lee, Hyun Bin Cho, Yoon Ho Choi, Abdul Qayyum, Imran Razzak, Bram van Ginneken, Hans G. Lemij, and Clara I. Sánchez. 2023. Airogs: Artificial intelligence for robust glaucoma screening challenge. arXiv preprint arXiv:2302.01738. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378. Feltrin. Fernando mri kaggle.com/datasets/fernando2rad/ brain-tumor-mri-images-17-classes. Kaggle. Brain tumor https://www. 2022. classes. images 17 Mohammad Fraiwan, Ziad Audat, Luay Fraiwan, and Tarek Manasreh. 2022. Using deep transfer learning to detect scoliosis and spondylolisthesis from x-ray images. Plos one, 17(5):e0267851. Huazhu Fu, Fei Li, José Ignacio Orlando, Hrvoje Bogunovic, Xu Sun, Jingan Liao, Yanwu Xu, Shaochong Zhang, and Xiulan Zhang. 2019. Palm: Pathologic myopia challenge. Ioannis Giotis, Nynke Molders, Sander Land, Michael Biehl, Marcel Jonkman, and Nicolai Petkov. 2015. Med-node: computer-assisted melanoma diagnosis system using non-dermoscopic images. Expert systems with applications, 42(19):65786585. Haifan Gong, Guanqi Chen, Ranran Wang, Xiang Xie, Mingzhi Mao, Yizhou Yu, Fei Chen, and Guanbin Li. 2021. Multi-task learning for thyroid nodule segmentation with thyroid region prior. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 257261. IEEE. Haifan Gong, Jiaxin Chen, Guanqi Chen, Haofeng Li, Fei Chen, and Guanbin Li. 2022. Thyroid region prior guided attention for ultrasound segmentation of thyroid nodules. Computers in Biology and Medicine, 106389:112. Shivanand Gornale and Pooja Patravali. 2020. Digital knee x-ray images. Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri. 2021. Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18201828. David Gutman, Noel CF Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, and Allan Halpern. 2016. Skin lesion analysis toward melanoma detection: challenge at the international symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1605.01397. Saba Hesaraki. 2022. (busi). dataset images kaggle.com/datasets/sabahesaraki/ breast-ultrasound-images-dataset. gle. Breast ultrasound https://www. KagMd Nazmul Islam, Mehedi Hasan, Md Kabir Hossain, Md Golam Rabiul Alam, Md Zia Uddin, and Ahmet Soylu. 2022a. Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from ct-radiography. Scientific Reports, 12(1):114. Towhidul Islam, Mohammad Arafat Hussain, Forhad Uddin Hasan Chowdhury, and Riazul Islam. 2022b. web-scrapped skin image database of monkeypox, chickenpox, smallpox, cowpox, and measles. bioRxiv 2022.08.01.502199. Stefan Jaeger, Sema Candemir, Sameer Antani, YìXiáng Wáng, Pu-Xuan Lu, and George Thoma. 2014. Two public chest x-ray datasets for computeraided screening of pulmonary diseases. Quantitative imaging in medicine and surgery, 4(6):475. Debesh Jha, Pia Smedsrud, Michael Riegler, Pål Halvorsen, Thomas de Lange, Dag Johansen, and Håvard Johansen. 2020. Kvasir-seg: segmented polyp dataset. In MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 58, 2020, Proceedings, Part II 26, pages 451462. Springer. Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. 2022. Fives: fundus image dataset for artificial intelligence based vessel segmentation. Scientific data, 9(1):475. JR2NGB. 2019. Cataract dataset. https: //www.kaggle.com/datasets/jr2ngb/ cataractdataset. Kaggle. Nur Karaca. 2022. Nlm montgomery cxr https://www.kaggle.com/datasets/ set. nurkaraca/nlm-montgomerycxrset. Kaggle. Maggie, Karthik, 2019. tion. aptos2019-blindness-detection. Kaggle. Dane. detechttps://kaggle.com/competitions/ Sohier blindness and 2019 Aptos Andrey Katanskiy. 2019. Skin cancer isic. https://www.kaggle.com/datasets/ nodoubttome/skin-cancer9-classesisic. Kaggle. Jakob Nikolas Kather, Niels Halama, and Alexander Marx. 2018. 100,000 histological images of human colorectal cancer and healthy tissue. Daniel Kermany. 2018. Labeled optical coherence tomography (oct) and chest x-ray images for classification. Mendeley data. Felipe Campos Kitamura. 2018. Head ct - hemorrhage. Jorge Lazo, Benoit Rosa, Michele Catellani, Matteo Fontana, Francesco Mistretta, Gennaro Musi, Ottavio de Cobelli, Michel de Mathelin, and Elena De Momi. 2023. Semi-supervised bladder tissue classification in multi-domain endoscopic images. IEEE Transactions on Biomedical Engineering. Trang Le, Casper Winsnes, Ulrika Axelsson, Hao Xu, Jayasankar Mohanakrishnan Kaimal, Diana Mahdessian, Shubin Dai, Ilya Makarov, Vladislav Ostankovich, Yang Xu, et al. 2022. Analysis of the human protein atlas weakly supervised singlecell classification competition. Nature methods, 19(10):12211229. Phuc Le-Khac, Graham Healy, and Alan Smeaton. 2020. Contrastive representation learning: framework and review. Ieee Access, 8:193907193934. Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi, Kanae Kawai Miyake, Mia Gorovoy, and Daniel Rubin. 2017. curated mammography data set for use in computer-aided detection and diagnosis research. Scientific data, 4(1):19. Sangjune Lee, Poonam Yadav, Yin Li, Jason Meudt, Jessica Strang, Dustin Hebel, Alyx Alfson, Stephanie Olson, Tera Kruser, Jennifer Smilowitz, et al. 2024. Dataset for gastrointestinal tract segmentation on serial mris for abdominal tumor radiotherapy. Data in Brief, page 111159. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2024. Llavamed: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36. Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. 2019. Compositional generalization for primitive substitutions. arXiv preprint arXiv:1910.02612. Yuexiang Li, Nanjun He, and Yawen Huang. 2022. Single domain generalization via spontaneous amplitude spectrum diversification. In MICCAI Workshop on Resource-Efficient Medical Image Analysis, pages 3241. Springer. Jie Lian, Jingyu Liu, Shu Zhang, Kai Gao, Xiaoqing Liu, Dingwen Zhang, and Yizhou Yu. 2021. structureaware relation network for thoracic diseases detection and segmentation. IEEE Transactions on Medical Imaging, 40(8):20422052. Xiao Liang. 2021. Adam dataset. https: //www.kaggle.com/datasets/xiaoliang2121/ adamdataset. Kaggle. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Jacob Macdonald, Zhe Zhu, Brandon Konkel, and Mazurowski. 2020. Siim-acr pneumothorax seghttps://doi.org/10.5281/zenodo. mentation. 7774566. Zenodo. Scott Mader. 2017. Mias mammography. https://www.kaggle.com/datasets/kmader/ mias-mammography. Kaggle. Salman Maqbool, Aqsa Riaz, Hasan Sajid, and Osman Hasan. 2020. m2caiseg: Semantic segmentation of laparoscopic images using convolutional neural networks. arXiv preprint arXiv:2008.10134. Christian Matek, Sebastian Krappe, Christian Münzenmayer, Torsten Haferlach, and Carsten Marr. 2021. An expert-annotated dataset of bone marrow cytology in hematologic malignancies. The Cancer Imaging Archive. Sarah Matta, Mathieu Lamard, Philippe Zhang, Alexandre Le Guilcher, Laurent Borderie, Béatrice Cochener, and Gwenolé Quellec. 2024. systematic review of generalization research in medical image classification. arXiv preprint arXiv:2403.12167. Teresa Mendonca, Celebi, Mendonca, and Marques. 2015. Ph2: public database for the analysis of dermoscopic images. Dermoscopy image analysis. Meta AI. 2024. Llama 3.2: Revolutionizing customizhttps://ai.meta.com/blog/ and vision with open, edge ai able models. llama-3-2-connect-2024-vision-edge -mobile-devices/. Shentong Mo and Paul Pu Liang. 2024. Multimed: Massively multimodal and multitask medical understanding. arXiv preprint arXiv:2408.12682. Paul Mooney. 2017. https://www.kaggle.com/datasets/ paultimothymooney/blood-cells. Kaggle. Blood cell images. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. 2023. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR. Loris Nanni, Michelangelo Paci, Florentino Luciano Caetano dos Santos, Heli Skottman, Kati JuutiUusitalo, and Jari Hyttinen. 2016. Texture descriptors ensembles enable image-based classification of maturation of human stem cell-derived retinal pigmented epithelium. PLoS One, 11(2):e0149399. Hieu Nguyen, Ha Nguyen, Hieu Pham, Khanh Lam, Linh Le, Minh Dao, and Van Vu. 2023. Vindr-mammo: large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. Scientific Data, 10(1):277. Masoud Nickparvar. 2021a. Brain tumor mri https://www.kaggle.com/datasets/ dataset. masoudnickparvar/brain-tumor-mri-dataset. Kaggle. Msoud Nickparvar. 2021b. Brain tumor mri dataset. Nikita Orlov, Wayne Chen, David Eckley, Tomasz Macura, Lior Shamir, Elaine Jaffe, and Ilya Goldberg. 2010a. Automatic classification of lymphoma images with transform-based global features. IEEE transactions on information technology in biomedicine : publication of the IEEE Engineering in Medicine and Biology Society, 14:100313. Nikita Orlov, Wayne Chen, David Eckley, Tomasz Macura, Lior Shamir, Elaine Jaffe, and Ilya Goldberg. 2010b. Automatic classification of lymphoma images with transform-based global features. IEEE transactions on information technology in biomedicine : publication of the IEEE Engineering in Medicine and Biology Society, 14:100313. Silvia Ovreiu, Elena-Anca Paraschiv, and Elena Ovreiu. 2021. Deep learning & digital fundus images: Glaucoma detection using densenet. In 2021 13th international conference on electronics, computers and artificial intelligence (ECAI), pages 14. IEEE. Andre GC Pacheco, Gustavo Lima, Amanda Salomao, Breno Krohling, Igor Biral, Gabriel de Angelo, Fábio CR Alves Jr, José GM Esgario, Alana Simora, Pedro BC Castro, et al. 2020. Pad-ufes-20: skin lesion dataset composed of patient data and clinical images collected from smartphones. Data in brief, 32:106221. Sachin Panchal, Ankita Naik, Manesh Kokare, Samiksha Pachade, Rushikesh Naigaonkar, Prerana Phadnis, and Archana Bhange. 2023. Retinal fundus multi-disease image dataset (rfmid) 2.0: dataset of frequently and rarely identified diseases. Data, 8(2):29. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding multimodal large arXiv preprint language models to the world. arXiv:2306.14824. Hieu Pham, Thanh Tran, and Ha Quy Nguyen. 2022. Vindr-pcxr: An open, large-scale pediatric chest xray dataset for interpretation of common thoracic diseases. PhysioNet (version 1.0. 0), 10:2. Hieu Huy Pham, Nguyen Trung, and Ha Quy Nguyen. 2021. Vindr-spinexr: large annotated medical image dataset for spinal lesions detection and classification from radiographs. PhysioNet. Manuel Alejandro Rodríguez, Hasan AlMarzouqi, and Panos Liatsis. 2022. Multi-label retinal disease classification using transformers. IEEE Journal of Biomedical and Health Informatics. Konstantin Pogorelov, Kristin Ranheim Randel, Thomas de Lange, Sigrun Losada Eskeland, Carsten Griwodz, Dag Johansen, Concetto Spampinato, Mario Taschwer, Mathias Lux, Peter Thelin Schmidt, Michael Riegler, and Pål Halvorsen. 2017a. Nerthus: bowel preparation quality video dataset. In Proceedings of the 8th ACM on Multimedia Systems Conference, MMSys17, pages 170174, New York, NY, USA. ACM. Konstantin Pogorelov, Kristin Ranheim Randel, Carsten Griwodz, Sigrun Losada Eskeland, Thomas de Lange, Dag Johansen, Concetto Spampinato, Duc-Tien Dang-Nguyen, Mathias Lux, Peter Thelin Schmidt, Michael Riegler, and Pål Halvorsen. 2017b. Kvasir: multi-class image dataset for computer aided gastrointestinal disease detection. In Proceedings of the 8th ACM on Multimedia Systems Conference, MMSys17, pages 164169, New York, NY, USA. ACM. Praveen. 2019. Coronahack x-ray https://www.kaggle.com/datasets/ dataset. praveengovi/coronahack-chest-xraydataset. Kaggle. chest Pavle Prentasic, Sven Loncaric, Zoran Vatavuk, Goran Bencic, Marko Subasic, Tomislav Petkovic, Lana Dujmovic, Maja Malenica Ravlic, Nikolina Budimlija, and Rašeljka Tadic. 2013. Diabetic retinopathy image database(dridb): new database for diabetic retinopathy screening programs research. pages 711 716. Xianbiao Qi, Guoying Zhao, Jie Chen, and Matti Pietikäinen. 2016. Hep-2 cell classification: The role of gaussian scale space theory as pre-processing approach. Pattern Recognition Letters, 82:3643. Raddar. 2019. Chest x-rays (indiana university). https://www.kaggle.com/datasets/raddar/ chest-xrays-indiana-university?select= indiana_reports.csv. Kaggle. Tawsifur Rahman, Amith Khandakar, Muhammad Abdul Kadir, Khandaker Rejaul Islam, Khandakar Islam, Rashid Mazhar, Tahir Hamid, Mohammad Tariqul Islam, Saad Kashem, Zaid Bin Mahbub, et al. 2020. Reliable tuberculosis detection using chest x-ray with deep learning, segmentation and visualization. Ieee Access, 8:191586191601. MOHD ZAID RASHID. 2024. Oral cancer https://www.kaggle.com/datasets/ dataset. zaidpy/oral-cancer-dataset. Kaggle. Veronica Rotemberg, Nicholas Kurtansky, Brigid BetzStablein, Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al. 2021. patientcentric dataset of images and metadata for identifying melanomas using clinical context. Scientific data, 8(1):34. Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. 2024. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416. Salman Sajid. 2024. Oral diseases. https: //www.kaggle.com/datasets/salmansajid05/ oral-diseases/data. Kaggle. Shaker. 2018. Human sperm head morphology dataset (hushem). Mendeley Data, 3. Julio Silva-Rodríguez, Adrián Colomer, María Sales, Rafael Molina, and Valery Naranjo. 2020. Going deeper through the gleason scoring scale: An automatic end-to-end system for histology prostate grading and cribriform pattern detection. Computer Methods and Programs in Biomedicine, 195:105637. Eduardo Soares, Plamen Angelov, Sarah Biaso, Michele Higa Froes, and Daniel Kanda Abe. 2020. Sars-cov-2 ct-scan dataset:a large dataset of real patients ct scans for sars-cov-2 identification. Cold Spring Harbor Laboratory Press. Malliga Subramanian, Kogilavani Shanmugavadivel, Obuli Sai Naren, Premkumar, and Rankish. 2022. Classification of retinal oct images using deep learning. In 2022 International Conference on Computer Communication and Informatics (ICCCI), pages 17. Summers and Ronald. 2020. hcc. ChestXray-NIHCC/folder/36938765345. NIH. Chestxray nihttps://nihcc.app.box.com/v/ SunneYi. 2021. Chest CT-Scan images Dataset. Siham Tabik, Anabel Gómez-Ríos, José Luis MartínRodríguez, Iván Sevillano-García, Manuel Rey-Area, David Charte, Emilio Guirado, Juan-Luis Suárez, Julián Luengo, MA Valero-González, et al. 2020. Covidgr dataset and covid-sdnet methodology for predicting covid-19 based on chest x-ray images. IEEE journal of biomedical and health informatics, 24(12):35953605. Sucheng Ren, Xiaoke Huang, Xianhang Li, Junfei Xiao, Jieru Mei, Zeyu Wang, Alan Yuille, and Yuyin Zhou. 2024. Medical vision generalist: Unifying medical imaging tasks in context. arXiv preprint arXiv:2406.05565. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. 2024. Towards generalist biomedical ai. NEJM AI, 1(3):AIoa2300138. Peking University. 2019. Odir-2019 dataset. https://odir2019.grand-challenge.org/ introduction/. Grand Challenge. Preet Viradiya. 2020. Brain tumor dataset. https://www.kaggle.com/datasets/ preetviradiya/brain-tumor-dataset. Kaggle. Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, and Liwei Wang. 2025. Git: Towards generalist vision transformer through universal language interface. In European Conference on Computer Vision, pages 5573. Springer. Linda Wang, Zhong Qiu Lin, and Alexander Wong. 2020. Covid-net: tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images. Scientific Reports, 10(1):19549. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. Preprint, arXiv:2409.12191. Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. 2024c. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36. Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. 2017. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20972106. wjXiaochuangw. 2019. Covid-19-ct scan images. Zhenlin Xu, Marc Niethammer, and Colin Raffel. 2022. Compositional generalization in unsupervised compositional representation learning: study on disentanglement and emergent language. Advances in Neural Information Processing Systems, 35:25074 25087. Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomitchev, Mohannad Hussain, Paras Lakhani, Phil Culliton, and Shunxing Bao. 2019. Siim-acr pneumothorax segmentation. https://kaggle.com/competitions/ siim-acr-pneumothorax-segmentation. Kaggle. Yaya Zha. 2021. Rus-chn. https://aistudio.baidu. com/datasetdetail/69582/0. AI Studio. Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, and Tat-Seng Chua. 2023a. Next-chat: An lmm for chat, detection and segmentation. arXiv preprint arXiv:2311.04498. Edward Zhang and Sauman Das. 2022. Glaucoma detection. https://www.kaggle.com/datasets/ sshikamaru/glaucoma-detection. Kaggle. Ruipeng Zhang, Qinwei Xu, Chaoqin Huang, Ya Zhang, and Yanfeng Wang. 2022. Semi-supervised domain generalization for medical image analysis. In 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. 2023b. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601. Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Change Loy Chen, and Shuicheng Yan. 2024a. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023c. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415. Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena Yeung-Levy. 2024b. Why are visually-grounded language models bad at image classification? arXiv preprint arXiv:2405.18415. Jinyu Zhao, Yichen Zhang, Xuehai He, and Pengtao Xie. 2020. Covid-ct-dataset: ct scan dataset about covid-19. arXiv preprint arXiv:2003.13865. Chuang Zhu, Wenkai Chen, Ting Peng, Ying Wang, and Mulan Jin. Hard sample aware noise robust learning for histopathology image classification. IEEE transactions on medical imaging. Chuang Zhu, Wenkai Chen, Ting Peng, Ying Wang, and Mulan Jin. 2021. Hard sample aware noise robust learning for histopathology image classification. IEEE transactions on medical imaging, 41(4):881 894. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592. Xile Zhu. 2022. Lc25000. https://www.kaggle. com/datasets/xilezhu/lc25000. Kaggle. Абеуов Нурмхаммед Батыбеклы. 2021. Augemnted ocular https://www.kaggle.com/datasets/ diseases. nurmukhammed7/augemnted-ocular-diseases. Kaggle."
        },
        {
            "title": "A More Experiments",
            "content": "A.1 CG on Detection Tasks (RQ 4) Previous studies have shown that training on detection datasets can improve model performance in classification tasks. Building on this insight, we further examine its applicability to CG. For this investigation, all possible detection subset combinations in Med-MAT were selected to explore their impact on CG (Selection Strategy). Experimental Setup: We conducted generalization experiments for detection and classification. Specifically, we performed generalization validation on Next-Chat (Zhang et al., 2023a) and MiniGPT-v2 (Chen et al., 2023a). Next-Chat models the bounding box as an embedding and utilizes decoder for decoding, while MiniGPT-v2 treats the bounding box as text token, which are common approaches used by existing MLLM implementations for detection. By conducting CG validation using distinct bounding box modeling methods, we further demonstrate the broad applicability of the CG approach. Each experiment was conducted on 8 A800 (80GB) GPUs. The two backbone was trained separately in this experiment. For Next-Chat, we directly trained the model in its second training stage and fine-tuned it for 2 epochs on our composition datasets with learning rate of 2e-5, keeping all other training parameters at their default settings. Similarly, for MiniGPT-v2, we trained the backbone model from the second stage, starting with learning rate of 2e5 and gradually reducing it to 2e-6 over 3 epochs. The experiments in Table 5 and Table 6 present the results for Next-Chat and MiniGPT-v2, respectively. The separation regions closely align with the 4 Direction Types defined in the classification task, except for Fix Task, which is excluded due to the substantial differences between detection and classification tasks, making it impossible to find the fix one. The results demonstrate that joint training on detection and classification tasks effectively compositional generalizes to classification tasks in both backbones, leading to higher model accuracy. This indicates that CG is also present in these tasks. A.2 Other MLLMs (RQ 5) The choice to use models like LLaVA, MiniGPT-v2 and NExT-GPT is because their training data and overall process are publicly available, ensuring that they have not been too much exposed to medical images, thus preventing bias in the medical image knowledge injection into the MLLM. To ensure the experiment results are not influenced by the model choice, we also tested several other models on some subsets of Med-MAT and observed similar results. Selection Strategy: For testing, some generalized combinations were selected from classification tasks 3. Using random seed of 42, we shuffled each Direction Types combinations and selected the first two compositions as test data. Experimental Setup: We conducted experiments to evaluate the compatibility of CG across different backbone architectures. We selected two MLLMs with representative architectures, namely Qwen2-VL-7B-Instruct (Wang et al., 2024b) and Llama-3.2-11B-Vision-Instruct (Meta AI, 2024), to assess the performance of CG on these models. Each experiment involved full-parameter finetuning of all models over 5 epochs, utilizing 8 A800 (80GB) GPUs. The training was performed with batch size of 32 and learning rate set to 2e-6, ensuring that all parameters were updated to optimize the model performance. The experimental results presented in Table 7 and Table 8 demonstrate that the CG persists across these different backbone architectures. This observation indicates that CG is not confined to specific model type, thereby highlighting its universal applicability and robustness across various model frameworks. Such findings underscore the versatility of CG, suggesting it can be effectively integrated into wide range of models. The Dataset: Med-MAT This section provides an overview of Med-MAT. First, detailed explanation of MAT-Triplet will be presented in B.1. Next, the methods for constructing the QA formatting will be discussed in B.2. Finally, the data composition details and openTable 5: Result of NEXT-Chat on CG by using detection and classification tasks to generalize classification Target dataset. Generalization results on classification datasets: \"Related Combination\" is the training set, \"Target Subset\" is the goal. Baseline and Trained represent the models accuracy without training and trained on related data, respectively. Green section indicates successful generalization, while red section denotes failure. The 4 segmented areas represent different Direction Types: fixed modality, fixed area, and modality-area paired combinations. Related Combination Target Subset Baseline Trained Lung, Lung Det Lung, Lung Det Bones, Spinal Error Det Bones, Spinal Error Det Bones, Diseases Breast, Diseases Breast, Diseases Lung, Diseases MRI, Diseases Det X-ray, Lung Det End, Level CT, COVID Lung, Diseases Lung, Diseases Bones, Diseases Bones, Diseases End, Diseases X-ray, COVID Der, Skin, Cancer Det FP, Fundus, Diseases Der, Skin, Diseases Mic, Cell, Cancer Det CT, Kidney, Diseases Mic, Cell, Diseases 49 49 20 24 23 24 24 52 54 30 33 27 26 29 26 Table 6: Result of MiniGPT-v2 on CG by using detection and classification tasks to generalize classification Target dataset. Generalization results on classification datasets: \"Related Combination\" is the training set, \"Target Subset\" is the goal. Baseline and Trained represent the models accuracy without training and trained on related data, respectively. Green section indicates successful generalization, while red section denotes failure. The 3 segmented areas represent different Direction Types: fixed modality, fixed area, and modality-area paired combinations. Related Combination Target Subset Baseline Trained Lung, Lung Det Lung, Lung Det Bones, Spinal Error Det Bones, Spinal Error Det Bones, Diseases Breast, Diseases Breast, Diseases Lung, Diseases MRI, Diseases Det X-ray, Lung Det End, Level CT, COVID Lung, Diseases Lung, Diseases Bones, Diseases Bones, Diseases End, Diseases X-ray, COVID Der, Skin, Cancer Det FP, Fundus, Diseases Der, Skin, Diseases Mic, Cell, Cancer Det CT, Kidney, Diseases Mic, Cell, Diseases 41 41 31 31 24 27 20 47 49 35 37 26 23 30 24 Table 7: Result of Qwen2-VL on selected classification datasets in Med-MAT. Green section indicates successful generalization, while red section denotes failure. Related Combination Target Subset Baseline Trained Bones, State Lung, COVID CT, COVID CT, State X-ray, Lung X-ray, Lung FP, Fundus, Diseases Bones, Diseases Lung, Diseases X-ray, COVID X-ray, State CT, Lung CT, Lung(Cancer) Mic, Cell, Level FP, Fundus, Level Mic, Cell, Cell Identification FP, Fundus, Level Mic, Cell, Level Breast, Diseases Bones, Diseases X-ray, Diseases X-ray, Diseases CT, Brain(Cancer) CT, Brain 61 80 35 35 32 65 48 65 91 40 43 33 72 45 41 source specification will be provided in B.3. B.1 Details of MAT-Triplet stands MAT-Triplet for Medical Modality, Anatomical Area, and Medical Task. We define all samples in Med-MAT using these three components and integrate datasets with identical triplets into subsets."
        },
        {
            "title": "Medical Modality refers to different types of",
            "content": "techniques or methods used in medical imaging or data acquisition. Each modality is designed to present the human bodys structures or pathological features in unique ways, providing auxiliary support for clinical diagnosis and treatment. Most modalities exhibit significant visual differences, making them easily distinguishable. Med-MAT encompasses 11 modalities, including common ones such as Computed Tomography (CT), Magnetic Table 8: Result of Llama-3.2-Vision on selected classification datasets in Med-MAT. Green section indicates successful generalization, while red section denotes failure. Related Combination Target Subset Baseline Trained Bones, State Lung, COVID CT, COVID CT, State X-ray, Lung X-ray, Lung FP, Fundus, Diseases Bones, Diseases Lung, Diseases X-ray, COVID X-ray, State CT, Lung CT, Lung(Cancer) Mic, Cell, Level FP, Fundus, Level Mic, Cell, Cell Identification FP, Fundus, Level Mic, Cell, Level Breast, Diseases Bones, Diseases X-ray, Diseases X-ray, Diseases CT, Brain(Cancer) CT, Brain 52 64 33 33 31 49 55 10 59 75 38 41 29 57 61 32 Resonance Imaging (MRI), X-ray, Fundus Photography (FP), Endoscopy (End), Optical Coherence Tomography (OCT), and Ultrasound (US), as well as rare and specialized modalities like Colonoscopy (Co), Dermoscopy (Der), Digital Pathology (DP), and Microscopy (Mic). Anatomical Area refers to specific anatomical structures or regions within the human body or other organisms, defined by distinct anatomical characteristics to describe various body parts, their functions, and relative positions. Med-MAT encompasses 14 anatomical areas, including the cervix, kidney, lung, brain, intestine, bladder, fundus, retina, breast, bones, and chest. To further facilitate data description, additional categories such as skin, mouth, and cell are included as specialized anatomical areas. Medical Task refers to the specific detection task that needs to be performed on the dataset. MedMAT includes 13 distinct tasks, with classification tasks encompassing Quality Identification (image quality analysis), COVID Diagnosis, Cancer Diagnosis (determining the presence of specific disease), State (such as identifying brain hemorrhage), Level Identification (assessing disease severity), and Multiple Classification (classifying multiple diseases or cell types). Given the limited options of COVID Diagnosis and Cancer Diagnosis, these tasks can be interpreted as identifying whether patient is in diseased state. To enhance generalization and provide more diverse examples, these tasks are grouped under the broader category of State. In addition, we have 16 datasets defining segmentation or classification tasks with different objectives. B.2 QA construction method large amount of image-label datasets was collected to build the Med-MAT dataset. To ensure compatibility with MLLM training inputs and outputs, all data is transformed into questionanswering format. Questions are formulated based on modality, anatomical area, and medical task, with 6 question prompts applied to each subset. The labels within each data subset will be clustered to prevent redundant definitions of the same condition. Then, all training set and test set will be converted into multiple-choice questions following the template in Table 8. Each question will have up to four options, with distractor options randomly selected from the corresponding subset. B.3 Data composition and Open-source"
        },
        {
            "title": "Specification",
            "content": "Med-MAT is composed of multiple datasets. After being transformed into different QA formats, the new data is organized into several subsets to support generalization experiments in medical imaging. Table 9 shows all of our subset datasets, which are separated based on different combinations in MAT-Triplet. The specific MAT-Triplets are listed, along with the labels corresponding to the imagelabel datasets for each subset. Correspondingly, all the image-label datasets are also displayed in Table 10, which includes their names, descriptions of the tasks performed, download links, and the level of accessibility. All question-answering text datasets in MedMAT will be publicly available. To accommodate varying access permissions, we will release datasets based on their respective licenses: openly accessible datasets will be directly available, while restricted datasets can be accessed by applying through the links provided in this paper. We hope this dataset will support and advance future generalization experiments on medical imaging. Multiple-choice Questions Template <question> A. <option_1> B. <option_2> C. <option_3> D. <option_4> Answer with the options letter from the given choices directly. Figure 8: The Template of multiple-choice questions. Figure 9: Illustration of diverse samples with varying numbers of candidate options in the Med-MAT dataset. Table 9: The details of subset. In particular, Co stands for Colposcopy, CT represents Computed Tomography, DP refers to Digital Photography, FP is for Fundus Photography, MRI denotes Magnetic Resonance Imaging, OCT signifies Optical Coherence Tomography, Der refers to Dermoscopy, End stands for Endoscopy, Mic indicates Microscopy Images, and US represents Ultrasound. The blue section represents the classification dataset and the green section represents the detection Subset No. Modality Anatomical Area Task Datasets No. 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 Co CT CT CT CT CT Der Der DP DP End End End FP FP FP FP Mic Mic Mic Mic MRI OCT US X-ray X-ray X-ray X-ray X-ray X-ray X-ray X-ray Mic FP X-ray X-ray X-ray Der End End End Mic US US MRI MRI X-ray X-ray X-ray X-ray FP FP FP Cervix Kidney Lung Lung Brain Brain Skin Skin Mouth Mouth Intestine Bladder Intestine Fundus Fundus Fundus Fundus Cell Cell Cell Cell Brain Retina Breast Bones Bones Bones Lung Breast Lung Chest Brain Cell Fundus Bones Bones Breast Skin Intestine Intestine Intestine Cell Chest Thyroid Intestine Liver Lung Lung Bones Chest Fundus Fundus Fundus 1 2 3,4,6 5 7 8 10 9, 11-15, 71, 72, 74 16 17 18 19 20 21-23, 26-28, 31, 32, 75 Cervical Picture Quality Evaluation Kidney Diseases Classification COVID-19 Classification Lung Cancer Classification Brain Hemorrhage Classification Brain Cancer Classification Melanoma Type Classification Skin Diseases Classification Teeth Condition Classification Oral Cancer Classification Intestine Cleanliness Level Cancer Degree Classification Intestine Diseases Classification Eye Diseases Classification Multiple-labels Eye Diseases Classification 24, 25, 68 Blindness Level Retinal Images Quality Evaluation Cell Type Classification Prostate Cancer Degree Classification Multiple-labels Blood Cell Classification Cancer Classification Head Diseases Classification Retina Diseases Classification Breast Cancer Classification Degree Classification of Knee Fractured Classification Vertebrae Diseases Classification COVID-19 and Pneumonia Classification Breast Diseases Classification Tuberculosis Classification Multiple-labels Chest Classification Tumor Classification Multi-labels Diseases Level Identification Level Identification Spinal lesion Classification Multi-labels Diseases 29 30 33, 36-38, 39-41, 44, 65, 70 34 35 42, 67 44, 45 46, 47 48 49, 53 50, 51 52 54-57, 60, 62, 81 58, 78 59, 79 61, 73, 76, 77, 80, 85, 87 63 84 66 69 86 Lesion Det/Seg PolyP Det/Seg Surgical Procedures Det/Seg Multi-labels Det/Seg Cancer Cell Det/Seg Cancer Det/Seg Thyroid Nodule Region Det/Seg Multi-labels Det/Seg Liver Det/Seg Lung Det/Seg Pneumothorax Det/Seg Spinal Anomaly Det Multi-labels Det Vessel Seg Optic Disc and Cup Seg Optic Disc Seg 88-91 92-93 94 95 96 97 98 103 104, 105 99 106 100 101, 102 107 108 109 Table 10: The details of the medical datasets are provided No. Name Description Citation (BenO et al., 2017) (Islam et al., 2022a) (Soares et al., 2020) (Zhao et al., 2020) (SunneYi, 2021) (wjXiaochuangw, 2019) (Kitamura, 2018) (Data, 2023) (Giotis et al., 2015) (Rotemberg et al., 2021) (Pacheco et al., 2020) (Islam et al., 2022b) (Gutman et al., 2016) (Combalia et al., 2019) (Katanskiy, 2019) (Sajid, 2024) (RASHID, 2024) (Pogorelov et al., 2017a) (Lazo et al., 2023) (Pogorelov et al., 2017b) (Ovreiu et al., 2021) (Батыбеклы, 2021) (Cen et al., 2021) (Rodríguez et al., 2022) (Panchal et al., 2023) (Cardozo et al., 2023) (Cardozo et al., 2023) (Liang, 2021) (Karthik et al., 2019) (Prentasic et al., 2013) (Zhang and Das, 2022) (de Vente et al., 2023) (Qi et al., 2016) (Silva-Rodríguez et al., 2020) (Mooney, 2017) (Bukun, 2019) (Zhu et al.) (Shaker, 2018) (Matek et al., 2021) (Kather et al., 2018) (Orlov et al., 2010a) (Cukierski, 2018) (Zhu, 2022) (Feltrin, 2022) Intel & MobileODT Cervical Screening Cervix Type in Screening 1 Normal or Cyst or Tumor CT Kindney Dataset 2 COVID19, Classification Dataset SARS-COV-2 Ct-Scan 3 COVID19, Classification Dataset. COVID CT COVID-CT 4 5 Cancer Classification Chest CT-Scan COVID19, Classification COVID-19-CT SCAN IMAGES 6 Head Hemorrhage Head CT 7 Head Cancer 8 CT of Brain 9 MED-NODE Melanoma or Naevus Melanoma, Benign or Malignant 10 ISIC 2020 Skin Multi Classification 11 PED-UFES-20 Skin Desease Multi Classification 12 Web-scraped Skin Image 13 ISBI 2016 Skin Lesion Classification Skin Desease Multi Classification 14 ISIC 2019 Skin Cancer Multi Classification 15 Skin Cancer ISIC Teeth condition classification 16 Dental Condition Dataset 17 Oral Cancer Dataset Oral cancer Classification Cleanliness level 18 The Nerthus Dataset Canser Degree Classification 19 Endoscopic Bladder Tissue Multi Disease Classification 20 Kvasir 21 ACRIMA Glaucoma Multi Classification of eye diseases 22 Augemnted ocular diseases AOD Multi Classification of eye diseases 23 JSIEC Multi Classification of eye diseases 24 Multi-Label Retinal Diseases Multi Classification of eye diseases 25 RFMiD 2.0 Ocular toxoplasmosis 26 ToxoFundus(Data Processed Paper) Ocular toxoplasmosis 27 ToxoFundus(Data Raw 6class All) Age-related Macular Degeneration 28 Adam dataset Blindness Level Identification 0 4 29 APTOS 2019 Blindness Quality Testing of Retinal Images 30 DRIMBD Glaucoma Classification 31 Glaucoma Detection 32 AIROGS Glaucoma Classification Multi Classification 33 ICPR-HEp-2 Cancer Degree Classification 34 SICAPv2 Blood Cell Classificaion (Multi) 35 Blood Cell Images Cell type and beginormag 36 BreakHis Multi Classification of pathologists 37 Chaoyang Sperm Head Morphology Classificaion 38 HuSHeM Bone Marrow Cell Classification 39 Bone Marrow Cell Classification Multi Classification 40 NCT-CRC-HE-100K Multi Classification 41 Malignant Lymphoma Classification Cancer Classification 42 Histopathologic Cancer Detection 43 LC25000 Multi Classification of Lung and Colon Multi Classification 44 Brain Tumor 17 Classes Pituitary or Glioma or Meningioma or Notumor (Nickparvar, 2021a) 45 Tumor Classification (Orlov et al., 2010b) Multi Classification of eye diseases 46 Malignant Lymphoma Classification (Subramanian et al., 2022) Multi Classification of eye diseases 47 Retinal OCT-C8 48 BUSI (Al-Dhabyani et al., 2020) Breast Cancer (Gornale and Patravali, 2020) 49 Digital Knee X-Ray Images Degree Classification of Knee (Nickparvar, 2021b) 50 Bone Fracture Multi-Region X-ray Data Fractured Classification 51 Fracture detection (Batra, 2024) Fractured Classification (Fraiwan et al., 2022) Vertebrae 52 The vertebrae X-ray image (Chen, 2018) Knee Osteoarthritis with severity grading 53 Knee Osteoarthritis Dataset (Jaeger et al., 2014) COVID19, Classification Dataset. 54 Shenzhen Chest X-Ray Set 55 Chest X-ray PD (Asraf and Islam, 2021) COVID and Pneumonia (Chowdhury et al., 2020) 56 COVID-19 CHEST X-RAY DATABASE COVID and Pneumonia (Tabik et al., 2020) 57 COVIDGR (Mader, 2017) 58 MIAS (Rahman et al., 2020) 59 Tuberculosis Chest X-Ray Database (Kermany, 2018) 60 Pediatric Pneumonia Chest X-Ray COVID19, Classification Multi Classification of Breast Tuberculosis Pneumonia Classification No. Name Description Citation Table 11: Continued from Table 10. (Wang et al., 2017) (Praveen, 2019) (Viradiya, 2020) (Groh et al., 2021) (Nanni et al., 2016) (Benítez et al., 2021) (Codella et al., 2019) (University, 2019) (Zha, 2021) (Zhu et al., 2021) (Codella et al., 2019) (Codella et al., 2018) (Lian et al., 2021) (Ali et al., 2022) (JR2NGB, 2019) (Raddar, 2019) (Arevalo, 2020) (Lee et al., 2017) (Karaca, 2022) (Summers and Ronald, 2020) (Wang et al., 2020) (Nguyen et al., 2023) (Acevedo et al., 2020) (Le et al., 2022) (Anouk Stein et al., 2018) (Pham et al., 2021) (Pham et al., 2022) (Mendonca et al., 2015) (Gutman et al., 2016) (Gutman et al., 2016) (Codella et al., 2018) (Bernal et al., 2015) (Jha et al., 2020) (Maqbool et al., 2020) Pnemonia Classifcition with Virus type Tumor Classification Multi Classification Multi Classification Diabetic Retinopathy Level Cancer Classification Multiple Labels Classification Bone Age Classification Multi Classification of pathologists Multi Classification Multi Classification Multi Classification Only Monkeypox Multi Classification Multi-label Classification Multi-label Classification Multi Classification Tuberculosis Multi-label Classification COVID19, Classification Multi-label Classification Multi Classification Multi-label Classification (Only green) 61 Random Sample of NIH Chest X-Ray Dataset Multi Classificaiton of Chest 62 CoronaHack-Chest X-Ray 63 Brain Tumor Dataset 64 Fitzpatrick 17k (Nine Labels) 65 BioMediTech 66 Diabetic retinopathy 67 Leukemia 68 ODIR-5K 69 Arthrosis 70 HSA-NRL 71 ISIC 2018 (Task 3) 72 ISIC 2017 (Task 3) 73 ChestX-Det 74 Monkeypox Skin Lesion Dataset 75 Cataract Dataset 76 ChestX-rays IndianaUniversity 77 CheXpert v1.0 small 78 CBIS-DDSM 79 NLM-TB 80 ChestXray-NIHCC 81 COVIDx CXR-4 82 VinDr-Mammo 83 PBC dataset normal DIB 84 Human Protein Atlas 85 RSNA Pneumonia Detection Challenge 2018 Multi-label Classification 86 VinDr-SpineXR 87 VinDr-PCXR 88 PH2 89 ISBI 2016 (Task3B) 90 ISIC 2016 (Task 1) 91 ISIC 2017 92 CVC-ClinicDB 93 Kvasir-SEG 94 m2caiseg 95 EDD 2020 96 SICAPv2 97 BUSI 98 TN3K 99 NLM-TB 100 VinDr-SpineXR 101 VinDr-PCXR 102 ChestX-Det 103 UW-Madison Gl Tract Image Segmentation 104 Duke Liver Dataset MRI v1 105 Duke Liver Dataset MRI v2 106 SIIM-ACR Pneumothorax Segmentation 107 FIVES 108 RIM-ONE DL 109 PALM19 Multi Classification of Bones Diseases Multi-label Classification Melanoma Segmentation Melanoma Segmentation Melanoma Segmentation Melanoma Segmentation Polyp Segmentation Polyp segmentation Surgical Instrument Segmentation Multiple Diseases Segmentation in Intestine (Ali et al., 2020) Cancer Cells Segmentation Cancer Segmentation Thyroid Nodule Segmentation Lung Segmentation (With left or right) Spinal X-ray Anaomaly Detection Multiple Diseases Segmentation in Chest Multiple Diseases Segmentation in Chest Surgical Instrument Segmentation Liver Segmentation Liver Segmentation Pneumothorax Segmentation Fundus Vascular Segmentation Optic Disc and Cup Segmentation Optic Disc Segmentation (Silva-Rodríguez et al., 2020) (Hesaraki, 2022) (Gong et al., 2022) (Gong et al., 2021) (Pham et al., 2021) (Pham et al., 2022) (Lian et al., 2021) (Lee et al., 2024) (Macdonald et al., 2020) (Macdonald et al., 2020) (Zawacki et al., 2019) (Jin et al., 2022) (Batista et al., 2020) (Fu et al., 2019)"
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}