{
    "paper_title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
    "authors": [
        "Yi Wang",
        "Zhitong Xiong",
        "Chenying Liu",
        "Adam J. Stewart",
        "Thomas Dujardin",
        "Nikolaos Ioannis Bountos",
        "Angelos Zavras",
        "Franziska Gerken",
        "Ioannis Papoutsis",
        "Laura Leal-Taixé",
        "Xiao Xiang Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 9 4 8 1 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
            "content": "Yi Wang1 Zhitong Xiong1 Chenying Liu1,2 Adam J. Stewart1 Thomas Dujardin1 Nikolaos Ioannis Bountos3,4 Ioannis Papoutsis Angelos Zavras3,4 Franziska Gerken5 Laura Leal-Taixe5 Xiao Xiang Zhu 1,2 1 Technical University of Munich 2 Munich Center for Machine Learning 3 National Technical University of Athens & National Observatory of Athens 4 Harokopio University of Athens 5 NVIDIA"
        },
        {
            "title": "Abstract",
            "content": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earths surface, and overlook valuable metadata beyond imagery. In this work, we take step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, massivescale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earths surface to its atmosphere; 2) CopernicusFM, unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https:// github.com/zhu-xlab/Copernicus-FM . 1. Introduction Earth observation (EO) satellites provide critical means of monitoring planetary dynamics, capturing land cover changes, atmospheric composition, and other important environmental phenomena [51, 71, 74]. Recent advances in self-supervised learning have revolutionized the usage of big EO data, enabling the development of generalpurpose foundation models to learn generalized representations from unlabeled imagery at scale [60, 76]. Despite Figure 1. Overview of our efforts towards unified Copernicus foundation model, from pretraining to benchmarking. rapid progress, the evolution of EO foundation models faces critical limitations in three dimensions: sensor diversity, model flexibility, and evaluation breadth. First, current pretraining datasets predominantly focus on highto medium-resolution sensors like Sentinel1/2 [19, 58] and Landsat [67] observing the Earths surface [6, 54, 62]. This excludes lower-resolution but temporally rich missions like Sentinel-3 [17] and Sentinel-5P [59], which provide near-daily global coverage of land, oceanic, and atmospheric variables critical for climate studies. Second, most existing EO foundation models adopt rigid architectures tailored to one or more specific sensor modalities, lacking the capacity to dynamically adapt to new spectral bands or non-spectral input. While recent efforts [10, 69] introduce some spectral flexibility, they still lack mechanisms to handle non-spectral variables that possess significant amount of EO data. Furthermore, current foundation model evaluation benchmarks primarily focus on surface applications using RGB, multispectral, or SAR sensors [37, 43], while overlooking coarse-scale sensors and atmospheric tasks. These limitations hinder the develop1 ment of versatile multimodal foundation models that integrate EO with weather and climate research, highlighting both critical challenge and promising opportunity in the era of global environmental change. To address these challenges, we introduce three synergistic contributions that enhance the scalability, versatility, and multimodal integration of EO foundation models. First, we introduce Copernicus-Pretrain, one of the largest and most diverse EO pretraining datasets to date, comprising 18.7 million aligned observations from all major operational Copernicus Sentinel missions (Sentinel-1 to Sentinel5P). Unlike prior datasets focusing on fine-grained surface observations, Copernicus-Pretrain enables holistic modeling of Earth system interactions by integrating atmospheric variables and coarse-scale observations with wider and more frequent coverage. Second, we propose CopernicusFM, unified foundation model capable of processing any spectral or non-spectral sensor using dynamic hypernetworks. With additional support for metadata integration, it is valuable for wide range of practical applications. Third, we establish Copernicus-Bench, comprehensive evaluation benchmark with 15 downstream tasks hierarchically organized across preprocessing (e.g., cloud removal), base applications (e.g., land cover classification), and specialized applications (e.g., air quality estimation). This benchmark enables systematic assessment of foundation model performances across various Sentinel missions on different levels of practical applications. Our work opens new frontiers in three directions: (1) scaling EO pretraining to unified multimodal datasets, breaking the traditional silos between surface and atmospheric observations; (2) verifying that dynamic architectures can overcome sensor heterogeneity, longstanding challenge in multi-source remote sensing; and (3) providing the first evidence that joint cross-modal pretraining enhances performance on both surface and atmospheric tasks. Furthermore, our efforts create novel opportunities to integrate EO foundation models with weather and climate prediction systemsfor instance, using compressed EO embeddings (with rich semantic information) in addition to geographical coordinates to support climate modeling. Sec. 1 gives an introduction, Sec. 2 reviews prior work, Sec. 3 introduces the Copernicus-Pretrain dataset, Sec. 4 proposes the Copernicus-FM model, Sec. 5 presents the CopernicusBench benchmark, Sec. 6 discusses the potential of bridging EO and climate, and Sec. 7 concludes the paper. 2. Related work EO pretraining datasets Early efforts like fMoW [14], Million-AID [40], and SEN12MS [53] pioneered largescale pretraining with supervised datasets before the era of self-supervised learning. SeCo [42] introduced Gaussian sampling around populated regions to enrich the landscape diversity with multiseasonal time series, which was further extended by SSL4EO-S12 [62] and SSL4EO-L [54] through overlap and NaN filtering on Sentinel-1/2 and Landsat series. SatlasPretrain [6] and Major TOM [23] boost the dataset sizes through more dense global coverage. Recently, increasing efforts have been spent to broaden sensor diversity such as SpectralEarth [12] for hyperspectral pretraining and MMEarth [45] that gathers Sentinel-1/2, elevation, and several EO products together. Our CopernicusPretrain dataset aligns all primary Sentinel missions (1 5P) with extended global coverage (e.g., polar regions), enabling joint surfaceatmosphere modeling at scale. EO foundation models Single-sensor models dominate early foundation model research, which can be categorized based on pretraining strategies into: 1) contrastive methods like GASSL [5], SeCo [42], MATTER [1], CACo [41], SatMIP [11], etc., 2) masked image modeling (MIM) methods such as SatMAE [16], Prithvi [35], SpectralGPT [32], Scale-MAE [50], and many others [39, 45, 47, 56, 57, 65], and 3) hybrid methods like GFM [44], SoftCon [64], SARJEPA [38], etc. Most of these models focus on optical data. In trend towards multimodal pretraining, mainstream approaches use either separate encoders (DeCUR [63], CROMA [24], SkySense [26], etc.) or joint encoders with few fixed modalities [2, 3, 27, 34, 61, 70, 73]. While flexible multimodal architectures like FoMo-Net [10], SenPaMAE [49], and DOFA [69] support input with any channels, they are restricted to spectral sensors and ignore metadata. Our Copernicus-FM enhances this approach through dynamic hypernetworks that adapt to arbitrary spectral/nonspectral inputs and support metadata integration, effectively unifying surface and atmospheric data streams. EO benchmarks Existing benchmarks for evaluating EO foundation models vary widely in scope and focus but are, in general, limited in sensor and task diversity. SustainBench [72] targets sustainable development goals with 15 socioeconomic tasks, GEO-Bench [37] standardizes 12 classification/segmentation tasks with majority focusing on optical imagery, PhilEO Bench [22] consists of three Sentinel-2-derived tasks, and FoMo-Bench [10] focuses on forest monitoring with 15 multimodal datasets. Recent efforts like PANGAEA [43] address geographical bias and model generalizability by aggregating diverse datasets but remain surface-centric. Our Copernicus-Bench fills the gap with 15 hierarchical tasks spanning three application levels, covering all major Sentinel missions and encompassing both surface and atmosphere. 3. Copernicus-Pretrain We introduce unified dataset, Copernicus-Pretrain, containing aligned imagery organized in dense regional grids Table 1. Copernicus-Pretrain dataset statistics. Modality GSD Image size # Grid cells # Patches # Timestamps # Total images Sentinel-1 GRD Sentinel-2 TOA Sentinel-3 OLCI Sentinel-5P CO Sentinel-5P NO2 Sentinel-5P SO2 Sentinel-5P O3 Copernicus DEM Copernicus-Pretrain SAR MS MS atmos. atmos. atmos. atmos. elevation 2642642 10 10 26426413 969621 300 2828 1 km 2828 1 km 2828 1 km 2828 1 km 960960 30 247,723 247,723 281,375 306,097 291,449 262,259 306,218 297,665 1,067,267 1,067,267 281,375 306,097 291,449 262,259 306,218 297,665 312,567 3,879,597 4 4 8 112 112 112 112 1 4,227,387 4,218,065 2,189,561 2,104,735 1,752,558 1,366,452 2,556,631 297, 18,713,054 Figure 2. Schematic of the Copernicus-Pretrain dataset. is the number of local patches. Grid cells are upscaled for ease of visualization. from all major Sentinel missions in operation (Sentinel-1 SAR, Sentinel-2 multispectral reflectance, Sentinel-3 multispectral radiance, and Sentinel-5P atmospheric variables), as well as an elevation product Copernicus DEM GLO30 [21]. Copernicus-Pretrain significantly extends the scale in data size and modality range of existing EO foundation model research, and provides opportunities for bridging EO and weather/climate studies. 3.1. Data collection The Copernicus-Pretrain dataset is designed for consistency with the ERA5 [31] reanalysis dataset to provide convenient alignment between EO imagery and weather/climate data. The workflow of dataset curation is as follows. First, we divide the globe into 1M 0.25 0.25 grid cells following the coordinate mapping of ERA5. Each cell thus covers surface area of about 28 km 28 km, forming the basic sample unit of the dataset. With main interest and data availability in land and its atmosphere, we apply the land mask to filter land grids with 0.5 buffer around the coastline, resulting in about 393K grids. We then use Google Earth Engine [25] to download Sentinel images for each grid cell around the anchor year 2021. For Sentinel-3/5P and DEM, we cover the whole cell with patch sizes of about 96 96, 28 28, and 960 960 pixels for each corresponding modality. For Sentinel-3, we filter out cloudy tiles with bright pixel percentages above 20% and randomly download eight images from the year. For Sentinel-5P, we select four atmospheric variables: NO2, CO, SO2 and O3, filter out fully NaN tiles, and download 12 monthly mean images across the year for each variable. It is worth noting that for Sentinel-3 and 5P, it is possible to have fewer images available due to cloud and NaN filtering, thus the final sequence lengths can vary for different grids. For DEM, time series is not applicable, and we simply download one image for the grid. For Sentinel-1 and 2, the data volume is too large to cover the whole grid cell and the data can be extremely re3 dundant. Therefore, we collect images by sampling local patches within the cells. To optimize the diversity of the data, we do not directly conduct uniform sampling inside grid, but follow Gaussian sampling strategy [42, 54, 62] to sample locations around top-10K populated cities with standard deviation of 50 km. We utilize this strategy to sample 1M locations, each downloading 4 seasonal 264 264 image patches. We uniformly sample and download images for an additional 40K polar locations. Next, we group these local patches into the predefined 0.25 0.25 grid cells according to their center locations. For those remaining cells that do not contain any local patches, we newly sample and download 12 local patches to fill in the gaps. After the downloading process is finished, series of quality checks are conducted, removing images that are either corrupted or still dominated by NaN values. In the end, we get 310K grid cells with at least one sensor available, and 220K grids with all modalities available. Fig. 2 (right) illustrates an example cell containing all eight modalities. 3.2. Dataset characteristics Figure 3. Global distribution of the Copernicus-Pretrain dataset. In total, Copernicus-Pretrain consists of 18M images grouped into 310K 0.25 0.25 cells, densely covering the whole land surface and near-land ocean with eight distinct Sentinel modalities. On average, each cell contains 4 local S1/S2 time series pairs (16 images each), 8 S3 images, 7/6/5/8 S5P CO/NO2/SO2/O3 images, and 1 DEM image. Fig. 3 shows the global distribution of the full dataset, while the detailed statistics are shown in Tab. 1. The distribution and statistics of the all-modality-aligned subset and other detailed analyses can be found in the appendix. 4. Copernicus-FM Based on the Copernicus-Pretrain dataset, we introduce Copernicus-FM, new EO foundation model that 1) can process any spectral or non-spectral input modalities with varied spatial resolutions, and 2) when available, flexibly integrates metadata information like geolocation, geographical area, and time. Fig. 4 illustrates the general pretraining framework of Copernicus-FM, which uses unified hy4 pernetworks to dynamically patchify different modalities into patch tokens, integrates metadata as Fourier encodings added to the patch tokens, and performs masked image modeling (MIM) with auxiliary continual distillation as the training objectives. 4.1. Model architecture Given grid sample unit with all modalities, one image is sampled from the local patches (for S1/S2) and time series for each modality. This results in 8 images with varied spatial and channel sizes from distinct modalities, serving as the main input to the model. Dynamic patch embedding with spectral and variable hypernetworks We use single unified architecture to deal with different input modalities. This is implemented by extending the wavelength-conditioned dynamic patch embedding design from DOFA [69], where the core idea is to use hypernetwork to dynamically generate kernel weights for the 2D convolution patch embedding layer. As presented in Fig. 4, denoting one spectral image (e.g., from S1/2/3) as RCHW , each channel has corresponding central wavelength λ {λ1, ..., λC} and bandwidth δ {δ1, ..., δC} acquired from the sensors spectral response. The wavelengths and bandwidths serve as input to the spectral hypernetwork to generate patch embedding weights. First, we use Fourier encoding as introduced in Bodnar et al. [8] to encode wavelength or bandwidth value into D-dimensional vector, (cid:20) FE(x) = cos 2πx ωi , sin (cid:21) , 2πx ωi 0 < D/2, (1) where ωi are log-spaced values between the minimum and maximum: (cid:18) ωi = exp log ωmin + log ωmax log ωmin D/2 1 (cid:19) . (2) One specific Fourier encoding is designed for wavelengths and bandwidths respectively, where ωmin and ωmax are defined based on the corresponding value ranges. The resulting vectors Vλ RCD and Vδ RCD are then added together to form the spectral encodings Vspec RCD. Following DOFA [69], Vspec are further transformed through MLP and multi-head attention layers to get weight vectors Mw RCp2D and bias vectors Mb RCD, where is the expected patch size. The weight vectors are then reshaped into the convolution kernel Kconv RDCpp. Originally, fixed patch size is required for all modalities, which can be acceptable when the scales do not differ much. However, the significant resolution gaps between our input modalities will lead to exploded memory Figure 4. The general pretraining pipeline of Copernicus-FM. One image for each modality is sampled from common grid cell in Copernicus-Pretrain, which is then patchified with kernel weights generated by the spectral or variable hypernetwork, based on the modalitys spectral response or variable name. Further, Fourier-encoded metadata encodings are incorporated into the patch tokens. We conduct masked image modeling with auxiliary continual distillation for pretraining: masking and reconstructing masked-out patches for each modality, and distilling S1/2 or S2-derived RGB representations from powerful specialized teachers such as DINOv2 [48]. and compute costs. To tackle this issue, we adapt the idea of FlexiVit [7] to dynamically reshape the kernel weights into suitable patch size for each modality. Finally, the convolution operation for patch embedding is performed using the reshaped weights conv and biases Mb. The above spectral hypernetwork can process any modality with spectral response regardless of spatial and channel dimensions. However, it can not natively process non-spectral modalities such as atmospheric constituents from S5P and elevation maps from DEM. To bridge this gap, we introduce variable hypernetwork to generate weights for non-spectral modalities. Since these modalities do not have common meta-attribute like wavelength or bandwidth, we propose to directly encode their variable names using modern large language models (LLMs) with general scientific knowledge. Specifically, we use one frozen LLM encoder to encode the variable names of non-spectral inputs into D-dimensional vectors. This is done as preprocessing step through one-time inference offline, thereby introducing zero additional cost to the general model framework. Similar to the spectral hypernetwork, the language-guided variable encodings Vvar are further transformed through MLP and attention layers to get weight and bias vectors, and reshaped to modality-specific patch sizes for the convolution kernel of the patch embedding layer. we introduce additional metadata encodings in parallel to the positional encodings to integrate metadata information when available. Similar to wavelength and bandwidth encoding in the spectral hypernetwork, we again use Fourier encoding (Eqs. (1) and (2)) to unify different metadata with their corresponding value ranges. As shown in Fig. 5, we consider three common types of metadata for one input image: geolocation, spatial coverage, and time. For geolocation, the central coordinates of one image (longitude and latitude) are encoded into D/2dimensional vectors and then concatenated to the location encoding Loc RD. For spatial coverage, the patch area (in km2) is calculated from the ground sample distance (GSD) and patch size, and then encoded into the area encoding Area RD. For time, the temporal difference (in days) between the acquisition date of the image and reference date is calculated, and then encoded into the time encoding Time RD. These metadata encodings are further processed by corresponding MLP layer, expanded across the patch dimensions, and added together into the positional encodings. In practice, the metadata may not always be available. Therefore, we introduce learnable token for each metadata when it is unavailable. We simulate such metadata-missing scenarios by randomly dropping part of the metadata during each iteration. 4.2. Training objectives Metadata integration with unified Fourier encoding The dynamic patch embedding layer patchifies the input image into sequence of patch tokens. In the standard Transformer architecture, positional encodings RN (where is the number of patches) are added along with classification token to the patch tokens. In Copernicus-FM, Following DOFA [69], we combine MIM and continual distillation for the pretraining of Copernicus-FM. Specifically, MIM is conducted by MAE-style [29] masked reconstruction for each modalitythe patch tokens are randomly masked out and sent through the encoder and lightweight Transformer decoder to reconstruct the masked patches. Figure 5. Dynamic patch embedding (left) and metadata integration (right) of Copernicus-FM. Table 2. Ablation study of Copernicus-FM. OA: overall accuracy, mAP: mean average precision, and RMSE: root mean squared error. EuroSAT-S1 EuroSAT-S2 (OA ) EuroSAT-RGB LC100-S3 AQ-O3-S5P (mAP ) (RMSE ) Baseline [69] + dynamic patch size ... + bandwidth (Fourier encoding) ... + variable hypernetwork ... + metadata encoding ... + continual distillation 56.3 56.5 0.2 57.5 1.0 77.9 22.4 81.0 2.9 87.6 88.9 1.3 88.9 0.0 88.9 0.0 89.5 0.6 62.2 65.4 3.2 65.8 0.4 78.5 12.7 78.9 0.4 86.7 87.1 0.4 86.6 0.5 90.7 4.1 90.7 0.0 2218.0 1710.7 507.3 1598.1 112.6 839.3 758.8 811.6 27. dynamic patch predictor similar to the patch embedding layer in the encoder is used for prediction. Following He et al. [29], we conduct reconstruction directly on the flattened patch tokens. Thus, fully-connected layer is used at the end of the predictor instead of convolutional layer. Meanwhile, we conduct auxiliary continual distillation with small loss weight, using powerful single-modal or general-domain foundation models to serve as an anchor to guide and refine the latent space. For example, we distill S2-derived RGB representations from frozen DINOv2 [48] encoder with cosine-similarity loss. This helps improve the out-of-the-box representation quality of MIMbased pretraining, reduces compute cost with faster convergence, and also implicitly boosts the models general knowledge beyond our specific pretraining data sources. 4.3. Implementation details We pretrain Copernicus-FM with ViT-Base on the Copernicus-Pretrain dataset (220K grids with all modalities available) for 100 epochs. Data augmentations include simple resized cropping and horizontal flipping. Input image sizes are as shown in Fig. 4, with patch sizes 16 16 for S1/2, 8 8 for S3, 4 4 for S5P, and 64 64 for DEM. We use Llama 3.2 [20] to encode variable names. The Fourier encoding value ranges are [1e2, 1e9] for wavelengths, [1, 1e9] for bandwidths, [1e-4, 720] for longitudes/latitudes, [1e-3, 5.1e8] for patch area, and [1, 365.25] for time. The drop probability for metadata is 0.7. The masking ratio for MIM is 0.7. We distill RGB from DINOv2 [48], and S1/2 from SoftCon [64] with loss weights of 0.1 and 0.2, respectively. More details can be found in the appendix. 4.4. Ablation studies We conduct ablation studies on different components of Copernicus-FM with ViT-Small on 10K subset of Copernicus-Pretrain, and evaluate the pretrained encoders on range of downstream tasks covering different modalities. Specifically, we conduct k-NN classification on EuroSAT-SAR (S1) [65], EuroSAT-MS (S2) [30], and EuroSAT-RGB [30], linear probing on LC100Cls-S3 (multi-label), and dense regression with frozen encoder on AQ-O3-S5P (air pollutant regression of O3). The main ablation results are shown in Tab. 2, where consistent improvement can be observed when gradually adding the spectral hypernetwork bandwidth, variable hypernetwork, metadata encoding, and continual distillation. Among them, the benefits of metadata encoding appear to be the most significant, especially in non-optical modalities. This highlights the importance of metadata beyond pure imagery for remote sensing applications. Detailed ablation on each metadata (location matters most), ablation of the metadata dropping ratio (higher is better), as well as other minor ablations can be found in the appendix. 6 Table 3. Characteristics of datasets in Copernicus-Bench. seg, cls, cd, and reg represent segmentation, classification, change detection, and regression, respectively. *Time series support (default mode is 1 image for seg and 2 for cd). Geolocation metadata not available. Level Name Task # Images Image Size # Classes Source L1 L3 Cloud-S2 Cloud-S3 EuroSAT-S1 EuroSAT-S2 BigEarthNet-S1 BigEarthNet-S2 LC100Cls-S3 DFC2020-S1 DFC2020-S2 LC100Seg-S3 Flood-S1 LCZ-S2 Biomass-S3 AQ-NO2-S5P AQ-O3-S5P seg seg cls cls cls cls cls seg seg seg cd cls reg reg reg 1699/567/551 1197/399/399 16200/5400/5400 16200/5400/5400 11894/6117/5991 11894/6117/5991 5181/1727/1727* 3156/986/986 3156/986/986 5181/1727/1727* 3000/1000/1000* 15000/5000/5000 3000/1000/1000* 1480/493/494* 1480/493/494* 51251213 25625621 64642 646413 1201202 12012012 969621 2562562 25625613 969621 (288288) 2242242 323210 969621 (288288) 56561 56561 4 5 10 10 19 19 23 10 10 23 3 17 1 1 1 CloudSEN12+ [4] new EuroSAT-SAR [65] EuroSAT [30] License CC0-1.0 CC-BY-4.0 CC-BY-4.0 MIT BigEarthNet v2.0 [15] CDLA-Permissive-1.0 BigEarthNet v2.0 [15] CDLA-Permissive-1.0 new DFC2020 [28] DFC2020 [28] new Kuro Siwo [9] So2Sat LCZ42 [75] new new new CC-BY-4.0 CC-BY-4.0 CC-BY-4.0 CC-BY-4.0 MIT CC-BY-4.0 CC-BY-4.0 CC-BY-4.0 CC-BY-4.0 Table 4. Benchmark results with representative single-, dual-, and multi-modal foundation models on Copernicus-Bench. We report threerun averages with standard deviations. *: patch size 16 for S1/2 (8 for S3, 4 for S5P). Best scores in bold. Metric Supervised Supervised Random SoftCon [64] CROMA [24] DOFA [69] Copernicus-FM Backbone Modality Cloud-S2 Cloud-S3 EuroSAT-S1 EuroSAT-S2 BigEarthNet-S1 BigEarthNet-S2 LC100Cls-S3 DFC2020-S1 DFC2020-S2 LC100Seg-S3 Flood-S1 LCZ-S2 Biomass-S3 AQ-NO2-S5P AQ-O3-S5P ViT-S/16 ViT-B/16 ViT-B/16 mIoU mIoU OA OA mAP mAP mAP mIoU mIoU mIoU mIoU OA RMSE RMSE RMSE 64.2 0.9 61.7 0.7 81.7 0.7 97.5 0.0 78.1 0.6 83.6 0.4 91.3 0.3 49.9 0.4 65.3 0.6 20.1 0.4 78.0 0.1 86.6 0.7 68.1 0.3 3.4 0.0 1781.3 29. 59.4 1.0 63.0 0.8 81.5 0.9 97.6 0.1 81.2 0.5 86.4 0.4 91.4 0.5 50.8 0.5 66.2 0.7 19.3 0.5 78.3 0.3 85.3 0.8 68.3 0.4 3.4 0.0 1766.8 22.1 60.4 0.2 60.9 0.0 75.4 0.4 92.5 0.1 66.1 0.1 73.3 0.1 88.9 0.1 45.4 0.1 62.3 0.0 18.2 0.1 75.1 0.1 77.4 0.1 68.7 0.5 3.4 0.0 1741.6 11.5 ViT-B/14 S1/S2 66.9 0.3 83.6 0.1 96.7 0.0 81.6 0.0 86.1 0.0 52.8 0.6 64.1 0.3 77.2 0.1 83.6 0.2 ViT-B/8 S1+S2 65.0 0.2 83.9 0.1 97.0 0.1 72.8 0.0 78.8 0.0 52.7 0.1 66.5 0.0 77.4 0.1 84.1 0.0 ViT-B/16 All (spectral) 65.0 0.2 58.2 0.1 81.7 0.1 97.2 0.1 74.3 0.0 79.7 0.0 89.5 0.0 49.7 0.1 61.8 0.1 16.5 0.1 76.0 0.1 83.0 0.3 74.1 0.1 3.3 0.0 1755.6 19.8 ViT-B/16* All 66.7 0.1 62.0 0.7 87.2 0.1 97.9 0.1 83.3 0.0 84.6 0.0 93.3 0.4 52.4 0.1 64.5 0.1 24.1 0.0 77.7 0.0 84.4 0.0 66.3 0.1 2.8 0.0 789.4 2.6 5. Copernicus-Bench For thorough evaluation of Copernicus-FM, we curate benchmark suite, Copernicus-Bench, covering various Sentinel missions with hierarchical downstream tasks. 5.1. Datasets in Copernicus-Bench Copernicus-Bench consists of 15 datasets with varied Sentinel modalities organized into three application levels in practice: preprocessing, base applications, and specialized applications. Level-1 includes two cloud detection tasks from S2/3, level-2 includes 8 land use land cover classification/segmentation tasks from S1/2/3, and level-3 includes 5 specialized tasks from S1/2/3/5P. Among all the datasets, 9 are derived from existing datasets with permissive licenses, and 6 are newly curated to fill in the gap in ML-ready datasets for S3/S5P. The detailed characteristics are shown in Tab. 3. The curation process for each dataset and detailed benchmark analyses can be found in the appendix. 5.2. Benchmark results We benchmark supervised training and frozen-encoder evaluation of several representative single-, dual-, and multimodal foundation models on Copernicus-Bench. For clasTable 5. Linear regression results on 10-year mean and standard deviation (variability) of 6 climate parameters. We report three-run average RMSE scores with standard deviation (error bar). Best scores in bold. Input Source Coord. (raw) Coord. (FE) Embed. Embed. + coord. Coord. (SatCLIP [36]) Temperature (C) Precipitation (m) Surface press. (Pa) Sea-level press. (Pa) wind (m/s) wind (m/s) Avg Std Avg Std Avg Std Avg Std Avg Std Avg Std 2.09 0.42 3.06 0.50 141.38 290.68 7305.91 0.79 8.36 0.01 0.02 0.00 0.00 0.00 0.00 0.37 101.09 200.93 6537.54 0.61 3.99 0.18 0.09 0.00 0.00 0.00 0.00 1.78 1627.21 197.17 97.53 0.33 0.47 2.66 0.01 0.00 0.00 0.00 30.13 0.71 0.62 174.80 0.32 1.98 0.01 0.00 0.00 0.00 33.64 0.08 0.21 0.32 97.74 202.76 3178.91 0.48 3.67 0.19 0.27 0.03 0.00 0.00 0.00 8. 1676.39 78.53 1.72 1.58 1.35 0. 164.84 0.01 108.12 0.01 117.57 0.86 90.37 0.15 110.71 0.41 0.51 0.55 0.80 0.95 1.12 0.58 0.00 0.00 0.00 0.00 0.96 0.54 0.00 0.00 0.00 0.00 0.43 0.94 0.00 0.00 0.00 0.00 0.91 0.43 0.00 0.00 0.00 0.00 0.92 0.45 0.00 0.00 0.00 0. 0.43 0.42 0.79 0.78 0.43 0. sification tasks, we perform linear probing with batch size 64 and the SGD optimizer for 50 epochs; for segmentation and regression tasks, we train UPerNet [68] decoder on top of the frozen encoder with batch size 16 and the AdamW optimizer for 50 epochs; for change detection tasks, we separately encode the preand post-event images and calculate their feature map differences as input to UPerNet decoder, hyperparameters following the segmentation design. For supervised baselines, we train the model from scratch for 80 epochs. We conduct simple grid search for the learning rates and report test metrics under the best validation scores. All experiments are conducted on single GPU with three runs to get the mean and standard deviation. The results are shown in Tab. 4. It can be seen that our Copernicus-FM is comparable to or better than state-of-theart singleor multimodal foundation models on their applicable tasks, and largely improves downstream performances on S3 and S5P tasks. Encouragingly, our results outperform supervised training on the majority (11/15) of tasks, despite utilizing fewer trainable parameters and iteration steps. The results also verify the benefits of cross-modal pretraining on both surface and atmospheric applications. 6. Bridging EO & climate via grid embeddings While Copernicus-FM has demonstrated its benefits for various EO tasks, one of its most exciting potentials lies in bridging EO with weather and climate analysisan intersection that is still largely unexplored. Thanks to the gridbased dataset structure in Copernicus-Pretrain, we achieve direct alignment between EO imagery and climate parameters from ERA5[31], one of the most widely used reanalysis datasets for weather and climate research. This allows Copernicus-FM-encoded grid embeddings to serve as semantically rich geographical representations that could potentially enhance climate modeling. To evaluate this potential, we curate simple set of climate tasks predicting the 10-year mean and standard deviation of 6 important climate parameters (2 mean air temperature, annual total precipitation, surface pressure, mean sea-level pressure, and u/v component of 10-meter wind speed) using geocoordinates or geographical representations. We randomly sample 10K grids around the globe, split them into train/val/test subsets, and calculate the corresponding climate parameters from ERA5 as targets. As baseline, the central coordinates of the grids are sent through linear regression model for prediction. As shown in Tab. 5, the two baseline models with raw or Fourierencoded coordinates as input verify the basic correlation between geography and climate. We then use CopernicusFM-derived grid embeddings as input, obtained by simply averaging model-encoded images from various modalities. These embeddings provide richer, high-level representation of each grids environmental characteristics and consistently outperform coordinate-based baselines. Moreover, combining grid embeddings with coordinates yields the best overall performance, demonstrating the complementary nature of spatial context and EO-derived semantic features. Notably, even when compared against location encodings from specialized location encoder trained on EO images [36], the grid embeddings still outperform, emphasizing the unique value of EO representations for climate studies. Looking ahead, we envision incorporating these grid embeddings into ML-based training for medium-range weather forecasting, expanding the set of static variables with EO-derived visual representations, or the set of dynamic variables with representation time series. 7. Conclusion This work presents series of efforts towards nextgeneration EO foundation models, advancing existing approaches in data, model flexibility, and benchmarking. We introduce Copernicus-Pretrain, extending existing pretraining datasets to all major Copernicus Sentinel missions, encompassing both Earths surface and its atmosphere. We 8 propose Copernicus-FM, leveraging dynamic hypernetworks to process any spectral or non-spectral modality, enhancing adaptability across diverse EO sensors. We establish Copernicus-Bench, comprehensive benchmark with hierarchical downstream tasks across various Sentinel modalities. Furthermore, we demonstrate the benefits of EO grid embeddings in simple climate prediction task, highlighting their potential in bridging EO with weather and climate studies. This paves the way towards unified EO and climate foundation models, unlocking new possibilities for integrated Earth system understanding."
        },
        {
            "title": "References",
            "content": "[1] Peri Akiva, Matthew Purri, and Matthew Leotta. Selfsupervised material and texture representation learning for remote sensing tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82038215, 2022. 2 [2] Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. AnySat: An Earth observation model for arXiv preprint any resolutions, scales, and modalities. arXiv:2412.14123, 2024. 2 [3] Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. OmniSat: Self-supervised modality fusion for Earth observation. In European Conference on Computer Vision, pages 409427. Springer Nature Switzerland Cham, 2024. 2 [4] Cesar Aybar, Lesly Bautista, David Montero, Julio Contreras, Daryl Ayala, Fernando Prudencio, Jhomira Loja, Luis Ysuhuaylas, Fernando Herrera, Karen Gonzales, et al. CloudSEN12+: The largest dataset of expert-labeled pixels for cloud and cloud shadow detection in Sentinel-2. Data in Brief, 56:110852, 2024. 7, 21 [5] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, and Stefano Ermon. Geography-aware self-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1018110190, 2021. [6] Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. SatlasPretrain: largescale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1677216782, 2023. 1, 2, 13 [7] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One model for all patch sizes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1449614506, 2023. 5 [8] Cristian Bodnar, Wessel Bruinsma, Ana Lucic, Megan Stanley, Johannes Brandstetter, Patrick Garvan, Maik Riechert, Jonathan Weyn, Haiyu Dong, Anna Vaughan, et al. arXiv Aurora: foundation model of the atmosphere. preprint arXiv:2405.13063, 2024. 4, 17, 18 [9] Nikolaos Ioannis Bountos, Maria Sdraka, Angelos Zavras, Andreas Karavias, Ilektra Karasante, Themistocles Herekakis, Angeliki Thanasou, Dimitrios Michail, and Ioannis Papoutsis. Kuro Siwo: 33 billion m2 under the water. global multi-temporal satellite dataset for rapid flood mapIn Advances in Neural Information Processing Sysping. tems, pages 3810538121, 2024. 7, 21 [10] Nikolaos Ioannis Bountos, Arthur Ouaknine, Ioannis Papoutsis, and David Rolnick. FoMo-Bench: multi-modal, multi-scale and multi-task forest monitoring benchmark for remote sensing foundation models. 39th Annual AAAI Conference on Artificial Intelligence, 2025. 1, 2, [11] Jules Bourcier, Gohar Dashyan, Karteek Alahari, and Jocelyn Chanussot. Learning representations of satellite images from metadata supervision. In European Conference on Computer Vision, pages 5471. Springer, 2024. 2 [12] Nassim Ait Ali Braham, Conrad Albrecht, Julien Mairal, Jocelyn Chanussot, Yi Wang, and Xiao Xiang Zhu. SpectralEarth: Training hyperspectral foundation models at scale. arXiv preprint arXiv:2408.08447, 2024. 2, 13 [13] M. Buchhorn, B. Smets, L. Bertels, B. De Roo, M. Lesiv, N.- E. Tsendbazar, M. Herold, and S. Fritz. Copernicus global land service: Land cover 100m: collection 3: epoch 2019: Globe (version v3.0.1), 2020. 22 [14] Gordon Christie, Neil Fendley, James Wilson, and Ryan In Proceedings Mukherjee. Functional map of the world. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61726180, 2018. 2, 13 [15] Kai Norman Clasen, Leonard Hackel, Tom Burgert, Gencer Sumbul, Begum Demir, and Volker Markl. reBEN: Refined BigEarthNet dataset for remote sensing image analysis. arXiv preprint arXiv:2407.03653, 2024. 7, 21 [16] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David Lobell, and Stefano Ermon. SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery. Advances in Neural Information Processing Systems, 35:197211, 2022. [17] Craig Donlon, Berruti, Buongiorno, M-H Ferreira, Femenias, Frerick, Goryl, Klein, Laur, Mavrocordatos, et al. The global monitoring for environment and security (GMES) Sentinel-3 mission. Remote sensing of Environment, 120:3757, 2012. 1 [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 20 [19] Matthias Drusch, Umberto Del Bello, Sebastien Carlier, Olivier Colin, Veronica Fernandez, Ferran Gascon, Bianca Hoersch, Claudia Isola, Paolo Laberinti, Philippe Martimort, et al. Sentinel-2: ESAs optical high-resolution mission for GMES operational services. Remote sensing of Environment, 120:2536, 2012. 1 [20] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6 9 [21] European Space Agency. Copernicus global digital elevation model. Accessed: 2025-02-26, 2024. Distributed by OpenTopography. [22] Casper Fibaek, Luke Camilleri, Andreas Luyts, Nikolaos Dionelis, and Bertrand Le Saux. PhilEO Bench: Evaluating geo-spatial foundation models. In IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium, pages 27392744. IEEE, 2024. 2, 21 [23] Alistair Francis and Mikolaj Czerkawski. Major TOM: Expandable datasets for Earth observation. In IGARSS 20242024 IEEE International Geoscience and Remote Sensing Symposium, pages 29352940. IEEE, 2024. 2, 13 [24] Anthony Fuller, Koreen Millard, and James Green. CROMA: Remote sensing representations with contrastive radaroptical masked autoencoders. Advances in Neural Information Processing Systems, 36, 2024. 2, 7 [25] Noel Gorelick, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote sensing of Environment, 2017. 3 [26] Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, et al. SkySense: multi-modal remote sensing foundation model towards universal interpretation for Earth observation imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2767227683, 2024. 2 [27] Boran Han, Shuai Zhang, Xingjian Shi, and Markus Reichstein. Bridging remote sensors with multisensor geospatial foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2785227862, 2024. [28] Michael Schmitt; Lloyd Hughes; Pedram Ghamisi; Naoto Yokoya; Ronny Hansch. 2020 IEEE GRSS data fusion contest, 2019. 7, 21 [29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 5, 6, 20 [30] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. 6, 7, 21 [31] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andras Horanyi, Joaquın Munoz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The ERA5 global reanalysis. Quarterly journal of the royal meteorological society, 146(730):19992049, 2020. 3, 8 [32] Danfeng Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, Naoto Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, et al. SpectralGPT: Spectral remote sensing founIEEE Transactions on Pattern Analysis and dation model. Machine Intelligence, 2024. 2 [33] Jan Horalek, Leona Vlasakova, Marketa Schreiberova, Nina Beneˇsova, Philipp Schneider, Pavel Kurfurst, Frederic Tognet, Jana Schovankova, Ondˇrej Vlˇcek, Marta Garcia Vivanco, Mark Theobald, and Victoria Gil. ETC HE report 2023/3: Air quality maps of EEA member and cooperating countries for 2021. PM10, PM2.5, O3, NO2, NOx and BaP spatial estimates and their uncertainties. Technical report, European Environment Agency (EEA), 2024. Report provides air quality maps and exposure estimates for pollutants in EEA member and cooperating countries for 2021. 22 [34] Jeremy Irvin, Lucas Tao, Joanne Zhou, Yuntao Ma, Langston Nashold, Benjamin Liu, and Andrew Ng. USat: unified self-supervised encoder for multi-sensor satellite imagery. arXiv preprint arXiv:2312.02199, 1(2):3, 2023. 2 [35] Johannes Jakubik, Sujit Roy, CE Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, et al. Foundation models for generalist geospatial artificial intelligence. arXiv preprint arXiv:2310.18660, 2023. [36] Konstantin Klemmer, Esther Rolf, Caleb Robinson, Lester Mackey, and Marc Rußwurm. SatCLIP: Global, generalpurpose location embeddings with satellite imagery. arXiv preprint arXiv:2311.17179, 2023. 8 [37] Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan Sherwin, Hannah Kerner, Bjorn Lutjens, Jeremy Irvin, David Dao, Hamed Alemohammad, Alexandre Drouin, et al. GEOBench: Toward foundation models for Earth monitoring. Advances in Neural Information Processing Systems, 36: 5108051093, 2023. 1, 2, 21 [38] Weijie Li, Wei Yang, Tianpeng Liu, Yuenan Hou, Yuxuan Li, Zhen Liu, Yongxiang Liu, and Li Liu. Predicting gradient is better: Exploring self-supervised learning for SAR ATR with joint-embedding predictive architecture. ISPRS Journal of Photogrammetry and Remote Sensing, 218:326338, 2024. 2 [39] Zhihao Li, Biao Hou, Siteng Ma, Zitong Wu, Xianpeng Guo, Bo Ren, and Licheng Jiao. Masked angle-aware autoencoder for remote sensing images. In European Conference on Computer Vision, pages 260278. Springer, 2024. 2 [40] Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and Deren Li. On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and Million-AID. IEEE Journal of selected topics in applied earth observations and remote sensing, 14:42054230, 2021. 2 [41] Utkarsh Mall, Bharath Hariharan, and Kavita Bala. Changeaware sampling and contrastive learning for satellite images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52615270, 2023. [42] Oscar Manas, Alexandre Lacoste, Xavier Giro-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal Contrast: Unsupervised pre-training from uncurated remote sensing data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 94149423, 2021. 2, 4, 13 [43] Valerio Marsocci, Yuru Jia, Georges Le Bellier, David Kerekes, Liang Zeng, Sebastian Hafner, Sebastian Gerard, Eric Brune, Ritu Yadav, Ali Shibli, et al. PANGAEA: global and inclusive benchmark for geospatial foundation models. arXiv preprint arXiv:2412.04204, 2024. 1, 2 10 [44] Matıas Mendieta, Boran Han, Xingjian Shi, Yi Zhu, and Chen Chen. Towards geospatial foundation models via continual pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1680616816, 2023. 2 [45] Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, and Nico Lang. MMEarth: Exploring multi-modal pretext tasks for geospatial representation learning. In European Conference on Computer Vision, pages 164182. Springer, 2024. 2, 13 [46] Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, and Neil Houlsby. In-domain representation learning for remote sensing. arXiv preprint arXiv:1911.06721, 2019. 21 [47] Mubashir Noman, Muzammal Naseer, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Fahad Shahbaz Khan. Rethinking transformers pre-training for multispectral satellite imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2781127819, 2024. 2 [48] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5, 6, 17, [49] Jonathan Prexl and Michael Schmitt. SenPa-MAE: Sensor parameter aware masked autoencoder for multi-satellite selfsupervised pretraining. arXiv preprint arXiv:2408.11000, 2024. 2 [50] Colorado Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-MAE: scale-aware masked autoencoder for multiscale geospatial In Proceedings of the IEEE/CVF representation learning. International Conference on Computer Vision, pages 4088 4099, 2023. 2 [51] Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno Carvalhais, and Prabhat. Deep learning and process understanding for data-driven Earth system science. Nature, 566(7743):195204, 2019. 1 [52] M. Santoro and O. Cartus. ESA biomass climate change initiative (biomass cci): Global datasets of forest above-ground biomass for the years 2010, 2015, 2016, 2017, 2018, 2019, 2020 and 2021, v5.01, 2024. 22 [53] Michael Schmitt, Lloyd Haydn Hughes, Chunping Qiu, and Xiao Xiang Zhu. SEN12MSa curated dataset of georeferenced multi-spectral Sentinel-1/2 imagery for deep learning and data fusion. arXiv preprint arXiv:1906.07789, 2019. 2, 13 [54] Adam Stewart, Nils Lehmann, Isaac Corley, Yi Wang, YiChia Chang, Nassim Ait Ait Ali Braham, Shradha Sehgal, Caleb Robinson, and Arindam Banerjee. SSL4EOL: Datasets and foundation models for Landsat imagery. Advances in Neural Information Processing Systems, 36: 5978759807, 2023. 1, 2, 4, 13 [55] Adam J. Stewart, Caleb Robinson, Isaac A. Corley, Anthony Ortiz, Juan M. Lavista Ferres, and Arindam Banerjee. TorchIn Proceedings Geo: Deep learning with geospatial data. of the 30th International Conference on Advances in Geographic Information Systems, pages 112, Seattle, Washington, 2022. Association for Computing Machinery. 31 [56] Xian Sun, Peijin Wang, Wanxuan Lu, Zicong Zhu, Xiaonan Lu, Qibin He, Junxi Li, Xuee Rong, Zhujun Yang, Hao Chang, et al. RingMo: remote sensing foundation model with masked image modeling. IEEE Transactions on Geoscience and Remote Sensing, 61:122, 2022. 2 [57] Maofeng Tang, Andrei Cozma, Konstantinos Georgiou, and Hairong Qi. Cross-Scale MAE: tale of multiscale exploitation in remote sensing. Advances in Neural Information Processing Systems, 36:2005420066, 2023. 2 [58] Ramon Torres, Paul Snoeij, Dirk Geudtner, David Bibby, Malcolm Davidson, Evert Attema, Pierre Potin, Bj Orn Rommen, Nicolas Floury, Mike Brown, et al. GMES Sentinel-1 mission. Remote sensing of environment, 120:924, 2012. 1 [59] Pepijn Veefkind, Aben, McMullan, Forster, De Vries, Otter, Jacques Claas, HJ Eskes, JF De Haan, Kleipool, et al. TROPOMI on the ESA Sentinel-5 Precursor: GMES mission for global observations of the atmospheric composition for climate, air quality and ozone layer applications. Remote sensing of environment, 120:7083, 2012. 1 [60] Yi Wang, Conrad Albrecht, Nassim Ait Ali Braham, Lichao Mou, and Xiao Xiang Zhu. Self-supervised learning in remote sensing: review. IEEE Geoscience and Remote Sensing Magazine, 10(4):213247, 2022. 1 [61] Yi Wang, Conrad Albrecht, and Xiao Xiang Zhu. Selfsupervised vision transformers for joint SAR-optical repreIn IGARSS 2022-2022 IEEE Internasentation learning. tional Geoscience and Remote Sensing Symposium, pages 139142. IEEE, 2022. 2 [62] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad Albrecht, and Xiao Xiang Zhu. SSL4EOS12: large-scale multimodal, multitemporal dataset for IEEE Geoself-supervised learning in Earth observation. science and Remote Sensing Magazine, 11(3):98106, 2023. 1, 2, 4, 13, [63] Yi Wang, Conrad Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, and Xiao Xiang Zhu. Decoupling common and unique representations for multimodal self-supervised learning. In European Conference on Computer Vision, pages 286303. Springer, 2024. 2 [64] Yi Wang, Conrad Albrecht, and Xiao Xiang Zhu. Multilabel guided soft contrastive learning for efficient Earth observation pretraining. IEEE Transactions on Geoscience and Remote Sensing, 2024. 2, 6, 7, 20 [65] Yi Wang, Hugo Hernandez Hernandez, Conrad Albrecht, and Xiao Xiang Zhu. Feature guided masked autoencoder for self-supervised learning in remote sensing. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024. 2, 6, 7, 20, 21 [66] Jan Wevers, Dagmar Muller, Grit Kirches, Ralf Quast, and Carsten Brockmann. IdePix for Sentinel-3 OLCI algorithm theoretical basis document, 2022. 22 [67] Michael Wulder, David Roy, Volker Radeloff, Thomas Loveland, Martha Anderson, David Johnson, Sean Healey, Zhe Zhu, Theodore Scambos, Nima Pahlevan, et al. Fifty years of Landsat science and impacts. Remote Sensing of Environment, 280:113195, 2022. 1 [68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European conference on computer vision (ECCV), pages 418434, 2018. 8 [69] Zhitong Xiong, Yi Wang, Fahong Zhang, Adam Stewart, Joelle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, and Xiao Xiang Zhu. Neural plasticity-inspired foundation model for observing the Earth crossing modalities. arXiv e-prints, pages arXiv2403, 2024. 1, 2, 4, 5, 6, 7, 20 [70] Zhitong Xiong, Yi Wang, Fahong Zhang, and Xiao Xiang Zhu. One for all: Toward unified foundation models for Earth vision. arXiv preprint arXiv:2401.07527, 2024. 2 [71] Zhitong Xiong, Fahong Zhang, Yi Wang, Yilei Shi, and Xiao Xiang Zhu. EarthNets: Empowering artificial intelligence for Earth observation. IEEE Geoscience and Remote Sensing Magazine, 2024. 1 [72] Christopher Yeh, Chenlin Meng, Sherrie Wang, Anne Driscoll, Erik Rozi, Patrick Liu, Jihyeon Lee, Marshall Burke, David Lobell, and Stefano Ermon. SustainBench: Benchmarks for monitoring the sustainable dearXiv preprint velopment goals with machine learning. arXiv:2111.04724, 2021. 2, 21 [73] Lixian Zhang, Yi Zhao, Runmin Dong, Jinxiao Zhang, Shuai Yuan, Shilei Cao, Mengxuan Chen, Juepeng Zheng, Weijia Li, Wei Liu, et al. A2-MAE: spatial-temporalspectral unified remote sensing pre-training method based arXiv preprint on anchor-aware masked autoencoder. arXiv:2406.08079, 2024. [74] Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and Friedrich Fraundorfer. Deep learning in remote sensing: comprehensive review and list of resources. IEEE geoscience and remote sensing magazine, 5(4):836, 2017. 1 [75] Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou, Hossein Bagheri, Matthias Haberle, Yuansheng Hua, Rong Huang, et al. So2Sat LCZ42: benchmark data set for the classification of global local climate zones. IEEE Geoscience and Remote Sensing Magazine, 8(3):7689, 2020. 7, 21 [76] Xiao Xiang Zhu, Zhitong Xiong, Yi Wang, Adam Stewart, Konrad Heidler, Yuanyuan Wang, Zhenghang Yuan, Thomas Dujardin, Qingsong Xu, and Yilei Shi. On the foundations arXiv preprint of Earth and climate foundation models. arXiv:2405.04285, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Copernicus-Pretrain This section reports more detailed characteristics and statistical analyses for the Copernicus-Pretrain dataset. A.1. Comparison to existing EO pretraining datasets Tab. 6 shows detailed comparison between Copernicus-Pretrain and several existing EO pretraining datasets. Table 6. comparison of existing EO pretraining datasets."
        },
        {
            "title": "Dataset",
            "content": "fMoW [14] SEN12MS [53] SeCo [42] SSL4EO-S12 [62] SSL4EO-L [54] SatlasPretrain [6] MMEarth [45] SpectralEarth [12] Major TOM [23] Copernicus-Pretrain"
        },
        {
            "title": "Resolution",
            "content": "# Time stamps # patches # pixels RGB, MS SAR, MS MS SAR, MS MS SAR, MS, RGB SAR, MS, height, landcover, etc. HS SAR, MS SAR, MS, S3, DEM, S5P 0.310 10 10 10 30 0.510 1015 30 10 10 m1 km 3 1 5 4 4 10 1 123 1 1 2M 540K 1M 3M 5M >10M 6M 540K 8M 19M 50B 35B 70B 140B 348B 17T 120B 10B 6.8T 920B A.2. Extended statistics All-modality-aligned subset The Copernicus-Pretrain dataset contains 310K grids with at least one modality, of which 220K have all eight modalities. Tab. 7 shows the detailed characteristics of the 220K subset, and Fig. 6 presents its global distribution. We refer to the full dataset (grids with at least one modality) as union, and the all-modality-aligned subset (grids with all modalities) as joint. Table 7. Copernicus-Pretrain dataset characteristics (joint 220K subset). image size # grid cells # patches # timestamps # total images Sentinel-1 GRD Sentinel-2 TOA Sentinel-3 OLCI Sentinel-5P CO Sentinel-5P NO2 Sentinel-5P SO2 Sentinel-5P O3 Copernicus DEM 264x264 264x264 96x96 28x28 28x28 28x28 28x28 960x 219,543 219,543 219,543 219,543 219,543 219,543 219,543 219,543 996,978 996,978 219,543 219,543 219,543 219,543 219,543 219,543 Copernicus-Pretrain - 219,543 3,311, 4 4 8 112 112 112 112 1 - 3,948,217 3,948,217 1,720,881 1,548,349 1,394,800 1,188,864 1,750,542 219,543 15,720,353 Statistics of local patches Fig. 7 shows the histograms of the number of local patches across grids for S1/2 in the full datasets (union), and Fig. 8 shows the histograms for the joint subset. Statistics of time series. Fig. 9 presents the histograms of the time series lengths for S1 and S2 in the full dataset, while Fig. 10 shows the corresponding histograms in the joint subset. Similarly, Fig. 11 (left and right) presents S3 in the full dataset and joint subset, and Figs. 12 and 13 present S5P in the full dataset and joint subset. 13 Figure 6. Global distribution of the joint subset of the Copernicus-Pretrain dataset. Figure 7. Histogram of local patch numbers for S1 and S2 (union). Figure 8. Histogram of local patch numbers for S1 and S2 (joint). 14 Figure 9. Histogram of time series lengths for S1 and S2 (union). Figure 10. Histogram of time series lengths for S1 and S2 (joint). Figure 11. Histogram of time series lengths for S3 (left: union; right: joint). 15 Figure 12. Histogram of time series lengths for S5P (union). Figure 13. Histogram of time series lengths for S5P (joint). B. Copernicus-FM This section reports more implementation details, analyses, visualizations, and ablation studies for the Copernicus-FM foundation model. Unless explicitly noticed, for most ablation experiments, we pretrain ViT-Small on 10K-grid subset of Copernicus-Pretrain for 100 epochs with continual distillation only from DINOv2 [48] for efficiency. B.1. Spectral hypernetwork We use unified Fourier encoding [8] to encode wavelengths and bandwidths for all spectral channels, which are added together and serve as input to the spectral hypernetwork to generate patch embedding weights. Wavelength and bandwidth details Tab. 8 lists the detailed wavelength and bandwidth values for each spectral sensor in the Copernicus-Pretrain dataset used during our Copernicus-FM pretraining. Wavelengths (nm) Sensor S1 GRD 5e7, 5e7 S2 TOA S3 OLCI 440, 490, 560, 665, 705, 740, 783, 842, 860, 940, 1370, 1610, 2190 400, 412.5, 442.5, 490, 510, 560, 620, 665, 673.75, 681.25, 708.75, 753.75, 761.25, 764.375, 767.5, 778.75, 865, 885, 900, 940, 1020 Bandwidths (nm) 1e9, 1e9 20, 65, 35, 30, 15, 15, 20, 115, 20, 20, 30, 90, 180 15, 10, 10, 10, 10, 10, 10, 10, 7.5, 7.5, 10, 7.5, 7.5, 3.75, 2.5, 15, 20, 10, 10, 20, 40 Table 8. Wavelengths and bandwidths for different spectral sensors in Copernicus-FM pretraining. Fourier encoding visualization Fig. 14 illustrates the Fourier encoded wavelengths and bandwidths (with 128 feature dimensions) for 13 S2 bands and 1 S1 band. Figure 14. Fourier encoding visualization for wavelengths and bandwidths of S2 and S1. B.2. Variable hypernetwork We use large language model with general cross-domain knowledge to encode variable names for non-spectral modalities. The resulting variable encodings serve as input to the variable hypernetwork to generate patch embedding weights. Language encoding visualization Fig. 15 presents t-SNE plot of Llama-3.2-encoded variable names. We compare the variable names in our pretraining dataset with other out-of-domain concepts. The figure indicates that the language model does have meaningful knowledge of these different names S5P variables are gathered together, EO modalities are far away from other domains like games or mountains, and concepts within subdomain are further well clustered. Figure 15. t-SNE visualization of the language encodings of different variable names. B.3. Metadata integration We use unified Fourier encoding [8] to integrate metadata as encoding vectors added to the positional encodings. Fourier encoding visualization Figs. 16 and 17 illustrate the Fourier encoded metadata (location, area, and time) for few representative example values as below: location (lon + 180): 0, 45, 90, 135, 180, 225, 270, 315, 360, 360; location (lat + 90): 0, 45, 90, 135, 180, 0, 45, 90, 135, 180; area (in km2): 0.1, 1, 10, 100, 1000, 1e4, 1e5, 1e7, 1e8, 5.1e8; time (in days): 1, 7, 30, 90, 180, 365.25, 730.5, 1826.25, 3652.5. Figure 16. Fourier encoding visualization for geolocation (longitudes and latitudes). Ablation on metadata dropping ratio In practice, metadata is not always available as input. We thus randomly drop part of the metadata during pretraining, and use learnable metadata tokens to fill missing metadata encodings. To choose the best 18 Figure 17. Fourier encoding visualization for area (left) and time (right). metadata dropping probability, Tab. 9 conducts corresponding ablation study, where we perform k-NN evaluation on three image classification tasks. The table suggests that relatively high dropping ratio helps improve the models performance. Table 9. Ablation study on the dropping ratio of metadata. We report overall accuracy with k-NN evaluation. EuroSAT-S1 EuroSAT-S2 EuroSAT-RGB metadata (drop 0.1) metadata (drop 0.3) metadata (drop 0.5) metadata (drop 0.7) metadata (drop 0.9) 77.8 73.7 77.8 81.0 78.5 88.7 86.3 89.6 89.5 88.2 79.9 77.5 78.7 78.9 74.8 Ablation on metadata details Moving further, we wonder how much benefit each metadata component brings to the model, as well as how the format of each metadata will affect the performance. To answer these questions, Tab. 10 conducts additional ablation on specific metadata components. Results show that geolocation gives the most significant improvement, followed by area and time. Interestingly, geographic coordinates perform better than Cartesian coordinates despite their distortion in high-latitude regions. Using the area corresponding to the true surface coverage (e.g., after cropping and resizing the true surface coverage is smaller) is necessary, without which the performance begins to drop. Using the day of the year and absolute days above one-year-period perform similarly. We use the latter such that its convenient to extend to long time series in the future. Table 10. Ablation study on the benefits of each metadata type. We report overall accuracy with k-NN evaluation. Gray rows are alternative formatting options for the metadata. Performance increases/decreases are compared to the best formatting option of previous metadata. EuroSAT-S1 EuroSAT-S2 EuroSAT-RGB no metadata + location (x,y,z) /+ location (lon,lat) + area (raw) /+ area (aug) + time (dayofyear) /+ time (absolute) 56.9 75.8 18.9 78.2 21.3 77.8 0.4 80.3 2.2 80.0 0.3 81.0 0.7 88.3 88.7 0.5 88.7 0.4 88.1 0.6 89.3 0.6 89.5 0.2 89.5 0.2 70.1 73.3 2.8 76.5 6.5 73.7 2.8 77.4 0.8 78.9 1.5 78.9 1.5 19 B.4. Pretraining details Data We pretrain Copernicus-FM on the joint 220K-grid subset of Copernicus-Pretrain, with each grid being one sample unit containing aligned images from all eight modalities. For fast data loading, we convert the raw dataset into webdataset1 format, with one grid cell being one minimum sample in the shards. During training, one image from each modality is sampled from one grid cell to construct the input for each iteration. For S1/2, we normalize the image values with channelwise mean and standard deviation. For S3, we multiply each channel with its corresponding scale factor2. For S5P, we use the raw values, and replace NaN pixels with zero. For DEM, we divide the pixel values by 10000. We apply simple data augmentations to each modality, including random resized cropping with scale [0.2, 1.0] to its corresponding input size and random horizontal flipping. Each image comes with its metadata, including geolocation (central coordinates in lon/lat), patch area (calculated from GSD and patch size in km2), and time (number of days since reference date 1970-01-01). The geolocation and patch area are adapted dynamically based on the cropping parameters in data augmentation. Note that despite this adaptation, due to geographical projection the patch area doesnt strictly reflect the surface area, but is accurate enough for our pretraining purpose. While S1/2/3 images have exact acquisition dates, S5P images are monthly mean and DEM doesnt have specific acquisition date. Therefore, we use the first day of the month for one S5P image, and the first day of the year 2015 for all DEM images. Model We use standard vision Transformer [18] for the core backbonee.g., ViT-Base has 768 hidden dimensions, 12 Transformer blocks, and 12 attention heads. The MLP and attention architectures for the spectral and variable hypernetworks are identical to Xiong et al. [69]. For the light decoder to conduct masked image modeling (MIM) pretraining, we also follow Xiong et al. [69] and He et al. [29] with 512 hidden dimensions, 8 Transformer blocks, and 16 attention heads. For continual distillation, projector is used to project the output feature from the student to the frozen teacher model, both after global average pooling. Loss We conduct MIM and continual distillation for pretraining. For MIM, we generally follow He et al. [29] to reconstruct masked-out patches for each input modality. The masking ratio is 70% for all modalities following previous performance studies of MIM in EO [62, 65]. For distillation, we distill RGB channels of S2 from frozen DINOv2 [48] (ViT-Base with patch size 14) with loss weight 0.1, and full channels of S1 and S2 from frozen SoftCon [64] (ViT-Base with patch size 14) with loss weight 0.2. The former serves as an anchor to control the latent space with general vision knowledge, such that the model can be used on high-resolution or RGB data despite only being pretrained on medium to low resolution Sentinel images. The latter serves as an accelerator to make training converge faster, as well as offering global representation guidance complementary to the main MIM objective. Our preliminary experiments suggest the benefits of the latter S1/2 distillation decrease with longer training times and larger models. Training We pretrain Copernicus-FM on 220K Copernicus-Pretrain grids for 100 epochs. The effective batch size is 288. The basic learning rate is 1.5e-4 for batch size 256, and is linearly scaled for varied batch sizes. We warm up the learning rate for 10 epochs, and then apply cosine decay schedule. We use the AdamW optimizer, with weight decay of 0.05. One training run takes 512 GPU hours on NVIDIA A100 GPUs, or 128 node hours on one compute node with 4 A100 (40GB). 1https://github.com/webdataset/webdataset 2https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S3_OLCI C. Copernicus-Bench This section presents curation details, more characteristics, and additional visualizations for datasets within CopernicusBench, as well as implementation details for the benchmark. C.1. Comparison to existing EO benchmarks Tab. 11 shows detailed comparison between Copernicus-Bench and several existing EO benchmarks. Table 11. comparison of existing EO benchmarks. # tasks task types modalities resolution task range SustainBench [72] GEO-Bench [37] FoMo-Bench [10] PhilEO Bench [22] Copernicus-Bench (ours) 15 12 16 3 15 cls, seg, reg cls, seg cls, seg, obj seg, reg RGB, MS RGB, MS, HS, SAR RGB, MS, HS, SAR MS surface 0.630 0.115 surface 0.0160 surface surface 10 cls, seg, reg, cd MS, SAR, atmos. var. 101000 surface, atmosphere C.2. Benchmark curation Copernicus-Bench consists of 15 datasets organized into 3 levels of tasks covering all primary Copernicus Sentinel missions. Among them, nine are derived from existing datasets with permissive licenses, and six are newly curated to fill in the gaps of ML-ready datasets for S3/5P sensors. Sourced datasets Nine out of 15 datasets in Copernicus-Bench are extracted or adapted from existing datasets: Cloud-S2: This is multi-class cloud segmentation dataset derived from CloudSEN12+ [4], one of the largest Sentinel-2 cloud and cloud shadow detection datasets with expert-labeled pixels. We take 25% samples with high-quality labels, and split them into 1699/567/551 train/val/test subsets. EuroSAT-S1 and EuroSAT-S2: These two are multi-class land use/land cover classification datasets taken from EuroSAT [30] and EuroSAT-SAR [65]. We follow the train/val/test splits defined in Neumann et al. [46] with 16200/5400/5400 train/val/test images. Images of the two datasets are one-to-one paired, thus they can also be combined to serve as multimodal image classification dataset. These two datasets do not have time metadata. BigEarthNet-S1 and BigEarthNet-S2: These two datasets are sourced from BigEarthNet-v2 [15], large-scale S1/2 dataset for multilabel land use/land cover classification. We sample 5% subset (11894/6117/5991 images) from each of the official train/val/test splits, respectively. Images from the two datasets are again one-to-one paired, thus they can be combined to serve as multimodal multilabel image classification dataset. In addition, each S1/2 image pair has corresponding land cover map in 100 resolution, thus they can also be used as pixel-level segmentation datasets. DFC2020-S1 and DFC2020-S2: These two are land use/land cover segmentation datasets derived from the IEEE GRSS Data Fusion Contest 2020 (DFC2020) [28]. We take S1/2 images and 10 m-resolution labels from the original test set, and further split them into 3156/986/986 train/val/test subsets. Again, images from S1 and S2 datasets are one-to-one paired, thus they can be combined to serve as multimodal semantic segmentation dataset. These two datasets do not have geolocation and time metadata. Flood-S1: This is flood segmentation dataset extracted from large flood mapping dataset Kuro Siwo [9]. The original dataset is organized according to various flooding events around the globe. We take random subset of samples that contain at least the water class to construct 3000/1000/1000 train/val/test subsets. Each sample contains two preand one postevent S1 SAR image, forming time-series segmentation or change detection dataset. By default, we use one pre-event and one post-event image in Copernicus-Bench. LCZ-S2: This is multi-class scene classification dataset derived from So2Sat-LCZ42 [75], large-scale local climate zone classification dataset. We randomly select 25K samples from the training set of the cultural-10 version to construct new 15000/5000/5000 train/val/test subsets. The original data contains also S1 data, thus this dataset can also be extended to an S1 task and multimodal task. This dataset does not have geolocation and time metadata. 21 New datasets Six of 15 datasets in Copernicus-Bench are newly curated: Cloud-S3: This is cloud segmentation dataset with raw images from Sentinel-3 OLCI and labels from the IdePix [66] classification algorithm. We first download few large cloudy S3 tiles (about 4800 400 pixels) distributed across the globe, and then apply the IdePix algorithm using the ESA SNAP toolbox to get multi-class cloud masks. After that, we manually check the quality of the generated masks, filter out low-quality tiles, and get seven big tiles with high-quality cloud labels. Next, we remap the label IDs, georeference the tiles to GeoTIFFs, and use GDAL to crop the large tiles into small patches with size 256 256 pixels. We remove boundary patches filled with NaN pixels, and split all high-quality patches into 1197/399/399 train/val/test subsets. The class names for the cloud masks are: invalid, clear, cloud-sure, cloudambiguous, cloud-shadow, and snow-ice, of which invalid should be ignored during training. Apart from the multi-class labels, for each image we also have one binary cloud mask. Therefore, the Cloud-S3 dataset can serve as both multi-class and binary segmentation dataset. LC100Cls-S3 and LC100Seg-S3: These two datasets are based on Sentinel-3 OLCI images and CGLS-LC100 [13] land cover maps. CGLS-LC100 is product in the Copernicus Global Land Service (CGLS) portfolio and delivers global 23class land cover map at 100 spatial resolution. We pick the map product for 2019, and sample and download S3 images and LC100 labels for about 10K locations across the globe using GEE. For each location, we download land cover map with about 288 288 pixels, and four seasonal S3 OLCI images each with about 96 96 pixels (300 resolution). Despite using bright pixel percentage to simulate cloud filtering, the resulting images still contain large volume of clouds. To tackle this issue, we train cloud detection model based on the previously introduced Cloud-S3 dataset and filter out model-detected cloudy images. After final quality check, we get about 8K samples each with land cover map and time series of S3 images. We divide them into 5181/1727/1727 train/val/test subsets to construct the LC100Seg-S3 dataset. For LC100Cls-S3, we integrate multi-label annotations from the land cover maps for each sample, constructing multilabel classification dataset. Note that the number of S3 time stamps for different samples may differ because of the cloud filtering process. Apart from the time series, we also pre-select one image for each sample, constructing the static version of LC100Cls-S3 and LC100Seg-S3, which is the default mode in Copernicus-Bench. Biomass-S3: This regression dataset is based on Sentinel-3 OLCI images and CCI biomass [52]. The biomass product is part of the European Space Agencys Climate Change Initiative (CCI) program and delivers global forest above-ground biomass at 100 spatial resolution. We pick the product for 2020, and the layer of above ground biomass (AGB, unit: tons/ha, i.e. Mg/ha) as regression ground truth, which is defined as the mass, expressed as oven-dry weight of the woody parts (stem, bark, branches and twigs) of all living trees excluding stump and roots. We sample representative regions across the globe, and download corresponding S3 images (one for each season) from GEE and biomass maps from the CCI open data portal. We crop the S3 images into patches with about 96 96 pixels (300 resolution), and the corresponding biomass maps into patches with about 288 288 pixels. Similar to LC100Cls-S3 and LC100Seg-S3, the resulting S3 images contain large volume of clouds, thus we use again the cloud detection model to filter out cloudy images. After final quality check, we acquire 5K samples each with biomass map and time series of S3 images. We divide them into 3000/1000/1000 train/val/test subsets to construct the Biomass-S3 dataset. Note that the number of S3 time stamps for different samples may differ because of the cloud filtering process. Apart from the time series, we also pre-select one image for each sample, constructing the static version of Biomass-S3, which is also the default mode in Copernicus-Bench. AQ-NO2-S5P and AQ-O3-S5P: These two regression datasets are based on Sentinel-5P NO2 and O3 images and EEA air quality data products [33]. The European Environment Agency (EEA) air quality product provides values for the human health related indicators of air pollutants at 1 km2 grid covering the whole Europe, combining monitoring air quality data in regression-interpolation-merging-mapping methodology and the observational values of the air quality monitoring stations used in the interpolation. We pick the products in 2021 for NO2 (annual average concentration) and Ozone (O3, 93.2 percentile of maximum daily 8-hour means, SOMO35) as regression ground truth. We sample and download S5P NO2 (tropospheric NO2 column number density) and O3 (O3 column number density) images from GEE, and EEA NO2 and O3 maps from EEA datahub3. We use sample patch size of about 56 56 pixels for both S5P and EEA. For S5P, we download two versions: 1) annual mean, and 2) seasonal mean for each season. After filtering out NaN patches and final quality check, we get 1480/493/494 train/val/test samples for both NO2 and O3, each with an annual mode of 1 S5P image and seasonal mode of 4 S5P images. Annual is the default mode in Copernicus-Bench. C.3. Benchmark characteristics Example visualization Figs. 18 to 27 visualize some examples for each dataset in Copernicus-Bench. 3https://www.eea.europa.eu/en/datahub Figure 18. Copernicus-Bench-Cloud-S2. Figure 19. Copernicus-Bench-Cloud-S3. Left: multi-class mode. Right: binary mode. Figure 20. Copernicus-Bench-EuroSAT-S1 and Copernicus-Bench-EuroSAT-S2. Figure 21. Copernicus-Bench-BigEarth-S1 and Copernicus-Bench-BigEarth-S2. 23 Figure 22. Copernicus-Bench-LC100Cls-S3 and Copernicus-Bench-LC100Seg-S3. By default we pick one image per time series as static mode. Figure 23. Copernicus-Bench-DFC2020-S1 and Copernicus-Bench-DFC2020-S2. Figure 24. Copernicus-Bench-Flood-S1. Figure 25. Copernicus-Bench-LCZ-S2. Figure 26. Copernicus-Bench-Biomass-S3. By default we pick one image per time series as static mode. 24 Figure 27. Copernicus-Bench-AQ-NO2-S5P and Copernicus-Bench-AQ-O3-S5P. By default we pick the annual mode. Geographical distribution Fig. 28 illustrates the geographical distribution of datasets in Copernicus-Bench. Note that DFC2020-S1, DFC2020-S2, and LCZ-S2 do not have geolocation metadata. Metadata information Tab. 12 lists the metadata information of the datasets in Copernicus-Bench. Table 12. Copernicus-Bench metadata availability. Level Name Task Sensor L1 L2 L3 Cloud-S2 Cloud-S3 EuroSAT-S1 EuroSAT-S2 BigEarthNet-S1 BigEarthNet-S2 LC100Cls-S3 DFC2020-S1 DFC2020-S2 LC100Seg-S3 Flood-S1 LCZ-S2 Biomass-S3 AQ-NO2-S5P AQ-O3-S5P seg seg cls cls cls cls cls seg seg seg cd cls reg reg reg S2 TOA S3 OLCI S1 GRD S2 TOA S1 GRD S2 SR S3 OLCI S1 GRD S2 TOA S3 OLCI S1 GRD S2 TOA S3 OLCI S5P NO2 S5P Bands All 13 bands All 21 bands VV, VH All 13 bands VV, VH 12 bands (no B10) All 21 bands VV, VH All 13 bands All 21 bands VV, VH 10 bands (no B1, B9, B10) All 21 bands tropospheric NO2 column number density O3 column number density Location Time Area C.4. Benchmark implementation We run all benchmark experiments on single GPU, repeating three runs with different random seeds. We first benchmark two supervised baselines with ViT-S/16 and ViT-B/16, and then conduct frozen-encoder transfer learning for set of pretrained models. For classification tasks, linear layer is appended on top of the encoder; for segmentation and regression tasks, UPerNet decoder with an auxiliary FCN decoder is appended on top of the encoder; for the flood segmentation task, we follow the segmentation design except that both preand post-event images are sent through the encoder and the difference features are sent to the decoder. We use simplified data augmentations for training sets: horizontal and vertical flipping for classification tasks, and 90rotation, horizontal and vertical flipping for segmentation and regression tasks. No augmentation is used for validation and testing sets. Data normalization is performed on the input according to the pretrained models preference. For most cases, normalization is performed by subtracting the channel-wise mean and dividing by standard deviation based on the pretrainedmodel-preferred statistics. If there is no preference, we recommend using the statistics calculated from the training set of Figure 28. Geographical distribution of datasets in Copernicus-Bench. each dataset as standard. For regression tasks, we do mean/std (of the training set) normalization also on the targets to stabilize training. The predicted output is later converted back to the original scale to compute evaluation metrics. We run each experiment for 50 epochs, and report the test set metrics based on the best validation scores. For classification tasks, we use batch size of 64, the SGD optimizer, and cross entropy loss or multilabel soft margin loss for single-label or multi-label cases. For segmentation tasks, we use batch size of 16, the AdamW optimizer, and cross entropy loss. For regression tasks, we use batch size of 16, the AdamW optimizer, and L1 loss. Specially for air quality regression tasks (NO2 and O3), the targets may contain NaN pixels, thus we customize masked L1 loss where the NaN pixels do not contribute to the loss calculation. For each model and dataset, we look for the best learning rate with simple grid search from the pool [1e-4,1e-3,1e-2] (for AdamW) and [1e-2,1e-1, 1, 10] (for SGD). In most cases, the best learning rate is consistent across models but slightly varies across datasets. 26 D. Bridging EO and climate with grid embeddings D.1. Climate prediction visualization Figs. 29 and 31 visualize the prediction results on 10-year mean/std of the six climate parameters, comparing using the geocoordinates or combination of coordinates and Copernicus-FM-generated grid embeddings as input data. Figs. 30 and 32 further plot the prediction error (Target-Prediction) of using only coordinates, coordinates and embeddings, and only embeddings. The figures show that using raw coordinates captures the general distribution of the climate parameters but tends to be over-smooth, while introducing EO-generated grid embeddings can capture finer details and extremes. Figure 29. Visualization of climate prediction (10-year mean) results comparing different input sources. 27 Figure 30. Visualization of climate prediction (10-year mean) errors comparing different input sources. 28 Figure 31. Visualization of climate prediction (10-year std) results comparing different input sources. 29 Figure 32. Visualization of climate prediction (10-year std) errors comparing different input sources. 30 E. License All codes, datasets, and model weights will be publicly released under permissive licenses. All codes will be released on GitHub under the Apache-2.0 license, including the curation codes of Copernicus-Pretrain and Copernicus-Bench, the pretraining codes of Copernicus-FM, and the benchmarking codes for Copernicus-Bench. The Copernicus-Pretrain dataset, the newly-curated datasets in Copernicus-Bench, and the pretrained weights of Copernicus-FM will be released under the CC-BY-4.0 license, copy of which will be hosted on public platforms like Hugging Face. We will also contribute our dataset, model, and benchmark to popular open-source libraries such as TorchGeo [55]."
        }
    ],
    "affiliations": [
        "Harokopio University of Athens",
        "Munich Center for Machine Learning",
        "NVIDIA",
        "National Technical University of Athens & National Observatory of Athens",
        "Technical University of Munich"
    ]
}