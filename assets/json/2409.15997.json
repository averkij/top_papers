{
    "paper_title": "Improvements to SDXL in NovelAI Diffusion V3",
    "authors": [
        "Juan Ossa",
        "Eren Doğan",
        "Alex Birch",
        "F. Johnson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this technical report, we document the changes we made to SDXL in the process of training NovelAI Diffusion V3, our state of the art anime image generation model."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 2 7 9 9 5 1 . 9 0 4 2 : r Improvements to SDXL in NovelAI Diffusion Juan Ossa Anlatan LLC juan@anlatan.ai Eren Dogan Anlatan LLC eren@anlatan.ai Alex Birch Anlatan LLC F. Johnson Anlatan LLC"
        },
        {
            "title": "Abstract",
            "content": "In this technical report, we document the changes we made to SDXL in the process of training NovelAI Diffusion V3, our state of the art anime image generation model."
        },
        {
            "title": "Introduction",
            "content": "Diffusion based image generation models have been soaring in popularity recently, with variety of different model architectures being explored. One such model, Stable Diffusion, has achieved high popularity after being released as Open Source. Following up on it, Stability AI released SDXL, larger and extended version following its general architecture [24]. We chose SDXL as the basis for our latest image model, NovelAI Diffusion V3, and made several enhancements to its training practices. This technical report is structured as follows. In Section 2, we describe our enhancements in detail. Following that, we evaluate our contributions in Section 5. Finally, we draw conclusions in Section 6."
        },
        {
            "title": "2 Enhancements",
            "content": "In this section, we present the enhancements we applied to SDXL to improve generation results. 2.1 v-Prediction Parameterization We uptrained SDXL from ϵ-prediction to v-prediction[27] parameterization. This was instrumental to our goal of supporting Zero Terminal SNR (see section 2.2). The ϵ-prediction objective (\"wheres the noise?\") is trivial at SNR=0 (\"everything is noise\"), hence ϵ loss fails to teach the model how to predict an image from pure noise. By comparison, v-prediction transitions from ϵ-prediction to x0-prediction as appropriate, ensuring that neither high nor low SNR timesteps are trivially predicted. As secondary benefit, we sought to access the merits described in [9]: improved numerical stability, elimination of colour-shifting[31] at high resolutions, and faster convergence of sample quality. 2.2 Zero Terminal SNR Stable-diffusion[26] and SDXL were trained with flawed noise schedule[20], limiting imagegeneration to always produce samples with medium brightness. We aimed to remedy this. Diffusion models[10] such as these learn to reverse an information-destroying process (typically[2] the application of Gaussian noise). principled implementation of diffusion should employ noise schedule that spans from pure-signal to pure-noise. Unfortunately SDXLs noise schedule stops short of pure-noise  (fig. 1)  . This teaches the model bad lesson: \"there is always some signal in the noise\". This assumption is harmful at inference-time, where our starting point is pure-noise, and results in the creation of non-prompt-relevant features  (fig. 2)  . Technical report. σ = 0 σ = 0.447 σ = 3. σ = 14.6 Figure 1: Noise is added to sample, until the final training timestep σmax, where Gaussian noise with standard deviation 14.6 is added. This amount of noise does not sufficiently destroy the signal in the image; the lowest frequencies (in particular its average colour) remain discernable. ZTSNR no ZTSNR Figure 2: Prompting the model to generate \"completely black\". model trained to predict images from infinite-noise (ZTSNR) can comply with the prompt. Whereas if we begin inference from timestep with finite noise, the model outputs an image with medium brightness, trying to match the mean colour it sees in the starting noise, and consequently generates non-prompt-relevant sample. We trained NAIv3 on noise schedule with Zero Terminal SNR, to expose SDXL to pure-noise during training. We train the model up to noise levels so high that it can no longer rely on mean-leakage, and learns to predict relevant mean colours and low frequencies from the text condition  (fig. 3)  instead. The introduction of ramp up to infinite-noise aligns the training schedule with the inference schedule. 2 ZTSNR no ZTSNR Figure 3: Absence of ZTSNR step introduces spurious high-contrast coarse features, attempting to pull the canvass mean (latent) colour back to 0, the average value of the Gaussian noise provided at the start of inference. Concretely, this can mean an opposing colour is added to the background, or hair colour and clothing details can disobey the prompt. σ 1 σ = 56. σ = 25.9 σ = 16.0 σ = 11.1 ZTSNR no ZTSNR (no σ = step) Figure 4: Intermediate denoising predictions with and without ZTSNR. The ZTSNR regime proposes relevant average canvas colour at σ 1, whereas the non-ZTSNR regime understands that in signal noised up only to σ = 56, the mean colour should still be discernable, and incorrectly concludes that the average colour should be 0, adding white to the canvas in order to achieve this. 3 ZTSNR poses practicality issues when implemented in the EDM[16] framework (i.e. via the kdiffusion[6] library). We explain in appendix how to overcome such issues. 2.3 Sampling from High-Noise Timesteps Improves High-Resolution Generation Curving the schedule to converge on σ 1 confers ramp of higher sigmas along the way  (fig. 5)  . This helps to resolve another problem with SDXL: its σmax is not high enough to destroy the low frequencies of signal in high-resolution images. In fact σmax was not increased since SD1[26], despite the increase in target resolution. As resolution increases (or rather as the amount of redundant signal increases increasing latent channels could be another way to increase redundancy): more noise is required to achieve comparable SNR (i.e. destroy an equivalent proportion of signal)[12]. Figure 5: Schedule with/without ZTSNR. Large features, such as close-to-camera limbs can lose coherence in SDXLs default regime of σmax = 14.6. Doubling σmax (for example) recovers coherence  (fig. 6)  . In practice we hit higher sigmas than this in our standard 28-step native schedule, which is uniform linear spacing of timesteps over the 1000-step ZTSNR schedule, the highest of which is approx. σ 1. 1For the σ = step, in practice we use an approximation σ = 20000 . See appendix for the rationale behind this practical approximation, and more principled approach. 4 σmax = 14. σmax = 29.0 Figure 6: Effect of raising σmax. SDXLs default σmax = 14.6 is insufficient for global coherence on high-resolution images, resulting in multi-body artifacting. Doubling to σmax = 29.0 resolves this artifacting. In practice we raise σmax even higher than this, to σmax 1, to achieve relevant mean colour too. σmax = 14.6 σmax = 29.0 σ = 14.6 σ = 10. σ = 8.3 σ = 6.6 σ = 5.4 σ = 29.0 σ = 17.8 σ = 12. σ = 9.2 σ = 7.2 Figure 7: Intermediate denoising predictions, on different σmax regimes. rule of thumb is that if you double the canvas length (quadrupling the canvas area): you should double σmax (quadrupling the noise variance) to maintain SNR. This is an upper bound which assumes that the extra signal is fully redundant. The approximation is better for high-resolution images, and sufficient for our purposes. More recently, non-commercial, research-licensed version of SDXL (CosXL)[23] has been released with v-prediction and ZTSNR support. 2.4 MinSNR We used MinSNR[7] loss-weighting to treat diffusion as multi-task learning problem, balancing the learning of each timestep according to difficulty, and avoiding focusing too much training on low-noise timesteps."
        },
        {
            "title": "3 Dataset",
            "content": "Our dataset consisted of approximately 6 million images gathered from crowd-sourced platforms. It was enriched with highly detailed, tag-based labels. The images are mostly illustrations in styles commonly found in Japanese animation, games and pop-culture."
        },
        {
            "title": "4 Training",
            "content": "We trained the model on our 256x H100 cluster for many epochs and roughly 75k H100 hours. We also used staged approach, with later stages consisting of more curated, higher quality data. We trained in float32, with tf32[19] optimization. Our total compute budget was above that of the original SDXL training run, allowing us to thoroughly adapt the model to our datas distribution. Adaptation to the changes described in Section 2 was quite fast. While the initial step, initialized from the original SDXL weights, showed only noise, our first set of samples, output after about 30 minutes (wall time) of training, was already coherent. 4.1 Aspect-Ratio Bucketing As in previous NovelAI Diffusion models[1], we prepared like-aspect minibatches via aspect-ratio bucketing. This enabled us to frame images better than in center-crop regime, and achieve better token-efficiency than in padding regime. In sections 4.1.1 to 4.1.3, we will reiterate our approach here for the sake of completeness. 4.1.1 Reasoning Existing image generation models[26] are very prone to producing images with unnatural crops. This is due to training practicality: uniform batches are simple to implement. Often practitioners opt to train on square data, taking crops from the center of the image. This is not conducive to modeling typical image data distributions, as most photos and artworks are not square. As consequence, humans are often generated without feet or heads, and swords consist of only blade with hilt and point outside the frame. As we are creating an image generation model to accompany our storytelling experience, it is important for our model to be able to produce proper, uncropped characters, and generated knights should not be holding metallic-looking straight line extending to infinity. Another issue with training on cropped images is that it can lead to mismatch between the text and the image. For example, an image with \"crown\" tag will often no longer contain crown after center crop is applied, leaving the monarch thereby decapitated  (fig. 8)  . 6 Figure 8: Center-cropping (demonstrated here with dark letterboxing) can remove details crucial for prompt-relevance; in this case \"crown\" has been excluded from the image. We found that using random crops instead of center crops only slightly improves these issues. Using Stable Diffusion with variable image sizes is possible, although it can be noticed that going too far beyond the native resolution of 512 512 tends to introduce repeated image elements, and very low resolutions produce indiscernible images. This is likely to be due to its lack of an explicit position embedding, resulting in model which determines position from side-channels like convolution padding[14, 17]. Some training-free approaches seek to generalize inference to resolutions outside of the training distribution via techniques such as convolution dilation [13, 8, 30] and attention entropy scaling [15, 4]. We chose instead to train on varied image sizes. Training on single, variable sized samples would be trivial, but also extremely slow and more liable to training instability due to the lack of regularization provided by the use of mini batches. 4.1.2 Custom Batch Generation As no existing solution for this problem seems to exist, we have implemented custom batch generation code for our dataset that allows the creation of batches where every item in the batch has the same size, but the image size of batches may differ. We do this through method we call aspect ratio bucketing. An alternative approach would be to use fixed image size, scale each image to fit within this fixed size and apply padding that is masked out during training. Since this leads to unnecessary computation during training, we have not chosen to follow this alternative approach. In the following, we describe the original idea behind our custom batch generation scheme for aspect ratio bucketing. First, we have to define which buckets we want to sort the images of our dataset into. For this purpose, we define maximum image size of 512 768 with maximum dimension size of 1024. Since the maximum image size is 512 768, which is larger than 512 512 and requires more VRAM, per-GPU batch size has to be lowered, which can be compensated through gradient accumulation. 7 We generate buckets by applying the following algorithm: Set the width to 256. While the width is less than or equal to 1024: Find the largest height such that height is less than or equal to 1024 and that width multiplied by height is less than or equal to 512 768. Add the resolution given by height and width as bucket. Increase the width by 64. The same is repeated with width and height exchanged. Duplicated buckets are pruned from the list, and an additional bucket sized 512 512 is added. Next, we assign images to their corresponding buckets. For this purpose, we first store the bucket resolutions in NumPy array and calculate the aspect ratio of each resolution. For each image in the dataset, we then retrieve its resolution and calculate the aspect ratio. We compute the logarithms of each aspect ratio, in order to compare in log-space2. The image angle is subtracted from the array of bucket angles, allowing us to efficiently select the closest bucket according to the absolute value of the difference between aspect ratios. image_bucket = argmin(abs(log(bucket_aspects) log(image_aspect)))) (1) The images bucket number is stored associated with its item ID in the dataset. If the images aspect ratio is very extreme and too different from even the best-fitting bucket, the image is pruned from the dataset. Since we train on multiple GPUs, before each epoch, we shard the dataset to ensure that each GPU works on distinct subset of equal size. To do this, we first copy the list of item IDs in the dataset and shuffle them. If this copied list is not divisible by the number of GPUs multiplied by the batch size, the list is trimmed, and the last items are dropped to make it divisible. We then select distinct subset of world_sizebsz item IDs according to the global rank of the current process. The rest of the custom batch generation will be described as seen from single shard of these processes and operate on the subset of dataset item IDs. 1 For the current shard, lists for each bucket are created by iterating over the list of shuffled dataset item IDs and assigning each ID to the list corresponding to the bucket that best fits that images aspect ratio. Once all images are processed, we iterate over the lists for each bucket. If its length is not divisible by the batch size, we remove the last elements on the list as necessary to make it divisible and add them to separate catch-all bucket. As the overall shard size is guaranteed to contain number of elements divisible by the batch size, this process is guaranteed to produce catch-all bucket with length divisible by the batch size as well. When batch is requested, we randomly draw bucket from weighted distribution. The bucket weights are set as the size of the bucket divided by the size of all remaining buckets. This ensures that even with buckets of widely varying sizes, the custom batch generation does not introduce bias during training. If buckets were chosen without weighting, small buckets would empty out early during the training process, and only the biggest buckets would remain towards the end of training. batch of items is finally taken from the chosen bucket. The items taken are removed from the bucket. If the bucket is now empty, it is deleted for the rest of the epoch. The chosen item IDs and the chosen buckets resolution are now passed to an image-loading function. 4.1.3 Image Loading Each item IDs image is loaded and processed to fit within the bucket resolution. For fitting the image, two approaches are possible. 2aspect ratios compared in log-space have the desirable property that 1:1 aspect ratio can be equidistant from buckets 2:1 and 1:2, each of which fit an equal proportion of its area. First, the image could be simply rescaled. This would lead to slight distortion of the image. For this reason, we have opted for the second approach: The image is scaled, while preserving its aspect ratio, in such way that it: Either fits the bucket resolution exactly if the aspect ratio happens to match, or it extends past the bucket resolution on one dimension while fitting it exactly on the other. In the latter case, random crop is applied. As we found that the mean aspect ratio error per image is only 0.033, these random crops only remove very little of the actual image, usually less than 32 pixels. The loaded and processed images are finally returned. 4.2 Conditioning Like in our previous models, we used CLIP[25] context concatenation and conditioned on CLIPs penultimate hidden states (on this part, no change was required; SDXL base already does this). To produce SDXLs pooled CLIP condition given multiple concatenated CLIP contexts: we take mean average over all CLIP segments pooled states. 4.3 Tag-based Loss Weighting During training, we employed tag-based loss weighting scheme, which keeps track of how often tags in certain tag type classes occur. Images with tags that are overly common within their class have their loss downweighted, while images with tags that are rare within their class may have their loss up-weighted. This allows our model to better learn concepts from rare tags while reducing the influence of over-represented concepts. 4.4 VAE Decoder Finetuning As in NovelAI Diffusion V1, we finetune the Stable-Diffusion (this time SDXL) VAE decoder, which decodes the low-resolution latent output of the diffusion model, into high-resolution RGB images. The original rationale (in V1 era) was to specialize the decoder for producing anime textures, especially eyes. For V3, an additional rationale emerged: to dissuade the decoder from outputting spurious JPEG artifacts, which were being exhibited despite not being present in our input images."
        },
        {
            "title": "5 Results",
            "content": "We find empirically that our model produces relevant, coherent images at CFG[11] scales between 3.55. This is lower than the default of 7.5 recommended typically for SDXL inference, and suggests that our dataset is better-labelled."
        },
        {
            "title": "6 Conclusions",
            "content": "NovelAI Diffusion V3 is our most successful image generation model yet, generating 4.8M images per day. From this strong base model we have been able to uptrain suite of further products, such as Furry Diffusion V3, Director Tools, and Inpainting models."
        },
        {
            "title": "7 Acknowledgements",
            "content": "Thanks to Stefan Baumann for the ZTSNR derivations and for reviewing this report. Thanks also to Tanishq Abraham for review. Additionally, we would like to thank Stefan Baumann and Katherine Crowson for explaining practical approaches for implementing ZTSNR in k-diffusion[6], and for explaining heuristics to scale σmax with image resolution."
        },
        {
            "title": "References",
            "content": "[1] Anlatan, Novelai improvements on stable diffusion, 2022. [Online]. Available: https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac [2] A. Bansal, E. Borgnia, H.-M. Chu, J. S. Li, H. Kazemi, F. Huang, M. Goldblum, J. Geiping, and T. Goldstein, Cold diffusion: Inverting arbitrary image transforms without noise, 2022. [Online]. Available: https://arxiv.org/abs/2208. [3] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, W. Manassra, P. Dhariwal, C. Chu, Y. Jiao, and A. Ramesh, Improving Image Generation with Better Captions, OpenAI, Tech. Rep., 2023. [4] D. Chiang and P. Cholak, Overcoming theoretical limitation of self-attention, 2022. [Online]. Available: https://arxiv.org/abs/2202.12172 [5] S. Chintala, Pytorch discussion: Normalization in the mnist example. [Online]. Available: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/7 [6] K. Crowson, R. Beaumont, T. Abraham, J. Whitaker, and storyicon, crowsonkb/k-diffusion: v0.1.1.post1, Dec. 2023. [Online]. Available: https://doi.org/10.5281/zenodo. [7] T. Hang, S. Gu, C. Li, J. Bao, D. Chen, H. Hu, X. Geng, and B. Guo, Efficient diffusion training via min-snr weighting strategy, 2024. [Online]. Available: https://arxiv.org/abs/2303.09556 [8] Y. He, S. Yang, H. Chen, X. Cun, M. Xia, Y. Zhang, X. Wang, R. He, Q. Chen, and Y. Shan, Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models, 2023. [Online]. Available: https://arxiv.org/abs/2310.07702 [9] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, and T. Salimans, Imagen video: High definition video generation with diffusion models, 2022. [Online]. Available: https://arxiv.org/abs/2210.02303 [10] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, 2020. [Online]. Available: https://arxiv.org/abs/2006.11239 [11] J. Ho and T. Salimans, Classifier-free diffusion guidance, 2022. [Online]. Available: https://arxiv.org/abs/2207.12598 [12] E. Hoogeboom, J. Heek, and T. Salimans, Simple diffusion: End-to-end diffusion for high resolution images, 2023. [Online]. Available: https://arxiv.org/abs/2301.11093 [13] L. Huang, R. Fang, A. Zhang, G. Song, S. Liu, Y. Liu, and H. Li, Fouriscale: frequency perspective on training-free high-resolution image synthesis, 2024. [Online]. Available: https://arxiv.org/abs/2403.12963 [14] M. A. Islam, M. Kowal, S. Jia, K. G. Derpanis, and N. D. B. Bruce, Position, padding and predictions: deeper look at position information in cnns, 2021. [Online]. Available: https://arxiv.org/abs/2101.12322 [15] Z. Jin, X. Shen, B. Li, and X. Xue, Training-free diffusion model adaptation for variable-sized text-to-image synthesis, 2023. [Online]. Available: https://arxiv.org/abs/2306.08645 [16] T. Karras, M. Aittala, T. Aila, and S. Laine, Elucidating the design space of diffusion-based generative models, 2022. [Online]. Available: https://arxiv.org/abs/2206.00364 [17] T. Karras, M. Aittala, S. Laine, E. Härkönen, J. Hellsten, J. Lehtinen, and T. Aila, Alias-free generative adversarial networks, 2021. [Online]. Available: https://arxiv.org/abs/2106.12423 [18] T. Karras, M. Aittala, J. Lehtinen, J. Hellsten, T. Aila, and S. Laine, Analyzing and improving the training dynamics of diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, pp. 24 17424 184. [19] P. Kharya, Tensorfloat-32 in the a100 gpu accelerates ai training, hpc up to 20x. [Online]. Available: https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/ 10 [20] S. Lin, B. Liu, J. Li, and X. Yang, Common diffusion noise schedules and sample steps are flawed, 2024. [Online]. Available: https://arxiv.org/abs/2305.08891 [21] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, Dpm-solver++: Fast solver [Online]. Available: for guided sampling of diffusion probabilistic models, 2023. https://arxiv.org/abs/2211. [22] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon, Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. [Online]. Available: https://arxiv.org/abs/2108.01073 [23] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach, Tensorfloat-32 in the a100 gpu accelerates ai training, hpc up to 20x. [Online]. Available: https://huggingface.co/stabilityai/cosxl [24] , SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, 2023. [25] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, 2021. [Online]. Available: https://arxiv.org/abs/2103.00020 [26] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 10 68410 695. [27] T. Salimans and J. Ho, Progressive distillation for fast sampling of diffusion models, 2022. [Online]. Available: https://arxiv.org/abs/2202.00512 [28] W. S. Sarle, comp.ai.neural-nets faq, part 2 of 7: Learning. [Online]. Available: http://www.faqs.org/faqs/ai-faq/neural-nets/part2/ [29] B. P. Welford, Note on method for calculating corrected sums of squares and [Online]. Available: products, Technometrics, vol. 4, no. 3, pp. 419420, 1962. https://www.tandfonline.com/doi/abs/10.1080/00401706.1962.10490022 [30] H. Wu, S. Shen, Q. Hu, X. Zhang, Y. Zhang, and Y. Wang, Megafusion: Extend diffusion models towards higher-resolution image generation without further tuning, 2024. [Online]. Available: https://arxiv.org/abs/2408. [31] H. Yu, L. Shen, J. Huang, H. Li, and F. Zhao, Unmasking bias in diffusion model training, 2024. [Online]. Available: https://arxiv.org/abs/2310.08442 11 Practicalities for Implementing ZTSNR in k-diffusion/EDM ZTSNR poses practicality issues when implemented in the EDM[16] framework (i.e. via the kdiffusion[6] library). Models using the EDM formulation are expected to accept VE (VarianceExploding, in this case σ = ) noise, but this poses numeric representability issues. Likewise, EDM-space samplers are forced to take an infinitely large Euler step with an infinitesimally small velocity. We discuss two solutions for how overcome such practical issues. A.1 Principled Implementation of ZTSNR The usual prescription of k-diffusion/EDM is to start with VE noise and scale it to VP (VariancePreserving, i.e. unit variance). Practically, we can neither numerically represent σ = random tensor nor can we divide it by to convert it to variance-preserving formulation. We must find way to skip this step. The only problems are practical/numeric; algebraically we possess alternative ways to compute this. First we will need bypass within the k-diffusion model wrapper, to \"pass unit-variance pure-noise directly to the model\", instead of converting σ = VE noise to VP noise. We also need to add special case to discretization, to map σ = to the index of the maximum timestep. Next we will need to special-case how we sample from σ = ; this is the first step of ZTSNR inference schedule (though we may skip it if we are doing img2img[22]). A.1.1 Sampling from the Infinite-Noise Timestep During inference, we sample with the goal of predicting the clean image. This is not the objective our neural network was trained on. SDXL was trained to predict the noise present in the sample. And in NAIv3 we adapted SDXL (section 2.1) to instead predict velocity v[27], an objective which transitions from noise-prediction to image-prediction as SNR changes. k-diffusion provides model wrappers to adapt our neural network Fθ() (which may predict noise, velocity, or some other objective) into an abstract denoiser Dθ which predicts the clean image. Concretely, the wrapper invokes the model through the parameterization of the Karras preconditioner[16]. On top of this Dθ denoiser abstraction, k-diffusion provides samplers which iterate on the clean-image predictions, converging on fully-denoised image. The Karras preconditioner describes how clean-image denoiser Dθ can be related to our trained network Fθ() through noise level σ-dependent scaling functions cskip(σ), cout(σ), and cin(σ). Dθ(x; σ) = cskip(σ)x + cout(σ)Fθ(cin(σ)x; σ), (2) These scaling functions allow our model Fθ() to transition between predicting noise content and clean sample as the noise level changes. By assigning values to these scaling functions, we can expose which terms can be eliminated at σ = . To sample an image from Dθ, we can take Euler steps to bring the noise level down from σi to σi+1, σi+1 < σi, xi+1 xi + (σi+1 σi) xi Dθ(xi; σi) σi . (3) and in so doing iterate towards clean image. But in its current form, we cannot handle Euler step down from σ = . The step size σi+1 σi would be infinite, and the derivative xiDθ() has infinite terms on its numerator and denominator. The infinite numerator is due to the noised image xi being formulated as VE, which at σ = entails infinite variance. Moreover the step is accumulated by summation with an infinite term xi. We can make this tractable by formulating special-case Euler step for the first step = 0 of sampling. We decompose the initial VE noise into standard Gaussian and its standard deviation, via the relationship x0 = σ0n, where (0, I). By assigning infinite standard deviation σ0 , we can σ 12 eliminate σ0 terms outside the denoiser Dθ by rearranging eq. (3) as follows: x1 σ0n + (σ1 σ0) σ0n Dθ(σ0n; σ0) = σ0n + (σ1 σ0) (n Dθ(σ0n; σ0) σ0 ) σ0 σ0 = σ0n + (σ1n σ0n (σ1 σ0) Dθ(σ0n; σ0) = σ1n (σ1 σ0) Dθ(σ0n; σ0) = σ1n σ1 σ0 = σ1n σ1 σ0 = σ1n + Dθ(σ0n; σ0) σ0 Dθ(σ0n; σ0) + Dθ(σ0n; σ0) Dθ(σ0n; σ0) σ0 ) (4) (5) (6) (7) (8) (9) (10) Next we expand the Karras preconditioner Dθ to reveal its relationship with the underlying neural network Fθ. Posing the Euler step in terms of these scaling factors will expose σ0 terms, and create an opportunity to eliminate them as σ0 . In our case, Fθ has v-prediction objective. The Karras preconditioner can be adapted to v-prediction model using very similar scalings to the EDM denoiser, except with negative cout(σ) [6]: cskip(σ) = cout(σ) = cin(σ) = σ2 data σ2 + σ2 data σ σdata + σ2 σ2 1 data σ2 + σ2 data (11) (12) (13) These scaling factors simplify as σ0 , giving us cskip(σ) = 0 cout(σ) = σdata, we can pose our Euler step eq. (10) in terms of the Karras preconditioner eq. (2), then apply these scalings (eq. (13)) to eliminate all σ0 terms outside the neural network Fθ: (14) (15) x1 σ1n + cskip(σ0)σ0n + cout(σ0)Fθ(cin(σ0)σ0n; σ0) = σ1n σdataFθ(cin(σ0)σ0n; σ0) σ0 σ2 0 = σ1n σdataFθ(n; σ0). = σ1n σdataFθ + σ2 data n; σ (16) (17) (18) (19) Now, by special-casing the noise level conditioning mechanism for the zero-terminal SNR step σ0, the first sampling step no longer contains any non-finite terms and thus can directly be implemented. After completing this ZTSNR step, we would hand over to conventional sampler (in much the same way as img2img[22] is implemented) to continue denoising from finite sigma. We do not attempt to incorporate this ZTSNR step into multistep (such as in DPM++ 2M [21]), as this would expose us to the \"infinite step size\" problem again. This is not big loss; even if we were to implement ZTSNR multistep, the step size ratio hlast would be infinite, so DPM++ multistep (which incorporates former steps in inverse proportion to this step size ratio) would use an infinitessimal amount of the ZTSNR steps estimate. A.2 Trivial Implementation of (Almost) Zero Terminal SNR We opted for simpler solution: use k-diffusion as usual, but clamp the sigmas by which our schedule is defined, to maximum of 20000. Noise is drawn in float32, as is the scaling of VE noised latents to 13 VP. σ = 20000 is more than sufficient to destroy the lowest frequencies of the noise. Empirically, we find that even much lower sigma of somewhere around 136317 is sufficient to prevent our model expecting mean leakage in the noise we provide, at our standard canvas size of 832 1216px. VAE Scale-and-Shift We disclose the scale-and-shift of our anime dataset, in hopes of encouraging scale-and-shift of training practices. Whilst this technique was not used on NAIv3, it is practice we would like to raise awareness of for future model training. B.1 Background: Current Latent Scaling Practices Conventionally, latent diffusion setups adapt from the VAEs latent distribution to the diffusion models distribution like so: scale factor of 0.18215 (SD1) or 0.13025 (SDXL) is used Latents encoded by the VAE would be multiplied by this scale factor before being (noised and) given to the diffusion model. Denoised latents output by the diffusion model would be divided by this scale factor before being given to the VAE decoder. This practice was established in [26], where the value comes from the (reciprocal of the) standard deviation of single batch of images. The intention is to allow the diffusion model to train on data which has unit variance, by scaling down the comparatively high-variance VAE output. This is all the more important when training using the EDM formulation[16], which is parameterized on σdata (the standard deviation of the dataset), which in latent diffusion is typically configured to 1. Scaling our data to have unit variance helps to satisfy this property we claim of our dataset. B.2 Proposal: Something Entirely More Gaussian We suggest few changes to this practice: Apply scale and shift; seek to center our data on its mean. This gives our data mean of 0 and unit variance, making it standard Gaussian. Meancentering the data may help the model to work without biases in its conv_in and conv_out. Apply per-channel scale-and-shift This makes each channel standard Gaussian. Benefits of standardizing neural network inputs in this way are discussed in [28]. Comes with potential downside of decorrelating the channels, which may make it harder to identify signals such as \"all channels high = white\". So there is question of which benefit is preferable to have. This is not new idea, but it is underutilized in the SDXL community. Per-channel scale-and-shift has been applied to the latent distribution of the original stable-diffusion[26] VAE finetunes, for the training of diffusion decoders[3] and image-generative diffusion models[18]. Per-channel scale-andshift is prevalent idea in pixel-space training, with common dataloader examples normalizing using ImageNet statistics[5]. To advance SDXL training practices, we share the scale-and-shift of our anime dataset  (fig. 9)  , computed over 78224 images using Welford average[29]. This online average enables us to accumulate an average over multiple batches without precision loss or memory growth. Though it is not an entire-dataset average, it is satisfactorily converged."
        },
        {
            "title": "Variable",
            "content": "0 1 2 3 µ σ 4.8119 9. 0.1607 6.2753 1.3538 7.5978 -1.7753 5.9956 Figure 9: Scale-and-shift of latent anime illustrations encoded by SDXL VAE Notice how if we average σ over all channels, we get 7.4467, whose reciprocal is 0.1343 nearly the 0.13025 with which SDXL is typically configured. Notice also how the channels are not meancentered, and their variances. . . vary. To adapt latents encoded by the SDXL VAE into standard Gaussians: one would subtract these means (µ) then divide by these stds (σ), encoded . sub_ ( means ) . div_ ( stds ). convenient implementation of this is available in torchvision . transforms . v2 . functional . normalize. To adapt the diffusion models standard Gaussian output to the VAE decoder distribution: one would multiply these stds (σ), then add these means (µ), encoded . mul_ ( stds ) . add_ ( means ). The hope is that after applying this scale-and-shift: one should observe that (on average) perchannel standard deviations . std (( -2 , -1) ) are closer to 1 and that per-channel means . mean (( -2 , -1) ) are closer to 0 than before (note: we assume channels-first image data, as is conventional for convolutional models). These statistics are dataset-dependent; illustration datasets will exhibit more variance than natural images, and if white backgrounds are well-represented then the mean of the distribution will be elevated. It is worth measuring statistics for whichever dataset is to be used, but the statistics we share should be applicable to anime illustration datasets."
        }
    ],
    "affiliations": [
        "Anlatan LLC"
    ]
}