{
    "paper_title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
    "authors": [
        "Dingjie Song",
        "Sicheng Lai",
        "Shunian Chen",
        "Lichao Sun",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 3 2 8 3 0 . 1 1 4 2 : r Preprint - Work in Progress Both Text and Images Leaked! SYSTEMATIC ANALYSIS OF MULTIMODAL LLM DATA CONTAMINATION Dingjie Song,1, Sicheng Lai,1, Shunian Chen1, Lichao Sun2, Benyou Wang1 1The Chinese University of Hong Kong, Shenzhen 2Lehigh University https://github.com/MLLM-Data-Contamination/MM-Detect"
        },
        {
            "title": "ABSTRACT",
            "content": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced."
        },
        {
            "title": "INTRODUCTION",
            "content": "The development of MLLMs has exceeded expectations (Liu et al., 2023a; Lin et al., 2023), showcasing extraordinary performance on various multimodal benchmarks (Lu et al., 2022; Liu et al., 2023b; Song et al., 2024), even surpassing human performance. However, due to the partial obscurity associated with MLLMs training (OpenAI, 2023; Reid et al., 2024), it remains challenging to definitively ascertain the impact of training data on model performance, despite some works showing the employment of the training set of certain datasets (Liu et al., 2023a; Chen et al., 2023; Bai et al., 2023b). The issue of data contamination, occurring when training or test data of benchmarks is exposed during the model training phase (Xu et al., 2024), could potentially instigate inequitable performance comparisons among models. This not only creates dilemma for users in model selection but also poses significant hurdle to further advancements in this domain. While numerous works in the field of LLMs have proposed methods for detecting data contamination (Yeom et al., 2018; Deng et al., 2024; Dong et al., 2024), MLLMs, due to their various modalities and multiple training phases (Liu et al., 2023a; Li et al., 2023), face limitations when applying these methods. Therefore, there is pressing need for more suitable multimodal contamination detection framework specifically tailored for MLLMs. In this study, we carry out systematic analysis of multimodal data contamination. Initially, we define Multimodal Data Contamination, as it pertains to the modality of data sources exposed to the MLLMs, into two categories: Unimodal Contamination and Cross-modal Contamination. Subsequently, we unveil detection framework for multimodal data contamination, MM-Detect, which incorporates two methods, Option Order Sensitivity Test and Slot Guessing for Perturbation Caption, designed to handle two common types of Visual Question Answering (VQA) tasks: multiplechoice and caption-based questions, respectively. Then, applying MM-Detect on eleven widely-used MLLMs across five prevalent VQA datasets, we observe that both open-source and proprietary MLLMs do exhibit contamination, with the degree of contamination varying across different models. Benyou is the corresponding author (wangbenyou@cuhk.edu.cn); means equal contribution. 1 Preprint - Work in Progress Figure 1: description of Multimodal Data Contamination (left) and the overview of proposed MM-Detect framework (right). To corroborate the validity and sensitivity of our approach, we deliberately induce contamination in MLLMs, thus simulating feasible real-world scenarios. Experimental results indicate that our method proves to be quite effective and sensitive in identifying varying degrees of contamination. Interestingly, our findings reveal that not only does leakage in the multimodal benchmark test set play role, but the training set can also contribute significantly to enhancing the models performance. To further delve into the stage where contamination is introduced, we employ heuristic method. This method seeks to distinguish whether the contamination originates from the pre-training phase of LLMs or the multimodal training phase. Our findings suggest that the contamination observed in some MLLMs may not necessarily stem from the multimodal training phase. Instead, it could potentially be traced back to the pre-training stage of their respective LLMs. To the best of our knowledge, our work is the first effort to systematically analyze multimodal data contamination. In conclusion, our research makes several important contributions: We formulate the definition for multimodal contamination detection and present the MMDetect framework, comprising two innovative methods specifically designed for effective contamination detection in MLLMs. We demonstrate that leakage from multimodal benchmark data can significantly boost the models performance on test sets, with this enhancement intensifying as the degree of contamination increases. By employing heuristic method, we pioneer the exploration into the stage at which contamination is introduced, revealing that it may stem not solely from the multimodal data but could also from the LLMs."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "We formally define the problem of contamination in multimodal benchmark datasets and describe the unique challenges in multimodal contamination detection. 2.1 DEFINITION OF MULTIMODAL DATA CONTAMINATION In contrast to single-modal contamination, multimodal contamination may arise from both unimodal and multimodal data sources, as depicted in Figure 1. The training data for MLLMs generally consists of pure text pre-training data Dpretrain and multimodal alignment or instruction-following data Dvision. Consider an instance (x, i, y) from benchmark dataset D, where represents the text input, is the image input, and is the label. Data contamination in multimodal models can be categorized into the following two cases: Unimodal Contamination: The pair (x, y) or the input appears in Dpretrain. Cross-modal Contamination: The triplet (x, i, y) appears in Dvision. In both cases, models trained on these data may gain an unfair advantage. 2.2 CHALLENGES IN MULTIMODAL DETECTION The challenges of multimodal contamination detection mainly arise from two aspects. 2 Preprint - Work in Progress Challenge I: Inefficiency of Unimodal Methods. Despite the prevalence of unimodal detection methods, their application in multimodal scenarios often encounters difficulties. For example, retrieval-based methods Brown et al. (2020); Touvron et al. (2023a) attempt to detect contamination by retrieving large-scale corpora used for model training. Yet, they struggle when retrieving multimodal information. Similarly, logits-based methods (Shi et al., 2024; Yeom et al., 2018) rely on observing the distribution of low-probability tokens in model outputs, but the disparity in token probability distributions is less pronounced in instruction-tuned models. Masking-based methods Deng et al. (2024), which assess training contamination by evaluating models ability to predict specific missing or masked text, face challenges when images in multimodal samples provide clues, leading to overestimated contamination detection. Finally, comparison-based methods Dong et al. (2024) that measure contamination by comparing model outputs with benchmark data prove to be ineffective for image caption tasks due to low output similarity. To validate these inefficiencies, we conducted experiments with compelling results, which are detailed in Appendix A. Challenge II: Multi-stage Training in MLLMs. Another challenge in detecting contamination in multimodal models is the multi-stage nature of their training (Yin et al., 2023). Each stage may be subject to data contamination. Initially, the pretraining corpus could contain the textual components of questions from benchmark test samples. Moreover, in certain native multimodal model training (Reid et al., 2024), test samples may be entirely exposed. Subsequently, during multimodal fine-tuning, the model may utilize training samples of some benchmarks, leading to skewed performance improvements. Furthermore, some models employ extensive mixed image-text data from the internet for modality alignment training (Lin et al., 2023; Bai et al., 2023b), potentially introducing additional contamination. Given these challenges, the development of an effective detection framework for multimodal contamination becomes an urgent need. Based on the discussion above, we have designed detection method specifically tailored for multimodal contamination, with particular focus on VQA tasks. Additionally, we have developed heuristic method to trace the introduction of contamination across different training phases."
        },
        {
            "title": "3 MM-DETECT",
            "content": "In this section, we introduce the multimodal contamination detection framework, MM-Detect. The core philosophy of MM-Detect is to detect the unusual discrepancies in model performance before and after semantic-irrelevant perturbations. As depicted in Figure 1, this framework operates in two primary steps: The first step is to generate perturbed datasets using two innovative methods: the Option Order Sensitivity Test (Section 3.1) and the Slot Guessing for Perturbation Captions (Section 3.2), tailored for evaluating multiple-choice and image captioning tasks, respectively. The second step involves the application of predefined metrics to detect contamination (Section 3.3), conducting thorough analyses at both the dataset and instance levels. 3.1 OPTION ORDER SENSITIVITY TEST This method is based on reasonable and intuitive premise that if the models performance is highly sensitive to the order of the options, as shown in Figure 2, it indicates potential contamination, leading the model to memorize certain canonical order of the options. Method Formulation. Let represent dataset comprising datapoints. For each datapoint di, where {1, . . . , n}, there is question symbolized by Qi, an image represented by Ii, and list of choices is denoted by Ai, such that Ai = {a1 } and is the number of choices for that datapoint. The correct answer is A. The list is randomly shuffled to generate A, ensuring that the symbolized as ac in differs from its index in A, thereby altering the correct answers index of the correct answer ac Figure 2: An example of Option Order Sensitivity Test applied to contaminated model. , where ac , . . . , am , a2 Preprint - Work in Progress position. The final prompts, both before and after the shuffling, are the concatenation of the image, question, and choices: = Concat(I, Q, A), = Concat(I, Q, A), where and are the prompts fed into the model, and and remain constant."
        },
        {
            "title": "3.2 SLOT GUESSING FOR PERTURBATION CAPTION",
            "content": "This method is based on the intuition that if model can predict missing part of sentence but fails with the back-translated version (from English to Chinese, then back to English), it likely indicates that the model has encountered the original sentence during training. As shown in Figure 3, the keywords identified are woods and bike. Since the image contains woods, correct guess by the model may stem from its multimodal capabilities rather than data contamination. However, if the model fails to predict bike, which is also present in the image, this may indicate potential leakage of this instance. Method Formulation. Let be dataset containing datapoints. For each datapoint di, {1, . . . , n}, there is corresponding caption Si describing the image features. We first apply back-translation function1 to Si: Figure 3: An example of Slot Guessing for Perturbation Caption. = fback-translate(S) to obtain the back-translated sentence S. Next, we perform keyword extraction2 on both and S: = fkeyword(S), = fkeyword(S), where and are the keywords extracted from and S, respectively. We then use masking function fmask to replace the keywords in the sentences with [MASK]: Smask = fmask(S, K), mask = fmask(S, ). The final prompt can be represented as the concatenation of the image, the instruction and the masked sentence: = Concat(I, Q, Smask), = Concat(I, Q, mask), where is the image and is the instruction guiding the model to complete the mask word prediction task. 3.3 DETECTION METRICS Having introduced two detection methods, we now delineate the metrics for the detection pipeline, which consists of two primary steps. Step 1: Benchmark Atomic Metrics Calculation. This step assesses the models performance on benchmark before and after perturbation. We denote the correct rate (CR) and perturbed correct rate (PCR) uniformly for both Option Order Sensitivity Test (using Accuracy) and Back-Translation Keyword Guessing (using Exact Match). Here, and are the counts of correct answers before and after perturbation, respectively. They are calculated as: CR = D , CR = . 1We use Google-Translate API for Python to implement the back-translation. 2We employ the Stanford POS Tagger (Toutanvoa & Manning, 2000), targeting nouns, adjectives, or verbs, as they encapsulate the sentences core meaning. 4 Preprint - Work in Progress Step 2: Contamination Degree Analysis. This step quantifies the models contamination degree based on the performance variation preand post-perturbation. Specifically, we introduce two metrics to evaluate contamination at both dataset and instance levels. Dataset Level. We evaluate the reduction in atomic metrics, denoted as : = CR CR This reduction indicates the models familiarity or memory of the original benchmark relative to the perturbed set, thereby offering insights into potential contamination at the dataset level. significant negative suggests potential extensive leakage in the benchmark dataset, leading to highly perturbation-sensitive model performance. Instance Level. Despite non-significant or positive , contamination may still occur at the instance level, as some instances may still have been unintentionally included during training. To identify such instances, we compute X, the count of cases where the model provided correct answers before perturbation but incorrect answers after. The instance leakage metric IL is then obtained by dividing by the dataset size: IL = , where larger IL indicates higher likelihood of instance leakage. For further details on our atomic metrics computation, please refer to Appendix B."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we demonstrate the practicality of our methodology in verifying contamination of multimodal benchmark datasets across several MLLMs. 4.1 SETUP Models. We conducted extensive evaluations on eight open-source MLLMs, including LLaVA-1.57B Liu et al. (2023a), VILA1.5-3B Lin et al. (2023), Qwen-VL-Chat Bai et al. (2023b), fuyu-8b3, idefics2-8b Laurençon et al. (2024), Phi-3-vision-128k-instruct Abdin et al. (2024), Yi-VL-6B AI et al. (2024), InternVL2-8B Chen et al. (2023; 2024b), as well as three proprietary MLLMs: GPT-4o OpenAI (2023), Gemini-1.5-Pro Reid et al. (2024), and Claude-3.5-Sonnet4. Benchmark Datasets. Our analysis leverages two multi-choice datasets: ScienceQA Lu et al. (2022) and MMStar Chen et al. (2024a), along with three caption datasets: COCO-Caption2017 Lin et al. (2015), NoCaps Agrawal et al. (2019), and Vintage5. MMStar and Vintage, owing to their recent inception, serve to contrast contamination levels with other datasets. We randomly selected 2000 and 1340 samples from ScienceQAs training and test sets, respectively, with 1000 samples from the other datasets. Given the unavailability of public test labels for COCO-Caption2017 and NoCaps, we used their validation sets. 4.2 MAIN RESULTS Multi-choice Datasets. As shown in Table 1, for the training set of ScienceQA, Claude-3.5-Sonnet exhibited significant of -5.3, indicating extensive potential contamination. VILA1.5-3Bs high IL suggests that the model may have encountered some benchmark instances during training. For the test set of ScienceQA, all models except Claude-3.5-Sonnet showed insignificant , indicating low likelihood of large-scale leakage from the benchmark. However, the significant IL from fuyu-8b may indicate that the model has seen some benchmark instances or similar training data. It is noteworthy that even the latest benchmarks may still be contaminated. For MMStar, fuyu-8b showed significant , and Claude-3.5-Sonnet exhibited high IL. Since the data sources for MMStar are older datasets, which have been filtered for image-text relevance, the experimental results align with expectations. 3https://www.adept.ai/blog/fuyu-8b 4https://www.anthropic.com/news/claude-3-5-sonnet 5https://huggingface.co/datasets/SilentAntagonist/vintage-artworks-60k-captioned 5 Preprint - Work in Progress Model Metric ScienceQA Training Set ScienceQA Test Set MMStar Validation Set CR PCR IL CR PCR IL CR PCR IL LLaVA-1.5-7B VILA1.5-3B Qwen-VL-Chat fuyu-8b idefics2-8b Phi-3-vision-128k-instruct Yi-VL-6B InternVL2-8B GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet 59.7 57.7 58.4 36.5 85.1 90.5 60.5 94.1 69.9 68.5 70.3 58.6 58.3 60.8 37.5 84.0 90.4 61.8 93.9 70.0 67.9 65. Open-source MLLMs -1.1 0.6 2.5 1.0 -1.2 -0.1 1.3 -0.3 14.5 13.3 13.4 4.6 10.0 2.0 60.3 60.3 60.3 37.4 84.0 88.4 59.5 92.3 61.6 59.8 60.4 36.9 84.3 89.1 61.3 93.1 Proprietary MLLMs 0.1 -0.6 -5.3 2.7 6.6 69.1 66.5 67.3 69.7 66.2 64.9 1.3 -0.5 0.1 -0.5 0.3 0.7 1.8 0.8 0.6 -0.3 -2. 10.5 14.8 13.7 14.9 2.8 3.9 9.6 1.7 2.8 7.1 38.9 38.6 40.9 28.2 48.2 48.7 38.8 56.9 48.6 45.7 36.3 41.7 37.6 44.2 27.0 49.3 51.9 44.0 60.1 50.5 45.5 36. 2.8 -1.0 3.3 -1.2 1.1 3.2 5.2 3.2 1.9 -0.2 0.1 11.0 13.2 7.9 7.2 9.3 5.1 9.4 9.9 15.9 Table 1: Comparison of MLLMs performance on different multi-choice datasets. Bold values indicate the most significant or IL. denotes that is significant so that IL will not be calculated. Model Metric COCO Validation Set NoCaps Validation Set Vintage Training Set CR PCR IL CR PCR IL CR PCR IL LLaVA-1.5-7B VILA1.5-3B Qwen-VL-Chat fuyu-8b idefics2-8b Phi-3-vision-128k-instruct Yi-VL-6B InternVL2-8B GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet 34.6 19.1 32.2 9.6 43.5 38.8 43.9 53.3 58.1 57.5 53.7 34.0 20.5 30.3 10.6 42.3 39.3 43.3 51. 54.4 55.3 51.0 Open-source MLLMs -0.6 1.4 -1.9 1.0 -1.2 0.5 -0.6 -1.4 19.0 13.0 7.8 19.4 19.4 30.9 19.1 28.7 10.0 42.6 36.9 37.2 48.0 28.5 20.5 27.3 9.8 37.5 33.3 36.1 46. Proprietary MLLMs -3.7 -2.2 -2.7 54.2 51.2 50.8 55.1 52.0 51.5 -2.4 1.4 -1.4 -0.2 -5.1 -3.6 -1.1 -1. 0.9 0.8 0.7 13.0 8.3 19.4 18.7 20.0 10.8 1.5 15.1 2.4 18.5 17.4 3.3 28.0 36.3 35.2 10.1 2.2 15.4 3.3 17.0 11.7 4.2 28. 38.4 33.0 -0.7 0.7 0.3 0.9 -1.5 -5.7 0.9 0.7 2.1 -2.2 9.0 1.5 12.4 2.3 2.8 18.8 20.1 21.3 Table 2: Comparison of MLLMs performance on different caption datasets. We have not detected the contamination of Gemini-1.5-Pro on Vintage yet. Caption Datasets. From Table 2, we observe that for the validation sets of COCO-Caption2017 and NoCaps, more than half of the models exhibited significant performance drops due to perturbations, indicating that these two benchmarks may face serious leakage issues. For the training set of Vintage, idefics2-8b showed significant , and Claude-3.5-Sonnet had the highest IL. Takeaways Multimodal data contamination, at both dataset and instance levels, is prevalent in open-source and proprietary MLLMs across multi-choice and image caption datasets."
        },
        {
            "title": "INTENTIONAL CONTAMINATION",
            "content": "This section utilizes intentional contamination to address three research questions: RQ1: Can MM-Detect effectively identify contamination? RQ2: What is MM-Detects sensitivity level? RQ3: Does training set leakage result in evaluation bias? To tackle these questions, we followed the recipe of LLaVA and train multiple 7B parameter models on intentionally contaminated training data from downstream task examples during the visual instruction tuning phase, subsequently evaluating their contamination degree. Preprint - Work in Progress"
        },
        {
            "title": "5.1 MM-DETECT IS AN EFFECTIVE DETECTOR",
            "content": "We reproduced the LLaVA-1.5-7B model experiment to obtain control model, LLaVA-1.5-7B-nocont, which is devoid of intentional data contamination. Subsequently, we randomly incorporated 2000 training samples from ScienceQA and 1000 validation samples from NoCaps into Dtuning, where Dtuning denotes the data used by LLaVA for visual instruction tuning. This data was used to train contaminated model, LLaVA-1.5-7B-cont, for comparison with LLaVA-1.5-7B-no-cont. Models ScienceQA Train Set PCR CR NoCaps Val. Set PCR CR LLaVA-1.5-7B-cont LLaVA-1.5-7B-no-cont 72.9 61.8 67.9 61.2 -5.0 -0.6 38.2 33.0 32.8 32.1 -5.4 -0. Table 3: Detection results after actively contaminating the model with the ScienceQA training set and NoCaps validation set, showcasing the effectiveness of our method in accurately identifying contamination. As depicted in Table 3, the inclusion of contaminated data during the training phase increases the models familiarity with the data and its sensitivity to perturbations. This leads to an average CR increase of 8.2%, an average PCR increase of 3.7%, and an average decrease of 4.5%. These changes illustrate our successful detection of data contamination. 5.2 MM-DETECT IS SENSITIVE TO CONTAMINATION DEGREES To investigate the sensitivity of MM-Detect, We selected 1340 examples from the ScienceQA test set to train three models: fully contaminated model (LLaVA-1.5-7B-cont-100%) incorporating all 1340 examples into Dtuning, partially contaminated model (LLaVA-1.5-7B-cont-50%) with 670 examples mixed in, and minimally contaminated model (LLaVA-1.5-7B-cont-10%) with 134 examples mixed in. This approach demonstrates our methods robustness in reflecting various degrees of model contamination. Figure 4 reveals our testings high sensitivity. As contamination degrees rise from 10% to 50%, theres significant increase of 8.7% in CR, 7.3% in PCR, and decrease of 1.4% in . When contamination escalates from 50% to 100%, CR again significantly rises by 5.3%, PCR increases by 3.6%, and decreases by 1.7%. These findings suggest that our tests can address the binary classification challenge of detecting models contamination, and more importantly, they can reflect varying degrees of contamination. Figure 4: MM-Detect captures the increasing contamination levels of models on ScienceQA (test set) and reflects them in the atomic metrics. 5.3 TRAINING SET LEAKAGE LEADS TO UNFAIRNESS We investigates whether training set leakage leads to evaluation bias. As Dtuning already includes the training set of COCO2017, we remove that portion from Dtuning to obtain LLaVA-1.5-7B-no-coco, thereby simulating scenario where the model hasnt encountered the training set. Model CR PCR LLaVA-1.5-7B-cont LLaVA-1.5-7B-no-cont 64.3 61.4 63.8 61.5 -0.5 0.01 Model CR PCR LLaVA-1.5-7B-cont LLaVA-1.5-7B-no-coco 38.1 32.5 34.9 31.9 -3.2 -0.6 Table 4: Detection results on the ScienceQA test set. LLaVA-1.5-7B-cont was trained on 2000 training samples from ScienceQA. Table 5: Detection results on the COCOCaption2017 validation set. LLaVA-1.5-7B-nocont: does not include benchmark data but contains the training set from COCO2017. 7 Preprint - Work in Progress Tables 4 and 5 demonstrate that training set leakage substantially enhances model performance on test or validation sets, with an average increase of 4.3% in CR and 2.7% in PCR. These findings imply that training using the training set could potentially lead to disproportionate performance advantages."
        },
        {
            "title": "Takeaways",
            "content": "Both training and test set leakage can result in unfairness, and the degree of contamination can be detected through MM-Detect."
        },
        {
            "title": "6 AT WHICH STAGE IS CONTAMINATION INTRODUCED?",
            "content": "In this section, we will explore the source of contamination. The training data for most of the MLLMs we examined is openly stated, which prompts the question: If the contamination did not originate from the multimodal training phase, could it have been introduced during the unimodal training phase? In this context, unimodal contamination refers to instances where the text inputs or labels from the benchmark data have already been exposed during the pre-training phase of the LLMs utilized by these models (Section 2.1). To explore this possibility, we examined the LLMs used by the previously tested MLLMs and performed series of experiments. 6.1 HEURISTIC EXPERIMENT FOR UNIMODAL CONTAMINATION DETECTION We employed heuristic approach based on the intuition that if the LLM can correctly answer an image-required question without the image, it may indicate the leakage of that instance. Experiment Setup. The benchmark for this experiment was MMStar, which inherently consists of image-text related questions. The models tested include LLaMA2-7b Touvron et al. (2023b) used by LLaVA-1.5 and VILA, Qwen-7B Bai et al. (2023a) used by Qwen-VL, Mistral-7B-v0.1 Jiang et al. (2023) used by idefics2, Phi-3-small-128k-instruct Abdin et al. (2024) used by Phi-3-vision, Yi-6B AI et al. (2024) used by Yi-VL, and Internlm2-7B Cai et al. (2024) used by InternVL2. To reduce the possibility of the model making random guesses, we appended the prompt, If you do not know the answer, output dont know, to the instructions. The metric we reported was the ContRate, representing the frequency these models correctly answered questions without the necessity of image input. It is worth noting that we did not conduct experiments on Fuyu-8b and proprietary models since they did not disclose the unimodal language model and training data used. Model ContRate ILM LLaMA2-7b (LLaVA-1.5 & VILA) Qwen-7B (Qwen-VL) Internlm2-7B (InternVL2) Mistral-7B-v0.1 (idefics2) Phi-3-small-128k-instruct (Phi-3-vision) Yi-6B (Yi-VL) 25.6 13.2 11.0 10.7 6.1 3.4 11.0 13.2 5.1 7.9 7.2 9.3 Table 6: Contamination rates of the LLMs used by multimodal models. ILM denotes the IL of the corresponding MLLMs. Main Results. From Table 6, we can observe that LLaMA2-7b and Qwen-7B still exhibit high contamination rates even without image inputs, with LLaMA2-7bs rate reaching 25.6%. This indicates that the pre-training corpora of these LLMs may have mixed in text inputs or labels from multimodal benchmarks. More importantly, we find that most of these LLMs with higher contamination levels correspond to multimodal models like VILA1.5-3B, which has significant , and LLaVA-1.5-7B and Qwen-VLChat, which exhibit high IL. This suggests that the contamination suffered by these three MLLMs may not solely originate from the multimodal training phase but rather from the pre-training phase of the LLMs. 8 Preprint - Work in Progress"
        },
        {
            "title": "CONTAMINATION DETECTION",
            "content": "To investigate the origins of cross-modal contamination, we scrutinize the visual instruction tuning data of MLLMs. We delve into the construction process of three benchmark datasets: ScienceQA, COCO Caption, and Nocaps, comparing them with the training data and its sources of various open-source MLLMs to analyze the degree of overlap. Model ScienceQA COCO Caption Nocaps Phi-3-Vision VILA Idefics2 LLaVA-1.5 Yi-VL Qwen-VL-Chat InternVL2 0.7 -0.5 0.3 1.3 1.8 0.1 0.8 0.5 1.4 -1.2 -0.6 -0.6 -1.9 -1. -3.6 1.4 -5.1 -2.4 -1.1 -1.4 -1.8 As Table 7 illustrates, MLLMs marked in red and yellow typically exhibit significant contamination degree. Yet, even MLLMs labeled in green arent exempt from the risk of crossmodal contamination. This is because some models have been trained on large-scale interleaved image-text datasets (e.g., OBELICS (Laurenon et al., 2023)), datasets derived from online sources (e.g., Conceptual Caption (Sharma et al., 2018)), or in-house data. Furthermore, some models havent fully disclosed their training data, which may lead to overlooked potential leaks in benchmark datasets. Table 7: Depiction of the overlap between the training data of MLLMs and the benchmarks, as well as the contamination degree of MLLMs on benchmarks. Green signifies no overlap, yellow suggests potential overlap, and Red indicates partial or entire overlap. Takeaways The contamination in MLLMs may not only stem from cross-modal contamination but also from unimodal contamination, both of which can significantly impact the overall performance."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "In this study, we introduce and validate multimodal data contamination detection framework, MM-Detect, providing new perspectives for evaluating contamination in MLLMs. We discovered that popular multimodal models exhibit varying degrees of data contamination, which directly impacts their performance and generalization ability. In addition, our experimental indicates that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage in the multimodal benchmark training set. Furthermore, we found that the contamination in multimodal models may not solely originate from the cross-modal contamination but could also stem from the unimodal contamination. Future work will focus on two key areas: Firstly, standardizing the use of multimodal datasets and reporting potential contamination impacts to minimize contamination, thereby enhancing data consistency and quality. Secondly, creating continuously updated benchmarking system for the ongoing evaluation of multimodal model performance. This will support advancements and broader applications in this field."
        },
        {
            "title": "LIMITATIONS",
            "content": "We acknowledge several limitations in detecting test set contamination. First, this work is limited to discussions around visual modalities, and does not yet cover other modalities such as audio or video. Second, we only selected widely used and representative multimodal datasets for detection, including multiple-choice datasets and caption datasets, without testing additional datasets. However, we speculate that the method Slot Guessing for Perturbation Caption may also apply to other types of Image Feature Analysis benchmarks. 9 Preprint - Work in Progress"
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024. doi: 10.48550/ARXIV.2404.14219. URL https: //doi.org/10.48550/arXiv.2404.14219. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, October 2019. doi: 10.1109/iccv.2019.00904. URL http://dx.doi.org/10.1109/ICCV.2019.00904. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023a. doi: 10.48550/ARXIV.2309.16609. URL https://doi.org/10.48550/arXiv.2309.16609. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023b. URL https://arxiv.org/abs/2308.12966. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, 10 Preprint - Work in Progress Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, and et al. Internlm2 technical report. CoRR, abs/2403.17297, 2024. doi: 10.48550/ARXIV.2403.17297. URL https://doi.org/10.48550/arXiv.2403.17297. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024b. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. In Kevin Duh, Helena GómezAdorno, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 87068719. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.NAACL-LONG.482. URL https://doi.org/10.18653/v1/2024.naacl-long.482. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 12039 12050. Association for Computational Linguistics, 2024. URL https://aclanthology.org/ 2024.findings-acl.716. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? CoRR, abs/2405.02246, 2024. doi: 10.48550/ARXIV.2405.02246. URL https://doi.org/10.48550/arXiv.2405.02246. Hugo Laurenon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023. URL https://arxiv.org/abs/2306.16527. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. CoRR, abs/2312.07533, 2023. doi: 10.48550/ARXIV.2312.07533. URL https://doi.org/10.48550/ arXiv.2312.07533. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. 11 Preprint - Work in Progress Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. CoRR, abs/2310.03744, 2023a. doi: 10.48550/ARXIV.2310.03744. URL https://doi. org/10.48550/arXiv.2310.03744. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? CoRR, abs/2307.06281, 2023b. doi: 10.48550/ARXIV.2307.06281. URL https://doi.org/10.48550/arXiv.2307.06281. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=zWqr3MQuNs. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024. Kristina Toutanvoa and Christopher D. Manning. Enriching the knowledge sources used in maximum entropy part-of-speech tagger. In Hinrich Schütze and Keh-Yih Su (eds.), Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, EMNLP 2000, Hong Kong, October 7-8, 2000, pp. 6370. Association for Computational Linguistics, 2000. doi: 10.3115/1117794.1117802. URL https://aclanthology.org/W00-1308/. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023a. doi: 10.48550/ARXIV.2307.09288. URL https: //doi.org/10.48550/arXiv.2307.09288. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor 12 Preprint - Work in Progress Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL https: //doi.org/10.48550/arXiv.2307.09288. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 31st IEEE Computer Security Foundations Symposium, CSF 2018, Oxford, United Kingdom, July 9-12, 2018, pp. 268282. IEEE Computer Society, 2018. doi: 10.1109/CSF.2018.00027. URL https://doi.org/10.1109/CSF.2018.00027. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023."
        },
        {
            "title": "A INEFFICIENCY OF UNIMODAL METHODS",
            "content": "We demonstrate the results of traditional unimodal contamination detection methods applied to MLLMs. A.1 LOGITS-BASE These methods determine contamination by observing the distribution of low-probability tokens in model outputs. However, MLLMs typically undergo instruction fine-tuning, which enhances their instruction-following capabilities, leading to less significant differences in token probability distributions. As shown in Table 8, LLaVA-1.5-13b exhibits extremely low perplexity on multimodal benchmark datasets. Dataset Perplexity Split ScienceQA MMStar COCO-Caption2017 NoCaps 1.4498 1.4359 1.7530 1.8155 Training Set Validation Set Validation Set Validation Set Table 8: Perplexity of LLaVA-1.5-13b on various multimodal benchmarks (100 samples randomly selected from each dataset). A.2 MASKING-BASE These methods involve masking phrases or sentences and providing data from the benchmark to guide the model in filling in the missing parts. However, multimodal datasets often contain images that include the masked portions of sentences, effectively providing answers to the model. This results in significantly higher success rates for multimodal models in predicting missing parts compared to unimodal language models, leading to exaggerated contamination detection. As shown in Table 9, LLaVA-1.5-13b has high probability of Exact Match for predicting the masked word. Dataset Exact Match ROUGE-L F1 Split COCO-Caption2017 NoCaps 0.24 0.22 0.36 0. Validation Set Validation Set Table 9: Contamination detection of LLaVA-1.5-13b using TS-Guessing (question-based) on various multimodal benchmarks (100 samples randomly selected from each dataset). 13 Preprint - Work in Progress A.3 COMPARISON-BASE These methods identify contamination by comparing the similarity between model outputs and benchmark data. However, MLLMs often undergo data augmentation, causing their outputs to diverge significantly from the labels in benchmark data, making effective contamination detection challenging. From Table 10, we can see that CDD indicates contamination level of 0% across all multimodal benchmark datasets. Dataset Contamination Level Split COCO-Caption2017 NoCaps 0.0000% 0.0000% Validation Set Validation Set Table 10: Contamination detection of LLaVA-1.5-13b using CDD (Contamination Detection via output Distribution) on various multimodal benchmarks (100 samples randomly selected from each dataset)."
        },
        {
            "title": "B DETAILS OF CALCULATING ATOMIC METRICS",
            "content": "We formulate the detection pipeline algorithm utilized in MM-Detect, as shown in Algorithm 1. Algorithm 1 Calculation of Atomic Metrics 1: Input: benchmark with sequence of datapoints The corresponding detecting method d1, d2, . . . , dN 2: for = 1 to do 3: 4: end for 5: Compute = CR CR, 6: if > 1 then 7: 8: 9: else 10: 11: end if Compute IL = return IL return"
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong, Shenzhen",
        "Lehigh University"
    ]
}