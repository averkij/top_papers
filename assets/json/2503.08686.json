{
    "paper_title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models",
    "authors": [
        "Jialv Zou",
        "Bencheng Liao",
        "Qian Zhang",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 8 6 8 0 . 3 0 5 2 : r OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models Jialv Zou1, Bencheng Liao2,1, Qian Zhang3 Wenyu Liu1 Xinggang Wang1,(cid:66) 1 School of EIC, Huazhong University of Science & Technology 2 Institute of Artificial Intelligence, Huazhong University of Science & Technology 3 Horizon Robotics Figure 1. Comprehensive comparison between OmniMamba and other unified understanding and generation models. (a) Our OmniMamba is trained on only 2M image-text pairs, which is 1000 times less than Show-o. (b) With such limited data for training, our OmniMamba significantly outperforms Show-o across wide range of benchmarks and achieves competitive performance with JanusFlow. Black metrics are for the multimodal understanding benchmark, while the blue metric is for the visual generation task. (c)-(d) We compare the speed and memory of OmniMamba with other unified models on the same single NVIDIA 4090 GPU. OmniMamba demonstrates up to 119.2 speedup and 63% GPU memory reduction for long-sequence generation."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through unified next-token prediction paradigm. Intern of Horizon Robotics. (cid:66) Corresponding author: xgwang@hust.edu.cn The model fully leverages Mamba-2s high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across 1 benchmarks, despite being trained on merely 2M imagetext pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to 119.2 speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/ OmniMamba 1. Introduction In recent years, Large Language Models (LLMs) [2, 5, 15, 59, 60] have achieved remarkable advancements, igniting significant research interest in extending their fundamental capabilities to the visual domain. Consequently, researchers have developed series of Multimodal Large Language Models (MLLMs) for tasks such as multimodal understanding [42, 43, 75, 77] and visual generation [31, 55]. Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46, 64, 66, 67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models [16, 40], and vector-quantized autoregressive models [56]. Unfortunately, the significant domain gap between image and text presents critical challenge for unified multimodal generative models: preserving generation capabilities without degrading understanding performance requires an extensive volume of image-text pairs for training, as illustrated in Fig. 1. This not only leads to poor training efficiency but also creates substantial barrier to the broader development of such models, as only small fraction of researchers possess the resources to undertake such computationally demanding studies. Moreover, most existing unified multimodal generative models rely on Transformerbased LLMs [61]. However, their quadratic computational complexity results in slow inference speeds, rendering them less practical for real-time applications. The challenges faced by existing unified multimodal generative models naturally lead us to ponder: can model be developed that achieves both training efficiency and inference efficiency? To address this, we introduce OmniMamba, novel unified multimodal generative model that requires only 2M image-text pairs for training. Built on the Mamba2-1.3B [10] model as the foundational LLM with unified next token prediction paradigm to generate all modalities, OmniMamba leverages the linear computational complexity of state space models (SSMs) to achieve significantly faster inference speeds. Furthermore, to empower the Mamba-2 LLMwhose foundational capabilities are relatively weaker compared to the extensively studied Transformer modelsto efficiently learn mixed-modality generation with limited training data, we propose novel model architectures and training strategies. To enhance the models capability in handling diverse tasks, we incorporate task-specific LoRA [25]. Specifically, within each Mamba-2 layers input linear projection, we introduce distinct LoRA modules for multimodal understanding and visual generation. During task execution, the features are modulated by both the linear projection and the corresponding task-specific LoRA, while the irrelevant LoRA components are deactivated. Furthermore, we propose the decoupled vocabularies to guide the model in generating the appropriate modality, which requires more data for the model to learn. On the data front, we further propose novel two-stage decoupled training strategy to address the data imbalance between the two tasks, significantly improving training efficiency. Trained on only 2M image-text pairs, our proposed OmniMamba outperforms Show-o [67] on multiple multimodal understanding benchmarks and also matches the performance of JanusFlow [46], which was introduced by DeepSeek AI. Moreover, it achieves the best visual generation performance on the MS-COCO dataset [39]. Notably, OmniMamba demonstrates 119.2 speedup at sequence length of 16k and 63% GPU memory reduction at sequence length of 23k, compared to Show-o. Furthermore, at sequence length of 100k, it achieves 10.2 speedup and 40.4% memory savings compared to JanusFlow. Our main contributions can be summarized as follows: We introduce OmniMamba, the first Mamba-based unified multimodal understanding and visual generation model to the best of our knowledge. By novelly adopting decoupled vocabularies and task-specific LoRA, OmniMamba achieves effective training and inference. We propose novel decoupled two-stage training strategy to address the issue of data imbalance between tasks. With this strategy and our model design, OmniMamba achieves competitive performance using only 2M imagetext pairs for training-up to 1,000 times fewer than previous SOTA models. Comprehensive experimental results show that OmniMamba achieves competitive or even superior performance across wide range of vision-language benchmarks and MS-COCO generation benchmark, with significantly improved inference efficiency, achieving up to 119.2 speedup and 63% GPU memory reduction for long-sequence generation on NVIDIA 4090 GPU. 2. Related Work Multimodal Understanding The remarkable advancements in LLMs have catalyzed the development of Large Vision-Language Models (LVLMs). Some representative works, such as the LLaVA series [43, 79], BLIP series [35, 36], and MiniGPT-4 [77], have demonstrated strong multimodal understanding capabilities. These models align the features obtained from pretrained vision encoders with the feature space of LLMs through feature projectors, enabling pretrained LLMs to transfer their understanding and reasoning abilities to multimodal scenarios. Visual Generation In recent years, diffusion models [24] have made remarkable progress, leading to models [12, 49, 54] with strong visual generation capabilities. Building on these advancements, flow-based generative models [16, 40] have achieved superior results with fewer sampling steps. Additionally, some works [56, 71] have successfully integrated autoregressive models into this domain, achieving notable performance. Unified Understanding and Generation The remarkable advancements of LLMs in the fields of multimodal understanding and visual generation have naturally sparked researchers interest in training single LLM for both tasks. Early works [13, 1820] integrated pretrained diffusion modules as tools into LLMs, essentially forming combination of two expert systems rather than utilizing single LLM to perform both tasks. This approach results in more complex model architecture and often leads to suboptimal outcomes. Show-o [67] integrates next-token prediction with discrete diffusion, enabling adaptive handling of mixed-modality inputs and outputs. Meanwhile, JanusFlow [46] merges autoregressive models with rectified flow, cutting-edge technique in visual generation. In contrast, Emu3 [64] asserts that next-token prediction holds the greatest potential for achieving multi-modal generation, relying exclusively on this paradigm to manage both text and image generation tasks. Although these methods achieve outstanding performance, they are all based on Transformers, whose quadratic computational complexity presents significant drawback, particularly when handling longsequence generation tasks and high-resolution image generation. To address this challenge, we propose OmniMamba, which employs unified next token prediction paradigm to generate both text and image modalities, aiming to extend the linear computational complexity of the linear models to the field of multimodal generation. Linear Model linearcomplexity models have emerged as strong competitors to Transformers. Mamba [21], selective state space model, In recent years, series of has garnered widespread attention for its competitive performance and faster inference speeds compared to Transformers. Building on this, Mamba-2 [10], an enhanced version of Mamba, achieves performance on par with Transformers while being 2-8 times faster. Similarly, GLA [68] leverages gated linear attention to achieve linear complexity, maintaining competitive performance with Transformers. The success of linear-complexity models in the field of natural language processing (NLP) has inspired its application in the visual domain, where it has been extensively studied in traditional image tasks [45, 78], multimodal understanding [27, 38, 50, 74], and visual generation [26, 34]. In this paper, we aim to design unified multimodal understanding and visual generation model based on Mamba-2, which maintains competitive performance with Transformer-based unified models while offering significantly faster inference speeds. 3. Method 3.1. Overall Architecture Our ultimate goal is to design unified multimodal understanding and visual generation model that achieves both training and inference efficiency using only 2M image-text pairs for training. We believe the key to realizing this goal can be summarized in one word: decoupling. To this end, we propose OmniMamba, the architecture of which is illustrated in Fig. 2. Success necessitates standing on the shoulders of giants. We observe Emu3 [64], an autoregressive-based model which employs vast amounts of data and 8 billion model parameters. Despite these advantages, its final performance remains suboptimal, falling short of JanusFlow [46], hybrid generative paradigm-based model with significantly less data and fewer parameters. We argue that this discrepancy stems not from the inherent superiority of the hybrid generative paradigm but from Emu3s tight coupling design, it uses the same vocabulary and encoder for all tasks and modalities. While this design aligns with the original intention of unified model, it may lead to inefficient data utilization. In the following, we will introduce our model by focusing on the concept of decoupling. 3.2. Decoupling Encoders for the Two Tasks Previous works have explored using single vision encoder for both tasks. For example, Show-o [67] employs MAGVIT-v2 [70] to encode images into discrete tokens for both understanding and generation tasks. TransFusion [76] utilizes shared U-Net or linear encoder to map images into continuous latent space for both tasks. Emu3 trains its vision encoder based on SBER-MoVQGAN5, enabling the encoding of video clips or images into discrete tokens. 3 Figure 2. Architecture of the proposed OmniMamba MMU refers to multimodal understanding, while T2I refers to text-to-image generation. OmniMamba employs next-token prediction paradigm for both multimodal understanding and visual generation tasks. To address the distinct requirements of each tasksemantic information extraction for multimodal understanding and high-fidelity image compression for visual generation, we utilize separate encoders and heads. Furthermore, we purpose decoupled vocabularies to guide modality-specific generation and task-specific LoRA for parameter-efficient adaptation. Following prismatic VLMs [30], we fuse DINOv2 [48] and SigLIP [73] as an encoder to extract continuous features for multimodal understanding. The key idea is that integrating visual representations from DINOv2, which capture low-level spatial properties, with the semantic features provided by SigLIP leads to further performance improvements. For visual generation, we use an image tokenizer trained with LlamaGen [56] to encode images into discrete representations. This tokenizer was pretrained on ImageNet [11] and further fine-tuned on combination of 50M LAION-COCO [33] and 10M internal high aesthetic quality data. 3.3. Decoupling Vocabularies for the Two Tasks Unlike Emu3 and Show-o, which use large unified vocabulary to represent both text and image modalities, to disentangle modality-specific semantics, we employ two separate vocabularies for each modality. This design explicitly separates the two modalities, providing additional modalitylevel prior knowledge. As result, the model does not need to learn whether the output should be text or image, instead, it ensures the correct output modality by indexing the corresponding vocabulary. Our subsequent ablation experiments also confirm that OmniMambas dual-vocabulary design is one of the key factors for efficient training. 3.4. Task Specific LoRA To enhance the models adaptability to specific tasks, we introduce task-specific adapters. We hypothesize that explicitly parameterizing the selection in SSMs based on task can enhance the data efficiency of multimodal training [14]. Figure 3. The Mamba-2 block with task-specific LoRA. It is worth noting that while the Mamba-2 Block in the Mamba-2 paper has two input projectors, the actual code implementation separates the feature dimensions from single projector output. For simplicity, we depict only one input projector in our illustration. Our task-specific LoRA is applied to this entire input projector. However, JanusFlow [46] has shown that such unified encoder design is suboptimal. We believe this is primarily because multimodal understanding requires rich semantic representations for complex reasoning, whereas visual generation focuses on precisely encoding the spatial structure and texture of images. The inherent conflict between these two objectives suggests that unified encoder design may not be the optimal choice. Therefore, OmniMamba adopts decoupled vision encoder design. Figure 4. Training strategy of OmniMamba. The trainable components are indicated by flame symbol, while the frozen ones are represented by snowflakes. The dashed arrows indicate that this route is temporarily dropped and does not participate in model training. In OmniMamba, Specifically, to avoid introducing excessive parameters, we use LoRA [25] as the adapter. taskspecific LoRA is applied only to the input projection of each Mamba-2 layer, as illustrated in Fig 3. When performing specific task, the input linear projection and task-specific LoRA work together to effectively address the task. For instance, when the model performs multimodal understanding (MMU) task, the MMU LoRA route is activated, while the text-to-image (T2I) LoRA route is dropped. Explicitly activating the corresponding adapter to assist in task execution helps improve data efficiency in training [14, 63]. 3.5. Decoupled Training Strategy We propose decoupled two-stage training strategy to address data imbalance between understanding and generation tasks while improving training efficiency, as illustrated in Fig. 4. This approach consists of (1) Task-Specific Pre-Training stage for module-specific initialization and modality alignment, and (2) Unified Fine-Tuning stage for unified multi-task training. Decoupling Rationale The first stage separates multimodal understanding (MMU) and text-to-image (T2I) generation tasks to prioritize modality alignment without data ratio constraints. Unlike joint pre-training methods (e.g., JanusFlow [46] with fixed 50:50 MMU-T2I data ratio), our approach trains task-specific modules independently, enabling flexible dataset scaling (665K MMU vs. 83K T2I samples). Only randomly initialized componentslinear projection and MMU LoRA for understanding, T2I LoRA and image head for generationare trained, while the core Mamba-2 model remains frozen. This eliminates competition between tasks during early learning and allows asymmetric data utilization. Stage 1: Task-Specific Pre-Training It contains: MMU Pre-Training: Trains the linear projection and MMU LoRA to align visual-textual representations. The T2I LoRA path is disabled to isolate understanding-task learning. T2I Pre-Training: Optimizes the T2I LoRA and image decoder for visual synthesis. The MMU LoRA path is disabled to focus on generation capabilities. Stage 2: Unified Fine-Tuning Inspired by multi-task frameworks [30, 79], we freeze the visual encoder and train all other modules while preserving task-specific LoRA independence. During each forward pass: (1) MMU and T2I computations use their respective LoRA branches; (2) Losses from both tasks are summed for unified backward pass. This balances parameter sharing (via the frozen backbone) and task specialization (via isolated LoRA paths), enabling synergistic learning while mitigating interference between understanding and generation objectives. 3.6. Training Details Data Formats Following Show-o [67], we use special tokens to unify the data formats for both multimodal understanding and visual generation tasks. The multimodal understanding data is structured as: [MMU][SOI]{image tokens}[EOI][SOT]{text tokens}[EOT]. While the visual generation data is: [T2I][SOT]{text tokens}[EOT][SOI]{image tokens}[EOI]. Specifically, [MMU] and [T2I] is pre-defined task token used to guide the model in performing the corresponding task. [SOT] and [EOT] are used to represent the beginning and end of text tokens, respectively. Similarly, [SOI] and [EOI] represent the beginning and end of image tokens. Training Objective Since OmniMamba uses the autoregressive paradigm to handle both multimodal understanding and visual generation tasks, we only need to use the"
        },
        {
            "title": "LLM Params",
            "content": "Res. POPE MME-P VQAv2test GQA MMMU Und. Only"
        },
        {
            "title": "Unified",
            "content": "LLaVA-Phi [79] LLaVA [43] Emu3-Chat [64] LLaVA-v1.5 [42] InstructBLIP [8] MobileVLM [6] MobileVLM-V2 [7] LLaVA-v1.5-Phi-1.5 [67] LWM [44] Chameleon [58] LaVIT [29] Emu3 [64] Janus [66] JanusFlow [46] Show-o [67] OmniMamba Phi-2-2.7B Vicuna-7B 8B from scratch Vicuna-13B Vicuna-13B 336 224 512 448 224 MobileLLaMA-1.4B 336 MobileLLaMA-1.4B 336 336 Phi-1.5-1.3B LLaMA2-7B 7B from scratch 7B from scratch 8B from scratch 256 512 256 512 DeepSeek-LLM-1.3B 384 DeepSeek-LLM-1.3B 384 512 384 Phi-1.5-1.3B Mamba-2-1.3B 85.0 76.3 85.2 86.3 78.9 84.5 84.3 84.1 75.2 - - 85.2 87.0 88.0 80.0 86.3 1335.1 809.6 - 1500.1 1212.8 1196.2 1302.8 1128.0 - - - 1243.8 1338.0 1333.1 1097.2 1290. 71.4 - 75.1 81.8 - - - 75.3 55.8 - 66.0 75.1 77.3 79.8 69.4 77.7 - - 60.3 64.7 49.5 56.1 59.3 56.5 44.8 - 46.8 60.3 59.1 60.3 58.0 60.8 - - 31.6 - - - - 30.7 - 22.4 - 31.6 30.5 29.3 26.7 30. Table 1. Comparison with other methods on multimodal understanding benchmarks. Und. only refers to models that only perform multimodal understanding task, while Unified refers to models that unify both multimodal understanding and visual generation tasks. Models with similar number of parameters to ours are highlighted in light blue for emphasis. standard cross-entropy loss for next-token prediction during training. 4. Experiment 4.1. Data To achieve the goal of data efficiency, we aim to train OmniMamba using as few high-quality image-text pairs as possible. Multimodal Understanding Data In the first pretrain stage, the training data consists of 676K image-text pairs, all of which are sourced from publicly available datasets. These includes 118K images from COCO [39] and 558K images from LLaVA-1.5 pre-training data [42]. In the second fine-tune stage, we also exclusively use publicly available datasets follow Cobra [74]. 1. The mixed dataset used in LLaVA-1.5 [42] consists of 665K visual multi-turn conversations. 2. LVIS-Instruct-4V [62], which comprising 220K images accompanied by visually aligned and context-aware instructions generated by GPT-4V. 3. LRV-Instruct instruction [41], large-scale visual dataset of 400K samples, designed to mitigate hallucination issues across 16 vision-and-language tasks. Visual Generation Data To facilitate better reproducibility and further exploration by the community, we using only 83K images from the MS-COCO 2014 dataset [39] for textto-image generation training. Overall, the training data for our unified multimodal generation model consists of fewer than 2 million image-text pairs. In contrast to previous works that rely on over 100M or even over 1B pairs, our approach is highly training efficient. 4.2. Implementation Details Our core model is based on Mamba-2-1.3B, which consists of 48 layers of Mamba-2 blocks. In our primary experiments, the input image resolution for the multimodal understanding task is 384, while the image resolution for the visual generation task is 256. For multimodal understanding, we combine DINOv2 [48] and SigLIP [73] as the image encoder, while for visual generation, we use the VQVAE trained by LlamaGen [56] as the image encoder. We incorporate task-specific LoRA into the input projector of each Mamba-2 block and set the LoRA rank to 8, which results in only 0.65% increase in parameters. All training stages use the AdamW optimizer with β1 set to 0.9 and β2 set to 0.95. We adopt cosine annealing with warm-up as the learning rate schedule. Weight decay is set to 0, and gradient clipping is applied with threshold of 1.0. Other detailed hyper-parameters are shown in the appendix. All of our training is conducted on NVIDIA A800 GPUs with BF16 precision. 4.3. Quantitative Results Multimodal Understanding We evaluate OmniMambas multimodal understanding capabilities on wide range including POPE [37], of vision-language benchmarks,"
        },
        {
            "title": "Params",
            "content": "Images FID-30K Gen. Only"
        },
        {
            "title": "Unified",
            "content": "DALLE [53] GLIDE [47] DALLE 2 [52] SDv1.5 [54] PixArt [3] Imagen [55] Parti [69] Re-Imagen [4] U-ViT [1] CoDI [57] SEED-X [20] LWM [44] DreamLLM [13] Show-o [67] OmniMamba 250M 250M 650M 2000M 25M 960M 4.8B 50M 12B 5B 6.5B 0.9B 0.6B 7B 20B 2.5B 45M 83k(coco) 400M - - - 35M - 17B 7B 7B 1.3B 1.3B 83k(coco) 27.5 12.24 10.39 9.62 7.32 7.27 7.23 6.88 5.95 22.26 14.99 12.68 8.76 9.24 5.50 Table 2. Compare visual generation capability with other methods on MS-COCO validation dataset. Gen. only refers to models that only perform visual generation task, while Unified refers to models that unify both multimodal understanding and visual generation tasks. Model Genavg (Image/s) otal (s) Show-o [67] JanusFlow [46] OmniMamba 0.81 1.02 5.68 19.66 15.64 2.81 Table 3. Image Generation Speed in Visual Generation Task. OmniMamba achieves 7.0 faster image generation speed compared to Show-o and 5.6 faster compared to JanusFlow. MME [17], GQA [28], MMMU [72]. The results are shown in Tab 1. Compared to models with similar number of parameters, OmniMamba surpasses understandingspecific models such as LLaVA-v1.5-Phi-1.5 [67], MobileVLM [6], and MobileVLMv2 [7]. It also outperforms the unified understanding and generation model Showo [67] and achieves competitive performance compare to the state-of-the-art unified model JanusFlow [46]. Notably, while Show-o utilizes 2B image-text pairs and JanusFlow leverages over 65M image-text pairs, OmniMamba achieves competitive performance by using only 2M imagetext pairs for training. Visual Generation We evaluate OmniMamba for text-toimage generation on the widely recognized MS-COCO [39] benchmark dataset. To quantify image quality, we report the FID score [23]. Consistent with previous literature, we randomly select 30K prompts from the MS-COCO validation set and generate corresponding images to compute the FID score. The results are shown in Tab 2, where our model achieves the best visual generation performance on the MSCOCO validation dataset. Notably, models such as Showo [67] and PixArt [69] are trained on external large-scale datasets and further fine-tuned on COCO-like datasets (e.g., OpenImages [32]) before evaluating zero-shot on the MSCOCO validation set. In contrast, to avoid introducing excessive additional data, both our model and U-ViT [1] are trained solely on the MS-COCO training set and evaluated on the MS-COCO validation set. 4.4. Qualitative Results We present qualitative evaluations of our OmniMamba for both multimodal understanding and visual generation tasks. Fig 5 showcases our models capabilities in scene description and text-guided generation. Additional visualization results can be found in the appendix. 4.5. Inference Speed and GPU Memory usage We compared the generation speed and GPU memory usage of OmniMamba with other Transformer-based models in both multimodal understanding and visual generation tasks. All the evaluations were done on the same single NVIDIA 4090 GPU with FP16 precision. In multimodal understanding task, all models received the same example image. We used the same prompt, Please describe the image in detail. and removed the token generation limit to test their generation speed. The results are shown in the Fig 1. OmniMamba demonstrates 119.2 speedup at sequence length of 16k, and saves 63.0% GPU memory at sequence length of 23k compared to Show-o-256. Meanwhile, at sequence length of 100k, OmniMamba achieves 10.2 speedup and 40.4% GPU memory savings compared to JanusFlow-384, which is accelerated by FlashAttention-2 [9]. Notably, Show-o-256 indicates that the input image resolution is 256. Due to the design of its omni-attention mechanism being incompatible with FlashAttention-2, it was not used during testing. Similarly, JanusFlow-384 represents an input image resolution of 384, with FlashAttention-2 applied during testing for acceleration. In visual generation task, we used the same prompt, Picture. The models generate images with batch size of 16 and resolution of 256. As shown in Tab. 3, our model achieves image generation speeds that are 7.0 faster than Show-o and 5.6 faster than JanusFlow. 4.6. Ablation Studies We conducted series of ablation studies to verify the effectiveness of each design in OmniMamba. In this section, all ablation studies are conducted based on Mamba2-370M, with the understanding visual encoder replaced by CLIP [51], an input resolution of 224, and reduced number 7 Figure 5. Qualitative results of OmniMamba on multimodal understanding and visual generation. #Exp Decoupling Vocabularies Task Specific LoRA POPE MME GQA FID-30K 1 2 3 80.8 81.2 81.9 1036 1003 53.6 54.0 55.3 19.1 14.4 10.3 Table 4. Ablation studies on decoupling Vocabularies and task specific LoRA in OmniMamba of training steps (kept consistent across all ablation experiments), while keeping all other settings unchanged. Impact of Decoupling Vocabulary for the Two Tasks To guide the model in generating specific modalities from structural design perspective, we employ modalitydecoupled vocabularies in the default OmniMamba. We conducted ablation studies on this design. As shown in Tab 4, Exp1 and Exp3 utilize modality-unified vocabulary and modality-decoupled vocabularies, respectively. The results demonstrate that the decoupled vocabularies enable more efficient model training and yield better performance. Notably, when using the modality-unified vocabulary for visual generation, the model occasionally produces textrelated tokens, which requires additional post-processing to ensure correct visual generation. This further indicates that the model needs additional training to effectively learn modality-specific generation. Impact of Task Specific Adapter We conducted ablation studies on the introduced task-specific adapter module, and the results are shown in Tab 4. Exp2 and Exp3 represent the model without and with the task-specific adapter, respectively. The experiments demonstrate that the task-specific adapter helps the model efficiently learn both multimodal understanding and visual generation with minimal amount of training image-text pair data (2M). 5. Conclusion We presented OmniMamba, the first Mamba-2-based unified multimodal understanding and visual generation framework that achieves competitive performance with remarkable inference and training efficiency. By introducing three key innovations: decoupled vocabularies to disentangle modality-specific semantics, task-specific LoRA modules for parameter-efficient adaptation, and two-stage decoupled training strategy to resolve data imbalance, OmniMamba achieves comparable performance with JanusFlow and even surpasses Show-o using only 2M image-text pairs for training. Moreover, OmniMamba exhibits outstanding inference efficiency, It achieves 119.2 speedup with sequence length of 16k and 63% reduction in GPU memory at sequence length of 23k, compared to Show-o. With 8 sequence length of 100k, it delivers 10.2 speedup and saves 40.4% of GPU memory compared to JanusFlow. These results validate that our proposed OmniMamba is both training and inference efficient, with the potential to enable more ordinary researchers to participate in the wave of unified model innovation. However, due to the limited scale of training data, our models performance remains slightly below SOTA methods. Exploring the trade-off between training data volume and model performance will be key focus of our future work."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 7 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 7 [4] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022. 7 [5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling Journal of Machine Learning Research, with pathways. 24(240):1113, 2023. 2 [6] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. 6, [7] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. 6, 7 [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 6 [9] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 7 [10] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. 2, 9 [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 4 [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [13] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 3, 7 [14] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et al. Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. arXiv preprint arXiv:2312.09979, 4(7), 2023. 4, 5 [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3 [17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 7 [18] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. 3 [19] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. [20] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3, 7 [21] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. [22] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 12 [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 5 [26] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: dit-style zigzag mamba diffusion model. In European Conference on Computer Vision, pages 148 166. Springer, 2024. [27] Wenjun Huang, Jiakai Pan, Jiahao Tang, Yanyu Ding, Yifei Xing, Yuhe Wang, Zhengzhuo Wang, and Jianguo Hu. Mlmamba: Efficient multi-modal large language model utilizing mamba-2. arXiv preprint arXiv:2407.19832, 2024. 3 [28] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 7 [29] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, et al. Unified language-vision pretraining in llm with dynamic arXiv preprint discrete visual tokenization. arxiv 2024. arXiv:2309.04669. 6 [30] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprint arXiv:2402.07865, 2024. 4, 5 [31] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36, 2024. 2 [32] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 7 [33] LAION. Laion-coco 600m. https://laion.ai/ blog/laion-coco, 2022. [34] Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, and Guoqi Li. Scalable autoregressive image generation with mamba. arXiv preprint arXiv:2408.12245, 2024. 3 [35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 3 [37] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 6 [38] Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, and Xinggang Wang. Multimodal mamba: Decoder-only multimodal state space model via quadratic to linear distillation. arXiv preprint arXiv:2502.13145, 2025. [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2, 6, 7 [40] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [41] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large In The multi-modal models via robust instruction tuning. Twelfth International Conference on Learning Representations, 2023. 6 [42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 6 [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 3, 6 [44] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv e-prints, pages arXiv2402, 2024. 6, 7 [45] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan Liu. Vmamba: Visual state space model, 2024. [46] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. 2, 3, 4, 5, 6, 7 [47] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 7 [48] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4, 6 [49] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 [50] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. 3 [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya 10 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7 [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 7 [53] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 7 [54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 7 [55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2, 7 [56] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 3, 4, [57] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36, 2024. 7 [58] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 6 [59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [61] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [62] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. 6 [63] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang One-peace: Exploring one general representaZhou. arXiv preprint tion model toward unlimited modalities. arXiv:2305.11172, 2023. 5 [64] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, 3, 6 [65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 12 [66] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 2, [67] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2, 3, 5, 6, 7 [68] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Gated linear attention transarXiv preprint Panda, and Yoon Kim. formers with hardware-efficient training. arXiv:2312.06635, 2023. 3 [69] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 7 [70] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 3 [71] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [72] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 7 [73] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 4, 6 [74] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. 3, 6 [75] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. 2 [76] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 11 [77] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2, [78] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. 3 [79] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. Llava-phi: Efficient multi-modal assistant with small In Proceedings of the 1st International language model. Workshop on Efficient Multimedia Computing under Limited, pages 1822, 2024. 3, 5,"
        },
        {
            "title": "Appendix",
            "content": "A. Training Details The detailed training hyper-parameters are listed in Tab 5. The first stage separates multimodal understanding (MMU) and text-to-image (T2I) generation tasks to prioritize modality alignment without data ratio constraints. To this end, we employ larger learning rate during the first pretraining stage and smaller learning rate in the second finetuning stage. The batch size ratio represents the proportion between multimodal understanding data and visual generation data. All training is conducted on NVIDIA A800 GPUs using BF16 precision. of up to 2048 tokens, limiting its ability to handle ultra-long sequences and hindering its extension to advanced techniques such as Chain-of-Thought (CoT) [65] or reinforcement learning [22]. Enhancing Mamba-2s foundational capabilities and its capacity to model ultra-long sequences will be critical areas for future investigation. C. Additional Qualitative Results C.1. Multimodal Understanding We validate the multimodal understanding capabilities of our OmniMamba and other approaches across three aspects: scene description, spatial reasoning, and counting with tricky questions, which are shown in Table 68. Spatial Reasoning: User Is the bicycle parked to the right of the dog in this image? OmniMamba The bicycle is actually parked to the left of the dog, not to the right. Show-o Yes, the bicycle is parked to the left of the dog. Stage 1: MMU Stage 1: T2I Stage 2: Unify JanusFlow No. Learning Rate Warm-up Steps Training Steps Batch Size 1e-3 100 5k 256: 8e-4 1000 100k 0:720 1e-4 0 150k 3:48 Table 6. OmniMamba can accurately determine spatial positions, JanusFlow made the correct judgment, but its response was overly concise, while Show-o lacks spatial reasoning capabilities, as indicated by the red text. Table 5. Hyper-parameters of OmniMamba. The batch size ratio refers to the proportion between multimodal understanding data and visual generation data. Counting with Tricky Questions: B. Limitations Although our OmniMamba achieves promising results with very small amount of data, the limited data volume still renders our model suboptimal. Furthermore, unlike previous works that leverage large-scale, high-quality datasets such as LAION-aesthetics, we rely solely on the MS-COCO dataset for visual generation. As result, the quality of generated images, particularly for human faces, remains less refined. Exploring the trade-off between dataset scale and model performance will be key focus of our future work. Additionally, while Mamba-2 demonstrates exceptional its foundational capabilities remain inference efficiency, weaker compared to the extensively studied Transformer. Furthermore, Mamba-2 has only been trained on sequences 12 User The animals in the picture look like cows. How many are there? OmniMamba There are three goats in the image, not cows. Show-o There are three cows in the image. JanusFlow There are three animals in the picture. Table 7. OmniMamba correctly outputs the quantity while avoiding the users trap question, whereas Show-o, despite counting correctly, was misled by the question, as indicated by the red text. On the other hand, JanusFlows response was not accurate enough. Scene Description: User Please describe it in detail. OmniMamba The image depicts serene scene of cat and dog sharing peaceful moment together. The cat, exhibiting gray and white coat, is curled up comfortably, its head resting on the dogs body. The dog, with its brown fur, is curled up in similar position, with its head resting on the cats back. Their eyes are gently closed, signifying deep sleep. The background is blurred white, allowing the focus to remain on the sleeping feline and its canine friend. The image exudes warmth and affection, showcasing the close bond between the two animals. Show-o The image features white and brown cat lying on bed, curled up and sleeping. The cat is resting its head on stuffed animal, which is teddy bear. The teddy bear is positioned on the right side of the cat, with its head resting on the cats back. The scene captures peaceful and cozy moment between the cat and the stuffed animal. Table 8. OmniMamba can accurately describe the information in the scene, whereas Show-o made mistake about the color of the cat and misidentified the dog as teddy bear, as indicated by the red text. C.2. Visual Generation We provide additional visualization results of visual generation to further validate our generation capabilities, as shown in Fig 6. 13 Figure 6. Qualitative results of OmniMamba visual generation. Prompts are randomly drawn from the MS-COCO validation set."
        }
    ],
    "affiliations": [
        "Horizon Robotics",
        "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
        "School of EIC, Huazhong University of Science & Technology"
    ]
}