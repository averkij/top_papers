{
    "paper_title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
    "authors": [
        "Hongchen Wei",
        "Zhenzhong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long video understanding from the perspective of context windows, aiming to apply LMMs to long video tasks without retraining on long video datasets. We first conduct an in-depth analysis of why pretrained LMMs struggle to understand lengthy video content, identifying that discrepancies between visual and language modalities lead to different context windows for visual and language tokens, making it difficult to directly extend the visual tokens to match the language context window. Based on this, we propose to adapt LMMs for long video understanding tasks by extending the visual context window, eliminating the need for retraining on large scalelong video datasets. To further mitigate the significant memory consumption caused by long sequences, we introduce a progressive pooling inference strategy that selectively adjusts the spatial resolution of frame embeddings, reducing the number of visual tokens while retaining important spatial information. Across multiple long video understanding benchmarks, our method consistently improves the performance as the number of video frames increases. On the MLVU benchmark, our method outperforms GPT-4o, even though our model size is only 7B. Additionally, in the 256-frame setting, our method reduces memory usage by approximately 45% compared to the baseline, without introducing any performance loss."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 2 8 1 0 0 2 . 9 0 4 2 : r VISUAL CONTEXT WINDOW EXTENSION: NEW PERSPECTIVE FOR LONG VIDEO UNDERSTANDING Hongchen Wei and Zhenzhong Chen School of Remote Sensing and Information Engineering, Wuhan University"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long video understanding from the perspective of context windows, aiming to apply LMMs to long video tasks without retraining on long video datasets. We first conduct an in-depth analysis of why pretrained LMMs struggle to understand lengthy video content, identifying that discrepancies between visual and language modalities lead to different context windows for visual and language tokens, making it difficult to directly extend the visual tokens to match the language context window. Based on this, we propose to adapt LMMs for long video understanding tasks by extending the visual context window, eliminating the need for retraining on large-scale long video datasets. To further mitigate the significant memory consumption caused by long sequences, we introduce progressive pooling inference strategy that selectively adjusts the spatial resolution of frame embeddings, reducing the number of visual tokens while retaining important spatial information. Across multiple long video understanding benchmarks, our method consistently improves the performance as the number of video frames increases. On the MLVU benchmark, our method outperforms GPT-4o, even though our model size is only 7B. Additionally, in the 256-frame setting, our method reduces memory usage by approximately 45% compared to the baseline, without introducing any performance loss. Project page: https://hcwei13.github.io/Visual-Context-Window-Extension/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Multimodal Models (LMMs), built on pre-trained Large Language Models (LLMs) and trained on massive image-text pairs, have shown remarkable capabilities in image understanding (Li et al., 2023b; Gao et al., 2023; Dai et al., 2023; Zhu et al., 2023; Ye et al., 2023; Li et al., 2023a; Liu et al., 2023a). Recently, by segmenting high-resolution images into multiple sub-images for input, LMMs have not only improved in fine-grained image understanding but also demonstrated zero-shot video understanding capabilities (Liu et al., 2024b; Yao et al., 2024; Li et al., 2024a). Despite these advancements, current LMMs are still limited to short video understanding tasks and face difficulties when applied to long videos due to the excessive sequence lengths involved. Several approaches (Li et al., 2023d; Jin et al., 2023; Song et al., 2024) have explored using visual resamplers to reduce the number of visual tokens, allowing the models to process more video frames. However, this token reduction inevitably leads to loss of critical information, negatively affecting performance. Recent efforts (Xue et al., 2024; Liu et al., 2024c) have tackled this issue by incorporating long video-text pair datasets during pre-training. However, this approach faces significant challenges due to the high computational cost associated with the quadratic complexity of the attention mechanism (Vaswani et al., 2017) and the scarcity of high-quality long video-text data. Corresponding author. 1 (a) (b) Figure 1: (a) The blue curve of illustrates the accuracy comparison of different video sequence lengths on LongVideoBench (180s-600s) (Wu et al., 2024). The yellow curve shows the sliding window perplexity (S = 256) of ten 128k Proof-pile documents, and for the sake of comparison, we take the negative of the perplexity. Visualization of visual embeddings (output of the modality projection layer) and language embeddings in the language decoder using t-SNE. The visual embeddings and language embeddings form two distinct clusters. To alleviate the high computational costs and data collection challenges associated with long video understanding, we approach the problem from the perspective of the context window. First, we observe that in recent open-source LMMs, language decoders generally support longer language modeling (Yao et al., 2024; Li et al., 2024a). For instance, the latest LMM, LLaVA-OneVision (Li et al., 2024a), employs Qwen2 (Yang et al., 2024) as its language decoder. As illustrated in Figure 1a, the performance of LLaVA-OneVision in language understanding tasks improves consistently as the input sequence length increases (yellow curve). However, for visual understanding tasks, the performance initially improves but then declines as sequence length grows (blue curve). Further visualization of the latent space inside the language decoder shows that visual and language embeddings form distinct clusters (Figure 1b), indicating significant modal differences in the latent space. This explains the performance of LMMs on visual understanding tasks shown in Figure 1a. We believe that due to the differences between the visual and language modalities, LMMs pre-trained on short visual sequences cannot directly extrapolate visual tokens to the effective context window size of the language decoder. Therefore, we redefine the context window in LMMs as two distinct windows: the visual context window, representing the maximum length of visual tokens during pre-training, and the language context window, referring to the maximum length of language tokens during pre-training. Building on this observation, we propose to extend the commonly used language context window extension method, YaRN (Peng et al., 2024), to LMMs for long video understanding. Specifically, we redefine the scaling factor of the base frequency in positional embeddings as the ratio of the visual context window to the target context window. By modulating the rotational frequency of the positional embeddings, we expand the effective range of the visual context window, enabling LMMs to handle longer video sequences. Additionally, to alleviate the rapid memory consumption caused by long sequences, we propose progressive pooling strategy to handle video frame embeddings. Specifically, considering the redundancy between consecutive frames in the same eventsuch as static backgroundwe uniformly sample the video frames into multiple groups. We assume that each group represents an event, and In each group, the first frames embedding we control the group size through hyperparameters. retains higher spatial resolution, while the subsequent frames are pooled with larger stride to lower resolutions. We believe the first frame preserves rich spatial, fine-grained information, while the remaining frames reduce intra-group redundancy. This approach minimizes the loss of spatial information while reducing the number of visual tokens. Across multiple long video understanding benchmarks, our method consistently improves performance as the number of video frames increases. Notably, on the MLVU benchmark (Zhou et al., 2024), our method outperforms GPT-4o. Most importantly, our approach does not require retraining, allowing it to benefit from continuous advancements in open-source LMMs. In summary, our paper makes the following key contributions: We exploit the modality difference between visual and language tokens in the language decoder to redefine the effective context window in LMMs: the visual context window and the language context window. We propose method to extend positional embeddings within the visual context window, enabling LMMs to handle long video tasks without the need for training on long video-text paired data. We introduce progressive pooling strategy for visual frame embeddings, mitigating reducing memory consumption in long video sequences."
        },
        {
            "title": "2.1 ROTARY POSITION EMBEDDINGS",
            "content": "Rotary Position Embeddings (RoPE) (Su et al., 2024) introduce rotation matrix to incorporate relative positional information into the self-attention mechanism, enhancing the models ability to capture positional relationships between words. Given sequence = {wi}N i=1, the query and key vectors are computed as: qm = fq (xm, m), kn = fk (xn, n), where and are positions in the sequence. The unnormalized attention scores are then calculated by dot-producting two vectors: qT mkn. To incorporate relative positional information, the query and key vectors are represented in complex form: i=1 with corresponding embeddings = {xi}N fq (xm, m) = eimΘ (Wqxm) , fk (xn, n) = einΘ (Wkxn) , (1) where Θ = diag (cid:0)θj = b2j/d, [1, 2, . . . , d/2](cid:1) and = 10000 is the diagonal matrix. In real coordinates, RoPE can be expressed using the following function: fq (xm, m) = Rm (Wqxm) = cos mθ1 sin mθ1 cos mθ1 sin mθ1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 cos mθd/2 sin mθd/2 cos mθd/2 sin mθd/2 Wqxm. (2) Therefore, when the word embedding xm at position is multiplied by matrix Rm, and the word embedding xn at position is also multiplied by matrix Rn, resulting in the transformed query and key vectors, the attention weights will inherently include the relative positional information. We provide more detailed derivation of RoPE in Appendix A.1. 2.2 RELATED WORK Large Multimodal Models LMMs typically consist of visual encoder, pre-trained LLM, and modality projection module that converts visual content into token sequences for the LLM. Leveraging large amounts of high-quality image-text paired data, LMMs have shown strong capabilities in image understanding (Li et al., 2023b; Gao et al., 2023; Dai et al., 2023; Zhu et al., 2023; Ye et al., 2023; Li et al., 2023a; Liu et al., 2023a; 2024b; Yao et al., 2024; Li et al., 2024a). By sampling videos into multiple frames, LMMs can extend to video understanding tasks (Xu et al., 2024; Chen et al., 2023a; Maaz et al., 2024; Liu et al., 2023b; Li et al., 2023c; 2024b). Examples include VideoChatGPT (Maaz et al., 2024), VideoChat2 (Li et al., 2024b), and PLLaVA (Xu et al., 2024), which enhance LMMs video understanding through high-quality data and fine-tuning methods. However, these methods face challenges with long videos due to the large number of visual tokens generated per frame. To address this, visual token compression methods have been proposed (Li et al., 2023d; Jin et al., 2023; Song et al., 2024). For instance, LLaMA-VID (Li et al., 2023d) uses only two tokens per 3 Figure 2: Examples of RoPE embeddings under different context extension methods. Upper: RoPE directly extrapolated beyond the pre-training range. Middle: YaRN interpolating and extrapolating different RoPE dimensions beyond the pre-training range. Down: Our method further distinguishes between visual and language context windows in YaRN, allowing for different interpolation and extrapolation of RoPE dimensions. frame, and MovieChat (Song et al., 2024) introduces memory mechanism to compress long video tokens into fixed size. These methods, however, often result in information loss. In recent work, LongVILA (Xue et al., 2024) attempted to introduce long video-text pairs into the training of LMMs to expand the context window size. LongVA (Zhang et al., 2024) expands the context window by continuously training LLMs on long texts, transferring its long text understanding capabilities to long video understanding. However, they inevitably introduce high computational costs and data collection challenges. Context Window Extension for LLMs The fixed context length during pre-training limits the inference performance of language models in scenarios involving long sequence inputs. To address this issue, researchers have proposed series of RoPE-based language positional embedding extension methods, such as Position Interpolation (PI) (Chen et al., 2023b; kaiokendev, 2023), NTK Interpolation (bloc97, 2023), and YaRN (Peng et al., 2024). Specifically, PI scales the positions of long texts that exceed the context window down to the original window size. However, it compresses distances between nearby tokens, which can degrade performance. NTK interpolation extends the context window by adjusting the rotational speed of RoPE through reducing the base frequency. Building upon NTK interpolation, YaRN further distinguishes between high-frequency and low-frequency information to accommodate different RoPE embeddings."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we first introduce the corresponding modifications of the language position embedding extension method to the visual context window. We then further discussed another factor that limit long video understanding: memory constraints. 3.1 VISUAL CONTEXT WINDOW EXTENSION In Section 2.1, we describe the commonly used position embedding method in LLMs and LMMs, RoPE (Rotary Position Embedding). LLMs typically have fixed context window size, and when the input sequence exceeds this limit, the model struggles to accurately understand positional information, leading to decline in performance. As shown in Figure 1a, LMMs encounter similar issues when processing long video sequences. To address this, we adapt the language position embedding extension method, YaRN (Peng et al., 2024), for the visual context window to better support long video understanding. Figure 2 illustrates an example of our method. In our approach, we define the training context length for visual data 4 Figure 3: Pipeline of progressive pooling strategy. train (i.e., visual context window), and the extended context length as Lv as Lv define the scaling factor as follows: test. Consequently, we = . (3) test Lv Lv train Then, we selectively interpolate the hidden dimensions based on the wavelength λi of the RoPE embeddings: λi = 2π θi = 2πb 2i . (4) Following this, we define ri = Lv to determine which dimensions require interpolation. Finally, train λi following YaRN, combining the scaling factor with the wavelength λi, the base frequency is modified as follows: θnew = (cid:20) γi + (1 γi) (cid:21) 1 θi, γi = 1, 0, riα βα , ri > β ri < α otherwise, (5) where, α and β are hyperparameters. When ri < α, we apply linear interpolation proportionally based on s. When ri > β, no interpolation is applied. For cases between α and β, we apply linear interpolation transition. We provide detailed derivations of context window extension method in Appendix A.2. It is important to note that our modifications to YaRN are minimal (only redefining the scaling factor s), ensuring simplicity and compatibility with various acceleration techniques, such as flash-attention (Dao et al., 2022). 3.2 PROGRESSIVE POOLING In this section, we discuss another factor that limits the performance of long video understanding: memory constraints. Taking LLaVA-OneVision as an example, given video uniformly sampled into video frames, the visual encoder and multimodal projection module process these frames to obtain the video sequence embeddings Fv RN 729d. To reduce the number of visual tokens, LLaVA-OneVision performs bilinear pooling with stride of 2 on each video frame embedding, which then serves as the input to the LLM decoder. However, even after bilinear pooling, video sequence of 256 frames generates 50,176 tokens. Long sequences contribute to high memory consumption. Inference in LMMs can be divided into two stages: prefill and decoding. During the prefill stage, all visual tokens are projected into high-dimensional space and stored as KVCache for efficient decoding later. This incurs substantial memory costs. Even with bilinear pooling, processing 256 frames generates 50,176 tokens, requiring approximately 73 GB of GPU memory. This greatly limits the deployment of LMMs for long video understanding. To alleviate excessive memory consumption, we propose progressive pooling strategy. As shown in Figure 3, we first uniformly divide the video sequence embeddings Fv into multiple groups, with division stride defined as K. We assume that each group represents an event. Considering the 5 Table 1: Performance evaluation on VideoMME (Fu et al., 2024) benchmark. * indicates the results of reproduction. Methods Frames Short Medium Long Overall Qwen-VL-Chat-7B (Bai et al., 2023) VideoLLaVA-7B (Lin et al., 2023) VideoChat2-Mistral-7B (Li et al., 2024b) VideoLLaMA2-7B (Cheng et al., 2024) LLaVA-NeXT-Qwen2-7B (Liu et al., 2024b) LLaVA-OneVision-7B* (Li et al., 2024a) Chat-UniVi-V1.5-7B (Jin et al., 2024) ST-LLM-7B (Liu et al., 2024d) LongVA-7B (Zhang et al., 2024) LongVILA-8B (Xue et al., 2024) Ours 4 8 16 16 32 32 64 64 128 256 256 512 46.9 45.3 48.3 56.0 58.0 69.3 45.7 45.7 61.1 61. 72.7 71.9 38.7 38.0 37.0 45.4 47.0 55.1 40.3 36.8 50.4 49.7 58.2 58.7 37.8 36.2 33.2 42.1 43.4 49.7 35.8 31.3 46.2 39.7 52.9 51.3 41.1 39.9 39.5 47.9 49.5 58.2 40.6 37.9 52.6 50. 61.3 60.6 redundancy between consecutive frames in the same event, such as static background, we retain only the first frame of each group at higher spatial resolution. The remaining frames within each group are stored at lower spatial resolution using larger pooling stride. Specifically, the video sequence embeddings Fv are divided into multiple groups, each containing frames, resulting in total of = groups.: w=1. In each group, the first frame Fv,w,1 is retained at high resolution: i=1 {{Fv,w,j}K {Fv,i}N j=1}M (6) v,w,1 = Pool(Fv,w,1, stride = sh). The remaining frames are pooled at lower resolution with larger stride sl (sh<sl), resulting in: v,w,j = Pool(Fv,w,j, stride = sl)}K {F low-res j=2. (7) (8) high-res Finally, the processed frames are reassembled into new video sequence embedding new v,w,1 , low-res = {{F high-res new v,w,2 , . . . , low-res v,w,K }M w=1}, : (9) where Pool(, stride) represents the pooling operation with the specified stride. The progressive pooling strategy significantly reduces the number of visual tokens while preserving the integrity of spatial information."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETTING We evaluate the long video understanding capabilities of our method on three key benchmarks: VideoMME (Fu et al., 2024), MLVU (Zhou et al., 2024), and LongVideoBench (Wu et al., 2024). VideoMME is widely used benchmark for assessing the ability of LMMs to handle long videos in real-world scenarios. It divides the test set into three subsets based on video length: short videos (< 2 minutes), medium-length videos (4 to 15 minutes), and long videos (30 to 60 minutes), with durations ranging from 11 seconds to 1 hour. MLVU offers diverse collection of video lengths, types, and evaluation tasks. It includes long video understanding tasks (TR: Topic Reasoning, AR: Anomaly Recognition), single-detail long video understanding tasks (NQA: Needle QA, ER: Ego Reasoning, PQA: Plot QA), and multi-detail long video understanding tasks (AO: Action Order, AC: Action Count). The benchmark includes videos of various types, such as movies, surveillance footage, egocentric videos, cartoons, and game videos, with lengths ranging from 3 minutes to over 2 hours. LongVideoBench focuses on long-span understanding, particularly on referential reasoning problems that depend on long-frame inputs and cannot be resolved using only single or sparse frames. It evaluates videos of varying lengths, including (8s, 15s], (15s, 60s], (180s, 600s], and (900s, 3600s]. Table 2: The overall performances on MLVU (Zhou et al., 2024). Two input strategies are used by the LMMs in evaluation: Uniform Sampling, which evenly samples frames from the video; Frame Rate Sampling (N fps), which samples frames per second. denotes proprietary models. Methods GPT-4o (OpenAI, 2024) LLaMA-VID-7B (Li et al., 2023d) LLaVA-1.6-7B (Liu et al., 2024b) InternVL-1.5-7B Chen et al. (2024b) LLaVA-OneVision-7B* (Li et al., 2024a) TimeChat-7B (Ren et al., 2024) LongVA-7B (Zhang et al., 2024) MovieChat-7B (Song et al., 2024) Ours Frames 0.5 fps 1 fps 16 16 32 96 256 2048 256 Holistic Single Detail Multi Detail M-Avg TR 87.4 50.8 60.6 78.8 88.6 23.1 83.3 29.5 87.5 87.1 AR 74.5 34.5 41.0 67.0 74.0 27.0 58.5 25.0 74.5 76. NQA 64.8 30.1 43.1 52.7 73.0 24.5 69.3 24.2 76.3 75.5 ER 57.1 32.7 38.4 43.5 62.2 28.4 50.0 24.7 65.3 65.3 PQA 65.1 32.5 41.0 54.4 67.9 25.8 67.2 25.8 75.9 76.1 AO 56.7 23.9 25.5 32.8 43.2 24.7 38.6 28.6 52.9 52.5 AC 46.3 27.8 25.7 23.8 28.6 32.0 27.2 22.8 31.6 37.4 64.6 33.2 39.3 50.4 64.2 30.9 56.3 25.8 68.6 69. Table 3: Performance evaluation on LongVideoBench (Wu et al., 2024) benchmark. Methods Frames LLaVA-1.5-13B (Liu et al., 2024a) LLaVA-Next-Mistral-7B (Liu et al., 2024b) VideoLLaVA-7B (Lin et al., 2023) VideoChat2-7B (Li et al., 2024b) LLaVA-Next-Video-34B (Liu et al., 2024b) PLLaVA-34B (Xu et al., 2024) LLaVA-OneVision-7B* (Li et al., 2024a) LongVA-7B (Zhang et al., 2024) Ours 8 8 8 8 8 8 32 256 256 Duration Group (s) (8, 15] 49.0 53.4 43.1 49.3 57.6 60.1 68.8 57.4 68.8 66.1 (15, 60] 51.1 57.2 44.6 49.3 61.6 66.8 70.4 60.4 69.2 67.4 (180, 600] 41.8 46.9 36.4 39.0 48.7 50.8 54.6 47.3 56.1 58.5 (900, 3600] 39.6 42.1 34.4 37.5 45.9 49.1 48.1 44.7 51.2 52.1 Avg 43.4 49.1 39.1 39.3 50.5 53.2 56.0 49.7 57.5 58.0 4.2 IMPLEMENTATION DETAILS To validate the effectiveness of our approach, we use the latest LMM, LLaVA-OneVision 7B, as the backbone and baseline model. This model employs classic multimodal encoder-decoder architecture, consisting of visual encoder (SigLIP (Zhai et al., 2023)), an LLM decoder (Qwen2), and multimodal projection module (MLP). For each video frame, the visual encoder and multimodal projection module encode the frame into video sequence embeddings Fv RN 729d. Through bilinear pooling with stride of 2, this is reduced to Fv RN 196d. Following the default settings in YaRN, we set the hyperparameters α and β (in Section 3.1) to 1 and 32, respectively. Previous research (Peng et al., 2024; Chen et al., 2023b; Ding et al., 2024) has shown that fine-tuning after interpolation enhances models ability to interpret scaled RoPE embeddings. Therefore, we compare the results of both tuning-free and fine-tuned approaches. Specifically, we randomly sample 10K instances from the allava instruction dataset (Chen et al., 2024a) and fine-tune the LLaVA-OneVision language decoder using LoRA (Hu et al., 2022), setting lora to 64 and lora α to 16. The learning rate is set to 1e 5 with batch size of 1. In our experiments, unless otherwise stated, we use the tuning-free model to present our results. The default parameters for the progressive pooling method are: division stride = 4, high-resolution pooling stride sh = 2, and low-resolution pooling stride sl = 8. 4.3 QUATITATIVE RESULTS Results on VideoMME Table 1 presents the results on the VideoMME benchmark. Compared to the baseline model, LLaVA-OneVision, our method shows consistent improvements across all intervals for short, medium, and long videos. Notably, for long videos, the accuracy improved by 7 Table 4: Performance evaluation of different context window extension methods on the VideoMME. Frames Short Medium Long Overall LLaVA-OneVision-7B LLaVA-OneVision-7B + YaRN Ours (Tuning-free) w/o progressive pooling Ours (Fine-tuning) w/o progressive pooling 256 256 256 64.9 67.6 71.6 71.9 53.3 56.3 59.1 60.2 50.4 51.7 52.2 53.2 56.2 58.5 61.0 61.8 Table 5: Ablation studies on the VideoMME benchmark, where all videos are uniformly sampled to 256 frames. Specifically, sh represents the high-resolution pooling stride for the first frame of each group; sl indicates the low-resolution pooling stride for the remaining frames within each group; and denotes the grouping stride, which refers to the number of frames within each group. (sh, sl), Memory (GB) Short Medium Long Overall (2, 2), 0 (4, 4), 0 (8, 8), 0 (2, 4), 4 (2, 8), 4 (2, 4), 8 (2, 8), 8 (2, 4), 16 (2, 8), 16 73 37 29 45 40 41 35 40 31 71.6 70.8 68.1 72.4 72.7 70.1 69.7 68.6 70.3 59.1 59.0 56.2 58.3 58.2 57.6 56.4 57.4 56.3 52.2 51.2 49.7 51.3 52.9 50.8 51.4 51.4 50. 61.0 60.3 58.0 60.7 61.3 59.5 59.2 59.1 59.1 In comparison to the latest long video understanding models, our approach continues to 3.2%. achieve optimal performance. For instance, compared to LongVILA-8B, which was pre-trained on long video-text pairs, our method demonstrates an improvement of 10.8%. Crucially, our method achieves these gains without requiring any pre-training or fine-tuning on long video-text pairs. Results on MLVU and LongVideoBench MLVU and LongVideoBench are two benchmarks specifically designed to evaluate long video understanding tasks. Table 2 presents the results on MLVU, where our method significantly outperforms all comparison models, even surpassing GPT4o. Table 3 provides the results on LongVideoBench, where test samples are categorized into various duration intervals to highlight different models performance in long video comprehension. Our method shows slight performance drop in the intervals (8, 15] and (15, 60] when sampling 512 frames compared to the baseline LLaVA-OneVision. This performance drop in shorter intervals can be attributed to the fact that dense frame sampling results in excessively long input sequences for shorter videos, which leads to attention distraction and degrades model performance. Using different frame sampling strategies for videos of varying durations can alleviate this issue. 4.4 ABLATION STUDIES To validate the effectiveness of the proposed module, we conducted experiments on VideoMME, focusing on visual context window extension and progressive pooling strategies. Visual Context Window Extension Table 4 presents the comparative results under the scenario of uniformly sampling 256 frames, including direct extrapolation, YaRN interpolation, and our method. It is noteworthy that all results in the table did not utilize the progressive pooling strategy. The results indicate that using YaRN interpolation improves model performance, confirming the effectiveness of positional interpolation. Our method, which applies interpolation on the visual context window, achieves significant performance enhancement compared to YaRN. Additionally, we fine-tuned the model using 10K image-text pairs after interpolation, further improving model performance. This aligns with the conclusions drawn from context window extension methods in LLMs. Progressive Pooling Table 5 presents the comparative results of different pooling strategies and progressive pooling parameters on VideoMME. It is important to note that all experiments in the table utilized visual context window extension. The upper half of the table displays the results of uniform pooling with pooling strides of 2 (the default pooling strategy of the baseline model), 4, and 8 Figure 4: Visualization of the Needle in the Long Video Haystack Experiment, where green represents correct answers, while red indicates incorrect answers. Left: progressive pooling parameters are set to sh = 2, sl = 8, = 4. Right: progressive pooling parameters are set to sh = 2, sl = 4, = 4. Our method enables LMMs, pre-trained on short videos (32 frames), to be extended to 1024 frames without requiring fine-tuning. 8. It is evident that as the pooling stride increases, memory consumption decreases gradually, but performance declines progressively. The lower half of the table shows the results of our proposed progressive pooling strategy. We conducted experiments with varying pooling strides and grouping strides, comparing performance under different parameters. The results indicate that the optimal performance occurs at sh = 2, sl = 8, and = 4. In this setting, compared to the baseline method (with uniform pooling stride of 2), our approach reduces memory usage by approximately 45% while achieving superior performance. This is because shorter sequence lengths mitigate the issue of attention distraction. Additionally, we found that the pooling stride has smaller impact on the model, while the grouping stride has significant effect. This may be due to larger grouping strides leading to greater intra-group scene variation, resulting in loss of spatial information. 4.5 VISUAL NEEDLE-IN-A-HAYSTACK As shown in Figure 4, we utilize V-NIAH (Zhang et al., 2024) to measure the models long-context capabilities. Probes are inserted at different positions within the video, and question-answering task is conducted; response is considered correct only when it matches the answer (indicated in green), otherwise, it is deemed incorrect (indicated in red). It is evident that our method demonstrates outstanding performance across different progressive pooling parameters, effectively extending the models visual context window to 1024 frames without requiring fine-tuning. 4.6 QUALITATIVE RESULTS It is evident that Figure 5 illustrates the qualitative results of our method in video captioning. LLaVA-OneVision-7B generates incorrect descriptions when the default input is set to 32 frames. When directly extrapolated to 256 frames, the model appears to forget information from the middle section of the video, only describing the beginning and the end. In contrast, our method generates accurate and detailed descriptions for the input video when 256 frames are provided. 4.7 CONCLUDING REMARKS In this paper, we address the long video understanding issue from the perspective of context windows, effectively avoiding the resource consumption associated with training from scratch. By redefining the effective context window of LMMs into visual and language context windows, we propose the visual context window extension. This approach allows LMMs trained on short videos to be applied to long video understanding tasks without fine-tuning. Additionally, we introduce progressive pooling strategy to mitigate memory consumption issues caused by long sequences. In 256-frame setting, this strategy reduces memory usage by approximately 45% without introducing any performance loss. We hope this work will advance research in long video understanding and provide insights for the design of future long video understanding models. 9 Figure 5: Qualitative results from different methods demonstrate that our approach exhibits accurate and detailed video captioning capabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) conURL text size without any fine-tuning and minimal perplexity degradation., 2023. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware scaled rope allows llama models to have/. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024a. Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023a. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023b. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024b. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. ArXiv, abs/2305.06500, 2023. 10 Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, New Orleans, LA, USA, 2022. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, Virtual, 2022. Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, pp. 1370013710, 2024. kaiokendev. Things Im learning while training superhot., 2023. URL https://kaiokendev. github.io/tilextending-context-to-8k. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. ArXiv, abs/2305.03726, 2023a. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. 2023b. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023c. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pp. 2219522206, 2024b. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023d. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pp. 2629626306, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024c. 11 Ruyang Liu, Chen Li, Yixiao Ge, Ying Shan, Thomas Li, and Ge Li. One for all: Video conversation is feasible without video instruction tuning. arXiv preprint arXiv:2309.15785, 2023b. Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In ECCV, 2024d. Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, pp. 12585 12602, Bangkok, Thailand, 2024. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In ICLR, Vienna, Austria, 2024. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In CVPR, pp. 1431314323, 2024. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, pp. 1822118232, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, In NeurIPS, pp. 59986008, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CA, USA, 2017. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024. Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. ArXiv, abs/2304.14178, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pp. 1194111952, Paris, France, 2023. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv, abs/2304.10592, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ROTARY POSITION EMBEDDINGS Rotational Position Embeddings (RoPE) (Su et al., 2024) introduce rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation, enabling the model to capture the relative positional relationships between words, thereby enhancing its performance in processing sequential data. Given sequence = {wi}N i=1, where represents the sequence length and wi represents the i-th word. Its corresponding word embeddings are = {xi}N i=1, where xi is the embedding of the i-th word. Before calculating attention, it is necessary to incorporate positional information into the word embeddings and transform them into the query vectors and the key vectors. qm = fq (xm, m) Rd, kn = fk (xn, n) Rd, where and represent different positions, respectively. Next, attention is computed using the query and key vectors. (10) softmax , (11) (cid:19) (cid:18) qT mkn where qm, kn are considered as column vectors so that qT mkn is simply the Euclidean inner product. To incorporate relative positional information, we express the inner product between the query and key vectors as function, denoted as g(). fq (xm, m) , fk (xn, n) = (xm, xn, n) . (12) For the function g(), it is evident that the inner product encodes positional information only in relative form (i.e., n). The next goal is to find an appropriate function g() that conforms to the aforementioned relation. Specifically, we first represent the query and key vectors in complex form. The representations of the query and key vectors are as follows: fq (xm, m) = eimΘ (Wqxm) , fk (xn, n) = einΘ (Wkxn) . (13) For the sake of clarity and ease of understanding in the subsequent formulas, where i2 = 1 is the imaginary unit and Θ = diag (cid:0)θj = b2j/d, [1, 2, . . . , d/2](cid:1) is the diagonal matrix. RoPE associates each (complex-valued) hidden neuron with distinct frequency θj. The benefit of this approach is that the dot product between the query and key vectors depends only on the relative distance n. This process is represented by the following formula: fq (xm, m) , fk (xn, n) = (cid:10)eimΘ (Wqxm) , einΘ (Wkxn)(cid:11) (cid:16) = Re eiΘ(mn)x mW qWkxn (cid:17) (14) =g (xm, xn, n) , where Re() is the real part of complex number and () represents the conjugate complex number of (). According to Eulers formula, ei(mn)Θ = cos((m n)Θ) + sin((m n)Θ). In real coordinates, RoPE can be expressed using the following function: fq (xm, m) = Rm (Wqxm) = cos mθ1 sin mθ1 cos mθ1 sin mθ1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 cos mθd/2 sin mθd/2 cos mθd/2 sin mθd/2 Wqxm. 13 (15) (16) Therefore, when the word embedding xm at position is multiplied by matrix Rm, and the word embedding xn at position is also multiplied by matrix Rn, resulting in the transformed query and key vectors, the attention weights will inherently include the relative positional information. This is because the following identity holds: (RmWqxm) (RnWkxn) = (Wqxm)R mRn(Wkxn) = (Wqxm)Rnm(Wkxn). (17) A.2 VISUAL CONTEXT WINDOW EXTENSION In this section, we provide more detailed derivation of the visual context window extension based on YaRN. Unlike the context window extension methods used in LLMs, we first define the visual context window (Lv test), with the scale factor representing the ratio between the two: train), and the extended context window (Lv = . (18) test Lv Lv train Based on the derivation of RoPE, the inner product between the query and key vectors can be expressed in complex form as follows: (Rmq) (Rnk) = Re d/21 (cid:88) q[2i:2i+1]k [2i:2i+1]ei(mn)θi (19) i=0 where = Wqxm and = Wkxn. According to Eulers formula, ei(mn)θi can be represented as point on the unit circle, where controls the angle on the circle. Therefore, we define λd as the wavelength of the RoPE embedding in the d-th hidden dimension. λi = 2π θi = 2πb 2i . (20) The wavelength describes the token length required for the RoPE embedding to complete full rotation (2π) in dimension d. Next, we define r, which represents the ratio between the original context size and the wavelength. This ratio determines which positional dimensions require interpolation. Following YaRN, we introduces two hyperparameters to control the boundaries of the interpolation strategy. r(i) = λi . (21) θnew = (cid:20) γi + (1 γi) (cid:21) 1 θi, γi = 1, 0, riα βα , ri > β ri < α otherwise, (22) When ri < α, linear interpolation is applied proportionally based on s. When ri > β, no interpolation is applied. Otherwise, linear interpolation transition is applied between the above two cases."
        }
    ],
    "affiliations": [
        "School of Remote Sensing and Information Engineering, Wuhan University"
    ]
}