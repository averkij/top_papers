{
    "paper_title": "Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System",
    "authors": [
        "Longfei Yun",
        "Yihan Wu",
        "Haoran Liu",
        "Xiaoxuan Liu",
        "Ziyun Xu",
        "Yi Wang",
        "Yang Xia",
        "Pengfei Wang",
        "Mingze Gao",
        "Yunxiang Wang",
        "Changfan Chen",
        "Junfeng Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 0 4 6 8 1 . 2 0 6 2 : r Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System Longfei Yun, Yihan Wu, Haoran Liu, Xiaoxuan Liu, Ziyun Xu, Yi Wang, Yang Xia, Pengfei Wang, Mingze Gao, Yunxiang Wang, Changfan Chen, Junfeng Pan Meta Equal Contribution, {loyun,yihanwu}@meta.com Modern large-scale ranking systems operate within sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), framework that reframes ranking optimization as an autonomous discovery process within programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability. Correspondence: panjunfeng@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Modern large-scale ranking systems orchestrate heterogeneous product surfaces, ranging from discoveryoriented recommendation to community-driven social interfaces, serving billion-scale global user base with multifaceted preferences. Over decades of iterative development, these systems have evolved into highly intricate architectures where numerous optimization layers concurrently target diverse and often conflicting metrics. Consequently, the primary bottleneck to system advancement has shifted from pure signal estimation to the engineering context constraint: the arduous translation of product intuition and domain expertise into auditable, executable hypotheses. Current industry workflows remain tethered to manual intervention, relying on domain experts to navigate the combinatorial complexity of multiobjective trade-offs, treatment interpretability, and the rigorous alignment with evolving business criteria. This manual dependency creates scalability barrier, leaving high-value policies undiscovered within the search space. Furthermore, traditional optimization approaches, such as uplift modeling (Künzel et al., 2019; Zhao et al., 2017; Wei et al., 2024), treat personalization as static model selection task. While these methods may identify statistically promising interventions, they frequently fail to account for operational constraints or feature instability, resulting in optimal policies that are brittle or undeployable in production environments. To bridge this gap, we introduce GEARS (Generative Engine for Agentic Ranking Systems), framework that reframes ranking optimization as an autonomous discovery process within programmable experimentation environment. Rather than attempting to solve for optimality in single inference step, GEARS treats the experimentation ecosystem as an interactive environment where agents navigate through executable operations to generate hypotheses and evaluate feasibility. This agentic approach is grounded by two core innovations. First, we introduce Specialized Agent Skills, which encapsulate expert ranking knowledge into reusable reasoning computational capabilities. This architecture enables Vibe Optimization, allowing operators to steer systems via high-level intent while the agent handles the translation into explicit algorithmic constraints. Second, to ensure production reliability, the framework incorporates Deterministic Lifecycle Governance. By registering 1 validation hooks that audit feature stability and statistical robustness, GEARS automatically filters out policies that overfit transient signals, ensuring that recommended configurations are durable and deployable. Experimental validation demonstrates that this approach consistently discovers superior, near Paretoefficient policies while significantly reducing human engineering overhead. key innovation of GEARS is the introduction of Specialized Agent Skills, which are modular resources that encapsulate expert ranking knowledge into reusable computational capabilities. These skills integrate structured instructions, operational procedures, and access to domain artifacts. This architecture allows agents to perform advanced analyses such as feature explanation, feature stability audits, and guardrail interpretation. Such capabilities support paradigm we term Vibe Optimization, in which operators guide ranking systems through high-level intent rather than low-level parameter tuning. By translating qualitative intuition into explicit algorithmic constraints, GEARS enables agents to autonomously generate policies that are interpretable, stable, and aligned with product goals. Granting autonomy to agents in production environments introduces the challenge regarding long-term policy stability. In real-world systems, policies that appear statistically strong in short-term experiments often fail to generalize due to temporal shifts, sampling noise, or reliance on unstable feature distributions. Aggregate signals may produce apparent gains within limited observation windows yet degrade once exposed to broader cohorts or longer-term dynamics. The risk in automated optimization is the selection of brittle policies that cannot sustain performance in deployment. GEARS addresses this challenge through Deterministic Lifecycle Governance, validation framework that registers hooks around every executable action to audit policy robustness. Rather than relying on point-in-time metrics, these hooks evaluate feature stability, cohort consistency, and performance persistence. By filtering policies that overfit transient signals, GEARS ensures that recommended configurations are durable and deployable. This stability-first design grounds agent-driven exploration in long-term reliability rather than shortterm gains. While GEARS is designed as general framework, personalization represents particularly illustrative application. Personalization amplifies the complexity of ranking decisions through high-dimensional feature interactions and cohort-specific trade-offs, making manual optimization challenging. We evaluate GEARS in various experiments to demonstrate its effectiveness. Experimental results show that the framework consistently discovers superior policies while maintaining guardrail compliance and deployment stability. Our contributions are threefold: 1. Agentic Ranking Framework:. We propose new formulation of ranking optimization as an autonomous discovery process over programmable experimentation environment. 2. Skill-Based Agent Architecture: We introduce Specialized Agent Skills that externalize expert ranking knowledge into modular, executable reasoning capabilities. 3. Production Validation: We demonstrate the effectiveness of GEARS in real-world environments, showing its ability to discover optimal trade-off policies while significantly reducing human engineering overhead."
        },
        {
            "title": "2 Related Works",
            "content": "The Evolution of Context Engineering LLMs have evolved from simple instruction-following systems into core reasoning engines, necessitating shift from prompt engineering to the formal discipline of Context Engineering (Mei et al., 2025; Amatriain, 2024; Ye et al., 2024; Velásquez-Henao et al., 2023). This paradigm shift reconceptualizes the input context not as monolithic, static string, but as dynamically structured assembly of informational components. The development of Tool-Integrated Reasoning (TIR) has further transformed LLMs from passive text generators into world interactors capable of autonomous environmental manipulation through structured function calling (Qian et al., 2025; Mialon et al., 2023; Dong et al., 2025; Chen et al., 2022; Li et al., 2023). Furthermore, contextual self-refinement mechanisms, such as Self-Refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023), demonstrate that models can improve their own output quality through iterative feedback loops and reflective episodic memory. To address the inherent statelessness of LLMs, research into Memory Systems has introduced OS-inspired hierarchical storage and cognitive architectures. Implementations such as MemGPT (Packer et al., 2023) leverage virtual memory paging to traverse limited context windows, while frameworks like MemoryBank (Zhong et al., 2023) utilize cognitive principles, such as the Ebbinghaus forgetting curve, to dynamically update memory strength. Within the domain of Multi-Agent Systems (MAS), current research prioritizes sophisticated communication protocols and orchestration 2 mechanisms (Anthropic, 2024, 2025b,a). Uplift Modeling and HTE Traditional uplift modeling approaches, which seek to identify user segments most responsive to specific interventions, can be broadly categorized into meta-learner based methods, tree-based methods, and neural network-based methods. Meta-learner based methods utilize existing prediction models to estimate individual treatment effects (ITE). Approaches such as S-learner and T-learner (Künzel et al., 2019) either combine treatment variables with user features in single model or build separate models for control and treatment groups, respectively. While effective, these paradigms can suffer from performance degradation when there is significant imbalance in data between groups. Tree-based (Zhao et al., 2017; Radcliffe and Surry, 2011; Athey and Imbens, 2016; Nandy et al., 2023) methods employ decision trees or forests to partition the user population into subgroups based on their sensitivity to different treatments, using treatment information as part of the splitting criteria. These methods are valued for their interpretability and ability to uncover heterogeneous treatment effects. Neural network-based methods (Wei et al., 2024; Louizos et al., 2017; Bica et al., 2020; Sun and Chen, 2024; Shi et al., 2019) leverage the representational power of deep learning to model complex user responses and estimate uplift. By introducing flexible architectures, these methods can capture intricate relationships between user features and interventions, and have been widely adopted in domains such as online marketing and precision targeting. Meta has also been investing in this area over decade, such as Smart Scorer Peysakhovich and Lada (2016); Lada et al. (2019),but those methods are only intended to solve algorithmic problems, without incorporating any ranking context. Despite their strengths, most existing approaches rely heavily on offline features and static user profiles, which limits their ability to respond to real-time shifts in user behavior or to dynamically adapt intervention strategies. In practice, even when an algorithm delivers the best offline result, it may never ship, because large-scale ranking systems are highly complex. Any deployable solution must account for business objectives, existing ranking policies, and cannibalization across products and systems. Furthermore, the design and deployment of these models often require manual workflows that are time-consuming and resource-intensive. Adaptive Experimentation (AE) Distinct from static prediction models, Adaptive Experimentation (AE) focuses on optimizing outcomes through sequential decision-making and active exploration. AX methodologies, including Multi-Armed Bandits and Bayesian Optimization, are designed to balance exploration and exploitation to identify optimal configurations or policies dynamically. In the context of large-scale ranking, AE has been utilized to automate the tuning of system parameters and personalization strategies. For instance, recent works at Meta (Olson et al., 2025; Wu et al., 2022; Bakshy et al., 2018) demonstrate how AE can be deployed to efficiently search vast parameter spaces, allowing ranking systems to adapt to user feedback more rapidly than traditional A/B testing cycles would permit. Figure 1 End-to-end workflow for experiment-driven optimization with GEARS: starting from an experiment link, the GAS agent searches over candidate policies to generate insights (e.g., trade-offs between topline metrics), followed by feature understanding, validation and recommendations (stability and interpretability checks), and culminating in an auto-generated iterated ranking configuration."
        },
        {
            "title": "3 The GEARS System",
            "content": "Motivation. Current state-of-the-art personalization frameworks, including Deep Uplift Modeling and emerging Generative Recommendation approaches, treat ranking optimization as static signal estimation problem. While these models excel at minimizing prediction error, they are \"blind\" to the engineering context: the dense web of infrastructure constraints, feature stability requirements, and non-differentiable business \"vibes\" that dictate deployment feasibility. standard HTE model may identify statistically optimal cohort based on transient feature, unaware that the feature is deprecated or unstable. Conversely, generic \"LLM-as-Ranker\" agents, while flexible, often suffer from hallucination and \"context rot\", generating policies that are semantically plausible but operationally brittle. GEARS addresses this 3 \"Deployment Gap\" by reframing optimization not as one-shot inference task, but as an autonomous discovery loop. Unlike baselines that only solve for performance, GEARS explicitly models the process of experimentation, utilizing Deterministic Governance to filter out high-risk policies that purely statistical methods would erroneously promote. As illustrated in Figure 1, GEARS transforms ranking from manual, disjointed process into cohesive autonomous discovery loop. The system takes high-level product intent as input and navigates programmable environment to output productionready configuration. This workflow is driven by three core components: (1) Intent translation (2) Policy Selection, and (3) Deterministic Governance for production safety."
        },
        {
            "title": "3.1 Intent-Conditioned Personalization",
            "content": "GEARS is general agentic framework for ranking optimization; in this work we focus on personalization as an illustrative setting where (i) the policy space is combinatorial (cohorts treatments constraints) and (ii) deployment feasibility depends on stability and infrastructure constraints beyond offline optimality. The goal of this module is to translate high-level operator intent into an executable search specification that can be systematically explored, evaluated, and audited by GEARS. Concretely, given natural-language intent describing desired trade-offs (e.g., prioritizing long-term engagement while maintaining guardrails), the system constructs search specification consisting of objectives, cohort constraints, and experiment parameters. This specification is then used to generate high-recall set of candidate policies, which are subsequently interpreted and validated by downstream Skills and Governance modules. Importantly, this stage focuses on structured exploration rather than final policy selection. Candidate Generation via GAS. To efficiently explore vast, combinatorial policy space, GEARS leverages GAS (Wu et al., 2025), large-scale Heterogeneous Treatment Effect (HTE) framework designed to match optimal interventions to specific user segments across conflicting objectives. Because these objectives often conflict with each other, GAS identifies policies that represent different trade-offs rather than single global optimum. These trade-offs are commonly characterized using the notion of Pareto frontier, where improving one metric would necessarily degrade another. However, standard scalarization approaches used to approximate the Pareto frontier Figure 2 Tolerance-based frontier expansion allows GEARS to surface both convex and near-frontier candidates, enabling more stable and preference-aligned policy selection. tend to emphasize convex regions of the trade-off space. In real ranking systems, desirable operating points may lie in non-convex regions or may be intentionally chosen to balance stability, risk, or business constraints rather than purely maximizing metrics. As result, relying solely on convex Pareto solutions can overlook practically valuable policies. Tolerance-Based Frontier Expansion. As shown in Figure 2, standard GAS employs random weight search to approximate the Pareto frontier by sampling objective weight combinations and selecting top-performing policies. While effective, this procedure tends to emphasize convex Pareto-optimal solutions, potentially overlooking non-convex or nearoptimal policies that may offer better stability or operational robustness. To address this limitation, GEARS introduces tolerance-based Pareto filtering mechanism that admits both optimal and near-optimal candidates. For each metric, tolerance band proportional to uncertainty is defined, allowing policies within this margin to be retained. This approach expands the candidate frontier, enabling downstream modules to reason over richer policy space that better reflects real-world trade-offs."
        },
        {
            "title": "3.2 Insight-Driven Policy Selection",
            "content": "While candidate generation algorithms identify policies that are near Pareto-efficient in offline metrics, these candidates are frequently undeployable in production systems due to unobserved factors such as feature instability, infrastructure constraints, or misalignment with specific business guardrails. Evaluating true deployment feasibility traditionally requires synthesis of ranking expertise and platform-specific infrastructure awarenessa procedural, context-heavy task that is easily overlooked by general-purpose LLMs and difficult to automate. Traditional workflows address this challenge through close collaboration between engineers and data scientists, combining statistical analysis with infrastructure knowledge. However, this reasoning process is procedural, context-dependent, and difficult to automate using static optimization or prompt-only approaches. To bridge this gap, GEARS introduces an Insight Generator powered by Specialized Agent Skills. Distinct from conventional static prompt templates, these skills are modular, filesystem-based resources that encode engineering expertise into executable decision procedures. Each skill is structured as comprehensive tool comprising three components: (1) lightweight metadata for efficient agent routing, (2) structured step-by-step analytical instructions, and (3) direct references to internal artifacts such as SQL scripts and codebases. This architecture enables the agent to autonomously execute complex, domainspecific tasks, such as trade-off diagnosis and feature interpretation. However, the execution of these skills remains limited without platform-specific context (e.g., past experience). To address this, GEARS integrates Domain Knowledge Brain, curated reasoning backbone that grounds the LLM in high-fidelity historical data and domain expert learnings. By supplying the critical \"contextual evidence\" behind past experiments, the Knowledge Brain systematizes innovation and ensures that the agents policy recommendations are empirically valid and deeply integrated with complex system dynamics. Finally, to orchestrate these rich informational components without succumbing to \"context rotting\", the performance degradation LLMs experience with overly long prompts, GEARS employs progressive disclosure strategy. Information is dynamically loaded in three distinct stages: (1) compact metadata is first exposed for skill retrieval; (2) structured instructions are injected only upon skill activation; and (3) specific internal scripts are accessed purely during the execution phase, with subtasks delegated to subagents. This staged design maintains clean context window, externalizes expertise efficiently, and prevents reasoning drift during long-horizon policy exploration."
        },
        {
            "title": "3.3 Deterministic Lifecycle Governance",
            "content": "While agentic exploration enables scalable discovery over large policy spaces, the primary challenge in ranking systems is not identifying policies that perform well in short-term experiments, but ensuring that they remain stable under temporal shifts, cohort variations, and infrastructure dynamics. Prior work (Cemri et al., 2025) has shown that multi-agent systems often produce outputs that appear locally reasonable yet fail to generalize due to insufficient verification. In ranking environments, such failures typically arise from overfitting transient signals, reliance on unstable features, or cohort-specific artifacts that do not persist over time. To address this challenge, GEARS introduces Deterministic Lifecycle Governance mechanism that enforces long-term stability throughout the agents decision process. GEARS systematically validates every intermediate and final decision against reproducible stability criteria. As result, policy selection is driven not only by short-term metric gains but by durability across time and user segments. The governance layer evaluates proposed actions using deterministic checks that assess statistical reliability, feature consistency, and robustness over 6 months window. These checks ensure that metric improvements are not driven by noisy or transient features, and that policies maintain performance across temporal slices. If candidate fails to meet these invariants, the system rejects the policy and provides structured feedback to the agent, triggering refinement loop."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we mainly investigate the following research questions: How well does GEARS perform in offline policy selection under multi-metric constraints? How do Skills enable GEARS to discover policies that are both stable and deployable? Can the agent achieve metric improvement in real-world setting?"
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Dataset We constructed benchmark dataset utilizing 20 internal experiments. For each experiment, we initially ran the GAS (Wu et al., 2025) algorithm to generate hundreds of policy candidates with their corresponding metric measurements and confidence intervals. Subsequently, for each experiment, we automatically synthesized five distinct instruction types representing common policy selection scenarios: Maximize Both: Find policies that jointly optimize two metrics Maximize with Constraint: Optimize primary metric while ensuring secondary metric does not regress Tradeoff Analysis: Identify Pareto-optimal policies representing different tradeoff points between competing metrics Efficiency Optimization: Select policies with the highest composite efficiency score Single Metric: Maximize single target metric regardless of others This process yielded total of 100 instructions (20 experiments 5 instruction types), which were used to evaluate performance in real-world settings. For each instruction, we computed the ground-truth as the top-5 policies according to the specified optimization criteria, enabling evaluation of both top-1 accuracy and ranking quality. Baselines To assess the effectiveness of our proposed GEARS framework, we benchmark its performance against several established prompting strategies: Naive Prompting: Directly queries the LLM with the task instruction and data without additional reasoning guidance. Chain-of-Thought (CoT) (Wei et al., 2022): Encourages step-by-step reasoning by prompting the model to first understand the objective, analyze the data, apply selection criteria, and then provide recommendations. Self-Consistency (Wang et al., 2022): Samples multiple reasoning paths with temperature-based diversity (T = 0.7) and aggregates predictions via Borda count voting to improve robustness. Self-Refine (Madaan et al., 2023): two-stage approach where the model first generates initial recommendations, then critically reviews and refines its own output to correct potential errors. Code-as-Action (Wang et al., 2024): Instead of only generating text, the LLM generates and executes code to solve the task. This makes outputs verifiable and reproducible, reducing errors and hallucinations in computationor data-driven settings. Metrics To evaluate policy selection performance, we employ standard ranking and retrieval metrics widely adopted in information retrieval and recommender systems. Precision@K measures the fraction of recommended policies within the top-K that are included in the ground-truth set. Recall@K quantifies the proportion of groundtruth policies that appear within the top-K predictions. NDCG@K (Normalized Discounted Cumulative Gain) evaluates not only whether the model retrieves relevant policies but also whether they are ranked near the top: NDCG@K = DCG@K IDCG@K , DCG@K = (cid:88) i= 2reli 1 log2(i + 1) , where reli {0, 1} indicates whether the policy at position belongs to the ground-truth set. Top-1 Accuracy measures whether the highestranked prediction exactly matches the best ground-truth policy. Top-1 in GT reports whether the top-ranked prediction belongs to the ground-truth set (a relaxed version of Top-1 Accuracy). Ranking Correlation assesses the agreement between predicted and ground-truth rankings via Spearmans ρ, capturing global ordering fidelity. We report results for {1, 3, 5}. Together, these metrics capture complementary aspects of performance: correctness (Top-1 Accuracy), coverage (Recall@K), precision-coverage tradeoff (Precision@K), ranking quality (NDCG@K), and global ordering (Ranking Correlation). Implementation Details All experiments use Claude Sonnet (ant) as the backbone LLM. For SelfConsistency, we sample 5 responses per instruction with temperature 0.7 and aggregate via Borda count. For Self-Refine, we use single refinement iteration."
        },
        {
            "title": "4.2 Structured Policy Selection and Compo-",
            "content": "nent Analysis We evaluate GEARS under an offline policy selection setting, where the model is given tabular experiment records of multiple candidate policies and is instructed to output ranked list of recommended 6 Table 1 Quantitative comparison of policy selection performance. We evaluate the proposed GEARS framework against five state-of-the-art baselines. Performance is measured across five key dimensions: Ranking Quality (nDCG@k), Precision (Prec@k), Global Rank Correlation, Recall (Rec@k), and Top-1 Performance. Method Ranking Quality Precision@k Global Recall@k Top-1 Performance nDCG@1 nDCG@ nDCG@5 Prec@1 Prec@3 Prec@5 Rank Corr. Rec@ Rec@3 Rec@5 Top-1 Acc Top-1 in GT Naive CoT Wei et al. (2022) Self-Consistency Wang et al. (2022) Self-Refine Madaan et al. (2023) Code-as-Action Wang et al. (2024) GEARS w/o Bash GEARS w/o Skill Ours (GEARS) 0.57 0.68 0.57 0.61 0.77 0.40 0.87 0.70 0.80 0.74 0.78 0.87 0.42 0.91 0.74 0.83 0.76 0.80 0.87 0.42 0. 0.57 0.68 0.57 0.61 0.77 0.40 0.87 0.39 0.44 0.39 0.44 0.52 0.18 0.57 0.28 0.30 0.28 0.31 0.33 0.11 0. 0.20 0.31 0.13 0.34 0.59 0.80 0.72 0.36 0.43 0.33 0.37 0.45 0.24 0.53 0.67 0.73 0.67 0.74 0.84 0.32 0. 0.77 0.82 0.78 0.83 0.88 0.32 0.90 0.44 0.57 0.37 0.50 0.68 0.26 0.77 0.57 0.68 0.57 0.61 0.77 0.40 0. 0.94 0.96 0.96 0.94 0.60 0. 0.82 0.56 0.94 0.95 0.86 0. policies. Each candidate policy is associated with multiple metrics (e.g., primary objective and guardrail metrics). Table 1 reports the policy selection performance across suite of ranking and decision-oriented metrics. GEARS consistently outperforms all baselines across most metrics, indicating stronger reliability in selecting high-quality policies under multi-metric constraints. We further conduct ablation studies to isolate the contribution of each component in GEARS. Removing the bash-based filtering stage (GEARS w/o Bash) leads to substantial degradation in performance, especially on Top-1 accuracy and ranking quality. This suggests that deterministic pre-filtering of suboptimal candidates is critical for stabilizing downstream reasoning over tabular decision spaces. In contrast, removing the Skill module (GEARS w/o Skill) results in moderate but consistent drop in performance, indicating that structured, reusable Skills provide additional gains beyond simple filtering by improving the models interpretation of selection rules and ranking criteria."
        },
        {
            "title": "Recommendation",
            "content": "Table 2 Feature Stability Benchmark (6-Month Window). Binary cuts align engagement feature stability with feature set baselines. Feature Class Baseline Product Engagement Product Product Feature Name Feature Set Feature 2 Feature 3 Feature 4 Feature 5 Shift (Quantile) 6% 16% 30% 50% N/A Shift (Binary) 2% 4% 10-12% 20% 30% Status Benchmark Stable Stable (Binary) Unstable Unstable To ensure that the policies generated by GEARS are deployable and robust against temporal distribution shifts, we established rigorous quantitative benchmark for checking feature stability. 4.3.1 Benchmarking Methodology We define the User-Cohort Shift Ratio (Rshift) as the primary metric for stability: the percentage of users who migrate from their assigned cohort (bucket) to different one over 6-month window. To set realistic baseline, we benchmarked perceived stable feature set using two cohort definitions: Quantile Cuts (4 equal buckets) and Binary Cuts (p25/p75 thresholds). Figure 3 Pareto efficiency of generated policies. We plot the performance of all candidate policies, with the Pareto frontier (dark blue line) indicating the optimal trade-off curve. Annotated stars mark key policies of interest. 4.3.2 Empirical Baselines and Thresholds Our analysis revealed that even the baseline exhibits drift. As shown in Table 2, feature set shifted by 6% over 6 months, establishing lower bound for unavoidable natural drift. Product features, such as Feature 4, showed high volatility (50% shift) under quantile cuts, making them unsuitable for long-term policy targeting. Based on these benchmarks, we implemented validation hook within GEARS: 7 Pre-Search Filter: Features must exhibit Rshift 15% (Binary) or 45% (Quantile) to enter the search space. This benchmarking process allowed GEARS to automatically disqualify high-lift but unstable features (e.g., Feature 4) that baseline methods would have erroneously selected. Figure 4 The backtest results indicate that the metrics improvement achieved by the selected policy remains consistent over period of one month. As illustrated in Figure 3, to validate the efficacy of our stability governance, we conducted controlled selection from randomized experiment with four policy candidates. After filtering out high-variance candidates, the remaining policy (Best for Metric 2 ) demonstrated consistent metric improvement over one-month period Figure 4."
        },
        {
            "title": "4.4 Broad Adoption and Real-World Impact",
            "content": "Table 3 Experimental results of GEARS across diverse surfaces. dash () indicates that the specific metric was not applicable or not the primary optimization target for that surface. Domain Metric 1 (%) Metric 2 (%) Metric 3 (%) Surface 1 Surface 2 Surface 3 Surface 4 Surface 5 Surface 6 Surface 7 Surface 8 Surface 9 0.14 0.10 0.10 0.042 0.011 0.0406 0.13 0.089 0.017 0.37 0. 0.08 0.08 0.02 Table 3 demonstrates that GEARS delivers improvements across various experimental surfaces. In each deployment, we observe clear and measurable gains in the key metrics. Taken together, these results support the papers claim that GEARS is an effective, general-purpose mechanism for turning cohort-level personalization into repeatable efficient general Agent 8 framework, rather than one-off optimization tied to single surface or single metric."
        },
        {
            "title": "5 Practical Evaluation",
            "content": "We present an anonymized case study to illustrate how GEARS can recommend actionable optimization policies under multi-objective constraints with minimal manual iteration. In 5.1, we study large-scale recommendation setting where two primary objectives exhibit consistent trade-off, and show how GEARS discovers cohorts that admit differentiated treatments while respecting additional guardrail metrics."
        },
        {
            "title": "Scale Recommendation",
            "content": "Table 4 Quantitative results for Baseline Treatment Arms. The table reports the percentage lift relative to the control group (mean standard error). Treatment Arm Treatment 1 Metrics 1 0.049% 0.043 Metrics 2 +0.282% 0.074 Treatment +0.036% 0.034 0.289% 0.073 Definitions of the metrics can be found in 4.1. In large-scale recommendation systems, improving one engagement objective often comes at the expense of another, creating persistent multi-objective optimization challenge where globally uniform treatments can lead to near zero-sum outcomes. As shown in Table 4, two competing global variants highlight this tension: treatment improves Metric 1 but degrades Metric 2, while another treatment improves Metric 2 at the cost of Metric 1. GEARS addresses this by replacing manual slice-anddice analyses with an agentic personalization workflow. Given high-level natural language prompt (e.g., How can find the tradeoff between metric 1 and metric 2 for this experiment), the agent iteratively explores high-dimensional cohort space, proposes candidate segments, and validates them against both primary objectives and guardrail metrics. The outcome is set of cohort-specific policies where at least one primary objective improves with statistical significance while the other objectives remain neutral within acceptable bounds. GEARS has found that different types of users react differently to changes in the mix of content they see. For example, users who are very active benefit more from the first treatment, which improves one key metric but doesnt affect another much. On the other hand, less active users prefer the second treatment, which helps them become more engaged. Deploying the resulting cohort-targeted policy yields statistically significant lift on the prioritized metric while maintaining neutrality on the competing metric. Operationally, GEARS automated what was previously multi-week, expert-driven discovery process, and leverages the multi-agent system to explain how is the decision was made and the rationale behind the decision in current ranking context. This significantly improves system efficiency and enabling broader exploration of the policy space through effective human-AI collaboration."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced GEARS, transformative framework that fundamentally shifts the ranking optimization paradigm from labor-intensive manual tuning to an autonomous, agentic discovery process. By synergizing three critical components: Vibe-Driven Personalization to bridge the semantic gap between human intent and numerical objectives, Specialized Agent Skills to mitigate context rot during longhorizon reasoning tasks, and Deterministic Lifecycle Governance to enforce rigorous production safety standards; GEARS effectively overcomes the inherent scalability limitations and fragility of traditional uplift modeling. Our extensive real-world deployments validate the frameworks robustness, demonstrating that it not only navigates complex, non-convex optimization landscapes with greater precision than static baselines but also drastically accelerates the experimental feedback loop. Ultimately GEARS establishes new standard for AI-driven ranking infrastructure, paving the way for self-optimizing systems that are both flexible and operationally safe."
        },
        {
            "title": "References",
            "content": "Xavier Amatriain. Prompt design and engineering: Introduction and advanced methods. arXiv preprint arXiv:2401.14423, 2024. Anthropic. Model context protocol, 2024. https://www. Acanthropic.com/news/model-context-protocol. cessed: 2026-01-12. Anthropic. Hooks guide, 2025a. https://code.claude. com/docs/en/hooks-guide. Accessed: 2026-01-12. Anthropic. Agents and tools: Agent skills overview, https://platform.claude.com/docs/en/ Accessed: 2025b. agents-and-tools/agent-skills/overview. 2026-01-12. Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27):73537360, 2016. Eytan Bakshy, Lili Dworkin, Brian Karrer, Konstantin Kashin, Benjamin Letham, Ashwin Murthy, and Shaun Singh. Ae: domain-agnostic platform for adaptive experimentation. In Conference on neural information processing systems, pages 18, 2018. Ioana Bica, James Jordon, and Mihaela van der Schaar. Estimating the effects of continuous-valued interventions using generative adversarial networks. Advances in Neural Information Processing Systems, 33:16434 16445, 2020. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2022. Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llmbrained multi-tool reasoner via reinforcement learning. arXiv preprint, 2025. Sören Künzel, Jasjeet Sekhon, Peter Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10):41564165, 2019. Akos Lada, Alexander Peysakhovich, Diego Aparicio, and Michael Bailey. Observational data for heterogeneous treatment effects with application to recommender systems. In Proceedings of the 2019 ACM Conference on Economics and Computation, pages 199213, 2019. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Fei-Fei Li, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language model-augmented code emulator. International Conference on Machine Learning, 2023. Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. Advances in neural information processing systems, 30, 2017. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. Neural Information Processing Systems, 2023. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334, 2025. G. Mialon, Roberto Dessì, M. Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, R. Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: survey. Trans. Mach. Learn. Res., 2023. Preetam Nandy, Xiufan Yu, Wanjun Liu, Ye Tu, Kinjal Basu, and Shaunak Chatterjee. Generalized causal tree for uplift modeling. In 2023 IEEE International Conference on Big Data (BigData), pages 788798. IEEE, 2023. Miles Olson, Elizabeth Santorella, Louis Tiao, Sait Cakmak, Mia Garrard, Samuel Daulton, Zhiyuan Jerry Lin, Sebastian Ament, Bernard Beckerman, Eric Onofrey, et al. Ax: platform for adaptive experimentation. In AutoML 2025 ABCD Track, 2025. Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez. Memgpt: Towards llms as operating systems, 2023. https:// arxiv.org/abs/2310.08560v2. Alexander Peysakhovich and Akos Lada. Combining observational and experimental data to find heterogeneous treatment effects. arXiv preprint arXiv:1611.02385, 2016. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025. https://arxiv.org/abs/2504.13958v1. Nicholas Radcliffe and Patrick Surry. Real-world uplift modelling with significance-based uplift trees. White Paper TR-2011-1, Stochastic Solutions, pages 133, 2011. 10 neer. In Findings of the Association for Computational Linguistics: ACL 2024, pages 355385, 2024. Yan Zhao, Xiao Fang, and David Simchi-Levi. Uplift modeling with multiple treatments and general response types. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 588596. SIAM, 2017. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory, 2023. https://arxiv. org/abs/2305.10250. Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment effects. Advances in neural information processing systems, 32, 2019. Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. Neural Information Processing Systems, 2023. Zexu Sun and Xu Chen. 3 tn: Multi-gate mixture-ofexperts based multi-valued treatment network for uplift modeling. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 50655069. IEEE, 2024. J. D. Velásquez-Henao, Carlos Jaime Franco-Cardona, and Lorena Cadavid-Higuita. Prompt engineering: methodology for optimizing interactions with ailanguage models in the field of engineering. DYNA, 2023. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yuxiang Wei, Zhaoxin Qiu, Yingjie Li, Yuke Sun, and Xiaoling Li. Multi-treatment multi-task uplift modeling for enhancing user growth. arXiv preprint arXiv:2408.12803, 2024. Han Wu, Sarah Tan, Weiwei Li, Mia Garrard, Adam Obeng, Drew Dimmery, Shaun Singh, Hanson Wang, Daniel Jiang, and Eytan Bakshy. Interpretable personalized experimentation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 41734183, 2022. Yihan Wu, Mingze Gao, Haoran Liu, Weiwei Li, Kevin Han, Junfeng Pan, Xinyi Zhang, Jiawei Wen, and Gedi Zhou. Gas: Large-scale heterogeneous personalization in social network applications at meta. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, KDD 25, page 50495058, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714542. doi: 10.1145/3711896.3737225. https://doi.org/10.1145/ 3711896.3737225. Qinyuan Ye, Mohamed Ahmed, Reid Pryzant, and Fereshte Khani. Prompt engineering prompt engi-"
        },
        {
            "title": "A Notation",
            "content": "Experimental Setup and Notation We consider randomized online experiment (A/B test) with set of actions (treatments) = {a1, . . . , aM }, and control action a0. We evaluate performance metrics {δ1, . . . , δK}. Each user ui is associated with D-dimensional feature vector {X1(ui), . . . , XD(ui)}, Xd(ui) . Users are randomly assigned to one of the treatment groups {U1, . . . , UM } or the control group U0. We write δk(ui, aj) for the observed outcome of user ui under action aj on metric δk. Average Treatment Effect (ATE) The average treatment effect of treatment aj relative to control a0 on metric δk is ATE(aj, δk) = δk(ui, aj) δk(ui, a0). 1 Uj (cid:88) uiUj 1 U0 (cid:88) uiU0 Heterogeneous Treatment Effect (HTE) The individual-level heterogeneous treatment effect of treatment aj relative to control a0 for user ui on metric δk is HTE(ui, aj, δk) = δk(ui, aj) δk(ui, a0). By leveraging flexible architectures, HTE are able to model complex interactions between user features and interventions, and have seen widespread adoption in areas like online marketing and precision targeting."
        },
        {
            "title": "B GAS",
            "content": "GAS is user-segment level HTE algorithm to improve personalization strategies and overcome obstacles. We build our vibe-driven personalization module on top of GAS. Let denote collection of non-overlapping user segments, and let be segment (a subset of users). The segment-level heterogeneous treatment effect of aj relative to a0 on metric δk is HTE(S, aj, δk) = 1 (cid:88) uiS (δk(ui, aj) δk(ui, a0)) . Policy Parameterization through Quantile-Based Segmentation. GAS defines candidate policies through quantilebased cohort segmentation over user features. Let Q(X, p) denote the 100p-th percentile of feature , with Q(X, 0) = . Let Z+ denote the number of quantile bins. Individual split. For feature X, define segments: Sind (X) = (cid:18) X, (cid:26) (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) 1 (cid:18) < X(u) X, (cid:19)(cid:27) . Binary split. For threshold index i0 {1, . . . , 1}: Sbin 1,i0 (X) = Sbin 2,i0 (X) = (cid:26) (cid:26) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Q(X, 0) < X(u) X, (cid:18) (cid:18) X, (cid:19) i0 < X(u) Q(X, 1) (cid:19)(cid:27) i0 (cid:27) , . These segmentation strategies define large combinatorial space of candidate policies across cohorts and treatments. 12 Finding Pareto Policies policy is mapping from segments to actions and can be represented as set of (segment, action) pairs: where the segments {Sb}B b=1 form partition of the user population. = {(S1, a1), . . . , (SB, aB)}, In order to identify Pareto-optimal policies that jointly optimize multiple metrics, GAS adopts an efficient weight-search procedure. Specifically, it defines family of linear scalarized rewards over metrics using set of weight vectors RK . For each segment Sb (with associated action/policy ab), GAS evaluates the weighted average metric lift δk(ui, ab) across users ui Sb, and selects the set of policies induced by maximizing this scalarized objective over W. The resulting Pareto policy set is Ppareto := arg max (cid:88) (cid:88) b=1 k= wk Sb (cid:88) uiSb δk(ui, ab) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Tolerance-based Frontier Expansion Algorithm algorithm 1 outlines the tolerance-based frontier expansion process, designed to identify both Pareto-optimal and operationally robust near-optimal policy candidates. The procedure consists of two main stages. In Step 1, the algorithm performs candidate collection via random-weight search: it samples multiple weight vectors to compute scalarized objective scores across all metrics, retaining the Top-K policies for each weight configuration. In Step 2, it applies tolerance-based Pareto filtering mechanism. Instead of strict Pareto dominance, the algorithm defines per-metric tolerance margin proportional to the estimates uncertainty (τ σm(p)). candidate policy is only discarded if it is strictly dominated by another policy beyond this tolerance threshold. This approach effectively expands the candidate set to include valuable non-convex or near-frontier policies that might offer better real-world stability."
        },
        {
            "title": "D Comprehensive Performance Evaluation",
            "content": "This section provides multidimensional visualization comparing the performance of the GEARS framework against several established baseline methods. As illustrated in Figure 5. The radar plot intuitively demonstrates GEARSs consistent superiority and robustness across all evaluated dimensions. Figure 5 comprehensive evaluation of GEARS against baselines. We visualize the performance across ten distinct metrics, covering Ranking Quality (nDCG@1, 3, 5), Precision (Precision@1, 3, 5), Recall (Recall@1, 3, 5), and Top-1 Accuracy. 13 Algorithm 1: Tolerance-based Personalization: Random-weight Top-K + Tolerance Pareto Filtering Input: Policy set P; metrics = 1, . . . , (assume maximize); estimated means µm(p) and uncertainties σm(p) for each P; number of random weights ; top-K per weight K; tolerance hyperparameter τ 0. Output: Final candidate policy set Cτ (Pareto-optimal and near-Pareto policies). Step 1: Candidate collection via random weight search. Sample weight vectors {w(i)}W i=1 Initialize candidate set ; for 1 to do from the simplex 1; foreach do Sw(i)(p) (cid:80)M m=1 w(i) µm(p); Let (i) TopK(cid:0){Sw(i)(p)}pP , K(cid:1) ; (i); // Top-K policies by weighted score Step 2: Tolerance-based Pareto filtering (near-frontier admission). Define per-metric tolerance margin for candidate p: Define tolerance-dominance τ (maximize case) as: ϵm(p) τ σm(p). (cid:16) m, µm(q) µm(p) ϵm(p) (cid:17) (cid:16) m, µm(q) > µm(p) + ϵm(p) (cid:17) . Initialize Cτ ; foreach do dominated false; foreach do if = and τ then dominated true; break; if not dominated then Cτ Cτ {p}; return Cτ ;"
        }
    ],
    "affiliations": [
        "Meta"
    ]
}