{
    "paper_title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "authors": [
        "Huanyao Zhang",
        "Jiepeng Zhou",
        "Bo Li",
        "Bowen Zhou",
        "Yanzhe Dan",
        "Haishan Lu",
        "Zhiyong Cao",
        "Jiaoyang Chen",
        "Yuqian Han",
        "Zinan Sheng",
        "Zhengwei Tao",
        "Hao Liang",
        "Jialong Wu",
        "Yang Shi",
        "Yuanpeng He",
        "Jiaye Lin",
        "Qintong Zhang",
        "Guochen Yan",
        "Runhao Zhao",
        "Zhengpin Li",
        "Xiaohan Yu",
        "Lang Mei",
        "Chong Chen",
        "Wentao Zhang",
        "Bin Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 6 7 8 2 1 . 2 0 6 2 : r BrowseComp-V3: Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents Huanyao Zhang1,,, Jiepeng Zhou2,, Bo Li1,, Bowen Zhou1,, Yanzhe Dan3,, Haishan Lu1, Zhiyong Cao4, Jiaoyang Chen5, Yuqian Han1, Zinan Sheng1, Zhengwei Tao1, Hao Liang1, Jialong Wu1, Yang Shi1, Yuanpeng He1, Jiaye Lin6, Qintong Zhang1, Guochen Yan1, Runhao Zhao1, Zhengpin Li1, Xiaohan Yu7, Lang Mei7, Chong Chen7,, Wentao Zhang1,, Bin Cui1, 1PKU 2HKUST(GZ) 3OUC 4CASIA 5HITSZ 6THU 7Huawei Cloud BU Core Contributor Project Leader Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs), leveraging their increasingly advancing autonomous planning and tool use capabilities, are evolving into intelligent agents capable of performing web browsing for multimodal deep search. However, existing benchmarks remain limited in terms of task complexity, information searchability, and evaluation dimensions, thereby hindering comprehensive assessments of multimodal browsing agents deep search capabilities in open-world environments. To bridge these gaps, we present BrowseComp-V3, novel benchmark comprising 300 meticulously hand-crafted, challenging questions across diverse domains. By emphasizing deep, multi-level, and cross-modal multi-hop reasoning, we ensure that these tasks necessitate the use of web browsing tools and cannot be resolved solely through the models parametric knowledge. Moreover, we strictly enforce the public searchability of all supporting evidence and incorporate an expert-validated, subgoal-driven process evaluation mechanism, thereby enabling fine-grained characterization of search behaviors and systematic analysis of capability boundaries. Beyond the dataset, we provide OmniSeeker, general multimodal browsing agent framework, and conduct comprehensive evaluation on MLLMs. The results demonstrate that even state-of-the-art models, such as GPT-5.2, achieve only 36% accuracy. Further analysis reveals critical bottlenecks in existing models regarding multimodal information integration and fine-grained perception, highlighting fundamental lack of native multimodal reasoning capabilities."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models OpenAI [2025]; Pichai et al. [2025]; Bai et al. [2025]; Meta [2025]; Li et al. [2024a]; Shi et al. [2025a] have demonstrated substantial performance gains across complex tasks. By integrating linguistic comprehension, visual perception, and tool-use capabilities, these models are increasingly evolving into autonomous agents capable of independent exploration and decision-making. Consequently, an increasing body of research is exploring how MLLMs can leverage external search and browsing tools to address multimodal deepsearch challenges in open-world environments OpenAI [2025]; Google [2024]; Wu et al. [2025a]; Geng et al. [2025]; Huang et al. [2026]. Despite the rapid evolution of model capabilities, benchmarks for multimodal browsing and deep search remain noticeably underdeveloped. Existing studies Jiang et al. [2024]; Geng et al. [2025]; Li et al. [2025a]; Tao et al. [2025a] frequently exhibit shortcomings in task complexity, information searchability, and evaluation dimensions, hindering fair, holistic, and reproducible assessments of multimodal browsing agents. Existing methods still exhibit certain limitations: i) Insufficient Task Complexity. Early benchmarks Cheng et al. [2025]; Geng et al. [2025] are predominantly Preprint. Under review. Table 1: Comparison of our benchmark against representative deep search benchmarks along eight dimensions. indicates that the benchmark only partially satisfies the corresponding criterion. Benchmarks Multimodal Context inputs Multi-round Interaction(>2) Thinking with Images Multi-image Reasoning Public-search Answerable Hop-based Difficulty Analysis Human-validated Trajectories Fine-grained Progress Metrics InfoSeek Chen et al. [2023] Enc-VQA Mensink et al. [2023] MMSearch Jiang et al. [2024] DynVQA Li et al. [2024b] SimpleVQA Cheng et al. [2025] LiveVQA Fu et al. [2025a] BrowseComp Wei et al. [2025] FactualVQA Wu et al. [2025a] BrowseComp-VL Geng et al. [2025] MM-BrowseComp Li et al. [2025a] MMSearch-Plus Tao et al. [2025a] BrowseComp-V3 (Ours) confined to shallow retrieval within two hops, with visual information concentrated in the initial stage. Consequently, they fail to reflect the intricacies of real-world, deep multimodal search scenarios. ii) Inaccessibility of Key Information. The core evidence in subsequent benchmarks Fu et al. [2025a]; Li et al. [2025a]; Tao et al. [2025a] is often derived from sources that are not publicly searchable by tools, such as videos or proprietary documents, which undermines the reproducibility and fairness. iii) Narrow Evaluation Dimensions. Existing studies Jiang et al. [2024]; Li et al. [2024b] primarily focus on the accuracy of the final answer but lack systematic characterization of the reasoning process. This makes it challenging to diagnose specific failure modes or define the capability boundaries of the models. To address these gaps, we present BrowseComp-V3, novel benchmark specifically designed to evaluate multimodal deep browsing and search capabilities. BrowseComp-V3 comprises 300 carefully curated, highly complex questions spanning 24 distinct sub-domains, which systematically assess multimodal browsing agents in open-world settings. key feature of our work is the emphasis on deep, multi-level, and cross-modal reasoning, where critical evidence is strategically interleaved across textual and visual modalities within and across web pages. This design effectively precludes \"shortcut\" successes derived solely from text-based heuristics or models reliance on internal parametric knowledge. Furthermore, we ensure that all critical evidence is accessible via standard public search engines and provide manually annotated gold-standard search trajectories to guarantee fairness and reproducibility. Finally, we introduce expert-validated intermediate sub-goals for each task, enabling fine-grained evaluation of the search process to precisely identify the capability boundaries and failure modes of the evaluated models. Our primary contributions are summarized as follows: We present BrowseComp-V3, which, to the best of our knowledge, represents the first multimodal deep search benchmark to concurrently feature extensive search depth, public search accessibility, and process-oriented evaluation mechanisms. We systematically define and categorize multimodal deep search scenarios. Through processoriented evaluation, we provide more comprehensive characterization of multimodal browsing agents capabilities and limitations. We develop OmniSeeker, unified multimodal browsing agent framework. By integrating diverse web search and visual perception tools, OmniSeeker rivals the performance of state-ofthe-art closed-source systems and substantially enhances open-source models performance on multimodal deep search tasks."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Multimodal Large Language Models Multimodal large language models OpenAI [2025]; Pichai et al. [2025]; ByteDance Seed [2025]; Bai et al. [2025] have demonstrated remarkable proficiency across diverse spectrum of tasks, such as VQA Fu et al. [2025b]; Cheng et al. [2025]; Zhang et al. [2025]; Wu et al. [2025a], grounding Kazemzadeh et al. [2014], OCR Masry et al. [2022]; Shi et al. [2025b]; Mathew et al. [2021]; Shi et al. [2025c], and multimodal reasoning Lu et al. [2023]; Wang et al. [2025, 2024]. Nevertheless, MLLMs inherently struggle with the real-time acquisition of up-to-date information, posing substan2 Figure 1: An overview of the data construction process of BrowseComp-V3. tial hurdles when addressing knowledge-intensive or information-retrieval queries. Consequently, contemporary research has pivoted toward tool-augmented frameworks to empower MLLMs as autonomous agents, capable of dynamically retrieving and incorporating external knowledge. 2.2 Tool-Enhanced Browsing Agents Driven by the escalating tool-calling proficiency of LLMs/MLLMs Guo et al. [2025]; Yang et al. [2025]; OpenAI [2025], tool-enhanced browsing agents have emerged as pivotal research frontier. To enable precise retrieval and reasoning in dynamic web environments, recent studies advocate leveraging supervised fine-tuning and reinforcement learning to enhance agents reasoning and decision-making capabilities Jin et al. [2025]; Li et al. [2025b]; Wu et al. [2025b]; Tao et al. [2025b]; Song et al. [2025]; Zheng et al. [2025]. This paradigm, initially validated in textual agents, has been rapidly extended to the multimodal domain Wu et al. [2025a]; Mei et al. [2025]; Hong et al. [2025]; Huang et al. [2026]. This evolution has significantly expanded the search depth and adaptive boundaries of agents when navigating complex tasks. 2.3 Multimodal Browsing Benchmarks Traditional multimodal browsing benchmarks typically decouple visual understanding from text retrieval and focus on simple two-hop retrieval tasks Chen et al. [2023]; Mensink et al. [2023]; Cheng et al. [2025]; Geng et al. [2025]. As visual agents advance, performance on such tasks has largely saturated. BrowseComp Wei et al. [2025] evaluates text-only agents in open-world settings by requiring large-scale web navigation, offering valuable guidance for multimodal task construction. Inspired by this paradigm, recent benchmarks such as MM-BrowseComp Li et al. [2025a] and MMSearch-Plus Tao et al. [2025a] incorporate multi-hop designs and fine-grained visual reasoning to enhance reasoning depth. However, existing benchmarks still suffer from key limitations: critical information often resides in videos or non-searchable documents, tool support is insufficient, and evaluation primarily measures final-answer correctness while overlooking the quality of reasoning. To bridge this gap, we propose BrowseComp-V3, ensuring all critical information comes from publicly accessible resources during task design. We also introduce the Process Score metric to evaluate multimodal browsing agents comprehensively."
        },
        {
            "title": "3 The BrowseComp-V3 Dataset",
            "content": "BrowseComp-V3 is developed by dedicated team of over 20 researchers, including Masters and Ph.D. candidates with expertise in artificial intelligence and related fields. The entire workflow adheres to predefined design principles and multi-stage quality control pipeline, as delineated in the following subsections. 3 3.1 Design Principles BrowseComp-V3 follows 3 core design principles that address key limitations of existing benchmarks in task complexity, information searchability, and evaluation dimensions. Multi-dimensional Cross-modal Coverage. To more faithfully simulate real-world search scenarios, we augment task complexity along two distinct dimensions. Specifically, we extend search depth via multi-hop variations and categorize cross-modal interaction complexities into 3 hierarchical levels: intra-region alignment, inter-region integration, and inter-image reasoning. Process-oriented Granular Evaluation. Datasets should incorporate expert-validated sub-goals to enable systematic tracking of intermediate reasoning steps. This design ensures granular tracking of evidence acquisition phases, thereby permitting rigorous diagnostic analysis of failure modes and an accurate delineation of model capability boundaries. High Reliability and Reproducibility. For rigorous evaluation, we adopt 3 filtering criteria: (1) Evidence Traceability. Require all evidence be publicly accessible through search tools with complete manual annotation trajectories. (2) Temporal Stability. Prioritize temporally invariant, objective knowledge to eliminate dynamic web content fluctuations. (3) Answer Objectivity. Enforce concise, verifiable answers to enable standardized automated evaluation. 3.2 Data Construction Pipeline As illustrated in Figure 1, BrowseComp-V3 construction follows closed-loop quality assurance framework comprising 5 stages: Stage 1: Initialization and Guideline Formulation The experts defines core evaluation dimensions (domain diversity, task hierarchy, and hop distribution) and constructs Initial Exemplars comprising Visual Inputs, Queries, Sub-goals, Answer, and Metadata. These exemplars, together with Instruction Documents, establish the gold standard for subsequent large-scale annotation. Stage 2: Tool-Augmented Exploratory Annotation Annotators are assigned sub-tasks according to domain expertise and conduct exploratory web searches using suite of specialized tools, including TextSearch, WebVisit, ImageSearch, ImageCrop, and ReverseImageSearch. They document complete interaction trajectories, partition complex tasks into pivotal sub-goals, and annotate the capabilities required to acquire each critical piece of evidence. Stage 3: Dual-Verification and Adversarial Filtering The original dataset undergoes two sequential screening phases. First, in the human verification loop, verifiers replicate the annotated search trajectories and evaluate logical coherence, evidentiary support, and answer accuracy. Samples that fail verification are returned for revision. Second, state-of-the-art (SOTA) multimodal large models filter out trivial examples, ensuring the retention of challenging samples that involve long-tail knowledge or complex reasoning requirements. Stage 4: Structured Data Formatting The verified samples are post-processed and converted into unified JSON format, with standardized input/output fields, sub-goals, and interaction trajectories. This formatting ensures both human readability and machine interpretability, enabling automated evaluation pipelines. Stage 5: Expert Quality Control Before the formal release, domain experts audit the structured data for safety, privacy compliance, and factual accuracy. Only approved samples are included in the final dataset, ensuring ethical and professional standards. 3.3 Dataset Statistics Figure 2 (left) illustrates the categorical distribution of BrowseComp-V3. The dataset comprises 5 balanced categories: Science, Technology, Society, Culture, and Life. Additional statistical metrics, including basic statistics, task levels and difficulty distributions, are provided in Figure 2 (right). 4 Type Statistic Number Basic Statistics Category Statistics Task Level Difficulty Distribution Total questions Total images Maximum question length Maximum answer length Average question length Average answer length Primary Secondary Level 1 Level 2 Level 3 Easy Medium Hard Expert 300 383 134 23 58.58 2.47 5 89 140 71 45 139 86 30 Figure 2: Statistics of BrowseComp-V3. (Left) Category distribution across primary domains. (Right) Summary of statistics."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Evaluated Models We systematically evaluate BrowseComp-V3under 4 representative settings, as detailed below: Human. To assess human performance, we recruit participants with PhD-level expertise who independently solve each problem utilizing standard web browser. Participants can freely browse publicly accessible web resources to gather evidence and produce verifiable answers. Tool-Free MLLMs. We benchmark multiple SOTA MLLMs in tool-free setting, where models must generate answers directly without access to external tools or search capabilities. Specifically, we evaluate the following models: GPT-5.2 OpenAI [2025], o4-mini OpenAI [2025], GPT4o Hurst et al. [2024], Gemini-3-Flash-Preview Google [2025], Claude-Sonnet-4.5 Anthropic [2025], Doubao-Seed-1.8 ByteDance Seed [2025], MiMo-V2-Flash Team et al. [2026], Qwen3VL-235B-A22B-Instruct Bai et al. [2025], and Qwen3-VL-8B-Instruct Bai et al. [2025]. Tool-Augmented MLLMs. Additionally, we evaluate tool-augmented model services accessed through their official web platforms, with the maximum reasoning mode enabled to elicit their full capabilities. Concretely, we evaluate the following models: GPT-5.2-Thinking OpenAI [2026], Gemini-3-Pro-Preview Google [2026], and Claude-Sonnet-4.5-Thinking Anthropic [2026]. OmniSeeker. Lastly, we evaluate models using OmniSeeker, our custom-built multimodal browsing agenta unified and transparent framework equipped with standardized tools, including TextSearch, WebVisit, ImageSearch, ImageCrop, and ReverseImageSearch. Implementation Details We employ unified and rigorous evaluation protocol across all four settings. For the human baseline, participants have up to 30 minutes per question; if they cannot reach reliable conclusion within this time limit, they may terminate the task and document their key exploration steps. For Tool-Augmented MLLMs, we enable the most advanced reasoning mode available to ensure unconstrained model performance. For Tool-Free MLLMs, models receive only the question text and original images without any tool access, and must directly generate the key information and final answer. Under the OmniSeeker setting, we limit interactions to maximum of 20 rounds per question. The retrieval module uses Serper1 and returns the top 5 results; image retrieval outputs are embedded into the dialogue context as base64-encoded data; the webpage access module uses Jina2 to retrieve and parse webpage content; and image cropping is performed programmatically, with cropped images returned to the model. Evaluation Metrics We employ both result-level and process-level metrics. At the result level, we use Success Rate to measure whether tasks are completed successfully. At the process level, we introduce Process Score to quantify how much progress model makes toward problem resolution 1https://serper.dev 2https://jina.ai 5 Table 2: Performance on BrowseComp-V3. Results are reported in terms of Success Rate and Process Score under the Pass@1 setting. Avg. denotes the average performance, while Sci., Tech., Soc., Cul., and Lif. correspond to the Science, Technology, Society, Culture, and Life categories, respectively. Bold numbers indicate the best-performing model within each group. Model Success Rate (SR, %) Process Score (PS, %) Avg. Sci. Tech. Soc. Cul. Lif. Avg. Sci. Tech. Soc. Cul. Lif. Human Browser 68.03 72. 70.00 73.33 68.00 54.00 82.93 87. 85.25 84.19 82.73 74.32 Tool-Augmented MLMs GPT-5.2-Thinking Gemini-3-Pro-Preview Claude-Sonnet-4.5-Thinking 39.13 22.90 18.33 26.00 18.00 22.00 48.00 16.00 16.00 38.67 21.33 17.33 37.33 24.00 21.33 46.00 34.00 14. 66.05 62.43 47.73 61.11 62.02 58.61 79.74 73.78 58.41 54.87 48.70 34.79 71.41 66.33 54.33 64.73 62.50 35. GPT-5.2 o4-mini GPT-4o Gemini-3-Flash-Preview Claude-Sonnet-4.5 Doubao-Seed-1.8 MiMo-V2-Flash Qwen3-VL-235B-A22B-Instruct Qwen3-VL-8B-Instruct GPT-5.2 o4-mini GPT-4o Gemini-3-Flash-Preview Claude-Sonnet-4.5 Doubao-Seed-1.8 MiMo-V2-Flash Qwen3-VL-235B-A22B-Instruct Qwen3-VL-8B-Instruct 6.00 7.33 2.67 12.00 4.00 9.00 3.00 3.33 1.00 36.00 26.00 11.41 23.67 22.67 33.67 16.67 14.33 5.33 0.00 0.00 0.00 8.00 4.00 8.00 2.00 4.00 0.00 50.00 22.00 14.00 32.00 32.00 42.00 18.00 16.00 2. Tool-Free MLMs 14.00 16.00 10.00 18.00 6.00 16.00 4.00 4.00 2.00 4.00 2.67 2.67 12.00 2.67 1.33 2.67 4.00 1.33 5.33 6.67 1.33 10.67 5.33 13.33 4.00 2.67 0.00 OmniSeeker (Ours) 28.00 24.00 14.00 24.00 20.00 28.00 12.00 14.00 2. 33.33 25.33 10.67 18.67 21.33 37.33 10.67 14.67 9.33 33.33 34.67 14.67 25.33 24.00 38.67 29.33 17.33 8.00 8.00 14.00 0.00 12.00 2.00 8.00 2.00 2.00 2.00 38.00 20.00 2.00 20.00 16.00 18.00 10.00 8.00 2.00 25.02 29.08 11.26 40.76 25.74 34.74 8.12 20.52 6.64 57.70 44.66 24.15 47.37 54.17 58.44 31.33 26.68 13. 17.50 29.48 6.78 38.98 33.06 36.26 4.52 26.34 3.28 67.23 43.70 22.32 50.84 60.94 52.94 34.84 28.56 8.02 44.50 46.12 26.52 61.94 47.56 51.00 17.28 35.38 15.36 55.40 52.11 43.36 68.35 64.25 69.70 35.04 35.93 19.30 19.22 17.91 9.45 31.89 15.97 22.28 5.43 11.69 6.39 49.34 40.73 17.35 40.15 45.29 57.46 24.45 22.00 15. 25.25 29.04 7.60 39.72 24.53 39.87 9.40 19.64 4.09 60.49 48.72 25.93 43.68 56.73 66.27 42.43 28.36 15.55 21.43 28.44 8.66 36.60 13.06 27.96 4.70 14.39 5.48 58.81 37.94 13.04 39.27 46.78 42.42 17.76 20.05 6.38 during multi-step search and reasoningspecifically, the proportion of critical sub-goals successfully completed. This metric is formally defined as: ProcessScore(q) = ˆGq Gq , (1) where Gq denotes the set of ground-truth sub-goals required to solve problem q, and ˆGq denotes the set of sub-goals achieved by the model or human. 4.2 Main Results Based on the experimental results in Table 2, we summarize our key findings as follows: (1) Performance Gap and Benchmark Difficulty. Humans significantly outperform all models on BrowseComp-V3tasks, achieving an average Success Rate of 68.03% and Process Score of 82.93%. In contrast, no model achieves more than 40% SR. This gap both highlights the limitations of current MLLMs in multimodal deep search tasks and validates the benchmarks ability to capture real-world search complexity. (2) Critical Role of Tool Augmentation. Without tool access, most models achieve only approximately 10% SR. Tool augmentation yields substantial performance improvements, indicating that parameterized knowledge alone cannot adequately capture dynamic, cross-modal evidence chains on the open web. This highlights the importance of external retrieval and interactive capabilities for deep multimodal reasoning. 6 Figure 3: Difficulty and Ability Analysis (3) Effectiveness and Generalizability of OmniSeeker. Empirical evidence confirms that OmniSeeker provides unified and efficient tool-calling framework. When equipped with OmniSeeker, all models consistently achieve substantial improvements, reaching performance comparable to specialized proprietary systems. (4) Value of Process-Level Evaluation. We observe notable gap between PS and SR, with PS typically exceeding SR. This indicates that while models can complete individual sub-goals, they often fail to maintain logical consistency across long-sequence tasks. Therefore, fine-grained process-level evaluation is essential for identifying where and why models fail, thereby revealing their capability boundaries. (5) Competitive Performance of Open-Source Models. While proprietary models (e.g., GPT5.2 OpenAI [2025]) remain the top performers, high-performance open-source models are rapidly closing the gap. Notably, Doubao-Seed-1.8 ByteDance Seed [2025] achieves 33.67% SR when equipped with OmniSeeker. This demonstrates that high-quality open-source models possess strong capacity for complex reasoning and provide promising path toward developing cost-effective, high-performance web browsing agents."
        },
        {
            "title": "5 Further Analysis",
            "content": "5.1 Fine-grained Analysis Table 3: PS across Different Models and Levels Model GPT-5.2 Claude-Sonnet-4.5 Doubao-Seed-1.8 MiMo-V2-Flash Qwen3-VL-235B L1 0.6176 0.5708 0.6185 0.3776 0.3262 L2 0.5528 0.5353 0.5652 0.2638 0.2308 L3 0.5792 0.5186 0.5838 0.3420 0. Task Level As shown in Table 3, model performance declines substantially as task complexity increases from Level 1 to Levels 2 and 3. This reveals that while models can effectively perform unitary visual search, they face significant challenges in inter-region integration and interimage relational reasoning. Search Depth As illustrated in Figure 3 (Left), SR for both humans and models decline with increasing search depth, yet exhibit distinct patterns. Human performance drops sharply with longer search paths, whereas model performance declines more gradually. This discrepancy implies that models leverage internalized parametric knowledge as compensatory mechanism to mitigate the impact of search complexity. Ability Boundaries Figure 3 (Right) reveals distinct bottlenecks for humans and models. Human performance limitations are primarily in TextSearch, due to constraints in attention span and cognitive load when processing voluminous text. In contrast, multimodal integration remains the primary bottleneck for all models. 7 Figure 4: Test Time Scaling Figure 5: Failure Mode Analysis 5.2 Test Time Scaling We evaluate how test-time compute affects performance on BrowseComp-V3. Our key findings are as follows: Scaling Interaction Steps. As shown in Figure 4 (Left), increasing the maximum number of interaction turns substantially improves performance. Notably, Qwen3-VL-235B exhibits stronger scaling advantage than its 8B counterpart. This suggests that larger models have stronger long-horizon reasoning capabilities, allowing them to better utilize additional interaction steps for iterative refinement. Scaling Sampling Consistency. Figure 4 (Right) shows the performance of Qwen3-VL-235B as we increase the number of independent samples (N ). Among the three strategies, Best-of-N scales most effectively, continuously improving performance with increasing . 5.3 Failure Mode Analysis We analyze the error distributions of four representative models, as shown in Figure 5. Our key findings are summarized below: Multimodal Grounding and Perception. Across all models, Visual Grounding and Perception Failure dominate the error distribution. This indicates that current MLLMs struggle to accurately extract and perceive visual information in complex, noisy web environments. Multimodal Progress, Planning Constraint. Closed-source models substantially reduce perception and grounding errors compared to open-source models. However, with improved multimodal capabilities, long-horizon planning becomes the main bottleneck limiting further improvements in SOTA models."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce BrowseComp-V3, comprehensive benchmark for the evaluation of multimodal deep browsing and search capabilities. The benchmark consists of 300 rigorously curated and annotated questions designed to systematically remedy three core limitations of existing evaluation paradigms: task complexity, information searchability, and evaluation dimensions. Empirical results reveal that SOTA MLLMs achieve under 40% SR, underscoring substantial gap relative to human performance. These findings confirm the effectiveness and discriminative power of BrowseComp-V3in simulating open-world multimodal deep search scenarios. Further analysis reveals critical deficiencies in current models capacity to integrate and comprehend multimodal information, whereas the process-level evaluation and Test-Time Scaling analysis offer potential pathways for enhancing model capabilities via methodologies such as reinforcement learning. Additionally, our agent framework, OmniSeeker, achieves performance comparable to leading closed-source models, offering an open alternative for developing multimodal browsing agents. In conclusion, BrowseCompV3provides comprehensive platform for evaluating and advancing multimodal browsing agents. Its process-level evaluation and fine-grained capability analysis will catalyze future breakthroughs in multimodal deep search."
        },
        {
            "title": "References",
            "content": "OpenAI. Introducing GPT-5.2. https://openai.com/index/introducing-gpt-5-2/, December 2025. Accessed: 2026-01-29. Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu. new era of intelligence with gemini 3. Google Blog, November 18 2025. URL https://blog.google/products-and-platforms/products/gemi ni/gemini-3/. Accessed: 2026-02-12. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. Meta. Llama 4 Herd. Meta blog, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intellige nce/. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, et al. Mavors: Multi-granularity video representation for multimodal large language model. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1099411003, 2025a. OpenAI. Introducing deep research. OpenAI blog, 2025. URL https://openai.com/index/introducing -o3-and-o4-mini/. Google. Gemini Deep Research. Google blog, 2024. URL https://gemini.google/overview/deep-res earch/?hl=en. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025a. Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. 9 Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, et al. Vision-deepresearch: Incentivizing deepresearch capability in multimodal large language models. arXiv preprint arXiv:2601.22060, 2026. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, et al. Mmsearch: Unveiling the potential of large models as multi-modal search engines. In The Thirteenth International Conference on Learning Representations, 2024. Shilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang, Chenchen Jing, Zhen Li, Chuanhao Li, Jiayi Tian, Chenchen Zhang, Tianhao Peng, Yancheng He, Jihao Gu, Yuanxing Zhang, Jian Yang, Ge Zhang, Wenhao Huang, Wangchunshu Zhou, Zhaoxiang Zhang, Ruizhe Ding, and Shilei Wen. Mm-browsecomp: comprehensive benchmark for multimodal browsing agents, 2025a. URL https://arxiv.org/abs/2508.13186. Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, and Lingpeng Kong. Mmsearch-plus: simple yet challenging benchmark for multimodal browsing agents. arXiv preprint arXiv:2508.21475, 2025a. Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 46374646, 2025. Mingyang Fu, Yuyang Peng, Dongping Chen, Zetong Zhou, Benlin Liu, Yao Wan, Zhou Zhao, Philip Yu, and Ranjay Krishna. Seeking and updating with live visual knowledge. In The Thirtyninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025a. Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip Yu, Fei Huang, et al. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937, 2024b. Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713, 2023. Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo, and Vittorio Ferrari. Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31133124, 2023. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. ByteDance Seed. Seed1.8: generalized agentic model that can efficiently and accurately accomplish complex tasks in real-world scenarios. https://seed.bytedance.com/en/seed1_8, December 2025. Accessed: 2026-01-29. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025b. YiFan Zhang, Yang Shi, Weichen Yu, Qingsong Wen, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, and Rong Jin. Debiasing multimodal large language models via penalization of language priors. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 42324241, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 10 Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, et al. Realunify: Do unified models truly benefit from unification? comprehensive benchmark. arXiv preprint arXiv:2509.24897, 2025b. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, et al. Mme-videoocr: Evaluating ocr-based capabilities of multimodal llms in video scenarios. arXiv preprint arXiv:2505.21333, 2025c. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and Yisen Wang. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint arXiv:2511.21395, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025b. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency, 2025b. URL https://arxiv.org/abs/2505.22648. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025b. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. Lang Mei, Zhihan Yang, Xiaohan Yu, Huanyao Zhang, and Chong Chen. Ai-searchplanner: Modular agentic search via pareto-optimal multi-objective reinforcement learning. arXiv preprint arXiv:2508.20368, 2025. Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, and Xing Yu. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025. OpenAI. Introducing OpenAI o3 and o4-mini. https://openai.com/index/introducing-o3-and-o4-mini/, April 2025. Accessed: 2026-01-29. OpenAI Aaron Hurst, Adam Lerer, Aditya Ramesh, Alec Radford, et al. Gpt-4o system card. ArXiv, abs/2410.21276, 2024. URL https://api.semanticscholar.org/CorpusID:273662196. Google. Gemini 3 flash: frontier intelligence built for speed. https://blog.google/products-and-platf orms/products/gemini/gemini-3-flash/, December 2025. Accessed: 2026-01-29. Anthropic. Introducing Claude Sonnet 4.5. https://www.anthropic.com/news/claude-sonnet-4-5, September 2025. Accessed: 2026-01-29. Core Team, Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, et al. Mimo-v2-flash technical report, 2026. URL https://arxiv.org/abs/2601.02780. OpenAI. ChatGPT: smart and simple AI, 2026. URL https://chatgpt.com/. Accessed: 2026-01-29. Google. Gemini, 2026. URL https://gemini.google.com/. Accessed: 2026-01-29. Anthropic. Claude: Think fast, build faster, 2026. URL https://claude.ai/. Accessed: 2026-01-29."
        }
    ],
    "affiliations": [
        "CASIA",
        "HITSZ",
        "HKUST(GZ)",
        "Huawei Cloud BU",
        "OUC",
        "PKU",
        "THU"
    ]
}