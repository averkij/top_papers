{
    "paper_title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
    "authors": [
        "Qi Guo",
        "Jianing Wang",
        "Deyang Kong",
        "Xiangyu Xi",
        "Jianfei Zhang",
        "Yi Lu",
        "Jingang Wang",
        "Wei Wang",
        "Shikun Zhang",
        "Wei Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions."
        },
        {
            "title": "Start",
            "content": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Qi Guo 1 2 Jianing Wang 2 Deyang Kong 1 2 Xiangyu Xi 2 Jianfei Zhang 2 Yi Lu 2 Jingang Wang 2 Wei Wang 2 Shikun Zhang 1 Wei Ye 1 6 2 0 2 9 ] . [ 1 4 4 3 8 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Parallel thinking has emerged as new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions. 1. Introduction Despite the rapid evolution of Large Reasoning Models (LRMs) (Guo et al., 2025; Team et al., 2025; Yang et al., 2025a), their capacity to solve complex problems in sin1National Engineering Research Center for Software Engineering, Peking University, Beijing, China 2Meituan Group, Beijing, China. Correspondence to: Jianing Wang <lygwjn@gmail.com>, Wei Ye <wye@pku.edu.cn>. Preprint. February 10, 2026. gle inference pass remains constrained. Following the success of Gemini (Luong et al., 2025) in the International Mathematical Olympiad competitions, parallel thinking has emerged as promising reasoning paradigm for addressing such challenges. Parallel thinking allows the model to concurrently explore multiple potential solution paths, subsequently aggregating the outcomes of these explorations to synthesize the final answer. Such paradigm encourages the model to broadly search for potential correct trajectories within the solution space, while simultaneously mitigating the risk of biased outcomes associated with single exploration trajectory (Pan et al., 2025; Zheng et al., 2025b). Prior approaches rely on Supervised Fine-Tuning (SFT) over extensive corpus of high-quality parallel reasoning trajectories (Wen et al., 2025; Yang et al., 2025b; Ning et al., 2023). This process not only incurs heavy resource consumption for trajectory synthesis but also risks superficial pattern imitation, thereby limiting effectiveness on challenging tasks. Consequently, recent studies have shifted toward utilizing Reinforcement Learning (RL) to empower models to autonomously acquire parallel thinking under practical problem-solving scenarios (Zheng et al., 2025b). However, the application of RL in parallel thinking remains underexplored, with existing works predominantly concentrating on the aggregation phase over the exploration phase. Crucially, prior studies indicate that the effectiveness of the aggregation phase depends heavily on the quality of the paths found during exploration, which sets natural limit on overall performance of parallel thinking (Wang et al., 2025). In this paper, we investigate the impact of the path exploration stage on the performance of parallel thinking. We present theoretical analysis within the Reinforcement Learning with Verifiable Rewards (RLVR), and reveal that overall performance is fundamentally constrained by mutual information saturation among explored paths. Specifically, under the naive parallel thinking paradigm where paths are sampled independently, optimizing the information gain between trajectories becomes challenging. In addition, since LRMs tend to suffer from mode collapse, the generated paths often exhibit high redundancy (Shumailov et al., 2023; Shao et al., 2024). Consequently, when tackling complex OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration problems, multiple parallel paths often converge on the same incorrect answers rather than covering diverse regions of the solution space. Based on these findings, we propose Outline-guided Parallel Exploration (OPE), framework that requires the model to explicitly partition the solution space using outlines before path reasoning, thereby maximizing the mutual information between generated paths and the correct solution. We start from cold-start stage to instill outline planning capabilities, where the model learns to analyze problems and generate outlines for diverse reasoning strategies. Building on this, we introduce an iterative RL strategy to jointly optimize outline planning and outlineguided reasoning capabilities. Specifically, we first conduct Outline Planning RL to encourage the model to generate diverse outlines that define multiple distinct problem-solving directions. This is followed by Path Reasoning RL, which refines the models ability to execute reasoning steps under the guidance of these outlines. These two training phases are interleaved at fixed step intervals to facilitate the synergistic evolution of both capabilities. Experimental results across serveral mathematical reasoning benchmarks demonstrate that OPE leads to significant improvements in parallel thinking performance, with particularly substantial gains on the most challenging tasks. Furthermore, comparative analysis shows that, compared to naive parallel thinking which samples each path independently, OPE exhibits superior scaling properties at test time and alleviates the issue of overthinking during path reasoning, enabling the model to reach solutions more efficiently through targeted exploration. The contributions of this paper can be summarized as: We provide theoretical perspective on parallel thinking RL, and formally identify mutual information saturation during the exploration phase as the fundamental bottleneck limiting performance. We introduce Outline-guided Parallel Exploration (OPE), novel paradigm designed to mitigate mutual information saturation by explicitly partitioning the solution space with outlines. We implement OPE via novel iterative RL strategy that synergistically optimizes outline planning and path reasoning capabilities. Extensive experiments demonstrate that OPE achieves superior performance across complex benchmarks. Further analysis reveals that OPE exhibits better testtime scaling properties and effectively alleviates the overthinking issue, establishing solid baseline and offering new perspectives for future research. 2. Related Work 2.1. Parallel Thinking Parallel thinking typically involves generating multiple reasoning trajectories, followed by an aggregation phase to derive the final solution. The majority of existing research focuses on the aggregation phase, which can be broadly categorized into two streams. (1) Selection and Ranking Methods: These approaches evaluate individual paths via scoring or pairwise comparisons to identify the optimal solution. Early works employed Verifiers or Reward Models to assign scalar scores to reasoning steps or final answers, selecting the candidate with the highest confidence (Cobbe et al., 2021; Lightman et al., 2023). More recent methods utilize pairwise ranking mechanisms to filter out low-quality trajectories (Li et al., 2023). (2) Summary Methods: Instead of selecting single path, these methods aim to synthesize final answer by aggregating information from all generated paths. This includes heuristic strategies such as SelfConsistency, which relies on majority voting to determine the consensus answer (Wang et al., 2022b), along with its various extensions. Other approaches leverage the inherent summarization capabilities of LRMs to merge diverse reasoning chains into coherent conclusion (Jiang et al., 2023). In contrast, the path exploration phase has received comparatively less attention. Skeleton of Thought (Ning et al., 2023) decomposes problems into sub-tasks to accelerate inference, and Leap (Luo et al., 2025b) enhances efficiency through fixed information exchange mechanisms between paths. However, these methods are often constrained to complex architectural designs. Distinctly, OPE enables the model to explicitly determine the exploration distribution to identify potential solutions. 2.2. RLVR RLVR leverages verifiable outcome signals, such as rulebased answer matching or model-based evaluation, to guide model optimization, thereby circumventing the reliance on human annotation for reward signals. This paradigm has demonstrated remarkable efficacy in enhancing the reasoning capabilities of Large Language Models (LLMs) (Trinh et al., 2024; Shao et al., 2024). Among diverse array of RLVR algorithms, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) eliminates the need for separate value network and reduces training instability by utilizing this group-relative baseline and demonstrates significant advantages in mathematical reasoning tasks. Specifically, for group of outputs {o1, . . . , oG} sampled for query q, GRPO estimates the advantage of each output oi as Ai = (ri mean(r))/std(r), where is the vector of rewards. Building on this foundation, several variants have been proposed to further refine sample efficiency and stability (Yue et al., 2025; Zheng et al., 2025a; Yu et al., 2025). 2 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Figure 1. Overview of OPE framework. (Left) Naive parallel thinking samples reasoning paths independently. Due to mode collapse, these paths often exhibit high redundancy and tend to converge on the same incorrect answer. (Middle) OPE mitigates this by explicitly generating diverse outlines to partition the solution space into distinct directions (four different strategies). This structured exploration maximizes the coverage of potential solutions, enabling the model to successfully locate the correct reasoning trajectory. (Right) The OPE training pipeline consists of Cold Start phase using synthesized data, followed by novel Iterative RL strategy. Despite the extensive exploration of RL in the reasoning scenarios, the application of RL specifically to parallel thinking remains underexplored. This paper bridges this gap by providing theoretical analysis of parallel thinking and offer empirical insights for future research. the overall inference probability is derived by marginalizing over the space of generated paths: πθ(AQ) = (cid:88) πθ(AP, Q)πθ(PQ) (3) 3. Parallel Thinking with RLVR 3.2. Parallel Thinking Optimization In this section, we present theoretical analysis of the parallel thinking paradigm. We focus on the RLVR settings and identify that Mutual Information Saturation among generated paths during the exploration phase as fundamental bottleneck that constrains the reasoning performance. 3.1. Formulation of Parallel Thinking Let denote the input query drawn from the data distribution D, and represent the corresponding ground-truth Answer. We define as the answer generated by unified policy model πθ. The reasoning workflow in parallel thinking can be formally decomposed into two distinct phases: 1. Exploration: Given Q, the model samples set of parallel reasoning paths = {P1, . . . , PN } from the policy: Pi πθ(Q), {1, . . . , } (1) 2.Aggregation: Based on the query and the set of explored paths P, the model synthesizes the final answer A: πθ(P, Q) (2) We formulate the parallel paths as intermediate variables that bridge the query and the final answer. Consequently, The fundamental objective of RLVR is to optimize the policy parameters θ to maximize the expected reward over the data distribution D: J(θ) = EQD (cid:2)EAπθ(Q) [r(Q, A)](cid:3) (4) where r(Q, A) serves as the reward function. In the RLVR setting, this reward is usually deterministic and binary (i.e., = 1 if the generated answer matches the ground truth , and 0 otherwise). Consequently, maximizing the expected reward J(θ) is asymptotically equivalent to maximizing the log-likelihood of generating the ground truth given Q: max θ J(θ) max θ log πθ(Y Q) (5) Given that the generation of the ground truth depends on the intermediate reasoning paths P, the log-likelihood can be expressed as log (cid:80) πθ(Y P, Q)πθ(PQ). Since directly optimizing the resulting log-sum is intractable, we follow standard practice and consider its Evidence Lower Bound (ELBO): log πθ(Y Q) EPπθ(Q) [log πθ(Y P, Q)] (6) OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration From the perspective of Information Theory (Reza, 1994), the expectation term EP [log πθ(Y P, Q)] corresponds to the negative Conditional Entropy, denoted as H(Y P, Q). Recall the definition of Mutual Information (MI): I(P; Q) = H(Y Q) H(Y P, Q) (7) Since the ground truth is deterministic for given query in the RLVR setting, the entropy H(Y Q) remains constant (Y is not random variable given Q). Consequently, maximizing the lower bound LELBO is mathematically equivalent to maximizing the mutual information: max θ LELBO max θ I(P; Q) (8) Figure 2. Comparisons of Pass@k curves and Maj@k curves on the HMMT-25 benchmark. This derivation reveals that the optimization of parallel thinking is fundamentally to generate reasoning paths that maximize the information gain regarding the ground-truth. 3.3. Mutual Information Saturation among Paths Instead of analyzing the mutual information between the set and directly, we can simplify the formulation by decomposing the total mutual information item into the sum of marginal contributions from each individual path using the chain rule: I(P; Q) = (cid:88) i=1 I(Pi; P1:i1, Q) (9) In the naive parallel thinking paradigm described in Section 3.1, paths are sampled independently and identically (i.i.d.). However, since LRMs typically undergo extensive post-training, they often exhibit the risk of mode collapse, which leads to significant semantic redundancy among sampled trajectories. Consequently, the marginal information gain diminishes rapidly: The results showed in Figure 2 reveal critical discrepancy: while Pass@k continues to rise as the sample size increases, Maj@k plateaus after approximately 20 samples. This indicates that although the model possesses the intrinsic capability to solve the problems (evidenced by the rising Pass@k), the vast majority of sampled paths converge to incorrect answers. Consequently, the correct signal is drowned out by redundant errors, thereby imposing hard ceiling on the performance of aggregation. 4. Methodology To mitigate the bottleneck of Mutual Information Saturation, we propose Outline-Guided Path Exploration (OPE), which introduces set of explicit reasoning outlines during the exploration process to dynamically partition the solution space. In this section, we first analyze how the introduction of outlines theoretically overcomes the saturation bottleneck. Subsequently, we detail the practical implementation of OPE, which involves cold-start stage followed by an iterative RL strategy. I(Pi; P1:i1, Q) as (10) 4.1. Breaking Saturation with OPE We term this phenomenon Mutual Information Saturation. Intuitively, this implies that when tackling complex problems, the model tends to fall into repetitive failure modes, where large number of parallel paths converge to serveral incorrect answers rather than exploring diverse regions of the solution space. As result, simply scaling the number of paths yields diminishing returns. To empirically validate this phenomenon, we conduct experiments on the challenging mathematical benchmark HMMT25 (Balunovic et al., 2025). Specifically, we perform = 256 independent samplings for each query using DeepSeek-R1-Distill-Qwen-7B (Guo et al., 2025) and analyze the scaling trends of two key metrics: Pass@k, which measures the probability that at least one path in samples is correct, and Maj@k, which represents the accuracy of the answer derived from majority voting over samples. Under the naive parallel thinking paradigm, paths are sampled i.i.d., making it intrinsically difficult to optimize the marginal information gain I(Pi; P1:i1, Q). To address this, it is desirable to rationally partition the solution space to maximize the coverage of potential solutions before conducting concrete path exploration. OPE achieves this by guiding the model to explicitly plan the directions of exploration in the form of set of distinct outlines = {O1, . . . , ON } before executing reasoning steps. Formally, the exploration phase πθ(PQ) is reformulated as hierarchical process: πθ(P, OQ) = πθ(OQ) (cid:89) i=1 πθ(PiOi, Q) (11) serve as structural directives, guiding the subsequent parallel generation towards distinct directions, each Oi corresponding to different strategy for solving the problem. 4 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Figure 1 illustrates the structural differences between OPE and naive parallel exploration. Under this new paradigm, the original optimization objective transforms into maximizing the joint mutual information I(P, O; Q). Using the chain rule, this objective can be orthogonally decomposed into two components: I(P, O; Q) = I(O; Q) (cid:125) (cid:123)(cid:122) (cid:124) Planning Gain + I(P; O, Q) (cid:125) (cid:123)(cid:122) Reasoning Gain (cid:124) (12) 1. Planning Gain (I(O; Q)): This term measures the extent to which the set of outlines covers the solution space containing the ground truth. Since outlines are significantly shorter than full reasoning paths, they can be generated as coherent sequence, enabling the model to explicitly manage diversity among outlines and avoid the diminishing returns associated with i.i.d. sampling. 2. Reasoning Gain (I(P; O, Q)): This term quantifies the models ability to generate correct reasoning trajectories given the constraints of the outlines. OPE aims to effectively transform the optimization of the total mutual information I(P, O; Q) into the optimization of the two objectives independently. tags <outline i> . . . </outline i>. We then concatenate the generated outlines along with path token <path i> to the original query and require model to generate corresponding reasoning path strictly following the guidance of Oi. Considering the context window constraints and the complexity of the reasoning tasks, we set = 4 and limit the maximum generation length per query to 8k tokens. To ensure fair comparison with naive parallel thinking, we instruct gpt-oss-20b to directly generate reasoning paths for these queries, thereby constructing cold-start dataset of identical size for naive parallel thinking. Outline Planning RL This stage aims to maximize the Planning Gain I(O; Q) described in Section 4.1, which is equivalent to maximizing the expected reward of the generated outlines O. However, unlike complete reasoning paths, outlines serve as high-level strategic plans and do not contain full problem resolutions, making them unsuitable for direct evaluation via verifiable reward functions. To address this, we estimate the quality of by the success rate of the downstream reasoning paths generated following its guidance. Specifically, for each outline Oi O, we append the <path i> token and sample distinct reasoning paths, resulting in total of trajectories. The reward function for is then defined as the average accuracy of these generated paths: 4.2. OPE with RLVR Rplan(O) = In this section, we describe the practical implementation of OPE on Qwen3-8B-Base (Yang et al., 2025a) (the state-ofthe-art base model in 8B size). The pipeline begins with cold-start phase to instill the OPE reasoning pattern into the model. Subsequently, we present RL training tailored to the two optimization objectives analyzed in Section 4.1 using GRPO (Shao et al., 2024), and propose an iterative training strategy to further enhance the performance. Data Given that the primary goal of parallel thinking is to address complex reasoning tasks, we utilize the mathematical training corpus in GURU (Cheng et al., 2025), high-quality dataset derived from OR1 (He et al., 2025), DAPO (Yu et al., 2025), and DeepScaler (Luo et al., 2025a) sources after deduplication and difficulty filtering. In this paper, we specifically employ its subsets from OR1 and DAPO (36k samples in total), excluding DeepScaler data due to its relatively lower difficulty. Cold Start Inspired by Parallel-R1 (Zheng et al., 2025b), which demonstrates that zero-shot trajectory synthesis is more stable on simpler queries, we select subset of 5.4k queries from the full dataset where Qwen2.5-7B-Math (Team et al., 2024) achieves pass rate greater than 0.5 for our cold-start data synthesis. We then prompt gpt-oss20b (OpenAI, 2025) to first generate brief analysis of the query, followed by distinct reasoning outlines in the 1 (cid:88) (cid:88) I(Pi,k ) (13) k=1 i=1 where denotes that path successfully reaches the ground-truth answer . This formulation encourages the model to generate outlines that lead to high-probability success regions in the solution space. Path Reasoning RL This phase aims to optimize I(P; O, Q). Since each path Pi represents complete reasoning trajectory, its correctness can be directly verified against the ground truth. Specifically, we optimize the generation of paths conditioned on given outline Oi. Using the concatenation of the query, the outline, and the path token <path i> as input, we employ the verifiable outcome as the reward function: Rreason(Pi) = I(Pi ) (14) Formally, the RL training in this phase differs from standard GRPO only by the inclusion of the <path i> token. Therefore, to make fair comparisons, we train the naive parallel baseline using GRPO with the same reward function. Iterative OPE Training. As indicated by the optimization objectives above, the training for outline planning and path reasoning are formally independent. Intuitively, the effectiveness of an outline is bounded by the problem-solving capability of models, while the reasoning performance is conversely dependent on the quality of the guiding outlines. 5 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Table 1. Main results across various benchmarks under both Cold Start and RL phases. SC denotes Self-Consistency aggregation, while LRM refers to summary aggregation. The best results are highlighted in bold, and the second best results are underlined. Exploration Aggregation SFT (Cold Start) Naive OPE RL Approaches Naive OPE Random SC BoN LRM Random SC BoN LRM Random SC BoN LRM Random SC BoN LRM Math500 AMC AIME 24 AIME 25 HMMT-25 BeyondAIME AVG. Datasets 78.35 79.80 90.00 93.40 74.55 77.80 88.00 90.40 83.75 86.00 91.00 91.40 86.65 90.20 93.80 92.20 58.94 60.25 80.75 87.00 53.37 55.00 72.75 81. 66.81 69.00 85.25 86.00 71.12 75.75 87.75 88.75 16.43 15.21 30.63 41.88 15.81 17.71 29.27 38.23 23.54 23.65 43.12 45.42 29.90 31.67 47.50 49. 17.11 17.92 28.54 33.12 13.59 14.27 23.44 29.06 22.03 24.58 32.29 36.15 21.46 22.81 34.69 34.17 6.56 5.21 13.12 17.08 3.67 4.48 9.69 14. 9.71 8.75 16.15 17.40 10.47 11.25 19.17 20.62 4.58 3.80 10.60 16.30 4.58 5.10 10.60 15.80 7.40 7.70 15.20 16.50 11.28 11.40 20.40 19. 30.33 30.36 42.27 48.13 27.60 29.06 38.96 44.88 35.54 36.61 47.17 48.81 38.48 40.51 50.55 50.77 Motivated by this interdependence, we adopt an iterative optimization strategy to achieve co-optimization of outline planning and path reasoning capabilities. In practice, we first conduct Outline Planning RL for fixed number of steps. We then utilize the final checkpoint to generate outlines for training data, which serves as the input for the subsequent Path Reasoning RL phase. This cycle is repeated multiple times to achieve further improvements. Algorithm 1 illustrates the complete workflow of the OPE training. 5. Experiments 5.1. Setup Baselines. We select Naive Parallel Thinking as our primary baseline. This baseline is established with the same size cold-start data synthesized, followed by standard GRPO training, which is described in Section 4.2. To analyze how the exploration of paths impacts the subsequent aggregation phase, we select four distinct aggregation strategies: Random: Randomly selects one generated path and uses its output as the final answer. Self-Consistency (SC) (Wang et al., 2022a): Determines the final answer via majority voting across all generated paths. Best-of-N (BoN): Selects the correct answer if at least one path in the set is correct. This serves as the theoretical upper bound of the exploration phase. LRMs-Based Summary: Prompts the LRM to reflect on the generated paths and synthesize final answer. Training Recipes. For the cold-start phase, we conduct supervised fine-tuning with lr = 1e-5 for 2 epochs. The subsequent RL training is conducted using the VeRL framework (Sheng et al., 2025), with batch size of 256 and learning rate fixed at 1e-6. We randomly sample 2k queries from the 36k dataset as validation set, utilizing the remaining queries for RL training. For all RL stages, we set the rollout size to 8. In outline planning RL, each aspect Oi of the outlines is randomly paired with = 4 sampled paths to obtain stable reward estimates. Following the practice in (Cheng et al., 2025), the naive RL training is conducted for 2 epochs (270 steps in total). To align the OPE training at the step level, we train the Outline Planning RL for 70 steps, then switch to Path Reasoning RL for 65 steps, and repeat this cycle twice to match the total training steps. Detailed training parameters are listed in Appendix A.2. Evaluation. We selected six benchmarks with various difficulty including MATH-500 (Hendrycks et al., 2021), AMC23, AIME24 & 25, BeyondAIME (ByteDance-Seed, 2025), and HMMT-25 (Balunovic et al., 2025). We duplicated the queries in AMC23 and BeyondAIME 10 times, and those in AIME24 & 25, and HMMT-25 32 times to ensure statistically stable evaluations. We report accuracy as the primary metric, employing the math-verify library 6 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration (Kydlıcek, 2025) to rigorously assess the equivalence between the predictions and the ground truth. Regarding the aggregation strategies, when tie occurs in Self-Consistency, we select the answer with the longer average output length. For LRMs-Based Summary, we employ the Qwen3-8B (Yang et al., 2025a) with generic summary prompt. More details of our evaluation are provided in Appendix A.3. 5.2. Main Results Table 1 presents the comparison between Naive and OPE exploration strategies across various aggregation methods and benchmarks. The results yield several key insights: OPE consistently outperforms naive baselines. While OPE initially underperforms Naive exploration in the Cold Start phase, which may be attributed to the domain shift introduced by the outline-guided reasoning format, it demonstrates superior scalability and adaptability during RL training. After RL, OPE achieves the highest average performance across all aggregation methods. Notably, OPE yields the most significant gains in Self-Consistency (SC) aggregation, improving the average accuracy from 36.61% to 40.51%. This improvement indicates that OPE effectively alleviates the bottleneck in majority voting (Figure 2) caused by mutual information saturation. OPE demonstrates superior improvements on challenging tasks. The performance advantage of OPE becomes increasingly pronounced as task difficulty increases. While the improvement on the relatively simpler MATH-500 dataset is moderate (93.80% vs. 91.00% using BoN), the gains on the more challenging BeyondAIME benchmark are substantial, with OPE achieving 20.40% accuracy in BoN compared to the Naive baselines 15.20% (+5.20%). This trend suggests that by structuring the exploration process, OPE helps the model find correct paths that have low probabilities and are often missed by simple independent sampling. OPE breaks the information bottleneck for LRMs-Based summary. Different the other three rule-based aggregation method that rely solely on the extracted answer in each path, LRMs-based summary can analyze the detailed information within each generated path and perform holistic reasoning over all candidates. While naive parallel thinking benefits from RL training under rule-based aggregation strategies, its improvement under the LRMs-Based Summary approach remains limited (from 48.13% to 48.81%). This suggests that in the naive setting, mutual information saturation among paths restricts the amount of new knowledge available for LRM aggregation. In contrast, with OPE, the average accuracy increases from 44.88% to 50.77%. These results demonstrate that the advantage of OPE lies not only in increasing the probability of discovering correct answers, but also in providing the summary model with more diverse set of reasoning knowledge and informative traces within Table 2. Average results across benchmarks of different stages. Stage Rand Self BoN LRM Cold Start 27.60 29.06 38.96 44.88 Iter 1 + Outline RL + Path RL Iter 2 + Outline RL + Path RL 31.82 37.03 32.55 38.22 43.61 49.18 48.41 50.11 37.32 38. 38.90 40.51 49.97 50.55 50.47 50.77 the explored paths, enabling more effective aggregation. OPE expands the frontier of path explorations. The BoN metric serves as proxy for the upper bound of the exploration phase. OPE consistently achieves the highest BoN scores across all benchmarks after RL training, improving the average accuracy by 3.38% (50.55% vs. 47.17%). Note that the parallel sampling budget is constrained to = 4, which highlights the models outline planning capability of identify high-probability directions within the solution space, showcasing its potential in low-resource scenarios. 5.3. Ablations To validate the effectiveness of our proposed iterative cooptimization strategy, we conduct ablation studies focusing on the incremental gains from each training stage. Table 2 reports the average accuracy across all benchmarks for each phase. The initial Outline RL significantly expands the exploration frontier, while the subsequent Path RL effectively converts this potential into reliable execution. The second iteration yields further gains across all metrics, confirming that outline planning and path reasoning are mutually reinforcing: better outlines guide more effective reasoning, which in turn enables the validation of more complex outlines. We further examine whether the performance gains achieved in the Path Reasoning RL phase result from genuine improvements in path optimization, as opposed to Figure 3. Comparison between continued Outline Planning RL (from 70 steps) and Path Reasoning RL, evaluated using average accuracy across all benchmarks with Random aggregation. 7 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Table 3. Statistical comparison (N = 256). Unique Ans denotes the average number of distinct answers per query, and Avg Length reports the average token count of correct reasoning paths. Method Unique Ans Avg Tokens Naive OPE 23.5 27.6 2217.3 1890.9 merely benefiting from additional training steps. To this end, we extend the Outline Planning RL stage for an equal number of steps and compare its outcomes with those of Path Reasoning RL. As illustrated in Figure 3, Path Reasoning RL consistently yields superior results compared to the extended Outline Planning RL, confirming that the iterative strategy is crucial for fully realizing the potential of OPE. 6. Analysis 6.1. Statistical Analysis While the main results demonstrate the overall effectiveness of OPE, we further analyze its impact from two perspectives: outline planning and path reasoning. From the outline perspective, we examine whether explicit outlines facilitate effective partitioning of the solution space. As shown in Table 3, OPE generates greater number of unique answers (27.6 vs. 23.5), indicating broader and more diverse exploration compared to the naive baseline. This diversity suggests that OPE successfully mitigates mode collapse and guides the model to cover more distinct solution regions. From the path reasoning perspective, we investigate whether outline-guided reasoning leads to clearer and more efficient problem solving. Results in Table 3 demonstrate that OPE yields significantly more concise reasoning paths, with an average correct path length of 1,891 tokensabout 10% shorter than the naive approach (2,217 tokens). This reduction in length implies that the model, when conditioned on well-structured outlines, can focus its reasoning and avoid redundant thinking patterns. 6.2. Test-Time Scaling Analysis Although main results show that OPE effectively expands the reasoning frontier, as evidenced by the substantial improvements in BoN accuracy, it is constrained by fixed outline budget (e.g., = 4 outlines per inference). To investigate whether OPE maintains its advantage under increased computational resources, we evaluate its scaling properties by performing multiple independent OPE samplings for each query. Figure 4 illustrates the Pass@k scaling curves across different benchmarks. We observe that OPE exhibits superior scaling laws compared to the Naive baseline. For instance, on MATH-500 and AMC, OPE rapidly approaches nearly 100% Pass@k as the sampling budget increases, while the naive approach saturates at sigFigure 4. Pass@k scaling comparisons. nificantly lower performance ceiling. On more challenging benchmarks, the performance gap between OPE and naive continues to widen as the sample count increases, further highlighting OPEs advantage in discovering difficult solutions through diverse exploration. This trend validates its potential to scale with additional computational resources, allowing performance to continue improving without being limited by the initial outline budget. 6.3. Failure Modes Analysis We conduct analysis of query-level outcomes to understand the failure modes of naive and OPE strategies. We find that OPE successfully solves substantially greater number of queries where the naive baseline fails (see Table 4), demonstrating OPEs enhanced capability to discover correct solutions in regions of the solution space. To further analyze these failure modes, we count the frequency of correct reasoning paths for queries where only one method succeeds (see Table 5). Notably, in naive correct, OPE wrong cases, the correct path typically appears only once (72%), indicating these queries are solved mainly as result of sampling stochasticity rather than robust reasoning. In contrast, for OPE correct, naive wrong instances, OPE is able to consistently produce multiple correct paths (about 40% queries with more than 1 correct paths), highlighting the effectiveness of outline planning in reliably covering broader and more challenging portions of the solution space. 8 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration 7. Conclusion"
        },
        {
            "title": "Impact Statement",
            "content": "In this paper, we introduce Outline-guided Parallel Exploration (OPE), enhancing parallel thinking in LRMs by explicitly partitioning the solution space with diverse reasoning outlines and leveraging iterative reinforcement learning. Experiments show that OPE significantly enhances performance and scalability, especially for challenging tasks, by mitigating mutual information saturation and improving exploration, indicating its potential for advancing parallel reasoning in complex scenarios. This paper presents work whose goal is to advance the field of Large Reasoning Models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Balunovic, M., Dekoninck, J., Petrov, I., Jovanovic, N., and Vechev, M. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. URL https://matharena.ai/. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school [https://huggingface.co/ olympiads. datasets/ByteDance-Seed/BeyondAIME] (https://huggingface.co/datasets/ ByteDance-Seed/BeyondAIME), 2025. Cheng, Z., Hao, S., Liu, T., Zhou, F., Xie, Y., Yao, F., Bian, Y., Zhuang, Y., Dey, N., Zha, Y., et al. Revisiting reinforcement learning for llm reasoning from crossdomain perspective. arXiv preprint arXiv:2506.14965, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, J., Liu, J., Liu, C. Y., Yan, R., Wang, C., Cheng, P., Zhang, X., Zhang, F., Xu, J., Shen, W., et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jiang, D., Ren, X., and Lin, B. Y. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. Kydlıcek, H. Math-verify: Math verification library. URL https://github. com/huggingface/math-verify, 2025. Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and Chen, W. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, 2023. 9 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Zhang, T., Li, L. E., et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025a. Luo, T., Du, W., Bi, J., Chung, S., Tang, Z., Yang, H., Zhang, M., and Wang, B. Learning from peers in reasoning models. arXiv preprint arXiv:2505.07787, 2025b. Luong, T., Lockhart, E., et al. Advanced version of gemini with deep think officially achieves gold-medal standard at the international mathematical olympiad. Google DeepMind Blog, 1, 2025. Ning, X., Lin, Z., Zhou, Z., Wang, Z., Yang, H., and Wang, Y. Skeleton-of-thought: Large language models can do parallel decoding. Proceedings ENLSP-III, 2023. OpenAI. gpt-oss-120b gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b. Wang, Z., Niu, B., Gao, Z., Zheng, Z., Xu, T., Meng, L., Li, Z., Liu, J., Chen, Y., Zhu, C., et al. survey on parallel reasoning. arXiv preprint arXiv:2510.12164, 2025. Wen, H., Su, Y., Zhang, F., Liu, Y., Liu, Y., Zhang, Y.-Q., and Li, Y. Parathinker: Native parallel thinking as new paradigm to scale llm test-time compute. arXiv preprint arXiv:2509.04475, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Pan, J., Li, X., Lian, L., Snell, C., Zhou, Y., Yala, A., Darrell, T., Keutzer, K., and Suhr, A. Learning adaptive parallel reasoning with language models. arXiv preprint arXiv:2504.15466, 2025. Yang, X., An, Y., Liu, H., Chen, T., and Chen, B. Multiverse: Your language models secretly decide how to parallelize and merge generation. arXiv preprint arXiv:2506.09991, 2025b. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, Y., Yuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Zheng, T., Zhang, H., Yu, W., Wang, X., Dai, R., Liu, R., Bao, H., Huang, C., Huang, H., and Yu, D. Parallelr1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980, 2025b. Reza, F. M. An introduction to information theory. Courier Corporation, 1994. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 1279 1297, 2025. Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., and Anderson, R. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023. Team, M. L., Li, B., Lei, B., Wang, B., Rong, B., Wang, C., Zhang, C., Gao, C., Zhang, C., Sun, C., et al. Longcatflash technical report. arXiv preprint arXiv:2509.01322, 2025. Team, Q. et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3), 2024. 10 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Algorithm 1 OPE Training k=1 I(Pi,k ) k=1 πθ(Q, Oi) Append token <path i> and sample paths: {Pi,k}K {Phase 1: Outline Planning RL (Optimize π(OQ))} for = 1 to Splan do end for (cid:80)K Compute Reward: Rplan(Oi) = 1 Update θ via GRPO to maximize Rplan Sample batch of queries Generate outline sets = {O1, . . . , ON } πθ(Q) for each outline Oi do 1: Input: Dataset D, Initial Model πθ, Iterations = 2, Steps Splan = 70, Steps Sreason = 65 2: Hyperparameters: Outline Count , Path Samples for Reward Estimation 3: Output: Optimized Model πθ 4: for = 1 to do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: end for 30: return πθ Sample batch (Q, Oi) Dreason Construct Inputs: = [Q, Oi, <path i>] Sample paths: πθ(X) Compute Reward: Rreason(P ) = I(P ) Update θ via GRPO to maximize Rreason end for {Transition: Construct Dataset for Phase 2} Initialize Dreason for each query do end for {Phase 2: Path Reasoning RL (Optimize π(P O, Q))} for = 1 to Sreason do Generate outlines πθ(Q) using current policy Dreason Dreason {(Q, Oi) Oi O} end for Table 4. Cross-matrix statistics of query success: Naive (rows) vs. OPE (columns). OPE Correct OPE Incorrect Naive Correct Naive Incorrect 1595 402 232 2551 Table 5. Distribution of correct path occurrences per query, split into Naive correct, OPE wrong and OPE correct, Naive wrong cases. Frequency of correct paths (Total N=4 paths) Naive correct, OPE wrong OPE correct, Naive wrong 1 2 3 4 167 / 232 (71.98%) 38 / 232 (16.38%) 17 / 232 (7.33%) 10 / 232 (4.31%) 243 / 402 (60.45%) 93 / 402 (23.13%) 40 / 402 (9.95%) 26 / 402 (6.47%) OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration (a) Cold Start Hyperparameter Epochs Batch Size Max Length Learning Rate (LR) Minimum Learning Rate LR Decay Style LR Warmup Fraction Adam Beta1 Adam Beta2 Weight Decay Value 2 256 8,192 1 105 1 107 Cosine 0.1 0.9 0.95 1 10 Table 6. Training Hyperparameters (b) RL configs using VERL. Hyperparameter Epochs data.train batch size data.max prompt length data.max response length actor rollout ref.actor.optim.lr actor rollout ref.actor.ppo mini batch size actor rollout ref.actor.ppo micro batch size per gpu actor rollout ref.rollout.n actor rollout ref.rollout.name Value 1 256 2,048 8,192 1 106 32 4 8 vllm A. Implementation Details A.1. Data Construction To construct data for the cold-start phase, we leveraged the metadata provided in the GURU (Cheng et al., 2025) dataset, which includes pass rate statistics evaluated by Qwen2.5-7B-Math. We specifically selected the subset of queries with pass rate greater than 0.5, ensuring that the problems are solvable. Using these queries, we prompted gpt-oss-20b to generate data following the OPE format. The prompt used for synthesis is illustrated in Appendix B.1. To ensure the quality of the synthesized trajectories, we employed rejection sampling: for each query, we generated multiple candidate trajectories and retained only those that yielded the correct final answer. A.2. Training All training experiments, including both the Cold Start SFT and the Iterative RL phases, were conducted on cluster of 32 NVIDIA H800 GPUs (configured as 4 nodes 8 GPUs). The detailed hyperparameters for both stages are listed in Table 6. A.3. Evaluation During the evaluation phase, we adhered to the recommended settings for the Qwen3 model family, setting the sampling temperature to 0.6 and top-p to 0.95. To ensure consistency with the training phase and accommodate complex reasoning chains, we set the maximum output token limit for each path to 8K tokens. We extract the final answer from the models output by parsing the content within the last boxed{} command. The extracted answer is then compared against the ground truth using the math-verify (Kydlıcek, 2025) library to determine correctness. B. Prompts B.1. System Prompt For Cold Start Data Synthesis & OPE Training System Prompt For Cold Start Data Synthesis & OPE Training You are an advanced reasoning assistant capable of Outline-Guided Parallel Thinking. Instructions: 1. Analysis & Planning: Analyze the query and generate several distinct and diverse outlines representing different solving strategies. Each outline explores different logical direction. 2. Independent Parallel Exploration: Generate reasoning paths strictly following each corresponding outline. Path Pi must execute the strategy prescribed in Outline Oi without deviation. OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration B.2. System Prompt LRMs-Based Summary System Prompt For LRMs-Based Summary You are great reasoner. Here is problem, and multiple thinkers attempt to give their thought processes independently. Each thinker has written its own thought process towards the final answer. Each thinker is encouraged to take the other thinkers progress into account to reach the final answer. # ====== Problem ====== {problem} # ====== Thinkers Thought Process ====== Thinker 1: {path 1} Thinker 2: {path 2} Thinker 3: {path 3} Thinker 4: {path 4} Look at the above problem and thought process from each thinker, summarize from these thought processes and finally give your answer within boxed. Summarize their thinking on the problem and try to summarize the thinking of these thinkers. Analyze the differences in thinking between these thinkers and try to analyze which thought process is correct. Note: It is generally believed that when most thinkers get the same answer, the answer may be correct. But you cant do it so superficially, because the correct answer may come from very few thinkers, or even no thinker gives the correct answer. For this reason, when you summarizing, you NEED adhere to the principles of professionalism and critical thinking, carefully identify these thought processes, and give summary and final answer. If you realize that none of these thinkers have answered correctly, you can even learn from the wrong experiences in the thought process of these thinkers and re-think the given problem to give the answer you think is most correct. Please DO NOT just solve the given problem independently like other thinkers, but summarize the thought process of all thinkers. In other words, you need to give the summary first, and then give the final answer within boxed, you can re-think this problem only if you realize that none of these thinkers have answered correctly. C. Case Study Case Study The problem asks for the sum of all positive divisors of 9! that end with the digit 1. To solve it, we need to understand the structure of 9!, identify which of its divisors satisfy the units-digit condition, and then sum those divisors. Key considerations include the prime factorization of 9!, the properties of divisors, and the behavior of units digits under multiplication. The solution space is finite but large, so an efficient strategy involves leveraging modular arithmetic and combinatorial counting of divisor pairs. The challenge lies in filtering divisors by their last digit while keeping track of their sum. <outline 1>**Prime Factorization Lens** Examine the prime factorization of 9! to understand the building blocks of its divisors and how their units digits are determined.</outline 1> <outline 2>**Modular Arithmetic Filter** Use congruence modulo 10 to identify which divisor combinations yield units digit of 1, narrowing the set of candidates.</outline 2> <outline 3>**Divisor Pair Symmetry** Exploit the fact that divisors come in pairs whose product is 9! to simplify the counting and summing process.</outline 3> <outline 4>**Combinatorial Counting Strategy** Count the number of divisors with the desired units digit by analyzing the exponents in the prime factorization and applying combinatorial principles.</outline 4> 13 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration <path 1> We need to compute sum of positive divisors of 9! that have units digit 1. We must use prime factorization lens. So we need to factor 9!, find divisors, filter by units digit 1, sum. Lets do step by . . . Both 1 and 81 are divisors of 9! (since 9! contains 34). **Step 7 Compute the sum** = 4 2034 = 81 1 + 81 = 82 82 </path 1> <path 2> We need to compute sum of positive divisors of 9! that have units digit 1. We must use modular arithmetic filter. Lets proceed. First compute 9! = 9*8*7*6*5*4*3*2*1. Calculate: 9! = 362880. But we . . . sidered all a,b,e combinations. The only ones that gave units digit 1 were the ones we found. Thus sum is 106. Thus final answer: 106 . The final answer in this path: 106 </path 2> <path 3> We need to compute sum of positive divisors of 9! that have units digit 1. We must use the outline: Divisor Pair Symmetry. So we should use the fact that divisors come in pairs whose product is 9!. We . . . uct 42 2. Not 1. a=8,b=1,d=1: u2=6, u3=3, u7=7 product 126 6. Not 1. Thus only 1 and 21. Thus sum = 22. We should output 22 . The final answer in this path: 22 </path 3> <path 4> We need to compute sum of positive divisors of 9! that have units digit 1. We must use the outline: Combinatorial Counting Strategy. So we need to analyze prime factorization of 9!, determine which di . . . = 1, = 1 = 21 = 4, = 0 = 81 Thus the divisors of 9! with units digit 1 are 1, 21, 81. ***3. Sum of these divisors*** 1 + 21 + 81 = 103. </path 4> 103 OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration Figure 5. Comparison of the performance curves of Naive and OPE approaches across different datasets as training steps progress."
        }
    ],
    "affiliations": [
        "Meituan Group, Beijing, China",
        "National Engineering Research Center for Software Engineering, Peking University, Beijing, China"
    ]
}