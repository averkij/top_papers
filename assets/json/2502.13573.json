{
    "paper_title": "Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective",
    "authors": [
        "Yuan Yao",
        "Xiaopu Zhang",
        "Yu Zhang",
        "Jian Jin",
        "Qiang Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semi-supervised heterogeneous domain adaptation (SHDA) addresses learning across domains with distinct feature representations and distributions, where source samples are labeled while most target samples are unlabeled, with only a small fraction labeled. Moreover, there is no one-to-one correspondence between source and target samples. Although various SHDA methods have been developed to tackle this problem, the nature of the knowledge transferred across heterogeneous domains remains unclear. This paper delves into this question from an empirical perspective. We conduct extensive experiments on about 330 SHDA tasks, employing two supervised learning methods and seven representative SHDA methods. Surprisingly, our observations indicate that both the category and feature information of source samples do not significantly impact the performance of the target domain. Additionally, noise drawn from simple distributions, when used as source samples, may contain transferable knowledge. Based on this insight, we perform a series of experiments to uncover the underlying principles of transferable knowledge in SHDA. Specifically, we design a unified Knowledge Transfer Framework (KTF) for SHDA. Based on the KTF, we find that the transferable knowledge in SHDA primarily stems from the transferability and discriminability of the source domain. Consequently, ensuring those properties in source samples, regardless of their origin (e.g., image, text, noise), can enhance the effectiveness of knowledge transfer in SHDA tasks. The codes and datasets are available at https://github.com/yyyaoyuan/SHDA."
        },
        {
            "title": "Start",
            "content": "Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective Yuan Yao, Xiaopu Zhang, Yu Zhang, Member, IEEE, Jian Jin, and Qiang Yang, Fellow, IEEE 1 5 2 0 2 9 1 ] . [ 1 3 7 5 3 1 . 2 0 5 2 : r AbstractSemi-supervised heterogeneous domain adaptation (SHDA) addresses learning across domains with distinct feature representations and distributions, where source samples are labeled while most target samples are unlabeled, with only small fraction labeled. Moreover, there is no one-to-one correspondence between source and target samples. Although various SHDA methods have been developed to tackle this problem, the nature of the knowledge transferred across heterogeneous domains remains unclear. This paper delves into this question from an empirical perspective. We conduct extensive experiments on about 330 SHDA tasks, employing two supervised learning methods and seven representative SHDA methods. Surprisingly, our observations indicate that both the category and feature information of source samples do not significantly impact the performance of the target domain. Additionally, noise drawn from simple distributions, when used as source samples, may contain transferable knowledge. Based on this insight, we perform series of experiments to uncover the underlying principles of transferable knowledge in SHDA. Specifically, we design unified Knowledge Transfer Framework (KTF) for SHDA. Based on the KTF, we find that the transferable knowledge in SHDA primarily stems from the transferability and discriminability of the source domain. Consequently, ensuring those properties in source samples, regardless of their origin (e.g., image, text, noise), can enhance the effectiveness of knowledge transfer in SHDA tasks. The codes and datasets are available at https://github.com/yyyaoyuan/SHDA. Index TermsHeterogeneous domain adaptation, noise, transferability, discriminability. I. INTRODUCTION In recent years, supervised learning techniques have undergone significant advancements with sufficient high-quality labeled samples [1][4]. In practice, however, it is often prohibitive to collect abundant high-quality labeled samples due to concerns about privacy, confidentiality, copyright, etc. To mitigate this challenge, domain adaptation (DA) methods Yuan Yao is with the Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing 100095, China. (e-mail: yaoyuan.hitsz@gmail.com) Xiaopu Zhang is with the Department of Research and Development, Inspur Computer Technology Co., Ltd., Beijing 100095, China (e-mail: zhangxiaopu@inspur.com) Yu Zhang is with the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China. (e-mail: yu.zhang.ust@gmail.com) Jian Jin is with the Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing 100095, China. (e-mail: jin.jian@caict.ac.cn) Qiang Yang is with the Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and also with WeBank, Shenzhen 518052, China. (e-mail: qyang@cse.ust.hk) Corresponding authors: Yu Zhang and Jian Jin. Fig. 1. Example scenario of SHDA with textual source domain and visual target domain. Here, all texts are labeled, but most images remain unlabeled, with only small number having labels. Also, there is no one-toone relationship between texts and images. We do not know what knowledge is transferred across heterogeneous domains. [5][8] have been proposed to improve the learning performance in label-insufficient target domain by drawing upon knowledge from related label-sufficient source domain. Those methods have achieved remarkable progress in various practical applications [9][14]. In general, most existing DA methods [15][21] assume that the original feature representation of source samples is identical to that of target ones. Accordingly, they cannot be directly utilized to handle the heterogeneous scenarios, where source and target samples are characterized by distinct feature representations. However, those heterogeneous scenarios are common in many practical applications [22], [23], such as cross-modal image recognition [24], [25] and cross-lingual text categorization [26][28]. To tackle those scenarios, researchers have formulated an important but challenging problem, i.e., semi-supervised heterogeneous domain adaptation (SHDA) [22], [23]. As illustrated in Fig. 1, under the SHDA setting, source and target samples originate from different feature spaces, such as text and image. Also, source samples are labeled, while the target domain has limited labeled samples and substantial amount of unlabeled ones. In addition, there is no one-toone correspondence, i.e., pair information, between source and target samples. Numerous SHDA methods have been developed [24], [25], [28][30], resulting in improved transfer performance across heterogeneous domains. Since samples from the two domains could be very dissimilar due to the heterogeneous feature spaces, we pose question: What is the transferable knowledge in SHDA? This is an essential issue of SHDA, and however, it has not been well-explored. To explore the above problem in depth, we perform comprewhich has the potential to inspire more intriguing research. Our observations indicate that the essence of transferable knowledge in SHDA primarily lies in the transferability and discriminability of the source domain, regardless of its origin (e.g., image, text, and noise). We open-source the codes and datasets used in this paper at https://github.com/yyyaoyuan/SHDA, including seven typical SHDA methods and several popular datasets, which, to our humble knowledge, is the first relatively comprehensive SHDA open-source repository. The remaining parts of this paper are organized as follows. In Section II, we first provide an overview of SHDA. Then, Section III offers the detailed experimental setups. Next, we perform extensive experiments in Sections IV-VI to explore the transferable knowledge in SHDA. Subsequently, in Section VII, we present several insightful discussions. Finally, we make conclusions in Section VIII. II. OVERVIEW In this section, we begin by defining SHDA, followed by concise review. Finally, we summarize the pipeline of SHDA. TABLE NOTATIONS. Notation Description Xs / Xt Ds / Dt Dl / Du xs / xu / xl ys / yl ns / nl / nu Source/Target feature space Source/Target domain Labeled/Unlabeled target domain the i-th sample in Ds / Dl / Du One-hot label of xs Number of samples in Ds / Dl / Du Number of categories / xl A. Notations and Definition i, yl , ys }nu i)}nl )}ns i=1, where xs Let Xs Rds and Xt Rdt be the source and target feature spaces, respectively. The source domain is denoted by Ds = {(xs Xs is the i-th source sample, and ys is its corresponding one-hot label over categories. Similarly, we denote the target domain by Dt = Dl Du = (xu i=1, where xl i=1{xu {(xl ) Xt is the i-th labeled (unlabeled) target sample, and yl is its associated one-hot label for xl among the categories. Based on those notations as summarized in Table I, the SHDA task is defined as follows. Definition 1: (SHDA). Under the SHDA setting, source domain Ds and target domain Dt are given, with samples drawn from distinct distributions. Also, source and target samples share the same categories, but there is no one-to-one correspondence between them. Moreover, Xs = Xt, ns nl, and nu nl. The goal is to train high-quality model using samples from both Ds and Dt and then apply the trained model to classify samples in Du. B. Overview Existing SHDA methods can be roughly categorized into two approaches, i.e., the shallow projection approach and deep projection approach. In the following, we provide review for those two approaches. Fig. 2. Experimental results on the NUS-WIDE+ImageNet-8 dataset [31], [32], which demonstrates that noise may contain transferable knowledge. Here, Text Image is vanilla SHDA task, whilst Noise Image is specialized SHDA task with pure noise as the source sample. In addition, SVMt and NNt are two supervised learning methods, whereas SHFA, CDLS, DDACL, TNT, STN, SSAN, and JMEA are seven SHDA methods. hensive experiments across nearly 330 SHDA tasks using two supervised learning methods and seven typical SHDA methods, including SVMt [33], NNt [34], SHFA [35], CDLS [36], DDACL [27], TNT [37], STN [24], SSAN [29], and JMEA [25]. Specifically, we first investigate how the category and feature information of source samples influence the performance of the target domain. To our surprise, this seemingly significant information is not dominant in affecting the performance of the target domain. Accordingly, we first hypothesize that noise drawn from simple distributions, e.g., Gaussian distribution, as source samples may contain transferable knowledge. Then, we conduct extensive experiments to verify this hypothesis, exemplified by the results on the NUS-WIDE+ImageNet-8 dataset [31], [32] shown in Fig. 2. Here, Text Image is vanilla SHDA task, while Noise Image is specialized SHDA task with pure noise as source samples. Our findings reveal that all the methods demonstrate comparable performance on both tasks. Based on this observation, we empirically confirm through extensive experiments that noise may indeed contain transferable knowledge, which can thus be utilized as source samples to improve the performance of the target domain. Building on the pivotal observation above, we synthesize various noise domains to conduct series of experiments aimed at uncovering the mystery of transferable knowledge in SHDA. Concretely, we first develop unified Knowledge Transfer Framework (KTF) and then perform large-scale experiments by creating various noise domains. Based on the KTF, we analyze the correlation between the transferability/discriminability of source samples and the performance improvement ratio in the target domain [38]. As result, we find that the core of transferable knowledge mainly lies in the transferability and discriminability of the source domain. Consequently, regardless of the origin of source samples (e.g., image, text, and noise), maintaining their transferability and discriminability is crucial for ensuring effective knowledge transfer in SHDA tasks. We highlight the contributions of this paper as follows. To the best of our knowledge, we are the first to empirically investigate the transferable knowledge in SHDA. We observe that noise drawn from simple distributions as source samples may contain transferable knowledge, 3 1) Shallow Projection Approach: Most existing SHDA methods fall into this approach, primarily utilizing the classifier adaptation and distribution alignment mechanisms for domain adaptation. Specifically, HFA [39], SHFA [35], and MMDT [40], [41] employ the classifier adaptation mechanism, which uses all samples from both domains to learn domain-shared classifier, aligning the discriminative structures of both domains. For instance, MMDT projects target samples into the source domain by training domain-shared support vector machine on labeled cross-domain samples. HFA and SHFA first augment the projected source and target samples with the original features and then learn support vector machine shared between domains. LS-UP [42], PA [43], SGW [44], and KPG [30] adopt the distribution alignment mechanism, which learns optimal projections by reducing the distributional divergence between domains. For example, PA first learns common subspace by dictionary-sharing coding and then alleviates the distributional divergence between domains. Recently, KPG regards labeled cross-domain samples as key samples to guide the correct matching in optimal transport. SCP-ECOC [45], SDASL [46], G-JDA [47], CDLS [36], SSKMDA [48], DDACL [27], and KHDA [25] take into account both the classifier adaptation and distribution alignment strategies. For example, G-JDA and CDLS perform the distribution alignment and classifier adaptation strategies in an iterative manner, and DDACL learns domain-shared classifier by both reducing the distributional discrepancy and enlarging the prediction discriminability. 2) Deep Projection Approach: With the advancement of deep learning techniques [4], some studies have utilized them to address the SHDA problem. Specifically, DTN [49] reduces the divergence of the parameters in the last layers between the source and target projection networks. TNT [37] simultaneously considers feature projection, sample categorization, and domain adaptation with deep neural networks. Deep-MCA [50] utilizes deep neural network to complete the heterogeneous feature matrix and find better measure function for distribution alignment across domains. STN [24] adopts the soft-labels of unlabeled target samples to align the conditional distributions between domains, and builds non-linear source and target projection networks. SSAN [29] considers the implicit semantic correlation and explicit semantic alignment mechanisms in heterogeneous transfer network. PMGN [28] constructs an end-to-end graph prototypical network to learn the domaininvariant category prototype representations, which not only mitigates the distributional divergence but also enhances the prediction discriminability. Recently, JMEA [25] jointly trains transferable classifier and semi-supervised classifier to screen high-confidence pseudo-labels for unlabeled target samples. C. Pipeline In summary, most SHDA methods generally follow the pipeline illustrated in Fig. 3. Specifically, they employ classification adaptation and distribution alignment mechanisms to jointly learn the source and target feature projectors, along with the classifier, from scratch in semi-supervised fashion. Note that the feature projectors are unique to each domain. Overall, SHDA methods excel at adapting to samples originating from distinct Fig. 3. In general, the SHDA pipeline integrates the classification adaptation and distribution alignment mechanisms to jointly learn the source and target feature projectors, along with the classifier, from scratch in semi-supervised manner. Notably, the feature projectors are unique to each domain. feature spaces, enabling effective domain adaptation even when the source and target domains differ significantly in their feature representations. This characteristic enhances their generality compared to homogeneous domain adaptation methods [15] [21], while also prompting our interest in investigating the fundamental issues of SHDA. III. SETUPS FOR EMPIRICAL STUDIES In this section, we introduce the experimental setup in detail, including datasets, baselines, and the evaluation metric. A. Datasets Following [24], [28], [29], we adopt three widely-used SHDA datasets, including Office+Caltech-10 [51], [52], Multilingual Reuters Collection [53], and NUS-WIDE+ImageNet-8 [31], [32]. The Office+Caltech-10 dataset comprises two sub-datasets, i.e., Office [51], and Caltech-256 [52]. The Office dataset includes 4,652 images across 31 classes collected from three different domains: Amazon (A), Webcam (W), and DSLR (D). The Caltech-256 (C) dataset consists of 30,607 images of 256 objects. We select 10 shared categories from those two datasets to create the Office+Caltech-10 dataset. In addition, we represent each image using two kinds of features: 800-dimensional SU RF (S800) [54] and 4096-dimensional DeCAF6 (D4096) [55]. In the following experiments, we designate A, C, and as source domains and C, W, and as target ones. For the source domain, we treat all images as labeled samples. As for the target domain, we randomly choose three images in each category as labeled samples, and the remaining images are regarded as unlabeled samples. The Multilingual Reuters Collection dataset [53] includes 111,740 articles, which are classified into six categories and written in five distinct languages: English (E), French (F), German (G), Italian (I), and Spanish (S). We utilize the bagof-words representation with term frequency-inverse document frequency features to represent each article. Subsequently, by following [24], [27], [29], [36], the principal component analysis [56] is performed to reduce the dimensionalities of features to 1131, 1230, 1417, 1041, and 807 for E, F, G, I, and Method Type URL for Code Publication TABLE II BASELINES UTILIZED IN THE PAPER. 4 ACM TIST 2011 Supervised Learning OSDI 2016 Supervised Learning Shallow Projection SHDA https://github.com/wenli-vision/SHFA release TPAMI 2014 Shallow Projection SHDA https://github.com/yaohungt/Cross-Domain-Landmarks-Selection-CDLS-/tree/master CVPR 2016 https://www.csie.ntu.edu.tw/cjlin/libsvm/ https://github.com/tensorflow/tensorflow SVMt [33] NNt [34] SHFA [35] CDLS [36] DDACL [27] Shallow Projection SHDA https://github.com/yyyaoyuan/DDA TNT [37] STN [24] SSAN [29] JMEA [25] Deep Projection SHDA Deep Projection SHDA Deep Projection SHDA Deep Projection SHDA https://github.com/wyharveychen/TransferNeuralTrees https://github.com/yyyaoyuan/STN https://github.com/BIT-DA/SSAN https://github.com/fang-zhen/Semi-supervised-Heterogeneous-Domain-Adaptation Pattern Recognition 2020 ECCV 2016 ACM MM 2019 ACM MM 2020 TPAMI 2023 S, respectively. In subsequent experiments, we designate the domain as the target domain, while the remaining domains are treated as the source domains. For the source domain, we randomly select 100 articles per category as labeled samples. As for the target domain, we randomly pick up five and 500 articles in each category as labeled and unlabeled samples, respectively. The NUS-WIDE+ImageNet-8 dataset contains the NUSWIDE [31] and ImageNet [32] datasets. The former comprises 269,648 images along with their corresponding tags from Flickr, and the latter consists of 3.2 million images and 5,247 synsets. By following [24], [29], [37], we select eight overlapping categories from those two datasets to build the NUS-WIDE+ImageNet-8 dataset. Also, we utilize the tag from NUS-WIDE and the image from ImageNet as the Text and Image domains, respectively. Furthermore, in line with [24], [29], [37], we adopt five-layer neural network to extract the 64-dimensional features for representing texts from the Text domain. Also, we employ the D4096 features to characterize the images from the Image domain. In the following experiments, we randomly sample 100 texts from each category within the Text domain to serve as labeled source samples. From the Image domain, three images per category are randomly selected as labeled target samples, whereas the remaining images are treated as unlabeled target samples. B. Baselines In the experiments, we utilize nine baselines, including SVMt [33], NNt [34], SHFA [35], CDLS [36], DDACL [27], TNT [37], STN [24], SSAN [29], and JMEA [25]. Here, SVMt and NNt are two supervised learning methods, while SHFA, CDLS, DDACL, TNT, STN, SSAN, and JMEA are SHDA methods. Among those SHDA methods, SHFA, CDLS, and DDACL belong to the shallow projection approach, while TNT, STN, SSAN, and JMEA belong to the deep projection approach. For clarity, we summarize all the baselines in Table II and list their details below. SVMt [33]. It solely utilizes labeled target samples to learn support vector machine. We utilize LIBSVM [33] to implement SVMt, and the regularization parameter (i.e., Eq. (1) in [33]) is set to 1. NNt [34]. It employs labeled target samples to train simple neural network. We implement NNt using the TensorFlow framework [34] with the following objective function as min f,gt 1 nl nl(cid:88) i="
        },
        {
            "title": "Lce",
            "content": "(cid:0)yl i, (gt(xl i))(cid:1) + τ (cid:0) gt2 + 2 (cid:1), (1) where Lce(, ) is the cross-entropy loss function, gt() is single-layer fully connected network with the Leaky ReLU activation function [57], and () is softmax classifier. We optimize Eq. (1) by utilizing the Adam optimizer [58] with learning rate of 0.01, and empirically set τ = 0.001. Also, the dimensionality of hidden layer representations is set to 256, and the number of iterations is specified as 100. SHFA [35]. It first augments projected source and target samples with original ones and then learns support vector machine in semi-supervised manner. For all tasks, we employ the default parameter settings described in Section 4.1 of [35], and the parameter λ (i.e., Eq (16) in [35]) is empirically fixed to 1. CDLS [36]. It identifies representative cross-domain samples during distribution alignment. The recommended parameter settings detailed in Section 4.1 of [36] are used on all tasks. DDACL [27]. It learns softmax classifier by both aligning the distributions across domains and enlarging the discriminability of cross-domain samples. As described in Section 5.1 of [27], we utilize the default parameter settings for all tasks, and the parameter τ (i.e., Eq (12) in [27]) is empirically set to 0.001. TNT [37]. It jointly performs feature projection, sample categorization, and distribution alignment in unified neural network framework. For all tasks, we follow the suggested parameter settings outlined in Section 4.1 of [37]. STN [24]. It adopts soft-labels of unlabeled target samples to reduce the conditional distributional divergence across domains and learns transferable classifier using labeled cross-domain samples. Following [24], we utilize the default parameter settings on all tasks. SSAN [29]. It learns heterogeneous transfer network by taking the implicit semantic correlation and explicit semantic alignment strategies into consideration. As presented in Section 4.1 in [29], the recommended settings of parameter are used in all the experiments, and the number of epochs is set to 1000. JMEA [25]. It simultaneously learns transferable classifier and semi-supervised classifier to acquire high-confident pseudo-labels for unlabeled target samples. For all tasks, we adopt the suggested parameter settings in Section 8.2 of [25] except for the parameter ρ (see Algorithm 2 in [25]). This parameter is empirically fine-tuned to achieve good performance across distinct tasks. Specifically, for tasks from the Office+Caltech-10 and Multilingual Reuters Collection datasets, ρ is set to be 0.0001. As for tasks from the NUSWIDE+ImageNet-8 dataset, ρ is set to 0.001."
        },
        {
            "title": "Given that",
            "content": "the data-loading modules within the open5 Fig. 5. The orders of category indices for source and target samples on all datasets. Here, we preserve the order of category indices for target samples while exclusively modifying that of source samples. Consequently, the task is considered as vanilla SHDA task only when the category indices of both source and target samples are aligned in order 1. is mapped to unique category index for learning and this index is merely numerical identifier without any semantic meaning. Thus, in the following experiments, we aim to explore how randomly permutating the category index of all source samples belonging to specific category to other categories could impact the performance of the target domain. To this end, we construct set of category-permutated SHDA tasks. Fig. 4 provides an instance of the category-permutated SHDA task, where the source and target domains have the same categories but with different orders of category indices. According to this setting, we design eight groups of transfer directions: (S800) (D4096), (S800) (D4096), (S800) (D4096), Text Image, S, S, S, and S. Here, the first three groups are from the Office+Caltech-10 dataset with total of 10 categories. Hence, we construct 10 SHDA tasks for each group by changing the order of category indices for source samples. Specifically, for source samples within the same category, we randomly permutate their category index to correspond to distinct category. For instance, if the category index of source samples is 1, we randomly change it to 5. As depicted in Fig. 5, the order 1 denotes the ground-truth order, while the other orders are permutated, resulting in shifts in the category information. It is essential to underscore that the order of category indices for target samples strictly adheres to the ground-truth order and remains unchanged throughout all tasks. Consequently, the SHDA task is considered vanilla SHDA task only when the order of category indices for source samples adheres to the order 1; otherwise, it is identified as category-permutated SHDA task. Similarly, we adopt the same operations to create eight SHDA tasks for the fourth group, and six SHDA tasks for each of the last four groups, based on their respective numbers of categories. Thus, we build total of 62 SHDA tasks. Fig. 6 shows average accuracies of all baselines w.r.t. distinct orders of category indices for source samples on all the above 62 tasks. According to the results, we can observe that as the orders of category indices for source samples change, the accuracies of all methods remain almost unchanged. The observation implies that those SHDA methods do not require the actual semantic categories in the source domain to perfectly correspond to those in the target domain. In other words, those SHDA methods primarily rely on aligning the category indices Fig. 4. An illustration of the category-permutated SHDA task, where source and target samples have identical categories but with different orders of category indices. source codes of various baselines are distinct, we standardize those modules into unified format. This allows for the flexible loading of experimental datasets by editing the corresponding configuration files. Please see details at https://github.com/yyyaoyuan/SHDA. C. Evaluation Metric Following [24], [28], [29], we employ the classification accuracy on unlabeled target samples as the evaluation metric, which is calculated as Accuracy = xu Du (cid:101)yu : xu : xu xu Du = yu , (2) and yu where (cid:101)yu denote the predicted and ground-truth one-hot labels for unlabeled target sample xu , respectively. Moreover, for fair comparison, we report the average classification accuracy for each method based on 10 random trials. IV. STUDY ON THE CATEGORY AND FEATURE INFORMATION OF SOURCE SAMPLES As illustrated in Fig. 3, in the SHDA problem, source and target samples are characterized by completely different types of features. Moreover, there is lack of paired crossdomain samples to facilitate learning the correspondence between source and target samples. However, even under such challenging circumstances, SHDA methods still yield effective transfer performance. This inspires us to delve deeper into source samples, each of which comprises both category and feature information. Accordingly, in this section, we investigate how the category and feature information of source samples influence the performance on the target domain, respectively. A. Study on Category Information of Source Samples via Category-permutated SHDA Tasks We first study how the category information of source samples affects the performance of the target domain. Under the SHDA setting, since source samples are labeled, the primary connection between source and target samples is the presence of small number of labeled target samples. Accordingly, the category information of those labeled target samples plays vital role in adapting source samples. Usually each category 6 (a) (S800) (D4096) (b) (S800) (D4096) (c) (S800) (D4096) (d) TextImage (e) ES (f) FS (g) GS (h) IS Fig. 6. Classification accuracies (%) with distinct orders of category indices for source samples. between source and target samples. One important reason is that the source and target feature projectors have completely different architectures and are trained from scratch. Therefore, permutating the category indices of source samples does not significantly affect the training of the target feature projector. Moreover, in Section VII-A we conduct several additional experiments under the homogeneous setting to empirically verify this perspective. Overall, all the results indicate that the category information of source samples is not primary factor influencing the performance of the target domain in SHDA. B. Study on Feature Information of Source Samples via Crossdataset SHDA Tasks In the aforementioned experiments, we change the category information of source samples, while their feature information remains unchanged. For instance, in the 10 SHDA tasks with the transfer direction of (S800) (D4096), the feature information of source samples is all S800. Consequently, in the subsequent experiments, our goal is to investigate the impact on the performance of the target domain when utilizing different feature representations for source samples. For this purpose, we design series of cross-dataset SHDA tasks. An example of the cross-dataset SHDA task is illustrated in Fig. 7, where source and target samples have different categories but are forcibly mapped to the same category indices. Adhering to the above setting, we treat the domains of Image and as two target domains, each comprising eight and six categories, respectively. For the former, we choose each source domain from domain set {Text, (S800), (S800), (S800), (D4096), (D4096), (D4096)}. As there are total of 10 categories in the domains of A, C, and W, we only utilize the samples belonging to the first eight categories as source samples. Accordingly, source and target samples can be assigned to the same category indices from 1 Fig. 7. Example illustration of the cross-dataset SHDA task. Here, source and target samples have different categories but are forcibly mapped to the same category indices. to 8. As for the latter, we adopt each domain from set {E, F, G, I, (S800), (S800), (S800), (D4096), (D4096), (D4096), Text} as the source domain. Analogously, for each domain in {A, C, W, Text}, we solely employ the samples associated with the first six categories as source samples. Thus, both source and target samples can be allocated to identical category indices ranging from 1 to 6. As result, we establish 18 SHDA tasks in total. Among those tasks, Text Image, S, S, S, and are vanilla SHDA tasks, while the rest tasks are cross-dataset SHDA ones. Fig. 8 presents the accuracies of all methods w.r.t. different source samples with distinct feature information. Based on the results, we can observe that the accuracy curves of most SHDA methods are almost stable. This is surprising observation as it indicates that those SHDA methods can achieve effective knowledge transfer across completely unrelated datasets. The underlying cause for this phenomenon may be that during domain adaptation, those SHDA methods rely solely on matching category indices to align source samples with target samples, even when source and target samples are completely 7 (a) Target domain: Image (b) Target domain: Fig. 8. Classification accuracies (%) of different source samples with distinct feature information. unrelated. By doing so, the distributions of source and target domains are aligned, thereby enhancing the performance of the target domain. On the whole, all the results imply that the feature information of source samples is not dominant factor in affecting the performance of the target domain in SHDA. C. Summary In summary, we make the following important observation. Observation 1: The category and feature information of source samples are not primary factors influencing the performance of the target domain in SHDA. V. STUDY ON NOISE AS SOURCE SAMPLES Based on Observation 1, it is evident that the performance of the target domain is not significantly influenced by either the category information or the feature information of the source samples. This observation indicates that the transferable knowledge from the source to the target domain may not inherently rely on the specific semantic categories or detailed feature representations in the source domain. Motivated by this insight, we revisit the necessity of utilizing vanilla source samples in SHDA tasks and propose an innovative hypothesis: Noise drawn from random distribution, when used as source samples, may still encapsulate transferable knowledge capable of supporting the adaptation. Next, we undertake comprehensive series of experiments to empirically confirm this hypothesis. A. Study on Source Samples via Noise-injection SHDA Tasks We first investigate the impact of injecting different proportions of noise into source samples on the performance of the target domain. To achieve this, we design several noise-injection SHDA tasks. Fig. 9 illustrates an example of noise-injection SHDA tasks, where source samples are mixed with distinct ratios of noise. Abiding by this the tasks of and example, we initially select (S800) (D4096) as the base tasks. Then, we utilize two different Gaussian mixture distributions to construct two distinct noise domains, i.e., NE and NA. In particular, to establish the NE domain, we directly sample noise from six distinct Gaussian distributions based on the number and dimensionalities of samples in each category of the domain. Here, each Gaussian distribution is characterized by unique mean sampled from standard Gaussian distribution and shares Fig. 9. Example illustration of the noise-injection SHDA task. Here, source samples are mixed with distinct ratios of noise. the same covariance, which is set to the identity matrix. This configuration simplifies the setup while ensuring that each distribution represents noise belonging to distinct categories. Similarly, we adopt the same strategy to create the NA domain using 10 different Gaussian distributions. Finally, we inject noise from the NE and NA domains into samples within the and (S800) domains, respectively. This injection is performed by using varying ratios, i.e., NEα = α (cid:102)NE + (1 α)(cid:101)E and NAα(S800) = α (cid:102)NA + (1 α)(cid:101)A(S800), where α ranges from 0 to 1 in an increment of 0.2, and (cid:101)X denotes the sample matrix from the domain, where samples are arranged in ascending order based on their category index. In principle, the larger the value of α, the higher the component of noise. When α equals zero, NEα and NAα (S800) become the source domains of and (S800), respectively. Conversely, when α equals one, NEα and NAα (S800) degenerate into the noise domains of NE and NA, respectively. Accordingly, we generate 12 noise-injection domains: NE0.0, NE0.2, NE0.4, NE0.6, NE0.8, NE1.0, NA0.0 (S800), NA0.2 (S800), NA0.4 (S800), NA0.6 (S800), NA0.8 (S800), and NA1.0 (S800), where the subscript denotes the value of α. As result, we construct total of 12 noise-injection SHDA tasks, i.e., NE0.0 S, NE0.2 S, NE0.4 S, NE0.6 S, NE0.8 S, NE1.0 S, NA0.0 (S800) (D4096), NA0.2 (S800) (D4096), NA0.4 (S800) (D4096), NA0.6 (S800) (D4096), NA0.8 (S800) (D4096), and NA1.0 (S800) (D4096). Fig. 10 shows the accuracies of all methods w.r.t. distinct ratios of noise on all the above tasks. From the results, we can see that as the proportion of noise increases, the performance of all methods remains nearly unchanged. Even when the source domains entirely degenerate into noise domains, i.e., α = 1, the performance of the target domain is still uncompromised. The results imply that even if source samples are disturbed by noise, it has no significant impact on the performance of the target domain. Those interesting observations again indicate that the category and feature information of source samples are not primary factors influencing the performance of the target domain. This aligns with the findings from the above experiments in Section IV. B. Study on Source Samples via Noise-based SHDA Tasks Building upon the above experimental results, we find that using noise drawn from random Gaussian mixture distributions 8 Fig. 11. Example illustration of the noise-based SHDA task. Here, source samples consist of noise drawn from random distribution without any semantic meaning, where the category indices of the target domain are randomly and uniquely assigned to each category of source noise. TABLE III THE STATISTICS OF NORMS OF THE MEANS AND COVARIANCES FOR THE NOISE DOMAINS, WHERE DENOTES THE TOTAL NUMBER OF CATEGORIES IN EACH NOISE DOMAIN, AND µc, Σc REPRESENT THE MEAN AND COVARIANCE OF CATEGORY IN EACH NOISE DOMAIN, RESPECTIVELY. Domain 1 (cid:80)C c=1 µc2 1 (cid:80)C c=1 ΣcF N6 1 N6 2 N6 3 N6 4 N6 5 N10 1 N10 2 N10 3 N10 4 N10 5 12.62 24.44 36.16 46.74 60.43 19.45 38.43 57.24 77.02 95. 105.34 210.32 315.39 420.53 525.05 164.80 330.82 496.16 661.60 824.46 methods remains steady despite variations in the means and covariances of source noise. Those results imply that the performance of the target domain is not sensitive to changes in the mean and covariance of source noise. 2) Study on Source Noise with Different Sample Numbers: To evaluate how the number of source noise affects the performance of the target domain, we construct 10 noise domains based on Gaussian mixture distributions, each containing varying number of source noise. Specifically, for each noise domain, we sample noise directly from (i.e., = 6 or 10) distinct Gaussian distributions. Each Gaussian distribution is characterized by unique mean drawn from the standard Gaussian distribution and shares common covariance matrix, which is set to the identity matrix. Moreover, the number of source noise per category differs across domains, ranging from 300 to 700 in an increment of 100. Additionally, the dimensionality of noise is consistently fixed at 300 across all noise domains. Based on the number of noise per category 300, NS6 in those noise domains, we denote them by NS6 400, NS6 600, and NS10 300, NS10 700, respectively, where the superscript denotes the total number of categories and the subscript represents the corresponding number of noise per category. As result, we build total of 10 noise-based SHDA tasks, i.e., NS6 S, NS6 600 S, NS6 (D4096), NS10 NS10 700 (D4096). We plot the accuracies of all methods w.r.t. the number of 400 S, NS6 500 300 (D4096), NS10 400 600 (D4096), and 700 S, NS10 500 (D4096), NS10 300 S, NS 700, NS10 400, NS10 500, NS10 500, NS6 600, NS6 (a) Target domain: (b) Target domain: (D4096) Fig. 10. Classification accuracies (%) with different proportions of nosies. as source samples (we refer to them as source noise for brevity) to perform SHDA is feasible and effective. To delve deeper into the influence of source noise on the performance of the target domain, we establish several noise-based SHDA tasks as exemplified in Fig. 11. Since the source noise lacks semantic meaning, we randomly and uniquely assign the category indices of all categories in the target domain to each category of source noise. Next, we consider the domains of and (D4096) as the target domains, respectively, and investigate how the following factors of source noise affect the performance of the target domain: (i) the mean and covariance of source noise; (ii) the number of source noise; (iii) the dimensionality of source noise; and (iv) the distribution of source noise. 2 1) Study on Source Noise with Different Means and Covariances: To explore the influence of source noise with different means and covariances on the performance of the target domain, we create 10 different noise domains, each derived from unique Gaussian mixture distribution. For each distribution, distinct means and variances are generated, where = 6 for the domain and = 10 for the (D4096) domain. The means are represented as cδ µc (c = 1, 2, . . . , C), while the variances are expressed as cδ Σc. Each mean µc is sampled from standard Gaussian distribution, and each variance Σc = PSD( Σ+Σ ), where Σ is matrix with elements drawn from standard Gaussian distribution. The operator PSD() sets all negative eigenvalues of its input matrix to zero while retaining nonnegative eigenvalues, ensuring that the resulting matrix is positive semi-definite. The scaling factor δ varies from 0.2 to 1.0 in increments of 0.2, producing 5 distinct Gaussian mixture distributions for each of the and domains. We summarize the norms of the means and covariances for those distributions in Table III. According to the number of categories in those 5, N10 noise domains, we denote them by N6 1 , N10 4 , and N10 5 , respectively. Here, the superscript denotes the total number of categories, while the subscript serves as unique identifier to distinguish between noise domains with distinct statistical properties. Furthermore, for all noise domains, the number of noise in each category is set to 100, with each noise having dimensionality of 300. Therefore, we build 10 noise-based SHDA tasks in total, i.e., N6 1 S, 1 (D4096), N10 4 S, N6 N6 2 (D4096), N10 4 (D4096), and N10 3 (D4096), N10 5 (D4096). 5 S, N10 2 S, 3 S, N6 2 , N10 3 , N10 2, N6 4, N6 3, 1, N6 Fig. 12 plots the accuracies of all methods w.r.t. various source noise characterized by distinct means and covariances. Based on the results, we find that the performance of all 9 (a) Target domain: (b) Target domain: (D4096) (a) Target domain: (b) Target domain: (D4096) Fig. 12. Classification accuracies (%) with various noise domains characterized by distinct means and covariances. Fig. 14. Classification accuracies (%) with different noise domains characterized by distinct dimensionalities. (a) Target domain: (b) Target domain: (D4096) Fig. 13. Classification accuracies (%) with different noise domains characterized by distinct sample numbers. source noise in Fig. 13. From those results, we find that the performance of all methods is almost constant as the number of samples changes. Those results suggest that the changes in the number of source noise do not have significant impact on the performance of the target domain. 3) Study on Source Noise with Different Dimensionalities: To assess how the dimensionality of source noise affects the performance of the target domain, we construct 10 noise domains with different dimensionalities, each sampled from unique Gaussian distribution. In particular, for each noise domain, noise is sampled directly from (i.e., = 6 or 10) different Gaussian distributions. Each of those Gaussian distributions has unique mean drawn from the standard Gaussian distribution, and all share common covariance that is set to the identity matrix. For distinct noise domains, the dimensionalities of noise range from 100 to 500 with an increment of 100. In addition, we fix the number of noise per category to 500 across different noise domains. According to the number of dimensionalities and categories in those noise 300, ND6 domains, we name them as ND6 400, ND6 300, ND10 100, ND10 500, respectively. Consequently, we construct 10 noise-based SHDA tasks in total, i.e., ND6 S, ND10 (D4096), ND10 500 300 500 (D4096). The accuracies of all baselines w.r.t. the dimensionality of source noise is presented in Fig. 14. We find that when varying the dimensionality of source noise, the performance of all methods remains relatively stable, which indicates that variations in the dimensionality of source noise do not significantly affect the performance of the target domain. 300 S, ND6 200 (D4096), ND10 200 S, ND6 100 (D4096), ND10 400 (D4096), and ND10 100, ND6 400, and ND10 400 S, ND6 100 S, ND 500, ND10 200, ND10 200, ND6 4) Study on Source Noise with Different Distributions: In the above experiments, all noise domains are drawn from (a) Target domain: (b) Target domain: (D4096) Fig. 15. Classification accuracies (%) with different noise domains characterized by distinct types of distributions. Gaussian distributions. To study the impact of noise domains with different types of distributions on the performance of the target domain, we generate six noise domains using different types of distributions, i.e., NG6, NU6, NL6, NG10, NU10, and NL10, where the superscript represents the total number of categories. Specifically, we utilize the same noise generation process described in the previous section to create the NG6 and NG10 domains, respectively. For the construction of the domains of NU6 and NU10, we sample noise per category from Uniform distribution, i.e., (10, 10), respectively. We build the NL6 and NL10 domains by sampling noise within each category from Laplace distribution, i.e., L(0, 1), respectively. Additionally, for fair comparison, in all noise domains, we fix the number of noise within each category to 100, and the dimensionality of noise is set to 300. As result, we establish six SHDA tasks in total, i.e., NG6 S, NU6 S, NL6 S, NG10 (D4096), NU10 (D4096), and NL10 (D4096). According to the results plotted in Fig. 15, we can observe that using different kinds of distributions has relatively minor impact on the performance of all methods. Those results suggest that similar phenomena observed with Gaussian distributions could occur with other types of distributions, indicating relatively general phenomenon. Those observations once again demonstrate that noise sampled from simple distributions may contain transferable knowledge. C. Summary All the above experimental results confirm that source noise drawn from simple distributions can yield effective knowledge transfer in SHDA tasks, thereby enhancing the performance of the target domain. This is in line with our expectations and corroborates our hypothesis. Accordingly, we summarize those results into pivotal observation. Observation 2: Noise drawn from random distributions could contain transferable knowledge for SHDA. Similar to Ll, we adopt the softmax classifier () and crossentropy loss Lce(, ) to instantiate the empirical risk of source noise, i.e., Ls. Thus, Ls is formulated by 10 VI. STUDY ON TRANSFERABLE KNOWLEDGE IN SHDA THROUGH SOURCE NOISE Observation 2 reveals that akin to vanilla source samples, source noise may harbor transferable knowledge. This observation is both surprising and intriguing, prompting us to explore further what knowledge from the source domain is useful for the performance of the target domain in SHDA. Accordingly, in this section, we utilize source noise to delve deeper into the transferable knowledge in SHDA, as it allows us to flexibly construct various source domains. A. Unified Knowledge Transfer Framework To gain deeper understanding of transferable knowledge in SHDA, we develop unified Knowledge Transfer Framework (KTF) to perform large-scale analysis experiments. Specifically, we first construct common subspace that serves as shared representation space for both the source and target domains. Within this subspace, we directly generate source noise, eliminating the need to learn source feature projector. This strategy not only simplifies the analysis but also facilitates more direct and focused exploration of the transferable knowledge encapsulated within the source noise. For simplicity, we denote source noise in the common subspace by Ds = {(xs )}ns i=1, where xs is the i-th source noise in the common subspace and ys is its associated one-hot label over categories. Then, drawing on several typical designs used in most SHDA methods [24], [25], [27], [29], [36], [47], we incorporate three crucial factors into KTF. (1) The empirical risk of labeled target samples, i.e., Ll, which characterizes the discriminability of labeled target samples with smaller values indicating higher discriminability. (2) The empirical risk of source noise, i.e., Ls, which quantifies the discriminability of source noise with smaller values signifying higher discriminability. (3) The distributional divergence between domains, i.e., Ls,t, which reflects the transferability of source noise with smaller values suggesting stronger transferability. Accordingly, the objective function of KTF is formulated as , ys Ll + βLs + µLs,t + τ (cid:0) gt2 + 2 (cid:1), (3) min f,gt where β, µ, and τ are positive trade-off parameters. Recall that gt() is single-layer fully connected network with the Leaky ReLU activation function [57] to project target samples into the common subspace and () is the domain-shared classifier. As result, the knowledge from source noise will be transferred into the target domain by optimizing the problem (3). Next, we elaborate on how to instantiate Ll, Ls, and Ls,t, respectively. The empirical risk of labeled target samples, i.e., Ll, refers to the average loss incurred by classifier when trained on labeled target samples. To achieve this, we utilize the softmax classifier to instantiate () and cross-entropy loss Lce(, ) to instantiate Ll. As result, we formulate Ll as Ll = 1 nl nl(cid:88) i="
        },
        {
            "title": "Lce",
            "content": "(cid:2)yl i, (gt(xl i))(cid:3). (4) Ls = 1 ns ns(cid:88) i="
        },
        {
            "title": "Lce",
            "content": "(cid:2)ys , (xs )(cid:3). (5) The distributional divergence between the source and target domains, i.e., Ls,t, aims to quantify the discrepancy in their distributions. To this end, we adopt simple yet effective method, Soft Maximum Mean Discrepancy [24], to instantiate Ls,t, which considers both the marginal and conditional distributional divergence. Accordingly, we formulate Ls,t as Ls,t ="
        },
        {
            "title": "1\nC + 1",
            "content": "C (cid:88) c=0 (cid:13) (cid:13)mc mc (cid:13) 2 (cid:13) , (6) where we assign all source noise and target samples to the 0-th category in the respective domains, mc represents the average of source noise in the c-th category, and mc denotes the average of target samples for the c-th category. Specifically, mc is defined as mc ="
        },
        {
            "title": "1\nIc(˜xs\ni )",
            "content": "(cid:80)ns i=1 ns(cid:88) i=1 Ic(xs )xs , (7) where Ic(x) is an indicator function that equals 1 if the sample belongs to category c, and 0 otherwise. Moreover, since the target domain contains large number of unlabeled samples, we follow [24] to adopt the soft-labels of unlabeled target samples provided by () and gt() to estimate mc . Hence, mc is defined by mc = (cid:80)nl i=1 gt(Ic(xl (cid:80)nl i) + (cid:80)nu i) + (cid:80)nu i,c stands for the predicted probability by () and gt() i=1 ˆyu i=1 ˆyu i,c i)xl Ic(xl i,cgt(xu ) (8) i= , where ˆyu that xu belongs to category c. In the implementation, we employ single-layer fully connected network with Leaky ReLU [57] and softmax activation functions to instantiate gt() and () in problem (3), respectively. We empirically set hyperparameters β and τ to be 0.1 and 0.05 for all tasks, respectively. Regarding the hyperparameter µ, we empirically set it to 0.1 for tasks with the target domain of S, and to 1 for tasks with the target domain of (D4096). Also, we fix the dimensionality of the common subspace to 256 and the number of iterations to 600. Moreover, we utilize the Adam optimizer [58] with learning rate of 0.001 to optimize problem (3). B. Analysis In this section, we utilize KTF to analyze the essence of transferable knowledge stored in source noise. 1) Analysis on Transferable Knowledge: Based on the objective function of KTF formulated in problem (3), we find that there are two primary factors (i.e., Ls and Ls,t) closely related to the transferable knowledge. The former represents the discriminability of the source domain, while the latter characterizes the transferability of the source domain. To analyze the impact of those two factors on the performance 11 (a) Target domain: (b) Target domain: (c) Target domain: (D4096) (d) Target domain: (D4096) Fig. 16. Correlation between Ls and Pr, as well as between Ls,t and Pr. Here, Ls represents the discriminability of the source domain, Ls,t characterizes the transferability of the source domain, and Pr denotes the performance improvement ratio in the target domain. of the target domain, we analyze the correlation between them and the performance improvement ratio in the target domain [38], respectively, where the performance improvement ratio denoted by Pr is defined as the ratio of the performance of KTF to that of NNt on unlabeled target samples. Hence, the larger Pr is, the better the transfer performance is. To assess such correlation, we employ the Spearman correlation coefficient [59], given its robustness to outliers and nonlinear property. The Spearman correlation coefficient ranges from 1 to 1 with values close to 1 or 1 indicating strong monotonic relationships, and values close to 0 indicating weak or no monotonic relationship. creases, Pr gradually decreases, with the Spearman correlation coefficient as -0.9935 and -0.88793 in the target domains of and (D4096), respectively. Both indicate strong negative correlation between Ls and Pr. Since smaller Ls corresponds to higher discriminability of the source domain, improving the discriminability of the source domain is crucial to ensure the positive transfer from the source domain to the target domain. (2) Fig. 16b and Fig. 16d illustrate that with an increase in Ls,t, Pr decreases gradually. Also, the Spearman correlation coefficients between Ls,t and Pr in the target domains of and (D4096) are -0.9935 and -0.87712, respectively. Those results imply that there is strong negative correlation between Ls,t and Pr. Since lower Ls,t indicates stronger transferability of the source domain, it is necessary to enhance the transferability of the source domain to achieve the positive transfer. (3) Based on all the above observations, we find that both the discriminability and transferability of the source domain strongly correlate with the transfer performance. Moreover, since the above experiments use randomly sampled source noise, it reveals an insightful observation: regardless of the origin of the domain of for source samples (e.g., image, text, noise), ensuring their discriminability and transferability in the common subspace can guarantee the transfer performance. This also explains why utilizing source noise can achieve comparable performance to that of vanilla source samples. 2) Analysis via Feature Visualization: To intuitively understand why positive transfer occurs when the source domain exhibits good discriminability and transferability, we utilize the t-SNE technique [60] to conduct feature visualization. Concretely, we first select two tasks that result in positive transfer, i.e., N6 and N10 (D4096), and then visualize their transfer results when the number of iterations is set to 1, 200, 400, and 600, respectively. The visualization results are shown in Fig. 17, which offers the following observations. (1) Based on Fig. 17a and Fig. 17e, we can see that at the first iteration, source noise is well separable in the common subspace, which intuitively implies that source noise has good discriminability. Also, we find that unlabeled target samples exhibit substantial overlap and are not easy to distinguish. This is reasonable because the target projector and classifier are in the early stages of learning, leading to poor discriminability of the target domain. (2) From Figs. 17b-17d and Figs. 17f17h, we find that as the training iteration proceeds, source noise maintains high discriminability and target samples from different categories become separated progressively. Also, the We begin by choosing the and (D4096) domains as the target domains, respectively. Then, we construct 200 noise domains, each derived from distinct Gaussian mixture distribution. For each distribution, we generate distinct means and variances, where = 6 for the domain and = 10 for the (D4096) domain. The means are expressed as cδ µc (c = 1, 2, . . . , C), and the variances as cδ Σc. Each mean µc is sampled from standard Gaussian distribution, and each variance Σc = PSD( Σ+Σ ), where Σ is matrix with 2 elements drawn from standard Gaussian distribution. The scaling factor δ ranges from 0.05 to 9.95 with step size of 0.1, resulting in 100 distinct Gaussian mixture distributions for each of the and domains. Additionally, to better simulate various practical scenarios, we randomly assign the number of samples for each category in each noise domain from the range of 100 and 1000. The dimensionality of each noise in all noise domains is uniformly set to 256, aligning with the dimensionality of the common subspace in KTF. Therefore, we build 100 noise-based SHDA tasks with as the target domain and another 100 with (D4096) as the target domain. For each transfer task, we record the values of Ls, Ls,t, and Pr every 10 iterations to alleviate correlation and capture essential trends. With 600 iterations for KTF, this results in 60 tuples {(Li i=1 for each transfer task. Furthermore, to capture the overall trends across different tasks within the same target domain, we average the 60 tuples generated for each transfer task to produce the final 60 tuples. Accordingly, we can analyze the correlation between Ls and Pr as well as between Ls,t and Pr. s,t, r)} s, Li Fig. 16 plots the curves of Pr as Ls and Ls,t change, respectively, and provides the corresponding Spearman correlation coefficients. We can summarize several insightful observations. (1) Fig. 16a and Fig. 16c show that as Ls in12 (a) N6 S: = 1 (b) N6 S: = 200 (c) N6 S: = (d) N6 S: = 600 (e) N10 (D4096): = 1 (f) N10 (D4096): = 200 (g) N10 (D4096): = 400 (h) N10 (D4096): = 600 t-SNE visualization on the tasks of N6 and N10 (D4096). Here, the + sign denotes source sample, the sign represents labeled Fig. 17. target sample, and the sign stands for an unlabeled target sample. Each color corresponds to distinct category, and is the current number of iterations. (a) N6 S: = 1 (b) N6 S: = 200 (c) N6 S: = 400 (d) N6 S: = 600 (e) N10 (D4096): = 1 (f) N10 (D4096): = (g) N10 (D4096): = 400 (h) N10 (D4096): = 600 Fig. 18. Cosine similarity between category means of source and target samples on the task of N6 and N10 (D4096). Here, denotes the current number of iterations, and the x-axis and y-axis of each plot correspond to category means in the source and target domains, respectively. distributions of both domains are gradually aligned. Those results indicate that due to the high discriminability of the source domain, it can be utilized as guidance information to enhance the discriminability of the target domain by aligning the distributions of both domains. In other words, as the transferability of the source domain continues to improve, its discriminability is gradually transferred to the target domain, thereby enhancing the discriminability of the target domain and resulting in positive transfer. 3) Analysis via Alignment Visualization: To gain clear understanding of whether the discriminability of the source domain gradually transfers to the target domain as its transferability increases, we visualize the alignment process. To be specific, we utilize the cosine similarity to measure the alignment between category means of source and target samples. The cosine similarity outputs score between 1 and 1, with higher scores indicating better alignment. Specifically, the alignment score across category means of source and target samples is calculated as αc,k = mc mc , mk mk , (9) and mc where mk are defined in Eq. (7) and Eq. (8), respectively. Note that we utilize the ground-truth labels of unlabeled samples to calculate mc , which can better reflect the discriminability of the target domain. Fig. 18 visualizes the alignment processes on the tasks of N6 and N10 (D4096), respectively. We can observe that as the number of iterations increases, the similarity between category means of source and target samples from 13 (a) Cs (D4096) Ct (D4096) (b) Ws (D4096) Wt (D4096) (c) Images Imaget (d) Ss St the discriminability of Fig. 19. Classification accuracies (%) with distinct orders of category indices for source samples on homogeneous transfer tasks. the same category steadily improves. Those results indicate that the target domain is steadily approaching that of the source domain. Given the high discriminability of the source domain, the target domain inherits its discriminability to some extent, leading to positive transfer performance. In nutshell, those observations provide evidence that as the transferability of the source domain increases, the discriminability of the source domain is progressively transferred to the target one. Wt (D4096), Images Imaget, and Ss St, where the subscripts (i.e., and t) denote the source and target domains, respectively. Following the category-permutated setting detailed in Section IV-A, we create 10 transfer tasks for the first two groups, eight for the third group, and six for the last group, based on the number of categories in each. Also, the groundtruth order is designated as order 1, while the remaining orders are permutated, leading to changes in the category information (please refer to Fig. 5 for details). Accordingly, we establish total of 34 homogeneous transfer tasks. C. Summary In summary, building on all the aforementioned experimental results, we can summarize them into an insightful observation. Observation 3: The principal source of transferable knowledge in SHDA tasks lies in the transferability and discriminability of the source domain. Also, regardless of the domain from text, noise), which source samples originate (e.g., as the transferability of its discriminability gradually transfers to the target domain, leading to positive transfer. Consequently, ensuring those properties in the source domain is crucial for achieving good transfer performance in SHDA. the source domain improves, image, VII. DISCUSSION In this section, we commence by conducting additional experiments to verify the influence of the feature projector on the performance of the target domain, aligning with the findings in Section IV-A. Then, we discuss several studies closely related to our observations. Finally, we highlight the potential value of those observations. A. Additional Experiments on Category-permutated Homogeneous Transfer Tasks"
        },
        {
            "title": "To assess the influence of",
            "content": "the feature projector on the performance of the target domain, we build series of category-permutated homogeneous transfer tasks. In particular, we first choose the domains of (D4096), (D4096), Image, and S. Then, we randomly and uniformly partition all samples in each domain into two parts: one for the source domain and the other for the target domain. For the source domain, we utilize all samples as labeled samples. As for the target domain, we randomly select 1% of the samples to be labeled, and the rest samples are considered as unlabeled ones. Consequently, we construct four groups of homogeneous transfer directions: Cs (D4096) Ct (D4096), Ws (D4096) In addition, we develop Homogeneous Classification Network (HoCN) to evaluate the performance of the target domain. Concretely, HoCN projects labeled samples from both domains into common subspace by training domainshared feature projector and classifier. Thus, we formulate the objective function of HoCN as min f,g 1 nl nl(cid:88) i="
        },
        {
            "title": "Lce",
            "content": "(cid:2)yl i, (g(xl i))(cid:3) + β ns ns(cid:88) i="
        },
        {
            "title": "Lce",
            "content": "(cid:2)ys , (g(xs ))(cid:3) + τ (cid:0) g2 + 2 (cid:1), (10) where g() stands for single-layer fully connected network with the Leaky ReLU activation function [57], while β and τ are two trade-off parameters empirically set to 0.01 and 0.005, respectively. Note that when β is set to zero, the problem in Eq. (10) degenerates into supervised learning problem that only utilizes labeled target samples for training. We denote the optimal model for this problem as HoCNβ=0. Fig. 19 shows the accuracies of HoCN and HoCNβ=0 w.r.t. different orders of category indices for source samples on all the above tasks. We can summarize several insightful observations. (1) When the category index of source samples is the groundtruth order, i.e., order 1, HoCN significantly outperforms HoCNβ = 0 on all the tasks. This is reasonable because source and target samples originate from the same domain, and HoCN uses more labeled samples than HoCNβ = 0. (2) When the category indices of source samples do not follow the ground-truth order, HoCN yields extremely poor performance. This implies that the order of category indices for source samples is crucial in scenarios where both source and target samples share feature projector. One important reason is that it is challenging to classify source and target samples, belonging to the same category, into different categories using shared feature projector. This disrupts the learning of the feature projector, leading to poor performance. Overall, all the observations provide evidence that the heterogeneity of the 14 source and target feature projectors is the primary cause of the phenomenon observed in Fig. 6. B. Comparison with Related Studies In the experiments presented in this paper, pivotal observation is that noise may contain transferable knowledge under the SHDA setting, which seems bit counter-intuitive. In reality, however, several studies [61][63] have paid attention to the value of noise for tackling distinct machine learning tasks. For instance, Baradad et al. [61] utilize noise to deal with the representation learning problem. Specifically, they pre-train visual representation learner with contrastive loss using noise generated from simple distributions, such as randomly initialized deep generative models. Their experiments demonstrate that the noise effectively enhances the representation ability of the visual representation learner. Another example is that Luo et al. [63] adopt noise to handle the non-independently and identically distributed (non-i.i.d.) problem in federated learning. Specifically, they first estimate the global mean and covariance information for each category. Then, based on such information, they sample noise from Gaussian mixture distribution to finetune the classifier on the server. Their experiments reveal that the noise substantially improves the classification performance. Similar to [63], Tang et al. [62] also apply noise to tackle the non-i.i.d. issue in federated learning. In particular, they first upsample pure Gaussian noise and then align the distributions of noise and vanilla samples in each client. Their experimental results verify that federated learning could significantly benefit from the noise. Overall, those studies indicate that noise can be beneficial for several machine learning tasks, which aligns with our observation to certain extent. Different from the above studies, we conduct comprehensive analytical experiments to delve deeper into the reason behind the effectiveness of noise for SHDA. C. Potential Value in Practical Applications Vanilla DA methods [5][7], [22] assume that source samples are publicly available. However, in many practical applications, it is often not easy to acquire those samples due to privacy, confidentiality, and copyright issues. To escape from this dilemma, potential solution, i.e., source-free domain adaptation (SFDA) (see the left in Fig. 20) [64][67], has been proposed in recent years. As rule, SFDA methods utilize well-trained source model to initialize target model and then adapt it using unlabeled target samples. While source samples are not publicly accessible under the SFDA setting, the source model trained on those source samples remains necessary. However, in several practical applications with strict privacy requirements, it may be challenging to ascertain the relationship between source and target samples based solely on public source model. This challenge hinders the further development of SFDA as we face an issue to determine which well-trained source models to be utilized for the target task. However, unlike the SFDA, our observations offer another promising solution (see the right in Fig. 20). It neither requires access to source samples nor well-trained source model. Instead, it directly samples noise from random distribution as source samples Fig. 20. comparison of the SFDA and our solution. To escape from the dilemma of unavailable access to source samples, SFDA methods rely on using well-trained source model, whereas our solution merely requires sampling noise from random distribution as substitute for source samples. and then performs domain adaptation in semi-supervised fashion. Accordingly, it eliminates both the dependence on publicly available source samples and models. As result, we believe that our observations provide new perspective to address the aforementioned dilemma, thus holding significance for various practical applications. VIII. CONCLUSION This paper conducts an in-depth empirical study to investigate the transferable knowledge in SHDA. First, we find that the category and feature information of source samples are not the primary factors affecting the performance of the target domain. Then, we observe that noise sampled from several simple distributions as source samples contributes to effective knowledge transfer. Next, we perform series of experiments to analyze the transferable knowledge in SHDA by constructing various noise domains. Building on extensive experimental results, we observe that both the transferability and discriminability of the source domain are strongly correlated with the performance improvement ratio in the target domain. Accordingly, we hold an opinion that the transferability and discriminability of the source domain are the dominant factors of the transferable knowledge in SHDA. Therefore, it is vital to ensure those properties in the source domain to achieve effective knowledge transfer. One promising direction for future work is to establish theoretical foundations that support those observations."
        },
        {
            "title": "REFERENCES",
            "content": "[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in NeurIPS, vol. 25, 2012. [2] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016, pp. 770778. [3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, in NeurIPS, vol. 30, 2017. [4] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature, vol. 521, no. 7553, pp. 436444, 2015. [5] S. J. Pan and Q. Yang, survey on transfer learning, TKDE, vol. 22, no. 10, pp. 13451359, 2010. [6] G. Csurka, Comprehensive Survey on Domain Adaptation for Visual Applications. Springer International Publishing, 2017, pp. 135. [7] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, comprehensive survey on transfer learning, Proceedings of the IEEE, vol. 109, no. 1, pp. 4376, 2020. [8] Q. Yang, Y. Zhang, W. Dai, and S. J. Pan, Transfer learning. Cambridge, U.K.: Cambridge Univ. Press, 2020. [9] Y. Zhang, P. David, H. Foroosh, and B. Gong, curriculum domain adaptation approach to the semantic segmentation of urban scenes, TPAMI, vol. 42, no. 8, pp. 18231841, 2019. 15 [10] Y. Ge, D. Chen, and H. Li, Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification, in ICLR, 2020. [11] X. Peng, Z. Huang, Y. Zhu, and K. Saenko, Federated adversarial domain adaptation, in ICLR, 2020. [12] C. Liu, K. Li, M. Stopa, J. Amano, and Y. Fu, Discovering informative and robust positives for video domain adaptation, in ICLR, 2023. [13] L. Hoyer, D. Dai, H. Wang, and L. Van Gool, Mic: Masked image consistency for context-enhanced domain adaptation, in CVPR, June 2023, pp. 11 72111 732. [14] P. Oza, V. A. Sindagi, V. V. Sharmini, and V. M. Patel, Unsupervised domain adaptation of object detectors: survey, TPAMI, 2023. [15] M. Long, J. Wang, G. Ding, S. J. Pan, and S. Y. Philip, Adaptation regularization: general framework for transfer learning, TKDE, vol. 26, no. 5, pp. 10761089, 2013. [16] M. Long, J. Wang, Y. Cao, J. Sun, and S. Y. Philip, Deep learning of transferable representation for scalable domain adaptation, TKDE, vol. 28, no. 8, pp. 20272040, 2016. [17] M. Long, Y. Cao, Z. Cao, J. Wang, and M. I. Jordan, Transferable representation learning with deep adaptation networks, TPAMI, vol. 41, no. 12, pp. 30713085, 2018. [18] T. Xu, W. Chen, P. Wang, F. Wang, H. Li, and R. Jin, Cdtrans: Crossdomain transformer for unsupervised domain adaptation, in ICLR, 2022. [19] X. Chen, S. Wang, M. Long, and J. Wang, Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation, in ICML, vol. 97, 2019, pp. 10811090. [20] Y. Zhang, T. Liu, M. Long, and M. Jordan, Bridging theory and algorithm for domain adaptation, in ICML, 2019, pp. 74047413. [21] H. Rangwani, S. K. Aithal, M. Mishra, A. Jain, and V. B. Radhakrishnan, closer look at smoothness in domain adversarial training, in ICML, 2022, pp. 18 37818 399. [22] O. Day and T. M. Khoshgoftaar, survey on heterogeneous transfer learning, Journal of Big Data, vol. 4, no. 1, p. 29, 2017. [39] L. Duan, D. Xu, and I. W. Tsang, Learning with augmented features for heterogeneous domain adaptation, in ICML, 2012, pp. 711718. [40] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell, Efficient learning of domain-invariant image representations, in ICLR, 2013. [41] J. Hoffman, E. Rodner, J. Donahue, B. Kulis, and K. Saenko, Asymmetric and category invariant feature transformations for domain adaptation, IJCV, vol. 109, no. 1, pp. 2841, 2014. [42] Y. H. H. Tsai, Y. R. Yeh, and Y. C. F. Wang, Heterogeneous domain adaptation with label and structure consistency, in ICASSP, 2016, pp. 28422846. [43] J. Li, K. Lu, Z. Huang, L. Zhu, and H. T. Shen, Heterogeneous domain adaptation through progressive alignment, TNNLS, pp. 111, 2018. [44] Y. Yan, W. Li, H. Wu, H. Min, M. Tan, and Q. Wu, Semi-supervised optimal transport for heterogeneous domain adaptation, in IJCAI, 7 2018, pp. 29692975. [45] M. Xiao and Y. Guo, Semi-supervised subspace co-projection for multiclass heterogeneous domain adaptation, in ECML PKDD, 2015, pp. 525540. [46] T. Yao, Y. Pan, C. W. Ngo, H. Li, and T. Mei, Semi-supervised domain adaptation with subspace learning for visual recognition, in CVPR, 2015, pp. 21422150. [47] Y. T. Hsieh, S. Y. Tao, Y. H. H. Tsai, Y. R. Yeh, and Y. C. F. Wang, Recognizing heterogeneous cross-domain data via generalized joint distribution adaptation, in ICME, 2016, pp. 16. [48] M. Xiao and Y. Guo, Feature space independent semi-supervised domain adaptation via kernel matching, TPAMI, vol. 37, no. 1, pp. 5466, 2015. [49] X. Shu, G.-J. Qi, J. Tang, and J. Wang, Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation, in ACM MM, 2015, pp. 3544. [50] H. Li, S. J. Pan, R. Wan, and A. C. Kot, Heterogeneous transfer learning via deep matrix completion with adversarial kernel embedding, in AAAI, vol. 33, 2019, pp. 86028609. [51] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, Adapting visual category [23] R. Bao, Y. Sun, Y. Gao, J. Wang, Q. Yang, H. Chen, Z.-H. Mao, and models to new domains, in ECCV, 2010, pp. 213226. Y. Ye, survey of heterogeneous transfer learning, 2023. [52] G. Griffin, A. Holub, and P. Perona, Caltech-256 object category dataset, [24] Y. Yao, Y. Zhang, X. Li, and Y. Ye, Heterogeneous domain adaptation California Institute of Technology, Tech. Rep. 7694, 2007. via soft transfer network, in ACM MM, 2019, p. 15781586. [25] Z. Fang, J. Lu, F. Liu, and G. Zhang, Semi-supervised heterogeneous domain adaptation: Theory and algorithms, TPAMI, vol. 45, no. 1, pp. 10871105, 2023. [26] J. T. Zhou, I. W. Tsang, S. J. Pan, and M. Tan, Multi-class heterogeneous domain adaptation, JMLR, vol. 20, no. 57, pp. 131, 2019. [27] Y. Yao, Y. Zhang, X. Li, and Y. Ye, Discriminative distribution alignment: unified framework for heterogeneous domain adaptation, Pattern Recognition, vol. 101, p. 107165, 2020. [28] Z. Wang, Y. Luo, Z. Huang, and M. Baktashmotlagh, Prototype-matching graph network for heterogeneous domain adaptation, in ACM MM, 2020, p. 21042112. [29] S. Li, B. Xie, J. Wu, Y. Zhao, C. H. Liu, and Z. Ding, Simultaneous semantic alignment network for heterogeneous domain adaptation, in ACM MM, 2020, p. 38663874. [30] X. Gu, Y. Yang, W. Zeng, J. Sun, and Z. Xu, Keypoint-guided optimal transport with applications in heterogeneous domain adaptation, in NeurIPS, vol. 35, 2022, pp. 14 97214 985. [31] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, Nus-wide: real-world web image database from national university of singapore, in CIVR, 2009, pp. 48:148:9. [32] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in CVPR, 2009, pp. 248255. [33] C.-C. Chang and C.-J. Lin, Libsvm: library for support vector machines, ACM TIST, vol. 2, no. 3, pp. 127, 2011. [34] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, Tensorflow: system for large-scale machine learning, in OSDI, 2016, pp. 265283. [35] W. Li, L. Duan, D. Xu, and I. W. Tsang, Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation, TPAMI, vol. 36, no. 6, pp. 11341148, 2014. [36] Y. H. H. Tsai, Y. R. Yeh, and Y. C. F. Wang, Learning cross-domain landmarks for heterogeneous domain adaptation, in CVPR, 2016, pp. 50815090. [37] W.-Y. Chen, T.-M. H. Hsu, Y.-H. Tsai, Y.-C. F. Wang, and M.-S. Chen, Transfer neural trees for heterogeneous domain adaptation, in ECCV, 2016. [38] W. Ying, Y. Zhang, J. Huang, and Q. Yang, Transfer learning via learning to transfer, in ICML, 2018, pp. 50855094. [53] M. Amini, N. Usunier, and C. Goutte, Learning from multiple partially observed views - an application to multilingual text categorization, in NeurIPS, 2009, pp. 2836. [54] H. Bay, T. Tuytelaars, and L. V. Gool, Surf: Speeded up robust features, in ECCV, 2006, pp. 404417. [55] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, Decaf: deep convolutional activation feature for generic visual recognition, in ICML, 2014, pp. 647655. [56] I. T. Jolliffe and J. Cadima, Principal component analysis: review and recent developments, Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, vol. 374, no. 2065, p. 20150202, 2016. [57] A. L. Maas, A. Y. Hannun, A. Y. Ng et al., Rectifier nonlinearities improve neural network acoustic models, in ICML, vol. 30, no. 1, 2013, p. 3. [58] D. P. Kingma and J. Ba, Adam: method for stochastic optimization, in ICLR, 2015. [59] C. Spearman, The proof and measurement of association between two things, The American Journal of Psychology, vol. 15, no. 1, pp. 72101, 1904. [60] L. Van der Maaten and G. Hinton, Visualizing data using t-sne. JMLR, vol. 9, no. 11, 2008. [61] M. Baradad Jurjo, J. Wulff, T. Wang, P. Isola, and A. Torralba, Learning to see by looking at noise, in NeurIPS, vol. 34, 2021, pp. 25562569. [62] Z. Tang, Y. Zhang, S. Shi, X. He, B. Han, and X. Chu, Virtual homogeneity learning: Defending against data heterogeneity in federated learning, in ICML, vol. 162, 1723 Jul 2022, pp. 21 11121 132. [63] M. Luo, F. Chen, D. Hu, Y. Zhang, J. Liang, and J. Feng, No fear of heterogeneity: Classifier calibration for federated learning with non-iid data, in NeurIPS, 2021, pp. 59725984. [64] J. Li, Z. Yu, Z. Du, L. Zhu, and H. T. Shen, comprehensive survey on source-free domain adaptation, TPAMI, 2024. [65] J. Liang, D. Hu, and J. Feng, Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation, in ICML, 2020, pp. 60286039. [66] M. Jing, J. Li, K. Lu, L. Zhu, and H. T. Shen, Visually source-free domain adaptation via adversarial style matching, TIP, 2024. [67] Y. Luo, Z. Wang, Z. Chen, Z. Huang, and M. Baktashmotlagh, Sourcefree progressive graph learning for open-set domain adaptation, TPAMI, 2023. Yuan Yao received the Ph.D. degree from the Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, in 2021. He previously served as senior research and development engineer at Baidu, China. He is currently working as researcher with the Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology. His research interests include transfer learning, federated learning, and artificial intelligent watermarking. Xiaopu Zhang received the M.Eng. and Ph.D. degrees in instrumentation science and technology from Jilin University (JLU), China, in 2016 and 2019, respectively. He is currently research fellow with Inspur Computer Technology Co., Ltd., Beijing, China. Prior to join Inspur, he was post-doctoral fellow with the School of Automation, Beijing Institute of Technology, Beijing, China. His research interests include machine learning and signal processing Yu Zhang (Member, IEEE) is an associate professor with the Department of Computer Science and Engineering, Southern University of Science and Technology. His research interests mainly include artificial intelligence and machine learning, especially in multi-task learning, transfer learning, dimensionality reduction, metric learning, and semi-supervised learning. He has published book Transfer Learning and about 80 papers on top-tier conferences and journals. He serves as reviewer for various journals and area chairs/(senior) program committee members for several top-tier conferences. He has won the best article awards in UAI 2010 and PAKDD 2019, and the best student article award in WI 2013. 16 Jian Jin received the B.E. degree from Beijing Jiaotong University, Beijing, China, in 1999, and the Ph.D. degree from the University of Chinese Academy of Sciences, Beijing, in 2019. He is currently the Director of the Institute of Industrial Internet and Internet of Things, China Academy of Information and Communications Technology. His research interests include domain name, industrial Internet identity, and blockchain. Qiang Yang (Fellow, IEEE) is fellow of Canadian Academy of Engineering (CAE) and Royal Society of Canada (RSC), Chief Artificial Intelligence Officer of WeBank, chair professor of Computer Science and Engineering Department, Hong Kong University of Science and Technology (HKUST). He is the conference chair of AAAI-21, the honorary vice president of Chinese Association for Artificial Intelligence(CAAI), the president of Hong Kong Society of Artificial Intelligence and Robotics (HKSAIR) and the president of Investment Technology League (ITL). He is fellow of AAAI, ACM, CAAI, IEEE, IAPR, AAAS. He was the founding editor in chief of the ACM Transactions on Intelligent Systems and Technology (ACM TIST) and the founding editor in chief of IEEE Transactions on Big Data (IEEE TBD). He received the ACM SIGKDD Distinguished Service Award, in 2017. He had been the founding director of the Huaweis Noahs Ark Research Lab between 2012 and 2015, the founding director of HKUSTs Big Data Institute, the founder of 4Paradigm and the president of IJCAI (2017-2019). His research interests are transfer learning, federated learning, artificial intelligence, machine learning, data mining and planning."
        }
    ],
    "affiliations": [
        "Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing 100095, China",
        "Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, and also with WeBank, Shenzhen 518052, China",
        "Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
        "Department of Research and Development, Inspur Computer Technology Co., Ltd., Beijing 100095, China",
        "Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing 100095, China"
    ]
}