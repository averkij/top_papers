{
    "paper_title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography",
    "authors": [
        "Mengchen Zhang",
        "Tong Wu",
        "Jing Tan",
        "Ziwei Liu",
        "Gordon Wetzstein",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 8 0 7 0 . 4 0 5 2 : r GenDoP: Auto-regressive Camera Trajectory Generation as Director of Photography Mengchen Zhang1,2, Tong Wu3(cid:66), Jing Tan4, Ziwei Liu5, Gordon Wetzstein3, Dahua Lin2,4(cid:66) 1Zhejiang University, 2Shanghai Artificial Intelligence Laboratory, 3Stanford University, 4The Chinese University of Hong Kong, 5Nanyang Technological University zhangmengchen@zju.edu.cn, {wutong16,gordon.wetzstein}@stanford.edu {tj023,dhlin}@ie.cuhk.edu.hk, ziwei.liu@ntu.edu.sg Figure 1. Overview. Top: DataDoP data construction. Given RGB video frames, we extract RGBD images and camera poses, then tag the pose sequence with different motion categories (in different colors). With LLM, we generate two types of captions from motion tags and RGBD inputs: Motion Caption describes the camera movements, while Directorial Caption describes the camera movements along with their interaction with the scene and directorial intent. Bottom: Our GenDoP method supports multi-modal inputs for trajectory creation. The generated camera sequence can be easily applied to various video generation tasks, including text-to-video (T2V) [13] and image-to-video (I2V) generation [15]. GenDoP paves the way for future advancements in camera-controlled video generation."
        },
        {
            "title": "Abstract",
            "content": "Camera trajectory design plays crucial role in video production, serving as fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, 1 large-scale multi-modal dataset containing 29K realworld shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movements generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes new standard for learningbased cinematography, paving the way for future advancements for camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/. 1. Introduction In video production, the camera serves as the window of observation, playing crucial role in presenting scene content, conveying the directors intent, and achieving visual effects. In recent years, video generation technology has advanced [1, 3, 24, 49], and several cutting-edge studies have explored camera-controlled video generation [13, 15, 30, 38]. However, these works often rely on predefined, simplistic camera trajectories to demonstrate their results. The generation of artistic, expressive, and intentional camera movements remains largely unexplored. Trajectory generation has been long-standing problem. Traditional approaches include optimization-based camera motion planning [4, 11, 28] and learning-based camera control [5, 9, 17, 20]. However, these techniques demand geometric modeling or cost function engineering for each motion, which limits creative synthesis. Meanwhile, oversimplified procedural systems impede precise text control. Recent advances in diffusion-based camera trajectory generation [8, 21, 25] have expanded creative possibilities for text-driven cinematography. However, CCD [21] and E.T. [8] inherit structural biases from human-centric tracking datasets, constraining camera movements to oversimplified character-relative motion patterns. Director3D [25] introduces object/scene-centric 3D trajectories from multiview datasets [46, 50], but the lack of trajectory-level captions limits text-to-motion alignment. As result, the generated paths are driven by geometric plausibility rather than directorial intent. These dataset constraints hinder the creation of artistically coherent free-moving trajectories that interpret creative vision without relying on specific subjects. In this work, we tackle the problems above with several key designs. First, we introduce DataDoP Dataset, multimodal, free-moving camera motion dataset extracted from real video clips, which includes accurate camera trajectories extracted by state-of-the-art and scene compositions. We extract camera trajectories and corresponding depth maps using MonST3R [47], and employ GPT-4o to generate comprehensive descriptions of the camera trajectories and scene focus, capturing both motion dynamics and directorial intent. DataDoP comprises over 29K shots, totaling 11M frames, with corresponding camera trajectories and diverse textual descriptions. Furthermore, given the inherently sequential nature of camera trajectories, we propose GenDoP, which treats camera parameters as discrete tokens and leverages an auto-regressive model for camera trajectory generation. Our model incorporates multi-modal condition as inputs, including fine-grained textual descriptions and optionally RGBD information from the first frame, to produce stable, complex, and instruction-aligned camera movements. We conduct rigorous human validation to ensure the dataset quality. Extensive experiments confirm that GenDoP outperforms state-of-the-art methods [8, 21, 25] across fine-grained textual controllability, motion stability, and complexity, while exhibiting enhanced robustness. As AIdriven video creation evolves, multi-modal camera trajectory generation emerges as timely and crucial direction. We believe that this work paves the way for future advancements in camera-controlled video generation and wide range of trajectory-related downstream applications. 2. Related Work Camera trajectory datasets. While several existing datasets document camera trajectories, their cinematographic expressiveness remains constrained. Datasets such as MVImgNet [46], RealEstate10K [50], and DL3DV10K [27] provide calibrated trajectories through structured capture methods, but predominantly focus on basic paths around static objects or scenes. These datasets lack the sophisticated cinematographic language necessary for narrative-driven sequencing and intentional viewpoint control. CCD [21] and E.T. [8] emphasize human-centric trackIn ing but are confined to reactive tracking mechanisms. contrast, DataDoPs camera movement is driven by the compositional logic of the scene and the narrative demands. We underscore DataDoPs unique contribution to the field of artistic camera trajectory generation. Camera trajectory generation. Early efforts in trajectory generation generally consist optimization-based motion planning [4, 11, 28, 29] and learning-based camera control [5, 9, 17, 20]. Recent progress focuses on integrating camera motion with scene and character dynamics. CCD [21] introduced camera diffusion model using text and keyframe controls, generating motion in character-centric coordinates. E.T. [8] improves to incorporate both character trajectories and camera-character text descriptions as control and generates trajectories in the global coordinates. On the other hand, Director3D [25] Dataset Traj Type Domain Traj Scene Intent #Vocab #Sample Caption Statistics #Frame #Avg (s) MVImgNet [46] RealEstate10k [50] DL3DV-10K [27] Object/Scene-Centric Object/Scene-Centric Object/Scene-Centric CCD [21] E.T. [8] DataDoP (Ours) Tracking Tracking Free-Moving Captured Youtube Captured Synthetic Film Film - - - 48 1790 22K 79K 10K 25K 115K 29K 6.5M 11M 51M 4.5M 11M 11M 10 5.5 85 7.2 3.8 14. Table 1. DataDoP Dataset. We compare the DataDoP dataset to other datasets containing camera trajectories. DataDoP is large dataset focusing on artistic, free-moving trajectories, each accompanied by high-quality caption annotations. The provided captions detail the camera movements, their interactions with scene content, and the underlying directorial intent. To capture more intricate camera movements, each video clip spans 10-20 seconds, averaging 14.4 seconds. trains DiT-based framework on object/scene-level multiview datasets to generate object/scene-centric camera trajectories. NWM [2] employs conditional DiT to plan camera trajectory via agents egocentric views. Unlike the diffusion models above, our approach uses an auto-regressive framework to generate trajectories that are precisely controlled by text instructions and RGBD spatial information. Auto-regressive models. Auto-regressive (AR) modeling employs tokenizers to transform inputs into discrete tokens and formulates generation as next-token prediction task with transformers. In recent years, great advancements are witnessed in auto-regressive modeling in image [12, 32, 40, 44], video [23, 41, 43], and 3D generation [7, 37, 39]. Early approaches [32, 44] serialize images into patch tokens and train transformer to autoregressively model the text and image tokens in sequential data stream. VAR [40] reformulates auto-regressive image generation as coarse-to-fine next-scale prediction. VideoPoet[23] leverages bidirectional attention for multimodal input conditioning in auto-regressive video generation. Our work extends auto-regressive modeling to camera trajectory generation, leveraging the discrete nature of camera tokens. Compared to diffusion-based methods, our model generates precise, coherent, and intricate artistic trajectories for long camera pose sequences. 3. DataDoP Dataset We introduce DataDoP, camera trajectory dataset extracted from long shots in artistic films, including both movies and documentaries, designed to capture freemoving, intricate, and expressive camera movements. As shown in Fig. 1, each sample in DataDoP consists of shotlevel camera trajectory, accompanied by the corresponding RGBD images and two types of trajectory captions: Motion captions, which accurately describe the camera motion alone, and Directorial captions, which detail the camera movements, their interaction with the scene, and the directorial intent. We describe the data construction pipeline in Sec. 3.1 and the dataset statistics in Sec. 3.2. 3.1. Dataset Construction Pre-processing. We curate and filter artistic videos from the internet, which are then segmented into shots using PySceneDetect 1. Captions are removed using VSR 2, after which the shots are merged with publicly available subset from MovieShots [33]. filtering process is applied to retain shots between 10 and 20 seconds in length, while removing those that are excessively dark or nearly static. Since our dataset focuses on free-moving camera trajectories, which enable unrestricted 3D camera motion within scenes and events, rather than tracking moving people or objects, we specifically filter for this category of data. GPT4o [18] was used to categorize the shots, removing those with static cameras or object-tracking motion. For details, please refer to Appendix A.2. Trajectory extraction. We then utilize MonST3R [47] to estimate the geometry of dynamic scenes. Camera trajectories are extracted along with the corresponding depth maps. The trajectories are subsequently cleaned, smoothed, and interpolated into fixed-length sequences. Motion tagging. We then partition the camera trajectories into segments of motion tags. Compared to existing datasets [8, 21], our captions explicitly incorporate descriptions of camera rotation, enabling more fine-grained characterization of camera movements. As result, our motion tags include both translation and rotation components (see Fig. 2a). For camera translation, excluding the static state, we consider six fundamental motions across three degrees of freedom: lateral (left/static/right), vertical (up/static/down), and depth (forward/static/backward). Each translation motion can be categorized into one, two, or three motions, resulting in total of 27 possible combinations. For camera rotation, aside from the static state, we consider six fundamental motions across three degrees of freedom: pitch (up/down), yaw (left/right), roll(left/right), resulting in 7 base actions. We do not consider the combination of these rotations, as in practical scenarios, rotation typically involves only one of these basic motions at time. We simplify by assuming that camera translation and rota1https://github.com/Breakthrough/PySceneDetect 2https://github.com/YaoFANGUK/video-subtitle-remover 3 (a) Distribution of Translation and Rotation Motion Tags. (b) Diverse Trajectories. Figure 2. Dataset Statistics. (a) The figure illustrates the composition and distribution of 27 translation motions (left) and 7 rotation motions (right), emphasizing the complexity and diversity of trajectories in our DataDoP dataset. (b) Based on the same caption, our dataset includes diverse trajectories that still conform to the given caption. As shown in the figure, the trajectories exhibit variations in terms of length, direction, and speed, effectively showcasing the diversity within our dataset. Score Acc Kappa Video-Traj Traj-Motion Alignment Traj-Directorial 0.863 0. 0.913 0.530 0.858 0.502 Quality 0.945 0.551 Table 2. Dataset User Study. Our user study demonstrates that our dataset exhibits excellent quality and human-alignment, with proven reliability of the results. tion are completely independent, which results in total of 27 7 possible combinations for camera motion tags. We adopt the motion tagging method from E.T. [8] to process the camera trajectories. For translation, we use an initial velocity threshold and velocity difference thresholds in different directions to determine the dominant velocity direction combinations. For rotation, we use an initial rotational velocity threshold to identify the unique dominant rotational direction. Finally, we combine the translation and rotation information to generate the complete tags, and apply smoothing to remove noise and sparse tags. These methods provide coarse temporal description of the camera trajectories, as shown in Fig. 1. Caption generation. Finally, we generate two types of trajectory captions based on the motion tags obtained in the previous stage, as shown in Fig. 1. First, we structure the motion tags by incorporating context, instructions, constraints, and examples, and then leverage GPT-4o to generate Motion captions that describe the camera motion alone. Next, we extract 16 evenly spaced frames from the shots to create 4 4 grid and prompt GPT-4o to consider both the previous caption and the image sequence. This enables GPT-4o to generate Directorial captions that describe the camera movement, the interaction between the camera and the scene, as well as the directorial intent. Further details can be found in Appendix A.2. 3.2. Dataset Statistics Trajectory types. We classify camera trajectories into four types: Static, Object/Scene-Centric, Tracking, and FreeMoving. Static shots keep the camera fixed. Object/SceneCentric shots capture multi-view data focusing on specific objects or scenes. Tracking shots track moving subject. Free-Moving shots allow unrestricted 3D camera motion, enabling complex scene exploration and dynamic framing, crucial for cinematic storytelling and creative expression. As shown in Tab. 1, DataDoP stands out by uniquely focusing on artistic, free-moving trajectories, capturing the directors creative vision and offering significant cinematic and artistic value. Unlike tracking shots, where the camera follows specific object, free-moving shots fluidly navigate the scene, enhancing visual storytelling without constraints. Data scale. DataDoP is built on long shots from the Internet. As shown in Tab. 1, it consists of 29K samples, spanning 12M frames and totaling 113 hours of footage, all with high-quality trajectory annotations. The dataset focuses on long shots averaging 14.4 seconds, capturing more complex camera movements compared to other datasets. While DL3DV-10K [27] has longer average duration, its camera trajectories lack directorial intent, emphasizing scene-level consistency rather than creative camera work. Statistics. We present the dataset statistics across four dimensions: Alignment, Quality, Complexity, and Diversity. To evaluate Alignment and Quality, we conducted user study with 8 experts. We selected 100 samples, including original videos, camera trajectories, and two captions: Motion and Directorial. The samples were split into two sets, each labeled by four users. For Alignment, we assess the consistency between the trajectory and video (VideoTraj), the motion caption and trajectory (Traj-Motion), and the directorial caption with both the trajectory and video scene (Traj-Directorial). For Quality, we assess whether the camera trajectory is free of breaks, roughness, or jitter. We use Fleiss Kappa [10] to measure inter-rater agreement among multiple users. As shown in Tab. 2, our dataset achieves high accuracy in both Alignment and Quality, with all Kappa values exceeding 0.4, confirming the reliability 4 Figure 3. Our Auto-regressive Generation Model. Our model supports multi-modal inputs and generates trajectories based on these inputs. By treating the task as an auto-regressive next-token prediction problem, the model sequentially generates trajectories, with each new pose prediction influenced by previous camera states and input conditions. of the results. For Complexity, as illustrated in Fig. 2a, we present the composition and distribution of motion tags within the dataset. For Diversity, as shown in Fig. 2b, the trajectories, while remaining consistent with the caption, exhibit significant variations in length, direction, and speed, effectively showcasing the diversity within our dataset. 4. Method 4.1. Overview We introduce GenDoP here, an auto-regressive method for camera trajectory generation. Previous trajectory generation methods [2, 16, 21, 25, 26, 39] largely relied on diffusion models [34], which often result in discontinuous and unstable trajectories (See Fig. 4). In contrast, we pioneer the application of auto-regressive models to trajectory generation. Auto-regressive models are well-suited for this task due to their ability to capture sequential dependencies. In trajectory, each poses position and orientation depend on the previous one, making the framework ideal for modeling the temporal and spatial continuity of trajectories. By conditioning each pose on its predecessor, the model effectively generates realistic and coherent 3D camera trajectories. GenDoP automatically constructs the cameras 3D motion path based on an input caption or appearance and geometry from the initial frame, capturing changes in both position and orientation. As illustrated in Fig. 3, GenDoP takes text description , optionally combined with the initial frames RGBD image (I0, D0), as input and generates the corresponding camera trajectory C. camera trajectory = {x0, x1, . . . , xN 1} is defined as sequence of consecutive camera poses, where each pose xi = [RitiKi] comprises rotation matrix Ri (orientation), translation vector ti (position), and an intrinsic matrix Ki (projection parameters). The intrinsic matrix Ki can be simplified to (fx, fy), assuming fixed principal point (cx, cy) and image dimensions (H, ). Our goal is to derive an auto-regressive generation function such that 5 = (T [, (I0, D0)]). The trajectory tokenization process is detailed in Sec. 4.2, while the generation method is comprehensively described in Sec. 4.3. 4.2. Camera Trajectory Tokenization 0 = Auto-regressive models commonly process information as discrete token sequences, making compact tokenization essential for efficient representation without sacrificing accuracy. Videos are naturally serialized into discrete frames and in this sense, camera trajectories from videos can be easily tokenized into discrete camera pose xi = [RitiKi] at each frame. This simplicity facilitates efficient tokenization, enabling compact encoding of the trajectory. Canonical normalization. We first establish scaleinvariant trajectory representation via canonical normalization. The camera frame is aligned as the world reference, setting Rnorm = and tnorm = 0. Subsequent poses are relativized through rigid transformation: Rnorm 0 Ri, ˆti = 0 (ti t0) for [1, ). Scale normalization then computes = max1i<N ˆti2 and projects translations to = ˆti/(s + ϵ) with ϵ = 105, maintainunit space via tnorm ing geometric consistency and numerical stability. Trajectory tokenization. For the resulting normalized parameters Rnorm and tnorm , we compute the correspondi ing quaternion representation for Rnorm and normalize all parameters to the range [0, 1], resulting in the vector (r1, r2, r3, r4, t1, t2, t3). Subsequently, the focal lengths fx, fy and the scale size are also normalized, yielding (f1, f2, s), which are then concatenated with the previously computed values. Finally, these parameters are multiplied by the discrete bin size and converted into integer values. Thus, for each xi, we can tokenize it into an integer vector of length 10, where the values are within the range [0, B]. As result, each camera trajectory can be tokenized into an integer vector of length 10N . Auxiliary tokens. Similar to prior auto-regressive approaches [7, 37, 39], we prepend BOS token at the beginning of trajectory sequence, append an EOS token at i Condition Method Dataset Text-Trajectory Alignment CLaTr-CLIP F1-Score Trajectory Quality Coverage CLaTr-FID Alignment User Study (AUR) Quality Complexity Motion Directorial CCD [21] E.T. [8] Director3D [25] Director3D [25] GenDoP(Ours) CCD [21] E.T. [8] Director3D [25] Director3D [25] GenDoP(Ours) Pre-trained Pre-trained Pre-trained DataDoP DataDoP Pre-trained Pre-trained Pre-trained DataDoP DataDoP RGBD&Text GenDoP (Ours) DataDoP 0.297 0.330 0.058 0.391 0.400 0.315 0.319 0.126 0.361 0.399 0.388 5.288 2.450 0.000 31.689 36.179 4.247 0.000 0.000 23.505 32. 30.231 0.332 0.020 0.171 0.839 0.872 0.416 0.014 0.348 0.802 0.854 0.855 357.822 609.906 542.385 31.979 22.714 240.216 758.923 348.312 35.538 34. 33.653 3.013 1.227 2.313 3.753 4.693 2.950 1.309 2.333 3.808 4.617 - 3.022 1.067 3.110 3.260 4.573 3.050 1.092 2.867 3.467 4. - 3.273 1.067 2.453 3.493 4.713 3.217 1.184 2.342 3.683 4.575 - Table 3. Quantitative Results. We present the quantitative results of our GenDoP across two text-conditional generation tasks and an RGBD & Text-conditioned task, comparing it with human-tracking methods CCD [21] and E.T. [8], as well as the object/scenecentric method Director3D [25]. Our model consistently outperforms all baselines across all metrics and caption subsets, confirming the effectiveness of both our dataset and auto-regressive framework, positioning GenDoP as state-of-the-art trajectory generation model. the end, and use PAD tokens to fill the necessary positions. During the tokenization process, we obtain + 1 integer values. Consequently, this tokenized representation can be discretized through learnable codebook R(B+4)L, where is the latent dimension. 4.3. Auto-regressive Generation We employ transformer-based auto-regressive architecture to establish bidirectional mapping between fixedlength camera trajectories and their compact latent representations. Although raw camera trajectories may vary in length, their spatial paths remain consistent after interpolation, allowing us to target fixed-length camera trajectories. Text-conditioned encoder. Our architecture comprises text encoder ET and an auto-regressive decoder for the base text-conditioned model, as shown in Fig. 3. The text encoder ET utilizes the pretrained and learnable text encoder from Stable Diffusion 2.1 (SD2.1) [34] to extract semantic features, which are then processed through an MLP to generate textual latent code ZT RMT L, where MT denotes the textual latent size and is the latent dimension. RGBD-conditioned encoder. For the RGBD-conditioned model, we introduce two separate encoders: EI for RGB image and ED for depth. Specifically, we expand the depth to R3HW to ensure it can be processed by the encoder. Both encoders use the pretrained and learnable CLIP Vision Model [19, 31, 35] to extract features, which are then passed through MLPs to generate latent codes ZI RMI and ZD RMDL. The final latent representation is the concatenation of the textual, RGB, and depth codes: = [ZT ; ZI ; ZD] RM L, = MT + MI + MD. (1) This combined representation integrates both visual and geometric modalities, conditioning the trajectory generation on the accompanying textual information. Auto-regressive decoder. The decoder is an autoregressive transformer designed to generate trajectory token sequence from the latent code and previously token IDs. We adopt the OPT architecture [48] as the decoder, as utilized in prior works [7, 39]. The latent code is prepended to the input sequence, positioned before the BOS token. For each token prediction, the decoder queries the learnable codebook R(B+4)L using the previous token IDs y0:P 1, producing the corresponding continuous token embeddings [y0:P 1] RP L, where denotes the length of the previous token sequence. The input embeddings for the decoder are then computed as: XP = PosEmbed([Z; [y0:P 1]]) R(M +P )L. (2) Stacked causal self-attention layers are then employed to predict the next feature based on XP . linear projection is applied to map the predicted feature to classification logits, which are subsequently used to retrieve the corresponding token ID yP . This process ultimately generates fixedlength trajectory token sequence. Loss function. The model is optimized with weighted sum of cross-entropy loss and regularization term: = CrossEntropy(S[1 :], ˆS[:, 1]) + λZ2 2, (3) where is the one-hot ground truth token sequence, ˆS is the predicted logits, and is the latent code. 5. Experiments 5.1. Experimental Setting Our GenDoP framework implements three conditional generation paradigms: (1) Motion captions for isolated camera motions, (2) Directorial captions for scene-synchronized trajectories, and (3) RGBD&Text, novel approach that integrates images and depth maps with Directorial captions through hierarchical feature fusion. All experiments for both training and inference are carried out with an Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz and single NVIDIA A100-SXM4-80GB GPU. We maintain consistency in parameters and strategies 6 Figure 4. Qualitative Results of Text-conditioned Trajectory Generation. We offer comparative analysis of text-conditioned trajectory generation in the figure. Our models trajectories (color-coded to highlight text alignment) remain stable and closely follow the instructions, while other models exhibit significant jitter or fail to match the instructions well. throughout training to ensure uniformity across the experimental setup. The image resolution is set to = 512, with trajectory length of = 60, discrete bin size = 256, and latent dimension = 1024. The textual latent size is MT = 77, the image latent size is MV = 257, and the depth latent size is MD = 257. The backbone OPT Transformer consists of 12 layers with 12 attention heads each. Training converges after 8 hours on single A100 GPU, yielding an inference throughput of approximately 3 seconds per trajectory. For evaluation, 3k samples are randomly selected from the DataDoP dataset as the test set, with the remaining data forming the training corpus. Implementation specifics are detailed in Appendix C.1. 5.2. Quantitative Results Metrics. We obtain the Contrastive Language-Trajectory embedding (CLaTr) [8] by leveraging the DataDoP dataset with CLIP-like approach [31]. random subset of 3k samples is selected as test set, from which CLaTr embeddings for both ground truth (GT) and generated data are extracted. Using these embeddings, we evaluate the model with two main metrics. (1) Text-Trajectory Alignment: We measure the similarity between text and trajectory embeddings using the CLaTr-CLIP (analogous to CLIP-Score [19]). We also replicate the motion tagging step from Sec. 3.1 to obtain the GT motion tags, which are then compared with the generated tags. Classifier F1-Score is computed by verifying the generated motion tags against the GT labels. (2) Trajectory Quality: The alignment between GT and generated trajectories is evaluated using CLaTr-FID (analogous to FID [14]). Additionally, Coverage evaluates how well the generated data spans the range of real data, with higher values indicating broader representation of the data distribution. Results. We report the quantitative results of our GenDoP across two text-conditional generation paradigms (Motion / Directorial) in Tab. 3. We compare it with previous trajectory generation methods. For the human-tracking methods, CCD [21] and E.T. [8], we assume the character remains static to simplify the camera trajectory inference process. For the object/scene-centric, text-only conditioned method, Director3D[25], in addition to the pretrained model, we also train version using DataDoP to emphasize the significance and effectiveness of our dataset for camera trajectory generation tasks. For the RGBD&Text-conditioned task, novel paradigm introduced by us, we present only the metric results for GenDoP. Our model demonstrates consistent superiority across all metrics and caption subsets, primarily due to the enhanced trajectory complexity and trajectory-aware captions in our dataset. This innovation enables more precise motion representation, significantly enhancing text-trajectory alignment. This is demonstrated by Director3D models trained on our dataset, which show dramatic leap in CLaTr-CLIP scores from 0 to over 30, transitioning from object-centric to trajectory-enriched training. Despite sharing the same training data, GenDoP outperforms DataDoP-trained Director3D by 4.5 (Motion) and 9.1 (Directorial) for CLaTrCLIP, while reducing CLaTr-FID by 9.3 (Motion) and 1.3 (Directorial), as confirmed by user studies. These results validate the effectiveness of our auto-regressive framework. Additionally, GenDoP demonstrates exceptional versatility in handling RGBD&Text-conditional tasks, showing strong multi-modal integration for high-quality trajectory generation under complex constraints. Collectively, the experiments confirm the effectiveness of both our dataset and auto-regressive framework, establishing GenDoP as stateof-the-art model for trajectory generation. User study. To establish human-aligned evaluation metrics, we engaged 27 domain experts in user study centered on three critical dimensions: Alignment (trajectory consistency with input text), Quality (smoothness, logical coherence, and seamless connectivity between sequential actions), and Complexity (kinematic sophistication of motion sequences under input constraints). We employed the Average User Ranking (AUR) metric to evaluate model performance, where domain experts assigned ranking scores (1-5) to the five competing models per task. Higher ranking scores indicate superior performance. We comparatively assessed five models on text-conditioned tasks (with 10 samder varying input conditions, as shown in Fig. 5. The results demonstrate that both models generate command-compliant trajectories when given identical textual inputs. However, the RGBD&Text-conditioned model shows superior scene adaptation by leveraging RGBD to incorporate geometric and contextual constraints. Specifically, as shown in the first row of Fig. 5, the spatial information from RGBD effectively mitigates ambiguities, i.e., left and forward in the text. This multimodal conditioning enables precise alignment with the 3D scene structure. 5.4. Ablation Studies Canonical normalization. We experiment with an alternative strategy that skips canonical normalization, directly using trajectories from Monst3r [47] with scale normalization for tokenization feasibility. These trajectories are scenecentered, with the 3D space focused around the scene. In contrast, canonical normalization transforms them into firstperson tracking paths. As shown in the table Tab. 4, applying canonical normalization significantly improves both alignment and quality, providing more consistent camera movements align with the instructions. Trainable Encoder. Contrary to conventional practice in text/image-conditional generation where pretrained encoders remain frozen to preserve prior knowledge, our experiments demonstrate comprehensive performance gains (see Tab. 4) by employing trainable encoders. This improvement arises from the encoders ability to adapt and bridge cross-modal gaps: through joint optimization, the visual encoder creates geometry-aware trajectory embeddings, while the text encoder learns motion-semantic relationships, resulting in more accurate alignment between text and camera movements. 6. Conclusion We propose DataDoP, pioneering dataset of expressive, free-moving camera trajectories from artistic videos, and GenDoP, an auto-regressive multimodal model for trajectory generation. Our approach innovatively incorporates RGBD information as input, enabling spatial data to guide trajectory supervision. This sets new benchmark, achieving state-of-the-art performance with superior controllability and intent alignment compared to existing methods. Limitations and future work. Currently, our multimodal approach combines text and first-frame RGBD to generate trajectories. Meanwhile, our dataset also extracts 4D point cloud during the extraction process but remains underexplored. Looking ahead, we aim to incorporate more modalities to enhance the adaptability and contextual awareness of the generated trajectories. In addition, we plan to unify trajectory and camera-controlled video creation for iterative creation of both trajectories and video content, establishing seamless pipeline for automated, artistic film production. Figure 5. Qualitative Results of RGBD & Text-conditioned Generation. This figure compares the impact of incorporating RGBD input on trajectory generation under identical text conditions. While both models generate command-compliant trajectories, the RGBD & Text-conditioned model demonstrates superior scene adaptation by utilizing RGBD data to integrate geometric and contextual constraints. Ablation Text-Traj Alignment Trajectory Quality Encoder Norm F1-Score CLaTr-CLIP Coverage CLaTr-FID 0.400 0.322 0. 36.179 14.917 31.420 0.872 0.766 0.866 22.714 68.590 22.841 Table 4. Ablation Study. We conduct an ablation study to evaluate the effectiveness of canonical normalization (see Sec. 4.2) and the trainability of the encoder (see Sec. 4.3). ples per task), excluding RGBD&Text-conditional scenarios with single-model baselines. As evidenced in Tab. 3, our approach outperformed others across all metrics, with results closely matching the earlier quantitative findings, validating its perceptual and technical coherence. 5.3. Qualitative Results Text-conditioned Generation. We present comparative analysis of Text-conditioned trajectory generation in Fig. 4. Our model not only achieves superior text-trajectory alignment but also maintains high-quality trajectory generation. Furthermore, the intricate input conditions highlight its capacity to produce sophisticated outputs with high-level complexity. In contrast, the DataDoP-trained Director3D captures basic motion patterns but exhibits trajectory jitter and instability. Furthermore, its object-centric variant pre-trained on [46, 50] generates orbit-dominated trajectories that exhibit no text correspondence, despite improved smoothness. Other baselines exhibit notably inferior performance in both text-trajectory alignment and quality. RGBD & Text-conditioned Generation. We conduct comparative analysis of our trajectory generation model un-"
        },
        {
            "title": "Appendix",
            "content": "The appendix provides detailed supplementary material on the DataDoP dataset and GenDoP method. It outlines data availability, ensuring compliance with YouTubes policies and detailing our data sharing practices. The dataset construction process is described in detail, including shot collection, quality filtering, and semantic categorization using GPT-4o [18]. Additionally, we provide further dataset statistics. The appendix also explains the tokenization details of camera trajectory data for model processing. Finally, it includes information on the experimental setup, along with additional ablation studies A. DataDoP Dataset A.1. Data Availability Statement and Clarification We are dedicated to upholding transparency and compliance in our data collection and sharing practices. Please take note of the following: Publicly Available Data: The data utilized in our studies is sourced from publicly available repositories. We do not access any exclusive or private data sources. Data Sharing Policy: Our data sharing policy is in line with established practices from previous works, such as [6]. Instead of providing raw data, we furnish YouTube video IDs essential for accessing the content. Usage Rights: The data we release is exclusively meant for research purposes. Any commercial use is not permitted under this agreement. Compliance with YouTube Policies: Our data collection and sharing practices strictly adhere to YouTubes data privacy and fair use policies. We ensure that user data and privacy rights are respected throughout the process. Data License: The data is distributed under Creative Commons Attribution 4.0 International License (CC BY 4.0). Furthermore, the DataDoP dataset is provided solely for informational purposes. The copyright for the original video content remains with the respective owners. All DataDoP videos are sourced from the internet and are not owned by our institution. We disclaim responsibility for the content and interpretation of these videos. In relation to the future open-source version, researchers must agree not to reproduce, duplicate, sell, trade, resell, or exploit any portion of the videos or derived data for commercial purposes, and refrain from copying, publishing, or distributing any part of the DataDoP dataset. A.2. Construction Details Pre-processing. Our dataset construction involves multistage curation process: Shot Collection: curated collection of cinematographically significant films and documentaries forms the foundation of our dataset. Using PySceneDetect1, we extract 43k initial shots through content-aware boundary detection. An optimized VSR pipeline2 is employed to eliminate textual overlays while maintaining visual integrity. To enhance processing speed and reduce misclassification, we focus the check area on the lower 1/5 of the frame. Finally, we merge this dataset with publicly available subset of MovieShots [33] to further diversify stylistic elements. Quality Filtering: We retained only shots with duration between 10 and 20 seconds. Statistics can be seen in Fig. R1a. Then we exclude sequences with static frames and low-light conditions. For each shot, we calculate the pixel-wise similarity between all pairs of consecutive frames. The similarity between two frames F1 and F2 is defined as: S(F1, F2) = (cid:80) i,j I(F1(i, j) = F2(i, j)) , where F1(i, j) and F2(i, j) represent the pixel values at position (i, j) in frames F1 and F2, respectively, and and are the height and width of the frames. To identify static frames, we compute the average similarity between all consecutive frame pairs in the shot. Specifically, for shot with frames, the average similarity is calculated as: = 1 1 1 (cid:88) k=1 S(Fk, Fk+1), where Fk and Fk+1 are consecutive frames in the shot, and 1 is the number of consecutive frame pairs. If the average similarity exceeds threshold (e.g., > 0.6), the entire shot is considered static and excluded. For each shot, the average brightness (mean gray value) for all frames is computed using the following formula: = 1 (cid:88) (cid:88) (cid:88) k=1 i=1 j=1 Fk(i, j), where is the total number of frames in the shot, and are the height and width of each frame, and Fk(i, j) represents the pixel value at position (i, j) in frame k. If the average brightness of shot is below predefined threshold (e.g., < 15), the shot is classified as too dark and excluded from the dataset. Semantic Filtering: We developed an automated categorization pipeline using GPT-4o. Following the definitions in Sec. 3.2, shots are classified into categories. Leveraging GPT-4o [18], we automate the categorization of shots into Static, Free-Moving, and Tracking. Shots 1https://github.com/Breakthrough/PySceneDetect 2https://github.com/YaoFANGUK/video-subtitle-remover 9 (a) Shot Length. (b) Trajectory Scale. Figure R1. Dataset Statistics in terms of video shot length and trajectory scale. classified as Object/Scene-Centric, which are common in multi-view datasets, are not considered in this study. We then discard the Static and Tracking shots. The detailed process and examples are shown in Fig. R2. Trajectory Extraction. We use MonST3R [47] to estimate the geometry of dynamic scenes, generating time-varying dynamic point cloud along with per-frame camera poses and intrinsics in feed-forward manner. This enables efficient video depth estimation and reconstruction [42]. Camera trajectories, along with the corresponding depth maps, are extracted for further processing. These trajectories undergo series of steps including cleaning, smoothing, interpolation, and standardization into fixed-length sequences, ensuring their suitability for subsequent training. Cleaning the Trajectories: To clean the camera trajectories, we first extract the camera translations from the transformation matrices and compute the velocities between consecutive frames. threshold is determined based on the 95th percentile of the velocity distribution, with an outlier exclusion factor α (set to 18.0). Frames with velocities exceeding this threshold are discarded. The remaining valid frames are then grouped into consecutive segments, ensuring that each segment contains at least 5 frames. Smoothing the Trajectories: After the trajectories have been cleaned, Kalman [22] filter, based on Constant Velocity model, is applied to smooth the valid frames within each segment. The smoothing process is performed using process and measurement noise standard deviations of 0.5 and 1.0, respectively. The smoothed segments are subsequently recombined with the original poses, resulting in cleaned and smoothed camera trajectory. This smoothing step serves to reduce noise and enhance the stability and accuracy of the trajectory, facilitating more reliable analysis in subsequent stages. 10 Interpolation into Fixed-Length Sequences: To ensure consistency across the trajectory data for downstream deep learning tasks, we standardize trajectories of varying lengths into fixed-length input sequences, addressing issues related to inconsistent time steps. First, spherical linear interpolation (SLERP) [36] is applied to the rotational component, while the translational component is interpolated linearly, ensuring smooth transitions between frames. The interpolated data is then padded to fixed length of 120 frames, ensuring uniform time steps across all trajectory samples. This process guarantees that the input sequences are consistent in length and temporal structure, providing stable and reliable training data for deep learning models. Motion Tagging. We present the distribution of translation and rotation combinations in Fig. R3. As shown, simpler motion combinations are more frequent, but motion tags still exhibit high diversity and complexity. Caption Generation. In Fig. R4, we present the specific prompts and cases for generating two types of captions. B. GenDoP Method B.1. Camera Trajectory Tokenization Details (rotation) and tnorm The camera trajectory tokenization process converts continuous camera parameters into discrete tokens. For the normalized parameters Rnorm (transi lation) obtained after canonical normalization, the rotation matrix Rnorm is converted into unit quaternion 3, 2, 1, = (r = 2, 1, (t 3) are processed. This results in the combined vector (r 2, 1, 3). Subsequently, the focal lengths fx, fy, and the scale factor are also acquired. The process involves three key stages: Rotation and Translation Normalization: The rotation 4), and the translation values tnorm 3, 4, 1, 2, Figure R2. Semantic Filtering. Following the definitions in Sec. 3.2, shots are classified. Leveraging GPT-4o [18], we automate shot categorization into Static, Free-Moving, and Tracking. Shots categorized as Object/Scene-Centric, common in multi-view datasets, are not considered in films. and translation components are tokenized as follows: and translation. rk = tk = + 1 2 , + 1 , {1, . . . , 4}, {1, . . . , 3}. This normalization maps values from the range [1, 1] to [0, 1], while preserving the constraints on both rotation Focal Length Adaptation: Normalize focal (fx, fy) relative to the principal points (cx, cy): lengths f1 = fx 10cx , f2 = fy 10cy . Here, cx and cy typically represent half the image dimensions. The factor of 10 ensures that the focal length values remain within the range (0, 1), accommodating typi11 Figure R3. Tag Distribution. The distribution of Translation and Rotation combinations is shown in the figure. Different tag modes are represented by shades of yellow, ranging from deep to light: Static, Translation only, Rotation only, and both Translation and Rotation. cal focal lengths. Scale Parameter Transformation: Apply logarithmic compression to the scale factor s: = log10(s) + 2 4 . This transformation enables linear representation of multiplicative scale changes across three orders of magnitude (0.01 100). Parameter Clamping and Discretization: All normalized parameters are clamped to the range [0, 1] before discretization into bins: ptoken = , r1, . . . , t3, f1, f2, s. This process generates compact 10-dimensional token that preserves the relative geometric relationships between parameters. The hyperparameter controls the trade-off between quantization error and codebook size. C. Experiments C.1. Experiments Setting Details We train GenDoP with batch size of 16 using the AdamW optimizer, with learning rate of 1e-5, (β1, β2) = (0.9, 0.95), and weight decay of 0.01. The KL loss weight is set to 1e-8. We use gradient accumulation step of 1. Training is performed using bfloat16 mixed precision. The model converges with the best results at the 100th epoch. C.2. Additional Qualitative Results In the supplementary video, we present additional cases with text prompts randomly generated by the LLM model [18]. These text prompts have never been seen in the training set, creating certain gap compared to the captions in our dataset. However, as shown in the video, despite the differences between the generated prompts and the training data, our model is still able to generate precise, high-quality, complex, and artistic trajectories. Furthermore, we use TrajectoryCrafter [45] to showcase how our trajectory generation method can be applied to camera-controlled video generation. This allows for the creation of videos that align with the camera descriptions provided, ensuring the generated video sequences match the specific visual and motion criteria described by the camera control inputs. 12 Figure R4. Caption Generation. We structure the motion tags by incorporating context, instructions, constraints, and examples, and then leverage GPT-4o to generate Motion captions that describe the camera motion alone. Next, we extract 16 evenly spaced frames from the shots to create 4 4 grid, prompting GPT-4o to consider both the previous caption and the image sequence. This enables GPT-4o to generate Directorial captions that describe the camera movement, the interaction between the camera and scene, and the directorial intent. C.3. Additional Ablation Studies We conduct ablation experiments on several hyperparameters, as shown in Tab. S1, including the number of discrete bins, trajectory length, and model size. These parameters correspond to the discrete bin size B, the trajectory length, and the model size (as detailed in Sec. 5.1). Specifically, for the small size, the latent dimension is = 512, with the backbone OPT Transformer consisting of 8 layers and 8 attention heads per layer. For the base size, the latent dimension is = 1024, with 12 layers and 12 attention heads. For the large size, the latent dimension is = 1536, with 16 layers and 24 attention heads. The results indicate that optimal performance is achieved 13 Ablation Text-Trajectory Alignment Trajectory Quality F1-Score CLaTr-Score Coverage CLaTr-FID Discrete bins Traj length Model size 64 128 256 512 1024 15 30 60 small base large 0.394 0.409 0.400 0.391 0.393 0.398 0.400 0. 0.389 0.400 0.398 33.594 35.824 36.179 35.201 34.277 34.576 36.179 34.523 32.868 36.179 33.843 0.751 0.851 0.872 0.882 0.884 0.863 0.872 0. 0.880 0.872 0.888 49.854 24.748 22.714 23.633 24.979 22.238 22.714 26.307 25.604 22.714 20.474 Table S1. Ablation Study on Hyperparameters. We conduct ablation experiments on several hyperparameters, including the number of discrete bins, trajectory length, and model size. These parameters correspond to the discrete bin size B, the trajectory length , and the model size (as detailed in Sec. 5.1). The results show that the optimal performance is achieved when the number of discrete bins is set to 256, the trajectory length to 30, and the model size to base. when the number of discrete bins is set to 256, the trajectory length to 30, and the model size to base. Notably, when the model size is set to large, although the performance in Text-texttt Alignment decreases, the Trajectory Quality improves. We speculate that this may be due to the larger models tendency to overfit, learning better trajectory quality while failing to follow the text instructions effectively."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Samuel Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, WeiCheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical AI. CoRR, abs/2501.03575, 2025. 2 [2] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, CoRR, and Yann LeCun. Navigation world models. abs/2412.03572, 2024. 3, 5 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. 2 [4] James F. Blinn. Where am i? what am looking at? [cinematography]. IEEE Computer Graphics and Applications, 8 (4):7681, 1988. [5] Rogerio Bonatti, Wenshan Wang, Cherie Ho, Aayush Ahuja, Mirko Gschwindt, Efe Camci, Erdal Kayacan, Sanjiban Choudhury, and Sebastian A. Scherer. Autonomous aerial cinematography in unstructured environments with learned artistic decision-making. J. Field Robotics, 37(4):606641, 2020. 2 [6] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. CoRR, abs/2411.00769, 2024. 9 [7] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang. Meshanything: Artistcreated mesh generation with autoregressive transformers. CoRR, abs/2406.10163, 2024. 3, 5, 6 [8] Robin Courant, Nicolas Dufour, Xi Wang, Marc Christie, and Vicky Kalogeiton. E.T. the exceptional trajectories: Text-to-camera-trajectory generation with character awareness. In ECCV (4), pages 464480. Springer, 2024. 2, 3, 4, 6, 7 [9] Steven Mark Drucker, Tinsley A. Galyean, and David Zeltzer. CINEMA: system for procedural camera movements. In SI3D, pages 6770. ACM, 1992. 2 [10] Joseph L. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378382, 1971. 4 [11] Quentin Galvane, Marc Christie, Christophe Lino, and Remi Ronfard. Camera-on-rails: automated computation of constrained camera paths. In MIG, pages 151157. ACM, 2015. 14 [12] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. CoRR, abs/2412.04431, 2024. 3 [13] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 1, 2 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NIPS, pages 66266637, 2017. 7 [15] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. CoRR, abs/2406.10126, 2024. 1, 2 [16] Biaozhang Huang, Xinde Li, Chuanfei Hu, and Heqing Li. Stochastic human motion prediction using quantized conditional diffusion model. Knowl. Based Syst., 309:112823, 2025. [17] Chong Huang, Chuan-En Lin, Zhenyu Yang, Yan Kong, Peng Chen, Xin Yang, and Kwang-Ting Cheng. Learning to film from professional human motion videos. In CVPR, pages 42444253. Computer Vision Foundation / IEEE, 2019. 2 [18] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. 3, 9, 11, 12 [19] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. 6, 7 [20] Hongda Jiang, Bin Wang, Xi Wang, Marc Christie, and Baoquan Chen. Example-driven virtual cinematography by learning camera behaviors. ACM Trans. Graph., 39(4):45, 2020. 2 [21] Hongda Jiang, Xi Wang, Marc Christie, Libin Liu, and Baoquan Chen. Cinematographic camera diffusion model. Comput. Graph. Forum, 43(2):iiii, 2024. 2, 3, 5, 6, 7 [22] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. 10 [23] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, MingHsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language In ICML. OpenRemodel for zero-shot video generation. view.net, 2024. 3 [24] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. CoRR, abs/2412.03603, 2024. 2 [25] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. In NeurIPS, 2024. 2, 5, 6, 7 [26] Jing Liang, Amirreza Payandeh, Daeun Song, Xuesu Xiao, and Dinesh Manocha. DTG : Diffusion-based trajectory generation for mapless global navigation. In IROS, pages 5340 5347. IEEE, 2024. 5 [27] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, and Aniket Bera. DL3DV-10K: large-scale scene dataset for deep learning-based 3d vision. In CVPR, pages 2216022169. IEEE, 2024. 2, 3, [28] Christophe Lino and Marc Christie. Intuitive and efficient camera control with the toric space. ACM Trans. Graph., 34 (4):82:182:12, 2015. 2 [29] Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, and Weiming Zhi. Splatraj: Camera trajectory generation with semantic gaussian splatting. CoRR, abs/2410.06014, 2024. 2 [30] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David B. Lindell. Sg-i2v: Self15 [43] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021. [44] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Trans. Mach. Learn. Res., 2022, 2022. 3 [45] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models, 2025. 12 [46] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Mvimgnet: largescale dataset of multi-view images. In CVPR, pages 9150 9161. IEEE, 2023. 2, 3, 8 [47] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. CoRR, abs/2410.03825, 2024. 2, 3, 8, 10 [48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068, 2022. 6 [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. [50] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Trans. Graph., 37(4): 65, 2018. 2, 3, 8 guided trajectory control arXiv preprint arXiv:2411.04989, 2024. 2 in image-to-video generation. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 6, 7 [32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821 8831. PMLR, 2021. 3 [33] Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and Dahua Lin. unified framework for shot type classification based on subject centric lens. In ECCV (11), pages 1734. Springer, 2020. 3, [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10674 10685. IEEE, 2022. 5, 6 [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 6 [36] Ken Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques, pages 245254, 1985. 10 [37] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes In CVPR, pages 19615 with decoder-only transformers. 19625. IEEE, 2024. 3, 5 [38] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. 2 [39] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. CoRR, abs/2409.18114, 2024. 3, 5, 6 [40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. [41] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. CoRR, abs/2410.02757, 2024. 3 [42] Tong Wu, Jiaqi Wang, Xingang Pan, Xudong Xu, Christian Theobalt, Ziwei Liu, and Dahua Lin. Voxurf: Voxel-based efficient and accurate neural surface reconstruction. In ICLR. OpenReview.net, 2023."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}