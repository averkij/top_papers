{
    "paper_title": "Solving Inequality Proofs with Large Language Models",
    "authors": [
        "Jiayi Sheng",
        "Luna Lyu",
        "Jikai Jin",
        "Tony Xia",
        "Alex Gu",
        "James Zou",
        "Pan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 2 9 7 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Solving Inequality Proofs with Large Language Models",
            "content": "Jiayi Sheng β, Luna Lyu α, Jikai Jinα, Tony Xiaα, Alex Guγ, James Zou α, Pan Lu α α Stanford University β UC Berkeley γ Massachusetts Institute of Technology Website: https://ineqmath.github.io/ Code Dataset Leaderboard"
        },
        {
            "title": "Abstract",
            "content": "Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop novel LLM-as-judge evaluation framework, combining final-answer judge with four step-wise judges designed to detect common reasoning flaws. systematic evaluation of 29 leading LLMs on IneqMath reveals surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and critical gap for current LLMs between merely finding an answer and constructing rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Figure 1: Final-answer accuracy versus overall accuracy for leading LLMs across different categories on the IneqMath benchmark of Olympiad-level inequality problems. Overall accuracy, measuring both answer correctness and step soundness, is substantially lower than final-answer accuracy for all model types. This highlights critical gap: while LLMs may find correct final answers to these inequality problems, their reasoning is often unsound. Each model used its optimal maximal tokens. * Co-first authors. Co-senior authors. (cid:12){jamesz,panlu}@stanford.edu Preprint."
        },
        {
            "title": "Introduction",
            "content": "Mathematical inequalities are fundamental to diverse fields such as analysis, optimization, and probability theory, with applications spanning scientific modeling, economics, and competitive mathematics. Proving an inequality is complex endeavor, demanding not just calculation but sophisticated blend of intuition for discovering tight bounds, strategic insight for selecting and applying classical theorems (e.g., AM-GM, Cauchy-Schwarz), and precise symbolic transformations. These skills are hallmarks of advanced mathematical reasoning, distinguishing inequality proving from general math problem-solving. Automating this process would therefore have broad impact: it could supply automated theorem provers (ATPs) with missing lemmas, accelerate formal verification processes, and serve as demanding testbed for general-purpose reasoners. However, despite impressive advancements in LLMs like DeepSeek-R1 [11] and OpenAI o3 [44], as well as in ATPs themselves [13, 15, 23, 31, 46, 67], automating inequality proving remains challenging frontier. major bottleneck in advancing LLM capabilities for inequality proving is the scarcity of suitable benchmarks. Existing resources fall short in several ways: general ATP collections like MiniF2F [74] and ProofNet [5] contain few inequalities; synthetic datasets such as INT [58] and AIPS [57] offer scale but may lack structural diversity due to template-based generation; and curated collections like ChenNEQ [6] are often too small for extensive training. More fundamentally, most existing datasets adopt fully formal representation, where problems and proofs are encoded in systems like Lean [8] or Isabelle [38]. While formal mathematical reasoning offers correctness guarantees and is vital research direction, LLMs, trained on vast corpora of natural language, often exhibit strong informal reasoning capabilities. This suggests LLMs might solve problems informally even when struggling with the exacting syntax of formal provers. Our work, therefore, aims to explore and benchmark these informal abilities, complementing formal mathematical AI by focusing on mode of reasoning closer to human intuition and the preliminary, often less structured, stages of mathematical discovery. To bridge this gap between formal rigor and intuitive problem-solving, we propose an informal yet verifiable formulation (2). Rather than requiring full, machine-checkable proofs within formal systems, we reformulate inequality problems into two concrete, automatically verifiable subtasks: (i) Bound estimationdetermine the largest (or smallest) constant that preserves the inequality; and (ii) Relation predictionidentify which relation (>, , =, , or <) holds between two expressions. Both tasks can be presented in natural language and LATEX, solved step-by-step by an LLM, and their final answers (a constant or relation symbol) can be automatically checked. This preserves the creative essence of inequality proving while avoiding the heavy overhead of formal proof assistants. Building on this formulation, we present IneqMath (3), the first large-scale dataset of Olympiad-level inequalities written entirely in informal language. The test set comprises 200 original problems, each crafted and reviewed by IMO-level medalists to ensure both originality and difficulty. The training corpus includes 1,252 problems sourced from advanced textbooks, automatically rephrased by LLMs into our subtasks and then meticulously reviewed by human experts. key feature is that each training problem is accompanied by up to four step-wise solution paths, providing rich data for training LLMs on fine-grained reasoning. Additionally, 76.8% of the training problems are annotated with 83 named theorems across 29 categories relevant to their solutions. As shown in Table 2, IneqMath surpasses prior resources in scale, diversity, and alignment with human-like, informal problem-solving approaches. However, producing the correct final answer is insufficient; the reasoning process itself must be sound. To rigorously assess this, we introduce an LLM-as-judge evaluation framework (4). This framework comprises high-precision final-answer judge to verify the answer equivalence, complemented by four specialized step-wise judges for step soundness. These step-wise judges are designed to detect the frequent reasoning flaws identified in our pilot studies: inappropriate reliance on toy case examples, unaddressed logical gaps, unjustified numeric approximations, and numeric calculation errors. Validated on manually labeled development set solutions, these judges demonstrate high reliability (F1 > 0.9 on average) and offer scalable method to scrutinize the deductive integrity of LLM-generated proofs. We evaluate 29 leading LLMs ranging from chat models to advanced reasoning LLMs, both opensource and proprietary (5). As key results highlighted in Figure 1, several key findings emerge. While specialized reasoning LLMs (e.g., o1 [41]) achieve higher final-answer accuracy than general-purpose chat models (e.g., GPT-4o [39]), this advantage often collapses under step-wise scrutiny. Once 2 our judges inspect every reasoning step, overall accuracy plummets by up to 65.5%. Indeed, even top-performing models like o1 achieve less than 10% overall accuracy  (Table 4)  , exposing fragile deductive chains and significant gap between finding an answer and constructing rigorous proof. Our in-depth study (5.3) reveals that while larger model sizes correlate with improved final-answer accuracy, their impact on overall accuracy is limited (e.g., o1 achieves only 8.0% overall accuracy). Similarly, extending test-time computation through longer reasoning chains offers diminishing returns for overall correctness (e.g., o1s 8.0% overall accuracy remains unchanged when scaling max completion tokens from 5K to 40K, while o3 [44] saturates around 31%). These findings suggest that current scaling approaches are insufficient for robust deductive reasoning in IneqMath. Instead, we explore promising improvement strategies, demonstrating potential gains from methods such as theorem-guided reasoningby providing golden theorems (improving overall accuracy by up to 11% for o3-mini [43] ) and critic-guided self-refinement (e.g., 5% absolute increase in overall accuracy for Gemini 2.5 Pro [19]). In summary, our work makes four key contributions: 1) We introduce an informal reformulation of inequality proving, decomposing the task into two verifiable subtasks (2). 2) We release IneqMath, an expert-curated benchmark of Olympiad-level inequalities and training corpus enriched with step-wise solutions and theorem annotations (3). 3) We develop modular LLM-as-judge framework that rigorously evaluates both final answers and proof step soundness (4). 4) We conduct systematic empirical study (5) that exposes pronounced gap between LLM performance and mathematical rigor, highlighting avenues for future research."
        },
        {
            "title": "2 Task Formalization: An Informal Perspective",
            "content": "Inequality proof problems require demonstrating that specified inequality holds under given conditions, such as proving + 2 ab for all positive real numbers and b. Traditionally, these problems are formalized in proof assistants like Lean or Isabelle, represented as tuple (S0, I, ), where S0 is the initial state, is the inequality, and is set of premises. The proof process, often modeled as Markov Decision Process, constructs step-by-step solution verified by the system. However, this formal approach demands expertise in specialized tools, while informal proofs in natural language, though more intuitive, are difficult to verify automatically due to their unstructured nature. To address these challenges, we propose an informal perspective that reformulates inequality proof problems into two verifiable subtasks: bound estimation and relation prediction. IneqMath Training Example 1: Bound Problem Question: Find the maximal constant such that for all real numbers a, b, c, the inequality holds: (cid:112)a2 + (1 b)2 + (cid:112)b2 + (1 c)2 + (cid:112)c2 + (1 a)2 Solution: Applying Minkowskys Inequality to the left-hand side we have (cid:112)a2 + (1 b)2 + (cid:112)b2 + (1 c)2 + (cid:112)c2 + (1 a)2 (cid:112)(a + + c)2 + (3 c)2 By denoting + + = x, we get (cid:112)(a + + c)2 + (3 c)2 = (cid:115) (cid:18) 2 (cid:19)2 3 2 + 9 (cid:114) 9 2 = 3 2 . Minkowskys Inequality Theorem: For any real number 1 and any positive real numbers a1, a2, . . . , an, b1, b2, . . . , bn (cid:33) 1 (ai + bi)r (cid:32) (cid:88) i=1 (cid:32) (cid:88) (cid:33) 1 ar + (cid:32) (cid:88) br (cid:33) 1 i=1 i= This bound estimation task involves finding an optimal constant for given inequality. For example, in + ab for a, > 0, the objective is to find the largest C. Formally, bound estimation problem instance is triple: Πbound = (cid:0)f (x), g(x), D(cid:1), where Rn. 3 Here, f, : are two expressions involving variables = (x1, . . . , xn) within specified domain (e.g., xi > 0, (cid:80) xi = 1), and g(x) > 0, D. The goal is to determine the extremal: = sup{C : (x) Cg(x), D} or = inf{C : (x) Cg(x), D}. The relation prediction task requires determining the correct relationship between two expressions. For instance, given expressions (x) = + and g(x) = 2 ab, the goal is to identify the relation (in this case, ) that holds for a, > 0. Formally, relation prediction problem instance is triple: Πrel = (cid:0)f (x), g(x), D(cid:1), where (x) and g(x) are expressions over variables in domain Rn. The goal is to find the relation between (x) and g(x) (i.e. >, , =, , <, or none of the above). IneqMath Training Example 2: Relation Problem Question: Let a, b, be positive real numbers such that abc = 1. Consider the following expressions: + Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above Solution: From the AM-GM Inequality, we have + a + + + + + ) ( + 3 + + (cid:32)(cid:114) = + + + (cid:32)(cid:114) bc + (cid:114) ca + (cid:114) (cid:33) ab bc + (cid:33) (cid:114) ca + (cid:32)(cid:114) ca + (cid:114) + ab c) + + (cid:33) (cid:32)(cid:114) + + 3 6 ab + + 2( = (cid:114) (cid:33) bc abc = + + + 3. AM-GM Inequality Theorem: If a1, a2, . . . , an are nonnegative real numbers, then (cid:88) ai (a1a2 . . . an) 1 1 i=1 with equality if and only if a1 = a2 = . . . = an. This is special case of the Power Mean Inequality. These subtasks are chosen because they frequently appear in mathematical problem-solving, simplify the evaluation process, and crucially, retain the core reasoning challenges inherent in original inequality proof problems. An ideal LLM solution should not only produce the correct final answer but also present clear, logically sound, and complete derivation. This includes strategic application of theorems, accurate symbolic manipulations and calculations, and justification of all critical steps. 3 IneqMath: The Inequality Problem Dataset This section describes the data curation and statistics of IneqMath, novel collection of inequality problems designed to support the informal perspective on solving and proving inequalities. Test data curation. To mitigate contamination from common sources (textbooks, contests, online resources) potentially in LLM training corpora, we commissioned IMO-level medalists to design novel inequality problems. These underwent rigorous review by separate expert group and were validated only upon unanimous confirmation of solvability, soundness, and ground truth correctness. Problems identified as easier by experts were excluded from the test set (repurposed for development) to ensure high challenge level. See the developed curation tool in A.1. We host an online leaderboard1 with automatic submission and evaluation, providing reliable and fair community platform. 1https://huggingface.co/spaces/AI4Math/IneqMath-Leaderboard 4 Training data curation. Training problems were sourced from two advanced textbooks featuring graduate-level and Olympiad-style inequality proof problems. We parsed these textbooks to extract proof problems, their step-wise solutions, and relevant theorems. We developed two LLM-based rephrasers to transform each source problem into two sub-tasks defined in 2: bound estimation and relation prediction. For instance, source problem like Prove + 2 ab for a, R+ would be rephrased into bound estimation task (e.g., Determine the the maximal constant such that ab holds for a, R+) and relation prediction task (e.g., Determine the inequality + relation in the expression + ( ) 2 Crucially, while rephrased problems are altered from the source proof problem in the format, they preserve the core mathematical reasoning and solution stepssuch as applying relevant theorems, determining boundary conditions, and verifying inequalities. An annotation tool (see A.1) was developed to facilitate human expert review and correction of the LLM-rephrased problems. Extracted theorems were curated, each including its name, natural language definition, and list of training problems where it is applicable. ab that holds for a, R+). Key statistics. As shown in Table 1, the IneqMath dataset comprises 200 test problems for benchmarking, 100 development problems with public ground truth, and 1,252 training problems split evenly between bound estimation and relation prediction tasks. Each training problem includes step-wise solutions, with up to four solutions per problem, and 76.8% (962 problems) are annotated with relevant theorems. The dataset features 83 named theorems across 29 categories, with their distribution illustrated in Figure 2. Test problem examples are provided in A.3. Statistic Theorem categories Named theorems Training problems (for training) - With theorem annotations - With solution annotations - Avg. solutions per problem - Max solutions per problem Dev problems (for development) Test problems (for benchmarking) Number Bnd. Rel. 29 83 1252 962 1252 1.05 4 100 200 - - 626 482 626 1.06 4 50 96 - - 626 480 626 1.05 4 50 104 Table 1: Statistics of the IneqMath dataset. Figure 2: Distribution of theorem categories. Comparison to existing datasets. As summarized in Table 2, IneqMath stands out for: (1) providing expert-curated training and test sets, (2) offering rich annotations with step-wise solutions and 83 grounded theorems, and (3) adopting an informal, accessible format for inequality proving through bound estimation and relation prediction, evaluated via LLM-as-judge. This design bridges the gap between formal proof systems and intuitive mathematical reasoning, making IneqMath unique resource for advancing LLM capabilities in problem solving and theorem proving. Potential contamination statement. To ensure rigorous evaluation, the IneqMath test set was commissioned from IMO-level medalists to feature novel problems, minimizing prior LLM pre-training exposure. The poor performance across models (5.2), particularly on overall accuracy (which demands step-wise correctness), strongly suggests the benchmark poses significant reasoning challenge, regardless of potential familiarity with underlying mathematical concepts. We therefore believe the IneqMath test set effectively probes novel problem-solving capabilities, and our conclusions on current LLM limitations in rigorous inequality proving remain robust."
        },
        {
            "title": "4 Fine-grained Informal Judges for Inequality Solving",
            "content": "The test split of the IneqMath dataset serves as our benchmark, comprising 200 Olympiad-level inequality problems that challenge both humans and current LLMs. Traditional evaluation methods fall short in this setting: expert annotation is accurate but prohibitively labor-intensive, while automated techniques such as string matching or value equivalence fail to capture step-by-step correctnessan essential aspect of inequality problem solving. To address this, we propose fine-grained LLM-asjudge framework, consisting of final-answer judge for verifying the predicted answer (4.1) and four specialized step-wise judges targeting common reasoning flaws (4.2). solution is deemed correct 5 Datasets Training Test / Dev #Theorem Solution Category Format Evaluation Data Source Data Annotation Problem and Evaluation Synthesized INT [58] AIPS [57] Data compilation MO-INT [57] MINIF2F [74] Autoformalization Autoformalization ProofNet [5] FormalMATH [69] Autoformalization leanWorkbook [68] Autoformalization Autoformalization Data compilation Proof or Bluff [45] Synthesized Synthesized CHAMP [35] Putnam Axiom [20] LiveMathBench [33] Autoformalization Data compilation Data compilation IneqMath (Ours) Expert annotated Expert annotated 35 8 83 Proof Proof Proof Proof Proof Proof Proof Proof Open Open Open Symbolic DSL Symbolic DSL Symbolic DSL Formal Formal Formal Formal Formal Formal Formal Informal Human judge Informal Human judge Informal Answer checking Informal Answer checking MC, Open Informal LLM-as-judge Table 2: Comparison of datasets for inequality and theorem proving. IneqMath provides expert-annotated training and test/dev sets, featuring high-quality named theorems and step-wise solutions for model development. Unlike prior datasets using synthesis or autoformalization, IneqMath presents problems in informal language across multiple-choice (MC) and open-ended (Open) formats, and employs LLM-as-judge for evaluation. overall only if it passes all five judges. As shown in Table 3, these judges achieve strong alignment with human annotations (F1 = 0.93), providing scalable yet reliable alternative to manual evaluation. LLM-as-Judge Judge type Accuracy Precision Recall F1 score Final Answer Judge Answer checking Toy Case Judge Logical Gap Judge Numerical Approximation Judge Numerical Computation Judge Step soundness Step soundness Step soundness Step soundness Average - 1. 0.91 0.96 0.96 0.71 0.91 1.00 0.86 0.95 0.95 0.68 0.89 1. 0.97 0.98 0.98 0.98 0.98 1.00 0.91 0.96 0.96 0.80 0.93 Table 3: Performance metrics of LLM-as-judge framework on development set. 4.1 Final Answer Judge LLM-generated solutions to IneqMath problems typically involve multiple reasoning steps followed by concluding answer statement. However, the final answer may vary in phrasing, structure, or numeric format, especially for bound estimation problems. For example, = 1 are 2 mathematically equivalent but differ in form. Recent work [34] evaluates LLM outputs via format normalization and exact string matching, without accounting for mathematical equivalence. To address this, we propose two-stage Final Answer Judge: it first identifies the concluding sentence with the predicted answer, then performs robust equivalence checking to assess mathematical correctness, even when the form differs from the reference. Prompt details and examples are in B.1. and = 2 2 4.2 Four Step-wised Judges Toy Case Judge. Inequality problems in IneqMath often require reasoning over continuous domains (e.g., all a, b, > 0), where specific numerical examples alone are insufficient for valid proof. LLM frequently generalize incorrectly from such examplese.g., claiming an inequality holds universally because it holds for = 1, = 2, = 3. Prior work [14] flags these under broad logical flaw category, lacking granularity for targeted analysis. Our Toy Case Judge addresses this by detecting unjustified generalization from toy examples. It prompts an LLM to flag conclusions based solely on specific instances without broader justification. See B.2 for prompts and examples. IneqMath inequality problems often involve multi-step derivations (e.g., Logical Gap Judge. algebraic manipulation, constrained optimization, functional transformations) needing explicit justification. LLMs, however, often skip key reasoning steps or assert conclusions without support (e.g., stating an optimal bound without derivation). Existing step-level evaluations [62] assess validity 6 (a) Final answer (b) Toy case (c) Logical gap (d) Numerical approximation (e) Numerical computation Figure 4: Confusion matrices for five judges, which exhibit strong agreement with human labels. and redundancy but lack granularity for such logical omissions. Our Logical Gap Judge addresses this by flagging missing transitions, unjustified claims, and vague derivations, especially in steps involving inequality transformations or bound estimation (see B.3 for details). Inequality problems in IneqMath often demand exact symbolic Numerical Approximation Judge. reasoning, where the use of numeric approximationse.g., replacing 2 with 1.414can compromise mathematical rigor. However, many LLM-generated solutions resort to such approximations during intermediate steps, leading to inaccurate or non-generalizable conclusions. To address this, we introduce Numerical Approximation Judge that flags inappropriate use of numeric approximationsspecifically when they affect derivations or final answers. Approximations used solely for intuition or side remarks are allowed. See B.4 for prompt details and examples. Numerical Computation Judge. Many IneqMath problems require explicit numerical computations after variable assignment (e.g., evaluating 27 or summing rational terms). While symbolic 2 reasoning is vital, arithmetic accuracy is equally crucial for overall correctness. Prior work (e.g., EIC-Math [27]) categorizes broad error types but often overlooks subtle miscalculations in multi-step derivations. Our Numerical Computation Judge addresses this by verifying arithmetic steps once variables are instantiated. It prompts an LLM to extract numerical expressions, convert them into Python code, and evaluate using floating-point arithmetic within small tolerance. This enables high-precision checking of both intermediate and final results. To further improve precision and mitigate floating-point issues, we encourage using symbolic mathematics packages such as SymPy, particularly for handling fractions and decimal numbers. Additional details are provided in B.5. 4.3 Effectiveness Verification of Judges holistic LLM judge baseline. To motivate our specialized judging system, we first evaluate heuristic LLM-as-judge baseline. This prompts single, general-purpose LLM to holistically assess IneqMath solution correctness, based on both final answer accuracy and step-wise soundness across the four reasoning categories in 4.2. As shown in the confusion matrix (Figure 3) using 80 human-annotated development examples, this naive approach exhibits poor agreement with human labels, underscoring its unreliability for rigorous evaluation in this domain. Figure 3: Confusion matrix for the judge baseline. In contrast, our proposed suite of five specialized judges Performance of our fine-grained judges. exhibits strong alignment with human evaluations. Figure 4 presents the confusion matrices for each judge on the same development set. The final answer judge (using GPT-4o-mini) achieves near-perfect agreement, while the four step-wise judges (chosen for balance of performance and cost as detailed in B.6) also demonstrate high fidelity. This confirms that decomposing the complex evaluation task into targeted sub-problems allows LLMs to serve as reliable evaluators. Qualitative analysis of judge disagreements. Despite the strong aggregate performance (overall F1 = 0.93, Table 3), LLM-as-judge evaluations are not perfect. Acknowledging the skepticism surrounding LLM-based evaluation, we conducted qualitative analysis of failure cases where our judges assessments diverged from human annotations. Detailed examples are provided in B.7. These instances underscore that while highly effective, our LLM judges can still struggle with the deep, nuanced understanding that characterizes expert human mathematical reasoning. Models Random Guess Frequent Guess Qwen2.5-Coder-32B [24] Llama-4-Scout [37] Qwen2.5-72B [47] Llama-4-Maverick [36] Qwen2.5-7B [48] Gemini 2.0 Flash-Lite [17] GPT-4o mini [40] GPT-4.1 [42] GPT-4o [39] Gemini 2.0 Flash [16] Grok 3 [60] QwQ-32B [3] Deepseek-R1 (Llama-70B) [9] Deepseek-R1 (Qwen-14B) [10] Deepseek-R1 [11] Qwen3-235B-A22B [49] Overall Acc () Bnd. All Rel. Answer Acc () All Bnd. Rel. No Toy Case Step Acc () No Logic. Gap No Approx. Error No Cal. Error All Bnd. Rel. All Bnd. Rel. All Bnd. Rel. All Bnd. Rel. - - - - - - 8.5 18.0 Heuristic Methods 0.0 16.7 9.4 26.0 - - - - Open-source Chat LLMs - - - - - - - - - - - - - - - - - - - - 1.539.0 1.532.0 2.539.5 2.538.0 3.032.0 1.531.5 2.037.5 2.538.0 3.034.5 3.046.0 3.551.0 2.047.5 3.550.0 5.035.5 5.044.5 6.035. 1.050.0 1.928.9 2.144.8 1.020.2 3.147.9 1.931.8 2.143.7 2.932.7 2.138.5 3.826.0 40.5 51.0 30.8 33.5 46.9 21.2 42.0 51.0 33.7 40.5 45.8 35.6 35.0 40.6 29.8 36.0 27.1 44.2 30.5 15.6 44.2 54.5 53.1 55.8 42.5 28.1 55.8 44.5 32.3 55.8 3.0 3.5 5.0 4.0 4.5 3.8 2.1 4.2 2.9 4.2 5.8 3.8 4.2 5.8 3.1 Proprietary Chat LLMs 2.141.7 1.022.1 1.041.7 2.933.6 0.031.3 4.844.2 2.138.5 3.830.8 3.156.3 2.936.5 4.262.5 2.940.4 33.0 43.8 23.1 39.5 42.7 36.5 40.5 31.3 49.0 37.5 40.6 34.6 49.0 59.4 39.4 54.5 66.7 43.3 11.5 11.5 11.5 29.0 11.5 45.2 16.0 12.0 19.0 32.0 21.9 43.0 15.5 13.5 17.3 17.0 13.7 20.2 3.8 3.1 3.5 2.1 2.9 2.5 8.3 11.5 10.0 3.1 3.8 3.5 13.5 7.3 19.2 16.0 11.6 20.2 Open-source Reasoning LLMs 2.152.1 1.943.3 5.253.1 1.947.1 6.336.4 3.834.7 4.263.5 5.826.9 3.132.3 8.737. 49.5 54.2 45.2 53.5 58.3 49.0 40.5 42.7 38.5 49.5 67.7 32.7 41.0 35.4 46.2 26.0 25.0 26.9 23.0 24.0 22.1 21.0 18.8 23.1 57.0 53.1 60.9 35.0 30.2 39.4 29.5 20.1 37.5 26.0 20.9 30.8 21.0 19.8 22.1 17.5 6.3 27.9 36.0 26.0 45.2 90.5 93.0 91.0 89.0 92.5 73.0 90.0 59.5 92.5 55.5 36.0 21.0 35.5 35.5 81.0 31. 96.9 94.8 94.8 91.7 96.9 77.1 91.7 66.7 93.8 60.4 42.1 20.8 38.5 38.5 95.8 28.1 84.6 91.3 87.5 86.5 88.5 69.2 88.5 52.9 91.4 51.0 30.8 21.2 32.7 32.7 67.3 33. 89.6 88.5 92.5 92.7 95.0 94.8 95.0 92.7 92.7 93.0 87.5 92.3 95.2 97.1 93.3 87.5 90.5 92.7 93.0 92.7 93.5 94.0 93.8 94.5 94.8 93.0 93.3 93.3 94.2 94.2 94.2 96.8 90.4 87.0 87.0 85.0 95.0 92.5 93.8 91.3 82.3 84.6 89.6 91.7 78.8 99.0 91.3 91. Proprietary Reasoning LLMs Claude 3.7 Sonnet [4] Gemini 2.5 Flash [18] Grok 3 mini [61] Gemini 2.5 Pro [19] o1 [41] o3-mini [43] o4-mini [44] o3 [44] Average Accuracy () Average Error Rate () 2.144.8 1.935.6 2.040.0 3.11.1 5.80.9 4.51.0 4.268.7 7.762.5 6.065.5 7.31.0 4.81.0 6.01.0 7.355.2 8.753.8 8.054.5 9.553.0 7.362.5 11.544.3 15.549.5 14.648.9 16.350.0 21.016.0 18.811.4 23.120.2 5.038.0 4.5 42.9 5.533.5 95.038.0 95.5 42.9 94.533.5 5.5 7. 42.0 46.9 37.5 4.2 6.7 71.5 72.9 70.2 8.3 5.8 62.5 62.5 62.5 62.5 69.8 55.8 65.0 63.5 66.3 37.0 30.2 43.3 49.0 36.5 60.6 88.0 84.4 91.3 24.0 16.7 30.8 88.5 83.3 93.3 34.5 37.5 31.7 37.0 34.4 39.4 62.0 58.3 65.4 93.5 91.7 95.2 3.1 4.8 4.0 13.5 7.3 19.2 19.5 11.5 26.9 19.0 12.5 25.0 17.5 12.5 22.1 22.0 17.7 26.0 26.0 25.0 26.9 39.5 28.1 50.0 93.5 91.3 95.8 100.0 100.0 100.0 53.5 44.2 63.5 100.0 100.0 100.0 75.0 99.0 86.5 63.5 92.7 77.5 82.7 90.6 86.5 84.6 99.0 91.5 93.0 90.6 95.2 100.0 100.0 100.0 91.0 87.5 94.8 99.5 100.0 99.0 99.5 100.0 99.0 93.3 96.9 95.0 93.3 92.7 93.0 97.1 96.9 97. 43.0 47.4 39.0 57.0 52.6 61.0 40.3 34.8 45.5 59.7 65.2 54.5 15.0 11.0 18.7 85.0 89.0 81.3 73.1 26.9 77.9 22.1 68.8 31. 93.2 93.7 6.3 6.8 92.8 7.2 Table 4: Evaluation performance of chat and reasoning LLMs on the IneqMath benchmark (the test set). Bnd. denotes bound problems and Rel. denotes relation ones. We report: (1) Overall Acc, which reflects the correctness of both the final answer and intermediate steps; (2) Answer Acc, which measures final answer correctness alone; and (3) Step Acc, which evaluates the accuracy of intermediate steps across four error categoriesToy Case, Logical Gap, Numerical Approximation, and Numerical Calculation. Blue superscripts indicate accuracy drop (Overall Acc - Answer Acc) from step-wise errors. Underlining denotes best result within each model category; boldface highlights best overall performance. Default max token limit for reasoning LLMs is 10K."
        },
        {
            "title": "5 Experiments in IneqMath",
            "content": "5.1 Experimental Setups We conduct systematic evaluation of 29 leading LLMs on the inequality problems in the IneqMath test set. The evaluated models span two categories: general-purpose chat models (both open-source and proprietary) and specialized reasoning LLMs designed for complex, multi-step problem-solving. All models are prompted in zero-shot setting with the problem statement and the instruction: Please solve the problem with clear, rigorous, and logically sound steps to encourage detailed reasoning. Model responses are assessed using our LLM-as-judge framework (4). We report three key metrics: Answer Acc: Measures the predicted answer correctness, verified by the final-answer judge (4.1). Step Acc: Aggregates the correctness of individual reasoning steps as determined by our four specialized step-wise judges (4.2), which target common flaws. Overall Acc: The primary metric, which deems solution correct only if it achieves both correct final answer and flawless step-wise reasoning (i.e., passes all five judges). response is thus considered fully correct (Overall Acc) only if it produces the right final answer through logically valid steps, passing scrutiny from all judges. Additional setup details are in C.1. 5.2 Main Evaluation Results Table 4 presents the performance of the evaluated LLMs on IneqMath. Our analysis reveals several critical insights into current LLM capabilities for inequality proving: 8 1) Reasoning LLMs achieve higher final-answer accuracy. Models like o1 (62.5% Answer Acc) and Grok 3 mini (71.5% Answer Acc) significantly outperform their general-purpose chat counterparts (e.g., GPT-4o at 37.5%, Grok 3 at 54.5%) in identifying the correct final answer. This suggests specialized architectures or training for reasoning improve their search ability to find answers. 2) Step-wise scrutiny reveals dramatic performance drop. The advantage in Answer Acc often masks underlying reasoning flaws. Overall Acc plummets when steps are evaluated. For instance, Grok 3 minis accuracy drops by 65.5% (from 71.5% Answer Acc to 6.0% Overall Acc), and o3-mini by 53.0%. This stark discrepancy underscores the fragility of LLM-generated deductive chains. 3) Robust proof construction remains major challenge. Even top models like o1 achieve low Overall Acc (8.0%). Many large models, despite moderate Answer Acc, also score poorly (e.g., Grok 3 at 3.5% Overall Acc). This indicates fundamental gap between finding plausible answer and constructing mathematically rigorous, step-by-step derivation. 5.3 In-depth Study Failure solution analysis. As shown in Table 4, where we report average error rates for overall accuracy, final-answer accuracy, and step-wise accuracy across four categories, the most common step-wise errors in LLM-generated solutions are logical gaps (85.0% average failure rate across models) and improper generalization from toy cases (59.7%). Less frequent, but still significant, are errors from numerical approximations (26.9%) and miscalculations (6.8%). detailed examination of incorrect solutions (see examples in C.2.1-C.2.4) highlights these prevalent error patterns, which often undermine proofs even when LLMs produce the correct final answer. Beyond these step-wise errors, LLMs also struggle to derive correct final answers for complex problems (C.2.5), indicating deeper challenges in theorem application and symbolic manipulation. Figure 5: Model-size scaling law (Answer Acc). Figure 6: Model-size scaling law (Overall Acc). Scaling law in model size. Figure 5 shows how final-answer accuracy (which evaluates only the correctness of the final predicted answer) scales with model size for LLMs. As model size increases, we observe steady improvement in answer accuracy, reflecting an empirical scaling law that larger models are better at inferring correct bounds and inequality relationships. However, this trend does not hold well when considering overall accuracywhich requires both correct answer and valid intermediate reasoning stepsas shown in Figure 6. In this latter case, the scaling curve flattens, indicating that increased model size alone is insufficient to eliminate step-by-step reasoning errors. Scaling law in test-time computation. Extended test-time computation, allowing longer reasoning chains, is common strategy for complex problem-solving [11]. We investigated its impact on overall accuracy in IneqMath by varying the maximum completion tokens for reasoning LLMs. Figure 7 shows that while models like Gemini 2.5 Pro and o3 initially improve with more tokens, performance gains saturate (e.g., beyond 20K tokens). This indicates that merely increasing computational budget offers diminishing returns for achieving rigorous, step-wise correct proofs, highlighting the need for more than just longer thought processes. 9 Figure 7: Scaling law in test-time computation for reasoning LLMs. 5.4 Exploring Improvement Strategies Retrieving relevant theorems as hints. To assess theorembased hints, we provide models with the top-k most frequent theorems from our IneqMath training corpus when solving 40-problem test subset. As shown in Figure 8, providing one or two such theorems decreases overall accuracy for weaker models (e.g., Grok 3 mini, o3-mini, o4-mini), likely due to misapplication or distraction by potentially irrelevant information. Conversely, stronger models like Gemini 2.5 Pro benefit from these hints, suggesting advanced reasoning is crucial to effectively use such guidance. These results underscore the potential of theorem-guided reasoning but also highlight the critical need for more sophisticated theorem retrieval mechanisms (e.g., RAG [25, 21]) to reliably enhance LLM performance in inequality proving. Detailed experiments are available in C.4. Figure 8: Model performance with retrieved theorems as hints. Self-improvement via critic as feedback. Allowing an LLM to critique and revise its own reasoning has been shown to improve performance on complex tasks [70, 52]. To explore whether this holds for inequality proving, we drew 40 random test problems from IneqMath and ran one round of self-critique. As Figure 9 shows, self-critique consistently improves performancee.g., Gemini 2.5 Pros overall accuracy rises from 43% to 48%. This upward trend underscores self-critique as promising, supervisionfree method to enhance logical rigor and solution quality of LLMs in inequality reasoning. More details are in C.5."
        },
        {
            "title": "6 Related Work",
            "content": "Figure 9: Model performance via self-critic as feedback. Datasets for inequality and theorem proving. One of major bottlenecks in advancing LLM capabilities for inequality proving is the scarcity of suitable datasets. Existing resources fall short in several ways: general ATP collections like MiniF2F [74] and ProofNet [5] contain few inequalities; synthetic datasets such as INT [58] and AIPS [57] offer scale but often lack structural diversity due to their template-based generation; and curated collections like ChenNEQ [6] are often too small for extensive training. More fundamentally, most existing datasets [72, 54, 65, 26, 53, 23] adopt fully formal representation, where problems and proofs are encoded in systems like Lean [8] or Isabelle [38]. While formal mathematical reasoning offers correctness guarantees and is vital research direction, LLMs, trained on vast corpora of natural language, often exhibit strong informal reasoning capabilities. Therefore, our IneqMath adopts an informal perspective, reformulating inequality proof problems into two verifiable subtasksbound estimation and relation prediction. These problems within IneqMath were crafted and reviewed by IMO-level medalist experts. Other informal reasoning datasets [45, 35, 20, 33] typically lack annotated solutions, theorem references, or corresponding training data. To address these gaps, IneqMath introduces 1,252 inequality problems for training, each annotated with theorems (from set of 83 named theorems spanning 29 categories) relevant to its solution, which comprises up to four steps. Methods for inequality and theorem proving. Proving inequalities is complex endeavor, demanding intuition for discovering tight bounds, strategic insight for selecting and applying classical theorems, and precise symbolic transformations. Traditional automated theorem provers (ATPs) primarily operate within formal systems like Lean [8] or Isabelle [38], requiring problems and proofs to be encoded in specialized languages. Inspired by the mathematical reasoning capabilities of LLMs [73], significant body of recent work has focused on integrating LLMs with these formal ATPs. These approaches often model theorem proving as Markov Decision Process (MDP), training LLMs to select appropriate tactics and premises to construct proofs verifiable by the formal system [13, 15, 23, 31, 46, 67, 63, 29, 30, 71, 12, 55, 50, 56]. For instance, systems like GoedelProver [32] leverage large Lean corpora to train models for tactic prediction, enabling end-to-end formal proof generation. Other methods incorporate tree-search techniques to navigate the vast search space of premises within these formal frameworks [59, 28, 64]. 10 However, LLMs are fundamentally trained on vast corpora of natural language, suggesting their inherent strengths may lie in informal reasoning, which is often closer to human intuition and the preliminary stages of mathematical discovery. This highlights gap and an opportunity for methods that leverage these informal capabilities. Our work diverges from the purely formal paradigm by proposing an informal yet verifiable approach to inequality proving, aiming to benchmark and enhance LLM performance in setting that better aligns with their human-like problem-solving, while also exploring potential improvement strategies like theorem-guided reasoning and self-refinement. LLM-as-judge for math problem solving. Reliable evaluation of mathematical problem-solving necessitates assessing not only final answer correctness but also the logical soundness of each reasoning step, significant challenge for automated systems. Traditional methods are often inadequate: expert annotation is labor-intensive and unscalable for large-scale evaluation [45, 35], while automated techniques such as string matching or value equivalence overlook crucial step-by-step proof correctness [22, 20, 33, 34]. While LLMs show promise as evaluators (LLM-as-judge), their capacity for detailed, step-wise mathematical judgment is still developing. Existing step-level LLM judges [62, 14], for instance, may assess general step validity but often lack the granularity to identify specific, nuanced reasoning flaws. Similarly, frameworks like EIC-Math [27] provide broad error categories but can miss subtle yet critical issues in multi-step derivations, such as minor miscalculations or unstated assumptions. To address these limitations and rigorously assess informal mathematical proofs like inequality solving, our LLM-as-judge framework combines high-precision final-answer judge with four step-wise judges targeting common errors: toy case overgeneralization, logical gaps, unjustified numeric approximations, and numeric calculation mistakes."
        },
        {
            "title": "7 Conclusion",
            "content": "In summary, we introduce an informal yet verifiable task formulation for inequality proving, decomposing it into bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated benchmark of Olympiad-level inequalities with training corpus featuring step-wise solutions and theorem annotations. Our novel LLM-as-judge evaluation framework, comprising final-answer judge and four step-wise judges, enables rigorous assessment. Our comprehensive evaluation of diverse leading LLMs reveals critical gap: while LLMs may achieve high final-answer accuracy, this often plummets by up to 65.5% under step-wise scrutiny, with top models like o1 achieving less than 10% overall accuracy. This discrepancy exposes fragile deductive chains for current LLMs in constructing rigorous proofs. We further find that scaling model size or increasing test-time computation yields limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is partially supported by the Hoffman-Yee Research Grants program at Stanford HAI. We thank Yu (Bryan) Zhou for early discussion and feedback."
        },
        {
            "title": "References",
            "content": "[1] Meta AI. Llama 3.1 8b. https://huggingface.co/meta-llama/Llama-3.1-8B, 2024. Accessed: 2025-05-15. [2] Meta AI. Llama 3.2 3b. https://huggingface.co/meta-llama/Llama-3.2-3B, 2024. Accessed: 2025-05-15. [3] Alibaba Qwen Team. QwQ-32B. https://huggingface.co/Qwen/QwQ-32B, 2025. Hugging Face model card. [4] Anthropic. Claude 3.7 Sonnet. https://www.anthropic.com/claude/sonnet, 2025. Anthropic model card. [5] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward William Ayers, and Dragomir Radev. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2024. 11 [6] Evan Chen. brief introduction to olympiad inequalities, 2014. Accessed: 2025-03-19. [7] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, Singapore, December 2023. Association for Computational Linguistics. [8] Leonardo Mendonça de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In CADE, 2015. [9] DeepSeek-AI. DeepSeek-R1-Distill-Llama-70B. https://huggingface.co/deepseek-ai/ DeepSeek-R1-Distill-Llama-70B, 2025. Hugging Face model card. [10] DeepSeek-AI. DeepSeek-R1-Distill-Qwen-14B. https://huggingface.co/deepseek-ai/ DeepSeek-R1-Distill-Qwen-14B, 2025. Hugging Face model card. [11] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [12] Kefan Dong and Tengyu Ma. Beyond limited data: Self-play llm theorem provers with iterative conjecturing and proving. arXiv preprint arXiv:2502.00212, 2025. [13] Kefan Dong, Arvind V. Mahankali, and Tengyu Ma. Formal theorem proving by rewarding LLMs to decompose proofs hierarchically. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [14] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-MATH: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, 2025. [15] Fabian Gloeckle, Jannis Limperg, Gabriel Synnaeve, and Amaury Hayat. ABEL: Sample efficient online reinforcement learning for neural theorem proving. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [16] Google DeepMind. Gemini 2.0 Flash. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-0-flash, 2025. Vertex AI model card. [17] Google DeepMind. Gemini 2.0 Flash-Lite. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-0-flash-lite, 2025. Vertex AI model card. [18] Google DeepMind. Gemini 2.5 Flash. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-5-flash, 2025. Vertex AI model card. [19] Google DeepMind. Gemini 2.5 Pro. https://deepmind.google/technologies/gemini/ pro/, 2025. Google DeepMind model card. [20] Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno de Moraes Dumont, and Sanmi Koyejo. Putnam-AXIOM: functional and static benchmark for measuring higher level mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [21] Shailja Gupta, Rajesh Ranjan, and Surya Narayan Singh. comprehensive survey of retrievalaugmented generation (rag): Evolution, current landscape and future directions. arXiv preprint arXiv:2410.12837, 2024. [22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [23] Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX: Neural theorem proving with (long- )contexts. In The Thirteenth International Conference on Learning Representations, 2025. 12 [24] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [25] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc., 2020. [26] Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence Paulson. Isarstep: benchmark for high-level mathematical reasoning. In International Conference on Learning Representations, 2021. [27] Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, and Fuli Feng. Evaluating mathematical reasoning of large language models: focus on error identification and correction. arXiv preprint arXiv:2406.00755, 2024. [28] Yang Li, Dong Du, Linfeng Song, Chen Li, Weikang Wang, Tao Yang, and Haitao Mi. Hunyuanprover: scalable data synthesis framework and guided tree search for automated theorem proving. arXiv preprint arXiv:2412.20735, 2024. [29] Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, and Xiaoxing Ma. Proving olympiad inequalities by synergizing LLMs and symbolic reasoning. In The Thirteenth International Conference on Learning Representations, 2025. [30] Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, and Dong Yu. Mps-prover: Advancing stepwise theorem proving by multi-perspective search and data curation. arXiv preprint arXiv:2505.10962, 2025. [31] Haohan Lin, Zhiqing Sun, Sean Welleck, and Yiming Yang. Lean-STar: Learning to interleave thinking and proving. In The Thirteenth International Conference on Learning Representations, 2025. [32] Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, et al. Goedel-prover: frontier model for open-source automated theorem proving. arXiv preprint arXiv:2502.07640, 2025. [33] Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024. [34] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. [35] Yujun Mao, Yoon Kim, and Yilun Zhou. CHAMP: competition-level dataset for fine-grained analyses of LLMs mathematical reasoning capabilities. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1325613274, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [36] Meta Platforms, Inc. Llama-4-Maverick-17B-128E. https://huggingface.co/ meta-llama/Llama-4-Maverick-17B-128E, 2025. Hugging Face model card. [37] Meta Platforms, Inc. Llama-4-Scout-17B-16E. https://huggingface.co/meta-llama/ Llama-4-Scout-17B-16E, 2025. Hugging Face model card; accessed 2025-05-12. [38] Tobias Nipkow, Markus Wenzel, and Lawrence C. Paulson. Isabelle/HOL: Proof Assistant for Higher-Order Logic. Springer, 2002. [39] OpenAI. GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024. OpenAI blog post. [40] OpenAI. GPT-4o mini. gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024. post. https://openai.com/index/ OpenAI blog 13 [41] OpenAI. OpenAI o1. https://openai.com/o1/, 2024. OpenAI official announcement. [42] OpenAI. GPT-4.1. https://openai.com/index/gpt-4-1/, 2025. OpenAI model announcement. [43] OpenAI. Openai o3-mini system card, January 2025. Accessed: 2025-03-19. [44] OpenAI. OpenAI o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, 2025. OpenAI official announcement. [45] Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. [46] Gabriel Poesia, David Broman, Nick Haber, and Noah Goodman. Learning formal mathematics from intrinsic motivation. Advances in Neural Information Processing Systems, 37:4303243057, 2024. [47] Qwen Team. Qwen2.5-72B. https://huggingface.co/Qwen/Qwen2.5-72B, 2024. Hugging Face model card. [48] Qwen Team. Qwen2.5-7B. https://huggingface.co/Qwen/Qwen2.5-7B, 2024. Hugging Face model card. [49] Qwen Team. Qwen3-235B-A22B. https://huggingface.co/Qwen/Qwen3-235B-A22B, 2025. Hugging Face model card. [50] Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, and Chong Ruan. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition, April 2025. [51] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [52] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Lei Han, Haitao Mi, and Dong Yu. Toward self-improvement of LLMs via imagination, searching, and criticizing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [53] George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition. arXiv preprint arXiv:2407.11214, 2024. [54] Nguyen Duy Tung. 567 nice and hard inequalities, April 2012. Accessed: 2025-03-19. [55] Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, Jianqiao Lu, Hugues de Saxce, Bolton Bailey, Chendong Song, Chenjun Xiao, Dehao Zhang, Ebony Zhang, Frederick Pu, Han Zhu, Jiawei Liu, Jonas Bayer, Julien Michel, Longhui Yu, Léo Dreyfus-Schmidt, Lewis Tunstall, Luigi Pagani, Moreira Machado, Pauline Bourigault, Ran Wang, Stanislas Polu, Thibaut Barroyer, Wen-Ding Li, Yazhe Niu, Yann Fleureau, Yangyang Hu, Zhouliang Yu, Zihan Wang, Zhilin Yang, Zhengying Liu, and Jia Li. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354, 2025. [56] Haiming Wang, Huajian Xin, Chuanyang Zheng, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, and Xiaodan Liang. LEGO-prover: Neural theorem proving with growing libraries. In The Twelfth International Conference on Learning Representations, 2024. [57] Chenrui Wei, Mengzhou Sun, and Wei Wang. Proving olympiad algebraic inequalities without human demonstrations. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 14 [58] Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. {INT}: An inequality benchmark for evaluating generalization in theorem proving. In International Conference on Learning Representations, 2021. [59] Zĳian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2. 5-stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems. arXiv preprint arXiv:2410.15700, 2024. [60] xAI. Grok 3. https://x.ai/news/grok-3, 2025. xAI official announcement. [61] xAI. Grok 3 Mini. https://x.ai/news/grok-3, 2025. xAI official announcement. [62] Shĳie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. CoRR, abs/2404.05692, 2024. [63] Huajian Xin, Daya Guo, Zhihong Shao, Z.Z. Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Advancing theorem proving in LLMs through large-scale synthetic data. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [64] Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen. Bfs-prover: Scalable best-first tree search for llm-based automatic theorem proving. arXiv preprint arXiv:2502.03438, 2025. [65] Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In International Conference on Machine Learning, pages 69846994. PMLR, 2019. [66] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [67] Ziyu Ye, Jiacheng Chen, Jonathan Light, Yifei Wang, Jiankai Sun, Guohao Li, Mac Schwager, Philip Torr, Yuxin Chen, Kaiyu Yang, Yisong Yue, and Ziniu Hu. Reasoning in reasoning: hierarchical framework for (better and faster) neural theorem proving. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [68] Huaiyuan Ying, Zĳian Wu, Yihan Geng, JIayu Wang, Dahua Lin, and Kai Chen. Lean workbook: large-scale lean problem set formalized from natural language math problems. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [69] Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, and Weiyang Liu. Formalmath: Benchmarking formal mathematical reasoning of large language models, 2025. [70] Mert Yuksekgonul, Federico Bianchi, Jack Boen, et al. Optimizing generative ai by backpropagating language model feedback. Nature, 639:609616, 2025. Published 19 March 2025. [71] Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, et al. Deeptheorem: Advancing llm reasoning for theorem proving through natural language and reinforcement learning. arXiv preprint arXiv:2505.23754, 2025. [72] Haoyu Zhao, Yihan Geng, Shange Tang, Yong Lin, Bohan Lyu, Hongzhou Lin, Chi Jin, and Sanjeev Arora. Ineq-Comp: Benchmarking human-intuitive compositional reasoning in automated theorem proving on inequalities. arXiv preprint arXiv:2505.12680, 2025. [73] Zhiyu Zhao, Yongcheng Zeng, Ning Yang, and Guoqing Liu. Enhancing mathematical reasoning in language models through focused differentiation training, 2025. [74] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: cross-system benchmark for formal olympiad-level mathematics. In International Conference on Learning Representations, 2022."
        },
        {
            "title": "Appendix Contents",
            "content": "A Dataset Curation Details A.1 Data Annotation Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Prompts for Rephrasing Problems . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Benchmark Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Fine-grained Informal Judge Details B.1 Final Answer Judge . B.2 Toy Case Judge . . B.3 Logical Gap Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Numerical Approximation Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Numerical Computation Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Development Performance of Judges . . . . . . . . . . . . . . . . . . . . . . . . . B.7 Judge Failure Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experimental Details for Inequality Solving C.1 Experimental Setups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Model Failure Solution Examples . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Taking Annotated Theorems as Hints . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Retrieval as Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Self-improvement via Critic as Feedback . . . . . . . . . . . . . . . . . . . . . . . Limitations Broader Impacts 17 17 18 20 22 24 25 27 28 30 35 35 37 42 45 51"
        },
        {
            "title": "A Dataset Curation Details",
            "content": "A.1 Data Annotation Tool Figure 10: The interface of our developed tool for checking and editing the bound problems. Figure 11: The interface of our developed tool for checking and editing the relation problems. 17 A.2 Prompts for Rephrasing Problems Prompt for Rephrasing Proofs to Bound Problems Task: Transform the given inequality problem into bound prediction problem by introducing constant and determining its optimal value. Instructions: 1. Analyze the original problem, focusing on its structure and potential for transformation. 2. Introduce constant by either replacing an existing constant or creating new relationship between expressions. 3. Determine whether to find the minimal or maximal value of that satisfies the inequality for all relevant variables. 4. Consider factors such as homogeneity, existing constraints, and the domain of variables (e.g., positive reals, all reals). 5. Ensure the rephrased problem maintains the mathematical essence and constraints of the original. Output format: Provide your response in the following structure: <Analysis>: Concise explanation of key features and transformation approach. <Conclusion>: YES or NO, followed by brief summary of the transformation. <Rephrased Problem>: Transformed problem statement, focusing on finding the optimal C. <Answer>: =< value >. Key considerations: 1. For double inequalities, attempt to rephrase as single bound prediction problem when possible. 2. In homogeneous inequalities, focus on the ratios between variables rather than their absolute values. 3. Incorporate any existing constraints into the rephrased version of the problem. 4. Clearly specify the domain of the variables in the rephrased problem statement. 5. Ensure that the rephrased problem is logically equivalent to the original. Example: Original problem: Let a, b, R+. Prove the inequality abc (1 + a)(a + b)(b + c)(c + 16) 1 81 <Analysis>: To turn this into bound prediction problem, we can focus on the following steps: 1. The left side is rational expression that is always positive for a, b, R+. 2. The right side is fixed constant 1 . 81 3. We replace the constant 1 81 holds for all positive a, b, c? 4. This approach allows us to determine the tightest possible upper bound for the left-hand expression. 5. If we find the smallest that works, we prove the original inequality and show its the best possible. <Conclusion>: YES, the inequality can be rephrased as bound prediction problem. By replacing the constant 1 with variable C, we can determine the tightest upper bound for the given rational expression, 81 effectively proving the original inequality and demonstrating its optimality. with variable and ask: What is the smallest such that the inequality <Rephrased problem>: Coll Determine the minimal constant such that the following inequality holds for all a, b, c: abc (1 + a)(a + b)(b + c)(c + 16) C. <Answer>: = 1 81 . Now, please rewrite the following problem: Original problem: {problem} 18 Prompt for Rephrasing Proofs to Relation Problems Task: Transform the given inequality proof problem into relation prediction problem. Instructions: 1. Analyze the original problem, identifying key components such as variables, domains, conditions, and the main inequality. 2. Rephrase the problem by maintaining the original expressions and replacing the relation symbol with blank to be filled. 3. Preserve any additional conditions or constraints from the original problem in your rephrased version. 4. Change the task from Prove to Determine the correct inequality relation to fill in the blank. 5. Provide set of options for the relation, always including , , =, <, >, and None of the above. 6. Determine the correct answer based on your modification and analysis. Output format: Provide your response in the following structure: <Analysis>: Detailed step-by-step analysis of the original problem and your approach to rephrasing it. <Conclusion>: YES or NO, followed by brief explanation of whether and how the problem can be effectively rephrased. <Rephrased Problem>: Transformed problem statement. Options: (A) (B) (C) = (D) < (E) > (F) None of the above <Answer>: Option letter. Key considerations: 1. Maintain the original mathematical expressions and any given conditions as much as possible. 2. Ensure the rephrased problem captures the essence and complexity of the original problem. 3. For problems with multiple inequalities, focus on one main inequality for the relation prediction task. 4. When dealing with complex fractions or expressions, keep them intact to maintain the problems difficulty level. 5. If the relation depends on specific values of the variables or cannot be definitively determined, consider using None of the above as the correct answer. Example: Original problem: Let a, b, R+. Prove the inequality 1 81 abc (1 + a)(a + b)(b + c)(c + 16) <Analysis>: To rephrase it to relation prediction problem, we can focus on the following steps: 1. The original problem is proof task for an inequality involving positive real numbers a, b, and c. 2. The left side of the inequality is complex fraction 3. The right side is constant fraction 1 81 4. The original inequality uses the less than or equal to () relation, which needs to hold for all positive real values of a, b, and c. 5. We can transform the proof task into determining the correct relation between the left and right sides of the inequality. abc (1+a)(a+b)(b+c)(c+16) . . <Conclusion>: YES, the inequality can be effectively rephrased as relation prediction problem. <Rephrased problem>: Let a, b, R+. Consider the following inequality: abc (1 + a)(a + b)(b + c)(c + 16) ( ) 1 81 . Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above <Answer>: Now, please rewrite the following problem: Original problem: {problem} A.3 Benchmark Examples IneqMath Testing Example 1: Bound Problem Question: Let x, y, > 0 such that + + = 1. Determine the minimal constant such that the following inequality holds for all x, y, z: xy(y + 4z) + yz(z + 4x) + zx(x + 4y) C. IneqMath Testing Example 2: Bound Problem Question: Let a1, a2, . . . , an be real numbers and be non-empty subset of {1, 2, . . . , n}. Find the largest constant such that the following inequality holds for all a1, a2, . . . , an and S: (cid:32) (cid:88) (cid:33)2 ai 2C (cid:88) (ai + + aj)2 . iS 1ijn IneqMath Testing Example 3: Bound Problem Question: Let a1, a2, . . . , an > 0 such that a1 + a2 + . . . + an < 1. Determine the minimal constant such that the following inequality holds for all a1, a2, . . . , an: a1 a2 . . . an (1 a1 a2 . . . an) (a1 + a2 + . . . + an) (1 a1) (1 a2) . . . (1 an) 3 nn1 . 20 IneqMath Testing Example 4: Relation Problem Question: Let a, b, c, x, y, be real numbers such that + + = 1 and x2 + y2 + z2 = 1. Consider the following expression: Determine the correct inequality relation to fill in the blank. a(x + b) + b(y + c) + c(z + a) ( ) 1. Options: (A) (B) (C) = (D) < (E) > (F) None of the above IneqMath Testing Example 5: Relation Problem Question: In the plane of the acute-angled triangle ABC, let be line such that u, v, are the lengths of the perpendiculars from A, B, respectively to L. Consider the following inequality: u2 tan + v2 tan + w2 tan ( ) . where is the area of the triangle. Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above IneqMath Testing Example 6: Relation Problem Question: Let a, b, be the sides of any triangle. Consider the following inequality: (cid:32) (cid:88) 3 cyc (cid:33) ab (1 + 2 cos(c)) ( ) 2 (cid:32) (cid:88) cyc (cid:112)(c2 + ab(1 + 2 cos(c))) (b2 + ac(1 + cos(b))) (cid:33) . Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above 21 Fine-grained Informal Judge Details B.1 Final Answer Judge Prompt for Final Answer Judge: Answer Extraction for Bound problems You are an expert at extracting numbers from answer sentences. Below are examples of sentences and the corresponding numbers: Example 1: answer is = 2. Answer: = 2 Example 2: answer is = 1 2 Answer: = 1 2 . Example 3: answer is = 2 . Answer: = 2 Now, extract the number from the following sentence: {answer_sentence}. Make sure to return the answer in format as C=<extracted_answer>, where <extracted_answer> is the extracted number or expression. Prompt for Final Answer Judge: Answer Extraction for Relation Problems You are an expert at extracting option letters (A, B, C, D, E, F) from answer sentences. The options are given below: A: (A) B: (B) C: (C) = D: (D) < E: (E) > F: (F) None of the above Below are examples of sentences and the corresponding option letters: Example 1: answer is (B) . Answer: Example 2: answer is (E) >. Answer: Example 3: answer is: . Answer: Now, extract the option letter from the following sentence: {answer_sentence}. Make sure to return the option letter only, without any other characters. 22 Prompt for Final Answer Judge: Answer Equivalence Verification You are an expert at verifying mathematical expression equivalence. Analyze if two expressions are exactly equivalent by following these strict rules: Required Analysis Steps: 1. Check if both expressions are valid mathematical forms. 2. If either expression is not mathematical (e.g., text or undefined), return False. 3. For numerical expressions: - Direct equality (e.g., 2 = 2) True. - Different representations of same value (e.g., 1/2 = 0.5, - Decimal approximations vs exact values (e.g., 2π = 6.28318) False. 4 = 2) True. 4. For algebraic expressions: - Must have clear, valid transformation path between forms. - If transformation requires multiple non-obvious steps False. - Verify equivalence through algebraic proof when possible. - For complex expressions, use techniques like squaring or substitution to verify. Equivalence Criteria: - Must have exactly same deterministic value. - Must be provably equivalent through valid mathematical operations. - Different notations of same exact value are equivalent. - Decimal approximations are NOT equivalent to exact expressions. - No rounding or approximations allowed. - If equivalence cannot be conclusively proven False. Example pairs and their analysis: Ground truth: = 2 Prediction: = 2 Analysis: The expressions are identical in both form and value, representing the same integer 2. Equivalent: True Ground truth: = 1.5 Prediction: = 3 2 Analysis: The decimal 1.5 and fraction 3 2 Equivalent: True are different representations of the same number (1.5 = 3 2 ). Ground truth: = 2π Prediction: = 6.28318530718 Analysis: While 6.28318530718 is decimal approximation of 2π, they are not symbolically equivalent expressions. Equivalent: False (cid:113) 1 6 Ground truth: = Prediction: = 1 6 Analysis: These are equivalent through the property (cid:112) Equivalent: True = when a, > 0. (cid:113) 3 2 )2 = 3 2 = 9 8 = ( 3 2 2 )2. Ground truth: = Prediction: = 3 2 2 (cid:113) 3 Analysis: The expressions differ as proven when squared: ( Equivalent: False Now analyze these expressions: Ground truth: {ground_truth} Prediction: {prediction} 23 B.2 Toy Case Judge Prompt for Toy Case Judge Task: Evaluate the logical rigor of solution to an inequality problem, focusing specifically on whether the direction of the inequality was justified using toy cases or special value substitution. Instructions: 1. Carefully read the reasoning process used to solve the inequality. 2. Identify whether the direction of the inequality was determined by testing special values, trying toy cases, or relying on extreme-case analysis, rather than providing general proof valid over the entire domain. 3. If the model uses toy case (e.g., setting variable to 0, 1, or choosing symmetric/equal values) or considers variable tending to 0 or (extreme-case reasoning) to conclude the inequality direction, this should be flagged as logically unsound unless it is later supported by rigorous or general argument. 4. Substituting special values for the purpose of verifying equality or testing sharpness is acceptable and should not be flagged. 5. If toy case is used to show that the inequality does not hold (i.e., the two sides are incomparable), this is acceptable and should not be flagged. 6. Trying toy cases or substituting special values for the purpose of exploring or analyzing the problemwithout using them to directly conclude the inequality directionis acceptable and should not be flagged. 7. The goal is to confirm whether the final conclusion is justified for all variables in the given domain by using sound and formal reasoning. Output Format: <Analysis>: Brief explanation of whether toy cases, special values, or extreme-case reasoning were used to justify inequality direction, and whether this undermines the rigor of the proof. <Flagged Reasoning Step (if applicable)>: Quote or summarize the step(s) where toy cases or extreme cases were used to justify the inequality. <Answer>: True or False. It indicates whether the solutions reasoning is logically valid (True) or contains flaws due to unjustified use of toy cases, special value substitution, or extreme-case reasoning (False). Key Considerations: 1. Check which steps are used to justify the final general conclusion. 2. Toy case examples (e.g., setting , or choosing values like) should not be used to conclude general inequality directions. 3. These techniques are allowed for illustration or equality testing, but not as replacement for full justification. 4. Extreme-case analysis (e.g., letting or) used as substitute for proof to justify the final general conclusion should be flagged. 5. Do not provide improvement suggestionssimply judge whether the logic used is valid or flawed. 6. Only reasoning steps that fail to generalize and is used to justify the final general conclusion should be flagged. Examples of inputs and outputs: {examples} Now analyze the following problem and solution: Original Problem: {query} Solution: {response} Judge Example 1: Toy Case Response: Lets test the specific values. Case 1: If = = c, then: (cid:18) (cid:19)2 + Case 2: If = 0 and = = 1, then: + + + (cid:18) (cid:19)2 + = 1 4 + 1 2 + 1 4 = 1. (cid:18) (cid:19)2 + + + + (cid:18) (cid:19)2 + = 0 + 1 + 1 = 2. From the test cases, we can conclude that the minimum value of the expression was 1. Therefore, the expression is always at least 1. The answer is = 1 . Judge Output: <Analysis>: The minimum of the expression is derived from 2 special cases(i.e. = = and = 0, = = 1). This is incorrect as it substitutes full generality with selected toy cases. <Flagged Reasoning Step (if applicable)>: From the test cases, we can conclude that the minimum value of the expression was 1. Therefore, the expression is always at least 1. <Answer>: alse B.3 Logical Gap Judge Prompt for Logical Gap Judge Task: Evaluate the logical rigor of proposed solution to mathematical inequality problem. Focus on whether the reasoning includes non-trivial claims made without justification, logical leaps, or unsupported assertions, while allowing for valid optimization-based, algebraic, or analytic analysis when properly demonstrated. This prompt does not evaluate whether the direction of an inequality was justified using toy cases, special values, or asymptotic behavior. That aspect is handled separately. Instructions: 1. Carefully read the entire reasoning process used to solve the inequality. 2. Identify whether the solution includes: - Any non-obvious (non-trivial) claims or transformations stated without justification or explanation. - Any logical gaps or skipped steps that lead to intermediate or final conclusions. 3. All significant transformationsespecially involving inequalities, bounds, or extremal behaviormust be supported by: Algebraic manipulation, Well-known identities or theorems, Valid analytical tools (e.g., convexity, derivatives, limits) or step-by-step numeric or symbolic reasoning. 4. Optimization methods (e.g., Lagrange multipliers, derivative-based analysis) are valid only if the analysis is explicitly shown: - If solution invokes optimization or analytical techniques, it must demonstrate key steps, derivative conditions, or critical point verification. - Statements such as solving the constrained optimization problem confirms... without any derivation or argument are considered unjustified. - You do not need to assess whether toy cases, special values, or extreme behavior were used to infer the inequality direction. That responsibility lies outside the scope of this Judge. 5. Simple algebra or widely known transformations (e.g., AM-GM, factoring identities) may be used without full derivation. 6. The goal is to assess whether each important conclusion within the reasoningnot just the final answeris logically supported and rigorously justified. 25 Prompt for Logical Gap Judge (Continued) Output Format: <Analysis>: Step-by-step explanation of whether the reasoning is logically sound. Highlight any unjustified claims or skipped steps, unless they are supported by valid asymptotic, numeric, or analytic reasoning. <Flagged Reasoning Step (if applicable)>: Quote or summarize the specific step(s) where an unjustified claim or logical leap occurred. <Answer>: True or False. True if the reasoning is valid; False if it contains unjustified steps or unsupported claims. Key Considerations: 1. Focus on whether each major step (not just the final answer) is logically justified. 2. Non-trivial algebraic identities or inequality steps must be explained unless they are well-known. 3. Minor simplifications and standard techniques are acceptable without proof. 4. Do not flag steps involving toy cases, extreme values, or special substitutions used to infer inequality directionthose are out of scope here. 5. Claims like numerical check shows must include specific values, results, or graphs to be valid. 6. Optimization-based arguments (e.g., Lagrange multipliers, critical point methods) must include demonstrated steps or analytic structure. If only the method is named but not applied, the reasoning should be flagged. 7. Do not flag steps used solely for equality verification, sharpness testing, or illustration. 8. Do not provide improvement suggestionssimply judge whether the logic is valid or flawed as presented. Examples of inputs and outputs: {examples} Now analyze the following problem and solution: Original Problem: {query} Solution: {response} Judge Example 2: Logical Gap Response: ... numerical check confirms that the minimum occurs when = 1, and the expression equals 4. So the largest constant such that the inequality holds for all positive a, b, is: = 4 Judge Output: <Analysis>: The statement numerical check confirms that the minimum occurs when = 1 is made without showing any actual numerical data, evaluations, or visual/analytic evidence to support this claim. <Flagged Reasoning Step (if applicable)>: numerical check confirms that the minimum occurs when = 1. <Answer>: alse 26 B.4 Numerical Approximation Judge Prompt for Numerical Approximation Judge Task: Evaluate the logical rigor of solution to an inequality problem, specifically checking whether approx2 with 1.414) were improperly used in the reasoning process. imate numerical substitutions (e.g., replacing Instructions: 1. Carefully read the entire reasoning process used to solve the inequality. 2. Identify whether the solution includes: - Any replacement of exact expressions (such as radicals, fractions, or constants like π) with approximate decimal values. 3.Strict rules for use of approximate values: - If approximated values are directly involved in any operations (such as addition, subtraction, multiplication, or division), it must immediately be considered invalid, regardless of whether the operation is for comparing sizes or for further reasoning! - Examples of invalid actions: Approximating 5 2.236 and then using it to compute 5 + 3 approximately, or Approximating π 3.14 and then evaluating π/2 based on 3.14. 4. Approximate substitutions are allowed only under the following conditions: If approximate numerical 2) that humans can readily estimate, it is comparison is used between simple numbers (e.g., acceptable. 5. Approximate substitution is invalid and must be flagged in these cases: 4, 1 , 17, 23) where human - If an approximate value is introduced for complex irrational number (e.g., mental estimation is impractical, even for comparison purposes. - If any approximation alters the rigor of the argument. 6. You do not need to judge whether the final inequality direction is correctonly whether improper approximation substitution occurred. Output Format: <Analysis>: Step-by-step explanation of whether approximate numerical values were improperly substituted for exact expressions. Clarify whether approximations were used only illustratively or improperly incorporated into reasoning. <Flagged Reasoning Step (if applicable)>: Quote or summarize the specific step(s) where inappropriate approximations were made. <Answer>: True or False. True if the reasoning maintains acceptable rigor regarding approximations; False if it violates the rules. Key Considerations: 1. If approximated values are directly involved in any operations (such as addition, subtraction, multiplication, or division), it must immediately be considered invalid, regardless of whether the operation is for comparing sizes or for further reasoning. 2. Comparing simple, common values via approximation (e.g., estimation is reasonable. 3. Approximate values of complicated irrational numbers (e.g., comparison. 4. Any use of approximate values for calculations (such as adding, subtracting, multiplying, or dividing approximate numbers) is strictly invalid, even if the final result seems close. Only comparing two simple exact numbers by approximation is acceptable; calculating further with approximated values is never allowed. 5. Widely known exact simplifications (e.g., 6. Do not suggest improvementsonly judge whether the solution follows the rules. 2 1.414 vs 1.5) is acceptable if human 23, π5/4) are invalid even for 2 = 0.5) are acceptable. 4 = 2, 1 17, Examples of inputs and outputs: {examples} Now analyze the following problem and solution: Original Problem: {query} Solution: {response} 27 Judge Example 3: Numerical Approximation Response: ... When = 89, = = 45.5: cos 89 0.01745 cos 45.5 0.7009 Compute: So the sum: cos cos cos cos cos cos cos cos cos = = = 0.01745 0.70092 0.7009 0.7009 0.01745 0.7009 0.01745 0.7009 0.01745 0. 0.0355 = = 0.7009 0.01223 0.7009 0.01223 57.33 57. 0.0355 + 57.33 + 57.33 114.7 > 114 which concludes that > 114. ... Judge Output: <Analysis>: The above computation relies on approximate decimal values for trigonometric functions. Using such coarse approximationsespecially in the denominatorcan lead to significant magnification of numerical error. So comparing the approximation of with 114 to conclude the relationship with and 114 is not mathematically rigorous. <Flagged Reasoning Step (if applicable)>: So the sum 0.0355 + 57.33 + 57.33 114.7 > 114, which concludes that > 114. <Answer>: alse B.5 Numerical Computation Judge Prompt for Numerical Computation Judge Task: Evaluate the correctness of numerical computations in solution to mathematical inequality problem. Focus on verifying whether each calculation step is numerically valid, allowing for some error tolerance. Instructions: 1. Carefully read through the entire solution. 2. Identify all numerically verifiable expressions, including: - Exact value computations when variables are assigned specific numbers, allowing for floating-point operations. 3. Do not extract: - Symbolic manipulations or transformations. - Expressions involving symbolic variables or operations. - Inequalities; only extract equations. - Approximate equations (e.g., using approximately equal to). 4. Python validation rules: - Convert all operations to floating-point calculations. - Allow small tolerance for numerical comparisons (e.g., abs(lhs - rhs) < 1e-2). - Set the final result of each check to variable answer, where answer = True if the verification succeeds, otherwise False. 28 Prompt for Numerical Computation Judge (Continued) 5. Final output format: - Extracted expressions: List all selected expressions in natural math form. - Analysis: Clearly state why every expression meets the criteria for verification or doesnt. - Validation Codes: Provide Python code snippets to verify each expression. Output Format: <Analysis>: Reason why the expression needs to be verified and extracted or not. <Expressions>: List(Expression 1, Expression 2, ...) <Codes>: # Validation for Expression 1 import sympy as sp # your checking code here # Validation for Expression 2 import sympy as sp # your checking code here Key Considerations: - Only extract expressions involving explicit numerical computations. - All selected expressions must be verifiable numerically, allowing for some error tolerance. - Use floating-point operations for all checks. - Do not extract approximate equations. - Do not extract expressions involving symbolic manipulations or transformations. Examples: {examples} Now analyze the following problem and solution: Original problem: {query} Solution: {response} Judge Example 4: Numerical Computation Response: ... Based on the previous steps, φ(t) = + , 27 2 t3 + 3 81 t4 , φ(t) = 1 So φ(t) 0 for 3 with equality only at = 3. Hence φ is increasing on [3, ) and attains its minimum at = 3: φ(3) = 3 + + = 4. 27 2 3 Therefore (a, b, c) 4 for every positive a, b, c, and equality occurs when = 3. ... <Analysis>: φ(3) = 3 + 27 So this is the only expression that should be extracted. 27 + 2 3 = 4 is the only numerical expression without any variables. <Expressions>: List(φ(3) = 3 + 27 + 2 3 = 4) <Codes>: # Validation for Expression 1: φ(3) = 3 + 27/27 = 4 from sympy import Rational phi_val = Rational(3) + Rational(27, 27) answer = (phi_val == 4) 29 B.6 Development Performance of Judges During development, we experimented with different LLMs as backends for the judges, with detailed results presented in Table 5. Balancing F1 score, inference cost, and latency, we select GPT-4o mini for the final-answer judge due to its perfect F1 score (1.0) and the lowest cost among candidates. For the four step-wise judges, we use o4-mini for Toy Case Judge and Logical Gap Judge, o1 for Numerical Approximation Judge, and GPT-4.1 mini for Numerical Calculation Judge, which attains F1 scores above 0.9 on average. Metrics GPT-4o mini GPT-4o GPT-4.1 mini GPT-4.1 o3-mini o4-mini o1 o3 Accuracy (%) Precision (%) Recall (%) F1 score 100.0 100.0 100.0 1.0 Accuracy (%) Precision (%) Recall (%) F1 score Accuracy (%) Precision (%) Recall (%) F1 score Accuracy (%) Precision (%) Recall (%) F1 score Accuracy (%) Precision (%) Recall (%) F1 score 80.0 89.3 65.8 0. 64.6 82.4 35.9 0.50 80.0 82.1 78.0 0.80 65.0 64.2 91.5 0.75 Final Answer Judge - - - - Toy Case Judge 88.8 82.2 97.4 0.89 - - - - 90.0 87.5 92.1 0.90 Logical Gap Judge 78.8 71.0 97.5 0.82 75.0 67.9 95.0 0. - - - - 86.3 84.6 86.8 0.86 71.3 63.5 100.0 0.78 Numerical Approximation Judge 71.3 63.9 97.5 0.77 60.0 56.2 100.0 0. 87.5 80.4 100.0 0.89 Numerical Calculation Judge 62.5 62.0 93.6 0.75 70.9 67.6 97.9 0.80 61.3 60.3 100.0 0.75 - - - - 91.3 87.8 94.7 0.91 80.0 75.0 90.0 0.82 72.5 65.1 100.0 0.79 58.2 58.2 100.0 0.74 100.0 100.0 100.0 1.0 - - - - 91.3 86.0 97.4 0.91 96.3 95.1 97.5 0.96 87.5 83.0 95.1 0.89 80.0 71.2 97.4 0.82 72.2 64.4 97.4 0.78 96.3 95.2 97.6 0. - - - - 91.3 90.0 92.1 0.91 90.8 97.0 84.2 0.90 96.3 95.2 97.6 0.96 73.7 69.2 100.0 0.82 71.3 69.4 67.2 67.1 100.0 100.0 0.80 0. Table 5: Performance of LLM-based judges across different model backends. 30 B.7 Judge Failure Examples Judge Failure Example 1: Toy Case Problem: Let a, b, be three non-negative real numbers. Find the largest constant such that the following inequality holds for all a, b, 0: (cid:0)a2 + 1(cid:1) (cid:0)b2 + 1(cid:1) (cid:0)c2 + 1(cid:1) (ab + bc + ca abc)2 + abc Response snippet: Let = = = t, the inequality becomes: After expanding the both sides and dividing both sides by t3 (for > 0): (t2 + 1)3 (3t2 t3)2 + Ct3 6t2 6t + 3 + 1 t3 Now, lets minimize (t) = 6t2 6t + 3 + 1 t3 for > 0. ... (Checking the minimum) So, at = 1, (1) = 4 is indeed the minimum. From the above, for all other values of t, (t) > 4, so the final answer is = 4 . Expected evaluation for the judge: The solution applies toy-case simplification by assuming = = = t, and analyzes (t) to draw conclusions about the general case. However, using the symmetric case alone does not guarantee correctness for arbitrary a, b, c, as it only examines one case in the domain. This reasoning lacks general validity. Therefore, the answer is False . Wrong evaluation from the judge: The solution applies general calculus-based method by reducing to the symmetric case = = = t. It computes (t) to find the global minimum of (t). This is logical solid. The response didnt use any special case to draw the final conclution. Therefore, the answer is True . Expert comment: The model incorrectly treats rigorous analysis within symmetric toy case (i.e., = = c) as general proof for all a, b, 0 (red-highlighted part). While the calculus applied to the reduced case is mathematically sound, it only verifies the inequality along one-dimensional slice of the domain. The failure to recognize this as toy-case argument highlights current limitation of the toy-case judge. Future work should focus on distinguishing between mathematically valid special-case analysis and truly general reasoning in inequality proofs. 31 Judge Failure Example 2: Logical Gap Problem: Let x, y, [0, 1]. Determine the minimal constant such that the following inequality holds for all x, y, z: x(x y)(z x) C. Solution snippet: With out loss of generality, we can assume that = = 1. Then, (x, 1, 1) = x(x 1)2. Take derivative: Set to zero: Quadratic formula: dx (x3 + 2x2 x) = 3x2 + 4x 3x2 + 4x 1 = 0 = 3x2 4x + 1 = 0 4 = 16 12 6 = 4 2 6 = 1, 1 3 3 , 1: Check at = 0, 1 - = 0: 0 - = 1 3 ( 1 3 - = 1: 1(1 1)2 = 0 3 1)2 = : 1 3 ( 2 3 )2 = 1 3 4 9 = 4 So, maximum is 0, minimum is 4/27. The answer is = 4 27 . Expected evaluation for the judge: The solution begins with \"without loss of generality\" (WLOG) assumption by setting = = 1, but provides no justification that this restriction preserves generality over all x, y, [0, 1]. This is non-trivial reduction and cannot be accepted without proof or argument showing the extremal case occurs under such setting. As result, there is logical gap at the very first step. Therefore the answer is False . Wrong evaluation from the judge: The response finds the maxima of the expression on [0,1] by taking derivatives and checking endpoint. All derivative steps are shown, all critical points are checked, and no unsupported non-trivial claim is made. Therefore the answer is True . Expert comment: The model fails to question the initial assumption = = 1 made under \"without loss of generality\" claim. It incorrectly accepts this reduction as valid and proceeds to verify the logical steps under that restricted case only (red-highlighted part). However, the validity of the WLOG assumption is itself non-trivial and requires justification. Ignoring this unjustified narrowing of the domain represents key limitation of the current logical gap judgeit focuses only on deductive soundness within given case, while overlooking whether the case itself is validly chosen. Addressing such domain-level reasoning gaps remains an important direction for future work. 32 Judge Failure Example 3: Numerical Approximation Problem: Let x, y, be positive real numbers such that xy + yz + zx 3. Find the largest constant such that the following inequality holds for all x, y, satisfying the given constraint: 4x + 5y + 4y + 5z + 4z + 5x Solution snippet: ... Therefore, our inequality becomes: After Simplifying, we can get: Lets calculate this value: (cid:115) 3 3 abc 779abc (cid:115) 3 3 1 779 3 (7791/6) 1. Therefore, the largest constant that satisfies the inequality is approximately 1.0845. The answer is = 1.0845 . Expected evaluation for the judge: The solution replaces an exact symbolic expression with numerical approximation (e.g., 3 7791/6 1.0845) and uses that approximation as the final value for C. This constitutes form of approximate reasoning, since it treats the numerical value as if it were an exact bound and uses to derive the final answer. Therefore, the reasoning is not fully rigorous, and the answer is False . Wrong evaluation from the judge: Providing decimal approximation for the final constant is acceptable, because it is not used to support further reasoning or intermediate steps in the proof. Since the approximation serves only as restatement of the exact result, it does not compromise the rigor of the overall argument. Therefore, the answer is True . Expert comment: The model incorrectly assumes that providing numerical approximation for the final answer does not constitute further use of approximation in reasoning (red-highlighted part). However, in this case, the decimal value is not merely illustrativeit is used as the final reported value of C, effectively replacing the exact symbolic expression. The judge fails to distinguish between benign approximations used for interpretation and those used as part of the core logical argument. This reflects current limitation of the Numerical Approximation Judge, and addressing this distinction should be focus of future work. 33 Judge Failure Example 4: Numerical Calculation Problem: Let a, b, be positive real numbers such that + + = 3. Determine the maximal constant such that the following inequality holds for all a, b, c: 1 + 2b3 + 1 + 2c3 + 1 + 2a3 C. Solution snippet: ... Lets check some special cases: - = = 1.4, = 3 2.8 = 0.2 - 1 + 2t3 = 1 + 2 (1.4)3 = 1 + 2 2.744 = 1 + 5.488 = 6.488 - 1 + 2c3 = 1 + 2 (0.2)3 = 1 + 2 0.008 = 1 + 0.016 = 1.016 So, the sum is: 0.216 + 1.378 + 0.0308 1.625 As for another cases, - = = 0.5, = 2 - 1 + 2t3 = 1 + 2 0.125 = 1.25 - 1 + 2c3 = 1 + 2 8 = 17 - = 0.4 + 0.0294 + 1.6 = 3.1294 Expected evaluation for the judge: <Extracted Expressions>: 1. 1 + 2 (1.4)3 = 1 + 2 2.744 2. 1 + 2 2.744 = 1 + 5.488 3. 1 + 5.488 = 6.488 4. 0.216 + 1.378 + 0.0308 1.625 5. 1 + 2 0.125 = 1.25 6. 1 + 2 8 = 17 7. = 0.4 + 0.0294 + 1.6 = 3.1294 <Codes>: (Correct codes for verifying the listed expressions above.) <Answer>: False Wrong evaluation from the judge: <Extracted Expressions>: 1. 1 + 2 (1.4)3 = 1 + 2 2.744 <Codes>: (Correct codes for verifying the listed expressions above.) <Answer>: True Expert comment: The red-highlighted part contains an incorrect computation, but the numerical computation judge fails to extract the corresponding expression for verification. This highlights limitation of our current system: although the judge can correctly evaluate expressions once they are identified, its inability to extract certain arithmetic computationsespecially when embedded in multi-line or composite expressionsprevents it from detecting numerical errors. This extraction gap limits the systems overall reliability. Addressing this limitation is an important goal for future work."
        },
        {
            "title": "C Experimental Details for Inequality Solving",
            "content": "C.1 Experimental Setups We design task-specific prompts for the two problem types in IneqMath: bound problems and relation problems. These prompts guide models to produce clear, rigorous reasoning steps and provide answers in consistent, machine-parsable format. The query formats are shown below. Query Prompt for Bound Problems in IneqMath Task description: Please solve the problem with clear, rigorous, and logically sound steps. At the end of your response, state your answer in exactly this format: The answer is = X, where is your calculated numerical bound value. Example: The answer is = 1. Problem: {bound_problem} Query Prompt for Relation Problems in IneqMath Task description: Please solve the problem with clear, rigorous, and logically sound steps. At the end of your response, state your answer in exactly this format: The answer is (Letter) Symbol, where Letter is one of the given options. Example: The answer is (A) . Problem: {relation_problem} We evaluate diverse set of 29 leading LLMs, as listed in Table 6. Each model is accessed via its official API using standardized decoding parameters. By default, we set the maximum token output to 10,000 (via max_tokens=10K), temperature to 0.0, and top_p to 0.99, for all models where these settings are applicable. For reasoning models, the default reasoning effort is chosen as medium. Model-specific parameters are specified in the table. 35 # Model Name Model Engine Name Source Unique Params 1 2 3 4 5 6 7 8 9 Gemma-2B [51] Gemma-2-9B [51] Llama-4-Maverick [36] Llama-4-Scout [37] Llama-3.1-8B [1] Llama-3.2-3B [2] Qwen2.5-Coder-32B [24] Qwen2.5-7B [48] Qwen2.5-72B [47] Open-source Chat LLMs Link gemma-2b-it Link gemma-2-9b-it Llama-4-Maverick-17B-128E-Instruct-FP8 Link Link Llama-4-Scout-17B-16E-Instruct Link Llama-3.1-8B-Instruct-Turbo Link Llama-3.2-3B-Instruct-Turbo Link Qwen2.5-Coder-32B-Instruct Link Qwen2.5-7B-Instruct-Turbo Link Qwen2.5-72B-Instruct-Turbo max_tokens=6K max_tokens=6K - - - - - - Proprietary Chat LLMs 10 Gemini 2.0 Flash [16] 11 Gemini 2.0 Flash-Lite [17] 12 GPT-4o [39] 13 GPT-4o mini [40] 14 GPT-4.1 [42] 15 Grok 3 [60] gemini-2.0-flash gemini-2.0-flash-lite gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gpt-4.1-2025-04-14 grok-3-beta Open-source Reasoning LLMs 16 DeepSeek-R1 [11] DeepSeek-R1 17 DeepSeek-R1 (Llama-70B) [9] DeepSeek-R1-Distill-Llama-70B 18 DeepSeek-R1 (Qwen-14B) [10] DeepSeek-R1-Distill-Qwen-14B 19 Qwen3-235B-A22B [49] 20 QwQ-32B [3] 21 QwQ-32B-preview Qwen3-235B-A22B-fp8-tput QwQ-32B QwQ-32B-Preview 22 Claude 3.7 Sonnet [4] 23 Gemini 2.5 Flash [18] 24 Gemini 2.5 Pro [19] 25 Grok 3 mini [61] 26 o1 [41] 27 o3 [44] 28 o3-mini [43] 29 o4-mini [44] Proprietary Reasoning LLMs claude-3-7-sonnet-20250219 gemini-2.5-flash-preview-04-17 gemini-2.5-pro-preview-03-25 grok-3-mini-beta o1-2024-12-17 o3-2025-04-16 o3-mini-2025-01-31 o4-mini-2025-04-16 Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link max_output_tokens=10K max_output_tokens=10K - - - - - - - - - - - max_output_tokens=10K max_output_tokens=10K max_completion_tokens=10K max_completion_tokens=10K max_completion_tokens=10K max_completion_tokens=10K Table 6: List of LLMs evaluated in our experiments. Default parameters are max_tokens=10K, temperature=0, and top_p=0.99 (if applicable for the respective model); model-specific parameters are specified in the table. 36 C.2 Model Failure Solution Examples C.2.1 Model Failure Solution Example 1 Failure Example 1: Correct Answer with Incorrect Steps (Toy Case) Problem: Let a, b, be positive real numbers. Consider the following inequality: + ab + 3 3 abc (cid:114) 3 () + 2 + + 3 Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above Ground truth: (A) Concise solution: Applying AM GM we get (cid:114) 3 ab + 2 3(cid:113) ab ab = ab Using the AM GM three times and add them together, we obtain 3 (cid:114) 1 2 + 3 + + (cid:32) (cid:114) + 3 ab + 3 + 2 (cid:33) abc 3 i.e. + ab + 3 abc (cid:114) + 3 ab + 3 (cid:114) abc 3 + 2 + 2 + + 3 Therefore, the answer is (A) . Response by GPT-4.1: Lets try to find relation or test with specific values: <Test 1>: = = = 1 + LHS : 1 1 + 3 3 Equality holds for = = c. <Test 2>: = 1, = 4, = 16 1 1 1 (cid:114) = 1, RHS : 1 1 + 1 2 1 + 1 + 1 3 = 1. LHS = 1 + 2 + 4 3 = = 3 (cid:114) 686 54 7 3 (cid:114) , RHS = 1 7 = 3 (cid:114) 35 2 = 3 (cid:114) 945 54 5 So, (Trying other special cases with LHS < RHS ...) LHS < RHS. Therefore, the numerical tests strongly support: LHS RHS. So, the answer is (A) . Expert comment: The model arrives at the correct answer by testing special cases like = = = 1 and = 1, = 4, = 16 (red-highlighted part), observing that LHS RHS numerically. However, it does not provide general proof or inequality-based reasoning to justify the result. This means the model is guessing the inequality direction based on limited examples, which is methodologically invalid despite yielding the correct final answer. C.2.2 Model Failure Solution Example 2 Failure Example 2: Correct Answer with Incorrect Steps (Logical Gap) Problem: Let a, b, c, d, be positive real numbers such that + + + + = 5. Find the minimal constant such that the following inequality holds for all a, b, c, d, satisfying the given constraint: abc + bcd + cde + dea + eab C. Ground truth: = 5 Concise solution: Without loss of generality, we may assume that = min{a, b, c, d, e}. By AM GM , we have abc + bcd + cde + dea + eab = e(a + c)(b + d) + bc(a + e) (cid:18) + + + 2 (cid:19)2 + (cid:18) + + + (cid:19)3 = + (5 2e)3 27 e(5 e)2 4 5(e 1)2(e + 8) 108 = 5 5 Equality holds if and only if = = min{a, b, c, d, e} = 1, i.e. = = = = = 1. Therefore, the answer is = 5 . Response by o4-mini: By symmetry under the cyclic shift (a, b, c, d, e) (cid:55) (b, c, d, e, a), any interior maximizer must have Since + + + + = 5, this forces = = = = e. = = = = = 1. At this point, = 5 (1 1 1) = 5. Since we know what the global maximum is reached when every variables are the same (i.e. a=b=c=d=e), the global maximum of under + + + + = 5 is attained at the interior point = = = = = 1, and its value is Therefore, the answer is = 5 . = 5. Expert comment: The model incorrectly assumes that symmetry implies the maximum is always achieved in every cases (red-highlighted part). This is not generally true and requires proof in each case. In this problem, the model happens to guess correctly that the global maximum occurs at = = = = e, but the reasoning is unjustified and relies on flawed heuristic rather than rigorous argument. C.2.3 Model Failure Solution Example 3 Failure Example 3: Correct Answer with Incorrect Steps (Numerical Approximation) Problem: Let a, b, be positive real numbers such that + + = 3. Consider the inequality: a2 + 2b3 + b2 + 2c3 + c2 + 2a () 1. Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above Ground truth: (B) Concise solution: Applying AM GM gives us a2 + 2b3 = 2ab3 + 2b3 2ab3 3 3 ab4 = 2ba2/3 3 Analogously, we can got a2 + 2b2 + b2 + 2c2 + c2 + 2a2 (a + + c) (a + + c) (a + + c) 2 3 2 3 2 ba2/3 + cb2/3 + ac2/3(cid:17) (cid:16) b(2a + 1) + c(2b + 1) + a(2c + 1) 3 3(a + + c) + 2(a + + c)2 9 = Response by o3-mini: Step 1. Check the equality case. Take = = = 1, we have: a2 + 2b3 + b2 + 2c3 + c2 + 2a3 = 1 3 + 1 3 + 1 3 = 1 This suggests that if an inequality holds, it must achieve equality at = = = 1. Step 2. Test an unbalanced case. Take = 2.5, = 0.4, = 0.1, so + + = 3. a2 + 2b3 = b2 + 2c3 = c2 + 2a3 = (2.5)2 2.5 + 2 (0.4)3 = (0.4)2 0.4 + 2 (0.1)3 = (0.1)2 0.1 + 2 (2.5)3 = 6.25 2.5 + 2 0.064 = 6.25 2.628 2.377. 0.16 0.4 + 0.002 = 0.16 0.402 0.397. 0.01 0.1 + 2 15.625 = 0.01 31.35 0.00032. Adding all three terms: We conclude that: 2.377 + 0.397 + 0.00032 2.774 > 1. a2 + 2b3 + for all positive a, b, with + + = 3, with equality only when = = = 1. Therefore, the final answer is (B) . b2 + 2c3 + + 2a3 1 c2 Expert comment: The model not only uses special cases to judge the inequality but also relies on numerically summing approximate valuesan unsound mathematical approach. This method suffers from precision loss when summing approximations (see red-highlighted part), especially when the result is near 1. While the sum in this case is clearly greater than 1, such reasoning can be misleading in borderline cases. Thus, the models conclusion lacks rigorous justification. 39 C.2.4 Model Failure Solution Example Failure Example 4: Correct Answer with Incorrect Steps (Numerical Computation) Problem: Let a, b, be the lengths of the sides of triangle, such that + + = 3. Consider the following inequality: a2 + b2 + c2 + 4abc 3 () 13 Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above Ground truth: (B) Concise solution: Let = + y, = + and = + x, we have + + = 3 2 Since AM GM , we get . xyz (cid:19) (cid:18) + + 3 = 1 8 . Now we obtain a2 + b2 + c2 + 4abc 3 = = (cid:0)a2 + b2 + c2(cid:1) (a + + c) + 4abc 3 (cid:0)(x + + z)3 xyz(cid:1) 4 4 3 (cid:32)(cid:18) 3 2 (cid:19)3 (cid:33) 1 = 13 3 Equality occurs if = = z, i.e. = = = 1. Therefore, the answer is (B) . Response by GPT-4o: Let = + y, = + z, = + x. Then + + = 2(x + + z) = 3 + + = 3 2 . By the AMGM inequality, xyz (cid:19)3 (cid:18) + + (cid:19)3 = (cid:18) 1 2 = 1 6 . Now compute: a2 + b2 + c2 + 4abc 3 = 4 3 (cid:0)(x + + z)3 xyz(cid:1) (cid:19)3 (cid:32)(cid:18) 3 2 4 3 1 6 (cid:33) = 79 18 > 13 3 Finally, test equality case = = = 1. Then: a2 + b2 + c2 + 4abc 3 = 3 + 4 3 = 13 3 . So equality holds when = = = 1. Therfore, the answer is (B) . Expert comment: The numerical mistake is that (cid:0) 1 (red-highlighted part) instead 2 of the correct 1 . This overestimates the final value of the expression. However, the direction of 8 the inequality remains correct purely by coincidence. In this case, the miscalculation happens to not affect the inequality direction, but the reasoning itself is not mathematically sound. (cid:1)3 was incorrectly computed as 1 6 40 C.2.5 Model Failure Solution Example 5 Failure Example 5: Incorrect Answer with Incorrect Steps Problem: Let a, b, be non-negative real numbers. Consider the following inequality: 4b2 + bc + 4c2 + 4c2 + ca + 4a2 + 4a2 + ab + 4b2 () 1 + + . Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above Ground truth: (B) Concise solution: By the Cauchy-Schwarz inequality we have 4b2 + bc + 4c2 (a + + c)2 4a (b2 + c2) + 4b (c2 + a2) + 4c (a2 + b2) + 3abc (cid:88) cyc Based on Schurs inequality, So, Then, a3 + b3 + c3 + 3abc (cid:0)b2 + c2(cid:1) + (cid:0)c2 + a2(cid:1) + (cid:0)a2 + b2(cid:1) (a + + c)3 4a (cid:0)b2 + c2(cid:1) + 4b (cid:0)c2 + a2(cid:1) + 4c (cid:0)a2 + b2(cid:1) + 3abc (a + + c)2 4a (b2 + c2) + 4b (c2 + a2) + 4c (a2 + b2) + 3abc 1 + + Equality holds when a=b=c. Therefore, the answer is (B) . Response by GPT-4o mini: Case 2: = 2, = 1, = 0 LHS = 2 4 + 0 + 0 + 1 0 + 0 + 16 + 0 = 1 2 + 1 16 = 9 16 , RHS = 1 2 + 1 + 0 = 1 3 . So LHS > RHS. Case 3: = 0, = 1, = 2 LHS = 0 + 2 16 + 2 8 = 1 16 + 1 4 = 5 16 , RHS = 1 0 + 1 + 2 = 1 3 . So LHS < RHS. Conclusion: One case gives >, one gives <. Therefore, the inequality does not always hold in single direction. Therefore, the answer is (F) None of the above Expert comment: The models conclusion is incorrect due to numerical error in Case 3. Specifically, the term 2 16 was mistakenly written as 1 (red-highlighted part), which caused the left-hand side (LHS) to be 16 underestimated. As result, the model falsely concluded that LHS < RHS in that case, leading it to believe the inequality does not always hold in one direction. In fact, correcting this error restores the inequality LHS RHS, consistent with the correct answer (B) . 41 C.3 Taking Annotated Theorems as Hints Prior studies, such as TheoremQA [7] and LeanDojo [66], show that explicitly providing relevant theorems aids LLMs in mathematical reasoning. To quantify this benefit on IneqMath, we evaluated models on 200 training problems where the annotated golden theorems were provided as hints. Results (Figure 12) reveal consistent uplift in overall accuracy across models, with gains reaching up to 11% (e.g., for o3-mini), alongside moderate improvements in answer accuracy (Figure 13). Figure 12: Model performance with annotated theorems as hints (Overall Accuracy). Figure 13: Model performance when taking annotated theorems as hints (Answer Accuracy). The following example illustrates how providing LLMs with access to correct theorems significantly improves their capacity to generate proofs with robust logical structures and step-wise soundness. In an initial attempt, the o4-mini model arrived at the correct answer by substituting two sets of special values; however, this method resulted in flawed step-wise solution. When subsequently equipped with the golden theorem, the model successfully generated correct solution through its proper application. 42 Taking Annotated Theorems as Hints: Example Problem: Let a, b, be the lengths of the sides of triangle, such that + + = 3. Determine the largest constant such that the following inequality holds for all a, b, c: a2 + b2 + c2 + 4abc 3 C. Related Theorem: Theorem Zdravko (Mean inequalities): Let a1, a2, . . . , an be positive real numbers. The numbers (cid:114) 1 + a2 a2 2 + + a2 QM = GM = a1a2 an , AM = a1 + a2 + + an and HM = 1 a1 + 1 a2 + + 1 an are called the quadratic, arithmetic, geometric and harmonic mean for the numbers a1, a2, . . . , an, respectively, and we have Equalities occur if and only if a1 = a2 = = an. QM AM GM HM Response by o4-mini without theorem hint (Incorrect response ): Let (a, b, c) = a2 + b2 + c2 + 4 3 abc. 1. Equilateral case: Let = = = 1. Then + + = 3, and (1, 1, 1) = 12 + 12 + 12 + 4 3 1 1 1 = 3 + 4 3 = 13 3 2. Degenerate case: Let = = 0.75, = 1.5. Then, (0.75, 0.75, 1.5) = 0.752 + 0.752 + 1.52 + 4 0.75 0.75 1.5 = 4.5 Thus, on the boundary we have (a, b, c) 4.5. more detailed check of the two-equal-sides slice = = x, = 3 2x shows that (x) attains its unique minimum 13/3 at = 1. No other interior configuration yields smaller value. Therefore, the answer is = 13 3 . (Continued) 43 Taking Annotated Theorems as Hints: Example (Continued) Response by o4-mini with theorem hit (Correct response ): Set + + = 3. By the quadraticarithmetic mean inequality (QMAM) in Mean inequalities theorem, a2 + b2 + c2 3 (cid:16) + + 3 (cid:17)2 = 1 = a2 + b2 + c2 3. By the AMGM inequality in Mean inequalities theorem, Hence abc + + 3 = 1 = abc 1. a2 + b2 + c2 + 4abc 3 3 + 4 1 3 = 3 + 4 3 = 13 3 . Equality occurs when = = = 1, which indeed form an equilateral triangle with sum 3. Thus the largest constant for which the inequality holds for all triangles is The answer is = 13/3 . = 13 3 . In the version without the theorem, Expert comment: the model relies on toy-case evaluationsspecifically symmetric and degenerate configurationsto guess the correct bound (red-highlighted part). In addition, its reasoning includes clear logical gap: it claims that more detailed check of the two-equal-sides slice = = x, = 3 2x shows the minimum occurs at = 1, but provides no explanation or computation to support this claim (red-highlighted part). This reveals that without access to the theorem, the model struggles to find valid proof strategy and falls back on incomplete heuristics. inequality theorems are provided, the model quickly applies the Once the relevant QMAM and AMGM inequalities in Mean inequalities theorem correctly (blue-highlighted part). It uses these tools to derive general lower bound valid for all triangles, leading rigorously to the correct constant = 13 . This contrast clearly demonstrates the value of theorem access 3 in enabling the model to reason with precision and mathematical completeness. 44 C.4 Retrieval as Augmentation Retrieving relevant theorems as hints. We also evaluate the impact of theorem-based hints on answer accuracy. This evaluation was conducted on the same 40-problem subset used in the main experiments, with models receiving the top-k most frequent theorems from the IneqMath training set as hints. As shown in Figure 14, providing one or two retrieved theorems tends to reduce final-answer accuracy for weaker models, such as Grok 3 mini and o3-mini. This drop is likely caused by misapplication or distraction from the core strategy, as the retrieved theorems may not align well with the problem at hand. Figure 14: Model performance when taking most frequent theorems as hints (Answer Accuracy). The following example demonstrates how supplying irrelevant theorems can mislead LLMs and degrade their reasoning quality. Initially, the o3-mini model approached the problem correctly using symmetric substitution and algebraic manipulation. However, after being provided with the Mean Inequality theoremunrelated to the actual solutionit abandoned its structured strategy and instead attempted to apply AMGM in Mean inequalities directly, without proper justification. This misstep led to an incorrect proof, emphasizing the importance of providing contextually relevant theorems rather than generic or loosely related ones. 45 Retrieving Relevant Theorems as Hints: Example Problem: Let a, b, be positive real numbers. Consider the following inequality: (cid:88) cyc + (cid:112)(a + b)(a + c) () 2 (cid:88) cyc 2a 2b + . Determine the correct inequality relation to fill in the blank. Options: (A) (B) (C) = (D) < (E) > (F) None of the above Response by o4-mini without theorems (Correct response ): First note the elementary two-term bound, valid for all positive u, v: 2u 2v + + 2u 2w + + (cid:112)(u + v)(u + w) = (cid:0)(cid:112)(u + v)(u + w) u(cid:1)2 (cid:112)(u + v)(u + w) (2v + w)(2w + v) > 0. If expanding and simplifying that difference you arrive precisely at + (cid:112)(u + v)(u + w) Now set (u, v, w) = (a, b, c), and sum the resulting three inequalities cyclically. (cid:105) On the right you get (cid:80) 2u 2v + 2u 2w + + < . (cid:104) 2a 2b+c + 2a 2c+b cyc = 2 (cid:80) 2a 2b+c cyc . While on the left you recover (cid:80) b+c . (a+b)(a+c) cyc Thus in full generality (cid:88) cyc + (cid:112)(a + b)(a + c) < 2 (cid:88) cyc 2a 2b + , Therefore, the answer is (D) < Irrelevant theorem provided: Theorem Zdravko (Mean inequalities): Let a1, a2, . . . , an be positive real numbers. The numbers (cid:114) 1 + a2 a2 2 + + a2 QM = GM = a1a2 an , AM = a1 + a2 + + an and HM = 1 a1 + 1 a2 + + 1 an are called the quadratic, arithmetic, geometric and harmonic mean for the numbers a1, a2, . . . , an, respectively, and we have Equalities occur if and only if a1 = a2 = = an. QM AM GM HM Response by o4-mini with theorem provided (Incorrect response ): First, when = = = 1, we have LHS = (cid:88) cyc + (cid:112)(a + b)(a + c) = 3, RHS = 2 (cid:88) cyc 2a 2b + = 4. Hence at = = c, LHS = 3 < RHS = 4. more detailed inequality-chasing via AMGM in Mean inequalities theorem shows the strict < persists for all positive a, b, c. Therefore, the answer is (D) < Expert comment: Without theorems, the model solves the problem accurately by constructing detailed inequality using symmetric substitution and algebraic manipulation. However, when given the Mean Inequality theoremwhich is unrelated to the problemit becomes confused. Instead of following the previous structured reasoning, it resorts to surface-level AMGM (Mean inequalities) without general proof or justification (red-highlighted part). This highlights the importance of supplying relevant theorems. 46 Retrieving training problems as demonstrations. Building on our observation that providing relevant theorems can enhance performance in inequality reasoning (5.4, C.3, C.4), we now investigate whether using training problems with step-wise solutions as demonstrations is similarly beneficial. For this study, we selected training problems whose solutions utilize the top-k most frequent theorems. As shown by the overall accuracy in Figure 15, Grok 3 minis performance improves from baseline of 10% (with no demonstration problem) to 13% when provided with one such problem. However, its accuracy drops sharply to 3% when two problems are used as demonstrations. Similarly, Gemini 2.5 Pro peaks at 53% accuracy with one demonstration problem, declining to 45% with two. o4-mini reaches 23% accuracy with one demonstration problem, 3% increase from its 20% baseline (without demonstrations). The answer accuracy, presented in Figure 16, exhibits similar instability. These varied outcomes suggest that while limited guidance can aid reasoning, an excess of demonstrations may overwhelm the model or exhaust its context capacity, leading to performance degradation. Figure 15: Model performance when taking example solutions associated with the top-k frequent theorems as hints (Overall Accuracy). Figure 16: Model performance when taking example solutions associated with the top-k frequent theorems as hints (Answer Accuracy). The following example showcases how well-matched questionsolution pair can help LLMs overcome initial reasoning flaws and adopt more rigorous proof techniques. The model originally relied on symmetry-based heuristics and invoked second-derivative arguments without computation, resulting in weak justification. After being presented with relevant example involving convexity and Jensens inequality, it successfully reproduced the key stepsverifying convexity analytically and applying the correct inequalities with full justification. This illustrates the potential of high-quality exemplars to steer models toward structured and mathematically sound reasoning. 47 Retrieving Training Problems as Demonstrations: Example Question: Let a, > 0 such that + = 1. Determine the largest constant such that the following inequality holds for all a, b: (cid:18) + 1 (cid:19)2 (cid:18) + + (cid:19)2 1 C. Ground truth: = 25 2 Response by o4-mini without examples (Incorrect response ): Define (a) = (cid:0)a + 1 (1 a) + 1 1a 1. The function is symmetric about = 1 2 2. At = 1 4 = 25 2 . 2 , we have (cid:0) 1 (cid:1) = 2 25 0 < < 1. (cid:17) (cid:1)2 + (cid:16) 2 , . Hence any interior extremum must occur at = 1 . 3. Checking the second derivative shows this is indeed minimum. The answer is = 25 2 . Example questions:Let a, b, be positive real numbers such that a2 + b2 + c2 = 3abc. Find the largest constant such that the following inequality holds for all a, b, satisfying the given constraint: Example solution: The given inequality is equivalent to b2c2 + c2a2 + a2b2 + + (cid:0)a3 + b3 + c3(cid:1) (a + + c) 9a2b2c2 Applying the Cauchy-Schwarz inequality we have (cid:0)a3 + b3 + c3(cid:1) (a + + c) (cid:0)a2 + b2 + c2(cid:1)2 Since a2 + b2 + c2 = 3abc we obtain (cid:0)a3 + b3 + c3(cid:1) (a + + c) (cid:0)a2 + b2 + c2(cid:1)2 = (3abc)2 = 9a2b2c2 Equality holds if and only if = = = 1. Therefore, the answer is = 9 . Response by o4-mini with examples (Correct response ): Let a, > 0 with + = 1. Define the function (a) = = 1 a. (cid:17)2 (cid:17)2 (cid:16) (cid:16) , + 1 + (cid:16) + 1 (cid:17)2 Convexity argument: consider the function h(x) = Compute its second derivative: h(x) = x2 + 2 + 1 x2 , Hence is convex on (0, ). Since and satisfy + = 1, by Jensens inequality for the convex function we have h(a) + h(b) 2 (x > 0). h(x) = 2 + 6x4 > 0, (cid:16) a+b + 1 > 0 2 + 2 25 4 (cid:16) 1 2 (cid:16) 5 = (cid:16) 1 (cid:17)2 (cid:17)2 = = = (cid:17) (cid:17) . Thereforef (a) = h(a) + h(b) 2 4 = 25 2 ., i.e. The answer is = Expert comment: The initial solution lacks rigor: it incorrectly assumes symmetry implies unique minimum, and references the second derivative without computing it (red-highlighted part). However, after being provided with related example that uses careful convexity argument and the Cauchy-Schwarz inequality, the model is able to imitate the correct reasoning. It verifies convexity via the second derivative, correctly applies Jensens inequality, and explicitly justifies the minimum (blue-highlighted part). This shows that with the right examples, the model can internalize and reproduce rigorous proof techniques. 25 2 . C.5 Self-improvement via Critic as Feedback In addition to overall accuracy, we also evaluate answer accuracy within the same self-critique setup. Using 40 randomly selected problems from the IneqMath benchmark, we assess whether one round of self-revision improves the correctness of final answers. As Figure 17 shows, models like o3-mini and o4-mini gain 25% in answer accuracy after revision. This result further supports self-critique as lightweight and supervision-free approach to improve solution reliability in inequality problems. Figure 17: Model performance when taking one round of critic as feedback (Answer Accuracy). The following example illustrates how self-critique can help LLMs identify and correct subtle reasoning flaws. In the initial attempt, the model arrives at the correct numerical result but relies on unjustified assumptions. After generating self-critique, it recognizes the logical gaps and revises its solution by explicitly verifying key conditionsleading to more rigorous and mathematically sound argument. This demonstrates that even single round of self-feedback can meaningfully improve the quality of reasoning. Self-improvement via Critic as Feedback: Example Problem: Let a, > 0 such that + = 1. Determine the largest constant such that the following inequality holds for all a, b: (cid:18) + 1 (cid:19)2 (cid:18) + + (cid:19)2 1 C. Ground truth: = 25 Initial response by o4-mini (Incorrect response ): Define (a) = (cid:0)a + 1 (1 a) + 1 1a 1. The function is symmetric about = 1 2 2. At = 1 4 = 25 2 . 2 , we have (cid:0) 1 (cid:1) = 2 25 (cid:17)2 (cid:1)2 + (cid:16) 2 , 0 < < 1. . Hence any interior extremum must occur at = 1 2 . 3. Checking the second derivative shows this is indeed minimum. The answer is = (Continued) 25 2 . 49 Self-improvement via Critic as Feedback: Example (Continued) Self-critic feedback by o4-mini: These are what make this solution incompletely justified: symmetry alone guarantees (1/2) = 0, but it does not by itself rule out other critical points = 1/2. In general, symmetric 1-function can have multiple stationary points symmetrically placed about the center. You assert checking the second derivative shows this is indeed minimum, but you never exhibit (1/2) or argue it is positive. Refined solution by o4-mini (Correct response ): Define the function (a) = (cid:0)a + 1 for > 0. Then (a) = g(a) + g(1 a). (1 a) + 1 1a (cid:1)2 + (cid:16) (cid:17)2 , for (0, 1). Let g(x) = (cid:0)x + (cid:1)2 Compute: g(x) = x2 + 2 + 1 x2 , g(x) = 2x 2 x3 , g(x) = 2 + 6 x4 > 0. Hence, is strictly convex on (0, ). Since (a) = g(a) + g(1 a) and each term is positive for (0, 1), we have (a) > 0. Thus, is strictly convex on (0, 1) strictly convex 1-function on an open interval has at most one critical point, which, if it exists, is its global minimum. We compute: (a) = g(a) g(1 a) = By symmetry, we can solve it and get = 1 2 = 1 2 . Therefore, the smallest possible value is = (cid:19) (cid:18) 1 2 = 25 2 . . Hence the unique solution of (a) = 0 in (0, 1) is Expert comment: The initial solution gets the correct value = 25 , but lacks rigor: 2 it incorrectly assumes symmetry implies unique minimum, and references the second derivative without computing it (red-highlighted part). The critic accurately points out these gaps. The refined solution corrects them by proving convexity through second derivatives and is the unique minimizer through first derivatives (blue-highlighted part). confirming = 1 2 This demonstrates that self-criticism effectively improves the mathematical soundness of the argument."
        },
        {
            "title": "D Limitations",
            "content": "While our work introduces novel dataset and evaluation judges for LLM-based inequality proving, we acknowledge several limitations that warrant discussion and offer avenues for future research. Potential for data contamination. Although we took significant measures to mitigate data leakage by commissioning novel test problems curated by experts, keeping ground truth answers private, and utilizing an online leaderboard for evaluation, residual risk of contamination remains. LLMs possess vast training corpora, and it is possible they have encountered problems with similar structures or underlying principles during pre-training, potentially inflating performance beyond true generalization capabilities. Our expert curation and review process aimed to minimize this, but perfect isolation from prior knowledge is challenging to guarantee. Training dataset scale and scope. The IneqMath training set, while meticulously curated with 1,252 problems featuring step-wise solutions, multiple solution paths, and theorem annotations, is modest in size compared to the massive datasets often used for pre-training or fine-tuning large models. We prioritized quality and depth (step-wise solutions, theorems) to the challenging Olympiad-level domain over sheer quantity. While sufficient for benchmarking current models, post-training, and exploring test-time techniques, this scale might be insufficient for training highly specialized models from scratch or for capturing the full diversity of inequality types. Future work could focus on scaling up the dataset while maintaining quality, potentially through community contributions. Inherent inaccuracies in LLM-as-judge evaluation. Our LLM-as-judge framework demonstrates high reliability on our development set (F1= 1.0 for final-answer judge, > 0.9 average for step-wise judges). However, while significantly more scalable than human expert evaluation, these judges are still imperfect. As illustrated by examples in B.7, they can occasionally misinterpret complex reasoning, overlook subtle logical flaws, or fail to correctly assess nuanced mathematical arguments. The current set of step-wise judges targets common failure modes but does not cover all possible error types, such as the correctness of complex symbolic transformations or the optimal choice of strategy. Potential improvements include using more powerful (but potentially more expensive) LLMs as judge backends (e.g., o3), developing specialized judges trained on annotated errors, or adding judges for specific mathematical operations like symbolic manipulation verification. Mitigation, not elimination, of answer guessability. The inclusion of step-wise judges significantly mitigates the issue of models guessing the correct final answer without sound reasoning. However, it does not eliminate this possibility entirely. model might still arrive at the correct bound or relation through chance or heuristics and support it with plausible-sounding, yet flawed, intermediate steps capable of misleading one or more judges. The requirement to pass all judges reduces this risk, but the fundamental challenge of distinguishing genuine mathematical insight from convincing yet spurious reasoning remains. Computational cost of evaluation. While more efficient than manual expert grading, our multijudge evaluation protocol is computationally more intensive than simple final-answer checking (e.g., string matching). Evaluating each solution requires multiple LLM inferences (one for the final answer, four for step-wise checks). This cost scales linearly with the number of models and problems being evaluated and could become factor in very large-scale benchmarking efforts."
        },
        {
            "title": "E Broader Impacts",
            "content": "This research focuses on advancing the mathematical reasoning capabilities of LLMs, specifically in the domain of inequality proving. While the work is primarily foundational and unlikely to lead directly to malicious applications such as disinformation or surveillance, potential negative societal impacts could arise from the misuse or misinterpretation of the technology. The most significant risk stems from over-reliance on LLM-generated proofs that may appear correct superficially (achieving high answer accuracy) but contain critical logical flaws, as demonstrated by the sharp drop in performance under our step-wise evaluation. If such flawed proofs were uncritically accepted in fields requiring mathematical rigor, such as scientific modeling, engineering design, or financial analysis, it could lead to incorrect conclusions, faulty systems, or economic miscalculations. Our contribution of 51 rigorous, step-wise evaluation methodology serves as potential mitigation strategy by promoting transparency and enabling the identification of fragile reasoning chains, thereby encouraging cautious deployment and emphasizing the need for verification, especially in high-stakes applications. The public release of the IneqMath benchmark further supports community efforts in understanding and improving the reliability of LLM reasoning."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Stanford University",
        "UC Berkeley"
    ]
}