{
    "paper_title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging",
    "authors": [
        "Salah Eddine Bekhouche",
        "Gaby Maroun",
        "Fadi Dornaika",
        "Abdenour Hadid"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical image segmentation is crucial for many healthcare tasks, including disease diagnosis and treatment planning. One key area is the segmentation of skin lesions, which is vital for diagnosing skin cancer and monitoring patients. In this context, this paper introduces SegDT, a new segmentation model based on diffusion transformer (DiT). SegDT is designed to work on low-cost hardware and incorporates Rectified Flow, which improves the generation quality at reduced inference steps and maintains the flexibility of standard diffusion models. Our method is evaluated on three benchmarking datasets and compared against several existing works, achieving state-of-the-art results while maintaining fast inference speeds. This makes the proposed model appealing for real-world medical applications. This work advances the performance and capabilities of deep learning models in medical image analysis, enabling faster, more accurate diagnostic tools for healthcare professionals. The code is made publicly available at \\href{https://github.com/Bekhouche/SegDT}{GitHub}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 5 9 5 5 1 . 7 0 5 2 : r SegDT: Diffusion Transformer-Based Segmentation Model for Medical Imaging Salah Eddine Bekhouche1[0000000155387407], Gaby Maroun1[0009000656081817], Fadi Dornaika1,2[0000000165819680], and Abdenour Hadid3[000000019092735X] 1 University of the Basque Country UPV/EHU, San Sebastian, Spain {sbekhouche001,gmaroun001}@ikasle.ehu.eus fadi.dornaika@ehu.eus 2 IKERBASQUE, Basque Foundation for Science, Bilbao, Spain 3 Sorbonne University Abu Dhabi, Abu Dhabi, UAE abdenour.hadid@sorbonne.ae Abstract. Medical image segmentation is crucial for many healthcare tasks, including disease diagnosis and treatment planning. One key area is the segmentation of skin lesions, which is vital for diagnosing skin cancer and monitoring patients. In this context, this paper introduces SegDT, new segmentation model based on diffusion transformer (DiT). SegDT is designed to work on low-cost hardware and incorporates Rectified Flow, which improves the generation quality at reduced inference steps and maintains the flexibility of standard diffusion models. Our method is evaluated on three benchmarking datasets and compared against several existing works, achieving state-of-the-art results while maintaining fast inference speeds. This makes the proposed model appealing for real-world medical applications. This work advances the performance and capabilities of deep learning models in medical image analysis, enabling faster, more accurate diagnostic tools for healthcare professionals. The code is made publicly available at GitHub. Keywords: Medical image segmentation Diffusion Transformer Skin lesion segmentation."
        },
        {
            "title": "Introduction",
            "content": "Skin cancer is major global health concern, and its early detection is crucial for improving survival rates. Medical image segmentation plays vital role in this process by enabling precise lesion boundary delineation. Among the successful approaches to automatic image segmentation is deep learning which has revolutionized the field, with Convolutional Neural Networks (CNNs) like UNet [18] and DeepLabV3+ [3] demonstrating interesting performance. However, CNNs can struggle with long-range dependencies, limiting their ability to segment complex or irregularly shaped lesions. As an alternative, Transformers [20], The supports of TotalEnergies and Sorbonne University Abu Dhabi are fully acknowledged (Abdenour Hadid). 2 SE. Bekhouche et al. inspired by their success in Natural Language Processing (NLP) and vision, offer an appealing solution by capturing global context through self-attention, with models like TransUNet [2] and Swin-UNet [1] showing improved accuracy (although their high computational cost can hinder real-world applications). Diffusion models, on the other hand, have recently emerged as powerful technique, achieving state-of-the-art results in various medical imaging tasks [21] by iteratively refining segmentation through denoising process. While highly accurate, their computational cost and long inference times pose challenge for real-world deployment. To address some of these limitations, we propose SegDT, an extra small Diffusion Transformer (DiT) model designed for efficient segmentation on low-cost GPUs. SegDT integrates rectified flow to accelerate inference while maintaining high segmentation accuracy. The main contributions of this work are: (i) compact DiT architecture optimized for resource-constrained GPUs is proposed; (ii) methodology for the incorporation of rectified flow for efficient inference with reduced sampling steps is described; and (iii) Extensive experiments are conducted on three benchmarking datasets, achieving state-of-the-art performances, compared to exiting methods."
        },
        {
            "title": "2 Related Work",
            "content": "This section discusses relevant prior work in medical image segmentation, focusing on CNNs, Transformers, hybrid architectures, and diffusion models, with an emphasis on approaches related to skin lesion segmentation and efficient architectures."
        },
        {
            "title": "2.2 Transformer-based Segmentation\nTransformers, initially developed for NLP [20], have recently shown remarkable\nsuccess in computer vision [7]. Their key strength lies in the self-attention mech-\nanism, which allows them to capture long-range dependencies and global context\neffectively. In medical image segmentation, TransUNet [2] was a pioneering work,",
            "content": "SegDT: Diffusion Transformer-Based Segmentation Model 3 combining the strengths of transformers and U-Nets. It utilizes transformer encoder to extract global features and U-Net decoder to preserve local details. While TransUNet demonstrated the potential of transformers, its computational complexity can be limiting factor. Swin-UNet [1] addressed this issue by introducing hierarchical Swin Transformer architecture with shifted windows, enabling efficient computation of self-attention. This approach balances local and global feature extraction, achieving competitive performance with reduced computational cost. However, transformer-based models can still be resourceintensive, motivating the development of more resourceefficient architectures."
        },
        {
            "title": "2.3 Hybrid Architectures: CNN-Transformers",
            "content": "Recognizing the complementary strengths of CNNs and transformers, researchers have explored hybrid architectures. DS-TransUNet [14] incorporates deformable self-attention into the TransUNet framework, allowing the model to focus on relevant regions in the image. BRAU-Net++ [13] combines convolutional feature extractors with transformer-based global reasoning, achieving state-of-the-art results on several benchmarks. These hybrid approaches aim to leverage the local feature extraction capabilities of CNNs and the global context modeling of transformers. While effective, these methods often come with increased complexity and resource requirements. MobileUNETR [17] takes different approach by focusing on efficiency. It proposes lightweight transformer-based model optimized for mobile and edge devices. While sacrificing some accuracy compared to larger models, it achieves faster inference, making it suitable for resource-constrained environments."
        },
        {
            "title": "2.4 Diffusion Models for Segmentation",
            "content": "Diffusion models have recently emerged as powerful tool for image generation and segmentation. They operate by progressively adding noise to an image until it becomes pure noise (forward diffusion) and then learning to reverse this process to generate the original image (reverse diffusion). In medical image segmentation, MedSegDiff [21] integrates transformers within diffusion framework, enabling the model to capture fine-grained anatomical details. MedSegDiff-V2 [22] further improves upon this by incorporating multi-resolution features. While these diffusion-based models have demonstrated impressive performance, their computational cost and long inference times can be limiting factor for real-time deployment. Our work addresses this limitation by introducing SegDT, Diffusion Transformer (DiT) model that incorporates rectified flow to accelerate inference, making it more suitable for real-world applications. Unlike previous diffusion models that often rely on U-Net architectures, we explore more efficient transformerbased design. Furthermore, we consider rectified flow approach to achieve highquality segmentations with fewer sampling steps, significantly reducing the inference time. This is key difference between our work and previous diffusion-based segmentation methods. 4 SE. Bekhouche et al."
        },
        {
            "title": "3 Proposed Method\nThis section details SegDT, our proposed architecture for segmenting skin lesions\nin medical images. Fig. 1 illustrates the SegDT architecture, which comprises a\nVariational Autoencoder (VAE) encoder, a DiT, and a VAE decoder. The VAE\ncomponents are based on the pretrained Tiny AutoEncoder for Stable Diffusion\n(TAESD)4, chosen for its compact size and computational efficiency. TAESD’s\nsmall footprint and fast encoding/decoding speeds contribute to SegDT’s overall\nefficiency by generating a compact latent representation, compressing the width\nand height 8 times. This smaller latent space allows for a more efficient DiT,\nreducing the computational burden and further accelerating the segmentation\nprocess. The DiT component is based on the DiT-XS (extra-small) variant, which\nemploys a patch size of 2, as detailed in [16]. DiTs have demonstrated strong\nperformance in image generation, making them a suitable foundation for our\nsegmentation task. SegDT is designed to address the challenges of precise lesion\nboundary delineation in noisy medical images - a critical requirement for accu-\nrate dermatological diagnosis and treatment planning. Furthermore, the model’s\ncompact architecture and the integration of rectified flow during inference enable\nrapid and efficient analysis by reducing the required number of inference steps.",
            "content": "Fig. 1: Overview of the SegDT inference architecture for medical image segmentation."
        },
        {
            "title": "3.1 Rectified Flow for Efficient Inference",
            "content": "In the forward diffusion process, segmentation label x0 (represented by its encoded latent representation z0 Z) is gradually perturbed by adding Gaussian noise over timesteps. This results in sequence of noisy segmentations x1, x2, . . . , xT (with corresponding noisy latent representations z1, z2, . . . , zT Z). The reverse diffusion process aims to reconstruct the original segmentation mask x0 (or z0) from the noisy xT (or zT ), conditioned on latent representation of the corresponding medical image. Inspired by efficient diffusion sampling techniques like those used in Denoising Diffusion Implicit Models (DDIMs) [19], we employ rectified flow approach to learn modified, more efficient reverse process. 4 https://github.com/madebyollin/taesd SegDT: Diffusion Transformer-Based Segmentation Model 5 Instead of directly predicting the noise added at each timestep, our model learns velocity field vθ(z, t, y) : [0, ] in the latent space Z. This velocity field represents the direction and magnitude of the change required to move from zt to zt1, conditioned on zt, the time step [0, ], and the latent representation of the image y. The reverse diffusion process then follows this learned velocity field. We approximate the reverse trajectory using numerical integration method, such as Eulers method. zt1 = zt + vθ(zt, t, y)t, (1) where is the size of the integration step. In our normalized formulation, we assume that the continuous time interval is [0, 1]. When this interval is divided into discrete timesteps of , each step corresponds to an increment of = 1 . The velocity field vθ is learned by minimizing loss function. Given noisy latent representation zt (obtained by adding noise to the encoded ground truth mask z0), the model predicts the velocity vθ(zt, t, y). This prediction is compared to target velocity derived from the known noise added to z0 to obtain zt. common metric for this comparison is the Mean Squared Error (MSE). This training process enables the model to accurately predict the velocity needed to reverse the diffusion process. Learning velocity field, rather than directly predicting noise, can be seen as form of rectification, promoting smoother and more predictable reverse It also provides increased flexibility for incorporating condidiffusion process. tioning information, such as the anatomical context provided by y. Although the concept of learning velocity field for diffusion has been explored (e.g., [15]), the specific term \"rectified flow\" and its precise definition are still evolving within the diffusion modeling community. Our use of this term emphasizes the goal of more direct, efficient path from noisy latent to segmentation."
        },
        {
            "title": "3.2 Transformer-Based Diffusion Model (SegDT)\nThis section describes the architecture of SegDT, a transformer-based diffusion\nmodel for image segmentation. SegDT employs a pre-trained VAE to map images\ninto a latent space. This latent representation is then processed by a DiT. SegDT\nemploys 12 DiT blocks.",
            "content": "During training, the input to the DiT consists of latent representations of ground truth segmentation masks. In contrast, during inference, the model generates segmentations by processing randomly sampled, noisy latent vectors initialized with fixed seed. Critically, the VAE decoder component is not utilized during training, as the loss function is computed directly within the latent space. This approach significantly reduces training time. The SegDT architecture comprises the following components: VAE Encoder pre-trained VAE encoder is employed to map input images of size into lower-dimensional latent space. During training, this encoder also processes corresponding ground-truth masks in the same manner. The resulting latent representation has dimensions , where = 4, 6 SE. Bekhouche et al. and the spatial dimensions are reduced by factor of 8, such that = H/8 and = W/8. This latent representation serves as the input to the DiT model. Patch Embedder patch embedding module, utilizing shared architecture for both input and conditional tensors, is applied. For the input, it processes the latent representation from the VAE encoder. During inference, when groundtruth masks are not available, it processes randomized tensor for the conditional input. The input tensor is flattened and then linearly projected to dimensional embedding space, where = 192 is the embedding dimension derived from the DiTs token length. The number of patches, S, is determined by = , where = 2 is the patch size used in the DiT model, and = 256 represents the total number of patches. 2 Time Step Embedder time step embedding module is used to generate D-dimensional vector (where = 192, consistent with the patch embedding dimension) that encodes the diffusion time step t. This embedding vector is then used to condition the DiT blocks, providing information about the current stage of the diffusion process. DiT Blocks series of DiT blocks form the core of SegDT. Each block receives the embedded latent representation z, the timestep embedding t, and the condition embedding as input. These blocks progressively refine the latent representation z. The detailed architecture of each DiT block is shown in Fig. 2. Each DiT block comprises: Adaptive Layer Normalization (adaLN): Normalizes activations and then modulates them using the parameters α1, α2 (for scaling), β1, β2 (for shifting), and γ1, γ2 (for scaling) to shift and scale the normalized activations. Self-Attention: Models global spatial relationships within the image embedding. Cross-Attention: Integrates contextual guidance from the condition embedding y. Feed-Forward Networks (FFN): Enhances feature representations. The FFN hidden layer size is determined by mlp_ratio hidden_size. GELU with an approximate tanh calculation is used as the activation function. DropPath Regularization: Applies DropPath regularization to prevent overfitting. Linear and Reshape After the DiT blocks, linear layer is applied to the SD output, transforming it to tensor of shape 8C . This linear transformation projects each patch embedding from dimension to new dimension of 8C (equivalent to 2C 2). Subsequently, this tensor is reshaped to 2C , aligning with the VAEs latent spatial dimensions (H ) but with twice the channel depth (2C instead of ). Within this 2C tensor, the first channels represent the predicted noise, and the subsequent channels represent the predicted variance, primarily used during training. SegDT: Diffusion Transformer-Based Segmentation Model 7 Fig. 2: Detailed architecture of single DiT block. VAE Decoder At inference time, after this denoising process, the models output 2C (only the first channels) is fed into pre-trained VAE decoder. This decoder then reconstructs an image from this representation, producing 3-channel image of size W . Finally, this reconstructed image is converted into the final segmentation mask through binarization."
        },
        {
            "title": "4.1 Datasets",
            "content": "The ISIC 2016 [10], 2017 [6], and 2018 [5] challenges are widely recognized benchmarks in medical image segmentation, specifically for skin lesion analysis. These challenges provided large, labeled datasets for detecting and segmenting melanoma from dermoscopic images. The ISIC 2016 dataset included 900 training images and 335 testing images, each with corresponding segmentation masks, allowing evaluation of lesion detection and boundary accuracy. The ISIC 2017 dataset expanded on this with 2000 training images, 150 validation images, and 600 testing images, all with segmentation masks. The ISIC 2018 dataset further increased in size, containing 2594 training images, 100 validation images, and 1000 testing images, ensuring more robust assessment of segmentation models."
        },
        {
            "title": "4.2 Metrics",
            "content": "The metrics widely used for evaluating skin lesion segmentation on these datasets include: Dice Similarity Coefficient (Dice), Intersection over Union (IoU), Pixel Accuracy (ACC), Sensitivity (SE), and Specificity (SP). These metrics assess the overlap and classification accuracy between predicted segmentation masks (SM) and ground truth masks (GM). 8 SE. Bekhouche et al. Dice = IoU = 2SM GM SM + GM SM GM SM GM ACC = + P + + + SE = P + = SM GM GM SP = N + (2) (3) (4) (5) (6) Where SM GM is the number of shared pixels, SM and GM are the total pixels in each mask, and TP, TN, FP, and FN are true positives, true negatives, false positives, and false negatives, respectively. 4."
        },
        {
            "title": "Implementation Details",
            "content": "SegDT was trained with Adam Optimizer with learning rate of 1e-4 and batch size of 32 for 100 epochs. The learning rate was reduced by factor of 10 after 50 epochs. During training, no explicit data enhancement techniques were used. The experiments were carried out on two NVIDIA RTX 3090 GPUs. The input images were bilinearly resized to achieve the target dimensions of the 256 256 pixels and then normalized to the [0, 1] range. In the model, these dimensions correspond to = 3, = 256, and = 256. The final segmentation mask was obtained by binarizing the reconstructed image using fixed threshold of 0.2. This threshold was selected based on performance on held-out validation set. We explored thresholds ranging from 0.1 to 0.5 in increments of 0.05 and selected 0.2 because it yielded the best overall performance in terms of the Dice score on the validation set. The mlp_ratio and hidden_size parameters for the DiT blocks were set to 4 and 192, respectively."
        },
        {
            "title": "4.4 Results and Discussion",
            "content": "To validate SegDTs accuracy and generalization, we conducted extensive experiments on the ISIC 2016, 2017, and 2018 skin lesion image datasets. We compared SegDTs performance with several state-of-the-art methods and analyzed the performance of different methods under different scenarios and challenging settings. Segmentation Performance Table 1 presents the results of the segmentation in the three ISIC datasets. SegDT achieves state-of-the-art or highly competitive performance across all datasets and most metrics. In ISIC 2016, SegDT achieves SegDT: Diffusion Transformer-Based Segmentation Model 9 the highest Dice score (94. 76%), IoU (91. 40%) and accuracy (97. 08%). Although DU-Net + achieves slightly higher sensitivity, SegDT demonstrates significantly higher specificity (99. 44%), indicating its ability to accurately identify healthy tissue, which is crucial in clinical settings. In ISIC 2017, SegDT again achieved competitive results, with the highest Dice score (91. 70%) and accuracy (95. 49%). Although DU-Net+ shows slightly better IoU and Sensitivity, SegDT maintains significantly higher Specificity (98.74%). In the largest dataset, ISIC 2018, SegDT achieves the highest Dice score (94. 51%) and IoU (90. 43%), and competitive accuracy and sensitivity. In particular, SegDT achieves the highest Specificity (97.43%), further highlighting its strength in identifying healthy tissue. Table 1: Segmentation results on the ISIC 2016, 2017, and 2018 datasets. Method Dataset Dice IoU ACC SE SP Flops (G) Params (M) - - MobileUNETR [17] GU-Net [4] AM-Net [24] DU-Net+ [12] SegDT MobileUNETR [17] GU-Net [4] PCCTrans [8] AM-Net [24] DU-Net+ [12] SegDT 92.80 87.47 96.59 93.03 96.87 2016 - 89.75 82.41 2016 93.29 88.44 95.57 88.73 97.02 2016 2016 92.46 85.25 96.89 95.75 95.64 2016 94.76 91.40 97.08 93.35 99.44 86.84 79.00 94.46 85.18 96.93 2017 - 93.94 88.98 2017 84.65 2017 93.28 87.30 80.02 95.13 81.70 97.99 2017 2017 90.46 85.20 95.54 94.07 93.85 2017 91.70 84.70 95.49 87.39 98.74 89.31 80.68 94.91 87.28 97.37 SynergyNet-8s2h [9] 2018 90.03 96.49 2018 PCCTrans [8] 90.10 84.01 95.61 92.24 91.18 2018 BRAU-Net++ [13] 90.74 84.56 94.40 92.55 95.03 2018 MobileUNETR [17] 92.42 86.46 - 2018 GU-Net [4] 92.93 88.13 97.41 95.12 96.76 2018 DU-Net+ [12] 2018 SLP-Net [23] 88.21 80.61 93.87 89.30 95.36 2018 94.51 90.43 96.81 95.21 97.43 SegDT - - - - - - - - - - 1.30 - 54.00 3.68 1.30 - 38.50 - 54.00 3.68 - 38.50 22.45 1.30 - 54.00 2.30 3.68 3.00 - 39.00 9. 3.00 - 50.80 - 39.00 9.95 - 50.80 50.76 3.00 - 39.00 0.20 9.95 The inclusion of GFLOPs and parameter counts in Table 1 highlights the efficiency of SegDT. Compared to DU-Net+, SegDT achieves significantly better performance with drastically fewer GFLOPs (3.68 vs. 54.00) and parameters (9.95M vs. 39.00M). This demonstrates SegDTs ability to achieve superior results with much lighter computational footprint, making it more suitable for resource-constrained environments. Although MobileUNETR and SLP-Net have lower GFLOPs and parameter counts, SegDT offers better balance between performance and efficiency. SegDTs performance is achieved with relatively small increase in computational cost compared to the most efficient models, but 10 SE. Bekhouche et al. with significant improvement in segmentation accuracy, especially in terms of specificity. This trade-off is often desirable in medical image analysis, where accuracy is paramount. SegDT offers compelling advantage in inference speed compared to IDDPM. SegDT achieves segmentation quality comparable to IDDPM with only 15 inference steps, while IDDPM requires 35 steps. This substantial reduction in steps, facilitated by SegDTs use of rectified flow, reduces the inference time by almost 2 times, making SegDT significantly faster and crucial for practical applications. Qualitative Analysis Figure 3 shows example segmentation results in the ISIC datasets, including both well-segmented cases and challenging cases. The left examples demonstrate SegDTs ability to accurately delineate lesion boundaries even with variations in shape, size, and texture. The challenging cases on the right highlight some limitations. For instance, in some cases, SegDT struggles with lesions that have irregular borders or are very small. This could be due to the limited receptive field of the transformer blocks or the difficulty in capturing fine-grained details in extremely small lesions. Further investigation is needed to address these limitations. Fig. 3: Segmentation results of SegDT on the ISIC datasets. The figure shows examples of well-segmented cases (left) and challenging cases (right). Each example consists of six columns: the first two are from ISIC 2016, the middle two from ISIC 2017, and the last two from ISIC 2018. Within each column, the first row shows the original image, the second row the ground truth segmentation mask, and the third row the predicted segmentation mask. The fourth row visualizes the segmentation on the original image, with the ground truth contour in green and the predicted mask contour in blue."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presented SegDT, novel DiT model for efficient medical image segmentation, specifically targeting skin lesion segmentation. SegDT leverages SegDT: Diffusion Transformer-Based Segmentation Model 11 compact DiT architecture and incorporates rectified flow for accelerated inference on resource-constrained GPUs. Our approach addresses the limitations of existing diffusion models, which often suffer from high computational costs and long inference times, hindering their practical application in real-world clinical settings. SegDT demonstrates its effectiveness on ISIC datasets (2016, 2017, 2018) by achieving competitive, state-of-the-art segmentation performance with significantly faster inference speeds. This efficiency, enhanced by the use of rectified flow, is crucial for rapid clinical analysis in real-world applications. Furthermore, SegDTs compact architecture enables deployment on low-cost GPUs, broadening the accessibility of advanced medical image segmentation and contributing to the democratization of cutting-edge diagnostic tools. Future work will focus on further optimizing SegDTs architecture and exploring additional techniques to enhance its performance and efficiency. We also plan to investigate the models generalizability to other medical image segmentation tasks and datasets. Furthermore, we will explore incorporating additional clinical information, such as patient metadata, to further improve segmentation accuracy and clinical utility. We believe that SegDT represents promising step towards the development of efficient and accurate medical image segmentation models for real-world clinical applications."
        },
        {
            "title": "References",
            "content": "1. Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swinunet: Unet-like pure transformer for medical image segmentation. In: European conference on computer vision. pp. 205218. Springer (2022) 2. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021) 3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence 40(4), 834848 (2018) 4. Cheng, D., Gai, J., Yang, B., Mao, Y., Gao, X., Zhang, B., Jing, W., Deng, J., Zhao, F., Mao, N.: Gu-net: Causal relationship-based generative medical image segmentation model. Heliyon 10(18) (2024) 5. Codella, N., Rotemberg, V., Tschandl, P., Celebi, M.E., Dusza, S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., et al.: Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368 (2019) 6. Codella, N.C., Gutman, D., Celebi, M.E., Helba, B., Marchetti, M.A., Dusza, S.W., Kalloo, A., Liopyris, K., Mishra, N., Kittler, H., et al.: Skin lesion analysis toward melanoma detection: challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In: 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018). pp. 168172. IEEE (2018) 7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is 12 SE. Bekhouche et al. worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020) 8. Feng, Y., Su, J., Zheng, J., Zheng, Y., Zhang, X.: parallelly contextual convolutional transformer for medical image segmentation. Biomedical Signal Processing and Control 98, 106674 (2024) 9. Gorade, V., Mittal, S., Jha, D., Bagci, U.: Synergynet: Bridging the gap between discrete and continuous representations for precise medical image segmentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 77687777 (2024) 10. Gutman, D., Codella, N.C., Celebi, E., Helba, B., Marchetti, M., Mishra, N., Halpern, A.: Skin lesion analysis toward melanoma detection: challenge at the international symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1605.01397 (2016) 11. Jha, D., Riegler, M.A., Johansen, D., Halvorsen, P., Johansen, H.D.: Resunet++: An advanced architecture for medical image segmentation. IEEE Access 7, 21455 21467 (2019) 12. Kaur, R., Ranade, S.K.: Du-net+: fully convolutional neural network architecture for semantic segmentation of skin lesions. Signal, Image and Video Processing 19(1), 152 (2025) 13. Lan, L., Cai, P., Jiang, L., Liu, X., Li, Y., Zhang, Y.: Brau-net++: U-shaped hybrid cnn-transformer network for medical image segmentation. arXiv preprint arXiv:2401.00722 (2024) 14. Lin, A., Chen, B., Xu, J., Zhang, Z., Lu, G., Zhang, D.: Ds-transunet: Dual swin transformer u-net for medical image segmentation. IEEE Transactions on Instrumentation and Measurement 71, 115 (2022) 15. Liu, X., Gong, C., Liu, Q.: Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003 (2022) 16. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF Inter. Conf. on Computer Vision. pp. 41954205 (2023) 17. Perera, S., Erzurumlu, Y., Gulati, D., Yilmaz, A.: Mobileunetr: lightweight endto-end hybrid vision transformer for efficient medical image segmentation. arXiv preprint arXiv:2409.03062 (2024) 18. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 234241. Springer (2015) 19. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020) 20. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) 21. Wu, J., Fu, R., Fang, H., Zhang, Y., Yang, Y., Xiong, H., Liu, H., Xu, Y.: Medsegdiff: Medical image segmentation with diffusion probabilistic model. In: Medical Imaging with Deep Learning. pp. 16231639. PMLR (2024) 22. Wu, J., Ji, W., Fu, H., Xu, M., Jin, Y., Xu, Y.: Medsegdiff-v2: Diffusion-based medical image segmentation with transformer. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38, pp. 60306038 (2024) 23. Yang, B., Zhang, R., Peng, H., Guo, C., Luo, X., Wang, J., Long, X.: Slp-net: An efficient lightweight network for segmentation of skin lesions. Biomedical Signal Processing and Control 101, 107242 (2025) 24. Yang, Z., Chen, R., Lin, C.: Am-net: network with attention and multi-scale feature fusion for skin lesion segmentation. IEEE Sensors Journal (2025)"
        }
    ],
    "affiliations": [
        "IKERBASQUE, Basque Foundation for Science, Bilbao, Spain",
        "Sorbonne University Abu Dhabi, Abu Dhabi, UAE",
        "University of the Basque Country UPV/EHU, San Sebastian, Spain"
    ]
}